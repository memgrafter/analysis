---
ver: rpa2
title: 'Aya Expanse: Combining Research Breakthroughs for a New Multilingual Frontier'
arxiv_id: '2412.04261'
source_url: https://arxiv.org/abs/2412.04261
tags:
- expanse
- multilingual
- languages
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Aya Expanse model family introduces 8B and 32B parameter multilingual
  language models that achieve state-of-the-art performance across 23 languages. The
  key innovations include multilingual data arbitrage for synthetic data generation,
  iterative multilingual preference training, and model merging.
---

# Aya Expanse: Combining Research Breakthroughs for a New Multilingual Frontier

## Quick Facts
- **arXiv ID**: 2412.04261
- **Source URL**: https://arxiv.org/abs/2412.04261
- **Reference count**: 16
- **Primary result**: Aya Expanse 32B achieves state-of-the-art multilingual performance, surpassing Llama 3.1 70B (twice its size) with 54.0% win-rate

## Executive Summary
Aya Expanse introduces a family of 8B and 32B parameter multilingual language models that achieve state-of-the-art performance across 23 languages. The models combine three key innovations: multilingual data arbitrage for synthetic data generation, iterative multilingual preference training, and model merging. These methods enable Aya Expanse to outperform leading open-weight models including Gemma 2, Qwen 2.5, and Llama 3.1, achieving up to 76.6% win-rate on Arena-Hard-Auto. The models show significant improvements over their predecessor Aya 23, with up to 16% increase in language understanding and 1.8x increase in mathematical reasoning.

## Method Summary
Aya Expanse builds on Cohere's Command series base models (8B with 8K context, 32B with 128K context) and applies a sophisticated post-training pipeline. The process begins with multilingual data arbitrage, strategically sampling from diverse teacher models to generate high-quality synthetic data for each language. This is followed by iterative multilingual preference training, combining offline training on arbitrage-generated pairs with online iterative DPO using self-generated completions ranked by an internal reward model. Model merging is applied at multiple stages, training separate models for different language families and combining them using weighted averaging to leverage cross-lingual transfer and diversity.

## Key Results
- Aya Expanse 32B surpasses Llama 3.1 70B (twice its size) with 54.0% win-rate on Arena-Hard-Auto
- Achieves 76.6% win-rate on m-ArenaHard, outperforming all leading open-weight models
- Shows up to 16% improvement in Global-MMLU and 1.8x improvement in MGSM over Aya 23
- Model merging provides up to 3x larger gains at 32B scale compared to 8B scale

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual data arbitrage strategically samples from a diverse pool of teacher models to generate high-quality synthetic multilingual datasets, addressing the absence of a universally effective teacher across all languages.
- Mechanism: Instead of relying on a single teacher model, data arbitrage leverages an Arbiter (internal Reward Model) to evaluate and select optimal generations from multiple models in the pool for each prompt in a given language. This exploits performance variations among models to improve multilingual generations.
- Core assumption: The Arbiter (Reward Model) can accurately score and differentiate the quality of multilingual completions across diverse language families, and the diversity among teacher models provides complementary strengths.
- Evidence anchors:
  - [abstract]: "multilingual data arbitrage for synthetic data generation"
  - [section]: "In our recent work [Odumakinde et al., 2024], we demonstrate that these limitations can be addressed through 'data arbitrage' – strategically sampling from a pool of teacher models."
  - [corpus]: Weak or missing direct evidence on Arbiter effectiveness in the corpus.

### Mechanism 2
- Claim: Iterative multilingual preference training with online data outperforms offline-only or online-only variants, unlocking substantial gains across all languages.
- Mechanism: The process involves first training on offline preference data (high and low reward responses from the arbitrage stage) followed by online iterative DPO where multiple online generations are sampled, ranked with an internal Reward Model, and used for further training. This combination is better than either method alone.
- Core assumption: The combination of offline and online preference training provides complementary benefits, and the internal Reward Model can effectively rank online generations to guide preference learning.
- Evidence anchors:
  - [abstract]: "iterative multilingual preference training"
  - [section]: "While Dang et al. [2024] also show that preference training with online data outperforms its offline variant, during training of Aya Expanse, we found that the combination of first preference-training with offline data followed by preference-training with online data to be better than either online or offline training alone."
  - [corpus]: Weak or missing direct evidence on the specific combination effectiveness in the corpus.

### Mechanism 3
- Claim: Model merging at scale (32B) provides significantly larger gains than at smaller scales (8B), optimizing performance by leveraging cross-lingual transfer and diversity among language families.
- Mechanism: Multiple separate models trained on different language families are merged using weighted averaging. Cross-lingual transfer provides performance benefits while linguistic differences differentiate checkpoints. Merging is more effective at 32B scale (up to 3x gains) compared to 8B scale.
- Core assumption: Cross-lingual transfer provides significant performance benefits, and diversity among language families provides sufficient differentiation between checkpoints for effective merging. Merging is inherently more effective at larger scales.
- Evidence anchors:
  - [abstract]: "model merging"
  - [section]: "In addition to weighted linear averaging, we experiment with multiple merging techniques... However, we found weighted averaging to be the most consistent method... Interestingly, we observed significantly larger gains from merging at the 32B scale compared to the 8B scale – up to 3x."
  - [corpus]: Weak or missing direct evidence on merging effectiveness at different scales in the corpus.

## Foundational Learning

- Concept: Multilingual synthetic data generation through data arbitrage
  - Why needed here: Aya Expanse requires high-quality training data across 23 languages, but lacks a single effective teacher model for all languages. Data arbitrage addresses this by leveraging diverse teacher models.
  - Quick check question: What is the role of the Arbiter (Reward Model) in the data arbitrage process?

- Concept: Multilingual preference optimization and alignment
  - Why needed here: Standard preference training is challenging in multilingual settings due to limited high-quality datasets and optimization difficulties across diverse languages. Aya Expanse uses novel techniques to overcome these challenges.
  - Quick check question: Why does the combination of offline and online preference training work better than either method alone?

- Concept: Model merging for multi-tasking optimization
  - Why needed here: Choosing optimal data mixtures for complex multi-stage training pipelines is difficult. Model merging provides an alternative approach to enable complex multi-tasking at reduced computational cost.
  - Quick check question: How does cross-lingual transfer contribute to the effectiveness of model merging?

## Architecture Onboarding

- Component map: Base pre-trained model (Cohere Command series) → Multilingual data arbitrage for SFT → Offline preference training → Online iterative preference training (3 iterations) → Model merging (weighted averaging) → Final instruction tuning. System supports 23 languages with specialized datasets and evaluation frameworks.

- Critical path: The critical path involves the post-training pipeline: data arbitrage → offline preference training → online iterative preference training → merging. Each stage builds upon the previous one to achieve final model performance.

- Design tradeoffs: The system trades computational cost (especially for merging and iterative training) for performance gains across languages. It also balances diversity among teacher models against the need for a reliable Arbiter. The choice of weighted averaging over other merging techniques prioritizes consistency over potential gains from more complex methods.

- Failure signatures: Poor multilingual performance indicates Arbiter failure or insufficient teacher model diversity. Performance regressions in specific languages suggest issues with the preference training balance or merging strategy. Suboptimal scaling behavior may indicate problems with the merging approach at different parameter scales.

- First 3 experiments:
  1. Test the Arbiter's ability to score and differentiate multilingual completions across diverse language families using a small set of teacher models.
  2. Compare the effectiveness of offline-only, online-only, and combined offline-online preference training on a multilingual dataset to validate the iterative approach.
  3. Evaluate model merging gains at different scales (e.g., 8B vs 32B) using models trained on different language family clusters to assess cross-lingual transfer and scaling behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multilingual data arbitrage approach scale to languages with very limited or no available teacher models?
- Basis in paper: [explicit] The paper discusses multilingual data arbitrage but does not address extreme low-resource language scenarios where teacher models may be unavailable.
- Why unresolved: The methodology relies on sampling from a pool of teacher models, but no information is provided about fallback strategies when suitable teachers don't exist for certain languages.
- What evidence would resolve it: Results from experiments testing the approach on languages with varying levels of available training data and teacher model quality, including completely low-resource languages.

### Open Question 2
- Question: What are the long-term effects of iterative multilingual preference training on model behavior and performance stability?
- Basis in paper: [inferred] The paper mentions running 3 iterations of online preference training but doesn't discuss potential long-term effects or optimal iteration limits.
- Why unresolved: While the paper found minimal gains beyond 3 iterations, it doesn't explore how repeated preference training might affect model behavior over extended training periods or across different tasks.
- What evidence would resolve it: Long-term studies tracking model performance and behavior across many more iterations, including potential degradation or emergence of reward hacking behaviors.

### Open Question 3
- Question: How do the gains from model merging scale with different model sizes and architectures?
- Basis in paper: [explicit] The paper notes that merging showed significantly larger gains at 32B scale compared to 8B scale, but doesn't explore the scaling relationship in detail.
- Why unresolved: The paper only compares 8B and 32B scales, leaving questions about how merging effectiveness varies across other model sizes and architectures.
- What evidence would resolve it: Systematic experiments testing merging effectiveness across a wide range of model sizes and architectures, including smaller and larger models than those tested in this paper.

## Limitations

- The effectiveness of the Arbiter (Reward Model) in accurately scoring multilingual completions across diverse language families lacks direct empirical validation in the corpus.
- The claimed superiority of combining offline and online preference training, as well as the scaling benefits of model merging, are asserted based on training observations rather than systematic ablation studies.
- Specific implementation details needed for exact reproduction, particularly the teacher model pool composition and reward model architecture, are not disclosed.

## Confidence

- **High confidence**: The overall methodology (multilingual data arbitrage, iterative preference training, model merging) is clearly described and follows established practices in the field. The reported performance improvements on specific benchmarks are directly measurable and verifiable through the provided evaluation framework.
- **Medium confidence**: The claimed mechanisms for why these methods work (Arbiter effectiveness, offline-online training combination, cross-lingual transfer in merging) are plausible based on related work, but lack direct empirical validation in the Aya Expanse context.
- **Low confidence**: The specific implementation details that would be needed for exact reproduction, particularly the teacher model pool composition, the exact reward model architecture and training procedure, and the precise merging weights and schedules.

## Next Checks

1. **Arbiter Validation**: Conduct a controlled experiment to test the Arbiter's ability to score and differentiate multilingual completions across diverse language families using a small set of teacher models (3-5 models covering different language families). Measure inter-annotator agreement between the Arbiter and human judges on a held-out test set of multilingual completions.

2. **Preference Training Ablation**: Systematically compare offline-only, online-only, and combined offline-online preference training on a multilingual dataset (using a subset of 5-6 languages). Measure pairwise win-rates and benchmark performance after each training variant to validate the claimed superiority of the iterative approach.

3. **Merging Scaling Study**: Train separate models for different language family clusters (e.g., Romance languages, East Asian languages, Middle Eastern languages) at both 8B and 32B scales. Apply model merging and measure performance gains relative to individual models and baseline merged models. Quantify the contribution of cross-lingual transfer by varying the number of shared languages between clusters.