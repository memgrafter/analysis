---
ver: rpa2
title: 'CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for LLMs'
arxiv_id: '2405.17233'
source_url: https://arxiv.org/abs/2405.17233
tags:
- quantization
- outlier
- claq
- precision
- adaptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CLAQ, a column-level adaptive quantization
  framework for low-bit post-training quantization of large language models (LLMs).
  CLAQ addresses the performance degradation in extremely low-bit quantization (2-3
  bits) by introducing three column-level strategies: K-Means based dynamic centroid
  generation, outlier-guided adaptive precision search, and dynamic outlier reservation.'
---

# CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for LLMs

## Quick Facts
- arXiv ID: 2405.17233
- Source URL: https://arxiv.org/abs/2405.17233
- Authors: Haoyu Wang; Bei Liu; Hang Shao; Bo Xiao; Ke Zeng; Guanglu Wan; Yanmin Qian
- Reference count: 40
- Key outcome: CLAQ achieves 6.47 perplexity on WikiText2 with 3-bit quantization and 27.64 perplexity with 2-bit quantization, outperforming existing approaches in ultra-low bit scenarios.

## Executive Summary
This paper introduces CLAQ, a column-level adaptive quantization framework for low-bit post-training quantization of large language models. The method addresses the performance degradation in extremely low-bit quantization (2-3 bits) through three innovative strategies: K-Means based dynamic centroid generation, outlier-guided adaptive precision search, and dynamic outlier reservation. CLAQ significantly outperforms existing approaches on LLaMA and Yi models across various bit-widths, particularly excelling in ultra-low bit scenarios where traditional quantization methods struggle.

## Method Summary
CLAQ is a training-free post-training quantization (PTQ) framework that employs three column-level strategies to improve low-bit quantization performance. The method uses K-Means clustering to dynamically generate quantization centroids that better match parameter distributions, outlier ratio as a sensitivity metric to guide adaptive precision assignment, and dynamic outlier reservation to preserve critical parameters in full precision. These strategies work together to minimize information loss during quantization while maintaining model accuracy, particularly in the challenging 2-3 bit quantization regime.

## Key Results
- Achieves 6.47 perplexity on WikiText2 with 3-bit quantization
- Achieves 27.64 perplexity on WikiText2 with 2-bit quantization
- Outperforms baseline methods significantly in ultra-low bit scenarios (2-3 bits)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Column-level adaptive quantization improves performance in low-bit quantization by tailoring quantization precision to the sensitivity of each column.
- **Mechanism:** The method uses outlier ratio as a metric to determine the sensitivity of each column to quantization errors. Columns with higher outlier ratios are assigned higher precision to preserve important information, while less sensitive columns use lower precision.
- **Core assumption:** The distribution of outliers within each column is a reliable indicator of that column's sensitivity to quantization errors.
- **Evidence anchors:**
  - [abstract]: "We present a column-wise outlier ratio based quantization sensitivity metric, namely Outlier Order, to assess the importance of parameters in LLMs."
  - [section 3.2]: "The ratio of outliers Rj of the j-th column Wj is defined as: Rj = Card (|Wj| > |W | Ã— S)/i"
- **Break condition:** If the outlier ratio does not correlate well with quantization sensitivity, the adaptive precision approach would fail to improve performance.

### Mechanism 2
- **Claim:** K-Means based dynamic centroid generation improves quantization accuracy by better matching the parameter distribution.
- **Mechanism:** Instead of using uniformly spaced quantization levels, K-Means clustering dynamically generates quantization centroids that conform to the distribution of the LLM's parameters, minimizing information loss.
- **Core assumption:** The distribution of parameters within each column can be effectively approximated by K-Means clustering.
- **Evidence anchors:**
  - [abstract]: "Firstly, a K-Means clustering based algorithm is proposed that allows dynamic generation of quantization centroids for each column of a parameter matrix."
  - [section 3.1]: "Our K-Means based quantization reduces storage requirements by storing a single vector of K-Means centroids as codebook per column, yet not losing the performance."
- **Break condition:** If the K-Means clustering fails to capture the parameter distribution accurately, quantization errors would increase.

### Mechanism 3
- **Claim:** Dynamic outlier reservation preserves model performance by retaining critical parameters in full precision.
- **Mechanism:** Columns with higher outlier ratios are allocated more full-precision outlier retention, while columns with lower outlier ratios have fewer parameters preserved in full precision. This selective retention focuses on preserving the most important information.
- **Core assumption:** Retaining full-precision outliers in sensitive columns is more effective at preserving model performance than quantizing all parameters with higher precision.
- **Evidence anchors:**
  - [abstract]: "Finally, a dynamic outlier reservation scheme is developed to retain some parameters in their original float point precision, in trade off of boosted model performance."
  - [section 3.4]: "Based on Outlier Order, OR involves reserving FP16 precision outliers for columns with a higher concentration of outliers to preserve model capability, whereas columns with a lower outlier ratio are allocated less reservation budget."
- **Break condition:** If the retained outliers are not critical for model performance, the dynamic outlier reservation would not provide significant benefits.

## Foundational Learning

- **Concept:** K-Means clustering
  - **Why needed here:** K-Means clustering is used to dynamically generate quantization centroids that conform to the distribution of the LLM's parameters, improving quantization accuracy.
  - **Quick check question:** How does K-Means clustering differ from using uniformly spaced quantization levels?

- **Concept:** Outlier detection
  - **Why needed here:** Outlier detection is used to identify parameters that are sensitive to quantization errors, guiding the adaptive precision and outlier reservation strategies.
  - **Quick check question:** What is the definition of an outlier in the context of LLM quantization?

- **Concept:** Post-training quantization (PTQ)
  - **Why needed here:** PTQ is the primary approach used in this paper to compress LLMs without fine-tuning, making it suitable for resource-constrained deployment scenarios.
  - **Quick check question:** What are the advantages of PTQ over quantization-aware training (QAT)?

## Architecture Onboarding

- **Component map:** K-Means clustering module -> Outlier detection module -> Adaptive precision module -> Outlier reservation module
- **Critical path:** The critical path involves clustering, outlier detection, precision assignment, and outlier reservation, all guided by the outlier ratio metric.
- **Design tradeoffs:** The tradeoff is between model size and performance. Adaptive precision and outlier reservation improve performance but increase model size slightly.
- **Failure signatures:** If the quantization performance degrades significantly, it could indicate issues with the K-Means clustering, outlier detection, or precision assignment.
- **First 3 experiments:**
  1. Implement K-Means clustering for a single column and verify that the generated centroids better match the parameter distribution compared to uniformly spaced levels.
  2. Apply outlier detection to a sample matrix and verify that the detected outliers correlate with quantization sensitivity.
  3. Implement adaptive precision assignment for a sample matrix and verify that columns with higher outlier ratios receive higher precision.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of outlier standard S impact the trade-off between model accuracy and computational efficiency in the Outlier Order metric?
- Basis in paper: [explicit] The paper mentions that S = 13 is the optimal setting for outlier definition, but the choice of S can significantly influence the number of outliers detected and their distribution across columns.
- Why unresolved: While the paper provides ablation study results for different S values, it does not explore the full range of potential trade-offs between accuracy and computational efficiency.
- What evidence would resolve it: Systematic experiments varying S across a wide range of values, coupled with a detailed analysis of the impact on model accuracy, computational cost, and outlier distribution.

### Open Question 2
- Question: What is the optimal strategy for assigning different bit-widths to different parameter matrices in the adaptive precision quantization scheme?
- Basis in paper: [inferred] The paper mentions that the heuristic-based adaptive precision search algorithm can potentially improve performance, but it does not provide a definitive solution for optimal bit-width assignment across different parameter matrices.
- Why unresolved: The paper acknowledges that the current two-level precision approach may not be optimal for all scenarios, especially when the assignable precision bit-width exceeds 0.5 bits.
- What evidence would resolve it: Development and evaluation of an end-to-end, efficient, and interpretable adaptive precision search strategy that consistently improves performance across different model architectures and quantization scenarios.

### Open Question 3
- Question: How does the choice of calibration data affect the performance of the quantized models, and what is the optimal calibration data selection strategy?
- Basis in paper: [explicit] The paper mentions that the choice of calibration data can impact experimental outcomes, but it does not provide a comprehensive analysis of the relationship between calibration data and model performance.
- Why unresolved: The paper only provides a brief comparison of results using C4 and Wikitext2 as calibration datasets, without exploring the full range of potential calibration data choices and their impact on different model architectures and tasks.
- What evidence would resolve it: Systematic experiments using various calibration datasets, including synthetic data, and a detailed analysis of the relationship between calibration data characteristics, model architecture, and task performance.

## Limitations
- The column-level approach assumes outlier ratios consistently indicate quantization sensitivity across different LLM architectures and domains.
- The method requires additional storage for full-precision outliers and precision metadata, partially offsetting compression benefits.
- The evaluation focuses primarily on perplexity metrics, with limited analysis of how quantization affects model robustness or out-of-distribution generalization.

## Confidence
- **High confidence:** The fundamental premise that column-level adaptation improves low-bit quantization performance is well-supported by quantitative results showing CLAQ's superiority over baseline methods across multiple models and bit-widths.
- **Medium confidence:** The effectiveness of the three proposed strategies (K-Means centroids, adaptive precision, outlier reservation) is demonstrated, but the specific design choices for thresholds and ratios could be architecture-dependent and may not generalize universally.
- **Low confidence:** Claims about CLAQ's ability to "push the limits" of low-bit quantization lack comparison against the most recent quantization techniques developed after the paper's submission, as the field is rapidly evolving.

## Next Checks
1. **Architecture generalization test:** Apply CLAQ to a different class of models (e.g., decoder-only vs encoder-decoder architectures) to verify whether outlier ratio-based sensitivity metrics generalize across model types.

2. **Distribution robustness analysis:** Evaluate CLAQ's performance when applied to models trained on different data distributions (e.g., specialized domain models vs general-purpose LLMs) to test the assumption that parameter distributions follow predictable patterns suitable for K-Means clustering.

3. **End-to-end deployment validation:** Implement CLAQ-compressed models in an actual inference pipeline to measure real-world latency improvements and memory savings, accounting for the overhead of precision metadata management and outlier reservation handling.