---
ver: rpa2
title: Grounding Language in Multi-Perspective Referential Communication
arxiv_id: '2410.03959'
source_url: https://arxiv.org/abs/2410.03959
tags:
- speaker
- listener
- human
- referring
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies referring expression generation and comprehension
  in multi-agent embodied environments where agents have different visual perspectives.
  A scene generation platform creates photorealistic 3D environments with controlled
  difficulty via agent orientation and adversarial referent placement.
---

# Grounding Language in Multi-Perspective Referential Communication

## Quick Facts
- arXiv ID: 2410.03959
- Source URL: https://arxiv.org/abs/2410.03959
- Reference count: 15
- Open-weight speaker model improves from 58.9% to 69.3% communicative success when fine-tuned with evidence of communicative success from human listeners

## Executive Summary
This work studies referring expression generation and comprehension in multi-agent embodied environments where agents have different visual perspectives. A scene generation platform creates photorealistic 3D environments with controlled difficulty via agent orientation and adversarial referent placement. Human experiments with 2,970 referring expressions show human-human pairs achieve 87.6% communicative success, while automated models lag behind. Fine-tuning an open-weight vision-language model using evidence of communicative success from human listeners improves performance from 58.9% to 69.3% communicative success, outperforming proprietary models.

## Method Summary
The approach involves generating 3D scenes with two agents and three referent objects using ScanNet++ environments, collecting human referring expressions and listener selections through crowdsourcing, evaluating vision-language models as speakers and listeners, and fine-tuning speaker models using preference-based learning with rewards derived from listener selections. The training procedure uses PPO with rewards calculated as the difference between the likelihood of the speaker's expression describing the listener's chosen target versus the intended target.

## Key Results
- Human-human pairs achieve 87.6% communicative success, significantly higher than automated models
- Fine-tuning LLaVA-1.5 using evidence of communicative success improves performance from 58.9% to 69.3%
- Automated models struggle with perspective misalignment and ambiguity errors in multi-perspective environments

## Why This Works (Mechanism)

### Mechanism 1
Learning from communicative success improves referring expression generation by using listener selections as implicit feedback about utterance meaning. The system collects data during interaction where a speaker generates expressions and a listener selects referents, using these selections to fine-tune the speaker model even when the listener chooses incorrectly.

### Mechanism 2
Preference-based learning that explicitly pairs intended and empirically chosen referents is more effective than learning only from successful examples. The reward function maximizes the difference between the likelihoods of the speaker's expression describing the listener's chosen target versus the intended target, creating a pairwise preference learning signal.

### Mechanism 3
Multi-perspective environments with controlled difficulty improve model robustness by requiring consideration of different visual perspectives. The environment generates scenes with varying relative orientations between speaker and listener (0° to 180°), and includes adversarial referent placement that minimizes communicative success, forcing models to reason about spatial relationships from different viewpoints.

## Foundational Learning

- **Spatial reasoning in multi-agent environments**: Needed to understand how objects relate to each other in 3D space from different viewpoints. Quick check: If a speaker sees object A to the left of object B, but the listener sees them swapped due to their different perspective, how should the speaker describe their relationship?

- **Pragmatic language understanding**: Required for successful communication that goes beyond literal meaning to understand what makes a referring expression effective in context. Quick check: Why might "the ball near the lamp" be less effective than "the ball on your left" when the speaker and listener have different views?

- **Learning from interaction data**: The fine-tuning approach relies on collecting data during deployment and using it to improve the model. Quick check: What information can be extracted from cases where the listener selects the wrong referent?

## Architecture Onboarding

- **Component map**: Scene generation pipeline -> Speaker models -> Listener selection -> Reward calculation -> Fine-tuning -> Improved speaker
- **Critical path**: Speaker model generates expression → Listener selects referent → Reward calculation based on listener choice → Fine-tuning updates speaker
- **Design tradeoffs**: Open-weight vs proprietary models (performance vs accessibility), human vs automated listeners (quality vs cost), single-shot vs interactive communication (simplicity vs realism)
- **Failure signatures**: Low communicative success rates across all model combinations, no improvement from fine-tuning despite data collection, adversarial referent placement not reducing success rates
- **First 3 experiments**:
  1. Test all model combinations on random referent placement scenes to establish baseline
  2. Test the same models on adversarial referent placement scenes to measure difficulty impact
  3. Fine-tune LLaVA-1.5 using preference learning with automated listener data and measure improvement

## Open Questions the Paper Calls Out

- **Open Question 1**: How would the performance of the automated models change if the task were extended to multi-turn conversations rather than single-shot references? The paper suggests evaluating models in an interactive version as future work since they only evaluate single-turn reference games.

- **Open Question 2**: How would the results differ if the experiments were conducted across multiple languages with different spatial concept structures? The paper's limitations section explicitly states that spatial concepts and referential strategies vary across language communities and that future work should explore this.

- **Open Question 3**: What would be the impact of using more sophisticated listener models or ensemble methods on the learning from communicative success approach? The paper only experiments with LLaVA-1.5 as the automated listener and notes that human listeners provide higher-quality feedback, suggesting room for improvement.

## Limitations
- Evaluation primarily relies on automated listener models for fine-tuning, with limited human data collection
- Study does not address potential bias introduced by the specific choice of vision-language models
- Controlled difficulty through adversarial placement may not fully capture real-world multi-perspective communication complexity

## Confidence

- **High Confidence**: The finding that human-human pairs achieve 87.6% communicative success is well-supported by the data collected
- **Medium Confidence**: The improvement from 58.9% to 69.3% through fine-tuning is credible but depends on automated listener data quality
- **Low Confidence**: The effectiveness of adversarial referent placement in creating genuinely challenging scenarios is less certain

## Next Checks

1. **Human-in-the-Loop Validation**: Conduct additional experiments pairing the fine-tuned speaker model with human listeners to verify that the 69.3% improvement generalizes beyond automated evaluation.

2. **Cross-Model Generalization**: Test the fine-tuning approach with other vision-language models not included in the original study to assess whether the improvement is model-specific or generalizable.

3. **Real-World Transfer**: Evaluate the fine-tuned models on real-world multi-perspective communication tasks, such as robot navigation instructions or remote collaboration scenarios, to assess practical applicability.