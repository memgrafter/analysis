---
ver: rpa2
title: Investigating the Role of Prompting and External Tools in Hallucination Rates
  of Large Language Models
arxiv_id: '2410.19385'
source_url: https://arxiv.org/abs/2410.19385
tags:
- question
- strategy
- hallucinations
- answer
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides an empirical evaluation of different prompting
  strategies and agent architectures aimed at reducing hallucinations in large language
  models (LLMs). Various prompting techniques (such as Chain-of-Thought, Self-Consistency,
  and Chain-of-Verification) and frameworks (like Multiagent Debate and Chat Protect)
  were applied to benchmark datasets to assess their accuracy and hallucination rates.
---

# Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models

## Quick Facts
- **arXiv ID**: 2410.19385
- **Source URL**: https://arxiv.org/abs/2410.19385
- **Reference count**: 28
- **Key outcome**: This paper provides an empirical evaluation of different prompting strategies and agent architectures aimed at reducing hallucinations in large language models (LLMs).

## Executive Summary
This study systematically evaluates various prompting strategies (Chain-of-Thought, Self-Consistency, Chain-of-Verification) and agent architectures (Multiagent Debate, Chat Protect) to assess their effectiveness in reducing hallucination rates in large language models. The research tests these approaches across benchmark datasets including GSM8K, TriviaQA, and MMLU, examining both accuracy and hallucination metrics. A key finding is that the optimal prompting technique varies by problem type, with Self-Consistency showing particular strength on mathematical reasoning tasks while simpler approaches often outperform more complex methods in reducing hallucinations.

## Method Summary
The study employed a comprehensive experimental framework testing eight different prompting strategies across multiple benchmark datasets. Researchers used the LLaMA 3.1 8B model with temperature settings of 0.2, 0.5, and 0.8, applying both closed-book question-answering and tool-calling agent architectures. The evaluation framework incorporated four types of tests: control (no prompting), prompt-only (with prompting but no tool use), closed-book (no tool use), and tool use (with external tools). Hallucination rates were measured alongside accuracy metrics to provide a balanced assessment of each approach's effectiveness.

## Key Results
- Self-Consistency strategy achieved the best results on GSM8K mathematical reasoning tasks but performed similarly to control on knowledge-based benchmarks
- Chat Protect strategy achieved highest accuracy by sacrificing the number of questions answered, with temperature controlling the trade-off between accuracy and response quantity
- Tool-calling agents exhibited significantly higher hallucination rates due to the added complexity of external tool usage

## Why This Works (Mechanism)
The effectiveness of different prompting strategies appears to depend on how well they align with the underlying task structure. Self-Consistency works particularly well for mathematical reasoning because it generates multiple solution paths and selects the most consistent answer, which helps filter out reasoning errors that could lead to hallucinations. For knowledge-based tasks, simpler prompting approaches may work better because they reduce the cognitive load on the model and minimize opportunities for the model to generate incorrect information. The Chat Protect strategy's trade-off between accuracy and response quantity suggests that more careful, deliberative processing can reduce hallucinations at the cost of throughput. Tool-calling agents show higher hallucination rates likely because the complexity of integrating external information introduces additional failure modes and opportunities for the model to hallucinate tool outputs or misinterpret results.

## Foundational Learning
The study demonstrates that LLMs can benefit from different prompting strategies depending on task characteristics. The Self-Consistency approach leverages the model's ability to generate diverse reasoning paths and identify consensus, which is particularly effective for tasks with clear logical structures like mathematics. The Chat Protect strategy shows that LLMs can be guided toward more careful, accurate responses through specific prompting techniques that encourage verification and hesitation. The elevated hallucination rates in tool-calling agents suggest that while LLMs can effectively use external tools, the added complexity requires careful architectural design to maintain reliability. These findings indicate that LLM performance is highly sensitive to both prompting strategy selection and task alignment, with optimal performance requiring careful matching of approach to problem type.

## Architecture Onboarding
The study tests various prompting strategies within the LLaMA 3.1 8B model architecture, examining how different prompt structures affect model behavior. Chain-of-Thought prompting requires the model to explicitly format its reasoning steps, which can help with transparency but may also increase hallucination risk if the reasoning contains errors. Self-Consistency generates multiple reasoning paths and uses voting to select answers, which can improve accuracy but at the cost of computational resources. Chain-of-Verification adds an additional verification step to check answers, potentially reducing hallucinations but increasing response latency. Chat Protect uses a cautious approach that prioritizes accuracy over response quantity, suggesting that architectural modifications to prompting can trade off between different performance metrics. Tool-calling agents extend the basic model architecture by incorporating external tool usage, which significantly increases complexity and hallucination risk but also enables the model to access external knowledge and computational capabilities.

## Open Questions the Paper Calls Out
**Open Question 1**
- Question: How do different temperature settings interact with prompting techniques to affect hallucination rates in LLMs?
- Basis in paper: [explicit] The paper discusses testing temperature values of 0.2, 0.5, and 0.8 across various prompting strategies and observes that higher temperatures can increase hallucination rates in some cases while decreasing them in others (e.g., Chat Protect).
- Why unresolved: The paper only tests a limited range of temperature values and doesn't provide a comprehensive model for how temperature interacts with different prompting strategies across various task types.
- What evidence would resolve it: A systematic study testing a wider range of temperature values (e.g., 0.1 to 1.0 in 0.1 increments) across all prompting strategies on multiple benchmark datasets, measuring hallucination rates and accuracy at each temperature point.

**Open Question 2**
- Question: Why does the Self-Consistency strategy perform significantly better on mathematical reasoning tasks (GSM8K) compared to knowledge-based tasks (TriviaQA and MMLU)?
- Basis in paper: [explicit] The paper observes that SC strategy achieves the best results on GSM8K but performs similarly to the control method on TriviaQA and MMLU benchmarks.
- Why unresolved: The paper notes this difference but doesn't investigate the underlying reasons why SC works well for mathematical reasoning but not for factual recall or multiple-choice questions.
- What evidence would resolve it: Comparative analysis of the reasoning paths generated by SC across different task types, identifying whether the strategy fails because mathematical problems have more diverse solution paths while factual questions have more consistent correct answers.

**Open Question 3**
- Question: How do tool-calling agents affect hallucination rates differently in LLMs with varying parameter sizes?
- Basis in paper: [inferred] The paper mentions that the 8 billion parameter LLaMA 3.1 model is relatively small compared to state-of-the-art models like GPT-4, and suggests caution when augmenting smaller models with external tools.
- Why unresolved: The study only tests tool-calling agents on a relatively small 8B parameter model, without investigating whether larger models would show different patterns of hallucination when using external tools.
- What evidence would resolve it: Comparative experiments testing the same agent architectures with models of varying sizes (e.g., 8B, 70B, 175B parameters) on identical benchmarks, measuring hallucination rates and accuracy across model sizes.

**Open Question 4**
- Question: What is the optimal balance between the number of questions answered and accuracy when using the Chat Protect strategy?
- Basis in paper: [explicit] The paper shows that CP strategy achieves the highest accuracy by sacrificing the number of questions answered, and that temperature controls this trade-off.
- Why unresolved: The paper doesn't investigate what the optimal temperature setting is for different application contexts where either answering more questions or maintaining higher accuracy is more important.
- What evidence would resolve it: Application-specific testing of CP strategy across different temperature settings in contexts with varying tolerance for unanswered questions (e.g., medical diagnosis vs. creative writing), identifying optimal temperature settings for each use case.

**Open Question 5**
- Question: How does the complexity of reasoning steps in Chain-of-Thought prompting affect hallucination rates?
- Basis in paper: [inferred] The paper observes that the control strategy outperformed the CoT strategy on GSM8K, suggesting that explicitly formatted reasoning steps may increase complexity and hallucination risk.
- Why unresolved: The paper doesn't investigate whether simpler or more complex reasoning steps in CoT prompting lead to higher hallucination rates, or whether there's an optimal level of reasoning detail.
- What evidence would resolve it: Controlled experiments varying the complexity of reasoning steps required in CoT prompts (e.g., simple vs. detailed step-by-step explanations) while measuring hallucination rates and accuracy across different task types.

## Limitations
- The study focused primarily on closed-book question-answering and mathematical reasoning tasks, leaving open questions about generalizability to other domains like creative writing or code generation
- The comparison between prompting techniques and agent architectures represents a trade-off analysis rather than comprehensive optimization across all possible configurations
- The study examined hallucination rates as a binary classification, which may oversimplify the nuanced spectrum of output quality issues
- All experiments were conducted using a single 8B parameter model, limiting conclusions about how these findings generalize to larger or smaller models
- The benchmark datasets, while diverse, may not fully represent the complexity and variability of real-world applications

## Confidence
- **High confidence**: The observation that optimal prompting techniques vary by problem type is well-supported by the empirical data across multiple benchmark datasets
- **Medium confidence**: The finding that simpler techniques often outperform complex methods is supported but may be context-dependent and warrants further investigation across different task types
- **Medium confidence**: The elevated hallucination rates in tool-calling agents are demonstrated empirically, though the causal mechanisms require deeper investigation
- **Low confidence**: The specific temperature settings that optimize the trade-off between accuracy and hallucination rates require further systematic investigation across different prompting strategies

## Next Checks
1. Replicate the study using a broader range of task types including creative generation, code synthesis, and multi-modal inputs to assess generalizability of the prompting technique findings
2. Conduct ablation studies on tool-calling architectures to identify which specific components (tool selection, output parsing, tool chaining) contribute most significantly to hallucination rates
3. Implement longitudinal testing to evaluate whether hallucination patterns change over time with model updates, as newer versions may exhibit different behavior than the models used in this study
4. Test the prompting strategies on models with varying parameter sizes to determine whether the observed effectiveness patterns hold across different model scales
5. Develop a more nuanced hallucination measurement framework that captures the spectrum of output quality issues rather than treating hallucination as a binary classification