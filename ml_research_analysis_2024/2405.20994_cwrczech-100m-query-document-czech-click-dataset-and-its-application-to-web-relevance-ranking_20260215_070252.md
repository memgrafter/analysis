---
ver: rpa2
title: 'CWRCzech: 100M Query-Document Czech Click Dataset and Its Application to Web
  Relevance Ranking'
arxiv_id: '2405.20994'
source_url: https://arxiv.org/abs/2405.20994
tags:
- cwrczech
- dataset
- relevance
- clicks
- click
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CWRCzech, a 100M query-document click dataset
  for Czech relevance ranking, and demonstrates its effectiveness for training large
  language models. The dataset contains 100M query-document pairs with clicks, dwell
  times, and document positions, collected from Seznam.cz search engine logs.
---

# CWRCzech: 100M Query-Document Czech Click Dataset and Its Application to Web Relevance Ranking

## Quick Facts
- arXiv ID: 2405.20994
- Source URL: https://arxiv.org/abs/2405.20994
- Reference count: 40
- Primary result: 100M query-document Czech click dataset for training large language models in relevance ranking

## Executive Summary
This paper introduces CWRCzech, a 100M query-document click dataset for Czech relevance ranking, and demonstrates its effectiveness for training large language models. The dataset contains 100M query-document pairs with clicks, dwell times, and document positions, collected from Seznam.cz search engine logs. The authors release a manually annotated Czech test set of ~50k query-document pairs for evaluation. Models trained on CWRCzech surpass the performance of models trained on human-annotated data when evaluated on in-domain relevance ranking, with the best model achieving +2.5% NDCG@10 improvement for cross-encoders and +4% for bi-encoders compared to the baseline trained on human annotations.

## Method Summary
The core method involves fine-tuning three pretrained Czech language models (Small-E-Czech, RetroMAE-Small, and FERNET-C5) for relevance ranking using CWRCzech. The authors design a label formula combining clicks, dwell times, and document rank, and employ contrastive training with generated soft negative pairs. The dataset contains 100M query-document pairs with clicks, dwell times, and document positions, collected from Seznam.cz search engine logs. The models are trained using cross-encoder and bi-encoder architectures, with the latter employing contrastive loss with in-batch negatives. The authors show that models trained on 20M click-based pairs can achieve comparable performance to models trained on 1M human annotations.

## Key Results
- Models trained on CWRCzech achieve +2.5% NDCG@10 improvement for cross-encoders and +4% for bi-encoders compared to models trained on human annotations
- 20M click-based pairs can match the performance of 1M manually annotated data for Czech relevance ranking
- Contrastive training with soft negative pairs improves bi-encoder performance by +6 percentage points in NDCG@10

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining clicks, dwell time, and rank into a single pseudo-label captures multi-dimensional relevance signals that correlate with human annotations.
- Mechanism: The label formula integrates three user behavior signalsâ€”clicks as primary relevance indicator, dwell time as engagement proxy, and rank as position bias mitigationâ€”into a weighted composite score.
- Core assumption: User behavior data (clicks, dwell times, ranks) are noisy but unbiased proxies for relevance when aggregated at scale.
- Evidence anchors: [abstract] "show that models trained on data automatically harnessed at sufficient scale can surpass the performance of models trained on human annotated data"; [section 4.4.6] "The empirically most successful formula found in our preliminary experiments is the following: ð‘™(ð‘ž,ð‘‘) â† sÂ·log(1 + (wclicks(ð‘ž,ð‘‘) + views(ð‘ž,ð‘‘) / ranks(ð‘ž,ð‘‘) + ð¶)Â·dwelltimes(ð‘ž,ð‘‘))"
- Break condition: If position bias is not properly accounted for, or if user behavior patterns differ significantly from the training distribution, the pseudo-labels may become unreliable.

### Mechanism 2
- Claim: Training on 20M click-based pairs achieves comparable performance to 1M human annotations, demonstrating the scalability advantage of user behavior data.
- Mechanism: The large-scale click dataset provides sufficient statistical power to overcome individual noise, allowing models to learn robust relevance patterns.
- Core assumption: Sufficient volume of user behavior data can compensate for noise through aggregation effects.
- Evidence anchors: [abstract] "show that models trained on data automatically harnessed at sufficient scale can surpass the performance of models trained on human annotated data"; [section 6] "Our analysis of the usefulness of the automatically generated data concludes that for Czech relevance ranking, performance on 1M manually annotated data can be matched by roughly 20M of user behavior data"
- Break condition: If the dataset contains systematic biases (e.g., only popular queries), or if the user base is too homogeneous, the learned patterns may not generalize.

### Mechanism 3
- Claim: Contrastive training with soft negative pairs improves bi-encoder performance by providing explicit negative examples.
- Mechanism: In-batch negative sampling creates contrastive pairs where non-clicked documents serve as soft negatives, forcing the model to learn finer-grained distinctions between relevant and non-relevant documents.
- Core assumption: Non-clicked documents in search results are likely to be less relevant than clicked ones, providing useful negative examples for contrastive learning.
- Evidence anchors: [section 4.5.1] "we follow the standard contrastive framework and use the cross-entropy objective with in-batch negatives as a contrastive loss"; [section 6] "The effect of employing the additional contrastive loss... is quantified... Training with additional contrastive loss yields +6 percent points NDCG@10 increase compared to a non-contrastive baseline"
- Break condition: If the search engine's ranking is already near-optimal, most non-clicked documents may be genuinely irrelevant, making soft negatives too easy and reducing learning signal.

## Foundational Learning

- Concept: Click modeling and position bias
  - Why needed here: Understanding how clicks are biased by document position is crucial for interpreting the click dataset and designing appropriate labels
  - Quick check question: If a document appears in position 1 and position 10 with equal relevance, how would you expect click rates to differ?

- Concept: Contrastive learning objectives
  - Why needed here: The bi-encoder training uses contrastive loss with in-batch negatives, requiring understanding of how this differs from standard classification
  - Quick check question: What is the key difference between a contrastive loss and a standard cross-entropy loss when training ranking models?

- Concept: Logarithmic transformations for scale normalization
  - Why needed here: The label formulas use log transformations to compress the dynamic range of clicks and dwell times, making the optimization more stable
  - Quick check question: Why might a log transformation be preferred over raw counts when aggregating user behavior signals?

## Architecture Onboarding

- Component map: Data pipeline -> Label generation -> Model training (cross-encoder/bi-encoder) -> Evaluation
- Critical path: CWRCzech dataset -> label formula application -> model fine-tuning -> NDCG@10 evaluation
- Design tradeoffs: Cross-encoders provide better accuracy but are slower; bi-encoders are faster but require contrastive training for competitive performance
- Failure signatures: Low NDCG@10 improvement despite large dataset size may indicate label formula issues or insufficient contrastive signal
- First 3 experiments:
  1. Train a simple cross-encoder baseline using only click counts as labels to establish a performance floor
  2. Implement the full ClickDwellRank label formula and measure improvement over the click-only baseline
  3. Add contrastive training to the bi-encoder and compare performance with the cross-encoder baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between scale of click data and model performance for relevance ranking?
- Basis in paper: [explicit] The paper states "performance on 1M manually annotated data can be matched by roughly 20M of user behavior data" and suggests "a roughly linear trend, where each order of magnitude increase in training data size results in approximately a 2 percentage points gain in NDCG@10"
- Why unresolved: The analysis only covers three data points (10k, 100k, 1M, 10M, 100M). A more comprehensive study across multiple orders of magnitude with different model architectures could establish a precise scaling relationship.
- What evidence would resolve it: Systematic experiments varying click dataset sizes from 1M to 100M+ with multiple model architectures and learning curves would establish the exact scaling relationship.

### Open Question 2
- Question: How do different types of user behavior signals (clicks, dwell time, position) contribute to relevance ranking performance when combined in different ways?
- Basis in paper: [explicit] The paper explores various label formulations combining clicks, dwell times, and document positions, finding that the combination performs best, but doesn't systematically isolate the contribution of each signal.
- Why unresolved: The ablation study shows that combined signals outperform individual ones, but doesn't quantify the marginal contribution of each signal or explore alternative combinations.
- What evidence would resolve it: Controlled experiments testing all possible combinations of the three signals (clicks, dwell time, position) and their interactions would quantify the unique and joint contributions of each.

### Open Question 3
- Question: How robust are models trained on Czech click data to domain shifts in query intent (informational vs. navigational vs. transactional)?
- Basis in paper: [explicit] The paper notes that CWRCzech contains only informational intent queries while DaReCzech contains all intents, and shows that models trained on CWRCzech perform comparably on DaReCzech but doesn't systematically analyze performance across different intent types.
- Why unresolved: The evaluation on DaReCzech shows comparable performance but doesn't break down results by query intent type or test on datasets with more extreme domain shifts.
- What evidence would resolve it: Evaluation on datasets explicitly labeled by intent type, or on cross-lingual transfer tasks, would reveal the limits of the Czech click data's applicability to different domains.

## Limitations
- Limited analysis of how individual components (clicks, dwell time, rank) contribute to the final labels
- Dataset collected from a single search engine may contain domain-specific biases
- Contrastive training impact not clearly isolated from other aspects of the training framework

## Confidence
- High Confidence: Dataset creation methodology and basic training pipeline are well-documented and reproducible
- Medium Confidence: Claims about surpassing human-annotated data are supported by experimental results but may be sensitive to specific evaluation conditions
- Low Confidence: Claims about specific mechanisms by which signals combine are largely speculative without detailed analysis

## Next Checks
1. Ablation study on label components: Systematically remove each component (clicks, dwell time, rank) from the label formula and measure the impact on model performance
2. Cross-domain transfer analysis: Evaluate models trained on CWRCzech on relevance ranking tasks from other domains or search engines to assess generalization
3. Negative sampling strategy comparison: Implement and compare alternative negative sampling strategies against the soft negative pairs used in the current approach