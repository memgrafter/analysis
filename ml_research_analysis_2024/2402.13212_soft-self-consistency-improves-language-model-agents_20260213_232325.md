---
ver: rpa2
title: Soft Self-Consistency Improves Language Model Agents
arxiv_id: '2402.13212'
source_url: https://arxiv.org/abs/2402.13212
tags:
- soft-sc
- bash
- action
- webshop
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of majority voting-based
  self-consistency (SC) for LLM agents in interactive tasks with large, sparse action
  spaces. The authors propose Soft Self-Consistency (SOFT-SC), which replaces SC's
  exact-match scoring with continuous scores based on model likelihoods, enabling
  selection even when actions are sparsely distributed.
---

# Soft Self-Consistency Improves Language Model Agents

## Quick Facts
- arXiv ID: 2402.13212
- Source URL: https://arxiv.org/abs/2402.13212
- Authors: Han Wang; Archiki Prasad; Elias Stempel-Eskin; Mohit Bansal
- Reference count: 12
- Key outcome: SOFT-SC improves performance and efficiency on interactive tasks by using continuous likelihood scores instead of exact-match voting, requiring half as many samples as SC.

## Executive Summary
This paper addresses the inefficiency of majority voting-based self-consistency (SC) for language model agents in interactive tasks with large, sparse action spaces. The authors propose Soft Self-Consistency (SOFT-SC), which replaces SC's exact-match scoring with continuous scores based on model likelihoods, enabling selection even when actions are sparsely distributed. SOFT-SC improves both performance and efficiency on long-horizon interactive tasks like bash programming, online shopping, and household games, requiring half as many samples as SC for comparable or better results. For example, SOFT-SC achieves a 6.6% higher success rate than SC on WebShop with the same number of samples.

## Method Summary
SOFT-SC improves language model agents by replacing SC's exact-match scoring with continuous scores computed from token likelihoods. The method generates multiple action trajectories using temperature-based sampling, then scores each action by aggregating the probabilities of its constituent tokens (using mean, min, or product aggregation depending on the domain). The action with the highest score is selected and executed. This approach allows selection even when no exact matches exist in the sampled trajectories, making it particularly effective in sparse action spaces. The method was evaluated across three interactive domains: bash programming, online shopping (WebShop), and household tasks (ALFWorld).

## Key Results
- SOFT-SC achieves 6.6% higher success rate than SC on WebShop with the same number of samples
- Requires only k=5 samples to match or exceed SC's performance with k=10 (2.2% improvement on ALFWorld)
- Maintains performance advantages across model sizes from 7B to 70B parameters on CodeLlama models
- Demonstrates consistent improvements across three distinct interactive domains

## Why This Works (Mechanism)

### Mechanism 1
SOFT-SC improves performance in sparse action spaces by using continuous likelihood scores instead of exact match voting. By aggregating token probabilities (min, mean, or product), SOFT-SC assigns a confidence score to each action, allowing selection even when no exact matches exist. Core assumption: The likelihood of tokens in an action correlates with the action's correctness or quality. Evidence: SOFT-SC replaces SC's discontinuous scoring with continuous scores computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Break condition: If token likelihoods do not correlate with action quality, or if the aggregation method poorly represents overall action confidence.

### Mechanism 2
SOFT-SC is more sample efficient because it can distinguish quality actions without requiring exact matches. By scoring actions continuously, SOFT-SC can select good actions earlier, reducing the number of samples needed compared to SC's need for exact matches. Core assumption: Continuous scoring can differentiate between good and bad actions even when they are not identical. Evidence: SOFT-SC improves both performance and efficiency on long-horizon interactive tasks, requiring half as many samples as SC for comparable or better performance. Break condition: If the continuous scoring fails to differentiate action quality, or if the cost of computing likelihoods outweighs the sample savings.

### Mechanism 3
SOFT-SC scales better with model size because larger models have better-calibrated likelihoods. As model size increases, token likelihoods become more reliable indicators of action quality, enhancing SOFT-SC's performance gains. Core assumption: Larger models produce more accurate token probabilities that better reflect action correctness. Evidence: Scaling trends for CodeLlama models ranging from 7B to 70B parameters show that SOFT-SC always outperforms SC. Break condition: If larger models do not have better-calibrated likelihoods, or if the improvement plateaus before reaching practical model sizes.

## Foundational Learning

- Concept: Token-level probability aggregation
  - Why needed here: SOFT-SC relies on aggregating token probabilities to score actions; understanding aggregation methods (min, mean, product) is crucial.
  - Quick check question: How would you compute the score for an action "ls -l" using mean probability aggregation?

- Concept: Self-consistency and majority voting
  - Why needed here: SOFT-SC is a variant of self-consistency; understanding the limitations of majority voting in sparse action spaces is key to appreciating SOFT-SC's improvements.
  - Quick check question: Why does majority voting fail when there are many valid, distinct actions?

- Concept: Calibration and its impact on confidence scoring
  - Why needed here: The paper investigates whether calibration affects SOFT-SC's performance; understanding calibration metrics (ECE, AUROC) is important.
  - Quick check question: What does a low Expected Calibration Error (ECE) indicate about a model's confidence scores?

## Architecture Onboarding

- Component map: Input -> LLM sampling -> Scoring module (token likelihoods) -> Selection module (highest score) -> Execution module -> Environment feedback loop

- Critical path: 1. Receive task description and environment state 2. Generate k action trajectories using LLM sampling 3. Score each action using aggregated token probabilities 4. Select action with highest score 5. Execute selected action in environment 6. Receive observation and reward

- Design tradeoffs: Scoring method (mean vs. min vs. product aggregation) affects sensitivity to low-probability tokens; sample size (k) balances diversity against computational cost; threshold selection balances early stopping efficiency with quality assurance.

- Failure signatures: SOFT-SC underperforms SC (scoring method poorly captures action quality); no improvement with increased k (actions too diverse or scoring ineffective); performance degrades with larger models (token likelihoods not better calibrated).

- First 3 experiments: 1. Implement SOFT-SC with mean probability aggregation on Bash dataset; compare against greedy decoding and SC with k=5. 2. Vary aggregation method (min, mean, product) on WebShop dev set; select best method and evaluate on test set. 3. Implement adaptive SOFT-SC with threshold tuning on ALFWorld dev set; compare sample efficiency against adaptive SC.

## Open Questions the Paper Calls Out

### Open Question 1
Does SOFT-SC maintain its performance advantages when applied to other types of sequential decision-making tasks beyond the three evaluated (bash programming, online shopping, and household games)? Basis: The paper demonstrates effectiveness on three specific interactive domains but does not explore broader applicability to other sequential decision-making tasks. Why unresolved: Evaluation is limited to three specific domains, leaving open whether advantages generalize to other task types. What evidence would resolve it: Testing SOFT-SC on diverse sequential decision-making tasks and comparing performance to SC across these domains.

### Open Question 2
How does SOFT-SC's performance change when the underlying language model's probability estimates are systematically biased or poorly calibrated? Basis: The paper investigates calibration but finds only moderate negative correlation with AUROC and no significant correlation with ECE, concluding well-calibrated models are not required. Why unresolved: Does not explore scenarios with systematically biased probability estimates or different types of calibration errors. What evidence would resolve it: Systematic experiments varying calibration properties and measuring impact on SOFT-SC performance.

### Open Question 3
What is the computational trade-off between SOFT-SC and SC when scaling to very large action spaces or very long action sequences? Basis: The paper demonstrates better scaling with model size and efficiency gains but does not extensively analyze computational complexity as action space or sequence length grows. Why unresolved: Shows SOFT-SC requires fewer samples but lacks detailed analysis of computational requirements scaling. What evidence would resolve it: Detailed computational complexity analysis and empirical measurements of runtime and memory usage as action space size and sequence length increase.

## Limitations

- Aggregation methods are chosen heuristically without rigorous justification for their selection in different domains
- The relationship between token likelihoods and action quality is assumed but not empirically validated
- Claims about scaling with model size rely on a single model family (CodeLlama) without broader validation

## Confidence

**High Confidence Claims:**
- SOFT-SC outperforms SC in sparse action spaces (verified across three distinct domains)
- SOFT-SC requires fewer samples than SC for comparable performance (demonstrated with concrete k=5 vs k=10 comparisons)
- SOFT-SC maintains advantages across model sizes (7B-70B parameters on CodeLlama models)

**Medium Confidence Claims:**
- The relationship between token likelihoods and action quality is assumed but not directly tested
- The selection of aggregation methods is heuristic rather than theoretically grounded
- Calibration is not required for SOFT-SC's effectiveness, though this is asserted rather than proven

**Low Confidence Claims:**
- Claims about scaling with model size rely on a single model family without broader validation
- The paper does not explore the computational overhead of likelihood calculation versus the sample efficiency gains
- No analysis of how SOFT-SC performs when actions have similar likelihood distributions but different qualities

## Next Checks

1. **Correlation Analysis**: Conduct experiments to empirically validate the core assumption that token likelihoods correlate with action quality by comparing SOFT-SC scores against ground truth action correctness across different aggregation methods.

2. **Aggregation Method Benchmarking**: Systematically compare min, mean, and product aggregation methods across all three domains using the same models to determine if the current heuristic choices are optimal or if a domain-agnostic method exists.

3. **Calibration Impact Study**: Design experiments with both calibrated and uncalibrated versions of the same model to test whether calibration status affects SOFT-SC's performance, particularly focusing on models known to have poor calibration.