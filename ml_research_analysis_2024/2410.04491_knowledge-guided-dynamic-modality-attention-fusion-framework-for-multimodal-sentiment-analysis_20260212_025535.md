---
ver: rpa2
title: Knowledge-Guided Dynamic Modality Attention Fusion Framework for Multimodal
  Sentiment Analysis
arxiv_id: '2410.04491'
source_url: https://arxiv.org/abs/2410.04491
tags:
- modality
- sentiment
- multimodal
- text
- kuda
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KuDA, a knowledge-guided dynamic modality
  attention fusion framework for multimodal sentiment analysis. KuDA dynamically adjusts
  the contribution of each modality based on their importance in a given sample, rather
  than treating them equally or always using text as the dominant modality.
---

# Knowledge-Guided Dynamic Modality Attention Fusion Framework for Multimodal Sentiment Analysis

## Quick Facts
- **arXiv ID**: 2410.04491
- **Source URL**: https://arxiv.org/abs/2410.04491
- **Reference count**: 12
- **Key outcome**: KuDA achieves state-of-the-art performance on four benchmark datasets, outperforming existing methods by 8.32% on CH-SIMSv2 and improving accuracy on MOSI and MOSEI.

## Executive Summary
This paper introduces KuDA, a knowledge-guided dynamic modality attention fusion framework for multimodal sentiment analysis. Unlike existing methods that treat modalities equally or statically prioritize text, KuDA dynamically adjusts each modality's contribution based on its importance for each sample. The framework injects sentiment knowledge into unimodal representations using adapters and employs a dynamic attention fusion module to select the dominant modality for sentiment analysis. Extensive experiments on four benchmark datasets demonstrate that KuDA achieves SOTA performance, with ablation studies confirming the effectiveness of each component.

## Method Summary
KuDA uses a two-stage training approach: first pretraining unimodal sentiment knowledge via adapters and decoders, then training the full model with dynamic attention fusion and correlation estimation loss. The framework extracts features using pretrained encoders (BERT for text, Transformer Encoders for vision/audio), injects sentiment knowledge through adapters, computes sentiment ratios from unimodal predictions, performs dynamic attention fusion across modality levels, applies correlation estimation loss, and predicts final sentiment scores.

## Key Results
- KuDA achieves state-of-the-art performance on CH-SIMSv2, outperforming existing methods by 8.32%
- The framework improves accuracy on MOSI and MOSEI datasets compared to baseline approaches
- Ablation studies confirm the effectiveness of each component, particularly the dynamic attention fusion module

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic attention fusion allows the model to adaptively select the dominant modality for each sample rather than treating modalities equally or statically prioritizing text.
- Mechanism: KuDA uses sentiment knowledge injection to compute sentiment ratios for each modality, then dynamically adjusts attention weights during fusion based on these ratios and cross-modal attention similarity.
- Core assumption: The difference between unimodal and multimodal sentiment scores inversely correlates with a modality's contribution weight, and this relationship can be modeled with an exponential function.
- Evidence anchors:
  - [abstract]: "KuDA dynamically adjusts the contribution of each modality based on their importance in a given sample"
  - [section]: "we choose the inverse proportional function exp (−kx) and normalization operation...to convert the unimodal sentiment score into sentiment ratio Rm"
  - [corpus]: Weak - corpus mentions similar dynamic attention methods but lacks direct comparison to KuDA's specific approach
- Break condition: If the sentiment knowledge injection fails to accurately predict unimodal sentiment scores, the derived sentiment ratios become noisy, breaking the dynamic attention mechanism.

### Mechanism 2
- Claim: Knowledge injection through adapters enhances unimodal representations with sentiment-specific information, improving modality selection accuracy.
- Mechanism: BERT and Transformer encoders extract semantic features, which are then passed through adapters that inject sentiment knowledge. This creates knowledge-sentiment representations used for both unimodal prediction and dynamic fusion guidance.
- Core assumption: Adapters can effectively inject sentiment knowledge into pretrained models without catastrophic forgetting of general semantic information.
- Evidence anchors:
  - [section]: "we use the adapter to inject unimodal sentiment knowledge...the adapter is plugged outside of the encoder and stacked with identical blocks"
  - [abstract]: "injects sentiment knowledge into unimodal representations and uses a dynamic attention fusion module"
  - [corpus]: Missing - corpus lacks specific evidence about adapter-based knowledge injection effectiveness
- Break condition: If adapters overfit to sentiment knowledge during pretraining, they may lose general semantic understanding needed for accurate multimodal fusion.

### Mechanism 3
- Claim: Correlation estimation loss reinforces the contribution of the dominant modality by maximizing similarity between multimodal representations and their corresponding unimodal knowledge-enhanced representations.
- Mechanism: KuDA uses Noise-Contrastive Estimation to compute a correlation loss between the final multimodal representation and each unimodal representation, encouraging the multimodal output to retain dominant modality information.
- Core assumption: Maximizing correlation between multimodal and dominant unimodal representations improves sentiment prediction by preserving modality-specific sentiment clues.
- Evidence anchors:
  - [section]: "we estimate the correlation of the multimodal representation F L and the unimodal representations U m...through the Contrastive Predictive Coding...to form the correlation estimation (CE) loss"
  - [abstract]: "with the obtained multimodal representation, the model can further highlight the contribution of dominant modality through the correlation evaluation loss"
  - [corpus]: Weak - corpus mentions correlation-based methods but doesn't specifically address this correlation estimation approach
- Break condition: If the CE loss weight (α) is too high, the model may over-prioritize dominant modality features at the expense of beneficial cross-modal interactions.

## Foundational Learning

- Concept: Multimodal sentiment analysis fundamentals
  - Why needed here: Understanding how different modalities (text, vision, audio) contribute to sentiment expression is crucial for grasping why dynamic modality attention matters
  - Quick check question: Why might treating all modalities equally in sentiment analysis lead to suboptimal performance?

- Concept: Attention mechanisms and dynamic weighting
  - Why needed here: KuDA's core innovation relies on dynamically adjusting attention weights based on modality importance, requiring understanding of attention mechanisms
  - Quick check question: How does cross-modal attention differ from standard self-attention in transformer architectures?

- Concept: Contrastive learning and Noise-Contrastive Estimation
  - Why needed here: The correlation estimation loss uses NCE framework, which requires understanding of contrastive learning principles
  - Quick check question: What is the key difference between contrastive learning and traditional supervised learning objectives?

## Architecture Onboarding

- Component map: Feature Extractors (BERT for text, Transformer Encoders for vision/audio) -> Adapters (knowledge injection) -> Decoders (unimodal sentiment prediction) -> Sentiment Ratio Computation -> Dynamic Attention Fusion Blocks -> Correlation Estimation -> MLP Output
- Critical path: Feature extraction → knowledge injection → dynamic attention fusion → correlation estimation → final prediction
- Design tradeoffs:
  - Two-stage training vs. end-to-end: Two-stage prevents overwriting pretrained knowledge but introduces error propagation risk
  - Fixed sentiment ratios during inference vs. dynamic: Fixed ratios simplify inference but lose sample-specific adaptation
  - Adapter-based knowledge injection vs. fine-tuning: Adapters preserve general knowledge but may be less flexible than full fine-tuning
- Failure signatures:
  - Performance degradation when removing any single modality indicates successful dynamic adaptation
  - Correlation between unimodal sentiment scores and final predictions should be lower than with static fusion methods
  - Attention weight visualizations should show dominant modality concentration varying by sample
- First 3 experiments:
  1. Ablation study removing the dynamic attention fusion module to measure performance drop and verify its contribution
  2. Visualization of attention weight distributions across different samples to confirm dynamic modality selection
  3. Comparison of correlation between multimodal representations and unimodal features with and without correlation estimation loss

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of KuDA change when using different pretrained language models for knowledge injection, such as RoBERTa or XLNet, instead of BERT?
- Basis in paper: [inferred] The paper mentions using BERT for text modality, but does not explore the impact of using other pretrained language models.
- Why unresolved: The paper focuses on BERT as the text encoder and does not compare its performance with other models.
- What evidence would resolve it: Conducting experiments with different pretrained language models for knowledge injection and comparing their performance on the MSA task.

### Open Question 2
- Question: What is the impact of using different sentiment analysis datasets for pretraining the knowledge injection module on the final performance of KuDA?
- Basis in paper: [explicit] The paper mentions using CH-SIMS and CH-SIMSv2 datasets for pretraining, but does not explore the effect of using other sentiment analysis datasets.
- Why unresolved: The paper uses specific datasets for pretraining without comparing the results with other datasets.
- What evidence would resolve it: Conducting experiments with different sentiment analysis datasets for pretraining and comparing the performance of KuDA on the MSA task.

### Open Question 3
- Question: How does the performance of KuDA change when using different fusion strategies, such as concatenation or addition, instead of the dynamic attention fusion module?
- Basis in paper: [explicit] The paper introduces the dynamic attention fusion module but does not compare its performance with other fusion strategies.
- Why unresolved: The paper focuses on the proposed dynamic attention fusion module without exploring alternative fusion methods.
- What evidence would resolve it: Conducting experiments with different fusion strategies and comparing their performance with the dynamic attention fusion module on the MSA task.

## Limitations

- The paper lacks detailed ablation studies isolating the specific contributions of adapter-based knowledge injection from other components
- Reliance on external unimodal sentiment labels for pretraining introduces potential data leakage concerns that aren't fully addressed
- The corpus provides limited direct evidence about the effectiveness of the adapter-based knowledge injection mechanism

## Confidence

- **High confidence**: Core claim that KuDA achieves SOTA performance on four benchmark datasets, supported by extensive quantitative comparisons
- **Medium confidence**: Dynamic attention mechanism's effectiveness, as demonstrated but could benefit from more granular analysis
- **Low confidence**: Adapter-based knowledge injection mechanism, lacking detailed ablation studies and corpus evidence

## Next Checks

1. **Ablation study**: Remove the correlation estimation loss entirely and measure performance degradation to quantify its specific contribution versus the dynamic attention fusion alone.

2. **Cross-dataset generalization**: Train KuDA on one dataset (e.g., MOSI) and evaluate on another (e.g., MOSEI) without fine-tuning to test the robustness of the dynamic attention mechanism across different data distributions.

3. **Attention visualization analysis**: Generate and analyze attention weight distributions across samples, particularly focusing on cases where audio or vision unexpectedly becomes the dominant modality, to verify the mechanism's decision-making process.