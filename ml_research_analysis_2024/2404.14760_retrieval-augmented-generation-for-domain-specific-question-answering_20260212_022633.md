---
ver: rpa2
title: Retrieval Augmented Generation for Domain-specific Question Answering
arxiv_id: '2404.14760'
source_url: https://arxiv.org/abs/2404.14760
tags:
- adobe
- language
- retriever
- documents
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a retrieval-augmented question-answering system
  for Adobe products. The core idea is to train a retriever on domain-specific data
  (Adobe help documents and user click logs) using contrastive learning, and then
  finetune a large language model to generate answers based on the retrieved passages.
---

# Retrieval Augmented Generation for Domain-specific Question Answering

## Quick Facts
- arXiv ID: 2404.14760
- Source URL: https://arxiv.org/abs/2404.14760
- Reference count: 20
- Key outcome: Retrieval-augmented QA system for Adobe products using contrastive learning for retriever training and LLM finetuning, achieving relevance score of 0.515 on Acrobat QA test set

## Executive Summary
This paper presents a retrieval-augmented question-answering system specifically designed for Adobe products. The system addresses the challenge of providing accurate, up-to-date, and concise answers to user queries about Adobe products by combining domain-specific retrieval with fine-tuned generation. The approach leverages Adobe's help documents and user click logs to train both a retriever and generator model, aiming to reduce hallucinations and improve answer quality compared to general-purpose models like ChatGPT and GPT-4.

## Method Summary
The proposed system employs a two-stage approach: first, a retriever is trained on domain-specific data using contrastive learning to identify relevant passages from Adobe help documents. The retriever is fine-tuned on curated positive and negative passages to improve its ability to match user queries with appropriate content. Second, a large language model is fine-tuned to generate answers based on the retrieved passages. The system also incorporates query augmentation through product identification to handle ambiguous queries that could apply to multiple Adobe products. This end-to-end approach aims to provide more accurate and contextually relevant answers specific to Adobe products compared to general-purpose models.

## Key Results
- Achieves relevance score of 0.515 on Acrobat QA test set, outperforming embedding model baselines combined with GPT-4
- Demonstrates improved performance compared to GPT-4 (0.705) on domain-specific queries
- Successfully reduces hallucinations by grounding answers in retrieved passages from official documentation
- Shows effectiveness of query augmentation via product identification for handling ambiguous queries

## Why This Works (Mechanism)
The system works by combining the strengths of information retrieval with language generation in a domain-specific context. The retriever, trained with contrastive learning on Adobe's proprietary data, can effectively identify relevant passages from help documents that match user queries. This retrieved context is then used by the fine-tuned generator to produce answers that are grounded in actual documentation rather than relying solely on the model's parametric knowledge. The query augmentation step helps disambiguate queries that could apply to multiple products, ensuring the retriever searches the most relevant document subset. This approach addresses the limitations of both standalone retrieval systems (which may return irrelevant or incomplete information) and general-purpose LLMs (which may hallucinate or lack up-to-date product-specific information).

## Foundational Learning
- **Contrastive learning for retrievers**: Why needed - to effectively learn representations that distinguish relevant from irrelevant passages in domain-specific context; Quick check - verify that positive/negative pairs are properly constructed and balanced
- **Domain-specific fine-tuning**: Why needed - to adapt general-purpose models to the terminology, structure, and requirements of Adobe product documentation; Quick check - ensure training data covers diverse query types and product features
- **Query augmentation via product identification**: Why needed - to handle ambiguous queries that could apply to multiple products and improve retrieval precision; Quick check - validate accuracy of product classification on ambiguous queries
- **Retrieval-augmented generation pipeline**: Why needed - to combine precise information retrieval with coherent answer generation while reducing hallucinations; Quick check - measure hallucination rate compared to baseline models
- **Click log utilization**: Why needed - to incorporate real user behavior patterns into training for more user-aligned retrieval; Quick check - ensure click logs represent diverse user intents and are properly anonymized
- **Relevance scoring methodology**: Why needed - to quantitatively evaluate answer quality in a domain-specific context; Quick check - validate scoring rubric against human judgment on diverse queries

## Architecture Onboarding

Component map: User Query -> Product Identification -> Retriever (fine-tuned) -> Retrieved Passages -> Generator (fine-tuned) -> Answer

Critical path: User Query → Product Identification → Retriever → Generator → Answer

Design tradeoffs:
1. Domain-specific vs. general-purpose training: Specialized training on Adobe data improves domain accuracy but limits generalizability
2. Retriever complexity vs. speed: More sophisticated retrievers may improve accuracy but increase latency
3. Fine-tuning extent vs. hallucination risk: More fine-tuning improves domain fit but may increase hallucination risk if not properly grounded

Failure signatures:
- High precision but low recall in retrieval: indicates overly strict similarity thresholds or insufficient training data diversity
- Generated answers that contradict retrieved passages: suggests generator not properly conditioned on retrieved context
- Slow response times: may indicate inefficient retriever indexing or model size issues
- Product misidentification leading to irrelevant answers: suggests query augmentation module needs improvement

First experiments:
1. Ablation study removing query augmentation to measure its impact on ambiguous query handling
2. Comparison of different retriever architectures (dense vs. sparse) on retrieval accuracy
3. Testing with out-of-domain queries to evaluate generalization boundaries

## Open Questions the Paper Calls Out
None

## Limitations
- Domain-specificity constraints limit generalizability to other domains without substantial retraining
- Proprietary nature of training data prevents independent verification of results
- Resource-intensive approach requiring training both retriever and generator models
- Evaluation appears limited to a single domain (Adobe products) and dataset

## Confidence
- High confidence: The core methodology of using contrastive learning for retriever training and finetuning for generation is technically sound and well-established
- Medium confidence: Performance metrics and comparisons with GPT-4 are difficult to fully verify without access to proprietary test set
- Medium confidence: The claim of reduced hallucinations is plausible but evaluation methodology is not fully detailed

## Next Checks
1. Cross-domain evaluation: Test the system on help documents from multiple different product domains to assess generalizability
2. Ablation studies: Systematically remove components to quantify their individual contributions to overall performance
3. Long-term performance monitoring: Deploy in production for 3-6 months to evaluate real-world performance and degradation over time