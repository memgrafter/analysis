---
ver: rpa2
title: 'DREAM: Domain-agnostic Reverse Engineering Attributes of Black-box Model'
arxiv_id: '2412.05842'
source_url: https://arxiv.org/abs/2412.05842
tags:
- target
- domain
- black-box
- training
- outputs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reverse engineering attributes
  of black-box models without access to the target model's training dataset. The authors
  propose DREAM, a domain-agnostic framework that casts this problem as out-of-distribution
  (OOD) generalization.
---

# DREAM: Domain-agnostic Reverse Engineering Attributes of Black-box Model

## Quick Facts
- arXiv ID: 2412.05842
- Source URL: https://arxiv.org/abs/2412.05842
- Reference count: 40
- This paper proposes a domain-agnostic framework for reverse engineering attributes of black-box models without access to training data.

## Executive Summary
This paper addresses the challenging problem of reverse engineering attributes of black-box models without access to the target model's training dataset. The authors propose DREAM, a domain-agnostic framework that casts this problem as out-of-distribution (OOD) generalization. By learning domain-invariant features from white-box model outputs across multiple domains, DREAM can infer attributes of black-box models even when the target domain is unknown. The framework employs a multi-discriminator GAN to learn these invariant features, which are then used by a domain-agnostic meta-model for attribute prediction.

## Method Summary
DREAM uses a multi-discriminator GAN (MDGAN) to learn domain-invariant features from the probability outputs of white-box models trained on multi-domain data. The MDGAN consists of a generator that maps multi-domain outputs into a shared latent space and multiple discriminators that enforce domain alignment. Once trained, the generator produces domain-invariant features that preserve attribute-relevant information while removing domain-specific variations. A domain-agnostic meta-model is then trained on these features to classify the attributes of black-box models. The approach requires datasets with overlapping label spaces between source and target domains to ensure sufficient information transfer.

## Key Results
- DREAM achieves higher accuracy in attribute classification compared to baselines like KENNEN, SVM, and other OOD generalization techniques
- The method demonstrates robustness in scenarios with domain shift and different numbers of classes between source and target domains
- Experimental results on PACS and MEDU datasets show consistent performance improvements over existing methods

## Why This Works (Mechanism)

### Mechanism 1
Domain-agnostic reverse engineering works because it learns domain-invariant features from model outputs, bypassing the need for the target model's training data. The MDGAN architecture maps multi-domain outputs into a shared latent space where discriminators enforce domain alignment, enabling the meta-model to generalize across unseen domains. This assumes outputs from white-box models trained on different domains contain sufficient information to infer model attributes even when the target domain is unknown.

### Mechanism 2
Multi-discriminator GAN effectively learns domain-invariant features from probability outputs rather than traditional images. Multiple discriminators create adversarial pressure forcing the generator to produce features that cannot be distinguished by domain, while the meta-model learns to classify attributes from these invariant features. This assumes probability outputs from different domains can be embedded into a shared feature space that preserves attribute-relevant information while removing domain-specific variations.

### Mechanism 3
The overlapping label space between source and target domains ensures sufficient information overlap for attribute inference. By collecting data with overlapping label spaces, the outputs of white-box models include similar information to the black-box model, enabling effective domain-invariant feature learning. This assumes that even with different data styles, models trained on different domains with overlapping label spaces produce outputs that contain comparable attribute-relevant information.

## Foundational Learning

- **Concept: Out-of-Distribution (OOD) Generalization**
  - Why needed here: The target black-box model is trained on an unknown domain, requiring the model to generalize beyond the training domains.
  - Quick check question: What happens to the meta-model's performance when the target domain is completely outside the source domain distribution?

- **Concept: Generative Adversarial Networks (GANs)**
  - Why needed here: GANs are used to learn domain-invariant features by creating adversarial pressure between generators and discriminators.
  - Quick check question: How does the multi-discriminator setup differ from standard GANs in terms of feature learning objectives?

- **Concept: Domain Adaptation vs. Domain Generalization**
  - Why needed here: Unlike domain adaptation, we cannot access target domain data during training, requiring true generalization capabilities.
  - Quick check question: What is the key difference in training strategy between domain adaptation and the approach used in DREAM?

## Architecture Onboarding

- **Component map**: MDGAN (Generator + M Discriminators) → Domain-Agnostic Meta-Model → Attribute Prediction
- **Critical path**: Query white-box models → Generate multi-domain outputs → MDGAN feature learning → Meta-model training → Black-box inference
- **Design tradeoffs**: More discriminators improve domain alignment but increase computational cost; larger modelsets improve attribute coverage but require more training time
- **Failure signatures**: Poor performance on specific attributes indicates insufficient information preservation; domain shift causes performance degradation; mode collapse in GAN training
- **First 3 experiments**:
  1. Train MDGAN on PACS modelset with Photo/Sketch as sources and Cartoon as target, measure attribute accuracy
  2. Test performance degradation when removing label space overlap between source and target domains
  3. Compare MDGAN feature learning against direct MLP baseline on the same multi-domain outputs

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of DREAM scale with the number of white-box models trained, and is there a point of diminishing returns? The paper mentions constructing modelsets with different sizes (1K to 5K) and observes that performance fluctuates without consistently increasing. A systematic study varying the number of white-box models and measuring performance would identify the optimal size and any diminishing returns.

### Open Question 2
Can DREAM be extended to handle scenarios where the label space of the target black-box model is not fully known or overlaps only partially with the source domains? The paper assumes complete overlap of label spaces between source and target domains, but this may not always be the case in real-world applications. Experiments testing DREAM's performance with varying degrees of label space overlap would validate this assumption.

### Open Question 3
How does DREAM perform in terms of computational efficiency compared to other black-box attribute inference methods, especially when dealing with large-scale models? The paper acknowledges the significant computational resources required to train numerous white-box models with different attribute combinations. A comprehensive analysis of computational resources required by DREAM and other methods would inform practical deployment considerations.

## Limitations

- The paper's reliance on label space overlap between source and target domains is not well-validated across varying degrees of overlap
- The MDGAN architecture lacks detailed specifications that would enable faithful reproduction
- Experimental validation is limited to two datasets with relatively modest domain shifts

## Confidence

- **High Confidence**: The core framework of using OOD generalization for attribute reverse engineering is theoretically sound and well-motivated
- **Medium Confidence**: The MDGAN approach for learning domain-invariant features from probability outputs is novel but lacks comparative ablation studies
- **Low Confidence**: Claims about robustness to varying numbers of classes and extreme domain shifts are not adequately supported by experimental evidence

## Next Checks

1. **Label Space Overlap Analysis**: Systematically vary the degree of label space overlap between source and target domains to quantify the minimum overlap required for effective attribute inference
2. **Extreme Domain Shift Test**: Evaluate performance when target domain is drawn from completely different data modalities (e.g., medical images vs. natural images) to assess true generalization capabilities
3. **Architecture Ablation Study**: Compare MDGAN feature learning against simpler baselines like direct MLP on multi-domain outputs to isolate the contribution of the adversarial training approach