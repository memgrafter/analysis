---
ver: rpa2
title: 'PDDLEGO: Iterative Planning in Textual Environments'
arxiv_id: '2405.19793'
source_url: https://arxiv.org/abs/2405.19793
tags:
- file
- problem
- room
- location
- coin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PDDLEGO, a method for tackling partially-observed
  environments by iteratively constructing a planning representation (PDDL) during
  exploration. Unlike prior work limited to fully-observed settings, PDDLEGO recursively
  refines the problem file using environmental observations to plan sub-goals until
  the end-goal is reached.
---

# PDDLEGO: Iterative Planning in Textual Environments

## Quick Facts
- arXiv ID: 2405.19793
- Source URL: https://arxiv.org/abs/2405.19793
- Reference count: 11
- 43% more efficient plans than end-to-end LLM planning on Coin Collector; 98% success rate on Cooking World vs 4% for end-to-end LLMs

## Executive Summary
PDDLEGO introduces a method for tackling partially-observed environments by iteratively constructing PDDL representations during exploration. Unlike prior work limited to fully-observed settings, PDDLEGO recursively refines problem files using environmental observations to plan sub-goals until the end-goal is reached. Evaluated on Coin Collector and Cooking World simulations, PDDLEGO achieves 43% more efficient plans than end-to-end LLM planning on Coin Collector and 98% success rate on Cooking World, where end-to-end LLMs fail (4%). The approach improves efficiency, stability, and interpretability, though it requires domain file annotation and is slower than direct action generation.

## Method Summary
PDDLEGO uses LLMs to iteratively construct and refine PDDL problem files from textual observations in partially-observed environments. The agent explores, generates PDDL representations, and uses symbolic planners to find action sequences. When the end-goal cannot be directly planned, the method falls back to sub-goals (e.g., reaching unvisited rooms) that enable further exploration. Three approaches are compared: Action-gen (LLM directly generates actions), PDDL-gen (LLM generates complete problem files), and PDDL-edit (LLM generates incremental edits to problem files). The method requires hand-annotated domain files and tests on Coin Collector and Cooking World simulations using GPT-3.5 Turbo and GPT-4 Turbo.

## Key Results
- 43% more efficient plans than end-to-end LLM planning on Coin Collector simulation
- 98% success rate on Cooking World vs 4% for end-to-end LLM approaches
- Better stability with smaller variance across runs compared to direct action generation
- Requires domain file annotation but provides more interpretable and debuggable solutions

## Why This Works (Mechanism)

### Mechanism 1: Symbolic Planning Efficiency
PDDLEGO leverages deterministic symbolic planners instead of LLM-generated actions, removing variability and errors inherent in LLM-generated actions. By translating textual observations into PDDL problem files, classical planners generate more reliable action sequences, especially in complex environments with many action types and strict preconditions.

### Mechanism 2: Iterative Exploration for Partial Observability
The method handles partially-observed environments by iteratively refining the PDDL representation through exploration. The agent discovers unknown goal states and connectivity information that cannot be specified in a single-shot problem file, falling back to sub-goals when the end-goal is initially unreachable.

### Mechanism 3: Separation of Parsing and Planning
PDDLEGO improves stability and interpretability by separating environment parsing from planning. LLMs only handle parsing observations into PDDL format while deterministic planners handle the planning task, ensuring consistent plans across runs and making failures easier to diagnose.

## Foundational Learning

- **PDDL and symbolic planning**: Why needed - PDDLEGO relies on translating textual observations into PDDL problem files that can be solved by classical planners. Quick check - What are the two main components of a PDDL instance, and what does each define?

- **Partially-observed environments and exploration**: Why needed - PDDLEGO specifically addresses environments where not all information is available initially and must be discovered through exploration. Quick check - Why can't traditional single-shot PDDL generation methods work in partially-observed environments?

- **LLM prompting for structured output**: Why needed - PDDLEGO uses LLMs to generate PDDL problem files and edits, requiring careful prompt design to produce valid PDDL. Quick check - What prompt technique does PDDLEGO use to improve PDDL generation reliability compared to direct generation?

## Architecture Onboarding

- **Component map**: Environment simulator -> LLM API wrapper -> PDDL parser and validator -> Classical planner -> Sub-goal manager -> Execution controller

- **Critical path**: Observation → LLM PDDL generation → PDDL validation → Planning → Action execution → New observation

- **Design tradeoffs**: PDDL generation vs. direct action generation (slower but more reliable); full problem file generation vs. incremental editing (faster but may accumulate inconsistencies); fixed sub-goals vs. dynamic sub-goal generation (simpler but less flexible)

- **Failure signatures**: Planner fails to find solution (likely PDDL representation error); LLM generates invalid PDDL (prompt engineering issue); agent gets stuck in loops (sub-goal structure inadequate)

- **First 3 experiments**: 1) Implement Coin Collector with PDDL-edit approach using a simple domain file; 2) Add error handling for invalid PDDL generation (retries, validation); 3) Implement sub-goal fallback mechanism for when end-goal planning fails

## Open Questions the Paper Calls Out

### Open Question 1
How does PDDLEGO scale to environments with significantly larger state spaces and more complex sub-goal hierarchies? The paper mentions manual annotation becomes impractical for large environments, but tests are limited to small-scale simulations.

### Open Question 2
What is the theoretical relationship between the quality of LLM-generated PDDL and the performance of downstream symbolic planners? The paper shows dependence between PDDL quality and planning success but doesn't analyze specific PDDL errors or quantify planner tolerance to imperfections.

### Open Question 3
How would PDDLEGO perform with alternative sub-goal selection strategies beyond the monotonic "unvisited room" approach? Only one sub-goal strategy is tested, and the paper acknowledges its limitations for general applicability.

### Open Question 4
What is the trade-off between PDDLEGO's planning efficiency and the computational cost of iterative PDDL generation? The paper states PDDLEGO is "about 5x slower" than direct action generation but doesn't analyze the overall resource consumption including LLM API costs and generation time.

## Limitations

- Core efficiency claim (43% improvement) lacks ablation studies showing contribution of PDDL correctness versus planner quality versus exploration strategy
- 98% vs 4% success rate comparison may conflate multiple failure modes without systematic analysis of whether failures stem from PDDL generation errors, insufficient exploration, or planner limitations
- Claims about interpretability improvements are largely anecdotal without systematic evaluation of failure diagnosis efficiency

## Confidence

- **High**: The mechanism of separating environment parsing from planning is well-established in classical AI planning literature and implementation details are sufficiently specified
- **Medium**: Efficiency and stability improvements are supported by experimental results but lack robustness analysis across different environments and domain complexities
- **Low**: Claims about interpretability improvements are largely anecdotal, relying on qualitative statements about human debugging rather than systematic evaluation

## Next Checks

1. **Ablation Study**: Run experiments comparing PDDLEGO against versions with controlled PDDL generation errors to isolate the contribution of symbolic planning versus exploration strategy to the reported efficiency gains.

2. **Failure Mode Analysis**: Systematically categorize and quantify failure types (invalid PDDL, insufficient exploration, planner timeout) across all experimental conditions to understand the reliability breakdown.

3. **Domain Complexity Scaling**: Test PDDLEGO on environments with increasing action types and connectivity complexity to determine where the approach breaks down and how performance degrades relative to the reported benchmarks.