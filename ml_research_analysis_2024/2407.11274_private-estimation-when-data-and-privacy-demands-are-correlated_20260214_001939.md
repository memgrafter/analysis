---
ver: rpa2
title: Private Estimation when Data and Privacy Demands are Correlated
arxiv_id: '2407.11274'
source_url: https://arxiv.org/abs/2407.11274
tags:
- privacy
- estimation
- setting
- data
- correlated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work studies mean and frequency estimation under heterogeneous
  differential privacy constraints where users have varying privacy demands. Two settings
  are considered: correlated (arbitrary correlation between data and privacy) and
  weakly-correlated (data permuted randomly before assignment).'
---

# Private Estimation when Data and Privacy Demands are Correlated

## Quick Facts
- arXiv ID: 2407.11274
- Source URL: https://arxiv.org/abs/2407.11274
- Reference count: 40
- One-line primary result: Proposed algorithms for mean and frequency estimation under heterogeneous differential privacy achieve minimax optimal rates in several regimes by assigning different weights to users.

## Executive Summary
This work studies mean and frequency estimation under heterogeneous differential privacy constraints where users have varying privacy demands. Two settings are considered: correlated (arbitrary correlation between data and privacy) and weakly-correlated (data permuted randomly before assignment). For each setting, algorithms are proposed for both PAC and mean-squared error metrics, achieving minimax optimal rates in several regimes. The core method assigns different weights to users instead of equal weighting, balancing bias and privacy noise. Experiments show the proposed algorithms outperform baseline methods, with a fast heuristic variant performing well across all settings and metrics.

## Method Summary
The paper proposes algorithms HPF (Heterogeneous Private Frequency) and HPM (Heterogeneous Private Mean) for frequency and mean estimation respectively under heterogeneous differential privacy. The key innovation is assigning different weights to users based on their privacy demands rather than using uniform weighting. For the correlated setting, weights are optimized to minimize an upper bound on the error metric (PAC or MSE). For the weakly-correlated setting, a random permutation is applied to break correlation between data and privacy demands before applying the weighting scheme. The algorithms add Laplace noise scaled by the maximum weighted privacy level and include clipping to ensure valid outputs. A fast heuristic variant (HPF-A/HPM-A) is also proposed for practical efficiency.

## Key Results
- Algorithms achieve minimax optimal rates in several regimes for both PAC and MSE error metrics.
- Weight optimization via convex programs (cvxpy) or fast heuristic HPF-A/HPM-A.
- Experiments show proposed algorithms outperform baseline methods across correlated and weakly-correlated settings.
- Fast heuristic variant performs well across all settings and metrics, offering practical efficiency.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reweighting users based on privacy demands reduces total noise while maintaining privacy.
- Mechanism: Assign different weights $w_i$ to each user instead of equal weighting. Users with lower privacy demands (higher $\epsilon_i$) get higher weights, reducing the Laplace noise magnitude needed to satisfy DP.
- Core assumption: The total privacy noise scales with $\|\frac{w}{\epsilon}\|_\infty$, so concentrating weight on less private users reduces noise.
- Evidence anchors:
  - [abstract] "The core method assigns different weights to users instead of equal weighting, balancing bias and privacy noise."
  - [section] "Broadly, our proposed algorithms assign different weights to different users instead of equally weighing the users for the estimation task."
  - [corpus] Weak corpus support; neighboring papers focus on correlated noise or decentralized synthesis, not reweighting.
- Break condition: If privacy demands are uniformly high or if weights concentrate too heavily on few users, bias dominates and estimation becomes poor.

### Mechanism 2
- Claim: Randomly permuting the dataset breaks correlation between data and privacy demand.
- Mechanism: In the weakly-correlated setting, apply a uniformly random permutation $\sigma$ to the dataset before assignment, so user $i$ with privacy demand $\epsilon_i$ receives data point $x_{\sigma(i)}$.
- Core assumption: The permutation decouples data values from privacy demands, eliminating worst-case adversarial correlations.
- Evidence anchors:
  - [abstract] "second, where correlations are weakened by random permutation of the dataset."
  - [section] "Thus, to model such setting without meaningful correlations, we formulate the weakly correlated setting."
  - [corpus] No direct corpus evidence; this is a novel modeling choice in the paper.
- Break condition: If the number of users is small or if the correlation structure is very strong, permutation may not sufficiently decorrelate data and privacy.

### Mechanism 3
- Claim: The optimization of weights balances bias (from unequal weighting) and variance (from privacy noise).
- Mechanism: Solve a quadratic program to find weights $w^*$ minimizing an upper bound on the error metric (PAC or MSE), e.g., $\min_{w \in \Delta_n} \|w - 1/n\|_1^2 + \log^2(k/\beta)\|\frac{w}{\epsilon}\|_\infty^2$.
- Core assumption: The error bound is tight enough that optimizing it yields near-optimal weights for the actual error metric.
- Evidence anchors:
  - [section] "To obtain tight upper bounds on the minimax rates, we consider slightly different algorithms tailored to each combination of problem, setting, and error metric."
  - [section] "These performance guarantees translate to minimax optimality in several instances..."
  - [corpus] Weak corpus evidence; related papers focus on correlated noise, not weight optimization.
- Break condition: If the error bound is loose or if the optimization landscape is ill-conditioned (e.g., extreme privacy demands), the heuristic weights may perform poorly.

## Foundational Learning

- Concept: Differential Privacy (DP) and Heterogeneous Differential Privacy (HDP)
  - Why needed here: The paper studies estimation under DP constraints where each user has a different privacy level. Understanding DP definitions and how privacy scales with $\epsilon$ is crucial.
  - Quick check question: If user $i$ has privacy level $\epsilon_i$, what is the probability ratio bound for neighboring datasets differing only in user $i$?

- Concept: Laplace mechanism and sensitivity
  - Why needed here: The algorithms add Laplace noise scaled by the maximum weighted privacy level $\|\frac{w}{\epsilon}\|_\infty$. Knowing how sensitivity affects noise magnitude is key to understanding the trade-off.
  - Quick check question: If the sensitivity of a query is $\Delta$, what is the variance of Laplace noise added for $\epsilon$-DP?

- Concept: Minimax estimation and PAC vs MSE criteria
  - Why needed here: The paper optimizes for both PAC error (quantile-based) and mean squared error. Understanding these error metrics and their implications for algorithm design is essential.
  - Quick check question: In the PAC framework with error probability $\beta$, what is the target error level $\alpha$ if the algorithm must be within $\alpha$ of the true statistic with probability $1-\beta$?

## Architecture Onboarding

- Component map: Input data and privacy demands -> Weight optimization (QP) or heuristic -> Laplace noise addition scaled by $\|\frac{w}{\epsilon}\|_\infty$ -> Clipping -> Estimated frequencies or mean
- Critical path:
  1. Parse input data and privacy demands
  2. Choose setting (correlated/weakly-correlated) and error metric (PAC/MSE)
  3. Solve weight optimization problem (or use heuristic weights)
  4. Add Laplace noise scaled by $\|\frac{w}{\epsilon}\|_\infty$
  5. Clip output to valid range
  6. Return estimate
- Design tradeoffs:
  - Weight optimization vs heuristic: Optimal weights give best theoretical guarantees but require solving a QP; heuristic weights (e.g., $w \propto 1 - e^{-\epsilon}$) are faster but may be suboptimal.
  - Correlated vs weakly-correlated: Correlated setting allows worst-case data-privacy correlation but has higher error; weakly-correlated breaks correlation via permutation but may not reflect all real-world scenarios.
  - PAC vs MSE: PAC focuses on high-probability error bounds; MSE focuses on average error. Different weight optimizations are needed for each.
- Failure signatures:
  - High error in experiments: May indicate poor weight choice, extreme privacy demands, or small sample size.
  - Numerical instability in weight optimization: May occur with large $n$ or wide range of $\epsilon$ values; consider using the faster heuristic or scaling $\epsilon$.
  - Permutation not breaking correlation: If correlation structure is very strong or $n$ is small, the weakly-correlated setting may not help.
- First 3 experiments:
  1. Synthetic dataset with uniform privacy demands: Verify that algorithm reduces to standard DP frequency/mean estimation.
  2. Synthetic dataset with one user demanding no privacy: Check that algorithm can leverage public data while protecting private users.
  3. Real-world dataset (e.g., UC salary) with privacy demands correlated to salary bins: Compare correlated vs weakly-correlated performance and validate weight optimization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed algorithms perform under a threat model where the adversary does not have access to the privacy demands of the users?
- Basis in paper: [explicit] Section 7 mentions this as a future direction and notes that standard DP definitions may not be suitable for this new threat model.
- Why unresolved: The paper focuses on a strong threat model where the adversary knows the privacy demands. The alternative model with hidden privacy demands is explicitly stated as an open problem.
- What evidence would resolve it: Designing and testing mechanisms that leak limited information about both the dataset and potentially correlated privacy demands under this new threat model would resolve this question.

### Open Question 2
- Question: Can the log(k) factor difference between the upper and lower bounds for Ef_wc(k,ε) be improved?
- Basis in paper: [explicit] Theorem 3 states that Ef_wc(k,ε) ≲ log(k)²Estat(ε) but notes that "The log-factor difference in the upper and lower bounds for Ef_wc(k,ε) can probably be improved with a tighter lower bound."
- Why unresolved: The authors acknowledge the gap but do not provide a tighter lower bound, suggesting this is an open problem.
- What evidence would resolve it: A proof showing either a tighter lower bound that matches the upper bound up to a constant factor, or an improved upper bound that eliminates the log(k) factor.

### Open Question 3
- Question: What happens to the performance guarantees when there are users with no privacy demand (ε→∞), i.e., public data?
- Basis in paper: [inferred] Theorem 2 assumes ∥ε∥∞ ≤ 1 and states "The downside to this regime is that it does not explain what occurs if there are some users desiring no privacy (ε→∞)." However, the algorithm HPF-WP can handle such ε.
- Why unresolved: The analysis breaks down when some users have ε→∞, even though the algorithm itself can handle this case. The relationship between the minimax rate and the presence of public data is not characterized.
- What evidence would resolve it: A theorem that characterizes the minimax rate in the presence of some public data (ε→∞ for some users) and shows how it compares to the case with no public data.

## Limitations

- The theoretical guarantees depend on tight error bounds that may not hold in all regimes, particularly with extreme privacy demand distributions.
- The effectiveness of random permutation in breaking correlation is reasonable but not rigorously validated across diverse correlation structures.
- Performance of heuristic weight schemes versus optimal weights is not thoroughly characterized, particularly in high-dimensional settings.

## Confidence

- **High confidence**: The algorithmic framework and basic privacy guarantees are well-established. The weight optimization approach is sound and the error metrics are correctly defined.
- **Medium confidence**: The theoretical analysis of minimax optimality in several regimes is compelling, but the tightness of error bounds and the general applicability of the weight optimization approach warrant further investigation.
- **Low confidence**: The effectiveness of the random permutation in breaking correlation for all possible correlation structures is not fully explored, and the performance of heuristic weights across diverse settings needs more thorough characterization.

## Next Checks

1. **Error Bound Validation**: Conduct experiments to empirically verify the tightness of the error bounds used for weight optimization, particularly in high-privacy regimes or with extreme privacy demand distributions.
2. **Correlation Structure Analysis**: Test the weakly-correlated setting with various correlation structures (e.g., block correlation, monotonic correlation) to assess the robustness of the permutation approach in breaking correlation.
3. **Heuristic Weight Characterization**: Compare the performance of heuristic weight schemes (e.g., HPF-A/HPM-A) against optimal weights across a wider range of problem parameters, including different data distributions, privacy demand patterns, and sample sizes.