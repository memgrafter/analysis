---
ver: rpa2
title: 'CULTURE-GEN: Revealing Global Cultural Perception in Language Models through
  Natural Language Prompting'
arxiv_id: '2404.10199'
source_url: https://arxiv.org/abs/2404.10199
tags:
- culture
- symbols
- cultures
- generations
- cultural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework for extracting and analyzing global
  cultural perceptions from large language models (LLMs) through natural language
  prompting. The authors generate responses on 8 culture-related topics for 110 countries
  and regions using three state-of-the-art LLMs (GPT-4, LLaMA2-13b, Mistral-7b), then
  extract culture symbols from these generations using an unsupervised method.
---

# CULTURE-GEN: Revealing Global Cultural Perception in Language Models through Natural Language Prompting

## Quick Facts
- arXiv ID: 2404.10199
- Source URL: https://arxiv.org/abs/2404.10199
- Authors: Huihan Li; Liwei Jiang; Jena D. Hwang; Hyunwoo Kim; Sebastin Santy; Taylor Sorensen; Bill Yuchen Lin; Nouha Dziri; Xiang Ren; Yejin Choi
- Reference count: 14
- Key outcome: Large language models exhibit cultural markedness by using distinctive vocabulary and parenthetical explanations to differentiate marginalized cultures from default cultures, with geographic discrepancies across regions.

## Executive Summary
This paper presents a framework for extracting and analyzing global cultural perceptions from large language models (LLMs) through natural language prompting. The authors generate responses on 8 culture-related topics for 110 countries and regions using three state-of-the-art LLMs (GPT-4, LLaMA2-13b, Mistral-7b), then extract culture symbols from these generations using an unsupervised method. The study reveals that models exhibit "cultural markedness" by using distinctive vocabulary (e.g., "traditional") and parenthetical explanations to differentiate marginalized cultures from default cultures, with geographic discrepancies in markedness and uneven diversity across different geographic regions.

## Method Summary
The paper employs natural language prompting to generate culture-conditioned responses from three LLMs (GPT-4, LLaMA2-13b, Mistral-7b) across 110 countries/regions and 8 culture-related topics. Culture symbols are extracted using an unsupervised sentence-probability ranking method, and the study measures cultural markedness, diversity of culture symbols, and analyzes geographic patterns. The analysis examines the correlation between culture symbol diversity and culture-topic frequency in training data, using proxy datasets for open-source models and acknowledging limitations in accessing proprietary model training data.

## Key Results
- Models exhibit cultural markedness through distinctive vocabulary (e.g., "traditional") and parenthetical explanations to differentiate marginalized cultures
- Geographic discrepancy exists in markedness, with Asian, Eastern European, and African-Islamic cultures showing higher levels of marking
- Models show uneven diversity in culture symbols across different geographic regions
- Diversity of culture symbols shows moderate-to-strong correlation with culture-topic frequency in training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Culture-conditioned generations contain "markers" that distinguish marginalized cultures from default cultures
- Mechanism: The LLM assigns distinctive vocabulary (e.g., "traditional") and parenthetical explanations to cultures perceived as non-default, creating an implicit "othering" effect
- Core assumption: The LLM has learned cultural associations that treat certain cultures as default while marking others as "different"
- Evidence anchors:
  - [abstract]: "We discover that culture-conditioned generation consist of linguistic 'markers' that distinguish marginalized cultures apart from default cultures."
  - [section 5.1]: "We discover two types of 'markers' in LLM's culture-conditioned generations: 1) using the word 'traditional' while mentioning culture name (vocabulary) and 2) adding parentheses that explain the generated symbols (parentheses)."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.432, average citations=0.0. Top related titles include works on cultural bias and alignment in LLMs.

### Mechanism 2
- Claim: The diversity of culture symbols extracted from LLM generations correlates with the frequency of culture-topic co-occurrence in training data
- Mechanism: LLM cultural knowledge is shaped by the representation of cultures in training data, with more frequently occurring culture-topic pairs leading to more diverse cultural symbol generation
- Core assumption: Training data distribution directly influences the model's learned cultural associations
- Evidence anchors:
  - [section 5.2]: "We find that the diversity of culture symbols have moderate-to-strong correlation with the co-occurrence frequency of a culture name and topic-related keywords in training corpora"
  - [abstract]: "The diversity of culture symbols shows moderate-to-strong correlation with culture-topic frequency in training data"
  - [corpus]: Limited evidence of direct correlation studies; corpus indicates related work on cultural bias in training data.

### Mechanism 3
- Claim: Geographic regions show different levels of cultural markedness and diversity in LLM generations
- Mechanism: The LLM's perception of cultures is influenced by regional cultural hierarchies and relationships, leading to varying degrees of markedness and diversity across geographic regions
- Core assumption: The LLM has learned regional cultural relationships and hierarchies from training data
- Evidence anchors:
  - [section 5.1]: "We find that models tend to precede generations with the word 'traditional' for cultures in Asian, Eastern European, and African-Islamic countries"
  - [section 5.2]: "We find large discrepancy among geographic regions for all topics and all models"
  - [corpus]: Weak evidence; corpus contains related work on cultural bias but limited specific geographic analysis.

## Foundational Learning

- Concept: Cultural markedness and "othering"
  - Why needed here: Understanding how LLMs distinguish between default and marginalized cultures is central to the paper's findings on cultural bias
  - Quick check question: Can you explain the difference between vocabulary markers (e.g., "traditional") and parentheses markers in the context of cultural markedness?

- Concept: Culture symbols and their extraction
  - Why needed here: The paper's methodology relies on extracting culture symbols from LLM generations to analyze cultural knowledge and representation
  - Quick check question: How does the paper define culture symbols, and what method does it use to extract them from LLM generations?

- Concept: Correlation analysis and geographic regions
  - Why needed here: The paper uses correlation analysis to link cultural diversity to training data frequency and examines geographic patterns in cultural representation
  - Quick check question: What statistical method does the paper use to measure the correlation between cultural diversity and training data frequency, and how does it categorize cultures geographically?

## Architecture Onboarding

- Component map: Prompt generation -> LLM generation (GPT-4, LLaMA2-13b, Mistral-7b) -> Culture symbol extraction -> Markedness analysis -> Diversity calculation -> Geographic pattern analysis -> Correlation analysis

- Critical path:
  1. Prompt generation for all culture-topic combinations
  2. LLM generation with consistent hyperparameters
  3. Culture symbol extraction using GPT-4-turbo-preview
  4. Assignment of symbols to cultures based on probability ranking
  5. Analysis of markedness, diversity, and geographic patterns

- Design tradeoffs:
  - Using natural language prompts vs. structured queries for cultural knowledge extraction
  - Unsupervised symbol extraction vs. human-labeled culture symbols
  - Analysis of model generations vs. probing model internals for cultural knowledge

- Failure signatures:
  - Low diversity of culture symbols across all regions
  - Inconsistent markedness patterns that don't align with known cultural biases
  - Weak or no correlation between cultural diversity and training data frequency

- First 3 experiments:
  1. Reproduce the markedness analysis for a subset of cultures and topics to verify the "othering" phenomenon
  2. Calculate the diversity of culture symbols for a single geographic region and compare it to the overall average
  3. Measure the correlation between cultural diversity and training data frequency for a single topic across all cultures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model size and architecture (e.g., decoder-only vs. encoder-decoder) impact the degree of cultural markedness observed in generations?
- Basis in paper: [inferred] The paper analyzes three models of different sizes (GPT-4, Llama2-13b, Mistral-7b) and observes differences in markedness patterns, but does not explicitly test the effect of architecture or provide a systematic analysis of how size correlates with markedness.
- Why unresolved: The paper does not perform controlled experiments varying model size while holding other factors constant, nor does it compare decoder-only vs. encoder-decoder architectures directly.
- What evidence would resolve it: A controlled study training models of varying sizes with identical architectures and datasets, then measuring markedness in their generations, would clarify the relationship between size, architecture, and markedness.

### Open Question 2
- Question: What is the relationship between the degree of cultural markedness and the downstream performance of LLMs on culturally-sensitive tasks?
- Basis in paper: [explicit] The paper identifies cultural markedness as a phenomenon where LLMs distinguish marginalized cultures using distinctive vocabulary and linguistic structures, but does not investigate how this affects task performance.
- Why unresolved: The paper focuses on analyzing the presence of markedness but does not explore its impact on model behavior in practical applications.
- What evidence would resolve it: Experiments measuring LLM performance on culturally-sensitive tasks (e.g., translation, dialogue, content moderation) while varying the degree of markedness in the model's training data or generations would reveal the practical consequences of this phenomenon.

### Open Question 3
- Question: How do different fine-tuning techniques (e.g., instruction tuning, RLHF) influence the cultural markedness exhibited by LLMs?
- Basis in paper: [explicit] The paper mentions that GPT-4 is an aligned model and discusses the potential effect of alignment and instruction tuning on cultural perceptions, but does not conduct a direct comparison between aligned and unaligned versions of the same model.
- Why unresolved: The paper lacks a systematic analysis of how specific fine-tuning techniques impact markedness, as it only compares different models with different training regimes.
- What evidence would resolve it: Training multiple versions of the same base model with different fine-tuning techniques (e.g., standard fine-tuning, instruction tuning, RLHF) and measuring the degree of markedness in their generations would clarify the impact of these techniques on cultural bias.

## Limitations

- Reliance on GPT-4 for both generating and analyzing culture symbols introduces potential circularity and amplifies its own cultural biases
- Unsupervised culture symbol extraction lacks human validation of whether extracted symbols accurately represent cultural associations
- Use of training data proxies (RedPajama) rather than actual training corpora of proprietary models limits validity of correlation analyses

## Confidence

- High confidence in the basic methodology and data collection approach
- Medium confidence in the markedness analysis and geographic patterns
- Low confidence in the correlation analysis between cultural diversity and training data frequency due to proxy data usage

## Next Checks

1. Conduct a human validation study where cultural experts review and validate the extracted culture symbols against ground truth cultural knowledge for a subset of cultures
2. Perform an ablation study using different culture symbol extraction methods (e.g., supervised vs unsupervised) to assess the robustness of markedness findings
3. Obtain actual training data statistics for GPT-4 (if possible) or use additional proxy datasets to verify the correlation between cultural diversity and training data frequency