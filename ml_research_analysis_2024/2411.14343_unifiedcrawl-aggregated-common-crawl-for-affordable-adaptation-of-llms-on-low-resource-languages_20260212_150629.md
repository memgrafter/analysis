---
ver: rpa2
title: 'UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs on
  Low-Resource Languages'
arxiv_id: '2411.14343'
source_url: https://arxiv.org/abs/2411.14343
tags:
- language
- languages
- dataset
- data
- crawl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents UnifiedCrawl, a method to efficiently collect
  text data for low-resource languages from the entire Common Crawl corpus using minimal
  compute resources. The approach involves filtering and extracting monolingual datasets
  much larger than previously available sources.
---

# UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs on Low-Resource Languages

## Quick Facts
- arXiv ID: 2411.14343
- Source URL: https://arxiv.org/abs/2411.14343
- Authors: Bethel Melesse Tessema; Akhil Kedia; Tae-Sun Chung
- Reference count: 27
- One-line primary result: Efficient data extraction from Common Crawl combined with QLoRA fine-tuning significantly improves LLM performance on low-resource languages.

## Executive Summary
This paper addresses the challenge of improving large language model (LLM) performance on low-resource languages by efficiently extracting large monolingual datasets from the Common Crawl corpus. The proposed UnifiedCrawl method leverages in-memory filtering with DuckDB and HTTP range requests to minimize compute requirements while preserving language-specific content. By fine-tuning multilingual LLMs like XGLM using QLoRA adapters on these extracted datasets, the approach achieves significant improvements in language modeling perplexity and few-shot downstream task performance, demonstrating a practical solution for adapting LLMs to under-resourced languages.

## Method Summary
The UnifiedCrawl pipeline extracts monolingual datasets from Common Crawl by filtering the index using DuckDB to identify WARC files containing target language URLs, then downloading only those files via HTTP range requests to avoid full archive downloads. The extracted text undergoes deduplication using substring methods and filtering of short documents. These large monolingual datasets are then used to fine-tune pre-trained multilingual LLMs (XGLM) using QLoRA, which combines 4-bit quantization with LoRA adapters to enable efficient adaptation on consumer hardware with limited VRAM.

## Key Results
- UnifiedCrawl extracts monolingual datasets much larger than previously available sources for low-resource languages
- QLoRA fine-tuning on UnifiedCrawl datasets significantly reduces language modeling perplexity for low-resource languages
- Few-shot prompting scores on downstream tasks increase substantially, with XGLM-4.5B showing 24% improvement in F1 score

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Efficient data extraction from Common Crawl using DuckDB and HTTP range requests reduces memory and storage requirements while preserving language-specific content.
- Mechanism: The method filters the Common Crawl index in-memory using DuckDB, selects only WARC files containing target language URLs, and downloads them via HTTP range requests to avoid full archive downloads. This approach minimizes RAM and storage needs.
- Core assumption: The Common Crawl index contains accurate primary language annotations for URLs, and the target language content is sufficiently isolated to allow selective extraction.
- Evidence anchors:
  - [abstract]: "filters and extracts common crawl using minimal compute resources, yielding mono-lingual datasets much larger than previously available sources."
  - [section 3.1.1]: "We exploit this information to selectively extract a specific low-resource language from a single archive... Utilizing DuckDB (Raasveldt and MÃ¼hleisen, 2019), an in-memory analytical database management system, to filter the index shards corresponding to our target language."
  - [corpus]: Limited - the evidence relies on the assumption that language annotations are reliable, which is not directly validated in the paper.
- Break condition: If the Common Crawl index has poor or missing primary language annotations, the extraction would yield incomplete or incorrect datasets.

### Mechanism 2
- Claim: QLoRA enables efficient fine-tuning of large multilingual LLMs on consumer hardware by combining 4-bit quantization with LoRA adapters.
- Mechanism: QLoRA quantizes the LLM to 4-bit precision, drastically reducing memory usage, and freezes the base model weights while training only low-rank adapter matrices. This allows adaptation of models like 4.5B parameter XGLM on a single GPU with 10GB VRAM.
- Core assumption: The lightweight adapter matrices can effectively capture task-specific knowledge without full fine-tuning, and the quantization does not significantly degrade model quality.
- Evidence anchors:
  - [abstract]: "leveraging this data to fine-tuning multilingual LLMs via efficient adapter methods (QLoRA) significantly boosts performance on the low-resource language, while minimizing VRAM usage."
  - [section 2.5]: "QLoRA achieves a further reduction in memory usage, enabling the fine-tuning of a 65-billion-parameter model on a single 48GB GPU while maintaining full 16-bit fine-tuning task performance."
  - [corpus]: Supported by prior work citations (Dettmers et al., 2023) showing QLoRA effectiveness.
- Break condition: If the adapter rank is too low or the quantization too aggressive, the adapted model may underperform or fail to learn the target language patterns.

### Mechanism 3
- Claim: Fine-tuning multilingual LLMs on large monolingual datasets from Common Crawl improves language modeling perplexity and few-shot downstream performance for low-resource languages.
- Mechanism: The LLM, pre-trained on multilingual data but lacking sufficient target language examples, is adapted on a much larger monolingual corpus extracted via UnifiedCrawl. This additional in-domain training improves the model's language modeling and generalization to tasks in that language.
- Core assumption: The extracted monolingual dataset is large enough and representative enough to meaningfully improve the model's understanding of the target language.
- Evidence anchors:
  - [abstract]: "Our experiments show large improvements in language modeling perplexity and an increase in few-shot prompting scores."
  - [section 5.2.1]: "Both our fine-tuned models using QLoRA, XGLM-564M and XGLM- 4.5B exhibit significantly lower perplexity to that of the original XGLM models."
  - [section 5.2.2]: "For the XGLM-4.5B model, the F1 score increased by 24% from 8.0 to 9.9 after fine-tuning, and the EM score increased from 1.3 to 2.3."
  - [corpus]: The evidence is strong here - perplexity improvements and few-shot prompting score increases are directly measured and reported.
- Break condition: If the monolingual dataset contains too much noise or is not diverse enough, the model may overfit or fail to generalize.

## Foundational Learning

- Concept: Understanding of multilingual pre-training and its limitations
  - Why needed here: The paper builds on the idea that multilingual LLMs under-perform on low-resource languages due to limited training data. Understanding how these models are pre-trained and why they struggle with low-resource languages is crucial to appreciating the contribution of UnifiedCrawl and QLoRA fine-tuning.
  - Quick check question: Why do multilingual LLMs typically perform worse on low-resource languages compared to high-resource languages?

- Concept: Knowledge of data deduplication techniques
  - Why needed here: The paper employs substring deduplication to improve the quality of the extracted dataset by removing repetitive sequences. Understanding deduplication methods and their impact on training data quality is important for implementing and evaluating the UnifiedCrawl pipeline.
  - Quick check question: What is the purpose of deduplication in the context of web-scraped text data, and how might it affect model training?

- Concept: Familiarity with adapter-based fine-tuning methods (e.g., LoRA, QLoRA)
  - Why needed here: The paper uses QLoRA to efficiently adapt large LLMs on consumer hardware. Understanding how LoRA and QLoRA work, their advantages over full fine-tuning, and their limitations is essential for replicating the experiments and extending the approach.
  - Quick check question: How does LoRA reduce the memory and computational requirements of fine-tuning large language models compared to full fine-tuning?

## Architecture Onboarding

- Component map: Common Crawl index filtering (DuckDB) -> WARC file selection (HTTP range requests) -> HTML to text extraction (Trafilatura) -> Deduplication (substring deduplication) -> QLoRA fine-tuning -> Evaluation
- Critical path: Data extraction -> Deduplication -> QLoRA fine-tuning -> Evaluation
- Design tradeoffs:
  - Data extraction: Memory vs. speed tradeoff in index filtering; storage vs. completeness tradeoff in deduplication
  - Model adaptation: Adapter rank vs. performance vs. memory usage; quantization precision vs. model quality vs. hardware constraints
- Failure signatures:
  - Data extraction: Incomplete or incorrect datasets due to index filtering errors or missing WARC files
  - Model adaptation: Overfitting to the monolingual dataset, underfitting due to low adapter rank, or degraded performance due to aggressive quantization
- First 3 experiments:
  1. Verify the data extraction pipeline by running it on a small Common Crawl archive and checking the output dataset size and quality.
  2. Test QLoRA fine-tuning on a smaller multilingual model (e.g., XGLM-564M) with a subset of the UnifiedCrawl dataset to ensure the process works before scaling up.
  3. Evaluate the fine-tuned model on a simple language modeling task (e.g., next-word prediction) to confirm perplexity improvements before moving to downstream tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of UnifiedCrawl datasets compare to other multilingual datasets like Wikipedia or OSCAR for low-resource languages?
- Basis in paper: [explicit] The paper compares the size of UnifiedCrawl datasets to other datasets like Wikipedia, OSCAR, mC4, and CC-100, but does not provide a direct quality comparison.
- Why unresolved: The paper focuses on dataset size and model performance improvements but lacks a detailed analysis of data quality metrics such as diversity, relevance, or noise levels compared to existing datasets.
- What evidence would resolve it: A comparative analysis of data quality metrics, including language diversity, relevance, noise levels, and linguistic richness, across UnifiedCrawl and other datasets.

### Open Question 2
- Question: How do the improvements in perplexity and few-shot prompting scores translate to real-world applications for low-resource languages?
- Basis in paper: [explicit] The paper reports improvements in language modeling perplexity and few-shot prompting scores but does not explore practical applications or user studies.
- Why unresolved: The paper demonstrates technical improvements but does not investigate how these gains impact actual use cases, such as machine translation, text generation, or conversational AI for low-resource languages.
- What evidence would resolve it: Empirical studies or user feedback on the performance of fine-tuned models in real-world applications, such as translation accuracy, text generation quality, or user satisfaction.

### Open Question 3
- Question: What are the limitations of using QLoRA for fine-tuning multilingual models on low-resource languages, and how can these be addressed?
- Basis in paper: [explicit] The paper highlights the effectiveness of QLoRA in reducing memory usage and improving performance but does not discuss its limitations or potential improvements.
- Why unresolved: While QLoRA is shown to be effective, the paper does not explore scenarios where it might fail, such as handling extremely low-resource languages or specific linguistic challenges.
- What evidence would resolve it: Experiments comparing QLoRA with other fine-tuning methods, such as full fine-tuning or alternative adapter techniques, across a broader range of low-resource languages and tasks.

## Limitations
- The approach relies on the accuracy of Common Crawl's primary language annotations, which are not validated in the paper
- The effectiveness of substring deduplication on large-scale web data is assumed but not empirically evaluated
- Limited evaluation scope with only Question-Answering downstream tasks, lacking broader task coverage

## Confidence

- High Confidence: Technical feasibility of data extraction pipeline using DuckDB and HTTP range requests
- Medium Confidence: QLoRA effectiveness for efficient fine-tuning on consumer hardware (supported by prior work)
- Medium Confidence: Reported improvements in perplexity and few-shot scores (direct measurements but limited statistical validation)

## Next Checks

1. Validate Language Annotation Quality: Extract a small sample of URLs for a target low-resource language using the described filtering method. Manually verify the primary language of the extracted WARC files to assess the accuracy of the Common Crawl index annotations.

2. Benchmark Deduplication Methods: Compare the performance of substring deduplication against other deduplication techniques (e.g., MinHash, SimHash) on a subset of the UnifiedCrawl dataset. Measure the impact on dataset size, quality, and downstream model performance.

3. Extend Downstream Task Evaluation: Evaluate the fine-tuned models on a broader range of downstream tasks specific to low-resource languages (e.g., named entity recognition, machine translation, sentiment analysis) to assess the generalizability of the reported improvements beyond Question-Answering.