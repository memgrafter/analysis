---
ver: rpa2
title: 'Increasing the LLM Accuracy for Question Answering: Ontologies to the Rescue!'
arxiv_id: '2405.11706'
source_url: https://arxiv.org/abs/2405.11706
tags:
- query
- accuracy
- domain
- ontology
- repair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This research demonstrates that ontology-based query validation
  and LLM-powered repair significantly improves accuracy in question-answering systems
  built on enterprise data. The authors present a two-part method: (1) an Ontology-based
  Query Check (OBQC) that uses semantic constraints from an OWL ontology to detect
  SPARQL query errors, and (2) an LLM Repair that repairs queries based on explanations
  from the OBQC.'
---

# Increasing the LLM Accuracy for Question Answering: Ontologies to the Rescue!

## Quick Facts
- arXiv ID: 2405.11706
- Source URL: https://arxiv.org/abs/2405.11706
- Authors: Dean Allemang; Juan Sequeda
- Reference count: 21
- One-line primary result: Ontology-based query validation and LLM-powered repair increases accuracy from 43% to 73% in enterprise QA systems.

## Executive Summary
This research demonstrates that ontology-based query validation and LLM-powered repair significantly improves accuracy in question-answering systems built on enterprise data. The authors present a two-part method: (1) an Ontology-based Query Check (OBQC) that uses semantic constraints from an OWL ontology to detect SPARQL query errors, and (2) an LLM Repair that repairs queries based on explanations from the OBQC. Evaluated on a complex enterprise SQL dataset, the approach increases overall accuracy from 43% to 73%, reduces the error rate from 45% to 20%, and achieves 89% accuracy on low-complexity queries.

## Method Summary
The method combines ontology-based validation with LLM-powered repair to improve SPARQL query generation accuracy. The Ontology-based Query Check (OBQC) detects semantic violations by applying RDFS domain and range constraints to generated SPARQL queries. When errors are found, an LLM Repair process uses the error explanations to rewrite the query, with up to three iterative repair attempts. The system uses zero-shot prompting with GPT-4 to perform repairs without additional examples or context.

## Key Results
- Overall accuracy increased from 43% to 73% after applying OBQC and LLM Repair
- Error rate decreased from 45% to 20% through the iterative repair process
- 89% accuracy achieved on low-complexity questions
- Domain-related rules were most effective for query repairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ontology-based query validation catches semantic violations before execution
- Mechanism: The Ontology-based Query Check (OBQC) applies RDFS semantics (domain/range rules) to detect mismatches between generated SPARQL triples and the ontology
- Core assumption: SPARQL query patterns can be converted to RDF graphs and validated against ontology constraints
- Evidence anchors:
  - [abstract] "Ontology-based Query Check (OBQC): detects errors by leveraging the ontology of the knowledge graph to check if the LLM-generated SPARQL query matches the semantic of ontology"
  - [section] "We leverage this foundation to create an ontology-based query check system for SPARQL queries. This service takes two inputs: a SPARQL query and an ontology, and returns one output; a list of sentences that describe ways in which the SPARQL query deviates from the specifications in the ontology"
- Break condition: If ontology lacks disjointness axioms or uses complex OWL constructs, validation may miss violations

### Mechanism 2
- Claim: LLM-powered repair corrects semantic errors using ontology-based explanations
- Mechanism: The LLM Repair takes the SPARQL query and OBQC error explanations, then prompts the LLM to rewrite the query
- Core assumption: LLMs can understand structured error explanations and apply corrections to SPARQL syntax
- Evidence anchors:
  - [abstract] "LLM Repair: use the error explanations with an LLM to repair the SPARQL query"
  - [section] "What if we can pass the incorrect SPARQL query, with this explanation and prompt the LLM to rewrite the query?"
- Break condition: If explanations are too complex or LLM fails to parse error context, repair will fail

### Mechanism 3
- Claim: Iterative repair cycles converge on semantically valid queries
- Mechanism: Failed repairs are re-checked by OBQC, creating a feedback loop until success or maximum iterations
- Core assumption: Each repair attempt provides new information that improves subsequent queries
- Evidence anchors:
  - [abstract] "Our approach gives us the opportunity to understand the capability of an LLM to repair a SPARQL query and thus further improve the accuracy"
  - [section] "This cycle repeats until the check pass, or an upper limit of cycles is reached"
- Break condition: If LLM consistently fails to correct specific semantic patterns, accuracy gains plateau

## Foundational Learning

- Concept: RDFS domain and range constraints
  - Why needed here: Core mechanism for detecting semantic violations in generated SPARQL
  - Quick check question: If property :soldByAgent has domain :Policy, what type must the subject of a triple using this property have?

- Concept: SPARQL Basic Graph Patterns (BGPs)
  - Why needed here: OBQC extracts and validates BGPs against ontology semantics
  - Quick check question: How do you convert a SPARQL WHERE clause into a graph pattern that can be validated?

- Concept: Zero-shot prompting patterns for code generation
  - Why needed here: LLM Repair uses prompts without additional examples or context
  - Quick check question: What prompt structure successfully gets an LLM to rewrite code based on error descriptions?

## Architecture Onboarding

- Component map: Question → Zero-shot SPARQL generation → OBQC validation → LLM Repair → OBQC validation (repeat) → Execution
- Critical path: SPARQL generation → OBQC check → LLM Repair (if needed) → final validation → execution
- Design tradeoffs: Deterministic validation vs. probabilistic repair; complexity vs. accuracy gains
- Failure signatures: OBQC fires domain/range rules frequently; LLM Repair fails to converge after 3 attempts
- First 3 experiments:
  1. Test OBQC with known valid and invalid queries to verify rule firing
  2. Test LLM Repair with single-rule violations to verify repair capability
  3. Test end-to-end with simple questions to measure accuracy improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the achievable improvement in accuracy if more expressive ontologies are used that model complex domain and range relationships beyond RDFS?
- Basis in paper: [explicit] The paper notes that current rules follow a subset of RDFS semantics and mentions future work to handle OWL constructs like unions and intersections of classes.
- Why unresolved: The paper only implemented RDFS-based checks and acknowledges limitations with more complex OWL constructs.
- What evidence would resolve it: Testing the same approach with OWL ontologies containing unions, intersections, and other logical combinations of classes, and measuring the resulting accuracy improvements.

### Open Question 2
- Question: How does the effectiveness of ontology-based query validation and repair compare between different LLM models (e.g., GPT-4 vs. Claude vs. LLaMA)?
- Basis in paper: [inferred] The paper uses GPT-4 throughout its experiments but doesn't compare performance across different LLM models.
- Why unresolved: The study only tested with one LLM model, so comparative effectiveness across models is unknown.
- What evidence would resolve it: Replicating the experiments with different LLM models using identical ontologies and datasets to measure accuracy improvements.

### Open Question 3
- Question: What is the impact of adding explicit disjointness axioms to ontologies on the accuracy of LLM query repair?
- Basis in paper: [explicit] The paper mentions that hierarchical checks assume disjointness axioms not explicitly defined in OWL ontologies.
- Why unresolved: The study used ontologies without explicitly added disjointness axioms, so the potential impact of adding them is unknown.
- What evidence would resolve it: Comparing accuracy improvements between the current approach and one where explicit disjointness axioms are added to the ontologies.

## Limitations
- Evaluation focuses on a single enterprise SQL dataset with limited complexity variation
- Zero-shot LLM repair mechanism relies heavily on GPT-4 without comparative analysis of other models
- Iterative repair process capped at three attempts with no analysis of convergence behavior

## Confidence
- High confidence: Core finding that ontology-based validation improves SPARQL query accuracy
- Medium confidence: Overall accuracy improvements (43% to 73%) based on single benchmark dataset
- Low confidence: Generalizability of LLM repair mechanism across different domains and LLM models

## Next Checks
1. Test the OBQC mechanism on a diverse set of SPARQL queries with known semantic violations across multiple ontologies to verify rule completeness and accuracy detection rates.
2. Evaluate the LLM Repair approach with different LLM models (e.g., GPT-3.5, Claude, Llama) and varying prompt structures to establish robustness and identify optimal configurations.
3. Conduct ablation studies removing specific ontology constraints to measure their individual contribution to accuracy improvements and identify which semantic rules provide the most value.