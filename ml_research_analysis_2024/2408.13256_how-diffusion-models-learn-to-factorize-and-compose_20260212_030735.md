---
ver: rpa2
title: How Diffusion Models Learn to Factorize and Compose
arxiv_id: '2408.13256'
source_url: https://arxiv.org/abs/2408.13256
tags:
- gaussian
- data
- dataset
- training
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Diffusion models can generate novel combinations of features (compositional
  generalization) but the mechanism of how they achieve this is unclear. The authors
  use simplified 2D Gaussian bump datasets to study the model's learned representations.
---

# How Diffusion Models Learn to Factorize and Compose

## Quick Facts
- arXiv ID: 2408.13256
- Source URL: https://arxiv.org/abs/2408.13256
- Authors: Qiyao Liang; Ziming Liu; Mitchell Ostrow; Ila Fiete
- Reference count: 40
- Primary result: Diffusion models learn "hyper-factorized" representations where different values of the same feature are encoded orthogonally, enabling compositional generalization but poor interpolation.

## Executive Summary
Diffusion models demonstrate remarkable compositional generalization abilities, but the underlying mechanism remained unclear. This paper investigates how diffusion models learn to factorize and compose features by training on simplified 2D Gaussian bump datasets. The authors discover that models learn "hyper-factorized" representations where not only are different features encoded orthogonally, but different values of the same feature are also encoded in an almost orthogonal fashion. This unique representation structure explains why models can compose novel combinations of features (compositional generalization) but struggle with interpolation between known values.

The study reveals that compositional generalization requires observing the full extent of each independent latent feature along with a few compositional examples. The authors connect this learning behavior to percolation theory, explaining why there's a sudden onset of factorized representation learning at a critical threshold of correlated data. These findings suggest that diffusion models have inherent inductive biases favoring factorization and compositionality, which could enable more data-efficient training with appropriately structured datasets.

## Method Summary
The researchers created synthetic 2D datasets consisting of Gaussian bumps with known x,y coordinates, including both simple 2D Gaussian distributions and "sum of squares" (SOS) distributions. They trained conditional diffusion models (DDPMs) with explicit positional encoding of coordinates on these datasets. The models used a UNet architecture with 3 downsampling/upsampling blocks, self-attention layers, and skip connections. Internal representations were collected from layer 4 at the terminal diffusion timestep, then analyzed using dimensionality reduction techniques (UMAP/PCA) and topological methods including persistence diagrams. The models were evaluated based on their accuracy in predicting Gaussian center locations and their ability to generalize compositionally versus interpolatively.

## Key Results
- Diffusion models learn "hyper-factorized" representations where different values of the same feature are encoded orthogonally, not just different features
- Models demonstrate strong compositional generalization (combining features never seen together) but poor interpolation between known values
- Factorized representation learning exhibits a sudden onset threshold related to percolation theory, requiring sufficient correlated data for connectivity
- Models can achieve compositional generalization with minimal compositional examples when trained on data containing all independent factors of variation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models learn "hyper-factorized" representations where different values of the same feature are encoded orthogonally.
- Mechanism: The model treats continuous features more like categorical variables with nonzero overlaps between neighboring categories rather than fully continuous variables. This leads to orthogonal encoding not just between different features (x and y) but also between different values of the same feature.
- Core assumption: The model's architecture and training objective implicitly encourage this type of encoding, even though the input features are continuous.
- Evidence anchors:
  - [abstract]: "We found that the models learn factorized but not fully continuous manifold representations for encoding continuous features of variation underlying the data."
  - [section]: "In fact, the statistics from tests 2 and 3 of the model representation closely resemble those from test 1 of the 3D torus. This means that the model is encoding different values of x in an almost orthogonal, as opposed to parallel, fashion, similarly for different values of y."
  - [corpus]: Weak evidence - corpus contains related work on compositionality but not specific evidence about orthogonal encoding of feature values.
- Break condition: If the model is trained with different architectures or objectives that explicitly encourage continuous interpolation, or if the dataset contains different types of feature relationships.

### Mechanism 2
- Claim: Models can compositionally generalize when they observe the full extent of each independent latent feature along with a few compositional examples.
- Mechanism: By learning factorized representations where x and y are encoded independently, the model can combine features from different parts of the training distribution that were never seen together. The compositional examples provide the bridge for learning how to combine these factors.
- Core assumption: The model's learned representations are truly factorized and not coupled, allowing for independent manipulation of features.
- Evidence anchors:
  - [abstract]: "Models compose well but interpolate poorly. Models can compositionally generalize when they observe the full extent of each independent latent feature along with a few compositional examples."
  - [section]: "The resulting 2D+1D model, has high performance in generating both µx and µy across all of the test regions. This shows that given the necessary 'ingredients' (1D Gaussian stripes of all range of x and y), the model is able to compose them in a proper manner (2D Gaussian SOSs)."
  - [corpus]: Weak evidence - corpus contains related work on compositionality but not specific evidence about the requirement for full feature extent plus compositional examples.
- Break condition: If the model learns coupled representations instead of factorized ones, or if the compositional examples are insufficient to bridge the feature combinations.

### Mechanism 3
- Claim: Formation of factorized representation requires a threshold amount of correlated data, related to percolation theory.
- Mechanism: The model needs sufficient overlap between neighboring data points in the latent space to learn a connected manifold. Below a threshold, the representation cannot capture the relative spatial relationships between data points.
- Core assumption: The Gaussian bump datasets can be approximated as a Poisson Boolean model where overlap between neighboring points determines connectivity.
- Evidence anchors:
  - [abstract]: "Finally, we connect manifold formation in diffusion models to percolation theory in physics, offering insight into the sudden onset of factorized representation learning."
  - [section]: "We hypothesize that below a threshold amount of training data, the diffusion model cannot construct a faithful representation of the training dataset. From the percolation perspective, if there does not exist a large enough interconnected component within the dataset, the model will fail to learn the relative spatial location of the data points."
  - [corpus]: Weak evidence - corpus contains related work on compositionality but not specific evidence about percolation theory connection.
- Break condition: If the dataset structure changes such that overlap is not the determining factor for connectivity, or if the model architecture changes to not rely on manifold learning.

## Foundational Learning

- Concept: Factorized vs Coupled Representations
  - Why needed here: The entire paper hinges on understanding whether the model learns to represent features independently (factorized) or jointly (coupled), which determines its ability to generalize compositionally.
  - Quick check question: If a model learns a coupled representation of two features, how many training samples would it need to see all possible combinations versus a factorized representation?

- Concept: Manifold Learning and Dimensionality
  - Why needed here: The paper extensively analyzes the learned representations using dimensionality reduction and topological methods to understand the geometry of the learned feature space.
  - Quick check question: Why does the model's learned representation have higher effective dimension (around 7) than the theoretical minimum (4) for a factorized torus representation?

- Concept: Percolation Theory and Phase Transitions
  - Why needed here: The paper connects the sudden onset of factorized representation learning to percolation theory, explaining why there's a threshold effect in the amount of data needed.
  - Quick check question: What is the critical fraction of nodes that need to have non-zero overlaps in a percolation system for the entire system to become interconnected?

## Architecture Onboarding

- Component map:
  Input -> Conditional DDPM with UNet architecture -> Layer 4 outputs -> Dimensionality reduction (UMAP/PCA) -> Topological analysis -> Accuracy evaluation

- Critical path:
  1. Generate synthetic 2D Gaussian bump/SOS datasets with specified x,y coordinates
  2. Train conditional DDPM on these datasets with explicit positional encoding
  3. Collect internal representations from layer 4 at terminal diffusion timestep
  4. Apply dimensionality reduction (UMAP/PCA) to visualize learned manifolds
  5. Analyze topology using persistence diagrams and orthogonality/parallelism tests
  6. Evaluate compositional generalization by testing on held-out regions

- Design tradeoffs:
  - Simple synthetic datasets vs complex real images: Enables controlled analysis but may not capture all real-world complexities
  - Layer 4 vs bottleneck representation: Chosen due to skip connections diminishing bottleneck signals, but may miss some representation aspects
  - UMAP vs other dimensionality reduction: Balances preservation of local structure with global relationships

- Failure signatures:
  - Low accuracy on in-distribution data suggests training issues or insufficient model capacity
  - Non-toroidal topology in periodic boundary conditions suggests failure to learn factorized representations
  - Poor compositional generalization suggests insufficient coverage of feature ranges or lack of compositional examples
  - Linear scaling of data efficiency suggests failure to leverage factorized structure

- First 3 experiments:
  1. Train on complete 2D Gaussian bump dataset and verify learning of toroidal manifold structure
  2. Train on 2D Gaussian SOS dataset with central region held out and test compositional vs interpolation abilities
  3. Train on 1D Gaussian stripe dataset and verify inability to compose without compositional examples

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis based on simplified 2D synthetic datasets may not capture complexity of real-world data distributions
- Findings about "hyper-factorized" representations could be specific to Gaussian bump structure rather than general property
- Only analyzes representations at a single layer (layer 4), potentially missing how factorization emerges throughout network depth

## Confidence

**High Confidence**: The core empirical findings about factorized representations in diffusion models trained on 2D Gaussian datasets, including the orthogonal encoding of feature values and the distinction between compositional generalization and interpolation abilities.

**Medium Confidence**: The proposed mechanism explaining why diffusion models learn hyper-factorized representations, and the connection to percolation theory explaining the threshold effect in data requirements.

**Low Confidence**: The generalizability of these findings to complex real-world datasets and the practical implications for improving data efficiency in real diffusion model training scenarios.

## Next Checks
1. **Real-world dataset validation**: Train diffusion models on more complex synthetic datasets with multiple interacting features (beyond simple 2D Gaussians) to test whether hyper-factorization persists under more challenging conditions.

2. **Cross-architecture comparison**: Compare representation learning in diffusion models against other generative architectures (VAEs, GANs) on the same datasets to isolate what aspects of diffusion models specifically enable factorized representations.

3. **Layer-wise analysis**: Extend the analysis to examine how factorized representations develop across different layers of the diffusion model, particularly focusing on when and how the orthogonal encoding of feature values emerges during training.