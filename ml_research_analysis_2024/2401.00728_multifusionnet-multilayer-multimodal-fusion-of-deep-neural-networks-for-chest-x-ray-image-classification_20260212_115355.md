---
ver: rpa2
title: 'MultiFusionNet: Multilayer Multimodal Fusion of Deep Neural Networks for Chest
  X-Ray Image Classification'
arxiv_id: '2401.00728'
source_url: https://arxiv.org/abs/2401.00728
tags:
- fusion
- covid-19
- proposed
- images
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of accurately classifying lung
  diseases, particularly COVID-19, pneumonia, and normal cases, using chest X-ray
  images. The authors propose a novel deep learning-based multilayer multimodal fusion
  model that leverages the strengths of ResNet50V2 and InceptionV3 architectures.
---

# MultiFusionNet: Multilayer Multimodal Fusion of Deep Neural Networks for Chest X-Ray Image Classification

## Quick Facts
- arXiv ID: 2401.00728
- Source URL: https://arxiv.org/abs/2401.00728
- Reference count: 22
- Primary result: Achieves 97.21% accuracy for three-class lung disease classification and 99.60% for two-class classification

## Executive Summary
This study addresses the challenge of accurately classifying lung diseases (COVID-19, pneumonia, normal) from chest X-ray images using a novel deep learning approach. The authors propose MultiFusionNet, a multilayer multimodal fusion model that combines ResNet50V2 and InceptionV3 architectures through layer-wise and model-wise fusion. A key innovation is the FDSFM (Fusion of Different-Sized Feature Maps) module, which handles variable-sized feature maps from different convolutional layers. The model demonstrates significantly improved accuracy over single-model approaches and introduces a larger merged dataset (Cov-Pneum) to address data sparsity issues in medical imaging.

## Method Summary
The method involves multilayer multimodal fusion of pre-trained ResNet50V2 and InceptionV3 models. Feature maps from intermediate layers are extracted and fused using the FDSFM module to handle varying spatial dimensions. The fused features from both models are then combined through element-wise addition after global average pooling. The final classifier consists of dropout and dense layers producing softmax probabilities for three disease classes. The model is trained using Adam optimizer with categorical cross-entropy loss on the combined Cov-Pneum dataset.

## Key Results
- Achieves 97.21% accuracy for three-class classification (COVID-19, pneumonia, normal)
- Achieves 99.60% accuracy for two-class classification
- Demonstrates significant improvement over single-model approaches through multilayer and multimodal fusion

## Why This Works (Mechanism)

### Mechanism 1
Layer-wise fusion captures discriminative features from multiple convolutional layers, reducing feature loss compared to single-layer approaches. By extracting and fusing feature maps from intermediate layers (e.g., Block 3, Block 4, Block 5 of ResNet50V2), the model retains spatial and semantic information that would otherwise be lost in deeper layers. Core assumption: Intermediate convolutional layers contain complementary, non-redundant information useful for classification.

### Mechanism 2
The FDSFM module effectively handles variable-sized feature maps, enabling fusion across layers with different spatial dimensions. The module applies max pooling with filter size f and stride s to downsample each feature map to a common size, then uses 1x1 convolution to align channel dimensions before concatenation. Core assumption: Max pooling with appropriate f and s can reduce feature maps to a size where all can be fused without excessive information loss.

### Mechanism 3
Multimodal fusion at the model level (ResNet50V2 + InceptionV3) captures complementary feature representations, improving overall classification accuracy. Features from the two pre-trained models are pooled to the same spatial size via GAP2D, then combined element-wise via addition, producing a richer representation than either model alone. Core assumption: ResNet50V2 and InceptionV3 extract complementary, non-overlapping features from the same input images.

## Foundational Learning

- Concept: Transfer learning from ImageNet-pretrained CNNs
  - Why needed here: Medical imaging datasets are small and expensive to label; leveraging pre-trained models accelerates convergence and improves generalization.
  - Quick check question: Why is transfer learning particularly important for CXR classification compared to natural image tasks?

- Concept: Feature map spatial alignment and fusion
  - Why needed here: Different CNN layers output feature maps of varying spatial dimensions; effective fusion requires resizing to a common grid without losing discriminative content.
  - Quick check question: What are the trade-offs between using max pooling vs. interpolation for resizing feature maps before fusion?

- Concept: Multimodal data integration
  - Why needed here: Combining complementary feature extractors (ResNet and Inception) can capture diverse patterns in CXR images, leading to more robust classification.
  - Quick check question: How does element-wise addition fusion differ from concatenation in terms of parameter efficiency and representational capacity?

## Architecture Onboarding

- Component map: Input (224x224x3 CXR image) -> Backbone 1 (ResNet50V2) -> Backbone 2 (InceptionV3) -> Multilayer fusion module (FDSFM) for each backbone -> Multimodal fusion (element-wise addition after GAP2D) -> Classifier head (Dropout → Dense(256) → Dense(3)) -> Output (Softmax probabilities)

- Critical path:
  1. Load and preprocess CXR images (resize, normalize)
  2. Extract intermediate layer feature maps from both backbones
  3. Apply FDSFM to align and fuse feature maps per backbone
  4. Apply GAP2D to each fused backbone output
  5. Element-wise add backbone outputs
  6. Pass through classifier head
  7. Compute softmax loss and backpropagate

- Design tradeoffs:
  - Fusion granularity: Layer-wise (richer) vs. single-layer (simpler)
  - Fusion strategy: Addition (parameter-efficient) vs. concatenation (higher capacity)
  - Backbone choice: ResNet (residual connections) vs. Inception (multi-scale filters)
  - Dataset size: Larger (Cov-Pneum) vs. smaller (single-source) affects overfitting risk

- Failure signatures:
  - Overfitting: Training accuracy >> validation accuracy; consider stronger regularization or more data
  - Vanishing gradients: Monitor layer-wise gradients; may need residual scaling or learning rate adjustment
  - Poor multimodal benefit: If addition fusion yields no gain, backbones may be redundant; try concatenation or feature selection

- First 3 experiments:
  1. Train single backbone (ResNet50V2 only) with multilayer fusion, no multimodal fusion; establish baseline
  2. Add multimodal fusion with element-wise addition; measure accuracy gain
  3. Swap fusion strategy to concatenation; compare parameter count and accuracy; decide optimal fusion method

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed model perform on larger datasets with more diverse lung disease classifications beyond COVID-19, pneumonia, and normal cases? The authors mention extending the model to other disease classifications but do not provide results or validation on such datasets. Testing the model on a dataset with a broader range of lung diseases (e.g., tuberculosis, lung cancer) and comparing its performance to state-of-the-art methods for those diseases would resolve this question.

### Open Question 2
What is the impact of different fusion strategies (e.g., weighted sum vs. concatenation) on the model's performance in multilayer multimodal fusion? The authors mention using element-wise addition for multimodal fusion but do not explore other fusion strategies or their comparative performance. Conducting experiments with different fusion strategies (e.g., weighted sum, concatenation, attention-based fusion) and comparing their performance in terms of accuracy, precision, recall, and F1-score would resolve this question.

### Open Question 3
How does the model's performance vary with different input image sizes and resolutions, and what is the optimal size for balancing accuracy and computational efficiency? The authors use a fixed input size of (224, 224, 3) but do not explore the impact of different image sizes on the model's performance or computational requirements. Conducting experiments with different input image sizes and resolutions, measuring the model's performance and computational requirements (e.g., inference time, memory usage) for each size, and identifying the optimal trade-off between accuracy and efficiency would resolve this question.

## Limitations

- The model's performance relies heavily on the quality and size of the combined dataset (Cov-Pneum), and results may not generalize to other distributions
- The FDSFM module's implementation details are not fully specified, potentially affecting reproducibility
- The study focuses primarily on accuracy metrics without extensive analysis of model robustness or failure cases

## Confidence

- High confidence in the overall framework concept and experimental setup
- Medium confidence in the specific implementation details of the FDSFM module
- Medium confidence in the transferability of results to different datasets or clinical settings

## Next Checks

1. Implement ablation studies removing the FDSFM module to quantify its contribution to performance gains
2. Test the model on independent, external chest X-ray datasets to evaluate generalization capability
3. Conduct robustness testing with adversarial examples and noisy inputs to assess model stability