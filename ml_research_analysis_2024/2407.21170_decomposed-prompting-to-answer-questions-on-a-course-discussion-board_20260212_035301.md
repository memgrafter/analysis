---
ver: rpa2
title: Decomposed Prompting to Answer Questions on a Course Discussion Board
arxiv_id: '2407.21170'
source_url: https://arxiv.org/abs/2407.21170
tags:
- questions
- question
- answer
- conceptual
- course
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors introduce a two-stage question-answering system for
  course discussion boards, using decomposed prompting to classify student questions
  into four types: conceptual, homework, logistics, and not answerable. Classification
  accuracy reached 81% with a GPT-3 variant, showing sensitivity to task description,
  number of few-shot examples, and label wording.'
---

# Decomposed Prompting to Answer Questions on a Course Discussion Board

## Quick Facts
- arXiv ID: 2407.21170
- Source URL: https://arxiv.org/abs/2407.21170
- Reference count: 0
- 81% classification accuracy using text-davinci-003

## Executive Summary
This paper introduces a two-stage question-answering system for course discussion boards that uses decomposed prompting to classify and respond to student questions. The system classifies questions into four types (conceptual, homework, logistics, and not answerable) with 81% accuracy using a GPT-3 variant, then generates answers using specialized prompts for each type. For conceptual questions, the system's answers were evaluated against instructor responses using cosine similarity, ROUGE, and perplexity metrics, with 29% deemed good quality. The approach demonstrates that decomposing the task allows for tailored strategies per question type, though limitations include occasional misclassification and factual errors.

## Method Summary
The authors implement a mixture of experts approach using decomposed prompting to handle course discussion board questions. First, a classification prompt with few-shot examples categorizes incoming questions into four types. Then, depending on the classification, specialized answering prompts are used: conceptual questions are answered directly with context from course materials, homework questions incorporate relevant assignment sections, logistics questions draw from syllabus information, and not answerable questions are ignored. The system uses text-davinci-003 as the underlying LLM and evaluates conceptual question answers against instructor responses using multiple metrics.

## Key Results
- 81% classification accuracy achieved using descriptive prompts and 31 few-shot examples
- 29% of generated conceptual answers evaluated as good quality compared to instructor answers
- System performance is sensitive to task description quality, number of few-shot examples, and label wording
- Main failure modes include misclassification, factual errors, and mismatched difficulty levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposed prompting improves classification accuracy by tailoring the task description to specific question types.
- Mechanism: The system uses a detailed task description that outlines when each question type applies, helping the LLM distinguish between conceptual, homework, logistics, and not answerable questions.
- Core assumption: LLMs can effectively parse and apply task descriptions to categorize questions when provided with clear, context-specific instructions.
- Evidence anchors:
  - [abstract] "we achieve 81% classification accuracy"
  - [section] "Using our descriptive prompt, the classification accuracy is 81%"
  - [corpus] Weak - no direct corpus evidence on task description effectiveness
- Break condition: If the task description becomes too complex or ambiguous, the LLM may misclassify questions or fail to generalize across different course contexts.

### Mechanism 2
- Claim: Few-shot examples significantly improve classification performance by providing concrete patterns for the LLM to learn from.
- Mechanism: The system includes 31 in-context examples in the prompt, showing the LLM specific question-type pairs to learn from.
- Core assumption: LLMs can effectively learn from few-shot examples to improve their classification accuracy on unseen questions.
- Evidence anchors:
  - [abstract] "we achieve 81% classification accuracy"
  - [section] "Table 2 shows that using 31 examples produces the highest classification accuracy"
  - [corpus] Weak - no direct corpus evidence on few-shot example effectiveness
- Break condition: If too few examples are provided, the LLM may not learn the patterns well enough. If too many examples are provided, the prompt may exceed token limits or cause confusion.

### Mechanism 3
- Claim: Different question types require different answering strategies, and decomposing the task allows for specialized approaches.
- Mechanism: The system uses a mixture of experts approach, where each question type is handled by a different expert (prompt strategy).
- Core assumption: LLMs can effectively handle different types of questions when provided with specialized prompts and context.
- Evidence anchors:
  - [abstract] "This enables us to employ a different strategy for answering questions that fall under different types"
  - [section] "conceptual questions do not require specialized context in the prompt. However, homework questions require that we identify and provide relevant sections of the assignment handout"
  - [corpus] Weak - no direct corpus evidence on mixture of experts approach
- Break condition: If the question type classification is incorrect, the wrong expert may be used, leading to poor answers or inappropriate responses.

## Foundational Learning

- Concept: Task decomposition and modular design
  - Why needed here: The system breaks down the complex task of answering student questions into smaller, more manageable subtasks (classification and answering)
  - Quick check question: How does task decomposition help improve the overall system performance?

- Concept: Prompt engineering and few-shot learning
  - Why needed here: The system relies heavily on carefully crafted prompts and few-shot examples to guide the LLM's behavior
  - Quick check question: What are the key components of an effective prompt for this system?

- Concept: Classification accuracy and evaluation metrics
  - Why needed here: The system's performance is measured by its classification accuracy and the quality of its answers, which requires understanding of evaluation metrics
  - Quick check question: What metrics are used to evaluate the system's performance on conceptual questions?

## Architecture Onboarding

- Component map:
  Input -> Question Classification -> Question Answering -> Output

- Critical path:
  1. Receive student question
  2. Classify question type using Question Classification prompt
  3. If conceptual, use Conceptual prompt to generate answer
  4. If homework or logistics, provide relevant context and generate answer
  5. If not answerable, ignore question

- Design tradeoffs:
  - Accuracy vs. complexity: More complex prompts may improve accuracy but increase processing time
  - Generalization vs. specialization: More specialized prompts may work better for specific courses but may not generalize well
  - Cost vs. performance: More complex models or prompts may improve performance but increase costs

- Failure signatures:
  - Misclassification: Question is classified into wrong type, leading to inappropriate answer or ignored question
  - Factual errors: Answer contains incorrect information, especially for conceptual questions
  - Mismatched difficulty: Answer is too simple or too complex for the student's level
  - Incoherence: Answer is difficult to understand or follow

- First 3 experiments:
  1. Vary the number of few-shot examples in the classification prompt to find the optimal number for maximum accuracy
  2. Test different task descriptions in the classification prompt to see how they affect accuracy
  3. Evaluate the system's performance on different types of questions (conceptual, homework, logistics, not answerable) to identify areas for improvement

## Open Questions the Paper Calls Out
- How does the performance of the decomposed prompting system compare to end-to-end large language models for course discussion board question answering?
- What is the impact of fine-tuning the large language model on course-specific data for improving question classification and answer generation?
- How do different types of contextual information (e.g., assignment instructions, course syllabus) affect the accuracy of homework and logistics question answering?

## Limitations
- The system's performance depends heavily on the quality and specificity of the task description and few-shot examples
- Classification accuracy drops when questions require context from course materials to distinguish between types
- Factual errors and difficulty explaining complex concepts beyond course scope remain challenges

## Confidence
- Classification accuracy claim: Medium
- Answer quality evaluation: Medium
- Generalization to other courses: Low

## Next Checks
1. Test the system on course discussion boards from different disciplines and educational levels to assess generalization beyond the tested ML course context
2. Conduct human evaluations of answer quality, including student satisfaction surveys and expert assessments of factual accuracy and pedagogical appropriateness
3. Experiment with alternative few-shot example sizes and task description variations to determine optimal prompt engineering strategies for different question types and courses