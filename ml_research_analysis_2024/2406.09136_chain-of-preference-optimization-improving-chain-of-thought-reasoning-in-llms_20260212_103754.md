---
ver: rpa2
title: 'Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in
  LLMs'
arxiv_id: '2406.09136'
source_url: https://arxiv.org/abs/2406.09136
tags:
- reasoning
- thoughts
- arxiv
- data
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Chain-of-Preference Optimization (CPO) addresses the inefficiency
  of tree-of-thought (ToT) reasoning by learning preferred reasoning paths from ToT's
  search tree during training rather than at inference time. The method constructs
  paired preference data at each reasoning step by identifying thoughts included in
  ToT's final paths as preferred and their alternatives as dispreferred, then fine-tunes
  LLMs using direct preference optimization (DPO).
---

# Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs

## Quick Facts
- arXiv ID: 2406.09136
- Source URL: https://arxiv.org/abs/2406.09136
- Authors: Xuan Zhang; Chao Du; Tianyu Pang; Qian Liu; Wei Gao; Min Lin
- Reference count: 40
- Key outcome: CPO improves base LLM reasoning accuracy by 4.3% on average across seven datasets while maintaining CoT's fast inference speed (57.5× faster than ToT)

## Executive Summary
Chain-of-Preference Optimization (CPO) addresses the inefficiency of tree-of-thought (ToT) reasoning by learning preferred reasoning paths from ToT's search tree during training rather than at inference time. The method constructs paired preference data at each reasoning step by identifying thoughts included in ToT's final paths as preferred and their alternatives as dispreferred, then fine-tunes LLMs using direct preference optimization (DPO). Experiments show CPO improves base LLM reasoning accuracy by 4.3% on average across seven datasets while maintaining CoT's fast inference speed (57.5× faster than ToT), outperforming supervised fine-tuning baselines and achieving comparable results to ToT despite its computational overhead.

## Method Summary
CPO shifts the computational burden of ToT from inference to training by using ToT search to generate preference data that captures which intermediate reasoning steps lead to correct answers. The method constructs paired preference data at each reasoning step by running ToT search, then identifies thoughts included in ToT's final paths as preferred and their alternatives as dispreferred. This per-step preference data is used to fine-tune LLMs with direct preference optimization (DPO), training the model to generate reasoning paths aligned with ToT's preferences using standard CoT decoding at inference time.

## Key Results
- CPO improves base LLM reasoning accuracy by 4.3% on average across seven datasets
- Maintains CoT's fast inference speed (57.5× faster than ToT on average)
- Outperforms supervised fine-tuning baselines on reasoning tasks
- Achieves comparable results to ToT despite ToT's computational overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CPO leverages per-step preference supervision to improve reasoning accuracy over methods that supervise only final paths.
- Mechanism: By constructing preference pairs at each intermediate reasoning step (preferred thoughts vs. dispreferred siblings), CPO trains the model to align with ToT's preferred thoughts at every stage, not just the final output.
- Core assumption: The quality difference between preferred and dispreferred thoughts at each step is meaningful and detectable by the model.
- Evidence anchors:
  - [abstract] "CPO enables LLMs to generate the path preferred by ToT using CoT decoding at inference time."
  - [section 4.1] "The constructed dataset D includes preference data at every step of the reasoning chain."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.473, suggesting moderate relatedness in the reasoning optimization space.

### Mechanism 2
- Claim: CPO avoids the longest common prefix gradient cancellation problem that affects full-path preference optimization.
- Mechanism: By constructing preference pairs at each reasoning step rather than the full path level, CPO ensures gradients flow to all tokens in the reasoning chain, not just divergent portions.
- Core assumption: Gradient cancellation in full-path optimization significantly impairs learning.
- Evidence anchors:
  - [section 6] "The gradient will only be computed for the last token where the two sequences diverge" - explicitly describes LCP gradient cancellation.
  - [section 6] "CPO constructs preference data at every step in the reasoning chain, allowing optimization of the LLM on all steps in the reasoning path."
  - [corpus] Found 25 related papers, but none specifically addressing gradient cancellation in preference optimization, suggesting this may be a novel contribution.

### Mechanism 3
- Claim: CPO achieves ToT-level reasoning quality while maintaining CoT's inference speed by shifting computational burden to training.
- Mechanism: CPO uses ToT only during training to generate preference data, then fine-tunes the model so it can generate preferred reasoning paths directly at inference without tree search.
- Core assumption: The preference patterns learned during training generalize to new instances at inference.
- Evidence anchors:
  - [abstract] "CPO improves base LLM reasoning accuracy by 4.3% on average across seven datasets while maintaining CoT's fast inference speed (57.5× faster than ToT)"
  - [section 5.2] "CPO shifts this computational burden to the training phase, maintaining the low latency of CoT (i.e., 57.5× faster than ToT on average) during inference while providing comparable or superior performance."
  - [corpus] The neighbor papers focus on tree search improvements but don't address the inference speed tradeoff, suggesting CPO's approach to this problem may be unique.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: CPO builds on CoT as the base inference method that needs improvement
  - Quick check question: What distinguishes CoT from direct answer generation in LLMs?

- Concept: Tree-of-Thought (ToT) search algorithm
  - Why needed here: CPO uses ToT during training to generate preference data
  - Quick check question: How does ToT's multi-path exploration differ from CoT's single-path approach?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: CPO uses DPO as the fine-tuning algorithm to align with preferences
  - Quick check question: What distinguishes DPO from traditional reinforcement learning approaches?

## Architecture Onboarding

- Component map: ToT search → preference pair construction → training dataset → DPO fine-tuning → improved model
- Critical path: Data generation → Preference construction → DPO training → Inference
- Design tradeoffs: Training time vs. inference speed (ToT is slow at inference but fast at training data generation)
- Failure signatures: Model overfits to training instances, preferences don't generalize, gradient issues in optimization
- First 3 experiments:
  1. Run ToT on a small dataset to verify preference pair generation works correctly
  2. Apply DPO with synthetic preference pairs to confirm the optimization pipeline functions
  3. Test CPO on a single dataset with a small model to verify end-to-end improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CPO performance scale with different tree search algorithms (DFS, MCTS) compared to BFS?
- Basis in paper: [explicit] The paper mentions BFS is used for search and collection, but doesn't explore alternatives like DFS or MCTS
- Why unresolved: The paper only uses BFS with pruning, leaving performance comparisons with other search algorithms unexplored
- What evidence would resolve it: Comparative experiments showing CPO performance across different tree search algorithms on the same datasets

### Open Question 2
- Question: What is the optimal ratio of preferred to dispreferred thoughts for CPO training?
- Basis in paper: [explicit] The paper explores including 0-100% of dispreferred thoughts but doesn't determine optimal ratios
- Why unresolved: While the paper shows performance improves with more dispreferred data, it doesn't identify the sweet spot or diminishing returns
- What evidence would resolve it: Detailed ablation studies testing different ratios of preferred:dispreferred data across multiple datasets

### Open Question 3
- Question: Can CPO be effectively applied to multimodal reasoning tasks (vision-language)?
- Basis in paper: [inferred] The paper only tests text-based reasoning tasks and explicitly states limitations regarding vision-language models
- Why unresolved: The paper mentions this as a limitation but doesn't provide any preliminary experiments or analysis
- What evidence would resolve it: Experiments applying CPO to multimodal datasets like VQA or visual reasoning tasks with appropriate adaptations

## Limitations

- The effectiveness depends on ToT's ability to reliably identify better intermediate thoughts during training
- Potential overfitting risks when training on relatively small datasets with preference pairs
- Computational overhead of generating preference data via ToT during training could be substantial for very large datasets

## Confidence

- **High confidence**: CPO maintains CoT's fast inference speed while improving accuracy over base models (supported by concrete latency measurements and consistent accuracy gains across seven datasets)
- **Medium confidence**: CPO's per-step preference supervision provides meaningful advantages over full-path supervision (theoretically justified but limited empirical comparison with full-path baselines)
- **Medium confidence**: CPO achieves ToT-level reasoning quality (performance is "comparable" but not definitively superior to ToT)

## Next Checks

1. Implement a full-path preference optimization baseline and directly compare gradient magnitudes across common prefix tokens to empirically verify CPO's gradient cancellation advantage
2. Test CPO's generalization by training on a subset of reasoning types and evaluating on held-out reasoning categories to assess transfer capability
3. Measure the training time overhead of ToT-based preference data generation and calculate the break-even point where the training cost is justified by inference speed gains