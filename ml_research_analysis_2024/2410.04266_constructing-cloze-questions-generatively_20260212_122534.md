---
ver: rpa2
title: Constructing Cloze Questions Generatively
arxiv_id: '2410.04266'
source_url: https://arxiv.org/abs/2410.04266
tags:
- distractors
- answer
- distractor
- lexical
- wordnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents CQG, a generative method for constructing cloze
  questions that significantly outperforms state-of-the-art approaches. CQG uses neural
  networks and WordNet to generate both unigram and multigram distractors by selecting
  answer keys, segmenting them into instances, generating instance-level distractors
  using a transformer and sibling synsets, filtering inappropriate candidates, and
  ranking remaining distractors based on contextual semantic similarities.
---

# Constructing Cloze Questions Generatively

## Quick Facts
- arXiv ID: 2410.04266
- Source URL: https://arxiv.org/abs/2410.04266
- Reference count: 34
- Outperforms state-of-the-art methods with F1 scores up to 88.5% and NDCG@10 up to 97.7% for unigram distractors

## Executive Summary
This paper introduces CQG, a generative method for constructing cloze questions with high-quality distractors. The approach leverages neural networks and WordNet to generate both unigram and multigram distractors by selecting answer keys, segmenting them into instances, and generating instance-level distractors using a transformer and sibling synsets. CQG filters inappropriate candidates and ranks remaining distractors based on contextual semantic similarities, achieving superior performance compared to existing methods. The method demonstrates significant improvements in F1 scores, MRR, and NDCG@10 metrics while maintaining high-quality multigram distractors with over 81% contextual semantic similarity to ground truth.

## Method Summary
CQG is a generative pipeline for cloze question construction that operates through three main stages: Stem and Answer-key Selector (SAS), Instance-level Distractor Generator (IDG), and Answer-key Distractor Generator (ADG). The method selects answer keys from sentences, segments multigram keys into WordNet instances, generates distractor candidates using BERT and WordNet synsets, filters inappropriate candidates, and ranks remaining distractors using contextual embeddings, WordNet similarity, and prediction rewards. For multigram distractors, CQG combinatorially replaces instances with top-ranked candidates and validates phrases using Google's Ngram Viewer. The approach significantly outperforms state-of-the-art methods on both unigram and multigram distractor generation tasks.

## Key Results
- Achieves F1 scores up to 88.5%, MRR up to 94.4%, and NDCG@10 up to 97.7% for unigram distractors
- Generates multigram distractors with over 81% contextual semantic similarity to ground truth distractors
- Outperforms state-of-the-art methods across all evaluation metrics for both unigram and multigram distractors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CQG generates high-quality cloze distractors by leveraging WordNet synset relationships and contextual embeddings to select distractors that are semantically related yet contextually inappropriate.
- Mechanism: CQG first segments the answer key into instances, generates instance-level distractor candidates (IDCs) using a transformer and sibling synsets, filters inappropriate IDCs based on POS, NER, lexical labels, and semantic relatedness, and finally ranks and combines IDCs to form distractor candidates that are contextually and semantically closest to the answer key.
- Core assumption: Sibling synsets and contextual embeddings provide a good measure of semantic relatedness for distractor selection.
- Evidence anchors:
  - [abstract] "CQG selects an answer key for a given sentence, segments it into a sequence of instances, generates instance-level distractor candidates (IDCs) using a transformer and sibling synsets."
  - [section III] "AKS divides a multigram answer key into a sequence of instances, where an instance in a multigram answer key is the largest legitimate entry in WordNet."
  - [corpus] Weak - no direct evidence in corpus about synset usage for distractor quality.
- Break condition: If sibling synsets do not provide meaningful semantic relationships or if contextual embeddings fail to capture sentence-level context.

### Mechanism 2
- Claim: CQG outperforms state-of-the-art methods by using a combination of contextual embedding similarity, WordNet synset similarity, and prediction reward score to rank L-IDCs.
- Mechanism: CQG calculates a ranking score for each L-IDC as the product of contextual embedding similarity (ES), WordNet synset similarity reward (WS), and prediction reward score (PS). This multi-faceted ranking ensures that distractors are both contextually appropriate and semantically related to the answer key.
- Core assumption: The combination of ES, WS, and PS provides a better ranking than any single metric alone.
- Evidence anchors:
  - [abstract] "It then removes inappropriate IDCs, ranks the remaining IDCs based on contextual embedding similarities, as well as synset and lexical relatedness."
  - [section V-B] "We use the contextual embedding similarity of U and C as the baseline ranking of C, denoted by ES(U, C). We define WordNet synset similarity reward score WS(U, C) and prediction reward score PS(C) to adjust the baseline ranking score."
  - [corpus] Weak - no direct evidence in corpus about the effectiveness of the combined ranking approach.
- Break condition: If any of the individual ranking metrics (ES, WS, or PS) becomes unreliable or if the product of the metrics does not correlate with distractor quality.

### Mechanism 3
- Claim: CQG generates multigram distractors by combinatorially replacing instances in the answer key with top-ranked L-IDCs and filtering the resulting phrases using Google's Ngram Viewer.
- Mechanism: For each instance in the answer key, CQG selects top-ranked L-IDCs and replaces the instance with each IDC combinatorially to form new phrases. These phrases are then filtered using Google's Ngram Viewer to ensure they conform to human writing patterns.
- Core assumption: Combinatorially replacing instances and filtering with Ngram Viewer produces plausible multigram distractors.
- Evidence anchors:
  - [abstract] "forms distractor candidates by combinatorially replacing instances with the corresponding top-ranked IDCs, and checks if they are legitimate phrases."
  - [section V-C] "To form a distractor candidate, select an instance Ui with i ∈ [1, k]. Let Pi = {Ci1, . . . , Cim} be the set of L-IDCs for Ui. We replace Ui with a Cij ∈ Pi for one or more instances combinatorially to produce a set of new phrases."
  - [corpus] Weak - no direct evidence in corpus about the effectiveness of combinatorial replacement and Ngram filtering for multigram distractors.
- Break condition: If combinatorial replacement produces too many invalid phrases or if Ngram filtering fails to distinguish between plausible and implausible phrases.

## Foundational Learning

- Concept: WordNet synsets and lexical labels
  - Why needed here: WordNet provides a structured vocabulary of English words and phrases grouped into synsets, which CQG uses to find semantically related distractors and ensure lexical consistency.
  - Quick check question: What is the difference between a hypernym and a hyponym in WordNet?
- Concept: Contextual embeddings and transformer models
  - Why needed here: CQG uses transformer models like BERT to generate contextual embeddings for words and phrases, which are used to measure semantic similarity and rank distractor candidates.
  - Quick check question: How does BERT generate contextual embeddings for a given word in a sentence?
- Concept: Sense disambiguation
  - Why needed here: CQG uses sense disambiguation to determine the correct sense of a word or phrase in a given context, which is crucial for selecting appropriate distractors.
  - Quick check question: What is the difference between word sense disambiguation and entity linking?

## Architecture Onboarding

- Component map: SAS -> IDG -> ADG
- Critical path: SAS → IDG → ADG
- Design tradeoffs:
  - Using WordNet for distractor generation provides structured semantic relationships but may miss context-specific meanings.
  - Combinatorially replacing instances produces many distractor candidates but requires filtering to ensure plausibility.
  - Using Google's Ngram Viewer for filtering is effective but relies on external API and may not cover all valid phrases.
- Failure signatures:
  - SAS: Poor sentence ranking leads to irrelevant stems and answer keys.
  - IDG: Incorrect sense disambiguation or poor IDC generation results in inappropriate distractors.
  - ADG: Ineffective filtering or ranking produces distractors that are either too easy or too difficult.
- First 3 experiments:
  1. Evaluate SAS by comparing the relevance of selected stems and answer keys to the original article.
  2. Test IDG by generating distractors for a set of known answer keys and evaluating their semantic relatedness.
  3. Assess ADG by generating distractors for a set of stems and answer keys and evaluating their plausibility and difficulty.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CQG's performance change when using different transformer models (e.g., BERT-Base vs BERT-Large vs other transformer variants) for sense disambiguation and contextual embedding generation?
- Basis in paper: [explicit] The paper mentions using BERT as the transformer and notes "CSS/B and CSS/L denote the cosine similarity of contextual embeddings generated by, respectively, BERT-Base and BERT-Large" in Table II, but doesn't compare their performance differences or explore other transformer models.
- Why unresolved: The paper uses BERT but doesn't provide comparative analysis of different transformer models or their impact on CQG's performance.
- What evidence would resolve it: Experimental results comparing CQG's performance using different transformer models (BERT-Base, BERT-Large, RoBERTa, etc.) on the same evaluation metrics would clarify the impact of transformer choice.

### Open Question 2
- Question: What is the optimal balance between precision and recall for distractor generation in educational assessment contexts, and how does this vary by educational level or subject domain?
- Basis in paper: [explicit] The paper notes "CQGs are much better balanced between precision and recall, with higher F1 scores than those of SOTA results" but doesn't explore the optimal balance for different educational contexts or whether different educational levels require different trade-offs.
- Why unresolved: The paper achieves good F1 scores but doesn't investigate how the precision-recall balance affects educational effectiveness across different contexts.
- What evidence would resolve it: Empirical studies comparing student performance using CQG-generated distractors with different precision-recall balances across various educational levels and subject domains would provide insights into optimal settings.

### Open Question 3
- Question: How does the quality of CQG-generated distractors compare when applied to non-English languages, particularly those with different linguistic structures (e.g., agglutinative languages, languages with different word order)?
- Basis in paper: [explicit] The paper focuses exclusively on English language processing and WordNet, with no mention of multilingual applications or how the method would need to be adapted for other languages.
- Why unresolved: The method relies heavily on English-specific resources (WordNet, spaCy's English models) and linguistic assumptions that may not transfer to other languages.
- What evidence would resolve it: Implementing CQG with language-specific resources (e.g., BabelNet for multiple languages, language-specific NLP tools) and comparing the quality of generated distractors across languages would demonstrate generalizability.

## Limitations

- WordNet coverage limitations may affect performance on scientific and technical vocabulary
- Combinatorial approach to multigram distractor generation could face scalability issues with longer answer keys
- Method depends on external services (Google Ngram Viewer, ESCHER) which may introduce reproducibility issues

## Confidence

- High Confidence: The core mechanism of using contextual embeddings and WordNet synsets for unigram distractor generation is well-supported by experimental results with F1 scores up to 88.5%.
- Medium Confidence: The multigram distractor generation approach shows promise with over 81% contextual semantic similarity but evaluation methodology is less rigorous than for unigram distractors.
- Low Confidence: Human evaluation results are mentioned but lack detailed information about sample size, evaluator expertise, or specific rating criteria.

## Next Checks

1. **Domain Transfer Test**: Evaluate CQG on scientific and technical texts from domains not represented in the training corpus to assess WordNet's coverage limitations and the method's generalization capabilities.

2. **Scalability Analysis**: Systematically vary answer key length and instance complexity to identify breaking points in the combinatorial distractor generation process and quantify the effectiveness of the filtering pipeline.

3. **Ablation Study**: Remove individual components (WordNet synset similarity, contextual embedding ranking, Ngram filtering) to determine their relative contributions to overall performance and identify potential redundancy in the ranking mechanism.