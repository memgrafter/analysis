---
ver: rpa2
title: Dynamic Policy Fusion for User Alignment Without Re-Interaction
arxiv_id: '2409.20016'
source_url: https://arxiv.org/abs/2409.20016
tags:
- policy
- human
- fusion
- task
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of aligning deep reinforcement
  learning (RL) policies with human preferences without requiring additional environment
  interaction. The proposed method, Dynamic Policy Fusion, adapts a pre-trained task
  policy by inferring user intent from trajectory-level feedback using an LSTM-based
  approach and dynamically fusing it with the task policy.
---

# Dynamic Policy Fusion for User Alignment Without Re-Interaction

## Quick Facts
- **arXiv ID:** 2409.20016
- **Source URL:** https://arxiv.org/abs/2409.20016
- **Reference count:** 40
- **Primary result:** Proposes a method to align deep RL policies with human preferences using trajectory feedback without additional environment interaction.

## Executive Summary
This work addresses the challenge of aligning deep reinforcement learning policies with human preferences without requiring additional environment interaction. The proposed method, Dynamic Policy Fusion, adapts a pre-trained task policy by inferring user intent from trajectory-level feedback using an LSTM-based approach and dynamically fusing it with the task policy. The fusion mechanism modulates the influence of the user intent policy based on accumulated rewards, ensuring both task completion and alignment with user preferences. Experiments on 2D Navigation, Highway, and Pong environments demonstrate that the method outperforms baselines such as DQN, RUDDER, MORL, and static fusion by consistently balancing task performance with user-specific needs across avoidance, preference, and mixed personalization modes.

## Method Summary
The method employs an LSTM-based user intent policy that processes trajectory-level feedback to infer user preferences. This intent policy is dynamically fused with a pre-trained task policy using a gating mechanism that modulates their influence based on accumulated rewards. The fusion ensures that the resulting policy balances task completion with user alignment, adapting to different personalization modes (avoidance, preference, mixed) without requiring additional environment interactions. The approach leverages the strengths of both the task policy and user intent inference to achieve robust alignment.

## Key Results
- Outperforms baselines (DQN, RUDDER, MORL, static fusion) in balancing task performance with user alignment across multiple environments.
- Demonstrates consistent performance in 2D Navigation, Highway, and Pong tasks under avoidance, preference, and mixed personalization modes.
- Achieves user alignment without requiring additional environment interactions, relying solely on trajectory-level feedback.

## Why This Works (Mechanism)
The method works by dynamically fusing a pre-trained task policy with an LSTM-based user intent policy inferred from trajectory feedback. The fusion mechanism modulates the influence of the intent policy based on accumulated rewards, ensuring that the resulting policy balances task completion with user preferences. This approach avoids the need for re-training or additional interactions, leveraging trajectory-level feedback to infer and adapt to user intent effectively.

## Foundational Learning
- **LSTM-based intent inference**: Needed to process sequential trajectory data and extract meaningful patterns of user intent; quick check: verify LSTM outputs meaningful intent vectors from trajectory sequences.
- **Dynamic policy fusion**: Required to balance task policy and user intent policy based on real-time reward accumulation; quick check: test fusion weights' impact on policy performance under varying reward conditions.
- **Trajectory-level feedback**: Essential for inferring user preferences without additional interactions; quick check: assess feedback quality's impact on intent accuracy.

## Architecture Onboarding

**Component Map:** Pre-trained Task Policy -> LSTM User Intent Policy -> Dynamic Fusion Mechanism -> Final Policy

**Critical Path:** Trajectory feedback → LSTM intent inference → Dynamic fusion with task policy → Policy execution

**Design Tradeoffs:** The method trades computational complexity (due to LSTM inference) for reduced interaction overhead, enabling alignment without re-training. Static fusion is simpler but less adaptive to user preferences.

**Failure Signatures:** Poor user intent inference due to sparse or ambiguous feedback; ineffective fusion when task policy and intent policy conflict significantly; reduced performance in high-dimensional or non-stationary environments.

**First Experiments:** 1) Test LSTM intent accuracy on synthetic trajectory data; 2) Evaluate fusion performance with varying reward accumulation rates; 3) Compare static vs. dynamic fusion in controlled environments.

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on trajectory-level feedback, which may be sparse or ambiguous in real-world applications.
- Scalability to longer or more complex trajectories remains uncertain.
- Performance depends on the quality of the pre-trained task policy.

## Confidence
- **High:** Effectiveness of dynamic policy fusion in balancing task completion with user alignment across multiple environments.
- **Medium:** Generalizability of results to more complex, high-dimensional tasks or real-world scenarios.
- **Low:** Robustness to noisy or inconsistent feedback, as this aspect was not thoroughly explored.

## Next Checks
1. Test the method on high-dimensional tasks such as robotic manipulation or autonomous driving to assess scalability and robustness.
2. Evaluate the method's performance under varying levels of feedback sparsity and noise to understand its resilience to imperfect user input.
3. Investigate the method's adaptability to non-stationary user preferences by simulating dynamic preference shifts during policy execution.