---
ver: rpa2
title: Revisiting Multi-Modal LLM Evaluation
arxiv_id: '2408.05334'
source_url: https://arxiv.org/abs/2408.05334
tags:
- questions
- datasets
- visual
- bounding
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates recent multi-modal large language models
  (MLLMs) on challenging datasets designed to address known weaknesses in earlier
  evaluation benchmarks. The authors assess LLaVA 1.5, LLaVA-NeXT, BLIP2, InstructBLIP,
  GPT-4V, and GPT-4o on four specialized datasets: TDIUC (12 question types), TallyQA
  (simple/complex counting), DVQA (chart understanding with OCR), and VQDv1 (visual
  query detection requiring multiple bounding boxes).'
---

# Revisiting Multi-Modal LLM Evaluation

## Quick Facts
- arXiv ID: 2408.05334
- Source URL: https://arxiv.org/abs/2408.05334
- Reference count: 40
- Primary result: Multi-modal LLMs show significant weaknesses in visual grounding, complex reasoning, and handling synthetic data across specialized evaluation datasets

## Executive Summary
This paper evaluates recent multi-modal large language models (MLLMs) including LLaVA 1.5, LLaVA-NeXT, BLIP2, InstructBLIP, GPT-4V, and GPT-4o on challenging datasets designed to address known weaknesses in earlier evaluation benchmarks. The evaluation reveals significant limitations in visual grounding, complex reasoning, and synthetic data handling across all tested models. The authors created "slim" versions of existing datasets and integrated them into the LAVIS evaluation framework to make these challenging benchmarks more accessible for future research.

## Method Summary
The authors evaluate MLLMs on four specialized datasets: TDIUC (12 question types), TallyQA (simple/complex counting), DVQA (chart understanding with OCR), and VQDv1 (visual query detection requiring multiple bounding boxes). They created reduced "slim" versions of these large datasets to improve accessibility and integrated them into the LAVIS framework. The evaluation systematically tests different aspects of MLLM performance including visual grounding, reasoning, counting, and chart interpretation across multiple model architectures.

## Key Results
- VQDv1 results show all models struggle with multi-object detection, with LLaVA-NeXT achieving only 27.01% micro F1
- TDIUC reveals models perform well on attribute-based questions but poorly on positional reasoning (19.1% micro accuracy for LLaVA-NeXT) and counting tasks
- TallyQA shows dramatic performance drops for complex counting, with GPT-4o accuracy falling from 81.5% (simple) to 71.7% (complex)
- DVQA results indicate MLLMs struggle with chart interpretation, with LLaVA-NeXT reaching only 74.06% micro accuracy compared to 80.04% for a trained system

## Why This Works (Mechanism)
The evaluation framework successfully identifies specific weaknesses in MLLMs by using carefully designed datasets that target known failure modes. The synthetic data in TallyQA and DVQA provides controlled testing conditions that reveal fundamental limitations in counting and chart interpretation capabilities. The multi-task nature of the evaluation across different visual reasoning domains provides comprehensive coverage of MLLM capabilities.

## Foundational Learning

**Visual grounding** - The ability to associate textual descriptions with specific regions in images, critical for tasks like VQDv1 that require precise object localization. Quick check: Can the model identify and localize multiple objects in an image based on textual queries?

**Complex numerical reasoning** - The capability to perform counting and mathematical operations beyond simple identification, tested through TallyQA's complex counting questions. Quick check: Can the model count multiple object categories or nested groups accurately?

**Chart interpretation** - The skill of extracting and reasoning about information from visual data representations, essential for DVQA's chart-based questions. Quick check: Can the model correctly interpret bar charts, pie charts, and line graphs with embedded text?

## Architecture Onboarding

**Component map**: MLLM architecture -> Multi-modal fusion layer -> Vision encoder -> Language model -> Output generation

**Critical path**: Input image → Vision encoder → Multi-modal fusion → Language model → Answer generation

**Design tradeoffs**: The study highlights tradeoffs between model size/complexity and performance on specialized tasks. Smaller models like LLaVA-NeXT show competitive performance on some tasks but struggle more with complex reasoning compared to larger models like GPT-4V.

**Failure signatures**: 
- Poor performance on multi-object detection tasks (VQDv1)
- Significant accuracy drops on complex counting questions (TallyQA)
- Struggles with chart interpretation and OCR-dependent reasoning (DVQA)
- Weakness in positional reasoning and spatial understanding (TDIUC)

**First experiments**:
1. Evaluate model performance on single-object vs multi-object queries in VQDv1
2. Test counting accuracy on simple vs complex TallyQA questions
3. Compare chart interpretation performance with and without OCR assistance

## Open Questions the Paper Calls Out
None

## Limitations
- "Slim" dataset versions may not fully represent original benchmark difficulty or distribution characteristics
- Synthetic data used in TallyQA and DVQA may not accurately reflect real-world counting and chart interpretation challenges
- Evaluation methodology's focus on specific question types may not capture full complexity of real-world visual reasoning

## Confidence
- High confidence: Counting task performance drops (TallyQA simple vs complex) are robust across models
- Medium confidence: Multi-object detection results (VQDv1) are consistent but may not generalize to all scenarios
- Medium confidence: Chart interpretation limitations (DVQA) are evident but baseline comparison may not represent state-of-the-art

## Next Checks
1. Evaluate the same MLLMs on full versions of test datasets to verify "slim" dataset results are representative
2. Conduct human evaluation studies to validate synthetic data reflects real-world challenges
3. Test model performance on cross-dataset generalization with untrained question types