---
ver: rpa2
title: Gaussian Embedding of Temporal Networks
arxiv_id: '2405.17253'
source_url: https://arxiv.org/abs/2405.17253
tags:
- latent
- uncertainty
- nodes
- node
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TGNE, a Bayesian approach for embedding nodes
  of continuous-time temporal networks into a low-dimensional latent space using Gaussian
  distributions. TGNE builds on the Continuous Latent Position Model (CLPM) by representing
  nodes as piece-wise linear trajectories of Gaussian distributions, allowing it to
  capture both structural information and uncertainty around the trajectories.
---

# Gaussian Embedding of Temporal Networks

## Quick Facts
- arXiv ID: 2405.17253
- Source URL: https://arxiv.org/abs/2405.17253
- Reference count: 40
- This paper introduces TGNE, a Bayesian approach for embedding nodes of continuous-time temporal networks into a low-dimensional latent space using Gaussian distributions.

## Executive Summary
This paper presents TGNE (Temporal Gaussian Network Embedding), a Bayesian method for embedding nodes in continuous-time temporal networks using Gaussian distributions. The method extends the Continuous Latent Position Model by representing nodes as piece-wise linear trajectories of Gaussian distributions, capturing both structural information and uncertainty. Using variational inference, TGNE estimates posterior distributions of latent positions given observed interactions, with experiments showing competitive performance in reconstructing unobserved edge interactions while providing uncertainty estimates that align with temporal degree distributions.

## Method Summary
TGNE embeds temporal network nodes as Gaussian-distributed trajectories in latent space using a Bayesian framework. The method parameterizes node positions as piece-wise linear functions defined by critical points, with the generative model assuming Poisson process interactions between nodes whose rates depend on latent distances. A mean-field variational approach approximates the posterior over critical points, optimized via the evidence lower bound (ELBO) using the ADAM algorithm. The method employs negative sampling for scalability and handles cumulative rate calculations through either closed-form solutions or Riemann sums depending on the chosen distance metric.

## Key Results
- TGNE generates competitive time-varying embedding locations compared to common baselines for reconstructing unobserved edge interactions
- Uncertainty estimates from the variational posterior align with time-varying degree distribution in the network
- The method provides valuable insights into temporal dynamics through natural uncertainty quantification

## Why This Works (Mechanism)

### Mechanism 1
Variational inference in the CLPM allows efficient estimation of latent Gaussian trajectories while preserving uncertainty quantification. By parameterizing the posterior over critical points as a product of independent Gaussians and optimizing the ELBO, the method trades off between data fit (expected log-likelihood) and temporal smoothness (KL prior term). The core assumption is that the true posterior is well-approximated by the mean-field variational distribution. This works because it enables scalable optimization while maintaining principled uncertainty estimates. The break condition occurs if the true posterior has strong node-node or time-step correlations, causing the mean-field assumption to fail and lead to overconfident uncertainty estimates.

### Mechanism 2
Piecewise linear critical point trajectories enable closed-form (or efficiently approximated) cumulative rate calculations for Poisson processes. Assuming linearity between change points turns the rate integral into a tractable form (closed form for Euclidean distance, Riemann sum otherwise), allowing gradient-based optimization. The core assumption is that the true trajectory can be well-approximated by piecewise linear segments over fixed intervals. This works because it transforms an intractable integral into a computable form. The break condition is if the true node trajectory is highly non-linear, causing the piecewise approximation to incur large approximation error in the cumulative rate and hurting reconstruction quality.

### Mechanism 3
Uncertainty estimates naturally emerge from the variational posterior scale parameters and align with node degree variation over time. The variance of the Gaussian critical points propagates through the rate function to yield edge-level uncertainty; higher variance corresponds to sparser interaction history. The core assumption is that node degree variation over time is driven by latent position uncertainty. This works because it provides a principled way to quantify prediction confidence that reflects the data's temporal structure. The break condition is if the generative process for interactions is dominated by factors unrelated to latent positions (e.g., exogenous bursts), causing uncertainty to not correlate with observed degree changes.

## Foundational Learning

- Concept: Poisson process likelihood for event data
  - Why needed here: The model assumes events between node pairs are generated by independent Poisson processes whose rates depend on latent positions; understanding the likelihood is essential to grasp the objective.
  - Quick check question: Given a Poisson process with rate λ over interval [a,b], what is the probability of observing m events at times t1,...,tm?
    - Answer: exp(-Λ([a,b])) ∏ λ(ti), where Λ([a,b]) = ∫_a^b λ(s) ds.

- Concept: Variational inference and ELBO
  - Why needed here: The intractable posterior is approximated via a variational distribution; the ELBO is the tractable objective to maximize.
  - Quick check question: What two terms compose the ELBO for this model?
    - Answer: The expected log-likelihood of the data under the variational posterior and the KL divergence between the variational posterior and the prior.

- Concept: Gaussian random walk prior
  - Why needed here: It enforces temporal smoothness on latent trajectories, preventing overfit to noise in sparse temporal data.
  - Quick check question: How does the prior scale τ control trajectory smoothness?
    - Answer: Larger τ allows bigger jumps between consecutive critical points (less smoothness); smaller τ constrains them to stay close.

## Architecture Onboarding

- Component map:
  - Event list (i, j, t) → Node pairs and timestamps
  - Piecewise linear interpolation between critical points → Encoder
  - Poisson process rate function (Euclidean distance or dot product) → Likelihood
  - Product of independent Gaussians over (node, timestep) → Variational posterior
  - Gaussian random walk over critical points → Prior
  - ADAM on ELBO loss → Optimizer
  - Negative sampling for scalability → Inference

- Critical path:
  1. Initialize critical points and variational parameters
  2. Sample from variational posterior (reparameterization)
  3. Compute Poisson rates and cumulative rates (closed form or Riemann sum)
  4. Evaluate ELBO (expected log-likelihood + KL divergence)
  5. Backpropagate and update parameters

- Design tradeoffs:
  - Fixed vs. adaptive change points: Fixed simplifies implementation but may miss bursty regions
  - Mean-field vs. correlated posterior: Mean-field is scalable but may underestimate uncertainty
  - Closed-form vs. Riemann cumulative rate: Closed-form is exact but model-specific; Riemann is general but approximate

- Failure signatures:
  - Poor reconstruction → Check cumulative rate approximation accuracy or prior scale
  - Degenerate uncertainty (all low or all high) → Verify KL term is active; check data scaling
  - Training instability → Monitor ELBO components; reduce learning rate or increase prior strength

- First 3 experiments:
  1. Run on simulated SBM with known ground truth; verify that TGNE recovers cluster structure and uncertainty peaks during community change
  2. Vary prior scale τ on HighSchool dataset; observe trade-off between smoothness and fit in latent trajectories
  3. Compare AUC for link prediction on train vs. test edges; check if regularization improves generalization as in the paper

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TGNE perform on networks with millions of nodes compared to existing methods?
- Basis in paper: [explicit] The paper discusses scalability strategies like node-batching and negative sampling but does not provide empirical comparisons for large-scale networks.
- Why unresolved: The experiments were conducted on relatively small datasets (up to 1899 nodes), leaving uncertainty about TGNE's performance on larger networks.
- What evidence would resolve it: Runtime and accuracy benchmarks of TGNE on datasets with millions of nodes compared to state-of-the-art methods like temporal graph neural networks.

### Open Question 2
- Question: How does the choice of hyperparameters (number of change points K and prior scale τ) affect TGNE's performance across different network types?
- Basis in paper: [explicit] The paper discusses the effect of hyperparameters on uncertainty and embeddings but does not provide systematic guidelines for choosing them based on network characteristics.
- Why unresolved: While the paper shows qualitative effects, it lacks quantitative analysis across diverse network types (e.g., scale-free vs. random graphs).
- What evidence would resolve it: A comprehensive study varying K and τ on multiple network types, measuring reconstruction accuracy and uncertainty alignment.

### Open Question 3
- Question: Can TGNE be extended to handle dynamic node addition/removal in real-time?
- Basis in paper: [inferred] The paper mentions that TGNE is transductive and cannot embed unobserved nodes, suggesting limitations for dynamic networks.
- Why unresolved: The paper does not explore extensions to make TGNE inductive for dynamic graphs.
- What evidence would resolve it: A modified TGNE framework that incorporates amortized inference for dynamic node embeddings, validated on streaming network data.

## Limitations
- The mean-field variational assumption may fail when node interactions exhibit strong temporal or spatial correlations, leading to overconfident uncertainty estimates
- Piecewise linear trajectories may inadequately capture highly non-linear temporal dynamics, introducing approximation error in cumulative rate calculations
- The assumption that uncertainty correlates with degree variation may break down when interactions are driven by exogenous factors rather than latent position dynamics

## Confidence
- **High confidence**: The core mathematical framework (CLPM, variational inference, Poisson likelihood) is well-established and correctly implemented
- **Medium confidence**: Experimental results show competitive performance on reconstruction tasks, but lack of baseline comparisons beyond simple models limits generalizability claims
- **Low confidence**: The claim that uncertainty estimates "align with" time-varying degree distributions is supported only by qualitative observations without quantitative validation

## Next Checks
1. Test TGNE on datasets with known temporal burst patterns to verify that uncertainty estimates properly capture these dynamics
2. Compare TGNE's uncertainty calibration against Bayesian alternatives like dynamic latent space models with MCMC sampling
3. Perform ablation studies varying the number of change points K and prior scale τ to characterize their impact on reconstruction quality and uncertainty quantification