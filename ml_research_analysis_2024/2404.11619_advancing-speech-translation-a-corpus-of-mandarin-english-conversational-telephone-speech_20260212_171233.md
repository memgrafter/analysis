---
ver: rpa2
title: 'Advancing Speech Translation: A Corpus of Mandarin-English Conversational
  Telephone Speech'
arxiv_id: '2404.11619'
source_url: https://arxiv.org/abs/2404.11619
tags:
- speech
- data
- translation
- mandarin
- hkust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a 123.5-hour Mandarin-English speech translation
  corpus derived from CallHome Mandarin Chinese Speech and HKUST Mandarin Telephone
  Speech datasets. English translations were generated by bilingual annotators working
  from Mandarin transcripts, preserving disfluencies and code-switching.
---

# Advancing Speech Translation: A Corpus of Mandarin-English Conversational Telephone Speech

## Quick Facts
- arXiv ID: 2404.11619
- Source URL: https://arxiv.org/abs/2404.11619
- Reference count: 0
- Primary result: 123.5-hour Mandarin-English CTS corpus with English translations improves BLEU by 137% when used for MT fine-tuning

## Executive Summary
This paper introduces a 123.5-hour Mandarin-English speech translation corpus derived from CallHome Mandarin Chinese Speech and HKUST Mandarin Telephone Speech datasets. English translations were generated by bilingual annotators working from Mandarin transcripts, preserving disfluencies and code-switching. The dataset was split into 110.6 hours of training data (mixed CallHome and HKUST) and 4.0/4.5/4.4 hours of dev1/dev2/test sets (CallHome only). Evaluation of cascade speech translation systems showed that fine-tuning a general-purpose NLLB MT model on this corpus improved BLEU scores from 5.98 to 14.16 on the test set, a 137% relative improvement, demonstrating the critical importance of matched domain training data for conversational speech translation.

## Method Summary
The corpus combines CallHome Mandarin Chinese Speech (33.5 hours) and HKUST Mandarin Telephone Speech (90 hours) with English translations provided by bilingual annotators from Mandarin transcripts. The training set contains 110.6 hours of mixed data, while development and test sets use only CallHome data (4.0/4.5/4.4 hours). A cascade speech translation system was evaluated using a hybrid TDNN-LSTM ASR model (WER 26.7%) and NLLB MT model, with and without fine-tuning on the CTS training data.

## Key Results
- Fine-tuning NLLB MT model on CTS training data improves BLEU from 5.98 to 14.16 (137% relative improvement)
- ASR WER on test set is 26.7% using hybrid TDNN-LSTM model
- Training data consists of 110.6 hours (mixed CallHome and HKUST), with 4.0/4.5/4.4 hours dev1/dev2/test sets
- Annotators preserved disfluencies and code-switching in English translations

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a general-purpose MT model on domain-specific conversational speech data substantially improves BLEU scores. The NLLB model, trained on general text data, lacks exposure to conversational disfluencies, code-switching, and telephone-specific acoustic characteristics. Fine-tuning adapts the model parameters to the statistical patterns of the target domain, improving alignment between source and target distributions.

### Mechanism 2
Preserving disfluencies and code-switching in translations improves model robustness for real-world conversational speech. By explicitly including hesitations, repetitions, and code-switched tokens in the training targets, the model learns to generate output that mirrors the source's pragmatic structure rather than forcing fluency at the cost of meaning.

### Mechanism 3
Using matched source-language speech and target-language text enables effective end-to-end ST training and improves cascaded systems. Paired audio-transcript-translation triples allow gradient flow from translation loss back to the speech encoder, aligning acoustic features with semantic units. Even for cascaded systems, having in-domain MT weights reduces error propagation from ASR.

## Foundational Learning

- Concept: Domain adaptation and fine-tuning
  - Why needed here: The baseline NLLB model underperforms on conversational telephone speech; fine-tuning aligns its parameters to the target domain's linguistic patterns.
  - Quick check question: What is the expected change in BLEU if you fine-tune on a mismatched domain?

- Concept: BLEU score interpretation
  - Why needed here: BLEU is the primary evaluation metric; understanding its sensitivity to disfluencies vs. fluency is critical for interpreting results.
  - Quick check question: How does BLEU penalize ungrammatical but semantically correct translations?

- Concept: Cascaded vs. end-to-end speech translation
  - Why needed here: The paper uses a cascade (ASR → MT); knowing when to switch to end-to-end is essential for system design.
  - Quick check question: At what WER threshold does error propagation in a cascade typically dominate MT gains?

## Architecture Onboarding

- Component map: Mandarin audio -> Hybrid TDNN-LSTM ASR -> Mandarin transcripts -> Bilingual translation -> English text -> NLLB MT (fine-tuned) -> BLEU evaluation
- Critical path: 1) ASR inference on test audio -> hypotheses 2) MT (fine-tuned) translation of ASR output -> English text 3) BLEU calculation vs. reference translations
- Design tradeoffs: ASR hybrid vs. end-to-end (Hybrid offers better robustness to domain shift; end-to-end may simplify pipeline but needs paired audio-translation data); Fine-tuning vs. multi-domain training (Fine-tuning is cheaper but risks overfitting; multi-domain training is more general but needs more data)
- Failure signatures: BLEU plateau < baseline after fine-tuning (likely domain mismatch or overfitting); High WER (>30%) paired with low BLEU gain (ASR errors dominate downstream MT); Disproportionate disfluency preservation (translation instructions too literal; may need post-editing filtering)
- First 3 experiments: 1) Fine-tune NLLB on CTS train set with 10%, 50%, and 100% of data to assess scaling behavior 2) Compare cascaded BLEU vs. end-to-end ST trained on the same CTS data (if available) 3) Ablate disfluency preservation: train one MT model with cleaned transcripts and compare BLEU to the original

## Open Questions the Paper Calls Out

### Open Question 1
How does fine-tuning NLLB on this CTS corpus compare to using a speech translation model directly trained on paired speech-translation data? The authors only evaluated cascade systems with ASR followed by MT, not end-to-end speech translation models.

### Open Question 2
What is the impact of code-switching and disfluencies in the Mandarin speech on translation quality, and how can models be specifically adapted to handle these phenomena? While the corpus preserves these phenomena, the paper does not analyze their specific impact on translation quality or propose methods to better handle them.

### Open Question 3
How well do the domain-specific improvements generalize to other related domains or language pairs beyond Mandarin-English conversational telephone speech? The study is limited to Mandarin-English CTS, so it's unclear whether similar gains would be observed for other language pairs or related domains.

## Limitations
- Data quality concerns from combining two distinct sources (CallHome and HKUST) with different collection protocols
- Limited evaluation scope focusing only on BLEU scores while holding ASR performance constant
- Claims about generalization beyond the specific Mandarin-English CTS domain are not empirically validated

## Confidence

**High confidence (90-100%):** The empirical finding that fine-tuning improves BLEU scores from 5.98 to 14.16 on the test set is well-supported by the reported experiments.

**Medium confidence (70-89%):** The claim that this improvement demonstrates the "critical importance of matched domain training data" is reasonable but extrapolates from a single corpus and evaluation metric.

**Low confidence (0-69%):** The assertion that preserving disfluencies and code-switching in translations improves model robustness is stated but not empirically validated.

## Next Checks
1. Fine-tune the NLLB model on the CTS training data, then evaluate on a different conversational speech corpus (e.g., Fisher English or Switchboard) to quantify domain generalization limits.

2. Create two MT training sets from the CTS data—one preserving all disfluencies and code-switching, another with cleaned, fluent translations. Train separate models and compare BLEU scores, human evaluation of semantic accuracy, and fluency metrics.

3. Implement a pipeline where ASR and MT are optimized jointly for end-to-end translation quality. Measure whether the relative importance of domain-specific MT fine-tuning changes when ASR errors are varied systematically from 15% to 35% WER.