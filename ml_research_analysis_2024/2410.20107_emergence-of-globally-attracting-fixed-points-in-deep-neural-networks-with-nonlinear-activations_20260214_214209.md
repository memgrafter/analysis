---
ver: rpa2
title: Emergence of Globally Attracting Fixed Points in Deep Neural Networks With
  Nonlinear Activations
arxiv_id: '2410.20107'
source_url: https://arxiv.org/abs/2410.20107
tags:
- kernel
- fixed
- case
- have
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the global evolution of hidden representation
  similarity in deep neural networks under the mean-field regime. It introduces a
  deterministic kernel map governing the similarity between hidden representations
  across layers, derived using Hermite polynomial expansions.
---

# Emergence of Globally Attracting Fixed Points in Deep Neural Networks With Nonlinear Activations

## Quick Facts
- arXiv ID: 2410.20107
- Source URL: https://arxiv.org/abs/2410.20107
- Reference count: 40
- Deep neural networks with nonlinear activations converge to globally attracting fixed points in hidden representation similarity under mean-field regime

## Executive Summary
This paper analyzes the evolution of hidden representation similarity in deep neural networks using a mean-field approach. The authors derive a deterministic kernel map that governs how similarity between hidden representations changes across layers, using Hermite polynomial expansions of the activation functions. They prove that for nonlinear activations, this kernel sequence converges globally to a unique fixed point, which can represent either orthogonal or similar representations depending on the activation function and network architecture. The analysis extends to networks with residual connections and normalization layers, showing similar convergence behaviors with characterized rates.

## Method Summary
The authors introduce a mean-field regime analysis where they derive a deterministic kernel map governing the evolution of representation similarity across layers. Using Hermite polynomial expansions of the activation functions, they establish recurrence relations for the kernel sequence that tracks inner products between hidden representations. The fixed point analysis reveals global convergence to unique attractors, with the nature of the fixed point (orthogonal vs. similar) determined by the activation function's Hermite coefficients. The framework is extended to residual networks and batch normalization through modified recurrence relations that capture the architectural modifications.

## Key Results
- Kernel sequence governing representation similarity converges globally to unique fixed points for nonlinear activations
- Fixed points can be orthogonal (ρ* = 0) or similar (ρ* ∈ (0,1) or ρ* = 1) depending on activation function
- Convergence rates are characterized explicitly as exponential or polynomial decay
- Results extend to residual connections and normalization layers with similar convergence behaviors

## Why This Works (Mechanism)
The convergence to fixed points emerges from the contraction properties of the kernel map in the mean-field regime. The Hermite polynomial expansion of the activation function creates a recurrence relation where the kernel at layer l+1 depends on the kernel at layer l through a nonlinear transformation. This transformation has contraction properties that drive the kernel sequence toward fixed points, with the specific location of the fixed point determined by the Hermite coefficients of the activation function. The analysis reveals how architectural choices like residual connections modify the recurrence relation but preserve the global convergence property.

## Foundational Learning
- **Mean-field regime**: Why needed - Provides tractable analysis of infinite-width networks; Quick check - Verify that network width is large enough for approximation to hold
- **Hermite polynomial expansion**: Why needed - Enables analytical characterization of activation function effects on kernel evolution; Quick check - Confirm that expansion converges for the activation functions used
- **Fixed point analysis**: Why needed - Characterizes long-term behavior of representation similarity; Quick check - Validate that the fixed point equations have unique solutions
- **Kernel methods**: Why needed - Provides framework for tracking inner products between representations; Quick check - Ensure kernel computations are numerically stable
- **Contraction mapping**: Why needed - Guarantees global convergence to fixed points; Quick check - Verify contraction constants for different activation functions

## Architecture Onboarding
**Component map**: Input -> Layer 1 -> Layer 2 -> ... -> Layer L -> Output
**Critical path**: Kernel evolution across layers determines representation similarity trajectory
**Design tradeoffs**: Width vs. depth tradeoffs affect mean-field approximation quality; Residual connections speed convergence but may change fixed point location
**Failure signatures**: Oscillations in kernel values indicate breakdown of mean-field assumption; Multiple fixed points suggest instability
**First experiments**:
1. Verify kernel convergence predictions for different activation functions across network depths
2. Compare finite-width vs. mean-field predictions for representation similarity evolution
3. Test impact of residual connections on convergence speed and fixed point location

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis relies on mean-field regime assumptions that may not hold for finite-width networks
- Hermite polynomial truncation may affect approximation accuracy for practical networks
- Results focus on representation similarity evolution without direct task performance implications
- Extension to non-standard architectures like transformers and convolutional networks not fully characterized

## Confidence
- **High confidence**: Mathematical derivation of kernel map and fixed point analysis for studied activation functions
- **Medium confidence**: Extension to residual connections and normalization layers
- **Medium confidence**: Convergence rate characterizations

## Next Checks
1. Numerically verify convergence predictions for finite-width networks across different depths and widths
2. Test fixed point analysis for additional activation functions like GELU and SiLU
3. Empirically evaluate correlation between fixed point types and downstream task performance across datasets and architectures