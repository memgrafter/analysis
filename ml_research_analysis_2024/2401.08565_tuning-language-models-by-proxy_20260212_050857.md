---
ver: rpa2
title: Tuning Language Models by Proxy
arxiv_id: '2401.08565'
source_url: https://arxiv.org/abs/2401.08565
tags:
- proxy-tuning
- llama
- base
- https
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Proxy-tuning is a decoding-time algorithm that steers large language
  models by combining their predictions with those of small tuned models, without
  accessing the large models' parameters. By applying the difference between small
  tuned and untuned models as a logit offset to large models, proxy-tuning achieves
  88-91% of the performance of directly tuned large models across knowledge, reasoning,
  and safety benchmarks.
---

# Tuning Language Models by Proxy

## Quick Facts
- arXiv ID: 2401.08565
- Source URL: https://arxiv.org/abs/2401.08565
- Reference count: 40
- Primary result: Decoding-time algorithm achieves 88-91% of directly tuned performance without accessing large model parameters

## Executive Summary
Proxy-tuning is a decoding-time algorithm that steers large language models by combining their predictions with those of small tuned models, without accessing the large models' parameters. By applying the difference between small tuned and untuned models as a logit offset to large models, proxy-tuning achieves 88-91% of the performance of directly tuned large models across knowledge, reasoning, and safety benchmarks. The method also shows promise for domain adaptation (17-32% improvement on coding tasks) and task-specific fine-tuning (31% average improvement), while enabling temporal adaptation of black-box models like GPT-3.5.

## Method Summary
Proxy-tuning uses decoding-time experts (DEXPERTS) to shift base model predictions using differences between small tuned and untuned models. The method computes the logit difference between an expert model (small tuned) and an anti-expert model (small untuned), then applies this difference as an offset to the larger base model's logits. This enables steering large models without fine-tuning their parameters, requiring only API access for black-box models. The approach is evaluated across instruction-tuning, code adaptation, task-specific fine-tuning, and temporal adaptation scenarios.

## Key Results
- Achieves 88-91% of directly tuned large model performance across knowledge, reasoning, and safety benchmarks
- Improves coding task performance by 17-32% through domain adaptation
- Shows 31% average improvement on task-specific fine-tuning benchmarks
- Successfully adapts black-box models like GPT-3.5 for temporal knowledge updates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The difference between small tuned and untuned models captures task-specific behavior changes
- Mechanism: By computing the logit difference (sM+ - sM-) between a small tuned expert model and its untuned version, the method extracts the specific behavioral changes induced by tuning
- Core assumption: The logit differences between tuned and untuned small models represent the essence of what tuning accomplished
- Evidence anchors: [abstract], [section 2]

### Mechanism 2
- Claim: Larger models retain benefits of scale while adopting task-specific behaviors from smaller tuned models
- Mechanism: The ensemble combines the large model's pretraining knowledge with the small tuned model's task-specific adaptations through additive logit manipulation
- Core assumption: The larger model's superior pretraining scale provides complementary knowledge that enhances task-specific behaviors
- Evidence anchors: [abstract], [section 3]

### Mechanism 3
- Claim: Weak-to-strong generalization enables smaller tuned models to guide larger models effectively
- Mechanism: The small tuned model acts as a "weak" signal that can still effectively guide the "strong" larger model through logit manipulation
- Core assumption: The directional guidance provided by the small tuned model is sufficient to influence the larger model's behavior
- Evidence anchors: [section 7], [abstract]

## Foundational Learning

- Concept: Decoding-time model ensembles (DEXPERTS)
  - Why needed here: Proxy-tuning builds directly on DEXPERTS framework of combining expert and anti-expert models through logit manipulation
  - Quick check question: What is the mathematical formula for combining expert and anti-expert logits in DEXPERTS?

- Concept: Logit arithmetic and probability manipulation
  - Why needed here: The method relies on understanding how to manipulate model outputs through logit addition/subtraction
  - Quick check question: How does adding a constant to all logits in a distribution affect the softmax probabilities?

- Concept: Tokenization and vocabulary alignment
  - Why needed here: The method requires that small and large models share compatible vocabularies to combine their outputs meaningfully
  - Quick check question: What happens if expert and base models use different tokenizers with overlapping but non-identical vocabularies?

## Architecture Onboarding

- Component map: Base model (M) -> Expert model (M+) -> Anti-expert model (M-) -> Ensemble logic

- Critical path: For each generation step: 1) Run base model forward pass, 2) Run expert forward pass, 3) Run anti-expert forward pass, 4) Combine logits, 5) Apply softmax, 6) Sample from resulting distribution

- Design tradeoffs: Runtime vs. quality (requires 3x forward passes), model size vs. effectiveness (smaller experts work but larger experts may provide better guidance), white-box vs. black-box access (works with only API access)

- Failure signatures: Degradation in base model quality when guidance is too strong, failure to improve when expert/anti-expert differences are too small, runtime bottlenecks due to sequential execution

- First 3 experiments:
  1. Run each model component individually on sample inputs to verify they produce sensible outputs
  2. Test the logit combination with synthetic differences to understand the effect of different guidance strengths
  3. Implement the full ensemble and compare outputs against the base model on a simple task like sentiment classification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of anti-expert model affect the performance of proxy-tuning, particularly in scenarios where the anti-expert is significantly weaker or stronger than the base model?
- Basis in paper: [explicit] The paper uses LLAMA 2-7B as the anti-expert for steering LLAMA 2-13B and LLAMA 2-70B, but does not explore variations in anti-expert strength
- Why unresolved: The paper assumes a fixed anti-expert strength without exploring the impact of varying this choice
- What evidence would resolve it: Systematic experiments comparing proxy-tuning performance with anti-experts of varying strengths

### Open Question 2
- Question: Can proxy-tuning be effectively applied to models with non-autoregressive architectures, such as diffusion models, and what modifications would be necessary?
- Basis in paper: [inferred] The paper focuses on autoregressive language models and mentions non-autoregressive models only in passing
- Why unresolved: The paper does not explore the applicability of proxy-tuning to non-autoregressive architectures
- What evidence would resolve it: Experiments applying proxy-tuning to non-autoregressive models with necessary modifications

### Open Question 3
- Question: How does the temporal adaptation of black-box models via proxy-tuning scale with the size of the gap between the base model's training data cutoff and the target adaptation period?
- Basis in paper: [explicit] The case study on GPT-3.5 for temporal adaptation shows improvement, but does not explore varying sizes of temporal gaps
- Why unresolved: The paper does not investigate how the effectiveness of proxy-tuning for temporal adaptation changes with larger temporal gaps
- What evidence would resolve it: Experiments with base models trained on progressively older data and adaptation to increasingly recent events

### Open Question 4
- Question: What is the impact of proxy-tuning on the long-term behavior of language models, particularly in terms of knowledge retention and style consistency across extended interactions?
- Basis in paper: [inferred] The paper focuses on short-term generation and does not explore long-term model behavior or interactions
- Why unresolved: The paper does not address how proxy-tuning affects the model's behavior over extended interactions
- What evidence would resolve it: Longitudinal studies tracking model behavior and user interactions over time

### Open Question 5
- Question: How does the introduction of the hyperparameter α in proxy-tuning affect the trade-off between different desired attributes of generations, and can it be optimized for specific use cases?
- Basis in paper: [explicit] The paper briefly explores α's impact on informativeness and truthfulness in TruthfulQA but does not provide a comprehensive analysis or optimization strategy
- Why unresolved: The paper does not fully explore the potential of α for fine-tuning proxy-tuning's behavior across different tasks or use cases
- What evidence would resolve it: A systematic study of α's impact on various attributes of generations, coupled with an optimization framework for specific use cases

## Limitations

- Scale Dependency Uncertainty: The method's effectiveness may degrade when attempting to steer models with 100x or 1000x parameter differences beyond the tested 10x range
- Behavioral Transfer Fidelity: Complex reasoning chains or nuanced safety behaviors may not compress effectively into simple logit offsets
- Runtime Overhead Concerns: The 3x forward pass requirement creates significant latency that could be prohibitive for real-time applications

## Confidence

**High Confidence**: The proxy-tuning algorithm works as described for the tested model sizes and tasks, successfully steers black-box models like GPT-3.5, and shows measurable improvements in domain adaptation and task-specific fine-tuning

**Medium Confidence**: The 88-91% performance retention relative to direct tuning, the weak-to-strong generalization mechanism, and the claim that larger models retain benefits of pretraining scale

**Low Confidence**: Generalization to models with >10x parameter differences, transfer effectiveness for complex reasoning and nuanced safety behaviors, and real-world runtime performance with parallel execution

## Next Checks

1. **Scale Sensitivity Analysis**: Systematically test proxy-tuning effectiveness across exponentially increasing scale gaps (e.g., 1B→10B, 1B→70B, 1B→175B) to determine the practical limits of weak-to-strong transfer

2. **Behavioral Transfer Granularity**: Design experiments to isolate which specific behaviors transfer well versus poorly, testing simple fact recall versus multi-step reasoning versus nuanced safety judgments

3. **Runtime Optimization Benchmark**: Implement and benchmark parallel execution strategies to quantify the real-world latency impact, comparing against direct tuning runtime and exploring whether approximate or selective guidance can reduce overhead while maintaining performance