---
ver: rpa2
title: 'Plan of Thoughts: Heuristic-Guided Problem Solving with Large Language Models'
arxiv_id: '2404.19055'
source_url: https://arxiv.org/abs/2404.19055
tags:
- problem
- language
- reasoning
- which
- given
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Plan of Thoughts (PoT), a novel decoding
  scheme for large language models that extends the Tree of Thoughts approach by formalizing
  it as a Partially Observable Markov Decision Process (POMDP). The key innovation
  is using the language model's own reflections about the value of a state as a search
  heuristic, allowing for more efficient exploration of the solution space.
---

# Plan of Thoughts: Heuristic-Guided Problem Solving with Large Language Models

## Quick Facts
- arXiv ID: 2404.19055
- Source URL: https://arxiv.org/abs/2404.19055
- Authors: Houjun Liu
- Reference count: 5
- Primary result: 89.4% success rate on Game of 24 vs 74% for ToT and 4% for CoT

## Executive Summary
This paper introduces Plan of Thoughts (PoT), a novel decoding scheme for large language models that extends the Tree of Thoughts approach by formalizing it as a Partially Observable Markov Decision Process (POMDP). The key innovation is using the language model's own reflections about the value of a state as a search heuristic, allowing for more efficient exploration of the solution space. The authors leverage the POMCP solver to demonstrate superior performance on the Game of 24 task, achieving a success rate of 89.4% compared to 74% for Tree of Thoughts and 4% for Chain-of-Thought. The approach also exhibits better anytime performance characteristics, solving 83.7% of cases within 10 minutes (20% of the total allotted time).

## Method Summary
Plan of Thoughts formalizes Tree of Thoughts as a Partially Observable Markov Decision Process (POMDP) where the language model's own value judgments serve as search heuristics. The method uses a hybrid approach with GPT-3.5-Turbo-Instruct for thought generation and GPT-4-Turbo for evaluation, leveraging the POMCP solver for efficient exploration. The language model evaluates intermediate states and assigns "sure," "likely," or "impossible" labels to guide the search process, while maintaining the ability to dynamically allocate computational resources to more promising branches.

## Key Results
- 89.4% success rate on Game of 24 task (vs 74% for ToT, 4% for CoT)
- Solved 83.7% of cases within 10 minutes (20% of total time)
- Achieved results at lower costs than ToT through hybrid language modeling approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using the LM's own value judgments as a search heuristic improves exploration efficiency
- Mechanism: The LM evaluates intermediate states and assigns "sure," "likely," or "impossible" labels, which serve as approximate value estimates for guiding the POMCP solver's search
- Core assumption: Language models can accurately judge the value of partial solutions through single-token evaluation prompts
- Evidence anchors:
  - [abstract]: "with the LM's own reflections about the value of a state used as a search heuristic"
  - [section]: "We leverage this assumption by... using the LM's evaluation of the likelihood of a sequence... as a heuristic for the coherence and reasoning within a subsequence"
- Break condition: If the LM's value judgments become inconsistent or if the evaluation prompt leads to systematic bias in state valuation

### Mechanism 2
- Claim: POMCP solver provides better anytime performance compared to fixed tree search
- Mechanism: POMCP dynamically allocates computational resources to more promising branches while maintaining the ability to explore exhaustively when needed
- Core assumption: The POMCP algorithm can effectively balance exploration and exploitation using LM-generated value estimates
- Evidence anchors:
  - [abstract]: "we demonstrate a superior success rate of 89.4% on the Game of 24 task as compared to existing approaches while also offering better anytime performance characteristics than fixed tree-search"
  - [section]: "the POMCP solver has excellent anytime performance characteristics; the search tree for possible solutions will prioritize most possible solutions as rated by intermediate value"
- Break condition: If the value estimates are too noisy or if the branching factor becomes too high for effective anytime performance

### Mechanism 3
- Claim: Hybrid language modeling approach reduces computational costs while maintaining performance
- Mechanism: Using GPT-3.5 for thought generation and GPT-4 for evaluation leverages the strengths of each model appropriately
- Core assumption: GPT-4's superior reasoning capabilities are only needed for evaluation, not for generating candidate thoughts
- Evidence anchors:
  - [abstract]: "we were able to obtain these results at lower costs to ToT evaluations by using a hybrid language modeling approach"
  - [section]: "distinct from ToT, to perform language model, we use two separate language models. pevaluateθ and pvalueθ (for R and O respectively) were computed using GPT-4-Turbo... and pthoughtθ was computed using GPT-3.5-Turbo-Instruct"
- Break condition: If GPT-3.5's generation quality becomes a bottleneck or if the cost savings are outweighed by the need for more iterations

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The problem formulation requires modeling uncertainty in intermediate states and planning under partial observability
  - Quick check question: What are the key components of a POMDP tuple and how do they differ from a standard MDP?

- Concept: Monte Carlo Tree Search (MCTS) and POMCP
  - Why needed here: The solver algorithm needs to efficiently explore the solution space using sampling-based methods
  - Quick check question: How does POMCP differ from standard MCTS and why is it suitable for this problem?

- Concept: Language model prompting strategies
  - Why needed here: The approach relies on carefully crafted prompts for thought generation and evaluation
  - Quick check question: What are the key differences between thought generation and evaluation prompts in this framework?

## Architecture Onboarding

- Component map: Language Model (GPT-3.5 for generation, GPT-4 for evaluation) → POMCP Solver → Game State Manager → API Interface
- Critical path: State generation → Value evaluation → POMCP action selection → State transition → Repeat until solution or timeout
- Design tradeoffs: Fixed breadth search (ToT) vs. dynamic exploration (PoT), single LM vs. hybrid approach, computation time vs. success rate
- Failure signatures: Low accuracy despite high computation time, inconsistent value judgments, excessive rollback actions
- First experiments: 1) Implement basic POMCP solver with language model evaluation, 2) Test hybrid LM approach on simple problems, 3) Compare performance against ToT baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PoT's performance scale with different problem complexities in the Game of 24, and what is the relationship between problem difficulty and time-to-solve?
- Basis in paper: [explicit] The authors mention using a dataset sorted by rate of difficulty measured by weighted average time for solution, but do not provide detailed analysis of performance across different difficulty levels.
- Why unresolved: The paper only reports overall success rates and average time-to-solve metrics without breaking down performance by problem difficulty levels.
- What evidence would resolve it: Detailed analysis showing PoT's success rate and time-to-solve for different subsets of the Game of 24 dataset categorized by problem difficulty would provide insights into how PoT scales with problem complexity.

### Open Question 2
- Question: What is the impact of using different language models (e.g., GPT-4 vs GPT-3.5-Turbo-Instruct) on PoT's performance, and how does this compare to using a single language model for all tasks?
- Basis in paper: [explicit] The authors mention using a hybrid approach with GPT-3.5-Turbo-Instruct for state generation and GPT-4-Turbo for evaluation, which contributed to improved performance and efficiency.
- Why unresolved: The paper does not provide a detailed comparison of PoT's performance when using different combinations of language models or a single language model for all tasks.
- What evidence would resolve it: Comparative experiments showing PoT's performance using different language model combinations or a single language model would help understand the impact of the hybrid approach on performance.

### Open Question 3
- Question: How does PoT's performance compare to other multi-step reasoning approaches on tasks beyond the Game of 24, such as natural language tasks like crosswords or other mathematical reasoning problems?
- Basis in paper: [inferred] The authors mention that ToT achieved state-of-the-art results on the Game of 24 and other difficult natural-language tasks such as crosswords, but do not provide a direct comparison of PoT's performance on these tasks.
- Why unresolved: The paper only reports PoT's performance on the Game of 24 task, leaving its effectiveness on other multi-step reasoning tasks unexplored.
- What evidence would resolve it: Experiments demonstrating PoT's performance on a variety of multi-step reasoning tasks, including natural language tasks and other mathematical reasoning problems, would help establish its versatility and effectiveness across different domains.

## Limitations

- Prompt engineering sensitivity: The approach heavily depends on carefully crafted prompts for both state generation and evaluation, which could significantly impact reproducibility
- Generalization beyond Game of 24: The specific heuristics and value judgments may not transfer well to problems with different characteristics
- Computational cost considerations: The computational requirements for running POMCP with language model evaluations may still be prohibitive for many real-time applications

## Confidence

- High confidence: The claim that Plan of Thoughts outperforms both Chain-of-Thought (4% vs 89.4% success rate) and Tree of Thoughts (74% vs 89.4% success rate) on the Game of 24 task is well-supported by the experimental results presented.
- Medium confidence: The claim about anytime performance characteristics is supported by the data but could benefit from more extensive testing across different problem types and time constraints.
- Medium confidence: The assertion that the hybrid language modeling approach significantly reduces computational costs is supported by the comparison to ToT, but the actual cost savings in real-world applications may vary depending on usage patterns.

## Next Checks

1. **Prompt sensitivity analysis**: Systematically vary the prompts used for state generation and evaluation to quantify the impact on success rates and identify the most robust prompt formulations.

2. **Cross-domain generalization**: Test the approach on diverse reasoning tasks beyond Game of 24, such as mathematical proof generation, logical puzzle solving, and planning problems, to evaluate its broader applicability.

3. **Computational efficiency benchmarking**: Compare the actual computational costs (API calls, latency) of PoT against baseline methods in real-world scenarios with varying problem complexities and time constraints.