---
ver: rpa2
title: 'RAGGED: Towards Informed Design of Scalable and Stable RAG Systems'
arxiv_id: '2403.09040'
source_url: https://arxiv.org/abs/2403.09040
tags:
- retrieval
- performance
- reader
- passages
- colbert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RAGGED, a systematic framework for evaluating
  retrieval-augmented generation (RAG) systems across diverse retriever-reader configurations,
  retrieval depths, and datasets. The authors find that reader robustness to noise
  is the key determinant of RAG stability and scalability, with some readers benefiting
  from increased retrieval depth while others degrade due to sensitivity to distracting
  content.
---

# RAGGED: Towards Informed Design of Scalable and Stable RAG Systems

## Quick Facts
- arXiv ID: 2403.09040
- Source URL: https://arxiv.org/abs/2403.09040
- Reference count: 35
- Primary result: Reader robustness to noise is the key determinant of RAG stability and scalability

## Executive Summary
This paper introduces RAGGED, a systematic framework for evaluating retrieval-augmented generation (RAG) systems across diverse retriever-reader configurations, retrieval depths, and datasets. The authors find that reader robustness to noise is the key determinant of RAG stability and scalability, with some readers benefiting from increased retrieval depth while others degrade due to sensitivity to distracting content. To quantify these findings, they introduce two new metrics: RAG Stability Score (RSS) and RAG Scalability Coefficient (RSC). Their large-scale experiments on open-domain, multi-hop, and specialized-domain datasets show that retrievers, rerankers, and prompts influence performance but do not fundamentally alter reader-driven trends. The framework enables principled evaluation of RAG systems, guiding future research on optimizing retrieval depth and model robustness.

## Method Summary
RAGGED evaluates RAG systems by systematically varying retriever types (BM25, ColBERT, Contriever), reader models (FLAN, Llama, GPT, Claude), retrieval depths (k=1 to 50), and datasets (Natural Questions, HotpotQA, BioASQ). The framework measures retrieval quality with recall@k, reader performance with unigram F1, and RAG system stability with RSS and RSC metrics. Experiments use context truncation per model (2k for FLAN, 4k for LLaMA2, 8k for LLaMA3) and greedy decoding. The approach identifies two reader behaviors: improve-then-plateau models that benefit from increased context until performance plateaus, and peak-then-decline models that degrade when k exceeds their optimal threshold due to noise sensitivity.

## Key Results
- Reader robustness to noise is the primary driver of RAG stability, with improve-then-plateau readers maintaining performance at high k while peak-then-decline readers degrade
- RSS and RSC metrics effectively capture RAG system stability and scalability beyond traditional performance measures
- Reranking consistently outperforms prompting as a noise-filtering strategy, though prompting helps noise-sensitive readers
- Multi-hop questions benefit more from retrieval than single-hop questions, with models maintaining accuracy above no-context baselines longer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reader robustness to noise is the primary driver of RAG stability and scalability, not retriever quality.
- Mechanism: When a reader model is robust to noise, it can effectively filter irrelevant information from retrieved passages, allowing it to maintain or improve performance as more context is added. Noise-sensitive readers, however, are easily distracted by irrelevant content, leading to performance degradation.
- Core assumption: The ability to distinguish relevant from irrelevant information is a learned capability that varies significantly across reader models.
- Evidence anchors:
  - [abstract] "Our analysis reveals that reader robustness to noise is the key determinant of RAG stability and scalability."
  - [section 5] "Specifically, we observe two distinct trends in reader performance... improve-then-plateau models... peak-then-decline models."
  - [corpus] "RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots" suggests retrieval can be both beneficial and harmful, supporting the noise sensitivity concept.

### Mechanism 2
- Claim: Increasing retrieval depth (k) does not uniformly improve RAG performance; its effect depends on the reader's noise robustness.
- Mechanism: Robust readers (improve-then-plateau) continue to benefit from additional context until performance plateaus, while noise-sensitive readers (peak-then-decline) degrade when k exceeds their optimal threshold due to information overload.
- Core assumption: The optimal retrieval depth is not universal but varies based on the reader's ability to process and filter information.
- Evidence anchors:
  - [abstract] "Some readers benefit from increased retrieval depth, while others degrade due to their sensitivity to distracting content."
  - [section 5] "We observe two distinct trends in reader performance... improve-then-plateau models... peak-then-decline models."
  - [corpus] "RECON: Reasoning with Condensation for Efficient Retrieval-Augmented Generation" suggests context management is critical for RAG performance, supporting the depth-sensitivity mechanism.

### Mechanism 3
- Claim: Prompting strategies have mixed effects on noise filtering and cannot compensate for poor retrieval quality.
- Mechanism: Reranking consistently improves retrieval quality by elevating relevant documents, while prompting effects vary. Noise-robust models may already possess inherent relevance filtering, making prompting redundant, while noise-sensitive models may benefit from explicit relevance instructions.
- Core assumption: The effectiveness of prompting depends on the reader's pretrained ability to filter noise and the quality of the underlying retrieval.
- Evidence anchors:
  - [section 6.3] "Prompting helps the noise-sensitive model but has no impact on the noise-robust model."
  - [section 7.2] "Reranking consistently outperforms prompting as a noise-filtering strategy."
  - [corpus] "Evaluating the Robustness of Retrieval-Augmented Generation to Adversarial Evidence in the Health Domain" suggests robustness evaluation is critical, supporting the need for noise-aware prompting.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG) systems combine retrieval mechanisms with language models to enhance responses with external knowledge.
  - Why needed here: Understanding RAG fundamentals is essential to grasp how reader robustness affects system performance and why retrieval depth matters.
  - Quick check question: What are the two main components of a RAG system and how do they interact?

- Concept: Noise sensitivity in language models refers to their ability to filter irrelevant or misleading information from input.
  - Why needed here: Reader noise sensitivity is the central factor determining RAG stability and scalability, making it crucial to understand how different models handle noisy contexts.
  - Quick check question: How does noise sensitivity manifest differently in improve-then-plateau versus peak-then-decline reader models?

- Concept: Evaluation metrics for RAG systems should assess both performance and robustness to capture how models behave across different configurations.
  - Why needed here: The paper introduces RSS and RSC as complementary metrics to traditional F1, highlighting the need for comprehensive evaluation beyond accuracy alone.
  - Quick check question: What do RSS and RSC measure that traditional performance metrics like F1 do not capture?

## Architecture Onboarding

- Component map: Retriever (BM25, ColBERT, Contriever) → Reranker (optional) → Reader (FLAN, Llama, GPT, Claude) → Generator
- Critical path: Document retrieval → Context filtering → Answer generation → Evaluation
- Design tradeoffs: Retrieval depth vs. noise sensitivity, retriever quality vs. reader robustness, prompting vs. reranking
- Failure signatures: Performance degradation at high k for peak-then-decline models, low RSS indicating instability, poor BioASQ performance despite good retrieval
- First 3 experiments:
  1. Evaluate a single reader model across different retrieval depths (k=1 to 50) to identify if it exhibits improve-then-plateau or peak-then-decline behavior
  2. Compare reader performance with and without gold passages present to assess noise sensitivity
  3. Test the effect of reranking on reader performance to determine if it improves stability beyond prompting interventions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does reader architecture (e.g., encoder-decoder vs. decoder-only) fundamentally determine noise sensitivity, or is it primarily a function of training data diversity and denoising objectives?
- Basis in paper: [inferred] The paper observes distinct noise sensitivity patterns between FLAN (improve-then-plateau) and LLAMA (peak-then-decline) families but cannot fully attribute this to architecture alone, noting that context window size and model type do not fully explain the trends.
- Why unresolved: The paper lacks access to training details for closed-source models and does not conduct controlled architectural ablation studies to isolate the impact of architecture versus training objectives.
- What evidence would resolve it: Systematic experiments training identical architectures (encoder-decoder vs. decoder-only) with varied denoising objectives and noise exposure during pretraining would clarify whether architecture or training data diversity is the primary driver.

### Open Question 2
- Question: Can RAGGED's RSS and RSC metrics be adapted to capture directional sensitivity to under-retrieval versus over-retrieval, given that retrieval effects may not be symmetric?
- Basis in paper: [explicit] The authors acknowledge that RSS currently uses a symmetric window to evaluate performance variation around the optimal retrieval depth, but note that retrieval effects can be directionally asymmetric—adding irrelevant context may degrade performance differently than omitting high-quality content.
- Why unresolved: The current symmetric RSS metric may not fully capture the directional nature of retrieval failures, potentially masking important model-specific sensitivities to under- versus over-retrieval.
- What evidence would resolve it: Developing and validating directional extensions of RSS that separately measure performance sensitivity to left-side (under-retrieval) and right-side (over-retrieval) variations would reveal whether models exhibit asymmetric fragility patterns.

### Open Question 3
- Question: What is the optimal strategy for balancing retrieval depth in multi-hop questions where different hops may benefit from different context volumes?
- Basis in paper: [inferred] The paper shows that multi-hop questions benefit more from retrieval than single-hop questions and that models maintain accuracy above no-context baselines longer in HotpotQA, suggesting different reasoning stages may have different context requirements.
- Why unresolved: The paper does not investigate whether a single retrieval depth is optimal for all reasoning stages in multi-hop questions or whether adaptive retrieval strategies could better serve complex reasoning tasks.
- What evidence would resolve it: Experiments varying retrieval depth per hop in multi-step reasoning chains, combined with fine-grained performance analysis at each reasoning stage, would identify whether dynamic retrieval depth allocation improves multi-hop QA performance.

## Limitations

- The noise sensitivity definition relies primarily on empirical observation rather than explicit characterization of what makes certain readers more sensitive to irrelevant information
- The specific findings about reader behaviors may not extend to all knowledge domains or question types, as evidenced by domain-specific performance patterns in BioASQ
- The study focuses on greedy decoding and specific prompt formats, potentially limiting applicability to other generation strategies or prompting approaches

## Confidence

- **High Confidence**: The empirical finding that reader robustness to noise is the primary determinant of RAG stability (supported by consistent trends across multiple datasets and configurations)
- **Medium Confidence**: The claim that RSS and RSC metrics provide meaningful insights beyond traditional performance measures (requires broader adoption and validation)
- **Low Confidence**: The assertion that reranking universally outperforms prompting as a noise-filtering strategy (based on limited experimental conditions)

## Next Checks

1. Systematically compare noise sensitivity across different reader architectures (encoder-decoder, decoder-only, encoder-only) to identify architectural patterns underlying robustness

2. Design targeted prompt interventions for peak-then-decline readers to determine if sophisticated prompting can transform them into improve-then-plateau performers

3. Implement adaptive retrieval depth mechanisms that adjust k based on reader characteristics to test whether personalized context management can overcome inherent noise sensitivity limitations