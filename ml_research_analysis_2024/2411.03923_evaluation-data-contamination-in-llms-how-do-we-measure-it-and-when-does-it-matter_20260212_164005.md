---
ver: rpa2
title: 'Evaluation data contamination in LLMs: how do we measure it and (when) does
  it matter?'
arxiv_id: '2411.03923'
source_url: https://arxiv.org/abs/2411.03923
tags:
- contamination
- contaminated
- dataset
- match
- marked
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel analysis method, ConTAM, to empirically
  evaluate and compare different contamination metrics used to detect evaluation data
  contamination in large language models (LLMs). ConTAM assesses metrics based on
  whether they identify examples that actually inflate benchmark scores (measured
  via Estimated Performance Gain, EPG).
---

# Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?

## Quick Facts
- arXiv ID: 2411.03923
- Source URL: https://arxiv.org/abs/2411.03923
- Authors: Aaditya K. Singh; Muhammed Yusuf Kocyigit; Andrew Poulton; David Esiobu; Maria Lomeli; Gergely Szilvasy; Dieuwke Hupkes
- Reference count: 40
- Primary result: Proposes ConTAM framework to evaluate contamination metrics based on their ability to identify examples that inflate benchmark scores (EPG), finding that LONGEST_MATCH method and n ≤ 8, mincount = 1 settings reduce false negatives

## Executive Summary
This paper addresses the challenge of evaluating and comparing different contamination metrics used to detect evaluation data contamination in large language models. The authors propose ConTAM (Contamination Threshold Analysis Method), a novel framework that assesses metrics based on whether they identify examples that actually inflate benchmark scores, measured via Estimated Performance Gain (EPG). By analyzing four contamination metrics across 13 benchmarks and 7 models from two families, the study reveals that contamination has a much larger impact than reported in recent LLM releases, primarily due to false negatives in existing metrics. The LONGEST_MATCH method, which focuses on the longest contaminated substring rather than all matches, outperforms others. The paper also demonstrates that using n > 8 or minimum frequency > 1 leads to false negatives, while smaller models benefit more from contamination when performance gains are possible, though larger models excel at exploiting contamination overall.

## Method Summary
The authors introduce ConTAM, a methodology that compares contamination metrics by their ability to identify examples that inflate benchmark scores (EPG). The approach involves computing contamination scores using four different metrics (NGRAM_MATCH, TOKEN_MATCH, TOKEN_EXTEND, LONGEST_MATCH) with varying hyperparameters (n-gram length, minimum frequency, skip budget), then applying contamination thresholds to split benchmarks into clean/contaminated subsets. EPG is calculated as the difference in model performance between full and clean subsets, with z-scores used to select optimal thresholds per model-benchmark pair. The framework generates ConTAM plots showing EPG vs contamination percentage and compares metrics using average z-scores. The analysis covers 13 benchmarks and 7 models from Llama and Pythia families, using two pre-training corpora (Llama 1 corpus and The Pile).

## Key Results
- Contamination has a much larger impact on benchmark scores than reported in recent LLM releases, likely due to false negatives in existing metrics
- The LONGEST_MATCH method outperforms other metrics by focusing on the longest contaminated substring rather than all matches
- Using n > 8 or minimum frequency > 1 leads to many false negatives, missing examples that result in increased EPG
- Smaller models benefit more from contamination when performance gains are possible, though larger models excel at exploiting contamination overall

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LONGEST_MATCH method outperforms other metrics by focusing on the longest contaminated substring rather than all matches.
- Mechanism: By considering only the longest matching substring, the method avoids assigning high contamination scores to samples with multiple short, unrelated matches that may not meaningfully contribute to inflated benchmark scores.
- Core assumption: The longest contiguous match is more likely to reflect genuine contamination than the union of all shorter matches.
- Evidence anchors: [abstract] "considering only the longest contaminated substring generally provides a better signal than considering a union of all contaminated substrings"; [section 5.1.2] "LONGEST_MATCH is specifically better at quantifying contamination on these tasks that are used to evaluate more sophisticated reasoning"

### Mechanism 2
- Claim: Using smaller n (n ≤ 8) and mincount = 1 leads to fewer false negatives compared to larger n or higher mincount.
- Mechanism: Smaller n-gram lengths capture more subtle forms of contamination, and allowing mincount = 1 ensures even single occurrences are flagged, both reducing the chance of missing contamination that impacts model performance.
- Core assumption: Even infrequent or short matches can inflate benchmark scores if the model memorizes them.
- Evidence anchors: [abstract] "using values of n larger than 8 and setting a minimal count higher than one for occurrence in the pre-training corpus leads to many false negatives"; [section 6.1] "values of n larger than 8 lead to false negatives (examples that result in increased EPG but are not identified as contaminated for higher values of n)"

### Mechanism 3
- Claim: Larger models benefit more from contamination when performance gains are possible, though larger models excel at exploiting contamination overall.
- Mechanism: As model size increases, the capacity to memorize and leverage contaminated examples grows, allowing larger models to achieve greater performance gains from contamination.
- Core assumption: Model scale correlates with memorization ability and the capacity to exploit even subtle patterns in training data.
- Evidence anchors: [abstract] "smaller models benefit more from contamination when performance gains are possible, though larger models excel at exploiting contamination overall"; [section 5.2.3] "larger models can exploit examples that may look like false positives for smaller models"

## Foundational Learning

- Concept: Contamination metrics rely on string matching (n-grams) between evaluation samples and pre-training corpora.
  - Why needed here: The paper's core contribution is comparing how different string-matching strategies (e.g., longest vs. all matches) affect contamination detection and EPG.
  - Quick check question: What is the difference between NGRAM_MATCH and LONGEST_MATCH in terms of how they score contamination?

- Concept: Estimated Performance Gain (EPG) quantifies the impact of contamination on benchmark scores.
  - Why needed here: EPG is used to empirically ground contamination metrics, assuming that metrics detecting impactful contamination should correlate with higher EPG.
  - Quick check question: How is EPG calculated, and why is it a better measure than raw contamination percentage?

- Concept: Statistical significance testing (z-scores) is used to select contamination thresholds.
  - Why needed here: Z-scores help distinguish meaningful contamination from noise by comparing score distributions on clean vs. full benchmarks.
  - Quick check question: What does a high z-score indicate about the contamination threshold selected?

## Architecture Onboarding

- Component map: Contamination metrics (NGRAM_MATCH, TOKEN_MATCH, TOKEN_EXTEND, LONGEST_MATCH) -> Hyperparameter space (n, mincount, skip_budget) -> Benchmarks and models (13 benchmarks, 7 models, 2 corpora) -> Analysis pipeline (ConTAM plots, z-score thresholding, EPG calculation)

- Critical path: 1) Compute contamination scores for each sample using chosen metric and hyperparameters; 2) Apply contamination threshold to split benchmark into clean/contaminated subsets; 3) Calculate EPG as difference in model performance between full and clean subsets; 4) Use z-scores to select optimal threshold per model-benchmark pair; 5) Compare metrics and hyperparameters using ConTAM plots and average z-scores

- Design tradeoffs: Longer n-grams reduce false positives but increase false negatives; higher mincount reduces noise but may miss impactful contamination; skip budget accounts for formatting differences but adds complexity; threshold selection balances sensitivity vs. specificity

- Failure signatures: Low or negative EPG despite high contamination percentage → likely false positives; high EPG with very low contamination → likely false negatives; inconsistent results across model sizes → scale-dependent effects

- First 3 experiments: 1) Run all four metrics with n=8, mincount=1, skip_budget=0 on a small benchmark (e.g., COPA) and compare EPG and contamination percentages; 2) Vary n (8, 10, 13, 20) for NGRAM_MATCH on a medium-sized benchmark (e.g., HellaSwag) and plot EPG vs. n to observe false negative trends; 3) Compare LONGEST_MATCH vs. TOKEN_EXTEND on a high-contamination benchmark (e.g., TriviaQA) with skip_budget=0 and 5 to quantify the impact of the skip budget

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the location of contamination within a prompt (question vs answer) affect its impact on model performance?
- Basis in paper: [explicit] The paper shows that for HellaSwag and PiQA, contamination in the answer field leads to higher EPG than contamination in the question field (Figure 12).
- Why unresolved: The analysis is limited to two specific benchmarks and doesn't explore why this pattern exists or if it generalizes to other benchmarks.
- What evidence would resolve it: Testing contamination location effects across a broader range of benchmarks and investigating the underlying mechanisms (e.g., cloze-style evaluation artifacts).

### Open Question 2
- Question: Would embedding-based contamination detection methods provide more accurate results than text-based methods?
- Basis in paper: [inferred] The authors mention that embedding-based approaches could detect paraphrased contamination and are less sensitive to minor perturbations, but they haven't been thoroughly explored.
- Why unresolved: The paper only provides a brief pilot study showing recall issues, without a comprehensive comparison to text-based methods.
- What evidence would resolve it: A systematic evaluation comparing embedding-based and text-based methods across diverse benchmarks, including both verbatim and paraphrased contamination.

### Open Question 3
- Question: Does the frequency of contamination in pre-training data significantly impact its effect on model performance?
- Basis in paper: [explicit] The authors test mincount values and find that setting mincount > 1 leads to false negatives, but they don't explore the relationship between contamination frequency and performance impact.
- Why unresolved: The analysis focuses on whether contamination is detected rather than how its frequency in training data affects model performance.
- What evidence would resolve it: Experiments varying contamination frequency while keeping other factors constant, measuring both detection rates and performance impacts.

### Open Question 4
- Question: How does contamination affect post-trained models (SFT, RLHF) differently than pre-trained models?
- Basis in paper: [explicit] The authors acknowledge this as an interesting direction but focus only on pre-trained models.
- Why unresolved: The study deliberately excludes post-trained models, leaving open how fine-tuning and alignment procedures might alter contamination effects.
- What evidence would resolve it: Comparing contamination impacts across pre-trained, SFT-tuned, and RLHF-tuned versions of the same base models on identical benchmarks.

## Limitations

- The empirical grounding relies on proprietary pre-training corpora (Llama 1 and The Pile) that are not publicly available, limiting reproducibility and external validation
- The analysis depends on string-based contamination detection, potentially missing semantic or paraphrased contamination that could also impact model performance
- EPG assumes that any contamination will inflate scores, yet some "contaminated" examples may not meaningfully impact performance if memorization is imperfect

## Confidence

- **High Confidence**: The finding that contamination has a larger impact than reported in recent LLM releases, supported by systematic EPG measurements across multiple benchmarks and models; The superiority of LONGEST_MATCH over other metrics in terms of EPG correlation
- **Medium Confidence**: The claim that n > 8 and mincount > 1 lead to false negatives, based on correlation with EPG but without direct causal proof for every missed n-gram; The observation that larger models benefit more from contamination
- **Low Confidence**: The assertion that the longest substring is causally more meaningful than shorter matches, as the paper shows better EPG but does not provide direct evidence of causal significance

## Next Checks

1. **Replication with Alternative Pre-training Corpora**: Validate the core findings using publicly available pre-training corpora (e.g., C4, RefinedWeb) to confirm that the contamination patterns and metric performance are not specific to the proprietary datasets used in this study.

2. **Semantic Contamination Analysis**: Extend the analysis to include semantic similarity measures (e.g., SBERT-based cosine similarity) alongside string matching to assess whether paraphrased contamination also contributes to EPG and how it compares to exact string matches.

3. **Controlled Memorization Experiments**: Design controlled experiments where known contamination examples are inserted into training data at varying frequencies to directly measure the relationship between contamination frequency, memorization, and EPG, rather than relying on correlation in existing datasets.