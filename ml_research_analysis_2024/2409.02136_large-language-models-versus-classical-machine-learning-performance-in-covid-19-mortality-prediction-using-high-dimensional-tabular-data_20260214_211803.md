---
ver: rpa2
title: 'Large Language Models versus Classical Machine Learning: Performance in COVID-19
  Mortality Prediction Using High-Dimensional Tabular Data'
arxiv_id: '2409.02136'
source_url: https://arxiv.org/abs/2409.02136
tags:
- data
- performance
- llms
- supplementary
- validation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared classical machine learning (CML) models and
  large language models (LLMs) for predicting COVID-19 mortality using high-dimensional
  tabular clinical data from 9,134 patients. CMLs, particularly XGBoost and random
  forest, achieved superior performance with F1 scores of 0.87 and 0.83 for internal
  and external validation, respectively.
---

# Large Language Models versus Classical Machine Learning: Performance in COVID-19 Mortality Prediction Using High-Dimensional Tabular Data

## Quick Facts
- arXiv ID: 2409.02136
- Source URL: https://arxiv.org/abs/2409.02136
- Reference count: 40
- Classical ML models outperformed LLMs in COVID-19 mortality prediction from high-dimensional tabular data

## Executive Summary
This study directly compared classical machine learning (CML) models with large language models (LLMs) for predicting COVID-19 mortality using high-dimensional tabular clinical data from 9,134 patients. The research found that CML models, particularly XGBoost and random forest, achieved superior performance with F1 scores of 0.87 and 0.83 for internal and external validation, respectively. While LLMs showed promise when fine-tuned, they generally underperformed compared to CMLs for structured data tasks, with GPT-4's zero-shot classification achieving only an F1 score of 0.43.

## Method Summary
The study used a dataset of 9,134 COVID-19 patients with 281 features including demographic information, symptoms, comorbidities, and laboratory test results. Six CML models were trained and compared against three LLMs (GPT-4, Llama-3, Mistral-7b). CML models were trained using standard techniques including hyperparameter tuning, while LLMs were evaluated in zero-shot classification mode and with fine-tuning using QLoRA for Mistral-7b. Performance was measured using F1 scores, recall, and precision across internal and external validation sets. The CML models were implemented in Python using scikit-learn, while LLMs were accessed through their respective APIs or fine-tuned using Hugging Face transformers.

## Key Results
- XGBoost and random forest achieved F1 scores of 0.87 and 0.83 for internal and external validation, respectively
- GPT-4's zero-shot classification performance was limited with an F1 score of 0.43
- Fine-tuning Mistral-7b using QLoRA improved recall from 1% to 79%, yielding an F1 score of 0.74 in external validation

## Why This Works (Mechanism)
The superior performance of CML models stems from their optimization for structured data with explicit feature engineering and handling of missing values. CML algorithms like XGBoost and random forest are specifically designed to work with tabular data, making decisions based on statistical relationships between features and outcomes. LLMs, while powerful for unstructured text, lack inherent mechanisms for processing high-dimensional numerical and categorical data without extensive preprocessing. The fine-tuning of Mistral-7b demonstrated that LLMs can be adapted to structured data tasks, but this requires significant computational resources and specialized techniques like QLoRA.

## Foundational Learning

**Tabular data processing**: Understanding how different models handle structured data with mixed numerical and categorical features
- Why needed: Critical for comparing CML and LLM performance on clinical datasets
- Quick check: Verify model input requirements and preprocessing steps

**Zero-shot classification**: LLM's ability to perform tasks without task-specific training
- Why needed: Establishes baseline LLM performance on new tasks
- Quick check: Compare zero-shot performance across different LLM architectures

**Fine-tuning techniques**: Methods for adapting pre-trained models to specific tasks
- Why needed: Enables LLMs to handle structured data effectively
- Quick check: Monitor performance improvements during fine-tuning

**Imbalanced classification**: Handling datasets with unequal class distributions
- Why needed: COVID-19 mortality data typically has class imbalance
- Quick check: Verify evaluation metrics account for class imbalance

**Validation strategies**: Internal vs external validation approaches
- Why needed: Ensures model generalizability across different patient populations
- Quick check: Compare performance metrics between validation sets

## Architecture Onboarding

Component map: Data preprocessing -> Model training -> Validation -> Performance evaluation
Critical path: Feature engineering and missing value handling -> Model selection and training -> Cross-validation -> External validation
Design tradeoffs: CML models optimized for tabular data vs LLMs requiring extensive preprocessing
Failure signatures: Poor CML performance indicates feature engineering issues; poor LLM performance suggests tokenization or context length problems
Three first experiments:
1. Compare CML and LLM performance on a simplified dataset with fewer features
2. Test different missing value imputation strategies on both model types
3. Evaluate model performance across different patient demographic subgroups

## Open Questions the Paper Calls Out
None

## Limitations
- Limited generalizability to other clinical prediction tasks beyond COVID-19 mortality
- Computational resource requirements for LLM fine-tuning may limit practical implementation
- Potential preprocessing differences between CML and LLM approaches not fully addressed

## Confidence

High confidence in CML superiority for this specific COVID-19 mortality prediction task
Medium confidence in LLM fine-tuning potential, given the limited scope of testing (only Mistral-7b)
Low confidence in generalizability of results to other clinical prediction tasks

## Next Checks
1. Test the same CML and LLM approaches on multiple different clinical prediction tasks beyond COVID-19 mortality to assess generalizability
2. Conduct ablation studies to isolate the impact of data preprocessing choices on model performance differences
3. Evaluate model performance across different patient demographics and healthcare settings to assess potential bias and external validity