---
ver: rpa2
title: Multimodal Medical Disease Classification with LLaMA II
arxiv_id: '2412.01306'
source_url: https://arxiv.org/abs/2412.01306
tags:
- text
- multimodal
- vision
- fusion
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of multimodal medical data processing
  for disease classification using deep learning. The authors propose a transformer-based
  model using LLaMA II as the backbone language model to classify diseases from chest
  X-ray images and clinical reports.
---

# Multimodal Medical Disease Classification with LLaMA II

## Quick Facts
- arXiv ID: 2412.01306
- Source URL: https://arxiv.org/abs/2412.01306
- Authors: Christian Gapp; Elias Tappeiner; Martin Welk; Rainer Schubert
- Reference count: 16
- Primary result: 97.10% mean AUC on test dataset using early fusion parallel architecture

## Executive Summary
This study addresses the challenge of multimodal medical data processing for disease classification using deep learning. The authors propose a transformer-based model using LLaMA II as the backbone language model to classify diseases from chest X-ray images and clinical reports. They investigate different fusion strategies for merging text and vision information, including early, late, and mixed fusion approaches. The model is fine-tuned using Low Rank Adaptation (LoRA) for parameter-efficient training. Results show that the parallel architecture with early fusion achieves the best performance, with a mean AUC of 97.10% on the test dataset, outperforming previous models on the same multimodal dataset.

## Method Summary
The authors develop a transformer-based architecture using LLaMA II 7B as the backbone for multimodal medical disease classification. The model processes chest X-ray images and clinical reports through modality-specific encoders (text and vision layers) followed by cross-layer fusion mechanisms. Three fusion strategies are explored: early (parallel) fusion, late (serial) fusion, and mixed fusion. The model is fine-tuned using LoRA with ranks r∈{2,4,8} to reduce computational requirements. Training uses binary cross-entropy loss with Adam optimizer, weight decay of 1e-5, learning rate of 1e-4, batch size of 20, and 15 epochs on the OpenI dataset.

## Key Results
- Parallel architecture with early fusion achieves best performance at 97.10% mean AUC
- Late fusion serial architecture achieves 96.67% mean AUC
- Mixed fusion architecture with r=2 achieves 96.82% mean AUC
- All models outperform TransCheX baseline of 96.29% mean AUC

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early fusion of modality-specific features outperforms late fusion in multimodal medical classification.
- Mechanism: Combining raw, untouched text and vision features early in the architecture allows the model to capture cross-modal relationships from the start, leading to better discriminative patterns.
- Core assumption: The raw features contain sufficient complementary information that can be effectively merged before deep modality-specific processing.
- Evidence anchors:
  - [abstract] "Early fusion of modality specific features creates better results with the best model reaching 97.10% mean AUC than late fusion from a deeper level of the architecture (best model: 96.67% mean AUC)."
  - [section] "the early fusion of the two modalities text (clinical reports) and vision (2D chest X-rays) seems to perform slightly better than the late fusion (serial architecture)."
  - [corpus] Weak: No direct corpus evidence; related papers discuss fusion techniques but not direct comparison of early vs late fusion performance.
- Break condition: If modality-specific features are too heterogeneous or noisy at the raw level, early fusion could degrade signal quality.

### Mechanism 2
- Claim: Using LoRA for fine-tuning large language models reduces memory requirements while maintaining or improving performance.
- Mechanism: By freezing the pretrained LLaMA II weights and only updating low-rank decomposition matrices, the model trains efficiently without significant performance loss.
- Core assumption: The low-rank approximation captures sufficient task-relevant variation in the pretrained weights.
- Evidence anchors:
  - [section] "LoRA [11] enables to train a small amount of model parameters by freezing the pretrained model weights and injecting trainable rank decomposition matrices."
  - [section] "we use a model wrapper from Parameter Efficient Fine Tuning (PEFT) [13] in order to handle the LoRA layers."
  - [corpus] Weak: Related papers mention parameter-efficient fine-tuning but not specifically LoRA's effectiveness on LLaMA II for medical tasks.
- Break condition: If the rank r is too small, LoRA may fail to capture necessary task-specific adaptations.

### Mechanism 3
- Claim: Cross-layer attention enables effective multimodal information exchange in transformer-based architectures.
- Mechanism: Cross layers use queries from one modality and key-value pairs from the other, allowing each modality to attend to relevant features from the partner modality.
- Core assumption: Attention mechanisms can effectively model the dependencies between text and image features.
- Evidence anchors:
  - [section] "Text, vision and cross layers consist of attention blocks that compute Attention(Q, K, V ) = softmax(QK T/√dk)V."
  - [section] "The cross layer gets a query from one modality and a key-value pair from the other modality."
  - [corpus] Weak: Related papers discuss multimodal transformers but do not detail cross-layer attention mechanics.
- Break condition: If modality alignment is poor, cross-layer attention may propagate irrelevant or noisy information.

## Foundational Learning

- Concept: Multimodal fusion strategies (early, late, mixed)
  - Why needed here: Different fusion points impact how cross-modal interactions are captured and affect classification performance.
  - Quick check question: What is the main difference between early and late fusion in terms of when modality-specific features are combined?

- Concept: Low-rank adaptation (LoRA) in large language models
  - Why needed here: Enables efficient fine-tuning of large models like LLaMA II without full parameter updates.
  - Quick check question: How does LoRA reduce the number of trainable parameters compared to full fine-tuning?

- Concept: Attention mechanisms in transformers
  - Why needed here: Core building block for both modality-specific and cross-modal layers in the architecture.
  - Quick check question: What are the roles of Q, K, and V in the attention computation?

## Architecture Onboarding

- Component map:
  Input preprocessing: Text tokenizer → 4096 text features; Vision patches → 4096 vision features
  Modality-specific encoders: Text layers, Vision layers (3 each)
  Cross-modal fusion: Early (parallel), Late (serial), Mixed fusion strategies
  Output head: Dense Linear → Tanh → Dropout → Dense CLS → Sigmoid (14 classes)

- Critical path:
  1. Feature extraction (text + vision)
  2. Cross-layer fusion (position varies by architecture)
  3. Classification head processing
  4. Loss computation and LoRA parameter updates

- Design tradeoffs:
  - Early vs late fusion: Early fusion may capture interactions sooner but risks mixing raw signals; late fusion allows deeper modality-specific processing but may miss early interactions.
  - LoRA rank r: Higher r increases capacity but also memory and risk of overfitting; lower r saves resources but may underfit.

- Failure signatures:
  - Degraded AUC when LoRA rank is too low
  - Overfitting with high rank or insufficient dropout
  - Poor performance if cross-layer attention is misaligned or noisy

- First 3 experiments:
  1. Compare AUC for early vs late fusion with fixed LoRA rank (r=4)
  2. Sweep LoRA rank (r=2,4,8) within early fusion architecture
  3. Test mixed fusion architecture vs early fusion baseline with r=2

## Open Questions the Paper Calls Out
- Open Question 1: How does the proposed multimodal architecture generalize to other medical imaging modalities beyond chest X-rays, such as MRI or CT scans?
- Open Question 2: What is the optimal fusion strategy for multimodal data when dealing with more than two modalities, such as combining text, images, and physiological signals?
- Open Question 3: How does the model's performance scale with the size of the language model backbone (e.g., LLaMA II 7B vs. 30B vs. 70B) in terms of both accuracy and computational efficiency?
- Open Question 4: What is the interpretability and explainability of the model's predictions, particularly in identifying which features from text and images contribute most to disease classification?

## Limitations
- Single dataset evaluation (OpenI) with relatively small test set (377 samples)
- Limited comparison to only TransCheX baseline
- High computational requirements despite LoRA optimization
- Specific implementation details of cross-layer fusion mechanism not fully specified

## Confidence

- **High confidence**: The superiority of early fusion over late fusion is well-supported by the reported results (97.10% vs 96.67% mean AUC) and the architectural design is clearly described.
- **Medium confidence**: The effectiveness of LoRA for parameter-efficient fine-tuning is demonstrated, but the optimal rank selection (r=2 for mixed architecture) appears somewhat arbitrary without extensive ablation studies.
- **Low confidence**: The generalizability of these findings to other medical domains or different disease classification tasks remains unproven given the single-dataset evaluation.

## Next Checks

1. Cross-dataset validation: Test the model architecture and fusion strategies on an independent multimodal medical dataset (e.g., MIMIC-CXR) to assess generalizability beyond the OpenI dataset.

2. Fusion mechanism ablation: Systematically vary the number of cross layers and their positioning within the architecture to determine the optimal configuration for different fusion strategies.

3. Computational efficiency analysis: Measure actual GPU memory usage and training time across different LoRA ranks (r=2,4,8) to quantify the practical benefits of parameter-efficient fine-tuning in real-world deployment scenarios.