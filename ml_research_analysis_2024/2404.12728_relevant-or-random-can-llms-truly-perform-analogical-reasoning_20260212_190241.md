---
ver: rpa2
title: 'Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?'
arxiv_id: '2404.12728'
source_url: https://arxiv.org/abs/2404.12728
tags:
- problem
- examples
- problems
- reasoning
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  perform analogical reasoning by examining if self-generated relevant examples improve
  problem-solving more than irrelevant ones. Through extensive experiments on mathematical
  reasoning and other reasoning tasks, the authors find that self-generated irrelevant
  examples often achieve comparable or better performance than relevant ones on mathematical
  tasks, contrary to human analogical reasoning patterns.
---

# Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?

## Quick Facts
- arXiv ID: 2404.12728
- Source URL: https://arxiv.org/abs/2404.12728
- Authors: Chengwei Qin; Wenhan Xia; Tan Wang; Fangkai Jiao; Yuchen Hu; Bosheng Ding; Ruirui Chen; Shafiq Joty
- Reference count: 24
- This paper investigates whether LLMs can perform analogical reasoning by examining if self-generated relevant examples improve problem-solving more than irrelevant ones.

## Executive Summary
This paper investigates whether large language models (LLMs) can perform analogical reasoning by examining if self-generated relevant examples improve problem-solving more than irrelevant ones. Through extensive experiments on mathematical reasoning and other reasoning tasks, the authors find that self-generated irrelevant examples often achieve comparable or better performance than relevant ones on mathematical tasks, contrary to human analogical reasoning patterns. The key factor influencing performance is the accuracy of self-generated examples rather than their relevance. Based on this finding, the authors propose two novel methods that generate correct examples once and use them as in-context learning demonstrations for all test queries, significantly reducing inference costs while improving performance.

## Method Summary
The authors conduct experiments across multiple reasoning tasks (GSM8K, MATH, BBH, CommonsenseQA, MBPP, GPQA) using various LLMs (GPT-3.5, GPT-4o-mini, Llama-2, Llama-3, Llama-3.1, Qwen2.5). They compare the performance of self-generated relevant examples versus self-generated irrelevant examples on mathematical reasoning tasks. After finding that accuracy matters more than relevance, they propose two novel ICL-based methods (ICLmath and ICLbio) that generate manually verified correct examples once and reuse them as demonstrations for all test queries. The key innovation is shifting from per-query example generation to fixed demonstration sets.

## Key Results
- Self-generated random examples can achieve comparable or better performance than relevant ones on mathematical tasks (4% performance boost on GSM8K with random biological examples)
- Accuracy of self-generated examples is the key factor influencing performance, not semantic relevance
- ICLmath and ICLbio methods significantly reduce inference costs while improving performance by using fixed sets of manually verified correct examples
- LLMs cannot yet perform true analogical reasoning on mathematical tasks, instead relying on pattern matching of accurate solution steps

## Why This Works (Mechanism)

### Mechanism 1
LLMs perform analogical reasoning not through semantic relevance matching but through pattern matching of accurate solution steps. The model treats self-generated examples as demonstration patterns. When the reasoning path in the example is correct and follows a similar procedural structure to the target problem, the model can replicate the solution steps regardless of the example's surface-level topic.

### Mechanism 2
ICL-based methods that generate and verify examples once outperform per-query generation by eliminating variance in example quality. By fixing a set of accurate examples as demonstrations, the model receives consistent, high-quality reasoning patterns across all test queries, removing the variability introduced by per-query generation attempts.

### Mechanism 3
LLMs cannot perform true analogical reasoning on mathematical tasks because they lack the ability to map conceptual structures across domains. The model matches patterns based on surface features and procedural steps rather than understanding the underlying conceptual analogy between domains. When mathematical problems require domain-specific conceptual mappings, the model fails to transfer the conceptual structure.

## Foundational Learning

- **Semantic similarity vs. procedural similarity**: Why needed - The paper distinguishes between whether examples are semantically related to the query versus whether their solution procedures are similar. Quick check - If a math problem about calculating areas and a biology problem about population growth both use multiplication and division steps, would the model treat them as procedurally similar?

- **In-context learning (ICL) demonstration quality**: Why needed - The paper shows that ICL performance depends on the accuracy of demonstrations rather than their relevance to the query. Quick check - If you use 5 correct demonstrations versus 5 incorrect ones, which would you expect to produce better ICL results regardless of topic?

- **Zero-shot vs. few-shot prompting strategies**: Why needed - The paper compares different prompting approaches including self-generated examples versus no examples. Quick check - In what scenario would adding any examples (even irrelevant ones) improve performance over zero-shot prompting?

## Architecture Onboarding

- **Component map**: Problem generator -> Example verifier -> ICL demonstration assembly -> Target problem solving -> Performance evaluator

- **Critical path**: Problem generation → Example verification → ICL demonstration assembly → Target problem solving → Performance evaluation. The slowest step is typically problem generation when done per-query, which the paper addresses by using fixed example sets.

- **Design tradeoffs**: Generating relevant examples per-query provides semantic alignment but risks lower accuracy and higher computational cost; generating random accurate examples once provides cost savings and consistent quality but may miss domain-specific patterns; manual verification ensures accuracy but doesn't scale.

- **Failure signatures**: Performance drops when generated examples contain procedural errors, when examples are too domain-specific to provide useful patterns, or when the fixed example set lacks diversity to handle edge cases in the test set.

- **First 3 experiments**:
  1. Replicate the core finding by testing relevant vs. random examples on GSM8K with a different LLM to verify generalizability
  2. Test the ICLmath approach on a new mathematical reasoning dataset to confirm the fixed-example strategy works beyond the original datasets
  3. Perform ablation on example count (3 vs 5 vs 10) to determine the optimal number of demonstrations for the ICL approach

## Open Questions the Paper Calls Out

- **Open Question 1**: Do self-generated examples help LLMs perform analogical reasoning on mathematical tasks if the examples are both accurate and semantically relevant to the query? The paper only shows that accuracy matters and that semantic relevance does not matter when examples are inaccurate, but hasn't tested whether combining accuracy with semantic relevance could improve performance further.

- **Open Question 2**: Can the finding that accuracy matters more than relevance be generalized to other types of reasoning tasks beyond mathematical reasoning? While the paper mentions that findings can be generalized to other tasks, it does not provide systematic evidence across a wide range of reasoning task types.

- **Open Question 3**: What is the optimal balance between diversity and accuracy in self-generated examples for analogical reasoning? The paper only tests using a fixed set of accurate problems versus generating new relevant problems for each query, but does not explore the trade-off between diversity and accuracy.

## Limitations

- Limited scope of analogical reasoning tasks - findings primarily based on mathematical reasoning may not generalize to other analogical reasoning domains
- Manual verification bottleneck - proposed methods require human verification, limiting scalability to production settings
- Model-specific findings - experiments primarily use GPT-3.5 and Llama models, results may be model-dependent

## Confidence

- **High Confidence**: The finding that accuracy of self-generated examples matters more than their relevance is well-supported by experimental results across multiple datasets and models
- **Medium Confidence**: The conclusion that LLMs cannot perform true analogical reasoning on mathematical tasks is based on specific experimental setup and may not generalize to all forms of analogical reasoning
- **Low Confidence**: The claim that semantic relevance is completely unimportant for LLM reasoning is too strong - while relevance doesn't consistently improve performance, there may be edge cases where it provides benefits

## Next Checks

1. **Cross-Domain Analogical Reasoning Test** - Design a controlled experiment using non-mathematical analogical reasoning tasks (e.g., language analogies, visual reasoning) to determine if the irrelevance finding holds across different reasoning domains.

2. **Automated Example Verification Pipeline** - Implement and evaluate automated methods for verifying the correctness of generated examples (e.g., using a separate verifier model or rule-based checking) to address the scalability limitation of manual verification.

3. **Semantic Relevance Gradient Analysis** - Systematically vary the degree of semantic similarity between examples and target problems (from completely random to highly relevant) across multiple reasoning tasks to identify if there's a threshold effect or specific conditions where relevance does matter.