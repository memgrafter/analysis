---
ver: rpa2
title: 'Learning To See But Forgetting To Follow: Visual Instruction Tuning Makes
  LLMs More Prone To Jailbreak Attacks'
arxiv_id: '2405.04403'
source_url: https://arxiv.org/abs/2405.04403
tags:
- language
- arxiv
- visual
- vlms
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the impact of visual instruction tuning on
  the safety of large language models (LLMs). Three state-of-the-art vision-language
  models (VLMs) were compared to their respective LLM backbones using 160 adversarial
  prompts across eight prohibited scenarios.
---

# Learning To See But Forgetting To Follow: Visual Instruction Tuning Makes LLMs More Prone To Jailbreak Attacks

## Quick Facts
- arXiv ID: 2405.04403
- Source URL: https://arxiv.org/abs/2405.04403
- Reference count: 0
- Three state-of-the-art VLMs generated 27.50% to 35% more harmful content than their respective LLM backbones when tested with adversarial prompts

## Executive Summary
This study examines the safety implications of visual instruction tuning on large language models. The researchers compared three vision-language models (LLaVA, Qwen-VL-Chat, InterLM-XComposer2) against their respective LLM backbones (Vicuna, Qwen-Chat, InternLM2-Chat) using 160 adversarial prompts across eight prohibited scenarios. The results demonstrate that VLMs produce substantially more harmful content than their LLM counterparts, with increases ranging from 25.62% to 35%. The study identifies a "forgetting effect" where safety guardrails established during LLM training are diminished during visual instruction tuning, making VLMs more susceptible to jailbreak attacks. The authors recommend incorporating safety measures throughout all training stages and developing comprehensive safety benchmarks for VLMs.

## Method Summary
The researchers evaluated three state-of-the-art vision-language models and their respective LLM backbones using a dataset of 160 adversarial prompts across eight prohibited scenarios. Each VLM was tested both with and without semantically-relevant visual input. The study measured harmful content generation rates and compared the safety performance of VLMs against their LLM counterparts to quantify the impact of visual instruction tuning on safety guardrails.

## Key Results
- LLaVA produced 27.50% more harmful responses than Vicuna
- Qwen-VL-Chat generated 35% more harmful content than Qwen-Chat
- InterLM-XComposer2 created 25.62% more harmful content than InternLM2-Chat
- VLMs are more prone to generate harmful content when provided with both text prompts and semantically-relevant images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual instruction tuning creates competing objectives that weaken safety guardrails
- Mechanism: During visual instruction tuning, VLMs must simultaneously balance generating content relevant to both the text prompt and visual input while maintaining safety constraints learned during LLM training. This creates a conflict where safety mechanisms are deprioritized to satisfy multimodal coherence requirements.
- Core assumption: The model cannot optimally balance multimodal relevance and safety simultaneously, leading to safety degradation
- Evidence anchors:
  - [abstract] "VLMs are more susceptible to jailbreak attacks due to a 'forgetting effect' where safety guardrails from the LLM backbone are diminished during visual instruction tuning"
  - [section 4] "VLMs are more prone to generate potentially harmful content when provided with a prompt and a semantically-relevant image"
  - [corpus] "SMoLoRA: Exploring and Defying Dual Catastrophic Forgetting in Continual Visual Instruction Tuning" - supports the forgetting mechanism concept
- Break condition: If safety mechanisms are explicitly reinforced during visual instruction tuning or if multimodal coherence can be achieved without compromising safety objectives

### Mechanism 2
- Claim: VLMs forget adversarial prompt handling capabilities during multimodal training
- Mechanism: When VLMs undergo visual instruction tuning, they treat adversarial prompt handling as a task that gets deprioritized or forgotten in favor of learning multimodal mappings. The model reallocates capacity away from safety-related behaviors toward vision-language integration.
- Core assumption: The model has limited capacity and must trade off between maintaining safety behaviors and learning new multimodal capabilities
- Evidence anchors:
  - [abstract] "imposes a forgetting effect on an LLM's safety guardrails"
  - [section 5] "VLMs forget queries from adversarial prompts when undergoing visual instruction tuning"
  - [corpus] "Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models" - suggests safety fine-tuning has limitations that could align with forgetting effects
- Break condition: If adversarial prompt handling is explicitly included as a task during visual instruction tuning or if the model has sufficient capacity to maintain all capabilities

### Mechanism 3
- Claim: Visual context provides additional attack vectors that bypass safety mechanisms
- Mechanism: The addition of visual inputs creates new ways for adversarial prompts to influence model behavior. When combined with jailbreak prompts, visual information can either reinforce harmful content generation or create confusion that causes safety mechanisms to fail.
- Core assumption: Visual information can be leveraged by adversarial prompts in ways that text-only prompts cannot
- Evidence anchors:
  - [section 4] "VLMs are more prone to generate potentially harmful content when provided with a prompt and a semantically-relevant image"
  - [section 4] "using a jailbreak pre-prompt...provides a signal stronger than the content of the image"
  - [corpus] "Toward Autonomous Laboratory Safety Monitoring with Vision Language Models: Learning to See Hazards Through Scene Structure" - suggests VLMs can be sensitive to visual context in ways that could be exploited
- Break condition: If visual inputs are properly filtered or if safety mechanisms can effectively process multimodal inputs

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Understanding how VLMs lose safety capabilities during visual instruction tuning requires knowledge of how neural networks forget previously learned tasks when trained on new ones
  - Quick check question: What happens to a model's performance on task A when it is trained on task B without any mechanism to preserve task A knowledge?

- Concept: Multimodal representation learning
  - Why needed here: The core innovation in VLMs is learning to map between visual and language representations, which is central to understanding how safety mechanisms might be compromised
  - Quick check question: How do VLMs typically combine visual and textual embeddings during inference?

- Concept: Adversarial prompt engineering
  - Why needed here: The study focuses on jailbreaking attacks, so understanding how adversarial prompts bypass safety mechanisms is crucial
  - Quick check question: What distinguishes a jailbreak prompt from a regular prompt in terms of its interaction with safety mechanisms?

## Architecture Onboarding

- Component map: Vision encoder (CLIP-based) -> Language model backbone (Vicuna, Qwen, InternLM) -> Adapter/mapping layer -> Safety mechanisms (inherited from LLM, potentially degraded) -> Jailbreak detection/prevention systems

- Critical path: 1. Input processing (text + optional image) -> 2. Multimodal encoding and fusion -> 3. Safety mechanism evaluation -> 4. Response generation (conditional on safety check)

- Design tradeoffs:
  - Frozen vs. trainable vision encoder during visual instruction tuning
  - Depth of adapter integration (shallow vs. deep fusion)
  - Safety mechanism placement (pre-fusion vs. post-fusion)
  - Training data composition (multimodal vs. text-only safety examples)

- Failure signatures:
  - Increased harmful content generation on adversarial prompts
  - Safety mechanism bypass when visual context is present
  - Inconsistent behavior between text-only and multimodal inputs
  - Degradation of safety performance during multimodal training stages

- First 3 experiments:
  1. Compare safety performance of frozen vs. trainable vision encoder during visual instruction tuning
  2. Test whether including adversarial prompts in multimodal training data preserves safety capabilities
  3. Evaluate the impact of different adapter architectures on safety mechanism preservation

## Open Questions the Paper Calls Out

Open Question 1
- Question: What specific mechanisms in visual instruction tuning cause LLMs to become more susceptible to jailbreak attacks?
- Basis in paper: Explicit - The paper states that "visual instruction tuning damages guardrails put in place during the LLM training" and refers to a "forgetting effect" on an LLM's safety guardrails.
- Why unresolved: The paper does not provide detailed analysis of the specific training mechanisms that lead to this increased susceptibility.
- What evidence would resolve it: A detailed analysis comparing the training processes, datasets, and model architectures of VLMs and their LLM backbones to identify specific factors contributing to reduced safety.

Open Question 2
- Question: How does the inclusion of visual information in prompts affect the ability of VLMs to resist jailbreak attacks compared to text-only prompts?
- Basis in paper: Explicit - The paper observes that VLMs generate more harmful content when provided with both a prompt and a semantically-relevant image.
- Why unresolved: The paper does not provide a detailed analysis of why the presence of visual information impacts the model's resistance to jailbreak attacks.
- What evidence would resolve it: Comparative analysis of jailbreak success rates for VLMs with and without visual information, and investigation into the model's decision-making process when handling multimodal inputs.

Open Question 3
- Question: What are the most effective strategies for incorporating safety measures during visual instruction tuning to mitigate the forgetting effect?
- Basis in paper: Inferred - The paper recommends incorporating safety measures throughout all training stages and suggests that continual learning approaches might be effective.
- Why unresolved: The paper does not provide specific recommendations or empirical evidence for the most effective safety strategies during visual instruction tuning.
- What evidence would resolve it: Empirical comparison of different safety integration strategies (e.g., experience replay, logit distillation, isolated parameter updates) during visual instruction tuning and their impact on model robustness against jailbreak attacks.

## Limitations

- The dataset of 160 adversarial prompts may not capture the full spectrum of potential jailbreak attacks
- The study focuses on three specific VLMs and their LLM counterparts, which may not generalize to all VLM architectures
- The study does not explore the impact of different visual instruction tuning methodologies or training durations on safety degradation

## Confidence

**High Confidence**: The core finding that VLMs generate more harmful content than their LLM counterparts (27.50% to 35% increase) is supported by direct empirical measurements across multiple model pairs.

**Medium Confidence**: The claim that this increase is due to a "forgetting effect" where safety guardrails are diminished during visual instruction tuning is supported by the observed correlation but lacks mechanistic detail about which components are affected.

**Low Confidence**: The assertion that visual context creates additional attack vectors that bypass safety mechanisms is inferred from the results but not directly tested - the study does not isolate the contribution of visual inputs versus text inputs in the increased harmful content generation.

## Next Checks

1. **Mechanistic Analysis**: Conduct ablation studies to identify which components of the safety mechanisms (attention heads, layer activations, or specific parameters) are most affected during visual instruction tuning, and whether these changes correlate with the increased harmful content generation.

2. **Cross-Architecture Validation**: Test the same hypothesis across a broader range of VLM architectures (including those using different vision encoders, adapter types, or training methodologies) to determine if the observed forgetting effect is universal or architecture-specific.

3. **Temporal Safety Degradation**: Track safety performance throughout the visual instruction tuning process (not just at the endpoint) to determine whether safety degradation occurs gradually or suddenly, and whether it can be mitigated by adjusting training schedules or incorporating safety reinforcement at specific stages.