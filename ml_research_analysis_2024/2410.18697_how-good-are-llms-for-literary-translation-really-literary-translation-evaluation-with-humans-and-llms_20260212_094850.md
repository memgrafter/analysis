---
ver: rpa2
title: How Good Are LLMs for Literary Translation, Really? Literary Translation Evaluation
  with Humans and LLMs
arxiv_id: '2410.18697'
source_url: https://arxiv.org/abs/2410.18697
tags:
- translation
- human
- literary
- error
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating literary machine
  translation, which requires balancing cultural insight, authorial voice, creative
  interpretation, and aesthetics. The authors introduce LITEVAL-CORPUS, a comprehensive
  dataset with verified human translations and outputs from 9 MT systems across four
  language pairs, totaling over 2k translations and 13k sentences.
---

# How Good Are LLMs for Literary Translation, Really? Literary Translation Evaluation with Humans and LLMs

## Quick Facts
- **arXiv ID:** 2410.18697
- **Source URL:** https://arxiv.org/abs/2410.18697
- **Authors:** Ran Zhang; Wei Zhao; Steffen Eger
- **Reference count:** 40
- **Key outcome:** Human translators consistently outperform LLM translations, which tend to be more literal and less diverse

## Executive Summary
This paper addresses the critical challenge of evaluating literary machine translation, which requires balancing cultural insight, authorial voice, creative interpretation, and aesthetics. The authors introduce LITEVAL-CORPUS, a comprehensive dataset with verified human translations and outputs from 9 MT systems across four language pairs, totaling over 2k translations and 13k sentences. They systematically compare human evaluation schemes (MQM, SQM, BWS) using both students and professional translators, finding that MQM is inadequate for literary evaluation, while BWS and professional SQM perform best. Automatic metrics like GEMBA-MQM correlate moderately with human evaluations but struggle to distinguish human translations from LLM outputs.

## Method Summary
The authors created LITEVAL-CORPUS by collecting literary texts from German, Chinese, English, and Japanese sources, generating translations using 9 different MT systems (including 3 LLMs), and obtaining verified human translations. They conducted human evaluations using three schemes: MQM (error-based), SQM (adequacy-based), and BWS (ranking-based). Student annotators performed most evaluations, while professional translators evaluated samples from top-performing systems. Automatic metrics including GEMBA-MQM were computed for all translations. The study analyzed over 2,000 translations totaling 13,000 sentences across the four language pairs.

## Key Results
- Human translators consistently outperform LLM translations across all evaluation schemes and language pairs
- MQM is inadequate for literary evaluation, showing weaker correlations with other human judgments
- BWS and professional SQM evaluations perform best, with professionals showing higher agreement and more nuanced ratings
- Automatic metrics like GEMBA-MQM correlate moderately with human evaluations but cannot reliably distinguish human from LLM translations

## Why This Works (Mechanism)
The study's methodology succeeds because it addresses the unique challenges of literary translation evaluation through comprehensive data collection and systematic comparison of evaluation approaches. By creating a standardized corpus with verified human translations and multiple MT outputs, the authors establish a controlled environment for comparison. The use of multiple evaluation schemes allows for triangulation of quality assessments, while the inclusion of both student and professional annotators provides insights into different perspectives on translation quality.

## Foundational Learning
**MQM (Multidimensional Quality Metrics)**
- *Why needed:* Provides structured framework for identifying and categorizing translation errors
- *Quick check:* Can categorize and score different types of translation errors systematically

**SQM (Static Quality Metrics)**
- *Why needed:* Measures overall adequacy and fluency of translations on a continuous scale
- *Quick check:* Captures holistic quality judgments that reflect natural reading experience

**BWS (Best-Worst Scaling)**
- *Why needed:* Forces comparative judgments to improve inter-annotator agreement
- *Quick check:* Produces more consistent rankings by requiring explicit best/worst choices

**GEMBA-MQM**
- *Why needed:* Attempts to automate literary translation quality assessment
- *Quick check:* Correlates with human judgments but struggles with nuanced literary qualities

**Professional vs. Student Evaluators**
- *Why needed:* Different expertise levels may capture different aspects of translation quality
- *Quick check:* Professionals provide more nuanced assessments but at higher cost

## Architecture Onboarding

**Component Map**
LITEVAL-CORPUS (source texts) -> MT systems (9 outputs) -> Human evaluators (MQM, SQM, BWS) -> Automatic metrics (GEMBA-MQM) -> Quality assessment

**Critical Path**
1. Source text selection and MT system generation
2. Human evaluation scheme implementation
3. Automatic metric computation
4. Correlation analysis between evaluation methods

**Design Tradeoffs**
The study prioritized comprehensive evaluation coverage over depth in any single language pair, choosing to include four language pairs rather than focusing intensively on one. This enables generalizability but may sacrifice some language-specific insights.

**Failure Signatures**
- Low inter-annotator agreement indicates unclear evaluation guidelines
- Metrics failing to distinguish human from machine translations suggest insufficient linguistic sophistication
- MQM's inadequacy for literary texts indicates need for different error taxonomies

**Three First Experiments**
1. Replicate MQM evaluation with enhanced guidelines specifically for literary texts
2. Test additional automatic metrics on a subset of the corpus
3. Conduct cross-language comparison of evaluation scheme performance

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How do different human evaluation schemes (MQM, SQM, BWS) perform when comparing translations from multiple language pairs simultaneously?
- Basis in paper: [explicit] The paper compares evaluation schemes but only examines individual language pairs separately
- Why unresolved: The study analyzed each language pair independently rather than conducting cross-language comparisons
- What evidence would resolve it: A systematic comparison of the same evaluation schemes across all four language pairs using identical text samples

### Open Question 2
- Question: Would the inclusion of professional translators as evaluators for all language pairs significantly change the findings about MQM's inadequacy?
- Basis in paper: [inferred] Professional translators only evaluated samples from top systems, not the full corpus
- Why unresolved: The study used student evaluators for the majority of MQM and SQM evaluations
- What evidence would resolve it: A complete re-evaluation of the entire corpus using professional translators for all schemes

### Open Question 3
- Question: How do current LLM-based automatic metrics perform when evaluated on literary texts from low-resource languages?
- Basis in paper: [explicit] The study only covered four high-resource language pairs
- Why unresolved: The paper explicitly states it did not explore LLM performance in low-resource languages
- What evidence would resolve it: Testing the same metrics on parallel corpora for low-resource language pairs

### Open Question 4
- Question: What specific improvements to the MQM guidelines would better capture literary translation quality?
- Basis in paper: [explicit] The authors identify MQM's limitations but do not propose specific improvements
- Why unresolved: The paper notes this limitation but acknowledges it requires extensive research
- What evidence would resolve it: Development and validation of an enhanced MQM framework through iterative testing with literary translators

## Limitations
- The study covers only four high-resource language pairs, limiting generalizability to other literary translation contexts
- The evaluation timeframe (March-May 2024) may not reflect current LLM capabilities given rapid advancements
- Reliance on Amazon Mechanical Turk for student evaluators introduces potential quality control concerns despite standardized guidelines

## Confidence
- Human translators outperform LLM translations (High confidence): Consistently supported across evaluation schemes and annotator types
- MQM is inadequate for literary translation (High confidence): Clear performance gaps demonstrated between MQM and alternative schemes
- LLMs produce more literal translations (Medium confidence): Supported by evidence but interpretation may vary based on translation goals
- GEMBA-MQM correlates moderately with human judgments (Medium confidence): Correlation observed but limitations in distinguishing human vs. machine translations noted

## Next Checks
1. Replicate the study with newer LLM versions released after May 2024 to assess temporal validity
2. Expand evaluation to additional language pairs, particularly those with different typological features
3. Conduct A/B testing with the same annotators comparing MQM and SQM evaluations on identical passages to quantify scheme differences