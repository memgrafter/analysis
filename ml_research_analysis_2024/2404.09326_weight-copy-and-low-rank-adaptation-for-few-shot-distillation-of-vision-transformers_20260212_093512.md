---
ver: rpa2
title: Weight Copy and Low-Rank Adaptation for Few-Shot Distillation of Vision Transformers
arxiv_id: '2404.09326'
source_url: https://arxiv.org/abs/2404.09326
tags:
- wecolora
- distillation
- teacher
- knowledge
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WeCoLoRA, a two-step approach for few-shot
  knowledge distillation of vision transformers. It first copies weights from intermittent
  layers of a pre-trained teacher transformer into a shallower student model, with
  the reduction ratio controlling the compression.
---

# Weight Copy and Low-Rank Adaptation for Few-Shot Distillation of Vision Transformers

## Quick Facts
- arXiv ID: 2404.09326
- Source URL: https://arxiv.org/abs/2404.09326
- Authors: Diana-Nicoleta Grigore; Mariana-Iuliana Georgescu; Jon Alvarez Justo; Tor Johansen; Andreea Iuliana Ionescu; Radu Tudor Ionescu
- Reference count: 40
- Primary result: WeCoLoRA achieves up to 69.2% accuracy on ImageNet-1K using only 10% of training data

## Executive Summary
This paper introduces WeCoLoRA, a two-step approach for few-shot knowledge distillation of vision transformers that combines weight copying from intermittent teacher layers with an enhanced version of Low-Rank Adaptation (LoRA). The method first copies weights from pre-trained teacher transformers into shallower student models at reduced frequency, then applies LoRA to all transformer components to recover information processing from skipped layers. Experiments demonstrate that WeCoLoRA consistently outperforms state-of-the-art methods across ImageNet-1K and five downstream tasks while using only 0.25-10% of training data, with ablation studies confirming the effectiveness of each component.

## Method Summary
WeCoLoRA is a two-step few-shot knowledge distillation method for vision transformers. First, it copies weights from intermittent layers of a pre-trained teacher transformer into a shallower student model, with the reduction ratio r controlling compression. Second, it employs an enhanced LoRA that applies low-rank adaptation to all transformer components (queries, keys, values, output projections, and both feed-forward network layers) to distill knowledge and recover the information processing of skipped teacher layers. The method uses L1 loss between teacher and student embeddings in latent space, requiring no training labels, and trains only the newly added LoRA parameters while freezing copied weights.

## Key Results
- Achieves 69.2% accuracy on ImageNet-1K with only 10% of training data
- Outperforms state-of-the-art methods including DeiT, MiniViT, DMAE, and DoRA across all tested scenarios
- Ablation studies confirm both weight copying and enhanced LoRA components contribute to performance gains
- Latent space visualizations demonstrate WeCoLoRA learns more robust and discriminative features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Copying weights from intermittent teacher layers preserves learned feature hierarchies in the student.
- Mechanism: Vision transformers have consistent depth-wise structure with compatible input/output dimensions across blocks, enabling direct weight transfer from teacher layers to corresponding student layers at reduced frequency.
- Core assumption: Teacher's learned representations are general enough that skipping layers does not degrade essential feature hierarchies.
- Evidence anchors:
  - [abstract] "Leveraging the fact that vision transformers have a consistent depth-wise structure, we first copy the weights from intermittent layers of existing pre-trained vision transformers (teachers) into shallower architectures (students)"
  - [section] "Leveraging the fact that vision transformers have a consistent depth-wise structure, i.e. the input and output dimensions are compatible across transformer blocks. This allows us to directly copy the weights from intermittent layers of existing pre-trained vision transformers (teachers) into shallower architectures (students)."
- Break condition: If teacher-student depth ratio is too high, the copied layers may miss critical intermediate representations.

### Mechanism 2
- Claim: Enhanced LoRA applied to all transformer components recovers the information processing of skipped teacher layers.
- Mechanism: Standard LoRA only adapts query and value projections, but WeCoLoRA extends LoRA to query, key, value, output projection, and both feed-forward network layers in each transformer block.
- Core assumption: Low-rank adaptation is sufficient to approximate the behavior of entire skipped transformer blocks.
- Evidence anchors:
  - [abstract] "Next, we employ an enhanced version of Low-Rank Adaptation (LoRA) to distill knowledge into the student in a few-shot scenario, aiming to recover the information processing carried out by the skipped teacher layers."
  - [section] "We conjecture that it is better to apply LoRA to each component of the transformer block...Enhanced LoRA is able to better complement the weight copying mechanism by enabling a more faithful replication of the skipped teacher blocks."
- Break condition: If the rank k is too small, the low-rank approximation cannot capture the complexity of skipped layers.

### Mechanism 3
- Claim: Knowledge distillation in latent space with few samples creates robust embeddings transferable to downstream tasks.
- Mechanism: Instead of using classification labels, WeCoLoRA minimizes the absolute difference between teacher and student embeddings on unlabeled images, creating discriminative feature distributions.
- Core assumption: Teacher embeddings contain sufficient information to guide student learning without explicit labels.
- Evidence anchors:
  - [abstract] "Our distillation method does not require training labels, therefore, the knowledge distillation data set is D = {I 1, I 2, .., I n}, where I i is an image sample and n is the number of samples in the data set. This is because the knowledge distillation procedure is performed in the latent space."
  - [section] "The goal of applying knowledge distillation is to transfer the knowledge of the pre-trained teacher T into the LoRA-enhanced student S...We minimize the absolute difference between the embedding ET i ∈ Rt×d returned by the teacher T for image I i, and the embedding ES i ∈ Rt×d returned by the student S for the same image I i"
- Break condition: If teacher embeddings are poorly aligned with downstream task requirements, the distilled features may not transfer well.

## Foundational Learning

- Concept: Vision Transformer architecture and attention mechanism
  - Why needed here: Understanding the transformer block structure is essential for knowing which weights can be copied and where LoRA can be applied
  - Quick check question: What are the main components of a transformer block and their dimensional relationships?

- Concept: Low-Rank Adaptation (LoRA) and parameter-efficient fine-tuning
  - Why needed here: LoRA is the core adaptation mechanism that recovers skipped layers; understanding its limitations with standard application is key
  - Quick check question: How does standard LoRA differ from the enhanced version used in WeCoLoRA?

- Concept: Knowledge distillation and feature matching
  - Why needed here: The method relies on transferring knowledge through embedding similarity rather than traditional classification supervision
  - Quick check question: What is the difference between knowledge distillation using logits vs. feature embeddings?

## Architecture Onboarding

- Component map: Teacher ViT -> Weight copy module -> Student ViT (r× fewer layers) -> Enhanced LoRA modules -> Distillation loss -> Trained student
- Critical path: Weight copy → Enhanced LoRA insertion → Feature distillation training → Integration of trained LoRA weights
- Design tradeoffs:
  - Compression ratio r: Higher r creates smaller, faster students but requires more recovery via LoRA
  - LoRA rank k: Higher k provides better recovery but increases parameter count and risk of overfitting
  - Number of training samples: Fewer samples require stronger weight copying and appropriate k selection
- Failure signatures:
  - Poor downstream performance despite good training loss: Indicates teacher embeddings may not align with downstream task
  - Training instability: Suggests k is too large for the available data
  - Student collapses to trivial representation: Indicates r is too high relative to data availability
- First 3 experiments:
  1. Baseline test: Apply standard LoRA (only Q,V projections) with weight copying to confirm the need for enhancement
  2. Rank sensitivity: Vary k ∈ {64, 128, 256, 512} to find optimal tradeoff for given data budget
  3. Compression sensitivity: Test r ∈ {2, 3, 4} to find maximum compression maintaining acceptable performance

## Open Questions the Paper Calls Out
None

## Limitations
- Data efficiency claim: While strong performance is reported with 0.25-10% of ImageNet data, the selection method for these subsets significantly impacts results but is not clearly standardized
- Generalizability to other architectures: The weight copying mechanism relies on consistent transformer block structure and may not transfer directly to architectures with variable or non-uniform block designs
- Scalability to very high compression ratios: The paper doesn't explore scenarios where r≥8, which would be more relevant for extreme compression needs

## Confidence

- **High confidence**: The core mechanism of weight copying from intermittent layers works as described, supported by ablation studies showing it improves over baseline LoRA.
- **Medium confidence**: The enhanced LoRA design (applying to all transformer components) provides measurable benefits, though the exact magnitude of improvement varies by task and dataset.
- **Medium confidence**: The few-shot knowledge distillation approach creates transferable features, but downstream performance is still lower than full fine-tuning baselines, suggesting limitations in extreme data scarcity scenarios.

## Next Checks

1. **Ablation on rank selection strategy**: Systematically evaluate different methods for choosing LoRA rank k (grid search, learning rate-based, validation-based) to determine if the reported k∈{128,256,512} are optimal or if better strategies exist for specific data budgets.

2. **Cross-architecture generalization test**: Apply WeCoLoRA to non-ViT architectures (e.g., ConvNeXt, Swin Transformer) to validate whether the weight copying mechanism depends on ViT-specific properties or generalizes to other consistent block-structured architectures.

3. **Extreme compression evaluation**: Test compression ratios r=8, 16, 32 to determine the practical limits of the approach and identify at what point the recovery via enhanced LoRA becomes insufficient for maintaining reasonable performance.