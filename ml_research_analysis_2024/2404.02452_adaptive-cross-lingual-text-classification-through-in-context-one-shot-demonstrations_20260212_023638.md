---
ver: rpa2
title: Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations
arxiv_id: '2404.02452'
source_url: https://arxiv.org/abs/2404.02452
tags:
- language
- shot
- target
- data
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces In-Context Cross-lingual Transfer (IC-XLT),
  a novel method that leverages In-Context Tuning (ICT) to train a model on a source
  language and adapt it to target languages at inference using one-shot demonstrations.
  The approach enables effective cross-lingual text classification without gradient
  updates, outperforming prompt-based fine-tuning methods.
---

# Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations

## Quick Facts
- **arXiv ID:** 2404.02452
- **Source URL:** https://arxiv.org/abs/2404.02452
- **Reference count:** 40
- **Key outcome:** IC-XLT achieves higher F1 micro scores than prompt-based fine-tuning across multiple datasets and languages using one-shot demonstrations

## Executive Summary
This paper introduces In-Context Cross-lingual Transfer (IC-XLT), a novel method that leverages In-Context Tuning (ICT) to train a model on a source language and adapt it to target languages at inference using one-shot demonstrations. The approach enables effective cross-lingual text classification without gradient updates, outperforming prompt-based fine-tuning methods. IC-XLT achieves higher performance and smaller transfer gaps, especially in scenarios with limited source-language data, and shows strong correlation between improvement and target language representation in pretraining corpora. Results demonstrate superior F1 micro scores across multiple datasets and languages compared to existing Zero-Shot and Few-Shot baselines.

## Method Summary
IC-XLT combines ICT with a cross-lingual transfer approach that uses one-shot demonstrations during inference. The method first trains a model on source language data using ICT, then adapts to target languages by presenting one example demonstration in the target language alongside the test instance. This allows the model to perform classification in the target language without any gradient updates or additional training. The approach leverages the model's pretraining corpus representation of target languages to improve transfer performance. The one-shot demonstration provides context that bridges the linguistic gap between source and target languages during inference.

## Key Results
- IC-XLT outperforms prompt-based fine-tuning methods in cross-lingual text classification
- Achieves higher F1 micro scores across multiple datasets and languages compared to Zero-Shot and Few-Shot baselines
- Shows strong correlation between target language representation in pretraining data and performance improvement
- Particularly effective in scenarios with limited source-language training data

## Why This Works (Mechanism)
The method works by leveraging the model's existing cross-lingual capabilities developed during pretraining. By providing one-shot demonstrations in the target language, IC-XLT activates the model's understanding of that language's patterns and structures. The combination of ICT training on the source language with in-context adaptation using target language examples allows the model to transfer knowledge effectively without requiring language-specific fine-tuning. The pretraining corpus representation of target languages plays a crucial role in determining transfer effectiveness.

## Foundational Learning
- **In-Context Learning (ICL):** The ability of language models to learn tasks from examples provided in the prompt context without parameter updates
  - *Why needed:* Enables adaptation to new tasks and languages without retraining
  - *Quick check:* Can the model perform new tasks given only a few examples in the prompt?
- **Cross-lingual Transfer:** Transferring knowledge from one language to another for NLP tasks
  - *Why needed:* Allows models to work in multiple languages without language-specific training
  - *Quick check:* Does performance on target language exceed random or baseline levels?
- **Pretraining Corpus Representation:** The extent to which a language is represented in the pretraining data
  - *Why needed:* Determines the model's prior knowledge about a language
  - *Quick check:* Is there correlation between pretraining data quantity and downstream performance?
- **One-Shot Learning:** Learning from a single example
  - *Why needed:* Minimizes data requirements for adaptation
  - *Quick check:* Does the model maintain reasonable performance with only one demonstration?
- **Prompt-Based Fine-Tuning:** Using prompts to adapt models for specific tasks
  - *Why needed:* Provides baseline for comparison
  - *Quick check:* How does IC-XLT compare to traditional prompt-based approaches?

## Architecture Onboarding

**Component Map:** Pretrained Model -> ICT Training (Source) -> One-Shot Demonstration (Target) -> Inference

**Critical Path:** The critical path involves pretraining the model, applying ICT on source language data, then using one-shot demonstrations during inference for target languages. Each step builds upon the previous one without requiring additional training.

**Design Tradeoffs:** The approach trades computational efficiency (no gradient updates needed) for potentially reduced performance compared to full fine-tuning. It requires only one demonstration per target language but relies heavily on the quality of pretraining corpus representation.

**Failure Signatures:** Poor performance may occur when target languages have minimal representation in pretraining data, when source and target languages are linguistically distant, or when the one-shot demonstration is not representative of the target language's patterns.

**First Experiments:**
1. Evaluate IC-XLT performance across different language families to test cross-lingual generalization
2. Vary the number of one-shot demonstrations to find optimal configuration
3. Test on tasks beyond text classification to assess broader applicability

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focused primarily on F1 micro scores, limiting assessment of other performance metrics
- Scope appears limited to specific language pairs and datasets
- Implementation details for one-shot demonstrations are not fully specified
- Reliance on pretraining corpus representation may limit effectiveness for low-resource languages

## Confidence
- **Core findings (High):** Systematic comparison with established baselines supports the effectiveness of IC-XLT
- **Generalizability (Medium):** Results are promising but limited to specific language pairs and datasets
- **Implementation details (Medium):** Methodological description is clear but some specifics are missing
- **Broader applicability (Medium):** Strong results in classification but unclear for other NLP tasks

## Next Checks
1. Evaluate IC-XLT across a broader range of language families and typological diversity
2. Conduct ablation studies varying the number and quality of one-shot demonstrations to establish optimal configurations
3. Test the approach on tasks beyond text classification to assess generalizability to other NLP problems