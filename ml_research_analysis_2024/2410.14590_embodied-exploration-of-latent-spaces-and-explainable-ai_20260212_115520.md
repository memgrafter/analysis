---
ver: rpa2
title: Embodied Exploration of Latent Spaces and Explainable AI
arxiv_id: '2410.14590'
source_url: https://arxiv.org/abs/2410.14590
tags:
- latent
- explainable
- performers
- space
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an embodied exploration of latent spaces in
  AI through a performance integrating e-textiles, live coding, and a neural audio
  synthesis model (RAVE). The authors developed wearable e-textile sensors that captured
  performers' movements, which were then used to navigate and explore the latent space
  of a VAE-based voice model trained on the performers' vocal input.
---

# Embodied Exploration of Latent Spaces and Explainable AI

## Quick Facts
- arXiv ID: 2410.14590
- Source URL: https://arxiv.org/abs/2410.14590
- Authors: Elizabeth Wilson; Mika Satomi; Alex McLean; Deva Schubert; Juan Felipe Amaya Gonzalez
- Reference count: 13
- Key outcome: Demonstrated embodied exploration of latent spaces through performance integrating e-textiles, live coding, and neural audio synthesis

## Executive Summary
This paper presents an innovative approach to explainable AI through embodied exploration of latent spaces in neural audio synthesis models. The authors developed a performance system combining e-textile sensors, live coding, and a RAVE neural audio model to enable performers to navigate high-dimensional latent spaces through physical movement. The work demonstrates how bodily expression can serve as an intuitive interface for understanding complex AI systems, revealing insights about the stochastic nature of VAE models through repeated physical gestures. The interdisciplinary approach contributes to both explainable AI research and artistic performance practices.

## Method Summary
The method involves training a RAVE (Real-Time Audio Variational AutoEncoder) model on performers' vocal input data, then using e-textile pressure sensors worn on garments to detect body movements and gestures. These sensor readings are processed through Pure Data with ml.lib externals for gesture classification and parameter reduction, then mapped to control parameters in the RAVE model's latent space. The system creates a feedback loop where physical movements directly influence audio synthesis outputs, enabling performers to explore and understand the latent space through embodied interaction.

## Key Results
- Physical movement through e-textile sensors provides intuitive mapping between bodily expression and latent space navigation
- The stochastic nature of VAE models creates unique opportunities for embodied exploration through repeated movements
- Integration of multiple artistic domains (e-textiles, live coding, performance art) creates multi-modal exploration environment enhancing understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Physical movement through e-textile sensors creates an intuitive mapping between bodily expression and latent space navigation
- Mechanism: Pressure sensors on garments detect body movements and gestures, mapped to RAVE model parameters, creating direct physical-to-auditory feedback loop
- Core assumption: Performers' muscle memory and physical intuition can effectively navigate high-dimensional latent spaces through embodied interaction
- Evidence anchors: Abstract and section quotes about performers' tactile dialogue with algorithms and bodily engagement being crucial for understanding
- Break condition: If mapping between physical movements and latent space parameters becomes too complex or non-intuitive

### Mechanism 2
- Claim: Stochastic nature of VAE models creates unique opportunities for embodied exploration through repeated movements
- Mechanism: VAE models generate different outputs each time they sample from latent space, even with same input, creating "dialogue" between body and AI
- Core assumption: Variation in outputs from repeated gestures is perceptible and meaningful to performers
- Evidence anchors: Section quotes about VAE stochasticity enabling connections between bodily expression and system learning
- Break condition: If stochastic variation is too subtle to perceive through auditory feedback or too chaotic to form meaningful patterns

### Mechanism 3
- Claim: Integration of multiple artistic domains creates multi-modal exploration environment that enhances understanding
- Mechanism: Combining physical sensors, real-time sound manipulation, and performative movement creates multiple simultaneous feedback channels
- Core assumption: Multiple simultaneous feedback channels improve spatial awareness and understanding in high-dimensional spaces
- Evidence anchors: Abstract about interdisciplinary collaboration and section describing the four-hour durational performance
- Break condition: If multiple modalities create cognitive overload rather than enhanced understanding

## Foundational Learning

- Concept: Variational Autoencoders (VAEs) and latent space representation
  - Why needed here: Understanding how RAVE encodes and decodes audio through compressed latent space is fundamental to grasping bodily navigation
  - Quick check question: How does a VAE differ from traditional autoencoder in terms of latent space representation, and why is this difference crucial for embodied exploration?

- Concept: E-textile sensor technology and signal processing
  - Why needed here: Pressure sensors and their ability to detect bend, stretch, and pressure changes are physical interface between body movement and AI system
  - Quick check question: What types of physical movements can pressure sensors effectively detect, and what are limitations of textile-based sensors for capturing complex gestures?

- Concept: Real-time audio synthesis and neural audio models
  - Why needed here: RAVE model's ability to generate high-quality audio in real-time while being controlled by external parameters is core of performance
  - Quick check question: What are key architectural differences between RAVE and other neural audio synthesis models, and how do these affect suitability for real-time embodied control?

## Architecture Onboarding

- Component map: E-textile sensor network → Bela mini board → Pure Data with ml.lib → MaxMSP → RAVE model ↔︎ Live coders (TidalCycles) → Performers
- Critical path: Sensor input → Signal processing → Parameter mapping → RAVE model control → Audio output → Performer feedback
- Design tradeoffs:
  - Sensor resolution vs. computational complexity: Higher resolution provides granular control but increases computational load
  - Gesture recognition vs. continuous control: ML-based classification provides discrete modes but may lose subtle variations
  - Model fidelity vs. real-time performance: Higher quality requires more resources, potentially limiting responsiveness
- Failure signatures:
  - Unresponsive audio output: Sensor-to-parameter mapping issues or communication failures
  - Noisy or unstable audio: Sensor noise, insufficient filtering, or unstable parameter mappings
  - Lack of intuitive control: Poor mapping design between movements and latent space parameters
  - System lag or dropouts: Computational bottlenecks in signal processing chain
- First 3 experiments:
  1. Test individual sensor-to-parameter mappings with single e-textile sensors connected to single RAVE parameters
  2. Test gesture recognition pipeline by recording and classifying specific gestures using ML system
  3. Test multi-modal integration combining sensor input, live coding control, and RAVE synthesis in simple configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can embodied interactions with AI systems be systematically evaluated for effectiveness in explaining latent space exploration compared to traditional visualization techniques?
- Basis in paper: Explicit demonstration that performers' bodily movements provided intuitive ways to understand high-dimensional latent spaces
- Why unresolved: Presents anecdotal evidence from performance but lacks systematic comparison with other explainable AI approaches
- What evidence would resolve it: Controlled studies comparing embodied exploration with traditional visualization methods using standardized comprehension metrics

### Open Question 2
- Question: What are specific characteristics of VAE models that make them particularly suitable for embodied exploration compared to other generative models?
- Basis in paper: Explicit highlighting that VAE models' stochastic nature enabled connections between bodily expression and system learning
- Why unresolved: Authors demonstrate effectiveness with VAEs but don't explore whether approach works equally well with other model types
- What evidence would resolve it: Comparative studies using identical embodied interfaces with different generative model architectures

### Open Question 3
- Question: How can insights gained from embodied exploration of latent spaces be translated into more conventional explainable AI interfaces for non-artistic applications?
- Basis in paper: Inferred suggestion that bodily expression helped performers understand system intricacies implies potential for broader application
- Why unresolved: Work focuses on artistic performance contexts without addressing how principles could be adapted for practical applications
- What evidence would resolve it: Development and testing of embodied exploration interfaces in non-artistic domains with measurable improvements in user understanding

## Limitations
- Subjective nature of embodied understanding makes objective measurement of intuitive comprehension difficult
- Stochastic variation in VAE outputs may be too subtle for reliable embodied detection without controlled studies
- Generalizability of approach to other AI systems or latent space domains remains unproven

## Confidence
- High: Technical implementation details and system architecture are well-specified and reproducible
- Medium: Claim that physical movement creates intuitive navigation of latent spaces is supported by authors' experience but lacks quantitative validation
- Medium: Explanation of VAE stochasticity through embodied exploration is conceptually sound but not empirically verified

## Next Checks
1. Conduct controlled experiments comparing embodied latent space exploration with traditional parameter visualization techniques to quantify learning outcomes
2. Perform systematic psychophysical studies to determine minimum perceptible variation in VAE outputs detectable through auditory feedback
3. Test system with novice users unfamiliar with AI systems and performance art to assess accessibility and intuitive understanding across expertise levels