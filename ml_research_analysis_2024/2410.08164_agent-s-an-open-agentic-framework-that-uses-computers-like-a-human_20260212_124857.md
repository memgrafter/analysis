---
ver: rpa2
title: 'Agent S: An Open Agentic Framework that Uses Computers Like a Human'
arxiv_id: '2410.08164'
source_url: https://arxiv.org/abs/2410.08164
tags:
- agent
- task
- memory
- subtask
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Agent S introduces an open agentic framework for autonomous GUI
  interaction, addressing challenges of domain knowledge acquisition, long-horizon
  planning, and dynamic interface handling. It employs experience-augmented hierarchical
  planning that leverages external web knowledge and internal memory (narrative and
  episodic) for task decomposition and execution.
---

# Agent S: An Open Agentic Framework that Uses Computers Like a Human

## Quick Facts
- **arXiv ID:** 2410.08164
- **Source URL:** https://arxiv.org/abs/2410.08164
- **Reference count:** 37
- **Key outcome:** Agent S achieves 20.58% success rate on OSWorld, an 83.6% relative improvement over baselines, and demonstrates strong generalization to Windows systems

## Executive Summary
Agent S introduces an open agentic framework for autonomous GUI interaction that addresses three key challenges: domain knowledge acquisition, long-horizon planning, and dynamic interface handling. The framework employs experience-augmented hierarchical planning that leverages external web knowledge and internal memory systems for task decomposition and execution. An Agent-Computer Interface (ACI) provides precise grounding and bounded action space for MLLM-based agents. Evaluated on OSWorld, Agent S achieves state-of-the-art performance with a 20.58% success rate, demonstrating strong generalizability to Windows systems.

## Method Summary
Agent S combines experience-augmented hierarchical planning with an Agent-Computer Interface to automate complex GUI-based desktop tasks. The framework uses a dual retrieval system where the Manager queries both web knowledge (via Perplexica search) and narrative memory to plan subtasks, while Workers execute using episodic memory and trajectory reflection. ACI provides precise grounding through a dual-input strategy (visual input + accessibility tree) and constrains actions to bounded language-based primitives. The system continually updates narrative and episodic memories through self-supervised exploration and self-evaluation, enabling improvement without human feedback.

## Key Results
- Achieves 20.58% success rate on OSWorld benchmark
- Demonstrates 83.6% relative improvement over baseline approaches
- Shows strong generalization to Windows systems with 18.2% success rate on WindowsAgentArena

## Why This Works (Mechanism)

### Mechanism 1: Experience-augmented hierarchical planning
- **Claim:** Hierarchical planning with web knowledge and memory retrieval enables effective task decomposition
- **Mechanism:** Manager fuses web knowledge and narrative memory for subtask planning; Workers retrieve specific experiences from episodic memory
- **Core assumption:** External web knowledge provides up-to-date information while internal memories contain relevant past experiences
- **Evidence anchors:** Abstract mentions experience-augmented hierarchical planning; corpus section describes fused knowledge usage
- **Break condition:** When web knowledge becomes outdated or memory retrieval fails

### Mechanism 2: Agent-Computer Interface (ACI) with bounded action space
- **Claim:** ACI provides precise grounding and bounded action space for MLLMs
- **Mechanism:** Dual-input strategy with visual input and accessibility tree; bounded discrete space of language-based primitives
- **Core assumption:** MLLMs excel at language-based reasoning with structured, bounded action spaces
- **Evidence anchors:** Abstract describes ACI's role; corpus discusses bounded action space benefits
- **Break condition:** When accessibility tree becomes incomplete or OCR augmentation fails

### Mechanism 3: Self-supervised memory update
- **Claim:** Continual memory update enables improvement without human feedback
- **Mechanism:** Self-supervised exploration bootstraps memory; self-evaluator generates textual rewards for experience summarization
- **Core assumption:** Agents can learn from own experiences and generate meaningful summaries
- **Evidence anchors:** Abstract mentions continual improvement; corpus describes self-evaluation process
- **Break condition:** When self-evaluator generates poor quality summaries

## Foundational Learning

- **Concept:** Hierarchical planning decomposition
  - **Why needed here:** Complex GUI tasks require breaking down into sequential subtasks with dependencies
  - **Quick check question:** Can you explain why a flat planning approach would struggle with a task like "Create a spreadsheet with total sales, average monthly sales, and generate visualizations"?

- **Concept:** Memory retrieval and fusion
  - **Why needed here:** Agent needs to combine external knowledge with past experiences for informed decisions
  - **Quick check question:** How does fusing web knowledge with narrative memory differ from just using one source, and why is this beneficial for GUI task planning?

- **Concept:** Bounded action spaces for language models
  - **Why needed here:** Unbounded action spaces lead to safety issues and imprecise feedback
  - **Quick check question:** Why is allowing MLLMs to generate arbitrary code problematic for GUI automation, and how does constraining to primitives like click(element_id) solve this?

## Architecture Onboarding

- **Component map:** User task → Manager planning → Worker execution → ACI interaction → Self-evaluation → Memory update
- **Critical path:** User task → Manager planning → Worker execution → ACI interaction → Self-evaluation → Memory update
- **Design tradeoffs:**
  - Memory vs. performance: Larger memories improve retrieval quality but increase latency
  - Web knowledge vs. autonomy: External knowledge reduces memory needs but introduces dependency
  - Action granularity vs. efficiency: Finer-grained actions provide more control but increase complexity
- **Failure signatures:**
  - Planning failures: Incorrect subtask sequences or missing subtasks
  - Grounding failures: Repeated attempts to interact with wrong elements
  - Execution failures: Repetitive actions without adaptation or task divergence
  - Memory failures: Inability to retrieve relevant experiences or poor quality summaries
- **First 3 experiments:**
  1. Ablation of web knowledge retrieval to quantify its contribution
  2. Memory bootstrapping test with self-supervised exploration on synthetic tasks
  3. ACI grounding precision test comparing OCR augmentation enabled vs. disabled

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the self-supervised exploration task generation process affect the quality and diversity of initial memory construction?
- **Basis in paper:** [explicit] Paper describes two methods for creating exploration tasks: environment-independent and environment-aware
- **Why unresolved:** Paper mentions methods but doesn't analyze their relative effectiveness
- **What evidence would resolve it:** Comparative analysis of memory quality and downstream performance using different exploration strategies

### Open Question 2
- **Question:** What is the optimal balance between initial exploration data versus continual learning updates?
- **Basis in paper:** [inferred] Paper describes both self-supervised exploration and continual memory update
- **Why unresolved:** Paper shows both components are important but doesn't quantify optimal ratio
- **What evidence would resolve it:** Systematic experiments varying amounts of initial exploration data vs. continual updates

### Open Question 3
- **Question:** How does LLM backbone choice interact with experiential learning components?
- **Basis in paper:** [explicit] Paper uses GPT-4o and Claude-3.5-Sonnet and mentions future work with smaller models
- **Why unresolved:** Paper demonstrates framework with large models but doesn't explore smaller model adaptations
- **What evidence would resolve it:** Experiments training smaller open-source models with experiential learning framework

## Limitations

- **Critical dependency on external web knowledge:** Performance may be limited in domains with sparse online documentation or rapidly evolving interfaces
- **Small evaluation set:** OSWorld test set contains only 65 tasks, which may not fully capture generalizability to real-world scenarios
- **Unverified long-term effectiveness:** Self-supervised memory update's long-term effectiveness and quality of experience summaries are not empirically validated beyond initial bootstrapping

## Confidence

- **High Confidence:** ACI's bounded action space design and superiority over unbounded approaches
- **Medium Confidence:** Hierarchical planning mechanism's effectiveness in decomposing complex tasks
- **Low Confidence:** Self-supervised memory update's long-term effectiveness and quality of experience summaries

## Next Checks

1. **Web Knowledge Dependency Test:** Conduct ablation study where web knowledge retrieval is disabled to quantify its exact contribution to the 83.6% relative improvement
2. **Memory Quality Assessment:** Implement human evaluation protocol to assess quality of experience summaries stored in narrative and episodic memory
3. **Real-world Generalization Test:** Evaluate Agent S on larger, more diverse set of real-world GUI tasks beyond OSWorld benchmark