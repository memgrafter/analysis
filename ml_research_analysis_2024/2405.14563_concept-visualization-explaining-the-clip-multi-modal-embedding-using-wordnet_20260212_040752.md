---
ver: rpa2
title: 'Concept Visualization: Explaining the CLIP Multi-modal Embedding Using WordNet'
arxiv_id: '2405.14563'
source_url: https://arxiv.org/abs/2405.14563
tags:
- saliency
- image
- clip
- concept
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of explaining CLIP embeddings,
  which are widely used in computer vision but lack interpretability. The proposed
  Concept Visualization (ConVis) method generates saliency maps for any semantic concept
  using WordNet lexical definitions and CLIP's multi-modal embeddings.
---

# Concept Visualization: Explaining the CLIP Multi-modal Embedding Using WordNet

## Quick Facts
- arXiv ID: 2405.14563
- Source URL: https://arxiv.org/abs/2405.14563
- Reference count: 33
- One-line primary result: ConVis generates saliency maps using WordNet definitions and CLIP embeddings, achieving AUROC > 0.995 on OOD detection and enabling users to identify COCO captions 76.4% of the time

## Executive Summary
This paper introduces Concept Visualization (ConVis), a method to explain CLIP's multi-modal embeddings by generating saliency maps for semantic concepts sourced from WordNet. The approach uses WordNet synset definitions as semantic anchors, embedding them through CLIP's text encoder and comparing them to image patch embeddings via cosine similarity. ConVis provides task-agnostic explanations that reveal how CLIP perceives semantic relationships between concepts. The method is evaluated across three scenarios: out-of-distribution detection, weakly-supervised object localization, and user comprehension, demonstrating state-of-the-art performance in interpretability while maintaining practical applicability.

## Method Summary
ConVis generates saliency maps by embedding WordNet synset definitions through CLIP's text encoder and comparing them to multi-scale image patch embeddings from CLIP's image encoder. For each pixel location, small and large square patches are extracted and embedded, then averaged to account for objects of different sizes. The rank similarity function measures semantic alignment between image regions and WordNet concepts, aggregating scores across the "is a" relationship hierarchy. This produces task-agnostic saliency maps that highlight how CLIP relates images to semantic concepts, enabling explanations for any concept defined in WordNet without requiring task-specific supervision.

## Key Results
- ConVis achieves AUROC > 0.995 on ImageNet-1k OOD detection, significantly outperforming existing saliency methods
- Weakly-supervised object localization performance reaches MaxBoxAcc of 76.2%, competitive with specialized WSOL methods
- User study shows participants correctly identify COCO image captions from ConVis explanations 76.4% of the time, significantly above random chance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WordNet definitions embedded through CLIP's text encoder produce similarity scores that reliably reflect semantic relationships between visual concepts and synsets.
- Mechanism: The definition of a synset is embedded via CLIP's text encoder, and this embedding is compared via cosine similarity to image patch embeddings. The rank similarity function measures relative similarity across all synsets, making it robust to varying visual detail levels.
- Core assumption: CLIP's joint text-image training creates embeddings where semantically related concepts (including textual definitions of synsets) are close in embedding space.
- Evidence anchors:
  - [abstract]: "We devise a similarity metric between images and semantic concepts sourced from the WordNet lexical database"
  - [section]: "the definition d of a synset s entails its semantics, and as such its embedding ˜d = E T (def(s)) should be similar to the embedding ˜x = E I(x) of an image x that visually displays that concept"
- Break condition: If CLIP's text and image encoders learn incompatible semantic spaces, or if WordNet definitions are too abstract to align with visual concepts, the similarity metric would fail.

### Mechanism 2
- Claim: Aggregating rank similarity over all synsets in a "is a" relationship with the target synset produces more robust similarity scores than considering single synsets.
- Mechanism: For a target synset, the method computes max rank similarity over all its "is a" descendants in WordNet, capturing broader semantic relationships.
- Core assumption: Semantic relationships in WordNet accurately reflect how CLIP perceives concept relationships.
- Evidence anchors:
  - [abstract]: "ConVis makes use of lexical information from WordNet to compute task-agnostic Saliency Maps for any concept"
  - [section]: "Given a concept to be explained, however, the Saliency Map should highlight similarities between the image and any synset that is in a 'is a' relationship with the synset that describes the concept"
- Break condition: If WordNet's "is a" relationships don't match CLIP's learned semantic relationships, or if the hierarchy is too sparse, aggregation would not improve robustness.

### Mechanism 3
- Claim: Multi-scale patch embeddings capture objects of different sizes effectively, improving saliency map quality.
- Mechanism: For each pixel location, small and large square patches are embedded and averaged to create a representative embedding that accounts for objects at different scales.
- Core assumption: CLIP's image encoder can effectively embed patches at multiple scales while maintaining semantic information.
- Evidence anchors:
  - [section]: "To account for objects of different sizes, for each pixel location [i, j] we crop small and large square patches Sx(i, j) and Lx(i, j)"
  - [section]: "We chose this configuration to obtain patches covering enough area for CLIP to perform well, but small enough that most visual concepts could be recognized"
- Break condition: If CLIP's performance degrades significantly with smaller patches, or if the averaging operation loses critical semantic information, the multi-scale approach would fail.

## Foundational Learning

- Concept: CLIP's contrastive learning paradigm
  - Why needed here: Understanding how CLIP learns to align text and image embeddings in a shared space is crucial for grasping why WordNet definitions can serve as semantic anchors for image concepts.
  - Quick check question: What training objective does CLIP use to align text and image embeddings, and how does this enable semantic similarity between definitions and visual concepts?

- Concept: WordNet lexical database structure
  - Why needed here: The hierarchical organization of synsets and their semantic relationships ("is a" links) forms the foundation for expanding similarity scores beyond single concepts.
  - Quick check question: How does WordNet's "is a" relationship structure enable the aggregation of similarity scores across related concepts?

- Concept: Saliency map computation and evaluation
  - Why needed here: Understanding how saliency maps are generated and evaluated (e.g., via weakly supervised object localization) is essential for interpreting the experimental results.
  - Quick check question: What metrics are commonly used to evaluate saliency map quality, and how do they relate to the ability to localize objects without bounding box supervision?

## Architecture Onboarding

- Component map: WordNet synset filtering -> CLIP image encoder -> CLIP text encoder -> Patch extraction and multi-scale embedding -> Rank similarity computation -> Spatial aggregation for saliency map -> OOD detection/WSOL evaluation/User study

- Critical path:
  1. Filter WordNet to obtain relevant synsets
  2. Extract multi-scale patches from input image
  3. Embed patches using CLIP image encoder
  4. Embed WordNet definitions using CLIP text encoder
  5. Compute rank similarity scores for each patch
  6. Aggregate scores spatially to form saliency map
  7. Evaluate via OOD detection, WSOL, or user study

- Design tradeoffs:
  - Patch size vs. semantic recognition: Larger patches may lose small object details, while smaller patches may not capture enough context for CLIP to perform well
  - WordNet filtering: Limiting to physical objects reduces computational load but may miss abstract concepts
  - Multi-scale vs. single scale: Multi-scale captures different object sizes but increases computation time
  - Pre-computed embeddings: Pre-computing WordNet definition embeddings speeds up computation but requires storage

- Failure signatures:
  - Poor OOD detection performance: Indicates WordNet definitions don't align well with CLIP's semantic space
  - Low WSOL accuracy: Suggests saliency maps don't effectively highlight relevant image regions
  - User study failure: Implies explanations don't convey CLIP's perception clearly to humans
  - High computation time: May indicate inefficient patch extraction or similarity computation

- First 3 experiments:
  1. Verify CLIP's text and image encoders produce semantically meaningful embeddings by testing similarity between definitions and corresponding images
  2. Validate rank similarity aggregation by comparing performance with and without "is a" relationship expansion on a small subset of synsets
  3. Test multi-scale embedding by evaluating saliency map quality with different patch size combinations on a small image set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal patch size configuration for ConVis across different datasets and domains?
- Basis in paper: [explicit] The authors note "We chose this configuration to obtain patches covering enough area for CLIP to perform well, but small enough that most visual concepts could be recognized" and hypothesize "the optimal patch size may depend on the dataset"
- Why unresolved: The authors did not perform systematic hyperparameter tuning or ablation studies on patch size, instead using a fixed configuration (δS = 64, δL = 128, ω = 16) without optimization.
- What evidence would resolve it: Systematic experiments varying patch sizes across multiple datasets with different object scales, image resolutions, and semantic content to determine optimal configurations.

### Open Question 2
- Question: How does ConVis performance compare when using alternative knowledge graphs beyond WordNet?
- Basis in paper: [explicit] The authors state "Future developments will explore the use of WordNet elements other than synset definitions, as well as different knowledge graphs"
- Why unresolved: The current work is limited to WordNet, with only future plans mentioned for exploring alternatives.
- What evidence would resolve it: Comparative experiments using ConVis with WordNet versus other knowledge graphs (e.g., ConceptNet, DBpedia) on the same tasks and datasets.

### Open Question 3
- Question: Can ConVis be effectively extended to explain abstract concepts rather than just physical objects?
- Basis in paper: [explicit] The authors mention "Since our analysis was limited to physical objects, in the future we may explore Saliency Maps focused towards more abstract concepts"
- Why unresolved: The current work filters WordNet to only include physical objects, explicitly excluding abstract concepts.
- What evidence would resolve it: Experiments applying ConVis to explain abstract concepts (e.g., emotions, actions, relationships) and measuring performance through user studies or downstream tasks.

### Open Question 4
- Question: How does ConVis compare to post-hoc explanations of other VLP models like BLIP?
- Basis in paper: [explicit] The authors state "Furthermore, we plan to apply ConVis to other VLP frameworks such as BLIP"
- Why unresolved: The current work only evaluates ConVis on CLIP, with BLIP mentioned only as a future direction.
- What evidence would resolve it: Direct comparison of ConVis explanations versus explanations of BLIP on identical tasks and datasets, measuring both quantitative metrics and user comprehension.

## Limitations
- The method relies on the assumption that WordNet definitions semantically align with CLIP's embedding space, which is not explicitly validated
- Performance on abstract concepts is limited since the WordNet filtering focuses on physical objects
- The computational cost of embedding thousands of WordNet definitions for each image may limit real-time applications

## Confidence
**High Confidence Claims:**
- ConVis significantly outperforms existing saliency methods on OOD detection (AUROC > 0.995)
- The user study demonstrates ConVis's ability to communicate semantic concepts effectively (76.4% accuracy vs. random chance)
- ConVis provides task-agnostic explanations that work across different evaluation scenarios

**Medium Confidence Claims:**
- ConVis achieves competitive WSOL performance (MaxBoxAcc 76.2%)
- The multi-scale patch approach improves saliency map quality
- WordNet hierarchy aggregation provides more robust similarity scores

**Low Confidence Claims:**
- The semantic alignment between WordNet definitions and CLIP embeddings is optimal
- The chosen patch sizes are ideal for all image types
- The method would perform equally well on abstract concepts

## Next Checks
1. **Semantic Alignment Validation**: Conduct controlled experiments comparing similarity scores between WordNet definitions and their corresponding images versus unrelated definitions to quantify the semantic alignment assumption.

2. **Patch Size Sensitivity Analysis**: Systematically evaluate saliency map quality across a range of patch sizes and strides to identify optimal configurations for different object scales and image resolutions.

3. **Abstract Concept Performance**: Remove the physical object filter from WordNet and evaluate ConVis's ability to generate meaningful explanations for abstract concepts, measuring performance degradation compared to the physical object subset.