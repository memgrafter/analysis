---
ver: rpa2
title: Learning to Route for Dynamic Adapter Composition in Continual Learning with
  Language Models
arxiv_id: '2408.09053'
source_url: https://arxiv.org/abs/2408.09053
tags:
- learning
- task
- adapters
- modules
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces L2R, a continual learning method that trains
  parameter-efficient adapters in isolation and learns a routing function to dynamically
  compose them. L2R addresses interference during adapter training and suboptimal
  routing composition seen in existing methods.
---

# Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models

## Quick Facts
- arXiv ID: 2408.09053
- Source URL: https://arxiv.org/abs/2408.09053
- Reference count: 30
- L2R-wavg achieves up to 78% accuracy in class-incremental learning and matches upper-bound performance in task-incremental learning

## Executive Summary
This paper introduces L2R, a continual learning method that trains parameter-efficient adapters in isolation and learns a routing function to dynamically compose them. L2R addresses interference during adapter training and suboptimal routing composition seen in existing methods. The approach uses a memory of past examples to train a router network that selects appropriate adapters via Gumbel-sigmoid scores, supporting two composition strategies: weighted averaging of outputs (L2R-wavg) or merging adapter parameters (L2R-merge). Experiments on three benchmarks (MTL5, WOS, AfriSenti) across class- and task-incremental settings show L2R-wavg achieves up to 78% accuracy in CIL and matches upper-bound performance in TIL, outperforming baselines like DAM, EPI, and MoCL.

## Method Summary
L2R uses a sequential training approach where task-specific adapters are trained in isolation to prevent interference from previously learned tasks. After all adapters are trained, a router network is trained using a small memory containing examples from all tasks. The router learns to select appropriate adapters based on input characteristics, producing allocation vectors that guide adapter composition. Two composition strategies are supported: L2R-wavg, which uses weighted average of outputs, and L2R-merge, which merges adapter parameters. The router uses Gumbel-sigmoid activation to enable task-level modularity, allowing multiple adapters to be active simultaneously based on input structure.

## Key Results
- L2R-wavg achieves up to 78% accuracy in class-incremental learning on MTL5 benchmark
- Router performance improves with larger memory sizes, particularly on smaller datasets like WOS
- L2R-wavg outperforms L2R-merge due to interference issues when merging multiple adapter parameters
- Gumbel-sigmoid routing consistently outperforms softmax activation in the experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Isolating adapter training prevents interference from previously learned tasks, allowing each adapter to specialize for its target task.
- Mechanism: During training of a new adapter At, all previously trained adapters are deactivated. Only the current adapter At is activated and trained on the new task t while the backbone remains frozen.
- Core assumption: Task-specific adapters can learn specialized knowledge without being affected by frozen adapters from previous tasks.
- Evidence anchors:
  - [abstract]: "L2R is a method that isolates the training of new PEFT modules to avoid interference from previous knowledge when learning new tasks"
  - [section]: "Previous adapters are deactivated, allowing At to learn in isolation and specialize in task t"
  - [corpus]: No direct evidence found in corpus; method differs from replay-based approaches.
- Break condition: If adapters require interaction with previous knowledge during training to achieve optimal performance, isolation would harm learning.

### Mechanism 2
- Claim: Memory-based router learning enables dynamic composition of adapters during inference by learning routing functions from stored examples.
- Mechanism: After all adapters are trained, a router network is trained using a small memory of past examples. The router learns to select appropriate adapters based on input structure, producing allocation vectors that guide adapter composition.
- Core assumption: A small memory containing representative examples from all tasks is sufficient for the router to learn effective routing functions.
- Evidence anchors:
  - [abstract]: "L2R then learns to compose the learned modules by training a network of routers that leverages a small memory containing examples of previously seen tasks"
  - [section]: "we leverage the elements in memory to train the router network parameters... performs this process only once after the training phase to learn the routing functions"
  - [corpus]: No direct evidence found in corpus; method differs from rehearsal-based approaches.
- Break condition: If the memory is too small or unrepresentative, the router cannot learn effective routing functions for all tasks.

### Mechanism 3
- Claim: Gumbel-sigmoid distribution enables better task-level modularity compared to softmax by allowing multiple adapters to be active simultaneously based on input structure.
- Mechanism: Instead of softmax activation where modules compete, the router uses Gumbel-sigmoid to produce binary scalars indicating adapter activation. This emphasizes task-level modularity while respecting task hierarchy.
- Core assumption: Task-level modularity is more effective than activation competency for composing adapters in continual learning scenarios.
- Evidence anchors:
  - [section]: "we adopt an approach inspired by Ponti et al. (2023), emphasizing task-level modularity that respects task hierarchy, with more complex tasks encompassing simpler ones as sub-tasks"
  - [section]: "Table 2 shows the results, highlighting that using Gumbel-sigmoid during the training of the routing function consistently outperforms the use of Softmax activation"
  - [corpus]: No direct evidence found in corpus; method differs from standard routing approaches.
- Break condition: If softmax or other activation functions provide better performance for specific task distributions or when tasks are highly dissimilar.

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: L2R builds upon PEFT methods like adapters to achieve continual learning without full fine-tuning of the entire model
  - Quick check question: What is the key advantage of PEFT methods over full fine-tuning in terms of parameter count and training efficiency?

- Concept: Catastrophic forgetting
  - Why needed here: L2R addresses catastrophic forgetting by isolating adapter training and using memory for router learning
  - Quick check question: How does isolating adapter training help prevent catastrophic forgetting compared to traditional fine-tuning approaches?

- Concept: Mixture-of-experts (MoE) routing
  - Why needed here: L2R uses router networks inspired by MoE models to dynamically compose adapters based on input characteristics
  - Quick check question: What is the key difference between the routing mechanism in L2R and traditional MoE models?

## Architecture Onboarding

- Component map:
  Backbone PLM (frozen) -> Task-specific adapters (one per task, trained in isolation) -> Memory (stores examples from all tasks) -> Router network (learns to compose adapters using memory) -> Two composition strategies: L2R-wavg (weighted average of outputs) or L2R-merge (merged adapter parameters)

- Critical path:
  1. Sequential adapter training with isolation
  2. Memory population with examples from all tasks
  3. Router network training using memory
  4. Inference with dynamic adapter composition

- Design tradeoffs:
  - Isolation vs. joint training: Isolation prevents interference but may miss beneficial knowledge transfer
  - Memory size vs. performance: Larger memory improves router learning but increases storage requirements
  - Gumbel-sigmoid vs. softmax: Gumbel-sigmoid enables multi-adapter activation but may be less decisive than softmax

- Failure signatures:
  - Poor router performance when memory is too small or unrepresentative
  - Interference issues if isolation strategy is not properly implemented
  - Suboptimal compositions when Gumbel-sigmoid distribution doesn't match task characteristics

- First 3 experiments:
  1. Verify adapter isolation: Train two adapters sequentially and measure performance degradation compared to joint training
  2. Memory impact: Train router with varying memory sizes (1%, 10%, 30%) and measure composition quality
  3. Activation function comparison: Compare Gumbel-sigmoid vs softmax routing performance on a small benchmark task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does L2R's performance scale with different memory population strategies beyond random sampling, such as exemplar selection or gradient-based methods?
- Basis in paper: [inferred] The paper mentions that "more advanced memory population methods could enhance router performance" and suggests exploring this for future work.
- Why unresolved: The paper only evaluates random sampling for memory population, leaving the impact of other strategies unexplored.
- What evidence would resolve it: Comparative experiments showing L2R's performance with different memory population strategies (e.g., herding, coreset selection, or gradient-based sampling) across multiple benchmarks.

### Open Question 2
- Question: Does the routing function learned by L2R generalize effectively to completely unseen tasks, or does it require task-specific fine-tuning for new domains?
- Basis in paper: [explicit] The paper states that "L2R enables inference on any example without further adaptation" after router training, but doesn't test this claim with truly novel tasks.
- Why unresolved: The experiments only evaluate performance on seen tasks within the benchmark streams, not on genuinely novel tasks introduced after the router is trained.
- What evidence would resolve it: Experiments testing L2R's performance when routing to adapters for completely new task types not present during router training.

### Open Question 3
- Question: What is the theoretical relationship between memory size, adapter diversity, and optimal router performance, and can this relationship be used to predict required memory for new tasks?
- Basis in paper: [explicit] The paper shows that "L2R-wavg performance increases with larger memory sizes" and cites prior work showing "as the number of examples increases, the routing function reaches its optimal performance."
- Why unresolved: While the paper demonstrates a positive correlation between memory size and performance, it doesn't establish a predictive model or theoretical framework for determining optimal memory requirements.
- What evidence would resolve it: Mathematical analysis or empirical studies establishing how memory size requirements scale with the number of tasks, adapter complexity, and task similarity metrics.

## Limitations
- The approach's reliance on memory-based router learning introduces uncertainty about scalability to larger task sets, as memory requirements grow linearly with tasks.
- The method's effectiveness depends heavily on the assumption that task-level modularity (Gumbel-sigmoid routing) is superior to activation competency, which may not hold for highly overlapping or dissimilar tasks.
- The performance gap between L2R-wavg and L2R-merge suggests that parameter merging introduces interference that could limit the approach's applicability in memory-constrained scenarios.

## Confidence
**High Confidence**: The core mechanism of isolating adapter training to prevent interference is well-supported by the experimental results, showing consistent improvements over baselines across all three benchmarks.

**Medium Confidence**: The superiority of Gumbel-sigmoid over softmax routing is demonstrated but limited to the specific task distributions tested. The assumption that task-level modularity is generally preferable to activation competency requires further validation across diverse task sets.

**Low Confidence**: The method's scalability to scenarios with many tasks (>10) and its robustness to highly overlapping task distributions remain largely unexplored. The computational efficiency claims are based on a single comparison with MoCL and need broader benchmarking.

## Next Checks
1. **Memory Scaling Test**: Systematically evaluate router performance across memory sizes from 1% to 50% of training data to identify the optimal memory-to-task ratio and determine when memory becomes a bottleneck for performance.

2. **Task Overlap Sensitivity**: Design experiments with varying degrees of task overlap (from completely disjoint to highly similar tasks) to test the robustness of the Gumbel-sigmoid routing mechanism and identify failure modes when task hierarchy assumptions break down.

3. **Multi-task Generalization**: Test the router's ability to generalize to unseen tasks by holding out task examples during router training and measuring composition quality on these novel inputs, assessing whether the learned routing functions transfer beyond the memory distribution.