---
ver: rpa2
title: Explaining Learned Reward Functions with Counterfactual Trajectories
arxiv_id: '2402.04856'
source_url: https://arxiv.org/abs/2402.04856
tags:
- ctes
- reward
- quality
- criteria
- counterfactual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using counterfactual trajectory explanations
  (CTEs) to interpret learned reward functions in reinforcement learning. CTEs contrast
  an original trajectory with a counterfactual one and the rewards each receives,
  helping users infer which behaviors the reward function incentivizes.
---

# Explaining Learned Reward Functions with Counterfactual Trajectories

## Quick Facts
- arXiv ID: 2402.04856
- Source URL: https://arxiv.org/abs/2402.04856
- Authors: Jan Wehner; Frans Oliehoek; Luciano Cavalcante Siebert
- Reference count: 40
- Primary result: Counterfactual trajectory explanations (CTEs) effectively help proxy-human models learn reward functions, with MCTO algorithm outperforming baselines

## Executive Summary
This paper introduces counterfactual trajectory explanations (CTEs) as a method to interpret learned reward functions in reinforcement learning. The approach contrasts an original trajectory with a counterfactual one, showing how different actions would yield different rewards. Six quality criteria for CTEs are established, and two algorithms (Monte Carlo-based Trajectory Optimization and Deviate-and-Continue) are proposed to generate CTEs that optimize these criteria. The authors evaluate CTE informativeness using a proxy-human model that learns from CTEs to predict rewards, with higher correlation indicating greater informativeness.

## Method Summary
The method generates counterfactual trajectories that explain what changes to a given trajectory would maximize reward differences. Two algorithms are introduced: Monte Carlo-based Trajectory Optimization (MCTO) which optimizes CTE quality criteria directly, and Deviate-and-Continue which modifies trajectories through action deviations. The approach evaluates CTE informativeness by training a proxy-human model to predict rewards from CTEs, measuring correlation between predicted and actual rewards. The six quality criteria include validity (ensuring counterfactuals are possible), proximity (keeping changes minimal), realisticness, diversity, state importance, and plausibility.

## Key Results
- CTEs are informative for proxy-human model learning, with MCTO outperforming baseline methods
- Validity is the most important quality criterion for CTE informativeness
- Proximity, realisticness, diversity, and state importance also contribute to effective explanations
- CTEs show promise for interpreting learned reward functions but don't enable perfect understanding

## Why This Works (Mechanism)
The approach works by providing contrastive explanations that highlight which behaviors the reward function incentivizes. By showing how small changes to trajectories would affect rewards, users can infer the reward structure through comparison. The quality criteria ensure that counterfactuals are meaningful, achievable, and diverse enough to capture different aspects of the reward function.

## Foundational Learning
- Reinforcement Learning basics - Why needed: Understanding how agents learn from rewards; Quick check: Can explain policy optimization
- Counterfactual reasoning - Why needed: Core concept for generating alternative scenarios; Quick check: Can describe what-if analysis
- XAI (Explainable AI) methods - Why needed: Provides context for interpreting learned models; Quick check: Can list common XAI techniques
- Trajectory optimization - Why needed: Essential for generating counterfactual paths; Quick check: Can explain trajectory planning
- Reward function interpretation - Why needed: Main goal of the approach; Quick check: Can describe reward hacking and specification problems

## Architecture Onboarding

Component map:
Environment Dynamics -> Trajectory Generator -> CTE Quality Evaluator -> Proxy-Human Model

Critical path:
1. Environment generates original trajectory
2. CTE algorithm generates counterfactual trajectory
3. Quality evaluator scores CTE
4. Proxy-human model trains on CTEs
5. Model predicts rewards for new trajectories

Design tradeoffs:
- Complexity vs. interpretability: More sophisticated CTE generation vs. understandable explanations
- Quality criteria balance: Optimizing for multiple criteria vs. focusing on key aspects
- Computational cost: MCTO's optimization vs. Deviate-and-Continue's simplicity

Failure signatures:
- Low correlation between predicted and actual rewards indicates poor CTE quality
- Unrealistic counterfactuals suggest problems with validity constraints
- Over-optimized CTEs that violate proximity criteria

First experiments:
1. Test CTE generation on simple grid-world environments
2. Evaluate proxy-human model performance with varying numbers of CTEs
3. Compare MCTO vs Deviate-and-Continue on different quality criteria

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but raises implicit ones about the scalability of the approach to complex environments and the validity of using proxy-human models to evaluate human interpretability.

## Limitations
- Reliance on proxy-human model may not accurately reflect human understanding
- Tested only on four relatively simple environments, limiting scalability assessment
- Evaluation focuses on reward prediction rather than true understanding of reward structure

## Confidence

**High**
- CTE informativeness for proxy-human model

**Medium**
- MCTO algorithm performance
- Quality criteria importance ranking

**Low**
- Generalizability to complex environments
- Human interpretability of CTEs

## Next Checks
1. Conduct user studies with human participants to directly evaluate their ability to infer reward functions from CTEs, rather than relying solely on proxy models
2. Test the approach on more complex, high-dimensional environments to assess scalability and robustness
3. Investigate alternative evaluation metrics beyond reward prediction accuracy, such as measuring users' ability to predict agent behavior or identify reward function bugs