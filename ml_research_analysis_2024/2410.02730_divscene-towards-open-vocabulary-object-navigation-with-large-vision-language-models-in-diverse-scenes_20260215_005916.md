---
ver: rpa2
title: 'DivScene: Towards Open-Vocabulary Object Navigation with Large Vision Language
  Models in Diverse Scenes'
arxiv_id: '2410.02730'
source_url: https://arxiv.org/abs/2410.02730
tags:
- agent
- arxiv
- rotation
- navigation
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DIVSCENE, a large-scale dataset with 4,614
  houses across 81 scene types and 5,707 target object categories, addressing the
  challenge of open-vocabulary object navigation in diverse environments. The authors
  propose N ATVLM, an end-to-end embodied agent fine-tuned from a large vision-language
  model (LVLM) using imitation learning on BFS-generated shortest paths without human
  supervision.
---

# DivScene: Towards Open-Vocabulary Object Navigation with Large Vision Language Models in Diverse Scenes

## Quick Facts
- arXiv ID: 2410.02730
- Source URL: https://arxiv.org/abs/2410.02730
- Reference count: 40
- The paper introduces DIVSCENE dataset and N AT VLM agent achieving over 20% better success rates than GPT-4o in open-vocabulary object navigation.

## Executive Summary
This paper addresses the challenge of open-vocabulary object navigation in diverse 3D environments by introducing DIVSCENE, a large-scale dataset with 4,614 houses across 81 scene types and 5,707 target object categories. The authors propose N AT VLM, an end-to-end embodied agent fine-tuned from a large vision-language model using imitation learning on BFS-generated shortest paths. The agent incorporates Chain-of-Thought (CoT) explanation traces to improve navigation reasoning and demonstrates significant performance improvements over baselines, including GPT-4o, with strong few-shot learning abilities and generalization to out-of-distribution datasets.

## Method Summary
The approach involves fine-tuning the Idefics 2 large vision-language model through imitation learning on BFS-generated shortest paths from the DIVTRAJ dataset. The model generates navigation actions end-to-end from visual observations, incorporating CoT explanation traces to enhance reasoning. Training uses a learning rate of 2e-5, batch size of 64, and BF16 precision for 1 epoch on NVIDIA A100 GPUs. The agent navigates using the AI2THOR environment, with performance evaluated on success rate (SR), success weighted by path length (SPL), and success weighted by episode length (SEL) metrics.

## Key Results
- N AT VLM achieves over 20% higher success rates compared to GPT-4o baseline
- Strong few-shot learning performance across varying training data fractions
- Good generalization to out-of-distribution datasets beyond DIVSCENE
- CoT explanation traces significantly improve navigation reasoning and accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoT explanation traces improve LVLM navigation by forcing step-by-step reasoning about positional differences and obstacles
- Mechanism: The model generates intermediate reasoning steps (compare positions, check for obstacles, decide action) rather than directly outputting actions. This explicit decomposition helps the model align its understanding with navigation logic.
- Core assumption: The LVLM can effectively generate and follow structured reasoning steps when prompted, and these steps improve downstream action accuracy.
- Evidence anchors:
  - [abstract]: "We also introduce CoT explanation traces of the action prediction for better performance when tuning LVLMs."
  - [section]: "To improve the accuracy, we also manually collect complex CoT explanation traces of each prediction (Mitra et al., 2023; Ho et al., 2023) to help the LVLM understand the underlying rationale behind object navigation."
  - [corpus]: Weak - no direct corpus comparison of CoT vs non-CoT navigation performance.
- Break condition: If the model generates incorrect intermediate reasoning or skips steps, performance degrades despite CoT prompting.

### Mechanism 2
- Claim: Imitation learning on BFS-generated shortest paths provides effective training data without human supervision
- Mechanism: The BFS planner computes optimal paths using ground truth map information, and the agent learns to mimic these actions from visual observations alone. This avoids expensive human demonstrations while providing high-quality supervision.
- Core assumption: The BFS planner's shortest paths are optimal enough that imitating them transfers well to real navigation without ground truth access.
- Evidence anchors:
  - [abstract]: "Our agent achieves a success rate that surpasses GPT-4o by over 20%."
  - [section]: "With shortest-path episodes, we perform extensive experiments and analyses to evaluate our NATVLM agent."
  - [corpus]: Moderate - several papers (Ehsani et al., 2023; Brohan et al., 2022) use imitation learning, but specific BFS-based shortest path imitation is novel.
- Break condition: If the planner's assumptions (discrete grid, known obstacles) differ too much from real navigation, imitation may not generalize.

### Mechanism 3
- Claim: Fine-tuning LVLMs end-to-end on navigation tasks bridges the domain gap between pre-training and embodied navigation
- Mechanism: The LVLM learns to directly map visual observations to actions, eliminating the need for separate perception (captioning) and planning modules that might lose information.
- Core assumption: The LVLM's pre-trained vision and language capabilities transfer sufficiently to navigation tasks when fine-tuned on relevant data.
- Evidence anchors:
  - [abstract]: "By tuning the LVLM, we eliminate the domain gap between navigation tasks and the pre-training corpus."
  - [section]: "Meanwhile, our navigation is performed in an end-to-end manner with perception, circumventing the captioning model."
  - [corpus]: Strong - multiple papers (Yu et al., 2023; Chen et al., 2023) show domain gap issues for LLMs in navigation.
- Break condition: If the LVLM's pre-training corpus lacks sufficient navigation-relevant concepts, fine-tuning may not overcome the gap.

## Foundational Learning

- Concept: Breadth-First Search (BFS) path planning
  - Why needed here: BFS generates the expert trajectories that train the navigation agent
  - Quick check question: How does BFS guarantee finding the shortest path in an unweighted grid graph?

- Concept: Imitation learning vs reinforcement learning
  - Why needed here: The paper uses imitation learning on shortest paths rather than RL, avoiding reward shaping complexity
  - Quick check question: What are the key differences in data requirements between imitation learning and RL for navigation?

- Concept: Chain-of-Thought (CoT) reasoning in language models
  - Why needed here: CoT traces help the LVLM understand navigation reasoning rather than just mimicking actions
  - Quick check question: How does generating intermediate reasoning steps potentially improve model performance compared to direct action prediction?

## Architecture Onboarding

- Component map: LVLM (vision encoder + language model + modality projector) → instruction tuning on navigation episodes → BFS planner generates expert trajectories → CoT traces enhance training data → fine-tuned model outputs navigation actions
- Critical path: Visual observation + agent status + recent history → instruction encoding → LVLM generation → action output → environment response
- Design tradeoffs: End-to-end LVLM vs modular approach (separate perception + planning), CoT traces add training complexity but improve reasoning, BFS planner provides perfect supervision but assumes access to ground truth
- Failure signatures: Agent meanders in limited areas (memory limitation), fails on long trajectories (recent history constraint), performance drops without CoT (reasoning gap), poor generalization to new environments (distribution shift)
- First 3 experiments:
  1. Test LVLM with and without CoT traces on validation set to quantify reasoning benefit
  2. Compare imitation learning vs random action baseline to establish baseline improvement
  3. Evaluate few-shot learning performance with 20%, 40%, 60%, 80%, 100% training data to measure data efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would expanding the memory capacity of LVLMs impact long-horizon navigation performance?
- Basis in paper: [explicit] The authors identify as a limitation that their model fails on long-horizon tasks due to only having recent historical information, and suggest this as a promising direction for future work.
- Why unresolved: The paper does not conduct experiments with expanded memory mechanisms or compare different memory architectures for navigation tasks.
- What evidence would resolve it: Comparative experiments showing navigation performance improvements when implementing memory mechanisms (like recurrent networks or external memory) versus the current approach that only uses recent 8 steps of history.

### Open Question 2
- Question: How robust is the agent's navigation performance when faced with dynamic obstacles or changes in the environment?
- Basis in paper: [inferred] The paper evaluates navigation in static environments using BFS-generated shortest paths, but doesn't test performance under dynamic conditions where obstacles might move or appear/disappear.
- Why unresolved: All experiments and evaluations use pre-generated static shortest paths without introducing environmental changes or moving obstacles during navigation.
- What evidence would resolve it: Experiments introducing dynamic obstacles or environmental changes during navigation episodes to measure performance degradation compared to static conditions.

### Open Question 3
- Question: What is the impact of varying the number of CoT explanation traces provided during training on final navigation performance?
- Basis in paper: [explicit] The authors manually collect CoT explanation traces and show their importance, but only test one configuration of explanation traces without exploring the trade-off between explanation complexity and performance.
- Why unresolved: The paper only tests one approach to CoT explanations (manual collection for all steps) without comparing different levels of explanation detail or automated versus manual collection methods.
- What evidence would resolve it: Systematic experiments varying the number, detail level, and collection method (manual vs automated) of CoT explanations to identify optimal configurations for different performance metrics.

## Limitations
- The paper lacks direct ablation studies comparing CoT vs non-CoT performance on the same validation sets
- Claims about few-shot learning abilities and out-of-distribution generalization lack sufficient quantitative evidence
- The BFS planner's assumptions about discrete grids and known obstacles may not fully translate to real-world navigation scenarios

## Confidence

**High Confidence**: The core claim that imitation learning on BFS-generated paths improves navigation performance is well-supported by multiple prior works and the paper's experimental results showing 20%+ improvement over GPT-4o.

**Medium Confidence**: The effectiveness of end-to-end LVLM fine-tuning for bridging domain gaps is reasonably supported by the literature and experimental results, though the specific architecture choices could benefit from more ablation studies.

**Low Confidence**: Claims about few-shot learning abilities and strong out-of-distribution generalization lack sufficient quantitative backing, with the paper providing only qualitative descriptions rather than rigorous experimental validation.

## Next Checks

1. **Ablation Study Required**: Run controlled experiments comparing NATVLM performance with and without CoT traces on the same validation splits to quantify the exact contribution of Chain-of-Thought reasoning to navigation success rates.

2. **Few-Shot Learning Validation**: Systematically test the few-shot learning claims by evaluating performance across multiple training data fractions (20%, 40%, 60%, 80%, 100%) and report learning curves to demonstrate data efficiency claims.

3. **Out-of-Distribution Testing**: Conduct rigorous cross-dataset evaluation by testing the trained model on established benchmarks like RoboTHOR or HM3D to empirically validate generalization claims beyond the DIVSCENE dataset.