---
ver: rpa2
title: 'ControLRM: Fast and Controllable 3D Generation via Large Reconstruction Model'
arxiv_id: '2410.09592'
source_url: https://arxiv.org/abs/2410.09592
tags:
- generation
- image
- depth
- condition
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ControLRM, a fast and controllable 3D generation
  framework using a large reconstruction model (LRM). The method addresses limitations
  of existing 3D generation approaches that are either time-consuming (optimization-based)
  or lack controllability (feed-forward).
---

# ControLRM: Fast and Controllable 3D Generation via Large Reconstruction Model

## Quick Facts
- arXiv ID: 2410.09592
- Source URL: https://arxiv.org/abs/2410.09592
- Authors: Hongbin Xu; Weitao Chen; Zhipeng Zhou; Feng Xiao; Baigui Sun; Mike Zheng Shou; Wenxiong Kang
- Reference count: 40
- Primary result: Achieves 60-18x faster inference than MVControl while maintaining or improving generation quality on G-OBJ benchmark

## Executive Summary
This paper introduces ControLRM, a fast and controllable 3D generation framework that addresses limitations of existing 3D generation approaches. While optimization-based methods are time-consuming and feed-forward approaches lack controllability, ControLRM proposes an end-to-end feed-forward architecture that enables rapid inference while supporting multiple control conditions including edge, depth, normal, and sketch. The method leverages pre-trained large reconstruction model (LRM) weights through a joint training framework, achieving significantly faster inference speeds while maintaining or improving generation quality across multiple datasets.

## Method Summary
ControLRM introduces an end-to-end feed-forward architecture consisting of a 2D condition generator, condition encoder, and triplane decoder. The method employs a joint training framework that leverages pre-trained LRM weights to align 2D latents with 3D triplanes while supporting various control conditions. By freezing the pre-trained triplane decoder and training the condition encoder and 2D generator separately, the model captures generalizable 3D reconstruction patterns while enabling rapid inference. The framework supports multiple condition types including edge, depth, normal, and sketch, with two variants: ControLRM-T using a transformer backbone and ControLRM-D using a diffusion backbone.

## Key Results
- Achieves 60-18x faster inference than MVControl (0.503s vs 8.92s per sample)
- On G-OBJ benchmark, ControLRM-D achieves FID scores of 104.08 vs MVControl's 175.43
- Demonstrates strong generalization on zero-shot datasets including GSO and ABO
- Maintains controllability metrics (C-PSNR, S-PSNR, etc.) comparable to or better than slower optimization-based methods

## Why This Works (Mechanism)

### Mechanism 1
Joint training with pre-trained LRM weights significantly improves controllability while maintaining fast inference. By freezing the pre-trained triplane decoder and training the condition encoder and 2D generator separately, the model leverages strong 3D reconstruction priors learned from millions of 3D data while aligning 2D conditions with 3D representations. Core assumption: pre-trained triplane decoder captures generalizable 3D reconstruction patterns. Break condition: if the pre-trained triplane decoder lacks sufficient generalization to diverse input conditions.

### Mechanism 2
End-to-end feed-forward architecture eliminates misalignment between 2D and 3D representations. The model directly maps 2D conditions to triplane representations through a single forward pass, avoiding separate 2D generation and 3D reconstruction stages. Core assumption: direct 2D-to-3D mapping can maintain sufficient detail without intermediate optimization steps. Break condition: if direct mapping loses critical information during transformation.

### Mechanism 3
Multi-condition support enables comprehensive 3D controllability. The 2D condition generator accepts different types of visual conditions and text prompts, which are encoded and passed through cross-attention mechanisms to generate appropriate triplane representations. Core assumption: different visual conditions contain complementary information that can be effectively combined. Break condition: if certain conditions are inherently incompatible or the model cannot effectively learn relationships between different condition types.

## Foundational Learning

- Concept: Variational Autoencoder (VAE) framework
  - Why needed here: Understanding how LRM can be interpreted as a VAE helps explain the joint training approach and relationship between latent spaces
  - Quick check question: How does the ELBO formulation in Eq. 10-15 relate to the standard VAE objective, and what modifications are made for controllable 3D generation?

- Concept: Neural Radiance Fields (NeRF) and triplane representation
  - Why needed here: The triplane decoder generates triplane NeRF representations, which are fundamental to the 3D generation process
  - Quick check question: How does the bilinear sampling operation in Eq. 7 extract features from the triplane representation for arbitrary 3D points?

- Concept: Cross-attention mechanisms in transformers
  - Why needed here: Cross-attention layers enable the model to incorporate information from 2D conditions into the 3D generation process
  - Quick check question: What is the difference between the Cross-I and Cross-C attention layers in the model architecture, and how do they contribute to the joint training process?

## Architecture Onboarding

- Component map: 2D condition generator → 2D condition encoder → Cross-attention → Triplane decoder → NeRF rendering → Image output
- Critical path: 2D condition → Condition Encoder → Cross-attention → Triplane Decoder → NeRF rendering → Image output
- Design tradeoffs:
  - Using pre-trained LRM weights vs. training from scratch: Faster convergence and better quality vs. less flexibility in architecture changes
  - Joint training vs. sequential training: Better alignment vs. more complex training process
  - Transformer vs. diffusion for condition generator: Faster inference vs. potentially better handling of complex conditions
- Failure signatures:
  - Poor controllability metrics (C-PSNR, S-PSNR, etc.) indicate misalignment between input conditions and generated 3D content
  - High FID scores suggest poor generation quality or lack of diversity
  - Training instability may indicate issues with the joint training framework or loss balance
- First 3 experiments:
  1. Train ControLRM-T with only reconstruction loss (Lrecon) to verify basic functionality and compare with pre-trained LRM
  2. Add adversarial loss (Ladv) and evaluate improvement in generation quality and FID scores
  3. Test controllability with edge conditions only, measuring C-PSNR and C-SSIM against ground truth

## Open Questions the Paper Calls Out

### Open Question 1
How can the ControLRM framework be extended to support additional control conditions beyond edge, depth, normal, and sketch? The paper mentions this as a limitation, stating "Condition Expansion: While significant advancements have been made under four control conditions, it is crucial to extend this framework to encompass additional control conditions such as segmentations, pose, and others." This remains unresolved as the paper only demonstrates results for four specific control conditions and acknowledges the need for expansion without providing a methodology for incorporating new condition types.

### Open Question 2
What is the impact of using estimated versus ground truth depth and normal maps on ControLRM's performance? The paper discusses this issue, noting that "the extraction of depth and normal maps relies on pre-trained models supplied by MVControl" and that "this distribution discrepancy between the ground truth and estimated maps adversely impacts the performance of ControLRM." While the paper observes performance degradation with estimated maps, it does not systematically study the relationship between map quality and generation quality, or propose solutions to this limitation.

### Open Question 3
How does the choice of backbone (transformer vs. diffusion) affect ControLRM's controllability and generation quality across different condition types? The paper introduces two variants (ControLRM-T with transformer backbone and ControLRM-D with diffusion backbone) and notes that "ControLRM-T shows slightly inferior performance than MVControl under depth and normal conditions but excels in canny and sketch conditions." While the paper provides quantitative comparisons, it does not conduct a systematic analysis of which backbone is better suited for specific condition types or explore the reasons for the performance differences.

## Limitations
- Reliance on pre-trained LRM weights means inheriting any biases or limitations from original LRM training data
- Joint training framework introduces complexity that may affect stability when aligning diverse 2D conditions with 3D representations
- Evaluation limited to specific datasets (GSO and ABO) that may not represent full diversity of real-world applications

## Confidence

**High Confidence**: Claims about inference speed improvements (60-18x faster than MVControl) and basic functionality of end-to-end feed-forward architecture are well-supported by experimental results and ablation studies.

**Medium Confidence**: Claims about effectiveness of joint training framework and superiority over existing methods on benchmark datasets are supported but could benefit from additional ablation studies isolating contribution of each component.

**Low Confidence**: Claims about method's ability to handle arbitrary 2D conditions and robustness to diverse real-world scenarios are based on limited evaluation and would require more extensive testing on varied datasets.

## Next Checks

1. **Ablation Study on Training Components**: Conduct systematic ablation study isolating contribution of each loss component (reconstruction, adversarial, CLIP) and training branch (condition vs image) to quantify individual impact on controllability metrics and generation quality.

2. **Generalization Testing on Diverse Conditions**: Evaluate ControLRM's performance on wider range of 2D conditions beyond four specified types, including real-world sketches and user-drawn inputs, to assess robustness and practical applicability.

3. **Long-term Stability Analysis**: Train ControLRM for extended periods and monitor for training collapse or degradation in controllability metrics over time, particularly focusing on joint training framework's stability across different random seeds and hyperparameter settings.