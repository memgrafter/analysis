---
ver: rpa2
title: Consolidating Ranking and Relevance Predictions of Large Language Models through
  Post-Processing
arxiv_id: '2404.11791'
source_url: https://arxiv.org/abs/2404.11791
tags:
- ranking
- relevance
- methods
- pairwise
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of reconciling the relevance
  label generation and ranking performance of large language models (LLMs). While
  LLMs can generate relevance labels, their direct use leads to suboptimal ranking.
---

# Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing

## Quick Facts
- arXiv ID: 2404.11791
- Source URL: https://arxiv.org/abs/2404.11791
- Reference count: 15
- One-line primary result: Constrained regression post-processing aligns LLM relevance scores with pairwise ranking order while preserving label fidelity

## Executive Summary
This work addresses the challenge of reconciling the relevance label generation and ranking performance of large language models (LLMs). While LLMs can generate relevance labels, their direct use leads to suboptimal ranking. Pairwise ranking prompting (PRP), where LLMs compare documents pairwise, achieves strong ranking performance but produces poorly calibrated relevance scores. To bridge this gap, the authors propose post-processing methods that adjust LLM-generated relevance labels to align with the ranking order implied by PRP, while staying as close as possible to the original labels. The core method is a constrained regression approach that finds minimal perturbations to the relevance scores such that the resulting ranking matches PRP's pairwise preferences. Efficient variants reduce the number of pairwise constraints by using sliding window or top-vs-all selection strategies. Experiments on five public ranking datasets demonstrate that these methods effectively balance ranking and relevance prediction, outperforming both direct PRP and pseudo-rater approaches, and approaching state-of-the-art ranking performance with minimal loss in label accuracy.

## Method Summary
The authors propose a constrained regression framework to adjust LLM-generated relevance scores so they align with PRP-derived ranking order while staying close to original values. The method takes both pointwise relevance labels and pairwise preferences as input, then solves an optimization problem that minimizes perturbations to the relevance scores subject to pairwise ordering constraints. To improve efficiency, the authors develop SlideWin (using sliding window of k successive pairs from an initial ranking) and TopAll (using top-k vs all comparisons) variants that reduce the number of pairwise constraints from O(n²) to O(kn). The optimization can be solved efficiently using existing math libraries, making the approach practical for large-scale applications.

## Key Results
- Consolidation methods outperform both direct PRP and pseudo-rater approaches on NDCG@10 across all five datasets
- SlideWin and TopAll variants achieve comparable ranking performance to Allpair with significantly fewer pairwise comparisons (O(kn) vs O(n²))
- Methods approach state-of-the-art ranking performance while maintaining better relevance prediction accuracy (lower ECE) than PRP alone
- BM25 as base rater degrades relevance prediction performance but maintains ranking effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM relevance labels are poorly calibrated for ranking but PRP yields accurate pairwise orderings.
- Mechanism: The PRP prompt ("Is document A more relevant than document B to query Q?") generates pairwise preferences that aggregate into a document ranking score via win counting. These preferences are internally consistent for ordering but the resulting absolute scores are not calibrated to reflect true relevance magnitudes.
- Core assumption: LLM pairwise comparisons are reliable for relative ordering but not for absolute relevance estimation.
- Evidence anchors:
  - [abstract] "The pairwise ranking prompting (PRP) approach produces promising ranking performance through asking about pairwise comparisons, e.g., 'Is document A more relevant than document B to query Q?' Thus, while LLMs are effective at their ranking ability, this is not reflected in their relevance label generation."
  - [section 4.1] "The goal of the constrained regression methods is to adjust the LLM relevance predictions ˆy so that their order aligns with the ranking order of the PRP results ˆs."
- Break condition: If LLM pairwise judgments are noisy or inconsistent, the win-counting aggregation becomes unreliable, leading to incorrect ranking orders.

### Mechanism 2
- Claim: Constrained regression minimizes perturbation to original relevance scores while enforcing PRP-derived ordering constraints.
- Mechanism: The optimization problem (Eq. 7) finds minimal linear modifications δ to the relevance predictions such that for every PRP-derived pairwise constraint, the modified scores preserve the ordering (ˆyi + δi > ˆyj + δj). This keeps the final scores close to the original pseudo-rater predictions while aligning their order with PRP.
- Core assumption: The minimal perturbation assumption ensures that the adjusted scores remain interpretable as relevance labels while satisfying ranking constraints.
- Evidence anchors:
  - [section 4.1] "Our method takes both LLM generated relevance labels and pairwise preferences. The labels are then altered to satisfy the pairwise preferences of the LLM, while staying as close to the original values as possible."
  - [section 4.2] "The mathematical problem posed in Eq. 7 is a well-known constrained regression problem that can easily be solved with publicly available existing math libraries."
- Break condition: If the PRP-derived constraints are contradictory or if the original relevance scores are too far from the desired order, the optimization may fail or produce large perturbations that compromise label fidelity.

### Mechanism 3
- Claim: Efficient constrained regression variants (SlideWin, TopAll) reduce LLM calls while preserving most ranking-relevant constraints.
- Mechanism: Instead of using all o(n²) pairwise constraints, SlideWin uses o(kn) successive pair comparisons from an initial ranking to determine top-k results, and TopAll uses o(kn) constraints between top-k relevance predictions and all other documents. This focuses computational effort on the ranking-critical subset of constraints.
- Core assumption: Ranking performance is primarily determined by the correct ordering of top results; lower-ranked items are less critical for NDCG.
- Evidence anchors:
  - [section 4.2] "As the ranking performance focuses mostly on the top results (top 10 or top 20), PRP work (Qin et al., 2023) proposes to just run a sliding window sorting from some initial ranking to find the top-k results with o(kn) pair comparisons."
  - [section 6] "Our consolidation methods even outperform PRP+PWL, the one with extra data, in ECE on 4 out of 5 datasets and while keep ranking performance in NDCG@10 as good on all datasets."
- Break condition: If the initial ranking used to select constraints is very poor, the selected constraints may miss critical pairwise preferences needed to achieve correct final ordering.

## Foundational Learning

- Concept: Pairwise ranking aggregation via win counting
  - Why needed here: PRP generates pairwise preferences that must be converted into a full document ranking. Win counting (Eq. 4) provides a simple, interpretable aggregation method that produces a total order from pairwise judgments.
  - Quick check question: Given three documents A, B, C with pairwise preferences A>B, B>C, and A>C, what are the win counts and final ranking?

- Concept: Constrained optimization for score calibration
  - Why needed here: The core challenge is reconciling two different LLM outputs (relevance scores and pairwise orderings). Constrained regression provides a principled way to adjust one set of outputs (relevance scores) to match the constraints from another (pairwise orderings) while minimizing deviation from the original values.
  - Quick check question: If document A has relevance score 0.8, document B has 0.6, and PRP indicates A>B, what is the minimal perturbation needed to enforce the ordering constraint?

- Concept: Efficiency vs accuracy tradeoff in constraint selection
  - Why needed here: PRP requires o(n²) pairwise comparisons for n documents, which is computationally expensive. Understanding when and how to reduce this complexity without sacrificing ranking quality is crucial for practical deployment.
  - Quick check question: If you have a list of 100 documents and only consider the top 10 for pairwise comparisons with all others, how many constraints do you process instead of the full 4950?

## Architecture Onboarding

- Component map:
  Input layer: Query + candidate document list -> LLM pseudo-rater -> Generates pointwise relevance scores (ˆy)
  Input layer: Query + candidate document list -> LLM ranker -> Generates pairwise preferences via PRP
  Constraint selector: Slides window or selects top-k vs all pairs
  Constrained regression engine: Solves optimization problem (Eq. 7)
  Output layer: Ranking-aware relevance scores for both ranking and relevance prediction

- Critical path: Query → PRP pairwise preferences → Constraint selection → Constrained regression → Final scores → Ranking
  The pseudo-rater output is used but primarily as initialization for the constrained regression; the PRP preferences drive the final ordering.

- Design tradeoffs:
  - Accuracy vs efficiency: Full PRP (o(n²) constraints) vs SlideWin/TopAll (o(kn) constraints)
  - Label fidelity vs ranking alignment: The λ parameter in constrained regression controls the balance between staying close to original scores vs satisfying ranking constraints
  - LLM call cost vs post-processing cost: PRP requires many LLM calls, but constrained regression is computationally cheap

- Failure signatures:
  - Poor NDCG despite high ECE: Constraints may not be properly enforcing the PRP ordering, or the initial ranking used for constraint selection may be very different from PRP's true ranking
  - High ECE despite good NDCG: The constrained regression may be over-enforcing ranking constraints at the expense of relevance score fidelity
  - Degraded performance on both metrics: LLM pairwise preferences may be unreliable, or the constraint selection may be missing critical pairwise comparisons

- First 3 experiments:
  1. Run PRP on a small dataset (e.g., 10 queries) with all pairwise comparisons, then apply constrained regression with all constraints vs SlideWin (k=10) vs TopAll (k=10). Compare NDCG@10 and ECE to identify the efficiency-accuracy tradeoff.
  2. Test different initial rankings for SlideWin (BM25, PRater, random) to understand how the starting point affects the final ranking quality and efficiency.
  3. Vary the number of top-k results considered in TopAll (k=5, 10, 20) to find the optimal balance between ranking performance and computational cost on a medium-sized dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the constrained regression methods effectively consolidate listwise ranking results from large language models?
- Basis in paper: [inferred] The authors mention that their methods can be applied to listwise ranking results by decomposing them into pairwise comparisons, but suggest that more effective methods may exist for listwise rankers.
- Why unresolved: The paper primarily focuses on consolidating pairwise ranking results and does not extensively explore the consolidation of listwise ranking results.
- What evidence would resolve it: Experiments comparing the performance of constrained regression methods on listwise ranking results versus pairwise ranking results would provide evidence for the effectiveness of the methods on listwise rankers.

### Open Question 2
- Question: How do the constrained regression methods perform on datasets where large language models have suboptimal performance?
- Basis in paper: [explicit] The authors acknowledge that their framework assumes reasonable rating and ranking performance by LLMs, and note that more advanced adjustments may be required for scenarios where LLMs perform suboptimally.
- Why unresolved: The paper does not provide experimental results on datasets where LLMs have known performance issues, such as domains opaque to the underlying LLMs.
- What evidence would resolve it: Experiments evaluating the performance of the constrained regression methods on datasets known to be challenging for LLMs, such as those in domains with limited training data or specialized terminology, would provide evidence for the methods' robustness.

### Open Question 3
- Question: What is the impact of different initial ranking models on the performance of the constrained regression methods?
- Basis in paper: [explicit] The authors investigate the effects of different initial ranking models (BM25 vs. LLM pseudo-rater) on the performance of the constrained regression methods in the supplementary material.
- Why unresolved: While the authors provide some insights into the impact of initial ranking models, the analysis is limited to a specific dataset and does not explore the generalizability of the findings across different datasets or ranking scenarios.
- What evidence would resolve it: Experiments evaluating the performance of the constrained regression methods using different initial ranking models across multiple datasets and ranking scenarios would provide evidence for the methods' sensitivity to the choice of initial ranking model.

## Limitations

- The study focuses exclusively on TREC-style retrieval tasks and does not evaluate on diverse query types or non-TREC domains
- Limited ablation studies on prompt engineering choices that could affect both PRP and pseudo-rater performance
- No analysis of how the methods perform with different LLM model sizes or capabilities

## Confidence

- High confidence in the constrained regression framework and its mathematical formulation
- Medium confidence in the efficiency claims for SlideWin/TopAll methods, as the analysis assumes PRP preferences are relatively consistent
- Medium confidence in the generalizability across datasets, as all tested datasets share similar characteristics

## Next Checks

1. Test the constrained regression approach on non-TREC datasets with different relevance scales (e.g., 5-point vs binary relevance)
2. Evaluate the impact of prompt quality on both PRP and pseudo-rater outputs by systematically varying prompt templates
3. Measure the sensitivity of SlideWin/TopAll methods to the quality of the initial ranking used for constraint selection