---
ver: rpa2
title: Utilizing Large Language Models to Synthesize Product Desirability Datasets
arxiv_id: '2411.13485'
source_url: https://arxiv.org/abs/2411.13485
tags:
- word
- data
- sentiment
- review
- product
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This research demonstrates the potential of large language models\
  \ (LLMs), specifically gpt-4o-mini, for generating synthetic datasets tailored for\
  \ Product Desirability Toolkit (PDT) sentiment analysis. Three methods\u2014Word+Review,\
  \ Review+Word, and Supply-Word\u2014were tested to produce 1000 synthetic software\
  \ product reviews each."
---

# Utilizing Large Language Models to Synthesize Product Desirability Datasets

## Quick Facts
- arXiv ID: 2411.13485
- Source URL: https://arxiv.org/abs/2411.13485
- Reference count: 40
- Large language models (LLMs) can generate synthetic product reviews with strong sentiment alignment for Product Desirability Toolkit (PDT) analysis.

## Executive Summary
This research demonstrates that large language models, specifically GPT-4o-mini, can effectively generate synthetic datasets for Product Desirability Toolkit (PDT) sentiment analysis. Three methods were tested to produce 1000 synthetic software product reviews each, achieving Pearson correlations between target and evaluated sentiment scores ranging from 0.93 to 0.97. The Supply-Word method showed the highest text diversity and PDT term coverage, though with increased generation costs. Despite minor positive sentiment biases, the study confirms LLM-generated synthetic data offers a scalable, cost-effective solution for PDT dataset production, particularly valuable when real data is limited.

## Method Summary
The study used GPT-4o-mini via OpenAI API to generate synthetic PDT datasets using three methods: Word+Review (word then review), Review+Word (review then word), and Supply-Word (random word supply). Each method produced 1000 reviews with target sentiment scores ranging from 0.0 to 1.0. The generated datasets were scored using LLM sentiment analysis with a 'Complete' prompt approach, and evaluated for text diversity using compression ratio, POS compression ratio, ROUGE-L, and n-gram diversity metrics. Costs were calculated based on token usage.

## Key Results
- Sentiment alignment achieved Pearson correlations of 0.93-0.97 between target and evaluated scores across all methods
- Supply-Word method exhibited highest text diversity and coverage of all 118 PDT terms
- GPT-4o-mini's API cost is 6% of gpt-4o's, enabling cost-effective large-scale data generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4o-mini can generate synthetic product reviews that closely align with specified sentiment scores.
- Mechanism: The LLM interprets the provided sentiment score and PDT word list to generate contextually relevant text, with the "Supply-Word" method achieving highest Pearson correlations (0.97) by using the LLM's intrinsic understanding of word sentiment.
- Core assumption: GPT-4o-mini's pre-training enables accurate sentiment representation when provided with appropriate context.
- Evidence anchors:
  - [abstract]: "Results demonstrated high sentiment alignment across all methods, with Pearson correlations ranging from 0.93 to 0.97."
  - [section]: "Supply-Word exhibited the highest diversity and coverage of PDT terms, although with increased generation costs."
- Break condition: If the LLM's sentiment understanding significantly deviates from human interpretation, or if context provided is insufficient for accurate sentiment generation.

### Mechanism 2
- Claim: The "Supply-Word" method provides better text diversity and PDT word coverage compared to other methods.
- Mechanism: By randomly selecting from all 118 PDT words and generating reviews based on the word's inherent sentiment, the method ensures comprehensive coverage of the word set and varied text generation.
- Core assumption: Random selection from the complete PDT word set ensures representative sentiment distribution.
- Evidence anchors:
  - [abstract]: "Supply-Word exhibited the highest diversity and coverage of PDT terms, although with increased generation costs."
  - [section]: "Supply-Word used all 118 PDT as they were selected programmatically at random."
- Break condition: If the LLM's word sentiment understanding is biased or if the random selection process fails to produce balanced sentiment distribution.

### Mechanism 3
- Claim: GPT-4o-mini's cost efficiency makes it viable for large-scale synthetic data generation.
- Mechanism: With API costs at 6% of gpt-4o, the model enables cost-effective production of large datasets, though at the expense of longer generation times.
- Core assumption: The quality of synthetic data from gpt-4o-mini is sufficient for research purposes despite being a smaller model.
- Evidence anchors:
  - [abstract]: "Utilizing gpt-4o-mini, a cost-effective alternative to larger commercial LLMs"
  - [section]: "gpt-4o-mini’s API cost is 6% of gpt-4o’s, which makes it a better fit for large-scale data generation, assuming reasonable performance."
- Break condition: If the cost savings are outweighed by significantly increased processing time or if the quality of generated data is insufficient for the intended use case.

## Foundational Learning

- Concept: Product Desirability Toolkit (PDT)
  - Why needed here: Understanding the PDT framework is crucial as it defines the structure and purpose of the synthetic datasets being generated.
  - Quick check question: What is the primary purpose of the PDT in evaluating user experiences?

- Concept: Sentiment Analysis with LLMs
  - Why needed here: The effectiveness of the synthetic data generation relies on the LLM's ability to accurately represent sentiment in generated text.
  - Quick check question: How does the "Complete" scoring approach differ from the "Base+Adjust" method in terms of LLM sentiment analysis?

- Concept: Text Diversity Metrics
  - Why needed here: Evaluating the diversity of generated text is essential to ensure the synthetic data is representative of human-generated content.
  - Quick check question: What are the four text diversity metrics used in this study, and what does each measure?

## Architecture Onboarding

- Component map: Data Generation (GPT-4o-mini API with different prompting strategies) -> Scoring (LLM-based sentiment analysis using the "Complete" prompt) -> Evaluation (Text diversity analysis and cost calculation) -> Storage (Synthetic datasets and results in Zenodo)

- Critical path: 1. Generate synthetic PDT datasets using GPT-4o-mini 2. Score generated data using LLM sentiment analysis 3. Calculate text diversity metrics 4. Analyze costs and performance

- Design tradeoffs:
  - Cost vs. Quality: Using gpt-4o-mini reduces costs but may slightly impact data quality compared to larger models
  - Diversity vs. Efficiency: The "Supply-Word" method provides better diversity but increases generation costs and time
  - Automation vs. Control: Automated word selection may lead to bias, requiring manual verification for production use

- Failure signatures:
  - Low sentiment alignment scores (Pearson correlation < 0.9)
  - Limited coverage of PDT word list (missing more than 10 words)
  - High repetition in generated text (low text diversity scores)
  - Unexpected biases in sentiment distribution

- First 3 experiments:
  1. Generate a small dataset (100 reviews) using each method and manually verify sentiment alignment
  2. Test the impact of different PDT word list sizes on text diversity and sentiment alignment
  3. Compare the performance of gpt-4o-mini with a larger model (e.g., gpt-4o) on a subset of data to assess quality differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can synthetic PDT datasets be validated against real-world human-generated datasets to ensure they capture the same nuanced user sentiments and preferences?
- Basis in paper: [explicit] The paper mentions future work will include generating synthetic data for specific software products with corresponding human PDT datasets for direct comparisons.
- Why unresolved: While the paper demonstrates strong sentiment alignment between synthetic and target scores, it does not validate the synthetic data against actual human responses to confirm it captures real user experiences.
- What evidence would resolve it: Comparative analysis showing synthetic datasets produce sentiment scores and word choices statistically similar to human-generated PDT data for the same products.

### Open Question 2
- Question: What specific biases exist in LLM-generated synthetic PDT data, and can they be systematically identified and mitigated?
- Basis in paper: [explicit] The paper acknowledges that LLMs trained on human text contain inherent biases, and the synthetic data shows positive sentiment bias, particularly in the Supply-Word method.
- Why unresolved: The paper identifies some bias but doesn't provide systematic methods for bias detection, quantification, or mitigation strategies for synthetic PDT data generation.
- What evidence would resolve it: Development and validation of bias detection metrics specific to PDT data, along with proven techniques to reduce identified biases while maintaining data quality.

### Open Question 3
- Question: What is the optimal balance between dataset size, generation cost, and text diversity for synthetic PDT data to be practically useful in real-world applications?
- Basis in paper: [inferred] The paper shows that larger datasets like Supply-Word have better diversity but higher costs, while smaller methods are cheaper but less diverse, without determining the practical trade-offs.
- Why unresolved: The paper presents different methods with varying costs and diversity metrics but doesn't establish practical guidelines for when each method is most appropriate or how to optimize for specific use cases.
- What evidence would resolve it: Empirical studies determining the minimum dataset size needed for reliable sentiment analysis, cost-benefit analysis across different application scenarios, and identification of diversity thresholds for practical utility.

## Limitations
- Findings are based on synthetic data for software product reviews only, limiting generalizability to other domains
- The study uses a single LLM model (gpt-4o-mini), which may not represent performance of other models
- Random word selection in Supply-Word method may lead to imbalanced sentiment distributions without explicit validation

## Confidence

**High Confidence**: The sentiment alignment results (Pearson correlations 0.93-0.97) are robust given the clear methodology and consistent evaluation approach. The cost comparison between gpt-4o-mini and larger models is factual and verifiable.

**Medium Confidence**: The relative performance differences between the three generation methods are credible based on the reported metrics, but the practical significance of these differences in real-world applications is not fully established. The claim about Supply-Word providing "highest diversity" is supported by metrics but lacks comparative analysis with human-generated data.

**Low Confidence**: The assertion that synthetic data generation is "scalable" for production environments is based on limited testing (1000 reviews per method) and doesn't account for potential scaling challenges like API rate limits, increased costs, or quality degradation at larger scales.

## Next Checks

1. **Benchmark against human data**: Generate a small dataset of human-written software product reviews and compare text diversity metrics directly with the synthetic outputs to validate whether the synthetic data achieves human-comparable quality.

2. **Cross-domain testing**: Apply the same generation methods to a different product category (e.g., consumer electronics or books) to assess whether the sentiment alignment and diversity results hold across domains.

3. **Cost scaling analysis**: Conduct a cost experiment generating 10,000+ reviews to identify whether generation costs scale linearly or if batch processing or other factors introduce unexpected cost increases.