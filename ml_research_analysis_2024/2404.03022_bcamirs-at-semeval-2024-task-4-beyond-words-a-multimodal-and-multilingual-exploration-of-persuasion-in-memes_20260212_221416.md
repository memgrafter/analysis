---
ver: rpa2
title: 'BCAmirs at SemEval-2024 Task 4: Beyond Words: A Multimodal and Multilingual
  Exploration of Persuasion in Memes'
arxiv_id: '2404.03022'
source_url: https://arxiv.org/abs/2404.03022
tags:
- text
- meme
- memes
- llav
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of identifying rhetorical and psychological
  persuasion techniques in memes, a challenging multimodal task. The core idea is
  to bridge the gap between visual and textual modalities by generating descriptive
  captions for meme images using large multimodal models like GPT-4, and then using
  these captions alongside the original meme text for classification.
---

# BCAmirs at SemEval-2024 Task 4: Beyond Words: A Multimodal and Multilingual Exploration of Persuasion in Memes

## Quick Facts
- arXiv ID: 2404.03022
- Source URL: https://arxiv.org/abs/2404.03022
- Reference count: 27
- Primary result: Achieved top-3 rankings across all languages in Subtask 2a and top-4 in Subtask 2b of SemEval-2024 Task 4

## Executive Summary
This paper addresses the challenge of identifying rhetorical and psychological persuasion techniques in memes through a multimodal and multilingual approach. The core innovation involves generating descriptive captions for meme images using large multimodal models like GPT-4, then using these captions alongside the original meme text for classification. This intermediate captioning step aims to capture metaphorical and abstract visual semantics that may be missed by direct image-text models. The method uses a combination of RoBERTa and CLIP encoders fine-tuned on the augmented data. Results show the proposed approach significantly outperforms baselines, achieving top-3 rankings across all languages in one subtask and top-4 in another, with hierarchical F1 scores up to 71.1%.

## Method Summary
The approach bridges visual and textual modalities by first generating descriptive captions for meme images using GPT-4. These captions, along with the original meme text, are then encoded using RoBERTa and CLIP models, with their embeddings concatenated for final classification. The method specifically addresses the challenge of capturing metaphorical visual semantics that standard vision encoders struggle with. The ConcatRoBERTa architecture combines RoBERTa for textual features, CLIP for visual features, and generated captions as an additional textual stream, providing complementary information from each modality for persuasion technique detection.

## Key Results
- Achieved top-3 rankings across all languages in Subtask 2a and top-4 in Subtask 2b
- Hierarchical F1 scores up to 71.1% on the persuasion technique classification task
- Significant improvement over baselines, particularly for capturing abstract visual semantics
- GPT-4 generated captions outperformed fine-tuned models like LLaVA-1.5, attributed to domain disparity between standard captioning datasets and persuasion technique memes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generated captions capture metaphorical and abstract visual semantics that are difficult for direct image-text models to encode.
- Mechanism: By first prompting a large multimodal model (GPT-4) to generate descriptive captions for meme images, the intermediate step translates visual metaphors into textual descriptions that align better with the model's semantic understanding of persuasion techniques.
- Core assumption: Memes frequently rely on visual metaphors that standard vision encoders (like CLIP) are not trained to interpret semantically, while language models have stronger exposure to metaphorical language.
- Evidence anchors:
  - [abstract]: "The improvement achieved by the introduced intermediate step is likely attributable to the metaphorical essence of images that challenges visual encoders."
  - [section]: "Vision encoders, such as CLIP, are primarily trained to comprehend the visual aspects of an image, lacking a focus on the metaphorical meanings embedded in those visual elements. In contrast, language models are more adept at understanding metaphors, given their greater exposure to such linguistic nuances in textual data."
- Break condition: If the generated captions do not accurately capture the metaphorical intent of the meme or if the caption generation model fails to understand cultural or contextual references.

### Mechanism 2
- Claim: Combining RoBERTa with CLIP and generated captions provides a richer representation than end-to-end multimodal models alone.
- Mechanism: The ConcatRoBERTa architecture uses RoBERTa for textual features, CLIP for visual features, and the generated caption as an additional textual stream, then concatenates these embeddings for classification. This allows each modality to specialize and provides complementary information.
- Core assumption: Each modality (text, image, caption) contains unique and non-redundant information about persuasion techniques.
- Evidence anchors:
  - [section]: "Our best model, ConcatRoBERTa, achieving notable performance improvements."
  - [section]: "models incorporating all features (e.g., text, caption, and image) tend to perform better. However, captions appear to be more informative."
- Break condition: If the modalities are highly correlated or if caption generation introduces noise that degrades the quality of information.

### Mechanism 3
- Claim: GPT-4's zero-shot caption generation outperforms fine-tuned models because of domain mismatch between training data and persuasion technique memes.
- Mechanism: GPT-4, being a general-purpose multimodal model, can better handle the diverse and often toxic content of persuasion technique memes compared to models fine-tuned on datasets like MemeCap that focus on standard image captioning.
- Core assumption: The domain gap between standard captioning datasets and persuasion technique memes is significant enough to impact performance.
- Evidence anchors:
  - [section]: "we observed that even the caption generated by LLaV A-1.5-7B had some issues potentially leading to degraded performance on the Persuasion dataset."
  - [section]: "As mentioned earlier, it is due to the domain disparity between MemeCap and this task's dataset."
- Break condition: If GPT-4's caption generation fails on specific meme types or if the model becomes too conservative in its caption generation due to safety filters.

## Foundational Learning

- Concept: Hierarchical multi-label classification
  - Why needed here: The task requires identifying multiple persuasion techniques that are organized in a tree-like hierarchical structure, where each technique may have parent-child relationships.
  - Quick check question: What is the difference between hierarchical classification and flat multi-label classification in terms of evaluation metrics?

- Concept: Multimodal representation learning
  - Why needed here: The model must effectively combine visual features (from CLIP), textual features (from RoBERTa), and generated caption features to capture all relevant information for persuasion technique detection.
  - Quick check question: How does concatenating embeddings from different modalities differ from using attention mechanisms to fuse multimodal information?

- Concept: Contrastive learning and vision-language pretraining
  - Why needed here: CLIP's effectiveness comes from its pretraining on large-scale image-text pairs using contrastive objectives, which helps align visual and textual representations.
  - Quick check question: What is the key difference between CLIP's pretraining objective and traditional supervised image classification pretraining?

## Architecture Onboarding

- Component map: GPT-4-V(ision) -> Caption generation; Meme + Caption + Text -> [RoBERTa, CLIP] -> Concat -> MLP -> Labels

- Critical path:
  1. Input meme image and text
  2. Generate descriptive caption using GPT-4
  3. Encode text with RoBERTa
  4. Encode image with CLIP
  5. Concatenate all embeddings
  6. Classify using MLP

- Design tradeoffs:
  - Using GPT-4 adds computational cost and API dependency but provides superior captions
  - Concatenation is simpler than attention-based fusion but may not capture cross-modal interactions as well
  - Freezing CLIP preserves pretrained knowledge but prevents adaptation to the specific domain

- Failure signatures:
  - Poor performance on non-English languages suggests caption quality or translation issues
  - Degradation when using fine-tuned caption models indicates domain mismatch
  - Large gap between dev and test performance may indicate overfitting or distribution shift

- First 3 experiments:
  1. Compare ConcatRoBERTa with generated captions vs. LLaVA-1.5 with both image and text
  2. Evaluate impact of caption quality by comparing GPT-4 vs. fine-tuned LLaV A captions
  3. Test ablation of image features to measure their contribution when captions are available

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method change when using captions generated by different multimodal models like GPT-4 versus fine-tuned models like LLaVA?
- Basis in paper: [explicit] The paper compares GPT-4 generated captions with those from fine-tuned LLaVA in Table 2, showing improved results with GPT-4.
- Why unresolved: The paper does not provide a detailed analysis of why GPT-4 captions outperform LLaVA-generated ones beyond mentioning domain disparity.
- What evidence would resolve it: A detailed comparison of the semantic content and style of captions generated by GPT-4 versus LLaVA, along with their correlation to persuasion technique classification accuracy.

### Open Question 2
- Question: What is the impact of including image captions on the adversarial robustness of the proposed method compared to end-to-end approaches?
- Basis in paper: [inferred] The paper mentions studying adversarial robustness as a future direction, implying the current method's vulnerability to adversarial attacks is unknown.
- Why unresolved: The paper does not explore adversarial attacks or the robustness of the proposed method.
- What evidence would resolve it: Experiments evaluating the proposed method's performance against adversarial examples and comparing it to end-to-end approaches.

### Open Question 3
- Question: How does the proposed method perform on other types of multimodal data beyond memes, such as political cartoons or satirical images?
- Basis in paper: [inferred] The paper focuses on memes and their unique challenges, but does not explore the generalizability of the method to other multimodal data types.
- Why unresolved: The paper does not provide experiments or analysis on other multimodal data types.
- What evidence would resolve it: Experiments applying the proposed method to other multimodal datasets and comparing its performance to existing methods.

## Limitations

- Heavy reliance on GPT-4's caption generation creates external dependency and raises reproducibility concerns
- Improvements may be partially attributable to GPT-4's superior general capabilities rather than the specific captioning methodology
- Limited analysis of why GPT-4 captions outperform fine-tuned models beyond domain mismatch speculation

## Confidence

- **High confidence**: The empirical results showing ConcatRoBERTa with generated captions outperforms baselines across multiple languages and subtasks
- **Medium confidence**: The explanation that generated captions specifically capture metaphorical visual semantics better than direct image encoders
- **Medium confidence**: The claim that GPT-4 outperforms fine-tuned caption models due to domain mismatch

## Next Checks

1. Conduct systematic evaluation of how caption accuracy correlates with classification performance by manually annotating a subset of generated captions

2. Investigate whether attention-based fusion of multimodal features provides additional performance gains compared to simple concatenation

3. Perform detailed error analysis on low-resource languages to identify whether performance gaps stem from caption generation quality or other factors