---
ver: rpa2
title: Leveraging Reverberation and Visual Depth Cues for Sound Event Localization
  and Detection with Distance Estimation
arxiv_id: '2410.22271'
source_url: https://arxiv.org/abs/2410.22271
tags:
- audio
- sound
- visual
- detection
- distance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes two audio-visual deep learning architectures
  for sound event localization and detection with distance estimation. The AV-Conformer
  uses concatenated audio-visual embeddings processed by a conformer module, while
  the Depth-cued model adds direct/reverberant audio features and cubemap depth maps.
---

# Leveraging Reverberation and Visual Depth Cues for Sound Event Localization and Detection with Distance Estimation

## Quick Facts
- arXiv ID: 2410.22271
- Source URL: https://arxiv.org/abs/2410.22271
- Reference count: 0
- The AV-Conformer achieved 40.8% F1 score, 17.7° DOAE, and 30.5% RDE on the STARSS23 dataset

## Executive Summary
This paper proposes two audio-visual deep learning architectures for sound event localization and detection with distance estimation. The AV-Conformer uses concatenated audio-visual embeddings processed by a conformer module, while the Depth-cued model adds direct/reverberant audio features and cubemap depth maps. Both models outperformed the challenge baselines on the STARSS23 dataset, with the ensemble of all four submitted systems achieving 40.3% F1, 18.0° DOAE, and 29.7% RDE. The main challenge was detecting rare sound classes like "Knock".

## Method Summary
The paper addresses the DCASE2024 Task 3 challenge using two main architectures. The AV-Conformer processes concatenated audio-visual embeddings through a conformer module, while the Depth-cued model incorporates direct/reverberant audio components and cubemap depth maps. Both models predict multi-ACCDDOA vectors for simultaneous event detection and localization. The models are trained on the STARSS23 dataset using Adam optimizer for 40 epochs with batches of 32 inputs.

## Key Results
- AV-Conformer achieved 40.8% F1 score, 17.7° DOAE, and 30.5% RDE
- Depth-cued model achieved 30.7% F1 score, 18.3° DOAE, and 29.1% RDE
- Ensemble of all four submitted systems achieved 40.3% F1, 18.0° DOAE, and 29.7% RDE
- Main challenge was detecting rare sound classes like "Knock"

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining audio-visual embeddings through concatenation before conformer processing enables the model to leverage complementary spatial information from both modalities
- Mechanism: The audio encoder extracts features from intensity vectors and log-mel spectrograms, while the visual encoder processes video frames through ResNet50. These embeddings are concatenated and processed by a conformer module, allowing the model to fuse spatial and temporal information from both modalities before predicting multi-ACCDDOA vectors
- Core assumption: Audio and visual features are temporally aligned and contain complementary spatial information that can be effectively combined through simple concatenation
- Evidence anchors:
  - [abstract] "The AV-Conformer uses concatenated audio-visual embeddings processed by a conformer module"
  - [section 2] "The embeddings are then concatenated and processed by a Conformer module with 4 layers"
  - [corpus] Weak evidence - the corpus papers discuss audio-visual localization but don't specifically address the concatenation approach used here
- Break condition: If audio and visual features are not temporally aligned or if one modality dominates the other, the concatenation approach may fail to effectively fuse complementary information

### Mechanism 2
- Claim: Incorporating direct and reverberant audio components improves distance estimation by providing explicit acoustic cues about source proximity
- Mechanism: The model extracts direct sound using WPE dereverberation algorithm and estimates reverberant components by taking the difference between original and direct signals. These are converted to log-mel spectrograms and concatenated with standard audio features, providing explicit distance-related information to the network
- Core assumption: The ratio between direct and reverberant sound components contains reliable information about source distance that can be extracted through simple spectral analysis
- Evidence anchors:
  - [section 3.1] "Distance cues can be extracted from the relationship between the direct and reverberant components of the captured audio signals"
  - [section 3.1] "We decided to include both direct and reverberant components ('DR', for compactness) extracted from the Omnidirectional audio channel"
  - [corpus] No direct evidence in corpus papers about using direct/reverberant decomposition for distance estimation
- Break condition: If the room acoustics are highly variable or the source distance range is too large, the direct-reverberant ratio may not provide reliable distance cues

### Mechanism 3
- Claim: Using cubemap representation and depth maps reduces visual distortions and provides explicit spatial information for better localization
- Mechanism: The model converts equirectangular video frames to cubemap representation to reduce distortions, then extracts depth maps using "Depth Anything" model. These depth features are combined with audio features through a multi-head cross-attention mechanism, providing explicit 3D spatial information to complement audio cues
- Core assumption: Cubemap representation preserves more spatial information than equirectangular format, and depth maps provide reliable 3D spatial information that can be effectively fused with audio features
- Evidence anchors:
  - [section 3.2] "We converted the video frames to cubemap representations" and "To leverage the visual modality in support of the distance estimation task, we extracted depth features from video frames"
  - [section 3.3] "The main motivation for employing the ViT and the MHCA unit is to try to leverage the spatial accuracy provided by the visual modality"
  - [corpus] No direct evidence in corpus papers about using cubemap representation or depth maps for audio-visual localization
- Break condition: If depth estimation is inaccurate or cubemap conversion introduces artifacts, the visual spatial information may mislead the model rather than help it

## Foundational Learning

- Concept: Multi-ACCDOA vector representation for simultaneous event detection and localization
  - Why needed here: This task requires predicting both the presence of sound events and their 3D positions (x,y,z) plus distance simultaneously, which the multi-ACCDOA format supports
  - Quick check question: How many dimensions does the output vector have for 3 tracks, 4 positional values, and 13 classes?

- Concept: Temporal ensemble strategy for improving spatial accuracy
  - Why needed here: The temporal ensemble aggregates predictions from overlapping time windows to improve spatial localization while maintaining detection accuracy
  - Quick check question: What spatial threshold is used to determine if predictions from different time windows correspond to the same event?

- Concept: Data augmentation techniques specific to audio-visual tasks
  - Why needed here: Audio-visual datasets are limited, requiring augmentation strategies like audio channel swap (ACS) and audio-visual channel swap (AVCS) to improve generalization
  - Quick check question: What visual transformation is applied consistently with the ACS technique to maintain audio-visual correspondence?

## Architecture Onboarding

- Component map:
  - Audio encoder: CNN-Conformer processing log-mel spectrograms + intensity vectors (+ direct/reverberant features in depth-cued model)
  - Visual encoder: ResNet50 + Conformer for frame embeddings (equirectangular in AV-Conformer, cubemap in depth-cued model)
  - Frame encoder (depth-cued only): ViT processing central frame patches + depth map patches
  - Fusion mechanism: Concatenation (AV-Conformer) or multi-head cross-attention (depth-cued model)
  - Output layer: Fully connected layers predicting multi-ACCDOA vectors

- Critical path: Audio features → Audio encoder → Fusion → Output
  Visual features → Visual encoder → Fusion → Output
  (Depth features → Frame encoder → Fusion → Output in depth-cued model)

- Design tradeoffs:
  - Simple concatenation vs cross-attention: Concatenation is simpler but cross-attention may better exploit spatial visual information
  - Cubemap vs equirectangular: Cubemap reduces distortions but requires conversion and uses different encoder
  - Standard features vs direct/reverberant: Direct/reverberant features improve distance estimation but add complexity

- Failure signatures:
  - Poor F1 score with good localization: Model detects events but misses rare classes (especially "Knock")
  - Good F1 but poor localization: Model detects events but fails to accurately localize them
  - Distance estimation errors: Model predicts incorrect distances, possibly due to inadequate distance cues

- First 3 experiments:
  1. Compare AV-Conformer with and without direct/reverberant features to measure their impact on RDE
  2. Test cubemap conversion on AV-Conformer to isolate the effect of cubemap representation
  3. Evaluate temporal ensemble on depth-cued model to see if it improves spatial accuracy without hurting F1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the Depth-cued model perform worse on F1 score compared to the AV-Conformer despite using additional depth features?
- Basis in paper: [explicit] The paper states the Depth-cued model achieved a lower F1 score (30.7%) compared to AV-Conformer (40.8%), and suggests this may be due to rare sound classes not being detected.
- Why unresolved: The paper mentions this is "may be caused by sound classes that rarely appear in the training set" but doesn't provide detailed analysis to confirm this hypothesis.
- What evidence would resolve it: Detailed class-wise analysis showing which specific sound classes are being missed by the Depth-cued model, along with analysis of whether these correspond to rare classes in the training data.

### Open Question 2
- Question: How do the direct and reverberant audio features specifically contribute to distance estimation performance?
- Basis in paper: [explicit] The paper mentions these features were added to improve distance estimation but doesn't provide ablation studies showing their individual contribution.
- Why unresolved: While the paper states these features were added to improve distance estimation, no experiments were conducted to isolate their specific impact on the RDE metric.
- What evidence would resolve it: An ablation study comparing the Depth-cued model with and without the direct/reverberant features, showing the specific impact on RDE and other metrics.

### Open Question 3
- Question: Would the temporal ensemble strategy improve performance on rare sound classes like "Knock" and "Bell"?
- Basis in paper: [inferred] The paper notes that the Depth-cued model fails to detect "Knock" sounds entirely and rarely detects "Bell" sounds, but doesn't test whether temporal ensemble could help with these specific classes.
- Why unresolved: While the paper implements an ensemble of all three models to recover performance on rare classes, it doesn't specifically test if the temporal ensemble applied to the Depth-cued model alone would help with these classes.
- What evidence would resolve it: Testing the temporal ensemble strategy specifically on the Depth-cued model to see if it improves detection of rare classes while maintaining the lower RDE.

### Open Question 4
- Question: How would the model performance change with different cubemap conversion methods or depth estimation models?
- Basis in paper: [explicit] The paper mentions using a specific cubemap conversion and the "Depth Anything" model, but doesn't explore alternatives.
- Why unresolved: The paper implements one specific approach to cubemap conversion and depth estimation but doesn't compare it against other methods or analyze the sensitivity of results to these choices.
- What evidence would resolve it: Comparative experiments using different cubemap conversion methods and depth estimation models to evaluate their impact on the overall system performance.

## Limitations
- Limited test set size and single dataset evaluation reduce generalizability
- Incomplete ablation studies - specific contribution of cubemap conversion versus depth maps remains unclear
- Failure to detect rare classes like "Knock" suggests model struggles with class imbalance

## Confidence
- High Confidence: The core architectural approach of combining audio-visual features through concatenation or cross-attention is well-supported by the results
- Medium Confidence: The specific claims about why direct/reverberant features improve distance estimation are plausible but lack direct experimental validation
- Low Confidence: The paper doesn't provide sufficient evidence that the temporal ensemble strategy consistently improves localization without degrading detection performance

## Next Checks
1. **Ablation study isolating cubemap effects:** Train identical models with and without cubemap conversion while keeping all other factors constant to measure the specific contribution of cubemap representation to localization accuracy

2. **Cross-dataset validation:** Test the trained models on an independent audio-visual dataset with different room acoustics and camera perspectives to assess generalizability beyond STARSS23

3. **Class-balanced evaluation:** Create a balanced subset of the test data (including sufficient "Knock" examples) to determine if the model's poor performance on rare classes is due to class imbalance rather than fundamental limitations in detection capability