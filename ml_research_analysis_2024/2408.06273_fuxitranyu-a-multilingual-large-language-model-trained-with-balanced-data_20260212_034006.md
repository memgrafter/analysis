---
ver: rpa2
title: 'FuxiTranyu: A Multilingual Large Language Model Trained with Balanced Data'
arxiv_id: '2408.06273'
source_url: https://arxiv.org/abs/2408.06273
tags:
- multilingual
- language
- languages
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FuxiTranyu is an 8B-parameter multilingual large language model
  trained from scratch on 600B balanced tokens covering 43 natural and 16 programming
  languages. The model employs balanced sampling and high-quality pre-training data
  to mitigate cross-lingual performance gaps seen in other multilingual models.
---

# FuxiTranyu: A Multilingual Large Language Model Trained with Balanced Data

## Quick Facts
- arXiv ID: 2408.06273
- Source URL: https://arxiv.org/abs/2408.06273
- Reference count: 40
- Primary result: FuxiTranyu-8B achieves competitive multilingual performance through balanced training data, outperforming BLOOM-7B and PolyLM-13B on XCOPA and XStoryCloze

## Executive Summary
FuxiTranyu is an 8B-parameter multilingual large language model trained from scratch on 600B balanced tokens covering 43 natural and 16 programming languages. The model employs balanced sampling and high-quality pre-training data to mitigate cross-lingual performance gaps seen in other multilingual models. FuxiTranyu-8B achieves competitive results on multilingual discriminative tasks, outperforming BLOOM-7B and PolyLM-13B on XCOPA and XStoryCloze. On generative tasks, FuxiTranyu-8B-SFT and FuxiTranyu-8B-DPO outperform Llama-2-Chat-7B and Mistral-7B-Instruct-v0.1 on summarization and achieve strong translation performance, particularly in non-English directions. Neuron and representation interpretability analyses show that FuxiTranyu learns more language-agnostic representations than BLOOM-7B1, attributed to its balanced pre-training data, with deeper layers becoming more language-specific and attention components gaining importance in later layers.

## Method Summary
FuxiTranyu is trained using a GPT-2-style architecture with modifications including untied input/output embeddings, RoPE position encodings, RMSNorm normalization, and GeLU activation. The model is pre-trained on 606B tokens across 43 natural and 16 programming languages using balanced sampling to ensure each language receives sufficient token exposure. Training uses 32 A800 GPUs with AdamW optimizer, cosine learning rate scheduler, ZeRO-2, and FlashAttention V2. The base model is then fine-tuned with SFT and DPO using curated instruction and preference datasets. The balanced data approach aims to reduce cross-lingual performance gaps and promote more uniform language-agnostic representations.

## Key Results
- FuxiTranyu-8B outperforms BLOOM-7B and PolyLM-13B on XCOPA and XStoryCloze multilingual benchmarks
- FuxiTranyu-8B-SFT and FuxiTranyu-8B-DPO outperform Llama-2-Chat-7B and Mistral-7B-Instruct-v0.1 on summarization tasks
- Strong translation performance in non-English directions compared to other multilingual models
- Neuron and representation analyses show FuxiTranyu learns more language-agnostic representations than BLOOM-7B1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Balanced multilingual data allocation improves cross-lingual consistency and reduces language-specific neuron activation.
- Mechanism: During pre-training, balanced sampling ensures each language receives sufficient token exposure, preventing dominance by high-resource languages and promoting more uniform language-agnostic representations.
- Core assumption: Token balance directly translates to balanced neuron activation patterns and similar representation spaces across languages.
- Evidence anchors:
  - [abstract]: "Both neuron and representation interpretability analyses reveal that FuxiTranyu achieves consistent multilingual representations across languages."
  - [section 6.1]: "To better understand why models generate specific outputs for specific inputs in a multilingual context, we aim to reveal the model's internal mechanisms by evaluating the importance of neurons."
  - [corpus]: Weak. The corpus contains papers on multilingual LLMs but lacks direct evidence for balanced sampling improving cross-lingual consistency.
- Break condition: If a language is severely under-resourced, balanced sampling may not provide enough exposure to achieve parity with high-resource languages.

### Mechanism 2
- Claim: Deep layers in the model become more language-specific, while attention components gain importance in later layers for multilingual tasks.
- Mechanism: Shallow layers handle general multilingual patterns, while deeper layers specialize to language-specific nuances; attention mechanisms become more critical for disambiguating between languages.
- Core assumption: Language-specific information is encoded progressively deeper in the network, and attention is the primary mechanism for resolving cross-lingual ambiguity.
- Evidence anchors:
  - [abstract]: "Both neuron and representation interpretability analyses show that FuxiTranyu learns more language-agnostic representations than BLOOM-7B1, attributed to its balanced pre-training data, with deeper layers becoming more language-specific and attention components gaining importance in later layers."
  - [section 6.3]: "Our findings mirror previous conclusions: components in shallow layers exhibit low importance, whereas those in deep layers show high significance."
  - [corpus]: Weak. Corpus contains multilingual LLM research but no direct evidence for layer-wise language specificity or attention component importance.
- Break condition: If the model is shallow (few layers), language-specific distinctions may not emerge, leading to poorer multilingual performance.

### Mechanism 3
- Claim: Separating input and output embeddings (untied embeddings) improves multilingual performance by allowing distinct representations for input and output languages.
- Mechanism: Untied embeddings prevent interference between input and output representations, enabling more accurate generation across languages.
- Core assumption: Tied embeddings can cause cross-talk between input and output languages, hindering multilingual generation.
- Evidence anchors:
  - [section 3.2]: "We opt to separate the weights of the input and output embeddings to enhance performance, despite the resulting increase in total model parameters and memory usage."
  - [abstract]: Does not explicitly mention untied embeddings, but notes superior multilingual generative performance.
  - [corpus]: Weak. No corpus evidence directly linking untied embeddings to multilingual generative improvements.
- Break condition: If the model is very small (few parameters), the increased parameter count from untied embeddings may lead to overfitting or inefficiency.

## Foundational Learning

- Concept: Balanced sampling and its effect on multilingual representation learning
  - Why needed here: Understanding how balanced sampling influences cross-lingual consistency and neuron activation patterns is critical for replicating FuxiTranyu's success.
  - Quick check question: How does balanced sampling differ from proportional sampling, and what are the implications for low-resource language representation?

- Concept: Layer-wise specialization and attention mechanism importance in multilingual models
  - Why needed here: Recognizing how different layers and components handle multilingual data is essential for diagnosing performance issues and optimizing model architecture.
  - Quick check question: Why do attention components become more important in deeper layers for multilingual tasks?

- Concept: Untied embeddings and their impact on multilingual generation
  - Why needed here: Understanding the tradeoffs of untied embeddings helps in making informed architectural decisions for multilingual models.
  - Quick check question: What are the potential downsides of using untied embeddings, and when might they not be beneficial?

## Architecture Onboarding

- Component map: Base model (8B parameters, GPT-2 style) -> Untied input/output embeddings -> RoPE position encodings (4096 context) -> RMSNorm normalization -> GeLU activation -> 58 pre-training checkpoints

- Critical path:
  1. Balanced data collection and preprocessing
  2. Tokenizer training with fertility evaluation
  3. Pre-training with balanced sampling and optimized hyperparameters
  4. Instruction tuning and DPO for alignment

- Design tradeoffs:
  - Untied embeddings vs. tied embeddings: increased parameters vs. potentially better multilingual generation
  - Balanced sampling vs. proportional sampling: improved cross-lingual consistency vs. potentially slower convergence
  - GeLU vs. SwiGLU: similar performance with fewer parameters vs. potentially better performance with more parameters

- Failure signatures:
  - Poor performance on low-resource languages: likely due to insufficient token exposure during pre-training
  - Cross-lingual inconsistency: may indicate imbalanced data or insufficient multilingual representation learning
  - Overfitting on high-resource languages: suggests need for stronger regularization or more balanced sampling

- First 3 experiments:
  1. Evaluate fertility of tokenizer on target languages to ensure efficient tokenization
  2. Analyze neuron importance across layers and languages to identify potential imbalances
  3. Test cross-lingual transfer performance to assess representation consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the balanced data sampling strategy specifically impact the language-agnostic representations learned by FuxiTranyu compared to models trained on imbalanced data?
- Basis in paper: [explicit] The paper states that FuxiTranyu achieves consistent multilingual representations across languages due to its balanced pre-training data, contrasting with models like BLOOM-7B that suffer from cross-lingual inconsistency caused by imbalanced language resources.
- Why unresolved: While the paper claims balanced data leads to more language-agnostic representations, it doesn't provide a detailed quantitative analysis of how different degrees of data imbalance would affect the learned representations. The exact relationship between data balance and representation consistency remains unexplored.
- What evidence would resolve it: A controlled experiment training multiple FuxiTranyu variants with varying degrees of data imbalance (e.g., 10%, 30%, 50%, 70%, 90% English data) and comparing their representation similarity scores across languages would provide concrete evidence of the impact of data balance.

### Open Question 2
- Question: What are the specific mechanisms by which FuxiTranyu's architecture (e.g., untied embeddings, RoPE position encodings) contribute to its superior multilingual performance compared to other models?
- Basis in paper: [explicit] The paper describes several architectural modifications including untied embeddings, RoPE position encodings, and pre-normalization. It claims these contribute to FuxiTranyu's performance but doesn't provide ablation studies or detailed analysis of their individual impacts.
- Why unresolved: The paper presents the architectural choices as contributing factors but doesn't isolate their effects through systematic ablation experiments. It's unclear which specific modifications are most critical for the multilingual capabilities.
- What evidence would resolve it: Training multiple variants of FuxiTranyu with individual architectural components removed or modified (e.g., tied vs untied embeddings, different position encoding schemes) and comparing their multilingual performance on benchmarks like XCOPA and XStoryCloze would reveal which modifications are most impactful.

### Open Question 3
- Question: How does the neuron importance analysis correlate with actual model performance on low-resource languages, and can this be used to predict or improve performance on under-represented languages?
- Basis in paper: [explicit] The neuron analysis reveals that languages like Bengali and Tamil exhibit diminished importance in deep layers and underperform on tasks, correlating with their limited representation in pre-training data. However, the paper doesn't explore whether this analysis can be used predictively or to guide improvements.
- Why unresolved: While the paper identifies a correlation between neuron importance and performance for specific languages, it doesn't investigate whether this relationship is systematic enough to predict performance on other under-represented languages or to guide targeted improvements in model architecture or training.
- What evidence would resolve it: Conducting neuron importance analysis on additional low-resource languages not included in the current study, then comparing predicted performance based on neuron importance scores with actual task performance would validate whether this analysis can serve as a predictive tool. Additionally, testing whether targeted training or architectural modifications to increase neuron importance for specific languages improves their performance would demonstrate practical applications.

## Limitations

- The claims about balanced sampling improving cross-lingual consistency lack direct experimental validation through controlled ablation studies
- The architectural modifications (untied embeddings, RoPE position encodings) are presented as beneficial but their individual contributions are not tested through ablation experiments
- The neuron importance analysis shows correlation with performance but doesn't establish predictive capability or guide targeted improvements for under-represented languages

## Confidence

**High Confidence**: The experimental results demonstrating FuxiTranyu-8B's competitive performance on multilingual benchmarks (XCOPA, XStoryCloze, translation, summarization) are well-supported by the data presented. The comparison methodology against established models like BLOOM-7B and Llama-2-Chat-7B is appropriate and the results are statistically significant.

**Medium Confidence**: The neuron and representation interpretability analyses showing more consistent multilingual representations are credible but the attribution to balanced pre-training data is inferential rather than experimentally proven. The layer-wise importance patterns follow established trends in LLM research but haven't been validated specifically for the balanced sampling approach.

**Low Confidence**: The mechanistic claims about why balanced sampling works (cross-lingual consistency, reduced language-specific activation) and why untied embeddings help multilingual generation are speculative. These require direct experimental validation through controlled ablation studies that are not provided in the paper.

## Next Checks

1. **Controlled Sampling Experiment**: Train two identical models with the same architecture and data but different sampling strategies (balanced vs. proportional by language size) and compare their cross-lingual consistency and neuron activation patterns directly. This would validate whether balanced sampling is actually responsible for the observed improvements.

2. **Embedding Ablation Study**: Train FuxiTranyu variants with tied vs. untied embeddings using identical training regimes and compare multilingual generative performance across all target languages. This would directly test whether the increased parameter count provides measurable benefits.

3. **Layer-wise Intervention Analysis**: Perform targeted interventions on specific layers (freezing, masking, or replacing attention mechanisms) during inference on multilingual tasks to quantify the actual contribution of deeper layers and attention components to cross-lingual performance. This would validate the claimed layer-wise specialization mechanism.