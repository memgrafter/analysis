---
ver: rpa2
title: 'Libra-Leaderboard: Towards Responsible AI through a Balanced Leaderboard of
  Safety and Capability'
arxiv_id: '2412.18551'
source_url: https://arxiv.org/abs/2412.18551
tags:
- safety
- single
- prompt
- performance
- librai
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Libra-Leaderboard, a comprehensive framework
  for evaluating large language models (LLMs) that balances performance and safety.
  Unlike existing leaderboards that prioritize capabilities, Libra-Leaderboard uses
  a distance-to-optimal-score method to incentivize balanced optimization of both
  safety and performance.
---

# Libra-Leaderboard: Towards Responsible AI through a Balanced Leaderboard of Safety and Capability

## Quick Facts
- arXiv ID: 2412.18551
- Source URL: https://arxiv.org/abs/2412.18551
- Reference count: 40
- Primary result: Introduces a comprehensive framework for evaluating LLMs that balances safety and performance, finding significant safety gaps even among state-of-the-art models

## Executive Summary
This paper presents Libra-Leaderboard, a novel framework designed to address the critical gap in current LLM evaluation systems that prioritize capabilities while neglecting safety. The framework introduces a distance-to-optimal-score method that incentivizes balanced optimization of both safety and performance dimensions. Through comprehensive evaluation of 26 mainstream LLMs across 57 safety datasets covering four key safety dimensions, the study reveals significant safety performance gaps among even the most advanced models, with task scores ranging from 0.31 to 0.99 and substantial discrepancies across different safety tasks.

## Method Summary
Libra-Leaderboard combines safety and performance evaluation using a distance-to-optimal-score method that measures how close models are to the ideal point where both dimensions are maximized. The framework includes Libra-Eval, a unified safety evaluation system with 57 datasets covering direct risky prompts, adversarial attacks, instruction hierarchy attacks, and over-sensitivity. It features an interactive safety arena for real-time adversarial testing with 12 different attack methods, and employs a dynamic dataset strategy with quarterly updates to prevent data contamination. The evaluation uses LLM-as-a-Judge methods with structured JSON outputs and caching for reproducibility.

## Key Results
- Significant safety performance gaps found among state-of-the-art models, with average task scores ranging from 0.31 to 0.99
- Commercial models generally outperform open-source models but still exhibit notable weaknesses in handling specific safety issues
- The distance-to-optimal-score method successfully incentivizes balanced optimization, preventing models from excelling in one dimension at the expense of others
- The safety arena with adversarial modifications provides valuable real-world robustness testing beyond static benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The distance-to-optimal-score method incentivizes balanced optimization of safety and performance by rewarding models that achieve similar scores in both dimensions.
- Mechanism: Instead of averaging safety and performance scores, Libra-Leaderboard uses a metric that measures how close a model's scores are to the ideal point (1,1) where both safety and performance are maximized. This approach inherently rewards balanced improvements because increasing one metric alone contributes less than improving both jointly.
- Core assumption: Models can be effectively optimized for both safety and performance simultaneously without significant trade-offs.
- Evidence anchors:
  - [abstract] "Libra-Leaderboard uses a distance-to-optimal-score method to calculate the overall rankings. This approach incentivizes models to achieve a balance rather than excelling in one dimension at the expense of some other ones."
  - [section 3.4] "We adopt the third approach because it aligns with the principle that a true measure of overall quality should reflect how well both key metrics (safety and performance) are cultivated together."
- Break condition: If safety and performance optimization are fundamentally conflicting objectives, models might optimize for one dimension at the expense of the other.

### Mechanism 2
- Claim: The dynamic dataset strategy with quarterly updates prevents data contamination while maintaining evaluation relevance.
- Mechanism: In each evaluation round, a subset of safety datasets is sampled for testing, and all test instances from the previous round are released. This creates a rolling window of evaluation data that balances transparency with preventing models from being trained on test data.
- Core assumption: Regular dataset updates can keep pace with evolving model capabilities and attack methods.
- Evidence anchors:
  - [section 3.1] "To prevent data contamination, we implemented a quarterly update strategy. In each evaluation round, we sample a subset of safety datasets for testing and release all test instances from the previous round."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.489" - Moderate correlation with related work on comprehensive evaluation frameworks.
- Break condition: If model capabilities evolve faster than the update cycle, or if new safety threats emerge that aren't captured in the evaluation framework.

### Mechanism 3
- Claim: The integrated safety arena with adversarial modifications provides real-world robustness testing that complements static benchmarks.
- Mechanism: The arena allows users to interact with models using prompts modified by 12 different adversarial attack methods, providing real-time feedback and dynamic testing that static benchmarks cannot capture. This creates a feedback loop where user interactions inform the leaderboard rankings.
- Core assumption: Adversarial testing in interactive settings captures important safety dimensions not measured by static benchmarks.
- Evidence anchors:
  - [section 3.2] "The arena incorporates a suite of adversarial attack methods that users can apply to their input prompts. These modifications simulate realistic challenges that LLMs may encounter in deployment."
  - [section 4] "Commercial models generally outperform open-source models, but still exhibit notable weaknesses in handling specific safety issues."
- Break condition: If adversarial testing doesn't correlate well with real-world safety performance, or if user feedback introduces biases.

## Foundational Learning

- Concept: Distance-to-optimal-score calculation
  - Why needed here: Understanding how Libra-Leaderboard combines safety and performance scores differently from simple averaging is crucial for interpreting results and designing future improvements.
  - Quick check question: If a model scores (0.9, 0.9) and another scores (1.0, 0.7), which one would rank higher under the distance-to-optimal-score method?

- Concept: Adversarial attack taxonomy
  - Why needed here: The safety arena uses multiple types of adversarial modifications, and understanding these helps in evaluating model robustness and designing new safety tests.
  - Quick check question: What's the difference between persona modulation and deep inception attacks in terms of how they attempt to bypass safety guardrails?

- Concept: Multi-turn evaluation methodology
  - Why needed here: Many safety tasks require conversation context, and understanding how Libra-Eval handles multi-turn prompts is important for proper interpretation of results.
  - Quick check question: How does multi-turn evaluation differ from single-turn evaluation in terms of safety assessment?

## Architecture Onboarding

- Component map: Frontend UI -> Backend Libra-Eval -> Data pipeline with quarterly updates -> Evaluation engine -> Safety arena -> User feedback system
- Critical path: User query → Model response → Safety evaluation → Score aggregation → Leaderboard update
- Design tradeoffs: Balance between comprehensive safety coverage and evaluation speed; between dataset freshness and reproducibility
- Failure signatures:
  - If safety scores don't correlate with real-world model behavior
  - If adversarial modifications are too predictable or easily bypassed
  - If dataset contamination occurs despite quarterly updates

- First 3 experiments:
  1. Run Libra-Eval on a simple model (like Llama-3-8B) to understand the evaluation pipeline and scoring output
  2. Test the safety arena with basic adversarial modifications to see how models respond to different attack types
  3. Compare results from distance-to-optimal-score vs. simple averaging on the same dataset to verify the intended incentive structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between safety and capability in the distance-to-optimal-score method?
- Basis in paper: [explicit] The paper mentions that the current scoring system uses a distance-to-optimal-score approach to incentivize balanced improvements, but doesn't specify what the optimal balance should be.
- Why unresolved: The paper doesn't provide empirical evidence or theoretical justification for why their specific scoring method is optimal. Different weightings between safety and capability could potentially lead to better outcomes.
- What evidence would resolve it: Comparative experiments testing different scoring weightings and their effects on model development would help determine the optimal balance.

### Open Question 2
- Question: How can data contamination be effectively prevented in dynamic leaderboards?
- Basis in paper: [explicit] The paper acknowledges data contamination as a concern and implements quarterly updates, but doesn't fully address whether this is sufficient.
- Why unresolved: The quarterly update strategy may not be frequent enough to prevent all contamination, especially as models are continuously trained on new data.
- What evidence would resolve it: Longitudinal studies tracking model performance on held-out test sets over time would help evaluate the effectiveness of the current contamination prevention strategy.

### Open Question 3
- Question: How do different adversarial attack methods compare in terms of effectiveness and efficiency?
- Basis in paper: [inferred] The paper implements 12 different adversarial attack methods but doesn't compare their relative effectiveness or provide guidelines for choosing between them.
- Why unresolved: Without systematic comparison, it's unclear which attack methods are most valuable for evaluating model safety.
- What evidence would resolve it: Controlled experiments comparing the success rates and computational costs of different attack methods would help determine which are most useful.

### Open Question 4
- Question: How transferable are safety evaluation results across different languages and cultural contexts?
- Basis in paper: [explicit] The paper mentions some bilingual models and includes multilingual attack methods, but doesn't examine cross-cultural differences in safety evaluation.
- Why unresolved: Safety concerns and appropriate responses may vary significantly across different cultural contexts.
- What evidence would resolve it: Cross-cultural studies testing model safety across different languages and cultural contexts would help determine the generalizability of current evaluation methods.

## Limitations

- The distance-to-optimal-score method's effectiveness in preventing capability-safety trade-offs remains theoretical and needs long-term validation
- The safety arena's adversarial modifications may not capture all real-world attack vectors, particularly sophisticated multi-turn jailbreak attempts
- Quarterly dataset updates may be insufficient given rapid model evolution, potentially creating evaluation lag

## Confidence

- High confidence in core framework design and initial evaluation results
- Medium confidence in generalizability given the limited model sample size and single evaluation snapshot

## Next Checks

1. Track the same 26 models over 3-6 months to verify whether Libra-Leaderboard rankings remain stable or if models begin optimizing specifically for the distance-to-optimal-score metric
2. Test Libra-Eval's adversarial robustness by having red teams attempt to systematically bypass safety guardrails using techniques not covered in the current 12 attack types
3. Compare Libra-Leaderboard rankings with real-world incident data from deployed models to validate that higher-ranked models actually experience fewer safety failures in production