---
ver: rpa2
title: Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action
  Localization
arxiv_id: '2407.07024'
source_url: https://arxiv.org/abs/2407.07024
tags:
- action
- videos
- video
- self-training
- ov-tal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores using self-training with unlabeled YouTube videos
  to improve open-vocabulary temporal action localization (OV-TAL). Current OV-TAL
  methods rely on small labeled datasets, limiting generalization.
---

# Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action Localization

## Quick Facts
- arXiv ID: 2407.07024
- Source URL: https://arxiv.org/abs/2407.07024
- Authors: Jeongseok Hyun; Su Ho Han; Hyolim Kang; Joon-Young Lee; Seon Joo Kim
- Reference count: 40
- Primary result: Self-training with 100k unlabeled YouTube videos significantly improves cross-category and cross-domain generalization in open-vocabulary temporal action localization compared to using only labeled data

## Executive Summary
This paper addresses the challenge of improving generalization in open-vocabulary temporal action localization (OV-TAL) by leveraging unlabeled web videos through self-training. Current OV-TAL methods are limited by small labeled datasets, which restrict their ability to generalize to novel action categories and domains. The authors propose a two-stage self-training approach that first trains a class-agnostic action localizer on labeled data to generate pseudo-labels for unlabeled videos, then retrains the localizer using the combined dataset. Experiments demonstrate significant improvements in both cross-category and cross-domain generalization when using open-domain web videos compared to in-domain data, achieving state-of-the-art results without specialized loss functions or pseudo-labeling strategies.

## Method Summary
The approach involves two main stages: first, a class-agnostic action localizer is trained on labeled TAL datasets (ActivityNet, THUMOS14, FineAction) using a VLM (ViFi-CLIP) for feature extraction. This model detects action instances with actionness scores and temporal boundaries but doesn't classify them. In the second stage, this trained localizer generates pseudo-labels on unlabeled K600-derived YouTube videos, which are then combined with the original labeled data for retraining using a Mean Teacher framework. The final classification uses a separate open-vocabulary classifier with the VLM text encoder. The method employs score fusion via geometric mean of actionness and category scores, and critically freezes the VLM during training to prevent overfitting to base categories.

## Key Results
- Self-training with 100k unlabeled YouTube videos outperforms using in-domain data on both base and novel action categories
- Open-domain web videos provide better generalization than in-domain data from target benchmarks
- Freezing the pre-trained VLM and using it as a feature extractor prevents overfitting to base categories
- The approach achieves state-of-the-art results without specialized loss functions or pseudo-labeling strategies

## Why This Works (Mechanism)

### Mechanism 1
Self-training with unlabeled web videos improves cross-category and cross-domain generalization by exposing the model to a broader distribution of actions and domains. In stage 1, a class-agnostic action localizer trained on labeled data generates pseudo-labels for unlabeled videos. In stage 2, these pseudo-labels combined with original labeled data retrain the localizer, expanding its exposure beyond the limited labeled set. The core assumption is that pseudo-labels generated by the initial class-agnostic action localizer are sufficiently accurate to provide meaningful training signal. Break condition: Pseudo-label quality degrades significantly (e.g., below 50% IoU with ground truth), causing the model to learn incorrect patterns.

### Mechanism 2
Using open-domain web videos for self-training provides better generalization than in-domain data from the target benchmark. Open-domain videos contain a wider variety of action categories and domains not present in the target benchmark, forcing the model to learn more robust and generalizable features. The core assumption is that diversity in training data is more important for generalization than domain alignment with the target benchmark. Break condition: If open-domain videos are too noisy or contain irrelevant content, the model may struggle to learn useful patterns.

### Mechanism 3
Freezing the vision-language model (VLM) and using it as a frozen feature extractor prevents overfitting to base categories and maintains generalization to novel categories. By keeping the VLM frozen, the model avoids fine-tuning it on the limited base categories, preserving its ability to recognize a wider range of actions through its pre-trained knowledge. The core assumption is that the pre-trained VLM contains sufficient knowledge to recognize novel action categories without further fine-tuning. Break condition: If the pre-trained VLM lacks sufficient knowledge of the novel action categories, freezing it may limit the model's performance.

## Foundational Learning

- **Vision-Language Models (VLMs) like CLIP and ViFi-CLIP**: These provide pre-trained visual and text encoders that can be used to extract features from videos and action names, enabling open-vocabulary action localization. Quick check: What is the main advantage of using a pre-trained VLM like ViFi-CLIP over training a model from scratch for action localization?

- **Self-training**: This allows the model to leverage unlabeled web videos to improve its generalization ability beyond the limited labeled data. Quick check: What are the two main stages of the self-training approach used in this paper?

- **Class-agnostic action localization**: This detects action instances without classifying them, allowing the model to be trained on a broader range of actions and then classify them using a separate open-vocabulary classifier. Quick check: What is the difference between class-agnostic action localization and traditional action classification?

## Architecture Onboarding

- **Component map**: Video frames → VLM video encoder → Class-agnostic action localizer → Action instances → VLM text encoder → Action classification → Final output

- **Critical path**: Video frames are processed through the VLM video encoder to extract features, which are then passed to the class-agnostic action localizer to detect action instances. These instances are classified using the VLM text encoder, with final scores computed through geometric mean fusion.

- **Design tradeoffs**: Freezing VLM vs. fine-tuning - freezing preserves pre-trained knowledge but may limit adaptation to specific action categories; Open-domain vs. in-domain data - open-domain provides more diversity but may introduce noise; Pseudo-label threshold - higher thresholds improve quality but reduce quantity.

- **Failure signatures**: Poor cross-category generalization - model performs well on base categories but poorly on novel categories; Overfitting to target benchmark - model performs well on in-domain data but poorly on cross-dataset evaluation; Low pseudo-label quality - model fails to improve with self-training despite increased data.

- **First 3 experiments**: 1) Evaluate baseline performance on cross-category and cross-dataset benchmarks without self-training; 2) Test self-training with in-domain data to verify improvement over baseline; 3) Test self-training with open-domain data to compare against in-domain results and measure scalability.

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal balance between using in-domain versus open-domain videos for self-training in OV-TAL? The paper compares using in-domain data versus open-domain data and shows different performance characteristics, but doesn't determine the optimal combination or ratio. The paper only tests extreme cases (pure in-domain vs pure open-domain) and doesn't explore hybrid approaches or optimal mixing ratios. Systematic experiments varying the ratio of in-domain to open-domain videos in self-training, measuring the trade-off between domain specificity and generalization, would resolve this.

### Open Question 2
How do different video VLMs (beyond CLIP and ViFi-CLIP) impact the performance of OV-TAL when integrated with self-training? The paper mentions that "this architecture supports various CLIP-like VLMs" and shows some results with ViCLIP and ViFi, but doesn't comprehensively evaluate other VLMs like InternVL, MAViT, or proprietary models. Systematic evaluation of multiple VLMs (including newer ones) with self-training, comparing their effectiveness and identifying which characteristics make certain VLMs more suitable for OV-TAL, would resolve this.

### Open Question 3
What is the impact of video duration on the effectiveness of self-training for OV-TAL? The paper mentions that "FineAction contains videos from diverse domains" and shows different results compared to THUMOS14, which is limited to sports. The observation that "FineAction's action instances of CN are generally longer than others" suggests duration may be a factor. The paper doesn't systematically analyze how video duration affects the quality of pseudo-labels or the effectiveness of self-training. Controlled experiments varying video duration in self-training datasets, measuring the relationship between average action duration and cross-category/domain generalization performance, would resolve this.

### Open Question 4
How does the quality of pseudo-labels affect the scalability of self-training in OV-TAL? The paper performs sensitivity analysis on pseudo-label thresholds and shows positive correlation between actionness scores and tIoU with ground truth, but doesn't explore how pseudo-label quality impacts scalability limits. The paper demonstrates that pseudo-labeling works but doesn't investigate the relationship between pseudo-label quality and the maximum scalable dataset size. Experiments varying pseudo-label quality (through different thresholding strategies or filtering methods) while scaling dataset size, measuring the point at which lower quality pseudo-labels begin to degrade performance, would resolve this.

## Limitations

- The paper lacks detailed analysis of pseudo-label quality metrics and their correlation with final performance gains, despite the mechanism relying on accurate pseudo-labels
- Scalability claims are primarily demonstrated on a single VLM (ViFi-CLIP) and ActionFormer architecture, raising questions about generalizability to other architectures or VLMs
- The impact of unlabeled video volume on performance gains is shown qualitatively but lacks systematic analysis of diminishing returns or optimal data quantities

## Confidence

- **High Confidence**: The core finding that self-training with unlabeled web videos improves generalization over using only labeled data is well-supported by experimental results across multiple benchmarks
- **Medium Confidence**: The claim that open-domain videos outperform in-domain data is supported but could benefit from more rigorous analysis of video diversity and quality metrics
- **Low Confidence**: The assertion that freezing the VLM is universally optimal across all scenarios lacks comprehensive ablation studies across different VLMs and fine-tuning strategies

## Next Checks

1. **Pseudo-label Quality Analysis**: Conduct detailed analysis of pseudo-label quality metrics (e.g., tIoU distribution, precision-recall curves) across different video domains and thresholds to validate the core assumption about pseudo-label accuracy

2. **Architecture Generalization Study**: Test the self-training approach with alternative architectures (e.g., transformer-based localizers, different VLMs) to verify that improvements are not specific to the chosen implementation

3. **Data Volume Scalability Analysis**: Systematically evaluate performance gains across different volumes of unlabeled data (e.g., 10k, 50k, 100k, 200k videos) to identify optimal data quantities and saturation points