---
ver: rpa2
title: Augmented Bayesian Policy Search
arxiv_id: '2407.04864'
source_url: https://arxiv.org/abs/2407.04864
tags:
- policy
- function
- mean
- learning
- return
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Augmented Bayesian Search (ABS), a method
  that combines Bayesian Optimization (BO) and Reinforcement Learning (RL) for high-dimensional
  policy search. The key innovation is a novel mean function for the Gaussian Process
  (GP) prior that incorporates the action-value function of deterministic policies,
  bridging the gap between BO and policy gradient methods.
---

# Augmented Bayesian Policy Search

## Quick Facts
- arXiv ID: 2407.04864
- Source URL: https://arxiv.org/abs/2407.04864
- Reference count: 37
- This paper introduces Augmented Bayesian Search (ABS), a method that combines Bayesian Optimization (BO) and Reinforcement Learning (RL) for high-dimensional policy search.

## Executive Summary
This paper introduces Augmented Bayesian Search (ABS), a method that combines Bayesian Optimization (BO) and Reinforcement Learning (RL) for high-dimensional policy search. The key innovation is a novel mean function for the Gaussian Process (GP) prior that incorporates the action-value function of deterministic policies, bridging the gap between BO and policy gradient methods. ABS also features an adaptive aggregation scheme for multiple Q-function estimators. The authors theoretically ground their approach by deriving a bound on the impact of altering deterministic policies in Lipschitz MDPs. Empirically, ABS demonstrates competitive performance on high-dimensional MuJoCo locomotion tasks, outperforming existing BO schemes and establishing state-of-the-art performance for BO methods in this domain.

## Method Summary
ABS combines Gaussian Process Bayesian Optimization with reinforcement learning by introducing an advantage mean function that incorporates Q-function estimates from deterministic policies. The method uses an ensemble of Q-function approximators with adaptive aggregation based on validation performance. Local Bayesian Optimization via Maximum Probability of Descent (MPD) is employed to focus exploration in high-dimensional spaces. The approach is theoretically grounded in Lipschitz MDPs, with bounds on the impact of policy changes.

## Key Results
- ABS achieves state-of-the-art performance among Bayesian Optimization methods on high-dimensional MuJoCo tasks
- The adaptive aggregation of Q-function estimators outperforms both single critic and average ensembling baselines
- The advantage mean function improves Bayesian Optimization efficiency by incorporating MDP structure into the GP prior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Advantage Mean Function bridges the gap between Bayesian Optimization (BO) and deterministic policy gradient methods by aligning the GP posterior gradient with the deterministic policy gradient.
- Mechanism: The mean function bmϕ incorporates the action-value function of deterministic policies via the performance difference lemma. This steers the acquisition function to sample in regions where the residual term in Equation (5) is small, effectively focusing BO exploration on promising policy parameters.
- Core assumption: The MDP is (Lr, Lp)-Lipschitz and policies are Lπ-Lipschitz, ensuring the residual term is bounded by a function of ∥πx(s) - πθ(s)∥.
- Evidence anchors:
  - [abstract]: "a novel mean function for the Gaussian Process (GP) prior that incorporates the action-value function of deterministic policies"
  - [section]: "By using bmϕ, we leave the GP to model the residual term from Equation (5), which can be viewed as a second-order term"
  - [corpus]: Weak; corpus lacks direct evidence on Lipschitz MDPs or GP mean function design.
- Break condition: If the Lipschitz assumptions fail or the Q-function approximator is poor, the mean function no longer aligns the posterior gradient with the true policy gradient.

### Mechanism 2
- Claim: The adaptive aggregation of Q-function estimators improves both exploration and exploitation by weighting critics according to their validation performance.
- Mechanism: Multiple DroQ critics are trained in parallel. Their predictions are combined using softmax weights derived from eR2 validation scores, which measure how well each critic predicts returns for policies different from the central one. Resetting the worst critic reduces primacy bias and variance.
- Core assumption: The validation trajectories τ(s, πx(s); πθ) for s ~ dπx are not in the training set, making eR2 a proxy for generalization.
- Evidence anchors:
  - [abstract]: "adaptive aggregation scheme for multiple Q-function estimators"
  - [section]: "we use the coefficient of determination of emϕ on the dataset D... this method evaluates bQπθϕ on trajectories from policies other than πθ"
  - [corpus]: Weak; corpus does not discuss critic aggregation or validation schemes.
- Break condition: If validation and test distributions are too dissimilar, eR2 no longer correlates with test performance, and aggregation becomes unreliable.

### Mechanism 3
- Claim: Local Bayesian Optimization via Maximum Probability of Descent (MPD) reduces the number of samples needed in high-dimensional policy search by focusing on a subspace around the current central point.
- Mechanism: MPD iteratively samples acquisition points that maximize the probability of descent at θ, then moves θ along the most probable descent direction. The advantage mean function further constrains this search by incorporating MDP structure.
- Core assumption: The acquisition points z remain close to θ so that the residual term in Equation (5) remains bounded.
- Evidence anchors:
  - [abstract]: "local BO improves the handling of the high-dimensional spaces by promoting more targeted exploration"
  - [section]: "we demonstrate the behavior of the acquisition function of MPD... its tendency to keep the acquisition points z in the immediate vicinity of the central point θ"
  - [corpus]: Weak; corpus lacks direct evidence on MPD's local search behavior.
- Break condition: If the lengthscale hyperprior is too large or the covariance function overfits, acquisition points may drift far from θ, invalidating the residual bound.

## Foundational Learning

- Concept: Lipschitz continuity in MDPs
  - Why needed here: Guarantees that small changes in policy parameters lead to small changes in returns, enabling the residual term bound in Theorem 3.3.
  - Quick check question: In a Lipschitz MDP with Lr = 1, Lp = 0.1, Lπ = 0.2, and γ = 0.99, is the condition γLp(1 + Lπ) < 1 satisfied?

- Concept: Gaussian Process posterior with non-constant mean
  - Why needed here: The advantage mean function bmϕ replaces the constant prior, making the GP focus on modeling the residual term rather than the entire return function.
  - Quick check question: If the advantage mean function perfectly predicts the return, what does the GP posterior model?

- Concept: Performance difference lemma
  - Why needed here: Provides the mathematical link between policy returns and advantage functions, enabling the construction of the advantage mean function.
  - Quick check question: Write the performance difference lemma for deterministic policies in terms of dπx, dπθ, and Aπθ.

## Architecture Onboarding

- Component map: Policy parameter space -> GP prior (advantage mean + SE kernel) -> Central point θ -> rollout -> (bJ(πθ), bdπθ, trajectories) -> Q-function ensemble -> trained on replay buffer -> validation scores (eR2) -> Acquisition function -> samples z -> GP posterior update -> Adaptive aggregation -> weighted critic prediction -> new mean function
- Critical path: Central point rollout -> Q-function update -> acquisition point sampling -> GP posterior update -> θ update -> repeat
- Design tradeoffs:
  - Single vs ensemble critics: ensemble improves robustness but increases compute.
  - Fixed vs adaptive hyperpriors: adaptive reduces tuning burden but may be unstable early on.
  - Constant vs advantage mean: advantage improves BO efficiency but requires accurate Q-function.
- Failure signatures:
  - eR2 validation scores negative or near zero -> Q-function generalization poor.
  - Acquisition points far from θ -> residual bound invalid.
  - GP lengthscale too small -> overfitting, acquisition function noisy.
- First 3 experiments:
  1. Verify eR2 remains positive on held-out trajectories after a few BO iterations.
  2. Compare maximum return curves of ABS vs MPD on InvertedPendulum (low dim).
  3. Test effect of resetting worst critic by running with and without reset on Swimmer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ABS scale when using deep neural network policies instead of linear policies?
- Basis in paper: [inferred] The paper mentions that they were limited to linear policies due to computational considerations and suggests random projection methods and kernel functions for GPs as potential solutions for scaling to deep policies.
- Why unresolved: The paper only experiments with linear policies and does not provide empirical evidence for the performance of ABS with deep policies.
- What evidence would resolve it: Experimental results comparing ABS with linear and deep policies on high-dimensional tasks would demonstrate how well ABS scales with policy complexity.

### Open Question 2
- Question: How does the adaptive aggregation scheme for Q-function estimators affect the sample efficiency of ABS compared to other ensemble methods?
- Basis in paper: [explicit] The paper proposes an adaptive aggregation scheme using softmax weighting based on validation scores and compares it to average ensembling and single critic baselines in an ablation study.
- Why unresolved: While the ablation study shows the adaptive aggregation outperforms other methods, it does not quantify the impact on sample efficiency or compare to other ensemble methods in the literature.
- What evidence would resolve it: A detailed comparison of sample efficiency between ABS with adaptive aggregation and other ensemble-based RL methods on a range of tasks would provide insight into the effectiveness of the proposed aggregation scheme.

### Open Question 3
- Question: How sensitive is ABS to the choice of hyperparameters, particularly the learning rate and lengthscale prior for the GP?
- Basis in paper: [explicit] The paper performs a grid search over learning rates and lengthscale priors for different tasks and reports the best-performing hyperparameters.
- Why unresolved: The paper does not analyze the sensitivity of ABS to hyperparameter choices or provide guidelines for selecting appropriate hyperparameters for new tasks.
- What evidence would resolve it: An ablation study varying the learning rate and lengthscale prior across a range of values for each task would reveal the sensitivity of ABS to these hyperparameters and help establish guidelines for their selection.

## Limitations

- Empirical validation limited to 5 MuJoCo environments with relatively narrow comparison baselines
- Theoretical guarantees rely on Lipschitz MDP assumptions that may not hold in practice
- Computational complexity of ensemble Q-functions and adaptive aggregation may limit scalability

## Confidence

- Theoretical claims (Lipschitz bounds, residual analysis): High
- Advantage mean function mechanism: Medium - well-motivated but requires careful implementation of Q-function ensemble
- Adaptive aggregation scheme: Medium - innovative but validation metric correlation needs verification
- Empirical performance claims: Low-Medium - limited environment diversity and comparison baselines

## Next Checks

1. Test ABS performance degradation when ensemble Q-functions are initialized poorly or corrupted during training
2. Evaluate ABS on additional control tasks (e.g., HalfCheetah, Humanoid) and sparse-reward variants to assess robustness
3. Compare against modern policy gradient methods (PPO, SAC) to contextualize BO performance claims