---
ver: rpa2
title: One Step Diffusion via Shortcut Models
arxiv_id: '2410.12557'
source_url: https://arxiv.org/abs/2410.12557
tags:
- shortcut
- training
- diffusion
- step
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces shortcut models, a novel approach to generating
  high-quality samples in a single forward pass by conditioning the neural network
  on both the current noise level and the desired step size. This allows the model
  to skip ahead in the denoising process, overcoming the computational expense of
  iterative methods like diffusion and flow-matching.
---

# One Step Diffusion via Shortcut Models

## Quick Facts
- arXiv ID: 2410.12557
- Source URL: https://arxiv.org/abs/2410.12557
- Authors: Kevin Frans; Danijar Hafner; Sergey Levine; Pieter Abbeel
- Reference count: 11
- Primary result: Achieves competitive image generation quality with one-step sampling through conditioning on both noise level and desired step size

## Executive Summary
This paper introduces shortcut models, a novel approach to one-step diffusion that conditions the neural network on both the current noise level and the desired step size. Unlike previous methods requiring complex training regimes or multiple networks, shortcut models are trained in a single phase without schedules or careful warmups. The approach consistently outperforms prior methods including consistency models and reflow across various sampling step budgets, achieving competitive results while significantly reducing sampling time. The method demonstrates effectiveness beyond images in domains like robotic control, maintaining comparable performance with an order-of-magnitude lower inference cost.

## Method Summary
Shortcut models train a single neural network to generate high-quality samples in one step by conditioning on both the current noise level and desired step size. The model learns a mapping that directly predicts shortcut vectors from the current state to the desired next state, bypassing the need to follow the full ODE path step-by-step. Training uses a combination of flow-matching loss at small step sizes and self-consistency loss at larger step sizes, allowing the model to learn generation capabilities across a wide range of inference budgets. The approach uses classifier-free guidance during training and evaluation, particularly effective at small step sizes.

## Key Results
- Achieves competitive FID scores on CelebA-HQ-256 and ImageNet-256 with 1-step sampling compared to multi-step baselines
- Demonstrates one to two orders of magnitude speedup in inference time while maintaining generation quality
- Shows effectiveness in non-image domains like robotic control with significantly reduced computational cost
- Eliminates need for complex distillation schedules while maintaining or improving sample quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditioning the network on both the current noise level and the desired step size allows the model to skip ahead in the denoising process and generate high-quality samples in a single step.
- Mechanism: The shortcut model learns a mapping that directly predicts the shortcut vector from the current state to the desired next state, bypassing the need to follow the full ODE path step-by-step.
- Core assumption: The self-consistency property holds, meaning that one shortcut step of size 2d can be decomposed into two consecutive steps of size d.
- Evidence anchors:
  - [abstract] "Shortcut models condition the network not only on the current noise level but also on the desired step size, allowing the model to skip ahead in the generation process."
  - [section 3] "Our key intuition is that we can train a single model that supports different sampling budgets, by conditioning the model not only on the timestep t but also on a desired step size d."
  - [corpus] Weak evidence; no direct mention of shortcut models in the corpus.
- Break condition: If the self-consistency property does not hold, the shortcut model will fail to accurately predict the shortcut vectors.

### Mechanism 2
- Claim: Training shortcut models using a combination of flow-matching loss at small step sizes and self-consistency loss at larger step sizes allows the model to learn to generate high-quality samples across a wide range of inference budgets.
- Mechanism: The flow-matching loss grounds the model at small step sizes, while the self-consistency loss propagates the generation capability from multi-step to few-step to one-step.
- Core assumption: The combined objective can be trained jointly without requiring a separate distillation phase.
- Evidence anchors:
  - [abstract] "Compared to distillation, shortcut models reduce complexity to a single network and training phase and additionally allow varying step budgets at inference time."
  - [section 3] "The combined objective can be trained jointly, using a single model and over a single end-to-end training run."
  - [corpus] Weak evidence; no direct mention of shortcut models in the corpus.
- Break condition: If the combined objective cannot be trained jointly effectively, the model may fail to learn to generate high-quality samples at larger step sizes.

### Mechanism 3
- Claim: Using classifier-free guidance (CFG) during training and evaluation helps improve the quality of generated samples, especially at small step sizes.
- Mechanism: CFG provides a linear approximation of the tradeoff between class-conditional and unconditional denoising ODEs, allowing the model to better capture the data distribution.
- Core assumption: CFG is effective at small step sizes but error-prone at larger steps when linear approximation is not appropriate.
- Evidence anchors:
  - [section 3.1] "We find that CFG helps at small step sizes but is error-prone at larger steps when linear approximation is not appropriate. We therefore use CFG when evaluating the shortcut model at d = 0 but forgo it elsewhere."
  - [corpus] Weak evidence; no direct mention of shortcut models in the corpus.
- Break condition: If CFG is used inappropriately at larger sizes, it may lead to poor quality generated samples.

## Foundational Learning

- Concept: Ordinary Differential Equations (ODEs)
  - Why needed here: Shortcut models are trained to approximate the solution of a denoising ODE, which transforms noise into data.
  - Quick check question: What is the role of the velocity vector v_t in the denoising ODE?

- Concept: Knowledge Distillation
  - Why needed here: Shortcut models can be seen as a form of self-distillation, where the model learns to generate high-quality samples in a single step by conditioning on the desired step size.
  - Quick check question: How does shortcut models differ from traditional knowledge distillation approaches?

- Concept: Classifier-Free Guidance (CFG)
  - Why needed here: CFG is used in shortcut models to improve the quality of generated samples, especially at small step sizes.
  - Quick check question: What is the tradeoff between class-conditional and unconditional denoising ODEs in CFG?

## Architecture Onboarding

- Component map: Input (x_t, t, d) -> Shortcut Model Network -> Output (shortcut vector s)
- Critical path: Training loop: sample (x_0, x_1) pairs → compute shortcut targets using self-consistency property → optimize combined loss function
- Design tradeoffs: The main design tradeoff is between the number of self-consistency targets and the computational cost. Using more self-consistency targets can improve the quality of generated samples but increases the computational cost.
- Failure signatures: Common failure modes include mode collapse, where the model generates samples that are too similar to each other, and poor quality samples, where the model fails to capture the data distribution.
- First 3 experiments:
  1. Train a shortcut model on a simple dataset like CelebA-HQ and evaluate its performance on few-step and one-step generation.
  2. Compare the performance of the shortcut model to baseline diffusion and flow-matching models on the same dataset.
  3. Analyze the effect of different hyperparameters, such as the number of self-consistency targets and the weight decay, on the quality of generated samples.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the self-consistency loss in shortcut models provide implicit regularization that improves many-step generation quality compared to standard flow-matching models?
- Basis in paper: [inferred] The authors note that shortcut models display slightly better FID than comparable flow-matching models and hypothesize this may be due to implicit regularization from the self-consistency loss.
- Why unresolved: The authors explicitly state this is a blind hypothesis and leave investigation to future work.
- What evidence would resolve it: Ablation studies comparing shortcut models with and without self-consistency loss on many-step generation quality would determine if the loss provides regularization benefits.

### Open Question 2
- Question: Can one-step generation quality in shortcut models be improved without sacrificing many-step generation performance?
- Basis in paper: [explicit] The authors identify this as a limitation, noting "there remains a gap between many-step generation quality and one-step generation quality."
- Why unresolved: Current shortcut models show performance degradation when reducing sampling steps, despite maintaining quality at high step counts.
- What evidence would resolve it: Developing and testing modifications to the shortcut model training objective or architecture that close the gap between one-step and many-step generation quality.

### Open Question 3
- Question: How does the performance of shortcut models scale with model size in non-image domains compared to image domains?
- Basis in paper: [explicit] The authors show that shortcut model performance improves with model scale in image generation (Figure 5) and demonstrate effectiveness in robotic control, but don't directly compare scaling behavior across domains.
- Why unresolved: The paper demonstrates scaling in images and applicability in robotics separately, but doesn't investigate whether scaling laws transfer between domains.
- What evidence would resolve it: Systematic scaling studies of shortcut models across multiple non-image domains (e.g., audio, video, robotics) to compare scaling behavior with established image generation scaling laws.

## Limitations
- The gap between many-step and one-step generation quality remains a challenge, with one-step performance still inferior to multi-step approaches
- Generalizability to non-image domains is demonstrated but with limited experimental depth, particularly in robotic control applications
- Claims about eliminating the need for distillation approaches require more extensive benchmarking against specialized distillation techniques

## Confidence

**High Confidence**: The core mechanism of conditioning on both timestep and step size is technically sound and well-explained. The training methodology using combined flow-matching and self-consistency objectives is clearly specified and reproducible.

**Medium Confidence**: The empirical results showing performance improvements over baselines are compelling but limited to specific datasets and model architectures. The claims about training simplicity (single phase, no schedules) appear supported but lack extensive comparative analysis.

**Low Confidence**: The generalizability claims to non-image domains (robotic control) are supported by minimal experimental evidence. The assertion that shortcut models completely eliminate the need for distillation approaches requires more extensive benchmarking against specialized distillation techniques.

## Next Checks

1. **Ablation on conditioning mechanisms**: Systematically test the contribution of step-size conditioning versus timestep conditioning alone to quantify the exact performance gain from the shortcut approach.

2. **Cross-architecture generalization**: Evaluate shortcut models on diverse backbone architectures (beyond DiT) and different diffusion frameworks to test the universality of the approach.

3. **Extended domain testing**: Conduct more comprehensive experiments in non-image domains, particularly comparing robotic control performance with and without shortcut models across multiple task types and complexity levels.