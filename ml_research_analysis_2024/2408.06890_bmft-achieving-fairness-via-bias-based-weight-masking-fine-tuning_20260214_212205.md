---
ver: rpa2
title: 'BMFT: Achieving Fairness via Bias-based Weight Masking Fine-tuning'
arxiv_id: '2408.06890'
source_url: https://arxiv.org/abs/2408.06890
tags:
- fairness
- bias
- fine-tuning
- features
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses bias in medical image classification, particularly
  for skin lesion diagnosis, where models can exhibit unfair performance across different
  demographic groups. The proposed Bias-based Weight Masking Fine-Tuning (BMFT) method
  identifies and selectively fine-tunes model parameters that contribute most to biased
  predictions, without requiring access to original training data.
---

# BMFT: Achieving Fairness via Bias-based Weight Masking Fine-tuning

## Quick Facts
- arXiv ID: 2408.06890
- Source URL: https://arxiv.org/abs/2408.06890
- Reference count: 36
- This paper addresses bias in medical image classification, particularly for skin lesion diagnosis, where models can exhibit unfair performance across different demographic groups.

## Executive Summary
This paper addresses bias in medical image classification, particularly for skin lesion diagnosis, where models can exhibit unfair performance across different demographic groups. The proposed Bias-based Weight Masking Fine-Tuning (BMFT) method identifies and selectively fine-tunes model parameters that contribute most to biased predictions, without requiring access to original training data. The approach uses a two-step process: first fine-tuning a subset of feature extractor weights identified by a bias-importance mask, then reinitializing and fine-tuning the classification layer. Experiments across four dermatological datasets with skin tone and gender attributes show BMFT achieves significant improvements in both fairness (Equalized Odds) and classification performance (AUC and accuracy) compared to state-of-the-art methods, while requiring only 10% of original training epochs.

## Method Summary
BMFT uses a two-step fine-tuning process that operates without access to original training data. First, it generates a mask identifying weights contributing most to biased predictions using second-order derivatives of bias influence functions and Fisher Information Matrix. Then it fine-tunes only these masked weights in the feature extractor to reduce bias. Finally, it reinitializes and fine-tunes the classification layer to recover discriminative performance using the debiased features. The method requires only 5 epochs (10% of original training) and uses an external group-balanced dataset for fine-tuning.

## Key Results
- BMFT achieves significant improvements in Equalized Odds (fairness metric) across all four dermatological datasets tested
- The method improves both fairness and classification performance (AUC and accuracy) compared to state-of-the-art methods like Full FT, FairPrune, LLFT, and Diff-Bias
- BMFT requires only 10% of original training epochs (5 epochs) while maintaining strong performance across out-of-distribution test sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bias-based weight masking identifies parameters contributing to biased predictions while preserving core discriminative features
- Mechanism: The method calculates parameter importance for both bias (using second derivative of bias influence function) and prediction (using Fisher Information Matrix), then selects weights where bias importance exceeds loss importance by a threshold α
- Core assumption: Parameters contributing to bias can be identified through second-order sensitivity analysis and separated from core features
- Evidence anchors:
  - [abstract] "BMFT produces a mask over model parameters, which efficiently identifies the weights contributing the most towards biased predictions"
  - [section 2.2] "The importance of each weight for the bias can be computed using the second derivative of the bias influence function"
  - [corpus] No direct evidence found - this appears to be the novel contribution
- Break condition: If bias and core features are too entangled, the separation may not be clean enough to preserve performance while removing bias

### Mechanism 2
- Claim: Two-step fine-tuning strategy balances debiasing with performance preservation
- Mechanism: First step fine-tunes only masked (bias-contributing) parameters in the feature extractor to reduce bias influence. Second step reinitializes and fine-tunes the classification layer to recover discriminative performance using the debiased features
- Core assumption: Fine-tuning only bias-contributing parameters won't destroy core discriminative features while still reducing bias
- Evidence anchors:
  - [abstract] "two-step debiasing strategy, wherein the feature extractor undergoes initial fine-tuning on the identified bias-influenced weights, succeeded by a fine-tuning phase on a reinitialised classification layer"
  - [section 2.3] "To disentangle core features from biased features, whilst preserving predictive capability, a two-stage fine-tuning process is structured in an 'impair-repair' manner"
  - [corpus] No direct evidence found - this is the proposed novel approach
- Break condition: If the first step removes too much information or the second step overfits to the small external dataset

### Mechanism 3
- Claim: Using only 10% of original training epochs achieves effective debiasing without overfitting
- Mechanism: The pre-trained model already captures core features effectively, so only a small amount of fine-tuning is needed to adjust the bias-contributing parameters and classification layer
- Core assumption: Core features learned during pre-training are stable and don't require extensive retraining
- Evidence anchors:
  - [abstract] "Our post fine-tuning method requires just an additional five epochs, which is 10% of the initial 50-epoch training duration"
  - [section 2.3] "Our empirical findings indicate that only 10% of the total original training epochs are adequate for successful fine-tuning"
  - [corpus] No direct evidence found - this is an empirical observation from the paper
- Break condition: If the external dataset is too small or different from the original distribution, minimal fine-tuning may not be sufficient

## Foundational Learning

- Concept: Fisher Information Matrix for parameter importance
  - Why needed here: Used to quantify how much each parameter contributes to prediction performance
  - Quick check question: What does the diagonal of the Fisher Information Matrix represent in terms of parameter sensitivity?

- Concept: Second-order derivatives for sensitivity analysis
  - Why needed here: Used to measure how changes in parameters affect the bias influence function
  - Quick check question: Why is the second derivative more informative than the first derivative for identifying bias-contributing parameters?

- Concept: Equalized Odds fairness metric
  - Why needed here: The primary fairness metric used to evaluate whether the model treats different groups equally
  - Quick check question: How does Equalized Odds differ from Demographic Parity in terms of what fairness it measures?

## Architecture Onboarding

- Component map: Pre-trained feature extractor (E) → Mask-based parameter selection → Fine-tuned feature extractor → Reinitialized classification layer (C) → Predictions
- Critical path: Mask generation → Step 1: Feature extractor fine-tuning → Step 2: Classification layer fine-tuning → Evaluation
- Design tradeoffs: Mask-based fine-tuning trades some potential bias reduction for performance preservation vs full fine-tuning; reinitializing classification layer trades some fine-tuning efficiency for better separation of core vs bias features
- Failure signatures: High EOdds with low accuracy indicates over-debiasing; low EOdds with low accuracy indicates under-performance; high variance across different test sets indicates poor generalization
- First 3 experiments:
  1. Implement mask generation on a small external dataset and visualize the mask distribution across layers
  2. Test step 1 alone (feature extractor fine-tuning only) to measure bias reduction without classification layer impact
  3. Test step 2 alone (classification layer fine-tuning on reinitialized layer) to measure performance recovery on debiased features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BMFT vary with different values of the hyperparameter K (percentage of weights to be fine-tuned)?
- Basis in paper: [explicit] The paper mentions that hyperparameter tuning was performed and K = 50 was selected, but does not provide a comprehensive analysis of how different K values affect performance.
- Why unresolved: The paper only reports results for a single value of K, without exploring the sensitivity of the method to this hyperparameter.
- What evidence would resolve it: A detailed ablation study showing the performance of BMFT across a range of K values, including a plot of fairness and accuracy metrics against K.

### Open Question 2
- Question: How does BMFT perform when applied to other types of sensitive attributes beyond skin tone and gender?
- Basis in paper: [inferred] The paper focuses on skin tone and gender attributes in dermatological datasets, but does not explore the generalizability of BMFT to other sensitive attributes.
- Why unresolved: The experiments are limited to two specific attributes, leaving open the question of how BMFT would perform with other types of demographic or non-demographic sensitive attributes.
- What evidence would resolve it: Experiments applying BMFT to datasets with different types of sensitive attributes (e.g., age, socioeconomic status) and comparing its performance across these attributes.

### Open Question 3
- Question: What is the impact of using different pre-trained model architectures (beyond the ResNet variants tested) on the effectiveness of BMFT?
- Basis in paper: [explicit] The paper tests BMFT on different ResNet architectures but does not explore other model architectures.
- Why unresolved: The paper does not provide evidence on how BMFT performs with other popular architectures like Vision Transformers or EfficientNets.
- What evidence would resolve it: Experiments applying BMFT to a variety of pre-trained model architectures and comparing the results in terms of fairness and accuracy improvements.

### Open Question 4
- Question: How does BMFT's performance scale with the size of the external dataset used for fine-tuning?
- Basis in paper: [inferred] The paper uses a specific external dataset size but does not explore how varying this size affects the method's performance.
- Why unresolved: The relationship between external dataset size and BMFT's effectiveness is not investigated, leaving uncertainty about its performance in scenarios with limited external data.
- What evidence would resolve it: Experiments systematically varying the size of the external dataset used for fine-tuning and analyzing the impact on BMFT's fairness and accuracy metrics.

## Limitations
- The paper lacks transparency on how the group-balanced external dataset is constructed from limited data, which is critical for understanding real-world applicability
- Second-order derivative calculations for bias importance may be computationally expensive and face numerical stability issues
- The assumption that bias and core features can be cleanly separated through parameter masking is not empirically validated across diverse bias types

## Confidence
- High confidence: The two-step fine-tuning methodology and experimental design are clearly specified and reproducible
- Medium confidence: The effectiveness of bias-based weight masking for achieving both fairness and accuracy improvements, as results depend heavily on the quality of the external dataset construction
- Medium confidence: The 10% training epochs claim, as it's based on empirical observation from specific datasets and architectures

## Next Checks
1. Test BMFT on datasets with different bias types (e.g., geographic location, socioeconomic factors) to validate generalizability beyond skin tone and gender attributes
2. Conduct ablation studies comparing mask-based fine-tuning against random parameter selection to isolate the effect of the bias-importance identification
3. Evaluate the method's performance when the external dataset is significantly smaller or has different class distributions to test robustness to dataset quality variations