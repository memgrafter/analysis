---
ver: rpa2
title: On the Sample Efficiency of Abstractions and Potential-Based Reward Shaping
  in Reinforcement Learning
arxiv_id: '2404.07826'
source_url: https://arxiv.org/abs/2404.07826
tags:
- function
- agent
- learning
- policy
- abstraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work bridges gaps in the literature regarding the use of Potential-Based
  Reward Shaping (PBRS) for tackling sample inefficiency in Reinforcement Learning
  (RL). We provide theoretical motivations for why using the optimal value function
  as the potential function is beneficial, and analyze the bias induced by finite
  horizons in the context of PBRS.
---

# On the Sample Efficiency of Abstractions and Potential-Based Reward Shaping in Reinforcement Learning

## Quick Facts
- arXiv ID: 2404.07826
- Source URL: https://arxiv.org/abs/2404.07826
- Reference count: 40
- Key outcome: Achieves CNN-level performance on Atari games using simple fully-connected networks with Potential-Based Reward Shaping (PBRS) and abstractions

## Executive Summary
This work investigates how Potential-Based Reward Shaping (PBRS) combined with abstractions can improve sample efficiency in reinforcement learning. The authors provide theoretical justification for using optimal value functions as potential functions, showing that this approach simplifies the reshaped MDP's optimal value function to zero everywhere. They analyze bias introduced by finite horizons and demonstrate that abstractions can approximate optimal value functions to enable practical PBRS implementation. Through experiments on four environments including a goal-oriented navigation task and three Atari Learning Environments (ALE) games, they show that PBRS with abstractions significantly improves sample efficiency while achieving comparable performance to CNN-based solutions using simple fully-connected networks.

## Method Summary
The method involves creating abstractions of the original MDP through aggregation functions that map states to a simpler space, solving these abstractions to obtain optimal value functions, and using these as potential functions for PBRS. The reshaped reward is computed using F(s,a,s') = γϕ(s') - ϕ(s), where ϕ is the optimal value function of the abstraction. For the 8-rooms navigation task, Q-Learning is used with and without PBRS. For ALE games, both fully-connected and CNN-based DQN architectures are implemented using Mushroom-RL, with PBRS applied to the fully-connected version. The approach is evaluated on Freeway, Q*Bert, and Venture games using RAM states for fully-connected networks and image states for CNN comparison.

## Key Results
- PBRS with abstractions achieves the same performance level as CNN-based DQN solutions using only simple fully-connected networks
- Significant improvements in sample efficiency are observed across all tested environments when using PBRS with abstractions
- The Freeway game shows the most dramatic improvement, reaching optimal performance with substantially fewer interactions
- Q*Bert and Venture also benefit from PBRS, though to a lesser extent than Freeway

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using the optimal value function as the potential function makes the reshaped MDP's optimal value function zero everywhere, simplifying learning.
- Mechanism: The potential-based reward shaping formula F(s,a,s') = γϕ(s') - ϕ(s) combined with ϕ=V* results in the reshaped reward r'(s,a,s') = r(s,a,s') + γV*(s') - V*(s). By the Bellman equation, V*(s) = max_a [r(s,a,s') + γV*(s')], so the reshaped Q' satisfies Q'*(s,a) = Q*(s,a) - V*(s). Since V*(s) = max_a Q*(s,a), the optimal reshaped value is zero.
- Core assumption: The potential function exactly equals the true optimal value function of the task.
- Evidence anchors:
  - [abstract] "if we select the potential function to be the optimal value function of the problem at hand, the resulting reshaped problem will have an optimal value function that is zero everywhere"
  - [section 2.2] "if we use this reshaping approach, the following relationships between (action-)value functions hold: V^π_M'(s) = V^π_M(s) - ϕ(s)"
- Break condition: If the potential function is not exactly V*, the zero-value simplification no longer holds, potentially degrading performance.

### Mechanism 2
- Claim: Abstractions can approximate the optimal value function, enabling practical PBRS without knowing the true V*.
- Mechanism: An abstraction MDP Mα with aggregation function α maps the original state space to a smaller one. The optimal value function V*_Mα of this simpler MDP is computed and used as the potential function ϕ(s) = V*_Mα(α(s)). This approximates V* while being computationally tractable.
- Core assumption: The abstraction preserves enough structure of the original task that V*_Mα is a good approximation of V*.
- Evidence anchors:
  - [section 3] "An abstraction here represents a mapping of the original task to a simpler one, such that solving the simpler one can accelerate the learning phase in the original task itself."
  - [section 6.2.1] "Freeway represents the simplest task we tackled...we propose a very simple abstraction where we neglect the presence of the cars"
- Break condition: If the abstraction is too coarse or misses critical state distinctions, the approximation becomes poor and may introduce bias or suboptimal guidance.

### Mechanism 3
- Claim: In finite-horizon goal-oriented MDPs with deterministic transitions, PBRS preserves policy ordering when the potential is maximal at goal states.
- Mechanism: Theorem 3 proves that for deterministic goal-oriented MDPs, if ϕ(sG) = Φ for all goal states and Φ is bounded, then the optimal policy in the reshaped MDP remains optimal. The proof compares the expected reshaped returns of the optimal policy versus suboptimal policies, showing the ordering is preserved due to the potential being maximized at goals.
- Core assumption: The MDP is deterministic and goal-oriented, and the potential function is maximal at goal states.
- Evidence anchors:
  - [section 5.1] "Theorem 3 (Policy Ordering in Deterministic Goal-Oriented Episodic MDPs)...Then, the policy ordering in M is preserved in the reshaped MDP M′ for all the deterministic policies reaching the goal in at most H steps."
  - [section 6.1] "This task has a stochastic transition function, so it falls under the scope of Theorem 4. Additionally, we cannot compute the theoretical value of Φ. However, even setting Φ = 1 allows us not only to achieve the optimum"
- Break condition: If the MDP has stochastic transitions (without satisfying Theorem 4's conditions) or the potential is not maximal at goals, policy ordering may not be preserved.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and their components (states, actions, transition function, reward function, discount factor)
  - Why needed here: The entire paper builds on MDP theory as the foundation for understanding RL problems and how PBRS modifies them
  - Quick check question: What are the five components of an MDP tuple and how does each affect the learning problem?

- Concept: Value functions (V and Q) and their optimal versions
  - Why needed here: The paper's core mechanism relies on using V* as a potential function, and understanding value functions is essential to grasp how PBRS works
  - Quick check question: How do you compute V*(s) from Q*(s,a) and vice versa?

- Concept: Policy gradient theorem and baseline methods
  - Why needed here: Proposition 2 establishes the equivalence between PBRS and using value function estimates as baselines in policy gradient methods
  - Quick check question: What is the purpose of a baseline in policy gradient methods and how does it reduce variance?

## Architecture Onboarding

- Component map: Abstraction generator -> Value iteration solver -> Potential function mapper -> Reshaped environment wrapper -> RL agent -> Performance evaluator
- Critical path:
  1. Define abstraction (aggregation function α)
  2. Solve abstraction via value iteration to get V*_Mα
  3. Map V*_Mα to original state space as potential function ϕ(s) = V*_Mα(α(s))
  4. Create reshaped environment with modified rewards
  5. Train RL agent on reshaped environment
  6. Evaluate performance against baseline

- Design tradeoffs:
  - Abstraction complexity vs. approximation quality: More detailed abstractions provide better approximations but increase computational cost
  - Horizon length vs. bias: Longer horizons reduce bias but increase computational requirements
  - Potential function design: Using V*_Mα is theoretically motivated but may require custom F functions for complex transitions

- Failure signatures:
  - Poor performance despite reshaping: Likely indicates bad abstraction that doesn't capture task structure
  - High variance in learning curves: May indicate insufficient exploration or unstable potential function estimates
  - Convergence to suboptimal policies: Could mean bias introduced by finite horizon or poor potential function design

- First 3 experiments:
  1. Implement simple grid world with known optimal value function, compare PBRS with V* vs. no shaping
  2. Create abstraction for grid world, solve it, and use its value function as potential, comparing against experiment 1
  3. Test on Freeway with the simple abstraction described, comparing fully-connected DQN with and without reshaping against CNN-based DQN

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is the performance improvement from abstractions and PBRS to the quality and granularity of the abstraction used?
- Basis in paper: [explicit] The paper notes that "the quality of the abstraction, in terms of how well it approximates the original task, plays an important role" and observes significant performance differences between Freeway (simple abstraction) and Q*Bert/Venture (more complex abstractions).
- Why unresolved: While the paper demonstrates that better abstractions lead to greater improvements, it doesn't systematically quantify this relationship or establish a threshold for abstraction quality needed to achieve meaningful gains.
- What evidence would resolve it: Systematic experiments varying abstraction complexity on the same task, measuring performance as a function of abstraction quality metrics like state space reduction ratio or solution optimality gap.

### Open Question 2
- Question: What are the theoretical bounds on bias introduced by PBRS in general MDPs when using finite horizons, and how do these bounds depend on the potential function approximation error?
- Basis in paper: [explicit] Theorem 5 provides a condition for preserving policy ordering in general MDPs but notes it is "quite restrictive" and "very small, requiring an incredibly long horizon H."
- Why unresolved: The theorem gives a sufficient condition but not a tight bound, and the paper acknowledges the practical difficulty of satisfying this condition for complex tasks.
- What evidence would resolve it: Formal analysis deriving error bounds as a function of horizon length and potential function approximation error, validated through experiments on benchmark MDPs with known optimal policies.

### Open Question 3
- Question: Can abstractions be automatically learned from interactions with the environment rather than being handcrafted, and how would this impact sample efficiency?
- Basis in paper: [inferred] The paper uses handcrafted abstractions and mentions this as an important avenue for future work, noting that "devising ways to learn abstractions automatically from interactions becomes an important avenue for future developments."
- Why unresolved: The paper only evaluates pre-defined abstractions and doesn't explore learning methods, despite acknowledging the potential benefits and challenges of automation.
- What evidence would resolve it: Implementation and evaluation of automatic abstraction learning methods (e.g., state abstraction discovery algorithms) compared against handcrafted abstractions on the same tasks.

## Limitations

- Limited empirical evaluation: Only four environments tested, limiting generalizability to other RL domains
- Handcrafted abstractions: Relies on manually designed abstractions rather than learned ones, which may not scale to more complex tasks
- No comparison with other sample efficiency techniques: Does not benchmark against alternative methods like curriculum learning or meta-learning

## Confidence

- Theoretical mechanisms: High - The mathematical foundations and proofs are sound and well-established in RL literature
- Empirical results: Medium - Limited to four environments with no ablation studies on abstraction design choices
- Generalizability: Medium - Results show promise but may not extend to visually complex or high-dimensional tasks

## Next Checks

1. Test the proposed approach on visually complex environments (e.g., Atari games with more sophisticated graphics) to evaluate the limits of simple network architectures combined with PBRS
2. Conduct systematic ablation studies varying abstraction granularity to identify optimal trade-offs between approximation quality and computational efficiency
3. Implement and evaluate alternative potential functions (e.g., learned value function estimates) to assess robustness when exact optimal value functions are unavailable