---
ver: rpa2
title: Question-Analysis Prompting Improves LLM Performance in Reasoning Tasks
arxiv_id: '2407.03624'
source_url: https://arxiv.org/abs/2407.03624
tags:
- prompt
- question
- reasoning
- prompting
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Question-Analysis Prompting (QAP), a zero-shot
  prompting strategy that improves large language model performance on reasoning tasks
  by first making the model explain the question in n words before solving. Evaluated
  on GPT-3.5 and GPT-4 across arithmetic (GSM8K, AQuA, SAT) and commonsense (StrategyQA)
  datasets, QAP consistently outperforms state-of-the-art prompts including Chain-of-Thought,
  Plan-and-Solve, and Take A Deep Breath on AQuA and SAT datasets, and ranks among
  the top-2 prompts on 75% of all tests.
---

# Question-Analysis Prompting Improves LLM Performance in Reasoning Tasks

## Quick Facts
- arXiv ID: 2407.03624
- Source URL: https://arxiv.org/abs/2407.03624
- Reference count: 3
- QAP consistently outperforms state-of-the-art prompts on AQuA and SAT datasets

## Executive Summary
This paper introduces Question-Analysis Prompting (QAP), a zero-shot prompting strategy that improves large language model performance on reasoning tasks by first making the model explain the question in n words before solving. Evaluated on GPT-3.5 and GPT-4 across arithmetic (GSM8K, AQuA, SAT) and commonsense (StrategyQA) datasets, QAP consistently outperforms state-of-the-art prompts including Chain-of-Thought, Plan-and-Solve, and Take A Deep Breath on AQuA and SAT datasets, and ranks among the top-2 prompts on 75% of all tests. The optimal value of n depends on question difficulty, with longer explanations benefiting harder questions but potentially harming easier ones. A key finding is that QAP's effectiveness stems from improved question interpretation rather than just step-by-step calculation, though excessively large n values can lead to unfinished responses or confusion, particularly for simpler problems.

## Method Summary
QAP is a zero-shot prompting strategy that instructs models to explain a problem in at least n words before solving it. The method was evaluated across four datasets (GSM8K, AQuA, SAT, StrategyQA) using GPT-3.5 Turbo and GPT-4 Turbo via the OpenAI API with temperature and top-k set to 0 for deterministic outputs. The approach was compared against baseline, Chain-of-Thought (8-shot), Take A Deep Breath, and Plan-and-Solve prompts. Five QAP variants were tested with n values of 25, 50, 100, 150, and 200 words to determine optimal explanation length based on question difficulty.

## Key Results
- QAP consistently outperforms state-of-the-art prompts on AQuA and SAT datasets
- QAP ranks among top-2 prompts on 75% of all tests across arithmetic and commonsense tasks
- Optimal n value depends on question difficulty, with longer explanations benefiting harder questions but potentially harming easier ones
- QAP's effectiveness stems from improved question interpretation rather than just step-by-step calculation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QAP improves performance by enhancing question interpretation before step-by-step calculation
- Mechanism: The model explicitly restates the problem in its own words, forcing deeper comprehension and identifying key elements before attempting to solve
- Core assumption: Models benefit from explicit problem restatement as it reduces misinterpretation and increases focus on relevant information
- Evidence anchors:
  - [abstract] "A key finding is that QAP's effectiveness stems from improved question interpretation rather than just step-by-step calculation"
  - [section] "The key principle behind QAP is that the model should reiterate the problem in its own words before solving"
  - [corpus] Weak evidence - related work focuses on chain-of-thought and decomposition, but not explicit question restatement
- Break condition: When question interpretation doesn't add value (very simple problems) or when the model's initial comprehension is already sufficient

### Mechanism 2
- Claim: Configurable word count parameter (n) allows optimization based on question difficulty
- Mechanism: Different question complexities require different levels of explanation depth, and n controls this depth
- Core assumption: Harder questions benefit from more detailed analysis while simpler questions suffer from over-explanation
- Evidence anchors:
  - [abstract] "The optimal value of n depends on question difficulty, with longer explanations benefiting harder questions but potentially harming easier ones"
  - [section] "A key factor of QAP performance can be attributed to response length, where detailed responses are beneficial when answering harder questions, but can negatively affect easy questions"
  - [corpus] No direct evidence - corpus focuses on other prompting strategies without examining parameter optimization
- Break condition: When the relationship between n and question difficulty is not monotonic or when n selection becomes computationally prohibitive

### Mechanism 3
- Claim: Zero-shot nature of QAP makes it universally applicable without task-specific exemplars
- Mechanism: The simple instruction "Explain this problem in at least n words. Then solve for the answer" works across diverse reasoning tasks
- Core assumption: A general explanation requirement is sufficient for diverse problem types without needing task-specific demonstrations
- Evidence anchors:
  - [section] "QAP is a zero-shot prompt. In zero-shot prompting the model does not receive exemplars, but is given a specially crafted instruction on how to approach the task"
  - [section] "We compare QAP with other state-of-the-art prompts including chain-of-thought (CoT), Plan and Solve Prompting (PS+) and Take A Deep Breath (TADB)"
  - [corpus] Weak evidence - corpus neighbors focus on various prompting strategies but don't specifically address zero-shot universality
- Break condition: When task-specific exemplars would provide significantly better performance than the general instruction

## Foundational Learning

- Concept: Question comprehension and information extraction
  - Why needed here: QAP fundamentally relies on the model's ability to extract and restate key information from the question
  - Quick check question: Can the model accurately identify the main entities, relationships, and goal from a given word problem before attempting to solve it?

- Concept: Parameter tuning and optimization
  - Why needed here: The effectiveness of QAP depends on selecting the optimal value of n based on question difficulty
  - Quick check question: Given a dataset with known difficulty levels, can you determine the correlation between explanation length and performance to select optimal n values?

- Concept: Zero-shot prompting and instruction following
  - Why needed here: QAP is evaluated as a zero-shot method, requiring understanding of how models interpret and follow general instructions
  - Quick check question: How does a model's performance change when given general instructions versus specific exemplars for the same task?

## Architecture Onboarding

- Component map: Prompt generator → Parameter selector → Model response generator → Answer extraction and verification
- Critical path: Question → QAP prompt generation → n parameter selection → Model response generation → Answer extraction and verification
- Design tradeoffs: Longer explanations (higher n) improve comprehension for hard questions but risk over-explanation and unfinished responses for easy questions; zero-shot approach is universally applicable but may underperform task-specific exemplars
- Failure signatures: Performance degradation on simple questions with high n values; incomplete responses when n is too small for complex questions; model confusion when over-explaining straightforward problems
- First 3 experiments:
  1. Test QAP with varying n values (25, 50, 100, 150, 200) on a simple arithmetic dataset to identify the point where over-explanation begins to harm performance
  2. Compare QAP performance on "easy" vs "hard" questions using baseline accuracy as the difficulty metric to validate the difficulty-dependent optimization claim
  3. Implement a classifier to automatically predict question difficulty and select optimal n values, then evaluate against manual n selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of n for QAP across different problem types and model sizes?
- Basis in paper: Explicit - The paper states "The optimal value of n depends on question difficulty" and shows performance varies with n
- Why unresolved: The paper manually tested n=25,50,100,150,200 but doesn't provide a principled method for determining the optimal n value
- What evidence would resolve it: A systematic study showing accuracy curves across n values for different problem types, plus a proposed algorithm for selecting n based on problem characteristics

### Open Question 2
- Question: How does QAP compare to other prompting strategies when combined with them?
- Basis in paper: Explicit - The conclusion states "We plan to extend this work further by combining QAP with other prompt strategies"
- Why unresolved: The paper only evaluated QAP as a standalone prompting strategy, not in combination with others like CoT or TADB
- What evidence would resolve it: Experiments showing performance of QAP + CoT, QAP + TADB, etc., compared to individual strategies

### Open Question 3
- Question: What is the mechanism by which QAP improves model performance?
- Basis in paper: Explicit - The paper states "A key finding is that QAP's effectiveness stems from improved question interpretation rather than just step-by-step calculation"
- Why unresolved: While the paper identifies improved question interpretation as the mechanism, it doesn't provide detailed analysis of how this works or what aspects of interpretation are most important
- What evidence would resolve it: Detailed analysis of model attention patterns, comparison of interpretation quality with/without QAP, and ablation studies on different interpretation components

## Limitations

- QAP requires manual selection of optimal n value, which may not generalize well across different datasets and model sizes
- The zero-shot nature of QAP, while claimed as a benefit, is not directly compared against few-shot prompting approaches that might achieve similar or better results
- Performance degradation occurs on simple questions when n values are too high, leading to over-explanation and unfinished responses

## Confidence

**High Confidence**: The empirical observation that QAP improves performance on AQuA and SAT datasets is well-supported by the presented accuracy metrics and statistical comparisons against baseline prompts.

**Medium Confidence**: The claim that QAP's effectiveness stems from improved question interpretation rather than step-by-step calculation is inferred from results but not directly validated through mechanistic studies or ablation experiments.

**Low Confidence**: The assertion that QAP is universally applicable as a zero-shot method without task-specific exemplars is plausible but untested against few-shot alternatives that might outperform the zero-shot approach.

## Next Checks

1. **Ablation Study**: Implement a controlled experiment comparing QAP with and without the explanation component to directly measure the contribution of question interpretation to overall performance gains.

2. **Parameter Optimization**: Develop and validate an automated difficulty classifier to select optimal n values for each question, then measure whether this approach outperforms manual n selection or fixed-parameter QAP variants.

3. **Few-shot Comparison**: Test QAP against few-shot prompting approaches using the same exemplars as CoT to determine whether the zero-shot advantage comes at the cost of absolute performance.