---
ver: rpa2
title: Interpreting Arithmetic Mechanism in Large Language Models through Comparative
  Neuron Analysis
arxiv_id: '2409.14144'
source_url: https://arxiv.org/abs/2409.14144
tags:
- neurons
- table
- arxiv
- heads
- important
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a small set of attention heads crucial for
  arithmetic operations in large language models, each specialized for different operations.
  Through a novel Comparative Neuron Analysis (CNA) method, it uncovers a four-stage
  internal logic chain: feature enhancement by shallow feed-forward network (FFN)
  neurons, feature transfer via shallow attention layers, feature prediction by arithmetic
  heads, and prediction enhancement among deep FFN neurons.'
---

# Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis

## Quick Facts
- arXiv ID: 2409.14144
- Source URL: https://arxiv.org/abs/2409.14144
- Authors: Zeping Yu; Sophia Ananiadou
- Reference count: 35
- Primary result: Identifies specialized arithmetic heads and FFN neurons through CNA method, enabling model pruning and bias reduction

## Executive Summary
This paper investigates how large language models perform arithmetic operations by identifying specialized components and their interactions. Through systematic intervention experiments on Llama-7B, researchers discover that only a small set of attention heads handle arithmetic operations, with each head specializing in different operations. The study introduces a Comparative Neuron Analysis (CNA) method that reveals an internal four-stage logic chain from input to prediction, involving feature enhancement, transfer, prediction, and enhancement stages. The findings enable practical applications including model pruning that maintains 82.3% accuracy while using only 5% of deep FFN neurons, and model editing to reduce gender bias.

## Method Summary
The research employs a multi-faceted approach combining head interventions, neuron importance analysis, and comparative modeling. Researchers systematically zero out each of the 1,024 attention heads in Llama-7B to measure accuracy impacts on arithmetic tasks. The Comparative Neuron Analysis method compares neuron importance scores between original and intervened models to identify critical neurons along the computation path. The study also examines LoRA fine-tuning effects by analyzing changes in FFN neuron coefficient scores across different layer positions. For pruning applications, the method identifies and retains only the most important neurons while evaluating performance retention.

## Key Results
- Identified three specialized arithmetic heads (1722, 159, 1419) that each handle distinct arithmetic operations
- Developed CNA method revealing a four-stage internal logic chain: feature enhancement → feature transfer → feature prediction → prediction enhancement
- Demonstrated that pruning deep FFN neurons to 5% maintains 82.3% accuracy on 2-digit arithmetic, outperforming the original model's 62.9%
- Showed LoRA fine-tuning amplifies coefficient scores of important FFN neurons, with shallow layer placement yielding greater amplification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Arithmetic ability in LLMs is localized to a small set of specialized attention heads that store parameters for memorizing 1-digit operations.
- Mechanism: These arithmetic heads activate deep FFN neurons related to final predictions through an information flow chain involving feature enhancement, transfer, prediction, and enhancement stages.
- Core assumption: The observed accuracy drops when intervening these heads are primarily due to loss of 1-digit operation memorization rather than other computational failures.
- Evidence anchors:
  - [abstract] "We find arithmetic ability resides within a limited number of attention heads, with each head specializing in distinct operations."
  - [section 3.3] "Only three heads result in a decrease of 10% or more" and [section 3.4] "the decreases of 1D, 2D and 3D operations are similar."
  - [corpus] Found 25 related papers with average FMR=0.47, indicating moderate relevance but limited direct evidence for this specific mechanism.

### Mechanism 2
- Claim: LoRA fine-tuning enhances prediction probabilities by amplifying coefficient scores of important FFN neurons.
- Mechanism: LoRA adds low-rank matrices to attention layers that increase the activation strength of deep FFN neurons that are critical for final predictions.
- Core assumption: The correlation between LoRA layer placement and accuracy changes is due to its effect on FFN neuron coefficient scores rather than other factors.
- Evidence anchors:
  - [abstract] "we investigate the mechanism of LoRA, revealing that it enhances prediction probabilities by amplifying the coefficient scores of FFN neurons related to predictions."
  - [section 5] "Across all five fine-tuned models, the coefficient scores of 257164 and 195769 surpass those of the original model" and "fine-tuning LoRA in shallow layers yields a greater amplification."
  - [corpus] Weak evidence - corpus neighbors don't directly address LoRA's mechanism on FFN neurons.

### Mechanism 3
- Claim: The Comparative Neuron Analysis (CNA) method can identify critical neurons by comparing neuron importance scores between original and intervened models.
- Mechanism: CNA locates neurons whose importance score changes most significantly when specific heads are intervened, revealing the information flow chain from inputs to predictions.
- Core assumption: Importance score changes accurately reflect the causal contribution of neurons to prediction accuracy.
- Evidence anchors:
  - [abstract] "we introduce the Comparative Neuron Analysis (CNA) method, which identifies an internal logic chain consisting of four distinct stages from input to prediction"
  - [section 4.1] "The core idea of our proposed CNA method is comparing the same neuron across different models given the same input" and results showing "the most important neurons are in FFN layers."
  - [corpus] Limited direct evidence - corpus doesn't contain studies specifically validating CNA methodology.

## Foundational Learning

- Concept: Attention mechanisms and multi-head attention
  - Why needed here: The paper relies on understanding how attention heads process and route information, particularly the arithmetic-specialized heads.
  - Quick check question: How does the attention mechanism allow different heads to specialize in different operations?

- Concept: Feed-Forward Network (FFN) layers and neuron activation
  - Why needed here: The paper identifies specific FFN neurons as crucial for arithmetic predictions and analyzes their coefficient scores.
  - Quick check question: What is the role of FFN neurons in the residual stream, and how do coefficient scores affect their contribution?

- Concept: Causal mediation analysis and intervention methods
  - Why needed here: The paper uses head interventions and neuron importance comparisons to identify causal relationships.
  - Quick check question: What is the difference between zero intervention and noise intervention when studying model components?

## Architecture Onboarding

- Component map: input embedding → shallow FFN feature enhancement → shallow attention feature transfer → arithmetic head activation → deep FFN prediction enhancement → output prediction
- Critical path: For arithmetic operations, the most important computational path is through the arithmetic heads (1722, 159, 1419) that activate specific deep FFN neurons, which then enhance the probability of correct predictions.
- Design tradeoffs: The model trades parameter efficiency for specialization - only a few heads store arithmetic knowledge, making it vulnerable to head interventions but efficient in parameter usage.
- Failure signatures: When arithmetic heads are intervened, accuracy drops significantly (12.7%+ decrease), particularly for operations those heads specialize in. Random neuron interventions cause minimal accuracy changes (2.6%).
- First 3 experiments:
  1. Replicate the head intervention experiment by zeroing each of the 1,024 heads individually and measuring accuracy changes on 2-digit arithmetic.
  2. Apply the CNA method to identify important neurons for a simple arithmetic case (e.g., "3+5=") and verify their interpretability through vocabulary space projection.
  3. Test LoRA fine-tuning by adding low-rank matrices to different attention layers and measuring changes in FFN neuron coefficient scores and overall accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the same attention head store important parameters for different operations (e.g., head 1722 for both 1D+ and 1D-)?
- Basis in paper: [explicit] The paper observes that the same heads are crucial for 1-digit, 2-digit, and 3-digit operations, suggesting they store important parameters about 1-digit operations.
- Why unresolved: While the paper hypothesizes that heads store parameters for memorizing 1-digit operations, the exact reason for this shared functionality across different operations remains unexplained.
- What evidence would resolve it: Further investigation into the specific parameters stored in these heads and their relationship to different arithmetic operations would clarify this question.

### Open Question 2
- Question: How does LoRA fine-tuning in different layers affect the model's ability to learn and generalize arithmetic operations?
- Basis in paper: [explicit] The paper observes that LoRA fine-tuning in shallow layers yields greater amplification of FFN neurons' coefficient scores compared to deep layers, leading to improved accuracy.
- Why unresolved: The paper does not explore the underlying mechanisms of how LoRA interacts with different layers and their impact on arithmetic learning and generalization.
- What evidence would resolve it: Detailed analysis of the changes in FFN neurons' representations and their impact on arithmetic performance after LoRA fine-tuning in different layers would provide insights into this question.

### Open Question 3
- Question: Can the Comparative Neuron Analysis (CNA) method be applied to other tasks beyond arithmetic and gender bias reduction?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of CNA in identifying important neurons for arithmetic operations and reducing gender bias, suggesting its potential applicability to other tasks.
- Why unresolved: The paper does not explore the generalizability of CNA to other domains or tasks, limiting its scope of application.
- What evidence would resolve it: Applying CNA to various tasks, such as natural language understanding, image recognition, or code generation, and evaluating its performance would determine its broader applicability.

## Limitations
- The study identifies three arithmetic-specialized heads but provides limited validation that these heads exclusively handle arithmetic versus serving multiple functions.
- The CNA method's reliance on importance score changes assumes these scores accurately reflect causal contributions, but alternative attribution methods might yield different neuron sets.
- The pruning results showing 82.3% accuracy retention with 5% of deep FFN neurons may not generalize beyond the specific arithmetic task or model architecture tested.

## Confidence

**High Confidence**: The identification of arithmetic-specialized heads and their intervention effects (accuracy drops of 12.7%+ when heads are zeroed). The arithmetic head identification is directly observable through systematic intervention experiments with clear quantitative results.

**Medium Confidence**: The four-stage internal logic chain revealed by CNA and the LoRA mechanism explanation. While the CNA method identifies coherent patterns, the causal interpretation of the information flow requires additional validation through ablation studies.

**Low Confidence**: The human-interpretability claims for FFN neurons and the generalization of pruning results to other tasks. The vocabulary space projections provide suggestive evidence but don't conclusively demonstrate semantic interpretability.

## Next Checks

1. **Cross-method validation**: Apply SHAP or integrated gradients to the same intervened models and compare the identified important neuron sets with CNA results to verify robustness of the internal logic chain.

2. **Generalization testing**: Apply the same pruning strategy (retaining 5% of deep FFN neurons) to non-arithmetic tasks like commonsense reasoning or natural language inference to assess whether the identified neurons are task-specific or more general.

3. **Head multi-functionality probe**: Design experiments to test whether the identified arithmetic heads also activate during non-arithmetic contexts, using attention pattern visualization and probing with mixed arithmetic/non-arithmetic prompts.