---
ver: rpa2
title: 'Neuc-MDS: Non-Euclidean Multidimensional Scaling Through Bilinear Forms'
arxiv_id: '2411.10889'
source_url: https://arxiv.org/abs/2411.10889
tags:
- matrix
- eigenvalues
- neuc-mds
- stress
- symmetric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of classical MDS when applied
  to non-Euclidean and non-metric dissimilarity matrices, particularly the "dimensionality
  paradox" where increasing embedding dimensions can worsen STRESS error. The authors
  propose Neuc-MDS, which extends MDS by using symmetric bilinear forms instead of
  standard inner products, allowing negative eigenvalues of the dissimilarity Gram
  matrix to be incorporated.
---

# Neuc-MDS: Non-Euclidean Multidimensional Scaling Through Bilinear Forms

## Quick Facts
- arXiv ID: 2411.10889
- Source URL: https://arxiv.org/abs/2411.10889
- Reference count: 40
- Key outcome: Proposes Neuc-MDS and Neuc-MDS+ to resolve dimensionality paradox in MDS by using symmetric bilinear forms and eigenvalue selection/weighting, achieving lower STRESS and distortion than cMDS on 10 datasets.

## Executive Summary
This paper addresses the limitations of classical MDS (cMDS) when applied to non-Euclidean and non-metric dissimilarity matrices, particularly the "dimensionality paradox" where increasing embedding dimensions can worsen STRESS error. The authors propose Neuc-MDS, which extends MDS by using symmetric bilinear forms instead of standard inner products, allowing negative eigenvalues of the dissimilarity Gram matrix to be incorporated. They provide theoretical analysis showing Neuc-MDS has better asymptotic behavior than cMDS on random matrices, with monotonically decreasing STRESS as dimensions increase. Empirically, Neuc-MDS and its variant Neuc-MDS+ significantly outperform baselines (cMDS, Lower-MDS, SMACOF) on 10 diverse datasets, achieving lower STRESS, distortion, and additive error while resolving the dimensionality paradox. The method is also shown to be efficiently accelerated using landmark points.

## Method Summary
Neuc-MDS addresses the dimensionality paradox in MDS by extending the method to use symmetric bilinear forms, which allows incorporation of negative eigenvalues from the dissimilarity Gram matrix. The method involves computing the centered dissimilarity matrix B = -1/2 * C * D * C, performing eigendecomposition to get eigenvalues and eigenvectors, and then selecting k eigenvalues using a greedy algorithm (EV-Selection) that balances magnitude and sign. Neuc-MDS+ extends this by finding optimal linear combinations of eigenvalues rather than binary selection. Both methods optimize a lower bound of STRESS error. The approach is validated on synthetic and real-world datasets, showing improved performance over classical MDS and other baselines.

## Key Results
- Neuc-MDS resolves the dimensionality paradox, with STRESS monotonically decreasing as dimensions increase, unlike cMDS
- On 10 datasets (5 synthetic, 5 real-world), Neuc-MDS+ achieves lowest average distortion (1.12) and scaled additive error (0.05) compared to cMDS (1.27, 0.07) and Lower-MDS (1.30, 0.09)
- Landmark Neuc-MDS provides significant speedup (up to 3x faster) with minimal performance loss
- Theoretical analysis shows Neuc-MDS has better asymptotic behavior than cMDS on random matrices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extending MDS to use symmetric bilinear forms allows incorporation of negative eigenvalues from the dissimilarity Gram matrix, resolving the "dimensionality paradox" where increasing dimensions worsens STRESS in classical MDS.
- Mechanism: The bilinear form generalizes the standard inner product, enabling embeddings that respect non-Euclidean geometry. By keeping eigenvalues of largest magnitude (including negative ones), Neuc-MDS avoids the bias introduced by cMDS when it drops negative eigenvalues, which distorts the representation as dimensions increase.
- Core assumption: The Gram matrix's negative eigenvalues carry meaningful structural information about the dataset's intrinsic geometry, not noise.
- Evidence anchors:
  - [abstract] "The main idea is to generalize the standard inner product to symmetric bilinear forms to utilize the negative eigenvalues of dissimilarity Gram matrices."
  - [section] "When the input distance matrix is not a Euclidean distance matrix, this problem is called metric MDS...cMDS, keeping only positive eigenvalues, is intrinsically biased – the more positive eigenvalues used the more it deviates from the input data."
  - [corpus] Weak—neighbors do not discuss negative eigenvalues or bilinear forms in MDS context.
- Break condition: If negative eigenvalues are dominated by noise or measurement error, incorporating them could degrade embedding quality rather than improve it.

### Mechanism 2
- Claim: Optimizing eigenvalue selection via a greedy algorithm that balances magnitude and sign minimizes a lower bound of STRESS, improving embedding fidelity.
- Mechanism: The algorithm selects k eigenvalues by iteratively picking the one with the highest absolute value while maintaining a small sum of unselected eigenvalues. This approach minimizes C1 + C2 in the STRESS decomposition, where C1 captures squared dropped eigenvalues and C2 captures squared sums of dropped eigenvalues.
- Core assumption: The dominant contribution to STRESS comes from the first two terms (C1 + C2), allowing focus on their minimization without significantly compromising accuracy.
- Evidence anchors:
  - [section] "We propose Neuc-MDS, an efficient algorithm that finds the best subset of eigenvalues to minimize a lower bound of STRESS."
  - [section] "We provide an in-depth error analysis and proofs of the optimality in minimizing lower bounds of STRESS."
  - [corpus] Weak—no neighbor papers discuss eigenvalue selection algorithms for MDS variants.
- Break condition: If C3 (the third term in STRESS decomposition) becomes significant in practice, the greedy minimization of C1 + C2 may yield suboptimal results.

### Mechanism 3
- Claim: Extending eigenvalue selection from binary to continuous weights via linear combinations further reduces STRESS by better approximating the optimal embedding.
- Mechanism: Neuc-MDS+ replaces the binary indicator vector w with a continuous weight vector, finding the optimal linear combination of eigenvalues that minimizes the lower bound of STRESS. This approach relaxes the constraint that only k eigenvalues can be selected, allowing more nuanced embedding.
- Core assumption: The optimal embedding can be achieved through a linear combination of all eigenvalues, not just a subset, without introducing excessive complexity.
- Evidence anchors:
  - [section] "Our advanced algorithm, Neuc-MDS +, finds the best linear combination of eigenvalues to minimize the lower bound objective."
  - [section] "The first two terms, ˜C1 + ˜C2, as a lower bound of the STRESS, is minimized as 4 ¯wT λ(2) + 4( ¯wT λ)2 1 + k with ˜λ to be ˜λ∗ := λ ⊙ w + ¯wT λ 1 + k w."
  - [corpus] Weak—no neighbor papers discuss continuous eigenvalue weighting in MDS.
- Break condition: If the continuous weighting introduces numerical instability or overfitting, particularly with noisy data, the improved theoretical bound may not translate to practical gains.

## Foundational Learning

- Concept: Symmetric bilinear forms and their relationship to Gram matrices
  - Why needed here: Understanding how bilinear forms generalize inner products is essential to grasp why Neuc-MDS can handle non-Euclidean dissimilarities by incorporating negative eigenvalues.
  - Quick check question: Given a symmetric matrix A, how do you construct the corresponding symmetric bilinear form, and what does it mean for A to be positive semi-definite?

- Concept: Eigenvalue decomposition and its role in dimensionality reduction
  - Why needed here: The algorithm relies on eigendecomposition of the centered dissimilarity matrix to identify which eigenvalues (and their signs) to retain for embedding.
  - Quick check question: If a matrix has eigenvalues [5, 3, -2, -1], which would classical MDS keep and why? How does Neuc-MDS differ?

- Concept: STRESS as a metric for embedding quality and its decomposition
  - Why needed here: Understanding STRESS and its components (C1, C2, C3) is crucial for appreciating why minimizing a lower bound can be effective and when it might fail.
  - Quick check question: Write the STRESS formula and explain what each term (C1, C2, C3) represents in the context of eigenvalue selection.

## Architecture Onboarding

- Component map:
  Input: Dissimilarity matrix D (n×n) -> Preprocessing: Double centering via C = I - 1/n 1n 1n^T to obtain Gram matrix B -> Core: Eigenvalue decomposition of B to get Λ and U -> Selection: Greedy or continuous algorithm to select/weight eigenvalues -> Output: Low-dimensional embedding X and bilinear form parameters

- Critical path:
  1. Compute centered matrix B = -1/2 CDC
  2. Perform eigendecomposition B = UΛU^T
  3. Apply EV-Selection (greedy) or continuous weighting to choose eigenvalues
  4. Construct embedding X = √Λ · Diag(w) · U^T (dropping zero rows)
  5. Compute output dissimilarity matrix ˆD from X

- Design tradeoffs:
  - Binary vs. continuous eigenvalue weighting: Binary selection is simpler and faster but may miss optimal combinations; continuous weighting is more flexible but computationally heavier and potentially prone to overfitting.
  - Eigenvalue magnitude vs. sign: Prioritizing magnitude can capture important structure but may include noisy negative eigenvalues; balancing sign helps maintain interpretability but may miss critical negative components.

- Failure signatures:
  - If STRESS increases with dimension despite using Neuc-MDS, it may indicate C3 is significant or that negative eigenvalues are dominated by noise.
  - If output dissimilarities contain many negative values unexpectedly, it could signal the continuous weighting is too aggressive or the input data is particularly noisy.

- First 3 experiments:
  1. Run Neuc-MDS on a synthetic Random-simplex dataset and verify STRESS decreases monotonically with dimension, unlike cMDS.
  2. Compare STRESS and average distortion on a non-Euclidean genomics dataset (e.g., Renal) between cMDS, Neuc-MDS, and Neuc-MDS+.
  3. Apply Neuc-MDS to a small real-world dataset (e.g., MNIST with k-NN dissimilarities) and visualize the embedding to check for meaningful structure preservation.

## Open Questions the Paper Calls Out
- The paper mentions that Neuc-MDS produces negative distances and that downstream modules expecting non-negative values may need special handling. It also notes that Neuc-MDS+ produces fewer negative distances than Neuc-MDS while maintaining similar performance. However, it does not provide empirical evidence of how negative distances affect downstream tasks.
- The paper mentions that for large datasets, fast approximation algorithms for partial SVD can be applied, and that landmark MDS can significantly speed up computation without much loss in performance. However, it does not provide systematic analysis of scalability.
- The paper mentions that for real-world data, the Gram matrix is likely far from a random matrix, and that aggressive dimension reduction can only be a luxury for structured data. It also mentions that the analysis points out that random measurement noise cannot be ignored. However, it does not provide a detailed analysis of how different data structures affect the performance of Neuc-MDS compared to cMDS.

## Limitations
- The paper's claims rest on the assumption that negative eigenvalues carry meaningful structural information rather than noise, which is not thoroughly validated across diverse datasets.
- The empirical validation is limited to 10 datasets, and the conditions under which Neuc-MDS might fail (e.g., when C3 dominates or when noise overwhelms signal) are not explored.
- The complexity of Neuc-MDS+ with continuous eigenvalue weighting raises concerns about overfitting and numerical stability, particularly for noisy real-world data.

## Confidence
- **High confidence**: The theoretical framework for using symmetric bilinear forms to generalize MDS and the derivation of STRESS error bounds are mathematically sound and well-supported.
- **Medium confidence**: The greedy EV-Selection algorithm effectively minimizes the lower bound of STRESS in controlled settings, but its performance on noisy or high-dimensional data requires further validation.
- **Low confidence**: The practical benefits of Neuc-MDS+ over Neuc-MDS are not clearly demonstrated, and the risk of overfitting with continuous eigenvalue weighting is not adequately addressed.

## Next Checks
1. Test Neuc-MDS on synthetic datasets with varying levels of random noise to determine at what point negative eigenvalues become dominated by noise, degrading embedding quality.
2. Evaluate the computational cost and runtime of Neuc-MDS+ versus Neuc-MDS and baselines on large-scale datasets (e.g., full MNIST or CIFAR-10) to assess practical feasibility.
3. Apply Neuc-MDS to a diverse set of non-Euclidean datasets from different domains (e.g., social networks, biological interactions) to confirm consistent performance gains over cMDS and SMACOF.