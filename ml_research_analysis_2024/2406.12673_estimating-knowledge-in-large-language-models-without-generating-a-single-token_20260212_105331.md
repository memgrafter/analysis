---
ver: rpa2
title: Estimating Knowledge in Large Language Models Without Generating a Single Token
arxiv_id: '2406.12673'
source_url: https://arxiv.org/abs/2406.12673
tags:
- keen
- accuracy
- entity
- subject
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces KEEN, a probe that estimates how much knowledge\
  \ an LLM has about a specific entity by analyzing only the model's internal hidden\
  \ representations of that entity's name\u2014before generating any output. KEEN\
  \ is trained to predict either the model's average QA accuracy on questions about\
  \ the entity or the factuality of open-ended responses it would generate."
---

# Estimating Knowledge in Large Language Models Without Generating a Single Token

## Quick Facts
- **arXiv ID**: 2406.12673
- **Source URL**: https://arxiv.org/abs/2406.12673
- **Reference count**: 40
- **Primary result**: KEEN estimates LLM knowledge about entities from hidden representations with 0.58-0.77 correlation to ground truth measures

## Executive Summary
KEEN is a probe that estimates how much knowledge a large language model has about a specific entity by analyzing only the model's internal hidden representations of that entity's name—before generating any output. The method capitalizes on findings that entity representations in upper-intermediate layers encode rich factual attributes. KEEN achieves strong correlations (0.58-0.68 for QA accuracy, 0.66-0.77 for factuality) across 7 different models, outperforms baselines, and naturally aligns with hedging behavior while tracking knowledge changes after fine-tuning.

## Method Summary
KEEN extracts hidden states from the last subject token in upper-intermediate layers when an LLM processes "This document describes [entity]". These representations are normalized and averaged into a feature vector, then passed through a linear probe with sigmoid activation to predict a knowledge score in [0,1]. The probe is trained on MSE loss to predict either average QA accuracy on questions about the entity or the factuality of open-ended responses the model would generate. A vocabulary-projection variant offers interpretability by highlighting tokens that cluster knowledge or indicate gaps.

## Key Results
- KEEN correlates strongly with ground-truth knowledge measures (0.58-0.68 for QA accuracy, 0.66-0.77 for factuality) across 7 models of different sizes and families
- Outperforms baselines based on attention outputs, fully-connected activations, or entity popularity
- Naturally aligns with model hedging behavior—scores decrease as hedging fraction increases
- Tracks knowledge changes after fine-tuning: scores increase for fine-tuned entities while decreasing for others
- Vocabulary-projection variant identifies knowledge clusters and gaps through token analysis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Hidden representations of named entities encode rich factual attributes that can be linearly extracted
- **Mechanism**: When an LLM processes a named entity, upper-intermediate layer hidden states capture structured knowledge about the entity, which KEEN probes with a linear transformation
- **Core assumption**: Entity knowledge is centralized in the hidden representations of the last subject token during inference, and these representations are linearly separable by a probe
- **Evidence anchors**: Geva et al. (2023) showed LLMs construct information-rich representations of subjects that encode many attributes; subject attributes can be extracted with simple linear functions
- **Break condition**: If entity knowledge is distributed across multiple tokens or layers non-linearly, or if the model does not centralize knowledge in entity representations, the probe would fail

### Mechanism 2
- **Claim**: KEEN scores correlate with model hedging behavior because both reflect internal uncertainty about entity knowledge
- **Mechanism**: When an LLM is uncertain about an entity, it tends to hedge (e.g., "I don't know") on questions about that entity; KEEN estimates this uncertainty by analyzing how the model represents the entity internally
- **Core assumption**: The model's hedging behavior is directly related to its internal uncertainty about entity knowledge, which is captured in the entity's hidden representations
- **Evidence anchors**: KEEN QA probe score estimates entity-based knowledge and should correlate with the fraction of questions that a model hedges on about the entity
- **Break condition**: If hedging is based on factors unrelated to entity knowledge (e.g., prompt formatting or model safety training), the correlation would break

### Mechanism 3
- **Claim**: KEEN scores reflect changes in model knowledge after fine-tuning because the probe captures residual information in intermediate representations even when later layers suppress it
- **Mechanism**: When a model is fine-tuned on entity-specific data, intermediate representations of those entities change to encode new information; KEEN detects this change even if later layers suppress recall
- **Core assumption**: Fine-tuning modifies intermediate representations in a detectable way, and KEEN can capture these changes even when the model's final outputs don't reflect them
- **Evidence anchors**: Training LLaMA2 on Wikipedia articles about certain entities increases their KEEN score while scores for other entities tend to decrease; fine-tuning often doesn't erase residual information in LLMs
- **Break condition**: If fine-tuning completely overwrites entity representations or if the probe cannot detect subtle changes in intermediate states, the correlation would break

## Foundational Learning

- **Linear probing and representation analysis**: Why needed here - KEEN relies on training simple linear probes over hidden representations to estimate knowledge. Quick check question: What is the difference between training a linear probe and fine-tuning the entire model for a task?
- **Entity representation in transformers**: Why needed here - The method assumes that named entities are represented in specific ways in transformer hidden states. Quick check question: In transformer models, which positions and layers typically contain the most entity-specific information?
- **Correlation analysis and evaluation metrics**: Why needed here - KEEN's effectiveness is measured through Pearson correlation between predicted scores and ground truth knowledge measures. Quick check question: What does a Pearson correlation of 0.65 mean in the context of predicting model knowledge from representations?

## Architecture Onboarding

- **Component map**: Input → LLM forward pass → Extract last subject token representations from layers L ∈ {3/4 L + k | k ∈ {-1, 0, 1}} → Normalize and average representations → Linear probe with sigmoid → Output knowledge score
- **Critical path**: Input → LLM forward pass → Extract last subject token representations from layers L ∈ {3/4 L + k | k ∈ {-1, 0, 1}} → Normalize and average representations → Linear probe with sigmoid → Output knowledge score
- **Design tradeoffs**: Using hidden states provides better performance than self-attention outputs or fully-connected activations, but hidden states are larger and more complex. Vocabulary projections offer interpretability at minimal performance cost but require handling large vocabulary spaces. The choice of layers affects both performance and computational cost.
- **Failure signatures**: Low correlation with ground truth metrics suggests the probe isn't capturing the right features; poor generalization to unseen entities indicates overfitting; inconsistent scores across model runs suggest instability in representation extraction.
- **First 3 experiments**:
  1. Train KEEN on GPT2-XL using the standard layer configuration and evaluate correlation with QA accuracy on the test set to establish baseline performance.
  2. Compare KEEN performance using different feature types (hidden states vs. self-attention outputs vs. fully-connected activations) on the same model to validate the choice of representation source.
  3. Test transfer learning by training KEEN on QA data and evaluating its correlation with FActScore on the OEG dataset to verify cross-task generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can KEEN be extended to estimate entity knowledge for non-entity subjects, such as abstract concepts or processes?
- **Basis in paper**: The paper discusses KEEN's focus on named entities and acknowledges that not all subjects of questions are entities, citing "How does exercise influence mental health?" as an example where there is no clear subject for KEEN application.
- **Why unresolved**: The paper does not explore or provide methods for adapting KEEN to handle non-entity subjects, leaving the feasibility and effectiveness of such an extension unknown.
- **What evidence would resolve it**: Experimental results showing KEEN's performance when applied to a dataset of non-entity subjects, comparing its predictions to ground truth knowledge measures for these subjects.

### Open Question 2
- **Question**: How does the performance of KEEN vary across different transformer-based LLM architectures beyond those tested in the paper?
- **Basis in paper**: The paper states, "Our evaluation focuses only on transformer-based auto-regressive LLMs" and suggests that future work should study KEEN's applicability to other model architectures, mentioning Mamba as a potential candidate.
- **Why unresolved**: The paper only evaluates KEEN on GPT2, Pythia, LLaMA2, and Vicuna models, limiting the generalizability of the findings to other transformer architectures.
- **What evidence would resolve it**: Comprehensive evaluation of KEEN on a diverse set of transformer-based LLM architectures, including both auto-regressive and encoder-decoder models, with performance metrics compared across architectures.

### Open Question 3
- **Question**: Can KEEN be used to identify specific facts or knowledge gaps within an entity's representation, beyond estimating overall knowledge levels?
- **Basis in paper**: The paper mentions that while KEEN estimates the extent of a model's knowledge about a subject, it does not identify the presence or lack of knowledge about specific facts, using the example of Napoleon's military academy attendance.
- **Why unresolved**: The paper does not propose or demonstrate methods for using KEEN to pinpoint specific factual knowledge or gaps, focusing instead on overall knowledge estimation.
- **What evidence would resolve it**: Development and validation of a method that leverages KEEN's features to predict the model's knowledge of specific facts about an entity, with results compared to ground truth fact-level knowledge measures.

## Limitations
- **Representation centralization assumption**: The method assumes entity knowledge is captured in hidden representations of the last subject token, but this assumption is not empirically validated across different entity types or contexts
- **Cross-dataset generalizability**: KEEN shows lower correlation (0.63-0.77) on FActScore compared to QA accuracy, suggesting effectiveness may vary significantly across different knowledge assessment methodologies
- **Fine-tuning interpretation ambiguity**: The paper's interpretation that KEEN captures residual information despite catastrophic forgetting is speculative and not supported by ablation studies or controlled experiments

## Confidence
- **High confidence (0.8-1.0)**: Correlation results between KEEN scores and ground truth knowledge measures (0.58-0.77) are well-documented with multiple models and datasets; vocabulary projection variant's interpretability claim is supported by specific qualitative examples
- **Medium confidence (0.5-0.8)**: Mechanistic claims about why KEEN works (knowledge centralization, correlation with hedging) are plausible but rely on cited prior work rather than direct empirical validation within this paper
- **Low confidence (0.0-0.5)**: Interpretation of fine-tuning results and claims about capturing residual information despite catastrophic forgetting are speculative without ablation studies or controlled experiments

## Next Checks
1. **Layer-wise correlation analysis**: Systematically evaluate KEEN's performance across all transformer layers to identify the optimal layer range and validate the upper-intermediate layer assumption
2. **Cross-entity type generalization**: Test KEEN's correlation with ground truth across different entity categories (persons, locations, organizations, concepts) to determine if the representation assumption holds uniformly or varies by entity type
3. **Controlled fine-tuning ablation**: Design experiments that separately manipulate entity-specific knowledge addition, model confidence calibration, and domain adaptation to isolate whether KEEN's changes reflect residual information, confidence shifts, or other factors