---
ver: rpa2
title: 'LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context
  Inference'
arxiv_id: '2406.18139'
source_url: https://arxiv.org/abs/2406.18139
tags:
- cache
- multimodal
- look-m
- arxiv
- merging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LOOK-M, a fine-tuning-free framework for
  efficient multimodal long-context inference by compressing KV cache. Unlike single-modality
  LLMs, multimodal long-context MLLMs face significant challenges due to the predominance
  of image tokens, making traditional KV cache optimization methods unsuitable.
---

# LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference

## Quick Facts
- arXiv ID: 2406.18139
- Source URL: https://arxiv.org/abs/2406.18139
- Reference count: 12
- Primary result: Achieves up to 1.5x faster decoding and 80-95% KV cache memory reduction while maintaining or enhancing performance on long-context multimodal tasks

## Executive Summary
LOOK-M introduces a fine-tuning-free framework for efficient multimodal long-context inference by compressing the KV cache in Multimodal Large Language Models (MLLMs). Unlike traditional methods that are unsuitable for multimodal settings due to the predominance of image tokens, LOOK-M leverages the observation that models prioritize textual attention over image features during prompt prefill. By implementing a text-prior method for KV cache compression and employing merging strategies to preserve global contextual information, LOOK-M achieves significant memory savings while maintaining or even enhancing performance across various long-context multimodal tasks.

## Method Summary
LOOK-M is a fine-tuning-free framework that compresses KV cache in multimodal long-context inference. The method observes that during prompt prefill, models prioritize textual attention over image features, enabling safe pruning of image tokens. It implements a text-prior eviction strategy based on attention scores, followed by merging strategies (averaged, pivotal, weighted) to consolidate information from evicted image tokens into preserved ones. This "look-once" approach makes pruning decisions during a single prompt encoding pass, allowing the model to effectively see the full image just once while maintaining contextual information throughout generation.

## Key Results
- Achieves up to 1.5x faster decoding speed during generation
- Reduces KV cache memory usage by 80-95% across tested MLLM architectures
- Maintains or enhances performance across temporal multi-image, semantic multi-image, needle in haystack, and image retrieval tasks on MileBench benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model prioritizes textual attention over image features during prompt prefill, enabling safe pruning of image tokens.
- Mechanism: During prompt encoding, attention scores are computed for all tokens. Text tokens receive artificially boosted attention scores (via `Tp = Max(As)`) ensuring they are preserved in the compressed cache. Image tokens with lower attention scores are candidates for eviction.
- Core assumption: The model's internal reasoning relies more heavily on textual representations than raw visual features for understanding global context.
- Evidence anchors:
  - [abstract] "We observe that during prompt prefill, the model prioritizes more textual attention over image features, and based on the multimodal interaction observation, a new proposed text-prior method is explored to compress the KV cache."
  - [section 3.2] "We observe that during prompt prefill, the model prioritizes more textual attention over image features, and based on the multimodal interaction observation, a new proposed text-prior method is explored to compress the KV cache."
- Break condition: If the model architecture changes such that visual tokens become equally or more important for reasoning, the text-prior strategy could degrade performance.

### Mechanism 2
- Claim: Merging evicted image tokens into nearby preserved tokens preserves global context without full retention.
- Mechanism: After evicting low-attention image tokens, a similarity matrix is computed between evicted and preserved tokens. Three merging strategies (averaged, pivotal, weighted) consolidate information from evicted tokens into the most similar preserved tokens.
- Core assumption: Nearby tokens in embedding space share contextual relevance, so merging preserves semantic information even when tokens are pruned.
- Evidence anchors:
  - [section 3.3] "To mitigate the loss of context information following the eviction of multimodal KV pairs, we explore various merging strategies during the prompt encoding phase."
- Break condition: If the similarity metric fails to capture true semantic relationships (e.g., due to domain shift or poor embeddings), merging could introduce noise instead of preserving context.

### Mechanism 3
- Claim: A "look-once" eviction during prefill is sufficient because the model only needs to see the full image once to encode its meaning.
- Mechanism: All pruning decisions are made during the single prompt encoding pass. No further token-level decisions are needed during generation.
- Core assumption: Once visual information is encoded into the KV cache (even partially), the model can reason about it throughout the generation process without re-accessing the raw image.
- Evidence anchors:
  - [abstract] "The term Look-Once in our method implies that pruning occurs only once during multimodal long prompt encoding, and the model effectively sees the full image just once."
- Break condition: If the model requires iterative refinement of visual understanding during generation (e.g., for complex spatial reasoning), single-pass encoding may be insufficient.

## Foundational Learning

- Concept: KV cache in transformers
  - Why needed here: Understanding how keys and values are stored and reused during autoregressive generation is essential to grasp why cache compression is beneficial.
  - Quick check question: What grows linearly with sequence length in transformer inference, causing memory and latency bottlenecks?

- Concept: Attention mechanisms and attention scores
  - Why needed here: The method relies on attention scores to identify which tokens to preserve or evict. Understanding how attention weights are computed and interpreted is crucial.
  - Quick check question: In a self-attention layer, what does a high attention score between two tokens indicate about their relationship?

- Concept: Token merging and similarity metrics
  - Why needed here: The merging strategies use cosine similarity to determine which evicted tokens should be merged into which preserved tokens. Understanding similarity metrics is key to evaluating the merging logic.
  - Quick check question: Why might cosine similarity be preferred over Euclidean distance when comparing token embeddings in high-dimensional space?

## Architecture Onboarding

- Component map:
  Input preprocessor -> Visual encoder (VIT) -> Projection layer -> Transformer layers -> LOOK-M module -> Output decoder

- Critical path:
  1. During prefill, compute attention scores for all tokens
  2. Boost text token scores and compute cumulative attention
  3. Evict lowest-scoring image tokens, preserving recent and important tokens
  4. Merge evicted tokens into similar preserved tokens using one of the three strategies
  5. Use compressed KV cache for generation

- Design tradeoffs:
  - Compression ratio vs. accuracy: Higher compression risks losing context; merging mitigates this but adds computation
  - Merging strategy choice: Averaged merging is simple but may dilute important features; weighted merging preserves more but is more complex
  - Text-prior vs. neutral eviction: Text-prior is safe for most tasks but may hurt tasks where visual detail is critical

- Failure signatures:
  - Performance drop on tasks requiring fine-grained visual detail (e.g., precise spatial reasoning)
  - Inconsistent behavior across different model architectures (e.g., works well on LLaVA but poorly on InternVL)
  - Memory savings less than expected due to merging overhead or ineffective eviction

- First 3 experiments:
  1. Run LOOK-M with text-prior eviction only (no merging) on a small subset of MileBench to measure impact of eviction alone
  2. Compare the three merging strategies (averaged, pivotal, weighted) on a fixed eviction set to determine which preserves accuracy best
  3. Test LOOK-M at extreme compression ratios (e.g., 95% reduction) to find the breaking point for each task type

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Effectiveness depends on the assumption that models consistently prioritize textual attention over image features, which may not hold for all MLLM architectures
- Merging strategies may introduce computational overhead that could offset some performance gains
- Limited exploration of edge cases where fine-grained visual detail is critical, leaving uncertainty about performance in specialized domains

## Confidence
- High Confidence: Claims about achieving 80-95% KV cache memory reduction and up to 1.5x faster decoding are supported by experimental results on MileBench
- Medium Confidence: The assertion that LOOK-M maintains or enhances performance across various long-context multimodal tasks is supported by results but may vary with different MLLM architectures or task domains
- Low Confidence: The claim that the text-prior method is universally applicable across all MLLM architectures without modification requires more validation across diverse model types

## Next Checks
1. **Cross-architecture validation**: Test LOOK-M on a broader range of MLLM architectures beyond LLaVA-v1.5 and InternVL to verify the text-prior strategy's effectiveness across different model designs and training paradigms

2. **Edge case analysis**: Evaluate LOOK-M's performance on tasks requiring fine-grained visual detail (e.g., medical imaging, technical drawings) to identify potential breaking points where visual token preservation is critical

3. **Computational overhead measurement**: Quantify the additional computational cost introduced by the merging strategies to determine if the memory savings translate to net performance improvements across different hardware configurations