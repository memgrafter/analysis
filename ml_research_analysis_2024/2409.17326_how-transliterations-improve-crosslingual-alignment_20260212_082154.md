---
ver: rpa2
title: How Transliterations Improve Crosslingual Alignment
arxiv_id: '2409.17326'
source_url: https://arxiv.org/abs/2409.17326
tags:
- similarity
- alignment
- language
- pairs
- crosslingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how transliteration-based training improves
  crosslingual alignment in multilingual language models. The authors train five model
  variants on Polish-Ukrainian and Hindi-Urdu pairs, incorporating original and transliterated
  data with various alignment objectives (MLM, TCM, TLM).
---

# How Transliterations Improve Crosslingual Alignment

## Quick Facts
- arXiv ID: 2409.17326
- Source URL: https://arxiv.org/abs/2409.17326
- Authors: Yihong Liu; Mingyang Wang; Amir Hossein Kargaran; Ayyoob Imani; Orgest Xhelili; Haotian Ye; Chunlan Ma; François Yvon; Hinrich Schütze
- Reference count: 40
- Primary result: Transliterations improve crosslingual alignment by acting as intermediaries between languages with different scripts, with TCM objectives being particularly effective

## Executive Summary
This paper investigates how transliteration-based training improves crosslingual alignment in multilingual language models, specifically for Polish-Ukrainian and Hindi-Urdu pairs. The authors train five model variants incorporating original and transliterated data with various alignment objectives (MLM, TCM, TLM) and find that auxiliary transliteration-based objectives like TCM improve alignment by helping models distinguish matched from random sentence pairs. However, better alignment does not consistently translate to improved downstream performance, particularly for sequential tasks. The study reveals that transliterations act as intermediaries in crosslingual alignment, with TCM being particularly effective at improving sentence-level representations.

## Method Summary
The study uses the Glot500-c dataset containing parallel sentences in Polish-Ukrainian and Hindi-Urdu pairs, with Uroman transliterating text to Latin script. Five model variants are trained from scratch using MLM, TCM, and TLM objectives with SentencePiece Unigram tokenizer (30K vocab size). Models are trained on 8 NVIDIA RTX 2080Ti GPUs using AdamW optimizer with specific hyperparameters (learning rate 1e-4, warmup 1000 steps, weight decay 0.01), evaluated using sentence retrieval top-k accuracy on SR-B and SR-F datasets, and tested on downstream tasks including SIB200, WikiANN, and Universal Dependencies.

## Key Results
- TCM (Translation Contrastive Matching) objective significantly improves sentence-level crosslingual alignment compared to MLM-only baselines
- Transliterations act as intermediaries, first aligning languages with their Latin-script versions before improving direct crosslingual alignment
- Better sentence-level alignment does not consistently translate to improved downstream performance, especially for sequential tasks like NER and POS tagging
- Lexical overlap between transliterated versions (L1_Latn and L2_Latn) is higher than between original versions, facilitating alignment

## Why This Works (Mechanism)

### Mechanism 1
Transliterations improve alignment by acting as an intermediary between languages with different scripts. Transliterations allow the model to first align language L1 with its own Latin-script version, then align L2 with its Latin-script version, and finally benefit from increased lexical overlap between L1_Latn and L2_Latn to improve direct L1-L2 alignment.

### Mechanism 2
Auxiliary transliteration-based alignment objectives (especially TCM) improve alignment by helping models distinguish matched pairs from random pairs. TCM explicitly optimizes for higher similarity between matched sentence pairs while simultaneously reducing similarity between random pairs, creating a larger similarity gap that improves alignment quality.

### Mechanism 3
Better crosslingual alignment does not guarantee improved downstream performance, especially for sequential tasks. While alignment improves sentence-level representation similarity, this doesn't translate to better task performance when fine-tuning requires learning language-specific patterns or when token-level alignment is crucial.

## Foundational Learning

- **Crosslingual alignment definitions and metrics**: Understanding different alignment definitions (weak vs. strong) and sentence retrieval as the primary evaluation metric is crucial for interpreting alignment improvements and what constitutes "better" alignment.

- **Transliteration and script conversion**: Understanding how Uroman converts text between scripts and affects lexical overlap is fundamental to interpreting why transliterated data helps alignment.

- **Contrastive learning objectives**: Understanding how TCM works at the sentence level, including what constitutes positive and negative samples, is essential for grasping why contrastive objectives improve alignment.

## Architecture Onboarding

- **Component map**: SentencePiece tokenizer → Pretraining with MLM/TCM/TLM → Alignment evaluation (sentence retrieval) → Downstream task evaluation
- **Critical path**: Data preprocessing (original + transliteration) → Model training with multiple objectives → Alignment metric computation → Downstream task transfer
- **Design tradeoffs**: Using original tokenizer for transliterated text simplifies implementation but may hurt tokenization quality vs. learning separate tokenizers for each script
- **Failure signatures**: Poor retrieval performance despite training completion, minimal difference between matched and random pair similarities, downstream performance not improving with alignment metrics
- **First 3 experiments**:
  1. Train Model-1 (original data only) and verify baseline retrieval performance to establish alignment capability without transliterations
  2. Train Model-2 (original + transliterated data) and measure similarity changes to verify transliteration impact on lexical overlap
  3. Train Model-3 (original + transliterated + TCM) and compare retrieval performance to quantify TCM's contribution to alignment improvement

## Open Questions the Paper Calls Out

### Open Question 1
How do transliteration-based objectives affect token-level alignment compared to sentence-level alignment? The paper notes that better sentence-level alignments don't consistently lead to improved crosslingual transfer for sequential tasks, but doesn't explore token-level alignment effects separately.

### Open Question 2
Does the intermediary role of transliterations in crosslingual alignment generalize to more than two related languages? The study is limited to two language pairs, so the intermediary effect hasn't been tested in multilingual settings.

### Open Question 3
How does the quality of the transliteration system affect crosslingual alignment and downstream performance? The study uses Uroman without comparing different approaches or evaluating how transliteration quality impacts results.

## Limitations
- Limited generalization beyond script-converting pairs - findings may not apply to languages sharing scripts
- Downstream performance disconnect - improved alignment metrics don't consistently translate to better task performance
- Evaluation metric dependencies - heavy reliance on sentence retrieval may not capture full alignment quality complexity

## Confidence

**High Confidence**: TCM significantly improves sentence-level crosslingual alignment with strong empirical support across both language pairs.

**Medium Confidence**: The intermediary role of transliterations is reasonably supported but the mechanism is somewhat speculative.

**Low Confidence**: The claim about alignment not guaranteeing downstream performance is observed but not well-explained, particularly for sequential tasks.

## Next Checks

1. **Cross-Script vs. Same-Script Pairs**: Replicate the study with language pairs that share scripts to determine whether intermediary benefits apply only when script conversion occurs.

2. **Token-Level Alignment Probing**: Conduct token alignment analysis to understand the relationship between sentence-level improvements and token-level crosslingual correspondence.

3. **Alternative Downstream Tasks**: Evaluate models on additional downstream tasks that might be more sensitive to sentence-level alignment improvements, such as crosslingual document classification.