---
ver: rpa2
title: Bridging the Gap Between Approximation and Learning via Optimal Approximation
  by ReLU MLPs of Maximal Regularity
arxiv_id: '2409.12335'
source_url: https://arxiv.org/abs/2409.12335
tags:
- then
- function
- which
- relu
- regularity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper bridges the gap between approximation and learning\
  \ theory by identifying a highly structured class of ReLU multilayer perceptrons\
  \ (MLPs) that are both optimal function approximators and statistically well-behaved.\
  \ The authors show that any (L,\u03B1)-H\xF6lder function from [0,1]^d to [-n,n]\
  \ can be approximated to a uniform O(1/n) error on [0,1]^d with a sparsely connected\
  \ ReLU MLP having the same H\xF6lder exponent \u03B1 and coefficient L, width O(dn^(d/\u03B1\
  )), depth O(log(d)), and O(dn^(d/\u03B1)) nonzero parameters."
---

# Bridging the Gap Between Approximation and Learning via Optimal Approximation by ReLU MLPs of Maximal Regularity

## Quick Facts
- arXiv ID: 2409.12335
- Source URL: https://arxiv.org/abs/2409.12335
- Authors: Ruiyang Hong; Anastasis Kratsios
- Reference count: 40
- Key outcome: Identifies ReLU MLPs that are both optimal function approximators and statistically well-behaved

## Executive Summary
This paper resolves a fundamental tension in neural network theory by identifying a highly structured class of ReLU multilayer perceptrons that achieve both optimal approximation rates and strong generalization guarantees. The authors construct a class of sparsely connected ReLU MLPs with weights restricted to {0, ±1/2} (except in first/last layers) that can approximate any Hölder continuous function with the same regularity exponent. Crucially, these networks maintain bounded empirical Rademacher complexity even as they grow arbitrarily large, enabling statistical reliability in overparameterized regimes. The key innovation uses Kuhn triangulations to fit linear pieces together without introducing steep segments that would break regularity.

## Method Summary
The method constructs ReLU MLPs using Kuhn triangulations to achieve optimal approximation of Hölder functions while preserving their regularity. The construction involves partitioning the hypercube into simplices via Kuhn triangulation, building piecewise linear approximators that interpolate dyadic grid points exactly, and constraining weights to {0, ±1/2} in intermediate layers while allowing larger magnitudes only in the first and last layers. This approach achieves uniform O(1/n) approximation error for (L,α)-Hölder functions on [0,1]^d with width O(dn^(d/α)), depth O(log(d)), and O(dn^(d/α)) nonzero parameters. The empirical Rademacher complexity remains bounded due to the weight constraints, yielding sample complexity O(log(N)/√N) for N training samples.

## Key Results
- Any (L,α)-Hölder function from [0,1]^d to [-n,n] can be approximated to uniform O(1/n) error with a ReLU MLP having width O(dn^(d/α)), depth O(log(d)), and O(dn^(d/α)) nonzero parameters
- The constructed networks have weights and biases in {0, ±1/2} except in first/last layers (magnitude ≤ n), yet maintain optimal approximation power
- These networks achieve near-optimal sample complexity O(log(N)/√N) for N i.i.d. normalized sub-Gaussian training samples
- The construction resolves the McShane extension problem on suitable finite sets using neural networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Kuhn triangulations allow fitting linear pieces together without introducing steep segments that break regularity
- Mechanism: The Kuhn triangulation partitions the hypercube into simplices that preserve the target function's Hölder exponent globally, unlike standard trifling-region approaches that require steep interpolating segments between flat regions
- Core assumption: The Kuhn triangulation is the only triangulation (up to reflections) that preserves regularity for all Lipschitz functions with linear moduli of continuity
- Evidence anchors:
  - [abstract]: "We achieve this through a new construction that perfectly fits together linear pieces using Kuhn triangulations"
  - [section]: "The Kuhn triangulation is the only triangulation (up to reflections) that makes Lemma 7.8 true for the case n=1"
  - [corpus]: Weak evidence - corpus papers focus on approximation rates but don't discuss Kuhn triangulations specifically
- Break condition: If the target function has discontinuities that prevent piecewise linear approximation within the Hölder constraint

### Mechanism 2
- Claim: Restricting weights to {0, ±1/2} in intermediate layers while maintaining bounded weights in first/last layers preserves regularity while maintaining approximation power
- Mechanism: This structured weight constraint creates a "maximally regular" network class where empirical Rademacher complexity remains bounded even as depth and width grow, enabling generalization in overparameterized regimes
- Core assumption: Weight constraints don't reduce approximation power below optimal rates for Hölder functions
- Evidence anchors:
  - [abstract]: "whose weights and biases take values in {0,±1/2} except in the first and last layers which instead have magnitude at-most n"
  - [section]: "the empirical Rademacher complexity of our class remains bounded even when its depth and width become arbitrarily large"
  - [corpus]: Weak evidence - corpus papers don't discuss weight constraints for generalization
- Break condition: When the target function requires weight magnitudes beyond the constraint for optimal approximation

### Mechanism 3
- Claim: The construction interpolates grid points exactly while maintaining the same Hölder exponent as the target function
- Mechanism: By building piecewise linear approximators that pass through dyadic grid points and using Kuhn triangulation, the network achieves uniform O(1/n) error while preserving the modulus of continuity
- Core assumption: Any Hölder function can be interpolated on a sufficiently fine dyadic grid while maintaining its regularity
- Evidence anchors:
  - [abstract]: "any (L,α)-Hölder function from [0,1]^d to [-n,n] can be approximated to a uniform O(1/n) error"
  - [section]: "there exists an α-Hölder continuous ReLU MLP Φ : Rd → R with coefficient L extending f"
  - [corpus]: Weak evidence - corpus papers focus on approximation rates but not exact interpolation
- Break condition: When the Hölder exponent α approaches 1 (Lipschitz case) where different construction methods may be needed

## Foundational Learning

- Concept: Hölder continuity and moduli of continuity
  - Why needed here: The paper's approximation guarantees are stated in terms of Hölder functions and their moduli of continuity
  - Quick check question: What is the difference between a Lipschitz function and a general Hölder function?

- Concept: Rademacher complexity and generalization bounds
  - Why needed here: The paper proves generalization bounds using empirical Rademacher complexity
  - Quick check question: How does Rademacher complexity relate to the ability of a function class to generalize?

- Concept: Kuhn triangulation and its properties
  - Why needed here: The construction relies on Kuhn triangulation to preserve regularity
  - Quick check question: Why is Kuhn triangulation special compared to other triangulations of the hypercube?

## Architecture Onboarding

- Component map: Input layer → Multiple hidden layers with ReLU activation → Output layer
  - Hidden layers use weights in {0, ±1/2} except first/last layers
  - Network interpolates dyadic grid points exactly
  - Kuhn triangulation determines how linear pieces fit together

- Critical path: Grid point selection → Kuhn simplex determination → Weight assignment → Regularity preservation verification
  - Grid spacing determines approximation error (O(1/n))
  - Weight constraints preserve regularity while maintaining approximation power

- Design tradeoffs: Weight constraints vs. approximation flexibility
  - More restrictive weight sets improve generalization but may limit approximation power
  - Kuhn triangulation ensures regularity preservation but may require more parameters

- Failure signatures: Steep interpolating segments indicate regularity violation
  - Non-interpolation of grid points indicates construction error
  - Bounded weights exceeding constraints indicate implementation error

- First 3 experiments:
  1. Implement 1D approximation of Lipschitz functions using Kuhn triangulation
  2. Test weight constraint enforcement in intermediate layers
  3. Verify Hölder regularity preservation for piecewise linear functions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental limits of achieving both optimal approximation rates and statistical generalization in neural networks with different activation functions (e.g., sigmoid, tanh, polynomial) compared to ReLU?
- Basis in paper: The paper focuses specifically on ReLU MLPs and their unique ability to balance approximation power with generalization. The authors note that "optimal approximation rates can be achieved while simultaneously imposing regularity on Φ," but do not explore whether this is unique to ReLU.
- Why unresolved: The paper establishes results for ReLU networks but does not systematically compare these results to other activation functions. The construction relies on specific properties of ReLU (piecewise linearity, exact implementation of linear pieces) that may not generalize.
- What evidence would resolve it: Empirical and theoretical studies comparing approximation rates, Lipschitz constants, and generalization bounds across different activation functions when using the Kuhn triangulation-based construction. Proofs showing whether similar constructions can achieve optimal rates with bounded regularity for other activations.

### Open Question 2
- Question: How does the Kuhn triangulation-based construction scale to non-Euclidean domains (e.g., manifolds, graphs) and what are the implications for approximation and generalization in these settings?
- Basis in paper: The authors note that "our results imply that neural networks can solve the McShane extension problem on suitable finite sets" and discuss the geometric nature of their construction. The paper's focus is on Euclidean cubes [0,1]^d.
- Why unresolved: The paper's geometric construction relies heavily on the structure of Euclidean space and the Kuhn triangulation. Extension to non-Euclidean domains would require new triangulation methods and analysis of how regularity is preserved under different geometries.
- What evidence would resolve it: Development of analogous triangulation schemes for manifolds and graphs, followed by analysis of approximation rates and generalization bounds in these settings. Construction of neural networks that can achieve optimal approximation with maximal regularity on non-Euclidean domains.

### Open Question 3
- Question: What is the precise relationship between the parameter constraints (weights in {0, ±1/2}, biases zero except first/last layers) and the generalization performance, and can these constraints be relaxed while maintaining statistical reliability?
- Basis in paper: The authors show that "unlike previously known 'large' classes of universal ReLU MLPs, the empirical Rademacher complexity of our class remains bounded even when its depth and width become arbitrarily large." They note specific parameter restrictions but do not fully characterize their necessity.
- Why unresolved: The paper establishes that these constraints work but does not prove they are necessary or explore the tradeoff space between parameter flexibility and generalization. The generalization bound holds for this class but the fundamental reason for its success is not fully explained.
- What evidence would resolve it: Theoretical analysis proving lower bounds on generalization performance when constraints are relaxed, or constructive proofs showing alternative constraint sets that maintain the same statistical guarantees. Empirical studies mapping the generalization-accuracy tradeoff as constraints are varied.

## Limitations
- The Kuhn triangulation construction has implementation complexity that increases dramatically with dimension
- Weight constraints to {0, ±1/2} in intermediate layers may limit practical approximation power for certain function classes
- Generalization bounds assume sub-Gaussian data distributions, which may not hold for all real-world datasets

## Confidence
- High confidence: The approximation error bounds O(1/n) for Hölder functions - these follow directly from the piecewise linear construction and standard approximation theory
- Medium confidence: The generalization bounds O(log(N)/√N) - while the Rademacher complexity arguments are sound, the assumptions about data distribution are restrictive
- Medium confidence: The weight constraint results - the theoretical framework is robust, but practical implications for function approximation need empirical validation

## Next Checks
1. Implement the Kuhn triangulation construction in dimension d=4 and verify regularity preservation for a known Hölder function
2. Test the weight-constrained network's approximation power against unconstrained ReLU networks on a suite of Hölder functions
3. Validate the generalization bounds experimentally by training on synthetic sub-Gaussian data and measuring test error as a function of sample size