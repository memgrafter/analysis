---
ver: rpa2
title: What Evidence Do Language Models Find Convincing?
arxiv_id: '2402.11782'
source_url: https://arxiv.org/abs/2402.11782
tags:
- question
- text
- evidence
- answer
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how language models assess the credibility of
  conflicting real-world evidence when answering controversial questions. The authors
  create a new dataset, ConflictingQA, that pairs such questions with real web documents
  containing different facts, argument styles, and answers.
---

# What Evidence Do Language Models Find Convincing?

## Quick Facts
- arXiv ID: 2402.11782
- Source URL: https://arxiv.org/abs/2402.11782
- Authors: Alexander Wan; Eric Wallace; Dan Klein
- Reference count: 40
- Primary result: Models prioritize query relevance over human-valued credibility cues when evaluating conflicting evidence

## Executive Summary
This paper investigates how language models assess the credibility of conflicting real-world evidence when answering controversial questions. The authors create a new dataset, ConflictingQA, pairing controversial questions with real web documents containing different facts, argument styles, and answers. They measure the "convincingness" of each document by how often models agree with its answer when compared against contradictory documents. Through sensitivity and counterfactual analyses, they find that models overwhelmingly rely on the relevance of a document to the query, while largely ignoring stylistic features like scientific references, objectivity, or technical language that humans find important for credibility. A simple perturbation that adds query-relevant text greatly increases a document's convincingness, while adding stylistic features often has no effect or even a negative one. This suggests models do not align with human credibility judgments and highlights the importance of high-quality retrieved evidence and potential shifts in how models are trained to better align with human preferences.

## Method Summary
The authors introduce ConflictingQA, a dataset of controversial questions paired with real web documents containing conflicting answers. They measure document "convincingness" by having models compare pairs of contradictory documents and recording which answer is selected more often. The dataset includes documents with varying levels of factual accuracy, argument styles, and credibility indicators. The researchers then conduct sensitivity analyses by systematically modifying documents with different types of content (relevant text, scientific references, objective language, etc.) and measuring how these changes affect convincingness scores. They also perform counterfactual analyses to understand which features drive model judgments by comparing documents that differ in specific characteristics.

## Key Results
- Models overwhelmingly rely on query relevance as the primary predictor of convincingness
- Stylistic features like scientific references, objectivity, and technical language have minimal impact on convincingness scores
- Adding query-relevant text significantly increases convincingness, while adding stylistic features often has no effect or negative effects
- Models show poor alignment with human credibility judgments on the same document pairs

## Why This Works (Mechanism)
The study reveals that current language models prioritize content relevance over human-valued credibility indicators when evaluating evidence. This mechanism appears rooted in how models are trained on large corpora where topical alignment often correlates with factual accuracy. The sensitivity analysis demonstrates that models respond strongly to content matching query terms but show weak or inconsistent responses to stylistic credibility markers. The methodology of comparing model agreement across contradictory documents provides a quantitative measure of convincingness that reveals this prioritization pattern. The findings suggest that models have learned a simplified heuristic: if text is relevant to the question, it is more likely to be correct, regardless of other credibility indicators that humans would consider important.

## Foundational Learning
- **Query relevance**: The degree to which document content matches the question terms; needed to understand the primary driver of model convincingness judgments; quick check: measure TF-IDF or semantic similarity between query and document
- **Stylistic credibility markers**: Features like scientific citations, objective language, and technical terminology that humans associate with credibility; needed to understand what features models are ignoring; quick check: count citation markers, identify subjective language using sentiment analysis
- **Document convincingness**: The probability that a model will agree with a document's answer when compared against contradictory documents; needed as the core metric for evaluation; quick check: conduct pairwise comparisons between contradictory documents
- **Sensitivity analysis**: Method of systematically modifying document features to measure their impact on model judgments; needed to identify which features influence convincingness; quick check: apply controlled perturbations and measure score changes
- **Counterfactual analysis**: Comparing documents that differ in specific characteristics to isolate the effect of individual features; needed to understand causal relationships; quick check: create document pairs differing in only one feature
- **Model agreement**: The frequency with which multiple models select the same answer; needed as a proxy for convincingness; quick check: aggregate predictions from multiple model instances

## Architecture Onboarding

**Component Map**: ConflictingQA dataset creation -> Model pairwise comparison -> Sensitivity analysis -> Counterfactual analysis -> Result synthesis

**Critical Path**: Dataset construction → Model comparison setup → Sensitivity testing → Analysis of feature importance → Interpretation of alignment gaps

**Design Tradeoffs**: The study prioritizes real-world document analysis over synthetic data, sacrificing control for ecological validity. Using model agreement as a proxy for convincingness trades direct human evaluation for scalability. The focus on controversial questions maximizes the need for credibility assessment but may limit generalizability to factual queries.

**Failure Signatures**: 
- Overreliance on surface-level relevance matching rather than deep comprehension
- Inconsistent responses to stylistic features suggesting lack of learned patterns
- Poor generalization when documents contain mixed relevance and credibility signals
- Sensitivity to query phrasing that may artificially inflate relevance scores

**First Experiments**:
1. Replicate pairwise comparisons with additional model families to test generalizability
2. Conduct human evaluation studies on the same document pairs to quantify alignment gaps
3. Create controlled document pairs with isolated feature differences to test specific causal hypotheses

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis may reflect artifacts of the evaluation methodology rather than fundamental model limitations
- Model agreement as a proxy for convincingness assumes majority opinion correlates with credibility
- The study focuses on specific question types and domains that may not generalize broadly
- Results may reflect pattern-matching rather than genuine understanding of relevance

## Confidence
- **High confidence**: Models prioritize query relevance over stylistic features in credibility judgments
- **Medium confidence**: Stylistic features like scientific references and objectivity have minimal impact on model convincingness scores
- **Low confidence**: Whether these findings generalize beyond the specific question types and domains tested in ConflictingQA

## Next Checks
1. Test the same methodology across multiple model families (including smaller models and different architectures) to verify if query relevance dominance is universal
2. Conduct human evaluation studies comparing human and model credibility judgments on identical document pairs to quantify alignment gaps
3. Design controlled experiments isolating specific stylistic features (e.g., exact scientific citations vs. similar formatting without citations) to determine if certain features have threshold effects not captured in the current analysis