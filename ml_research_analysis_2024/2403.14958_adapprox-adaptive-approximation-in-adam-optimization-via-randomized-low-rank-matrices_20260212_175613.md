---
ver: rpa2
title: 'Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank
  Matrices'
arxiv_id: '2403.14958'
source_url: https://arxiv.org/abs/2403.14958
tags:
- adapprox
- approximation
- memory
- matrix
- moment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adapprox, a memory-efficient optimization
  algorithm for large-scale deep learning models. Adapprox addresses the high memory
  consumption of Adam by using randomized low-rank matrix approximation to compress
  the second moment.
---

# Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices

## Quick Facts
- arXiv ID: 2403.14958
- Source URL: https://arxiv.org/abs/2403.14958
- Reference count: 18
- Primary result: Achieves 33.8%-49.9% memory savings compared to AdamW with first moment enabled, and 83.8%-99.9% savings without it

## Executive Summary
Adapprox introduces a memory-efficient optimization algorithm for large-scale deep learning by combining randomized low-rank matrix approximation with adaptive rank selection. The method targets the high memory consumption of Adam optimization, which stores both first and second moment matrices. By approximating the second moment matrix using randomized SVD and dynamically adjusting the approximation rank based on error thresholds, Adapprox significantly reduces memory usage while maintaining or improving convergence speed and generalization performance compared to existing memory-efficient optimizers like Adafactor and CAME.

## Method Summary
Adapprox addresses Adam's memory inefficiency by approximating the second moment matrix Vt using randomized low-rank matrix approximation. The algorithm employs a Streamlined Randomized Subspace Iteration (S-RSI) with power iteration and oversampling to efficiently compute the low-rank factors Q and U'. An adaptive rank selection mechanism dynamically adjusts the rank k based on approximation error measurements, using a sigmoid-based expansion function to balance memory savings against approximation accuracy. Optionally, when β1>0, Adapprox includes a cosine similarity guidance strategy that scales updates based on their alignment with the running average, potentially improving stability and convergence speed.

## Key Results
- Achieves 33.8%-49.9% memory savings compared to AdamW with first moment enabled on GPT-2 pretraining
- Demonstrates 83.8%-99.9% memory savings when first moment is omitted
- Shows improved convergence speed and generalization performance compared to Adafactor and CAME baselines
- Maintains competitive or superior performance on downstream tasks (SQuAD v1.1, CoLA, MRPC, SST-2, MNLI-m) after pretraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Randomized low-rank matrix approximation reduces memory usage by capturing dominant singular values of the second moment matrix.
- Mechanism: The second moment matrix Vt in Adam often has a few large singular values; Adapprox uses randomized SVD to approximate Vt with QU' where Q and U have low rank, preserving the dominant structure while reducing memory from O(mn) to O(k(m+n)).
- Core assumption: The singular value spectrum of Vt decays rapidly, making low-rank approximation effective.
- Evidence anchors:
  - [abstract] "Adapprox employs randomized low-rank matrix approximation for a more effective and accurate approximation of Adam's second moment."
  - [section] "the second moment matrices often have a limited number of dominant singular values, with the rest exhibiting substantially lower magnitudes... Adapprox employs randomized low-rank matrix approximation (Liberty et al., 2007; Li et al., 2014; Batselier et al., 2018) to effectively approximate the second moment in Adam."
  - [corpus] Weak; neighbors do not discuss singular value decay in optimization contexts.
- Break condition: If Vt's singular values are flat or uniformly distributed, low-rank approximation will lose accuracy and hurt convergence.

### Mechanism 2
- Claim: Adaptive rank selection dynamically balances memory savings and approximation accuracy.
- Mechanism: At regular intervals, Adapprox measures the Frobenius norm error of the current rank-k approximation; if error exceeds a threshold, it increases k according to a sigmoid function, bounded by kmax, to restore accuracy.
- Core assumption: The singular value distribution changes slowly over training steps, so frequent recomputation of rank is unnecessary.
- Evidence anchors:
  - [abstract] "Adapprox features an adaptive rank selection mechanism, finely balancing accuracy and memory efficiency."
  - [section] "To address this, we develop an adaptive rank selection mechanism, which dynamically adjusts k for each target matrix... If ξ rise above a threshold ξthresh, kt is adjusted to kt + f(ξ), ensuring that it does not exceed the maximum rank kmax."
  - [corpus] Weak; neighbors focus on adaptive stepsizes, not rank adaptation.
- Break condition: If the singular value distribution shifts rapidly, the fixed update interval may cause large, unrecoverable errors.

### Mechanism 3
- Claim: Cosine similarity guidance improves stability and convergence by scaling updates based on alignment with the running average.
- Mechanism: When β1>0, compute cos(θ) between current update Mt and running average Mt; scale Mt by (1 - cos(θ) + ϵ)⁻¹ so aligned updates are accelerated, misaligned ones are dampened.
- Core assumption: Updates that are consistently misaligned with the running average are likely noisy or harmful, and reducing them stabilizes training.
- Evidence anchors:
  - [abstract] "it includes an optional cosine similarity guidance strategy to enhance stability and expedite convergence."
  - [section] "this involves a heuristic method that utilizes the cosine similarity between the current update and the running average of updates to assess the confidence level of the approximation... When θcos approaches 1, it indicates a better alignment... Conversely, a θcos value close to −1 suggests poor alignment."
  - [corpus] Weak; neighbors discuss adaptive momentum, not cosine similarity.
- Break condition: If gradient directions naturally oscillate (e.g., due to minibatch noise), the guidance may overly dampen legitimate updates and slow convergence.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) and low-rank matrix approximation
  - Why needed here: Understanding how Vt can be compressed by truncating its SVD to the top k singular values is the core of Adapprox's memory reduction.
  - Quick check question: If Vt has singular values [10, 5, 0.1, 0.01], what rank-k approximation error is incurred if k=2?

- Concept: Randomized algorithms for matrix approximation (e.g., randomized SVD)
  - Why needed here: Adapprox uses Gaussian sketching and power iteration to efficiently approximate SVD without full decomposition.
  - Quick check question: What is the role of the oversampling parameter p in the randomized SVD?

- Concept: Adaptive stepsize methods in optimization (Adam, Adafactor, CAME)
  - Why needed here: Adapprox builds on Adam's adaptive learning rates; understanding the differences in how they store and update moments is critical.
  - Quick check question: How does Adafactor's factorization differ from Adapprox's low-rank approximation in terms of update form?

## Architecture Onboarding

- Component map: Gradient computation -> S-RSI algorithm -> Adaptive rank selection -> Mt computation -> Update clipping -> Weight decay
- Critical path:
  1. Compute gradient Gt
  2. Reconstruct Vt from Qt-1 U'ᵀ
  3. Apply AS-RSI to get Qt, U'ᵀ, kt
  4. Compute Mt = Gt / (√Vt + ϵ)
  5. Apply update clipping and optional cosine scaling
  6. Update parameters with weight decay
- Design tradeoffs:
  - Memory vs. accuracy: larger kmax improves accuracy but increases memory and computation
  - Rank update frequency (Δs): more frequent updates improve adaptation but cost more computation
  - Omitting β1: saves memory but slows convergence (supported by ablation experiments)
- Failure signatures:
  - Divergence: likely caused by rank too small or ξthresh too high
  - Slow convergence: could be from rank selection too conservative or cosine guidance too aggressive
  - High memory usage: kmax set too high relative to actual Vt rank
- First 3 experiments:
  1. Run Adapprox with kinit=1, kmax=1, β1=0.9 on GPT-2 117M; compare memory vs. validation loss vs. Adafactor.
  2. Enable cosine similarity guidance; measure effect on training stability and convergence speed.
  3. Vary ξthresh and observe rank evolution and validation loss trends to tune adaptive mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Adapprox's adaptive rank selection mechanism perform on other large-scale models beyond GPT-2, such as BERT or RoBERTa?
- Basis in paper: [explicit] The paper mentions the evaluation of Adapprox on GPT-2 models and downstream tasks but does not explore its performance on other architectures like BERT or RoBERTa.
- Why unresolved: The paper focuses on GPT-2, leaving the question of how the adaptive rank selection mechanism generalizes to other model architectures open.
- What evidence would resolve it: Testing Adapprox on BERT and RoBERTa pretraining and fine-tuning tasks, comparing memory usage and performance metrics against AdamW, Adafactor, and CAME.

### Open Question 2
- Question: What is the impact of varying the power iteration parameter l and oversampling parameter p in the S-RSI algorithm on the approximation error and computational efficiency?
- Basis in paper: [explicit] The paper sets l=5 and p=5 as default values in experiments but does not explore the sensitivity of the algorithm to these parameters.
- Why unresolved: The paper does not provide an ablation study or sensitivity analysis for the S-RSI parameters, leaving their optimal values unclear.
- What evidence would resolve it: Conducting experiments with different combinations of l and p values, measuring the resulting approximation errors and computation times to identify the optimal settings.

### Open Question 3
- Question: How does the optional cosine similarity guidance strategy affect convergence speed and stability in different learning rate regimes?
- Basis in paper: [explicit] The paper mentions the optional cosine similarity guidance strategy but only uses it in the pretraining phase, not in fine-tuning.
- Why unresolved: The paper does not provide a comprehensive analysis of the strategy's effectiveness across different learning rate schedules and phases of training.
- What evidence would resolve it: Implementing the cosine similarity guidance strategy in both pretraining and fine-tuning phases, experimenting with various learning rate schedules, and comparing convergence speed and stability metrics against Adapprox without the strategy.

## Limitations

- The singular value decay assumption for second moment matrices is asserted but not empirically validated across diverse models and datasets.
- The adaptive rank selection mechanism relies on fixed update intervals (Δs=10), which may be suboptimal if singular value distributions change rapidly during training.
- The cosine similarity guidance strategy is presented as optional but its impact is not fully characterized; the element-wise application details are underspecified.

## Confidence

- **High confidence:** The randomized low-rank approximation mechanism (Mechanism 1) is well-grounded in established randomized linear algebra literature and the implementation details are sufficiently specified.
- **Medium confidence:** The adaptive rank selection (Mechanism 2) is theoretically sound but its practical effectiveness depends on the validity of the slow-changing singular value distribution assumption.
- **Low confidence:** The cosine similarity guidance strategy (Mechanism 3) lacks detailed implementation specifications and its benefits are not thoroughly validated through ablation studies.

## Next Checks

1. **Singular value spectrum validation:** For a range of transformer models and datasets, empirically measure the singular value decay of second moment matrices to verify the rapid decay assumption that enables low-rank approximation.

2. **Rank evolution sensitivity analysis:** Systematically vary the adaptive rank update interval Δs and threshold ξthresh across different model sizes and training stages to identify optimal values and validate the slow-changing distribution assumption.

3. **Cosine guidance ablation with gradient statistics:** Compare training dynamics with and without cosine guidance while tracking gradient alignment statistics, variance, and update magnitudes to quantify its stabilizing effects and identify when it may be counterproductive.