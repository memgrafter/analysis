---
ver: rpa2
title: 'Vector Grimoire: Codebook-based Shape Generation under Raster Image Supervision'
arxiv_id: '2410.05991'
source_url: https://arxiv.org/abs/2410.05991
tags:
- grimoire
- strokes
- im2vec
- figure
- vector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GRIMOIRE, a novel text-guided SVG generative
  model trained with only raster image supervision. The method decomposes images into
  vector patches using a Visual Shape Quantizer (VSQ) that learns a discrete codebook
  of shape primitives, then uses an Auto-Regressive Transformer (ART) to model the
  joint distribution of shape tokens, positions, and text descriptions.
---

# Vector Grimoire: Codebook-based Shape Generation under Raster Image Supervision

## Quick Facts
- arXiv ID: 2410.05991
- Source URL: https://arxiv.org/abs/2410.05991
- Authors: Moritz Feuerpfeil; Marco Cipriano; Gerard de Melo
- Reference count: 40
- Key outcome: GRIMOIRE achieves superior SVG reconstruction quality compared to Im2Vec with lower MSE values (e.g., 0.014 vs 0.050 on Fonts) and better CLIPScore metrics, while learning from raster images rather than requiring direct SVG supervision.

## Executive Summary
GRIMOIRE introduces a novel text-guided SVG generative model that learns from raster images without requiring direct SVG supervision. The method combines a Visual Shape Quantizer (VSQ) that maps raster patches to discrete shape primitives with an Auto-Regressive Transformer (ART) that models the joint distribution of shape tokens, positions, and text descriptions. This approach enables both high-quality reconstruction of existing images and text-to-SVG generation, achieving state-of-the-art performance on MNIST, Fonts, and FIGR-8 datasets while being extensible to additional SVG attributes.

## Method Summary
GRIMOIRE consists of two main components: a Visual Shape Quantizer that learns to map raster patches onto a discrete codebook of shape primitives through differentiable rasterization, and an Auto-Regressive Transformer that models the joint probability distribution over shape tokens, positions, and textual descriptions. The VSQ is trained first to reconstruct patches as Bézier curves using MSE loss, then the ART learns to generate new SVG sequences conditioned on text by modeling the causal sequence probability of quantized shape indices paired with position and text tokens.

## Key Results
- Achieves MSE of 0.014 on Fonts dataset compared to Im2Vec's 0.050
- Demonstrates CLIPScore improvements indicating better text-image alignment
- Successfully performs SVG completion tasks and generates new shapes from text prompts
- Shows extensibility to additional SVG attributes like stroke width and color

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: VSQ learns a discrete codebook of shape primitives that can reconstruct raster patches with high fidelity using differentiable rasterization.
- **Mechanism**: The VSQ encoder maps raster patches into a latent space, which is quantized into a discrete codebook via finite scalar quantization. The decoder maps codebook indices back to Bézier curves and rasterizes them through DiffVG. MSE loss between the rasterized output and original patch trains the system.
- **Core assumption**: The finite scalar quantization with equidistant points in a hypercube captures sufficient shape diversity to reconstruct patches.
- **Evidence anchors**:
  - [abstract]: "A Visual Shape Quantizer (VSQ) learns to map raster images onto a discrete codebook by reconstructing them as vector shapes"
  - [section 3.1]: Describes codebook definition, encoder/decoder structure, and reconstruction loss
  - [corpus]: Weak - related papers focus on tokenization or language modeling, not codebook-based reconstruction
- **Break condition**: If the codebook is too small or quantization too coarse, reconstruction quality degrades and the auto-regressive model cannot learn meaningful shape distributions.

### Mechanism 2
- **Claim**: Auto-regressive Transformer learns the joint distribution of shape tokens, positions, and text descriptions, enabling text-to-SVG generation.
- **Mechanism**: After VSQ training, each patch is mapped to a codebook index and paired with its discretized position and tokenized text. The Transformer models the causal sequence probability p(x_i | x_<i), enabling auto-regressive generation of new SVG sequences conditioned on text.
- **Core assumption**: The discrete token sequence preserves sufficient information about shape, position, and semantics for the Transformer to model the joint distribution effectively.
- **Evidence anchors**:
  - [abstract]: "an Auto-Regressive Transformer (ART) models the joint probability distribution over shape tokens, positions and textual descriptions"
  - [section 3.2]: Details sequence construction, token types, and Transformer architecture
  - [corpus]: Weak - related papers do not describe joint token-position-text modeling for vector graphics
- **Break condition**: If position quantization or text tokenization introduces too much noise, the Transformer cannot learn meaningful dependencies and generation quality suffers.

### Mechanism 3
- **Claim**: Geometric constraint loss improves stroke reconstruction by encouraging elongated, connected Bézier paths.
- **Mechanism**: The geometric loss penalizes irregular control point placement by measuring scaled inner distances between points in a stroke and their deviation from the mean. This encourages longer, more coherent strokes during reconstruction.
- **Core assumption**: Penalizing irregular control point distances leads to more natural-looking Bézier strokes that better approximate the original shapes.
- **Evidence anchors**:
  - [section 3.1]: "we propose a novel geometric constraint Lgeom, which punishes control point placement of irregular distances"
  - [section 7.6]: Reports qualitative improvements in stroke coherence when using the geometric constraint
  - [corpus]: Weak - no related work explicitly uses geometric constraints for vector graphics reconstruction
- **Break condition**: If the geometric constraint weight is too high, strokes may degenerate into overly stretched or unnatural shapes, hurting reconstruction quality.

## Foundational Learning

- **Concept**: Vector quantization and discrete latent representations
  - **Why needed here**: Enables mapping continuous raster patches to a finite set of shape primitives that can be efficiently modeled by an auto-regressive Transformer.
  - **Quick check question**: How does finite scalar quantization differ from vector quantization with learned embeddings, and why might it be preferable for this task?

- **Concept**: Differentiable rasterization of vector graphics
  - **Why needed here**: Allows backpropagating raster reconstruction loss through vector primitives (Bézier curves), enabling end-to-end training without direct SVG supervision.
  - **Quick check question**: What are the key advantages of DiffVG over approximate rasterization methods for this application?

- **Concept**: Auto-regressive sequence modeling with Transformers
  - **Why needed here**: Models the joint distribution of shape tokens, positions, and text descriptions to generate coherent SVG sequences from text prompts.
  - **Quick check question**: Why is causal attention necessary in the Transformer decoder for this task, and what would happen if bidirectional attention were used instead?

## Architecture Onboarding

- **Component map**: Raster patch → VSQ encoder → quantization → VSQ decoder → rasterization → MSE loss; patch + position + text → VSQ encoder → index sequence → ART → new index sequence → VSQ decoder → SVG output
- **Critical path**: Raster patch → VSQ encoder → codebook index → VSQ decoder → rasterization → MSE loss → VSQ training; then: patch + position + text → VSQ encoder → index sequence → ART → new index sequence → VSQ decoder → SVG output
- **Design tradeoffs**:
  - Codebook size vs. expressiveness: larger codebooks improve reconstruction but increase memory and training time
  - Segment count per shape: more segments allow finer detail but risk overfitting and increased token count
  - Position discretization: finer grids improve placement accuracy but increase token vocabulary size
- **Failure signatures**:
  - High MSE reconstruction loss: codebook too small or quantization too coarse
  - Low CLIPScore but reasonable FID: text conditioning not learned, focus on visual similarity only
  - Fragmented SVG outputs: missing or ineffective post-processing
  - Degenerate strokes: geometric constraint weight too high
- **First 3 experiments**:
  1. Train VSQ on MNIST with codebook size 4,375 and segment count 15; measure reconstruction MSE and qualitative patch quality
  2. Train ART on MNIST using VSQ encodings; generate digits from text and evaluate CLIPScore vs. ground truth
  3. Enable geometric constraint with weight 0.4; compare stroke coherence and reconstruction MSE on Fonts dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How would GRIMOIRE's performance change with different codebook sizes and dimensionalities for the Visual Shape Quantizer (VSQ)?
- **Basis in paper**: [explicit] The paper discusses using a codebook size of 4,375 with 5 dimensions (L = [7, 5, 5, 5, 5]) and mentions that 60.09% of the codebook was used for FIGR-8, with the top 10 strokes making up 41.24% of the dataset.
- **Why unresolved**: The paper doesn't systematically explore how different codebook sizes or dimensionalities affect reconstruction quality, generative performance, or efficiency. The authors mention that the top 24 and 102 strokes make up roughly 50% and 75% respectively, suggesting potential for optimization.
- **What evidence would resolve it**: Systematic experiments varying codebook size (e.g., 1,000, 4,375, 10,000) and dimensionality (e.g., 3, 5, 7 dimensions) while measuring reconstruction MSE, FID, CLIPScore, and computational efficiency on MNIST, Fonts, and FIGR-8 datasets.

### Open Question 2
- **Question**: Can GRIMOIRE effectively generate complex SVG attributes like gradients, patterns, or transparency (alpha values) beyond the basic color and stroke width demonstrated?
- **Basis in paper**: [explicit] The paper mentions that the VSQ can be extended to predict continuous values for any visual attribute supported by the differentiable rasterizer and briefly demonstrates color and stroke width prediction.
- **Why unresolved**: The paper only demonstrates basic color and stroke width extensions. Complex SVG features like gradients, patterns, or transparency require different modeling approaches and may interact with the vector quantization in non-trivial ways.
- **What evidence would resolve it**: Experiments extending the VSQ to predict gradient stops, pattern fills, or alpha values, with quantitative evaluation on datasets containing such features (if available) or qualitative assessment on synthetic data with ground truth attributes.

### Open Question 3
- **Question**: How does GRIMOIRE's text-conditioned generation compare to using class labels or other forms of weak supervision for datasets without textual descriptions?
- **Basis in paper**: [inferred] The paper demonstrates text-conditioned generation on MNIST, Fonts, and FIGR-8, but doesn't explore alternative conditioning methods for datasets lacking textual metadata.
- **Why unresolved**: While text conditioning shows superior performance, many real-world datasets lack textual descriptions. Alternative weak supervision methods (class labels, attribute tags, or even unsupervised clustering) could provide more general applicability.
- **What evidence would resolve it**: Comparative experiments using class labels, attribute tags, or unsupervised cluster assignments as conditioning signals on the same datasets, measuring FID, CLIPScore, and qualitative diversity of generated samples against the text-conditioned baseline.

## Limitations
- Evaluation primarily compares against Im2Vec with limited benchmarking against other SVG generation methods
- Qualitative evaluation is sparse and relies heavily on visual inspection rather than systematic user studies
- Geometric constraint loss effectiveness lacks comprehensive ablation studies for optimal weight parameters
- Scalability to complex real-world images beyond controlled datasets remains unproven

## Confidence

**High Confidence**: The core mechanism of using vector quantization for shape reconstruction from raster patches is well-established, and the MSE improvements over Im2Vec are substantial and statistically significant across multiple datasets.

**Medium Confidence**: The text-to-SVG generation capability shows promise through CLIPScore improvements, but the limited dataset diversity and lack of user studies for subjective quality assessment reduce confidence in real-world applicability.

**Low Confidence**: The geometric constraint loss effectiveness is demonstrated qualitatively but lacks rigorous quantitative validation. The optimal parameter settings for codebook size, segment count, and geometric constraint weight are not thoroughly explored.

## Next Checks

1. **Cross-dataset generalization test**: Evaluate GRIMOIRE on more complex, real-world datasets beyond MNIST, Fonts, and FIGR-8 to assess scalability and robustness to diverse visual content.

2. **Comprehensive ablation study**: Systematically vary codebook size, segment count, and geometric constraint weight to identify optimal configurations and quantify their individual contributions to reconstruction and generation quality.

3. **Extended benchmark comparison**: Compare GRIMOIRE against a broader range of SVG generation methods including SVGBuilder, StrokeNUWA, and semantic approaches to establish relative performance across multiple metrics and use cases.