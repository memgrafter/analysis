---
ver: rpa2
title: 'Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers
  Using Guided Masking'
arxiv_id: '2401.16575'
source_url: https://arxiv.org/abs/2401.16575
tags:
- masking
- image
- verb
- probing
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces guided masking as an alternative to image-text
  matching for probing multimodal transformer models. Instead of matching pairs, the
  method masks target words (e.g., verbs) and measures the model's accuracy in predicting
  them.
---

# Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking

## Quick Facts
- arXiv ID: 2401.16575
- Source URL: https://arxiv.org/abs/2401.16575
- Reference count: 8
- Primary result: Guided masking achieves over 75% accuracy in verb prediction on SVO-Probes and 80% on V-COCO, significantly outperforming prior image-text matching methods

## Executive Summary
This paper introduces guided masking as an alternative to image-text matching for probing multimodal transformer models' understanding of verbs. Instead of relying on holistic matching scores, the method masks target words (particularly verbs) and measures the model's accuracy in predicting them. Experiments on SVO-Probes and V-COCO datasets demonstrate that models like ViLBERT, LXMERT, UNITER, and VisualBERT achieve significantly higher accuracy in verb prediction than previously documented using image-text matching approaches. Vision ablation experiments confirm that visual input plays a crucial role in verb prediction, suggesting that multimodal transformers have stronger verb understanding than previously documented.

## Method Summary
The method involves masking linguistic tokens (particularly verbs) in image captions and using the model's masked language modeling head to predict the masked words. The accuracy of these predictions serves as a proxy for verb understanding. Vision ablation is performed by masking visual tokens associated with subjects in the image, and comparing prediction accuracy with and without visual input. The approach is evaluated on SVO-Probes and V-COCO datasets using multimodal transformer models (ViLBERT, LXMERT, UNITER, VisualBERT) that were pretrained on image-text pairs.

## Key Results
- Multimodal transformers achieve over 75% top-5 accuracy in predicting masked verbs on SVO-Probes dataset
- Models reach 80% accuracy on V-COCO dataset for masked verb prediction
- Vision ablation causes a performance drop of approximately 13%, confirming visual input's importance for verb understanding
- BERT baseline achieves only 36.1% accuracy, highlighting the contribution of multimodal pretraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Guided masking enables more precise probing of multimodal transformers than image-text matching by directly testing the model's ability to predict masked linguistic tokens.
- Mechanism: Instead of relying on holistic matching scores, the model's internal masked language modeling head is used to predict specific masked words (e.g., verbs). The accuracy of these predictions, especially under vision ablation, reveals the model's understanding of linguistic concepts and their grounding in visual features.
- Core assumption: The masked language modeling head in multimodal transformers retains sufficient capability to predict masked tokens accurately, and this prediction task is a valid proxy for measuring understanding.
- Evidence anchors:
  - [abstract] "The proposed approach ablates different modalities using masking and assesses the model's ability to predict the masked word with high accuracy."
  - [section] "We propose using the guided masking probing technique, which involves masking tokens representing specific linguistic aspects of language that we want to probe and quantify the model's ability to predict the masked token."
  - [corpus] Weak: No direct evidence found in corpus about accuracy of masked predictions specifically for verbs.

### Mechanism 2
- Claim: Vision ablation in guided masking reveals the extent to which visual features contribute to the prediction of masked words, indicating cross-modal grounding.
- Mechanism: By masking visual tokens associated with the subject of the activity (or the whole image), the model's performance in predicting the masked verb decreases if visual grounding is important. The degree of performance drop quantifies the visual contribution.
- Core assumption: Visual tokens are correctly identified and associated with linguistic subjects, and masking them removes relevant information for verb prediction.
- Evidence anchors:
  - [section] "We employed vision ablation to evaluate verb grounding in visual inputs of subjects engaged in activities... The model's ability to predict the masked word should drop when such vital visual information is ablated."
  - [section] "The ablation of the whole image leads to a drop in performance by around 13%."
  - [corpus] No direct evidence found in corpus about vision ablation effects on verb prediction.

### Mechanism 3
- Claim: Comparing guided masking results with a language-only model (BERT) isolates the contribution of multimodal pretraining to verb understanding.
- Mechanism: BERT's accuracy on the same masked verb prediction task provides a baseline for language-only understanding. The improvement of multimodal models over BERT indicates the added value of visual input and cross-modal training.
- Core assumption: BERT is a valid baseline for language-only understanding and that the multimodal models are initialized with BERT, allowing for a fair comparison.
- Evidence anchors:
  - [section] "Since the VL models are initialized with BERT, comparing the results of complete image ablation and BERT can also suggest how over-fitting BERT on CC captions boosts performance."
  - [section] "BERT's top 5 accuracy was only 36.1%."
  - [corpus] No direct evidence found in corpus about BERT vs. multimodal model comparison on verb prediction.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: Guided masking relies on the model's ability to predict masked words, which is a core capability developed during MLM pretraining.
  - Quick check question: Can you explain how MLM pretraining works in BERT and its variants?

- Concept: Cross-Modal Grounding
  - Why needed here: The paper argues that vision ablation reveals how visual features contribute to understanding verbs, which requires understanding cross-modal grounding.
  - Quick check question: What is cross-modal grounding and why is it important for multimodal models?

- Concept: Vision-Language Transformers
  - Why needed here: The paper studies specific architectures (ViLBERT, LXMERT, UNITER, VisualBERT) and their ability to understand verbs, requiring familiarity with these models.
  - Quick check question: What are the key differences between ViLBERT, LXMERT, UNITER, and VisualBERT in terms of architecture and pretraining objectives?

## Architecture Onboarding

- Component map: Image → Faster R-CNN → ROI features + position embeddings → multimodal transformer → [MASK] token prediction → evaluation (accuracy, vision ablation)
- Critical path: Image → Faster R-CNN → ROI features + position embeddings → multimodal transformer → [MASK] token prediction → evaluation (accuracy, vision ablation)
- Design tradeoffs: Guided masking offers more precise probing than image-text matching but requires masking tokens and may be sensitive to incorrect visual token identification. Vision ablation can reveal grounding but assumes accurate subject identification.
- Failure signatures: Low prediction accuracy for masked verbs even with full vision input may indicate poor MLM training or lack of verb understanding. Minimal performance drop under vision ablation may suggest over-reliance on language priors or incorrect visual token identification.
- First 3 experiments:
  1. Implement guided masking on a small subset of SVO-Probes to verify prediction accuracy for masked verbs.
  2. Apply vision ablation (masking subject ROI) to the same subset and measure the drop in prediction accuracy.
  3. Compare the results of the multimodal model with BERT on the same masked verb prediction task to isolate the contribution of visual input.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does guided masking perform when applied to multimodal transformers using ViT patch features instead of ROI features?
  - Basis in paper: [explicit] The paper mentions that guided masking could be extended to work with multimodal transformers using ViT patch features like ALBEF, VLMo, or X-VLM using a different method for vision ablation.
  - Why unresolved: The current study focuses on multimodal transformers that consider regions of interest (ROI) features obtained by object detectors as input tokens. The effectiveness of guided masking on transformers using ViT patch features is not yet tested.
  - What evidence would resolve it: Empirical results comparing guided masking performance on models using ROI features versus ViT patch features.

- **Open Question 2**: How does the accuracy of guided masking change when considering semantic variations beyond the top 5 predictions?
  - Basis in paper: [inferred] The paper acknowledges that predicting a fitting but different word for an image poses a challenge, and suggests that a more comprehensive evaluation beyond the top 5 predictions could yield even more substantial improvements.
  - Why unresolved: The current study limits the evaluation to the top 5 predictions, which may not capture all semantic variations that could be considered correct.
  - What evidence would resolve it: Results showing accuracy improvements when expanding the evaluation to include more than the top 5 predictions, considering caption semantic variety.

- **Open Question 3**: What is the impact of using different object detectors on the performance of guided masking in multimodal transformers?
  - Basis in paper: [explicit] The paper mentions that the proposed method is agnostic when studying specific aspects of language and could be applied to any model with masked language modeling as a pre-training objective.
  - Why unresolved: The current study uses Faster R-CNN for obtaining ROI features, but the impact of using different object detectors on guided masking performance is not explored.
  - What evidence would resolve it: Comparative results showing guided masking performance using different object detectors (e.g., YOLO, EfficientDet) in the multimodal transformer models.

## Limitations

- **Implementation Detail Uncertainty**: The paper lacks specific implementation details for guided masking and vision ablation procedures, creating uncertainty about the exact methodology used, particularly in visual token identification.
- **Evaluation Metric Sensitivity**: The top-5 accuracy metric may underestimate true understanding if predicted verbs are semantically similar but not exact matches to ground truth.
- **Dataset Bias Concerns**: Results may be influenced by biases in SVO-Probes and V-COCO datasets, potentially limiting generalizability to other domains.

## Confidence

- **High Confidence**: The claim that multimodal transformers achieve high accuracy (>75%) in predicting masked verbs on SVO-Probes and V-COCO datasets is supported by the experimental results presented in the paper.
- **Medium Confidence**: The assertion that vision ablation reveals the contribution of visual features to verb prediction is based on observed performance drops when visual tokens are masked.
- **Low Confidence**: The claim that guided masking provides a more precise probing technique than image-text matching lacks direct comparative evidence on the same datasets and models.

## Next Checks

1. **Implementation Verification**: Implement the guided masking and vision ablation procedures exactly as described in the paper and verify the reported accuracy results on a small subset of SVO-Probes and V-COCO datasets.

2. **Cross-Dataset Generalization**: Evaluate the multimodal models on additional datasets with diverse linguistic and visual content to assess the generalizability of the high verb prediction accuracy.

3. **Ablation Studies on Visual Token Identification**: Perform ablation studies on the visual token identification process to assess its impact on vision ablation results, systematically varying the accuracy of subject identification.