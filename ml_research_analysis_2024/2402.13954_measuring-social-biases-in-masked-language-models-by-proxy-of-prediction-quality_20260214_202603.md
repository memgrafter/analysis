---
ver: rpa2
title: Measuring Social Biases in Masked Language Models by Proxy of Prediction Quality
arxiv_id: '2402.13954'
source_url: https://arxiv.org/abs/2402.13954
tags:
- bias
- race
- biases
- mlms
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates social biases encoded in masked language models
  (MLMs) using proxy functions based on prediction quality. The authors propose several
  measures, including attention-weighted variants, to assess MLM preference for biased
  sentences targeting disadvantaged versus advantaged groups.
---

# Measuring Social Biases in Masked Language Models by Proxy of Prediction Quality

## Quick Facts
- arXiv ID: 2402.13954
- Source URL: https://arxiv.org/abs/2402.13954
- Reference count: 6
- This paper evaluates social biases in masked language models using proxy functions based on prediction quality, showing that proposed measures outperform existing methods in detecting biases.

## Executive Summary
This paper introduces a novel approach to measuring social biases in masked language models (MLMs) by leveraging prediction quality as a proxy. The authors propose several measures, including attention-weighted variants, to assess MLM preferences for biased sentences targeting disadvantaged versus advantaged groups. These measures are evaluated using benchmark datasets and compared against existing methods, demonstrating superior performance in detecting social biases. The study shows that all MLMs encode concerning social biases, with the proposed measures being particularly sensitive to biases introduced after retraining MLMs on biased datasets.

## Method Summary
The authors propose using prediction quality as a proxy to measure social biases in MLMs. They introduce several bias measures that compare the prediction quality of sentences containing biases against disadvantaged groups versus advantaged groups. These measures include attention-weighted variants that incorporate the attention scores from the MLMs. The proposed measures are evaluated on benchmark datasets and compared against existing bias detection methods. Additionally, the authors demonstrate the sensitivity of their measures by artificially injecting biases into MLMs and showing that their proposed measures can detect these biases more accurately than other methods.

## Key Results
- All MLMs evaluated encode concerning social biases, with proposed measures showing superior performance compared to existing methods
- The proposed measures demonstrate higher sensitivity and accuracy in detecting biases introduced after retraining MLMs on biased datasets
- Attention-weighted variants of the proposed measures show improved performance in certain scenarios

## Why This Works (Mechanism)
The proposed method works by leveraging the relationship between prediction quality and bias detection. By measuring how well an MLM can predict masked tokens in biased versus unbiased sentences, the authors create a proxy for bias that correlates with human judgments. The attention-weighted variants further enhance this by incorporating the model's internal attention mechanisms, which can reveal subtle patterns in how the model processes biased information.

## Foundational Learning
- **Masked Language Models**: Why needed - Core technology being evaluated for biases; Quick check - Understand how MLMs predict masked tokens
- **Prediction Quality Metrics**: Why needed - Forms the basis of the proxy measures; Quick check - Know how prediction accuracy is measured
- **Attention Mechanisms**: Why needed - Used in attention-weighted variants; Quick check - Understand how attention scores are calculated and used
- **Bias Detection Methods**: Why needed - Context for comparing proposed measures; Quick check - Familiarity with existing bias detection approaches
- **Benchmark Datasets**: Why needed - Used for evaluation; Quick check - Know the characteristics and limitations of the datasets used
- **Social Bias Categories**: Why needed - Defines the scope of bias being measured; Quick check - Understand the binary gender and race categories used

## Architecture Onboarding
**Component Map**: Input data -> Preprocessing -> MLM prediction -> Prediction quality calculation -> Bias measure computation -> Comparison with benchmarks
**Critical Path**: The most critical path is from MLM prediction to bias measure computation, as this directly impacts the accuracy of bias detection
**Design Tradeoffs**: Using prediction quality as a proxy trades computational efficiency for potential accuracy in bias detection. Attention-weighted variants add complexity but may capture more nuanced biases
**Failure Signatures**: If prediction quality doesn't correlate with bias, the proxy measures will fail. Similarly, if attention mechanisms don't capture relevant information, attention-weighted variants may underperform
**Three First Experiments**: 1) Evaluate proposed measures on a simple, well-understood bias dataset; 2) Compare prediction quality correlation with human bias annotations; 3) Test attention-weighted variants against non-weighted versions

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses primarily on binary gender and race categories, potentially overlooking intersectional biases and other demographic dimensions
- The evaluation is conducted on English-language datasets, limiting generalizability to multilingual contexts
- The artificial nature of post-retraining biases may not fully represent real-world scenarios

## Confidence
- High: The methodology for constructing proxy-based bias measures is well-founded and the comparative analysis against existing methods is rigorous
- Medium: The claim that proposed measures are more sensitive to post-retraining biases is supported, though the artificial nature of these biases may not fully represent real-world scenarios
- Medium: The superiority of proposed measures over existing methods is demonstrated, but the generalizability to other types of biases and contexts requires further validation

## Next Checks
1. Evaluate the proposed measures on multilingual datasets to assess their effectiveness across different languages and cultural contexts
2. Test the measures on real-world MLMs that have been fine-tuned on potentially biased data, rather than artificial bias injection
3. Extend the evaluation to include intersectional bias detection by incorporating multiple demographic attributes simultaneously