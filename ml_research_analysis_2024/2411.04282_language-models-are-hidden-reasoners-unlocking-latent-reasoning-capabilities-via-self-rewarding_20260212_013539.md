---
ver: rpa2
title: 'Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities
  via Self-Rewarding'
arxiv_id: '2411.04282'
source_url: https://arxiv.org/abs/2411.04282
tags:
- reasoning
- latro
- arxiv
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LaTRO is a framework that formulates LLM reasoning as latent variable
  sampling and optimizes it through variational approaches. The method enables models
  to self-improve reasoning capabilities by treating rationales as latent variables
  and using the model's own probability estimates for evaluation, without requiring
  external reward models or task-specific few-shot examples.
---

# Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding

## Quick Facts
- **arXiv ID**: 2411.04282
- **Source URL**: https://arxiv.org/abs/2411.04282
- **Reference count**: 33
- **Primary result**: LaTRO improves zero-shot accuracy by 12.5% over base models on GSM8K

## Executive Summary
LaTRO introduces a novel framework for unlocking latent reasoning capabilities in pre-trained language models by treating reasoning rationales as latent variables and optimizing them through variational approaches. The method enables self-improvement without external reward models or task-specific few-shot examples by using the model's own probability estimates for evaluation. The approach demonstrates consistent improvements across multiple model architectures including Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B on challenging reasoning datasets like GSM8K and ARC-Challenge.

## Method Summary
LaTRO formulates LLM reasoning as latent variable sampling where reasoning chains (rationales) are treated as unobserved variables that need to be inferred. The framework employs variational inference to optimize these latent rationales by maximizing the marginal likelihood of correct answers while using the model's own probability estimates as self-rewards. This self-rewarding mechanism allows the model to iteratively improve its reasoning capabilities without requiring external supervision or task-specific demonstrations. The optimization process discovers high-quality reasoning paths that the base model might not generate through standard prompting alone.

## Key Results
- Improves zero-shot accuracy by 12.5% over base models on GSM8K dataset
- Outperforms supervised fine-tuning by 9.6% on GSM8K reasoning tasks
- Demonstrates consistent gains across multiple architectures (Phi-3.5-mini, Mistral-7B, Llama-3.1-8B) and datasets (GSM8K, ARC-Challenge)

## Why This Works (Mechanism)
The core mechanism leverages the observation that pre-trained LLMs possess latent reasoning capabilities that are not directly accessible through standard prompting. By treating rationales as latent variables and applying variational optimization, LaTRO can explore the space of possible reasoning chains and identify those that lead to correct answers. The self-rewarding aspect is crucial - the model uses its own probability estimates to evaluate the quality of generated rationales, creating a self-supervised learning loop that progressively improves reasoning quality without external supervision.

## Foundational Learning
- **Variational Inference**: Statistical method for approximating intractable probability distributions, needed for optimizing latent variables when exact computation is impossible. Quick check: Verify understanding of ELBO (Evidence Lower Bound) formulation.
- **Latent Variable Models**: Framework where some variables are not directly observed but influence the observed data, essential for modeling reasoning as hidden processes. Quick check: Understand the relationship between observed answers and latent rationales.
- **Self-Supervised Learning**: Training paradigm where models generate their own supervision signals, critical for LaTRO's reward mechanism. Quick check: Confirm how probability estimates serve as rewards.
- **Chain-of-Thought Reasoning**: Method of prompting models to generate intermediate reasoning steps, foundational concept LaTRO builds upon. Quick check: Compare standard CoT vs LaTRO's latent variable approach.
- **Expectation-Maximization Algorithm**: Iterative optimization technique for parameter estimation with latent variables, conceptually similar to LaTRO's optimization. Quick check: Understand EM vs variational inference differences.
- **Reinforcement Learning from Human Feedback (RLHF)**: Related framework where models learn from reward signals, provides context for self-rewarding mechanisms. Quick check: Contrast external vs internal reward signals.

## Architecture Onboarding

**Component Map**: Data -> Base LLM -> Variational Optimizer -> Self-Reward Module -> Improved LLM

**Critical Path**: The variational optimizer samples different reasoning chains (rationales) from the base LLM, the self-reward module evaluates these chains using the model's own probability estimates, and the optimizer updates the latent variable distribution to favor high-reward rationales. This cycle repeats until convergence, producing an improved reasoning model.

**Design Tradeoffs**: LaTRO trades computational efficiency for improved reasoning performance by performing iterative optimization, whereas standard prompting is single-pass but limited. The self-reward mechanism eliminates dependency on external reward models but introduces potential for reward hacking. The variational approach provides principled uncertainty quantification but requires careful hyperparameter tuning.

**Failure Signatures**: Performance degradation may occur when reasoning tasks are substantially different from training distribution, the self-reward mechanism may get stuck in local optima favoring plausible-but-incorrect reasoning chains, and the computational overhead may outweigh benefits for simpler reasoning tasks where standard prompting suffices.

**First Experiments**: (1) Compare LaTRO's reasoning quality against standard chain-of-thought prompting on identical problems, (2) Measure improvement trajectory over optimization iterations to identify convergence behavior, (3) Test robustness by applying LaTRO to out-of-distribution reasoning tasks.

## Open Questions the Paper Calls Out
Major uncertainties remain about the scalability and generalizability of the LaTRO framework. While improvements are demonstrated across multiple model architectures and datasets, the method's performance on more diverse reasoning tasks and larger model scales has not been thoroughly evaluated. The reliance on self-rewarding introduces potential for reward hacking or suboptimal optimization paths that may not generalize to unseen problems. Additionally, the computational overhead of the variational optimization process compared to simpler prompting techniques requires further quantification.

## Limitations
- Performance on diverse reasoning tasks beyond GSM8K and ARC-Challenge remains unvalidated
- Potential for reward hacking or suboptimal optimization paths due to self-rewarding mechanism
- Computational overhead of variational optimization not fully quantified against simpler methods

## Confidence
- **High**: The core variational inference framework and self-rewarding mechanism are technically sound
- **Medium**: The generalizability of improvements across diverse reasoning domains
- **Medium**: The claimed performance improvements across different model scales and architectures

## Next Checks
1. Systematic evaluation of LaTRO's performance degradation when applied to reasoning tasks substantially different from training distribution
2. Ablation studies isolating the contribution of the variational optimization from the self-rewarding mechanism
3. Direct comparison of computational efficiency and final performance against established methods like chain-of-thought prompting and supervised fine-tuning under identical computational budgets