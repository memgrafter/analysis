---
ver: rpa2
title: 'Towards Socially and Morally Aware RL agent: Reward Design With LLM'
arxiv_id: '2401.12459'
source_url: https://arxiv.org/abs/2401.12459
tags:
- agent
- language
- reward
- items
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using large language models (LLMs) to provide
  reward signals for reinforcement learning agents, aiming to align agent behavior
  with human social and moral norms. The core method involves prompting an LLM to
  rank the appropriateness or desirability of agent actions (e.g., breaking a vase,
  killing a person, taking a shower in public) on a -10 to 10 scale.
---

# Towards Socially and Morally Aware RL agent: Reward Design With LLM

## Quick Facts
- arXiv ID: 2401.12459
- Source URL: https://arxiv.org/abs/2401.12459
- Authors: Zhaoyue Wang
- Reference count: 13
- One-line primary result: LLM-generated rewards enable RL agents to learn socially and morally aware behaviors in 2D grid worlds

## Executive Summary
This paper presents a novel approach to aligning reinforcement learning agents with human social and moral norms by using large language models (LLMs) to generate reward signals. The method prompts an LLM to rank the appropriateness of agent actions on a -10 to 10 scale, using these judgments as proxy rewards. A replay buffer mechanism compares trajectories to promote globally optimal policies, while context-specific prompts enable learning of situation-dependent social norms. Experiments in 2D grid worlds demonstrate the approach's effectiveness in avoiding negative side effects, achieving global optimality, and respecting context-dependent social norms, with human evaluation showing strong agreement with LLM judgments.

## Method Summary
The approach uses tabular Q-learning augmented with LLM-generated rewards in 2D grid world environments. The agent interacts with the environment using five actions (UP, DOWN, LEFT, RIGHT, USE) and receives state information and goals. An LLM (GPT3.5) is prompted to evaluate individual actions or compare trajectories, providing scalar rewards that update the agent's Q-values. A replay buffer stores trajectories and periodically prompts the LLM to compare pairs, rewarding states in the preferred trajectory to promote global optimality. Context-specific prompts enable learning of situation-dependent social norms, such as showering in private but not in public spaces.

## Key Results
- Agents successfully avoid negative side effects (e.g., breaking vases) using LLM-generated rewards
- Replay buffer mechanism enables convergence to globally optimal policies by preferring less severe negative consequences
- Context-dependent prompts allow agents to learn appropriate social norms based on situational context
- Human evaluation shows strong agreement with LLM judgments, validating the approach's alignment with human values

## Why This Works (Mechanism)

### Mechanism 1
The LLM serves as a proxy reward function encoding human moral and social values. By ranking actions on a -10 to 10 scale, it provides scalar feedback for RL agents to learn value-aligned behaviors without manual reward function design. This assumes the LLM has sufficient understanding of human norms to provide appropriate rewards. Evidence shows strong human-LLM agreement in evaluations, though misalignment could lead to undesirable agent behaviors.

### Mechanism 2
The replay buffer enables learning of globally optimal policies rather than locally optimal ones. By periodically comparing trajectories and rewarding states in the preferred ones, the agent learns to choose paths with less severe negative consequences. The LLM's ability to accurately compare trajectories is crucial for this mechanism. Evidence shows decreased frequency of negative interactions in later episodes, though the approach lacks theoretical guarantees for global optimality.

### Mechanism 3
Context-dependent prompts enable the LLM to provide rewards that reflect situation-appropriate social norms. By framing prompts within specific contexts (e.g., public vs. private spaces), the agent learns different behaviors for different situations. This relies on the LLM's ability to understand and respond appropriately to contextual information. Evidence demonstrates the agent's ability to shower in private but avoid it in public, though the approach's robustness to cultural variations is untested.

## Foundational Learning

- Concept: Reinforcement Learning (RL)
  - Why needed here: Provides the underlying learning framework for the agent
  - Quick check question: What is the difference between model-based and model-free RL?

- Concept: Reward Function Design
  - Why needed here: Focuses on using LLMs to automatically design reward functions aligned with human values
  - Quick check question: What are some common challenges in manually designing reward functions for RL?

- Concept: Large Language Models (LLMs)
  - Why needed here: Serves as the source of proxy reward signals and trajectory comparison
  - Quick check question: What are some key capabilities of LLMs that make them suitable for this application?

## Architecture Onboarding

- Component map: 2D Grid World Environment -> RL Agent (tabular Q-learning) -> LLM (GPT3.5) -> Replay Buffer -> Human Evaluation

- Critical path:
  1. Agent interacts with environment and receives state and goal
  2. Agent decides on action and interacts with environment
  3. LLM is prompted to evaluate action or compare trajectories
  4. LLM response is used to update agent's Q-values
  5. Process repeats until goal is reached or maximum steps are exceeded

- Design tradeoffs:
  - Using LLM for rewards vs. manually designing reward functions
  - Using replay buffer to compare trajectories vs. not using one
  - Using context-specific prompts vs. generic prompts for LLM

- Failure signatures:
  - Agent fails to reach goal
  - Agent learns to perform undesirable actions
  - LLM provides inconsistent or inappropriate rewards
  - Replay buffer fails to identify globally optimal trajectories

- First 3 experiments:
  1. Simple Vase: Test agent's ability to avoid negative side effects using LLM as reward signal
  2. Vase and Person: Test agent's ability to learn globally optimal policies using replay buffer and LLM comparison
  3. Public vs Private Sphere: Test agent's ability to learn context-dependent social norms using context-specific LLM prompts

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed approach scale to more complex environments with dynamic and context-dependent consequences? The current experiments use simple 2D grid worlds with static consequences. Scaling to more complex environments would require handling dynamic and context-dependent consequences, which is not addressed in the paper. Experiments in environments with probabilistic events or context-dependent consequences would resolve this.

### Open Question 2
How robust is the language model's reward signal to different interpretations of social norms and moral values across different cultures or individuals? While the paper shows strong agreement between human and LLM judgments, it does not address cultural variations or individual differences in moral values. Testing the approach with diverse cultural contexts and individual variations would resolve this question.

### Open Question 3
How does the proposed approach compare to other methods for incorporating human values into reinforcement learning, such as inverse reinforcement learning or direct human feedback? The paper mentions related work but does not directly compare the proposed approach to other methods. A comprehensive comparison of performance, scalability, and alignment with human values would resolve this question.

## Limitations

- Reliance on LLM judgments without formal guarantees of alignment with human values
- Experiments limited to small 2D grid worlds, leaving scalability to complex environments uncertain
- Approach requires careful prompt engineering and may be sensitive to prompt variations

## Confidence

- Mechanism 1 (LLM as proxy reward): Medium - Supported by human evaluation but lacks formal alignment guarantees
- Mechanism 2 (Replay buffer for global optimality): Low-Medium - Qualitative evidence but no theoretical bounds
- Mechanism 3 (Context-dependent rewards): Medium - Demonstrates capability in controlled experiments but limited scope

## Next Checks

1. Test the approach with adversarial prompts designed to elicit misaligned rewards from the LLM
2. Evaluate performance degradation when training and test distributions of social/moral scenarios differ
3. Scale up to more complex environments with larger state/action spaces to assess practical limitations