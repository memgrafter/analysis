---
ver: rpa2
title: Stochastic Kernel Regularisation Improves Generalisation in Deep Kernel Machines
arxiv_id: '2410.06171'
source_url: https://arxiv.org/abs/2410.06171
tags:
- kernel
- deep
- neural
- learning
- inducing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the generalisation gap between deep kernel
  machines (DKMs) and neural networks, particularly on complex image classification
  tasks like CIFAR-10. DKMs, while theoretically appealing as kernel-based models
  with representation learning, lagged behind neural networks in performance (92.7%
  vs ~95% accuracy on CIFAR-10).
---

# Stochastic Kernel Regularisation Improves Generalisation in Deep Kernel Machines

## Quick Facts
- arXiv ID: 2410.06171
- Source URL: https://arxiv.org/abs/2410.06171
- Authors: Edward Milsom; Ben Anson; Laurence Aitchison
- Reference count: 19
- Primary result: Achieves 94.5% test accuracy on CIFAR-10, matching neural network performance

## Executive Summary
This work addresses the generalisation gap between deep kernel machines (DKMs) and neural networks on complex image classification tasks. DKMs, while theoretically appealing as kernel-based models with representation learning, lagged behind neural networks (92.7% vs ~95% accuracy on CIFAR-10). The authors introduce two key improvements: stochastic kernel regularisation (SKR), which adds noise to learned Gram matrices during training, and single-precision floating-point arithmetic with Taylor approximation for numerical stability. These improvements enable DKMs to match neural network performance at 94.5% test accuracy on CIFAR-10 while maintaining the theoretical advantages of kernel methods.

## Method Summary
The authors improve DKM generalisation through two main contributions: (1) stochastic kernel regularisation (SKR), which introduces Wishart-distributed noise to Gram matrices during training to reduce overfitting, and (2) single-precision floating-point arithmetic (TF32) with Taylor approximation for log-determinant terms to enable faster, more stable training. The method is applied to a convolutional DKM architecture with ResNet20-inspired blocks, using inducing point schemes and mixup parameters. The resulting model achieves 94.5% test accuracy on CIFAR-10, matching neural network performance while maintaining the theoretical advantages of kernel methods.

## Key Results
- Achieves 94.5% test accuracy on CIFAR-10, matching similarly-sized neural networks
- Outperforms previous DKM approaches by over 1.8% on CIFAR-10
- Demonstrates 5× faster training using TF32 precision with Taylor approximations
- Shows improved numerical stability through combined SKR and low-precision training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stochastic kernel regularisation (SKR) reduces overfitting by introducing controlled noise into learned Gram matrices during training.
- Mechanism: By sampling from a Wishart distribution centered on the learned Gram matrices, SKR introduces variance inversely proportional to γ, preventing the Gram matrices from becoming too deterministic and overfitting the training data.
- Core assumption: Adding noise to Gram matrices during training, while keeping them deterministic at test time, provides a regularising effect similar to dropout in neural networks.
- Evidence anchors:
  - [abstract] "stochastic kernel regularisation, which adds noise to the learned Gram matrices during training"
  - [section 3.1] "Inspired by dropout in neural networks (Srivastava et al., 2014), we introduce random noise into the training process to reduce overfitting of representations"
  - [corpus] Weak evidence - no directly related papers found in corpus
- Break condition: If γ is too small (high variance noise), the optimisation may become unstable and fail to converge.

### Mechanism 2
- Claim: Using Taylor approximation for the log-determinant and trace terms in the KL divergence improves numerical stability, enabling lower-precision training.
- Mechanism: The Taylor expansion around λi = 1 (where λi are eigenvalues of G⁻¹K) replaces unstable operations like log-determinant with simpler Frobenius norm calculations that are more stable in low-precision arithmetic.
- Core assumption: The Gram matrices learned by DKMs are close to the NNGP kernel, so G⁻¹K ≈ I and the Taylor expansion is valid.
- Evidence anchors:
  - [section 3.2] "We replaced the log-determinant and trace terms with their second-order Taylor expansions"
  - [section 3.2] "This should be understood as a function, with two arguments, G and K. To evaluate the objective (Eq. 2), we would set G = Gℓ, and K = K(Gℓ)"
  - [corpus] Weak evidence - no directly related papers found in corpus
- Break condition: If the learned Gram matrices deviate significantly from the NNGP kernel (G⁻¹K far from I), the Taylor approximation becomes inaccurate.

### Mechanism 3
- Claim: Single-precision floating-point arithmetic with TF32 cores accelerates training 5×, enabling more epochs within computational budget.
- Mechanism: Modern GPUs are highly optimised for lower-precision operations, with TF32 offering 8× speedup over FP64 while maintaining reasonable precision (19 bits including sign bit).
- Core assumption: The numerical stability improvements (SKR and Taylor approximation) are sufficient to prevent overflow/underflow issues when using lower-precision arithmetic.
- Evidence anchors:
  - [section 3.2] "Using TF32 cores makes training roughly5× faster than the implementation in Milsom et al. (2024)"
  - [section 3.2] "Modern GPUs are highly optimised for lower-precision floating point operations"
  - [corpus] Weak evidence - no directly related papers found in corpus
- Break condition: If numerical stability improvements are insufficient, the lower-precision arithmetic will cause failures in Cholesky decompositions and other matrix operations.

## Foundational Learning

- Concept: Deep kernel machines (DKMs)
  - Why needed here: Understanding DKMs is fundamental to grasping how this work improves upon them. DKMs are kernel-based methods that learn representations by transforming Gram matrices through learnable mappings.
  - Quick check question: What is the key difference between DKMs and deep Gaussian processes? (Answer: DKMs work with Gram matrices instead of feature vectors, and the Gram matrices become deterministic in the infinite-width limit)

- Concept: Gaussian process kernel functions
  - Why needed here: The work uses convolutional kernel functions and depends on understanding how kernels operate on Gram matrices. The arccos and squared-exponential kernels are specifically mentioned.
  - Quick check question: Why can many kernel functions be computed using only pairwise dot products? (Answer: Because they depend on inputs only through their dot products, allowing reparameterisation in terms of Gram matrices)

- Concept: Stochastic regularisation techniques
  - Why needed here: The work draws direct inspiration from dropout and applies similar principles to kernel methods. Understanding dropout helps explain why adding noise to Gram matrices during training improves generalisation.
  - Quick check question: How does dropout in neural networks relate to stochastic kernel regularisation? (Answer: Both introduce controlled randomness during training to prevent overfitting, while maintaining deterministic behaviour at test time)

## Architecture Onboarding

- Component map:
  Input -> Convolutional blocks (ResNet20-inspired) -> SKR layer -> Taylor approximation -> GP classification with categorical likelihood -> Adam optimizer

- Critical path:
  1. Forward pass through convolutional DKMs with SKR sampling
  2. Compute sparse DKM objective with Taylor-approximated KL terms
  3. Backpropagate through SKR sampling and Taylor approximations
  4. Update inducing Gram matrices and mixup parameters

- Design tradeoffs:
  - Precision vs speed: Using TF32 enables 5× speedup but requires numerical stability improvements
  - Regularisation strength: SKR parameter γ must balance noise level against optimisation stability
  - Approximation accuracy: Taylor expansion must be accurate enough for convergence while providing stability

- Failure signatures:
  - Numerical errors in Cholesky decompositions (often from ill-conditioned Gram matrices)
  - Training instability with very small γ values in SKR
  - Poor generalisation if ν (KL regularisation strength) is too large

- First 3 experiments:
  1. Run CIFAR-10 classification with default hyperparameters to establish baseline performance
  2. Test ablation: Disable SKR (keep jitter only) to verify its contribution to accuracy
  3. Test ablation: Disable Taylor approximation to verify its contribution to numerical stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strength of stochastic kernel regularization (γ) for different network architectures and datasets?
- Basis in paper: [inferred] The paper shows that SKR with γ = Pℓᵢ/4 improves performance on CIFAR-10, but also notes that different values of γ affect the condition number of Gram matrices differently
- Why unresolved: The paper only tests a single value of γ and doesn't explore the full parameter space or test on other datasets
- What evidence would resolve it: Systematic experiments varying γ across different architectures (not just ResNet) and datasets, measuring both generalization performance and numerical stability

### Open Question 2
- Question: Does the Taylor approximation to the KL divergence terms actually improve generalization, or is the improvement solely due to better numerical stability?
- Basis in paper: [explicit] The paper states "we believe this would require further investigation to verify" regarding whether the Taylor approximation improves log-likelihood
- Why unresolved: The ablation study shows improved log-likelihood with the Taylor approximation, but the authors acknowledge this needs more investigation to separate stability effects from true regularization effects
- What evidence would resolve it: Experiments comparing the Taylor approximation to exact KL divergence on datasets where both can be computed stably, controlling for numerical precision effects

### Open Question 3
- Question: Why does SGD outperform Adam for neural networks but not for deep kernel machines on these tasks?
- Basis in paper: [explicit] The paper notes that "SGD is well known to train neural networks with better generalisation properties" and that SGD "achieved a higher test accuracy of 95.36%" for the neural network, but the authors "found it generally less stable than Adam" for DKMs
- Why unresolved: The paper observes this discrepancy but doesn't provide a theoretical explanation for why the optimization landscape differs between neural networks and DKMs
- What evidence would resolve it: Analysis of the optimization landscape and gradient statistics for both models, potentially through visualization of loss surfaces or study of gradient norms and directions

## Limitations

- Limited to CIFAR-10 and CIFAR-100 datasets, with no testing on larger or more complex datasets like ImageNet
- Theoretical justification for SKR relies on intuitive parallels to dropout rather than rigorous proofs of its regularising effect on kernel methods
- Taylor approximation's validity depends on Gram matrices being close to the NNGP kernel, which may not hold for all architectures or datasets

## Confidence

- Performance improvement: High - well-demonstrated empirically with significant accuracy gains
- SKR mechanism: Medium - intuitive parallels to dropout but lacks rigorous theoretical proof
- Numerical stability improvements: Medium - plausible but dependent on specific conditions and approximations

## Next Checks

1. Test SKR on larger datasets (ImageNet) to assess scalability and whether the regularisation benefits persist with more complex data.
2. Conduct ablation studies with different γ values in SKR to identify optimal regularisation strength and verify the claimed mechanism.
3. Compare training dynamics with and without Taylor approximation to quantify the numerical stability improvements and verify the approximation accuracy.