---
ver: rpa2
title: 'READoc: A Unified Benchmark for Realistic Document Structured Extraction'
arxiv_id: '2409.05137'
source_url: https://arxiv.org/abs/2409.05137
tags:
- document
- read
- table
- arxiv
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces READoc, a unified benchmark for document
  structured extraction (DSE) that addresses the fragmented and localized evaluation
  paradigms in existing benchmarks. READoc defines DSE as a realistic task of converting
  unstructured PDFs into semantically rich Markdown, using a dataset of 2,233 diverse
  real-world documents from arXiv and GitHub.
---

# READoc: A Unified Benchmark for Realistic Document Structured Extraction

## Quick Facts
- arXiv ID: 2409.05137
- Source URL: https://arxiv.org/abs/2409.05137
- Reference count: 33
- Primary result: Introduces READoc benchmark with 2,233 diverse documents for realistic document structured extraction

## Executive Summary
READoc addresses the fragmented evaluation landscape in document structured extraction (DSE) by introducing a unified benchmark that converts unstructured PDFs into semantically rich Markdown. The benchmark uses 2,233 diverse real-world documents from arXiv and GitHub, providing a more realistic evaluation framework than existing localized benchmarks. The study reveals significant performance gaps between current DSE systems and the realistic extraction objective, highlighting the need for new modeling paradigms.

## Method Summary
The READoc benchmark defines DSE as converting unstructured PDF documents into semantically rich Markdown format, moving beyond traditional sequential token labeling approaches. The authors developed a DSE Evaluation S3uite with three key modules: Standardization (normalizing evaluation outputs), Segmentation (dividing documents into logical sections), and Scoring (quantifying extraction quality). The benchmark was constructed using 2,233 diverse documents from arXiv and GitHub repositories, representing various document types and complexities. The evaluation framework provides unified assessment of state-of-the-art DSE systems, enabling direct comparison across different approaches.

## Key Results
- Expert visual models achieved 81.42% average score on arXiv documents
- Vision-language models and pipeline tools performed substantially worse than expert visual models
- Significant gaps exist between current DSE systems and realistic extraction objectives

## Why This Works (Mechanism)
The benchmark works by providing a unified evaluation framework that captures realistic document extraction challenges through standardized Markdown conversion. The three-module S3uite (Standardization, Segmentation, Scoring) enables comprehensive assessment of extraction quality across different document types and structures. The diverse document corpus from real-world sources (arXiv and GitHub) ensures the benchmark addresses practical extraction challenges rather than artificial scenarios.

## Foundational Learning
- Document structure understanding: Required for hierarchical table-of-contents construction and global document comprehension
- Visual document processing: Essential for formula and table recognition from PDF layouts
- Markdown semantics: Critical for converting extracted content into standardized, machine-readable format
- Multi-modal integration: Needed for combining visual and textual information from documents
- Evaluation standardization: Important for enabling fair comparison across different DSE approaches
- Document segmentation: Necessary for dividing complex documents into logical units

## Architecture Onboarding

Component Map: PDF Document -> S3uite (Standardization -> Segmentation -> Scoring) -> Performance Metrics

Critical Path: Document input flows through Standardization (normalizing formats) to Segmentation (logical division) to Scoring (quality assessment), with each module building on previous outputs.

Design Tradeoffs: The benchmark prioritizes realistic document complexity over controlled experimental conditions, sacrificing some reproducibility for practical relevance. The Markdown standardization simplifies evaluation but may not capture all document nuances.

Failure Signatures: Poor performance on complex tables, formulas, and hierarchical structures indicates limitations in visual understanding and layout comprehension. Inconsistent segmentation suggests inadequate document structure modeling.

First Experiments:
1. Test S3uite modules independently on simplified documents to verify baseline functionality
2. Evaluate cross-domain performance between arXiv and GitHub document types
3. Conduct ablation studies removing individual S3uite components to assess their contribution

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Relatively small dataset size (2,233 documents) may not capture full document diversity
- Evaluation framework may not fully account for all semantic and contextual elements
- Performance gaps could be influenced by specific evaluation criteria rather than inherent model limitations

## Confidence
- High confidence in identifying current system limitations and performance gaps
- Medium confidence in the proposed unified evaluation framework's comprehensiveness
- Medium confidence in the dataset's representativeness across document types

## Next Checks
1. Expand the dataset size and diversity by including additional document sources and types to validate the benchmark's generalizability
2. Conduct ablation studies on the evaluation S3uite components to determine the relative importance of Standardization, Segmentation, and Scoring modules
3. Implement cross-validation with human experts to verify the scoring accuracy and identify potential biases in the automated evaluation system