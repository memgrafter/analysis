---
ver: rpa2
title: 'Causal Interventions on Causal Paths: Mapping GPT-2''s Reasoning From Syntax
  to Semantics'
arxiv_id: '2410.21353'
source_url: https://arxiv.org/abs/2410.21353
tags:
- causal
- object
- reasoning
- attention
- because
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how GPT-2 small processes causal reasoning
  by analyzing attention patterns and activation responses to causal and semantically
  perturbed sentences. The authors generate a dataset of causal sentences with varied
  syntax and perform attention analysis to identify where syntactic and semantic processing
  occurs.
---

# Causal Interventions on Causal Paths: Mapping GPT-2's Reasoning From Syntax to Semantics

## Quick Facts
- arXiv ID: 2410.21353
- Source URL: https://arxiv.org/abs/2410.21353
- Authors: Isabelle Lee; Joshua Lum; Ziyi Liu; Dani Yogatama
- Reference count: 20
- Primary result: GPT-2 small separates syntactic and semantic processing across layers, with early layers detecting causal syntax and later layers specializing in semantic interpretation

## Executive Summary
This work investigates how GPT-2 small processes causal reasoning by analyzing attention patterns and activation responses to causal and semantically perturbed sentences. The authors generate a dataset of causal sentences with varied syntax and perform attention analysis to identify where syntactic and semantic processing occurs. Results show that causal syntax is localized in the first 2-3 layers, while later layers (especially layers 8-11) contain specific attention heads that respond strongly to nonsensical semantic perturbations. Activation patching reveals that a small set of attention heads consistently contribute to distinguishing causal semantics across different sentence templates. The study suggests a two-stage reasoning process in the model: initial syntactic detection followed by semantic interpretation in deeper layers.

## Method Summary
The researchers generated a dataset of causal sentences using templates that varied syntactic structures while maintaining causal relationships. They performed attention analysis to calculate the proportion of attention paid to causal delimiters and causal attention patterns, identifying where syntactic knowledge was concentrated. Activation patching was applied to contrastive pairs of causal sentences to identify attention heads responsible for distinguishing causal semantics. Per-layer logit analysis examined how causal relationships were progressively refined through the model's residual streams. The combination of these methods revealed a two-stage processing architecture where early layers handle syntax detection and later layers specialize in semantic interpretation.

## Key Results
- Causal syntax processing is localized within the first 2-3 layers of GPT-2 small
- Specific attention heads in layers 8-11 show heightened sensitivity to nonsensical semantic perturbations
- Activation patching identifies a small set of attention heads (layer 11 head 2, layer 10 head 0, layer 8 head 8) that consistently contribute to distinguishing causal semantics
- The model demonstrates a two-stage reasoning process: syntactic detection followed by semantic interpretation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-2 small separates syntactic and semantic processing across layers
- Mechanism: The model uses early layers (1-3) to detect syntactic structures like causal delimiters and phrase patterns, while later layers (8-11) specialize in semantic interpretation of causal relationships
- Core assumption: The model can independently process syntax and semantics as separate streams that combine later
- Evidence anchors:
  - [abstract] "causal syntax is localized within the first 2-3 layers, while certain heads in later layers exhibit heightened sensitivity to nonsensical variations of causal sentences"
  - [section 3.1] "the heads that pay particular causal attention, Pc, tend to be most concentrated in the first 2-3 layers"
  - [section 4.1] "few distinct attention heads in the middle to last few layers contribute most to the logit difference, especially layer 11 head 2, layer 10 head 0, and layer 8 head 8"
- Break condition: If semantic perturbations require syntactic re-analysis or if attention patterns become uniform across layers

### Mechanism 2
- Claim: Attention heads in specific layers specialize in causal relationship detection
- Mechanism: Individual attention heads become specialized detectors that activate when causal relationships are present versus absent, creating interpretable circuits for causal reasoning
- Core assumption: Attention heads can develop specialized functions beyond simple context copying
- Evidence anchors:
  - [abstract] "certain heads in later layers exhibit heightened sensitivity to nonsensical variations of causal sentences"
  - [section 4.1] "activation patching reveals that a small set of attention heads consistently contribute to distinguishing causal semantics across different sentence templates"
  - [section 2] "We recognize that humans comprehend reasoning in natural language in two steps... First, by identifying syntactic cues... Next, we consider the semantic relationships"
- Break condition: If attention heads show no specialization or if semantic processing requires different mechanisms

### Mechanism 3
- Claim: Residual stream analysis reveals causal relationship processing stages
- Mechanism: Per-layer logit analysis shows how causal relationships are progressively refined as information flows through the model, with semantic perturbations causing increasing loss in later layers
- Core assumption: The residual stream preserves interpretable intermediate representations of causal reasoning
- Evidence anchors:
  - [section 4] "we focus on scenarios where semantic perturbations can occur through straightforward word substitutions... we can then decompose the residual contribution per attention heads, per neurons, and analyze their implications"
  - [section 4.1] "We apply activation patching to contrastive pairs of causal sentences... By tracking the activation differences that result in changes to the final logit predictions"
  - [corpus] Weak evidence - no corpus neighbors directly address residual stream analysis for causal reasoning
- Break condition: If residual representations become too entangled to isolate causal processing

## Foundational Learning

- Concept: Attention mechanisms in transformers
  - Why needed here: The paper relies on analyzing attention patterns to identify where syntactic and semantic processing occurs
  - Quick check question: How does the attention mechanism compute which tokens to focus on, and what role does the scaling factor 1/√dK play?

- Concept: Activation patching/causal tracing
  - Why needed here: The method used to identify which model components contribute to distinguishing causal semantics from nonsensical variations
  - Quick check question: What does activation patching reveal about the causal relationship between input perturbations and model outputs?

- Concept: Residual streams in transformer architectures
  - Why needed here: The paper analyzes per-layer logit calculations from residual streams to understand how causal relationships are processed
  - Quick check question: How does information flow through residual connections, and why is this important for understanding intermediate representations?

## Architecture Onboarding

- Component map: GPT-2 small has 12 layers, each with 12 attention heads and MLPs. Early layers (1-3) handle syntax detection, middle layers (4-7) process intermediate representations, late layers (8-11) handle semantic interpretation of causal relationships
- Critical path: Input → Early attention heads (syntax) → Middle layers (integration) → Late attention heads (semantics) → Output prediction
- Design tradeoffs: Simpler models like GPT-2 small allow clearer interpretability but may lack the reasoning sophistication of larger models; the two-stage processing trades immediate semantic understanding for structured processing
- Failure signatures: Uniform attention patterns across layers, lack of specialized heads, semantic perturbations not reflected in later layer activations, residual stream analysis showing no progressive refinement
- First 3 experiments:
  1. Generate attention maps for causal vs non-causal sentences to verify syntactic processing localization in early layers
  2. Apply activation patching to identify which late-layer heads respond to semantic perturbations
  3. Perform per-layer logit analysis to trace how causal relationship understanding develops through the model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the identified attention heads in layers 8-11 specifically encode semantic relationships versus syntactic structures?
- Basis in paper: [explicit] The paper states that later layers (especially layers 8-11) contain specific attention heads that respond strongly to nonsensical semantic perturbations, suggesting they focus on semantic relationships.
- Why unresolved: The paper identifies which heads contribute most but doesn't analyze what specific semantic features these heads are detecting or how they differentiate between coherent and incoherent relationships.
- What evidence would resolve it: Detailed analysis of the attention patterns and activation values for these heads when processing different types of semantic relationships (e.g., temporal, causal, spatial) would clarify their specific roles.

### Open Question 2
- Question: Do the two-stage reasoning process (syntactic detection followed by semantic interpretation) observed in GPT-2 generalize to larger models or more complex causal reasoning scenarios?
- Basis in paper: [inferred] The paper notes that future work could explore more complex causal scenarios and compare findings with larger models, suggesting this generalization is untested.
- Why unresolved: The study is limited to GPT-2 small and straightforward causal sentences, leaving open whether this pattern holds as model capacity increases or when dealing with ambiguous reasoning.
- What evidence would resolve it: Applying the same analysis to GPT-3, GPT-4, or other transformer variants on increasingly complex reasoning tasks would demonstrate if the two-stage process scales.

### Open Question 3
- Question: What is the relationship between the activation patterns of specific attention heads and the model's ability to distinguish between different types of causal markers (because, so, therefore, etc.)?
- Basis in paper: [explicit] The paper analyzes attention paid to causal delimiters and shows varying patterns across different causal markers, but doesn't examine how head activations differ for these markers.
- Why unresolved: While the paper identifies heads sensitive to semantic perturbations, it doesn't analyze whether different heads specialize in recognizing different causal connectives or how they coordinate.
- What evidence would resolve it: Activation patching experiments comparing how different heads respond to perturbations in sentences with different causal markers would reveal specialization patterns.

## Limitations

- The study is limited to GPT-2 small and straightforward causal sentences, which may not generalize to more complex reasoning or larger models
- The analysis focuses on a specific type of causal reasoning and doesn't investigate whether the two-stage processing pattern holds for other types of causal relationships
- The paper doesn't establish whether the identified attention heads' specialization is consistent across different causal reasoning tasks or if they show redundancy

## Confidence

- High confidence in the core finding that GPT-2 small separates syntactic and semantic processing across layers
- Medium confidence in the claim about specific attention heads being specialized causal relationship detectors
- Low confidence in the assertion that this represents a general two-stage reasoning process in LLMs

## Next Checks

1. Test the same analytical methods on different types of causal reasoning (temporal, conditional, counterfactual) to verify if the two-stage processing pattern holds across causal domains.

2. Apply the attention analysis and activation patching framework to larger models (GPT-2 medium/large, GPT-Neo) to determine if the layer-wise specialization pattern scales or changes with model capacity.

3. Conduct ablation studies removing identified causal-specialized heads to measure performance degradation on causal reasoning tasks, establishing causal relationships between head activity and reasoning capability.