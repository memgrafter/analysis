---
ver: rpa2
title: Root Causing Prediction Anomalies Using Explainable AI
arxiv_id: '2403.02439'
source_url: https://arxiv.org/abs/2403.02439
tags:
- features
- feature
- data
- value
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel application of explainable AI (XAI)
  for root-causing performance degradation in machine learning models that learn continuously
  from user engagement data. The core method involves using feature ablation to estimate
  local feature importances (LFI) on control and anomaly data, aggregating LFIs to
  obtain global feature importances (GFI), and ranking features based on the shift
  in GFI between control and anomaly data.
---

# Root Causing Prediction Anomalies Using Explainable AI

## Quick Facts
- arXiv ID: 2403.02439
- Source URL: https://arxiv.org/abs/2403.02439
- Reference count: 17
- Key outcome: XAI approach achieved 68% recall vs 35% for MFC in identifying corrupted features causing prediction anomalies

## Executive Summary
This paper introduces a novel application of explainable AI (XAI) for root-causing performance degradation in continuously trained machine learning models, specifically addressing feature corruptions in user engagement systems. The method uses feature ablation to estimate local feature importances, aggregates them to global feature importances, and ranks features based on shifts between control and anomaly data. The approach was validated against 11 curated corruption cases and demonstrated superior recall compared to model-feature correlation methods.

## Method Summary
The core methodology involves using feature ablation through the Captum library to compute local feature importances (LFI) on both control and anomaly datasets. These LFIs are aggregated using median values with coverage weighting to obtain global feature importances (GFI). Features are then ranked by the absolute shift in GFI between control and anomaly data, with the top-ranked features presented to domain experts for triage. The approach addresses the challenge of identifying corrupted features in continuously trained models where traditional XAI methods that compare multiple models are impractical.

## Key Results
- XAI approach achieved 68% recall of corrupted features in top 30 ranked features versus 35% for MFC methods
- At-least-one recall was 100% for XAI versus 50% for MFC across 11 curated corruption cases
- The XAI method successfully identified corrupted features in all 11 cases, while MFC missed corruptions in 5 cases

## Why This Works (Mechanism)
The approach leverages feature attribution methods to quantify how much each feature contributes to prediction anomalies. By comparing attribution scores between normal (control) and anomalous data, it identifies features whose importance has shifted significantly, suggesting potential corruption. The use of coverage weighting ensures that features present across many examples are prioritized, while median aggregation reduces sensitivity to outliers.

## Foundational Learning
- **Feature ablation**: Systematically removing or perturbing features to measure their impact on model predictions; needed to quantify individual feature contributions to anomalies; quick check: perturb single feature and measure prediction change
- **Local vs Global feature importance**: Local importance measures feature impact on individual predictions, while global importance aggregates across dataset; needed to identify consistently problematic features; quick check: compute median LFI across multiple samples
- **Attribution sensitivity to baseline**: Feature attribution methods can produce different results based on the baseline reference point used; needed to ensure consistent and meaningful importance scores; quick check: compare attributions using different baseline values
- **Coverage weighting**: Assigning higher importance to features that appear across many examples rather than rare occurrences; needed to focus on systematically important features; quick check: verify coverage metric calculation
- **Median aggregation**: Using median instead of mean to reduce sensitivity to outliers in feature importance scores; needed for robust importance estimation; quick check: compare median vs mean aggregation results
- **Model-to-feature correlation (MFC)**: Baseline method that correlates model performance changes with individual feature values; needed as comparison baseline; quick check: compute correlation coefficients between features and model performance

## Architecture Onboarding

**Component Map**
Data Pipeline -> Feature Store -> Model Training -> Prediction Service -> Monitoring System -> XAI Root Cause Analysis

**Critical Path**
Control Data -> Feature Ablation (Captum) -> Local Feature Importances -> Global Feature Importances -> GFI Shift Ranking -> Expert Triage

**Design Tradeoffs**
- Static baseline vs data-dependent baseline: Static chosen for computational efficiency despite potential sensitivity issues
- Coverage weighting: Prioritizes consistently present features but may underweight important rare features
- Top-K ranking: Balances comprehensiveness with expert triage workload
- Median aggregation: Robust to outliers but may smooth out important variations

**Failure Signatures**
- Low recall: Poor baseline choice, insufficient sample size, or feature coverage issues
- High false positives: Seasonal shifts, unrelated feature drifts, or improper control data selection
- Computational inefficiency: Excessive sample size (N) for GFI estimation
- Expert triage overload: Too many features in top-K ranking

**3 First Experiments**
1. Apply feature ablation with zero baseline to synthetic corrupted data and verify corrupted features appear in top rankings
2. Compare XAI recall scores across different baseline values to assess sensitivity
3. Measure triage accuracy and time for domain experts reviewing top 30 ranked features

## Open Questions the Paper Calls Out
**Open Question 1**: What is the optimal sample size (N) for aggregating local feature importances across different model architectures and feature types?
- Basis: Paper states N=10,000 was conservative but model-dependent
- Unresolved: Authors only tested N=10,000 without systematic exploration
- Resolution: Empirical studies comparing standard errors and computational costs for various N values across diverse models

**Open Question 2**: How does baseline value choice affect precision and recall across different corruption types and distributions?
- Basis: Paper acknowledges attribution sensitivity to baseline but only describes their choice
- Unresolved: No comparison of alternative baseline strategies
- Resolution: Comparative studies of different baseline selection across various corruption scenarios

**Open Question 3**: Can the XAI approach be extended to handle continuous concept drift and label drift in addition to feature corruption?
- Basis: Paper mentions difficulty isolating feature drift from label and concept drifts
- Unresolved: Authors acknowledge limitation but propose no solutions
- Resolution: Development and validation of extended XAI methods for multiple drift types

## Limitations
- Method requires domain expert triage for top-K features, introducing potential subjectivity
- Performance metrics for the triage step are not provided, limiting assessment of end-to-end effectiveness
- Precise experimental setup details (baseline values, perturbation settings) are not fully specified

## Confidence
- Claim: XAI outperforms MFC in identifying corrupted features causing prediction anomalies
- Confidence: High (clear quantitative results and controlled comparison)
- Claim: Practical applicability and triage accuracy of the method
- Confidence: Medium (methodological details incomplete, expert triage introduces subjectivity)
- Claim: Reproducibility of exact results
- Confidence: Low (lack of precise experimental setup and data specifications)

## Next Checks
1. Re-implement feature ablation using Captum with varying baseline values and compare recall scores to assess sensitivity
2. Conduct a small-scale expert triage study to measure triage accuracy and time, validating the practical utility of the top K ranking
3. Apply the method to a new, independent dataset with known feature corruptions to test generalizability