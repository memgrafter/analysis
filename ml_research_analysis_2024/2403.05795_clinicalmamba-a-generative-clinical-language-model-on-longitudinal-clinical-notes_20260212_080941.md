---
ver: rpa2
title: 'ClinicalMamba: A Generative Clinical Language Model on Longitudinal Clinical
  Notes'
arxiv_id: '2403.05795'
source_url: https://arxiv.org/abs/2403.05795
tags:
- clinical
- language
- notes
- clinicalmamba
- mamba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ClinicalMamba is a clinical language model based on Mamba that
  is pretrained on longitudinal clinical notes to handle long context lengths. It
  achieves better perplexity and faster inference speeds compared to prior clinical
  models.
---

# ClinicalMamba: A Generative Clinical Language Model on Longitudinal Clinical Notes

## Quick Facts
- arXiv ID: 2403.05795
- Source URL: https://arxiv.org/abs/2403.05795
- Authors: Zhichao Yang; Avijit Mitra; Sunjae Kwon; Hong Yu
- Reference count: 22
- Key outcome: ClinicalMamba achieves better perplexity and faster inference speeds compared to prior clinical models, outperforming ClinicalLlama, ClinicalLongformer, and GPT-4 on clinical information extraction tasks

## Executive Summary
ClinicalMamba is a generative clinical language model based on the Mamba architecture, specifically designed to handle long-context longitudinal clinical notes. The model is pretrained on the MIMIC-III dataset and demonstrates superior performance on clinical information extraction tasks including cohort selection for clinical trials and ICD coding. With parameter sizes of 130M and 2.8B, ClinicalMamba achieves faster inference speeds while maintaining high accuracy, with the 2.8B model trained in under 60 hours on 4 GPUs.

## Method Summary
ClinicalMamba uses the Mamba architecture with selective state spaces to efficiently process long sequences of clinical notes. The model was pretrained on MIMIC-III using causal language modeling with two parameter configurations (130M and 2.8B). Training employed Adam optimizer with linear warmup and cosine decay learning rate scheduling, gradient clipping, and dropout. The pretrained models were then fine-tuned using a prompt-based approach on clinical information extraction tasks including cohort selection and ICD coding, with performance evaluated using micro precision, recall, F1 scores, and ROC-AUC metrics.

## Key Results
- ClinicalMamba achieved lower perplexity on MIMIC-III compared to existing clinical language models
- On clinical information extraction tasks, ClinicalMamba outperformed ClinicalLlama, ClinicalLongformer, and GPT-4 in both accuracy and inference speed
- The 2.8B parameter model was trained in under 60 hours on 4 GPUs, demonstrating efficient training scalability

## Why This Works (Mechanism)
ClinicalMamba leverages the Mamba architecture's selective state spaces to efficiently handle long sequences of clinical notes. The model's ability to process up to 16k tokens allows it to capture longitudinal patient information across multiple clinical notes. The selective state space mechanism enables faster inference compared to transformer-based models while maintaining strong performance on clinical tasks. The prompt-based fine-tuning approach allows the model to adapt to specific clinical information extraction tasks without extensive task-specific training.

## Foundational Learning
1. **Mamba Architecture** - A selective state space model that processes sequential data more efficiently than transformers
   - Why needed: Handles long sequences with linear computational complexity
   - Quick check: Verify model can process 16k token sequences efficiently

2. **Clinical Language Modeling** - Adapting language models to medical domain with specialized vocabulary and terminology
   - Why needed: Medical text contains domain-specific language requiring specialized training
   - Quick check: Evaluate model's ability to generate coherent clinical text

3. **Longitudinal Clinical Notes** - Patient records spanning multiple hospital visits and time periods
   - Why needed: Captures temporal progression of patient conditions
   - Quick check: Verify model maintains context across concatenated notes from different time periods

## Architecture Onboarding

**Component Map:** Clinical Notes -> Tokenizer -> Mamba Layers -> Output Layer -> Clinical Tasks

**Critical Path:** Input clinical notes are tokenized and processed through Mamba selective state space layers, then projected to output vocabulary for language modeling. For downstream tasks, prompt-based fine-tuning adapts the pretrained model to specific clinical information extraction tasks.

**Design Tradeoffs:** The Mamba architecture trades some representational power for significantly faster inference and lower memory requirements compared to transformers. The choice of 16k context length balances the need for long-term context with computational efficiency.

**Failure Signatures:** Training instability may occur with improper learning rate scheduling or insufficient gradient clipping. Poor downstream performance may indicate inadequate pretraining or incorrect prompt formulation for fine-tuning.

**Three First Experiments:**
1. Test model's ability to predict next tokens in clinical notes with 16k context
2. Evaluate perplexity on held-out MIMIC-III test set
3. Fine-tune on small sample of cohort selection task and measure prompt effectiveness

## Open Questions the Paper Calls Out
- How does ClinicalMamba perform on other clinical datasets beyond MIMIC-III, particularly those with different patient populations or healthcare systems?
- How would ClinicalMamba perform if trained on multimodal data, such as incorporating radiology images and electrocardiogram waveforms alongside clinical notes?
- How does ClinicalMamba compare to other parameter-efficient fine-tuning strategies, such as soft prompting or Low-Rank Adaptation (LoRa), on downstream clinical tasks?

## Limitations
- Limited to single hospital dataset (MIMIC-III), potentially limiting generalizability
- Does not incorporate multimodal data (images, waveforms) that are part of EHRs
- Only tested prompt-based fine-tuning, not exploring other parameter-efficient methods

## Confidence
- High confidence in the architectural innovations and training methodology described
- Medium confidence in the reported performance improvements on the tested tasks
- Low confidence in the generalizability of results to other clinical datasets or domains

## Next Checks
1. Verify the exact prompt templates and few-shot examples used for fine-tuning on cohort selection and ICD coding tasks
2. Replicate the training process with the specified hyperparameters and compare perplexity scores on MIMIC-III test set
3. Test the pretrained models on an independent clinical dataset to assess generalization beyond MIMIC-III