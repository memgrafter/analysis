---
ver: rpa2
title: How Do Recommendation Models Amplify Popularity Bias? An Analysis from the
  Spectral Perspective
arxiv_id: '2404.12008'
source_url: https://arxiv.org/abs/2404.12008
tags:
- popularity
- bias
- recommendation
- item
- singular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes how recommendation models amplify popularity
  bias by examining the spectral properties of the predicted score matrix. Two key
  insights emerge: (1) item popularity is memorized in the principal singular vector
  of the score matrix, and (2) dimension reduction amplifies the relative prominence
  of this principal spectrum, leading to bias amplification.'
---

# How Do Recommendation Models Amplify Popularity Bias? An Analysis from the Spectral Perspective

## Quick Facts
- arXiv ID: 2404.12008
- Source URL: https://arxiv.org/abs/2404.12008
- Reference count: 40
- Primary result: ReSN method reduces popularity bias by up to 25.2% while improving NDCG@20 by 14.4%

## Executive Summary
This paper analyzes how recommendation models amplify popularity bias through a spectral perspective. The authors identify two key mechanisms: item popularity is memorized in the principal singular vector of predicted score matrices, and dimension collapse amplifies the relative prominence of this principal spectrum. Based on these insights, they propose ReSN, a novel debiasing method that uses a spectral norm regularizer to penalize the magnitude of the principal singular value. Extensive experiments across seven real-world datasets and three testing paradigms demonstrate ReSN's effectiveness in mitigating popularity bias while maintaining or improving recommendation accuracy.

## Method Summary
ReSN introduces a spectral norm regularizer that penalizes the magnitude of the principal singular value in predicted score matrices. The method computes the spectral norm efficiently using an iterative approximation based on random vectors, avoiding full SVD computation. During training, the model minimizes both the original recommendation loss and the spectral norm penalty term. The regularization strength is controlled by a hyperparameter β, which is tuned via grid search. The approach is implemented as an additional loss term in standard embedding-based recommendation models, making it compatible with existing architectures.

## Key Results
- ReSN reduces the ratio of popular items in recommendations by up to 25.2% while improving NDCG@20 by up to 14.4%
- The method shows consistent improvements across all seven datasets and three testing paradigms
- ReSN outperforms baseline methods including causality-based and propensity-based approaches
- The spectral norm approximation method achieves 2-3 orders of magnitude speedup compared to full SVD computation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Item popularity is memorized in the principal singular vector of the predicted score matrix.
- **Mechanism:** The model's learned embeddings create a low-rank approximation of user-item interactions. The dominant singular vector captures the direction of maximum variance in this matrix, which aligns with item popularity due to the power-law distribution of interactions.
- **Core assumption:** The recommendation model has sufficient capacity to learn a low-rank embedding that captures the dominant pattern in the data.
- **Evidence anchors:**
  - [abstract]: "item popularity is memorized in the principal spectrum of the score matrix predicted by the recommendation model"
  - [section 3.1]: "The principal right singular vector q1 of the matrix Ŷ aligns significantly with the item popularity r"
  - [corpus]: Weak - no direct evidence in corpus about spectral decomposition in recommendation systems
- **Break condition:** If the data distribution is not long-tailed or the model capacity is insufficient to capture the dominant pattern.

### Mechanism 2
- **Claim:** Dimension reduction amplifies the relative prominence of the principal spectrum, leading to bias amplification.
- **Mechanism:** When embeddings are constrained to low dimensions, the singular values of other dimensions become zero or negligible. This forces the model to rely heavily on the principal singular vector, which encodes popularity, for ranking decisions.
- **Core assumption:** The model's embedding dimension is deliberately set lower than the optimal dimension needed to capture all relevant patterns.
- **Evidence anchors:**
  - [abstract]: "The dimension collapse phenomenon amplifies the relative prominence of the principal spectrum, thereby intensifying the popularity bias"
  - [section 3.2]: "A smaller d squeezes the dimensions (causing singular values of more dimensions to become zero), thereby relatively amplifying the effect of the principal spectrum"
  - [corpus]: Weak - no direct evidence in corpus about dimension collapse in recommendation systems
- **Break condition:** If the embedding dimension is set sufficiently high or if other regularization techniques prevent dimension collapse.

### Mechanism 3
- **Claim:** The spectral norm regularizer directly penalizes the magnitude of the principal singular value, mitigating popularity bias amplification.
- **Mechanism:** By adding a penalty term proportional to the spectral norm of the score matrix, the model is discouraged from relying heavily on the principal singular vector. This reduces the influence of popularity in ranking while preserving other useful signals.
- **Core assumption:** The spectral norm can be efficiently approximated without computing the full SVD of the score matrix.
- **Evidence anchors:**
  - [abstract]: "we propose a novel debiasing strategy that leverages a spectral norm regularizer to penalize the magnitude of the principal singular value"
  - [section 4.1]: "Our approach turns to penalize the spectral norm of the matrix before the introduction of the activation function"
  - [corpus]: Weak - no direct evidence in corpus about spectral norm regularization in recommendation systems
- **Break condition:** If the approximation of the spectral norm becomes inaccurate or if the regularization coefficient is set too high.

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD)**
  - Why needed here: SVD is used to decompose the predicted score matrix and analyze the relationship between its principal singular vector and item popularity.
  - Quick check question: What is the relationship between the singular values of a matrix and its rank?

- **Concept: Power-law distribution**
  - Why needed here: The paper assumes that item popularity follows a power-law distribution, which is a key assumption for the theoretical analysis.
  - Quick check question: How does the shape parameter of a power-law distribution affect the skewness of the data?

- **Concept: Gradient-based optimization**
  - Why needed here: The paper analyzes how gradient optimization dynamics contribute to dimension collapse and popularity bias amplification.
  - Quick check question: How do the gradients of the loss function with respect to the model parameters change during training?

## Architecture Onboarding

- **Component map:**
  Embedding layer -> Interaction layer -> Loss layer -> Regularization layer -> Optimization layer

- **Critical path:**
  1. Compute user and item embeddings
  2. Calculate predicted scores using embedding similarity
  3. Compute the loss function (original loss + spectral norm penalty)
  4. Calculate gradients with respect to model parameters
  5. Update model parameters using gradient descent

- **Design tradeoffs:**
  - Embedding dimension vs. model capacity: Higher dimensions allow for more expressive models but increase computational cost and risk of overfitting.
  - Regularization coefficient vs. bias mitigation: Higher coefficients reduce popularity bias but may harm recommendation accuracy.

- **Failure signatures:**
  - High spectral norm: Indicates excessive reliance on the principal singular vector and potential popularity bias.
  - Low spectral norm: May indicate underfitting or insufficient model capacity.
  - Slow convergence: Could be due to an inappropriate learning rate or model architecture.

- **First 3 experiments:**
  1. Train the model with different embedding dimensions and analyze the relationship between dimension size and popularity bias.
  2. Vary the regularization coefficient and observe its effect on both popularity bias and recommendation accuracy.
  3. Compare the performance of the spectral norm regularizer with other debiasing techniques (e.g., causality-based methods, propensity-based methods).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do unmeasured confounders affect the validity of causal graphs used to analyze popularity bias in recommendation systems?
- Basis in paper: [explicit] The paper mentions that causality-based methods rely on causal graphs which may be flawed due to unmeasured confounders (page 2).
- Why unresolved: While the paper acknowledges this limitation, it doesn't empirically investigate how different types of unmeasured confounders might affect the accuracy of bias analysis or whether alternative methods could mitigate this issue.
- What evidence would resolve it: Experimental studies comparing causal inference results with and without known confounders, or developing methods to detect and account for unmeasured confounders in recommendation systems.

### Open Question 2
- Question: How does the dimension collapse phenomenon vary across different optimization algorithms beyond gradient-based methods?
- Basis in paper: [inferred] The paper discusses dimension collapse in the context of gradient-based optimization but doesn't explore how other optimization approaches might behave differently (page 3-4).
- Why unresolved: The analysis focuses primarily on gradient descent dynamics, leaving open the question of whether alternative optimization methods like evolutionary algorithms or second-order methods would exhibit similar dimension collapse behavior.
- What evidence would resolve it: Comparative studies of dimension collapse across multiple optimization algorithms, including non-gradient-based methods, on the same recommendation tasks.

### Open Question 3
- Question: What is the relationship between the spectral norm regularization strength and the model's ability to capture other useful signals beyond popularity bias?
- Basis in paper: [explicit] The paper mentions that popularity bias isn't inherently detrimental and that item popularity can convey useful information (page 6), but doesn't systematically study the trade-off between debiasing and information retention.
- Why unresolved: While the paper demonstrates effectiveness of ReSN in reducing popularity bias, it doesn't provide a detailed analysis of how different regularization strengths affect the model's ability to capture other aspects of user preferences or item characteristics.
- What evidence would resolve it: Empirical studies measuring the impact of varying regularization strengths on multiple performance metrics beyond popularity bias, including diversity, novelty, and serendipity in recommendations.

## Limitations

- The theoretical analysis relies on simplifying assumptions about power-law distributions that may not hold across all datasets
- The dimension collapse analysis is based primarily on idealized gradient descent dynamics rather than real-world training scenarios
- The spectral norm approximation method's efficiency claims lack computational benchmarks against exact SVD computation

## Confidence

- **High confidence**: The empirical results showing ReSN's effectiveness in reducing popularity bias while maintaining accuracy (NDCG@20 improvements up to 14.4%) across multiple datasets and testing paradigms.
- **Medium confidence**: The theoretical explanation of how dimension collapse amplifies popularity bias, as this relies on idealized gradient descent assumptions that may not reflect actual training dynamics.
- **Medium confidence**: The efficiency claims of the spectral norm approximation method, as computational benchmarks are not provided to compare against full SVD computation.

## Next Checks

1. **Power-law verification**: Empirically test whether item popularity distributions across all seven datasets actually follow power-law distributions with appropriate goodness-of-fit tests.

2. **Approximation accuracy**: Measure the approximation error of the spectral norm computation method ||VU⊤e||₂ ||UV⊤VU⊤e||₂ against exact SVD computation across different matrix sizes and sparsities.

3. **Optimizer sensitivity**: Test ReSN's performance and behavior under different optimizers (AdamW, SGD with momentum) and initialization schemes to validate the gradient-based analysis assumptions.