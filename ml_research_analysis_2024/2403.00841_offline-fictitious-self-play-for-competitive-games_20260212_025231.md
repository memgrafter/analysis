---
ver: rpa2
title: Offline Fictitious Self-Play for Competitive Games
arxiv_id: '2403.00841'
source_url: https://arxiv.org/abs/2403.00841
tags:
- offline
- datasets
- learning
- policy
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Offline Fictitious Self-Play (OFF-FSP) is the first practical model-free
  offline RL algorithm for competitive games, addressing the challenge of learning
  Nash equilibria from fixed datasets without online interactions. It introduces a
  technique to simulate interactions with various opponents by adjusting dataset weights
  via importance sampling, enabling best-response learning and offline self-play.
---

# Offline Fictitious Self-Play for Competitive Games

## Quick Facts
- arXiv ID: 2403.00841
- Source URL: https://arxiv.org/abs/2403.00841
- Authors: Jingxiao Chen; Weiji Xie; Weinan Zhang; Yong yu; Ying Wen
- Reference count: 40
- Key outcome: Introduces OFF-FSP, the first model-free offline RL algorithm for competitive games, achieving significantly lower exploitability than baselines like OEF and BC

## Executive Summary
Offline Fictitious Self-Play (OFF-FSP) addresses the challenge of learning Nash equilibria in competitive games from fixed datasets without online interactions. The method introduces a novel technique to simulate interactions with various opponents by adjusting dataset weights via importance sampling, enabling best-response learning and offline self-play. To handle partially covered datasets, OFF-FSP combines single-agent offline RL with Fictitious Self-Play, constraining policies away from out-of-distribution actions using a NashConv surrogate loss. Experiments demonstrate that OFF-FSP achieves significantly lower exploitability than state-of-the-art baselines across matrix games, poker, and board games, and shows strong performance in a real-world human-robot competitive task.

## Method Summary
OFF-FSP tackles the offline competitive RL problem by simulating best-response learning against a mixture of opponent policies derived from the dataset. The core innovation is using importance sampling to reweight dataset trajectories, creating synthetic interactions with diverse opponents. This enables Fictitious Self-Play (FSP) in an offline setting. To address the challenge of partially covered datasets, the method incorporates a constrained policy optimization approach that uses a NashConv surrogate loss to keep learned policies close to the dataset distribution. This combination of importance-weighted sampling, best-response learning, and distribution-constrained optimization allows OFF-FSP to approximate Nash equilibria without requiring online environment interactions.

## Key Results
- OFF-FSP achieves significantly lower exploitability than OEF and BC baselines in matrix games, poker variants, and board games
- Demonstrates strong performance in a real-world human-robot competitive task, showcasing practical applicability
- Shows robustness in partially covered datasets by combining single-agent offline RL with Fictitious Self-Play through NashConv constraint

## Why This Works (Mechanism)
The mechanism works by enabling best-response learning in an offline setting through dataset weight adjustment via importance sampling. By reweighting trajectories to simulate interactions with various opponents, OFF-FSP can perform Fictitious Self-Play without online environment access. The NashConv surrogate loss ensures policies remain close to the dataset distribution, preventing exploitation of out-of-distribution actions. This combination allows the algorithm to approximate Nash equilibria by iteratively updating policies to best-respond to the current mixture of opponent strategies, all while constrained to the offline data.

## Foundational Learning

**Fictitious Self-Play**: Iterative best-response learning where each player adapts to the empirical distribution of opponents' past strategies. Needed to converge to approximate Nash equilibria in competitive games. Quick check: Verify that the algorithm can reproduce known equilibria in simple matrix games.

**Importance Sampling**: Technique for reweighting samples from one distribution to estimate expectations under another. Required to simulate interactions with diverse opponents from a fixed dataset. Quick check: Confirm that weight adjustments produce meaningful opponent diversity in the learned best responses.

**NashConv**: Measure of exploitability that quantifies how much a player can gain by deviating from a strategy. Essential for constraining policies to remain close to dataset distribution. Quick check: Validate that the NashConv-based constraint effectively prevents out-of-distribution policy updates.

## Architecture Onboarding

**Component Map**: Dataset -> Importance Sampling -> Best-Response Learner -> Policy Update -> NashConv Constraint -> Final Policy

**Critical Path**: The most important computational path is Dataset → Importance Sampling → Best-Response Learner → Policy Update, where the algorithm iteratively refines policies by best-responding to reweighted opponent distributions while maintaining dataset proximity through NashConv constraints.

**Design Tradeoffs**: The key tradeoff is between exploration (simulating diverse opponents via importance sampling) and exploitation (constraining to dataset distribution via NashConv). Too much exploration risks out-of-distribution actions, while too much constraint limits learning potential. The algorithm balances this by adjusting the strength of the NashConv constraint based on dataset coverage.

**Failure Signatures**: Poor performance may indicate insufficient dataset coverage, inappropriate importance sampling weights, or overly restrictive NashConv constraints. If exploitability remains high, check whether the dataset contains diverse enough opponent strategies or whether the weight adjustment mechanism is effectively creating meaningful opponent variations.

**3 First Experiments**:
1. Validate on a simple 2x2 matrix game with known Nash equilibrium to verify basic convergence
2. Test on Kuhn poker with full dataset coverage to compare against online FSP
3. Evaluate on Leduc poker with partial coverage to assess robustness to incomplete data

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily on controlled environments (matrix games, poker variants, board games) and one human-robot task; scalability to more complex real-world scenarios uncertain
- Performance in extremely limited or highly biased dataset regimes not thoroughly explored
- Claims of being "first" practical model-free offline RL algorithm for competitive games would benefit from more exhaustive literature survey

## Confidence

**High**: The technical formulation and implementation of OFF-FSP are internally consistent and well-described.

**Medium**: Claims about performance improvements over baselines are supported by experiments, but generalization to broader competitive domains is uncertain.

**Medium**: The claim of being "first" in the offline RL space for competitive games is plausible but requires more thorough literature review.

## Next Checks

1. Test OFF-FSP on a wider range of competitive games with varying degrees of dataset coverage and opponent diversity to assess robustness.

2. Compare OFF-FSP's performance with additional offline RL baselines in both competitive and cooperative settings to contextualize its advantages.

3. Evaluate the algorithm's sensitivity to hyperparameters (e.g., importance sampling weights, constraint strengths) across different problem scales and dataset qualities.