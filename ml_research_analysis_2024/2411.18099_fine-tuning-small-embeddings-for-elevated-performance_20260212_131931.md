---
ver: rpa2
title: Fine-Tuning Small Embeddings for Elevated Performance
arxiv_id: '2411.18099'
source_url: https://arxiv.org/abs/2411.18099
tags:
- data
- nepali
- language
- word
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study fine-tuned a small pre-trained BERT model (NpVec1) with
  six attention heads on a newly aggregated Nepali corpus containing both regularized
  and unregularized text. The corpus was carefully filtered to remove Hindi content,
  standardized for vowel consistency, and tokenized to handle Nepali agglutinative
  morphology.
---

# Fine-Tuning Small Embeddings for Elevated Performance

## Quick Facts
- arXiv ID: 2411.18099
- Source URL: https://arxiv.org/abs/2411.18099
- Reference count: 4
- Fine-tuning a small BERT model (NpVec1) on Nepali corpus improved clustering purity from 0.65 to 0.78 and F1-score from 0.74 to 0.81

## Executive Summary
This study fine-tuned a small pre-trained BERT model (NpVec1) with six attention heads on a newly aggregated Nepali corpus containing both regularized and unregularized text. The corpus was carefully filtered to remove Hindi content, standardized for vowel consistency, and tokenized to handle Nepali agglutinative morphology. Intrinsic evaluation using clustering purity across sentiment, relatedness, and named entity sets showed the fine-tuned model achieving 0.78 average purity versus 0.65 for the baseline NpVec1. Extrinsic evaluation on a news classification task yielded F1-scores of 0.81 for the fine-tuned model, 0.74 for NpVec1, and 0.87 for the larger nepaliBERT oracle. These results demonstrate that fine-tuning a small BERT model on diverse, unregularized Nepali text can substantially improve performance over the original pre-trained model, approaching the quality of larger models despite using far less data and compute.

## Method Summary
The method involved aggregating a Nepali corpus from news articles (regularized) and social media (unregularized), filtering out Hindi content, standardizing vowels, and applying lexical analysis for agglutinative morphology. The NpVec1 BERT model with six attention heads was then fine-tuned on this corpus using standard transfer learning procedures. Evaluation included intrinsic clustering purity metrics across three semantic domains (sentiment, relatedness, named entities) and extrinsic performance on a news classification task measuring F1-score.

## Key Results
- Fine-tuned model achieved 0.78 average clustering purity vs 0.65 baseline
- News classification F1-score improved from 0.74 to 0.81 after fine-tuning
- Fine-tuned model outperformed NpVec1 on both precision and recall metrics

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning on unregularized Nepali data improves embeddings by introducing linguistic diversity absent in the pre-training corpus. The pre-trained model learns a broad vocabulary from regularized (news) text, but unregularized (social media) data exposes it to colloquial forms, agglutinative morphology, and domain-specific slang. Fine-tuning adapts the attention heads to these variations, increasing the semantic richness captured in embeddings. Core assumption: The model's low parameter count (6 heads) allows rapid adaptation without catastrophic forgetting of the regularized patterns.

### Mechanism 2
Clustering purity scores improve because fine-tuning aligns embeddings with domain-specific semantic clusters. Intrinsic evaluation uses clustering of sentiment, relatedness, and named entity terms. Fine-tuning on a balanced mix of regularized and unregularized text causes embeddings to reflect both formal and informal usage contexts, tightening intra-cluster distances and pushing inter-cluster boundaries apart. Core assumption: The clustering purity metric is sensitive enough to detect semantic coherence changes induced by fine-tuning.

### Mechanism 3
Extrinsic classification F1 improves because fine-tuned embeddings capture morphology and syntax better suited to Nepali news text patterns. The fine-tuned embeddings better encode token-level features (e.g., agglutinative suffixes) that are crucial for downstream tasks like news classification, leading to more accurate feature representations for the classifier. Core assumption: The downstream classifier architecture is sensitive to the quality of input embeddings, not just its own learned parameters.

## Foundational Learning

- Concept: Subword tokenization and its impact on agglutinative languages
  - Why needed here: Nepali's rich morphology means that breaking words into morphemes can drastically affect embedding quality.
  - Quick check question: How does a BPE tokenizer differ from a wordpiece tokenizer when handling Nepali agglutinative suffixes?

- Concept: Transfer learning and fine-tuning dynamics
  - Why needed here: The study hinges on fine-tuning a small BERT model effectively; understanding catastrophic forgetting and learning rate schedules is key.
  - Quick check question: What happens to the pre-trained weights if the learning rate is set too high during fine-tuning?

- Concept: Intrinsic vs. extrinsic evaluation in NLP
  - Why needed here: The study uses both to validate embeddings; knowing when each is appropriate helps interpret results.
  - Quick check question: Can a model have high intrinsic purity but low extrinsic F1? Under what circumstances?

## Architecture Onboarding

- Component map: Web scrapers -> langdetect filter -> Hindi regex filter -> vowel standardization -> lexical analysis -> tokenizer -> BERT fine-tuner -> clustering algorithm -> news classifier
- Critical path: 1) Aggregate and preprocess corpus (filtering Hindi, standardizing vowels, tokenizing) 2) Load pre-trained NpVec1 model (6 heads, 300 dim) 3) Fine-tune on preprocessed corpus 4) Run intrinsic clustering purity evaluation 5) Run extrinsic classification task
- Design tradeoffs: Smaller model (6 heads) -> faster fine-tuning but less capacity for complex patterns; Unregularized data inclusion -> more diversity but more noise; No additional labeled data -> relies on transfer learning quality
- Failure signatures: Low clustering purity but high F1: embeddings may overfit to classification domain; High purity but low F1: embeddings capture semantics but not task-specific cues; Both metrics drop after fine-tuning: catastrophic forgetting or noisy fine-tuning data
- First 3 experiments: 1) Fine-tune with only regularized data -> measure purity/F1 vs. baseline 2) Fine-tune with only unregularized data -> measure purity/F1 vs. baseline 3) Vary learning rate (low/medium/high) during fine-tuning -> observe impact on both metrics

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the fine-tuned model change when trained on larger datasets beyond the 140.48M word token corpus? Basis: The paper mentions that the fine-tuned model significantly outperformed the NpVec1 baseline despite being trained on a smaller dataset, but does not explore performance with larger datasets. Unresolved because the study focused on demonstrating the effectiveness of fine-tuning with a smaller dataset. Evidence needed: Experiments comparing the fine-tuned model's performance with varying dataset sizes.

### Open Question 2
What are the specific linguistic challenges in Nepali that the fine-tuned model addresses better than the baseline, and how do these improvements manifest in real-world NLP applications? Basis: The paper discusses the complexity of Nepali due to its rich vocabulary and diverse grammatical structures, but does not detail specific linguistic challenges or their impact on applications. Unresolved because the study evaluates model performance but does not delve into the linguistic nuances or practical application outcomes. Evidence needed: A detailed linguistic analysis of the model's improvements and case studies of its application in tasks like sentiment analysis or machine translation.

### Open Question 3
How does the fine-tuned model's performance compare to other multilingual BERT models not specifically trained on Nepali, and what insights can be drawn from these comparisons? Basis: The paper compares the fine-tuned model to a larger nepaliBERT model but does not mention comparisons with other multilingual BERT models. Unresolved because the study focuses on models specifically trained on Nepali, leaving out comparisons with broader multilingual models. Evidence needed: Comparative experiments evaluating the fine-tuned model against multilingual BERT models on Nepali-specific tasks.

## Limitations
- The study does not report statistical significance testing for the observed improvements
- The comparison with the larger nepaliBERT oracle (0.87 F1) reveals substantial gaps remain compared to larger architectures
- The filtering effectiveness for Hindi content is not empirically validated

## Confidence
- High confidence: The observation that fine-tuning improves clustering purity metrics (0.78 vs 0.65 baseline) is well-supported by the intrinsic evaluation methodology and consistent results across all three test sets
- Medium confidence: The claim that fine-tuning substantially improves extrinsic classification performance (0.81 F1 vs 0.74 baseline) is supported by the experimental results, though the improvement represents a single data point without variance reporting or statistical testing
- Medium confidence: The assertion that fine-tuning on unregularized Nepali data introduces beneficial linguistic diversity is plausible given the corpus composition, but the filtering effectiveness is not empirically validated

## Next Checks
1. Conduct statistical significance testing (e.g., paired t-tests or bootstrap confidence intervals) on the F1-score improvements to establish whether the 7% gain is reliable across different train/test splits
2. Systematically evaluate the contribution of regularized vs. unregularized data by fine-tuning separate models on each corpus type independently, then comparing clustering purity and classification performance
3. Apply the fine-tuned model to additional downstream tasks beyond news classification (e.g., sentiment analysis, named entity recognition, or question answering) to assess whether performance improvements transfer to other domains and task types