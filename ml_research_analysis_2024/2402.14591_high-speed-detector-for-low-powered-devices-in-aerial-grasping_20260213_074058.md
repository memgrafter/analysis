---
ver: rpa2
title: High-Speed Detector For Low-Powered Devices In Aerial Grasping
arxiv_id: '2402.14591'
source_url: https://arxiv.org/abs/2402.14591
tags:
- detection
- object
- which
- dataset
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Fast Fruit Detector (FFD), a lightweight, single-stage,
  postprocessing-free object detector designed for real-time fruit detection on low-power
  edge devices. FFD uses a novel Latent Object Representation (LOR) module to generate
  object queries directly from backbone features, avoiding anchor boxes, NMS, and
  multi-scale feature fusion.
---

# High-Speed Detector For Low-Powered Devices In Aerial Grasping

## Quick Facts
- arXiv ID: 2402.14591
- Source URL: https://arxiv.org/abs/2402.14591
- Authors: Ashish Kumar; Laxmidhar Behera
- Reference count: 30
- Primary result: Fast Fruit Detector (FFD) achieves 100 FPS@FP32 inference on 10W NVIDIA Jetson-NX while maintaining high accuracy for real-time fruit detection

## Executive Summary
This paper introduces Fast Fruit Detector (FFD), a lightweight, single-stage object detector designed for real-time fruit detection on low-power edge devices like UAVs. FFD uses a novel Latent Object Representation (LOR) module to generate object queries directly from backbone features, eliminating the need for anchor boxes, NMS, and multi-scale feature fusion. This enables efficient 100 FPS inference on resource-constrained hardware while maintaining competitive accuracy. The authors also develop an occlusion-aware scene synthesis method for efficient training data generation and release a challenging fruit detection dataset.

## Method Summary
FFD is a single-stage detector that uses a VGG backbone with BatchNorm, followed by a novel Latent Object Representation (LOR) module. The LOR module generates object queries directly from backbone features using Query Transformation (QT) and Cross Channel Global Context (CCGC) operations, avoiding learned embeddings or anchor boxes. Tiled Hungarian matching is used for training, where ground-truth boxes are matched within local image tiles instead of the entire image. The detector is postprocessing-free, eliminating the need for NMS. Training uses synthetic scenes generated through occlusion-aware scene synthesis, where fruit instances are pasted onto base images at non-overlapping locations.

## Key Results
- Achieves 46.6 AP on custom fruit dataset and 44.6 AP on MinneApple benchmark (single-scale)
- Runs at 100 FPS@FP32 on NVIDIA Jetson-NX (10W device)
- Outperforms Faster-RCNN (multi-scale) on custom dataset (46.6 vs 45.9 AP) while being significantly faster
- Demonstrates comparable accuracy to YOLO-v8 while maintaining superior speed

## Why This Works (Mechanism)

### Mechanism 1
LOR enables postprocessing-free detection by generating object queries directly from backbone features using QT and CCGC operations, eliminating anchor boxes and NMS. Core assumption: Small objects can be detected accurately from single-scale low-resolution feature maps when enriched with global context. Evidence: [abstract] "FFD uses a novel Latent Object Representation (LOR) module to generate object queries directly from backbone features, avoiding anchor boxes, NMS, and multi-scale feature fusion." Break condition: If objects are too small or occluded to retain identity in low-resolution feature maps.

### Mechanism 2
Tiled Hungarian matching reduces computational complexity by matching ground-truth boxes only within local image tiles. Core assumption: Fruits in a single image tile are sparse enough that one-to-one matching per tile is computationally cheaper than global matching. Evidence: [abstract] "FFD, being only a single-scale detector, is more accurate than many representative detectors... while being considerably faster." Break condition: If tile size is too small, the number of tiles increases; if too large, sparsity benefit diminishes.

### Mechanism 3
Occlusion-aware scene synthesis enables training on small datasets without exhaustive manual labeling by generating realistic synthetic scenes with non-overlapping bounding boxes. Core assumption: Synthetic scenes with controlled overlap and visibility preserve enough visual diversity to train a robust detector. Evidence: [abstract] "(ii) a method to generate vast amounts of training data without exhaustive manual labelling of fruit images since they consist of a large number of instances, which increases the labelling cost and time." Break condition: If synthetic scenes are too unrealistic or lack sufficient diversity, model generalization to real images suffers.

## Foundational Learning

- Concept: Feature Pyramid Networks (FPN) and multi-scale detection
  - Why needed here: FFD deliberately avoids FPN to reduce computation; understanding why multi-scale helps detect small objects explains FFD's design choice to rely on single-scale with enhanced queries.
  - Quick check question: What is the main computational cost of FPN, and how does FFD avoid it?

- Concept: Transformer attention mechanisms and their computational cost
  - Why needed here: FFD removes transformer blocks from DETR but keeps its query-based formulation; knowing the cost of self/cross-attention explains the speedup.
  - Quick check question: Why does removing multi-head self-attention from DETR reduce inference time?

- Concept: Non-maximum suppression (NMS) and its role in object detection
  - Why needed here: FFD is postprocessing-free because its matching strategy inherently avoids duplicate detections; understanding NMS explains why this is significant.
  - Quick check question: How does one-to-one matching via Hungarian algorithm eliminate the need for NMS?

## Architecture Onboarding

- Component map: Backbone (VGG + BN) → LOR module (QT + CCGC) → Query matrix (Tq) → Classification FFN + Box regression FFN → Output

- Critical path:
  1. Backbone forward pass (5 stages, stride 2, final output 256×H/32×W/32)
  2. LOR transformation (3× parallel QT+CCGC)
  3. Spatial collapse to query matrix
  4. Two FFN heads (classification + box regression)
  5. Tiled Hungarian matching (training) or direct denormalization (inference)

- Design tradeoffs:
  - Single-scale vs multi-scale: Speed vs small object detection accuracy
  - VGG backbone vs deeper networks: Simplicity and low latency vs potential accuracy gains
  - Tile size 32×32: Balances query count and receptive field; smaller tiles increase computation, larger reduce context

- Failure signatures:
  - Low AP on small objects → likely insufficient receptive field or tile size too large
  - High inference latency → LOR or backbone too deep; check tile count
  - Overfitting on small dataset → insufficient augmentation or synthetic data quality

- First 3 experiments:
  1. Train FFD on synthetic scenes only; evaluate AP to confirm synthetic data quality
  2. Vary tile size (16×16, 32×32, 64×64); measure AP and inference time to find sweet spot
  3. Replace LOR with learned embeddings (DETR-style); compare speed and accuracy to quantify LOR benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FFD's performance scale with increasing numbers of object classes beyond fruit detection?
- Basis in paper: [explicit] The authors state FFD can be adapted to other robotic applications, but all experiments focus solely on fruit detection.
- Why unresolved: The paper does not test FFD on datasets with multiple object classes or evaluate its generalization to other object types.
- What evidence would resolve it: Experiments showing FFD's accuracy and speed on multi-class datasets like COCO or specialized robotics datasets with diverse objects.

### Open Question 2
- Question: What is the impact of using different backbone architectures (e.g., MobileOne, EfficientNet) on FFD's speed-accuracy tradeoff?
- Basis in paper: [explicit] The authors mention exploring Transformer-based Mobile backbones like MobileOne as a future direction, but only test VGG and ResNet-50.
- Why unresolved: The paper does not investigate how alternative backbones affect FFD's performance, particularly on resource-constrained devices.
- What evidence would resolve it: Comparative experiments replacing VGG/ResNet with MobileOne or EfficientNet variants, measuring FPS, AP, and parameter count.

### Open Question 3
- Question: How robust is FFD to extreme occlusions and cluttered scenes compared to state-of-the-art detectors?
- Basis in paper: [inferred] The authors introduce an occlusion-aware scene synthesis method, but only evaluate on their own dataset and MinneApple, which may not represent extreme occlusion scenarios.
- Why unresolved: The paper lacks evaluation on datasets specifically designed for occlusion robustness (e.g., COCO-Occluded, LVIS) or real-world cluttered environments.
- What evidence would resolve it: Benchmarking FFD against detectors like Cascade-RCNN or Oriented R-CNN on occlusion-heavy datasets, measuring AP under varying occlusion levels.

### Open Question 4
- Question: What is the minimum tile size that maintains acceptable accuracy without sacrificing too much speed?
- Basis in paper: [explicit] The ablation study tests tile sizes from 16×16 to 64×64, showing accuracy drops significantly at 64×64, but does not explore intermediate sizes or define a clear threshold.
- Why unresolved: The paper does not establish a quantitative trade-off between tile size, accuracy, and speed, nor does it consider non-square tile shapes.
- What evidence would resolve it: A detailed ablation across a wider range of tile sizes (e.g., 24×24, 40×40, 48×48) with corresponding FPS and AP metrics, identifying the optimal balance.

## Limitations

- Reliance on synthetic data for training raises questions about real-world performance transfer to novel fruit varieties or lighting conditions
- Fixed tile size (32×32) and single-scale approach may limit detection accuracy for very small or heavily occluded fruits
- VGG backbone, while efficient, may not achieve state-of-the-art accuracy compared to deeper architectures

## Confidence

- High confidence: The reported 100 FPS inference speed on Jetson-NX and the elimination of anchor boxes/NMS through tiled Hungarian matching are well-supported by the architectural description and computational analysis.
- Medium confidence: The AP improvements over Faster-RCNN and YOLO-v8 are demonstrated on the authors' dataset and MinneApple, but the synthetic data generation method's effectiveness across diverse real-world scenarios requires further validation.
- Low confidence: Claims about FFD's superiority on "challenging" fruit datasets are limited by the relatively small scale of the presented datasets and lack of testing on established benchmarks beyond MinneApple.

## Next Checks

1. Evaluate FFD on established object detection benchmarks (COCO, PASCAL VOC) to assess generalizability beyond fruit detection.
2. Conduct ablation studies varying tile sizes and query counts to determine optimal configurations for different object scales and densities.
3. Test model performance degradation when trained exclusively on real data versus synthetic data to quantify synthetic data's contribution to accuracy.