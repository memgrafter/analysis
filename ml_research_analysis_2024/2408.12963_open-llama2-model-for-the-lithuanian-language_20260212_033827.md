---
ver: rpa2
title: Open Llama2 Model for the Lithuanian Language
arxiv_id: '2408.12963'
source_url: https://arxiv.org/abs/2408.12963
tags:
- language
- llms
- lithuanian
- open
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the first open Llama2-based large language
  models (LLMs) for the Lithuanian language, including 7B and 13B parameter variants.
  The models were trained using full-parameter training on a Lithuanian subset of
  the CulturaX dataset, followed by fine-tuning on a translated Alpaca dataset and
  a custom Lithuanian question/answer (Q/A) dataset.
---

# Open Llama2 Model for the Lithuanian Language

## Quick Facts
- arXiv ID: 2408.12963
- Source URL: https://arxiv.org/abs/2408.12963
- Authors: Artūras Nakvosas; Povilas Daniušis; Vytas Mulevičius
- Reference count: 40
- First open Llama2-based LLMs for Lithuanian language (7B and 13B parameters)

## Executive Summary
This paper introduces the first open Llama2-based large language models for the Lithuanian language, with 7B and 13B parameter variants. The models were trained using full-parameter training on a Lithuanian subset of the CulturaX dataset, followed by fine-tuning on a translated Alpaca dataset and a custom Lithuanian question/answer dataset. The authors also translated popular LLM benchmarks into Lithuanian for standardized evaluation. Evaluation results show that the proposed models achieve significantly lower perplexity on a Lithuanian Q/A dataset compared to the baseline Llama3-8B model. However, benchmarking on language understanding tasks (MMLU) reveals that further improvements in pretraining data quality may be needed to achieve strong performance on these tasks.

## Method Summary
The authors trained Llama2-7B and Llama2-13B models using full-parameter training on the Lithuanian subset of the CulturaX dataset, rather than parameter-efficient fine-tuning (PEFT). This was followed by fine-tuning on a translated Alpaca dataset and a custom Lithuanian Q/A dataset (13,848 pairs). The training used 8xH100 GPUs with specific learning rates and batch sizes per model size, and a context length of 2048 tokens. The models were evaluated using perplexity on the Lithuanian Q/A dataset and language understanding accuracy on translated benchmarks including MMLU.

## Key Results
- LT-Llama2-7B and LT-Llama2-13B achieve significantly lower perplexity on Lithuanian Q/A tasks compared to Llama3-8B baseline
- Full-parameter training approach used instead of PEFT to capture full statistical patterns of Lithuanian language
- Translation of popular LLM benchmarks into Lithuanian enables standardized evaluation
- MMLU benchmark results suggest high-quality pretraining datasets may be essential for strong performance on language understanding tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Full-parameter training on the Lithuanian subset of CulturaX yields lower perplexity on Lithuanian language tasks compared to using models trained primarily on English data.
- Mechanism: Full-parameter training updates all weights of the model, allowing it to learn the full statistical patterns of the Lithuanian language from the training corpus, rather than just adjusting a subset of parameters via PEFT.
- Core assumption: The Lithuanian subset of CulturaX contains sufficient linguistic diversity and volume to effectively train the model to the target domain.
- Evidence anchors:
  - [abstract] "Evaluation results show that the proposed models achieve significantly lower perplexity on a Lithuanian Q/A dataset compared to the baseline Llama3-8B model."
  - [section] "According to Table 4, the proposed LT-Llama2-7B and LT-Llama2-13B models exhibit significantly lower average perplexity values compared to the Llama3-8B."
  - [corpus] Found related papers but no direct evidence on perplexity comparison between full-parameter vs PEFT approaches.
- Break condition: If the Lithuanian subset of CulturaX is too small or lacks linguistic diversity, full-parameter training may not outperform PEFT methods in terms of perplexity.

### Mechanism 2
- Claim: Translating popular LLM benchmarks into Lithuanian enables standardized evaluation of the model's language understanding capabilities.
- Mechanism: By translating existing benchmarks, the authors create a common evaluation framework that allows comparison of the model's performance on language understanding tasks with other models, even if those models were not specifically trained on Lithuanian data.
- Core assumption: The translated benchmarks accurately represent the original tasks and maintain the same level of difficulty and linguistic nuance.
- Evidence anchors:
  - [abstract] "The authors also translated popular LLM benchmarks into Lithuanian."
  - [section] "In this experiment, we evaluate the proposed LLMs using LM evaluation harness (LMEH) language understanding benchmarks [10] translated into Lithuanian language (see Table 5 for the download links)."
  - [corpus] No direct evidence found on the effectiveness of translating benchmarks for evaluation purposes.
- Break condition: If the translation process introduces significant bias or alters the semantics of the original benchmarks, the evaluation results may not accurately reflect the model's true language understanding capabilities.

### Mechanism 3
- Claim: The proposed models' performance on MMLU benchmarks suggests that high-quality pretraining datasets are essential for achieving strong performance on language understanding tasks.
- Mechanism: The MMLU benchmark covers a wide range of academic disciplines, and the authors hypothesize that the Lithuanian component of CulturaX, being primarily collected through web crawling, may not include data relevant to these specific tasks. Therefore, incorporating high-quality data relevant to the target tasks may improve the model's performance.
- Core assumption: The lack of improvement on MMLU benchmarks is due to the irrelevance of the pretraining data to the benchmark tasks, rather than other factors such as model architecture or training process.
- Evidence anchors:
  - [abstract] "benchmarking the proposed LLMs against language understanding tasks (MMLU) reveals that high-quality pretraining datasets may be essential for achieving models that perform efficiently on these benchmarks."
  - [section] "The results of these experiments hint that the Lithuanian component of CulturaX may not be sufficiently rich for modern LLM architectures."
  - [corpus] No direct evidence found on the relationship between pretraining data quality and performance on MMLU benchmarks.
- Break condition: If the lack of improvement on MMLU benchmarks is due to factors other than the quality of the pretraining data, such as the model architecture or training process, then incorporating high-quality data may not necessarily lead to improved performance.

## Foundational Learning

- Concept: Transformer architecture
  - Why needed here: The paper proposes Llama2-based models, which are based on the Transformer architecture. Understanding the fundamentals of Transformers is crucial for comprehending the model's design and training process.
  - Quick check question: What are the key components of the Transformer architecture, and how do they contribute to the model's ability to process sequential data?

- Concept: Neural scaling laws
  - Why needed here: The paper mentions that LLMs rely on increasingly large model parameterisations and training datasets, which are deemed essential according to neural scaling laws. Understanding these laws helps in grasping the relationship between model size, dataset size, and performance.
  - Quick check question: What are neural scaling laws, and how do they explain the importance of large models and datasets in achieving state-of-the-art performance?

- Concept: Perplexity as an evaluation metric
  - Why needed here: The paper uses perplexity to evaluate the models' performance on the Lithuanian Q/A dataset. Understanding perplexity and its interpretation is essential for assessing the models' language modeling capabilities.
  - Quick check question: How is perplexity calculated, and what does a lower perplexity value indicate about a model's performance on a given dataset?

## Architecture Onboarding

- Component map:
  Input layer -> Tokenization and embedding of Lithuanian text -> Encoder (Transformer layers with self-attention mechanisms) -> Output layer (Language modeling head for next token prediction)

- Critical path:
  1. Preprocess Lithuanian text data (tokenization, encoding)
  2. Train the model on the preprocessed data using full-parameter autoregressive pretraining
  3. Fine-tune the model on the translated Alpaca dataset and custom Lithuanian Q/A dataset
  4. Evaluate the model's performance using perplexity on the Lithuanian Q/A dataset and language understanding benchmarks

- Design tradeoffs:
  - Full-parameter training vs. PEFT: Full-parameter training allows the model to learn the full statistical patterns of the Lithuanian language but requires more computational resources, while PEFT is less computationally demanding but may result in less accurate models.
  - Dataset size and diversity: A larger and more diverse dataset may lead to better model performance but also requires more computational resources for training.

- Failure signatures:
  - High perplexity on the Lithuanian Q/A dataset: Indicates that the model struggles to predict the next token in Lithuanian text, possibly due to insufficient training data or poor model architecture.
  - Poor performance on language understanding benchmarks: Suggests that the pretraining data may not be sufficiently relevant to the target tasks or that the model architecture is not well-suited for these tasks.

- First 3 experiments:
  1. Train the model on a subset of the Lithuanian CulturaX data and evaluate its perplexity on the Lithuanian Q/A dataset to assess the impact of dataset size on model performance.
  2. Compare the performance of the full-parameter trained model with a PEFT-based model on the Lithuanian Q/A dataset to determine the effectiveness of each approach for this task.
  3. Evaluate the model's performance on a subset of the language understanding benchmarks to identify specific areas where the model may struggle and guide further improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of the Lithuanian component of CulturaX impact the performance of Llama2-based models on language understanding tasks like MMLU?
- Basis in paper: [explicit] The authors hypothesize that the Lithuanian component of CulturaX may not be sufficiently rich for modern LLM architectures, as the model does not show improvement on MMLU benchmarks compared to the initial Llama2 model.
- Why unresolved: The paper does not provide empirical evidence to support or refute this hypothesis. It only speculates that the dataset may lack relevant data for these specific tasks.
- What evidence would resolve it: Conducting experiments where the Lithuanian component of CulturaX is augmented with high-quality data relevant to MMLU tasks and comparing the performance of the resulting model with the original model on MMLU benchmarks.

### Open Question 2
- Question: Can parameter-efficient fine-tuning (PEFT) methods, such as LoRA or MoRA, be used to achieve efficient regional LLMs for Lithuanian while maintaining or improving performance on language understanding tasks?
- Basis in paper: [inferred] The paper mentions that some regional LLMs were trained using PEFT methods, which are less computationally demanding but potentially less efficient in performance compared to full-parameter training.
- Why unresolved: The paper does not investigate the use of PEFT methods for the proposed Lithuanian Llama2 models, so it is unclear whether these methods could be beneficial.
- What evidence would resolve it: Training the Lithuanian Llama2 models using PEFT methods and comparing their performance on language understanding tasks with the full-parameter trained models.

### Open Question 3
- Question: How does the proposed LT-Llama2-7B and LT-Llama2-13B models perform on downstream tasks beyond language understanding, such as sentiment analysis, robotics, causality, and multimodality?
- Basis in paper: [explicit] The authors mention that LLM representations are potentially useful in various scenarios, including sentiment analysis, robotics, causality, and multimodality, but they do not evaluate the proposed models on these tasks.
- Why unresolved: The paper focuses on evaluating the models on perplexity and language understanding tasks, leaving the performance on other downstream tasks unexplored.
- What evidence would resolve it: Conducting experiments to evaluate the proposed models on a diverse set of downstream tasks, such as sentiment analysis, robotics, causality, and multimodality, and comparing their performance with other models or baselines.

## Limitations

- The translation quality of benchmarks into Lithuanian is not validated, raising questions about semantic preservation and task fidelity
- The custom Lithuanian Q/A dataset (13,848 pairs) is relatively small compared to typical LLM training corpora
- The paper lacks direct ablation studies comparing full-parameter training with PEFT approaches to empirically validate claimed superiority

## Confidence

**High Confidence:** The claim that LT-Llama2 models achieve lower perplexity than Llama3-8B on Lithuanian Q/A tasks is supported by presented quantitative results in Table 4. The methodology for full-parameter training from Llama2 checkpoints is clearly specified with detailed hyperparameters.

**Medium Confidence:** The assertion that high-quality pretraining datasets are essential for strong MMLU performance is supported by observed results but lacks rigorous analysis of which specific data quality factors drive this relationship. The translation of benchmarks into Lithuanian is described but not validated for semantic preservation.

**Low Confidence:** The mechanism explaining why full-parameter training outperforms PEFT for this specific task is asserted rather than empirically demonstrated. The paper does not provide ablation studies comparing training approaches or analyzing the relationship between training data characteristics and downstream performance.

## Next Checks

1. **Benchmark Translation Validation**: Conduct human evaluation studies comparing translated benchmarks to their English originals to verify semantic preservation and task difficulty consistency across languages.

2. **Training Strategy Ablation**: Replicate the experiments using PEFT approaches (LoRA, QLoRA) with identical datasets and compare perplexity results to the full-parameter training to empirically validate the claimed superiority.

3. **Dataset Quality Analysis**: Perform detailed analysis of the Lithuanian CulturaX subset quality by sampling and categorizing content domains, checking for biases or gaps relevant to MMLU tasks, and correlating data characteristics with model performance on different benchmark categories.