---
ver: rpa2
title: A Model-based Multi-Agent Personalized Short-Video Recommender System
arxiv_id: '2405.01847'
source_url: https://arxiv.org/abs/2405.01847
tags:
- user
- learning
- which
- watchtime
- recommender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a model-based multi-agent personalized short-video
  recommender system (MMRF) to optimize user watch-time and multi-aspect explicit
  interactions over recommendation sessions. The core innovation is a collaborative
  multi-agent framework where multiple intelligent agents, each maximizing distinct
  user preferences, work together in an environment to improve watch-time optimization.
---

# A Model-based Multi-Agent Personalized Short-Video Recommender System

## Quick Facts
- arXiv ID: 2405.01847
- Source URL: https://arxiv.org/abs/2405.01847
- Authors: Peilun Zhou, Xiaoxiao Xu, Lantao Hu, Han Li, Peng Jiang
- Reference count: 21
- Primary result: Multi-agent framework successfully deployed at scale, improving watch-time and explicit interactions in short-video recommendations

## Executive Summary
This paper presents a model-based multi-agent personalized short-video recommender system (MMRF) that addresses the challenge of optimizing both user watch-time and multi-aspect explicit interactions in recommendation sessions. The system employs multiple intelligent agents, each maximizing distinct user preferences, that collaborate within an environment to enhance watch-time optimization. To tackle sample selection bias inherent in recommendation systems, the framework extends to a model-based approach by incorporating non-impression samples and simulating user feedback through a feedback fitting model. The approach demonstrates significant performance improvements through extensive offline experiments and online A/B testing, and has been successfully deployed in a real-world large-scale short-video sharing platform serving hundreds of millions of users.

## Method Summary
The MMRF framework introduces a collaborative multi-agent system where each agent is designed to optimize different aspects of user preferences. These agents work together in a shared environment to maximize user watch-time while also considering explicit interactions. To address sample selection bias, the system incorporates non-impression samples (items not shown to users) into the training process. A feedback fitting model is employed to simulate user responses, allowing the system to learn from both observed and unobserved interactions. The framework combines model-based reinforcement learning with personalized recommendation strategies to create a robust system capable of handling the complexities of short-video content delivery at scale.

## Key Results
- Successfully deployed in a large-scale short-video sharing platform serving hundreds of millions of users
- Significant improvements in both watch-time optimization and multi-aspect explicit interactions
- Effective mitigation of sample selection bias through incorporation of non-impression samples
- Strong performance demonstrated through both offline experiments and online A/B testing

## Why This Works (Mechanism)
The system works by creating a collaborative environment where multiple specialized agents, each optimized for different user preference dimensions, work together to maximize overall engagement. The model-based approach addresses a fundamental challenge in recommendation systems: the inability to learn from items users never see. By incorporating non-impression samples and using a feedback fitting model to simulate potential user responses, the system gains a more complete understanding of user preferences. This allows for more accurate recommendations that balance both implicit signals (watch-time) and explicit interactions (likes, shares, comments). The multi-agent collaboration enables the system to handle the complexity of short-video recommendations where user engagement is multifaceted and dynamic.

## Foundational Learning

**Reinforcement Learning for Recommendations** - Why needed: Traditional recommendation systems often optimize for single metrics without considering long-term user engagement. Quick check: Does the system maintain user engagement over extended periods?

**Sample Selection Bias Mitigation** - Why needed: Recommendation systems only observe feedback on shown items, creating biased training data. Quick check: How does incorporating non-impression samples affect recommendation diversity?

**Multi-Agent Collaboration** - Why needed: Single-agent systems struggle to balance competing objectives like watch-time and explicit interactions. Quick check: Do collaborating agents achieve better performance than individual agents optimizing single metrics?

**Feedback Simulation Models** - Why needed: Real user feedback is sparse and biased toward shown items only. Quick check: How accurately does the feedback fitting model predict actual user behavior?

**Model-based vs Model-free Approaches** - Why needed: Model-free methods require extensive real interactions, limiting exploration. Quick check: Does the model-based approach reduce the need for online exploration while maintaining performance?

## Architecture Onboarding

**Component Map**: User Interface -> Recommendation Engine -> Multi-Agent System -> Feedback Collection -> Model Training Pipeline -> Non-impression Sample Integration -> Updated Recommendation Model

**Critical Path**: The critical path flows from user interaction through the recommendation engine, where the multi-agent system generates recommendations, collects feedback, and updates the model. The incorporation of non-impression samples and feedback simulation occurs during the model training phase, creating a continuous learning loop that improves recommendations over time.

**Design Tradeoffs**: The system trades computational complexity for improved recommendation quality by using multiple specialized agents instead of a single unified model. The model-based approach requires additional infrastructure for simulating user feedback but provides better sample efficiency and reduced online exploration costs. The incorporation of non-impression samples increases training data diversity but requires assumptions about item quality and user preferences.

**Failure Signatures**: Performance degradation may occur if agent conflicts are not properly resolved, leading to suboptimal recommendations. The feedback simulation model may propagate errors if it poorly approximates real user behavior. Sample selection bias mitigation could introduce noise if non-impression samples are not representative of the overall item pool. System scalability issues may arise when handling hundreds of millions of users simultaneously.

**3 First Experiments**:
1. Conduct ablation studies comparing single-agent vs multi-agent performance to quantify collaborative benefits
2. Test feedback simulation accuracy by comparing simulated vs actual user responses on held-out data
3. Evaluate sample selection bias reduction by measuring recommendation diversity and coverage improvements

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Reliance on non-impression samples introduces assumptions about item representativeness that may not hold in practice
- Feedback fitting model could propagate errors if simulation deviates from real user behavior
- Paper lacks detailed analysis of agent conflict resolution mechanisms when preferences compete

## Confidence

| Claim | Confidence |
|-------|------------|
| Multi-agent collaborative framework effectiveness | Medium |
| Sample selection bias mitigation | Medium |
| Online A/B testing results | Low |
| Real-world deployment success | Low |

## Next Checks
1. Conduct ablation studies to isolate the contribution of each agent and verify collaborative gains exceed individual agent performance
2. Perform sensitivity analysis on non-impression sample selection to quantify bias reduction effectiveness
3. Extend online experiments with longer durations and multiple time periods to assess temporal stability of performance gains