---
ver: rpa2
title: Investigating Generalization Behaviours of Generative Flow Networks
arxiv_id: '2402.05309'
source_url: https://arxiv.org/abs/2402.05309
tags:
- gflownets
- training
- learning
- generalization
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates generalization behaviors of Generative Flow
  Networks (GFlowNets) in discrete spaces. The authors introduce novel graph-based
  benchmark tasks where reward difficulty can be varied, exact probability computation
  is possible, and unseen test sets can be constructed to measure generalization.
---

# Investigating Generalization Behaviours of Generative Flow Networks

## Quick Facts
- arXiv ID: 2402.05309
- Source URL: https://arxiv.org/abs/2402.05309
- Authors: Lazar Atanackovic; Emmanuel Bengio
- Reference count: 40
- Primary result: GFlowNets generalize well due to implicit structure in learned functions, but are sensitive to offline/off-policy training while reward prediction remains robust to monotonic transformations.

## Executive Summary
This work systematically investigates generalization behaviors of Generative Flow Networks (GFlowNets) in discrete spaces through novel graph-based benchmark tasks. The authors introduce environments where reward difficulty can be varied, exact probability computation is possible, and unseen test sets can be constructed to measure generalization. Through three experimental settings—distilling flows, memorization gaps, and offline/off-policy training—they empirically validate hypothesized generalization mechanisms. Key findings include that GFlowNets generalize well due to implicit structure in learned functions, offline/off-policy training can harm p(x) approximation while still learning reward structure, and monotonic reward transformations don't significantly affect generalization. The work provides systematic empirical validation of GFlowNet generalization properties and identifies both expected and surprising behaviors in different training regimes.

## Method Summary
The authors introduce novel graph-based benchmark tasks where reward difficulty can be varied, exact probability computation is possible, and unseen test sets can be constructed. They implement GNN-based parameterization of flow functions F and policies PF, using online and offline GFlowNet training with SubTB(1) objective. The evaluation uses Jensen-Shannon divergence and mean absolute error between logp(x) and learned logp(x;θ). Three main experimental settings are conducted: distilling flows by regressing to true flow values F(s→s′), memorization gap experiments by training with shuffled vs true rewards, and offline/off-policy training with different sampling distributions. The framework is also validated on hypergrid and sequence environments for comparison.

## Key Results
- GFlowNets generalize well due to implicit structure in learned functions (F and PF) that captures both reward and transition structure
- Offline/off-policy training can harm p(x) approximation while still learning reward structure effectively
- Monotonic reward transformations do not significantly affect generalization difficulty across tested hyperparameters
- Memorization gaps appear when training with de-structured data, confirming that structured flows reduce overfitting
- Policy interpolation experiments show GFlowNets are sensitive to deviations from on-policy training distribution

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GFlowNets generalize well because the functions they learn to approximate (F and PF) have implicit underlying structure that facilitates generalization.
- **Mechanism:** The state flows F and policies PF capture both reward structure and transition structure, acting as a regularizer that prevents overfitting and enables generalization to unseen states.
- **Core assumption:** The environment has meaningful structure that can be captured by flow functions, and this structure is beneficial for generalization.
- **Evidence anchors:** [abstract] "functions that GFlowNets learn to approximate have an implicit underlying structure which facilitate generalization"; [section] "regressing to R and PF induce curves with no plateau, while regressing to ˜R and P random F initially have very flat plateaus"; [corpus] Weak support from related GFlowNet generalization work.
- **Break condition:** If environment lacks meaningful structure or flow functions cannot capture it effectively, generalization breaks down.

### Mechanism 2
- **Claim:** Offline and off-policy training can harm p(x) approximation while still learning reward structure.
- **Mechanism:** When trained offline/off-policy, GFlowNets may fail to learn proper flow functions for p(x), but can still learn F(s) which implicitly predicts reward, creating a mismatch where rewards are predicted but samples aren't from correct distribution.
- **Core assumption:** Self-induced training distribution from PF is crucial for learning proper flow functions that determine p(x).
- **Evidence anchors:** [abstract] "GFlowNets are sensitive to being trained offline and off-policy"; [section] "Deviating from the on-policy training distribution induced by PF can negatively affect ability of GFlowNets' to learn p(x)"; [corpus] Weak evidence, contradicts some related work claims.
- **Break condition:** If training distribution deviates too far from self-induced distribution, p(x) approximation breaks down completely.

### Mechanism 3
- **Claim:** Monotonic reward transformations do not significantly affect generalization difficulty.
- **Mechanism:** Difficulty of learning p(x) depends more on task structure complexity than specific reward distribution shape; monotonic transformations preserve relative ordering of rewards, which matters for learning.
- **Core assumption:** Generalization difficulty is determined by task structure rather than reward distribution shape.
- **Evidence anchors:** [abstract] "reward implicitly learned by GFlowNets is robust to changes in training distribution"; [section] "Monotonic transformations, such as skew and reward tempering, do not have detrimental impact on generalization"; [corpus] Moderate support from reward shaping literature in RL.
- **Break condition:** If transformations are non-monotonic or fundamentally change task structure, generalization difficulty changes significantly.

## Foundational Learning

- **Concept:** Graph Neural Networks (GNNs)
  - **Why needed here:** Used to parameterize GFlowNet functions and understand task difficulty. Understanding GNN architectures and capabilities is crucial for interpreting results.
  - **Quick check question:** What are the key differences between GAT, GCN, and GIN architectures, and how might these differences affect their ability to learn different types of graph structures?

- **Concept:** Dynamic Programming
  - **Why needed here:** Used to compute p(x;θ), F(s), and PF exactly for graph environment, essential for creating benchmark and evaluating performance.
  - **Quick check question:** How does the topological ordering in the DAG enable efficient computation of flow values using dynamic programming?

- **Concept:** Memorization vs Generalization
  - **Why needed here:** Used to understand when models are generalizing versus simply memorizing training data, central to interpreting experimental results.
  - **Quick check question:** What is the key difference between training with structured data versus de-structured data in the memorization gap experiments?

## Architecture Onboarding

- **Component map:** Graph environment with varying reward functions (counting, neighbors, cliques) -> GNN-based parameterization of flow functions (F and PF) -> Training objectives (SubTB(1), offline/off-policy variants) -> Evaluation metrics (JS divergence, MAE on log probabilities) -> Benchmark comparison environments (hypergrid, sequence)

- **Critical path:**
  1. Define graph environment and reward functions
  2. Compute exact flow values using dynamic programming
  3. Train GNN to approximate flow functions (online or offline)
  4. Evaluate p(x;θ) approximation using test set
  5. Analyze generalization performance across different training regimes

- **Design tradeoffs:**
  - Online vs offline training: Online uses self-induced distribution but may have non-stationarity; offline has fixed data but may miss important exploration
  - Flow vs policy approximation: Learning F captures absolute magnitudes but is harder; learning PF is easier but may lose information
  - Reward complexity vs state space size: More complex rewards require larger state spaces for meaningful generalization tests

- **Failure signatures:**
  - JS divergence or MAE not decreasing during training
  - Memorization gaps appearing in de-structured training
  - Offline training failing to converge on p(x) approximation
  - Policy interpolation showing sensitivity to off-policy sampling

- **First 3 experiments:**
  1. Train GNN to regress to true flow values F(s→s′) and compare with online GFlowNet training
  2. Run memorization gap experiment by training with shuffled rewards vs true rewards
  3. Test offline training with different sampling distributions (uniform, log-rewards, proxy for on-policy)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific structural properties of flow functions (F and PF) enable generalization in GFlowNets, and can these be mathematically formalized?
- **Basis in paper:** [explicit] Paper confirms structured flows reduce memorization gaps and facilitate generalization, particularly when environment structure is maintained even with shuffled rewards
- **Why unresolved:** Demonstrates empirically that structured flows help with generalization but does not identify precise mathematical properties that drive this behavior
- **What evidence would resolve it:** Mathematical proofs characterizing relationship between flow function structure and generalization performance, or experiments isolating specific structural features and measuring their individual contributions

### Open Question 2
- **Question:** Why does offline and off-policy training of GFlowNets show such divergent behavior across different environments (graph vs sequence/hypergrid), and what underlying factors determine this?
- **Basis in paper:** [explicit] Finds offline/off-policy training can harm p(x) approximation while learning reward structure in graph tasks, but differs from sequence/hypergrid environments where p(x) is adequately modeled
- **Why unresolved:** Observes discrepancy but does not explain why environment complexity and structure would lead to different generalization behaviors under offline training conditions
- **What evidence would resolve it:** Controlled experiments varying environment complexity systematically, theoretical analysis of how offline training interacts with different state space structures

### Open Question 3
- **Question:** What is the mechanism by which GFlowNets implicitly learn to predict rewards (log R) on unseen states even when PF fails to approximate p(x) adequately?
- **Basis in paper:** [explicit] Observes learning F(s) can facilitate implicit reward prediction on unseen states during offline/off-policy training, even when PF does not adequately model p(x)
- **Why unresolved:** Documents phenomenon but does not explain why flow function F(s) would capture reward information that generalizes better than policy PF, or what properties of F make it more robust to distributional shifts
- **What evidence would resolve it:** Analysis of mathematical relationship between F(s) and R(s), experiments isolating F(s) learning from PF(s) learning

### Open Question 4
- **Question:** How can GFlowNet objectives be modified to better handle offline and off-policy training scenarios while maintaining generalization to unseen states?
- **Basis in paper:** [explicit] Finds offline/off-policy training can lead to poor p(x) approximation and suggests this is a challenge for GFlowNet development, though notes F(s) learning remains informative
- **Why unresolved:** Identifies practical challenge but does not propose or test specific algorithmic modifications to address offline/off-policy generalization problem
- **What evidence would resolve it:** New GFlowNet objectives or training algorithms designed for offline settings showing improved p(x) approximation on test sets

## Limitations
- Experiments conducted primarily on synthetic graph-based environments that may not capture real-world complexity
- Findings regarding offline/off-policy training sensitivity may be specific to discrete graph setting and not generalize to continuous or high-dimensional spaces
- Memorization gap experiments use shuffled reward baseline that may not represent all forms of de-structured data encountered in practice

## Confidence
- **High Confidence:** GFlowNets' ability to generalize due to implicit structure in learned functions is well-supported by multiple experimental settings and consistent with related work on flow-based models
- **Medium Confidence:** Sensitivity to offline/off-policy training is demonstrated empirically but may be context-dependent; aligns with known challenges in off-policy RL but contradicts some claims about GFlowNet robustness
- **Low Confidence:** Robustness of monotonic reward transformations is tested only within limited hyperparameter range and may not hold for more extreme transformations or different task structures

## Next Checks
1. **Generalization to Continuous Spaces:** Replicate offline/off-policy sensitivity experiments in continuous GFlowNet implementations to verify if observed behavior extends beyond discrete graph environments

2. **Architecture Sensitivity Analysis:** Systematically vary GNN architecture (GAT, GCN, GIN) and depth to determine how architectural choices affect generalization performance and offline training robustness

3. **Real-World Task Validation:** Apply benchmark framework to real-world combinatorial optimization problem (such as molecular design or program synthesis) to validate whether observed generalization behaviors transfer to practical applications