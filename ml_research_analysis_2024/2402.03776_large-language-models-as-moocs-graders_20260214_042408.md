---
ver: rpa2
title: Large Language Models As MOOCs Graders
arxiv_id: '2402.03776'
source_url: https://arxiv.org/abs/2402.03776
tags:
- grading
- answers
- each
- llms
- rubrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study explored the use of large language models (LLMs) to
  automate grading in massive open online courses (MOOCs), aiming to replace the current
  peer grading system. Three prompting strategies based on a variant of the zero-shot
  chain-of-thought technique were employed: using instructor-provided answers, combining
  instructor answers with rubrics, and using instructor answers with LLM-generated
  rubrics.'
---

# Large Language Models As MOOCs Graders

## Quick Facts
- arXiv ID: 2402.03776
- Source URL: https://arxiv.org/abs/2402.03776
- Authors: Shahriar Golchin; Nikhil Garuda; Christopher Impey; Matthew Wenger
- Reference count: 36
- One-line primary result: GPT-4 with instructor-provided answers and rubrics outperforms peer grading in MOOC assignment evaluation.

## Executive Summary
This study evaluates the use of large language models (LLMs) to automate grading in massive open online courses (MOOCs), aiming to replace the current peer grading system. The research employs three prompting strategies based on a variant of zero-shot chain-of-thought prompting across three MOOC courses using 18 distinct settings. Results demonstrate that GPT-4 with instructor-provided answers and rubrics most closely aligns with instructor grades compared to peer grading, though grading imaginative thinking in courses like History and Philosophy of Astronomy proved more challenging.

## Method Summary
The study uses 10 writing assignments each from three MOOCs: Introductory Astronomy, Astrobiology, and History and Philosophy of Astronomy. Three prompting strategies were employed: (1) Zero-shot-CoT with instructor-provided correct answers, (2) Zero-shot-CoT with instructor answers and rubrics, and (3) Zero-shot-CoT with instructor answers and LLM-generated rubrics. GPT-4 and GPT-3.5 were used as the LLMs, with performance evaluated through bootstrap resampling using 10,000 resampled samples to compare LLM grades against instructor and peer grades.

## Key Results
- GPT-4 with instructor-provided answers and rubrics outperforms both GPT-3.5 and peer grading in score alignment with instructor grades
- GPT-4 demonstrates consistent superiority over GPT-3.5, particularly in challenging courses requiring speculative thinking
- LLM-generated rubrics can match instructor-provided rubrics in grading performance, enabling fully automated grading

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM grading can better align with instructor grades than peer grading when provided with instructor-crafted rubrics and answers
- Mechanism: Instructor-provided rubrics give LLMs explicit scoring rules, reducing ambiguity and aligning evaluation logic with instructor expectations
- Core assumption: Instructor rubrics and correct answers are sufficiently detailed and consistent to guide LLM grading decisions
- Evidence anchors:
  - [abstract] "Zero-shot-CoT, when integrated with instructor-provided answers and rubrics, produces grades that are more aligned with those assigned by instructors compared to peer grading"
  - [section] "Our results suggested that the integration of Zero-shot-CoT with instructor-presented correct answers and rubrics provided by either the instructor or LLM, when using GPT-4 as the underlying foundation model, surpasses peer grading across all subjects"
  - [corpus] Weak corpus support; no direct studies comparing rubric-guided LLM grading to peer grading

### Mechanism 2
- Claim: GPT-4 consistently outperforms GPT-3.5 in grading alignment with instructors, even in challenging courses requiring speculative thinking
- Mechanism: GPT-4's larger model capacity and more recent training data provide better reasoning and contextual understanding for grading nuanced responses
- Core assumption: GPT-4's architectural and training advantages translate to grading accuracy beyond raw language modeling
- Evidence anchors:
  - [abstract] "GPT-4 using Zero-shot-CoT prompting with both instructor-provided answers and rubrics outperforms GPT-3.5 and peer grading in terms of score alignment with those given by instructors"
  - [section] "GPT-4 overall outperforms GPT-3.5 by generating grades that are more closely aligned with the grades provided by instructors, especially in the context of the History and Philosophy of Astronomy course"
  - [corpus] Weak corpus support; no comparative GPT-4 vs GPT-3.5 grading studies in the corpus

### Mechanism 3
- Claim: LLM-generated rubrics can match instructor-provided rubrics in grading performance, enabling fully automated grading
- Mechanism: LLMs can infer grading criteria from correct answers and generate detailed rubrics without human intervention
- Core assumption: LLMs can distill grading standards from answer content alone, producing rubrics that align with human grading logic
- Evidence anchors:
  - [abstract] "our findings demonstrate that this alignment is more pronounced in courses that require less creative and imaginative thinking"
  - [section] "From the results listed in both Table 1 and Table 2, it is clear that the most difficult course to grade is the History and Philosophy of Astronomy"
  - [corpus] Weak corpus support; no direct comparison of LLM-generated vs instructor rubrics in the corpus

## Foundational Learning

- Concept: Zero-shot Chain-of-Thought (Zero-shot-CoT) prompting
  - Why needed here: Guides LLM reasoning step-by-step, improving grading consistency and reducing hallucinations
  - Quick check question: What is the main benefit of adding a reasoning step to zero-shot prompts?

- Concept: Bootstrap resampling
  - Why needed here: Provides statistical confidence in grading performance estimates when using limited student data
  - Quick check question: Why resample 10,000 times when grading only 10 assignments?

- Concept: Rubric-based grading
  - Why needed here: Ensures grading fairness and transparency, especially for automated systems
  - Quick check question: What is the role of rubrics in LLM grading compared to peer grading?

## Architecture Onboarding

- Component map:
  - Student assignments, instructor answers, rubrics -> LLM engine (GPT-4/GPT-3.5 with Zero-shot-CoT) -> Rubric generator (GPT-4 for auto-generating rubrics) -> Scoring engine (Grade computation and comparison) -> Validation layer (Bootstrap resampling)

- Critical path:
  1. Load assignment and rubric data
  2. Generate LLM prompt with Zero-shot-CoT
  3. Run LLM to assign grades
  4. Compare LLM grades to instructor and peer grades
  5. Perform bootstrap resampling to validate results

- Design tradeoffs:
  - LLM choice (GPT-4 vs GPT-3.5): Accuracy vs cost
  - Rubric source (instructor vs LLM-generated): Control vs automation
  - Prompt complexity: Detail vs prompt token limits

- Failure signatures:
  - Grades drift significantly from instructor grades
  - Bootstrap resampling shows high variance
  - LLM-generated rubrics produce inconsistent scoring logic

- First 3 experiments:
  1. Test Zero-shot-CoT with instructor answers only on one assignment
  2. Compare GPT-4 vs GPT-3.5 grading performance on the same assignment
  3. Generate LLM rubrics from correct answers and test grading consistency

## Open Questions the Paper Calls Out

- How well would LLMs perform in grading MOOCs across diverse cultural contexts and educational systems?
  - Basis in paper: [explicit] The authors note their courses are "representative of the audience on Coursera as a whole" but acknowledge they might not be "universally representative of the global population"
  - Why unresolved: The study's data comes from a limited subset of students primarily from the US and India, and the authors themselves recognize potential sampling limitations
  - What evidence would resolve it: Testing LLM grading performance across MOOCs from multiple countries with diverse educational backgrounds and cultural contexts

- Can LLMs maintain grading consistency and fairness when evaluating assignments that require subjective or creative responses?
  - Basis in paper: [explicit] The authors found that grading the History and Philosophy of Astronomy course, which requires imaginative thinking, was "more challenging" compared to other courses
  - Why unresolved: While the study shows GPT-4 performs better than peer grading in these scenarios, the degree of consistency and potential biases in subjective grading remains unexplored
  - What evidence would resolve it: Comparative analysis of LLM grading consistency across multiple creative assignments and correlation with instructor grading patterns

- How scalable is the LLM grading approach when applied to MOOCs with tens of thousands of students?
  - Basis in paper: [explicit] The authors mention budget limitations prevented them from grading all assignments, leading them to use bootstrap resampling on a subset of 10 students per course
  - Why unresolved: The study uses resampling techniques to estimate performance but doesn't test the approach at actual MOOC scale
  - What evidence would resolve it: Implementation and evaluation of the LLM grading system on a full-scale MOOC with massive enrollment

## Limitations
- Study relies on a relatively small sample of 10 assignments per course (30 total), which may not capture full variability of student responses
- Performance of LLM grading in highly subjective domains like History and Philosophy of Astronomy remains unclear, as these courses showed the most deviation from instructor grades
- Study does not address potential bias in LLM grading or examine long-term consistency across multiple grading cycles

## Confidence
- **High confidence**: LLM grading with instructor rubrics and answers outperforms peer grading (supported by direct experimental results)
- **Medium confidence**: GPT-4 consistently outperforms GPT-3.5 across all course types (supported by comparative results, though sample size is limited)
- **Medium confidence**: LLM-generated rubrics can match instructor-provided rubrics in grading performance (supported by results but lacks comparative validation studies)

## Next Checks
1. Test grading performance on a larger sample (50+ assignments per course) to assess scalability and statistical robustness
2. Conduct a bias audit by analyzing grading consistency across different student demographics and response styles
3. Implement cross-validation by having a separate instructor cohort validate LLM grades to establish external reliability