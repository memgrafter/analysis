---
ver: rpa2
title: 'Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding'
arxiv_id: '2402.12374'
source_url: https://arxiv.org/abs/2402.12374
tags:
- tree
- algorithm
- sequoia
- tokens
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SEQUOIA, a speculative decoding method that
  improves upon existing approaches by optimizing the tree structure for scalability
  and using a robust sampling and verification algorithm. SEQUOIA employs dynamic
  programming to find an optimal token tree structure and modifies the SpecInfer algorithm
  by sampling without replacement from the draft model to avoid repeated mistakes.
---

# Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding

## Quick Facts
- arXiv ID: 2402.12374
- Source URL: https://arxiv.org/abs/2402.12374
- Authors: Zhuoming Chen; Avner May; Ruslan Svirschevski; Yuhsun Huang; Max Ryabinin; Zhihao Jia; Beidi Chen
- Reference count: 40
- Primary result: Up to 4.04× speedup for Llama2-7B on A100 GPU and 9.5× speedup for Llama3-70B-Instruct in offloading setting on L40 GPU

## Executive Summary
SEQUOIA introduces a speculative decoding method that significantly outperforms existing approaches by optimizing tree structure for scalability and using a robust sampling and verification algorithm. The method employs dynamic programming to find optimal token tree structures and modifies the SpecInfer algorithm by sampling without replacement from the draft model to avoid repeated mistakes. This approach achieves substantial speedups across different hardware platforms and demonstrates improved robustness across various temperatures and top-p parameters.

## Method Summary
SEQUOIA accelerates LLM inference by constructing optimized token trees where each node represents a potential sequence of tokens. The method uses dynamic programming to find the optimal tree structure based on acceptance rate vectors, which describe the probability that a draft model's tokens will be accepted by the target model. During inference, SEQUOIA samples tokens without replacement from the draft model distribution and verifies them using the target model, achieving faster generation while maintaining output quality. A hardware-aware optimizer automatically selects the optimal tree size and depth for specific hardware platforms.

## Key Results
- Achieves up to 4.04× speedup for Llama2-7B on A100 GPU
- Reduces latency to 0.60 s/token for Llama3-70B-Instruct in offloading setting on L40 GPU (9.5× speedup)
- Outperforms SpecInfer and top-k sampling in scalability and robustness across different temperatures
- Demonstrates logarithmic scaling of generated tokens with tree size under power-law acceptance rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SEQUOIA's dynamic programming algorithm finds the optimal tree structure that scales with tree size, unlike existing methods which asymptote.
- Mechanism: The algorithm maximizes expected token generation by solving a constrained optimization problem over tree topologies using dynamic programming to efficiently search the exponentially large space of possible structures.
- Core assumption: Under the positional acceptance assumption, acceptance dynamics can be completely described by an acceptance vector, allowing closed-form computation of expected token generation.
- Evidence anchors:
  - [abstract] "To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens."
  - [section] "To derive the SEQUOIA tree construction algorithm, we first express the tree construction problem as a constrained optimization problem, and then use dynamic programming to solve this problem optimally and efficiently."
  - [corpus] Weak - related work mentions dynamic token tree structures but doesn't provide specific evidence about SEQUOIA's optimality proof.
- Break condition: If the positional acceptance assumption fails (acceptance rates depend on context beyond position in tree), the closed-form equations and dynamic programming approach would no longer be valid.

### Mechanism 2
- Claim: SEQUOIA's sampling without replacement prevents the draft model from making the same mistake twice, improving robustness across temperatures.
- Mechanism: By sampling without replacement from the draft model and using the uniform distribution over non-rejected tokens when all draft model tokens are exhausted, SEQUOIA avoids repeatedly sampling and rejecting low-quality tokens that the draft model is confident in.
- Core assumption: The draft model's probability distribution over tokens remains meaningful even after removing previously sampled tokens, and the uniform distribution over non-rejected tokens provides a reasonable fallback.
- Evidence anchors:
  - [abstract] "SEQUOIA uses a novel sampling and verification method that outperforms prior work across different decoding temperatures."
  - [section] "It performs sampling without replacement using the draft model distribution. Second, if all the tokens with non-zero draft model probability have already been sampled and rejected, it uses the uniform distribution over all tokens that have not yet been sampled as the new draft model distribution."
  - [corpus] Weak - related work mentions token tree structures but doesn't provide specific evidence about sampling without replacement mechanisms.
- Break condition: If the draft model's probability distribution becomes uninformative after removing tokens (e.g., highly peaked distributions), the sampling without replacement approach may fail to find good tokens.

### Mechanism 3
- Claim: SEQUOIA's hardware-aware tree optimizer automatically selects optimal tree size and depth for specific hardware platforms, maximizing speedups.
- Mechanism: The optimizer measures hardware-dependent parameters (time to verify n tokens, time to draft 1 token) and searches over possible tree sizes and depths to find the combination that maximizes the speedup formula: Speedup(n,d) = G(n,d) / (t(n) + d·c).
- Core assumption: The relationship between tree depth and speculation time is linear and predictable for a given hardware platform, and the hardware parameters can be accurately measured.
- Evidence anchors:
  - [abstract] "Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a given hardware platform."
  - [section] "Letting G(n,d) denote the expected number of tokens generated by verifying the SEQUOIA tree of size n and depth d, t(n) denote the (hardware-dependent) amount of time it takes the target model to verify n tokens divided by the time to verify 1 token, and c denote the (hardware-dependent) time to draft 1 token by the time to verify 1 token, the speedup attained by SEQUOIA can be expressed as Speedup(n,d)= G(n,d) / (t(n)+d·c)."
  - [corpus] Weak - related work mentions hardware-aware optimization but doesn't provide specific evidence about SEQUOIA's optimizer implementation.
- Break condition: If the relationship between depth and speculation time becomes non-linear due to hardware-specific effects (caching, memory access patterns), the linear model used by the optimizer would be inaccurate.

## Foundational Learning

- Concept: Dynamic programming for tree optimization
  - Why needed here: The exponentially large space of possible tree topologies requires an efficient search algorithm that can find optimal structures without exhaustive enumeration.
  - Quick check question: What is the time complexity of the SEQUOIA dynamic programming algorithm in terms of tree size N and maximum branch factor B?

- Concept: Positional acceptance assumption in speculative decoding
  - Why needed here: This assumption allows SEQUOIA to use closed-form equations for expected token generation and simplifies the optimization problem to depend only on acceptance probabilities rather than full context.
  - Quick check question: How does the positional acceptance assumption differ from assuming independence between acceptance decisions at different nodes?

- Concept: Sampling without replacement and its effect on probability distributions
  - Why needed here: Understanding how removing tokens from a probability distribution affects its properties is crucial for SEQUOIA's robust sampling algorithm.
  - Quick check question: If you sample without replacement from a uniform distribution over n tokens, what is the probability distribution after k samples have been taken?

## Architecture Onboarding

- Component map: Dynamic programming engine -> Tree construction module -> Sampling and verification module -> Hardware-aware optimizer -> Integration layer

- Critical path:
  1. Measure acceptance rate vector on representative data
  2. Run dynamic programming to find optimal tree structure
  3. During inference: Build tree → Sample without replacement → Verify tokens → Accept/reject
  4. Hardware-aware optimizer selects parameters before deployment

- Design tradeoffs:
  - Tree depth vs. speculation time: Deeper trees generate more tokens but take longer to speculate
  - Tree size vs. memory usage: Larger trees provide better speedups but require more memory
  - Sampling without replacement vs. computational overhead: More robust but requires tracking rejected tokens

- Failure signatures:
  - Low acceptance rates: May indicate poor draft model quality or incorrect acceptance rate measurements
  - Memory errors: Tree size too large for available memory
  - Suboptimal speedups: Hardware-aware optimizer parameters incorrect or hardware model inaccurate

- First 3 experiments:
  1. Measure acceptance rate vector for your draft/target model pair on representative data
  2. Run dynamic programming with different depth limits to understand tradeoff between depth and token generation
  3. Implement hardware-aware optimizer and validate speedup predictions against actual measurements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal Sequoia tree structure change when using different token embedding dimensions or model architectures beyond the ones tested?
- Basis in paper: [inferred] The paper mentions that the optimal tree depends on the draft/target model pair, but only tests a limited set of configurations.
- Why unresolved: The dynamic programming approach requires computing acceptance rate vectors for each model pair, which is computationally expensive. The paper doesn't explore how tree structures generalize across different architectures.
- What evidence would resolve it: Experiments showing Sequoia tree structures for various model families (e.g., different embedding sizes, attention mechanisms, or architectures like Mamba) and measuring their acceptance rates.

### Open Question 2
- Question: What is the theoretical limit of Sequoia's scalability when the acceptance rate vector approaches a perfect power-law distribution?
- Basis in paper: [explicit] Theorem 3.6 shows Sequoia trees scale logarithmically under power-law acceptance rates, but this is a lower bound and doesn't explore the upper limits.
- Why unresolved: The paper provides a lower bound but doesn't analyze how close Sequoia can get to theoretical maximum token generation or what factors might limit it.
- What evidence would resolve it: Mathematical analysis deriving the maximum possible token generation rate under perfect power-law acceptance rates, and experimental validation showing how close Sequoia gets to this limit.

### Open Question 3
- Question: How does Sequoia's performance change when the draft model is much weaker than the target model (e.g., order of magnitude smaller)?
- Basis in paper: [inferred] The paper tests various draft/target pairs but doesn't explore extreme mismatches where the draft model's predictions are mostly incorrect.
- Why unresolved: The paper focuses on cases where the draft model is reasonably accurate, but doesn't test scenarios where it's significantly weaker, which could affect the acceptance rate dynamics.
- What evidence would resolve it: Experiments with draft models that have significantly lower perplexity than the tested configurations, measuring acceptance rates and speedup.

## Limitations

- The optimality of the dynamic programming algorithm depends on the positional acceptance assumption, which may not hold in practice when acceptance rates depend on full context rather than just position in the tree.
- The hardware-aware optimizer assumes linear relationships between depth and speculation time, which may not capture complex hardware effects like caching and memory access patterns.
- The sampling without replacement mechanism depends on the draft model maintaining meaningful probability distributions after token removal, which may fail for highly peaked distributions.

## Confidence

**High Confidence**: The empirical speedups reported (4.04× for Llama2-7B, 9.5× for Llama3-70B-Instruct) are well-supported by experimental results across multiple model pairs, datasets, and hardware configurations. The mechanism of sampling without replacement preventing repeated mistakes is straightforward and verifiable.

**Medium Confidence**: The optimality of the dynamic programming algorithm under the positional acceptance assumption is theoretically sound but depends on this assumption holding in practice. The scalability improvements over existing methods are demonstrated but may vary with different acceptance rate distributions.

**Low Confidence**: The hardware-aware optimizer's ability to automatically select optimal parameters for arbitrary hardware platforms relies on accurate measurements of hardware-dependent parameters, which may be difficult to obtain in practice and may not generalize across different workload patterns.

## Next Checks

1. **Accept rate vector validation**: Measure acceptance rate vectors for your specific draft/target model pair on representative data and verify that the SEQUOIA tree construction algorithm produces optimal structures (number of generated tokens should grow logarithmically with tree size).

2. **Hardware parameter measurement**: Implement the hardware-aware optimizer by measuring t(n) values and c ratios for your target hardware, then validate the predicted speedups against actual measurements across different tree sizes and depths.

3. **Robustness testing across temperatures**: Compare SEQUOIA's performance with sampling without replacement against baseline SpecInfer and top-k sampling across a range of temperatures (0.0 to 2.0) to verify the claimed robustness improvements and faster rejection rate decay.