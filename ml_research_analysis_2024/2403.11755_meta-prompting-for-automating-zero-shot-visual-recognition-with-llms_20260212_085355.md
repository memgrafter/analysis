---
ver: rpa2
title: Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs
arxiv_id: '2403.11755'
source_url: https://arxiv.org/abs/2403.11755
tags:
- prompts
- mpvr
- dataset
- zero-shot
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Meta-Prompting for Visual Recognition (MPVR),
  a method to automate zero-shot visual recognition by generating diverse, task-specific
  prompts using large language models (LLMs). MPVR addresses the challenge of manually
  crafting prompts for vision-language models (VLMs) by leveraging meta-prompting
  to generate task-specific LLM query templates and then populating them with class
  labels to obtain category-specific VLM prompts.
---

# Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs

## Quick Facts
- arXiv ID: 2403.11755
- Source URL: https://arxiv.org/abs/2403.11755
- Authors: M. Jehanzeb Mirza; Leonid Karlinsky; Wei Lin; Sivan Doveh; Jakub Micorek; Mateusz Kozinski; Hilde Kuehne; Horst Possegger
- Reference count: 40
- Key outcome: Up to 19.8% and 18.2% improvements on specific datasets using GPT and Mixtral LLMs respectively

## Executive Summary
This paper introduces Meta-Prompting for Visual Recognition (MPVR), a method that automates zero-shot visual recognition by generating diverse, task-specific prompts using large language models. MPVR addresses the challenge of manually crafting prompts for vision-language models by leveraging meta-prompting to generate task-specific LLM query templates and then populating them with class labels. The approach eliminates human effort in prompt design while producing visually diverse descriptions that enhance zero-shot classification performance. MPVR achieves significant improvements over CLIP and other baselines, with up to 19.8% and 18.2% gains on specific datasets.

## Method Summary
MPVR uses a two-stage meta-prompting approach to automate prompt generation for zero-shot visual recognition. First, it extracts task-specific query templates from an LLM using minimal task metadata (e.g., dataset name and category descriptions). Second, it populates these templates with class names to generate category-specific VLM prompts. The method ensembles multiple prompts per class to create rich visual representations, which are then used with dual-encoder vision-language models like CLIP for zero-shot classification. This eliminates manual prompt engineering while producing diverse visual descriptions that improve classification accuracy across diverse domains.

## Key Results
- MPVR achieves up to 19.8% and 18.2% improvements on specific datasets using GPT and Mixtral LLMs respectively
- Averaging 5.0% and 4.5% gains across 20 diverse datasets with GPT and Mixtral
- Outperforms training-based approaches and hand-crafted prompt templates
- Demonstrates robustness across different LLMs (GPT-4, Mixtral) and VLM backbones (CLIP variants)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Meta-prompting extracts task-specific visual styles from the LLM without manual prompt engineering
- Mechanism: The two-stage approach first asks the LLM to generate diverse task-specific query templates using minimal task metadata, then populates these templates with class names to generate visually diverse VLM prompts
- Core assumption: LLMs encode rich visual world knowledge that can be extracted through carefully structured meta-prompts
- Evidence anchors:
  - [abstract] "MPVR automatically produces a diverse set of category-specific prompts"
  - [section 3.1] "Meta-Prompting a Large Language Model" describes the two-step extraction process
  - [corpus] "Meta Prompting (MP), a framework that elevates the reasoning capabilities of large language models (LLMs) by focusing on the formal structure of a task rather than content-specific examples"

### Mechanism 2
- Claim: Ensembling diverse VLM prompts improves zero-shot classification accuracy
- Mechanism: Each class is represented by an average embedding of multiple task- and category-specific prompts, capturing different visual aspects of the class
- Core assumption: CLIP's text encoder responds favorably to semantically rich text descriptions rather than simple templates
- Evidence anchors:
  - [abstract] "embedding class names in a prompt of the form 'A photo of a <class name>' resulted in considerable performance growth"
  - [section 3] "Ensembling a larger number of class-specific VLM prompts improves the performance"
  - [corpus] "CLIP-2: Open Foundation and Fine-Tuned Chat Models" shows performance gains from prompt engineering

### Mechanism 3
- Claim: MPVR generalizes across different LLMs and VLM backbones
- Mechanism: The meta-prompting approach is LLM-agnostic and the generated prompts can be used with any dual-encoder VLM
- Core assumption: Different LLMs share common visual knowledge patterns that can be extracted through similar meta-prompting
- Evidence anchors:
  - [abstract] "MPVR generalizes effectively across various popular zero-shot image recognition benchmarks... when tested with multiple LLMs and VLMs"
  - [section 4.1] "We test our MPVR extensively on 20 diverse datasets... with both the GPT [4] and Mixtral [18] LLMs"
  - [corpus] "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision" demonstrates cross-model generalization

## Foundational Learning

- Concept: Prompt engineering and in-context learning
  - Why needed here: MPVR builds on these NLP techniques to extract visual knowledge from LLMs
  - Quick check question: What is the difference between zero-shot and few-shot prompting in LLMs?

- Concept: Vision-language models and dual-encoder architecture
  - Why needed here: Understanding how CLIP and similar models work is crucial for designing effective prompts
  - Quick check question: How does the cosine similarity between image and text embeddings enable zero-shot classification?

- Concept: Meta-learning and task abstraction
  - Why needed here: MPVR abstracts the task specification to generate appropriate prompts without manual engineering
  - Quick check question: How does meta-learning differ from traditional supervised learning?

## Architecture Onboarding

- Component map: Task description + class labels -> Meta-prompt + LLM -> Task-specific query templates -> Populate with class names -> Category-specific VLM prompts -> VLM text encoder -> Class embeddings -> Zero-shot classifier

- Critical path:
  1. Extract task metadata from public APIs/webpages
  2. Generate meta-prompt with fixed system prompt and in-context example
  3. First LLM call to get task-specific query templates
  4. Populate templates with class names
  5. Second LLM call to get category-specific VLM prompts
  6. Encode prompts with VLM text encoder
  7. Compute class embeddings and classify test images

- Design tradeoffs:
  - Fixed vs. adaptive in-context examples
  - Number of query templates vs. generation cost
  - Token limits vs. prompt richness
  - Closed-source vs. open-source LLM choice

- Failure signatures:
  - Poor performance on certain datasets → Check meta-prompt quality and task metadata
  - Inconsistent results → Check LLM API stability and prompt formatting
  - Slow generation → Check token limits and number of templates

- First 3 experiments:
  1. Run MPVR on a simple dataset (e.g., Flowers) with GPT and verify improvement over baseline
  2. Test MPVR with Mixtral LLM on the same dataset and compare results
  3. Evaluate MPVR on a more complex dataset (e.g., EuroSAT) and analyze failure cases

## Open Questions the Paper Calls Out
None

## Limitations
- MPVR's performance depends heavily on the LLM's visual knowledge representation, which may be limited for specialized domains
- Requires access to LLM APIs, introducing potential costs and latency that could limit scalability
- Relies on extracting task metadata from public sources, which may be incomplete or inaccurate for some datasets

## Confidence
- **High confidence**: The core mechanism of using meta-prompting to generate task-specific query templates is well-supported by consistent improvements across 20 diverse datasets
- **Medium confidence**: The claim of generalization across different LLMs and VLMs is supported but limited to GPT-4 and Mixtral for LLMs, and CLIP variants for VLMs
- **Medium confidence**: The assertion that semantically rich text descriptions outperform hand-crafted templates is well-supported, but the paper doesn't extensively explore the space of possible template designs

## Next Checks
1. Test MPVR on specialized domains (e.g., medical or satellite imagery) where LLMs may have limited visual knowledge to assess domain robustness
2. Compare MPVR against advanced training-based approaches on benchmark datasets to establish the practical value of the zero-shot approach
3. Analyze the sensitivity of MPVR performance to different task metadata extraction methods and completeness to understand robustness to imperfect task descriptions