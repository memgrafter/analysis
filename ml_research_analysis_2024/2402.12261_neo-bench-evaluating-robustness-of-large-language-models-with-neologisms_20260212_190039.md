---
ver: rpa2
title: 'NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms'
arxiv_id: '2402.12261'
source_url: https://arxiv.org/abs/2402.12261
tags:
- neologisms
- neologism
- words
- translation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NEO-BENCH, a benchmark designed to evaluate
  large language models' ability to handle neologisms (newly emerging words). The
  authors collected 2,505 neologisms from 2020-2023 using diverse methods including
  Google Trends analysis, news articles, and existing slang datasets.
---

# NEO-BENCH: Evaluating Robustism of Large Language Models with Neologisms

## Quick Facts
- arXiv ID: 2402.12261
- Source URL: https://arxiv.org/abs/2402.12261
- Authors: Jonathan Zheng; Alan Ritter; Wei Xu
- Reference count: 40
- Key outcome: Models with later knowledge cutoff dates show 54.7% higher accuracy on neologism tasks compared to older models

## Executive Summary
This paper introduces NEO-BENCH, a benchmark designed to evaluate large language models' ability to handle neologisms (newly emerging words). The authors collected 2,505 neologisms from 2020-2023 using diverse methods including Google Trends analysis, news articles, and existing slang datasets. They created four tasks to evaluate LLMs: Machine Translation, Cloze Question Answering, Definition Generation, and Perplexity. The key finding is that models with later knowledge cutoff dates perform better on neologism tasks, with newer models showing an average 54.7% improvement in accuracy compared to older models. Machine Translation quality drops by 43% when neologisms are present, and automatic evaluation metrics poorly correlate with human judgment for neologism translation quality. The benchmark reveals that neologisms affect models differently based on their linguistic structure, with lexical neologisms producing the highest perplexities but best downstream performance.

## Method Summary
The NEO-BENCH evaluation involves collecting neologisms from 2020-2023 using Google Trends analysis, news articles with 16 headline templates, and existing slang datasets. The collected neologisms are then used to construct benchmark tasks including 240 sentences for Machine Translation evaluation, 422 Cloze passages for Question Answering, 750 definition generation questions, and perplexity calculations. Models are evaluated on these tasks using automatic metrics (BLEU, COMET, perplexity) and human evaluation for translation quality. The evaluation framework compares models with different knowledge cutoff dates and analyzes performance across different linguistic types of neologisms (lexical, morphological, semantic).

## Key Results
- Models with later knowledge cutoff dates yield 54.7% higher accuracy in downstream tasks compared to older models
- Machine Translation quality drops by 43% when neologisms are present in test sentences
- Automatic evaluation metrics show poor correlation with human judgment for neologism translation quality (Spearman's ρ = 0.491 for COMETKiwi)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neologism performance degrades because models lack exposure during training
- Mechanism: Static LLMs trained on fixed datasets cannot adapt to words that emerge after their knowledge cutoff dates, causing higher perplexity and lower downstream accuracy
- Core assumption: Temporal drift between training data and test data creates a distribution shift that static models cannot handle
- Evidence anchors:
  - [abstract] "The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference"
  - [section 4] "Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks"
  - [corpus] "Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks" suggests temporal robustness is an active research area

### Mechanism 2
- Claim: Linguistic structure affects model performance on neologisms differently
- Mechanism: Lexical neologisms without derivations produce fragmented subword tokenization leading to highest perplexities but better downstream performance, while semantic neologisms use existing word forms resulting in lower perplexities but poor generation performance
- Core assumption: Tokenization quality directly impacts model's ability to understand and generate text with neologisms
- Evidence anchors:
  - [section 5] "lexical neologisms without derivations yield the highest perplexities and the most fragmented subword tokenization"
  - [section 5] "semantic neologisms that repurpose existing words result in literal definitions and translations"
  - [corpus] "NeoN: A Tool for Automated Detection, Linguistic and LLM-Driven Analysis of Neologisms in Polish" indicates linguistic analysis is relevant to neologism handling

### Mechanism 3
- Claim: Automatic evaluation metrics poorly correlate with human judgment for neologism translation quality
- Mechanism: Standard metrics like BLEU, COMET, and MetricX focus on token overlap and structural similarity rather than semantic equivalence, making them ineffective for evaluating neologism translation where paraphrasing is often required
- Core assumption: Neologism translation often requires creative paraphrasing rather than literal translation, which automatic metrics cannot capture
- Evidence anchors:
  - [section 4] "automatic metrics do not accurately measure the quality of translated sentences containing neologisms, evidenced by Spearman's ρ rank correlation between COMETKiwi and human judgment, which is 0.491"
  - [section 4] "translating neologisms often requires paraphrasing, resulting in low ρ for BLEU"
  - [corpus] "Can AI mimic the human ability to define neologisms?" suggests evaluating AI understanding of neologisms is challenging

## Foundational Learning

- Concept: Temporal data drift in machine learning
  - Why needed here: Understanding how model performance degrades when test data distribution shifts from training data is fundamental to interpreting NEO-BENCH results
  - Quick check question: If a model trained on 2020 data is evaluated on 2023 data, what performance degradation would you expect to see?

- Concept: Subword tokenization and its impact on model performance
  - Why needed here: NEO-BENCH shows that different linguistic types of neologisms produce different tokenization patterns, affecting model perplexity and downstream task performance
  - Quick check question: Why would a novel compound word like "doomscrolling" be tokenized differently than an existing word used with a new meaning like "ice" (for EVs)?

- Concept: Evaluation metrics for machine translation
  - Why needed here: Understanding the limitations of BLEU, COMET, and other automatic metrics is crucial for interpreting why NEO-BENCH finds poor correlation with human judgment for neologism translation
  - Quick check question: Why would BLEU score be particularly low for neologism translation compared to regular translation?

## Architecture Onboarding

- Component map: Neologism collection pipeline (Reddit, news articles, existing datasets) -> Benchmark task generation (machine translation, cloze QA, definition generation, perplexity ranking) -> Model evaluation (automatic metrics + human evaluation)
- Critical path: Neologism collection → Task construction → Model evaluation → Analysis of linguistic type effects → Correlation analysis between perplexity and downstream performance
- Design tradeoffs: Manual annotation for ground truth vs. automatic metric evaluation; breadth of neologism types vs. depth of analysis per type; computational cost of evaluating 5,002 sequences per passage vs. comprehensive perplexity ranking
- Failure signatures: High perplexity but good downstream performance (indicating tokenization issues), low perplexity but poor generation performance (indicating semantic understanding gaps), poor correlation between automatic metrics and human judgment (indicating evaluation methodology issues)
- First 3 experiments:
  1. Replicate the machine translation degradation study by creating minimal pairs of sentences with and without neologisms and evaluating commercial MT systems
  2. Test perplexity ranking of neologisms vs. common words using a base LLaMA model to verify the 226-point difference reported
  3. Conduct human evaluation of neologism translations to establish ground truth correlation with automatic metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between subword tokenization patterns and neologism perplexity across different linguistic types (lexical, morphological, semantic)?
- Basis in paper: [explicit] The paper shows lexical neologisms have more fragmented tokenizations (average 2.36 characters per token) compared to morphological (2.98) and semantic (3.24) neologisms, and lexical neologisms produce the highest perplexities but best downstream performance.
- Why unresolved: The paper demonstrates correlation but doesn't establish causation or explore whether specific tokenization patterns directly cause perplexity differences, or if other factors (like semantic familiarity) are driving the relationship.
- What evidence would resolve it: Controlled experiments varying tokenization schemes for the same neologisms while measuring perplexity and downstream task performance would clarify whether tokenization patterns are the primary driver of the observed effects.

### Open Question 2
- Question: How does instruction-tuning affect neologism handling differently across linguistic types, and what specific aspects of instruction-tuning cause the observed perplexity increases?
- Basis in paper: [explicit] The paper shows instruction-tuned models (Alpaca, OLMo-Instruct, Mistral-Instruct, LLaMA-2 Chat) have 125 lower neologism rankings than their pre-trained counterparts, with instruct models producing higher perplexities for neologisms.
- Why unresolved: The paper observes the effect but doesn't investigate whether it's due to dialogue-focused training data, preference for common generation, or other aspects of instruction-tuning methodology that might be less tolerant of rare or novel terms.
- What evidence would resolve it: Systematic comparison of instruction-tuned models trained on different types of instruction data (dialogue vs task-oriented vs general) while measuring neologism perplexity would identify which aspects of instruction-tuning contribute to the effect.

### Open Question 3
- Question: What is the optimal temporal filtering strategy for neologism collection that balances precision and recall across linguistic types?
- Basis in paper: [explicit] The paper describes using Google Trends with curve-fitting, argmax detection, and integrals to filter neologism candidates, achieving 0.2 precision and 0.625 estimated recall for Reddit data, but notes this varies by linguistic type and collection method.
- Why unresolved: The paper uses a single filtering method chosen based on overall performance but acknowledges different linguistic types (especially semantic neologisms) may require different temporal signals, and the current method has high precision but may miss relevant neologisms.
- What evidence would resolve it: Comparative evaluation of multiple temporal filtering strategies (different time windows, trend shapes, baseline comparisons) across all linguistic types while measuring precision/recall against human-annotated neologism sets would identify optimal parameters for each type.

## Limitations

- The correlation between perplexity and downstream performance is not consistent across all linguistic types, with lexical neologisms showing high perplexity but good downstream performance
- Automatic evaluation metrics show poor correlation with human judgment for neologism translation quality, suggesting current metrics may not adequately capture translation quality for novel terms
- The study focuses on English neologisms from 2020-2023, which may limit generalizability to other languages or time periods

## Confidence

- **High Confidence**: The finding that models with later knowledge cutoff dates perform better on neologism tasks is well-supported by systematic evaluation across multiple model families
- **Medium Confidence**: The observation that automatic evaluation metrics poorly correlate with human judgment for neologism translation quality, though the exact correlation values may vary with different evaluation protocols
- **Low Confidence**: The claim that lexical neologisms without derivations yield the highest perplexities but best downstream performance, as this requires further validation across more diverse linguistic phenomena

## Next Checks

1. **Correlation Validation**: Replicate the human evaluation of neologism translations with a larger sample size and different annotator pools to verify the weak correlation between automatic metrics and human judgment (currently ρ = 0.491)

2. **Linguistic Structure Analysis**: Test whether the observed relationship between linguistic type and performance holds for neologisms from different semantic domains (e.g., technology, culture, politics) beyond the current dataset

3. **Retrieval-Augmented Generation Test**: Evaluate whether retrieval-augmented generation approaches can close the performance gap between older and newer models on neologism tasks, particularly for lexical neologisms with fragmented tokenization