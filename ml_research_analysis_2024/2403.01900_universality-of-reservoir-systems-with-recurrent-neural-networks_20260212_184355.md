---
ver: rpa2
title: Universality of reservoir systems with recurrent neural networks
arxiv_id: '2403.01900'
source_url: https://arxiv.org/abs/2403.01900
tags:
- reservoir
- systems
- dynamical
- section
- approximation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes uniform strong universality for recurrent
  neural network (RNN) reservoir systems in approximating a class of contracting dynamical
  systems. The authors show that for any prescribed approximation error and a class
  of contracting dynamical systems characterized by the Barron class, one can construct
  an RNN reservoir system whose worst-case approximation error is uniformly bounded
  by that error across all systems in the class.
---

# Universality of reservoir systems with recurrent neural networks

## Quick Facts
- arXiv ID: 2403.01900
- Source URL: https://arxiv.org/abs/2403.01900
- Reference count: 20
- One-line primary result: RNN reservoir systems achieve uniform strong universality for approximating contracting dynamical systems in the Barron class

## Executive Summary
This paper establishes that recurrent neural network (RNN) reservoir systems can universally approximate a class of contracting dynamical systems with approximation errors that are uniformly bounded across the entire class. The authors prove that for any prescribed approximation error and class of contracting systems characterized by the Barron class, one can construct an RNN reservoir system whose worst-case approximation error is uniformly bounded by that error across all systems in the class.

The key contribution is demonstrating uniform strong universality through parallel concatenation of RNN reservoirs based on covering techniques, combined with internal approximation methods. For finite-length inputs, the system uses parallel concatenation with covering arguments, while for left-infinite inputs, a cascaded structure ensures the echo state property while maintaining approximation capability. The results are theoretical existence proofs rather than practical constructions, as the constructed systems are impractically large.

## Method Summary
The paper constructs RNN reservoir systems via parallel concatenation of multiple RNN reservoirs, each implementing a finite covering of the parameter space of feedforward neural networks (FNNs) that approximate the target dynamical systems. The authors first prove weak universality using internal approximation techniques, then upgrade to uniform strong universality through parallel concatenation and covering arguments. For left-infinite inputs, they cascade the concatenated reservoirs to ensure the echo state property while preserving approximation capability. The method relies on Barron class approximation bounds for FNNs and finite memory approximation for contracting systems.

## Key Results
- RNN reservoir systems achieve uniform strong universality for approximating contracting dynamical systems in the Barron class
- For finite-length inputs, parallel concatenation with covering arguments provides worst-case approximation error bounds independent of individual target systems
- For left-infinite inputs, cascaded concatenated RNN reservoirs maintain approximation capability while ensuring echo state property
- The constructed RNN reservoir systems are impractically large, serving as existence proofs rather than practical methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel concatenation of RNN reservoirs with covering arguments provides uniform strong universality for finite-length inputs.
- Mechanism: By covering the space of bounded-parameter FNNs with finite covering sets, and concatenating all covering FNNs in parallel, the system ensures that for any target dynamical system in the class, at least one component reservoir approximates it within the prescribed error. The readout then selects the best approximating component.
- Core assumption: The Barron class approximation error bounds hold uniformly across the class of target systems, and the covering FNNs are finite and computable.
- Evidence anchors:
  - [abstract]: "To show the universality, we construct an RNN reservoir system via parallel concatenation that has an upper bound of approximation error independent of each target in the class."
  - [section]: "The main idea for proving the uniform strong universality... is to upgrade the weak universality... via a combination of parallel concatenation... and covering."
  - [corpus]: Weak. Corpus contains related reservoir computing work but not explicit covering arguments or parallel concatenation for universality.
- Break condition: If the covering radius cannot be made sufficiently small relative to the approximation error, or if the covering set becomes infinite, the uniform bound fails.

### Mechanism 2
- Claim: Cascading the concatenated RNN reservoir ensures echo state property for left-infinite inputs.
- Mechanism: The cascade structure fixes the state based on the latest T inputs, guaranteeing uniqueness of the state sequence (echo state property) while preserving the approximation capability inherited from the concatenated base.
- Core assumption: Target dynamical systems in the contracting class have echo state property, and the cascade of the FNN is still an FNN with bounded parameters.
- Evidence anchors:
  - [abstract]: "For left-infinite inputs, they cascade concatenated RNN reservoirs to ensure the echo state property while maintaining approximation capability."
  - [section]: "Therefore, we propose use of a cascade structure of an RNN reservoir system for avoiding this problem."
  - [corpus]: Weak. No explicit mention of cascading for echo state property in related papers, though ESP is discussed in some.
- Break condition: If the cascade order T cannot be chosen large enough to satisfy the finite-memory approximation bound, or if the cascaded system loses the FNN structure, the method fails.

### Mechanism 3
- Claim: Finite memory approximation bounds error for contracting dynamical systems.
- Mechanism: For a uniformly state-contracting system, the state at time t depends mostly on the latest T inputs; earlier inputs have diminishing effect. This allows replacing the full left-infinite filter with a finite-length approximation whose error can be bounded by the contraction rate.
- Core assumption: The null sequence {Δc,t} goes to zero as t increases, i.e., the system is contracting.
- Evidence anchors:
  - [section]: "As shown in Figure 3, it simulates the original dynamical system with its latest length-T inputs."
  - [section]: "Note that the following bound becomes small as T increases because {Δc,t}∞t=1 is a null sequence."
  - [corpus]: Weak. No explicit discussion of finite memory approximation in the corpus.
- Break condition: If the system is not contracting (null sequence does not go to zero), the finite memory bound fails and approximation error does not diminish.

## Foundational Learning

- Concept: Barron class of functions and their approximation by feedforward neural networks.
  - Why needed here: The target dynamical systems' state maps are assumed to be in the Barron class, enabling uniform approximation by FNNs with explicit error bounds independent of the specific system.
  - Quick check question: What property of the Barron class ensures that the approximation error bound does not depend on the individual function but only on the class constant M?

- Concept: Echo state property (ESP) and its necessity for left-infinite inputs.
  - Why needed here: For left-infinite input sequences, a dynamical system must produce a unique state sequence for each input to have a well-defined filter; ESP guarantees this uniqueness.
  - Quick check question: Why does the internal approximation technique fail to guarantee ESP for the approximating RNN reservoir systems?

- Concept: Covering arguments in approximation theory.
  - Why needed here: Covering the parameter space of FNNs with finite epsilon-nets allows constructing a finite concatenated system that uniformly approximates all targets in the class.
  - Quick check question: How does the transitivity of the covering relation (Proposition 24) help in bounding the approximation error of the concatenated system?

## Architecture Onboarding

- Component map: Input sequence → Parallel bank of RNN reservoirs (each implements a covering FNN) → Concatenated state → Linear readout selecting best component → Output sequence → For left-infinite inputs: Same parallel bank → Cascade stage (order T) → Output

- Critical path:
  - Finite-length: Input → Parallel reservoirs → Readout selection → Output
  - Left-infinite: Input → Parallel reservoirs → Cascade → Readout selection → Output

- Design tradeoffs:
  - Larger T in cascade improves finite-memory approximation but increases scale exponentially
  - Smaller covering radius Γ improves approximation but requires more covering FNNs, increasing scale
  - Using ReLU vs sigmoid affects approximation bounds and smoothness

- Failure signatures:
  - Approximation error does not decrease with N → Covering radius too large or approximation bound too loose
  - Non-unique states for same input → ESP violated, cascade order insufficient
  - Readout cannot select correct component → Covering set too coarse or missing target FNN

- First 3 experiments:
  1. Implement a small parallel reservoir with 2-3 covering FNNs and test approximation on a simple contracting linear system with finite-length input
  2. Add a cascade stage to the above system and test with left-infinite constant input to verify ESP
  3. Increase the number of covering FNNs and measure worst-case approximation error against a family of target systems to confirm uniform bound

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of the concatenated RNN reservoir systems scale with respect to the input dimension E?
- Basis in paper: [explicit] Section 7.1 discusses the order of hidden nodes as O(N^(12D(D+E+2)N+3D)), but the dependence on E is not explicitly analyzed for practical implications.
- Why unresolved: The paper focuses on D and N in the complexity analysis, leaving the impact of input dimension E underexplored.
- What evidence would resolve it: Detailed experiments or theoretical analysis showing how reservoir size scales with E for practical datasets.

### Open Question 2
- Question: Can the uniform strong universality results be extended to non-contracting dynamical systems?
- Basis in paper: [inferred] Remark 5 and 6 discuss limitations of assumptions, and Section 6.5 mentions challenges with non-ESP systems, suggesting potential for extension.
- Why unresolved: The paper assumes contracting systems for ESP and boundedness, but does not explore non-contracting cases.
- What evidence would resolve it: Constructing reservoir systems for specific non-contracting classes or proving impossibility results.

### Open Question 3
- Question: What is the practical impact of the pseudo-dimension bounds on reservoir system performance?
- Basis in paper: [explicit] Section 7.4 introduces pseudo-dimension bounds (Proposition 48) and compares Hall and Hone, but does not empirically validate their significance.
- Why unresolved: The paper provides theoretical bounds but lacks experimental validation of their practical relevance.
- What evidence would resolve it: Empirical studies comparing generalization performance with pseudo-dimension predictions on real-world tasks.

## Limitations

- The constructed RNN reservoir systems are impractically large, serving as theoretical existence proofs rather than practical methods
- The paper doesn't provide complete implementation details for the covering techniques used to construct the parameter sets
- Universal constants in the approximation bounds are stated to exist but not specified

## Confidence

- Universality results for finite-length inputs: High
- Echo state property for left-infinite inputs: Medium
- Covering argument completeness: Low
- Practical applicability of results: Low

## Next Checks

1. Verify that the Barron class approximation error bounds hold uniformly across the target system class by testing with multiple contracting systems
2. Confirm that the cascade order T can be chosen to satisfy the finite-memory approximation bound for contracting systems
3. Check that the covering radius Γ scales appropriately with N (proportional to N^(-1/2)) as specified in Proposition 42