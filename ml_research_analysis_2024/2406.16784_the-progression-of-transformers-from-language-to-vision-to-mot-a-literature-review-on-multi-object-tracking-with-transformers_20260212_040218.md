---
ver: rpa2
title: 'The Progression of Transformers from Language to Vision to MOT: A Literature
  Review on Multi-Object Tracking with Transformers'
arxiv_id: '2406.16784'
source_url: https://arxiv.org/abs/2406.16784
tags:
- tracking
- object
- vision
- computer
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This literature review examines the progression of transformers
  from their origin in natural language processing to their application in computer
  vision and multi-object tracking (MOT). While transformers have revolutionized language
  tasks and achieved strong performance in image classification and object detection,
  their impact on MOT has been more limited.
---

# The Progression of Transformers from Language to Vision to MOT: A Literature Review on Multi-Object Tracking with Transformers

## Quick Facts
- **arXiv ID**: 2406.16784
- **Source URL**: https://arxiv.org/abs/2406.16784
- **Reference count**: 40
- **Primary result**: Transformer-based approaches have not yet surpassed traditional MOT methods like SORT and Siamese networks in accuracy and efficiency

## Executive Summary
This literature review traces the evolution of transformers from their NLP origins through vision applications to their adaptation for multi-object tracking (MOT). While transformers have revolutionized language tasks and achieved strong performance in image classification and object detection, their impact on MOT has been more limited. The review identifies several transformer-based MOT approaches including TransTrack, Trackformer, MOTR, and GTR, but notes that these methods still lag behind traditional approaches in terms of accuracy and efficiency. The highest-performing MOT methods continue to rely on traditional frameworks such as SORT and Siamese networks, which combine motion models with data association algorithms. The review highlights the ongoing challenge of effectively applying transformer architectures to the complex spatial-temporal reasoning required in MOT.

## Method Summary
The review synthesizes existing literature on transformer-based approaches to multi-object tracking, examining the progression from early transformer applications in NLP and computer vision to their adaptation for MOT. It analyzes various transformer MOT architectures including TransTrack, Trackformer, MOTR, and GTR, comparing their performance against traditional methods. The review evaluates these approaches across standard MOT benchmarks and metrics, identifying key limitations and open challenges in applying transformers to MOT. The methodology involves systematic literature review and performance comparison across published works in the field.

## Key Results
- Traditional MOT methods like SORT and DeepSORT continue to outperform transformer-based approaches in accuracy and efficiency metrics
- Transformer-based MOT methods show promise but have not achieved state-of-the-art performance on major benchmarks
- The computational complexity of transformer architectures, particularly quadratic self-attention, presents significant challenges for real-time MOT applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers struggle in MOT because the attention-based self-attention layers, while effective for global sequence modeling in NLP and static image tasks, are less suited to the dynamic, real-time spatial-temporal reasoning required in MOT.
- Mechanism: In MOT, objects undergo unpredictable motions, occlusions, and abrupt changes in appearance. The self-attention mechanism requires attending to all elements in a sequence, which becomes computationally expensive and less effective when the relevant context is local and time-sensitive rather than global.
- Core assumption: The key challenge in MOT is the real-time association of detections across frames, which benefits more from motion modeling and local appearance cues than from global contextual understanding.
- Evidence anchors:
  - [abstract] "Despite these advances, the highest-performing MOT methods still rely on traditional approaches such as SORT and Siamese networks rather than transformers."
  - [section] "The main issue with RNNs was that back-propagating the gradient through numerous time steps led to diminishing gradient values... making the model difficult to train."
  - [corpus] Weak evidence: Corpus neighbors focus on transformer-based MOT but lack citations or performance comparisons.
- Break condition: If a transformer-based MOT method demonstrates superior MOTA/IDF1 scores compared to SORT or Siamese-based approaches on MOT17 benchmark.

### Mechanism 2
- Claim: Traditional MOT methods like SORT and DeepSORT outperform transformers because they combine explicit motion models (Kalman filters) with simple data association (Hungarian algorithm), which is more effective for the short-term predictions needed in MOT.
- Mechanism: SORT uses a constant velocity motion model and the Hungarian algorithm for frame-to-frame association, which is computationally efficient and robust for short-term tracking. DeepSORT adds appearance features for re-identification, further improving performance without the complexity of transformers.
- Core assumption: The primary challenge in MOT is reliable short-term prediction and association, not long-range context modeling.
- Evidence anchors:
  - [section] "They purposefully ignore ReID since trying to account for it includes unnecessary complexity and precludes real time tracking."
  - [section] "ByteTrack achieved SOTA IDF1 and IDS scores when it came out."
  - [corpus] Weak evidence: Corpus neighbors focus on transformer-based MOT but lack citations or performance comparisons.
- Break condition: If a transformer-based MOT method demonstrates superior MOTA/IDF1 scores compared to SORT or Siamese-based approaches on MOT17 benchmark.

### Mechanism 3
- Claim: The computational complexity of transformers, particularly the quadratic complexity of self-attention, makes them less suitable for real-time MOT applications compared to lightweight traditional methods.
- Mechanism: Transformers require computing attention scores between all pairs of elements, leading to O(n^2) complexity. In MOT, where detections per frame can be numerous, this becomes computationally prohibitive for real-time applications.
- Core assumption: Real-time performance is critical for practical MOT applications, and computational efficiency is a key factor in method adoption.
- Evidence anchors:
  - [section] "Compared to traditional RNNs, the attention mechanism essentially avoids the vanishing gradient problem by decreasing the path length between long range dependencies."
  - [section] "In fact, it performs worse on detecting small objects and takes a very long time to train."
  - [corpus] Weak evidence: Corpus neighbors focus on transformer-based MOT but lack citations or performance comparisons.
- Break condition: If a transformer-based MOT method demonstrates comparable or better real-time performance (FPS) compared to SORT or Siamese-based approaches.

## Foundational Learning

- Concept: Sequence-to-sequence modeling
  - Why needed here: Understanding how transformers map input sequences to output sequences is fundamental to grasping their application in MOT, where object tracks are sequences of bounding boxes across frames.
  - Quick check question: What is the difference between an encoder-only, decoder-only, and encoder-decoder transformer architecture, and which is most commonly used in MOT?

- Concept: Attention mechanisms
  - Why needed here: Attention is the core mechanism that allows transformers to weigh the importance of different elements in a sequence. In MOT, understanding how attention can be used for object association is crucial.
  - Quick check question: How does the self-attention mechanism in transformers differ from the attention used in encoder-decoder models, and how might this impact their application in MOT?

- Concept: Object tracking metrics (MOTA, IDF1, HOTA)
  - Why needed here: Evaluating MOT performance requires understanding these metrics, which measure different aspects of tracking accuracy and association quality.
  - Quick check question: What is the difference between MOTA and IDF1, and why might a method with high MOTA not necessarily have high IDF1?

## Architecture Onboarding

- Component map: Backbone (CNN or transformer) -> Feature extraction -> Transformer encoder (global context) -> Transformer decoder (track queries + object queries) -> Bounding box and track ID prediction
- Critical path: Self-attention computation in encoder and cross-attention in decoder, involving multiple matrix multiplications and softmax operations
- Design tradeoffs: Representational power of transformers vs computational complexity; global context modeling vs local short-term prediction; joint detection and tracking vs separate components
- Failure signatures: Poor association of occluded objects (high IDF1), drift in track IDs over time, computational bottlenecks preventing real-time performance (low FPS)
- First 3 experiments:
  1. Implement a simple transformer-based MOT model using DETR as the backbone and compare its performance (MOTA, IDF1, FPS) to SORT on a small MOT dataset
  2. Experiment with different transformer architectures (encoder-only, decoder-only, encoder-decoder) and observe their impact on tracking accuracy and speed
  3. Implement a lightweight attention mechanism (e.g., deformable attention) and compare its performance and speed to standard self-attention in the context of MOT

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can transformer-based architectures eventually outperform traditional MOT methods like SORT and DeepSORT in terms of accuracy and efficiency?
- Basis in paper: [explicit] The review explicitly states that despite transformer progress in MOT, "the best, highest performing, object tracking methods do not use transformers" and "current SOTA does not use transformers"
- Why unresolved: While transformer-based MOT methods have been developed, they have not yet surpassed the performance of traditional methods that use Kalman filters and Hungarian algorithms. The review suggests this is an ongoing challenge requiring effective spatial-temporal reasoning.
- What evidence would resolve it: A transformer-based MOT method that achieves state-of-the-art MOTA, IDF1, and HOTA scores on major benchmarks like MOT17 and MOT20 while maintaining real-time processing speeds.

### Open Question 2
- Question: What architectural modifications or training strategies could enable transformers to better handle the spatial-temporal reasoning required in multi-object tracking?
- Basis in paper: [explicit] The review mentions several transformer MOT approaches (TransTrack, Trackformer, MOTR, GTR) but notes they still "lag behind traditional deep learning methods," suggesting current architectures are insufficient.
- Why unresolved: While various transformer-based MOT methods have been proposed, none have successfully captured the complex spatial-temporal dependencies needed for optimal tracking performance, particularly in handling occlusions and object re-identification.
- What evidence would resolve it: Development and validation of a transformer MOT architecture that achieves superior performance on occlusion-heavy datasets like DanceTrack and TAO-Open World, with demonstrated improvements in IDF1 and fragmentations metrics.

### Open Question 3
- Question: Is the superior performance of traditional MOT methods primarily due to their motion models and association algorithms rather than their detection capabilities?
- Basis in paper: [inferred] The review highlights that many top-performing methods like BoT-SORT and SMILEtrack use traditional frameworks but incorporate advanced motion models and appearance features, while transformer methods focus on joint detection and tracking.
- Why unresolved: The review suggests that current transformer MOT methods attempt to solve detection and tracking jointly, while traditional methods excel at tracking through robust motion and appearance models, but it's unclear which aspect is more critical.
- What evidence would resolve it: Comparative studies isolating the effects of detection accuracy versus tracking association algorithms, potentially through ablation studies or controlled experiments on datasets with known detection performance.

## Limitations

- Traditional MOT methods continue to outperform transformer-based approaches in accuracy and efficiency metrics
- The computational complexity of transformer architectures, particularly quadratic self-attention, limits real-time applicability
- Existing transformer MOT methods often rely on downstream association strategies rather than fully end-to-end approaches

## Confidence

- **High confidence**: Traditional MOT methods (SORT, DeepSORT) currently outperform transformer-based approaches in accuracy and efficiency metrics
- **Medium confidence**: Transformer architectures struggle with MOT due to computational complexity and the nature of MOT requiring short-term, local reasoning rather than global context modeling
- **Low confidence**: Specific claims about which transformer-based MOT methods are most promising, as the field is rapidly evolving and performance comparisons may quickly become outdated

## Next Checks

1. Implement and benchmark a current state-of-the-art transformer-based MOT method (e.g., GTR or TransTrack) against traditional methods (SORT, DeepSORT) on the MOT17 dataset using standardized metrics (MOTA, IDF1, FPS) to verify the review's claims about performance gaps.

2. Measure the actual computational requirements (FLOPs, inference time) of transformer-based MOT architectures compared to traditional methods across different hardware platforms to validate the review's concerns about computational complexity.

3. Develop and test a modified transformer architecture incorporating sparse attention mechanisms or temporal windowing to address the computational bottlenecks identified in the review, then evaluate whether such optimizations enable real-time performance without sacrificing tracking accuracy.