---
ver: rpa2
title: LLMs as Models for Analogical Reasoning
arxiv_id: '2406.13803'
source_url: https://arxiv.org/abs/2406.13803
tags:
- human
- condition
- reasoning
- performance
- subjects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models can match humans on complex analogical reasoning
  tasks requiring flexible re-representation of semantic information, as shown in
  novel tasks mapping words to abstract symbols. GPT-4, Claude 3, and Llama-405B performed
  at human level across multiple conditions, though humans and models differed in
  sensitivity to distractors and presentation order.
---

# LLMs as Models for Analogical Reasoning

## Quick Facts
- **arXiv ID**: 2406.13803
- **Source URL**: https://arxiv.org/abs/2406.13803
- **Reference count**: 17
- **Primary result**: LLMs match humans on complex analogical reasoning tasks requiring semantic re-representation

## Executive Summary
This study evaluates whether large language models can serve as models for analogical reasoning by testing their performance on novel tasks that require mapping words to abstract symbols. GPT-4, Claude 3, and Llama-405B demonstrated human-level performance across multiple experimental conditions, successfully handling complex analogical reasoning that involves flexible re-representation of semantic information. However, significant differences emerged in how humans and models process information, with models showing distinct sensitivities to distractors and presentation order that suggest fundamentally different reasoning mechanisms.

The research reveals that while LLMs can achieve comparable performance to humans on structured analogical tasks, they fail when semantic structures are misleading or when multiple properties must be integrated simultaneously. These findings challenge the assumption that high performance on analogical reasoning tasks indicates shared cognitive mechanisms, demonstrating instead that LLMs and humans employ different strategies to solve the same problems.

## Method Summary
The researchers designed novel analogical reasoning tasks where participants (both human and LLM) mapped words to abstract symbols based on semantic relationships. Multiple conditions tested performance under varying levels of complexity, including the presence of distractors and different presentation orders. The study compared three advanced LLMs (GPT-4, Claude 3, Llama-405B) against human participants across these conditions, measuring accuracy and response patterns to identify similarities and differences in reasoning approaches.

## Key Results
- LLMs matched human performance on complex analogical reasoning tasks requiring semantic re-representation
- Models and humans showed different sensitivities to distractors and presentation order effects
- LLMs struggled when semantic structure was misleading or when integrating multiple properties simultaneously

## Why This Works (Mechanism)
The study demonstrates that LLMs can perform analogical reasoning through pattern recognition and statistical generalization rather than through the same cognitive mechanisms humans use. The models appear to leverage their training data's statistical regularities to identify and apply analogical mappings, even when these require flexible re-representation of semantic relationships. However, the distinct performance patterns on distractor sensitivity and presentation order suggest that LLMs rely on different computational strategies than humans.

## Foundational Learning
- **Analogical reasoning**: The cognitive process of transferring information from a familiar situation (source) to an unfamiliar situation (target) based on structural similarities. Needed because the study's core question is whether LLMs can perform this human-like cognitive process.
- **Semantic re-representation**: The ability to flexibly transform and map semantic relationships between different domains. Quick check: Can the model successfully map semantic relationships from one domain to abstract symbols?
- **Distractor sensitivity**: How the presence of irrelevant information affects reasoning performance. Quick check: Does performance degrade predictably when irrelevant but semantically similar items are introduced?
- **Presentation order effects**: How the sequence of information presentation influences reasoning outcomes. Quick check: Does changing the order of analogy components affect accuracy differently for humans versus models?
- **Multi-property integration**: The ability to simultaneously consider and combine multiple semantic features. Quick check: Can the system handle analogies requiring simultaneous consideration of several properties?

## Architecture Onboarding

**Component Map**: Input Text -> Semantic Processing -> Pattern Matching -> Output Generation

**Critical Path**: The model processes input text to extract semantic relationships, identifies structural patterns from training data, applies analogical mappings to generate responses, with performance bottleneck potentially occurring at the pattern matching stage when faced with misleading semantic structures.

**Design Tradeoffs**: The study implicitly reveals tradeoffs between pure pattern matching (where models excel) and deeper structural understanding (where humans maintain advantages). Models trade computational efficiency and pattern recognition breadth for the nuanced semantic understanding humans demonstrate.

**Failure Signatures**: Models fail predictably when semantic structure conflicts with analogical structure, or when tasks require integrating multiple properties simultaneously. These failures manifest as systematic errors rather than random guessing, suggesting specific limitations in how models represent and combine semantic information.

**3 First Experiments**:
1. Test models on analogies with progressively more complex semantic relationships to identify performance thresholds
2. Compare model responses to human verbal protocols during task completion to map reasoning pathways
3. Systematically vary distractor types and quantities to quantify their differential impact on model versus human performance

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Findings based on a single novel task paradigm mapping words to abstract symbols, limiting generalizability
- Study relies on correlational evidence rather than mechanistic proof of different reasoning mechanisms
- Does not fully characterize the specific computational differences between model and human reasoning strategies

## Confidence
- High confidence in the finding that models and humans exhibit distinct performance patterns on specific task manipulations
- Medium confidence in human-level performance claims due to consistent but imperfect model-human correspondence
- Low confidence in mechanistic conclusions about reasoning differences given the exploratory nature of the analysis

## Next Checks
1. Test model and human performance on structurally identical but semantically distinct analogies (e.g., mathematical relationships mapped to non-mathematical symbols) to isolate whether differences stem from semantic vs. structural processing
2. Conduct ablation studies systematically removing distractor types to quantify their differential impact on model vs. human reasoning pathways
3. Apply process-tracing methods (verbal protocols for humans, attention visualization for models) during task performance to map the reasoning steps each system employs