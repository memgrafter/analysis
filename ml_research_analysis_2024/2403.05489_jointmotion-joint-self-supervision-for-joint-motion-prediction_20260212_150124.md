---
ver: rpa2
title: 'JointMotion: Joint Self-Supervision for Joint Motion Prediction'
arxiv_id: '2403.05489'
source_url: https://arxiv.org/abs/2403.05489
tags:
- motion
- prediction
- joint
- pre-training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a self-supervised pre-training method for joint
  motion prediction in self-driving vehicles. The method jointly optimizes a scene-level
  objective connecting motion and environments, and an instance-level objective to
  refine learned representations.
---

# JointMotion: Joint Self-Supervision for Joint Motion Prediction

## Quick Facts
- arXiv ID: 2403.05489
- Source URL: https://arxiv.org/abs/2403.05489
- Reference count: 40
- Primary result: Reduces joint final displacement error by 3-12% across three motion prediction models

## Executive Summary
This paper proposes JointMotion, a self-supervised pre-training method for joint motion prediction in self-driving vehicles. The method combines scene-level similarity learning between motion and environment contexts with instance-level masked polyline modeling to refine multimodal representations. An adaptive pre-training decoder enables the method to generalize across different fusion mechanisms and dataset characteristics, achieving significant improvements in joint motion prediction accuracy across multiple model architectures.

## Method Summary
JointMotion uses a dual-objective pre-training approach: (1) a scene-level objective that learns joint embeddings between past motion sequences and environment context via non-contrastive similarity learning with redundancy reduction, and (2) an instance-level objective that refines representations through masked autoencoding of multimodal polylines. The method employs an adaptive pre-training decoder that can handle both early and late fusion mechanisms, enabling transfer learning between different motion prediction architectures and datasets.

## Key Results
- Reduces Wayformer's joint final displacement error by 3%
- Reduces HPTR's joint final displacement error by 8%
- Reduces Scene Transformer's joint final displacement error by 12%
- Enables transfer learning between Waymo Open Motion and Argoverse 2 Motion Forecasting datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scene-level objective connects motion and environment by matching past motion sequences to environment context embeddings, enabling learning of interaction patterns.
- Mechanism: Past motion sequences and environment context (lanes, traffic lights) are encoded into joint embeddings via modality-specific encoders, pooled to be invariant to agent count, and similarity learned via redundancy reduction (approximating cross-correlation matrix to identity matrix).
- Core assumption: Motion sequences and environment context share an underlying joint embedding space where semantically similar pairs are close and redundant vector elements are minimized.
- Evidence anchors:
  - [abstract]: "Scene-level representations are learned via non-contrastive similarity learning of past motion sequences and environment context."
  - [section 3.1]: "Following [30], we use two separate MLPs with LayerNorm and ReLU activations as projectors... non-contrastive similarity learning via redundancy reduction."
  - [corpus]: Weak evidence - no direct corpus support for redundancy reduction in motion prediction; closest is T-JEPA using similar redundancy reduction concepts in tabular data.
- Break condition: If the redundancy reduction term fails to prevent representation collapse, embeddings could collapse to trivial solutions (e.g., zero vectors), losing discriminative power.

### Mechanism 2
- Claim: Instance-level masked polyline modeling refines learned representations by reconstructing masked elements of multi-modal polylines.
- Mechanism: Elements of polylines representing agent motion (positions, dimensions, velocity, yaw, etc.), lanes, and traffic light states are randomly masked and reconstructed from non-masked elements and context using transformer decoders.
- Core assumption: Masking and reconstructing polyline elements forces the model to learn detailed instance-level features that complement scene-level representations.
- Evidence anchors:
  - [abstract]: "At the instance level, we use masked autoencoding to refine multimodal polyline representations."
  - [section 3.2]: "Inspired by masked sequence modeling... we mask elements of polylines representing past motion sequences, lanes, and past traffic light states and learn to reconstruct them."
  - [corpus]: Weak evidence - while masked autoencoding is well-established in vision, specific application to motion prediction polyline reconstruction lacks direct corpus support.
- Break condition: If the masking ratio is too high, reconstruction becomes impossible, leading to poor learning; if too low, the objective becomes trivial and fails to refine representations.

### Mechanism 3
- Claim: The adaptive pre-training decoder enables generalization across different environment representations, fusion mechanisms, and dataset characteristics.
- Mechanism: Uses late fusion (modality-specific encoders + shared local decoder) for models like Scene Transformer, and early fusion (shared encoder + learned queries + shared decoder) for models like Wayformer, adapting to compressed latent representations.
- Core assumption: The decoder architecture can be adapted to different fusion mechanisms without losing the benefits of pre-training.
- Evidence anchors:
  - [abstract]: "We complement this with an adaptive pre-training decoder that enables JointMotion to generalize across different environment representations, fusion mechanisms, and dataset characteristics."
  - [section 3.2]: "For models with late or hierarchical fusion mechanisms... we use the modality-specific encoders... For models that employ early fusion mechanisms... we use learned queries..."
  - [corpus]: Weak evidence - no direct corpus support for adaptive pre-training decoders in motion prediction; related concepts exist in vision (e.g., MAE variants) but not directly transferable.
- Break condition: If the decoder cannot properly handle the compressed representations from early fusion, pre-training may fail to transfer effectively.

## Foundational Learning

- Concept: Non-contrastive similarity learning and redundancy reduction
  - Why needed here: Enables learning joint embeddings between motion and environment without requiring negative samples, which are difficult to define in this context
  - Quick check question: What prevents representation collapse in non-contrastive similarity learning?

- Concept: Masked autoencoding
  - Why needed here: Forces the model to learn detailed instance-level features by reconstructing masked elements of multi-modal polylines
  - Quick check question: Why reconstruct multiple features (position, dimensions, velocity, yaw, etc.) rather than just positions?

- Concept: Transformer architectures with different fusion mechanisms
  - Why needed here: Different motion prediction models use different fusion approaches (early vs late), requiring adaptable pre-training strategies
  - Quick check question: How does early fusion differ from late fusion in terms of representation learning?

## Architecture Onboarding

- Component map: Modality-specific encoders → Global average pooling → Projectors → Redundancy reduction loss; Modality-specific encoders (late fusion) or shared encoder (early fusion) → Masking → Adaptive decoder → Reconstruction loss
- Critical path: Encoder → Pooling → Projector → Loss computation (both scene-level and instance-level) → Decoder → Reconstruction
- Design tradeoffs:
  - Redundancy reduction vs contrastive learning: Avoids need for negative samples but requires careful tuning of reduction weight
  - Late fusion vs early fusion: Late fusion preserves modality-specific information but increases model size; early fusion is more efficient but loses some modality-specific detail
  - Masking ratio: 60% chosen to balance reconstruction difficulty and learning signal strength
- Failure signatures:
  - Poor scene-level learning: Embeddings show no meaningful similarity patterns between motion and environment
  - Failed instance-level learning: Reconstruction loss plateaus at high values or shows no improvement over random guessing
  - Transfer failure: Pre-trained model shows no improvement when fine-tuned on target dataset
- First 3 experiments:
  1. Verify scene-level objective: Train with only CME objective and check if motion-environment embeddings show meaningful similarity patterns
  2. Verify instance-level objective: Train with only MPM objective and check if reconstruction loss decreases and improves over baseline
  3. Verify adaptive decoder: Test pre-training with both late and early fusion configurations and verify successful transfer to fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the adaptive pre-training decoder generalize to fusion mechanisms beyond early and late fusion (e.g., cross-attention-based fusion)?
- Basis in paper: [explicit] The paper discusses generalization across early and late fusion mechanisms, but doesn't explore cross-attention-based fusion
- Why unresolved: The paper only evaluates two fusion types (early and late) in the experiments
- What evidence would resolve it: Testing the method on models with cross-attention-based fusion and comparing performance to early/late fusion variants

### Open Question 2
- Question: What is the optimal masking ratio for masked polyline modeling across different dataset characteristics and motion prediction architectures?
- Basis in paper: [explicit] The paper uses a fixed 60% masking ratio but doesn't explore optimal values
- Why unresolved: The 60% ratio was chosen without systematic exploration of the parameter space
- What evidence would resolve it: Ablation studies varying masking ratios (e.g., 40%, 60%, 80%) across multiple datasets and model architectures

### Open Question 3
- Question: How does the performance of JointMotion scale with increasing number of agents in complex traffic scenarios?
- Basis in paper: [inferred] The paper evaluates on standard datasets but doesn't analyze performance degradation with scenario complexity
- Why unresolved: The experiments use fixed validation splits without varying agent density or interaction complexity
- What evidence would resolve it: Testing on synthetic scenarios with controlled agent density and measuring performance degradation as interaction complexity increases

## Limitations

- The paper lacks explicit details on hyperparameter tuning, particularly for the redundancy reduction weight (λred = 0.005) and masking ratio (60%)
- The adaptive decoder mechanism is described conceptually but without implementation specifics for different fusion architectures
- The corpus search revealed minimal direct precedent for masked polyline modeling in motion prediction, suggesting this is a novel but potentially fragile approach

## Confidence

- **High**: The overall framework combining scene-level similarity learning with instance-level masked autoencoding is internally consistent and addresses a clear gap in motion prediction research
- **Medium**: The empirical results showing 3-12% reduction in joint final displacement error across three different model architectures appear robust but rely on proprietary datasets (Waymo, Argoverse 2) that limit independent verification
- **Low**: The claim about enabling transfer learning between Waymo and Argoverse 2 datasets is supported by only one experimental comparison, with no ablation studies on transfer mechanisms

## Next Checks

1. **Representation Collapse Test**: Verify that the redundancy reduction term effectively prevents trivial solutions by analyzing the learned embeddings' distribution before and after training - collapsed representations would appear as narrow clusters in embedding space
2. **Masking Sensitivity Analysis**: Systematically vary the masking ratio (40%, 60%, 80%) and measure the impact on both reconstruction accuracy and downstream motion prediction performance to determine optimal masking strategy
3. **Transfer Mechanism Dissection**: Conduct controlled experiments transferring pre-training between datasets with varying levels of domain similarity (e.g., Waymo → nuScenes) to identify which aspects of the pre-training generalize versus overfit to specific dataset characteristics