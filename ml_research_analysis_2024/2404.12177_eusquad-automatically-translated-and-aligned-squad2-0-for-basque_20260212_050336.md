---
ver: rpa2
title: 'EuSQuAD: Automatically Translated and Aligned SQuAD2.0 for Basque'
arxiv_id: '2404.12177'
source_url: https://arxiv.org/abs/2404.12177
tags:
- answer
- eusquad
- basque
- question
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EuSQuAD, the first automatically translated
  and aligned version of SQuAD2.0 for Basque. It contains over 142k question-answer
  pairs generated by translating and aligning SQuAD2.0 to Basque, using neural language
  models for alignment.
---

# EuSQuAD: Automatically Translated and Aligned SQuAD2.0 for Basque

## Quick Facts
- arXiv ID: 2404.12177
- Source URL: https://arxiv.org/abs/2404.12177
- Authors: Aitor GarcÃ­a-Pablos; Naiara Perez; Montse Cuadros; Jaione Bengoetxea
- Reference count: 24
- Primary result: EuSQuAD contains over 142k question-answer pairs for Basque, created by translating and aligning SQuAD2.0 using neural language models, with character-based models (CANINE-s) outperforming token-based ones (BERTeus) on QA tasks.

## Executive Summary
This paper introduces EuSQuAD, the first automatically translated and aligned version of SQuAD2.0 for Basque, a low-resource language. The authors created over 142k question-answer pairs by translating SQuAD2.0 contexts and questions to Basque using the Itzuli neural MT system, then aligning translated answers to context spans using semantic text similarity models (BERTeus and CANINE-s). The value of EuSQuAD is demonstrated through qualitative analysis and QA experiments, where models trained on EuSQuAD outperform those trained on SQuAD2.0, with character-based CANINE-s models achieving better results than token-based BERTeus. This work provides a large-scale Basque QA dataset and insights into alignment methods for synthetic datasets.

## Method Summary
The authors created EuSQuAD by translating SQuAD2.0 contexts and questions from English to Basque using the Itzuli neural MT system. They then used BERTeus and CANINE-s language models to compute semantic similarity between translated answers and context spans, extracting aligned answer spans for each question. The resulting dataset contains over 142k question-answer pairs. The authors evaluated the alignment quality through manual error analysis and demonstrated the utility of EuSQuAD by training QA models (mBERT, IXAmBERT, BERTeus) on the aligned data and testing them on a human-annotated Basque test set of 490 questions, measuring F1 and Exact Match scores.

## Key Results
- EuSQuAD contains over 142k question-answer pairs for Basque, created by translating and aligning SQuAD2.0 using neural language models.
- Models trained on EuSQuAD outperform those trained on SQuAD2.0 when evaluated on a human-annotated Basque test set of 490 questions.
- Character-based language models (CANINE-s) achieve better alignment and QA performance than token-based models (BERTeus) for Basque, an agglutinative language.

## Why This Works (Mechanism)
The authors hypothesize that character-based language models like CANINE-s are better suited for alignment tasks in agglutinative languages like Basque because they can capture morphological variations and longer character sequences more effectively than token-based models. By translating SQuAD2.0 to Basque and aligning answers using semantic similarity, they create a large-scale synthetic dataset that captures the structure and semantics of the original data while adapting it to the target language. This approach leverages the abundance of English QA data and the power of neural MT to bootstrap QA resources for low-resource languages.

## Foundational Learning
- **Neural Machine Translation**: Used to translate SQuAD2.0 from English to Basque; needed for creating parallel text data for alignment.
- **Semantic Text Similarity**: Measures the similarity between translated answers and context spans to identify aligned answer spans; critical for accurate alignment.
- **Agglutinative Languages**: Basque morphology requires capturing longer character sequences; character-based models may better handle this.
- **Synthetic Dataset Alignment**: Process of mapping translated text to original structure; essential for creating usable QA datasets from translations.
- **QA Model Training**: Fine-tuning language models on EuSQuAD for downstream QA tasks; demonstrates the utility of the aligned dataset.
- **Evaluation Metrics**: F1 and Exact Match scores used to measure QA model performance on the human-annotated test set.

## Architecture Onboarding

**Component Map:**
SQuAD2.0 (EN) -> Itzuli MT -> Translated SQuAD2.0 (EU) -> BERTeus/CANINE-s -> Aligned EuSQuAD -> QA Model Training -> Basque Test Set Evaluation

**Critical Path:**
1. Translate SQuAD2.0 contexts and questions to Basque using Itzuli MT
2. Compute semantic similarity between translated answers and context spans using BERTeus/CANINE-s
3. Extract aligned answer spans for each question to create EuSQuAD
4. Train QA models on EuSQuAD and evaluate on human-annotated Basque test set

**Design Tradeoffs:**
- Using all answers vs. only longest answer per question (tradeoff between coverage and ambiguity)
- Character-based vs. token-based language models for alignment (CANINE-s vs. BERTeus)
- Synthetic vs. human-annotated data for training (scale vs. quality)

**Failure Signatures:**
- High alignment error rate due to translation inaccuracies or morphological mismatches in Basque
- Lower QA performance due to domain shift between Wikipedia-based SQuAD contexts and news articles in test set
- Model overfitting to synthetic data and poor generalization to human-annotated examples

**First Experiments:**
1. Translate a small sample of SQuAD2.0 to Basque and manually inspect the translation quality
2. Compute semantic similarity between translated answers and context spans using BERTeus and CANINE-s
3. Extract aligned answer spans and compare the performance of BERTeus vs. CANINE-s on a small alignment task

## Open Questions the Paper Calls Out
- How does the performance of character-based language models like CANINE-s compare to token-based models for span alignment across different agglutinative languages beyond Basque?
- What is the impact of aligning all possible answers per question versus only the longest answer on the performance of downstream QA models?
- How does the quality of EuSQuAD compare to human-annotated datasets when used for fine-tuning generative models for instruction following tasks?

## Limitations
- The exact configuration of the Itzuli EN-EU neural MT system and its impact on alignment quality are not specified.
- The semantic text similarity model's exact parameters and training details are unclear.
- The human-annotated Basque test set is not publicly available for external validation.
- Domain mismatch between Wikipedia-based SQuAD contexts and news articles in the test set may affect generalization.

## Confidence
- **High**: Dataset creation methodology and alignment process
- **Medium**: Alignment accuracy and error analysis (based on manual inspection of a sample)
- **Medium**: Downstream QA performance improvements (limited to one test set)

## Next Checks
1. Conduct external evaluation on a different Basque QA dataset (if available) to verify generalization.
2. Perform ablation studies varying MT system parameters and alignment thresholds.
3. Compare EuSQuAD performance against other synthetic Basque QA datasets created through different alignment methods.