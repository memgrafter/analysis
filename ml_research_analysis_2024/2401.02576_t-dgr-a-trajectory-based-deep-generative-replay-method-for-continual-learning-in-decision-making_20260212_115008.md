---
ver: rpa2
title: 't-DGR: A Trajectory-Based Deep Generative Replay Method for Continual Learning
  in Decision Making'
arxiv_id: '2401.02576'
source_url: https://arxiv.org/abs/2401.02576
tags:
- learning
- task
- continual
- tasks
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in continual learning
  for decision-making tasks by proposing a non-autoregressive trajectory-based deep
  generative replay method (t-DGR). Unlike existing methods that generate trajectories
  autoregressively or individual state observations i.i.d., t-DGR generates state
  observations conditioned on the trajectory timestep.
---

# t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual Learning in Decision Making

## Quick Facts
- arXiv ID: 2401.02576
- Source URL: https://arxiv.org/abs/2401.02576
- Authors: William Yue; Bo Liu; Peter Stone
- Reference count: 14
- Primary result: State-of-the-art average success rate on Continual World benchmarks (CW10, CW20, BB10) using trajectory-based non-autoregressive generative replay

## Executive Summary
This paper introduces t-DGR, a trajectory-based deep generative replay method designed to address catastrophic forgetting in continual learning for decision-making tasks. Unlike traditional autoregressive approaches that generate trajectories step-by-step, t-DGR generates state observations conditioned on their trajectory timestep, ensuring equal sample coverage across all timesteps while avoiding compounding errors. The method demonstrates superior performance on Continual World benchmarks, achieving state-of-the-art average success rates compared to existing continual learning methods.

## Method Summary
t-DGR proposes a non-autoregressive trajectory-based deep generative replay approach that generates state observations conditioned on trajectory timesteps rather than generating them autoregressively or independently. The method operates by training a generative model that can produce complete trajectories for each task, conditioned on the task identifier and timestep within the trajectory. This conditioning ensures balanced sampling across all timesteps while avoiding the compounding errors typical of autoregressive generation. During continual learning, when a new task arrives, t-DGR generates synthetic trajectories from previous tasks to rehearse and mitigate forgetting, allowing the model to leverage past knowledge effectively even when task boundaries are blurry.

## Key Results
- Achieves state-of-the-art average success rate on Continual World benchmarks (CW10, CW20, BB10)
- Demonstrates superior performance in handling blurry task boundaries compared to existing methods
- Shows effective leverage of past knowledge through trajectory-based replay mechanism
- Outperforms Stable Replay and other methods that use diffusion models for trajectory generation

## Why This Works (Mechanism)
t-DGR works by generating trajectories in a non-autoregressive manner, where each state observation is generated independently conditioned on its timestep within the trajectory. This approach ensures that all timesteps receive equal representation in the generated data, avoiding the temporal bias that can occur with autoregressive methods. By conditioning generation on trajectory timesteps rather than generating sequentially, t-DGR avoids the compounding errors that accumulate in autoregressive approaches, where errors at earlier timesteps propagate and amplify through subsequent steps. The trajectory-based conditioning also allows the method to capture the full temporal structure of tasks, making it particularly effective when task boundaries are not clearly delineated.

## Foundational Learning
- Catastrophic forgetting: The phenomenon where neural networks lose previously learned knowledge when trained on new tasks. Critical for understanding the motivation behind replay-based continual learning methods.
- Quick check: Verify that replay buffer contains representative samples from all previous tasks to prevent forgetting.

- Autoregressive generation: A sequential generation approach where each output depends on previous outputs. Important to understand why t-DGR's non-autoregressive approach is advantageous.
- Quick check: Compare error accumulation rates between autoregressive and non-autoregressive trajectory generation.

- Trajectory conditioning: Generating data conditioned on full trajectory information rather than just current state. Essential for understanding t-DGR's approach to capturing temporal dependencies.
- Quick check: Validate that generated trajectories maintain task-specific temporal patterns.

## Architecture Onboarding

**Component Map:** Task Encoder -> Trajectory Generator -> State Decoder -> Replay Buffer -> Policy Network

**Critical Path:** Task Encoder and Trajectory Generator work together to produce synthetic trajectories, which are stored in the Replay Buffer and used to train the Policy Network, preventing catastrophic forgetting.

**Design Tradeoffs:** Non-autoregressive generation trades sequential coherence for error prevention and balanced timestep coverage. Trajectory conditioning adds complexity but captures richer temporal structure compared to independent state generation.

**Failure Signatures:** Poor performance on early tasks indicates insufficient replay coverage or quality; blurry task boundaries causing confusion suggests inadequate trajectory conditioning; compounding errors in generated trajectories point to issues with the non-autoregressive generation mechanism.

**3 First Experiments:**
1. Generate trajectories for a single task and visualize timestep distribution to verify balanced coverage
2. Compare performance of t-DGR versus autoregressive replay on a simple continual learning benchmark
3. Test the effect of trajectory conditioning by training with and without timestep information

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to Continual World benchmark suite, raising generalizability concerns to other domains
- Insufficient ablation study depth to determine whether trajectory-based conditioning or non-autoregressive generation is the primary performance driver
- Need for more rigorous validation of the "compounding errors" claim through qualitative analysis of generated trajectories

## Confidence

**High confidence in:** method's implementation and benchmark performance on Continual World tasks
**Medium confidence in:** claimed advantages of non-autoregressive generation over autoregressive approaches, attribution of performance gains to trajectory-based conditioning versus other architectural choices

## Next Checks
1. Test t-DGR on additional benchmark suites beyond Continual World to assess domain transferability
2. Conduct detailed qualitative analysis comparing generated trajectories from t-DGR versus autoregressive methods to validate the compounding error claim
3. Perform ablation studies isolating the effects of trajectory-based conditioning from non-autoregressive generation architecture