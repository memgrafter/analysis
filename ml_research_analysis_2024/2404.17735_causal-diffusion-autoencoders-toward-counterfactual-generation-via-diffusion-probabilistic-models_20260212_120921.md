---
ver: rpa2
title: 'Causal Diffusion Autoencoders: Toward Counterfactual Generation via Diffusion
  Probabilistic Models'
arxiv_id: '2404.17735'
source_url: https://arxiv.org/abs/2404.17735
tags:
- causal
- zcausal
- diffusion
- counterfactual
- causaldiffae
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CausalDiffAE, a diffusion-based causal representation
  learning framework for controllable counterfactual image generation. The core idea
  is to learn a causal latent space through a learnable stochastic encoder that maps
  images to causally related factors, while modeling stochastic variation via reverse
  diffusion.
---

# Causal Diffusion Autoencoders: Toward Counterfactual Generation via Diffusion Probabilistic Models

## Quick Facts
- arXiv ID: 2404.17735
- Source URL: https://arxiv.org/abs/2404.17735
- Reference count: 40
- Authors: Aneesh Komanduri; Chen Zhao; Feng Chen; Xintao Wu
- Key outcome: CausalDiffAE achieves DCI scores up to 0.999 on MorphoMNIST, Pendulum, and CausalCircuit datasets while generating more accurate counterfactuals than baseline methods like CausalVAE and DiffAE.

## Executive Summary
This paper introduces CausalDiffAE, a diffusion-based causal representation learning framework for controllable counterfactual image generation. The method learns a causal latent space through a learnable stochastic encoder that maps images to causally related factors, while modeling stochastic variation via reverse diffusion. The approach uses neural networks to parameterize causal mechanisms among latent factors and enforces disentanglement through a variational objective with label alignment priors. A DDIM-based procedure enables counterfactual generation through do-interventions on the learned causal variables.

## Method Summary
CausalDiffAE combines diffusion probabilistic models with causal representation learning by learning a causal latent space through a learnable stochastic encoder that maps images to causally related factors, while modeling stochastic variation via reverse diffusion. The method uses neural networks to parameterize causal mechanisms among latent factors and enforces disentanglement through a variational objective with label alignment priors. The approach is extended to handle limited label supervision by jointly training unconditional and conditional diffusion models, enabling granular control over intervention strength.

## Key Results
- Achieves DCI disentanglement scores up to 0.999 on MorphoMNIST, Pendulum, and CausalCircuit datasets
- Generates more accurate counterfactuals compared to baselines like CausalVAE and DiffAE as measured by effectiveness metrics
- Weak supervision variant reduces reliance on labeled data while maintaining strong performance
- Demonstrates controlled counterfactual generation through do-interventions on learned causal variables

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The causal latent space is learned through a learnable stochastic encoder that maps images to causally related factors, while stochastic variation is modeled via reverse diffusion.
- **Mechanism:** The encoder maps high-dimensional images to low-dimensional noise encodings, which are then transformed into causal latent factors using neural mechanisms parameterized by causal graph adjacency. This ensures the latent space is semantically meaningful and causally structured.
- **Core assumption:** The causal graph is known and the mapping from noise encodings to causal factors is one-to-one.
- **Evidence anchors:**
  - [abstract] "The core idea is to learn a causal latent space through a learnable stochastic encoder that maps images to causally related factors, while modeling stochastic variation via reverse diffusion."
  - [section] "To learn a meaningful representation, we propose to encode the input image x0 to a low-dimensional noise encoding u ∈ Rn. We then map the noise encoding to latent causal factors zcausal ∈ Rn corresponding to the abstract causal variables."
- **Break condition:** If the causal graph is unknown or misspecified, the causal encoding mechanism would fail to properly model the relations among latent factors, leading to entangled or non-causal representations.

### Mechanism 2
- **Claim:** Disentanglement of causal variables is enforced through a variational objective with a label alignment prior.
- **Mechanism:** The variational objective includes a KL divergence term that aligns the learned causal factors with auxiliary label information, ensuring that each latent factor corresponds to a distinct semantic variable.
- **Core assumption:** Auxiliary label information is available and is informative about the underlying causal factors.
- **Evidence anchors:**
  - [abstract] "To enforce the disentanglement of causal variables, we formulate a variational objective with label alignment priors."
  - [section] "To ensure the causal representation is disentangled, we incorporate label information y ∈ Rn as a prior in the variational objective to aid in learning semantic factors and for identifiability guarantees."
- **Break condition:** If label information is not available or is not informative, the alignment prior cannot effectively enforce disentanglement, resulting in entangled representations.

### Mechanism 3
- **Claim:** Counterfactual generation is enabled by performing do-interventions on learned causal variables and using a DDIM-based sampling procedure.
- **Mechanism:** The DDIM algorithm allows for deterministic sampling of counterfactuals by intervening on the causal latent factors and propagating the causal effects through the neural mechanisms, followed by reverse diffusion to generate the counterfactual image.
- **Core assumption:** The DDIM sampling procedure is deterministic and the causal mechanisms are parameterized correctly.
- **Evidence anchors:**
  - [abstract] "A DDIM-based procedure enables counterfactual generation through do-interventions on the learned causal variables."
  - [section] "We propose a DDIM-based counterfactual generation procedure subject to do-interventions."
- **Break condition:** If the DDIM sampling is not deterministic or the causal mechanisms are not parameterized correctly, the generated counterfactuals may not accurately reflect the intended interventions.

## Foundational Learning

- **Diffusion Probabilistic Models (DPMs)**
  - Why needed here: DPMs provide the generative framework for modeling the stochastic variation in the data, allowing for high-quality image generation.
  - Quick check question: What is the role of the noise scheduling parameter βt in the forward diffusion process of DPMs?

- **Structural Causal Models (SCMs)**
  - Why needed here: SCMs provide the causal framework for modeling the relations among latent factors, enabling counterfactual generation.
  - Quick check question: How does the adjacency matrix A in the SCM encode the causal graph among the latent factors?

- **Variational Autoencoders (VAEs)**
  - Why needed here: VAEs provide the variational inference framework for learning the latent space, ensuring that the learned representation is both generative and discriminative.
  - Quick check question: What is the purpose of the KL divergence term in the variational objective of VAEs?

## Architecture Onboarding

- **Component map:** Encoder -> Noise Encoding -> Causal Encoding -> Neural Mechanisms -> DDIM Decoder
- **Critical path:** 1. Encode image to noise encoding 2. Transform noise encoding to causal latent factors 3. Perform do-intervention on causal latent factors 4. Generate counterfactual image using DDIM
- **Design tradeoffs:** Using a known causal graph vs. learning the causal structure from data; Using label alignment priors vs. unsupervised disentanglement; Using DDIM for deterministic sampling vs. other sampling methods
- **Failure signatures:** Poor disentanglement (DCI scores below 0.9); Counterfactuals don't reflect causal effects; Slow sampling due to computational complexity
- **First 3 experiments:** 1. Verify that the encoder can map images to noise encodings and causal latent factors 2. Check that the variational objective enforces disentanglement by evaluating the DCI score 3. Test counterfactual generation by performing do-interventions on the causal latent factors and evaluating the effectiveness metric

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on known causal graphs for all datasets represents a significant limitation for real-world applications
- Performance claims on relatively simple datasets (MorphoMNIST, Pendulum) may not generalize to more complex real-world data
- Computational complexity of DDIM-based sampling for counterfactual generation is not discussed

## Confidence

**High Confidence**: The core methodology combining diffusion models with causal representation learning is technically sound and well-grounded in established frameworks (DPMs, SCMs, VAEs). The experimental results on synthetic datasets with known ground truth are reproducible and demonstrate clear improvements over baselines.

**Medium Confidence**: The claim that CausalDiffAE generates "more accurate counterfactuals" depends heavily on the specific evaluation metrics used. While effectiveness (MAE) is reported, other important aspects like realism, diversity, and semantic correctness are not thoroughly evaluated. The weak supervision extension's performance claims are based on limited experiments without ablation studies.

**Low Confidence**: The paper's claims about applicability to real-world scenarios are not substantiated with experiments on complex, high-dimensional datasets. The scalability analysis and computational requirements for counterfactual generation are not addressed.

## Next Checks

1. **Scalability Validation**: Test CausalDiffAE on high-resolution image datasets (e.g., CelebA-HQ) to evaluate performance degradation and computational requirements for counterfactual generation as image complexity increases.

2. **Causal Graph Sensitivity**: Conduct systematic experiments ablating the known causal graph assumption by using partially known or incorrect causal structures to quantify the impact on representation quality and counterfactual accuracy.

3. **Realism and Diversity Assessment**: Implement comprehensive perceptual studies or use established realism metrics (FID, IS) to evaluate whether high DCI scores translate to visually realistic and diverse counterfactual generations beyond simple geometric transformations.