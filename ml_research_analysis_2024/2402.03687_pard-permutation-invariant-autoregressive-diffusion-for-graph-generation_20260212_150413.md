---
ver: rpa2
title: 'Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation'
arxiv_id: '2402.03687'
source_url: https://arxiv.org/abs/2402.03687
tags:
- graph
- diffusion
- pard
- nodes
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PARD, a permutation-invariant autoregressive
  diffusion model for graph generation. It addresses the limitations of existing autoregressive
  and diffusion models by integrating them into a unified framework.
---

# Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation

## Quick Facts
- arXiv ID: 2402.03687
- Source URL: https://arxiv.org/abs/2402.03687
- Authors: Lingxiao Zhao; Xueying Ding; Leman Akoglu
- Reference count: 40
- Key outcome: Introduces PARD, a permutation-invariant autoregressive diffusion model for graph generation that achieves state-of-the-art performance on molecular and non-molecular datasets including the large MOSES dataset

## Executive Summary
PARD addresses limitations of existing autoregressive and diffusion models for graph generation by integrating them into a unified framework. It generates graphs in a block-wise, autoregressive fashion using a unique partial order for nodes and edges. Each block's conditional distribution is modeled by a shared diffusion model with an equivariant network, ensuring permutation invariance. The method introduces a higher-order graph transformer for improved efficiency and expressiveness, achieving state-of-the-art performance on molecular and non-molecular datasets without requiring extra features.

## Method Summary
PARD combines autoregressive and diffusion models for graph generation by decomposing graphs into blocks using a structural partial order algorithm. Each block is generated sequentially through a shared diffusion model with an equivariant network. The method employs a higher-order graph transformer with PPGN integration to achieve 3-WL expressiveness while improving memory efficiency. Training uses causal masking to enable parallel computation while preventing information leakage.

## Key Results
- Achieves state-of-the-art performance on molecular datasets (QM9, ZINC250K, MOSES) and non-molecular datasets (COMMUNITY-SMALL, CAVEMAN, CORA, BREAST, GRID)
- Generates valid, unique, and novel graphs without requiring extra features
- Successfully scales to large datasets like MOSES while maintaining quality
- Demonstrates superior performance across multiple evaluation metrics including validity, uniqueness, novelty, FCD, and scaffold similarity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PARD decomposes graph generation into block-wise autoregressive steps using a unique partial order
- Mechanism: The structural partial order algorithm assigns nodes to blocks based on their higher-order degrees, ensuring each block contains nodes with equivalent structural properties
- Core assumption: Nodes with equivalent higher-order degrees have equivalent structural roles in graph generation
- Evidence anchors:
  - [abstract] "contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges"
  - [section 3.2] "While finding a unique order for all nodes of a graph is NP-intermediate, we argue that finding a unique partial order, where certain nodes and edges are with the same rank, is easily achievable"
  - [corpus] Weak - no direct evidence in corpus, but related papers mention "Next-Scale Prediction" suggesting block-wise decomposition is emerging approach
- Break condition: If structural equivalence fails to capture meaningful graph properties, or if the partial order creates disconnected subgraphs

### Mechanism 2
- Claim: Discrete denoising diffusion within each block maintains permutation invariance while enabling graph transformations
- Mechanism: Forward diffusion adds noise to nodes/edges independently, breaking structural symmetries. Backward denoising progressively reconstructs the target graph structure
- Core assumption: Injecting noise breaks symmetries in structurally equivalent nodes, enabling their differentiation during denoising
- Evidence anchors:
  - [section 3.4] "Through a diffusion process that injects noise, a permutation equivariant network can progressively denoise to realize targeted graph transformations"
  - [section 3.3] "We hypothesize that a graph with lower 'energy' is hard to be transformed to a graph with higher 'energy' with equivariant networks"
  - [corpus] Weak - corpus doesn't directly address noise breaking symmetries in graph generation
- Break condition: If noise injection fails to break symmetries, or if denoising process cannot recover meaningful structural information

### Mechanism 3
- Claim: Higher-order graph transformer with PPGN integration achieves 3-WL expressiveness with improved memory efficiency
- Mechanism: Combines transformer's O(n²) efficiency with PPGN's edge-centric representation and 3-WL expressiveness. Uses reduced edge representation size while allocating larger hidden sizes to nodes
- Core assumption: Edge representations can be compressed without losing essential structural information for 3-WL expressivity
- Evidence anchors:
  - [section 4.1] "To enhance the memory efficiency of PPGN while maintaining the expressiveness equivalent to the 3-Weisfeiler-Lehman (3-WL) test, we introduce a hybrid approach that integrates Graph Transformers with PPGN"
  - [section 4] "PPGN [30] is still a natural choice that models edge (2-tuple) representations directly with 3-WL expressivity and O(n³) complexity in graph size"
  - [corpus] Weak - corpus doesn't provide evidence about memory efficiency improvements
- Break condition: If edge representation compression loses critical information, or if 3-WL expressivity cannot be maintained

## Foundational Learning

- Concept: Graph automorphism and structural equivalence
  - Why needed here: Understanding when nodes/edges are structurally equivalent is crucial for defining the partial order and understanding why pure equivariant networks fail
  - Quick check question: Can you explain why two nodes in a complete graph are structurally equivalent and what implications this has for graph generation?

- Concept: Denoising diffusion probabilistic models
  - Why needed here: The core mechanism relies on discrete denoising diffusion, requiring understanding of forward noise injection and backward denoising processes
  - Quick check question: What is the key difference between continuous and discrete denoising diffusion, and why is discrete diffusion more appropriate for graph generation?

- Concept: Graph neural network expressiveness (WL test)
  - Why needed here: Understanding why PPGN achieves 3-WL expressiveness and how transformer integration maintains this while improving efficiency
  - Quick check question: What does it mean for a GNN to achieve 3-WL expressiveness, and why is this important for graph generation tasks?

## Architecture Onboarding

- Component map: Graph → Partial order (Algo 1) → Block sequence → Size prediction → Diffusion (Algo 3) → Assembly
- Critical path: Graph → Partial order (Algo 1) → Block sequence → Size prediction → Diffusion (Algo 3) → Assembly
- Design tradeoffs:
  - Memory vs expressiveness: PPGN integration trades some memory efficiency for higher expressiveness
  - Training speed vs parallelism: Causal masking enables parallel training but may reduce expressivity
  - Diffusion steps vs quality: More steps improve quality but increase computation time
- Failure signatures:
  - Low validity scores: Indicates diffusion model failing to denoise properly
  - Poor uniqueness: Suggests block size prediction issues
  - Low atom/molecule stability: Points to structural generation problems
  - Training instability: May indicate issues with causal masking implementation
- First 3 experiments:
  1. Test partial order decomposition on small graphs with known symmetries
  2. Validate diffusion model on synthetic graphs with controlled structures
  3. Benchmark memory usage and quality trade-offs with different PPGN/transformer ratios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical characterization of the conditions for successful graph transformation when using equivariant networks?
- Basis in paper: [explicit] The paper hypothesizes that a graph with lower "energy" is hard to be transformed to a graph with higher "energy" with equivariant networks, and mentions that the theoretical characterization of these conditions is a valuable direction for future work
- Why unresolved: The paper does not provide a formal theory or proof for the conditions under which graph transformation using equivariant networks is possible or successful
- What evidence would resolve it: A formal proof or theorem that characterizes the necessary and sufficient conditions for successful graph transformation using equivariant networks, including the role of graph energy and symmetry

### Open Question 2
- Question: How does the causal matrix-matrix product in the PPGN-Transformer block affect the expressiveness of the model compared to the standard PPGN?
- Basis in paper: [explicit] The paper discusses the design of the causal PPGN-Transformer block and mentions that while it makes training more efficient with parallel support, it is likely that its expressiveness in graph distinguishing ability is reduced compared to the normal PPGN
- Why unresolved: The paper does not provide empirical or theoretical evidence to quantify the impact of the causal matrix-matrix product on the expressiveness of the PPGN-Transformer block
- What evidence would resolve it: Empirical results comparing the performance of the PPGN-Transformer block with the standard PPGN on tasks that require high expressiveness, such as graph classification or graph generation on complex datasets

### Open Question 3
- Question: How does the number of diffusion steps per block in PARD impact the quality of the generated graphs?
- Basis in paper: [explicit] The paper mentions that too few diffusion steps can hurt performance, while too many steps do not further improve performance, and provides an ablation study on QM9 with different numbers of diffusion steps per block
- Why unresolved: The paper does not provide a clear understanding of the optimal number of diffusion steps per block for different types of graphs or datasets
- What evidence would resolve it: A comprehensive study that systematically varies the number of diffusion steps per block across different graph datasets and evaluates the quality of the generated graphs using various metrics

## Limitations

- The paper lacks ablation studies on the structural partial order algorithm, making it difficult to assess whether the claimed advantages are truly attributable to the proposed method
- No direct evidence is provided showing how noise injection specifically breaks symmetries in structurally equivalent nodes during the denoising process
- The memory efficiency claims for the PPGN-transformer integration are asserted but not empirically validated through memory usage measurements

## Confidence

- High Confidence: Overall framework architecture and experimental results on standard benchmarks demonstrating state-of-the-art performance
- Medium Confidence: Block-wise autoregressive approach and its ability to capture complex graph dependencies through partial order decomposition
- Low Confidence: Specific mechanisms by which noise injection breaks symmetries in structurally equivalent nodes, and exact relationship between PPGN integration and 3-WL expressiveness maintenance

## Next Checks

1. **Symmetry Breaking Validation**: Design a controlled experiment with graphs containing multiple structurally equivalent nodes (e.g., complete graphs, regular graphs) to empirically demonstrate how the diffusion process differentiates between these nodes during denoising

2. **Memory Efficiency Benchmarking**: Implement systematic measurements comparing memory usage across different PPGN-transformer integration ratios, specifically tracking memory consumption at different graph sizes and diffusion step counts

3. **Partial Order Ablation**: Create ablation experiments comparing the proposed structural partial order algorithm against random orderings and existing graph ordering methods, measuring the impact on generation quality and validity across multiple dataset types