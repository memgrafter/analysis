---
ver: rpa2
title: 'TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation
  Models'
arxiv_id: '2410.23266'
source_url: https://arxiv.org/abs/2410.23266
tags:
- frames
- answer
- provided
- back
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study reveals that existing benchmarks likely overestimate
  Multimodal Foundation Models'' (MFMs) visual temporal reasoning capabilities, as
  many questions can be solved using single, few, or out-of-order frames. To address
  this, we introduce TOMATO, a novel benchmark designed with three principles: Multi-Frame
  Gain, Frame Order Sensitivity, and Frame Information Disparity.'
---

# TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models

## Quick Facts
- arXiv ID: 2410.23266
- Source URL: https://arxiv.org/abs/2410.23266
- Reference count: 40
- Existing benchmarks overestimate MFM visual temporal reasoning capabilities

## Executive Summary
This study reveals that existing benchmarks likely overestimate Multimodal Foundation Models' (MFMs) visual temporal reasoning capabilities, as many questions can be solved using single, few, or out-of-order frames. To address this, we introduce TOMATO, a novel benchmark designed with three principles: Multi-Frame Gain, Frame Order Sensitivity, and Frame Information Disparity. TOMATO comprises 1,484 questions across six temporal reasoning tasks applied to 1,417 videos, including 805 self-recorded and -generated videos. Comprehensive evaluation of 31 MFMs reveals a 57.3% human-model performance gap, with the best open-source model (Qwen2-VL-72B) achieving 37.9% accuracy. Our analysis uncovers deeper limitations in MFMs, showing they fail to interpret frames as continuous sequences, rely on common sense over visual input, and are susceptible to noisy information.

## Method Summary
TOMATO introduces three benchmarking principles: Multi-Frame Gain (κ) measures performance improvement when using multiple frames versus single frames, Frame Order Sensitivity (τ) evaluates whether temporal dependencies are required by comparing ordered vs. shuffled frames, and Frame Information Disparity (ρ) assesses whether information is evenly distributed across frames. The benchmark contains 1,484 questions across six temporal reasoning tasks (action count, direction, rotation, shape & trend, velocity & frequency, visual cues) applied to 1,417 videos. Zero-shot evaluation was conducted on 31 models (21 open-source, 10 proprietary) using 16 uniformly sampled frames per video.

## Key Results
- 57.3% human-model performance gap with best open-source model (Qwen2-VL-72B) achieving 37.9% accuracy
- TOMATO's Multi-Frame Gain κ of 66.3% far exceeds existing benchmarks (below 5%)
- TOMATO's Frame Order Sensitivity τ of 34.1% significantly higher than existing benchmarks (below 8%)
- TOMATO's Frame Information Disparity ρ of 4.6% much lower than existing benchmarks (over 27%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-frame reasoning can solve many video understanding tasks because question design often allows isolated frames to contain sufficient information.
- Mechanism: Questions in existing benchmarks are constructed such that key visual information appears in one or few frames, making temporal reasoning unnecessary.
- Core assumption: Benchmark creators unintentionally design questions where critical information is localized to specific frames rather than distributed across the video sequence.
- Evidence anchors:
  - [abstract]: "many questions can be solved by using a single, few, or out-of-order frames"
  - [section]: Table 3 shows Multi-Frame Gain κ values below 5% for existing benchmarks vs 66.3% for TOMATO
  - [corpus]: No direct corpus evidence found; this is inferred from benchmark analysis

### Mechanism 2
- Claim: Frame order sensitivity is low in existing benchmarks because temporal dependencies are not essential to answer questions correctly.
- Mechanism: Questions can be answered using frames in any order since the information required is static rather than sequential.
- Core assumption: Benchmark tasks are designed with static visual elements that don't require understanding temporal progression.
- Evidence anchors:
  - [abstract]: "many questions can be solved by using a single, few, or out-of-order frames"
  - [section]: Table 5 shows Frame Order Sensitivity τ values below 8% for existing benchmarks vs 34.1% for TOMATO
  - [corpus]: No direct corpus evidence found; this is inferred from benchmark analysis

### Mechanism 3
- Claim: Information is unevenly distributed across frames in existing benchmarks, allowing models to rely on key frames rather than comprehensive video understanding.
- Mechanism: Some frames contain disproportionately more information than others, making it possible to answer questions by focusing on these informative frames.
- Core assumption: Benchmark questions are designed such that some frames are significantly more informative than others.
- Evidence anchors:
  - [abstract]: "allow for more unevenly distributed information across the frames"
  - [section]: Table 6 shows Frame Information Disparity ρ values over 27% for existing benchmarks vs 4.6% for TOMATO
  - [corpus]: No direct corpus evidence found; this is inferred from benchmark analysis

## Foundational Learning

- Concept: Temporal reasoning in video understanding
  - Why needed here: TOMATO specifically targets models' ability to reason about visual information across time, which is fundamental to understanding video sequences
  - Quick check question: Can you explain the difference between recognizing objects in individual frames versus understanding their relationships across frames?

- Concept: Multi-frame analysis and temporal dependencies
  - Why needed here: The benchmark requires understanding how information changes and relates across multiple frames, which is essential for visual temporal reasoning
  - Quick check question: How would you determine if a question requires understanding the sequence of events versus just identifying static elements?

- Concept: Information distribution across video frames
  - Why needed here: Understanding whether information is evenly distributed or concentrated in specific frames is crucial for evaluating temporal reasoning capabilities
  - Quick check question: What metrics would you use to determine if information is evenly distributed across frames?

## Architecture Onboarding

- Component map:
  - Video preprocessing pipeline (frame extraction, sampling) -> Multimodal foundation model integration -> Temporal reasoning evaluation framework -> Benchmark question-answer pair management -> Human evaluation interface

- Data flow:
  - Raw video files → Frame extraction (16 uniformly sampled frames) → Multimodal model input → Question-answer generation → Evaluation metrics calculation → Results aggregation

## Open Questions the Paper Calls Out
- How can we design more effective visual temporal reasoning tasks that truly require understanding frame sequences?
- What architectural improvements are needed for MFMs to better capture temporal dependencies in videos?
- Can we develop training strategies that specifically target the weaknesses identified by TOMATO?
- How do different frame sampling strategies affect temporal reasoning performance?

## Limitations
- The benchmark may not cover all possible types of visual temporal reasoning tasks
- Self-recorded and generated videos may have different characteristics than real-world videos
- Limited evaluation to 31 models may not represent the full spectrum of MFMs
- Human evaluation was not conducted for all questions due to scale

## Confidence
High confidence in the core findings regarding limitations of existing benchmarks and the effectiveness of TOMATO's three principles. The significant performance gap between humans and models (57.3%) and the superior metrics of TOMATO compared to existing benchmarks provide strong evidence for the claims.

## Next Checks
- Verify the reproducibility of TOMATO's results on additional models
- Conduct ablation studies on the three principles to isolate their individual contributions
- Explore alternative frame sampling strategies and their impact on temporal reasoning
- Investigate whether fine-tuning on TOMATO improves model performance on other temporal reasoning tasks