---
ver: rpa2
title: 'VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning'
arxiv_id: '2403.13164'
source_url: https://arxiv.org/abs/2403.13164
tags:
- shot
- accuracy
- llav
- table
- induction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VL-ICL Bench demonstrates that popular multimodal ICL evaluation
  methods like VQA and image captioning are poor benchmarks, as they do not show significant
  performance improvement with additional examples and mainly learn answer formatting.
  The paper introduces a comprehensive benchmark suite with 10 tasks that test diverse
  capabilities including perception, reasoning, rule induction, long context, and
  both text-to-image and image-to-text generation.
---

# VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning

## Quick Facts
- arXiv ID: 2403.13164
- Source URL: https://arxiv.org/abs/2403.13164
- Reference count: 40
- Primary result: Popular multimodal ICL evaluation methods like VQA and image captioning are poor benchmarks, as they do not show significant performance improvement with additional examples and mainly learn answer formatting

## Executive Summary
VL-ICL Bench introduces a comprehensive benchmark suite with 10 tasks designed to properly evaluate multimodal in-context learning (ICL) capabilities. The paper demonstrates that commonly used evaluation methods like VQA and image captioning are inadequate benchmarks because they don't show meaningful performance improvement with additional examples, suggesting models primarily learn formatting rather than reasoning. The benchmark tests diverse capabilities including perception, reasoning, rule induction, long context handling, and both text-to-image and image-to-text generation. Evaluations across state-of-the-art models reveal that while some exhibit non-trivial ICL abilities, performance often plateaus or decreases with more examples due to context length limitations.

## Method Summary
The VL-ICL Bench evaluates vision-language models using a few-shot ICL protocol where support sets are sampled from training splits and test examples from testing splits. The benchmark consists of 10 carefully designed tasks that test various capabilities including fast binding, counting, operator induction, and interleaved reasoning. Performance is measured using accuracy across different shot settings (number of support examples), with summary metrics including zero-shot accuracy, peak accuracy over all shots, and ICL efficiency (area under the accuracy vs shots curve normalized above zero-shot baseline). The evaluation uses greedy decoding and is averaged over three random seeds.

## Key Results
- Popular multimodal ICL evaluation methods like VQA and image captioning are poor benchmarks as they show minimal performance improvement with additional examples
- LLaVA-OneVision-72B demonstrates the best overall image-to-text ICL performance, closely followed by GPT-4V
- Text-to-image models show more consistent shot scaling compared to image-to-text models
- Performance often plateaus or decreases with more examples due to context length limitations and token generation constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model improves performance with few-shot examples by learning task-specific formatting and demonstration patterns, not necessarily the underlying reasoning.
- Mechanism: When given a few examples in context, the model learns to match the output format, terminology, and structure demonstrated in the examples, which aligns better with evaluation metrics (e.g., exact string match for VQA).
- Core assumption: The model already has strong baseline capabilities for the task (e.g., VQA, captioning) from pretraining, so improvement comes from alignment rather than learning new reasoning.
- Evidence anchors:
  - [abstract]: "They neither exploit the strengths of ICL, nor test its limitations" and "popular multimodal ICL evaluation methods like VQA and image captioning are poor benchmarks"
  - [section 2.2]: "VLLMs learn about each dataset's preferred answer style... this is indeed a kind of ICL, but perhaps not what one expects to be learning in VQA"
  - [corpus]: Weak - related papers focus on few-shot VQA and ICL improvements but don't directly address the formatting vs reasoning distinction
- Break condition: If evaluation uses semantic matching (LLM judge) rather than exact string match, the performance improvement from shots disappears, indicating the learning was primarily about formatting.

### Mechanism 2
- Claim: Multimodal ICL faces unique challenges compared to text-only ICL due to increased token count and interleaving of multiple images.
- Mechanism: Each image in multimodal tasks translates to many tokens (e.g., 576 tokens per image in LLaVA), and interleaving multiple images increases context length beyond what the model was trained on, causing performance degradation with more shots.
- Core assumption: Models have fixed context windows and were trained on limited numbers of images per example.
- Evidence anchors:
  - [abstract]: "performance often plateaus or decreases with more examples due to context length limitations"
  - [section 4.2]: "some models obtain negative impact from more shots... We attribute this to difficulty of dealing with the larger number of images and tokens"
  - [section 4.3]: "one image translates to 576 tokens, causing an 8-shot setting to exceed a 4k context window"
  - [corpus]: Weak - related papers discuss ICL but don't specifically address context length challenges in multimodal settings
- Break condition: When context length is extended (e.g., using SelfExtend), performance may improve but doesn't always solve the issue, suggesting context length is necessary but not sufficient.

### Mechanism 3
- Claim: Model size affects emergent ICL capabilities in multimodal settings, with larger models showing better shot scaling.
- Mechanism: Larger models (e.g., 72B vs 7B parameters) can better handle the complexity of multimodal ICL tasks, including interleaved reasoning and longer context, showing non-trivial improvement with more shots.
- Core assumption: There's a threshold in model size where emergent ICL abilities appear, similar to findings in LLMs.
- Evidence anchors:
  - [abstract]: "performance often plateaus or decreases with more examples" but "some state-of-the-art models exhibit non-trivial in-context learning"
  - [section 4.3]: "we find that the 72B model demonstrates emergent ICL abilities... the 0.5B and 7B models perform worse than chance as shots increase"
  - [corpus]: Weak - related papers don't specifically discuss model size thresholds for multimodal ICL
- Break condition: Smaller models show degraded performance with more shots, performing worse than chance, while larger models continue to improve.

## Foundational Learning

- Concept: Tokenization and context length in multimodal models
  - Why needed here: Understanding how images translate to tokens and context limitations is crucial for interpreting why performance degrades with more shots
  - Quick check question: How many tokens does one image typically generate in LLaVA, and what happens when the total context length exceeds the model's training context window?

- Concept: Few-shot learning vs zero-shot learning distinction
  - Why needed here: The paper distinguishes between learning from examples (ICL) versus using pretrained knowledge (zero-shot), which is central to understanding the benchmark's purpose
  - Quick check question: What's the key difference between pθ(y∗|x∗, I, S) in ICL versus pθ(y∗|x∗, I) in zero-shot settings?

- Concept: Multimodal model architectures (cross-attention vs MLP vs Perceiver)
  - Why needed here: Different architectures handle interleaved image-text inputs differently, affecting ICL performance
  - Quick check question: How do cross-attention mechanisms differ from MLP-based approaches in processing interleaved image-text inputs?

## Architecture Onboarding

- Component map: VLLM = Base LLM + Vision Encoder + Stitching Mechanism; ICL pipeline = Task Description + Support Set + Query + Prediction
- Critical path: Task → Context Preparation → Model Inference → Evaluation; bottlenecks occur at context length and token generation
- Design tradeoffs: Token efficiency vs. perception quality; context length vs. computational cost; architecture complexity vs. training stability
- Failure signatures: Performance degradation with more shots; negative ICL efficiency; worse-than-chance performance; context overflow errors
- First 3 experiments:
  1. Compare exact match vs. semantic match evaluation on VQA tasks to confirm formatting vs reasoning learning
  2. Test different context lengths (e.g., using SelfExtend) on the same model to isolate context limitations
  3. Compare model sizes (0.5B, 7B, 72B) on interleaved tasks to observe emergent behavior thresholds

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark focuses on specific task types and may not represent all real-world multimodal ICL applications
- Evaluation methodology using greedy decoding may not fully capture model capabilities compared to more sophisticated decoding strategies
- Analysis of different architectures is limited to a few models, making generalization to broader architectural patterns uncertain

## Confidence
- Formatting vs reasoning learning claim: Medium confidence - strong evidence from exact vs semantic match comparison, but alternative explanations like ceiling effects possible
- Context length limitations: High confidence - well-supported by quantitative evidence (576 tokens per image, context overflow in 8-shot settings)
- Model size thresholds: Medium confidence - clear evidence of threshold behavior, but specific size threshold may be model-dependent

## Next Checks
1. **Semantic vs Exact Match Evaluation**: Re-run VQA and captioning tasks using LLM-based semantic evaluation rather than exact string match to determine if performance improvements persist, confirming whether ICL is learning reasoning versus formatting.

2. **Context Length Manipulation**: Systematically vary context length using SelfExtend or similar techniques across different model sizes to isolate whether context limitations are the primary driver of performance degradation with more shots.

3. **Cross-Architecture Comparison**: Evaluate a broader range of architectural variants (cross-attention, MLP, Perceiver) on the same VL-ICL Bench tasks to determine which architectural patterns consistently enable better multimodal ICL performance.