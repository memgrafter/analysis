---
ver: rpa2
title: Is Your Text-to-Image Model Robust to Caption Noise?
arxiv_id: '2412.19531'
source_url: https://arxiv.org/abs/2412.19531
tags:
- caption
- image
- captions
- generation
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how hallucinations in image captions\u2014\
  generated by Vision Language Models\u2014affect the robustness of text-to-image\
  \ generation models. The authors first construct a dataset with captions from multiple\
  \ captioning models and analyze the correlation between VLM confidence scores and\
  \ hallucination patterns."
---

# Is Your Text-to-Image Model Robust to Caption Noise?
## Quick Facts
- **arXiv ID**: 2412.19531
- **Source URL**: https://arxiv.org/abs/2412.19531
- **Reference count**: 40
- **Primary result**: A simple reweighting method leveraging VLM confidence scores improves text-to-image generation robustness against caption noise.

## Executive Summary
This paper investigates how hallucinations in image captions—generated by Vision Language Models—affect the robustness of text-to-image generation models. The authors first construct a dataset with captions from multiple captioning models and analyze the correlation between VLM confidence scores and hallucination patterns. They find that VLM token confidence scores can effectively indicate hallucinated content. Based on this insight, they propose a simple reweighting method that adjusts attention map weights in the T2I model using these confidence scores to mitigate the effect of caption noise. Experiments show that this approach improves generation quality across standard metrics (CLIP-Score, FID), enhances instruction-following accuracy on their InstructBench, and improves feature representation quality in linear probing evaluations. The results highlight the importance of addressing caption noise for robust T2I training and suggest a practical method leveraging VLM confidence scores.

## Method Summary
The authors address caption noise robustness by developing a reweighting method that leverages VLM token confidence scores. First, they construct a benchmark dataset using captions from multiple VLMs, analyzing how these captions affect T2I model performance. Through systematic evaluation, they discover that VLM confidence scores strongly correlate with hallucination patterns in captions. Building on this finding, they propose a simple yet effective reweighting approach that modifies attention map weights in the T2I model based on these confidence scores. This adjustment reduces the influence of potentially hallucinated caption tokens during generation, improving overall robustness to caption noise.

## Key Results
- The proposed reweighting method improves CLIP-Score and FID metrics compared to baseline models
- Instruction-following accuracy increases significantly on the proposed InstructBench
- Linear probing evaluations show enhanced feature representation quality in the modified models

## Why This Works (Mechanism)
The method works by recognizing that VLM confidence scores can serve as reliable indicators of hallucinated content in captions. When a VLM generates a caption with low confidence on certain tokens, these tokens are more likely to be hallucinated or incorrect. By reweighting attention maps in the T2I model based on these confidence scores, the approach effectively downweights potentially erroneous information during the generation process. This selective attention mechanism allows the T2I model to focus more on reliable caption components while reducing the impact of noise, resulting in more accurate and faithful image generation.

## Foundational Learning
- **Vision-Language Models (VLMs)**: Understand how VLMs generate captions and produce confidence scores for each token. Why needed: Essential for recognizing how caption noise originates and can be quantified. Quick check: Verify that VLM confidence scores correlate with semantic correctness in sample captions.
- **Attention Mechanisms in T2I Models**: Learn how cross-attention maps function in diffusion-based T2I architectures. Why needed: Critical for understanding where and how the reweighting method modifies model behavior. Quick check: Examine attention weight distributions with and without reweighting.
- **Diffusion-based Image Generation**: Understand the denoising process and how text conditioning guides image synthesis. Why needed: Provides context for how caption quality affects the generation pipeline. Quick check: Compare generated images with clean vs. noisy captions.
- **Evaluation Metrics for T2I**: Familiarize with CLIP-Score, FID, and instruction-following benchmarks. Why needed: Necessary for interpreting experimental results and validation. Quick check: Run baseline models through standard evaluation pipelines.
- **Hallucination Detection in VLMs**: Study methods for identifying and quantifying hallucinations in generated captions. Why needed: Forms the theoretical foundation for using confidence scores as a proxy. Quick check: Compare hallucination detection methods against ground truth annotations.

## Architecture Onboarding
**Component Map**: VLM Captioner -> Caption Noise Analyzer -> T2I Model with Attention Reweighting -> Image Generator
**Critical Path**: Caption generation → Hallucination analysis → Attention reweighting → Image synthesis
**Design Tradeoffs**: The method trades computational overhead from confidence score calculation against improved generation quality. Simpler than full hallucination detection but relies on VLM score reliability.
**Failure Signatures**: When VLM confidence scores poorly correlate with actual caption quality, or when attention reweighting excessively dampens valid caption information.
**First Experiments**: (1) Validate VLM confidence correlation with hallucination across diverse caption types. (2) Test reweighting impact on simple text-image alignment tasks. (3) Measure computational overhead versus quality improvements.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The reweighting method may not generalize well to diverse real-world scenarios where caption noise patterns differ substantially from the benchmark dataset
- Reliance on VLM confidence scores as indicators of hallucination may not consistently reflect semantic correctness across different domains and subject matters
- Experimental scope is somewhat limited in terms of model diversity, focusing primarily on diffusion-based approaches

## Confidence
- Core findings reproducibility: Medium
- Generalizability to other T2I architectures: Low
- Long-term stability of reweighting approach: Low

## Next Checks
1. Test the reweighting method across multiple T2I architectures beyond diffusion models to assess architectural independence
2. Evaluate performance on a more diverse set of caption noise patterns, including adversarial examples and domain-specific variations
3. Conduct long-term training experiments to monitor stability and identify any emergent side effects of the reweighting approach