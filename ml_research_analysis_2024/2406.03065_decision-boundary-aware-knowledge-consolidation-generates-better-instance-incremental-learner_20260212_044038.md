---
ver: rpa2
title: Decision Boundary-aware Knowledge Consolidation Generates Better Instance-Incremental
  Learner
arxiv_id: '2406.03065'
source_url: https://arxiv.org/abs/2406.03065
tags:
- data
- learning
- knowledge
- incremental
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new instance-incremental learning (IIL) setting
  where a model must improve performance on new observations of seen classes without
  access to old training data. The setting is more practical than existing IIL or
  class-incremental learning because it emphasizes performance promotion rather than
  just resisting forgetting.
---

# Decision Boundary-aware Knowledge Consolidation Generates Better Instance-Incremental Learner

## Quick Facts
- **arXiv ID**: 2406.03065
- **Source URL**: https://arxiv.org/abs/2406.03065
- **Reference count**: 40
- **Primary result**: Decision boundary-aware distillation with knowledge consolidation achieves +4.93% performance promotion on CIFAR-100 while maintaining minimal forgetting (-1.86%)

## Executive Summary
This paper introduces a new instance-incremental learning (IIL) setting where models must improve performance on new observations of seen classes without access to old training data. The authors propose a decision boundary-aware distillation method that preserves existing decision boundaries while learning from new data, using random Gaussian noise to manifest these boundaries for distillation. They also introduce a knowledge consolidation mechanism that transfers knowledge from student to teacher model via EMA. Experiments demonstrate significant performance improvements over existing approaches, with the teacher model itself becoming a better incremental learner than the student, overturning the conventional student-focused knowledge distillation paradigm.

## Method Summary
The method addresses IIL by first training a base model on initial data, then incrementally learning from new data while preserving knowledge of seen classes. The key innovation is decision boundary-aware distillation (DBD), which uses fused labels combining one-hot labels with teacher predictions to guide learning on new data. To manifest the decision boundary without old data, the approach adds Gaussian noise to input space ("dusting"), revealing boundary regions. Knowledge consolidation transfers student knowledge to the teacher via EMA, with adaptive momentum to balance learning and forgetting. The teacher model ultimately outperforms the student, challenging traditional knowledge distillation assumptions.

## Key Results
- Achieves +4.93% performance promotion on CIFAR-100 with minimal forgetting (-1.86%)
- Most rehearsal-based methods fail in this IIL setting where they traditionally excel
- Teacher model becomes a better incremental learner than the student model
- Significantly outperforms existing approaches including LwF, iCarl, PODNet, and Der

## Why This Works (Mechanism)

### Mechanism 1
The method uses decision boundary-aware distillation to balance learning new data while retaining old knowledge without access to old data. The approach uses a fused label combining one-hot labels with teacher model predictions to guide learning on new data, helping the model determine where to strengthen knowledge and where to retain it. Additionally, random Gaussian noise is added to manifest the decision boundary for distillation. The core assumption is that the learned decision boundary can be effectively approximated and utilized for knowledge distillation without access to old data.

### Mechanism 2
Knowledge consolidation through EMA (Exponential Moving Average) of the student model to the teacher model improves overall performance. After training the student model, its knowledge is consolidated to the teacher model using EMA, which helps balance learning and forgetting, and improves generalization. The core assumption is that EMA can effectively consolidate knowledge from the student to the teacher without causing significant forgetting.

### Mechanism 3
The teacher model can become a better incremental learner than the student model, overturning the conventional student-focused knowledge distillation paradigm. By consolidating knowledge from the student to the teacher, the teacher model achieves better generalization on both old and new data, making it a superior incremental learner. The core assumption is that the teacher model can effectively learn and generalize from the consolidated knowledge.

## Foundational Learning

- **Concept**: Catastrophic Forgetting (CF)
  - Why needed here: Understanding CF is crucial as the new IIL setting aims to promote model performance without access to old data, which inherently leads to CF.
  - Quick check question: What is catastrophic forgetting, and why is it a problem in incremental learning?

- **Concept**: Decision Boundary
  - Why needed here: The decision boundary is central to the proposed method as it is used to guide the learning process and preserve old knowledge while learning new data.
  - Quick check question: How does the decision boundary help in balancing learning new data and retaining old knowledge?

- **Concept**: Knowledge Distillation
  - Why needed here: Knowledge distillation is a key technique used in the proposed method to transfer knowledge from the teacher to the student model.
  - Quick check question: What is knowledge distillation, and how does it help in incremental learning?

## Architecture Onboarding

- **Component map**: Base model (M0) -> Teacher model (T) -> Student model (S) -> Decision Boundary-aware Distillation (DBD) -> Knowledge Consolidation (KC) via EMA -> Gaussian noise for input space dusting

- **Critical path**:
  1. Train base model M0 on base data
  2. Initialize teacher and student models with M0
  3. For each incremental phase:
     a. Apply DBD with fused labels and dusted input space
     b. Consolidate knowledge from student to teacher via KC
     c. Evaluate performance on test data and base data

- **Design tradeoffs**:
  - Using EMA for knowledge consolidation vs. direct learning
  - Balancing the frequency of EMA updates to avoid forgetting
  - Using fused labels vs. one-hot labels for learning new data

- **Failure signatures**:
  - Significant performance degradation on base data (forgetting)
  - Inability to learn new data effectively
  - Teacher model not outperforming student model

- **First 3 experiments**:
  1. Implement DBD with fused labels and dusted input space on a small dataset to verify its effectiveness in balancing learning and retaining knowledge.
  2. Test the impact of different EMA frequencies on the consolidation of knowledge from the student to the teacher model.
  3. Compare the performance of the teacher and student models after several incremental learning phases to validate the proposed method's effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
How does the decision boundary-aware distillation method perform when the incremental data contains significant domain shift or distribution shift from the base data? The paper mentions that the new IIL setting aims to promote the model's performance on new observations without access to old data, but does not provide experiments on significant domain shift scenarios.

### Open Question 2
What is the optimal frequency and momentum for knowledge consolidation in the KC-EMA strategy for different incremental learning tasks? The paper mentions that KC-EMA is performed every 5 epochs with adaptive momentum, but does not provide an analysis of different frequencies and momentums.

### Open Question 3
How does the decision boundary-aware distillation method handle cases where the incremental data contains noisy or mislabeled samples? The paper does not address the issue of noisy or mislabeled samples in the incremental data, which is a common problem in real-world applications.

## Limitations

- The method's effectiveness depends on the quality of decision boundary approximation through input space dusting, which may not reliably capture true boundaries
- Scalability to real-world scenarios with many more classes and frequent incremental updates remains unclear
- The optimal frequency and momentum for knowledge consolidation are not fully explored

## Confidence

- **High Confidence**: Experimental methodology and evaluation protocol are clearly specified with reproducible results on CIFAR-100 and ImageNet-100
- **Medium Confidence**: Teacher model outperforming student is well-supported but mechanism requires more rigorous analysis
- **Low Confidence**: Scalability to complex real-world scenarios with larger class sets and more incremental steps is unclear

## Next Checks

1. **Boundary Fidelity Analysis**: Conduct ablation studies varying Gaussian noise parameters and measure impact on decision boundary preservation accuracy; visualize dusted input space

2. **Teacher-Student Performance Gap Investigation**: Track performance trajectories of both models across all incremental phases to understand when and why teacher overtakes student; analyze consistency across different EMA schedules

3. **Real-world Scalability Test**: Implement method on more complex dataset (e.g., TinyImageNet) with increased incremental phases; evaluate whether performance promotion effect remains consistent as problem complexity grows