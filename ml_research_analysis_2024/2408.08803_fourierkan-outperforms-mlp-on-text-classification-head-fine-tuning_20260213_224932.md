---
ver: rpa2
title: FourierKAN outperforms MLP on Text Classification Head Fine-tuning
arxiv_id: '2408.08803'
source_url: https://arxiv.org/abs/2408.08803
tags:
- classification
- arxiv
- fr-kan
- head
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of Fourier-KAN (FR-KAN) as an alternative
  to MLP classification heads in resource-constrained text classification settings.
  FR-KAN leverages Fourier series to approximate univariate functions, offering smoother
  representations compared to B-splines in standard KAN.
---

# FourierKAN outperforms MLP on Text Classification Head Fine-tuning

## Quick Facts
- arXiv ID: 2408.08803
- Source URL: https://arxiv.org/abs/2408.08803
- Reference count: 40
- Primary result: FR-KAN achieves 10% accuracy and 11% F1 improvements over MLPs on text classification

## Executive Summary
This paper introduces Fourier-KAN (FR-KAN), a classification head architecture for text classification that replaces traditional MLP heads with a Fourier series-based Kolmogorov-Arnold Network. FR-KAN leverages Fourier series to approximate univariate functions, offering smoother representations than B-splines used in standard KAN. The approach is evaluated across seven transformer models and four text classification tasks, demonstrating significant performance improvements while being more computationally efficient and training faster with fewer parameters.

## Method Summary
The study evaluates FR-KAN as a classification head alternative to MLP and KAN on seven transformer models (BART, BERT, DeBERTa, DistilBERT, ELECTRA, RoBERTa, XLNet) fine-tuned on seven datasets. All experiments use linear probing with frozen transformer backbones, 5 epochs of training at learning rate 2e-5, batch size 64, and max length 512. Three classification head architectures are compared: MLP-1 (single hidden layer), KAN-1 (single KAN layer with B-splines), and FR-KAN-5 (single KAN layer with Fourier series and grid size 5).

## Key Results
- FR-KAN outperforms MLPs with average 10% accuracy and 11% F1 score improvements
- FR-KAN is more computationally efficient and trains faster with fewer parameters
- Convergence curves show FR-KAN reaches target accuracy/F1 scores earlier than MLP and KAN

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fourier series smoothers allow better representation of continuous univariate functions than B-splines in KAN for text classification heads.
- Mechanism: The Fourier-KAN head replaces B-spline residual functions with a truncated Fourier series. By Theorem 1 and Corollary 1, as the grid size increases, the Fourier series converges uniformly to the target function, yielding smoother functional approximations that better capture nuances in contextual embeddings.
- Core assumption: The function to be learned is smooth and continuous over the input domain.
- Evidence anchors:
  - [abstract] "FR-KAN leverages Fourier series to approximate univariate functions, offering smoother representations compared to B-splines in standard KAN."
  - [section] Theorem 1 and Corollary 1 formalize the convergence and truncation error of the Fourier series.
  - [corpus] Weak — neighboring papers discuss KAN and Fourier series but do not provide direct empirical support for smoother convergence in text classification.
- Break condition: If the underlying function has discontinuities or high-frequency components that the Fourier series cannot represent with the chosen grid size, performance degrades.

### Mechanism 2
- Claim: Fourier-KAN classification heads converge faster during fine-tuning than both MLP and spline-based KAN heads.
- Mechanism: The Fourier series representation provides a more stable gradient landscape due to smoother basis functions, reducing training instability and enabling earlier convergence in fewer epochs.
- Core assumption: Smoother activation surfaces lead to faster optimization in the parameter space of the classification head.
- Evidence anchors:
  - [abstract] "FR-KAN is more computationally efficient and trains faster with fewer parameters."
  - [section] Fig. 3(d) and 3(e) show FR-KAN reaching target accuracy/F1 scores earlier than MLP and KAN.
  - [corpus] Missing — no direct comparison of convergence curves in related literature.
- Break condition: If learning rate or batch size is mismatched, the smoother surface advantage may be negated.

### Mechanism 3
- Claim: FR-KAN requires fewer trainable parameters than MLP while maintaining or improving accuracy.
- Mechanism: By using a 1-layer FR-KAN architecture with a small grid size, the number of trainable coefficients is less than or comparable to the number of weights in an MLP hidden layer, yet the expressive power is higher due to the Fourier basis.
- Core assumption: The number of Fourier coefficients needed for good approximation is less than the number of MLP weights for equivalent accuracy.
- Evidence anchors:
  - [abstract] "FR-KAN is more computationally efficient and trains faster with fewer parameters."
  - [section] Table 3 shows parameter counts for DistilBERT + FR-KAN vs MLP across datasets, with FR-KAN having equal or fewer parameters.
  - [corpus] Weak — related papers discuss parameter efficiency in KAN variants but not directly comparing Fourier-KAN to MLP.
- Break condition: If grid size is increased too much to improve performance, parameter count grows and may exceed MLP's parameter budget.

## Foundational Learning

- Concept: Kolmogorov-Arnold Representation Theorem
  - Why needed here: Provides the theoretical basis for decomposing multivariate functions into sums of continuous univariate functions, which KAN and FR-KAN implement.
  - Quick check question: What is the form of the multivariate function decomposition given by the Kolmogorov-Arnold theorem?

- Concept: Fourier Series Convergence
  - Why needed here: Justifies the use of Fourier series as a smooth basis for approximating univariate functions in FR-KAN.
  - Quick check question: Under what conditions does a Fourier series converge uniformly to a continuous function?

- Concept: Linear Probing / Classification Head Fine-tuning
  - Why needed here: The experimental setting freezes the transformer backbone and only trains the classification head, making head architecture choice critical.
  - Quick check question: What is the difference between linear probing and full fine-tuning in transformer-based text classification?

## Architecture Onboarding

- Component map:
  Tokenizer → Transformer Encoder (frozen) → Contextual Embeddings → FR-KAN Layer → Output Classes

- Critical path:
  1. Tokenize input text.
  2. Pass through pre-trained transformer (frozen weights).
  3. Feed last hidden state to FR-KAN layer.
  4. Apply softmax to FR-KAN output for class prediction.
  5. Compute cross-entropy loss and backpropagate only through FR-KAN.

- Design tradeoffs:
  - Grid size G: Larger G increases expressive power but also parameter count and risk of overfitting.
  - Residual connection: Can stabilize training but adds parameters and may reduce interpretability.
  - Layer width: In FR-KAN, controlled by G; in MLP, by hidden layer width.

- Failure signatures:
  - Overfitting: Training accuracy >> validation accuracy, especially with large G.
  - Underfitting: Both training and validation accuracy low, suggesting insufficient grid size or learning rate.
  - Slow convergence: Possible if learning rate too low or initialization poor.

- First 3 experiments:
  1. Replace MLP with 1-layer FR-KAN (G=5) on DistilBERT + IMDb, compare accuracy/F1 to baseline MLP.
  2. Sweep grid size G=1,3,5,7 on same setup, record performance and parameter count.
  3. Compare training curves (loss, accuracy) of FR-KAN vs MLP vs KAN on SST-5 to observe convergence speed.

## Open Questions the Paper Calls Out

- Question: How do FR-KAN classification heads perform when fine-tuned on larger, more diverse text classification datasets beyond the seven tested in this study?
  - Basis in paper: [explicit] The paper tested FR-KAN on seven datasets but acknowledged potential performance differences with different dataset splits and training configurations.
  - Why unresolved: The study's limited dataset scope may not fully capture FR-KAN's generalizability and robustness across a wider range of text classification tasks.
  - What evidence would resolve it: Comprehensive experiments on larger and more diverse text classification datasets, including multi-label and imbalanced datasets, would provide insights into FR-KAN's broader applicability.

- Question: Can FR-KAN classification heads be effectively integrated as neural network layers within transformer architectures, rather than just as classification heads?
  - Basis in paper: [explicit] The authors suggest exploring FR-KAN's potential as a generalized MLP alternative within transformer architectures.
  - Why unresolved: The paper only evaluated FR-KAN as a classification head, leaving its potential as a transformer layer unexplored.
  - What evidence would resolve it: Implementing and evaluating FR-KAN layers within transformer architectures on various NLP tasks would demonstrate its effectiveness as a generalized MLP replacement.

- Question: How does the interpretability of FR-KAN compare to other interpretable alternatives, such as sparse autoencoders, in text classification tasks?
  - Basis in paper: [explicit] The paper acknowledges that FR-KAN's improved performance comes at the expense of interpretability compared to B-splines.
  - Why unresolved: The study did not compare FR-KAN's interpretability with other methods, leaving its trade-off between performance and interpretability unclear.
  - What evidence would resolve it: Conducting interpretability analyses and comparisons between FR-KAN and other interpretable methods, such as sparse autoencoders, would clarify its interpretability trade-offs.

## Limitations

- The empirical evaluation is limited to seven transformer models and four text classification datasets, which may not capture performance across the full diversity of NLP tasks and architectures.
- The paper does not report standard deviation or statistical significance testing across multiple runs, making it difficult to assess whether observed performance differences are robust.
- The comparison with KAN using B-splines is incomplete as the exact grid sizes and parameter configurations for the spline-based KAN are not specified, preventing fair architectural comparison.

## Confidence

- **High Confidence**: FR-KAN achieves faster convergence than MLP on the tested datasets. The convergence curves in Figure 3 provide clear visual evidence, and this claim is directly supported by the training curves without requiring additional assumptions.
- **Medium Confidence**: FR-KAN provides 10% accuracy and 11% F1 improvements over MLP. While the aggregate numbers are reported, the lack of variance metrics and statistical significance testing reduces confidence in the magnitude of these improvements.
- **Low Confidence**: FR-KAN uses fewer parameters than MLP while maintaining accuracy. Table 3 shows parameter counts for DistilBERT only, and the claim of parameter efficiency across all transformer models is not fully substantiated with comprehensive data.

## Next Checks

1. **Statistical Significance Testing**: Run each experiment configuration (transformer + dataset + head type) five times with different random seeds and compute confidence intervals and p-values to determine whether performance differences are statistically significant rather than due to random variation.

2. **Architectural Parameter Parity**: Implement both FR-KAN and MLP with matched parameter budgets by adjusting grid sizes and hidden layer widths, then compare performance to isolate whether improvements come from architectural advantages or simply from having more parameters.

3. **Function Smoothness Analysis**: Compute the spectral properties of the learned univariate functions in both FR-KAN and spline-based KAN on actual text classification tasks to empirically verify whether Fourier series indeed captures smoother functions, and whether function smoothness correlates with classification performance.