---
ver: rpa2
title: 'The Fragility of Fairness: Causal Sensitivity Analysis for Fair Machine Learning'
arxiv_id: '2410.09600'
source_url: https://arxiv.org/abs/2410.09600
tags:
- bias
- causal
- fairness
- sensitivity
- parity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a causal sensitivity analysis framework
  for evaluating fairness metrics in machine learning under measurement biases. The
  authors address three common biases: proxy label bias, selection bias, and extra-classificatory
  policy bias.'
---

# The Fragility of Fairness: Causal Sensitivity Analysis for Fair Machine Learning

## Quick Facts
- arXiv ID: 2410.09600
- Source URL: https://arxiv.org/abs/2410.09600
- Authors: Jake Fawkes; Nic Fishman; Mel Andrews; Zachary C. Lipton
- Reference count: 40
- Primary result: Fairness metrics are fragile to realistic levels of measurement bias, with demographic parity most robust and predictive parity metrics most fragile

## Executive Summary
This paper introduces a causal sensitivity analysis framework for evaluating fairness metrics in machine learning under measurement biases. The authors address three common biases: proxy label bias, selection bias, and extra-classificatory policy bias. They adapt automated discrete causal inference tools to create sensitivity bounds for parity metrics under these biases in the "oblivious setting" where covariates are unobserved. The framework allows practitioners to assess how measurement biases affect fairness evaluations by specifying causal graphs, sensitivity parameters, and domain constraints. Experiments across 14 benchmark datasets with multiple classifiers show that fairness metrics are fragile to realistic levels of bias - even minor violations can render evaluations meaningless.

## Method Summary
The framework leverages the function response framework from causal inference to represent SCMs with discrete observed variables using distributions over discrete latent variables. This converts the partial identification problem into polynomial programming problems solvable by branch and bound methods. Users specify causal graphs representing how the sampled population differs from the target population, then the framework marginalizes out unobserved variables while preserving causal structure. The system computes upper bounds on parity metrics by varying sensitivity parameters for proxy label bias, selection bias, and extra-classificatory policy bias.

## Key Results
- Demographic parity metrics demonstrate greater robustness to measurement biases compared to predictive parity metrics
- Even minor measurement bias violations can render fairness evaluations meaningless across all tested parity metrics
- Multiple biases compound unpredictably, with combinations of proxy label bias, selection bias, and extra-classificatory policy bias quickly degrading fairness metric reliability

## Why This Works (Mechanism)

### Mechanism 1
The causal sensitivity analysis framework works by converting fairness metric evaluation under measurement biases into tractable polynomial optimization problems. The framework leverages the function response framework from causal inference to represent SCMs with discrete observed variables using distributions over discrete latent variables. This converts the partial identification problem into polynomial programming problems solvable by branch and bound methods.

### Mechanism 2
The framework reveals that fairness metrics are fragile to even minor measurement biases, with demographic parity being most robust. By systematically varying sensitivity parameters for proxy label bias, selection bias, and extra-classificatory policy bias, the framework computes upper bounds on parity metrics. These bounds widen dramatically as bias parameters increase, showing that small violations can render fairness evaluations meaningless.

### Mechanism 3
The framework enables practitioners to understand how measurement biases affect fairness evaluations by precisely articulating the target population versus observed data. The framework requires users to specify causal graphs representing how the sampled population differs from the target population, then marginalizes out unobserved variables while preserving causal structure over observable variables.

## Foundational Learning

- **Structural Causal Models (SCMs) and Directed Acyclic Graphs (DAGs)**: The framework relies on expressing measurement biases as causal graphs, then marginalizing out unobserved variables while preserving causal structure over observable variables. Quick check: If we have a DAG with variables A, X, Y, Ŷ where X is unobserved, what does the marginalized graph over (A, Y, Ŷ) look like when X causes both Y and Ŷ?

- **Partial identification and sensitivity analysis**: The framework uses partial identification to find bounds on fairness metrics under different levels of measurement bias, then varies the sensitivity parameter to assess how violations affect these bounds. Quick check: Given a fairness metric Q and a set of causal models M, what optimization problem do we solve to find the bounds Q(C) under measurement bias?

- **Function response framework for discrete variables**: This framework converts SCMs with discrete variables into optimization problems over probability simplices, making the sensitivity analysis computationally tractable. Quick check: Why does having discrete observed variables allow us to represent SCMs using a fixed set of structural equations and a distribution over latent variables?

## Architecture Onboarding

- **Component map**: User config -> Bias configuration parser -> Fairness metric library -> Optimization backend -> Solver interface -> Results display
- **Critical path**: User configures bias → Parser generates optimization problem → Backend solves for bounds → Results displayed/returned
- **Design tradeoffs**: Flexibility vs. usability (config files allow complex analyses but require technical knowledge), computational tractability vs. realism (discrete assumption simplifies computation but may not capture all real-world scenarios)
- **Failure signatures**: Invalid DAG structures, unbounded optimization problems, solver timeouts, incorrect parity metric implementations
- **First 3 experiments**: 1) Recreate proxy label bias analysis from Fogliato et al. [26] on COMPAS dataset to verify implementation, 2) Test selection bias sensitivity on Adult dataset with varying P(S=1) parameters, 3) Combine proxy and selection biases on German Credit dataset to observe nonlinear interactions

## Open Questions the Paper Calls Out

### Open Question 1
How do measurement biases interact and compound when present simultaneously in real-world datasets? The paper explicitly demonstrates that combinations of proxy label bias, selection bias, and extra-classificatory policy bias can quickly render parity evaluations meaningless, with results showing non-linear compounding effects. This remains unresolved as the paper doesn't provide a systematic framework for predicting or quantifying these interactions across different types of biases and metrics.

### Open Question 2
Which fairness metrics remain most informative under realistic levels of measurement bias across different domains? The paper finds that demographic parity is most robust while predictive parity metrics are most fragile, but this ranking varies across datasets and domains. The paper doesn't establish a domain-specific framework for determining which metrics provide meaningful information under varying bias levels, or how this varies by application area.

### Open Question 3
How can practitioners determine the appropriate reference population for fairness evaluation when selection bias is present? The paper discusses selection bias and reference population choice but notes there are no universal solutions, depending on the specific task and deployment context. The paper acknowledges this is context-specific but doesn't provide concrete guidelines or decision frameworks for practitioners to follow when selecting reference populations.

## Limitations
- Requirement that all variables be discrete severely restricts applicability to real-world datasets
- Accuracy depends heavily on correctly specifying the causal graph structure, which may be difficult when selection mechanisms are unknown
- The framework provides bounds rather than point estimates, which can be conservative and may not reflect actual fairness violations

## Confidence

### High Confidence Claims
- The framework successfully converts fairness metric evaluation under measurement biases into polynomial optimization problems
- Demographic parity metrics demonstrate greater robustness to measurement biases compared to predictive parity metrics

### Medium Confidence Claims
- Minor measurement bias violations can render fairness evaluations meaningless
- The autobounds branch and bound solver provides tractable solutions for discrete variable problems

### Low Confidence Claims
- Real-world applicability of sensitivity parameters
- Generalizability across all fairness metrics and bias types

## Next Checks
1. Apply the framework to a deployed ML system where measurement biases are documented (e.g., medical diagnosis systems with known labeling errors) to verify that sensitivity bounds align with actual fairness violations.
2. Implement a continuous relaxation of the framework to assess how results change when moving from discrete to continuous variables, particularly for variables like age or income that commonly appear in fairness datasets.
3. Develop a sensitivity analysis that varies the causal graph structure itself (rather than just parameters) to understand how misspecification affects fairness metric bounds, using techniques like graph perturbation or counterfactual graph structures.