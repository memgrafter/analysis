---
ver: rpa2
title: 'GTA: Guided Transfer of Spatial Attention from Object-Centric Representations'
arxiv_id: '2401.02656'
source_url: https://arxiv.org/abs/2401.02656
tags:
- attention
- baseline
- performance
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of preserving high-quality attention
  representations during transfer learning with vision transformers (ViTs), especially
  when training data is limited. The authors observe that naive fine-tuning causes
  attention maps to degrade and focus on irrelevant regions.
---

# GTA: Guided Transfer of Spatial Attention from Object-Centric Representations

## Quick Facts
- **arXiv ID**: 2401.02656
- **Source URL**: https://arxiv.org/abs/2401.02656
- **Reference count**: 40
- **Primary result**: GTA improves fine-grained classification accuracy by 10.15% on CUB-200 at 15% training data

## Executive Summary
This paper addresses the challenge of preserving high-quality attention representations during transfer learning with vision transformers (ViTs), especially when training data is limited. The authors observe that naive fine-tuning causes attention maps to degrade and focus on irrelevant regions. To counter this, they propose Guided Transfer of spatial Attention (GTA), a regularization method that minimizes the L2 distance between self-attention logits of a pre-trained source model and a target model during fine-tuning. GTA explicitly guides the target model to maintain the object-focused attention patterns learned during pre-training. Experimental results on five fine-grained classification datasets show consistent accuracy improvements with GTA, particularly under low-data regimes (e.g., 15% training data). For instance, on CUB-200, GTA improves accuracy by 10.15% compared to baseline at 15% data. GTA also outperforms other transfer learning baselines and shows synergy with attention-based augmentations like TransMix. Ablation studies confirm the superiority of guiding attention logits over other features and highlight the importance of selecting appropriate regularization strength. Overall, GTA is a simple yet effective method for leveraging pre-trained attention knowledge in ViTs.

## Method Summary
GTA is a transfer learning regularization technique for vision transformers that preserves object-focused attention patterns during fine-tuning. The method works by adding an L2 regularization term to the classification loss, which minimizes the difference between self-attention logits (specifically, the [cls] token attention vector excluding the first element) of a pre-trained source model and the target model being fine-tuned. During training, both models process the same input, and the attention logits from corresponding layers are extracted and compared. The L2 distance between these logits is weighted by a hyperparameter λ and added to the classification loss. This encourages the target model to maintain similar attention patterns to the pre-trained source model, preventing degradation of attention maps that commonly occurs during standard fine-tuning. The approach is particularly effective when training data is limited, as it prevents the model from learning shortcuts by attending to irrelevant background regions. GTA was tested on ViT-small architecture with iBOT pre-trained weights, using AdamW optimizer with cosine annealing scheduler and RandAugment augmentation.

## Key Results
- GTA improves classification accuracy by 10.15% on CUB-200 at 15% training data compared to baseline fine-tuning
- GTA consistently outperforms baseline fine-tuning across all five fine-grained datasets at all training percentages (15%, 30%, 50%, 100%)
- GTA shows superior performance compared to other transfer learning baselines including L2-SP, BSS, and attention-only fine-tuning
- GTA demonstrates synergy with attention-based augmentations like TransMix, achieving additional performance gains when combined
- Ablation studies confirm that guiding attention logits is more effective than guiding other features like MSA outputs or block outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GTA prevents attention maps from degrading during fine-tuning by explicitly regularizing the self-attention logits between the pre-trained source model and the target model.
- Mechanism: During fine-tuning, GTA adds an L2 regularization term that minimizes the difference between the attention logits (specifically, the [cls] token attention vector excluding the first element) of the source and target models. This keeps the target model's attention focused on relevant object regions.
- Core assumption: The attention logits from the pre-trained source model encode useful spatial attention patterns that should be preserved during transfer learning.
- Evidence anchors:
  - [abstract] "GTA explicitly guides the target model to maintain the object-focused attention patterns learned during pre-training."
  - [section] "Our proposed method regularizes the self-attention maps between the source and target models. A target model can fully exploit the knowledge related to object localization properties through this explicit regularization."
- Break condition: If the source model's attention patterns are not relevant to the target task, or if the regularization strength is too high and prevents necessary adaptation.

### Mechanism 2
- Claim: GTA is particularly effective when training data is limited because it prevents the model from overfitting and learning shortcuts by focusing on irrelevant background regions.
- Mechanism: When data is scarce, ViTs tend to attend to non-object regions (background) as shortcuts. GTA's regularization term constrains the target model to maintain the source model's object-focused attention patterns, preventing this shortcut learning.
- Core assumption: Limited training data increases the likelihood of shortcut learning in ViTs due to their low inductive bias.
- Evidence anchors:
  - [abstract] "This phenomenon is more severe in ViT due to its low inductive bias."
  - [section] "Through the self-attention maps, we can visually see which image tokens are particularly attended to perform the target task. As shown in Figure 1, the visualization results indicate that ViT trained with basic fine-tuning tends to learn shortcuts, e.g., the features corresponding to the background (i.e., non-object area)."
- Break condition: When sufficient training data is available, the model may not need as much regularization and could benefit from more flexibility to adapt to the target task.

### Mechanism 3
- Claim: Guiding attention logits is more effective than guiding other features (like MSA outputs or block outputs) because attention logits contain distilled spatial mixing coefficients that preserve object-centric localization.
- Mechanism: The attention logits represent the attention weights before softmax normalization, encoding raw spatial mixing information. By regularizing these logits, GTA preserves the spatial attention patterns while allowing the model to learn task-specific features in later layers.
- Core assumption: Attention logits contain more direct and useful information about spatial attention patterns than other intermediate features.
- Evidence anchors:
  - [section] "Table 3 shows the effectiveness of guiding attention logits, particularly when contrasted with the utilization of two other outputs, the transformer block output z' and MSA output MSA(z) of the ViT architecture."
  - [section] "These results reveal the crucial importance of selecting attention logits for the guiding mechanism, implying that alternatives may causally lead to negative transfer."
- Break condition: If the attention logits from the source model are not well-aligned with the target task's spatial requirements, or if the regularization prevents necessary spatial adaptation.

## Foundational Learning

- Concept: Vision Transformers (ViTs) and their attention mechanism
  - Why needed here: GTA specifically operates on the attention mechanism of ViTs, so understanding how self-attention works is fundamental to implementing and modifying the method.
  - Quick check question: What is the difference between attention logits and attention weights in a ViT, and why does GTA regularize the logits specifically?

- Concept: Transfer learning and fine-tuning strategies
  - Why needed here: GTA is a transfer learning technique, and understanding standard fine-tuning approaches and their limitations is crucial for appreciating why GTA is needed and how it differs from other methods.
  - Quick check question: How does fine-tuning a pre-trained model differ from training from scratch, and what are the common challenges in fine-tuning?

- Concept: Regularization techniques in deep learning
  - Why needed here: GTA uses L2 regularization on attention logits, so understanding how regularization works and its effects on model training is important for tuning hyperparameters like λ.
  - Quick check question: What is the purpose of L2 regularization, and how does adding a regularization term to the loss function affect model training?

## Architecture Onboarding

- Component map: Source model (frozen pre-trained weights) -> Target model (being fine-tuned) -> Input preprocessing -> Forward pass through both models -> Extract attention logits from [cls] token -> Compute L2 distance between source and target logits -> Add distance to classification loss with weight λ -> Optimize target model

- Critical path: During training, the most critical steps are: (1) extracting the [cls] token attention vector from the source model, (2) extracting the corresponding attention vector from the target model, (3) computing the L2 distance between them, and (4) adding this to the classification loss with the appropriate weight λ.

- Design tradeoffs: The main tradeoff is between preserving pre-trained attention patterns (higher λ) and allowing the target model to adapt to the new task (lower λ). Too much regularization can prevent necessary adaptation, while too little defeats the purpose of GTA.

- Failure signatures: Common failures include: (1) the model ignoring the GTA regularization and behaving like standard fine-tuning, (2) the model becoming too rigid and unable to adapt to the target task, (3) choosing an inappropriate λ value leading to poor performance, (4) using a source model whose attention patterns are not relevant to the target task.

- First 3 experiments:
  1. Implement GTA with a simple ViT model on a small dataset (like CUB-200 with 15% data) and compare attention maps with standard fine-tuning to verify that GTA preserves object-focused attention.
  2. Experiment with different λ values (0.1, 1.0, 10.0, 100.0) on the same dataset to find the optimal regularization strength.
  3. Compare GTA's performance with standard fine-tuning and other transfer learning methods (like L2-SP or attention-only fine-tuning) on multiple datasets to validate its effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal regularization coefficient λ vary across different downstream tasks and dataset domains?
- Basis in paper: [explicit] The authors state that the optimal λ varies depending on the size and characteristics of the dataset, and is influenced by whether the task is near-domain or out-domain relative to the pre-trained model.
- Why unresolved: The paper only tested a limited range of λ values and did not provide a systematic method for selecting λ. The relationship between λ and domain similarity is qualitative.
- What evidence would resolve it: A comprehensive study varying λ across a wider range of tasks and domains, with quantitative analysis of how domain similarity affects the optimal λ value.

### Open Question 2
- Question: Does the attention regularization approach generalize to other self-supervised learning methods beyond iBOT and DINO?
- Basis in paper: [explicit] The authors note that GTA improved performance with both iBOT and DINO weights, but did not test other SSL methods like MAE or SimCLR.
- Why unresolved: The paper only tested two specific SSL methods, leaving open whether the approach is broadly applicable.
- What evidence would resolve it: Experiments applying GTA to models pre-trained with different SSL methods (e.g., MAE, SimCLR, Barlow Twins) and comparing the performance gains.

### Open Question 3
- Question: What is the impact of regularizing attention logits on the model's ability to learn task-specific attention patterns that differ from the source model?
- Basis in paper: [inferred] The method constrains attention logits to remain similar to the source model, which may conflict with learning task-specific patterns.
- Why unresolved: The paper focuses on preventing degradation but doesn't analyze whether this constraint might limit learning of new attention patterns.
- What evidence would resolve it: Comparative analysis of attention patterns learned by GTA models versus fine-tuned models on tasks where optimal attention differs significantly from the source model.

## Limitations

- The method's effectiveness is primarily demonstrated on fine-grained classification tasks, with unknown performance on more diverse domains like medical imaging or action recognition
- The optimal regularization strength λ requires careful tuning and may vary significantly across different tasks and domains
- The approach adds computational overhead during fine-tuning by requiring two models and attention logit computation at each iteration
- The paper doesn't explore whether alternative regularization strategies (e.g., KL divergence) might be more effective than L2 distance

## Confidence

- **High Confidence**: The empirical results showing GTA's consistent accuracy improvements across all five datasets, particularly under low-data regimes. The ablation study clearly demonstrates the superiority of guiding attention logits over other features.
- **Medium Confidence**: The claim that GTA prevents shortcut learning and background attention. While attention visualizations support this, the paper doesn't quantitatively measure background attention or provide rigorous statistical tests comparing attention distributions between methods.
- **Medium Confidence**: The mechanism explanation that attention logits contain more useful spatial information than other features. The ablation study shows better performance, but doesn't provide theoretical justification for why logits specifically are optimal.

## Next Checks

1. **Cross-domain validation**: Test GTA on datasets outside the fine-grained classification domain (e.g., medical imaging or satellite imagery) to verify whether pre-trained attention patterns remain beneficial when domain shift is more severe.

2. **Hyperparameter sensitivity analysis**: Conduct a systematic study of λ across all five datasets to determine if a single optimal value exists or if dataset-specific tuning is required. Include statistical significance testing of performance differences between λ values.

3. **Alternative regularization strategies**: Compare GTA with attention regularization using KL divergence instead of L2 distance, and investigate whether progressive reduction of regularization strength during training (curriculum learning approach) improves final performance while maintaining early training stability.