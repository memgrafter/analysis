---
ver: rpa2
title: 'Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias'
arxiv_id: '2403.07857'
source_url: https://arxiv.org/abs/2403.07857
tags:
- class
- generations
- data
- fairness
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of model-induced distribution
  shifts (MIDS) to unify several existing phenomena like fairness feedback loops and
  model collapse. It develops a framework to analyze how these shifts accumulate over
  multiple generations of models, finding that they can cause significant performance
  degradation, fairness issues, and underrepresentation of minority groups.
---

# Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias

## Quick Facts
- arXiv ID: 2403.07857
- Source URL: https://arxiv.org/abs/2403.07857
- Authors: Sierra Wyllie; Ilia Shumailov; Nicolas Papernot
- Reference count: 40
- Key outcome: Model-induced distribution shifts (MIDS) cause significant performance degradation, fairness issues, and underrepresentation of minority groups in sequential model training

## Executive Summary
This paper introduces the concept of model-induced distribution shifts (MIDS) to unify phenomena like fairness feedback loops and model collapse. The authors develop a framework showing how these shifts accumulate over generations, encoding model mistakes and biases into training data. They propose algorithmic reparation (AR) as an intentional MIDS to promote equity, demonstrating that AR interventions using stratified sampling can mitigate negative effects of other MIDS, particularly when combined with non-synthetic data.

## Method Summary
The authors create a framework for analyzing how model-induced distribution shifts accumulate over generations of models trained on synthetic data. They implement sequential classifier and generator settings across four datasets (ColoredMNIST, ColoredSVHN, CelebA, and FairFace), tracking accuracy, fairness metrics (DP, EOdds), and group representation. Algorithmic reparation interventions use stratified sampling to curate representative training batches, aiming to counteract bias amplification and model collapse effects.

## Key Results
- MIDS cause compounding performance degradation and fairness issues across generations
- Algorithmic reparation using stratified sampling can mitigate negative effects of other MIDS
- Co-occurring MIDS interact in complex ways, sometimes amplifying each other's negative effects
- Non-synthetic data can slow model collapse but may enable disparity amplification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model-induced distribution shifts (MIDS) propagate bias through generations by encoding model mistakes into ground truth.
- Mechanism: Each generation of models trains on data that includes outputs from previous generations. Errors, biases, and unfair predictions from prior models become part of the training data for the next generation, creating a compounding effect.
- Core assumption: The training data for each new generation includes outputs from previous generations, and these outputs contain encoded errors and biases.
- Evidence anchors:
  - [abstract] "When a model induces a distribution shift, it also encodes its mistakes, biases, and unfairnesses into the ground truth of its data ecosystem."
  - [section 2.1] "Once re-scraped, this polluted data becomes the ground truth for future generations of models, and the MIDS continues."
  - [corpus] Weak evidence for this specific claim, though the corpus mentions feedback loops and recursive training which are related concepts.
- Break condition: If models are trained exclusively on non-synthetic, high-quality ground truth data, or if explicit bias correction is applied at each generation.

### Mechanism 2
- Claim: Algorithmic reparation (AR) intentionally creates MIDS to promote equity by curating representative training batches.
- Mechanism: AR interventions use stratified sampling to create training batches that prioritize representation of intersectional identities, counteracting the negative effects of other MIDS by intentionally shifting the distribution.
- Core assumption: Stratified sampling can effectively counteract the bias introduced by other MIDS when applied systematically across generations.
- Evidence anchors:
  - [abstract] "We simulate AR interventions by curating representative training batches for stochastic gradient descent to demonstrate how AR can improve upon the unfairnesses of models and data ecosystems subject to other MIDS."
  - [section 3.4] "STAR creates model training batches by taking a biased sample according to these strata (within a resampling budget)."
  - [corpus] No direct evidence for AR in the corpus, though related work on bias mitigation is mentioned.
- Break condition: If the resampling budget is too small to achieve meaningful representation, or if the initial dataset is too biased for stratified sampling to correct.

### Mechanism 3
- Claim: Co-occurring MIDS can interact in complex ways, sometimes amplifying each other's negative effects.
- Mechanism: When multiple types of MIDS occur simultaneously (e.g., model collapse and fairness feedback loops), they can compound each other's effects. For example, model collapse might cause class imbalance, which then exacerbates fairness feedback loops as the classifier adapts to the skewed distribution.
- Core assumption: Multiple MIDS can co-occur in the same data ecosystem, and their effects are not simply additive but can interact in complex ways.
- Evidence anchors:
  - [section 2.1] "These enablers can also permit MIDS to co-occur: a pseudo-labeling model may annotate synthetic data created from generative models to use for supervised training, allowing model collapse and fairness feedback loops to co-occur."
  - [section 4.2.2] "We uncover co-operation between MIDS by evaluating the role of sequential classifiers in SeqGenSeqClass and SeqGenNonSeqClass."
  - [corpus] Weak evidence for this specific claim, though the corpus mentions related concepts like feedback loops and bias amplification.
- Break condition: If interventions are applied that target multiple MIDS simultaneously, or if the data ecosystem is structured to prevent co-occurrence.

## Foundational Learning

- Concept: Model collapse
  - Why needed here: Understanding how generative models trained on synthetic data lose information from the original distribution is crucial for grasping how MIDS affect data quality over generations.
  - Quick check question: What happens to the tails of the data distribution when generative models are trained on their own synthetic outputs over multiple generations?

- Concept: Performative prediction
  - Why needed here: This concept explains how model predictions can become part of the ground truth, creating feedback loops that perpetuate and amplify biases.
  - Quick check question: How does the concept of performative prediction relate to fairness feedback loops in the context of MIDS?

- Concept: Intersectionality in fairness
  - Why needed here: Algorithmic reparation is based on intersectionality theories, which recognize that harms compound at intersecting marginalized identities, making it essential for understanding AR's approach to equity.
  - Quick check question: Why is it important to consider intersectional identities when designing fairness interventions like algorithmic reparation?

## Architecture Onboarding

- Component map:
  - Data ecosystem: Original dataset, synthetic data, human-generated data
  - Models: Generative models (G), Classifiers (C), Annotators (AL, AS)
  - Interventions: Algorithmic reparation (AR) with stratified sampling (STAR)
  - Evaluation: Accuracy, fairness metrics (DP, EOdds), representation metrics

- Critical path:
  1. Initialize G0 from original dataset
  2. Train C0 using G0 and AL
  3. Generate synthetic data with Gi
  4. Train Ci using synthetic data and Ci-1 labels
  5. Apply AR interventions if enabled
  6. Evaluate performance and fairness

- Design tradeoffs:
  - Synthetic vs. non-synthetic data: Synthetic data enables scalability but risks model collapse; non-synthetic data maintains quality but may be limited in quantity.
  - Strict vs. flexible AR: Strict AR ensures representation but may slow convergence; flexible AR balances representation with model performance.

- Failure signatures:
  - Rapid decline in accuracy across generations
  - Increasing variance in model performance
  - Loss of representation for minoritized groups
  - Convergence to majority class or group

- First 3 experiments:
  1. Run SeqGenSeqClass with 100% synthetic data to observe model collapse and fairness degradation.
  2. Apply cla-STAR to SeqClass and measure its impact on fairness metrics.
  3. Compare SeqGenSeqClass with and without sequential classifiers to observe the interaction between model collapse and performative prediction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the rate of model collapse vary with different generative model architectures and training procedures?
- Basis in paper: [inferred] The paper discusses model collapse in generative models and its dependence on factors like the amount of synthetic training data and the complexity of the dataset. It mentions that different datasets exhibit different rates of model collapse, but does not systematically explore the impact of generative model architecture.
- Why unresolved: The paper focuses on demonstrating the existence and effects of model collapse rather than conducting a comprehensive ablation study on generative model design choices. It uses standard VAE architectures and hyperparameters without exploring variations.
- What evidence would resolve it: Experiments comparing model collapse rates across different generative architectures (GANs, diffusion models, autoregressive models), latent space dimensions, training objectives, and regularization techniques. Analysis of how architectural choices affect the tail information loss and mode mixing that characterize model collapse.

### Open Question 2
- Question: What is the long-term impact of algorithmic reparation interventions on data ecosystem dynamics and fairness metrics?
- Basis in paper: [explicit] The paper introduces algorithmic reparation (AR) as an intentional MIDS and demonstrates its short-term effectiveness in improving fairness metrics. However, it acknowledges limitations in fully evaluating AR and notes potential tension between FML metrics and AR goals.
- Why unresolved: The experiments only run for a limited number of generations (5-40), which may not capture the full long-term dynamics of AR interventions. The paper also does not explore how AR might interact with other MIDS over extended time periods or in more complex data ecosystems.
- What evidence would resolve it: Long-term simulations tracking fairness metrics, group representation, and overall ecosystem health over hundreds or thousands of generations. Analysis of how AR interventions might create new forms of bias or lead to unexpected equilibrium states in the data ecosystem.

### Open Question 3
- Question: How do different sampling strategies for non-synthetic data affect the interplay between model collapse and disparity amplification?
- Basis in paper: [explicit] The paper models disparity amplification by sampling non-synthetic data according to classifier label distributions over groups, and shows that this co-occurring with model collapse can lead to complex dynamics. It demonstrates that using non-synthetic data can slow model collapse but also enable disparity amplification.
- Why unresolved: The paper only explores one specific sampling strategy for non-synthetic data and does not investigate how different sampling approaches (e.g., stratified sampling based on different criteria, importance sampling) might affect the balance between model collapse and disparity amplification.
- What evidence would resolve it: Experiments comparing different non-synthetic data sampling strategies across multiple datasets, measuring their effects on model collapse rates, group representation, and fairness metrics. Analysis of how sampling strategies might be optimized to mitigate both model collapse and disparity amplification simultaneously.

## Limitations

- Empirical validation is limited to four datasets, which may not capture the full complexity of real-world scenarios
- The paper uses relatively simple CNNs and ResNets without exploring how more complex architectures might behave differently under MIDS
- The interaction effects between multiple co-occurring MIDS are explored but not fully characterized

## Confidence

- **High Confidence**: The theoretical framework connecting MIDS to fairness feedback loops and model collapse is well-grounded in existing literature. The core mechanism of bias propagation through generations is clearly articulated and supported by the empirical results.
- **Medium Confidence**: The effectiveness of algorithmic reparation (AR) interventions shows promise but requires further validation across diverse scenarios. The STAR algorithm's ability to mitigate MIDS effects appears promising but may depend heavily on implementation details and the specific nature of the bias being addressed.
- **Low Confidence**: The interaction effects between multiple co-occurring MIDS are explored but not fully characterized. The paper demonstrates that these interactions exist but doesn't provide a comprehensive framework for predicting or managing their combined effects.

## Next Checks

1. **Architecture Robustness Test**: Implement the same experiments using more complex model architectures (e.g., Vision Transformers, modern LLMs) to assess whether the MIDS effects and AR interventions generalize across different model families.

2. **Multi-MIDS Stress Test**: Design experiments that intentionally create scenarios with multiple simultaneous MIDS (e.g., combining model collapse, fairness feedback loops, and label noise) to better understand their interaction dynamics and the limits of AR interventions.

3. **Real-World Dataset Validation**: Apply the MIDS framework and AR interventions to real-world datasets with known bias issues (e.g., COMPAS, adult income dataset) to evaluate practical applicability and identify any limitations that may emerge in more complex, real-world scenarios.