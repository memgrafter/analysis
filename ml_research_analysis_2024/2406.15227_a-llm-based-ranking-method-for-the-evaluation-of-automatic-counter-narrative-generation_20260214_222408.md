---
ver: rpa2
title: A LLM-Based Ranking Method for the Evaluation of Automatic Counter-Narrative
  Generation
arxiv_id: '2406.15227'
source_url: https://arxiv.org/abs/2406.15227
tags:
- evaluation
- human
- generation
- metrics
- judgelm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to evaluate Counter Narrative
  (CN) generation using a Large Language Model (LLM) as an evaluator. The proposed
  method addresses the limitations of traditional automatic metrics, which often fail
  to capture the nuanced relationship between generated CNs and human perception.
---

# A LLM-Based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation

## Quick Facts
- arXiv ID: 2406.15227
- Source URL: https://arxiv.org/abs/2406.15227
- Authors: Irune Zubiaga; Aitor Soroa; Rodrigo Agerri
- Reference count: 13
- Primary result: Pairwise tournament evaluation achieves 0.88 Spearman correlation with human preference for CN generation ranking.

## Executive Summary
This paper introduces a novel approach to evaluate Counter Narrative (CN) generation using a Large Language Model (LLM) as an evaluator. The proposed method addresses the limitations of traditional automatic metrics, which often fail to capture the nuanced relationship between generated CNs and human perception. By comparing generated CNs pairwise in a tournament-style format, the study establishes a model ranking pipeline that achieves a high correlation with human preference, with a Spearman's rank correlation coefficient (ρ) of 0.88. The research also explores the potential of LLMs as zero-shot CN generators, comparing chat, instruct, and base models to determine the optimal choice for the task.

## Method Summary
The study employs a pairwise tournament evaluation method where generated CNs are compared head-to-head by JudgeLM, a Large Language Model acting as an evaluator. Points are awarded for wins in these matchups, creating a ranked list of models. The method uses CONAN and MT-CONAN datasets containing hate speech (HS) and counter-narrative (CN) pairs. Four LLMs (Mistral, Mistral-Instruct, Zephyr, and Llama-2-Chat) are evaluated in both zero-shot and fine-tuned settings. The pipeline includes CN generation, pairwise tournament evaluation with JudgeLM, and correlation analysis with manual human judgments to establish model rankings.

## Key Results
- Pairwise tournament evaluation achieves a Spearman's rank correlation coefficient (ρ) of 0.88 with human preference.
- Chat-aligned models outperform base and instruction-tuned models in zero-shot CN generation.
- Traditional automatic metrics (BLEU, ROUGE-L, BERTScore) show poor correlation with human judgment, with the highest ρ being 0.50 for BERTScore.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pairwise tournament evaluation decomposes a subjective multi-dimensional judgment task into simpler binary decisions, improving agreement and efficiency.
- Mechanism: Instead of evaluating all models against a reference, each generated CN is pitted against another in head-to-head matchups judged by an LLM. Points are awarded for wins, creating a ranked list.
- Core assumption: LLMs can reliably compare two CN outputs and determine which is more effective in the context of the HS.
- Evidence anchors:
  - [abstract] "comparing generated CNs pairwise in a tournament-style format, we establish a model ranking pipeline that achieves a correlation of 0.88 with human preference."
  - [section] "By comparing generated CNs pairwise in a tournament-style format, we establish a model ranking pipeline that achieves a correlation of 0.88 with human preference."
- Break condition: If the Judge Model's scoring function is not well-calibrated or if the prompt does not give enough context about HS-CN effectiveness, the binary comparisons may not reflect human preference.

### Mechanism 2
- Claim: Chat-aligned LLMs outperform base and instruction-tuned models in zero-shot CN generation due to safety fine-tuning and instruction-following capability.
- Mechanism: Chat models are trained to understand instructions and avoid harmful content, making them better suited to generate CNs that are coherent, relevant, and safe.
- Core assumption: Safety fine-tuning in chat models translates into better performance on CN generation tasks, even without domain-specific training.
- Evidence anchors:
  - [abstract] "We conclude that chat-aligned models in zero-shot are the best option for carrying out the task, provided they do not refuse to generate an answer due to security concerns."
  - [section] "Chat-aligned models exhibit superior performance, followed by the instruction-tuned model, while the base model demonstrates the lowest performance."
- Break condition: If the chat model refuses to generate a CN due to safety filters, the zero-shot performance advantage is lost.

### Mechanism 3
- Claim: Traditional automatic metrics (BLEU, ROUGE-L, BERTScore) have poor correlation with human judgment for CN generation due to the open-ended and creative nature of the task.
- Mechanism: These metrics rely on n-gram overlap or embedding similarity with references, which do not capture the nuanced effectiveness of a CN in countering HS.
- Core assumption: CN evaluation requires context-sensitive judgment that reference-based metrics cannot provide.
- Evidence anchors:
  - [abstract] "traditional automatic metrics correlate poorly with human judgements and fail to capture the nuanced relationship between generated CNs and human perception."
  - [section] "traditional metrics correlate poorly with human preference, with the highest ρ being the 0.50 obtained by BERTScore."
- Break condition: If a reference-based metric is specifically tuned for the CN task and trained on human judgments, it may improve correlation, but current metrics are not suitable.

## Foundational Learning

- Concept: Spearman's rank correlation coefficient (ρ)
  - Why needed here: Used to measure how well the ranking of models from automatic evaluation aligns with human preference ranking.
  - Quick check question: If two models swap ranks between automatic and human evaluation, does that always decrease Spearman's ρ?
- Concept: Zero-shot vs fine-tuned generation
  - Why needed here: The paper compares model performance in zero-shot setting (no training data) vs after fine-tuning on CN data to see if adaptation helps.
  - Quick check question: What is the main difference in behavior between a base model and a chat-aligned model in zero-shot CN generation?
- Concept: Inter-annotator agreement (IAA)
  - Why needed here: Used to assess reliability of manual evaluations; Kappa values indicate consistency among annotators.
  - Quick check question: What does a Kappa value of 0.42 indicate about the level of agreement between annotators?

## Architecture Onboarding

- Component map:
  - Data ingestion: CONAN and MT-CONAN datasets (HS-CN pairs)
  - Model generation: 4 LLMs (Mistral, Mistral-Instruct, Zephyr, Llama-2-Chat) in 2 settings (zero-shot, fine-tuned)
  - Evaluation pipeline: JudgeLM for pairwise comparisons + manual annotation for ground truth
  - Analysis: Correlation calculation, ranking generation, feature-wise evaluation
- Critical path:
  1. Load datasets and split into train/val/test
  2. Generate CNs using all models in both settings
  3. Run pairwise tournaments with JudgeLM
  4. Aggregate scores to produce rankings
  5. Compare automatic rankings with manual human judgments
- Design tradeoffs:
  - Using pairwise comparisons increases computational cost but improves correlation with human judgment vs reference-based metrics.
  - Zero-shot setting avoids data annotation costs but may be limited by model safety filters.
  - Fine-tuning improves base model performance but can degrade chat/instruction model performance if factual accuracy is not controlled.
- Failure signatures:
  - Low correlation between JudgeLM and human rankings: Indicates prompt or model calibration issues.
  - Chat models refusing to generate CNs: Indicates overly strict safety filters.
  - Fine-tuned models producing factually incorrect CNs: Indicates copying structure without understanding.
- First 3 experiments:
  1. Run JudgeLM pairwise evaluation on a small subset of tournaments and manually verify outcomes to ensure scoring logic is correct.
  2. Generate CNs with all models in zero-shot setting and inspect for refusal or incoherence to understand safety filter impact.
  3. Fine-tune one model on a small clean subset of data and compare output quality vs zero-shot to isolate fine-tuning effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of including false or misleading information in Counter-Narratives (CNs) on their effectiveness?
- Basis in paper: [explicit] The paper mentions that responses containing false information should be penalized, but it doesn't explore the extent to which this affects CN effectiveness.
- Why unresolved: The paper focuses on the correlation of automatic metrics with human preference but does not delve into the specific impact of factual accuracy on CN effectiveness.
- What evidence would resolve it: Conduct a study comparing the effectiveness of CNs with and without factual inaccuracies, measuring user perception and engagement metrics.

### Open Question 2
- Question: How does the size of the dataset (e.g., number of unique hate speech instances) affect the performance of fine-tuned models?
- Basis in paper: [explicit] The paper notes that the dataset used has significant repetition of certain hate speech instances, which might affect model performance.
- Why unresolved: While the paper suggests that reducing duplications could lead to more consistent learning outcomes, it does not provide a comprehensive analysis of the impact of dataset size and diversity on model performance.
- What evidence would resolve it: Perform experiments with datasets of varying sizes and levels of repetition, comparing the performance of fine-tuned models to determine the optimal dataset characteristics.

### Open Question 3
- Question: Can Retrieval-Augmented Generation (RAG) improve the truthfulness and overall effectiveness of CNs?
- Basis in paper: [inferred] The paper mentions that RAG could be explored to address the truthfulness issue, but it does not provide empirical evidence of its effectiveness.
- Why unresolved: The potential of RAG to enhance the factual accuracy of CNs is hypothesized but not tested, leaving its actual impact on CN generation unexplored.
- What evidence would resolve it: Implement RAG in the CN generation process and compare the truthfulness and user preference of CNs generated with and without RAG.

## Limitations
- The study relies on JudgeLM's calibration and does not detail its internal consistency or sensitivity to prompt variations.
- Only four LLMs were tested in zero-shot settings, leaving open the question of whether other architectures could perform better.
- Safety filters in chat models introduce variability through refusals that could skew results.
- Limited manual evaluation (720 tournaments vs. 82,008 automatic) raises questions about the completeness of the human preference ground truth.

## Confidence

- **High confidence** in the claim that pairwise tournament evaluation improves correlation with human judgment over traditional metrics. The strong Spearman correlation (0.88) and clear contrast with low metric correlations (max 0.50 for BERTScore) support this.
- **Medium confidence** in the claim that chat-aligned models are the best zero-shot option for CN generation. While the performance ranking is clear, the impact of refusal rates and the lack of fine-tuning data comparisons leave room for variability.
- **Low confidence** in the generalizability of JudgeLM's effectiveness across different CN datasets or tasks. The paper does not test JudgeLM on alternative datasets or tasks, nor does it report JudgeLM's own inter-annotator agreement or robustness to prompt changes.

## Next Checks

1. **JudgeLM Robustness Test**: Run the same pairwise tournament evaluation with multiple JudgeLM prompt variations and check for rank stability. If rankings change significantly, the evaluation pipeline may be sensitive to prompt engineering rather than true model capability.
2. **Safety Filter Impact Analysis**: Quantify the refusal rate of chat models in zero-shot CN generation and re-run the tournament excluding refused cases. Compare the resulting rankings to determine if refusals meaningfully affect the overall performance assessment.
3. **Cross-Dataset Validation**: Apply the full pipeline (CN generation, JudgeLM evaluation, and correlation with human judgment) to an independent CN dataset. A drop in correlation would indicate limited generalizability and the need for dataset-specific calibration.