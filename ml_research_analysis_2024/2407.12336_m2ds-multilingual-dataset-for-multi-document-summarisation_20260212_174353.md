---
ver: rpa2
title: 'M2DS: Multilingual Dataset for Multi-document Summarisation'
arxiv_id: '2407.12336'
source_url: https://arxiv.org/abs/2407.12336
tags:
- dataset
- multilingual
- datasets
- articles
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces M2DS, a pioneering multilingual dataset for
  multi-document summarization (MDS) covering English, Japanese, Korean, Tamil, and
  Sinhala. The dataset addresses the gap in multilingual MDS resources by compiling
  180,000 documents across 51,500 clusters from BBC articles (2010-2023).
---

# M2DS: Multilingual Dataset for Multi-document Summarisation

## Quick Facts
- arXiv ID: 2407.12336
- Source URL: https://arxiv.org/abs/2407.12336
- Reference count: 40
- Primary result: Introduces M2DS, a multilingual dataset for multi-document summarization covering English, Japanese, Korean, Tamil, and Sinhala

## Executive Summary
This study introduces M2DS, a pioneering multilingual dataset for multi-document summarization (MDS) covering five languages with 180,000 documents across 51,500 clusters from BBC articles. The dataset addresses the gap in multilingual MDS resources and demonstrates that state-of-the-art MDS models exhibit significant performance drops when applied to M2DS compared to English-centric datasets. Experiments show Llama 2 7B outperforms other models across languages while PRIMERA excels in English, highlighting the importance of task-specific models and transfer learning approaches for multilingual MDS.

## Method Summary
The study created M2DS by compiling documents from the M3LS dataset and BBC articles (2010-2023), forming clusters of related documents across five languages. Models were evaluated using ROUGE scores (R-1, R-2, R-L) after fine-tuning state-of-the-art MDS models including PRIMERA, PEGASUS, and LED. The dataset was split into 90-5-5 for non-English languages and 80-10-10 for English, with manual verification of sample clusters to ensure quality. Baseline comparisons included LEAD-3, RANDOM, CENTROID, and zero-shot Llama 2 performance.

## Key Results
- State-of-the-art MDS models show performance drops on M2DS compared to English-centric datasets
- Llama 2 7B outperforms other models across languages, demonstrating strong zero-shot capabilities
- PRIMERA excels in English but requires fine-tuning for optimal performance
- LEAD-3 baseline performs poorly across languages, indicating dataset quality exceeds TAC/DUC standards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset's multilingual nature reveals performance gaps in existing models that are masked in English-only evaluations.
- Mechanism: By introducing diverse linguistic structures and resource disparities, models trained on English-centric data struggle to generalize, leading to measurable performance drops when evaluated on M2DS.
- Core assumption: Linguistic diversity introduces unique challenges that cannot be captured in English-only datasets.
- Evidence anchors:
  - [abstract]: "Experiments show that state-of-the-art MDS models exhibit performance drops when applied to M2DS compared to English-centric datasets, highlighting the dataset's unique challenges."
  - [section]: "A noteworthy observation is the lower scores in LEAD-3, which extracts only the first three sentences as the summary. This suggests that our dataset exhibits better quality, addressing the issues found in TAC/DUC datasets..."
  - [corpus]: Weak evidence - corpus provides related work titles but no direct performance metrics across languages.
- Break condition: If a multilingual model shows no performance difference across languages, it would indicate the dataset lacks true linguistic diversity or the model has effectively generalized.

### Mechanism 2
- Claim: Llama 2 7B's strong performance across languages indicates its robustness to linguistic diversity without fine-tuning.
- Mechanism: As a large, pre-trained model with broad language coverage, Llama 2 can leverage transfer learning to maintain performance across multiple languages despite domain shifts.
- Core assumption: Large language models trained on diverse data can generalize across languages without language-specific fine-tuning.
- Evidence anchors:
  - [abstract]: "Baseline results demonstrate that Llama 2 7B outperforms other models, while PRIMERA excels in English."
  - [section]: "Furthermore, it is essential to emphasise the scalability of models like Llama 2, indicating their potential for handling larger datasets and their adaptability across various domains."
  - [corpus]: Weak evidence - corpus provides related work but no direct comparison of Llama 2 performance across languages.
- Break condition: If Llama 2 performance drops significantly on low-resource languages, it would indicate limitations in its transfer learning capabilities.

### Mechanism 3
- Claim: Task-specific models like PRIMERA may outperform general LLMs for MDS despite requiring fine-tuning.
- Mechanism: PRIMERA's design for MDS with features like document separator tokens and masked sentence strategies makes it more effective for the specific task, even if it requires more resources to adapt.
- Core assumption: Specialized architectures designed for specific tasks can outperform general models when properly adapted.
- Evidence anchors:
  - [abstract]: "PRIMERA excels in English" and "Llama 2 7B outperforms other models"
  - [section]: "A noteworthy observation is the lower scores in LEAD-3, which extracts only the first three sentences as the summary. This suggests that our dataset exhibits better quality..."
  - [corpus]: Weak evidence - corpus provides related work but no direct comparison of task-specific vs general models.
- Break condition: If PRIMERA performance does not improve significantly with fine-tuning, it would indicate the model's design is not effectively capturing MDS-specific features.

## Foundational Learning

- Concept: Multi-document summarization (MDS) fundamentals
  - Why needed here: Understanding how MDS differs from single-document summarization is crucial for appreciating the dataset's unique challenges
  - Quick check question: What are the key differences between MDS and SDS in terms of information overlap and coherence requirements?

- Concept: Cross-lingual transfer learning
  - Why needed here: The dataset's multilingual nature requires understanding how models can transfer knowledge across languages
  - Quick check question: How does transfer learning help models perform on languages they weren't explicitly trained on?

- Concept: Evaluation metrics for summarization (ROUGE)
  - Why needed here: Understanding how summarization quality is measured across different languages and domains
  - Quick check question: Why might ROUGE scores vary significantly between languages even for the same model?

## Architecture Onboarding

- Component map:
  M3LS dataset extraction -> BBC article collection -> Cluster formation -> Dataset formatting -> Model training and evaluation

- Critical path:
  1. Data extraction from M3LS and BBC sources
  2. Cluster formation by linking related articles
  3. Quality verification through manual sampling
  4. Model training and evaluation across languages
  5. Baseline comparison and analysis

- Design tradeoffs:
  - Dataset size vs. linguistic diversity: Larger datasets might sacrifice coverage of low-resource languages
  - Model complexity vs. performance: Simple baselines like LEAD-3 reveal dataset quality issues
  - Zero-shot vs. fine-tuned approaches: Llama 2 shows strong zero-shot performance while PRIMERA requires fine-tuning

- Failure signatures:
  - Low diversity in cluster sizes across languages
  - High similarity between model outputs and LEAD-3 baselines
  - Inconsistent performance drops across different language pairs
  - Manual verification reveals quality issues in extracted summaries

- First 3 experiments:
  1. Evaluate LEAD-3 baseline performance across all languages to establish dataset quality
  2. Compare zero-shot Llama 2 performance across languages to identify transfer learning effectiveness
  3. Fine-tune PRIMERA on English subset only, then test on other languages to measure cross-lingual generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of state-of-the-art MDS models on M2DS compare to their performance on existing English-centric MDS datasets, and what specific linguistic challenges contribute to any observed performance gaps?
- Basis in paper: [explicit] The paper states that "state-of-the-art MDS models exhibit performance drops when applied to M2DS compared to English-centric datasets" and suggests this could be due to models struggling to capture language-specific information.
- Why unresolved: While the paper demonstrates a performance drop, it does not conduct a detailed linguistic analysis to identify which specific aspects of each language (e.g., syntax, morphology, discourse structure) are most challenging for current models.
- What evidence would resolve it: A systematic error analysis comparing model predictions across languages, identifying patterns in what types of errors occur most frequently in each language, and correlating these with specific linguistic features.

### Open Question 2
- Question: To what extent can transfer learning from English-centric MDS models improve performance on M2DS, and what are the most effective strategies for adapting models across linguistic boundaries?
- Basis in paper: [explicit] The paper suggests "exploring transfer learning for multilingual MDS" as a future direction and notes that "understanding the impact of dataset quality on model evaluation is crucial."
- Why unresolved: The paper only presents baseline results without investigating transfer learning approaches or systematically comparing different adaptation strategies (e.g., fine-tuning, adapter layers, prompt engineering) across the five languages.
- What evidence would resolve it: Comparative experiments evaluating various transfer learning approaches, including zero-shot, few-shot, and fine-tuning strategies, with detailed analysis of which approaches work best for different language pairs.

### Open Question 3
- Question: How does the quality of M2DS compare to existing MDS datasets, and what specific curation practices could further improve its reliability and utility for training MDS models?
- Basis in paper: [explicit] The paper mentions manual verification of sample clusters and notes that "our dataset exhibits better quality, addressing the issues found in TAC/DUC datasets," but does not provide a comprehensive quality assessment.
- Why unresolved: While the paper claims higher quality based on low LEAD-3 scores, it lacks systematic evaluation of summary quality, inter-annotator agreement metrics, or comparison with other datasets using standardized quality measures.
- What evidence would resolve it: Comprehensive quality assessment including human evaluation of summaries, inter-annotator agreement studies, and comparison with established quality metrics across multiple MDS datasets.

## Limitations

- Limited manual verification with only 10 clusters across multiple languages, potentially insufficient for quality assessment
- CENTROID baseline implementation details unspecified, making baseline comparison less reliable
- Performance analysis lacks detailed cross-linguistic ablation study to identify specific language challenges

## Confidence

**High Confidence**: The dataset's creation methodology and basic experimental setup are well-documented and reproducible. The observation that LEAD-3 baseline performs poorly across languages provides strong evidence of dataset quality.

**Medium Confidence**: The comparative performance analysis between models (Llama 2 vs PRIMERA) is reliable for English but less certain for other languages due to limited fine-tuning data and evaluation depth.

**Low Confidence**: Claims about the specific mechanisms driving performance differences across languages lack sufficient empirical support, particularly regarding why certain models struggle more with specific language pairs.

## Next Checks

1. **Expanded manual verification**: Conduct comprehensive manual quality assessment of at least 50 additional clusters across all five languages to establish statistical significance of dataset quality claims.

2. **Cross-linguistic ablation study**: Systematically remove linguistic features (e.g., script complexity, word order) from the evaluation pipeline to isolate which aspects most impact model performance across languages.

3. **Fine-tuning resource analysis**: Test PRIMERA and other models with varying amounts of fine-tuning data per language to establish the relationship between training resources and cross-lingual generalization performance.