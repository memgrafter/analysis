---
ver: rpa2
title: 'Dissecting the Ullman Variations with a SCALPEL: Why do LLMs fail at Trivial
  Alterations to the False Belief Task?'
arxiv_id: '2406.14737'
source_url: https://arxiv.org/abs/2406.14737
tags:
- llms
- container
- modification
- label
- recognize
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates why large language models (LLMs) fail at
  modified Theory of Mind tasks using a method called SCALPEL, which incrementally
  modifies stimuli to test specific hypotheses about LLM failures. Applying this method
  to the transparent-access variation of the unexpected contents task, the authors
  found that LLMs perform poorly primarily because they fail to make essential common-sense
  inferences, such as recognizing that seeing through a transparent container implies
  knowing its contents.
---

# Dissecting the Ullman Variations with a SCALPEL: Why do LLMs fail at Trivial Alterations to the False Belief Task?

## Quick Facts
- arXiv ID: 2406.14737
- Source URL: https://arxiv.org/abs/2406.14737
- Authors: Zhiqiang Pi; Annapurna Vadaparty; Benjamin K. Bergen; Cameron R. Jones
- Reference count: 7
- One-line primary result: LLMs fail at modified Theory of Mind tasks primarily because they do not automatically infer that seeing through a transparent container means knowing its contents.

## Executive Summary
This study investigates why large language models (LLMs) fail at modified Theory of Mind tasks using a method called SCALPEL, which incrementally modifies stimuli to test specific hypotheses about LLM failures. Applying this method to the transparent-access variation of the unexpected contents task, the authors found that LLMs perform poorly primarily because they fail to make essential common-sense inferences, such as recognizing that seeing through a transparent container implies knowing its contents. Explicitly stating these inferences significantly improved LLM performance, with GPT-4 achieving 89.64% accuracy when the recognition of contents was made explicit. The results suggest that while LLMs do more than pattern matching, they still lack robust human-like Theory of Mind capabilities.

## Method Summary
The study uses a zero-shot prompting approach with temperature=1, sampling 20 responses per prompt to estimate error rates. The method incrementally modifies Theory of Mind task prompts to isolate specific missing inferences, then applies mixed-effects logistic regression to test the significance of each modification. The dataset comes from the Shapira et al. (2023) ADVersarial CommonSense with False Belief collection, modified with seven variations targeting different aspects of the transparent-access scenario.

## Key Results
- LLMs show poor performance on transparent-access Theory of Mind tasks without explicit inference cues
- Explicitly stating that seeing through a transparent container means knowing its contents dramatically improves performance (GPT-4 from ~20% to ~90% accuracy)
- The improvement pattern suggests LLMs encode visual-physical relationships only when explicitly stated, not implicitly derived
- GPT-4 significantly outperforms GPT-3.5 on tasks requiring explicit inference about contents recognition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs fail at transparent-access ToM tasks primarily because they do not automatically infer that seeing through a transparent container means knowing its contents.
- Mechanism: The SCALPEL method incrementally makes this inference explicit in the prompt text, and when made explicit, LLM performance jumps dramatically (GPT-4 from ~20% to ~90% accuracy).
- Core assumption: LLMs encode visual-physical relationships (transparency → visibility → knowledge) only when explicitly stated, not implicitly derived.
- Evidence anchors:
  - [abstract] "LLMs often do poorly because they fail to make essential common-sense inferences, such as that seeing a transparent container implies recognizing its contents."
  - [section] "LLMs’ performance improved as we made explicit the inference that by reading the label of the container, a person will likely also look at the container, and thus at its contents."
  - [corpus] No direct evidence found; corpus is small and citation counts are zero.
- Break condition: If future models (post-training or fine-tuned) show implicit handling of transparency → visibility → knowledge mapping without explicit cueing, this mechanism would be invalidated.

### Mechanism 2
- Claim: LLMs lack robust encoding of the sequence "reading label → looking at container → recognizing contents," even when the container is transparent.
- Mechanism: SCALPEL adds explicit statements about looking at the container and recognizing its contents, which incrementally improves accuracy, indicating that the original encoding omitted this causal chain.
- Core assumption: The LLM’s internal representation of the scenario does not automatically include the perceptual-action-knowledge inference sequence.
- Evidence anchors:
  - [section] "Although this modification makes the models less likely to produce the incorrect answer, other inferences required to produce the correct response are still missing."
  - [section] "By contrast, LLMs’ performance improved as we made explicit the inference that by reading the label of the container, a person will likely also look at the container, and thus at its contents."
  - [corpus] No direct evidence found; corpus is small and citation counts are zero.
- Break condition: If future studies show that simpler cues (e.g., adding only "looks at" without "recognizes") yield the same performance gain, the recognition step may be unnecessary, breaking this specific mechanism.

### Mechanism 3
- Claim: LLMs rely on pattern matching for ToM tasks but do not possess a robust, generalizable Theory of Mind that includes implicit perceptual and mentalistic inferences.
- Mechanism: When adversarial modifications (like transparency) disrupt superficial patterns, performance drops unless the missing inference is made explicit; this shows LLMs do more than shallow heuristics but lack human-like ToM robustness.
- Core assumption: The improvement from explicit inference is due to missing implicit reasoning, not just exposure to similar training examples.
- Evidence anchors:
  - [abstract] "Our results suggest that it is unlikely that recent LLMs exploit solely superficial cues to solve false belief tasks."
  - [section] "Inference such as ‘looking at a transparent container implies recognizing its contents’... are arguably crucial parts of a Theory of Mind."
  - [corpus] No direct evidence found; corpus is small and citation counts are zero.
- Break condition: If LLMs trained on richer multimodal or physical reasoning data show robust ToM without explicit inference cues, this mechanism would be weakened.

## Foundational Learning

- Concept: Transparent container semantics
  - Why needed here: Understanding that transparency in language should map to visibility and knowledge in reasoning tasks; this mapping is not implicit for LLMs.
  - Quick check question: If a container is described as "transparent," should an LLM automatically infer that a character who looks at it knows its contents? (Answer: In this study, no—explicit cueing is required.)

- Concept: Theory of Mind (ToM) task structure
  - Why needed here: False-belief tasks require reasoning about what another agent knows or believes; LLM failures reveal gaps in encoding these mental states.
  - Quick check question: In the classic unexpected contents task, what does a correct answer require the LLM to infer about the character’s knowledge? (Answer: That the character has no access to the true contents and believes the label.)

- Concept: Adversarial prompt modification
  - Why needed here: SCALPEL uses minimal, targeted changes to isolate which inference a model fails to make; understanding this design is key to interpreting results.
  - Quick check question: Why does changing "transparent" to "see-through" not improve performance, but adding "recognizes what is inside" does? (Answer: The latter explicitly states the missing inference; the former does not.)

## Architecture Onboarding

- Component map: Prompt generator -> LLM interface -> Response evaluator -> Statistics module -> Hypothesis tracker
- Critical path:
  1. Load base unexpected contents scenario.
  2. Generate SCALPEL variations (see-through, see-inside, read look, look read, recognize content, visualize, recognize label).
  3. For each variation, sample 20 responses from LLM.
  4. Compute accuracy (exact match to correct answer).
  5. Run mixed-effects logistic regression to test significance of each modification.
  6. Analyze which modifications improve accuracy and by how much.
- Design tradeoffs:
  - Temperature=1 vs 0: Higher temperature gives richer error distribution but more variance; 0 gives deterministic outputs but less diagnostic power.
  - Number of samples: 20 per item balances statistical power with API cost.
  - Modification granularity: Too coarse loses diagnostic value; too fine may not isolate a single inference.
- Failure signatures:
  - No improvement after a modification → the targeted inference was not the failure point.
  - Performance below chance even after explicit cues → additional missing inferences or deeper reasoning deficits.
  - Inconsistent by-item results → some scenarios may be inherently harder or rely on unstated world knowledge.
- First 3 experiments:
  1. Test "transparent" → "see-through" modification to check if word-level transparency semantics matter.
  2. Add "looks at the container" after reading label to test perceptual-action inference.
  3. Add "recognizes what is inside" to test mental-state inference explicitly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific internal representations do LLMs use to encode physical properties like transparency, and how do these representations differ from human cognitive representations of the same properties?
- Basis in paper: [explicit] The paper discusses how explicitly stating that a container is transparent does not improve model performance, suggesting that LLMs may not have the same understanding of transparency as humans
- Why unresolved: The study focuses on behavioral outcomes rather than the internal mechanisms or representations that LLMs use to process concepts like transparency
- What evidence would resolve it: Neural network interpretability studies examining how LLMs encode and process physical properties, or controlled experiments comparing LLM performance with varying degrees of transparency descriptions

### Open Question 2
- Question: Why does GPT-4 significantly outperform GPT-3.5 on the recognize content modification, and what specific differences in their architectures or training contribute to this performance gap?
- Basis in paper: [explicit] The paper notes that while GPT-4 achieves ~90% accuracy with the recognize content modification, GPT-3.5 only reaches slightly above chance performance
- Why unresolved: The study observes the performance difference but does not investigate the underlying architectural or training-related reasons for this discrepancy
- What evidence would resolve it: Comparative analysis of the two models' attention mechanisms, training data differences, or systematic ablation studies isolating the components responsible for this performance gap

### Open Question 3
- Question: Do humans also show degraded performance on transparent-access variations of false belief tasks, and if so, what cognitive mechanisms underlie this degradation?
- Basis in paper: [explicit] The authors note that J. W. A. Strachan et al. (2024) reported that human participants were also relatively unsuccessful in solving the Unexpected Contents task with the transparent access modification
- Why unresolved: The paper raises this parallel but does not explore whether human performance degradation stems from the same cognitive limitations as LLM failures or different mechanisms entirely
- What evidence would resolve it: Direct comparison studies of human and LLM performance on systematically varied transparent-access scenarios, combined with cognitive modeling to identify shared or divergent failure patterns

## Limitations

- Low corpus coverage and citation count (average neighbor citation count of 0.0) limits generalizability
- Unknown prompt fidelity due to unspecified exact text of original prompts from Shapira et al. (2023)
- Statistical power constraints with only 20 samples per item and 9 items per modification

## Confidence

- **High confidence** in the descriptive finding that LLMs fail on transparent-access ToM tasks and that explicitly stating inferences improves performance
- **Medium confidence** in the mechanism that LLMs fail to automatically infer "transparency → visibility → knowledge" relationships
- **Low confidence** in broader claims about LLMs lacking robust human-like ToM capabilities

## Next Checks

1. Apply SCALPEL to other ToM task variations (e.g., unexpected transfer, deception tasks) to test whether the "explicit inference needed" pattern holds across different mental state reasoning challenges.

2. Re-run the same experiment with current GPT-4 and newer model versions to assess whether the explicit-inference requirement persists as models evolve, particularly after training updates.

3. Test whether other missing inferences (e.g., temporal reasoning, counterfactual thinking) show similar improvement when made explicit, or whether the transparency-knowledge mapping is uniquely problematic for LLMs.