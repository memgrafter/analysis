---
ver: rpa2
title: "T\xFCrk\xE7e Dil Modellerinin Performans Kar\u015F\u0131la\u015Ft\u0131rmas\u0131\
  \ Performance Comparison of Turkish Language Models"
arxiv_id: '2404.17010'
source_url: https://arxiv.org/abs/2404.17010
tags:
- veri
- stir
- arxiv
- kullan
- soru
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the lack of comprehensive performance comparison
  of open-source Turkish language models. The authors evaluate seven Turkish language
  models using contextual learning and question-answering datasets, conducting both
  automatic and human evaluations.
---

# Türkçe Dil Modellerinin Performans Karşılaştırması Performance Comparison of Turkish Language Models

## Quick Facts
- arXiv ID: 2404.17010
- Source URL: https://arxiv.org/abs/2404.17010
- Reference count: 0
- Models evaluated: 7 Turkish language models (Trendyol-chat, openchat 3.5, deepseek-chat, Trendyol-base, Llama-2, Mistral, mGPT)

## Executive Summary
This study addresses the lack of comprehensive performance comparison of open-source Turkish language models by evaluating seven models using both contextual learning and question-answering datasets. The research employs both automatic metrics (ROUGE scores, accuracy) and human evaluation (9 judges assessing 1200 responses) to provide a holistic assessment of model performance. The primary finding reveals that continuing pretraining on Turkish corpora before fine-tuning with instructional datasets is more effective for adapting multilingual models to Turkish question-answering tasks than fine-tuning alone.

## Method Summary
The study evaluates seven Turkish language models using a combined approach of automatic metrics and human evaluation. Automatic evaluation employs ROUGE-1, ROUGE-2, and ROUGE-L scores along with normalized accuracy metrics on Turkish question-answering datasets. Human evaluation involves 9 judges assessing 1200 model responses using an Elo scoring system, achieving high inter-annotator agreement (0.957). The models are tested on both contextual learning tasks (ARC, HellaSwag, TruthfulQA, MMLU) and question-answering tasks (1000 examples from existing dataset + 300 newly prepared examples). The research specifically compares the effectiveness of pretraining continuation versus fine-tuning alone for Turkish adaptation.

## Key Results
- Continuing pretraining before fine-tuning with instructional datasets is more successful in adapting multilingual models to Turkish for question-answering tasks
- In-context learning performances do not strongly correlate with question-answering performances
- Trendyol-chat model achieved the best overall performance, followed by openchat 3.5 and deepseek-chat
- Human evaluation achieved high inter-annotator agreement (average correlation 0.957) across 1200 responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuing pretraining before fine-tuning with instructional datasets is more successful in adapting multilingual models to Turkish for question-answering tasks.
- Mechanism: Pretraining continuation on Turkish corpora builds domain-specific linguistic knowledge that fine-tuning alone cannot achieve, leading to better question-answering performance.
- Core assumption: Turkish linguistic features require deeper integration into model weights than can be achieved through fine-tuning alone.
- Evidence anchors:
  - [abstract] "continuing pretraining before fine-tuning with instructional datasets is more successful in adapting multilingual models to Turkish"
  - [section] "Trendyol-chat model's... advantage over Trendyol-base model in voting and rouge scores leads to this conclusion"
- Break condition: If the pretraining corpus is too small or unrepresentative of Turkish linguistic patterns, the benefit disappears.

### Mechanism 2
- Claim: In-context learning performances do not strongly correlate with question-answering performances.
- Mechanism: Different cognitive processes are involved in in-context learning versus fine-tuned question answering, leading to divergent performance patterns.
- Core assumption: In-context learning and fine-tuned question answering tap into different model capabilities that can develop independently.
- Evidence anchors:
  - [abstract] "in-context learning performances do not much related to question-answering performances"
  - [section] "experiments show that models' in-context learning performances do not correlate with question-answering performances"
- Break condition: If the tasks used to measure both abilities are too similar, correlation might appear where none exists.

### Mechanism 3
- Claim: Human evaluation provides different insights than automatic metrics like ROUGE.
- Mechanism: Human judges capture nuanced aspects of answer quality (relevance, coherence, cultural appropriateness) that automated metrics miss.
- Core assumption: Human linguistic intuition captures quality dimensions that statistical metrics cannot fully represent.
- Evidence anchors:
  - [abstract] "human evaluation involved 9 judges who assessed 1200 model responses, achieving high inter-annotator agreement (average correlation 0.957)"
  - [section] "This human evaluation process... captures how models are evaluated not just based on accuracy rates or performance measured by automatic metrics, but also based on how humans perceive the nuances and subtleties of language use"
- Break condition: If human judges are not properly trained or if cultural bias affects their judgments, the evaluation may not be reliable.

## Foundational Learning

- Concept: Pretraining vs Fine-tuning distinction
  - Why needed here: The study compares models with different pretraining histories, so understanding how these training phases differ is crucial for interpreting results.
  - Quick check question: What is the key difference between pretraining and fine-tuning in terms of model parameter updates?

- Concept: Multilingual model adaptation
  - Why needed here: The study focuses on adapting multilingual models to Turkish, requiring understanding of how language-specific adaptation works.
  - Quick check question: Why might continuing pretraining on Turkish data be more effective than fine-tuning alone for Turkish adaptation?

- Concept: In-context learning vs fine-tuned learning
  - Why needed here: The study finds these abilities don't correlate, so understanding their different mechanisms is important.
  - Quick check question: How does in-context learning fundamentally differ from learning through fine-tuning?

## Architecture Onboarding

- Component map:
  - Data preparation: Turkish question-answering datasets (1000 examples + 300 new examples)
  - Model evaluation: Automatic metrics (ROUGE-1, ROUGE-2, ROUGE-L) + human evaluation
  - In-context learning evaluation: ARC, HellaSwag, TruthfulQA, MMLU benchmarks
  - Comparison framework: Elo scoring system for human evaluation

- Critical path:
  1. Prepare Turkish datasets for both question-answering and in-context learning
  2. Run all seven models on both datasets
  3. Compute automatic metrics for question-answering
  4. Conduct human evaluation with 9 judges on 1200 responses
  5. Analyze correlations between different evaluation methods

- Design tradeoffs:
  - Human evaluation provides nuanced insights but is expensive and potentially subjective
  - Automatic metrics are scalable but may miss important quality dimensions
  - Using both pretrained and fine-tuned models provides comparison but increases complexity

- Failure signatures:
  - Low inter-annotator agreement (below 0.8) suggests unclear evaluation criteria
  - Correlation between human and automatic metrics below 0.5 suggests metrics are measuring different things
  - In-context learning and question-answering performances showing strong correlation would contradict study findings

- First 3 experiments:
  1. Run a subset of models (2-3) on both datasets and compare their ROUGE scores to establish baseline performance
  2. Conduct pilot human evaluation with 2-3 judges on 50 responses to test evaluation criteria and inter-annotator agreement
  3. Test correlation between human and automatic metrics on the pilot data before full evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the correlation between in-context learning performance and question-answering performance vary across different model architectures (e.g., transformer-based vs. other architectures)?
- Basis in paper: [explicit] The paper states that in-context learning performances do not strongly correlate with question-answering performances, but doesn't explore architectural differences.
- Why unresolved: The study focuses on Turkish language models without comparing architectural variations, leaving the impact of architecture on the correlation unexplored.
- What evidence would resolve it: Comparative analysis of different model architectures' performance on both in-context learning and question-answering tasks.

### Open Question 2
- Question: What specific characteristics of Turkish language make it challenging for multilingual models, and how do these challenges impact model performance compared to other languages?
- Basis in paper: [inferred] The study adapts multilingual models to Turkish, suggesting language-specific challenges, but doesn't analyze the linguistic features affecting performance.
- Why unresolved: The paper focuses on performance comparison rather than linguistic analysis of Turkish-specific challenges for language models.
- What evidence would resolve it: Detailed linguistic analysis comparing Turkish with other languages and its impact on model performance metrics.

### Open Question 3
- Question: How does the size of the Turkish instruction dataset affect the performance of multilingual models compared to continuing pretraining before fine-tuning?
- Basis in paper: [explicit] The paper finds that continuing pretraining before fine-tuning is more successful, but doesn't explore the impact of dataset size.
- Why unresolved: The study uses a fixed dataset size without varying it to determine its impact on performance.
- What evidence would resolve it: Experiments with varying sizes of Turkish instruction datasets to compare performance with pretraining continuation approach.

### Open Question 4
- Question: How do the performance differences between models change when tested on domain-specific Turkish datasets versus general Turkish datasets?
- Basis in paper: [inferred] The study uses general Turkish datasets, suggesting potential domain-specific variations in performance that aren't explored.
- Why unresolved: The paper doesn't test models on domain-specific datasets, leaving the generalizability of findings uncertain.
- What evidence would resolve it: Comparative testing of models on both general and domain-specific Turkish datasets across various fields (e.g., legal, medical, technical).

## Limitations
- The study lacks transparency regarding dataset specifics, particularly the Turkish question-answering dataset construction and the 300 newly prepared examples
- Inter-annotator agreement of 0.957 is exceptionally high, raising questions about evaluation criteria potentially being too narrow
- The study does not report statistical significance testing for performance differences between models

## Confidence

**High Confidence**: The finding that Trendyol-chat outperforms other models in Turkish question-answering tasks is supported by both automatic metrics and human evaluation with strong inter-annotator agreement.

**Medium Confidence**: The claim that in-context learning performances do not correlate with question-answering performances requires more statistical analysis to establish.

**Low Confidence**: The generalizability of the human evaluation findings is uncertain due to the limited number of judges (9) and the specific cultural context of Turkish language use.

## Next Checks
1. Conduct t-tests or ANOVA on the performance differences between models across both automatic metrics and human evaluation scores to determine which differences are statistically significant rather than due to chance variation.

2. Calculate Pearson or Spearman correlation coefficients between in-context learning and question-answering performances, including confidence intervals and significance testing to quantify the strength and reliability of the observed lack of correlation.

3. Repeat the human evaluation with judges from different Turkish-speaking regions or cultural backgrounds to test whether the high inter-annotator agreement is due to consistent evaluation criteria or potential cultural bias in the assessment framework.