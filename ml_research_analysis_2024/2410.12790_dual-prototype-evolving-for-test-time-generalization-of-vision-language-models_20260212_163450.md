---
ver: rpa2
title: Dual Prototype Evolving for Test-Time Generalization of Vision-Language Models
arxiv_id: '2410.12790'
source_url: https://arxiv.org/abs/2410.12790
tags:
- test
- performance
- conference
- test-time
- prototypes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dual Prototype Evolving (DPE), a novel test-time
  adaptation method for vision-language models (VLMs) that addresses the limitations
  of existing approaches by leveraging multi-modal knowledge accumulation. Unlike
  previous methods that adapt VLMs from a single modality, DPE creates and evolves
  two sets of prototypes - textual and visual - to progressively capture more accurate
  multi-modal representations for target classes during test time.
---

# Dual Prototype Evolving for Test-Time Generalization of Vision-Language Models

## Quick Facts
- arXiv ID: 2410.12790
- Source URL: https://arxiv.org/abs/2410.12790
- Authors: Ce Zhang; Simon Stepputtis; Katia Sycara; Yaqi Xie
- Reference count: 40
- Key outcome: DPE achieves average improvement of 3.55% and 4.30% over TPT in natural distribution shifts and cross-dataset generalization scenarios respectively

## Executive Summary
This paper introduces Dual Prototype Evolving (DPE), a novel test-time adaptation method for vision-language models (VLMs) that addresses the limitations of existing approaches by leveraging multi-modal knowledge accumulation. Unlike previous methods that adapt VLMs from a single modality, DPE creates and evolves two sets of prototypes - textual and visual - to progressively capture more accurate multi-modal representations for target classes during test time. The method introduces learnable residuals for each test sample to align prototypes from both modalities and optimize them using entropy minimization and alignment losses.

Extensive experiments on 15 benchmark datasets demonstrate DPE's superior performance, achieving an average improvement of 3.55% and 4.30% over the state-of-the-art TPT method in natural distribution shifts and cross-dataset generalization scenarios, respectively. The method also exhibits competitive computational efficiency, being 5× faster than TPT and over 10× faster than DiffTPT. DPE's effectiveness is validated across diverse recognition tasks, consistently outperforming existing test-time adaptation approaches while maintaining efficient inference.

## Method Summary
DPE is a test-time adaptation method for vision-language models that creates and evolves two sets of prototypes (textual and visual) during inference. For each test sample, DPE encodes the image through the visual encoder and uses text prompts through the textual encoder, then computes prototype-based predictions. The method optimizes learnable residuals for each test sample using entropy minimization and alignment losses, then updates both prototype sets through cumulative averaging (textual) and priority queue updates (visual). This dual prototype approach allows DPE to capture task-specific knowledge from multi-modalities while maintaining computational efficiency by avoiding backpropagation through the textual encoder.

## Key Results
- DPE achieves 3.55% average improvement over TPT on natural distribution shifts
- DPE achieves 4.30% average improvement over TPT on cross-dataset generalization
- DPE is 5× faster than TPT and over 10× faster than DiffTPT in computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPE captures task-specific knowledge from multi-modalities by evolving two sets of prototypes (textual and visual) during test time.
- Mechanism: The method maintains online prototypes that are progressively updated with each test sample using cumulative average and priority queue strategies. The visual prototypes store high-confidence features in a priority queue sorted by entropy, while textual prototypes are updated via cumulative averaging.
- Core assumption: Task-specific knowledge can be accumulated over time through online prototype updates without access to ground truth labels.
- Evidence anchors:
  - [abstract] "we create and evolve two sets of prototypes--textual and visual--to progressively capture more accurate multi-modal representations for target classes during test time"
  - [section 3.2] "we design them to be updated online through a cumulative average with each individual sample Xtest in the test stream"
  - [corpus] Weak - related work mentions cache-based methods but doesn't validate multi-modal prototype evolution specifically
- Break condition: If entropy-based pseudo-labeling becomes unreliable, the visual prototype evolution would accumulate incorrect samples, degrading performance.

### Mechanism 2
- Claim: Learnable residuals for each test sample align prototypes from both modalities to promote consistent multi-modal representations.
- Mechanism: The method introduces residual parameters that optimize the textual and visual prototypes for each test sample using both entropy minimization and alignment losses. The alignment loss uses contrastive InfoNCE to bring same-class prototypes closer.
- Core assumption: Multi-modal consistency can be improved by explicitly enforcing alignment between textual and visual prototypes through residual optimization.
- Evidence anchors:
  - [abstract] "we introduce and optimize learnable residuals for each test sample to align the prototypes from both modalities"
  - [section 3.3] "we apply an additional constraint to align the multi-modal prototypes during optimization, explicitly enforcing consistent multi-modal representations between dual sets of prototypes"
  - [corpus] Weak - related work mentions multi-modal alignment but doesn't specifically validate residual-based prototype alignment
- Break condition: If the alignment loss weight λ is too high, it could over-prioritize prototype alignment at the expense of minimizing prediction entropy.

### Mechanism 3
- Claim: DPE achieves superior computational efficiency by only optimizing prototype residuals in embedding space without backpropagation through the textual encoder.
- Mechanism: Unlike TPT and DiffTPT which require gradient backpropagation through the text encoder, DPE updates prototypes directly in the embedding space, reducing computational overhead while maintaining performance.
- Core assumption: Optimizing prototype residuals in embedding space can achieve similar performance to full backpropagation while being more computationally efficient.
- Evidence anchors:
  - [abstract] "our DPE requires only the optimization of multi-modal prototypes in the embedding space during test time, eliminating the need to backpropagate gradients through the textual encoder"
  - [section 3.3] "our method directly updates the prototype sets in the embedding space"
  - [table 3] "Our proposed method shows improved computational efficiency compared to other prompt tuning methods, for example, 5× faster than TPT [53]"
- Break condition: If the embedding space representation becomes less discriminative for certain domains, direct prototype optimization might not capture necessary adaptation.

## Foundational Learning

- Concept: Entropy minimization for test-time adaptation
  - Why needed here: Forms the basis for self-supervised learning without labels, ensuring predictions become more confident over time
  - Quick check question: How does entropy minimization help in test-time adaptation when no ground truth labels are available?

- Concept: Multi-modal representation learning
  - Why needed here: VLMs like CLIP have separate textual and visual encoders that need to be aligned for effective adaptation
  - Quick check question: Why is it important to align textual and visual representations in vision-language models?

- Concept: Online learning and prototype-based classification
  - Why needed here: The method needs to adapt to new test samples without re-training, requiring incremental updates to learned representations
  - Quick check question: What are the advantages of using prototypes for classification versus updating the entire model during test time?

## Architecture Onboarding

- Component map:
  - Visual encoder (Ev) and Textual encoder (Et) from pre-trained CLIP
  - Dual prototype sets (textual t, visual v) maintained in embedding space
  - Priority queue for visual prototype evolution
  - Residual parameters (ˆt, ˆv) for each test sample
  - Entropy minimization and alignment loss functions

- Critical path:
  1. Encode test sample to get visual features fv = Ev(Xtest)
  2. Compute current prototype-based predictions PProto
  3. Optimize residual parameters using entropy and alignment losses
  4. Update prototypes using optimized residuals
  5. Evolve prototypes for next sample (cumulative average for textual, priority queue for visual)

- Design tradeoffs:
  - Memory vs performance: Larger priority queue M stores more diverse samples but increases memory usage
  - Computational efficiency vs adaptation quality: Single-step vs multi-step residual optimization
  - Entropy threshold τt controls when to update textual prototypes - higher threshold means fewer updates but potentially better quality

- Failure signatures:
  - Performance plateaus or degrades: Priority queue may be filled with incorrect pseudo-labels
  - High computational cost: Residual optimization parameters may be too large or require multiple update steps
  - Unstable predictions: Entropy threshold τt may be set too low, causing noisy updates

- First 3 experiments:
  1. Implement basic prototype evolution with only cumulative average updates for textual prototypes on a single dataset
  2. Add priority queue mechanism for visual prototypes and test performance improvement
  3. Integrate residual optimization with both entropy and alignment losses, validate against baseline methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DPE's performance scale with increasing queue size M beyond the tested range?
- Basis in paper: [explicit] The paper mentions that increasing M beyond 3 leads to a slight decrease in performance, but does not explore the upper bounds of this effect.
- Why unresolved: The paper only tests M up to 7, leaving open the question of how much larger queue sizes would impact performance and computational efficiency.
- What evidence would resolve it: A systematic study varying M from 3 to 50+ with corresponding performance and runtime measurements.

### Open Question 2
- Question: Would incorporating additional modalities beyond text and vision (e.g., audio or depth) further improve DPE's performance?
- Basis in paper: [inferred] The paper focuses on dual prototypes but doesn't explore whether extending to more modalities could enhance performance, especially for datasets where additional sensory information exists.
- Why unresolved: The authors didn't test multimodal extensions beyond vision and text, which could be valuable for certain domains like robotics or medical imaging.
- What evidence would resolve it: Experiments comparing DPE against a multimodal version incorporating depth or audio data on appropriate datasets.

### Open Question 3
- Question: How would DPE perform in continual learning scenarios where new classes are introduced over time?
- Basis in paper: [explicit] The paper demonstrates performance on fixed-class datasets but doesn't address scenarios where new target classes emerge during deployment.
- Why unresolved: The authors only tested on static class sets, leaving open how the prototype evolution mechanisms would handle class expansion.
- What evidence would resolve it: Experiments where new classes are introduced during test-time adaptation and performance is tracked across these transitions.

### Open Question 4
- Question: What is the theoretical upper bound of DPE's performance improvement compared to zero-shot CLIP?
- Basis in paper: [explicit] The paper reports average improvements of 3.55% and 4.30% over TPT, but doesn't establish the maximum possible gain or identify where performance plateaus.
- Why unresolved: The experiments show consistent improvements but don't explore the limit of what can be achieved through test-time adaptation alone.
- What evidence would resolve it: Systematic testing across increasingly diverse and challenging distribution shifts to identify where DPE's advantages diminish.

## Limitations
- Performance claims based on single CLIP variant (ViT-B/16) and limited text prompts, may not generalize to other architectures
- Evaluation focuses on accuracy metrics without extensive analysis of calibration or robustness to extreme distribution shifts
- Computational efficiency claims presented as relative improvements without absolute runtime measurements or memory usage analysis

## Confidence
- High confidence: The core mechanism of dual prototype evolution (textual and visual) is well-defined and theoretically sound, with clear mathematical formulations for prototype updates and residual optimization. The computational efficiency improvements are demonstrated through direct comparisons with baseline methods.
- Medium confidence: The superiority of DPE over existing test-time adaptation methods is supported by extensive experiments, but the results are limited to specific datasets and CLIP variants. The ablation studies provide good insights into the importance of different components, though some design choices (like entropy threshold values) lack comprehensive sensitivity analysis.
- Low confidence: The paper's claims about DPE's effectiveness across diverse recognition tasks are based on natural distribution shifts and cross-dataset generalization scenarios, but the method's performance on adversarial examples or extreme domain shifts is not evaluated. The impact of different text prompt engineering strategies on DPE's performance is also not thoroughly investigated.

## Next Checks
1. **Cross-architecture validation**: Evaluate DPE with different vision-language model architectures (e.g., CLIP with different backbones, ALIGN, FLAVA) to verify the method's generalizability beyond the specific CLIP variant used in the paper.

2. **Prompt sensitivity analysis**: Systematically test the impact of different text prompt engineering strategies on DPE's performance, including the number of prompts, prompt diversity, and prompt quality, to understand how prompt selection affects the method's effectiveness.

3. **Robustness testing**: Evaluate DPE's performance on adversarial examples, out-of-distribution samples with extreme shifts, and datasets with noisy labels to assess the method's robustness beyond the natural distribution shifts tested in the paper.