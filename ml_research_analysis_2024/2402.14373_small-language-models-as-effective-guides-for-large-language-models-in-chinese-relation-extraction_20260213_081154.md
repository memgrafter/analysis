---
ver: rpa2
title: Small Language Models as Effective Guides for Large Language Models in Chinese
  Relation Extraction
arxiv_id: '2402.14373'
source_url: https://arxiv.org/abs/2402.14373
tags:
- relation
- llms
- types
- slcolm
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the long-tail relation extraction problem
  by proposing a collaborative framework, SLCoLM, which combines small language models
  (SLMs) and large language models (LLMs). The framework employs a "Training-Guide-Predict"
  strategy where an SLM-based model is trained to extract head relations, and its
  predictions guide LLMs to handle long-tail relations.
---

# Small Language Models as Effective Guides for Large Language Models in Chinese Relation Extraction

## Quick Facts
- arXiv ID: 2402.14373
- Source URL: https://arxiv.org/abs/2402.14373
- Authors: Xuemei Tang; Jun Wang
- Reference count: 13
- Primary result: SLCoLM improves long-tail relation extraction performance, achieving 50.82% Macro F1 vs 44.05% for best SLM baseline

## Executive Summary
This paper addresses the long-tail relation extraction problem by proposing SLCoLM, a collaborative framework that combines small language models (SLMs) and large language models (LLMs). The framework employs a "Training-Guide-Predict" strategy where an SLM is trained on head relations and its predictions guide LLMs to handle long-tail relations. Experiments on an ancient Chinese relation extraction dataset demonstrate significant improvements in long-tail relation performance while also improving overall relation extraction accuracy.

## Method Summary
SLCoLM uses a staged approach: first training an SLM-based model on the full dataset to learn task-specific patterns, then using the SLM's predictions to guide LLMs through demonstration-based prompting. For each test sample, relevant relation type definitions are selectively added to the LLM prompt based on semantic similarity and trigger words. The SLM and LLM outputs are merged using a strategy that preserves LLM improvements on weak SLM relations while maintaining SLM's strong predictions. The framework was tested using Spert as the SLM and GPT-3.5/ERNIE-4.0 as LLMs on an ancient Chinese relation extraction dataset.

## Key Results
- SLCoLM achieves 50.82% Macro F1 score on long-tail relations, significantly outperforming the best SLM baseline (44.05%)
- Overall relation extraction accuracy improves from 53.68% (best SLM) to 56.83% (SLCoLM)
- The framework demonstrates that task-specific knowledge from SLMs can effectively guide LLMs in handling data-scarce long-tail relations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SLMs trained on head relations can effectively guide LLMs to handle long-tail relations through task knowledge transfer.
- Core assumption: Task-specific knowledge captured by SLMs on head relations is generalizable enough to guide LLM predictions on related but data-scarce long-tail relations.
- Evidence anchors: Abstract mentions "task-specific SLM framework acts as a guider, transfers task knowledge to the LLM"; section describes SLM predictions guiding LLM generation process.
- Break condition: If long-tail relations have fundamentally different patterns that don't overlap with head relations, the SLM's learned knowledge may not transfer effectively.

### Mechanism 2
- Claim: Selective candidate relation type definitions reduce prompt length and knowledge redundancy while maintaining effectiveness.
- Core assumption: Selected candidate relations accurately represent true relations in each sample, and LLMs can effectively use these focused definitions to improve predictions.
- Evidence anchors: Section mentions "selectively add only relevant candidate relations to the prompt for each sample"; definitions are added "only when required."
- Break condition: If candidate selection misses important relations or includes too many irrelevant ones, prompting effectiveness will degrade.

### Mechanism 3
- Claim: The "Training-Guide-Predict" strategy effectively combines SLM's task-specific knowledge with LLM's domain knowledge for improved long-tail relation extraction.
- Core assumption: The staged approach allows each model to contribute its strengths - SLMs excel at learning from abundant head relation data, while LLMs can leverage their broader knowledge to improve on data-scarce long-tail relations.
- Evidence anchors: Abstract describes "Training-Guide-Predict" strategy; section proposes mechanism leveraging SLMs for head categories and LLM's domain knowledge for long-tail categories.
- Break condition: If training phase doesn't capture sufficient task knowledge, or if guidance phase fails to effectively transfer this knowledge to LLMs, the strategy will underperform.

## Foundational Learning

- Concept: Long-tail distribution and its impact on model performance
  - Why needed here: Understanding why standard models struggle with long-tail relations is crucial for appreciating why the SLM-LLM collaboration approach is necessary
  - Quick check question: If a relation type has only 5 training examples versus 500, what challenges would a model face when trying to learn this relation?

- Concept: Prompt engineering and demonstration-based learning
  - Why needed here: The effectiveness of SLCoLM depends heavily on constructing effective prompts with demonstrations for LLMs
  - Quick check question: How would you structure a prompt to show an LLM both the input text and expected output for a relation extraction task?

- Concept: Knowledge transfer between models
  - Why needed here: SLCoLM relies on transferring task-specific knowledge from SLMs to LLMs, which requires understanding how different model architectures can complement each other
  - Quick check question: What are the key differences between how SLMs and LLMs process and utilize training data?

## Architecture Onboarding

- Component map: SLM-based model (Spert) -> Candidate relation selection -> LLM (GPT-3.5/ERNIE-4.0) -> Merge module -> Final predictions

- Critical path:
  1. Train SLM on full dataset
  2. Generate predictions for validation and test sets
  3. Select candidate relations for each test sample
  4. Construct prompts with demonstrations and SLM predictions
  5. Run LLM inference
  6. Merge SLM and LLM outputs

- Design tradeoffs:
  - SLM vs LLM balance: Using stronger SLMs could reduce need for LLM guidance but increase computational cost
  - Candidate selection method: Semantic similarity is more general but trigger words are more precise for domain-specific relations
  - Merge strategy: Mode 3 (replace worst SLM relations with LLM predictions) showed best results but may not generalize to all datasets

- Failure signatures:
  - LLM predictions consistently worse than SLM predictions
  - Candidate relation selection missing too many true relations
  - Merge strategy causing more harm than benefit
  - LLM input length constraints preventing effective prompting

- First 3 experiments:
  1. Baseline comparison: Run SLM alone vs LLM alone on the dataset to establish performance gaps
  2. Ablation study: Remove candidate relation definitions from prompts to measure their impact
  3. Merge strategy comparison: Test all 4 merge modes to identify the most effective approach

## Open Questions the Paper Calls Out
The paper acknowledges several open questions including how the framework generalizes to other relation extraction datasets beyond ancient Chinese, the optimal number of demonstration samples to include in prompts, and how the approach performs with open-source LLMs versus closed-source models like GPT-3.5 and ERNIE-4.0.

## Limitations
- Framework effectiveness depends heavily on SLM prediction quality and candidate relation selection relevance
- Mode 3 merge strategy, while showing best results, may not generalize well to different datasets or LLM models
- Assumes task-specific knowledge from head relations can guide long-tail extraction, but this hasn't been validated across diverse domains

## Confidence
- High confidence: Overall framework design and superiority over baseline SLM-only approaches on long-tail relations
- Medium confidence: Specific effectiveness of mode 3 merge strategy and candidate relation selection method
- Low confidence: Generalizability to other languages, domains, or when using different LLM models

## Next Checks
1. Conduct ablation studies to isolate contribution of each component (SLM predictions, candidate selection, merge strategy) to overall performance
2. Test framework with different LLM models and sizes to assess robustness across model families
3. Evaluate performance on datasets with varying degrees of long-tail distribution to understand framework's applicability range