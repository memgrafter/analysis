---
ver: rpa2
title: Malicious Internet Entity Detection Using Local Graph Inference
arxiv_id: '2408.03287'
source_url: https://arxiv.org/abs/2408.03287
tags:
- graph
- domains
- vertices
- each
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses malicious entity detection in large-scale
  networks using graph-based machine learning. The authors propose a new method called
  HMILnet, which combines the Hierarchical Multiple Instance Learning (HMIL) paradigm
  with local graph inference.
---

# Malicious Internet Entity Detection Using Local Graph Inference

## Quick Facts
- arXiv ID: 2408.03287
- Source URL: https://arxiv.org/abs/2408.03287
- Reference count: 40
- One-line primary result: Proposed HMILnet method outperforms state-of-the-art PTP algorithm with up to threefold accuracy improvement when using additional data

## Executive Summary
This paper introduces HMILnet, a novel method for detecting malicious internet entities using local graph inference. The approach combines Hierarchical Multiple Instance Learning with neural network architectures to achieve high expressivity while maintaining scalability in large-scale networks. By focusing on local neighborhood subgraphs and treating each vertex as an independent sample, HMILnet can efficiently process massive heterogeneous graphs representing internet entity interactions. The method demonstrates superior performance compared to existing approaches, particularly when leveraging additional relational data beyond basic domain-client relationships.

## Method Summary
The method constructs a heterogeneous graph where vertices represent different types of internet entities (domains, clients, binaries, IPs, TLS certificates, WHOIS registries) connected by various binary relations. HMILnet processes each vertex's neighborhood independently using a hierarchical multiple instance learning framework, where features are extracted at different levels of the hierarchy. The architecture uses importance sampling to handle vertices with extremely large neighborhoods, ensuring computational tractability while maintaining detection accuracy. The neural network architecture is trained using weighted binary cross-entropy loss to predict malicious activity probability for each entity.

## Key Results
- HMILnet achieves up to threefold accuracy improvement over PTP when using additional relational data
- The method demonstrates strong generalization capabilities to previously unseen malicious domains in the Grill test
- Local inference approach reduces computational complexity from O(n²) to O(n) compared to standard GNNs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method leverages local inference on streamlined neighborhood subgraphs to maintain scalability in large-scale networks.
- Mechanism: Instead of globally computing representations for all vertices as in standard GNNs, HMILnet computes each vertex's representation independently by focusing on its local neighborhood. This reduces computational complexity because only relevant portions of the graph are processed for each vertex.
- Core assumption: Malicious activities are localized in the graph, forming communities with sharp boundaries. This means that the relevant information for detecting malicious vertices can be found within a small local neighborhood.
- Evidence anchors:
  - [abstract]: "The scalability is achieved by pursuing local graph inference, i.e., classifying individual vertices and their neighborhood as independent samples."
  - [section]: "Because local inference in the HMILnet-based algorithm treats each central vertex as a single training 'example', it is more straightforward to construct minibatches for training models and to employ all additional sampling techniques for vertices like stratified sampling or prioritized sampling."
  - [corpus]: Weak or missing; corpus neighbors focus on general network intrusion detection and graph neural networks, not specifically on local inference mechanisms.
- Break condition: If malicious activities are not localized and require global context to detect, the local inference approach will fail to capture the necessary information.

### Mechanism 2
- Claim: The HMILnet architecture naturally models hierarchical relationships in the data, allowing for rich feature extraction from complex interactions.
- Mechanism: HMILnet extends Multiple Instance Learning (MIL) to hierarchical cases, where instances in bags can be bags themselves or Cartesian products of such spaces. This allows the model to process data with tree-structured hierarchies, such as XMLs or JSONs, which are common in internet communication and data exchange.
- Core assumption: The data has a hierarchical structure that can be exploited by the HMILnet architecture. For example, a domain's interactions with different types of entities (clients, binaries, IPs) can be represented as a tuple of HMIL subsamples.
- Evidence anchors:
  - [abstract]: "High expressivity of the method is achieved with a neural network architecture HMILnet that naturally models this type of data and provides theoretical guarantees."
  - [section]: "HMILnet can be viewed as a feature extractor optimized for given hierarchical data (in the sense of the above definition)."
  - [corpus]: Weak or missing; corpus neighbors do not discuss hierarchical modeling or HMILnet architecture.
- Break condition: If the data does not have a hierarchical structure, the HMILnet architecture may not provide any advantage over other graph neural network architectures.

### Mechanism 3
- Claim: The method uses importance sampling to handle vertices with extremely large neighborhoods, ensuring that informative malicious domains are not missed during aggregation.
- Mechanism: Instead of including the full neighborhood in the bag b(v), the method uses importance sampling to include a randomly selected subset. Positive (malicious) and negative domains are sampled with different probabilities to ensure that the output is an unbiased estimate of the true value.
- Core assumption: The denylist is incomplete, and most domains in the neighborhood are likely 'unknown' (non-denylisted). Uniform random sampling would likely miss any informative denylisted malicious domains.
- Evidence anchors:
  - [section]: "Importance sampling [59], [60] is used to approximate the output of aggregation function a on bag b(v)... Positive (malicious) and negative domains are sampled to a smaller bag with different probabilities..."
  - [corpus]: Weak or missing; corpus neighbors do not discuss importance sampling or handling large neighborhoods.
- Break condition: If the denylist is complete or if the distribution of malicious domains in the neighborhood is uniform, importance sampling may not provide any advantage over uniform sampling.

## Foundational Learning

- Concept: Multiple Instance Learning (MIL)
  - Why needed here: MIL is the foundation for HMILnet, which extends MIL to hierarchical cases. Understanding MIL is crucial for understanding how HMILnet processes data with tree-structured hierarchies.
  - Quick check question: In MIL, what is the difference between a bag and an instance?

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: HMILnet-based inference is compared to GNNs, and understanding the differences between these approaches is important for understanding the advantages of HMILnet. GNNs are a family of methods designed for processing graphs with features on vertices or edges.
  - Quick check question: What is the main difference between spectral and spatial approaches in GNNs?

- Concept: Probabilistic Threat Propagation (PTP)
  - Why needed here: PTP is the state-of-the-art algorithm for malicious domain detection that HMILnet-based inference is compared to. Understanding the limitations of PTP is important for understanding the advantages of HMILnet.
  - Quick check question: What is the main limitation of PTP that prevents it from utilizing features on edges and vertices other than scalars?

## Architecture Onboarding

- Component map:
  - Leaf layers (bf, ef): Process individual vertices and edges
  - Bag layers (a, g): Aggregate representations of vertices and edges in a neighborhood
  - Product layers (r): Combine representations from different types of entities
  - Importance sampling: Handle vertices with large neighborhoods

- Critical path:
  1. Extract streamlined neighborhood subgraph for a central vertex
  2. Create HMIL samples for each type of entity in the neighborhood
  3. Process HMIL samples through HMILnet model
  4. Combine outputs from different entity types using product layer
  5. Output binary classification logits

- Design tradeoffs:
  - Local inference vs. global inference: Local inference is more scalable but may miss global context
  - Importance sampling vs. full neighborhood: Importance sampling reduces computational complexity but may miss informative domains
  - Hierarchical modeling vs. flat modeling: Hierarchical modeling can capture complex relationships but may be more difficult to implement

- Failure signatures:
  - Low performance on vertices with large neighborhoods: May indicate that importance sampling is not effective
  - Low performance on vertices with complex hierarchical relationships: May indicate that HMILnet architecture is not effective
  - High computational complexity: May indicate that local inference is not effective or that the graph is too large

- First 3 experiments:
  1. Compare HMILnet-based inference to PTP on a single relation (domain-client) to validate that HMILnet outperforms PTP on data that PTP can use
  2. Compare HMILnet-based inference to PTP on all relations to validate that HMILnet can utilize additional data that PTP cannot
  3. Evaluate HMILnet-based inference on previously unseen malicious domains (Grill test) to validate that the method generalizes beyond simple memorization of known threats

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed HMILnet-based approach perform when using multi-step (T > 1) neighborhood subgraphs instead of the single-step (T = 1) approach evaluated in the experiments?
- Basis in paper: [explicit] The paper states "We expect this to be sufficient in practice since malicious activities tend to be localized [10] and a few-step neighborhood will already cover most of the graph (traversing through vertices of large degrees). We leave the exploration of multi-step versions for future work."
- Why unresolved: The paper only evaluates the single-step neighborhood case (T = 1) and mentions that multi-step versions are left for future work.
- What evidence would resolve it: Experimental results comparing the performance of HMILnet-based inference with T = 1, T = 2, and potentially higher values of T on the same datasets used in the paper.

### Open Question 2
- Question: How does the performance of the proposed HMILnet-based approach compare to other state-of-the-art graph neural network methods when all available data (eleven relations) are used?
- Basis in paper: [inferred] The paper compares its approach to PTP (Probabilistic Threat Propagation) and shows significant improvement when using all available data. However, it does not compare to other GNN methods that can handle heterogeneous graphs and multiple relations.
- Why unresolved: The paper focuses on comparing with PTP, which has limitations in handling multiple relations and rich features, but does not compare to other GNN methods that could potentially handle the same data.
- What evidence would resolve it: Experimental results comparing the performance of HMILnet-based inference to other state-of-the-art GNN methods (e.g., heterogeneous GNNs) on the same datasets and using all available relations.

### Open Question 3
- Question: How does the proposed HMILnet-based approach perform when additional features beyond the raw interaction observations are incorporated into the model?
- Basis in paper: [explicit] The paper states "By enriching the graph with features extracted from the graph, HMILnet can select informative features or their combination during training. This selection is done manually in [52], where the author searches for optimal construction of graph for PTP algorithm [7]." However, the paper does not explore this direction in its experiments.
- Why unresolved: The paper only uses features extracted from the raw interaction observations encoded in the input graphs, without incorporating any additional features from individual entities.
- What evidence would resolve it: Experimental results comparing the performance of HMILnet-based inference with and without additional features (e.g., domain registration information, WHOIS data) on the same datasets used in the paper.

## Limitations

- The evaluation relies on a proprietary dataset, making independent validation challenging and limiting reproducibility
- Implementation details for the HMILnet architecture, particularly for wider and deeper variants, are underspecified
- The importance sampling mechanism may introduce bias in domains with highly skewed neighborhood distributions

## Confidence

- **High Confidence**: The scalability improvements through local inference are well-supported by the theoretical analysis and the clear computational complexity reduction (O(n) vs O(n²) in standard GNNs)
- **Medium Confidence**: The performance improvements over PTP are demonstrated but limited by the proprietary nature of the evaluation dataset. The threefold accuracy improvement claim needs independent verification
- **Low Confidence**: The generalization capabilities to unseen entities are primarily supported by the Grill test, but the test's design and the novelty of test domains are not fully detailed

## Next Checks

1. **Dataset Replication**: Obtain or simulate a comparable dataset with heterogeneous network interactions and a denylist to independently verify the performance claims against PTP
2. **Architecture Implementation**: Implement the HMILnet architecture with varying depths and widths to confirm the reported performance across different model configurations
3. **Importance Sampling Validation**: Conduct ablation studies to quantify the impact of importance sampling on performance, particularly in domains with large and heterogeneous neighborhoods