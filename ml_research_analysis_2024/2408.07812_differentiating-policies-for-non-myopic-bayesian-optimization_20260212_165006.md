---
ver: rpa2
title: Differentiating Policies for Non-Myopic Bayesian Optimization
arxiv_id: '2408.07812'
source_url: https://arxiv.org/abs/2408.07812
tags:
- acquisition
- optimization
- function
- rollout
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational challenges of non-myopic
  Bayesian optimization by developing efficient methods for estimating and differentiating
  rollout acquisition functions. The key insight is formulating non-myopic BO as a
  finite-horizon Markov decision process and using rollout methods with parameterized
  policies as a tractable approximation.
---

# Differentiating Policies for Non-Myopic Bayesian Optimization

## Quick Facts
- arXiv ID: 2408.07812
- Source URL: https://arxiv.org/abs/2408.07812
- Authors: Darian Nwankwo; David Bindel
- Reference count: 40
- Primary result: Non-myopic Bayesian optimization methods with rollout acquisition functions outperform myopic methods by 25-85% on synthetic test functions

## Executive Summary
This paper addresses the computational challenges of non-myopic Bayesian optimization by developing efficient methods for estimating and differentiating rollout acquisition functions. The key insight is formulating non-myopic BO as a finite-horizon Markov decision process and using rollout methods with parameterized policies as a tractable approximation. The authors introduce several technical innovations including smart linear algebra techniques using Schur's complement, quasi-Monte Carlo integration, common random numbers, and control variates to make the approach practical and computationally feasible.

## Method Summary
The method formulates non-myopic Bayesian optimization as a finite-horizon MDP where states represent the GP belief state and actions represent sample locations. Rollout acquisition functions are estimated using quasi-Monte Carlo integration with low-discrepancy sequences, and their gradients are computed for stochastic optimization of the sampling policy. Key innovations include efficient GP updates using Schur's complement, variance reduction through common random numbers and control variates using 1-step expected improvement, and full gradient derivations for policy optimization. The approach is tested on 15 synthetic test functions using a Matérn 5/2 ARD kernel with maximum likelihood hyperparameter learning.

## Key Results
- Non-myopic strategies consistently outperform myopic ones with 25-85% improvement in objective function reduction
- Variance reduction techniques dramatically decrease required Monte Carlo samples
- Smart linear algebra using Schur's complement eliminates redundant GP computations
- The method maintains reasonable computational costs while achieving superior optimization performance

## Why This Works (Mechanism)

### Mechanism 1
The rollout acquisition function can be efficiently differentiated by decomposing it into sample trajectory evaluation and gradient computation. The rollout acquisition function is expressed as an expectation over sample trajectories, where each trajectory represents a sequence of potential future states under the base policy. The gradient is computed by differentiating the minimum function value along each trajectory, which requires computing Jacobians of the inner optimization problems. The base policy (acquisition function) is differentiable and can be optimized using second-order methods.

### Mechanism 2
Variance reduction techniques dramatically decrease the number of Monte Carlo samples needed for rollout estimation. The method combines quasi-Monte Carlo integration using low-discrepancy sequences, common random numbers across neighboring estimates, and control variates using 1-step expected improvement. These techniques reduce both pointwise variance and covariance between estimates. The integrand has sufficient smoothness for quasi-Monte Carlo to be effective, and the control variate is highly correlated with the rollout acquisition function.

### Mechanism 3
Smart linear algebra using Schur's complement eliminates redundant computations in GP updates across rollout steps. Instead of naively updating covariance matrices and Cholesky factorizations at each rollout step, the method preallocates matrices and uses Schur's complement to update them efficiently. This reduces computational complexity from repeated O(n³) operations. The GP structure allows for incremental updates using matrix algebra techniques, and preallocation memory is available.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and dynamic programming
  - Why needed here: The paper frames non-myopic Bayesian optimization as an MDP where states represent the GP belief state and actions represent sample locations. Understanding MDPs is crucial for grasping the theoretical foundation.
  - Quick check question: In the MDP formulation, what represents the state space and what represents the action space?

- Concept: Gaussian Process regression and kernel methods
  - Why needed here: The entire Bayesian optimization framework relies on GPs to model the objective function and quantify uncertainty. Understanding GP inference, including mean and covariance updates, is essential.
  - Quick check question: How does the posterior mean and variance update when adding a new observation to the GP?

- Concept: Variance reduction techniques in Monte Carlo integration
  - Why needed here: The rollout acquisition function requires estimating high-dimensional integrals, and the paper's main efficiency gains come from variance reduction methods like quasi-Monte Carlo and control variates.
  - Quick check question: What is the key difference between Monte Carlo and quasi-Monte Carlo integration, and when does the latter provide advantages?

## Architecture Onboarding

- Component map: GP model (mean/covariance) -> Base policy optimization -> Trajectory generation -> Variance reduction (QMC/CRN/control variates) -> Rollout acquisition estimation -> Gradient computation -> Policy parameter update
- Critical path: The critical path is: GP state → base policy optimization → trajectory generation → variance reduction → rollout acquisition estimation → gradient computation → policy parameter update. Each component must complete before the next can begin.
- Design tradeoffs: The main tradeoff is between lookahead horizon length (h) and computational cost - longer horizons provide better policies but exponentially increase computation. The variance reduction techniques are essential to make longer horizons practical.
- Failure signatures: Common failures include: poor convergence of the inner optimization (base policy), high variance in rollout estimates (insufficient variance reduction), numerical instability in GP updates (Schur's complement issues), and optimization getting stuck in local optima (poor initialization or step size).
- First 3 experiments:
  1. Implement the 1-step rollout (h=1) with basic Monte Carlo integration to verify the framework works before adding complexity.
  2. Add quasi-Monte Carlo and measure the reduction in required samples for stable estimates.
  3. Implement the Schur's complement updates and benchmark against naive GP updates to verify computational savings.

## Open Questions the Paper Calls Out

- **Open Question 1**: How would incorporating a discount factor into the MDP formulation affect the performance of non-myopic BO strategies? The authors mention that their MDP model does not consider a discount factor, which may yield different behaviors.

- **Open Question 2**: Can the trajectory-based formulation for non-myopic acquisition functions be extended to other base acquisition functions beyond expected improvement? While the paper mentions the approach is base policy agnostic, it only demonstrates results using EI as the base policy.

- **Open Question 3**: What is the optimal horizon length for non-myopic BO strategies, and can it be adaptively determined during optimization? The authors note that a finite horizon should not be fixed for the entire iteration when comparing one-step against multi-step lookahead strategies.

## Limitations

- Computational complexity scales with rollout horizon length even with variance reduction techniques
- Method requires careful tuning of Monte Carlo sample size and may struggle with high-dimensional problems
- Reliance on second-order optimization for the base policy could fail for ill-conditioned problems

## Confidence

- **High confidence**: The core mechanism of using rollout methods with parameterized policies for non-myopic BO is sound and well-established in reinforcement learning. The variance reduction techniques are standard and well-validated.
- **Medium confidence**: The specific implementation details, particularly the Schur's complement updates and the full gradient derivations, appear correct but would benefit from code verification. The empirical results showing 25-85% improvement are promising but limited to synthetic functions.
- **Low confidence**: The scalability claims to higher-dimensional problems and the generalizability beyond synthetic test functions require further validation.

## Next Checks

1. Verify the Schur's complement implementation by comparing computational time and numerical accuracy against naive GP updates
2. Test the gradient computation by comparing analytical gradients against finite difference approximations
3. Implement a simple adaptive horizon selection mechanism and compare performance against fixed horizons on a subset of test functions