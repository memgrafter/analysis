---
ver: rpa2
title: 'debiaSAE: Benchmarking and Mitigating Vision-Language Model Bias'
arxiv_id: '2410.13146'
source_url: https://arxiv.org/abs/2410.13146
tags:
- gender
- bias
- dataset
- race
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study comprehensively evaluates demographic bias in vision-language
  models (VLMs) using five models and six datasets. It identifies limitations in current
  bias evaluation datasets, finding that portrait datasets like UTKFace and CelebA
  effectively measure bias, while scene-based datasets like PATA and VLStereoSet fail
  due to models guessing answers from text prompts without relying on images.
---

# debiaSAE: Benchmarking and Mitigating Vision-Language Model Bias

## Quick Facts
- arXiv ID: 2410.13146
- Source URL: https://arxiv.org/abs/2410.13146
- Reference count: 32
- Primary result: SAE-based steering method improves fairness by 5-15 points over baseline

## Executive Summary
This study evaluates demographic bias in five vision-language models across six datasets, identifying critical limitations in current bias evaluation methodologies. The research finds that portrait datasets like UTKFace and CelebA effectively measure bias, while scene-based datasets fail because models can guess answers from text prompts without using images. The authors introduce a novel Sparse Autoencoder (SAE)-based debiasing method that improves fairness metrics by 5-15 points. The study demonstrates that adversarial datasets are necessary to reveal true model biases and highlights the need for better-designed evaluation datasets.

## Method Summary
The researchers evaluate five VLMs (LLaVa v1.6 series, PaliGemma-2 series, CLIP L-224, CLIP L-336, Gemini 1.5 Flash) using six datasets including CelebA, PATA, UTKFace, VisoGender, VLStereoSet, and a newly introduced dataset. They assess model performance with and without images to determine visual reliance, test adversarial versions of datasets, and apply a Sparse Autoencoder-based steering method for debiasing. The SAE method uses pretrained GemmaScope features, hand-selecting 12 features related to fairness and gender equity, then applying steering through various methods (constant, conditional per input) with different scaling factors.

## Key Results
- Scene-based datasets like PATA and VLStereoSet fail as bias benchmarks because models can guess answers from text alone
- VisoGender dataset requires adversarial versions to reveal true biases, as models over-rely on textual information
- SAE-based steering improves fairness by 5-15 points over baseline on PaliGemma-2 models
- Portrait datasets (UTKFace, CelebA) remain effective for bias measurement while scene-based datasets need redesign

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Sparse Autoencoders can identify and modify latent features associated with fairness and gender equity in VLMs
- **Mechanism**: SAEs project hidden states to higher-dimensional sparse vectors where each dimension corresponds to interpretable concepts. By selecting features related to fairness and gender equity, the model's activations can be steered to reduce bias
- **Core assumption**: SAE features learned on LLMs transfer effectively to VLMs and capture meaningful concepts about fairness
- **Evidence anchors**:
  - [abstract]: "a novel debiasing method based on Sparse Autoencoders (SAEs). The SAE-based steering method improves fairness by 5-15 points over baseline"
  - [section]: "Using the pretrained GemmaScope Sparse Autoencoders... we hand select 12 features selecting for descriptions that mentioned either 'fairness', 'gender equity', or related concepts"

### Mechanism 2
- **Claim**: Effective bias evaluation datasets must require both visual and textual inputs for accurate predictions
- **Mechanism**: Datasets where models can guess answers from text alone fail to measure true VLM bias because they don't test the model's ability to integrate visual information
- **Core assumption**: Models should perform significantly better when images are included if they are properly utilizing visual information
- **Evidence anchors**:
  - [abstract]: "Scene-based datasets like PATA and VLStereoSet fail to be useful benchmarks for bias due to their text prompts allowing the model to guess the answer without a picture"
  - [section]: "This fact means for effective VLM bias benchmarking datasets, models should achieve around random or lower than random accuracy"

### Mechanism 3
- **Claim**: Adversarial datasets reveal hidden biases by forcing models to rely on visual information when textual cues are misleading
- **Mechanism**: By providing conflicting textual information, models are forced to actually process the image rather than relying on text cues, revealing true biases
- **Core assumption**: Models will show degraded performance on adversarial datasets if they are over-relying on textual information
- **Evidence anchors**:
  - [abstract]: "VisoGender dataset for pronoun resolution is too easy, revealing biases only when using an adversarial version"
  - [section]: "Model performance drops significantly when adversarial prompts are introduced to VisoGender, indicating an over-reliance on textual information"

## Foundational Learning

- **Concept**: Sparse Autoencoders and feature steering
  - Why needed here: The debiasing method relies on SAEs to identify and modify latent features associated with fairness
  - Quick check question: How do Sparse Autoencoders create interpretable features from model activations?

- **Concept**: Vision-Language Model architecture and bias evaluation
  - Why needed here: Understanding how VLMs integrate visual and textual information is crucial for evaluating bias
  - Quick check question: What are the key differences between text-only and vision-language model bias evaluation?

- **Concept**: Fairness metrics and evaluation methodologies
  - Why needed here: The study uses specific metrics like Demographic Parity Ratio and Resolution Bias to measure fairness improvements
  - Quick check question: How does Demographic Parity Ratio measure fairness in classification tasks?

## Architecture Onboarding

- **Component map**: Vision encoder (CLIP/ViT) → Text encoder (LLM) → Fusion layer → Output head → SAE layer inserted at fusion layer to modify activations → Evaluation datasets feeding into model pipeline
- **Critical path**: Vision input → Feature extraction → LLM processing → Bias evaluation → SAE intervention → Fairness measurement
- **Design tradeoffs**: 
  - Using SAEs adds computational overhead but provides interpretable debiasing
  - Dataset selection affects evaluation validity - must ensure visual dependence
  - Steering strength vs. model performance trade-off
- **Failure signatures**:
  - No improvement in fairness metrics despite SAE intervention
  - Performance degradation without corresponding fairness gains
  - Models maintain accuracy on adversarial datasets (indicates insufficient adversarial challenge)
- **First 3 experiments**:
  1. Evaluate baseline model performance on CelebA/UTKFace to establish fairness baseline
  2. Test SAE feature selection by running steering on single feature and measuring impact
  3. Compare SAE debiasing vs prompt-based debiasing on VisoGender dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can bias evaluation datasets be designed to prevent models from exploiting textual information while still maintaining task validity?
- Basis in paper: [explicit] The paper identifies that scene-based datasets like PATA and VLStereoSet allow models to guess answers from text prompts without relying on images, rendering them ineffective for bias evaluation.
- Why unresolved: While the paper identifies the problem with current datasets, it does not provide concrete solutions for redesigning these datasets to prevent text-based exploitation while maintaining their validity as bias benchmarks.
- What evidence would resolve it: Development and validation of new scene-based datasets where models' performance drops significantly when images are removed, indicating genuine reliance on visual information.

### Open Question 2
- Question: How generalizable are SAE-based debiasing techniques across different VLM architectures and datasets?
- Basis in paper: [explicit] The paper shows SAE-based steering improves fairness on PaliGemma-2 models for VisoGender dataset, but notes this needs expansion to more models and datasets.
- Why unresolved: The study only tested SAE debiasing on PaliGemma-2 models with VisoGender dataset. The effectiveness across different VLM architectures (LLaVa, CLIP) and other bias evaluation datasets remains unknown.
- What evidence would resolve it: Systematic testing of SAE debiasing across multiple VLM architectures and diverse bias evaluation datasets, measuring both performance and fairness improvements.

### Open Question 3
- Question: What is the relationship between dataset difficulty and its effectiveness at revealing model bias?
- Basis in paper: [inferred] The paper notes that VisoGender is too easy for models, hiding true biases, while portrait datasets are more challenging and effective at measuring bias.
- Why unresolved: The paper doesn't establish a clear theoretical framework or empirical evidence linking dataset difficulty to bias measurement effectiveness across different VLM types.
- What evidence would resolve it: Comparative analysis of multiple datasets with varying difficulty levels, measuring both model performance and the sensitivity of fairness metrics to model biases across different VLM architectures.

## Limitations

- SAE features trained on LLMs may not effectively transfer to VLMs, limiting debiasing generalizability
- Study focuses primarily on demographic attributes like gender and race, potentially overlooking other bias forms
- Evaluation relies on relatively small-scale datasets that may not capture full complexity of real-world bias scenarios

## Confidence

- **High**: The finding that text-only prompts can enable models to guess answers without visual input, making traditional VLM bias datasets unreliable
- **Medium**: The effectiveness of SAE-based steering for improving fairness metrics by 5-15 points
- **Medium**: The necessity of adversarial datasets to reveal true model biases
- **Low**: The long-term stability and generalization of SAE-based debiasing across diverse VLM architectures

## Next Checks

1. **Cross-architecture testing**: Apply the SAE-based debiasing method to VLMs with different architectural designs (e.g., different fusion strategies) to verify generalizability

2. **Long-term stability analysis**: Evaluate whether fairness improvements persist over extended use and across different task domains

3. **Alternative feature selection methods**: Compare the hand-selected fairness features with automated feature discovery methods to assess whether the improvements depend on specific feature choices