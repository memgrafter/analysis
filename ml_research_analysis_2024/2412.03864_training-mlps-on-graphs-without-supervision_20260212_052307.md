---
ver: rpa2
title: Training MLPs on Graphs without Supervision
arxiv_id: '2412.03864'
source_url: https://arxiv.org/abs/2412.03864
tags:
- simmlp
- graph
- node
- learning
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SimMLP introduces a self-supervised framework for training MLPs\
  \ on graphs that aligns GNN and MLP embeddings to capture structural information\
  \ without message passing. By maximizing mutual information between node features\
  \ and graph structures, SimMLP achieves equivalence to GNNs in the optimal case\
  \ while enabling 90-126\xD7 faster inference."
---

# Training MLPs on Graphs without Supervision

## Quick Facts
- arXiv ID: 2412.03864
- Source URL: https://arxiv.org/abs/2412.03864
- Reference count: 40
- One-line primary result: SimMLP achieves 90-126× faster inference than GNNs while matching performance on 20 datasets

## Executive Summary
SimMLP introduces a self-supervised framework that trains MLPs on graphs by aligning their embeddings with those from GNNs. The method maximizes mutual information between node features and graph structures, achieving GNN-level performance while enabling dramatically faster inference. Through extensive experiments across 20 datasets, SimMLP demonstrates state-of-the-art results, particularly excelling in inductive, cold-start, and noisy settings while maintaining strong robustness to feature/edge noise and label scarcity.

## Method Summary
SimMLP uses a self-supervised learning framework with two encoders: a GNN encoder that captures structure-aware embeddings and an MLP encoder that extracts structure-free embeddings. The method aligns these embeddings through a self-supervised loss that maximizes similarity while reconstructing node features. To prevent model collapse, SimMLP employs two strategies: approximating GNN message passing using MLPs and enhancing data diversity through augmentation. The model is pre-trained using this alignment loss and then fine-tuned for specific downstream tasks.

## Key Results
- Achieves 90-126× faster inference than GNNs while maintaining equivalent performance
- State-of-the-art results across 20 benchmark datasets covering node classification, link prediction, and graph classification
- Exceptional performance in inductive, cold-start, and noisy settings
- Strong robustness to feature/edge noise and label scarcity
- Preserves key inductive biases including homophily and local structure importance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SimMLP achieves equivalence to GNNs by maximizing mutual information between node features and graph structures
- Mechanism: The self-supervised alignment loss optimizes the covariance between GNN and MLP embeddings while preserving node feature information through reconstruction
- Core assumption: The observed graph G is sampled from a latent graph GI = (A, F) following the distribution G ~ P(GI)
- Evidence anchors:
  - [abstract]: "SimMLP is the first MLP learning method that can achieve equivalence to GNNs in the optimal case"
  - [section]: "We provide a comprehensive theoretical analysis, demonstrating the equivalence between SimMLP and GNNs based on mutual information and inductive bias"
  - [corpus]: Weak - no direct mention of mutual information equivalence in corpus
- Break condition: If the latent variable assumption fails or the mutual information cannot be accurately estimated

### Mechanism 2
- Claim: The two strategies prevent model collapse by handling heterogeneous information sources
- Mechanism: Strategy 1 approximates GNN message passing using MLPs to ensure consistent embedding spaces, while Strategy 2 uses augmentation to increase data diversity and create multiple node-structure pairs
- Core assumption: Message passing can be approximated as a combination of feature transformation and aggregation
- Evidence anchors:
  - [section]: "The key idea is to employ self-supervised learning to align the representations encoded by graph context-aware GNNs and neighborhood dependency-free MLPs"
  - [section]: "Training MLPs on graphs without supervision is a non-trivial task... naively applying the basic loss function results in model collapse"
  - [corpus]: Weak - corpus mentions SimMLP but doesn't detail the two strategies for preventing collapse
- Break condition: If the approximation becomes too inaccurate or augmentation destroys structural patterns

### Mechanism 3
- Claim: SimMLP preserves inductive biases (homophily and local structure importance) that are critical for generalization
- Mechanism: The alignment process inherently captures these biases through covariance maximization and reconstruction terms
- Core assumption: Inductive biases are measurable through smoothness (MAD) and min-cut metrics
- Evidence anchors:
  - [section]: "We investigate whether SimMLP and GNNs have similar inductive biases... SimMLP has two key inductive biases, i.e., homophily philosophy and local structure importance"
  - [section]: "Table 2... SimMLP demonstrates an inductive bias towards local structure importance, evidenced by the optimal average Min-Cut result"
  - [corpus]: Weak - corpus mentions related methods but doesn't specifically discuss inductive bias preservation
- Break condition: If the preserved biases don't generalize to new datasets or domains

## Foundational Learning

- Concept: Mutual Information Maximization
  - Why needed here: Forms the theoretical foundation for why SimMLP can achieve GNN equivalence by maximizing the correlation between node features and structures
  - Quick check question: What is the relationship between maximizing mutual information and minimizing conditional entropy?

- Concept: Information Bottleneck Principle
  - Why needed here: Explains how SimMLP achieves generalization by compressing graph information while preserving task-relevant information
  - Quick check question: How does the information bottleneck trade-off between compression and informativeness apply to the SimMLP objective?

- Concept: Inductive Bias in Graph Learning
  - Why needed here: Understanding homophily and local structure importance helps explain why SimMLP can generalize like GNNs
  - Quick check question: How do smoothness and min-cut metrics measure the preservation of inductive biases?

## Architecture Onboarding

- Component map:
  GNN Encoder -> MLP Encoder -> Projector -> Alignment Loss -> Backpropagation to MLP parameters

- Critical path: GNN Encoder → MLP Encoder → Projector → Alignment Loss → Backpropagation to MLP parameters

- Design tradeoffs:
  - GNN choice affects inductive bias (GCN for homophily, specialized GNNs for heterophily)
  - Augmentation ratio balances diversity vs. preserving structural integrity
  - Loss weighting (λ) trades off alignment vs. reconstruction importance
  - Message passing approximation depth affects accuracy vs. efficiency

- Failure signatures:
  - Model collapse: Training loss decreases but accuracy remains low
  - Over-smoothing: Node embeddings become too similar across the graph
  - Poor generalization: Performance drops significantly on inductive/cold-start settings

- First 3 experiments:
  1. Baseline test: Run SimMLP vs. vanilla MLP on Cora dataset to verify performance improvement
  2. Ablation study: Remove GNN encoder to confirm structural information is essential
  3. Inductive setting: Test on Cora-inductive split to verify generalization to unseen nodes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SimMLP's performance scale with increasing graph size and node count in real-world applications?
- Basis in paper: [inferred] The paper demonstrates effectiveness on 20 benchmark datasets but doesn't extensively explore scaling behavior on massive graphs. The computational efficiency section shows strong performance on OGB-Product but doesn't analyze scaling trends.
- Why unresolved: The paper lacks systematic analysis of performance degradation or maintenance as graph sizes increase from hundreds to millions of nodes, which is critical for practical deployment.
- What evidence would resolve it: Empirical studies showing accuracy, training time, and memory usage across graphs spanning multiple orders of magnitude in size, particularly in industrial-scale scenarios.

### Open Question 2
- Question: What is the theoretical relationship between the number of self-supervised pre-training epochs and final downstream task performance?
- Basis in paper: [explicit] The paper mentions using 1000 epochs for most datasets but doesn't analyze how varying pre-training duration affects convergence or downstream accuracy.
- Why unresolved: The optimal pre-training duration likely depends on graph characteristics, and understanding this relationship could significantly improve training efficiency and resource allocation.
- What evidence would resolve it: Empirical analysis showing downstream performance versus pre-training epochs across diverse graph types, identifying optimal stopping points and diminishing returns.

### Open Question 3
- Question: How does SimMLP's robustness to feature noise compare to GNNs when noise follows non-Gaussian distributions?
- Basis in paper: [explicit] The paper demonstrates robustness to Gaussian noise but only mentions that "real-world data often contains various types of noise" without systematic investigation.
- Why unresolved: Real-world graphs frequently contain structured noise patterns (e.g., adversarial attacks, systematic biases) that may affect SimMLP and GNNs differently due to their architectural differences.
- What evidence would resolve it: Comparative studies of performance degradation under various noise distributions (uniform, adversarial, bursty) across both methods, with analysis of which architectural components contribute to robustness.

## Limitations
- Theoretical assumptions about latent graph sampling may not hold in real-world scenarios
- Approximation quality of message passing is bounded by depth, limiting effectiveness on deeper graphs
- Augmentation strategy may introduce noise that distorts structural patterns in heterogeneous graphs

## Confidence
- Mechanism 1 (MI-based equivalence): Medium - Strong theoretical foundation but limited empirical validation of the latent variable assumption
- Mechanism 2 (Model collapse prevention): High - Well-supported by ablation studies and theoretical analysis
- Mechanism 3 (Inductive bias preservation): Medium - Empirical metrics show bias preservation, but generalizability across domains needs verification

## Next Checks
1. **Latent variable assumption test**: Generate synthetic graphs with known latent structure and verify if SimMLP recovers the true structure through controlled experiments
2. **Approximation depth analysis**: Systematically vary the message passing approximation depth (k) and measure performance degradation on graphs with different diameters
3. **Cross-domain generalization**: Evaluate SimMLP on graphs from fundamentally different domains (social, biological, citation) to test inductive bias preservation beyond benchmark datasets