---
ver: rpa2
title: 'BabyHGRN: Exploring RNNs for Sample-Efficient Training of Language Models'
arxiv_id: '2412.15978'
source_url: https://arxiv.org/abs/2412.15978
tags:
- language
- babylm
- hgrn2
- knowledge
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores RNN-based architectures as alternatives to
  transformers for sample-efficient language modeling. The authors train BabyHGRN,
  an HGRN2-based model, on limited datasets (10M and 100M words) and evaluate it against
  transformer baselines and other efficient RNN architectures.
---

# BabyHGRN: Exploring RNNs for Sample-Efficient Training of Language Models

## Quick Facts
- **arXiv ID:** 2412.15978
- **Source URL:** https://arxiv.org/abs/2412.15978
- **Reference count:** 5
- **Primary result:** HGRN2-based RNN architectures outperform transformer baselines on sample-efficient language modeling benchmarks

## Executive Summary
This paper explores RNN-based architectures as alternatives to transformers for sample-efficient language modeling. The authors train BabyHGRN, an HGRN2-based model, on limited datasets (10M and 100M words) and evaluate it against transformer baselines and other efficient RNN architectures. Results show BabyHGRN outperforms transformer models on BLiMP, EWoK, GLUE, and BEAR benchmarks, particularly in the 10M word track with a macro average of 63.3% compared to 60.8% for the BabyLlama baseline. Knowledge distillation further improves performance by 5.3 pp in the 10M setting. The findings suggest RNN architectures like HGRN2 are competitive with transformers in low-resource scenarios.

## Method Summary
The paper investigates sample-efficient language modeling using HGRN2, a recurrent architecture based on associative Hopfield networks. The authors train BabyHGRN on subsampled versions of ThePile (4.9M and 49.2M words) with 16k BPE vocabulary, comparing against transformer baselines. The base model (360M parameters) is trained for 5 epochs using cross-entropy loss, followed by knowledge distillation with a 1B parameter student model. Training uses distributed optimization across 4 RTX A6000 GPUs with Adam optimizer and linear scheduler. Zero-shot evaluation is performed on BLiMP, EWoK, GLUE, and BEAR benchmarks, with downstream fine-tuning for SuperGLUE tasks.

## Key Results
- BabyHGRN achieves 63.3% macro average on combined benchmarks in the 10M word track, outperforming BabyLlama's 60.8%
- Knowledge distillation improves BabyHGRN performance by 5.3 percentage points in the 10M setting
- BabyHGRN shows strong performance on knowledge probes (BLiMP, EWoK, BEAR) while maintaining competitive results on GLUE and SuperGLUE
- Training on Pile-derived datasets yields better performance than default BabyLM dataset, particularly in the 100M word track

## Why This Works (Mechanism)
None

## Foundational Learning
- **Sample-efficient training**: Training models on limited data (10M-100M words) to achieve competitive performance - needed to understand the research problem and benchmark setup; quick check: verify dataset sizes match reported values
- **Knowledge distillation**: Training a smaller model to mimic a larger teacher model - needed to understand the 5.3 pp performance improvement; quick check: confirm KL divergence is combined with cross-entropy in the loss
- **Recurrent neural networks (RNNs)**: Sequence modeling architectures that process inputs sequentially - needed to understand HGRN2 architecture differences from transformers; quick check: identify HGRN2 as based on associative Hopfield networks
- **BPE tokenization**: Byte-Pair Encoding for vocabulary creation - needed to understand input preprocessing and vocabulary size; quick check: verify 16k vocabulary matches BabyLlama BPE
- **Distributed training**: Parallel computation across multiple GPUs - needed to understand training infrastructure requirements; quick check: confirm 4 RTX A6000 GPUs are used
- **Benchmark evaluation**: Using multiple standardized tests (BLiMP, EWoK, GLUE, BEAR) - needed to assess model capabilities across linguistic dimensions; quick check: verify macro averaging is used for final scores

## Architecture Onboarding

**Component Map:**
BPE Tokenizer -> HGRN2 Recurrent Network -> Cross-Entropy/KL Loss -> Adam Optimizer -> GPU Cluster

**Critical Path:**
Tokenization → HGRN2 forward pass → Next-token prediction → Loss computation → Parameter update → Evaluation

**Design Tradeoffs:**
- RNN vs Transformer: Sequential processing vs parallel attention mechanisms
- Parameter scaling: 360M→1B for distillation trade-off between efficiency and performance
- Dataset composition: Pile-derived vs BabyLM baseline affects downstream performance

**Failure Signatures:**
- Poor BLiMP-Supplement performance indicates syntactic pattern coverage issues
- Knowledge distillation instability suggests temperature/weighting hyperparameter problems
- Suboptimal GLUE scores may indicate insufficient fine-tuning or architectural limitations

**First Experiments:**
1. Train base HGRN2 (360M) on 10M word dataset for 5 epochs, evaluate on BLiMP
2. Apply knowledge distillation to create 1B student, evaluate macro average improvement
3. Fine-tune on SuperGLUE tasks with learning rate 5e-5, measure downstream performance

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** How does the performance of HGRN2-based models scale when trained on datasets significantly larger than 100M words?
- **Basis in paper:** [explicit] The authors note that HGRN2 models outperform transformer baselines in low-resource settings (10M and 100M words) but do not explore performance at larger scales.
- **Why unresolved:** The paper focuses on sample-efficient training and does not test whether HGRN2 maintains its advantage as dataset size increases.
- **What evidence would resolve it:** Training HGRN2 on datasets exceeding 100M words and comparing its performance to transformer models on established benchmarks.

### Open Question 2
- **Question:** What specific aspects of the Pile dataset composition contribute to the improved performance of BabyHGRN compared to BabyLlama?
- **Basis in paper:** [explicit] The authors observe improved performance when training on their Pile-derived dataset versus the default BabyLM dataset, particularly in the 100M word track.
- **Why unresolved:** The paper does not perform ablation studies to identify which dataset components (e.g., web text, scientific literature, code) drive the performance gains.
- **What evidence would resolve it:** Systematic evaluation of HGRN2 models trained on individual Pile components or controlled variations of the dataset.

### Open Question 3
- **Question:** How do advanced RNN architectures like HGRN2 compare to other efficient transformer variants (e.g., Mamba, Hyena) in low-resource settings?
- **Basis in paper:** [inferred] The authors compare HGRN2 to traditional RNNs (LSTM) and other efficient architectures (Mamba, xLSTM) but do not directly compare to efficient transformer alternatives.
- **Why unresolved:** The paper establishes HGRN2's competitiveness but leaves open questions about its relative performance against newer efficient transformer designs.
- **What evidence would resolve it:** Head-to-head comparison of HGRN2 against modern efficient transformer architectures (Mamba, Hyena, etc.) trained under identical conditions.

## Limitations
- The exact sampling strategy for creating the 10M/100M word datasets from ThePile remains unspecified beyond rough domain ratios
- Knowledge distillation implementation details, particularly teacher-student temperature and weighting parameters, are underspecified
- Results are not directly comparable to original BabyLM Challenge submissions due to dataset differences

## Confidence
- **High Confidence:** The comparative performance of BabyHGRN against transformer baselines on the evaluated benchmarks
- **Medium Confidence:** The generalization of these results to other low-resource settings
- **Low Confidence:** The exact mechanisms by which HGRN2 achieves superior sample efficiency compared to transformers

## Next Checks
1. Recreate the 10M and 100M word datasets using the specified domain ratios and verify that the BabyHGRN performance matches the reported results when trained on these exact datasets
2. Conduct ablation studies varying the knowledge distillation temperature and weighting parameters to determine their impact on the 5.3 pp performance improvement
3. Test BabyHGRN on an additional low-resource language modeling benchmark not used in the original evaluation to assess generalizability beyond the BabyLM Challenge suite