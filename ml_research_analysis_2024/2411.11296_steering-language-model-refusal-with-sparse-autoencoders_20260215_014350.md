---
ver: rpa2
title: Steering Language Model Refusal with Sparse Autoencoders
arxiv_id: '2411.11296'
source_url: https://arxiv.org/abs/2411.11296
tags:
- steering
- feature
- refusal
- prompt
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates using sparse autoencoders (SAEs) to steer
  model activations at inference time for safer language model behavior. The core
  idea is to identify and amplify SAE features that mediate refusal behavior, aiming
  to improve robustness to unsafe prompts without retraining.
---

# Steering Language Model Refusal with Sparse Autoencoders

## Quick Facts
- arXiv ID: 2411.11296
- Source URL: https://arxiv.org/abs/2411.11296
- Authors: Kyle O'Brien; David Majercak; Xavier Fernandes; Richard Edgar; Blake Bullwinkel; Jingya Chen; Harsha Nori; Dean Carignan; Eric Horvitz; Forough Poursabzi-Sangdeh
- Reference count: 40
- One-line primary result: SAE-based steering can improve unsafe prompt refusals by 37.69% but causes 3.4× increase in safe prompt over-refusals and benchmark performance degradation

## Executive Summary
This work investigates using sparse autoencoders (SAEs) to steer model activations at inference time for safer language model behavior. The core idea is to identify and amplify SAE features that mediate refusal behavior, aiming to improve robustness to unsafe prompts without retraining. Experiments on Phi-3 Mini show that amplifying a refusal-mediating feature (22373) successfully increases unsafe prompt refusals by 37.69% and reduces attack success rates in multi-turn jailbreak scenarios. However, this comes at a cost: safe prompt refusals increase 3.4× and benchmark accuracy drops across factual recall and reasoning tasks. Ablation studies show similar performance regressions when steering unrelated features, suggesting these trade-offs are inherent to the steering approach rather than specific to safety features. The results highlight the promise and current limitations of SAE-based steering for practical safety interventions.

## Method Summary
The method involves training a Top-k sparse autoencoder on Phi-3 Mini's layer 6 activations using web text datasets, then identifying SAE features that activate during archetypal unsafe prompt refusals. At inference time, the identified refusal-mediating feature (22373) is clamped to high values (10-12) to amplify its influence on model behavior. This steering approach is evaluated using Wild Guard and XSTest benchmarks for safety metrics and MMLU/TruthfulQA/GSM8K for performance impact. The study also tests steering unrelated features like philosophy (Feature 216) to understand whether performance regressions are specific to safety features or inherent to the steering mechanism itself.

## Key Results
- Amplifying feature 22373 increased unsafe prompt refusals by 37.69% on Wild Guard
- Safe prompt over-refusals increased 3.4× when steering the refusal feature
- Benchmark performance dropped across MMLU, TruthfulQA, and GSM8K when steering either refusal or unrelated features
- Feature 22373 also reduced attack success rates in multi-turn jailbreak scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Amplifying feature 22373 directly increases refusal behavior for unsafe prompts
- Mechanism: The feature 22373 encodes safety-relevant information that triggers refusal responses when activated above a threshold. By clamping this feature to high values during inference, the model is forced to generate refusal patterns regardless of the prompt content.
- Core assumption: Feature 22373 is monosemantic and specifically mediates refusal behavior rather than being part of a distributed representation
- Evidence anchors:
  - [abstract] "amplifying SAE features that mediate refusal behavior"
  - [section] "Feature 22373 as having the strongest and most consistent relationship with increased Unsafe Prompt Refusals"
  - [corpus] Weak - neighboring papers discuss SAEs for steering but don't specifically validate 22373's monosemanticity
- Break condition: If feature 22373 is polysemantic or part of a distributed representation, amplifying it will cause unintended side effects across unrelated capabilities

### Mechanism 2
- Claim: Steering unrelated features (like philosophy) causes similar performance regressions as steering safety features
- Mechanism: SAE features are not modular and are entangled with general language model capabilities. Amplifying any feature disrupts the normal flow of information through the model, causing systematic degradation regardless of the feature's semantic meaning.
- Core assumption: Features mediate behaviors through direct causal pathways rather than emergent interactions
- Evidence anchors:
  - [section] "similar performance regressions when steering unrelated features" and "Feature 216 (Philosophy) leads Phi-3 Mini to discuss these topics even when they are entirely unrelated"
  - [section] "amplifying this feature results in performance degradation"
  - [corpus] Weak - neighboring papers don't provide evidence about feature entanglement across capabilities
- Break condition: If features are truly modular and only the safety-relevant features cause over-refusal, then steering unrelated features should not cause performance regressions

### Mechanism 3
- Claim: SAE reconstruction combined with residual error helps maintain model coherence while steering
- Mechanism: The SAE reconstructs the original activations, and the difference (error) is added back to preserve the original information flow. This allows steering to modify behavior while maintaining overall model structure.
- Core assumption: The reconstruction error contains sufficient information to maintain coherence
- Evidence anchors:
  - [section] "The steered reconstruction and error terms are combined and passed as the input to the next layer"
  - [section] "We can optionally include l as a countermeasure to the inherit reconstruction loss between x and ˆx"
  - [corpus] Weak - neighboring papers discuss SAE reconstruction but don't specifically validate the error term's role in maintaining coherence
- Break condition: If the reconstruction error is insufficient or corrupted, the model may become incoherent or generate nonsensical outputs

## Foundational Learning

- Concept: Sparse Autoencoders (SAEs) and their role in mechanistic interpretability
  - Why needed here: Understanding how SAEs decompose activations into interpretable features is fundamental to grasping how feature steering works
  - Quick check question: What is the difference between the encoder output z and the decoder output ˆx in an SAE?
- Concept: Activation steering vs weight-based fine-tuning
  - Why needed here: The paper contrasts inference-time steering with traditional weight updates, so understanding this distinction is crucial
  - Quick check question: How does feature steering differ from traditional safety fine-tuning in terms of model modification?
- Concept: Feature activation patterns and their relationship to behavior
  - Why needed here: The paper relies on identifying which features activate for specific behaviors, so understanding this mapping is essential
  - Quick check question: How does the paper identify which SAE features mediate refusal behavior?

## Architecture Onboarding

- Component map: Input activations -> SAE encoder -> feature vector z -> feature clamping -> SAE decoder -> reconstruction + error -> next model layer
- Critical path: Input activations -> SAE encoding -> feature clamping decision -> SAE decoding -> error addition -> model continuation
- Design tradeoffs: Feature granularity (k=32) vs reconstruction quality vs computational cost; single feature vs multiple feature steering
- Failure signatures: Increased over-refusal of safe prompts, degraded performance on unrelated benchmarks, model incoherence in generations
- First 3 experiments:
  1. Train SAE on Phi-3 Mini layer 6 activations using Fineweb dataset, verify reconstruction quality
  2. Identify features that activate during refusal responses to unsafe prompts, focusing on features like 22373 that show consistent behavior
  3. Test single feature amplification on a small subset of Wild Guard to measure refusal rate changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific SAE features interact with one another to mediate refusal behavior, and can this interaction be modeled or predicted?
- Basis in paper: [inferred] The paper notes that "The degree to which features work in isolation to mediate behavior, or whether behavior emerges from interactions among multiple features, remains an open research question."
- Why unresolved: The paper focuses on identifying and steering individual features rather than exploring their interactions. Understanding these interactions would require more complex modeling and experimentation.
- What evidence would resolve it: Experiments that systematically vary multiple features simultaneously and measure their combined effect on refusal behavior would provide insights into feature interactions. Additionally, developing a theoretical framework for predicting feature interactions could help clarify this relationship.

### Open Question 2
- Question: What are the specific mechanisms by which feature steering degrades performance on factual recall and reasoning tasks, and can these mechanisms be mitigated?
- Basis in paper: [explicit] The paper observes that "We were surprised that feature steering had such a negative influence on the model's overall performance across several standard benchmarks."
- Why unresolved: The paper identifies the problem but does not investigate the underlying mechanisms. Understanding these mechanisms would require detailed analysis of how steered features affect the model's internal processes.
- What evidence would resolve it: Detailed analysis of model activations and feature interactions during both steering and benchmark tasks could reveal the mechanisms of performance degradation. Experiments that test different steering strategies or feature combinations might also provide insights into potential mitigations.

### Open Question 3
- Question: How does the size and architecture of SAEs influence the effectiveness and trade-offs of feature steering, and what is the optimal configuration for balancing safety and performance?
- Basis in paper: [explicit] The paper states that "It may be that larger SAEs that typically have finer-grained features... could provide features that more precisely steer for safety while bypassing negative influences on performance."
- Why unresolved: The paper uses a relatively small SAE and does not explore how different configurations might affect steering outcomes. Finding the optimal configuration would require extensive experimentation with various SAE sizes and architectures.
- What evidence would resolve it: Systematic experiments comparing the performance of different SAE configurations on both safety and benchmark tasks would help identify the optimal setup. Additionally, analyzing the features learned by different SAE sizes could provide insights into their relative effectiveness.

## Limitations
- Feature monosemanticity assumptions may not hold, as SAE features often exhibit polysemantic behavior across different contexts
- Similar performance regressions observed for unrelated features suggest fundamental limitations rather than feature-specific issues
- Results are demonstrated only on Phi-3 Mini with a specific SAE configuration, limiting generalizability

## Confidence

**High Confidence**: The experimental methodology for measuring safety and performance impacts is sound, and the observed trade-offs between unsafe prompt refusal improvements and safe prompt over-refusal are clearly demonstrated.

**Medium Confidence**: The mechanism by which feature amplification increases refusal behavior is plausible but relies on assumptions about feature semantics that aren't fully validated. The similarity in performance regressions across different features suggests fundamental limitations but could also reflect SAE-specific artifacts.

**Low Confidence**: Claims about the specific semantic content of feature 22373 and its exclusive relationship to refusal behavior lack robust validation beyond activation pattern correlation.

## Next Checks

1. **Feature Polysemanticity Analysis**: Conduct systematic probing of feature 22373 across diverse prompt types to determine whether it consistently mediates refusal behavior or exhibits context-dependent semantics that could explain the observed trade-offs.

2. **SAE Architecture Ablation**: Repeat the steering experiments with different SAE configurations (varying k, different layers, alternative sparsity penalties) to determine whether performance regressions are inherent to feature steering or specific to the chosen SAE implementation.

3. **Feature Attribution Robustness**: Apply alternative interpretability methods (e.g., causal mediation analysis, integrated gradients) to verify that feature 22373 causally mediates refusal behavior rather than merely correlating with it during the activation analysis phase.