---
ver: rpa2
title: 'KernelSHAP-IQ: Weighted Least-Square Optimization for Shapley Interactions'
arxiv_id: '2405.10852'
source_url: https://arxiv.org/abs/2405.10852
tags:
- kernelshap-iq
- interactions
- shapley
- which
- approximation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of computing higher-order feature
  interactions in machine learning models using Shapley Interaction Indices (SII).
  The authors prove that SII is the solution to a weighted least squares optimization
  problem, which yields an optimal k-additive approximation of any cooperative game.
---

# KernelSHAP-IQ: Weighted Least-Square Optimization for Shapley Interactions

## Quick Facts
- arXiv ID: 2405.10852
- Source URL: https://arxiv.org/abs/2405.10852
- Reference count: 40
- Primary result: State-of-the-art performance for local feature interaction explanations using weighted least-squares optimization

## Executive Summary
This paper addresses the challenge of computing higher-order feature interactions in machine learning models by characterizing Shapley Interaction Indices (SII) as solutions to weighted least squares optimization problems. The authors prove that SII values minimize a weighted least squares objective while satisfying efficiency constraints, providing an interpretable k-additive approximation of cooperative games. Based on this theoretical foundation, they propose KernelSHAP-IQ, an extension of KernelSHAP that achieves superior performance for feature interaction explanations across various datasets and model classes.

## Method Summary
The paper presents KernelSHAP-IQ, a method that extends KernelSHAP to compute higher-order Shapley Interaction Indices (SII) through weighted least squares optimization. The approach iteratively solves optimization problems for each interaction order k, using residuals to update objectives for subsequent orders. The method incorporates Bernoulli weighting to aggregate SII values into k-Shapley values (k-SII), ensuring efficiency and interpretability. KernelSHAP-IQ employs a sampling strategy with border-trick to handle computational constraints, adjusting weights to approximate the true interaction effects.

## Key Results
- KernelSHAP-IQ achieves state-of-the-art performance for feature interaction explanations across multiple benchmark tasks
- Rigorous proof established for pairwise SII as WLS solutions, with empirically validated conjectures for higher orders
- Superior performance demonstrated on SOUM, LM, BR, CH, ViT, CNN, and AC tasks using MSE and Prec@10 metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SII can be computed as the solution to a weighted least squares optimization problem
- Mechanism: SII values minimize the WLS objective over all subsets subject to efficiency constraints by constructing matrix Xk and solving ϕ∗ = (XT
k WkXk)−1XT
k Wk · yk
- Core assumption: The weights µk ensure the inverse exists and the limit µ∞ → ∞ enforces efficiency
- Evidence anchors: [abstract], [section 3], Weak corpus support
- Break condition: Ill-conditioned weight matrix Wk or improper convergence of µ∞ → ∞

### Mechanism 2
- Claim: k-Shapley values provide interpretable k-additive approximation using Bernoulli weighting
- Mechanism: k-SII aggregates SII values up to order k using Bernoulli numbers to fairly distribute joint payouts
- Core assumption: Bernoulli weighting uniquely ensures efficiency for k-SII aggregation
- Evidence anchors: [abstract], [section 2.1], Weak corpus support
- Break condition: Incorrect Bernoulli number application or mis-implemented aggregation formula

### Mechanism 3
- Claim: KernelSHAP-IQ extends KernelSHAP to higher-order SII through iterative WLS solving
- Mechanism: Iteratively computes SII estimates by solving WLS for each order k and using residuals for next iteration
- Core assumption: Iterative approximation converges to true SII values when µ∞ → ∞ with proper sampling
- Evidence anchors: [abstract], [section 3.3], Weak corpus support
- Break condition: Improper residual computation or incorrect sampling weights preventing convergence

## Foundational Learning

- Concept: Shapley values and axiomatic foundations
  - Why needed here: Understanding SV is crucial as SII extends SV to interactions while preserving efficiency, symmetry, and linearity axioms
  - Quick check question: What is the efficiency axiom for Shapley values and why is it important?

- Concept: Cooperative game theory and characteristic functions
  - Why needed here: The paper works with transferable utility games ν: P(N) → R modeling payouts for feature subsets
  - Quick check question: How does a characteristic function ν(T) represent the value of coalition T in cooperative game theory?

- Concept: Weighted least squares optimization
  - Why needed here: Core mechanism relies on solving WLS problems to find SII values under constraints
  - Quick check question: What is the general form of a weighted least squares problem and how are constraints typically handled?

## Architecture Onboarding

- Component map: Sampling module -> WLS optimization module -> k-SII aggregation module -> Residual computation module
- Critical path: Sample subsets with adjusted weights → Compute coalition values ν(T) → Solve WLS for order k → Compute residuals → Aggregate to k-SII
- Design tradeoffs: Accuracy vs computational cost (higher k requires more computation), sampling efficiency (crucial for convergence), memory usage (storing SII values up to order k)
- Failure signatures: Poor convergence to true SII values, efficiency axiom violation in k-SII, numerical instability in matrix inversion, sampling bias affecting approximation quality
- First 3 experiments: 1) Verify SII computation for small synthetic games (n=3-5) with known ground truth, 2) Compare k-SII aggregation for k=1,2,3 on simple 2-player examples, 3) Test KernelSHAP-IQ convergence on small SOUMs with increasing budget

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the rigorous proof technique required to extend WLS representation of SII to higher orders (k > 2) beyond empirically validated conjectures?
- Basis in paper: [explicit] Authors unable to find closed-form solutions for k > 2 and suspect other proof techniques are required
- Why unresolved: Proof technique for k > 2 is unknown and existing methods cannot be extended
- What evidence would resolve it: Rigorous mathematical proof showing SII for k > 2 is WLS solution, or counterexample disproving conjecture

### Open Question 2
- Question: How do different values of infinite weight parameter µ∞ affect convergence of SII estimates for higher orders in KernelSHAP-IQ?
- Basis in paper: [explicit] Different µ∞ could resolve improper weighting issue in Conjecture 3.9 for higher orders
- Why unresolved: Impact of varying µ∞ on convergence and accuracy for k > 2 is unexplored
- What evidence would resolve it: Experimental results showing KernelSHAP-IQ performance with different µ∞ values for k > 2

### Open Question 3
- Question: Can link between SII and Shapley residuals be further explored to improve higher-order interaction approximation?
- Basis in paper: [inferred] Pairwise SII viewed as decomposition of Shapley residual, suggesting potential connection
- Why unresolved: Connection mentioned but not fully explored in context of higher-order interactions
- What evidence would resolve it: Theoretical or empirical analysis showing how incorporating Shapley residuals improves higher-order estimation

## Limitations

- Theoretical proofs established only for pairwise SII, with higher-order conjectures remaining empirically validated
- Convergence guarantees for KernelSHAP-IQ under finite sampling budgets remain unproven
- Computational complexity for large interaction orders not fully characterized

## Confidence

- High: Weighted least squares formulation for pairwise SII and k-SII aggregation using Bernoulli numbers
- Medium: Empirical performance claims across benchmarks and iterative approximation strategy
- Low: Theoretical guarantees for higher-order SII convergence and general validity of Conjecture 3.9

## Next Checks

1. **Convergence Analysis**: Systematically evaluate KernelSHAP-IQ's approximation error as function of sampling budget and interaction order k across multiple synthetic games with known ground truth

2. **Scalability Testing**: Measure computational runtime and memory requirements for increasing feature dimensions (n > 20) and interaction orders (k > 3)

3. **Cross-Model Robustness**: Validate KernelSHAP-IQ performance on additional model architectures and data modalities beyond current benchmark suite