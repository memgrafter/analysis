---
ver: rpa2
title: 'Unlearning as multi-task optimization: A normalized gradient difference approach
  with an adaptive learning rate'
arxiv_id: '2410.22086'
source_url: https://arxiv.org/abs/2410.22086
tags:
- unlearning
- learning
- ngdiff
- methods
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper formulates machine unlearning as a multi-task optimization\
  \ problem and proposes NGDiff, a novel approach using normalized gradient differences\
  \ combined with an adaptive learning rate scheduler. The method balances two objectives\u2014\
  maximizing utility on retaining data while minimizing memorization of forgetting\
  \ data\u2014through dynamic gradient normalization."
---

# Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate

## Quick Facts
- arXiv ID: 2410.22086
- Source URL: https://arxiv.org/abs/2410.22086
- Reference count: 40
- Key outcome: NGDiff improves both tasks simultaneously and converges monotonically with proper learning rate scheduling

## Executive Summary
This paper formulates machine unlearning as a multi-task optimization problem, proposing NGDiff (Normalized Gradient Difference) as a novel approach that uses normalized gradient differences combined with an adaptive learning rate scheduler. The method balances two objectives—maximizing utility on retaining data while minimizing memorization of forgetting data—through dynamic gradient normalization. NGDiff demonstrates superior performance on TOFU and MUSE datasets across multiple language models, achieving 40% higher model utility while maintaining comparable unlearning effectiveness.

## Method Summary
NGDiff is a normalized gradient difference approach for machine unlearning that treats the problem as multi-task optimization. The method computes gradients from both retaining and forgetting data sets, normalizes them, and combines them using a difference formulation. An adaptive learning rate estimator (GeN) automatically adjusts the learning rate based on forward-pass loss evaluations, eliminating the need for manual tuning. The algorithm converges monotonically when proper learning rate scheduling is applied, achieving Pareto optimal solutions between utility retention and forgetting effectiveness.

## Key Results
- NGDiff improves both tasks simultaneously and converges monotonically with proper learning rate scheduling
- 40% higher model utility while maintaining comparable unlearning effectiveness on TOFU and MUSE datasets
- Stable training without manual learning rate tuning across multiple language models (Llama2-7B, Mistral-7B, Phi-1.5, Falcon-1B, GPT2-XL)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic gradient normalization balances forgetting and retaining objectives simultaneously
- Mechanism: NGDiff uses normalized gradient differences (gR/||gR|| - gF/||gF||) to ensure gradients are positively correlated with retaining and negatively correlated with forgetting objectives at every iteration
- Core assumption: Local loss landscapes are convex along gradient directions, and retaining/forgetting gradients are not exactly parallel
- Evidence anchors:
  - [abstract]: "NGDiff improves both tasks simultaneously and converges monotonically with proper learning rate scheduling"
  - [section 4.2]: "Lemma 4: gNGDiff(gR, gF) satisfies Eq. (7) for any gR ∈ Rd and gF ∈ Rd"
  - [corpus]: Weak - related papers focus on unlearning but not this specific gradient normalization mechanism
- Break condition: When retaining and forgetting gradients become exactly parallel (gF ∥ gR)

### Mechanism 2
- Claim: Automatic learning rate adaptation prevents unstable training while optimizing both objectives
- Mechanism: GeN estimates optimal learning rate η* = g⊤RgNGDiff/g⊤NGDiffHRgNGDiff using forward passes only, avoiding Hessian computation
- Core assumption: Loss functions are locally quadratic and Hessian is positive definite along gradient directions
- Evidence anchors:
  - [section 4.3]: "GeN estimates two scalars – the numerator and denominator of Eq. (8) by analyzing the difference of loss values"
  - [section 4.1]: "LR and LF locally and directionally convex along the gradients"
  - [corpus]: Weak - GeN is cited but not extensively discussed in related unlearning papers
- Break condition: When g⊤NGDiffHRgNGDiff ≤ 0 or when curvature conditions fail

### Mechanism 3
- Claim: Multi-task optimization formulation provides Pareto optimal solutions for unlearning
- Mechanism: Unlearning formulated as finding Pareto optimal points between retaining utility and forgetting effectiveness through linear scalarization
- Core assumption: Solutions exist on Pareto frontier where improving one objective necessarily worsens the other
- Evidence anchors:
  - [section 3]: "In MTO, Pareto optimality is used to characterize the trade-offs between multiple objectives"
  - [section 3.1]: "Lemma 2 suggests that we can sweep through c ∈ [0, 1] and construct the Pareto frontier"
  - [corpus]: Moderate - related papers discuss Pareto optimality but not specifically in unlearning context
- Break condition: When objectives are not conflicting or when global minimum of scalarized loss is not achievable

## Foundational Learning

- Concept: Multi-task optimization and Pareto optimality
  - Why needed here: Unlearning inherently involves conflicting objectives (forgetting specific data vs maintaining general utility) that require trade-off analysis
  - Quick check question: If LR decreases by 10% and LF increases by 5%, is this necessarily a Pareto improvement?

- Concept: Gradient normalization and its impact on optimization dynamics
  - Why needed here: Standard gradient methods can cause one objective to dominate; normalization ensures balanced updates
  - Quick check question: What happens to the optimization direction when one gradient norm is much larger than the other?

- Concept: Automatic learning rate adaptation through second-order information
  - Why needed here: Manual learning rate tuning is impractical for large-scale unlearning; adaptive methods maintain stability
  - Quick check question: How does the estimated learning rate change when the loss landscape becomes steeper?

## Architecture Onboarding

- Component map: Data → Forward pass (R) → Backward (R) → Forward pass (F) → Backward (F) → NGDiff → AutoLR → Update
- Critical path: Data → Forward pass (retaining) → Backward (retaining) → Forward pass (forgetting) → Backward (forgetting) → NGDiff → AutoLR → Parameter update
- Design tradeoffs:
  - Memory vs computation: NGDiff requires 3 forward passes per update vs 2 for standard methods
  - Stability vs speed: AutoLR adds overhead but prevents divergence
  - Generality vs specificity: NGDiff works for any model but may not be optimal for specific architectures
- Failure signatures:
  - Utility collapse: Model forgets general knowledge (check if gradients become parallel)
  - Unlearning failure: Forgetting set remains memorized (check if gradient normalization is working)
  - Training instability: Loss oscillates or diverges (check AutoLR estimates and Hessian conditions)
- First 3 experiments:
  1. Verify gradient normalization: Check that g⊤RgNGDiff > 0 and g⊤FgNGDiff < 0 for random gradients
  2. Test AutoLR: Compare manual vs automatic learning rates on a small dataset
  3. Validate Pareto optimality: Sweep c parameter and plot utility vs forgetting trade-off curve

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the normalized gradient difference approach be effectively extended to multi-task optimization problems beyond two tasks?
- Basis in paper: [explicit] The paper focuses on framing unlearning as a two-task optimization problem, suggesting potential for extension.
- Why unresolved: The paper only demonstrates the effectiveness of NGDiff for two-task problems (retaining and forgetting), without exploring its scalability to more complex multi-task scenarios.
- What evidence would resolve it: Experimental results showing NGDiff's performance on problems with three or more tasks, comparing it against other multi-task optimization methods.

### Open Question 2
- Question: How does the performance of NGDiff vary across different types of language models, such as those with distinct architectures or pre-training objectives?
- Basis in paper: [explicit] The paper tests NGDiff on several models (Llama2-7B, Phi-1.5, Falcon-1B, GPT2-XL, Mistral-7B) but does not explore the impact of architectural differences.
- Why unresolved: While the paper shows NGDiff's effectiveness across various models, it does not analyze how specific architectural features influence its performance.
- What evidence would resolve it: Comparative studies of NGDiff's performance on models with different architectures (e.g., transformers vs. recurrent networks) or pre-training objectives (e.g., causal vs. masked language modeling).

### Open Question 3
- Question: Can the normalized gradient difference approach be adapted to handle non-differentiable components in the loss function, such as those arising from discrete operations or structured outputs?
- Basis in paper: [inferred] The paper's theoretical analysis relies on gradient-based optimization, which may not directly apply to non-differentiable components.
- Why unresolved: The paper does not address scenarios where the loss function includes non-differentiable elements, which are common in NLP tasks.
- What evidence would resolve it: Empirical results demonstrating NGDiff's effectiveness on tasks with non-differentiable components, such as structured prediction or discrete optimization problems.

### Open Question 4
- Question: What are the theoretical limits of unlearning effectiveness, and how do they relate to the trade-off between forgetting and utility in NGDiff?
- Basis in paper: [explicit] The paper discusses the trade-off between forgetting and utility but does not establish theoretical bounds on unlearning effectiveness.
- Why unresolved: While the paper provides empirical evidence of NGDiff's performance, it does not explore the fundamental limits of unlearning in terms of what can be theoretically achieved.
- What evidence would resolve it: Theoretical analysis establishing bounds on the minimum achievable memorization loss or maximum utility retention under various constraints, along with empirical validation.

## Limitations

- Theoretical assumptions about local convexity and non-parallel gradients may not hold in practice for complex language models
- GeN learning rate estimator implementation details are abstracted, making exact reproduction challenging
- Performance gains may be sensitive to dataset splits and random initialization, limiting generalizability

## Confidence

- **High Confidence**: The core multi-task optimization formulation and gradient normalization mechanism are mathematically sound and well-defined
- **Medium Confidence**: The adaptive learning rate estimation method is theoretically justified but lacks implementation specifics
- **Low Confidence**: The claimed 40% utility improvement and consistent superiority across all models may not generalize beyond the tested configurations

## Next Checks

1. **Gradient Direction Verification**: Implement a small-scale experiment to verify that normalized gradient differences maintain positive correlation with retaining gradients and negative correlation with forgetting gradients across different initialization points and dataset sizes

2. **Learning Rate Estimator Implementation**: Reconstruct the GeN learning rate estimator from the described forward-pass method and test its stability on a simple quadratic loss surface with varying curvature conditions

3. **Pareto Frontier Validation**: Systematically sweep the scalarization parameter c and plot the trade-off curve between utility and forgetting effectiveness to confirm that solutions lie on the theoretical Pareto frontier