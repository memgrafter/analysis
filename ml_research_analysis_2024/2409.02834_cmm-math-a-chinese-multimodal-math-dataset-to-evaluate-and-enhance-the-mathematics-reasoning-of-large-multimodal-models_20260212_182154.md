---
ver: rpa2
title: 'CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics
  Reasoning of Large Multimodal Models'
arxiv_id: '2409.02834'
source_url: https://arxiv.org/abs/2409.02834
tags:
- mathematical
- lmms
- multimodal
- reasoning
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CMM-Math, a Chinese multimodal math dataset
  designed to evaluate and enhance the mathematical reasoning capabilities of large
  multimodal models (LMMs). The dataset comprises over 28,000 high-quality samples
  across 12 grade levels, featuring various problem types with detailed solutions
  and multiple images in questions or options.
---

# CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of Large Multimodal Models

## Quick Facts
- arXiv ID: 2409.02834
- Source URL: https://arxiv.org/abs/2409.02834
- Reference count: 40
- Introduces CMM-Math, a Chinese multimodal math dataset, and Math-LMM, a math-specific LMM trained through a three-stage pipeline

## Executive Summary
This paper introduces CMM-Math, a Chinese multimodal math dataset containing over 28,000 high-quality samples across 12 grade levels, designed to evaluate and enhance the mathematical reasoning capabilities of large multimodal models (LMMs). The authors propose a three-stage training pipeline for Math-LMM that progressively builds multimodal reasoning capabilities: foundational pre-training aligns vision with language, foundational fine-tuning captures general task-solving abilities, and mathematical fine-tuning specializes in math reasoning. Comprehensive experiments show that state-of-the-art LMMs struggle with CMM-Math, while the proposed Math-LMM achieves accuracy scores up to 48.57% on CMM-Math and 36.3% on MATHVISTA.

## Method Summary
The method involves constructing a comprehensive Chinese multimodal math dataset (CMM-Math) with 28,069 problems across 12 grade levels and 13 subjects, featuring various problem types with detailed solutions and multiple images. The proposed Math-LMM is trained through three stages: foundational pre-training aligns the vision encoder with LLM via adapter pre-training using general multimodal datasets; foundational fine-tuning develops task-processing skills on foundation instruction datasets; and mathematical fine-tuning specializes the model with math-specific instruction datasets. The input format uses mixed interleaved instructions where images and text are encoded separately and combined in the exact order they appear in problems.

## Key Results
- State-of-the-art LMMs (Qwen2-VL-Instruct, InternLM-XComposer2-VL, CogVLM2, GPT-4o, Gemini) show poor performance on CMM-Math, highlighting the dataset's challenging nature
- Math-LMM outperforms open-source LMMs on CMM-Math with accuracy up to 48.57% and achieves 36.3% accuracy on MATHVISTA
- Performance varies significantly across grade levels and subjects, with analysis problems showing lower accuracy than multiple-choice problems

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The three-stage training pipeline progressively builds multimodal reasoning capabilities by first aligning vision with language, then learning task-solving patterns, and finally specializing in math.
- **Mechanism**: Stage 1 uses general multimodal datasets to align the vision encoder with LLM via adapter pre-training, ensuring visual features are properly mapped to text embeddings. Stage 2 fine-tunes the combined model on foundation instruction datasets to develop task-processing skills. Stage 3 specializes the model with math-specific instruction datasets, enhancing mathematical reasoning in multimodal contexts.
- **Core assumption**: Visual and textual modalities must be aligned before task-specific tuning can be effective; general task skills transfer to math reasoning.
- **Evidence anchors**: [abstract] "We train our model using three stages: foundational pre-training, foundational fine-tuning, and mathematical fine-tuning." [section] "The Math-LMM is trained using three stages: foundational pre-training that aligns visual information with LLM, foundational fine-tuning that captures the abilities of task solving, and mathematical fine-tuning that learns mathematical reasoning."
- **Break condition**: If adapter alignment fails in stage 1, subsequent fine-tuning will propagate misaligned features, causing poor multimodal reasoning regardless of task-specific data.

### Mechanism 2
- **Claim**: The mixed interleaved instruction format preserves the spatial and semantic context of math problems that contain multiple figures.
- **Mechanism**: Images are encoded separately, aligned via adapter, then interleaved with text tokens in the exact order they appear in the problem. This preserves the problem structure so the LLM can follow the logical flow of a math solution that refers to different images at different steps.
- **Core assumption**: The model's attention mechanism can correctly bind interleaved image-text segments if they are temporally ordered as in the original problem.
- **Evidence anchors**: [section] "We translate the input sample as a mix instruction to train Math-LMM. The template instruction is 'This is text. [IMAGE1]. This is text...'" [abstract] "We propose a Multimodal Mathematical LMM (Math-LMM) to handle the problems with mixed input of multiple images and text segments."
- **Break condition**: If the problem contains many loosely related images, the interleaved format may overload the attention span and cause the model to lose track of which image relates to which reasoning step.

### Mechanism 3
- **Claim**: Training on a large, grade-diverse, subject-diverse dataset with detailed solutions teaches the model to generalize across difficulty levels and mathematical domains.
- **Mechanism**: Over 28,000 samples across 12 grade levels and 13 subjects expose the model to a wide range of reasoning patterns. Detailed solutions provide step-by-step reasoning traces, enabling the model to learn not just answers but problem-solving strategies.
- **Core assumption**: Diversity in training data reduces overfitting to narrow problem types and improves transfer to unseen problems.
- **Evidence anchors**: [abstract] "CMM-Math contains over 28,000 high-quality samples... featuring a variety of problem types (e.g., choice, fill-in-the-blank, yes-no, and analysis) with detailed solutions across 12 grade levels." [section] "We evaluate the state-of-the-art open-source and close-source LMMs on our CMM-Math dataset. The results show that it is a challenge for the current foundation LMMs to handle multimodal mathematical reasoning tasks."
- **Break condition**: If the training data is biased toward certain subjects or grades, the model may fail on underrepresented problem types even with large overall dataset size.

## Foundational Learning

- **Concept**: Vision-language alignment via adapter pre-training
  - **Why needed here**: Without proper alignment, the LLM cannot meaningfully interpret visual embeddings, causing multimodal reasoning to fail regardless of downstream fine-tuning.
  - **Quick check question**: Can the adapter map an image of a triangle to the text "isosceles triangle" in embedding space before any math-specific training?

- **Concept**: Attention-based multimodal fusion
  - **Why needed here**: The interleaved text-image format relies on the model's attention to bind visual and textual cues correctly during reasoning.
  - **Quick check question**: Does the model attend to the correct image embedding when the text says "As shown in Figure 1"?

- **Concept**: Curriculum learning through grade-level diversity
  - **Why needed here**: Starting from simple problems and progressing to complex ones helps the model build reasoning skills incrementally, similar to human learning.
  - **Quick check question**: Does performance on level 1 (primary) improve before performance on level 12 (high school) during training?

## Architecture Onboarding

- **Component map**: Vision Encoder (DFN5B-H-14+) → Adapter (2-layer MLP with GELU) → LLM (Qwen2-7B-Instruct) → Output
- **Critical path**: Input image → vision encoder → adapter → interleaved token stream → LLM → output answer. Any bottleneck in adapter training or vision encoder quality directly impacts final reasoning performance.
- **Design tradeoffs**: Lightweight DFN5B-H-14+ (0.6B params) keeps inference on single 4090 feasible but may limit visual detail capture vs larger encoders. Two-layer MLP adapter is simple and fast but may underfit complex visual-text alignments vs cross-attention. Interleaved format preserves context but increases sequence length, potentially hitting context limits.
- **Failure signatures**: If adapter training diverges, vision embeddings become noisy → LLM attends to wrong regions → wrong answers. If vision encoder underfits, fine details in diagrams are lost → geometry reasoning fails. If interleaved format is too long, attention may skip early images → reasoning steps referencing them break.
- **First 3 experiments**:
  1. **Adapter alignment sanity check**: Feed a small set of paired image-text samples through the pipeline and verify the LLM can correctly identify the object in the image after adapter alignment.
  2. **Interleaving order test**: Create a toy problem with two images and verify the model outputs the correct answer only when images are in the correct interleaved order.
  3. **Grade-level ablation**: Train Math-LMM on only levels 1-6 vs all 12 levels and measure drop in performance on high-grade problems to confirm curriculum learning effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed three-stage training methodology (foundational pre-training, foundational fine-tuning, and mathematical fine-tuning) specifically impact the mathematical reasoning capabilities of large multimodal models compared to traditional single-stage training approaches?
- Basis in paper: [explicit] The paper describes the three-stage training methodology but does not provide a direct comparison with traditional single-stage approaches or quantify the specific benefits of each stage.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the proposed method rather than conducting a comparative analysis with other training methodologies.
- What evidence would resolve it: Comparative experiments showing performance differences between models trained with the three-stage approach versus those trained with single-stage approaches, along with ablation studies isolating the impact of each training stage.

### Open Question 2
- Question: What are the key challenges and limitations in adapting large multimodal models to handle complex Chinese multimodal mathematical problems, and how can these be addressed to improve model performance?
- Basis in paper: [inferred] The paper highlights that existing LMMs struggle with the CMM-Math dataset and identifies specific subjects and levels where performance is particularly low, but does not provide a detailed analysis of the underlying reasons or potential solutions.
- Why unresolved: While the paper identifies performance gaps, it does not delve into the specific technical challenges or propose targeted solutions for improving model capabilities in these areas.
- What evidence would resolve it: Detailed error analysis and failure case studies, along with proposed architectural or training modifications aimed at addressing identified weaknesses.

### Open Question 3
- Question: How does the performance of the proposed Math-LMM model on Chinese multimodal mathematical problems generalize to other languages and cultural contexts, and what modifications would be necessary to ensure effective cross-lingual and cross-cultural adaptation?
- Basis in paper: [inferred] The paper focuses on Chinese multimodal mathematical problems but does not explore the model's performance on problems from other languages or cultural contexts, nor does it discuss the necessary adaptations for such generalization.
- Why unresolved: The paper's scope is limited to Chinese multimodal mathematics, and there is no discussion of cross-lingual or cross-cultural considerations.
- What evidence would resolve it: Experiments evaluating the model's performance on multimodal mathematical problems from different languages and cultures, along with an analysis of the necessary modifications for effective adaptation.

## Limitations

- The exact implementation details of the adapter module and its alignment mechanism are not fully specified, which could affect reproducibility
- While the dataset is large and diverse, there is no analysis of potential biases in the training data that could impact model generalization
- The interleaved format, while preserving problem structure, may have limitations when dealing with problems containing many loosely related images, though this is not explicitly discussed or tested

## Confidence

- **High confidence**: Dataset construction methodology and basic claim that CMM-Math is a challenging benchmark for LMMs (supported by poor performance of state-of-the-art models)
- **Medium confidence**: Three-stage training pipeline's effectiveness (results show improvement but don't isolate contribution of each stage through ablation studies)
- **Medium confidence**: Mixed interleaved instruction format's benefits (mechanism is logical but not empirically validated against alternative input formats)

## Next Checks

1. **Adapter alignment validation**: Conduct a controlled experiment with a small set of image-text pairs to verify that the adapter properly aligns visual embeddings with textual representations before proceeding to downstream fine-tuning stages.

2. **Interleaved format ablation**: Compare Math-LMM performance when using pure text descriptions of problems versus the interleaved format to quantify the benefit of preserving the original problem structure with multiple images.

3. **Grade-level transfer analysis**: Train Math-LMM on subsets of grade levels (e.g., levels 1-6 only vs all 12 levels) and measure performance degradation on higher-grade problems to validate the curriculum learning hypothesis.