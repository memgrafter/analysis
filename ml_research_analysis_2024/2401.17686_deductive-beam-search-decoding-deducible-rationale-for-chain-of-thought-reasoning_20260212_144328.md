---
ver: rpa2
title: 'Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning'
arxiv_id: '2401.17686'
source_url: https://arxiv.org/abs/2401.17686
tags:
- reasoning
- step
- steps
- verifier
- deductive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Deductive Beam Search (DBS) integrates deductive reasoning into
  chain-of-thought (CoT) prompting to reduce intermediate reasoning errors. It uses
  a deductive verifier to evaluate whether each reasoning step is logically deducible
  from its premises, and performs step-wise beam search to select more reliable reasoning
  paths.
---

# Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning

## Quick Facts
- arXiv ID: 2401.17686
- Source URL: https://arxiv.org/abs/2401.17686
- Reference count: 19
- Improves chain-of-thought reasoning accuracy by 5.3%/3.2% on Llama2-7b/ChatGPT in single-chain settings

## Executive Summary
Deductive Beam Search (DBS) addresses the error accumulation problem in chain-of-thought reasoning by integrating deductive verification into beam search. The approach uses a trained verifier to evaluate whether each reasoning step is logically deducible from its premises, selecting only the most reliable paths. A novel two-stage training process creates a verifier capable of detecting both typical and subtle reasoning errors, using heuristic synthesis followed by LLM-generated hard negatives. Experiments across 8 datasets and 4 model scales demonstrate consistent accuracy improvements, with average gains of 5.3%/3.2% on Llama2-7b/ChatGPT in single-chain settings.

## Method Summary
DBS combines step-wise beam search with a deductive verifier trained through a two-stage process. First, the verifier learns to detect typical reasoning errors (grounding, logic, irrelevant) from heuristically synthesized data. Second, it refines its ability to catch subtle errors using LLM-generated reasoning steps as hard negatives. During inference, the LLM generates multiple candidate reasoning steps at each position, and the verifier scores each based on logical deducibility from previous steps. Beam search then selects the top-scoring paths, repeating until a final answer is produced. The approach uses Llama2-7b, 13b, 70b, and ChatGPT with beam size 5 and sampling times 10.

## Key Results
- Consistent accuracy improvements across 8 datasets spanning arithmetic, commonsense, and symbolic reasoning
- Average gains of 5.3%/3.2% on Llama2-7b/ChatGPT in single-chain settings, 3.9%/2.5% in multiple-chain settings
- Verifier reliably detects diverse error types and maintains performance across different model scales
- DBS shows better token efficiency compared to naive CoT prompting in some settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deductive verification constrains beam search to select reasoning steps that are logically deducible from their premises, reducing error accumulation.
- Mechanism: At each step, the verifier scores each candidate reasoning step by evaluating the logical coherence between the step and its context. Beam search then selects the top-scoring steps based on these deductive scores instead of LM confidence scores.
- Core assumption: The verifier can accurately detect logical deducibility between reasoning steps and their premises.
- Evidence anchors:
  - [abstract] "Our approach deploys a verifier, verifying the deducibility of a reasoning step and its premises, thus alleviating the error accumulation."
  - [section 2.3] "To verify the logical coherence between a reasoning step and its premises, we propose to train a deductive verifier"
- Break condition: If the verifier cannot accurately detect logical deducibility, the beam search will select incorrect reasoning steps, defeating the purpose of the approach.

### Mechanism 2
- Claim: The two-stage training process for the verifier enables it to detect both typical reasoning errors and more subtle, hard-to-detect errors.
- Mechanism: Stage 1 trains the verifier on heuristically synthesized errors (grounding, logic, irrelevant). Stage 2 uses the verifier from stage 1 to identify hard negatives from LLM-generated reasoning steps, which are then used to further train the verifier.
- Core assumption: LLM-generated reasoning steps contain errors that are more diverse and subtle than heuristically synthesized errors.
- Evidence anchors:
  - [section 3] "we propose a scalable and labor-free data construction method and a ranking-based training framework to teach the verifier to detect false reasoning steps"
  - [section 3.2] "we use the verifier from stage 1 to detect false reasoning steps generated by an actual language model"
- Break condition: If LLM-generated reasoning steps do not contain diverse or subtle errors, the hard negative synthesis step will not improve the verifier's performance.

### Mechanism 3
- Claim: Step-wise beam search balances exploration and exploitation in the reasoning space, improving the chances of finding deducible reasoning paths.
- Mechanism: The LLM samples multiple potential reasoning steps (exploration), and the verifier selects the most deducible ones (exploitation). This process repeats at each step, building up a reasoning path that is more likely to be logically sound.
- Core assumption: Sampling multiple potential reasoning steps and selecting the most deducible ones will lead to better overall reasoning performance than generating a single reasoning path.
- Evidence anchors:
  - [section 2.2] "Combining multi-step CoT reasoning with step-wise beam search balances exploration and exploitation in reasoning tasks"
  - [section 2.3] "LM generates n times for each beam at each step, sampling a total number of m Ã— n candidate reasoning steps"
- Break condition: If the sampling or selection process does not effectively explore the reasoning space or exploit deducible paths, the approach will not improve performance.

## Foundational Learning

- Concept: Deductive reasoning
  - Why needed here: The approach is based on the principle of deductive reasoning, where each reasoning step must logically follow from its premises.
  - Quick check question: Given the premises "All dogs are mammals" and "All mammals are animals", what can you deduce about dogs?

- Concept: Beam search
  - Why needed here: Beam search is used to explore multiple potential reasoning paths and select the most promising ones based on deductive scores.
  - Quick check question: In a beam search with beam size 3, how many candidate paths are considered at each step?

- Concept: Verification
  - Why needed here: The verifier is trained to detect logical deducibility between reasoning steps and their premises, enabling the beam search to select more reliable reasoning paths.
  - Quick check question: What is the difference between a grounding error and a logic error in the context of reasoning?

## Architecture Onboarding

- Component map:
  LLM -> Verifier -> Beam Search -> Final Answer

- Critical path:
  1. LLM generates potential reasoning steps
  2. Verifier scores each step based on logical deducibility
  3. Beam search selects top-scoring steps
  4. Process repeats until final answer is generated

- Design tradeoffs:
  - Sampling times (n) vs. computational cost: Increasing n improves exploration but also increases computational cost.
  - Beam size (m) vs. diversity: Increasing m improves the chances of finding a good reasoning path but also increases computational cost.
  - Verifier accuracy vs. training complexity: Improving verifier accuracy may require more complex training procedures and data.

- Failure signatures:
  - Performance does not improve over baseline methods
  - Verifier scores are not correlated with reasoning quality
  - Beam search selects incorrect reasoning steps

- First 3 experiments:
  1. Verify that the verifier can accurately detect logical deducibility between reasoning steps and their premises on a held-out dataset.
  2. Test the impact of beam size (m) and sampling times (n) on performance to find the optimal configuration.
  3. Compare the performance of DBS with and without the two-stage verifier training process to assess the impact of hard negative synthesis.

## Open Questions the Paper Calls Out
The paper mentions extending the approach to non-arithmetic reasoning tasks as future work, leaving the effectiveness of DBS on other types of reasoning tasks unexplored. While the paper evaluates DBS on models of various scales (7B, 13B, 70B, and ChatGPT), it does not explicitly investigate whether the benefits of DBS increase or decrease as models become larger and more complex. The paper demonstrates effectiveness on reasoning tasks with varying step lengths but does not explore its performance on extremely long reasoning chains involving complex problem-solving or multi-step planning.

## Limitations
- The heuristic error synthesis in stage 1 verifier training may not capture the full diversity of reasoning errors
- Computational overhead of step-wise beam search may limit practical deployment, particularly for real-time applications
- Evaluation focuses on accuracy improvements without extensively examining the quality of reasoning chains themselves

## Confidence
**High confidence**: The general framework of using deductive verification to improve reasoning quality is well-founded, and the empirical results showing consistent accuracy improvements across multiple datasets and model scales are compelling.

**Medium confidence**: The two-stage verifier training methodology is reasonable but relies on assumptions about the quality and diversity of LLM-generated hard negatives that need further validation. The specific parameter choices (beam size, sampling times) are shown to work but may not be optimal for all scenarios.

**Low confidence**: The approach's robustness to different reasoning task types beyond the tested datasets, and its scalability to much larger models or more complex reasoning domains, remain uncertain without additional testing.

## Next Checks
1. **Verifier error analysis**: Conduct a detailed error analysis of the verifier's performance on different error types (grounding, logic, irrelevant) to identify which error categories it handles well versus poorly, and whether the two-stage training effectively improves detection across all categories.

2. **Computational overhead measurement**: Systematically measure the actual computational overhead (time, tokens, energy) of DBS compared to standard CoT prompting across different beam sizes and sampling rates to determine practical deployment constraints.

3. **Generalization test**: Evaluate DBS on reasoning tasks significantly different from the training datasets (e.g., scientific reasoning, legal reasoning) to assess how well the verifier generalizes to novel reasoning domains and whether it maintains performance gains on tasks with different error distributions.