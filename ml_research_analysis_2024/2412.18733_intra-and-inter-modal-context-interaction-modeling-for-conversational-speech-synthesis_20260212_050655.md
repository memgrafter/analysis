---
ver: rpa2
title: Intra- and Inter-modal Context Interaction Modeling for Conversational Speech
  Synthesis
arxiv_id: '2412.18733'
source_url: https://arxiv.org/abs/2412.18733
tags:
- speech
- interaction
- text
- historical
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel conversational speech synthesis (CSS)
  system that explicitly models intra-modal and inter-modal interactions between multimodal
  dialogue history (MDH) and target utterance. The key innovation is using four contrastive
  learning-based interaction modules to capture semantic and prosodic relationships
  across text and speech modalities.
---

# Intra- and Inter-modal Context Interaction Modeling for Conversational Speech Synthesis

## Quick Facts
- arXiv ID: 2412.18733
- Source URL: https://arxiv.org/abs/2412.18733
- Authors: Zhenqi Jia; Rui Liu
- Reference count: 26
- Primary result: Proposed I3-CSS system achieves Naturalness DMOS of 3.864 and Prosody DMOS of 3.876 on DailyTalk dataset, outperforming advanced baselines

## Executive Summary
This paper introduces I3-CSS, a novel conversational speech synthesis system that explicitly models both intra-modal and inter-modal interactions between multimodal dialogue history and target utterance. The system uses four contrastive learning-based interaction modules to capture semantic and prosodic relationships across text and speech modalities. The approach demonstrates significant improvements in generating natural and expressive conversational speech with appropriate prosody, achieving state-of-the-art performance on the DailyTalk dataset.

The key innovation lies in the comprehensive modeling of interactions between historical and target utterances across both text and speech modalities, enabling the system to capture subtle variations in conversational prosody that are crucial for natural dialogue synthesis.

## Method Summary
The I3-CSS system processes multimodal dialogue history (MDH) consisting of previous dialogue turns and target utterance text and speech. Four contrastive learning-based interaction modules are employed: Historical Text-Next Text, Historical Speech-Next Speech, Historical Text-Next Speech, and Historical Speech-Next Text. Each module uses cross-attention mechanisms and contrastive loss functions to learn intra-modal and inter-modal relationships. The system extracts semantic features using Sentence-BERT and prosodic features using Wav2Vec 2.0 fine-tuned on IEMOCAP. Training is performed on A100 GPU with batch size 16 for 400k steps using Adam optimizer, with FastSpeech2 TTS Encoder for linguistic encodings and HiFi-GAN as vocoder.

## Key Results
- Achieved Naturalness DMOS of 3.864, significantly outperforming advanced baselines
- Achieved Prosody DMOS of 3.876, demonstrating superior prosodic modeling
- Showed consistent improvements across objective metrics (MAE-P, MAE-E, MAE-D)

## Why This Works (Mechanism)
The system works by explicitly modeling both intra-modal interactions (within the same modality between historical and target utterances) and inter-modal interactions (across text and speech modalities). The four interaction modules capture semantic relationships in text-text and speech-speech pairs, while also modeling cross-modal semantic-prosodic relationships in text-speech and speech-text pairs. This comprehensive interaction modeling enables the system to generate speech with appropriate conversational prosody that reflects the context of the dialogue.

## Foundational Learning

**Contrastive Learning**: Why needed - To learn meaningful representations by pulling together similar samples and pushing apart dissimilar ones. Quick check - Verify that positive pairs (historical and target utterance features) have higher similarity than negative pairs in the embedding space.

**Cross-attention Mechanisms**: Why needed - To capture interactions between different modalities and temporal contexts. Quick check - Ensure attention weights properly align relevant features between historical and target utterances.

**Multimodal Feature Extraction**: Why needed - To represent both semantic content and prosodic characteristics of speech. Quick check - Validate that Sentence-BERT captures semantic meaning while Wav2Vec 2.0 captures prosodic features effectively.

## Architecture Onboarding

Component Map: DailyTalk Dataset -> Feature Extraction (Sentence-BERT, Wav2Vec 2.0) -> Four Interaction Modules (Text-Text, Speech-Speech, Text-Speech, Speech-Speech) -> Cross-attention -> Contrastive Learning -> TTS Encoder -> HiFi-GAN Vocoder

Critical Path: Feature Extraction → Interaction Modules → Contrastive Learning → TTS Generation → Speech Synthesis

Design Tradeoffs: The system prioritizes comprehensive interaction modeling over computational efficiency, using four separate interaction modules instead of a unified approach. This increases model complexity but enables more precise control over different types of interactions.

Failure Signatures: Poor prosody modeling indicates insufficient contrast in contrastive learning objectives; mode collapse in embeddings suggests improper temperature or margin parameters; degraded naturalness points to inadequate semantic interaction modeling.

First Experiments:
1. Train with only text-text and speech-speech interaction modules to verify intra-modal modeling effectiveness
2. Train with only text-speech and speech-text interaction modules to verify inter-modal modeling effectiveness
3. Test different temperature parameters in contrastive loss to optimize embedding separation

## Open Questions the Paper Calls Out
None

## Limitations
- Limited dataset size (20 hours from only two speakers) may affect generalizability
- Missing specific implementation details for contrastive learning hyperparameters
- Computational complexity due to four separate interaction modules

## Confidence

| Claim | Confidence |
|-------|------------|
| Superior performance on DailyTalk dataset | Medium |
| Effective modeling of intra-modal interactions | Medium |
| Effective modeling of inter-modal interactions | Medium |
| Generalizability to other conversational datasets | Low |

## Next Checks
1. Re-implement the four interaction modules with detailed architectural specifications and test on a different conversational dataset to verify generalizability
2. Conduct ablation studies systematically removing each interaction module to quantify their individual contributions to the final performance
3. Perform analysis on the learned embeddings to verify that the contrastive learning objectives are effectively capturing the intended semantic and prosodic relationships across modalities