---
ver: rpa2
title: 'VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding
  in Video-LLMs'
arxiv_id: '2406.07476'
source_url: https://arxiv.org/abs/2406.07476
tags:
- arxiv
- video
- videollama
- preprint
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VideoLLaMA 2, a set of Video Large Language
  Models designed to advance spatial-temporal modeling and audio understanding in
  video-language tasks. The key innovation is a Spatial-Temporal Convolution (STC)
  connector that effectively captures spatial and temporal dynamics, combined with
  an integrated Audio Branch for joint training.
---

# VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs

## Quick Facts
- arXiv ID: 2406.07476
- Source URL: https://arxiv.org/abs/2406.07476
- Reference count: 32
- Introduces VideoLLaMA 2 with Spatial-Temporal Convolution (STC) connector and integrated Audio Branch

## Executive Summary
This paper introduces VideoLLaMA 2, a set of Video Large Language Models designed to advance spatial-temporal modeling and audio understanding in video-language tasks. The key innovation is a Spatial-Temporal Convolution (STC) connector that effectively captures spatial and temporal dynamics, combined with an integrated Audio Branch for joint training. This approach enables superior multimodal comprehension across video and audio-oriented tasks. Comprehensive evaluations show that VideoLLaMA 2 achieves competitive results among open-source models and even approaches some proprietary models on multiple benchmarks, demonstrating significant improvements in audio-only and audio-video question-answering tasks. The authors provide all models publicly to facilitate further research.

## Method Summary
VideoLLaMA 2 employs a modular architecture with separate visual and audio branches connected to a large language model (LLM). The visual branch uses CLIP (ViT-L/14) encoder followed by a Spatial-Temporal Convolution (STC) connector that performs early fusion of frame-level features through 3D convolution, reducing token count while preserving local details. The audio branch processes audio signals into log mel spectrograms using BEATs encoder, then projects features to match LLM dimensions. The model uses Mistral-Instruct (7B), Mixtral-Instruct (8×7B), and Qwen2-Instruct (7B/72B) as language decoders. Training follows a staged approach: video-only pre-training, video-only fine-tuning, audio-only pre-training, audio-only fine-tuning, and joint audio-video fine-tuning on diverse tasks including video captioning, classification, VQA, and instruction following.

## Key Results
- Achieves competitive results among open-source models on EgoSchema, Perception-Test, and Video-MME benchmarks
- Approaches GPT-4V performance on VC task with correctness score of 2.57 vs GPT-4V's 2.70
- Demonstrates significant improvements on audio-only AQA tasks on Clotho-AQA benchmark
- Scaling LLM from 7B to 8×7B parameters shows notable performance gains across multiple benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Spatial-Temporal Convolution (STC) connector effectively captures spatial-temporal dynamics by performing early fusion of frame-level features, reducing token count while preserving local details.
- Mechanism: The STC connector uses 3D convolution to downsample spatial-temporal tokens, followed by RegStage blocks that preserve local visual patterns. This allows the LLM to process fewer tokens while retaining rich spatial-temporal information.
- Core assumption: Early fusion of frame features before LLM processing is more effective than treating each frame independently and relying on the LLM for temporal reasoning.
- Evidence anchors:
  - [abstract] "Building upon its predecessor, VideoLLaMA 2 incorporates a tailor-made Spatial-Temporal Convolution (STC) connector, which effectively captures the intricate spatial and temporal dynamics of video data."
  - [section] "The STC Connector could preserve spatial and temporal local details more effectively than the Q-former while not producing a large number of video tokens."
  - [corpus] Weak - no direct comparison to Q-former or other connectors in corpus
- Break condition: If the 3D convolution downsampling causes excessive information loss that cannot be compensated by the RegStage blocks, performance would degrade on tasks requiring fine-grained spatial-temporal understanding.

### Mechanism 2
- Claim: Joint training of audio and video branches enables effective multimodal integration by allowing cross-modal interactions to occur within the highly capable language model.
- Mechanism: Separate visual and audio encoders process their respective modalities independently, then feed into the LLM where cross-modal reasoning happens. The audio branch uses BEATs encoder for detailed audio features and temporal dynamics.
- Core assumption: The LLM is sufficiently capable of integrating multimodal information when provided with properly aligned audio and visual features, making separate modality processing more efficient than early fusion.
- Evidence anchors:
  - [section] "Both branches operate independently, connecting pre-trained visual and audio encoders to an instruction-finetuned large language model in a modular fashion. This modality-specific independence... not only allows streamlined training by preserving the integrity of individual modal inputs, but also facilitates future expansions and adaptations."
  - [section] "These features are then processed through a MLP block to align with the dimension of LLMs, therefore providing a more cohesive understanding of the video content when combined with the visual and acoustic modalities."
  - [corpus] Weak - corpus mentions other approaches but doesn't directly compare this modular approach to alternatives
- Break condition: If the LLM cannot effectively integrate information from the two separate branches, the model would underperform compared to approaches with earlier cross-modal fusion.

### Mechanism 3
- Claim: Scaling the language model backbone significantly improves performance on multimodal tasks, with the model approaching proprietary models when using larger LLM variants.
- Mechanism: The model uses Mistral-Instruct (7B), Mixtral-Instruct (8×7B), and Qwen2-Instruct (7B/72B) as language decoders, with performance scaling observed as model size increases.
- Core assumption: The visual and audio processing components are sufficiently powerful that improvements in language understanding directly translate to better multimodal reasoning capabilities.
- Evidence anchors:
  - [section] "Scaling up the LLM backbone from Mistral (7B) to Mixtral (8×7B) further enhances model performance in MC-VQA. This upscaling results in notable improvements across multiple benchmarks, with VideoLLaMA 2-8×7B achieving the highest accuracies on Egoschema, Perception-Test, and Video-MME."
  - [section] "In the VC task, VideoLLaMA 2 performs well on the MSVC benchmark, scoring 2.57 in correctness and 2.61 in detailedness. While these scores are slightly lower than GPT4-V's 2.70 and 2.76, they are higher than all other open-source models."
  - [corpus] Moderate - corpus contains other VideoLLM papers but doesn't directly address the scaling relationship between LLM size and multimodal performance
- Break condition: If the visual and audio processing components become bottlenecks, further scaling of the LLM would yield diminishing returns or even degrade performance due to overfitting or increased computational complexity.

## Foundational Learning

- Concept: Video frame sampling and preprocessing
  - Why needed here: VideoLLaMA 2 processes videos by sampling a fixed number of frames (8, 16, or 32) and resizing them to 336×336 pixels before feeding into the visual encoder
  - Quick check question: If you have a 30-second video at 30fps and need to sample 16 frames evenly, how many seconds apart should each frame be sampled?

- Concept: Audio preprocessing and feature extraction
  - Why needed here: Audio signals are transformed into log mel spectrograms with 128 frequency bins before being encoded by BEATs, which captures detailed audio features and temporal dynamics
  - Quick check question: What is the purpose of converting raw audio waveforms into log mel spectrograms before feeding them into an audio encoder?

- Concept: Multimodal instruction tuning
  - Why needed here: The model is fine-tuned on diverse tasks including video captioning, video classification, VQA, and instruction following using data from multiple sources like VideoChat, Kinetics-710, and LLaVA
  - Quick check question: Why is it beneficial to mix image-text data (like LLaVA) with video-text data during instruction tuning of a video-language model?

## Architecture Onboarding

- Component map:
  - Visual Branch: CLIP (ViT-L/14) encoder → STC connector → LLM
  - Audio Branch: Audio preprocessing → BEATs encoder → MLP alignment → LLM
  - LLM: Mistral-Instruct/Mixtral-Instruct/Qwen2-Instruct as language decoder
  - Training pipeline: Video-only pre-training → Video-only fine-tuning → Audio-only pre-training → Audio-only fine-tuning → Joint audio-video fine-tuning

- Critical path: Video frames → CLIP encoder → STC connector → LLM (primary path for video understanding); Audio → BEATs encoder → MLP → LLM (secondary path for audio integration)

- Design tradeoffs:
  - Early temporal fusion vs. LLM-based temporal reasoning (STC connector enables early fusion)
  - Separate modality processing vs. early cross-modal fusion (modular design preserves individual modality integrity)
  - Model size vs. computational efficiency (scaling LLM improves performance but increases cost)

- Failure signatures:
  - Poor performance on temporal reasoning tasks despite good spatial understanding suggests STC connector issues
  - Weak audio-visual integration despite strong unimodal performance suggests LLM integration problems
  - Degradation when scaling LLM suggests bottleneck in visual/audio processing components

- First 3 experiments:
  1. Reproduce baseline VideoLLaMA performance on EgoSchema with 8 frames to verify visual processing pipeline
  2. Test STC connector ablation by comparing 2D pooling vs 3D convolution downsampling on MV-Bench
  3. Validate audio branch integration by evaluating audio-only AQA performance on Clotho-AQA before joint training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of VideoLLaMA 2 compare to other models when trained on larger datasets of static images?
- Basis in paper: [inferred] The paper mentions that LLaVA-NeXT-Video, which is trained primarily on static image data, shows advantages in benchmarks where static visual information is crucial for answering questions.
- Why unresolved: The paper does not directly compare the performance of VideoLLaMA 2 with other models when trained on larger datasets of static images.
- What evidence would resolve it: Conducting experiments to train VideoLLaMA 2 on larger datasets of static images and comparing its performance with other models on various benchmarks.

### Open Question 2
- Question: How does the performance of VideoLLaMA 2 change when using different pre-trained visual encoders?
- Basis in paper: [explicit] The paper mentions that VideoLLaMA 2 uses CLIP (ViT-L/14) as its primary visual encoder but prefers SigLip-so400m-384 in later variants due to its superior performance.
- Why unresolved: The paper does not provide a comprehensive comparison of VideoLLaMA 2's performance when using different pre-trained visual encoders.
- What evidence would resolve it: Conducting experiments to train VideoLLaMA 2 with different pre-trained visual encoders and comparing its performance on various benchmarks.

### Open Question 3
- Question: How does the performance of VideoLLaMA 2 change when using different large language models as the language decoder?
- Basis in paper: [explicit] The paper mentions that VideoLLaMA 2 uses Mistral-Instruct, Mixtral-Instruct, or Qwen2-Instruct as the language decoder, and scaling up the LLM backbone from Mistral (7B) to Mixtral (8×7B) enhances model performance.
- Why unresolved: The paper does not provide a comprehensive comparison of VideoLLaMA 2's performance when using different large language models as the language decoder.
- What evidence would resolve it: Conducting experiments to train VideoLLaMA 2 with different large language models as the language decoder and comparing its performance on various benchmarks.

## Limitations
- Modular design separating visual and audio processing may limit cross-modal interactions that could occur with earlier fusion approaches
- Evaluation focuses primarily on open-source benchmarks without extensive comparison to other state-of-the-art proprietary models beyond GPT-4V
- Scalability analysis is limited to progression from 7B to 8×7B and 72B parameter models, without exploring intermediate scales or cost-benefit tradeoffs

## Confidence
- High Confidence: The core claims about the STC connector's effectiveness in capturing spatial-temporal dynamics and the modular architecture's benefits for training efficiency are well-supported by the presented results and reasonable architectural choices.
- Medium Confidence: The assertion that joint training of audio and video branches enables effective multimodal integration is supported by results but lacks extensive ablation studies comparing different fusion strategies.
- Low Confidence: The claims about the STC connector being superior to Q-former without direct comparison in the corpus, and the specific design choices for the RegStage blocks, lack sufficient comparative evidence in the paper.

## Next Checks
1. **STC Connector Ablation Study**: Conduct a controlled experiment comparing the 3D convolution-based STC connector against alternative downsampling approaches (such as Q-former, 2D pooling, or temporal attention) on temporal reasoning tasks like MV-Bench and EgoSchema. This would validate whether the claimed advantages of early spatial-temporal fusion are genuine and significant.

2. **Audio Branch Contribution Analysis**: Perform a systematic ablation study isolating the contribution of audio features by evaluating models with: (a) video-only, (b) audio-only, (c) joint audio-video, and (d) early audio-visual fusion across audio-specific benchmarks like Clotho-AQA and audio-video tasks on MV-Bench. This would quantify the actual benefit of the separate audio branch approach.

3. **Cross-Modal Fusion Comparison**: Implement and evaluate an alternative architecture where audio and visual features are fused earlier in the pipeline (before the LLM) using attention mechanisms, and compare its performance against the modular approach on the same benchmarks. This would test the core assumption that LLM-based cross-modal integration is sufficient and whether earlier fusion might provide benefits.