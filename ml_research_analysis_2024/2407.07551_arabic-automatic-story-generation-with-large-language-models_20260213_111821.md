---
ver: rpa2
title: Arabic Automatic Story Generation with Large Language Models
arxiv_id: '2407.07551'
source_url: https://arxiv.org/abs/2407.07551
tags:
- story
- stories
- arabic
- prompt
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first LLM-based Arabic story generation
  system, focusing on Modern Standard Arabic (MSA) and two Arabic dialects (Egyptian
  and Moroccan). The authors develop a careful pipeline for acquiring high-quality
  translated stories from TinyStories and generate additional synthetic stories using
  GPT-4-Turbo with carefully designed prompts.
---

# Arabic Automatic Story Generation with Large Language Models

## Quick Facts
- arXiv ID: 2407.07551
- Source URL: https://arxiv.org/abs/2407.07551
- Reference count: 6
- Primary result: First LLM-based Arabic story generation system for MSA and two Arabic dialects (Egyptian and Moroccan)

## Executive Summary
This work presents the first large language model (LLM)-based system for Arabic story generation, focusing on Modern Standard Arabic (MSA) and two Arabic dialects (Egyptian and Moroccan). The authors develop a pipeline that translates English stories from TinyStories, generates additional synthetic stories using GPT-4-Turbo, and fine-tunes the AraLLaMa-2-base model using two strategies: direct fine-tuning on GPT-4-generated data and sequential fine-tuning (first on translated data, then on GPT-4-generated data). Manual and automatic evaluations show that their models produce coherent stories that follow instructions, performing competitively against stronger models including Command-R (35B) and GPT-3.5.

## Method Summary
The authors translate 1.13M English stories from TinyStories to Arabic using Google Translate, filtering translations with a 92% similarity threshold using multilingual sentence embeddings. They generate 3K synthetic stories using GPT-4-Turbo with custom prompts covering MSA, Egyptian, and Moroccan dialects. Two fine-tuning strategies are explored: direct fine-tuning on GPT-4-generated data (Model A) and sequential fine-tuning (translated data → GPT-4-generated data) (Model B). Both models use QLoRA fine-tuning with AdamW optimizer (learning rate 4e-5, batch size 1, 20 epochs) on AraLLaMa-2-base (7B parameters).

## Key Results
- Models produce coherent stories following instructions across MSA, Egyptian, and Moroccan varieties
- Sequential fine-tuning (Model B) outperforms direct fine-tuning on almost all metrics across dialects
- Models perform competitively against Command-R (35B) and GPT-3.5 in manual evaluation
- Instruction-following is particularly strong across all Arabic varieties

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on dialect-specific prompts improves dialectal story generation more than using MSA prompts.
- Mechanism: Dialects have different vocabularies, grammar, and idioms; exposure to these features during fine-tuning allows the model to learn and generate text that matches the target variety.
- Core assumption: GPT-4 can generate coherent stories when prompted in different Arabic varieties.
- Evidence anchors:
  - Based on our preliminary observations, GPT-4 is able to generate coherent stories from dialectal prompts (i.e., Egyptian and Moroccan). Hence, we ask two native speakers to translate our prompt template, originally written in MSA to Egyptian and Moroccan dialects.
  - We find that GPT-4 tends to score the MSA content higher than dialectal ones, even if the task is to generate a dialectal story. To mitigate this issue, we added this last feature where we explicitly ask GPT-4 if the generated content follows the required Arabic variety specified in the prompt or not, and to which degree.
- Break condition: If the model fails to distinguish between varieties in generated stories or if manual evaluation shows no improvement in dialect adherence.

### Mechanism 2
- Claim: Two-step fine-tuning (translated data → GPT-4-generated data) improves dialectal performance more than direct fine-tuning on GPT-4-generated data.
- Mechanism: Initial fine-tuning on translated data builds general Arabic fluency and MSA competence, while the second step adds variety-specific nuances and improves instruction-following in dialects.
- Core assumption: Translating from English stories preserves key narrative structures while adapting them to Arabic.
- Evidence anchors:
  - Model B which was exposed to additional training steps on translated data, performs better than Model A on almost all metrics across dialects, which proves indeed the intuition behind training on more data does help.
  - We translate 1.13M English stories generated by GPT-4 alongside their prompts from the TinyStories dataset (Eldan and Li, 2023) using Google translate API.
- Break condition: If the translation introduces artifacts that harm fluency or if the second fine-tuning step degrades MSA performance.

### Mechanism 3
- Claim: Prompt template diversity (12 features with controlled probabilities) yields a broader range of generated stories and improves evaluation coverage.
- Mechanism: Varying features like age, emotion, topic, and country forces the model to adapt its output to multiple contexts, increasing robustness and reducing repetitive patterns.
- Core assumption: GPT-4 can handle multi-feature prompts and produce coherent outputs.
- Evidence anchors:
  - Our template is designed in such a way that each feature has a probability p of appearance in a particular prompt. Meaning some features might be present in a prompt while others are not. Except for the following features where they appear in each prompt: age, number of characters, and country.
  - We design our prompt template with two goals in mind. These are (i) to ensure high quality of the generated output and (ii) make the generated output as diverse as possible.
- Break condition: If the model ignores features or produces incoherent stories when too many features are included.

## Foundational Learning

- Concept: Machine translation quality filtering using sentence embeddings.
  - Why needed here: Ensures only high-quality Arabic translations are used for fine-tuning, preventing noise from corrupting model learning.
  - Quick check question: What threshold did the authors use to filter translations, and why was that value chosen?

- Concept: QLoRA fine-tuning (4-bit quantization + LoRA adapters).
  - Why needed here: Allows efficient fine-tuning of large models on limited hardware while preserving base model weights.
  - Quick check question: Which layer dimensions and hyperparameters were used in the QLoRA layer?

- Concept: Automatic evaluation with GPT-4 as judge.
  - Why needed here: Provides a scalable, language-aware scoring mechanism for Arabic story quality across multiple criteria.
  - Quick check question: How did the authors mitigate GPT-4's MSA bias when evaluating dialectal stories?

## Architecture Onboarding

- Component map: TinyStories translation → filtering → GPT-4 generation → storage → QLoRA fine-tuning → evaluation → comparison
- Critical path: Data preparation → model fine-tuning → evaluation → comparison
- Design tradeoffs:
  - Model size (7B) vs. computational cost
  - Dataset size (3K samples) vs. coverage of dialects
  - Translation quality vs. dataset size (filtering reduces samples from 1.13M to 545K)
- Failure signatures:
  - Low variety scores → prompt template too restrictive or filtering too aggressive
  - Inconsistent dialect generation → insufficient dialect-specific fine-tuning data
  - Poor instruction-following → model overfits to training prompts or lacks contextual understanding
- First 3 experiments:
  1. Fine-tune AraLLaMa-2-base on 3K GPT-4-generated stories (Model A)
  2. Fine-tune on 545K translated stories for 15K steps, then on GPT-4 data (Model B)
  3. Evaluate both models using GPT-4 judge prompt and human comparison against Command-R and AceGPT-7B-Chat

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the two fine-tuning strategies (direct fine-tuning vs sequential fine-tuning) perform on dialects beyond Egyptian and Moroccan Arabic?
- Basis in paper: [explicit] The authors state that sequential fine-tuning showed advantages for dialectal performance and limited their work to Egyptian and Moroccan dialects, mentioning they tested other dialects but found the content mostly MSA.
- Why unresolved: The paper only reports results on Egyptian and Moroccan dialects. The authors mention testing other dialects but don't provide comparative results.
- What evidence would resolve it: Experimental results comparing both fine-tuning strategies across a broader range of Arabic dialects (Levantine, Gulf, Iraqi, etc.) with quantitative metrics.

### Open Question 2
- Question: What is the impact of dataset size on model performance, and would increasing training data beyond 3,000 samples significantly improve results?
- Basis in paper: [explicit] The authors acknowledge their training dataset consisted of only 3,000 samples and mention plans to generate more data with GPT-4o in the future.
- Why unresolved: The paper uses a relatively small dataset and doesn't explore the relationship between dataset size and performance through systematic experiments.
- What evidence would resolve it: Experiments showing model performance across different dataset sizes (e.g., 3K, 10K, 30K, 100K samples) with corresponding evaluation metrics.

### Open Question 3
- Question: How does the quality of translated data affect the performance of the sequential fine-tuning approach, and what is the optimal similarity threshold for filtering translations?
- Basis in paper: [inferred] The authors describe their filtering strategy using multilingual sentence embeddings with a 92% similarity threshold, but don't explore how different thresholds affect downstream model performance.
- Why unresolved: The paper uses a fixed similarity threshold (92%) without exploring how different thresholds impact the quality of the fine-tuned models.
- What evidence would resolve it: Comparative experiments using different similarity thresholds (e.g., 85%, 90%, 95%) for filtering translations, with corresponding model performance metrics on both MSA and dialectal story generation.

## Limitations
- Small evaluation dataset (300 samples) may not provide robust statistical significance across all metrics and varieties
- Heavy reliance on GPT-4 as automatic judge introduces potential MSA bias in evaluation
- Human evaluation involved only 3 raters, limiting generalizability of results

## Confidence
- **High confidence**: The basic premise that Arabic story generation using LLMs is feasible and that fine-tuning improves performance over zero-shot baselines. The manual evaluation showing reasonable instruction-following across varieties is well-supported.
- **Medium confidence**: The claim that two-step fine-tuning (translated data → GPT-4 data) outperforms direct fine-tuning on GPT-4 data. While supported by the results, the evaluation setup and potential GPT-4 judge bias create uncertainty.
- **Low confidence**: The claim that their models perform "competitively" against Command-R (35B) and GPT-3.5, given that these comparisons involve different model scales and the evaluation methodology has acknowledged limitations.

## Next Checks
1. Conduct a blind human evaluation where raters are unaware of which model generated which story to eliminate expectation bias, particularly focusing on dialectal variety adherence and instruction-following.
2. Perform ablation studies varying the amount of translated data in the first fine-tuning step to determine the optimal balance between MSA fluency and dialectal variety generation.
3. Test model generalization by evaluating on held-out dialectal prompts that were not seen during training to assess true instruction-following capability rather than memorization.