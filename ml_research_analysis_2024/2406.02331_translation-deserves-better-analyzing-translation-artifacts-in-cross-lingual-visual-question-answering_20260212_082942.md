---
ver: rpa2
title: 'Translation Deserves Better: Analyzing Translation Artifacts in Cross-lingual
  Visual Question Answering'
arxiv_id: '2406.02331'
source_url: https://arxiv.org/abs/2406.02331
tags:
- translation
- human
- evaluation
- language
- translate-test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Translation artifacts in machine-translated text can significantly
  impact cross-lingual visual question answering (VQA) performance. Models trained
  on translated text often outperform those trained on human-written text when evaluated
  on translated questions, suggesting translation artifacts introduce a distributional
  shift that affects model behavior.
---

# Translation Deserves Better: Analyzing Translation Artifacts in Cross-lingual Visual Question Answering

## Quick Facts
- arXiv ID: 2406.02331
- Source URL: https://arxiv.org/abs/2406.02331
- Authors: ChaeHun Park; Koanho Lee; Hyesu Lim; Jaeseok Kim; Junmo Park; Yu-Jung Heo; Du-Seong Chang; Jaegul Choo
- Reference count: 40
- Primary result: Translation artifacts in machine-translated text can significantly impact cross-lingual visual question answering (VQA) performance

## Executive Summary
This study investigates how translation artifacts in machine-translated text affect cross-lingual visual question answering performance. The authors demonstrate that models trained on translated text often outperform those trained on human-written text when evaluated on translated questions, suggesting that translation artifacts introduce a distributional shift that affects model behavior. Through extensive experiments across 14 models, 13 languages, and multiple translation systems, the study confirms this trend and proposes a simple data augmentation strategy using both human and machine-translated texts to mitigate these effects and improve overall performance.

## Method Summary
The research employs round-trip translation (RT) with German pivot to generate translation artifacts in the xGQA training data, using NLLB-200-3.3B as the primary machine translation system. VL models are fine-tuned on both human-written and RT-translated English training data, then evaluated on target language questions translated into English using a translate-test approach. The study compares performance when training and evaluation data origins are aligned versus mismatched, conducting statistical significance tests (t-tests) to validate findings. Human evaluation of translation quality is performed on sampled questions, and a data augmentation strategy combining human and machine-translated texts is tested as a mitigation approach.

## Key Results
- Models trained on translated text consistently outperform those trained on human text when evaluated on translated questions
- The performance difference is statistically significant across 14 models and 13 languages
- Simple data augmentation using both human and machine-translated texts mitigates translation artifact effects and improves overall VQA performance

## Why This Works (Mechanism)
Translation artifacts introduce a distributional shift that affects model behavior. When models are trained on translated text, they learn to exploit these artifacts as cues, leading to better performance on similarly translated test data. This creates a performance advantage when training and evaluation data share similar translation characteristics.

## Foundational Learning
1. **Round-trip translation (RT)**: Translating text to a pivot language and back to the source language. Why needed: Creates controlled translation artifacts for analysis. Quick check: Compare original and RT-translated text for obvious errors or unnatural phrasing.

2. **Cross-lingual visual question answering**: VQA models that can answer questions about images in languages different from the training language. Why needed: Central task being evaluated. Quick check: Ensure model can process both visual inputs and multilingual text.

3. **Translate-test approach**: Translating test questions into the training language for evaluation. Why needed: Standard practice in cross-lingual VQA research. Quick check: Verify translation quality of test questions before evaluation.

4. **Data augmentation with mixed translation sources**: Combining human and machine-translated training data. Why needed: Mitigates negative effects of translation artifacts. Quick check: Monitor training loss curves for convergence when using mixed data.

## Architecture Onboarding

**Component Map**: Training data (human/RT) -> VL model fine-tuning -> Evaluation on translated test data -> Performance comparison

**Critical Path**: RT translation of training data → Model fine-tuning on translated data → Translate-test evaluation → Performance analysis

**Design Tradeoffs**: 
- RT translation with German pivot provides controlled artifacts but may not represent all translation errors
- Translate-test approach is standard but introduces additional translation artifacts during evaluation
- Data augmentation improves robustness but requires more computational resources for training

**Failure Signatures**: 
- Performance degradation when training/evaluation data translation sources mismatch
- Models exploiting translation artifacts rather than visual content
- Inconsistent results across different translation systems

**First Experiments**:
1. Fine-tune a VL model on human-written training data and evaluate on both human and RT-translated test questions
2. Fine-tune the same model on RT-translated training data and evaluate on both human and RT-translated test questions
3. Implement data augmentation by mixing human and RT-translated training data, then evaluate performance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation methodology relies on translating target language questions into English, introducing additional translation artifacts
- RT translation with German pivot may not represent all types of translation errors that occur in real-world scenarios
- Human evaluation of translation quality only sampled 200 questions per language, potentially missing broader variability

## Confidence
- High confidence: Translation artifacts create measurable performance differences in cross-lingual VQA
- Medium confidence: RT translation with German pivot reliably introduces artifacts that affect model behavior
- Medium confidence: Simple data augmentation can mitigate translation artifact effects

## Next Checks
1. Replicate experiments using a different translation system (e.g., Google Translate or another NLLB variant) to verify that observed artifact patterns are consistent across translation systems rather than specific to NLLB-200-3.3B

2. Conduct experiments with direct human translation of training data (rather than machine translation) to establish a stronger baseline for comparison with RT-translated data

3. Evaluate the data augmentation strategy on a different cross-lingual VQA dataset (e.g., MaXM or a newly created dataset) to test generalizability beyond xGQA