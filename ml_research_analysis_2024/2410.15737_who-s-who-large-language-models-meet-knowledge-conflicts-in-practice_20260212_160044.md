---
ver: rpa2
title: 'Who''s Who: Large Language Models Meet Knowledge Conflicts in Practice'
arxiv_id: '2410.15737'
source_url: https://arxiv.org/abs/2410.15737
tags:
- subject
- knowledge
- conflicts
- questions
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WhoQA, a dataset designed to evaluate how
  large language models (LLMs) handle knowledge conflicts in retrieval-augmented generation
  settings. The dataset induces conflicts by asking about shared properties among
  entities with the same name, resulting in questions with up to 8 different answers.
---

# Who's Who: Large Language Models Meet Knowledge Conflicts in Practice

## Quick Facts
- arXiv ID: 2410.15737
- Source URL: https://arxiv.org/abs/2410.15737
- Reference count: 40
- Primary result: Knowledge conflicts significantly degrade LLM performance in retrieval-augmented generation settings, even when individual contexts are simple to answer.

## Executive Summary
This paper introduces WhoQA, a dataset designed to evaluate how large language models handle knowledge conflicts in retrieval-augmented generation settings. The dataset induces conflicts by asking about shared properties among entities with the same name, resulting in questions with up to 8 different answers. Experiments with 10 open-source LLMs and GPT-3.5-turbo show that knowledge conflicts significantly degrade model performance, even though individual contexts are simple to answer. The paper demonstrates that explicitly specifying the presence of conflicts in prompts improves performance for most models, highlighting the need for transparent conflict handling in RAG systems.

## Method Summary
The study uses the WhoQA dataset containing 5,152 questions across 13 Wikidata property types and 150,000 Wikipedia entities. Researchers evaluate 10 open-source LLMs and GPT-3.5-turbo using vLLM with bfloat16 floating-point format on 8 Nvidia A100 40GB GPUs. Experiments are conducted in three settings: simple QA (no conflict), knowledge conflict without specification, and knowledge conflict with specification. Models are evaluated using macro average accuracy over the number of conflicting answers, with sampling parameters including top_k = 50 tokens, top_p = 0.9, max_tokens = 512, and temperature = 0.9.

## Key Results
- Knowledge conflicts significantly degrade LLM performance in RAG settings, with models often missing answers or providing biased responses
- Explicitly specifying the presence of conflicts in prompts improves performance for most models, demonstrating the importance of conflict awareness
- LLMs perform better on questions with more conflicting answers, suggesting they are less sensitive to subtle conflicts within input contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge conflicts significantly degrade LLM performance even when individual contexts are simple to answer.
- Mechanism: When multiple conflicting contexts are provided, LLMs may either ignore the contextual knowledge and rely on their parametric memory, or prefer certain contextual knowledge based on popularity or input ordering. This leads to information loss and bias.
- Core assumption: LLMs are not inherently designed to recognize and transparently handle knowledge conflicts within their input contexts.
- Evidence anchors:
  - [abstract] "knowledge conflicts significantly degrades LLMs' performance in RAG settings"
  - [section] "without being noticed, LLMs are not sensitive to subtle knowledge conflicts"
- Break condition: If LLMs are explicitly trained or prompted to recognize and handle knowledge conflicts, this mechanism would break down.

### Mechanism 2
- Claim: Explicitly specifying the presence of conflicts in prompts improves LLM performance for most models.
- Mechanism: When LLMs are notified about conflicts through prompt engineering (e.g., few-shot examples), they can better handle the conflicting information by providing all available answers instead of selecting a single biased answer.
- Core assumption: LLMs can be guided through prompt engineering to recognize and properly handle knowledge conflicts.
- Evidence anchors:
  - [abstract] "explicitly specifying the presence of conflicts in prompts improves performance for most models"
  - [section] "We modify the prompt template to include few-shot examples, notifying LLMs of knowledge conflicts within their input context. Results in Table 1 show significant performance improvement for most models."
- Break condition: If certain LLMs (like Llama 3 models mentioned) already recognize conflicts without explicit specification, this mechanism wouldn't apply to them.

### Mechanism 3
- Claim: LLMs are less sensitive to subtle conflicts within their input contexts.
- Mechanism: As the number of conflicting answers increases, LLMs perform better, suggesting they struggle to detect subtle conflicts but can handle more obvious, multiple conflicting sources.
- Core assumption: The complexity of conflict detection follows a non-linear pattern where subtle conflicts are harder to detect than obvious ones.
- Evidence anchors:
  - [section] "Table 2 shows that LLMs generally perform better on questions with more conflicting answers. This suggests that LLMs are less sensitive to subtle conflicts within their input contexts."
- Break condition: If LLMs are fine-tuned specifically for conflict detection, they might become equally sensitive to both subtle and obvious conflicts.

## Foundational Learning

- Concept: Knowledge Conflicts in RAG Systems
  - Why needed here: Understanding how conflicting information from different sources affects LLM performance is crucial for building robust RAG systems.
  - Quick check question: What happens when an LLM receives two conflicting pieces of information about the same entity?

- Concept: Prompt Engineering for Conflict Handling
  - Why needed here: The paper shows that explicit conflict specification in prompts significantly improves performance, highlighting the importance of proper prompt design.
  - Quick check question: How would you modify a prompt to notify an LLM about potential knowledge conflicts?

- Concept: Multi-answer Question Answering
  - Why needed here: WhoQA is a multi-answer dataset, so understanding how to evaluate and generate multiple correct answers is essential.
  - Quick check question: How do you determine if an LLM's response to a multi-answer question is correct?

## Architecture Onboarding

- Component map: Data Collection -> Question Generation -> Context Retrieval -> LLM Evaluation -> Performance Analysis
- Critical path: Data Collection → Question Generation → Context Retrieval → LLM Evaluation → Performance Analysis
- Design tradeoffs:
  - Using Wikipedia as a trusted source ensures factual consistency but may limit the diversity of conflicts
  - Manual annotation ensures quality but is time-consuming and may introduce bias
  - Simple context matching makes the dataset accessible but may not challenge advanced multi-hop reasoning
- Failure signatures:
  - LLMs consistently missing answers from individual contexts (indicating conflict handling issues)
  - Models providing single answers when multiple are expected (indicating bias)
  - Performance degradation proportional to the number of conflicts (indicating sensitivity issues)
- First 3 experiments:
  1. Test a simple QA setting where each question is split into turns with single contexts to establish baseline performance
  2. Evaluate LLM performance with conflicting contexts without specifying conflicts to measure inherent conflict handling
  3. Test performance with explicitly specified conflicts using modified prompts to measure the impact of conflict awareness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on multi-hop questions with knowledge conflicts compared to single-hop questions?
- Basis in paper: [inferred] The paper mentions that the simplicity of the matching process results in mostly single-hop questions in WhoQA, and suggests that inducing knowledge conflicts for multi-hop questions could be a future direction.
- Why unresolved: The current WhoQA dataset primarily contains single-hop questions due to the simple matching process used to collect supporting contexts. The paper acknowledges that multi-hop questions could be more challenging and provide a better testbed for evaluating LLMs' ability to handle knowledge conflicts.
- What evidence would resolve it: Creating a new dataset with multi-hop questions that induce knowledge conflicts, and evaluating LLMs' performance on this dataset compared to single-hop questions in WhoQA.

### Open Question 2
- Question: What fine-tuning methods can improve LLMs' ability to handle knowledge conflicts and provide more informative responses?
- Basis in paper: [explicit] The paper mentions that looking at fine-tuning methods which control how LLMs deal with unfamiliar examples is a promising direction for future research.
- Why unresolved: The paper acknowledges that LLMs often miss some answers or provide biased responses when dealing with knowledge conflicts. While the paper suggests that explicitly specifying the presence of conflicts in prompts can improve performance, it leaves open the question of how to make LLMs' responses more informative in case of conflicts.
- What evidence would resolve it: Experimenting with various fine-tuning techniques that focus on teaching LLMs to handle knowledge conflicts, and evaluating the quality and informativeness of their responses in RAG settings.

### Open Question 3
- Question: How do LLMs' performance and behavior differ when dealing with knowledge conflicts in RAG settings compared to standalone question-answering settings?
- Basis in paper: [explicit] The paper primarily focuses on evaluating LLMs' performance in RAG settings with knowledge conflicts, but also mentions a simple QA scenario where only a single context is presented to the model, hence no conflict.
- Why unresolved: The paper shows that knowledge conflicts significantly degrade LLMs' performance in RAG settings, but it does not provide a direct comparison with standalone question-answering settings. Understanding how LLMs' behavior and performance differ in these two settings could provide insights into the challenges and opportunities of handling knowledge conflicts in RAG systems.
- What evidence would resolve it: Conducting experiments to compare LLMs' performance and behavior in RAG settings with knowledge conflicts and standalone question-answering settings, and analyzing the differences in their responses and accuracy.

## Limitations
- The paper's claims about LLMs' inherent inability to handle knowledge conflicts rely on experiments with specific open-source models and a single prompt engineering approach
- The manual annotation process for creating the WhoQA dataset introduces potential subjectivity in determining what constitutes a valid knowledge conflict
- The effectiveness of conflict detection may vary significantly across different model architectures and prompting strategies

## Confidence
- High: LLMs perform worse on questions with knowledge conflicts compared to simple QA settings
- Medium: Explicitly specifying conflicts in prompts improves LLM performance for most models
- Medium: LLMs are less sensitive to subtle conflicts within input contexts, showing better performance with more obvious conflicts

## Next Checks
1. Test additional LLM architectures beyond the 10 open-source models evaluated, including smaller and larger variants, to verify if the conflict handling patterns generalize across model scales.
2. Implement alternative prompt engineering techniques (chain-of-thought, direct instruction, or example-based approaches) to determine if conflict specification improvements are consistent across different prompting strategies.
3. Conduct human evaluation studies to assess whether LLM-generated answers with multiple conflicting contexts are more useful and informative than single-answer responses, even when accuracy metrics are lower.