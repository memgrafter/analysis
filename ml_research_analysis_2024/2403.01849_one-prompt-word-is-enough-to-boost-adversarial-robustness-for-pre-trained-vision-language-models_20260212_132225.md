---
ver: rpa2
title: One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language
  Models
arxiv_id: '2403.01849'
source_url: https://arxiv.org/abs/2403.01849
tags:
- adversarial
- robustness
- prompt
- class
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the vulnerability of large pre-trained vision-language
  models (VLMs) like CLIP to adversarial attacks. While most research focuses on adapting
  model weights for robustness, this paper investigates improving adversarial robustness
  by tuning the text prompt used by VLMs.
---

# One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models

## Quick Facts
- arXiv ID: 2403.01849
- Source URL: https://arxiv.org/abs/2403.01849
- Reference count: 40
- Key outcome: This work shows that tuning text prompts can significantly improve adversarial robustness of pre-trained VLMs like CLIP, with one learned "word" achieving +26.4% accuracy and +16.7% robustness improvements

## Executive Summary
This paper addresses the vulnerability of large pre-trained vision-language models (VLMs) like CLIP to adversarial attacks by proposing a novel approach that tunes text prompts rather than model weights. The key insight is that the choice of prompt significantly impacts both attack effectiveness and model resilience. The authors introduce Adversarial Prompt Tuning (APT), which learns a robust text prompt context through adversarial training. This parameter-efficient method requires only a single learned "word" added to the prompt and demonstrates strong performance across 15 datasets and various data sparsity schemes, outperforming hand-engineered prompts and other adaptation methods in both accuracy and robustness.

## Method Summary
The method proposes Adversarial Prompt Tuning (APT) to improve adversarial robustness of pre-trained VLMs by learning robust text prompt contexts instead of adapting model weights. APT works by optimizing learnable prompt context vectors through adversarial training, where the model learns text features that are less sensitive to input perturbations. The approach uses pre-trained frozen image and text encoders (ViT-B/32 backbone) and learns either unified or class-specific context prompts. During training, adversarial examples are generated and the prompt contexts are updated to minimize loss on these examples. The method is parameter-efficient, requiring only 256-512 additional parameters, and data-efficient, achieving significant improvements even with one-shot learning.

## Key Results
- APT achieves +26.4% accuracy and +16.7% robustness improvements in the most effective setting compared to baselines
- Outperforms hand-engineered prompts and other state-of-the-art adaptation methods across 15 datasets
- Demonstrates effectiveness in one-shot learning scenarios while maintaining strong performance with more data
- Shows consistent improvements across different data sparsity schemes (1-shot, 4-shot, 16-shot, full training)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial attack effectiveness on VLMs is highly sensitive to the text prompt used during attack generation
- Mechanism: Adversarial examples are generated by maximizing loss between image features and text features of the ground-truth class, where the text prompt determines text features through the frozen text encoder
- Core assumption: Frozen text encoder weights mean prompt changes directly affect attack effectiveness
- Evidence: [abstract] shows sensitivity to text prompt for both attack and defense; [section] explains gradients used for constructing adversarial examples are dependent on text features

### Mechanism 2
- Claim: Adversarial robustness of VLMs is sensitive to the prompt used for inference
- Mechanism: Classification depends on similarity scores between image and text features, which vary with different prompts
- Core assumption: Learned prompt contexts influence text feature representations used in similarity computation
- Evidence: [abstract] demonstrates sensitivity to text prompt; [section] observes that lowest robustness varies with inference prompt

### Mechanism 3
- Claim: Learning robust prompt contexts through adversarial training improves model resilience to adversarial attacks
- Mechanism: Optimizing prompt contexts to minimize loss on adversarial examples learns text features less sensitive to input perturbations
- Core assumption: Learned prompt contexts can capture invariances helping maintain correct classification under adversarial perturbations
- Evidence: [abstract] proposes method to improve resilience by learning robust text prompt; [section] describes training prompt contexts using adversarial training

## Foundational Learning

- Concept: Adversarial training
  - Why needed here: Paper uses adversarial training to optimize prompt contexts for robustness
  - Quick check question: What is the main difference between standard training and adversarial training in terms of the loss function?

- Concept: Vision-Language Models (VLMs)
  - Why needed here: Paper focuses on VLMs like CLIP, requiring understanding of their architecture
  - Quick check question: In CLIP, what is the role of the text encoder and how does it interact with the image encoder during inference?

- Concept: Prompt tuning
  - Why needed here: Paper proposes Adversarial Prompt Tuning, a form of prompt tuning
  - Quick check question: How does prompt tuning differ from fine-tuning in terms of what parameters are updated?

## Architecture Onboarding

- Component map: Frozen pre-trained image encoder (ViT-B/32) -> Frozen pre-trained text encoder (CLIP text encoder) -> Learnable prompt context vectors (unified or class-specific) -> PGD-based adversarial attack for training and evaluation

- Critical path:
  1. Load pre-trained image and text encoders (frozen)
  2. Initialize learnable prompt context vectors
  3. For each training iteration: Generate adversarial examples using current prompt, update prompt context vectors to minimize loss on adversarial examples
  4. Evaluate robustness using PGD attacks with learned prompt

- Design tradeoffs:
  - Unified vs. class-specific context: Unified is more parameter-efficient but may have less capacity to learn task-specific robustness
  - Prompt length (M): Longer prompts have more capacity but also more parameters to optimize
  - Adversarial attack strategy: On-the-fly is more effective but computationally expensive compared to constant

- Failure signatures:
  - No improvement in robustness: May indicate prompt capacity is insufficient or adversarial training is not effective
  - Overfitting to training data: May indicate need for more regularization or less complex prompt context
  - Poor generalization to new datasets: May indicate learned prompt is too dataset-specific

- First 3 experiments:
  1. Implement and evaluate basic Adversarial Prompt Tuning with unified context on a single dataset
  2. Compare effectiveness of unified vs. class-specific context on the same dataset
  3. Evaluate impact of prompt length (M) on robustness and accuracy trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sensitivity of adversarial robustness to text prompts generalize across different vision-language model architectures beyond CLIP?
- Basis in paper: Paper focuses specifically on CLIP-like VLMs and notes that adversarial examples are generated by searching for perturbations to maximize dissimilarity between image and text features
- Why unresolved: Study only examines CLIP and similar models; other VLM architectures may have different prompt-text encoding interactions
- What evidence would resolve it: Systematic experiments testing prompt sensitivity across multiple VLM architectures with varying text encoder designs

### Open Question 2
- Question: What is the theoretical relationship between prompt context length and adversarial robustness in VLMs?
- Basis in paper: Paper observes increasing context length improves accuracy for unified context variant but doesn't consistently improve robustness
- Why unresolved: Paper provides empirical observations but lacks theoretical analysis of why context length affects robustness differently than accuracy
- What evidence would resolve it: Mathematical analysis connecting prompt representation capacity, adversarial perturbation space, and robustness bounds

### Open Question 3
- Question: Can the learned prompt contexts be made interpretable while maintaining adversarial robustness?
- Basis in paper: Authors acknowledge interpretability limitations, noting that decoded words appear irrelevant to data and sometimes uninterpretable
- Why unresolved: Paper doesn't explore methods to constrain learned prompts toward meaningful representations while preserving robustness gains
- What evidence would resolve it: Experiments comparing robustness of constrained prompt learning against unconstrained approaches across multiple datasets

## Limitations
- Generalizability concerns: Learned prompts may overfit to specific image distributions used during training, with limited analysis of truly out-of-distribution performance
- Computational cost: Adversarial training process is computationally expensive despite parameter efficiency, potentially limiting practical deployment
- Evaluation scope: Robustness evaluation focuses on specific PGD attacks and perturbation budgets, effectiveness against other attack methods remains unclear

## Confidence
- **High Confidence**: Core observation that prompt choice significantly impacts both attack effectiveness and model robustness is well-supported by empirical evidence across multiple datasets
- **Medium Confidence**: Proposed APT method shows strong performance, but exact mechanism by which learned prompts improve robustness could benefit from deeper analysis
- **Low Confidence**: Claim that APT works effectively in one-shot settings is impressive but may not generalize across all vision tasks; paper doesn't explore failure cases

## Next Checks
1. **Cross-Domain Robustness**: Evaluate APT-learned prompts on datasets from completely different domains (e.g., medical imaging, satellite imagery) to assess generalization beyond the 15 tested datasets

2. **Alternative Attack Methods**: Test the robustness of APT-learned prompts against different attack algorithms (e.g., AutoAttack, DeepFool, CW attacks) and larger perturbation budgets to ensure comprehensive security

3. **Prompt Ablation Study**: Conduct systematic study varying prompt length, context type (unified vs. class-specific), and initialization strategies to better understand design space and identify optimal configurations for different tasks