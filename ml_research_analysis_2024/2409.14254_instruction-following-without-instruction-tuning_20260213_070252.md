---
ver: rpa2
title: Instruction Following without Instruction Tuning
arxiv_id: '2409.14254'
source_url: https://arxiv.org/abs/2409.14254
tags:
- instruction
- tuning
- response
- language
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies two forms of implicit instruction tuning
  that yield instruction-following behavior without explicit instruction-response
  training. First, training only on responses (without instructions) still produces
  instruction following, suggesting pretrained models have an inherent instruction-response
  mapping.
---

# Instruction Following without Instruction Tuning

## Quick Facts
- arXiv ID: 2409.14254
- Source URL: https://arxiv.org/abs/2409.14254
- Reference count: 40
- Primary result: Instruction following can emerge from response-only training and single-task fine-tuning without explicit instruction-response pairs

## Executive Summary
This paper challenges the conventional wisdom that instruction tuning is necessary for instruction following behavior in language models. The authors demonstrate that training solely on responses (without instructions) produces instruction following capabilities comparable to explicit instruction tuning. They further show that single-task fine-tuning on narrow domains like poetry or code generation generalizes to broad instruction-following behavior across unrelated tasks. To explain these phenomena, the authors propose that simple changes to a language model's conditional distribution can induce instruction following, and validate this by creating a rule-based adapter with three simple rules that achieves instruction-following performance comparable to instruction-tuned models.

## Method Summary
The paper explores three approaches to instruction following without explicit instruction tuning: response tuning (training only on responses without instructions), single-task fine-tuning on narrow domains, and a rule-based adapter that modifies the model's conditional distribution through three simple rules. The rule-based adapter combines a pretrained model with a rule-based language model using product-of-experts, applying rules that slowly increase EOS probability, penalize repetition, and modify probabilities of 15 specific tokens. Experiments use pretrained models like Llama-2-7B and OLMo-7B, with evaluation against instruction-tuned baselines using AlpacaEval.

## Key Results
- Training on responses only yields instruction following behavior comparable to instruction tuning
- Single-task fine-tuning on narrow domains (poetry, code, math) generalizes to broad instruction following
- A rule-based adapter with three simple rules achieves instruction-following performance matching instruction-tuned models
- Base models already contain latent instruction-response mappings not expressed due to low probability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Response tuning implicitly teaches instruction-response mapping by increasing probability of desirable responses.
- Mechanism: Base models already rank correct responses higher than random ones; response tuning amplifies this implicit preference.
- Core assumption: Pretrained models contain latent instruction-response mapping not expressed due to low probability.
- Evidence anchors:
  - [abstract] "training solely on responses, without any corresponding instructions, yields instruction following"
  - [section 4.2] "base models can rank a desired response for an instruction higher than a desired response for another instructions"
  - [corpus] FMR score 0.535 (moderate relatedness)
- Break condition: If pretrained models don't have implicit instruction-response ranking, response tuning fails.

### Mechanism 2
- Claim: Single-task finetuning on narrow domains generalizes to broad instruction following.
- Mechanism: Models learn task-specific constraints that apply broadly when instruction similarity is low.
- Core assumption: Task-specific finetuning changes model behavior enough to enable general instruction following.
- Evidence anchors:
  - [abstract] "instruction-response training on narrow-domain data like poetry still leads to broad instruction-following behavior"
  - [section 5.2] "similarity of an instruction to the GSM instructions relates to the similarity of the model response"
  - [corpus] FMR score 0.569 (moderate relatedness)
- Break condition: If finetuned task constraints are too domain-specific, generalization fails.

### Mechanism 3
- Claim: Simple rule-based adapters can induce instruction following behavior.
- Mechanism: Product-of-experts with three simple rules (EOS upweighting, token probability changes, repetition penalty) yields instruction following.
- Core assumption: Very simple changes to conditional distributions can cause instruction following.
- Evidence anchors:
  - [abstract] "hand-writing a rule-based language model which yields instruction following"
  - [section 6.1] "three rules are: slowly increasing the probability of ending the sequence, penalize repetition, and uniformly change 15 tokens' probabilities"
  - [corpus] FMR score 0.543 (moderate relatedness)
- Break condition: If rules don't align with instruction following patterns, adapter fails.

## Foundational Learning

- Concept: Language model pretraining and fine-tuning basics
  - Why needed here: Understanding how models learn from different training signals
  - Quick check question: What's the difference between training on responses only vs. instruction-response pairs?

- Concept: Product-of-experts model combination
  - Why needed here: Rule-based adapter uses this to combine base model with rule-based LM
  - Quick check question: How does product-of-experts differ from averaging distributions?

- Concept: Distribution shift and generalization
  - Why needed here: Single-task finetuning shows out-of-distribution generalization
  - Quick check question: Why might poetry-tuned models still generate recipes when asked?

## Architecture Onboarding

- Component map: Base language model -> Rule-based adapter (optional) -> Training pipeline
- Critical path: Data preparation -> model fine-tuning -> evaluation against baseline
- Design tradeoffs: Response tuning vs. instruction tuning data efficiency vs. performance
- Failure signatures: Low AlpacaEval win rates, responses not following instructions, domain collapse
- First 3 experiments:
  1. Run response tuning on Llama-2-7B and compare against base model
  2. Implement rule-based adapter and test win rate against instruction-tuned model
  3. Try single-task fine-tuning on GSM dataset and evaluate on recipe instructions

## Open Questions the Paper Calls Out

Here are 3 open questions based on the implicit instruction tuning paper:

### Open Question 1
- Question: How does the order of training (e.g., pretraining first vs. instruction tuning first) affect the ability to implicitly instruction tune?
- Basis in paper: [inferred] The paper discusses how pretrained models have an inherent instruction-response mapping, suggesting that the order of training may be important.
- Why unresolved: The paper does not explore the effects of different training orders on implicit instruction tuning.
- What evidence would resolve it: Experiments comparing the effectiveness of implicit instruction tuning when pretraining is done before or after instruction tuning.

### Open Question 2
- Question: Can the rule-based adapter be generalized to other tasks beyond instruction following, such as reasoning or planning?
- Basis in paper: [inferred] The paper shows that simple changes to a language model's distribution can cause instruction following, suggesting that similar techniques might work for other tasks.
- Why unresolved: The paper only explores the use of the rule-based adapter for instruction following.
- What evidence would resolve it: Experiments applying the rule-based adapter to other tasks and measuring its effectiveness.

### Open Question 3
- Question: How does the size of the pretraining dataset affect the ability to implicitly instruction tune?
- Basis in paper: [inferred] The paper discusses how pretrained models have an inherent instruction-response mapping, suggesting that the size of the pretraining dataset may be important.
- Why unresolved: The paper does not explore the effects of pretraining dataset size on implicit instruction tuning.
- What evidence would resolve it: Experiments comparing the effectiveness of implicit instruction tuning for models pretrained on datasets of different sizes.

## Limitations

- The claim that pretrained models contain latent instruction-response mappings is asserted but not directly proven
- Single-task fine-tuning generalization results don't distinguish between true instruction understanding versus pattern matching
- The rule-based adapter implementation details are insufficiently specified for reproduction

## Confidence

- **High confidence**: The observation that response tuning produces instruction following behavior (empirical result)
- **Medium confidence**: The product-of-experts adapter framework and basic rules (methodological approach)
- **Low confidence**: The claim that three simple rules can match instruction-tuned performance (specific implementation and results)

## Next Checks

1. **Mechanism validation**: Design an experiment to directly test whether pretrained models rank correct responses higher than random ones for instructions. Use controlled instruction-response pairs with clear correct/incorrect responses and measure ranking performance before and after response tuning.

2. **Generalization boundary testing**: Evaluate single-task fine-tuned models on instructions from completely unrelated domains (e.g., chess-tuned models on medical instructions) to determine if the generalization is truly broad or limited to surface-level patterns. Compare against models tuned on diverse instruction sets.

3. **Rule-based adapter ablation**: Systematically test each rule in isolation and in combinations to determine which components are actually necessary for instruction following. Also test with randomized token modifications to establish whether the specific tokens chosen matter or if any changes produce similar effects.