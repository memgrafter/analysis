---
ver: rpa2
title: 'A Survey on Model MoErging: Recycling and Routing Among Specialized Experts
  for Collaborative Learning'
arxiv_id: '2408.07057'
source_url: https://arxiv.org/abs/2408.07057
tags:
- expert
- routing
- learning
- data
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively categorizes Model MoErging (MoErging)
  methods, which aim to recycle independently-trained expert models for improved performance
  and generalization. It introduces a taxonomy covering expert training procedures,
  data privacy, routing datasets, granularity levels, expert selection and aggregation
  strategies, and application scenarios.
---

# A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning

## Quick Facts
- arXiv ID: 2408.07057
- Source URL: https://arxiv.org/abs/2408.07057
- Reference count: 28
- One-line primary result: Comprehensive survey and taxonomy of Model MoErging methods that recycle independently-trained expert models for improved performance and generalization

## Executive Summary
This survey provides a comprehensive overview of Model MoErging (MoErging) methods, which recycle independently-trained expert models to create aggregate systems with improved performance or generalization. The paper introduces a novel taxonomy that categorizes key design choices across expert training procedures, data privacy considerations, routing datasets, granularity levels, expert selection and aggregation strategies, and application scenarios. It groups existing methods into four broad categories: embedding-based, classifier-based, task-specific, and router-free approaches. The survey highlights the rapid development of MoErging research, identifies open problems for future investigation, and emphasizes the lack of comparative studies in this emerging field.

## Method Summary
The survey reviews 28 papers describing MoErging methods and constructs a taxonomy framework organizing design choices into three main categories: experts (specialized models trained independently), routing (strategies to select/aggregate experts), and application (downstream use cases). The method involves reviewing existing literature to understand current state of MoErging research, constructing a taxonomy framework with subcategories, and categorizing each surveyed method according to the taxonomy. The survey also discusses related fields like multitask learning, mixture-of-experts models, and model merging to provide context for MoErging approaches.

## Key Results
- Introduces a novel taxonomy covering expert training, data privacy, routing datasets, granularity levels, expert selection, aggregation strategies, and application scenarios
- Categorizes MoErging methods into four broad approaches: embedding-based, classifier-based, task-specific, and router-free
- Identifies open problems for future research including theoretical foundations and comparative studies
- Highlights rapid development of MoErging field with lack of comprehensive comparative analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MoErging improves generalization by composing independent expert models instead of joint training.
- Mechanism: Independent experts trained on diverse data are aggregated through learned or heuristic routing, avoiding catastrophic forgetting and enabling compositional skill remixing.
- Core assumption: Expert models retain useful task-specific knowledge that can be combined without joint retraining.
- Evidence anchors:
  - [abstract] states MoErging aims to "recycle expert models to create an aggregate system with improved performance or generalization."
  - [section] notes that unlike MoE models, experts are "not trained by a centralized body" and routing is "post-hoc."
  - [corpus] includes related works on collaborative MoE and expert routing, showing active research in this space.
- Break condition: If expert models interfere negatively when combined, or if routing cannot disambiguate relevant experts.

### Mechanism 2
- Claim: Routing granularity determines the trade-off between adaptability and computational cost.
- Mechanism: Routing can be performed at task, example, or token/step level; finer granularity increases adaptability but also computation and potential instability.
- Core assumption: The optimal routing frequency depends on the consistency of expert needs within and across inputs.
- Evidence anchors:
  - [section] explicitly discusses "Routing Input Granularity" and contrasts Task, Example, and Step-level routing.
  - [section] mentions that per-step routing "maximizes nuanced input adaptation but becomes computationally expensive."
  - [corpus] includes studies on token-level routing and expert selection, supporting the granularity trade-off.
- Break condition: If routing frequency is mismatched to the task, leading to either over-adaptation (cost) or under-adaptation (performance).

### Mechanism 3
- Claim: Expert selection strategy (sparse vs dense) balances efficiency and knowledge aggregation.
- Mechanism: Sparse selection activates only the most relevant experts, improving efficiency; dense selection uses all experts, maximizing knowledge integration but increasing computation.
- Core assumption: The relevance of experts can be accurately determined, and irrelevant experts do not overly degrade performance.
- Evidence anchors:
  - [section] describes "Expert Selection" as either Sparse (subset) or Dense (all experts).
  - [section] notes sparse selection is "crucial with large expert pools" but requires precise routing.
  - [corpus] includes works comparing sparse and dense routing strategies, validating the trade-off.
- Break condition: If routing accuracy is insufficient, sparse selection loses information; if irrelevant experts introduce noise, dense selection degrades performance.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) models
  - Why needed here: MoErging borrows ideas from MoE but differs in training and routing; understanding MoE helps distinguish the paradigms.
  - Quick check question: What is the main difference between MoE and MoErging in terms of expert training?

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: Expert models are typically created via PEFT methods like LoRA; understanding PEFT is essential for grasping expert modularity.
  - Quick check question: How do PEFT methods like LoRA make expert models lightweight and shareable?

- Concept: Task arithmetic and model merging
  - Why needed here: MoErging often uses merging techniques for expert aggregation; familiarity with task arithmetic and merging is crucial for understanding aggregation methods.
  - Quick check question: What is the difference between parameter aggregation and output aggregation in model merging?

## Architecture Onboarding

- Component map:
  Expert models -> Router -> Aggregation layer -> Application layer

- Critical path:
  1. Load expert models into memory.
  2. Process input through router to get expert weights/distribution.
  3. Activate selected experts (sparse) or all experts (dense).
  4. Aggregate outputs or parameters.
  5. Produce final output.

- Design tradeoffs:
  - Routing granularity vs. computational cost and stability
  - Sparse vs. dense expert selection for efficiency vs. knowledge integration
  - Output vs. parameter aggregation for flexibility vs. compactness
  - Shared vs. private expert data for routing quality vs. privacy

- Failure signatures:
  - High variance in output quality across inputs (routing instability)
  - Degraded performance compared to single expert (interference or poor aggregation)
  - Excessive latency or memory usage (routing or aggregation overhead)
  - Inability to generalize to new tasks (routing or expert mismatch)

- First 3 experiments:
  1. Load two or three expert adapters and apply task-level routing with uniform weights; measure performance vs. single expert.
  2. Implement example-level routing with sparse selection (top-1); compare to task-level routing.
  3. Swap parameter aggregation for output aggregation; measure impact on performance and model size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal routing granularity (step, example, or task level) for balancing performance and computational efficiency across diverse tasks and expert models?
- Basis in paper: [explicit] The survey discusses routing input granularity as a key design choice, noting trade-offs between adaptability, computational efficiency, and consistency of expert needs across inputs.
- Why unresolved: The paper highlights that different routing granularities have varying implications but does not provide definitive guidance on which granularity is optimal for specific scenarios. This likely depends on the specific task, expert models, and computational constraints.
- What evidence would resolve it: Systematic benchmarking studies comparing different routing granularities across a wide range of tasks and expert models, measuring both performance and computational costs.

### Open Question 2
- Question: How can MoErging methods effectively handle out-of-distribution tasks where no relevant expert model exists?
- Basis in paper: [explicit] The survey discusses out-of-distribution (OOD) generalization as a key application goal, noting that methods must extrapolate expert knowledge for novel tasks.
- Why unresolved: The paper acknowledges the challenge of OOD generalization but does not provide definitive solutions. This requires methods to handle tasks beyond the scope of available expert models.
- What evidence would resolve it: Empirical studies demonstrating the effectiveness of MoErging methods on truly out-of-distribution tasks, along with theoretical analysis of their generalization capabilities.

### Open Question 3
- Question: What are the theoretical foundations of MoErging that can guide the design of more effective methods?
- Basis in paper: [explicit] The survey mentions that a crucial direction for future work is developing robust theoretical frameworks for MoErging to understand its properties and guide method design.
- Why unresolved: The paper acknowledges the lack of theoretical understanding of MoErging, which limits the ability to design methods based on sound principles rather than heuristics.
- What evidence would resolve it: Theoretical analysis of MoErging methods, including convergence guarantees, bounds on performance, and insights into the conditions under which MoErging is beneficial.

## Limitations

- Rapidly evolving field with only 28 papers surveyed, potentially missing emerging approaches
- Lack of comparative studies makes relative effectiveness of different strategies largely theoretical
- Taxonomy may not capture corner cases or methods that don't cleanly map onto the framework
- Limited empirical validation of claims about compositional skill remixing and avoiding catastrophic forgetting

## Confidence

- Taxonomy framework and categorization: High
- Mechanism descriptions and design tradeoffs: Medium-High
- Empirical effectiveness claims: Medium-Low
- Open problem identification: Medium

## Next Checks

1. Replicate the routing granularity experiment by implementing token-level, example-level, and task-level routing on a standard benchmark (e.g., GLUE) using the same expert models, measuring both performance and computational overhead.

2. Conduct an ablation study comparing sparse vs. dense expert selection across different routing strategies to quantify the efficiency-generalization tradeoff.

3. Test the stability of the routing mechanism by evaluating performance variance across different input distributions and identifying conditions that lead to routing instability or interference between experts.