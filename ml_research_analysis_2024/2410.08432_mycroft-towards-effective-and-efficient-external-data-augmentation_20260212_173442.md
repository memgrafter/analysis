---
ver: rpa2
title: 'MYCROFT: Towards Effective and Efficient External Data Augmentation'
arxiv_id: '2410.08432'
source_url: https://arxiv.org/abs/2410.08432
tags:
- data
- mycroft
- dataset
- samples
- dhard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mycroft enables a model trainer to evaluate the utility of external
  data sources by having data owners share small, informative subsets of their data.
  It uses functional (gradient) and feature similarity to identify relevant samples,
  minimizing the amount of data exchanged.
---

# MYCROFT: Towards Effective and Efficient External Data Augmentation

## Quick Facts
- arXiv ID: 2410.08432
- Source URL: https://arxiv.org/abs/2410.08432
- Reference count: 40
- Mycroft enables rapid data source evaluation using small, informative subsets rather than full data sharing

## Executive Summary
MYCROFT addresses the challenge of efficiently evaluating external data sources for model augmentation by enabling data owners to share minimal, highly informative subsets rather than full datasets. The system uses functional (gradient) and feature similarity to identify samples that are most likely to improve model performance on target tasks. Across vision and tabular datasets, Mycroft achieves performance comparable to full-information sharing while requiring only a fraction of the data, outperforming random sampling by 21% on average for vision tasks.

## Method Summary
MYCROFT enables a model trainer to evaluate external data sources by having data owners share small, informative subsets identified through functional (gradient) and feature similarity. The model trainer sends a subset Dhard (where their model performs poorly) to data owners, who use orthogonal matching pursuit or feature similarity to select Duseful_i. These subsets are then evaluated and ranked by the model trainer, who fine-tunes on the most useful data. The approach minimizes data exchange while maintaining effectiveness, with submodularity guarantees for the optimization problem.

## Key Results
- Mycroft achieves performance matching full-information sharing using only a fraction of the data
- Outperforms random sampling by 21% on average for vision tasks
- Requires up to 20x less data for tabular tasks while maintaining effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Gradient Matching via OMP
- Claim: OMP effectively identifies informative samples by aligning functional gradients with those from a target subset
- Mechanism: OMP iteratively selects samples whose weighted gradients best approximate the gradient of loss on the target subset
- Core assumption: DO's model gradients align sufficiently with MT's model gradients for the target task
- Break condition: If DO's model architecture or training differs significantly from MT's

### Mechanism 2: Feature Similarity Matching
- Claim: Feature similarity in appropriate embedding spaces captures relevant data samples when gradient information is unavailable
- Mechanism: Samples are ranked by distance to target subset in learned feature space (CLIP for images, binning for tabular)
- Core assumption: Chosen feature space captures task-relevant similarities between samples
- Break condition: If feature space doesn't capture task-relevant similarities

### Mechanism 3: Composite Similarity Optimization
- Claim: Combining gradient and feature similarity through composite regularization improves sample selection
- Mechanism: Optimization objective includes both gradient alignment error and feature distance regularization
- Core assumption: Both gradient and feature similarities contain complementary information about sample utility
- Break condition: If one similarity measure is much more informative than the other

## Foundational Learning

- **Submodularity and greedy approximation algorithms**
  - Why needed here: Proves OMP can efficiently approximate optimal sparse sample selection
  - Quick check question: What property of the objective function allows OMP to provide a performance guarantee?

- **Feature extraction and embedding spaces**
  - Why needed here: Different domains require different feature representations to capture relevant similarities
  - Quick check question: Why does CLIP-based feature space work better than ResNet features for food image retrieval?

- **Gradient-based optimization and backpropagation**
  - Why needed here: Computing sample gradients relative to model parameters is essential for functional similarity matching
  - Quick check question: What information does the gradient of loss with respect to model parameters contain about a sample?

## Architecture Onboarding

- **Component map:** MT sends Dhard to DOs → Each DO runs DataSelect using OMP (if differentiable) or FeatureSimilarity → DOs return Duseful_i → MT evaluates and ranks DOs → MT fine-tunes on selected data
- **Critical path:** Dhard transmission → Data selection computation → Subset transmission → Model update
- **Design tradeoffs:** OMP provides theoretical guarantees but requires differentiable loss; feature similarity works universally but depends on feature space quality
- **Failure signatures:** Random-sampling baseline outperforms Mycroft (poor feature/gradient similarity); Mycroft matches random-sampling (broken similarity computation)
- **First 3 experiments:**
  1. Run Mycroft with OMP on simple vision task with small Dhard, verify outperforms random-sampling
  2. Test feature similarity alone on tabular dataset, compare to random-sampling
  3. Verify combining both methods (FuncFeat) improves performance over either method alone on task with both differentiable loss and good feature space

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Mycroft be extended to handle scenarios where Dhard is too limited to provide sufficient signal for data owners?
- Basis in paper: Assumes model trainer has access to Dhard they want to improve on
- Why unresolved: Current approach relies on having enough samples in Dhard to give data owners meaningful information
- What evidence would resolve it: Experimental results showing Mycroft's performance degradation with increasingly small Dhard datasets

### Open Question 2
- Question: How can Mycroft be adapted to explicitly promote diverse data selection?
- Basis in paper: Current method focuses on selecting data similar to Dhard through gradient and feature similarity
- Why unresolved: Doesn't explicitly optimize for diversity in selected subset
- What evidence would resolve it: Modified versions incorporating diversity metrics with experimental comparisons

### Open Question 3
- Question: How can Mycroft be made resistant to strategic or malicious data owners?
- Basis in paper: Assumes cooperative data owners who want to demonstrate utility of their data honestly
- Why unresolved: Doesn't consider adversarial scenarios where owners might manipulate selection process
- What evidence would resolve it: Formal threat models and security analysis under different types of adversarial behavior

## Limitations

- Strong dependence on quality of feature space and alignment between DO's and MT's model architectures
- Gradient-based approach requires differentiability of loss function, which may not always be available
- Experimental validation limited to vision and tabular datasets, uncertainty about performance on other modalities

## Confidence

- **High confidence:** Core mechanism of using functional and feature similarity to identify informative samples is theoretically sound and well-established
- **Medium confidence:** Claim of "rapidly matching performance of full-information sharing" is supported but may not hold for all data distributions
- **Low confidence:** Assertion of effectiveness "under noise" based on limited experiments with noisy CIFAR-100 labels

## Next Checks

1. **Architecture mismatch test:** Evaluate Mycroft's performance when DO's model architecture differs significantly from MT's (e.g., ResNet vs Vision Transformer)
2. **Cross-domain feature space evaluation:** Test feature similarity matching across domains where feature space may not capture task-relevant similarities (e.g., using CLIP features for medical imaging)
3. **Noise robustness expansion:** Systematically evaluate Mycroft's performance under various noise types and distributions to validate claimed robustness