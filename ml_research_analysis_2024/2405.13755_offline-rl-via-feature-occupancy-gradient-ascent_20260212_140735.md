---
ver: rpa2
title: Offline RL via Feature-Occupancy Gradient Ascent
arxiv_id: '2405.13755'
source_url: https://arxiv.org/abs/2405.13755
tags:
- policy
- lemma
- learning
- feature
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies offline reinforcement learning in large MDPs
  with linear function approximation. The authors propose a new algorithm based on
  gradient ascent in the space of feature occupancies, which are expectations of feature
  vectors under state-action distributions.
---

# Offline RL via Feature-Occupancy Gradient Ascent

## Quick Facts
- arXiv ID: 2405.13755
- Source URL: https://arxiv.org/abs/2405.13755
- Reference count: 40
- Primary result: Achieves improved sample complexity in offline RL with linear function approximation through gradient ascent in feature-occupancy space

## Executive Summary
This paper introduces FOGAS, a new algorithm for offline reinforcement learning in large MDPs with linear function approximation. The method performs gradient ascent in the space of feature occupancies, which are expectations of feature vectors under state-action distributions. By building a least-squares estimator of the transition model and optimizing an unconstrained primal objective, FOGAS achieves strong sample complexity guarantees that scale with the feature coverage ratio between the optimal policy and the data set. The algorithm is simple to implement and does not require prior knowledge of coverage ratios or projection steps.

## Method Summary
The FOGAS algorithm alternates between three main steps: (1) computing the best-response parameter θt using a least-squares estimator of the transition matrix, (2) updating the policy πt+1 via entropy-regularized mirror ascent, and (3) updating the feature occupancy λt+1 via mirror ascent with a stabilization term. The method builds a least-squares estimator of the transition matrix and uses it to define an unconstrained primal objective in feature-occupancy space. Mirror ascent is then applied to this objective, with policy updates performed via entropy-regularized mirror ascent. The algorithm includes a stabilization trick that eliminates the need for prior knowledge of coverage ratios and prevents iterates from diverging.

## Key Results
- Achieves sample complexity that scales with the feature coverage ratio between optimal policy and dataset
- Eliminates need for prior knowledge of coverage ratios through stabilization technique
- Improves upon recent work by eliminating suboptimal design choices and achieving better theoretical guarantees
- Achieves optimal dependence on accuracy ε while maintaining simplicity of implementation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient ascent in feature-occupancy space directly maximizes expected return without requiring explicit dual updates
- Mechanism: The algorithm builds a least-squares estimator of the transition matrix and uses it to define an unconstrained primal objective. Mirror ascent is then applied to this objective in the space of feature occupancies, with policy updates performed via entropy-regularized mirror ascent in the policy space
- Core assumption: The feature-occupancy vectors and value parameters are related through linear MDP structure, allowing the saddle-point objective to be reformulated as a primal-only optimization
- Evidence anchors: [abstract] "Starting from the classic linear-program formulation...we develop a new algorithm that performs a form of gradient ascent in the space of feature occupancies"; [section 3] "Our approach consist s of building a well-chosen estimator ˆf of f , and then maximizing the associated primal function"

### Mechanism 2
- Claim: The algorithm achieves improved sample complexity by requiring only single-direction feature coverage rather than full subspace coverage
- Mechanism: The theoretical analysis shows that the sample complexity depends on the feature coverage ratio between the optimal policy and the dataset, which only requires the empirical feature covariance to cover a single direction in feature space
- Core assumption: The data coverage can be measured in terms of feature covariance alignment rather than requiring coverage of the entire state-action space
- Evidence anchors: [abstract] "depends on a weak notion of coverage that only requires the empirical feature covariance matrix to cover a single direction in the feature space"; [section 2] "we show that the sample complexity of our method scales optimally with the desired accuracy level and depends on a weak notion of coverage"

### Mechanism 3
- Claim: The stabilization trick eliminates the need for prior knowledge of coverage ratios and prevents iterates from diverging
- Mechanism: A regularization term is added to the mirror ascent update that penalizes the norm of the feature occupancy vector, effectively bounding the iterates without requiring projection to a bounded set
- Core assumption: The regularization parameter can be chosen to balance between stability and convergence without prior knowledge of the problem structure
- Evidence anchors: [abstract] "the incorporation of a recently proposed stabilization trick that we make use of in our algorithm"; [section 3] "the second one has a stabilization effect whose role will be made clear later in the analysis"

## Foundational Learning

- Concept: Linear MDP structure and feature representation
  - Why needed here: The entire algorithm relies on the ability to express rewards and transitions as linear functions of features, which enables the primal reformulation and least-squares estimation
  - Quick check question: What properties must the feature map satisfy for the linear MDP assumption to hold?

- Concept: Mirror ascent and entropy regularization in policy optimization
  - Why needed here: The policy updates are performed via entropy-regularized mirror ascent, which provides stability and ensures exploration while optimizing
  - Quick check question: How does the entropy regularization term affect the convergence properties of the policy updates?

- Concept: Least-squares estimation and covariance matrix inversion
  - Why needed here: The algorithm builds a least-squares estimator of the transition matrix, which requires computing the empirical feature covariance and its inverse
  - Quick check question: What are the computational implications of inverting the empirical feature covariance matrix at each iteration?

## Architecture Onboarding

- Component map: Data → Feature extraction → Covariance computation → Transition estimation → Primal updates → Policy updates → Output
- Critical path: Data → Feature extraction → Covariance computation → Transition estimation → Primal updates → Policy updates → Output
- Design tradeoffs:
  - Mirror ascent vs. other optimization methods: Mirror ascent provides stability but may converge slower than gradient methods
  - Regularization strength: Balancing between stability and convergence rate
  - Feature dimension vs. sample size: Higher dimensions require more samples for accurate estimation
- Failure signatures:
  - Diverging feature occupancy vectors: Indicates insufficient regularization or poor data coverage
  - Slow policy improvement: Suggests inadequate exploration or poor feature representation
  - Numerical instability in covariance inversion: Indicates insufficient data or ill-conditioned features
- First 3 experiments:
  1. Test on small MDP with known optimal policy to verify convergence
  2. Vary regularization parameter to study stability vs. performance tradeoff
  3. Test with different feature representations to evaluate impact on sample complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the FOGAS algorithm be extended to achieve the optimal linear scaling with the feature coverage ratio without requiring prior knowledge of the coverage parameter?
- Basis in paper: Explicit. The paper notes that the sample complexity is of order d^2 * ||λ*||^2_Λ^(-1) / ε^2, but this can be improved to scale linearly with ||λ*||_Λ^(-1) if a tight upper bound is known
- Why unresolved: The paper suggests this scenario is unlikely and poses it as an open challenge for future work
- What evidence would resolve it: A proof that FOGAS or a variant can achieve the improved scaling without prior knowledge of the coverage parameter

### Open Question 2
- Question: Is it possible to reduce the computational complexity of FOGAS below O(n^2) while maintaining the same statistical guarantees?
- Basis in paper: Explicit. The paper notes that each iteration of FOGAS scales linearly with the sample size n, leading to an overall runtime complexity of O(n^2)
- Why unresolved: This limitation is shared with all methods using the same least-squares transition estimator, and the paper wonders if a substantial improvement is possible
- What evidence would resolve it: A variant of FOGAS with improved computational complexity or a lower bound proving that O(n^2) is necessary

### Open Question 3
- Question: Can FOGAS be extended to work under more general notions of function approximation beyond linear MDPs?
- Basis in paper: Explicit. The paper suggests that extending FOGAS to work with low inherent Bellman rank or linearly Qπ-realizable MDPs is possible but challenging, and extending to non-linear function approximation looks very challenging
- Why unresolved: The central role of feature occupancies in FOGAS is strictly tied to linear function approximation
- What evidence would resolve it: A proof that FOGAS or a variant can be extended to work with non-linear function approximation or a lower bound proving that such an extension is impossible

## Limitations

- Performance highly sensitive to regularization parameter and feature representation quality
- Theoretical guarantees assume full rank feature matrices and bounded value parameters, which may not hold in practice
- Computational complexity of inverting empirical covariance matrix at each iteration may be prohibitive for high-dimensional features

## Confidence

- Main claims about improved sample complexity through single-direction feature coverage: Medium confidence
- Claim that algorithm is simple to implement without prior knowledge of coverage ratios: High confidence
- Practical significance depends heavily on whether real-world offline datasets satisfy weak coverage conditions

## Next Checks

1. Test the algorithm on benchmark offline RL datasets (e.g., D4RL) to verify whether weak coverage conditions are satisfied in practice
2. Compare sample complexity empirically against state-of-the-art offline RL methods under varying data coverage scenarios
3. Analyze the sensitivity of performance to the regularization parameter across different problem instances