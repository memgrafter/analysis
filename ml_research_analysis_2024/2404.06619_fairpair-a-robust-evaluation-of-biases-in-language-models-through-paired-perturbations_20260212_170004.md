---
ver: rpa2
title: 'FairPair: A Robust Evaluation of Biases in Language Models through Paired
  Perturbations'
arxiv_id: '2404.06619'
source_url: https://arxiv.org/abs/2404.06619
tags:
- language
- bias
- john
- evaluation
- jane
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FairPair, a new evaluation framework for detecting
  biases in language models. The core idea is to create counterfactual pairs of text
  where the only difference is a demographic attribute, and then measure how differently
  the model treats the two pairs.
---

# FairPair: A Robust Evaluation of Biases in Language Models through Paired Perturbations

## Quick Facts
- arXiv ID: 2404.06619
- Source URL: https://arxiv.org/abs/2404.06619
- Reference count: 0
- Primary result: FairPair framework detects biases in language models by creating counterfactual pairs of text and comparing generated continuations while accounting for sampling variability

## Executive Summary
FairPair introduces a novel framework for evaluating biases in language models by creating counterfactual pairs of text that differ only in a demographic attribute. The method generates continuations for both the original and perturbed prompts, then compares these continuations while grounding the comparison in the same demographic group and accounting for sampling variability. This approach addresses limitations in prior work by ensuring fair comparison and incorporating the inherent stochasticity of language model generation. The authors demonstrate FairPair's effectiveness using the Common Sents dataset and evaluate several popular language models, finding that larger models tend to exhibit higher bias relative to their sampling variability.

## Method Summary
FairPair creates counterfactual pairs by perturbing demographic attributes in text prompts, then generates continuations for both original and perturbed versions using non-deterministic language models. The framework samples multiple continuations for each prompt to account for generation variability, then applies perturbation functions to these continuations to ensure grounded comparison. Bias is measured using scoring functions (sentiment dissimilarity and token dissimilarity) that compare the perturbed original continuation with the perturbed prompt continuation. The final FairPair metric normalizes bias by sampling variability, providing a robust measure of model bias that distinguishes systematic bias from random generation effects.

## Key Results
- FairPair successfully detects differential treatment patterns in language models, with prompts starting with "John" discussing occupational capabilities while "Jane" prompts focus on family, hobbies, and personality traits
- Larger language models (GPT-2 XL, GPT-J, LLaMa-13B, InstructGPT) exhibit higher bias relative to sampling variability compared to smaller models
- The method effectively accounts for sampling variability, with bias estimates stabilizing as the number of samples increases
- Qualitative analysis reveals systematic differences in how models generate content for different demographic groups, even when controlling for the perturbed attribute

## Why This Works (Mechanism)

### Mechanism 1
FairPair reduces superficial entity bias by grounding both counterfactual continuations in the same demographic group. By perturbing the original generation to match the perturbed prompt's entity, the method ensures that differences between g(p(x)) and p(g(x)) are not due to entity unfamiliarity but rather model bias in content generation. Core assumption: Token likelihoods are not the primary indicator of bias; rather, the content and context of the generated text matter. Evidence anchors: [abstract] "crucially, the paired continuations are grounded in the same demographic group, which ensures equivalent comparison." Break condition: If the perturbation function introduces additional changes beyond the target demographic attribute, it could confound the grounded comparison.

### Mechanism 2
FairPair accounts for sampling variability inherent in non-deterministic language models. By generating multiple continuations for both the original and perturbed prompts, FairPair calculates a bias metric that considers the inherent variability in the generation process. Core assumption: The variability in generated text is not solely due to bias but also due to the stochastic nature of the generation process. Evidence anchors: [abstract] "unlike prior work, our method factors in the inherent variability that comes from the generation process itself by measuring the sampling variability." Break condition: If the sampling variability is significantly larger than the bias, the method may not effectively detect subtle biases.

### Mechanism 3
FairPair uses a flexible scoring function to measure bias along different axes. The framework allows for various scoring functions (e.g., sentiment dissimilarity, token dissimilarity) to compare the generated text, enabling the detection of biases along different dimensions. Core assumption: Different scoring functions can capture different aspects of bias, and a combination of these functions can provide a comprehensive evaluation. Evidence anchors: [section] "Scoring Functions To compare two sequence of tokens u and v, we utilize two dissimilarity measures: • Sentiment dissimilarity: Given any sentiment scorer S, we set Φ(u, v) = |S(u) − S(v)|. Here we use the VADER sentiment classifier from Hutto and Gilbert (2014). • Token dissimilarity: Here we use Jaccard dissimilarity, namely, Φ(u, v) = (1 − |u∩v| |u∪v|)." Break condition: If the scoring function is not well-suited to the type of bias being evaluated, it may not effectively detect or measure the bias.

## Foundational Learning

- Concept: Counterfactual analysis
  - Why needed here: FairPair relies on creating counterfactual pairs of text to measure bias, requiring an understanding of how to construct and analyze these pairs.
  - Quick check question: How would you create a counterfactual pair of sentences to evaluate gender bias in a language model?

- Concept: Sampling variability
  - Why needed here: FairPair accounts for the inherent variability in non-deterministic language models by sampling multiple continuations, necessitating an understanding of how to measure and account for this variability.
  - Quick check question: Why is it important to consider sampling variability when evaluating bias in language models?

- Concept: Scoring functions for bias measurement
  - Why needed here: FairPair uses various scoring functions to compare generated text, requiring knowledge of how to design and apply these functions to measure bias.
  - Quick check question: What are the advantages and disadvantages of using sentiment dissimilarity versus token dissimilarity to measure bias in language models?

## Architecture Onboarding

- Component map: Dataset -> Perturbation Function -> Language Model -> Scoring Function -> Bias Metric
- Critical path: 1. Create Common Sents dataset with templates 2. Define perturbation function for demographic attributes 3. Sample continuations for original and perturbed prompts 4. Apply scoring functions to compare continuations 5. Calculate FairPair metric
- Design tradeoffs: Sampling size vs. computational cost (larger samples improve reliability but increase computational requirements); scoring function choice (different functions may be more or less effective at detecting certain types of bias)
- Failure signatures: High sampling variability relative to bias (method may not detect subtle biases); inaccurate perturbation function (invalid FairPair calculations); inappropriate scoring function (ineffective bias measurement)
- First 3 experiments: 1. Evaluate bias using FairPair with small dataset and single scoring function 2. Compare FairPair results with different sampling sizes to determine optimal sample count 3. Test different scoring functions (sentiment, token dissimilarity) for detecting various bias types

## Open Questions the Paper Calls Out

### Open Question 1
How does the FairPair metric perform when applied to different demographic axes beyond gender, such as race, age, or disability status? Basis in paper: [explicit] The paper mentions that the FairPair framework can be extended to other demographic groups and axes, such as those provided in the Holistic Bias dataset, but the authors only demonstrate its utility in the context of gender bias. Why unresolved: The authors do not provide empirical results or analysis for demographic axes other than gender. What evidence would resolve it: Applying the FairPair metric to datasets containing prompts with different demographic attributes (e.g., race, age, disability status) and comparing the results across these axes would provide insights into the metric's performance and generalizability.

### Open Question 2
How does the FairPair metric compare to other existing bias evaluation methods in terms of sensitivity and specificity for detecting subtle biases? Basis in paper: [inferred] The authors argue that FairPair is robust and flexible, capable of capturing subtle biases that appear in typical usage. However, they do not directly compare FairPair's performance to other methods. Why unresolved: The paper does not include a comparative analysis of FairPair against other bias evaluation methods. What evidence would resolve it: Conducting experiments that compare FairPair's ability to detect subtle biases against other established methods, such as CrowS-Pairs, StereoSet, or Holistic Bias, would provide insights into its relative performance.

### Open Question 3
How does the choice of scoring function (e.g., sentiment, Jaccard dissimilarity) impact the results and interpretability of the FairPair metric? Basis in paper: [explicit] The authors use sentiment dissimilarity and token dissimilarity (Jaccard) as scoring functions in their experiments, but they acknowledge that other scoring functions can be explored. Why unresolved: The paper does not investigate the impact of different scoring functions on the FairPair metric's results and interpretability. What evidence would resolve it: Applying FairPair with various scoring functions (e.g., toxicity, politeness, semantic similarity) and analyzing the differences in results and their implications for interpreting bias would provide insights into the role of scoring functions.

## Limitations

- The method's effectiveness depends heavily on the quality and precision of the perturbation function, which may introduce unintended changes beyond the target demographic attribute
- The framework requires substantial computational resources due to the need for multiple continuation samples per prompt
- The generalizability to demographic attributes beyond gender has not been thoroughly validated, limiting the framework's applicability to other forms of bias

## Confidence

**High Confidence**: The core mechanism of creating counterfactual pairs and grounding comparisons in the same demographic group is theoretically sound and well-supported by the evidence provided. The mathematical formulation of the FairPair metric and its distinction from prior work are clearly articulated.

**Medium Confidence**: The claim that larger models exhibit higher bias relative to sampling variability is supported by experimental results, but the sample size and diversity of models tested may not be sufficient for broad generalization. The qualitative analysis of differential treatment patterns provides insights but relies on subjective interpretation.

**Low Confidence**: The assertion that FairPair effectively detects biases across different types of demographic attributes (beyond the tested gender bias) and in real-world applications has not been thoroughly validated in the paper.

## Next Checks

1. **Perturbation Function Validation**: Conduct a systematic evaluation of the perturbation function's accuracy and precision. Test whether perturbations consistently change only the target demographic attribute without introducing other unintended modifications. This could involve human evaluation of perturbed sentences and analysis of perturbation failure modes.

2. **Optimal Sampling Size Determination**: Perform a convergence analysis to determine the minimum number of samples required for stable FairPair metric estimates. Vary the sample size (n) and measure how the bias and sampling variability estimates change. Identify the point of diminishing returns where additional samples no longer significantly improve estimate reliability.

3. **Cross-Attribute Generalization**: Apply FairPair to evaluate biases beyond gender, such as race, age, or socioeconomic status. Use diverse datasets and perturbation functions tailored to these attributes. Compare the results with established benchmarks for these attributes to assess FairPair's effectiveness in detecting biases across different demographic dimensions.