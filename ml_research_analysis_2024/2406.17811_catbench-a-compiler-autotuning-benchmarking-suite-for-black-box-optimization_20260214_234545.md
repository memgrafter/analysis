---
ver: rpa2
title: 'CATBench: A Compiler Autotuning Benchmarking Suite for Black-box Optimization'
arxiv_id: '2406.17811'
source_url: https://arxiv.org/abs/2406.17811
tags: []
core_contribution: CATBench is a comprehensive benchmarking suite for evaluating Bayesian
  optimization algorithms in compiler autotuning. It addresses the lack of standardized
  benchmarks in this domain by providing real-world compiler optimization tasks with
  complex search spaces involving discrete, categorical, and permutation parameters,
  as well as known and unknown constraints.
---

# CATBench: A Compiler Autotuning Benchmarking Suite for Black-box Optimization

## Quick Facts
- arXiv ID: 2406.17811
- Source URL: https://arxiv.org/abs/2406.17811
- Reference count: 40
- Key outcome: A comprehensive benchmarking suite for evaluating Bayesian optimization algorithms in compiler autotuning

## Executive Summary
CATBench is a benchmarking suite designed to evaluate Bayesian optimization algorithms for compiler autotuning. It addresses the lack of standardized benchmarks in this domain by providing real-world compiler optimization tasks with complex search spaces involving discrete, categorical, and permutation parameters, as well as known and unknown constraints. The suite includes ten benchmarks spanning tensor algebra, image processing, and clustering, using state-of-the-art compilers like TACO and RISE/ELEVATE.

The paper validates CATBench on several state-of-the-art algorithms, revealing their strengths and weaknesses. Results show that Bayesian optimization methods like BaCO significantly outperform non-BO algorithms such as NSGA-II and Random Search in both single-objective (minimizing compute time) and multi-objective (minimizing compute time and energy) contexts. The suite demonstrates its potential for advancing both Bayesian optimization and compiler autotuning research by providing a challenging and diverse set of optimization tasks.

## Method Summary
CATBench provides a unified, containerized interface for both surrogate and real-world compiler optimization tasks, enabling reproducible evaluation of optimization algorithms. The benchmarks support multi-fidelity and multi-objective evaluations, including execution time and energy consumption as objectives. The suite addresses the complexity of compiler autotuning through benchmarks that include discrete, categorical, and permutation parameters, along with known and unknown constraints. It covers three main domains: tensor algebra, image processing, and clustering, utilizing state-of-the-art compilers TACO and RISE/ELEVATE.

## Key Results
- BaCO significantly outperforms NSGA-II and Random Search in both single-objective (compute time) and multi-objective (compute time and energy) optimization contexts
- Non-BO algorithms like NSGA-II encounter numerical errors on certain benchmarks (SDDMM and MTTKRP)
- The suite successfully reveals performance differences between BO and non-BO algorithms, validating its effectiveness as a benchmarking tool

## Why This Works (Mechanism)
CATBench works effectively because it combines realistic compiler optimization tasks with controlled experimental conditions through Docker containers. The benchmarks' complex search spaces with discrete, categorical, and permutation parameters create meaningful challenges that differentiate algorithm performance. By supporting both surrogate and real-world benchmarks, CATBench enables systematic algorithm comparison while maintaining connection to practical compiler optimization problems. The inclusion of multi-objective optimization (compute time and energy) provides a more comprehensive evaluation framework that better reflects real-world compiler tuning requirements.

## Foundational Learning
- Compiler autotuning: Optimizing compiler parameters to improve code performance; needed to understand the domain where CATBench operates
- Black-box optimization: Optimization without access to the internal workings of the function being optimized; critical for understanding how compiler autotuning is framed as an optimization problem
- Bayesian optimization: A probabilistic approach to global optimization that uses a surrogate model to guide the search; central to the algorithms being benchmarked
- Multi-objective optimization: Simultaneously optimizing multiple conflicting objectives; essential for understanding the energy-time tradeoff being evaluated
- Docker containers: A virtualization method for packaging applications; crucial for ensuring reproducible benchmark environments
- Surrogate models: Simplified models that approximate complex functions; fundamental to understanding how Bayesian optimization algorithms work

## Architecture Onboarding

**Component Map:**
Container Interface -> Benchmark Generator -> Compiler -> Performance Metrics -> Optimization Algorithm

**Critical Path:**
Optimization algorithm proposes parameter configuration → CATBench generates corresponding benchmark code → Compiler compiles the code → Code executes and collects performance metrics → Optimization algorithm updates its model and proposes next configuration

**Design Tradeoffs:**
- Real vs. surrogate benchmarks: CATBench includes both real-world compiler optimization tasks and surrogate functions to balance realism with reproducibility
- Multi-objective vs. single-objective: The suite supports both types of optimization to reflect real-world needs and theoretical research requirements
- Hardware dependency: Including hardware-specific metrics like GPU energy consumption provides realistic benchmarks but reduces portability

**Failure Signatures:**
- Non-BO algorithms encountering numerical errors on certain benchmarks (SDDMM and MTTKRP)
- Variability in benchmark performance across different hardware setups
- Potential scalability issues when running complex benchmarks on resource-constrained environments

**3 First Experiments:**
1. Run single-objective optimization on TACO benchmarks (compute time) to establish baseline performance
2. Execute multi-objective optimization on RISE/ELEVATE benchmarks (compute time, GPU energy) to evaluate energy-time tradeoffs
3. Test NSGA-II and Random Search on SDDMM and MTTKRP benchmarks to characterize failure patterns

## Open Questions the Paper Calls Out
- How can the benchmark suite be extended to cover additional compiler optimization scenarios beyond tensor algebra, image processing, and clustering?
- What are the best practices for handling unknown constraints in compiler autotuning benchmarks?
- How can the suite better accommodate different hardware configurations while maintaining reproducible results?

## Limitations
- Hardware dependencies, particularly for RISE/ELEVATE benchmarks, create significant reproducibility challenges across different computing environments
- The complex parameter spaces and constraint handling mechanisms may not fully capture all real-world compiler optimization scenarios
- The benchmark suite focuses primarily on specific domains (tensor algebra, image processing, clustering) and may not represent the full diversity of compiler optimization tasks

## Confidence
- **High confidence**: The suite's ability to reveal performance differences between BO and non-BO algorithms
- **Medium confidence**: The generalizability of benchmark results across different hardware configurations
- **Medium confidence**: The completeness of the benchmark suite in representing real-world compiler optimization scenarios

## Next Checks
1. **Hardware Validation**: Test benchmark performance across multiple hardware configurations to assess hardware dependency and establish baseline performance metrics for different setups
2. **Algorithm Robustness**: Conduct systematic testing of NSGA-II and other non-BO algorithms on problematic benchmarks (SDDMM, MTTKRP) to characterize numerical error patterns and failure conditions
3. **Constraint Coverage**: Analyze the constraint specifications across all benchmarks to verify they adequately represent real-world compiler optimization constraints and identify any gaps in constraint modeling