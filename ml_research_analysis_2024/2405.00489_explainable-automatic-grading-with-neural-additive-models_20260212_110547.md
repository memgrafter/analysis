---
ver: rpa2
title: Explainable Automatic Grading with Neural Additive Models
arxiv_id: '2405.00489'
source_url: https://arxiv.org/abs/2405.00489
tags:
- neural
- data
- asag
- features
- additive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Neural Additive Model (NAM) for explainable
  automatic short answer grading (ASAG), addressing the black-box nature of current
  state-of-the-art ASAG models. The NAM combines the predictive performance of neural
  networks with the interpretability of additive models by learning a linear combination
  of jointly-trained neural networks, each attending to a single input feature.
---

# Explainable Automatic Grading with Neural Additive Models

## Quick Facts
- arXiv ID: 2405.00489
- Source URL: https://arxiv.org/abs/2405.00489
- Authors: Aubrey Condor; Zachary Pardos
- Reference count: 38
- Primary result: NAMs provide interpretable ASAG with competitive accuracy to logistic regression, though DeBERTa performs better

## Executive Summary
This paper introduces Neural Additive Models (NAMs) for explainable automatic short answer grading (ASAG), addressing the black-box nature of current state-of-the-art models. The NAM architecture combines neural network predictive power with additive model interpretability by learning a linear combination of jointly-trained neural networks, each attending to a single input feature. The study demonstrates that NAMs can achieve good predictive performance while providing interpretable feature importance visualizations, though they still lag behind transformer-based models like DeBERTa.

## Method Summary
The method employs feature engineering guided by a Knowledge Integration (KI) framework to create interpretable inputs reflecting student inclusion of predefined ideas. For each student response, n-grams are extracted and their semantic similarity to KI phrases is calculated using sentence-BERT embeddings and cosine similarity. These similarity scores form the input features for the NAM, which consists of multiple univariate neural networks combined linearly. The model is compared against logistic regression and DeBERTa on both a physics sound waves item and five science questions from the ASAP dataset.

## Key Results
- NAM outperforms logistic regression on the physics sound waves question while providing interpretable feature importance visualizations
- Shape functions show how individual features influence predicted scores across different classes
- DeBERTa still achieves higher predictive performance than NAM on both datasets
- Feature importance visualizations reveal which KI concepts are most predictive of scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NAM combines the predictive performance of neural networks with the interpretability of additive models.
- Mechanism: By restricting each neural network to attend to a single input feature, the model maintains additive structure while allowing non-linear transformations within each feature.
- Core assumption: The linear combination of univariate neural networks can approximate complex relationships in ASAG tasks while maintaining feature-level interpretability.
- Evidence anchors:
  - [abstract] "combines the performance of a NN with the explainability of an additive model"
  - [section] "NAMs learn a linear combination of jointly-trained NNs where each NN attends to a single input feature"
- Break condition: If the feature engineering fails to capture the essential information for grading, the NAM's predictive performance will degrade significantly.

### Mechanism 2
- Claim: Feature engineering using KI framework guides NAM to capture human-grading reasoning.
- Mechanism: By creating features that reflect inclusion/exclusion of predefined ideas, the model learns which concepts are important for scoring, mirroring human rubric-based grading.
- Core assumption: The inclusion of specific knowledge integration ideas as features is sufficient for the NAM to achieve good predictive power and interpretability.
- Evidence anchors:
  - [section] "We hypothesize that the inclusion (or exclusion) of predefined KI ideas as features will be sufficient for the NAM to have good predictive power"
  - [section] "We utilize KI phrases from the rubric that represent both correct, and incorrect mechanisms and conclusions"
- Break condition: If student responses express ideas in ways not captured by the engineered features, the model will miss important scoring information.

### Mechanism 3
- Claim: Shape function visualizations provide interpretable insights into feature contributions.
- Mechanism: Each feature has an associated shape function showing how different values of that feature influence the predicted score, allowing stakeholders to understand the model's reasoning.
- Core assumption: The jagged nature of shape functions reflects the true underlying relationships between features and scores, despite appearing less smooth than typical neural network outputs.
- Evidence anchors:
  - [section] "we can visualize five shape functions for each feature representing each of the five classes"
  - [section] "The shape functions are jagged-like with sharp jumps"
- Break condition: If the shape functions are too jagged or lack sufficient data density, the visualizations may not provide clear interpretability.

## Foundational Learning

- Neural Networks
  - Why needed here: Understanding how neural networks can be modified to maintain interpretability while preserving predictive power
  - Quick check question: What modification does NAM make to standard neural network architecture to enable interpretability?

- Feature Engineering
  - Why needed here: Creating meaningful input features from text responses is crucial for NAM's performance
  - Quick check question: How are the similarity scores between feature phrases and response n-grams calculated?

- Knowledge Integration Framework
  - Why needed here: Provides the theoretical foundation for selecting which features to engineer for ASAG
  - Quick check question: What is the core principle of the KI framework that guides feature selection?

## Architecture Onboarding

- Component map:
  Feature engineering pipeline (n-gram extraction → sentence-BERT embedding → cosine similarity) -> NAM core (multiple univariate neural networks + linear combination) -> Visualization module (feature importance + shape functions)

- Critical path:
  1. Preprocess student responses to extract n-grams
  2. Calculate cosine similarity between n-grams and KI phrases
  3. Feed similarity scores into NAM
  4. Train model and generate predictions
  5. Visualize feature importance and shape functions

- Design tradeoffs:
  - Feature engineering vs. end-to-end learning: NAM requires manual feature creation but gains interpretability
  - Model complexity vs. interpretability: More complex shape functions may capture nuances but reduce clarity
  - Data density vs. reliable shape functions: Sparse data regions may produce unreliable visualizations

- Failure signatures:
  - Poor predictive performance relative to black-box models
  - Shape functions with insufficient data density (light pink shading)
  - Feature importance concentrated on a few features only

- First 3 experiments:
  1. Compare NAM with logistic regression on a small subset to verify improved interpretability
  2. Test different n-gram sizes (1-5) to optimize feature representation
  3. Evaluate shape function smoothness with different regularization techniques

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Neural Additive Models (NAMs) compare to other explainable AI methods, such as SHAP values or LIME, in terms of performance and interpretability for automatic short answer grading?
- Basis in paper: [inferred] The paper mentions other explainable AI methods like SHAP values in the Related Work section but does not directly compare them to NAMs.
- Why unresolved: The paper only compares NAMs to logistic regression and DeBERTa, leaving the comparison with other explainable AI methods unexplored.
- What evidence would resolve it: Conducting experiments comparing NAMs to other explainable AI methods like SHAP values or LIME on the same datasets used in the paper.

### Open Question 2
- Question: Can the feature engineering approach used in this study be generalized to other domains beyond science questions, and how would it affect the performance of NAMs?
- Basis in paper: [explicit] The paper acknowledges that results are only shown for science questions and cannot conclude generalizability to other domains.
- Why unresolved: The study is limited to science questions, and the feature engineering approach may not be directly applicable to other domains without modification.
- What evidence would resolve it: Applying the feature engineering approach to short answer items from different subject areas and evaluating the performance of NAMs on these datasets.

### Open Question 3
- Question: How do educators perceive the interpretability of NAM visualizations, and do they find them helpful in understanding student performance?
- Basis in paper: [explicit] The paper mentions the need for qualitative interviews with educators to assess the understandability and helpfulness of NAM visualizations.
- Why unresolved: The paper does not include any qualitative feedback from educators regarding the NAM visualizations.
- What evidence would resolve it: Conducting interviews or surveys with educators to gather their opinions on the interpretability and usefulness of NAM visualizations in the context of automatic short answer grading.

## Limitations
- The model's predictive performance still lags behind DeBERTa, suggesting a trade-off between interpretability and accuracy
- Feature engineering relies on predefined KI phrases that may not capture all valid student response patterns
- Results are limited to science questions, with uncertain generalizability to other subjects and question types

## Confidence

- High confidence: The NAM architecture and training methodology are well-specified and reproducible
- Medium confidence: The predictive performance improvements over logistic regression are demonstrated, but the generalization across domains remains uncertain
- Low confidence: The claim that NAMs provide "sufficient predictive power" for ASAG tasks, given that DeBERTa still outperforms it

## Next Checks
1. Test NAM on a broader range of subjects and question types beyond physics and science to assess generalizability
2. Conduct ablation studies to determine which engineered features contribute most to predictive performance and interpretability
3. Compare NAM's performance with other interpretable models like SHAP or LIME applied to black-box ASAG models to evaluate the unique value proposition of the NAM approach