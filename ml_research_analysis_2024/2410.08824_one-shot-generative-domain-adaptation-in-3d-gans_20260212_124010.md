---
ver: rpa2
title: One-shot Generative Domain Adaptation in 3D GANs
arxiv_id: '2410.08824'
source_url: https://arxiv.org/abs/2410.08824
tags:
- domain
- image
- source
- target
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces 3D-Adapter, the first method for one-shot
  3D Generative Domain Adaptation (GDA). The method transfers a pre-trained 3D generator
  from a source domain to a new target domain using only a single reference image.
---

# One-shot Generative Domain Adaptation in 3D GANs

## Quick Facts
- arXiv ID: 2410.08824
- Source URL: https://arxiv.org/abs/2410.08824
- Reference count: 18
- Primary result: First method for one-shot 3D generative domain adaptation, achieving FID scores of 132.6 on Cartoons, 59.59 on Sketches, and 118.7 on Ukiyoe datasets

## Executive Summary
This paper introduces 3D-Adapter, the first method for one-shot 3D Generative Domain Adaptation (GDA). The method transfers a pre-trained 3D generator from a source domain to a new target domain using only a single reference image. 3D-Adapter achieves high fidelity, large diversity, cross-domain consistency, and multi-view consistency by selectively fine-tuning restricted weight sets, employing four advanced loss functions, and implementing a progressive two-step fine-tuning strategy. Extensive experiments demonstrate that 3D-Adapter outperforms state-of-the-art methods on one-shot 3D GDA.

## Method Summary
3D-Adapter adapts a pre-trained EG3D generator by selectively fine-tuning only the Tri-plane Decoder and Super-resolution Module (G2) rather than the entire model. The method employs four loss functions: domain direction regularization, target distribution learning, image-level source structure maintenance, and feature-level source structure maintenance. A progressive two-step fine-tuning strategy is used, first fine-tuning Tri-D with all four losses, then fine-tuning G2 with three losses. The method uses pre-trained CLIP models to guide the adaptation process through CLIP-space embeddings and token analysis.

## Key Results
- Achieves FID scores of 132.6 on Cartoons, 59.59 on Sketches, and 118.7 on Ukiyoe datasets
- Outperforms state-of-the-art one-shot GDA methods across all evaluated metrics
- Successfully extends to zero-shot scenarios with competitive results
- Maintains cross-domain consistency and multi-view consistency while achieving high fidelity and diversity

## Why This Works (Mechanism)

### Mechanism 1: Selective Fine-tuning
- Claim: Fine-tuning only Tri-plane Decoder and Super-resolution Module avoids severe overfitting and training instability
- Mechanism: Freezing most parameters preserves source domain knowledge while allowing texture/style adaptation
- Core assumption: Source generator's geometry and structure are preserved in frozen parameters
- Evidence anchors: [abstract] "selectively fine-tuning a restricted weight set", [section 3.2] "primarily affected texture information while making minimal changes to 3D geometry"
- Break condition: Large domain gaps may require more expressiveness than frozen parameters allow

### Mechanism 2: Four Loss Functions
- Claim: Four loss functions ensure high fidelity, large diversity, cross-domain consistency, and multi-view consistency
- Mechanism: Each loss addresses specific attributes - domain direction regularization for consistency, target distribution learning for style, source structure maintenance for geometry
- Core assumption: CLIP embeddings capture domain-relevant features for guidance
- Evidence anchors: [abstract] "four advanced loss functions", [section 3.3] detailed loss definitions
- Break condition: CLIP embeddings may not adequately represent domain gap or far target distributions

### Mechanism 3: Progressive Fine-tuning
- Claim: Two-step progressive fine-tuning balances adaptation quality and prevents underfitting/overfitting
- Mechanism: Sequential fine-tuning allows gradual adaptation without destabilizing the model
- Core assumption: Sequential approach allows learning coarse features then refining them
- Evidence anchors: [abstract] "progressive two-step fine-tuning strategy", [section 3.4] fine-tuning description
- Break condition: More than two steps may become computationally prohibitive without significant gains

## Foundational Learning

- **Neural Radiance Fields (NeRF)**: Understanding NeRF is essential as the method builds on EG3D's integration of NeRF-based rendering into GANs. Quick check: What is the difference between NeRF's volume rendering and EG3D's feature image rendering?

- **GANs and Domain Adaptation**: Knowledge of GAN training dynamics and domain adaptation is critical for understanding the method. Quick check: Why might fine-tuning the entire generator lead to severe overfitting with only one reference image?

- **CLIP Embeddings**: CLIP is used to guide adaptation via domain direction regularization and target distribution learning. Quick check: How does CLIP's image encoder help in aligning source and target domain distributions?

## Architecture Onboarding

- **Component map**: Mapping Network → Style-based Generator (G1) → Tri-plane Decoder (Tri-D) → Volume Rendering → Super-resolution Module (G2)
- **Critical path**: Fine-tune Tri-D (Step 1) → Fine-tune G2 (Step 2) → Generate adapted images with target domain fidelity and diversity
- **Design tradeoffs**: Freezing most parameters ensures stability but limits expressiveness; progressive fine-tuning balances adaptation speed and quality; CLIP-based losses improve stability but may not capture all domain-specific features
- **Failure signatures**: Underfitting (images lack target domain style), Overfitting (images replicate reference without diversity), Training instability (loss values fluctuate or diverge)
- **First 3 experiments**: 1) Fine-tune only Tri-D with adversarial loss, 2) Fine-tune only G2 with domain direction regularization, 3) Apply full progressive fine-tuning with all four losses

## Open Questions the Paper Calls Out

- **Open Question 1**: How can 3D-Adapter be extended to preserve cross-domain consistency for highly divergent target domains while maintaining high fidelity and diversity? The current loss functions are primarily designed for domains with more shared attributes.

- **Open Question 2**: Can 3D-Adapter be adapted to handle multi-domain generative domain adaptation, integrating knowledge from multiple learned domains? The current method is limited to single-domain adaptation.

- **Open Question 3**: What is the impact of different CLIP model architectures on 3D-Adapter's performance? The paper uses specific CLIP models but doesn't explore how different architectures affect adaptation quality.

## Limitations

- **CLIP representation validity**: CLIP was trained on natural images, not artistic domains, raising questions about whether CLIP embeddings adequately capture domain-relevant features for target domains like cartoons and sketches.

- **Generalization to extreme domain gaps**: The method was only tested on moderate domain shifts and remains unclear whether it would work for more extreme domain gaps like faces to medical imaging.

- **Computational overhead**: While CLIP-based losses are claimed to improve stability, the paper doesn't report computational overhead or memory requirements compared to traditional approaches.

## Confidence

- **High confidence**: The core technical approach of selective fine-tuning is well-grounded in standard transfer learning principles with clear experimental improvements
- **Medium confidence**: The effectiveness of progressive two-step fine-tuning is supported by ablation studies, but the specific choice of two steps could be arbitrary
- **Low confidence**: The claim that CLIP-based losses provide superior guidance compared to traditional adversarial losses is weakly supported without direct comparisons

## Next Checks

1. **Ablation on CLIP layer selection**: Conduct experiments varying the CLIP layer used for target distribution learning to determine if k=3 is optimal

2. **Cross-domain generalization test**: Apply the method to a more extreme domain gap (e.g., FFHQ → X-ray images or FFHQ → aerial photographs)

3. **Computational efficiency analysis**: Measure and compare training time, memory usage, and parameter counts between the proposed method and traditional full-model fine-tuning approaches