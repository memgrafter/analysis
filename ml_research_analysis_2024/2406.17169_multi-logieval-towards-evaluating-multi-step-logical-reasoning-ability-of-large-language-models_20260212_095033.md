---
ver: rpa2
title: 'Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of
  Large Language Models'
arxiv_id: '2406.17169'
source_url: https://arxiv.org/abs/2406.17169
tags:
- reasoning
- question
- then
- rules
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Multi-LogiEval is a comprehensive evaluation dataset for multi-step
  logical reasoning in large language models, covering propositional logic, first-order
  logic, and non-monotonic reasoning with over 30 inference rules and 60+ rule combinations
  across reasoning depths 1-5. The dataset was constructed via a two-stage method:
  generating meaningful rule combinations for multi-step reasoning and prompting LLMs
  to create natural language contexts and questions.'
---

# Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models

## Quick Facts
- **arXiv ID**: 2406.17169
- **Source URL**: https://arxiv.org/abs/2406.17169
- **Reference count**: 40
- **Primary result**: Multi-LogiEval is a comprehensive evaluation dataset for multi-step logical reasoning in large language models, covering propositional logic, first-order logic, and non-monotonic reasoning with over 30 inference rules and 60+ rule combinations across reasoning depths 1-5.

## Executive Summary
Multi-LogiEval is a comprehensive evaluation dataset designed to assess the multi-step logical reasoning capabilities of large language models across propositional logic, first-order logic, and non-monotonic reasoning. The dataset was constructed using a two-stage method that generates meaningful rule combinations and prompts LLMs to create natural language contexts and questions. The evaluation of six diverse LLMs in zero-shot chain-of-thought settings revealed a consistent performance degradation as reasoning depth increased, with open-source models showing particularly steep declines at higher depths. Manual analysis of reasoning chains uncovered important insights about contextual information impact, lack of correlation between chain length and accuracy, and the unexpected superior performance of smaller models like Mistral-7B over larger ones.

## Method Summary
The Multi-LogiEval dataset was constructed through a two-stage methodology. First, the authors generated meaningful combinations of inference rules from three logical domains: propositional logic (9 rules), first-order logic (9 rules), and non-monotonic reasoning (14 rules), creating over 60 unique rule combinations across reasoning depths 1-5. Second, these rule combinations were used to prompt LLMs to generate natural language contexts and corresponding questions that require multi-step logical reasoning to solve. The resulting dataset contains carefully crafted evaluation items that test the ability of models to apply multiple logical inference rules in sequence. Six LLMs (GPT-4, ChatGPT, Gemini, Yi-34B, Orca-2-13B, and Mistral-7B) were evaluated in a zero-shot chain-of-thought setting, with performance measured across different reasoning depths to assess how model accuracy degrades as logical reasoning complexity increases.

## Key Results
- All evaluated LLMs showed consistent performance degradation as reasoning depth increased, dropping from ~68% accuracy at depth-1 to ~43% at depth-5
- Open-source models (Yi-34B, Orca-2-13B, Mistral-7B) showed particularly steep performance declines at higher reasoning depths compared to proprietary models
- Mistral-7B outperformed larger models like Yi-34B and Orca-2-13B, demonstrating that model size does not necessarily correlate with multi-step logical reasoning ability
- Manual analysis revealed that contextual information significantly impacts reasoning performance, and there is no correlation between reasoning chain length and accuracy

## Why This Works (Mechanism)
The dataset works by systematically combining logical inference rules in meaningful ways that require multi-step reasoning, then embedding these logical structures within natural language contexts that test whether models can identify and apply the correct inference chains. The two-stage construction method ensures that questions are both logically sound and naturally phrased, making the evaluation more realistic and challenging. By varying the depth of reasoning required and testing across multiple logical domains, the dataset captures the full spectrum of multi-step logical reasoning capabilities.

## Foundational Learning
- **Propositional Logic**: Fundamental logical system dealing with propositions and their relationships using connectives like AND, OR, NOT. Needed to understand basic logical inference rules and how they combine in multi-step reasoning. Quick check: Can identify and apply rules like modus ponens, modus tollens, and disjunctive syllogism.
- **First-Order Logic**: Extension of propositional logic that includes quantifiers and predicates, allowing reasoning about objects and their properties. Needed to test more complex reasoning scenarios involving relationships between entities. Quick check: Can apply universal instantiation and existential generalization rules correctly.
- **Non-Monotonic Reasoning**: Logical framework where conclusions can be retracted in light of new evidence, unlike classical monotonic logic. Needed to test reasoning under uncertainty and default assumptions. Quick check: Can correctly apply circumscription and default logic rules in changing contexts.
- **Inference Rule Combinations**: Systematic pairing and sequencing of logical rules to create multi-step reasoning chains. Needed to construct evaluation items that require sequential application of multiple rules. Quick check: Can identify valid inference chains of depth 3-5 that follow logically from given premises.
- **Chain-of-Thought Prompting**: Technique where models are prompted to show their reasoning process step-by-step. Needed to evaluate not just final answers but the reasoning process itself. Quick check: Can generate coherent, step-by-step reasoning chains that lead to correct conclusions.

## Architecture Onboarding
**Component Map**: Rule Generation -> Context Creation -> Question Formation -> Model Evaluation -> Manual Analysis
**Critical Path**: The dataset construction follows: (1) Select logical domain and reasoning depth, (2) Generate valid rule combinations, (3) Create natural language context, (4) Formulate question, (5) Evaluate model responses, (6) Analyze reasoning chains.
**Design Tradeoffs**: Using LLM-generated contexts ensures natural language fluency but introduces potential biases; focusing on zero-shot evaluation provides baseline performance but may underestimate model capabilities; manual analysis provides qualitative insights but doesn't scale well.
**Failure Signatures**: Models struggle particularly with longer reasoning chains (>3 steps), fail to identify correct inference rules when context is complex, and show inconsistent performance across different logical domains despite similar rule complexity.
**First Experiments**:
1. Evaluate additional reasoning strategies (fine-tuning, few-shot prompting) on a subset of Multi-LogiEval items to establish upper bounds on model performance
2. Perform adversarial testing by systematically varying context complexity while keeping logical structure constant to identify specific failure patterns
3. Conduct ablation studies removing contextual information to isolate the impact of natural language understanding on logical reasoning performance

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset construction relies heavily on LLM-generated contexts and questions, potentially introducing biases and inconsistencies
- Evaluation focuses primarily on zero-shot chain-of-thought performance, potentially underestimating models' full reasoning capabilities
- Manual analysis of reasoning chains is subject to human interpretation and doesn't scale effectively for larger datasets
- The study evaluates only six specific LLMs, which may not comprehensively represent the broader landscape of available models

## Confidence
- Dataset comprehensiveness and coverage (High): Systematic construction approach and coverage of multiple logic types with varied inference rules provide strong evidence
- Performance degradation with reasoning depth (High): Consistent accuracy decline across all evaluated models from depth-1 to depth-5 is well-supported
- Open-source model performance limitations (High): Clear performance gap between open-source and proprietary models, particularly at higher reasoning depths
- Mistral-7B superior performance (Medium): Results show Mistral-7B outperforming larger models, but this finding may be context-dependent

## Next Checks
1. Conduct inter-rater reliability analysis on manual reasoning chain evaluations to quantify and address potential subjectivity
2. Expand evaluation to include additional reasoning strategies beyond zero-shot chain-of-thought, such as fine-tuned models and different prompting approaches
3. Perform adversarial testing by introducing controlled variations in rule combinations and contexts to assess model robustness and identify specific failure patterns