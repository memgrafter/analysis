---
ver: rpa2
title: Structured Evaluation of Synthetic Tabular Data
arxiv_id: '2403.10424'
source_url: https://arxiv.org/abs/2403.10424
tags:
- data
- metrics
- score
- columns
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a structured evaluation framework for synthetic
  tabular data, unifying existing metrics under a single mathematical objective: synthetic
  data should be drawn from the same distribution as the observed data. The framework
  decomposes this objective into substructures (marginals, pairwise, leave-one-out
  conditionals, full joint) and maps existing metrics to these substructures.'
---

# Structured Evaluation of Synthetic Tabular Data

## Quick Facts
- arXiv ID: 2403.10424
- Source URL: https://arxiv.org/abs/2403.10424
- Reference count: 40
- Key outcome: This paper presents a structured evaluation framework for synthetic tabular data, unifying existing metrics under a single mathematical objective: synthetic data should be drawn from the same distribution as the observed data.

## Executive Summary
This paper introduces a structured evaluation framework for synthetic tabular data that unifies existing metrics under a single mathematical objective: synthetic data should be drawn from the same distribution as observed data. The framework decomposes this objective into substructures (marginals, pairwise, leave-one-out conditionals, full joint) and maps existing metrics to these substructures, revealing relationships among metrics and enabling reasoning about their completeness. Experiments on three datasets with eight synthesizers show that structure-based methods (PCC, synthpop) outperform deep learning methods, especially on smaller datasets.

## Method Summary
The evaluation framework is based on a formal objective that synthetic data should match the joint distribution of real data. It decomposes this objective into substructures and maps existing metrics to these substructures, revealing relationships among metrics and enabling reasoning about their completeness. The framework motivates model-free baselines (self, perm, half) and introduces model-based metrics using probabilistic cross-categorization (PCC) as a surrogate. Synthesizers are trained on real data, synthetic samples are generated, and 32 evaluation metrics are computed across five repetitions.

## Key Results
- Structure-based methods (PCC, synthpop) outperform deep learning methods, especially on smaller datasets
- The structured approach provides a coherent, complete evaluation and identifies where synthesizers fall short along the structural spectrum
- Model-free metrics (resonance fidelity, application fidelity) correlate with model-based metrics (PCC-based) in all experiments
- ML efficacy and privacy metrics are anti-correlated across synthesizers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The structured evaluation framework unifies existing metrics under a single mathematical objective: synthetic data should be drawn from the same distribution as the observed data.
- Mechanism: By decomposing this objective into substructures (marginals, pairwise, leave-one-out conditionals, full joint), the framework reveals relationships among metrics and enables reasoning about their completeness.
- Core assumption: A single mathematical objective can capture the essential properties of synthetic tabular data quality.
- Evidence anchors:
  - [abstract] "we propose an evaluation framework with a single, mathematical objective that posits that the synthetic data should be drawn from the same distribution as the observed data"
  - [section 3] "Our framework stems from a formal objective that a synthesizer should produce samples from the same joint distribution as the real data"

### Mechanism 2
- Claim: The structured framework motivates model-free baselines (self, perm, half) and introduces model-based metrics using probabilistic cross-categorization (PCC) as a surrogate.
- Mechanism: The framework provides a principled way to generate baselines and introduces PCC-based metrics that use the same estimator and score for all data types and substructures.
- Core assumption: A good surrogate model with arbitrary conditioning ability can effectively evaluate all substructures.
- Evidence anchors:
  - [abstract] "Moreover, the framework motivates model-free baselines and a new spectrum of metrics"
  - [section 3.3] "The idea of this class of metrics is to learn a surrogate model of the true data-generating distribution"

### Mechanism 3
- Claim: The structured approach provides a coherent, complete evaluation and identifies where synthesizers fall short along the structural spectrum.
- Mechanism: By positioning metrics along a spectrum of structure and analyzing the necessity and sufficiency chain, the framework reveals the completeness of evaluation and highlights synthesizer weaknesses.
- Core assumption: The completeness of evaluation can be determined by analyzing the coverage of substructures.
- Evidence anchors:
  - [abstract] "Using our structured framework, we show that synthetic data generators that explicitly represent tabular structure outperform other methods"
  - [section 5] "The structured evaluation shows the degree and extent to which the full joint is mimicked, allowing users to select synthesizer with confidence"

## Foundational Learning

- Concept: Joint distribution of tabular data
  - Why needed here: The core objective of the framework is that synthetic data should be drawn from the same joint distribution as the observed data.
  - Quick check question: What is the difference between a marginal distribution and a joint distribution in the context of tabular data?

- Concept: Leave-one-out conditional distribution
  - Why needed here: The framework uses leave-one-out conditionals to evaluate machine learning efficacy and as a more complex substructure than marginals and pairwise distributions.
  - Quick check question: How does the leave-one-out conditional distribution relate to the full joint distribution?

- Concept: Probabilistic cross-categorization (PCC)
  - Why needed here: PCC is used as a surrogate model for model-based metrics and requires understanding its hierarchical Bayesian nonparametric architecture.
  - Quick check question: What are the key components of the PCC model and how does it enable arbitrary conditioning?

## Architecture Onboarding

- Component map:
  - Evaluation framework: Unifies existing metrics under a single mathematical objective
  - Substructures: Marginals, pairwise, leave-one-out conditionals, full joint, missingness
  - Metrics: Model-free (resonance fidelity, application fidelity) and model-based (PCC-based)
  - Baselines: Self, perm, half
  - Synthesizers: 8 methods evaluated, including PCC and synthpop

- Critical path:
  1. Define the mathematical objective (synthetic data should be drawn from the same distribution as observed data)
  2. Decompose the objective into substructures
  3. Map existing metrics to substructures
  4. Introduce model-free baselines and model-based metrics
  5. Evaluate synthesizers using the structured framework

- Design tradeoffs:
  - Model-free vs model-based metrics: Model-free metrics are more intuitive but may require different estimators for different data types; model-based metrics are more coherent but rely on a good surrogate model
  - Coverage vs complexity: More substructures provide better coverage but increase evaluation complexity
  - Computation vs accuracy: More accurate metrics may be more computationally expensive

- Failure signatures:
  - Poor correlation between model-free and model-based metrics
  - Inconsistent results across different substructures
  - Synthetic data generators that perform well on some substructures but poorly on others

- First 3 experiments:
  1. Evaluate a simple synthesizer (e.g., GaussianCopula) on a small dataset (e.g., student) using the structured framework
  2. Compare the results of model-free and model-based metrics for the same synthesizer and dataset
  3. Analyze the performance of different synthesizers along the structural spectrum to identify strengths and weaknesses

## Open Questions the Paper Calls Out
- Question: How does the structured evaluation framework perform on datasets with complex high-order interactions beyond pairwise relationships?
- Basis in paper: [inferred] The paper demonstrates evaluation on three datasets with pairwise and leave-one-out structures, but doesn't explore higher-order interactions like 3-way or leave-n-out distributions.
- Why unresolved: The paper focuses on a spectrum from marginals to full joint distributions but doesn't test the framework's effectiveness on more complex substructures that might be critical for certain domains.
- What evidence would resolve it: Experiments applying the framework to datasets known to contain significant 3-way or higher-order interactions, comparing performance with and without metrics targeting these structures.

## Limitations
- The framework's reliance on a single mathematical objective may not capture all relevant aspects of synthetic data quality for specialized use cases
- The PCC model introduces its own modeling assumptions that may not align perfectly with all data generating processes
- The framework has been validated on only three datasets, requiring broader validation across different data characteristics

## Confidence
- High confidence: The framework's ability to position existing metrics along a structural spectrum and reveal their relationships (supported by clear mathematical mapping in Section 3.2)
- Medium confidence: The claim that structure-based methods outperform deep learning methods on smaller datasets (based on three datasets, requires broader validation)
- Medium confidence: The completeness of evaluation claim based on structural coverage (logical but not empirically proven)

## Next Checks
1. Test the framework on datasets with different characteristics (high-cardinality categorical variables, highly skewed distributions) to assess robustness beyond the current evaluation
2. Compare PCC-based metrics against task-specific downstream evaluation (e.g., machine learning model performance) to validate surrogate effectiveness
3. Analyze the computational overhead of the structured evaluation across varying dataset sizes to determine scalability limits