---
ver: rpa2
title: A Generative Approach to Surrogate-based Black-box Attacks
arxiv_id: '2402.02732'
source_url: https://arxiv.org/abs/2402.02732
tags:
- attack
- others
- target
- surrogate
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a generative surrogate-based black-box attack
  method for crafting adversarial examples against deep neural networks. Unlike traditional
  discriminative surrogates that mimic target model outputs, the proposed approach
  trains a generative surrogate to learn the distribution of samples residing on or
  near the target model's decision boundaries.
---

# A Generative Approach to Surrogate-based Black-box Attacks

## Quick Facts
- arXiv ID: 2402.02732
- Source URL: https://arxiv.org/abs/2402.02732
- Reference count: 2
- Primary result: Generative surrogate achieves up to 15% higher attack success rates with one iteration versus hundreds needed by discriminative methods

## Executive Summary
This paper introduces a generative surrogate-based approach for crafting adversarial examples against deep neural networks in black-box settings. Unlike traditional methods that train discriminative surrogates to mimic target model outputs, this approach trains a generative surrogate to learn the distribution of samples residing on or near the target model's decision boundaries. The method demonstrates significantly improved attack success rates (up to 15% better) while requiring only a single attack iteration compared to hundreds needed by existing approaches. Experiments on CIFAR-10 and CIFAR-100 datasets show effectiveness in both untargeted and targeted attack settings under probability-only and label-only scenarios.

## Method Summary
The method trains a generative surrogate model using GAN architecture to learn the distribution of samples near decision boundaries. The training combines multiple loss components: realism (LG, LadvG), class-controlling loss (Lycnt), inter-class similarity loss (Lysim) to push samples toward boundaries, and intra-class diversity loss (Lydiv) to ensure coverage across all class boundary intersections. Once trained, the generator can directly produce adversarial examples by sampling from this learned distribution and finding the closest sample from a different class to any original sample. This enables a single-step attack process that achieves higher success rates with dramatically fewer iterations than traditional discriminative surrogate approaches.

## Key Results
- Achieves up to 15% higher attack success rates compared to state-of-the-art surrogate-based attacks
- Requires only one attack iteration versus hundreds needed by discriminative approaches
- Demonstrates effectiveness across both CIFAR-10 and CIFAR-100 datasets in untargeted and targeted settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative surrogate learns the distribution of samples residing on or close to target's decision boundaries, enabling direct sampling of adversarial examples.
- Mechanism: Instead of training a discriminative surrogate to mimic target outputs, the generative approach learns a distribution of realistic samples that are near decision boundaries. This distribution inherently contains adversarial examples because they are extremely close to original samples but belong to different classes.
- Core assumption: The distribution of samples near decision boundaries contains sufficient adversarial examples that are both realistic and effective across all class pairs.
- Break condition: If the target model has highly irregular decision boundaries or if the distribution learned by the generative surrogate doesn't adequately represent all class boundary intersections, the method would fail to find effective adversarial examples.

### Mechanism 2
- Claim: The generative surrogate achieves higher attack success rates with significantly fewer iterations (1 step vs hundreds) compared to discriminative approaches.
- Mechanism: By directly sampling from the learned distribution of boundary-adjacent samples, the attack can identify adversarial examples in a single step. The discriminative approach requires iterative optimization to find adversarial examples because it doesn't have direct access to the boundary distribution.
- Core assumption: The closest sample from a different class in the learned distribution will be an effective adversarial example that satisfies the perturbation constraint.
- Break condition: If the perturbation budget is too restrictive or if the learned distribution doesn't maintain sufficient proximity to the original samples, the single-step approach may fail to find valid adversarial examples.

### Mechanism 3
- Claim: The generative surrogate incorporates multiple loss components (realism, inter-class similarity, intra-class diversity) to create a comprehensive distribution of potential adversarial examples.
- Mechanism: The training combines GAN-based realism (LG, LadvG), class-controlling loss (Lycnt), inter-class similarity loss (Lysim) to push samples toward decision boundaries, and intra-class diversity loss (Lydiv) to ensure coverage across all class boundary intersections.
- Core assumption: Combining these losses creates a distribution that is both realistic and adequately dispersed across all decision boundaries, enabling effective targeted and untargeted attacks.
- Break condition: If the balance between these loss components is incorrect, the learned distribution might be too dispersed (losing proximity to boundaries) or too concentrated (missing certain class boundary regions).

## Foundational Learning

- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: The generative surrogate architecture is based on GANs, which learn to generate realistic samples by competing between generator and discriminator networks.
  - Quick check question: How does the adversarial training between generator and discriminator help the generator learn to produce realistic samples?

- Concept: Decision boundary learning in classification
  - Why needed here: Understanding how decision boundaries separate classes in high-dimensional spaces is crucial for comprehending why samples near these boundaries can be adversarial examples.
  - Quick check question: Why are samples that are extremely close to decision boundaries likely to be misclassified as adjacent classes?

- Concept: Black-box attack threat models
  - Why needed here: The paper operates under a black-box setting where the attacker can only query the target model, making it essential to understand the limitations and opportunities of such scenarios.
  - Quick check question: What are the key differences between query-based and surrogate-based black-box attacks in terms of query efficiency and practicality?

## Architecture Onboarding

- Component map: Random noise + class labels -> Generator -> Samples -> Discriminator (evaluates realism) and Substitute network (mimics target) -> Target network (black-box model being attacked) -> Attack module (finds closest different-class samples)

- Critical path: 1. Train generator with combined losses (realism, adversarial examples, inter-class similarity, intra-class diversity) 2. Use trained generator to find closest sample from different class to original sample 3. Validate perturbation constraint and output adversarial example

- Design tradeoffs:
  - Query budget vs. attack effectiveness: More queries during training improve generator quality but increase cost
  - Perturbation budget: Larger budgets make attacks easier but reduce imperceptibility
  - Distribution coverage: Balancing between concentrated boundary samples and diverse class coverage

- Failure signatures:
  - Low attack success rate despite high generator quality: Indicates distribution doesn't adequately represent boundary regions
  - High similarity between generated samples and originals without misclassification: Suggests insufficient boundary proximity
  - Mode collapse in generator: Results in limited diversity and poor coverage of decision boundaries

- First 3 experiments:
  1. Compare ASR of generative vs discriminative surrogate on a simple binary classification problem with known decision boundaries
  2. Vary the inter-class similarity loss weight to find optimal balance between boundary proximity and realism
  3. Test attack performance with different perturbation budgets to identify the minimum effective budget

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the limitations identified in the analysis, several important questions emerge:

### Open Question 1
- Question: How does the generative surrogate's performance scale with increasing dataset complexity beyond CIFAR-100 (e.g., ImageNet)?
- Basis in paper: The paper demonstrates effectiveness on CIFAR-10 and CIFAR-100 datasets, suggesting potential scalability issues for more complex datasets.
- Why unresolved: The paper does not explore performance on more complex datasets, leaving questions about scalability and adaptability to real-world scenarios.
- What evidence would resolve it: Experiments comparing GSBA performance across multiple dataset complexities, particularly large-scale datasets like ImageNet.

### Open Question 2
- Question: What is the impact of different generative model architectures (beyond GAN-based) on the effectiveness of the surrogate-based attack?
- Basis in paper: The paper uses a GAN-based architecture but does not explore alternative generative models, suggesting potential for improvement.
- Why unresolved: The choice of GAN architecture is not justified as optimal, and other architectures (e.g., VAEs, normalizing flows) might yield better results.
- What evidence would resolve it: Comparative experiments using different generative model architectures while keeping the attack strategy constant.

### Open Question 3
- Question: How does the generative surrogate perform under adaptive defenses specifically designed to detect or mitigate generative-based attacks?
- Basis in paper: The paper focuses on attack effectiveness but does not evaluate against defenses, particularly those targeting generative approaches.
- Why unresolved: Modern defense mechanisms might specifically target generative adversarial examples, potentially reducing GSBA's effectiveness.
- What evidence would resolve it: Experiments testing GSBA against state-of-the-art adversarial defenses, particularly those designed to counter generative attacks.

## Limitations
- The approach assumes that learning distributions near decision boundaries inherently contains effective adversarial examples for all class pairs, which may not hold for highly irregular decision boundaries.
- Single-step attack strategy effectiveness is contingent on sufficiently large perturbation budgets; extremely restrictive budgets may prevent finding valid adversarial examples.
- The balance between multiple loss components (realism, inter-class similarity, intra-class diversity) is critical but not fully explored, raising questions about robustness across different datasets and model architectures.

## Confidence
- **High Confidence**: The claim that generative surrogates can learn distributions of realistic samples is well-established through prior GAN research. The experimental methodology for evaluating attack success rates is standard and reproducible.
- **Medium Confidence**: The specific claim that learning distributions near decision boundaries inherently contains adversarial examples requires empirical validation across diverse model architectures and datasets beyond CIFAR. The single-step attack advantage needs verification on more complex models.
- **Low Confidence**: The paper doesn't adequately address scalability to larger datasets or more complex models, nor does it explore the robustness of the learned distributions to different training regimes and target model variations.

## Next Checks
1. **Decision Boundary Coverage Analysis**: Visualize the learned distribution of samples from the generative surrogate relative to the actual decision boundaries of the target model across multiple class pairs. This would validate whether the learned distribution actually contains adversarial examples or just samples near boundaries.

2. **Perturbation Budget Sensitivity Test**: Systematically vary the perturbation budget (Îµ) and measure the attack success rate and the average perturbation norm of successful attacks. This would reveal the minimum effective budget and whether the single-step approach remains viable under stricter constraints.

3. **Transferability Assessment**: Test whether adversarial examples generated using the generative surrogate on one target model can successfully attack different models (cross-model transferability). This would evaluate whether the learned distribution captures universal boundary characteristics or is model-specific.