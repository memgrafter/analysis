---
ver: rpa2
title: On the Proper Treatment of Tokenization in Psycholinguistics
arxiv_id: '2410.02691'
source_url: https://arxiv.org/abs/2410.02691
tags:
- surprisal
- reading
- focal
- language
- whitespace
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses how to compute language model surprisal for
  character substrings in psycholinguistic experiments when modern models operate
  on tokens rather than characters. It proposes marginalizing token-level language
  models into character-level ones so surprisal can be computed for arbitrary focal
  areas within a region of interest.
---

# On the Proper Treatment of Tokenization in Psycholinguistics

## Quick Facts
- arXiv ID: 2410.02691
- Source URL: https://arxiv.org/abs/2410.02691
- Reference count: 23
- Primary result: Surprisal of focal areas (especially first 3 characters or dynamic spans) better predicts reading behavior than surprisal of full regions in eye-tracking datasets.

## Executive Summary
This paper addresses a fundamental problem in psycholinguistics: computing language model surprisal for character substrings when modern models operate on tokens rather than characters. The authors propose marginalizing token-level language models into character-level ones, enabling surprisal computation for arbitrary focal areas within regions of interest. Across four eye-tracking datasets, they demonstrate that focal area surprisal (particularly for the first three characters or dynamically sized areas matching perceptual spans) often outperforms full region surprisal as a predictor of reading behavior, with R² improvements up to 0.04 for skip rates and 0.03 for gaze durations.

## Method Summary
The method involves marginalizing GPT-2 small token-level surprisal to character-level using a beam summing algorithm with beam size 5. The approach computes surprisal for focal areas defined in various ways: fixed-size (first 3 characters), dynamic (based on perceptual span research), and look-ahead areas spanning into subsequent regions. Linear regression models with baseline predictors (ROI length and log-frequency) and target predictors (focal area surprisal plus spillover effects) are evaluated using 10-fold cross-validation with 10 random seeds. Statistical significance is assessed via paired permutation tests.

## Key Results
- Surprisal of focal areas better predicts reading behavior than surprisal of full regions across four eye-tracking datasets
- First three characters of a region provide the best predictor of skip rate in CELER dataset (R² improvement up to 0.04)
- Dynamically sized focal areas with 8-character rightward word identification span perform on par with full ROI surprisal for Provo and MECO datasets
- Look-ahead focal areas spanning into subsequent regions improve predictions by capturing structural integration costs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Marginalizing token-level models into character-level ones allows computing surprisal for arbitrary character substrings misaligned with token boundaries.
- Mechanism: The beam summing algorithm approximates the marginalization by covering all token sequences whose character expansions include the target substring.
- Core assumption: The marginalization is computationally feasible via approximation despite exponential token space.
- Evidence anchors:
  - [abstract]: "Our proposal of marginalizing a token-level model into a character-level one solves this misalignment issue independently of the tokenization scheme."
  - [section]: "Vieira et al. (2024) show that − →pΣ(σ) can be computed with a finite summation... we use the beam summing algorithm proposed by Vieira et al. (2024) as a practical approximation algorithm."
  - [corpus]: Weak - corpus provides no direct evidence of computational feasibility.
- Break condition: If beam size is insufficient to cover all relevant tokenizations, approximation error becomes unacceptable.

### Mechanism 2
- Claim: Surprisal of focal areas (especially first few characters or dynamically sized spans) better predicts reading behavior than surprisal of full regions.
- Mechanism: Focal areas capture perceptual spans and lexical access processes that full regions obscure due to tokenization misalignment.
- Core assumption: Human reading processes operate at character-level granularity aligned with perceptual spans, not token boundaries.
- Evidence anchors:
  - [abstract]: "Empirically, we discover various focal areas whose surprisal is a better psychometric predictor than the surprisal of the region of interest itself."
  - [section]: "we observe that on the CELER dataset (Berzak et al., 2022), the surprisal of the first three characters is a significantly better predictor of skip rate."
  - [corpus]: Strong - corpus analysis shows specific focal areas (first 3 characters, dynamic spans) consistently outperform full ROI surprisal across multiple datasets.
- Break condition: If perceptual span theories change or tokenization schemes align perfectly with linguistic units.

### Mechanism 3
- Claim: Including look-ahead focal areas that span into subsequent regions improves prediction of reading times by capturing structural integration costs.
- Mechanism: Look-ahead areas allow the model to account for wrap-up effects and constituent boundary processing that occur during current ROI fixation.
- Core assumption: Readers integrate information across region boundaries during fixation, requiring anticipation of upcoming material.
- Evidence anchors:
  - [abstract]: "On the Provo and MECO datasets... the surprisal of the first characters of a region... is on par with the surprisal of the full region."
  - [section]: "the surprisal from the dynamically sized focal area, with a rightward word identification span of 8 characters, is on par with the full ROI's surprisal."
  - [corpus]: Moderate - corpus shows look-ahead areas improve predictions but effect varies across datasets.
- Break condition: If integration cost theories are disproven or look-ahead doesn't improve predictions across datasets.

## Foundational Learning

- Concept: Prefix probabilities and marginalization in language models
  - Why needed here: The paper relies on computing character-level prefix probabilities from token-level models via marginalization, which is the core algorithmic challenge.
  - Quick check question: Given a token-level LM with tokens "an", "ne", "e ", "lo", "ost", what is the character-level prefix probability for "ane"?

- Concept: Focal areas and perceptual spans in reading
  - Why needed here: The experimental design tests different focal area definitions based on perceptual span research, which is central to understanding why certain predictors work better.
  - Quick check question: If a reader fixates at position 5 in a 12-character word, which characters would be included in a focal area with 7-character rightward word identification span?

- Concept: Linear regression with R² and statistical significance testing
  - Why needed here: The paper uses linear modeling and cross-validation to evaluate predictive power, requiring understanding of these statistical methods.
  - Quick check question: If baseline R² = 0.3 and target R² = 0.33, what is the predictive power difference ∆R²?

## Architecture Onboarding

- Component map:
  - Input: Character string stimuli from psycholinguistic datasets
  - Tokenizer: BPE tokenizer that maps characters to tokens
  - Token-level LM: GPT-2 small providing token probabilities
  - Marginalization engine: Beam summing algorithm computing character-level probabilities
  - Focal area processor: Extracts substrings based on ROI and focal area definitions
  - Surprisal calculator: Computes negative log probabilities
  - Regression model: Linear model with baseline and target predictors
  - Evaluation: Cross-validation with R² and permutation tests

- Critical path:
  1. Load stimulus and ROI definitions
  2. For each ROI, compute focal area surprisal via marginalization
  3. Fit linear regression model with baseline + focal area surprisal
  4. Evaluate predictive power via cross-validation

- Design tradeoffs:
  - Beam size vs. approximation accuracy in marginalization
  - Fixed vs. dynamic focal area sizing based on perceptual span research
  - Including vs. excluding whitespace in ROIs based on dataset conventions
  - Linear vs. non-linear modeling of surprisal-reading time relationship

- Failure signatures:
  - Poor predictive power (∆R² ≈ 0) suggests focal area definition issues
  - High variance across folds indicates instability in marginalization approximation
  - Negative ∆R² suggests baseline predictors are overfitting

- First 3 experiments:
  1. Verify marginalization correctness by comparing full ROI surprisal to ground truth on simple tokenization schemes
  2. Test fixed-size focal area (first 3 characters) across all datasets to establish baseline predictive power
  3. Compare dynamic focal areas with different word identification spans to identify optimal span size

## Open Questions the Paper Calls Out

- Question: What is the exact functional relationship between surprisal and skip rates in psycholinguistic experiments?
  - Basis in paper: [inferred] The paper uses linear modeling for other reading time measurements but acknowledges that "this relationship has not yet been determined for skip rates" and that "no studies have examined skip rates using our focal area predictors."
  - Why unresolved: Previous studies have established linear relationships between surprisal and reading times, but skip rates may have different characteristics that require different modeling approaches. The authors note that skip rates "remain to be determined" for the focal area predictors used in this study.
  - What evidence would resolve it: Empirical studies comparing linear models versus non-linear approaches (like generalized additive models) specifically for skip rate prediction, with comprehensive testing across multiple datasets and focal area configurations.

- Question: How do self-paced reading paradigms interact with focal area predictors compared to eye-tracking paradigms?
  - Basis in paper: [explicit] The authors explicitly state "We do not analyze self-paced reading, where the challenges are likely even more complex" and note various complications like moving-window paradigms, centered presentation, and different presentation formats.
  - Why unresolved: Self-paced reading has fundamental differences from eye-tracking in how stimuli are presented and measured, including variations in whitespace handling, ROI presentation methods, and potential cognitive differences in processing strategies between the two paradigms.
  - What evidence would resolve it: Comparative studies applying the same focal area analysis to both eye-tracking and self-paced reading datasets, examining whether focal area predictors transfer between paradigms and identifying paradigm-specific effects.

- Question: To what extent do individual differences between participants affect the predictive power of focal area surprisal?
  - Basis in paper: [explicit] The authors note they "do not account for individual differences between participants in our analysis" and suggest "future research should investigate skip rates with modeling approaches capable of capturing non-linear relationships, such as mixed-effects models."
  - Why unresolved: The current analysis averages measurements across participants, potentially masking individual variation in reading strategies, language proficiency, and cognitive processing that could affect how focal area surprisal relates to reading behavior.
  - What evidence would resolve it: Mixed-effects models incorporating participant-level random effects, or individual-level analysis showing whether focal area surprisal effects vary systematically across different reader populations or individual characteristics.

## Limitations

- The beam summing approximation for marginalization introduces computational uncertainty, with no systematic evaluation of how beam size affects approximation accuracy
- The positive results may be partially dataset-specific rather than demonstrating a universal solution to tokenization misalignment
- The paper assumes tokenization misalignment is the primary source of error but does not thoroughly explore alternative explanations

## Confidence

**High Confidence:** The technical claim that marginalizing token-level models to character-level enables computation of surprisal for arbitrary focal areas is sound and well-supported.

**Medium Confidence:** The empirical finding that focal areas predict reading behavior better than full ROI surprisal is supported by multiple datasets, though the effect size is modest (∆R² up to 0.04).

**Low Confidence:** The assertion that tokenization misalignment is the primary reason for suboptimal performance of full ROI surprisal is plausible but not definitively proven.

## Next Checks

1. **Beam Size Sensitivity Analysis:** Systematically evaluate how varying beam sizes (e.g., 1, 3, 5, 10, 20) affects both approximation accuracy and predictive power to quantify the tradeoff between computational efficiency and model performance.

2. **Cross-Dataset Mechanism Analysis:** Conduct a detailed analysis of when and why focal areas outperform full ROI surprisal across different datasets, examining the relationship between tokenization schemes, ROI lengths, and predictive improvements.

3. **Alternative Approximation Methods Comparison:** Compare the beam summing algorithm with other marginalization approaches (e.g., Monte Carlo sampling, exact marginalization on simplified tokenization schemes) to determine whether improvements are due to the marginalization technique itself or specific implementation choices.