---
ver: rpa2
title: Large Language Models for Automatic Detection of Sensitive Topics
arxiv_id: '2409.00940'
source_url: https://arxiv.org/abs/2409.00940
tags:
- topics
- sensitive
- llms
- related
- mental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated five large language models (LLMs) for detecting
  sensitive mental well-being content in online conversations. Using two datasets
  of 1,000 manually labeled messages, the best-performing model, GPT-4o, achieved
  99.5% accuracy and an F1-score of 0.99.
---

# Large Language Models for Automatic Detection of Sensitive Topics

## Quick Facts
- arXiv ID: 2409.00940
- Source URL: https://arxiv.org/abs/2409.00940
- Reference count: 40
- Best-performing model achieved 99.5% accuracy and F1-score of 0.99

## Executive Summary
This study evaluates five large language models for detecting sensitive mental well-being content in online conversations. Using two datasets of 1,000 manually labeled messages, the research demonstrates that LLMs can effectively identify sensitive topics including suicidal ideation, self-harm, abuse, and severe mental health disorders. The best-performing model, GPT-4o, achieved exceptional accuracy rates exceeding 99%, with all models maintaining over 95% accuracy. The study also found that temperature settings had minimal impact on detection performance, and JSON-formatted prompts showed benefits for complex message classification.

## Method Summary
The study employed zero-shot prompting to evaluate five LLMs (GPT-4, GPT-4o, GPT-4-Turbo, Llama-3, and Solar) using two open-source datasets: "Topical-Chat" and "Mental-health-counseling-conversations." After preprocessing and human coding, a balanced dataset of 1,000 messages (500 sensitive, 500 non-sensitive) was created. Models were tested at temperature settings of 0.0, 0.3, 0.5, and 0.7 with five repetitions at temperature 0.0 and three repetitions at other temperatures. Performance was evaluated using accuracy, precision, recall, F1-score, and true negative rate metrics, with statistical analysis to determine significant differences between models.

## Key Results
- GPT-4o achieved the highest performance with 99.5% accuracy and 0.99 F1-score
- All five models achieved over 95% accuracy in sensitive content detection
- Temperature settings (0.0-0.7) showed no significant impact on model performance
- JSON-formatted prompts improved Solar model accuracy from 80.56% to 88.91% for complex messages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based moderation can achieve high detection accuracy for sensitive content.
- Mechanism: GPT-4o and GPT-4 achieved 99.5% accuracy by leveraging zero-shot prompting and powerful language understanding.
- Core assumption: High accuracy remains stable across different temperature settings (0.0-0.7).
- Evidence anchors:
  - [abstract] "The best-performing model, GPT-4o, achieved 99.5% accuracy and an F1-score of 0.99."
  - [section 6.2.1] "All models achieved an accuracy rate of over 95%."
  - [corpus] Weak evidence; only citation counts available.
- Break condition: Accuracy drops significantly when context is insufficient or when batch processing introduces errors.

### Mechanism 2
- Claim: Temperature settings do not significantly affect LLM detection performance.
- Mechanism: Testing across temperature settings (0.0, 0.3, 0.5, 0.7) showed no significant accuracy or F1-score changes.
- Core assumption: Deterministic behavior at temperature 0.0 does not guarantee consistent results due to GPU parallel processing non-determinism.
- Evidence anchors:
  - [section 6.3.1] "The results indicated that the accuracy did not significantly differ at different temperatures."
  - [section 7.3] "Despite setting the temperature to 0 in our study, some inconsistencies remained."
  - [corpus] Weak evidence; limited related work.
- Break condition: Performance changes become significant if temperature is increased beyond tested range or with different models.

### Mechanism 3
- Claim: JSON-formatted prompts improve LLM performance for complex tasks.
- Mechanism: Solar model showed higher accuracy with JSON prompts (88.91%) compared to natural language (80.56%) for detecting sensitive messages.
- Core assumption: Longer, more complex messages benefit from structured input formats.
- Evidence anchors:
  - [section 7.1] "Solar's accuracy for detecting sensitive messages was relatively high (T+ Mean=88.91%) when using JSON-formatted prompts."
  - [section 5.3] "We used JSON for the GPT, see Appendix B."
  - [corpus] Weak evidence; only citation counts available.
- Break condition: JSON formatting does not improve performance for shorter or simpler messages.

## Foundational Learning

- Concept: Zero-shot prompting
  - Why needed here: Allows evaluation of LLM baseline performance without model fine-tuning.
  - Quick check question: What is the difference between zero-shot and few-shot prompting?

- Concept: F1-score calculation
  - Why needed here: Balances precision and recall for evaluating sensitive content detection.
  - Quick check question: How is F1-score calculated from precision and recall?

- Concept: Temperature parameter in LLMs
  - Why needed here: Controls output randomness and affects model consistency.
  - Quick check question: What happens to LLM output when temperature is set to 0.0?

## Architecture Onboarding

- Component map:
  Input layer (JSON-formatted prompts with sensitive topic definitions) -> Processing layer (five LLM models: GPT-4, GPT-4o, GPT-4-Turbo, Llama-3, Solar) -> Output layer (Boolean classification: sensitive/non-sensitive) -> Evaluation layer (accuracy, precision, recall, F1-score metrics)

- Critical path:
  Prompt → LLM API call → Boolean response → Accuracy calculation → Error analysis

- Design tradeoffs:
  - Model accuracy vs. API cost (GPT-4o vs. GPT-4)
  - Batch processing speed vs. individual accuracy
  - JSON formatting complexity vs. performance improvement

- Failure signatures:
  - Response failure: Missing boolean output
  - Incorrect response: Wrong sensitivity classification
  - Inconsistent results: Temperature setting effects

- First 3 experiments:
  1. Test individual message processing vs. batch API calls for accuracy differences
  2. Compare JSON vs. natural language prompts for complex messages
  3. Evaluate temperature impact beyond 0.7 to find performance thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLM architectures (e.g., transformer variants, attention mechanisms) impact their ability to detect sensitive mental health content?
- Basis in paper: [inferred] The paper compares several LLMs but doesn't analyze architectural differences between them.
- Why unresolved: The study focuses on performance comparisons rather than architectural analysis. Different underlying architectures could explain performance variations.
- What evidence would resolve it: Controlled experiments testing identical tasks across different architectural designs while holding other variables constant.

### Open Question 2
- Question: What are the long-term effects of LLM-assisted content moderation on human moderator well-being and job satisfaction?
- Basis in paper: [explicit] The paper discusses how LLMs could relieve human moderators from tedious tasks but doesn't examine the long-term impact on moderators.
- Why unresolved: The study focuses on LLM performance metrics rather than human factors and psychological outcomes for moderators.
- What evidence would resolve it: Longitudinal studies tracking moderator stress levels, job satisfaction, and burnout rates before and after LLM integration.

### Open Question 3
- Question: How do cultural differences in expressing mental health concerns affect LLM detection accuracy across different populations?
- Basis in paper: [explicit] The paper acknowledges this as a limitation, noting that datasets were from the USA and cultural differences weren't examined.
- Why unresolved: The study used US-based datasets without testing cross-cultural generalizability.
- What evidence would resolve it: Testing the same LLM models on datasets from different cultural contexts and measuring performance variations.

## Limitations

- The preprocessing methodology for original datasets is not fully specified, particularly regarding duplicate removal and inappropriate content filtering
- Human coding process lacks detailed documentation including specific labeling criteria and inter-rater reliability measures
- The study only tested temperature settings up to 0.7, leaving open the question of performance at higher temperature values
- Batch processing via API calls may introduce accuracy degradation that wasn't fully characterized

## Confidence

**High Confidence**: The core finding that GPT-4o and GPT-4 achieve superior performance (99.5% accuracy, F1-score of 0.99) is well-supported by the experimental results across multiple repetitions and temperature settings.

**Medium Confidence**: The claim that temperature settings have minimal impact on performance is supported by the data but may not hold true for temperature values beyond those tested (0.0-0.7).

**Low Confidence**: The assertion that JSON-formatted prompts consistently improve performance for complex messages is based on a single model comparison and may not generalize across different LLM architectures or message types.

## Next Checks

1. **Preprocessing Validation**: Reconstruct the exact preprocessing pipeline for both datasets, documenting how duplicates were identified and removed, and what criteria were used to filter inappropriate content.

2. **Human Coding Verification**: Implement the human coding process with detailed instructions and measure inter-rater reliability using Cohen's kappa or similar metrics to ensure consistent labeling.

3. **Temperature Boundary Testing**: Extend temperature testing to values beyond 0.7 (up to 1.0) to identify potential performance thresholds and verify that the observed stability holds across the full range of possible temperature settings.