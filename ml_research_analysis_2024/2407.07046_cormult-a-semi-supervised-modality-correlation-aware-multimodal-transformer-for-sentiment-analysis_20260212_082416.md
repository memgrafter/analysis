---
ver: rpa2
title: 'CorMulT: A Semi-supervised Modality Correlation-aware Multimodal Transformer
  for Sentiment Analysis'
arxiv_id: '2407.07046'
source_url: https://arxiv.org/abs/2407.07046
tags:
- transformer
- modality
- multimodal
- sentiment
- correlation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multimodal sentiment analysis
  when modality correlations are weak, which is common in real-world data. Existing
  methods struggle in such scenarios due to their reliance on strong correlations
  between modalities.
---

# CorMulT: A Semi-supervised Modality Correlation-aware Multimodal Transformer for Sentiment Analysis

## Quick Facts
- arXiv ID: 2407.07046
- Source URL: https://arxiv.org/abs/2407.07046
- Authors: Yangmin Li; Ruiqi Zhu; Wengen Li
- Reference count: 40
- Primary result: Achieves 52.8% accuracy for 7-class classification and 86.7% F1 score on CMU-MOSEI dataset, outperforming state-of-the-art methods in weakly correlated multimodal sentiment analysis

## Executive Summary
This paper addresses the challenge of multimodal sentiment analysis when modality correlations are weak, which is common in real-world data. Existing methods struggle in such scenarios due to their reliance on strong correlations between modalities. The authors propose CorMulT, a two-stage semi-supervised model that first learns modality correlation coefficients using a contrastive learning module, and then fuses these correlations with modality representations for sentiment prediction. Evaluated on the CMU-MOSEI dataset, CorMulT achieves 52.8% accuracy for 7-class classification (Acc@7) and 86.7% F1 score, significantly outperforming state-of-the-art methods.

## Method Summary
CorMulT is a two-stage semi-supervised model that addresses multimodal sentiment analysis with weak modality correlations. The first stage involves a modality correlation contrastive learning module that learns correlation coefficients between different modalities using a triplet loss framework. This pre-training stage generates positive and negative samples through temporal shift, cross-sample mixing, and data perturbation strategies. The second stage employs a correlation-aware multimodal transformer that fuses the learned correlation coefficients with modality representations (audio: Mel spectrogram, text: BERT, visual: ResNet50) using cross-modal attention layers. The model outputs sentiment predictions through linear projection and softmax.

## Key Results
- Achieves 52.8% accuracy for 7-class classification (Acc@7) on CMU-MOSEI dataset
- Achieves 86.7% F1 score on CMU-MOSEI dataset
- Outperforms state-of-the-art methods in scenarios with weak modality correlations

## Why This Works (Mechanism)

### Mechanism 1
The modality correlation contrastive learning module improves sentiment analysis accuracy in scenarios with weak modality correlations by learning cross-modal alignment even when individual modalities are misaligned or noisy. A triplet loss framework enforces that correlated modalities (positive pairs) are closer in a shared correlation space than uncorrelated ones (negative pairs), allowing the model to extract robust sentiment-relevant features despite misalignment or noise. This works under the assumption that even when modalities are weakly correlated, there remains some shared sentiment-relevant information that can be captured by contrastive learning.

### Mechanism 2
The correlation-aware multimodal transformer module enhances sentiment prediction by weighting modality interactions according to learned correlation coefficients. Cross-modal attention layers are modulated by correlation coefficients learned in the pre-training stage, allowing the model to emphasize strongly correlated modality pairs and de-emphasize weakly correlated ones during fusion. This mechanism assumes that correlation coefficients learned during pre-training accurately reflect the importance of each modality pair for sentiment analysis in the downstream task.

### Mechanism 3
Modality-specific feature extraction methods preserve the unique characteristics of each modality, improving overall representation quality. Tailored approaches (Mel spectrogram for audio, ResNet50 for images, BERT for text) are used to extract modality-specific features before cross-modal fusion, ensuring that each modality's unique signal properties are maintained. This works under the assumption that the chosen feature extraction methods are optimal for capturing sentiment-relevant information in each modality.

## Foundational Learning

- Concept: Triplet loss in contrastive learning
  - Why needed here: Enables learning of modality correlations by pushing apart negative pairs while pulling together positive pairs in a shared space
  - Quick check question: In a triplet loss formulation, what should be the relationship between the distance of an anchor to a positive sample versus its distance to a negative sample?

- Concept: Cross-modal attention mechanisms
  - Why needed here: Allows the model to dynamically weight interactions between different modalities based on their learned correlations
  - Quick check question: How does cross-modal attention differ from self-attention in terms of the query, key, and value sources?

- Concept: Modality-specific feature extraction
  - Why needed here: Preserves the unique characteristics of each modality (audio, text, visual) before fusion, ensuring that sentiment-relevant information is not lost during preprocessing
  - Quick check question: Why might using a generic feature extractor across all modalities be less effective than modality-specific approaches for sentiment analysis?

## Architecture Onboarding

- Component map: Modality-specific feature extraction -> Contrastive learning module (MCE) -> Extract correlation coefficients -> Cross-modal transformer with weighted attention -> Sentiment prediction
- Critical path: Pre-training MCE → Extract correlation coefficients → Fusion with multimodal features → Sentiment prediction
- Design tradeoffs: Increased model complexity and training time due to two-stage architecture; potential overfitting to correlation patterns specific to the training dataset; reliance on quality of pre-training data and perturbation strategies
- Failure signatures: Degraded performance on strongly correlated data (overfitting to weak correlation patterns); high variance in correlation coefficients across different data splits; sentiment predictions that ignore one modality entirely (correlation coefficients near zero)
- First 3 experiments:
  1. Ablation study: Remove MCE pre-training and use uniform attention weights; compare accuracy drop
  2. Perturbation strategy analysis: Test different negative sample generation strategies (A, B, C, D) and measure impact on correlation learning quality
  3. Modality removal test: Train and evaluate with only two modalities (V+T, A+T, V+A) to assess the contribution of each modality to overall performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of CorMulT change when using different negative sample generation strategies (e.g., temporal shift, cross-sample mixing, data perturbation) beyond the combination strategy D? The paper discusses four strategies (A, B, C, D) and shows that Strategy D achieves the highest performance, but does not explore the individual impact of each strategy in depth.

### Open Question 2
How does the performance of CorMulT vary with different distance metrics (e.g., Euclidean, Manhattan, Chebyshev) in the Modality Correlation Evaluation (MCE) module beyond cosine similarity? The paper compares different distance metrics and finds cosine similarity to be optimal, but does not explore other potential metrics or combinations.

### Open Question 3
How does the performance of CorMulT scale with the size of the training dataset, particularly for datasets with varying levels of modality correlation? The paper evaluates CorMulT on CMU-MOSEI and CH-SIMS datasets but does not explore the impact of dataset size or correlation strength on performance.

## Limitations

- Performance claims rely heavily on CMU-MOSEI dataset, which may not fully represent real-world scenarios with extremely weak or absent modality correlations
- Two-stage training approach introduces significant complexity, and performance could degrade if pre-training stage fails to learn accurate correlation coefficients
- Perturbation strategies for generating negative samples may not cover all possible misalignment scenarios in real-world applications

## Confidence

- High Confidence: The general architecture of CorMulT (two-stage semi-supervised approach with modality-specific feature extraction) is well-specified and reproducible
- Medium Confidence: The claim that CorMulT outperforms state-of-the-art methods on CMU-MOSEI dataset is supported by reported metrics but lacks extensive ablation studies to isolate the contribution of each component
- Low Confidence: The assertion that CorMulT is particularly effective for weakly correlated data is based on the assumption that the contrastive learning module can learn meaningful correlations even in challenging scenarios, which may not hold for all types of weak correlations

## Next Checks

1. **Dataset Generalization Test**: Evaluate CorMulT on datasets with known weak modality correlations (e.g., social media posts with images, text, and audio) to verify its robustness beyond CMU-MOSEI
2. **Ablation Study on Perturbation Strategies**: Systematically test the impact of each negative sample generation strategy (temporal shift, cross-sample mixing, data perturbation) on the quality of learned correlation coefficients
3. **Correlation Coefficient Analysis**: Analyze the distribution of learned correlation coefficients across different data splits to identify potential overfitting or bias in the pre-training stage