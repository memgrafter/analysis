---
ver: rpa2
title: 'GTR: Improving Large 3D Reconstruction Models through Geometry and Texture
  Refinement'
arxiv_id: '2406.05649'
source_url: https://arxiv.org/abs/2406.05649
tags:
- images
- reconstruction
- input
- refinement
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GTR, a large 3D reconstruction model that
  converts multi-view images into high-quality 3D meshes with detailed textures. The
  approach improves upon LRM by replacing its DINO transformer encoder with a convolutional
  encoder to preserve high-frequency details, substituting deconvolution layers with
  pixel shuffle layers to eliminate grid artifacts, and using separate MLPs for density
  and color estimation.
---

# GTR: Improving Large 3D Reconstruction Models through Geometry and Texture Refinement

## Quick Facts
- arXiv ID: 2406.05649
- Source URL: https://arxiv.org/abs/2406.05649
- Reference count: 13
- Achieves PSNR of 29.79 and SSIM of 0.94 on Google Scanned Objects and OmniObject3D datasets

## Executive Summary
GTR is a large 3D reconstruction model that converts multi-view images into high-quality 3D meshes with detailed textures. Building on the LRM architecture, GTR introduces three key improvements: replacing the DINO transformer encoder with a convolutional encoder to better preserve high-frequency details, substituting deconvolution layers with pixel shuffle layers to eliminate grid artifacts, and using separate MLPs for density and color estimation. The model employs a two-stage training process combining volume rendering with differentiable mesh extraction, followed by a lightweight per-instance texture refinement that improves texture quality in just 4 seconds. GTR achieves state-of-the-art results on major 3D reconstruction benchmarks and enables downstream text/image-to-3D generation applications.

## Method Summary
GTR modifies the LRM architecture by replacing the DINO transformer encoder with a convolutional encoder, swapping deconvolution layers for pixel shuffle in the triplane upsampler, and using separate MLPs for density and color prediction. The model is trained in two stages: first optimizing a NeRF model via volume rendering using MSE and LPIPS loss on 4 input vs 4 supervision views, then refining geometry through differentiable mesh extraction with DiffMC and mesh rendering using MSE on images, depths, masks, and normals. A lightweight per-instance texture refinement fine-tunes triplane features and color models on mesh surfaces using L2 loss on input images, taking only 4 seconds per instance. The model uses 256³ triplane resolution, 512x512 image resolution, and requires camera parameters for training and refinement.

## Key Results
- Achieves PSNR of 29.79 and SSIM of 0.94 on Google Scanned Objects and OmniObject3D datasets
- Texture refinement procedure improves texture quality in just 4 seconds per instance
- Eliminates grid-shaped artifacts present in standard LRM reconstructions
- Enables downstream text/image-to-3D generation applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing DINO transformer with a convolutional encoder improves high-frequency detail preservation.
- Mechanism: DINO is trained for semantic understanding and discards low-level image details; a convolutional encoder trained from scratch retains these details better.
- Core assumption: Semantic understanding is not required for accurate geometry/texture reconstruction.
- Evidence anchors:
  - [abstract]: "DINO features tend to discard high-frequency image details, which are important for the precise reconstruction, from the input images."
  - [section 3.1]: "we propose to replace the DiNO (Caron et al., 2021) architecture with a convolutional encoder...it does not exhibit the bias of pre-trained DiNO."
  - [corpus]: Weak—no direct comparison found; assumption based on DINO's design intent.

### Mechanism 2
- Claim: Using pixel shuffle layers instead of deconvolution layers eliminates grid-shaped artifacts.
- Mechanism: Deconvolution in GANs is known to cause checkerboard/grid artifacts; pixel shuffle performs upsampling without introducing such patterns.
- Core assumption: Artifacts in previous LRMs stem from deconvolution operations.
- Evidence anchors:
  - [abstract]: "we noticed that reconstruction from standard LRM often exhibits regular grid artifacts...To address this, we replace all deconvolution layers with Pixelshuffle layers."
  - [section 3.1]: "Odena et al. (Odena et al., 2016) show that for 2D generators deconvolutions are the main source of grid-shaped artifacts."
  - [corpus]: Weak—no direct empirical comparison cited.

### Mechanism 3
- Claim: Differentiable mesh extraction via DiffMC enables full-resolution geometry supervision.
- Mechanism: DiffMC allows gradients to flow from mesh rendering back to NeRF density, enabling geometry refinement beyond volume rendering.
- Core assumption: Optimizing through explicit meshes yields higher geometric fidelity than implicit volume rendering alone.
- Evidence anchors:
  - [abstract]: "we extract meshes from the NeRF field in a differentiable manner and fine-tune the NeRF model through mesh rendering."
  - [section 3.2]: "we transfer the density field to an SDF field...This enables us to render full-resolution images for supervision."
  - [corpus]: Weak—DiffMC usage is common but specific effectiveness here not externally validated.

## Foundational Learning

- Concept: Neural Radiance Fields (NeRF)
  - Why needed here: Core representation for volumetric rendering before mesh extraction.
  - Quick check question: What is the role of the density MLP in NeRF, and how does it differ from the color MLP?

- Concept: Differentiable Marching Cubes (DiffMC)
  - Why needed here: Enables gradient flow from mesh rendering back to the implicit NeRF density field.
  - Quick check question: How does DiffMC handle gradients through the isosurface extraction step?

- Concept: Triplane representation
  - Why needed here: Compact 3D feature grid that maps to per-point density/color predictions.
  - Quick check question: Why does the transformer operate at a low triplane resolution and then upscale?

## Architecture Onboarding

- Component map: E -> T -> Pixel Shuffle -> fd, fc -> NeRF rendering -> DiffMC -> Mesh rendering
- Critical path: Encoder → Transformer → Upsampler → MLPs → NeRF rendering → DiffMC → Mesh rendering
- Design tradeoffs:
  - Conv encoder vs DINO: Simpler, no semantic bias, but requires training from scratch.
  - Pixel shuffle vs deconvolution: Fewer artifacts, but may lose some upsampling capacity.
  - Separate MLPs: Enables per-instance fine-tuning, but doubles inference cost.
- Failure signatures:
  - Grid artifacts → Deconvolution not fully removed.
  - Blurry textures → Geometry refinement insufficient or normal loss omitted.
  - Slow convergence → Encoder/transfomer capacity mismatch.
- First 3 experiments:
  1. Replace DINO with conv encoder; verify PSNR improves on validation set.
  2. Swap pixel shuffle for deconvolution; compare artifact visibility visually.
  3. Add DiffMC geometry refinement; measure geometry quality vs NeRF+MC baseline.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions in the text.

## Limitations
- Current approach requires camera-conditioned input and doesn't work with unposed images
- 256³ triplane resolution sometimes fails to capture fine details in complex assets
- Two-stage training process increases computational requirements compared to single-stage methods

## Confidence
- High: PSNR/SSIM improvements over LRM baseline, texture refinement speed, downstream application success
- Medium: Grid artifact elimination mechanism, DiffMC geometry refinement benefits
- Low: DINO encoder replacement rationale, pixel shuffle vs deconvolution artifact comparison

## Next Checks
1. Reconstruct objects using both DINO and convolutional encoders with identical training settings to measure high-frequency detail preservation differences
2. Compare pixel shuffle against deconvolution layers in the same upsampling context while measuring artifact intensity and frequency content
3. Validate DiffMC geometry refinement by measuring Chamfer Distance improvements before and after mesh extraction on identical density fields