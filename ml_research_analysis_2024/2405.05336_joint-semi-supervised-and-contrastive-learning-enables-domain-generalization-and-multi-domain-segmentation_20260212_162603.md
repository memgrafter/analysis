---
ver: rpa2
title: Joint semi-supervised and contrastive learning enables domain generalization
  and multi-domain segmentation
arxiv_id: '2405.05336'
source_url: https://arxiv.org/abs/2405.05336
tags:
- segclr
- domain
- data
- unet
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces SegCLR, a joint semi-supervised and contrastive
  learning framework for domain-generalized segmentation of retinal optical coherence
  tomography images. SegCLR trains a UNet model simultaneously with supervised and
  contrastive losses, leveraging both labeled and unlabeled data from multiple domains
  to learn robust, generalizable features.
---

# Joint semi-supervised and contrastive learning enables domain generalization and multi-domain segmentation

## Quick Facts
- arXiv ID: 2405.05336
- Source URL: https://arxiv.org/abs/2405.05336
- Reference count: 37
- SegCLR achieves Dice scores close to fully supervised upper bounds in unsupervised domain adaptation for retinal OCT segmentation

## Executive Summary
SegCLR introduces a joint semi-supervised and contrastive learning framework for domain-generalized segmentation of retinal optical coherence tomography images. The method trains a UNet model simultaneously with supervised and contrastive losses, leveraging both labeled and unlabeled data from multiple domains to learn robust, generalizable features. Across three clinical OCT datasets with domain shifts due to device or disease type, SegCLR consistently outperforms conventional supervised training, self-supervised pretraining, and domain adaptation baselines.

## Method Summary
SegCLR trains a UNet model with joint supervised and contrastive losses. The supervised loss anchors the model to the source domain segmentation task, while the contrastive loss forces the encoder to learn domain-invariant features by pulling positive pairs together and pushing negative pairs apart. The framework supports different projection heads (Cch preserving spatial context, Cpool with global pooling) and pair generation strategies (augmentations, slice coherence, or both). SegCLR can perform unsupervised domain adaptation with unlabeled target data or domain generalization without any target data.

## Key Results
- SegCLR consistently outperforms supervised training, self-supervised pretraining, and domain adaptation baselines across three clinical OCT datasets
- For unsupervised domain adaptation, SegCLR achieves Dice scores close to fully supervised upper bounds without requiring target domain labels
- Performance is minimally impacted by the amount of unlabeled target data, enabling effective zero-shot domain generalization
- SegCLR adds only ~6.85% more parameters during training compared to standard UNet

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SegCLR's joint training of supervised and contrastive losses enables better domain generalization by preserving source knowledge while learning robust representations.
- Mechanism: The supervised loss anchors the model to the source domain segmentation task, while the contrastive loss forces the encoder to learn domain-invariant features by pulling positive pairs together and pushing negative pairs apart. This dual objective prevents catastrophic forgetting and encourages generalizable feature extraction.
- Core assumption: The contrastive loss can extract useful features even when labeled data is limited, and these features are transferable across domains with different appearance and content.
- Evidence anchors:
  - [abstract]: "SegCLR trains a UNet model simultaneously with supervised and contrastive losses, leveraging both labeled and unlabeled data from multiple domains to learn robust, generalizable features."
  - [section II-C]: "Considering a source domain Ds and a target domain Dt, total loss L is calculated as follows: L = 1/2(Lcon x∈Ds + Lcon x∈Dt) + λ Lsup (x,y)∈Ds"
  - [corpus]: No direct evidence in corpus papers; this is a novel combination not explicitly covered in related works.

### Mechanism 2
- Claim: The choice of contrastive projection head (Cch vs Cpool) significantly impacts SegCLR's ability to learn spatially-aware features for segmentation.
- Mechanism: Cch uses a 1x1 convolution to aggregate features while preserving spatial context, allowing backpropagation from the contrastive loss to inform pixel-level differences. Cpool uses global pooling, which loses spatial information and may hinder learning relevant segmentation features.
- Core assumption: Spatial context is crucial for learning segmentation-relevant features, and the contrastive loss can effectively guide feature learning when spatial information is preserved.
- Evidence anchors:
  - [section II-E]: "Typical contrastive learning frameworks such as SimCLR and SimSiam use a projection (denoted herein by Cpool) where ρagg pool : Rw×h×c → R1×1×c is a global pooling operation... Such projection Cpool may be suboptimal for learning representations to effectively leverage segmentation information..."
  - [section IV-A]: "The only configurations for which the results do not follow this positive trend involve the slice-based pairing strategy Ps alone... This is likely because the pooling operation removes the spatial context between nearby and hence relatively similar slices, hence preventing the learning of relevant features from spatial correspondence."
  - [corpus]: No direct evidence in corpus papers; this is specific architectural choice for segmentation not covered in related works.

### Mechanism 3
- Claim: SegCLR's performance is minimally impacted by the amount of unlabeled target data, enabling effective zero-shot domain generalization.
- Mechanism: The joint training strategy ensures that the model learns robust features from the source domain and the limited target data, without overfitting to the specific characteristics of the target domain. This allows the model to generalize well even with very little or no target data.
- Core assumption: The source domain provides sufficient information for learning generalizable features, and the contrastive loss can effectively leverage even small amounts of target data to improve these features.
- Evidence anchors:
  - [abstract]: "Notably, we discover that the segmentation performance of SegCLR framework is marginally impacted by the abundance of unlabeled data from the target domain, thereby we also propose an effective domain generalization extension of SegCLR, known also as zero-shot domain adaptation, which eliminates the need for any target domain information."
  - [section IV-D]: "Results show that the source domain performance is not affected much by the amount of unlabeled data from a different domain... Interestingly, however, regardless of the amount of unlabeled data, SegCLR performs roughly 1% in Dice on average compared to Baseline."
  - [corpus]: No direct evidence in corpus papers; this is a novel finding about data efficiency not covered in related works.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: To learn domain-invariant features without relying on labeled data, which is scarce in medical imaging across different devices and diseases.
  - Quick check question: Can you explain how SimCLR's contrastive loss works to pull positive pairs together and push negative pairs apart?

- Concept: Domain adaptation and generalization
  - Why needed here: To handle the challenge of applying deep learning models trained on one dataset (e.g., one OCT device or disease) to another dataset with different characteristics.
  - Quick check question: What is the difference between unsupervised domain adaptation and domain generalization, and why is the latter more challenging?

- Concept: Semi-supervised learning
  - Why needed here: To leverage both labeled data (for the segmentation task) and unlabeled data (for learning generalizable features) in a single training framework.
  - Quick check question: How does joint training of supervised and unsupervised losses help in semi-supervised learning, and what are the challenges in balancing these two objectives?

## Architecture Onboarding

- Component map: UNet backbone (F) -> Encoder (E) -> Contrastive projection head (C) -> Supervised loss (Lsup) and Contrastive loss (Lcon)

- Critical path:
  1. Forward pass through UNet to get segmentation predictions and encoder features
  2. Apply pair generation to create positive and negative samples
  3. Pass encoder features through contrastive projection head
  4. Compute supervised and contrastive losses
  5. Backpropagate combined loss to update model parameters

- Design tradeoffs:
  - Cch vs Cpool: Cch preserves spatial context but adds a small number of parameters; Cpool is parameter-free but may lose important spatial information for segmentation.
  - Pa vs Ps vs Ps+a: Pa uses augmentations and is more general; Ps leverages slice coherence but may be less effective alone; Ps+a combines both but adds complexity.
  - λ (supervised loss weight): Higher λ emphasizes segmentation accuracy but may reduce the benefit of contrastive learning; lower λ may lead to poor segmentation performance.

- Failure signatures:
  - Poor performance on target domain: Could indicate that the contrastive loss is not learning useful features, or that the source and target domains are too different.
  - Significant drop in source domain performance: May suggest that the contrastive loss is overwhelming the supervised loss, leading to catastrophic forgetting.
  - Instability across random seeds: Could indicate sensitivity to hyperparameters or insufficient training data.

- First 3 experiments:
  1. Implement SegCLR with Cpool and Pa on a simple dataset (e.g., MNIST) to verify that the joint training works and improves over baseline.
  2. Compare Cch vs Cpool on a medical segmentation dataset (e.g., RETOUCH) to demonstrate the importance of spatial context preservation.
  3. Evaluate SegCLR's performance with varying amounts of unlabeled target data on a cross-device OCT dataset to confirm its data efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different self-supervised learning methods (SimCLR vs SimSiam) on the SegCLR framework's performance in domain generalization?
- Basis in paper: [explicit] The paper compares SimCLR and SimSiam in domain adaptation settings but does not extensively explore their performance differences in domain generalization scenarios.
- Why unresolved: The study primarily focuses on SimCLR and does not provide a comprehensive comparison of different self-supervised learning methods in domain generalization.
- What evidence would resolve it: Conducting experiments using SimSiam in domain generalization settings and comparing the results with SimCLR would provide insights into the effectiveness of different self-supervised learning methods in this context.

### Open Question 2
- Question: How does the choice of augmentation strategies (Pa, Ps, Ps+a) affect SegCLR's performance in domain generalization?
- Basis in paper: [explicit] The paper evaluates different augmentation strategies (Pa, Ps, Ps+a) in domain adaptation settings but does not extensively explore their impact on domain generalization.
- Why unresolved: The study primarily focuses on domain adaptation and does not provide a comprehensive analysis of the impact of augmentation strategies on domain generalization.
- What evidence would resolve it: Conducting experiments using different augmentation strategies in domain generalization settings and comparing the results would provide insights into their effectiveness in this context.

### Open Question 3
- Question: What is the optimal balance between supervised and contrastive losses (λ) in SegCLR for domain generalization?
- Basis in paper: [explicit] The paper mentions that λ is set to 20 heuristically but does not explore the impact of different values of λ on domain generalization performance.
- Why unresolved: The study does not provide a comprehensive analysis of the impact of λ on domain generalization, and the chosen value is based on heuristics rather than empirical evidence.
- What evidence would resolve it: Conducting experiments with different values of λ in domain generalization settings and comparing the results would provide insights into the optimal balance between supervised and contrastive losses for this task.

## Limitations

- Domain specificity: While SegCLR shows strong performance on retinal OCT datasets, its effectiveness on other medical imaging modalities or non-medical domains remains untested.
- Hyperparameter sensitivity: The balance between supervised and contrastive losses and the choice of projection head significantly impact performance, but systematic sensitivity analysis is lacking.
- Computational overhead: Although minimal parameter increase, the joint training process may increase training time and computational requirements compared to standard supervised training.

## Confidence

**High confidence**: The core claim that joint semi-supervised and contrastive learning improves domain generalization is well-supported by extensive experiments across three clinical OCT datasets, with consistent improvements over multiple baselines.

**Medium confidence**: The assertion that SegCLR is minimally impacted by the amount of unlabeled target data is supported by experiments, but these are limited to the specific OCT datasets studied. The zero-shot domain adaptation capability would benefit from validation on more diverse domain shifts.

**Medium confidence**: The choice of Cch over Cpool for preserving spatial context is well-justified theoretically and supported by ablation studies, but the performance gap between these options may vary depending on the specific segmentation task and dataset characteristics.

## Next Checks

1. **Cross-modal validation**: Apply SegCLR to a different medical imaging modality (e.g., chest X-rays or brain MRI) to assess whether the domain generalization benefits transfer beyond retinal OCT.

2. **Hyperparameter robustness study**: Systematically vary λ and test both Cch and Cpool projections across multiple random seeds to quantify the method's sensitivity to these critical design choices.

3. **Computational efficiency benchmarking**: Measure and compare training times and memory usage between SegCLR and baseline methods across different hardware configurations to quantify the practical overhead of the joint training approach.