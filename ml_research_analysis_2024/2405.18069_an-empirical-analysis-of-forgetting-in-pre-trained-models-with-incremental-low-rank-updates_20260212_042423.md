---
ver: rpa2
title: An Empirical Analysis of Forgetting in Pre-trained Models with Incremental
  Low-Rank Updates
arxiv_id: '2405.18069'
source_url: https://arxiv.org/abs/2405.18069
tags:
- learning
- task
- lora
- forgetting
- rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how using low-rank adaptation (LoRA) affects
  forgetting when incrementally updating pretrained models across tasks. The authors
  compare continual LoRA-based finetuning with direct transfer and full fine-tuning
  baselines, measuring accuracy and forgetting on both the pretrained ImageNet task
  and new fine-grained tasks (Cars, Flowers, Aircraft, Birds).
---

# An Empirical Analysis of Forgetting in Pre-trained Models with Incremental Low-Rank Updates

## Quick Facts
- arXiv ID: 2405.18069
- Source URL: https://arxiv.org/abs/2405.18069
- Reference count: 37
- Primary result: Lower LoRA ranks significantly reduce forgetting, especially for vision transformers, with gains orthogonal to regularization methods like LwF.

## Executive Summary
This paper investigates catastrophic forgetting in pretrained vision models when incrementally updating with low-rank adaptation (LoRA) across fine-grained classification tasks. The authors systematically compare continual LoRA-based fine-tuning with direct transfer and full fine-tuning baselines, measuring both accuracy and forgetting on the original ImageNet task and new downstream tasks. They find that lower LoRA ranks substantially reduce forgetting, particularly for vision transformers, and that this effect is independent of regularization methods. The study also reveals a novel "contextual forgetting" phenomenon in ViTs where forgotten ImageNet classes are semantically related to the downstream task domain—an effect not observed in ResNets.

## Method Summary
The authors incrementally update pretrained ViT and ResNet-50 models using LoRA adapters across four fine-grained classification tasks (Cars, Flowers, Aircraft, Birds). They systematically vary LoRA rank from 1-32 for ViT and 1-7 for ResNet, comparing performance with and without LwF regularization. After each task, LoRA weights are merged into the base model. The study evaluates average accuracy and forgetting on both the pretrained ImageNet task and downstream tasks, with additional analysis of forward transfer when revisiting tasks. The "long setting" extends the study by seeing each dataset twice.

## Key Results
- Lower LoRA ranks (especially rank 1-4) significantly reduce forgetting on the pretrained ImageNet task while maintaining downstream accuracy
- ViTs exhibit "contextual forgetting" where forgotten ImageNet classes are semantically related to the downstream task domain—not seen in ResNets
- The rank's impact on forgetting is orthogonal to gains from LwF regularization
- Forward transfer improves when revisiting tasks, with higher final accuracy when more data is seen in the second pass

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lower LoRA ranks reduce forgetting by limiting the capacity of parameter updates, thereby constraining the feature drift that leads to catastrophic forgetting.
- Mechanism: LoRA learns low-rank updates by decomposing weight changes into the product of two smaller matrices. Lower rank means fewer degrees of freedom, so updates are more constrained and less likely to overwrite pretrained representations.
- Core assumption: Pretrained representations are valuable and should be preserved; high-rank LoRA updates overwrite these representations.
- Evidence anchors:
  - [abstract] "lower LoRA ranks significantly reduce forgetting"
  - [section] "the rank has an impact on forgetting that is orthogonal to the one of LwF"
- Break condition: If the downstream task requires learning features that cannot be represented within the low-rank subspace, performance will degrade.

### Mechanism 2
- Claim: Vision transformers exhibit "contextual forgetting" where forgotten ImageNet classes are semantically related to the fine-grained downstream task domain.
- Mechanism: During LoRA updates, feature drift occurs more strongly for image regions and features relevant to the new task, causing correlated classes (e.g., sports cars when learning cars) to be forgotten more.
- Core assumption: Semantic similarity in class labels correlates with shared visual features, so drift in one domain affects related classes.
- Evidence anchors:
  - [abstract] "ViTs show 'contextual forgetting' where forgotten ImageNet classes are semantically related to the downstream task domain"
  - [section] "the most forgotten Imagenet categories after learning on the Cars dataset, using ViT with a LoRA adapter of rank 32... is 'sports cars'"
- Break condition: If the model architecture or LoRA rank reduces feature drift, contextual forgetting will not be observed.

### Mechanism 3
- Claim: Forward transfer improves when revisiting tasks, especially when more data is seen in the second pass, indicating that incremental LoRA updates can accumulate knowledge over time.
- Mechanism: Initial exposure to a task partially adapts the feature extractor; later exposure with more data further refines it, leading to higher accuracy on previously seen data.
- Core assumption: Knowledge gained in early tasks can be retained and built upon in later exposures.
- Evidence anchors:
  - [abstract] "Forward transfer improves when revisiting tasks, with higher final accuracy when more data is seen in the second pass"
  - [section] "the accuracy reached the second time the task is encountered and compare it to the accuracy reached when the task is encountered the first time"
- Break condition: If LoRA rank is too high, forgetting will dominate and forward transfer will be minimal.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why pretrained models lose performance on earlier tasks during incremental learning is central to interpreting the forgetting results.
  - Quick check question: What is the primary cause of catastrophic forgetting when training neural networks sequentially on different tasks?

- Concept: Low-rank adaptation (LoRA) and its parameter-efficient fine-tuning
  - Why needed here: The paper's core technique for incremental updates relies on LoRA; understanding how it works is essential for interpreting the rank-accuracy relationship.
  - Quick check question: How does LoRA achieve parameter efficiency while still allowing for task-specific adaptation?

- Concept: Task-incremental vs class-incremental learning
  - Why needed here: The paper distinguishes between these settings when evaluating forgetting and forward transfer; knowing the difference is key to interpreting results.
  - Quick check question: What is the main difference between task-incremental learning and class-incremental learning?

## Architecture Onboarding

- Component map:
  - Pretrained backbone (ViT or ResNet)
  - LoRA adapter per task (rank configurable)
  - Merging step: LoRA weights added to backbone weights after task completion
  - Evaluation: Task-specific accuracy and ImageNet forgetting

- Critical path:
  1. Load pretrained model
  2. Initialize LoRA adapter for task
  3. Fine-tune with LoRA
  4. Merge adapter into backbone
  5. Evaluate on all seen tasks

- Design tradeoffs:
  - Higher LoRA rank → more plasticity, less forgetting of downstream task, more forgetting of pretrained task
  - Lower LoRA rank → more stability, less forgetting of pretrained task, potentially less plasticity
  - ViT vs ResNet: ViTs show contextual forgetting; ResNets have higher forgetting overall

- Failure signatures:
  - Accuracy collapse on pretrained task indicates excessive forgetting
  - Very low accuracy on new task suggests rank too low for plasticity
  - Contextual forgetting manifests as semantic clustering in forgotten classes (ViT only)

- First 3 experiments:
  1. Compare direct transfer vs continual LoRA updates on Cars task (measure forgetting)
  2. Sweep LoRA rank from 1 to 32 on ViT, measure accuracy and forgetting on all tasks
  3. Apply LwF on top of LoRA, compare forgetting to LoRA-only baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of which layers to apply LoRA to (e.g., all attention matrices in ViT vs. last conv layer in ResNet) affect catastrophic forgetting across architectures?
- Basis in paper: [explicit] The authors note they chose to adapt only the last convolutional layer of each ResNet block to match the number of modifiable parameters in ViT, but acknowledge this choice is non-obvious and could affect results.
- Why unresolved: The paper focuses on comparing architectures with this specific layer choice but doesn't explore how different layer selections within each architecture would change forgetting patterns.
- What evidence would resolve it: Systematic experiments varying which layers LoRA is applied to within each architecture (e.g., adapting all conv layers in ResNet vs. specific attention matrices in ViT) while measuring forgetting and performance.

### Open Question 2
- Question: Why does ViT exhibit contextual forgetting (forgetting semantically related ImageNet classes) while ResNet does not, despite similar overall forgetting rates?
- Basis in paper: [explicit] The authors observe that ViT shows contextual forgetting for fine-grained downstream tasks, but ResNet-50 does not show this behavior in their analysis.
- Why unresolved: The paper hypothesizes this may be due to greater feature drift in ViT for related classes, but doesn't provide a mechanistic explanation for why this architecture-specific phenomenon occurs.
- What evidence would resolve it: Detailed analysis of feature space changes during training, including visualization of class representations and their evolution across tasks, to identify architectural differences in how feature drift manifests.

### Open Question 3
- Question: Would using per-class or per-domain LoRA adapters (rather than per-task) further reduce forgetting while maintaining plasticity?
- Basis in paper: [inferred] The authors mention this as a possibility in their discussion, noting that having one LoRA per class could allow specialization to different domains, but leave this for future work.
- Why unresolved: The paper only considers one LoRA adapter per task, so the potential benefits of more granular adapter allocation remain untested.
- What evidence would resolve it: Experiments comparing per-task LoRA with per-class or per-domain LoRA strategies across the same task sequences, measuring both forgetting and forward transfer capabilities.

### Open Question 4
- Question: How does the interaction between LoRA rank and regularization methods like LwF affect the stability-plasticity tradeoff across different architectures?
- Basis in paper: [explicit] The authors show that LoRA rank affects forgetting orthogonally to LwF gains, but don't systematically explore how different rank-regularization combinations perform across architectures.
- Why unresolved: While the paper demonstrates both factors independently affect forgetting, it doesn't explore the full combinatorial space of rank-regularization pairs across ViT and ResNet.
- What evidence would resolve it: Comprehensive experiments varying both LoRA rank and regularization strength across architectures, measuring final accuracy and forgetting to identify optimal combinations for each architecture type.

## Limitations

- The study focuses on a single pretrained backbone (ViT and ResNet-50) and a specific set of fine-grained classification tasks, limiting generalizability to other architectures and domains.
- The analysis of semantic clustering in forgotten ImageNet classes relies on manual inspection of a small set of examples rather than systematic quantitative analysis.
- The forward transfer results are based on a limited "long setting" with two passes per task, which may not represent realistic continual learning scenarios.

## Confidence

- **High confidence**: The core finding that lower LoRA ranks reduce forgetting is well-supported by systematic rank sweeps across multiple tasks and architectures.
- **Medium confidence**: The contextual forgetting phenomenon in ViTs is observed consistently, but the semantic clustering analysis relies on qualitative inspection.
- **Medium confidence**: The forward transfer results show promising trends, but the long setting has limited sample sizes and may not represent realistic continual learning scenarios.

## Next Checks

1. **Architecture Generalization**: Test the rank-forgetting relationship on additional backbone architectures (ConvNeXt, Swin) to verify if contextual forgetting is unique to ViTs or a broader phenomenon.

2. **Semantic Clustering Quantification**: Develop a quantitative metric for semantic similarity between forgotten ImageNet classes and downstream tasks, and test whether the observed clustering holds across all task pairs and rank values.

3. **Real-World Continual Learning**: Design a longer sequence of diverse tasks (beyond fine-grained classification) and evaluate whether the rank-accuracy relationship and forward transfer benefits persist in more realistic, non-i.i.d. data streams.