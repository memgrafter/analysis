---
ver: rpa2
title: 'LLaMA-MoE v2: Exploring Sparsity of LLaMA from Perspective of Mixture-of-Experts
  with Post-Training'
arxiv_id: '2411.15708'
source_url: https://arxiv.org/abs/2411.15708
tags:
- experts
- arxiv
- attention
- performance
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores sparsity in the LLaMA model by converting both
  the attention and MLP modules into Mixture-of-Experts (MoE) components. It investigates
  different expert construction strategies and granularities for both modules, and
  applies this sparsification to instructed LLMs to build instructed MoE models.
---

# LLaMA-MoE v2: Exploring Sparsity of LLaMA from Perspective of Mixture-of-Experts with Post-Training

## Quick Facts
- arXiv ID: 2411.15708
- Source URL: https://arxiv.org/abs/2411.15708
- Reference count: 31
- Key outcome: Achieves competitive performance with 50% fewer activated parameters by converting LLaMA attention and MLP modules to MoE with two-stage post-training

## Executive Summary
This paper explores sparsity in the LLaMA model by converting both attention and MLP modules into Mixture-of-Experts (MoE) components. It investigates different expert construction strategies and granularities for both modules, and applies this sparsification to instructed LLMs to build instructed MoE models. To recover performance loss from increased sparsity, a two-stage post-training strategy is designed. Experiments on LLaMA3-8B demonstrate the effectiveness of this approach, achieving competitive performance with 50% fewer activated parameters compared to the dense model, while showing favorable scaling properties with training data size.

## Method Summary
The method converts dense LLaMA models to MoE by constructing experts for both attention (via head grouping) and MLP (via neuron partitioning with shared expert). The router networks are initialized using clustering methods. A two-stage post-training strategy is then applied: first stage focuses on general abilities like conversation and email writing, while the second stage specializes in math and coding tasks. The model is trained with load balancing loss alongside cross-entropy loss to maintain balanced expert utilization.

## Key Results
- Achieves competitive performance with 50% fewer activated parameters compared to dense LLaMA3-8B
- Demonstrates favorable scaling properties with training data size
- Shows effectiveness of two-stage post-training strategy in recovering performance degradation from increased sparsity
- Validates the approach across multiple benchmarks including BoolQ, SciQ, PIQA, ARC-C, TruthfulQA, HellaSwag, MMLU, GSM8K, HumanEval, and IFEval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting both attention and MLP modules into MoE components reduces the number of activated parameters while maintaining model performance through selective expert routing.
- Mechanism: The MoE framework allows the model to activate only a subset of experts for each input token, keeping the computational cost constant while scaling up the total parameter count. This is achieved through a router network that assigns importance scores to each expert and activates the top-k experts based on the input.
- Core assumption: The router network can effectively learn to route tokens to the most relevant experts, ensuring that the activated experts contain the necessary knowledge for the given task.
- Evidence anchors:
  - [abstract]: "It contains multiple experts but only activates a small part of experts, thus effectively scaling the model parameter while keeping the activation constant."
  - [section]: "To calculate it, we multiply the fraction of tokens fi routed to a specific expert Ei by the total routing probability Pi assigned to Ei for a single batch, and then sum this across all experts NE."
  - [corpus]: "Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs)."
- Break condition: If the router network fails to learn effective routing patterns, it may consistently choose only a limited number of experts, leading to load imbalance and reduced overall performance.

### Mechanism 2
- Claim: The two-stage post-training strategy effectively recovers the performance degradation caused by increased sparsity through targeted fine-tuning on task-specific datasets.
- Mechanism: The first stage of post-training focuses on general abilities such as conversation and email writing, while the second stage specializes the model in math and coding tasks. This approach ensures that the model maintains its broader, more general skills while becoming more proficient in specific domains.
- Core assumption: The two-stage post-training process can effectively balance the model's general and specialized capabilities, preventing the loss of general skills during specialization.
- Evidence anchors:
  - [abstract]: "To counteract the performance degradation resulting from increased sparsity, we design a two-stage post-training strategy to enhance model performance."
  - [section]: "Specifically, in the first stage, we train the constructed MoE models with general abilities such as conversation and email writing. Subsequently, in the second stage, we equip the model with math and coding abilities."
  - [corpus]: "We propose a two-stage instruction tuning process to recover the performance of our partitioned MoE models."
- Break condition: If the balance between general and specialized training data is not properly maintained, the model may either lose its general capabilities or fail to specialize effectively in the target domains.

### Mechanism 3
- Claim: The residual MLP MoE construction method improves performance by extracting common knowledge as a shared expert while allowing the remaining parts to activate selectively.
- Mechanism: By identifying and extracting the most-shared neurons to form a shared expert, the residual MLP MoE ensures that common knowledge is always available. The remaining neurons are then partitioned into routed experts, allowing for selective activation based on the input.
- Core assumption: The shared expert captures the most essential and widely applicable knowledge, while the routed experts can handle more specific or nuanced aspects of the input.
- Evidence anchors:
  - [abstract]: "Considering there is shared knowledge between different downstream tasks, in this work, we also explore the residual version of MLP MoE... which extracts the common knowledge from the MLP layers to serve as shared experts."
  - [section]: "Following Zhu et al. (2024b), we retain NE functionally similar neuron sets to form the routed experts ER1, . . . ,ERNE, and set aside the most-shared neurons to form an always-activated shared expert ES."
  - [corpus]: "Deepseek-MoE (DeepSeek-AI, 2024) introduces shared experts which are dedicated to capturing and consolidating common knowledge across varying contexts."
- Break condition: If the partitioning of neurons into shared and routed experts is not optimal, it may lead to either underutilization of the shared expert or excessive reliance on the routed experts, affecting overall performance.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Understanding the MoE framework is crucial for comprehending how the model scales parameters while keeping computational costs constant.
  - Quick check question: How does the MoE architecture differ from a traditional dense model in terms of parameter activation?

- Concept: Router network and expert routing
  - Why needed here: The router network is responsible for selecting the most relevant experts for each input token, which is essential for the effectiveness of the MoE model.
  - Quick check question: What is the role of the router network in an MoE model, and how does it determine which experts to activate?

- Concept: Load balancing in MoE models
  - Why needed here: Load balancing ensures that all experts are utilized effectively, preventing some experts from being overused while others remain underutilized.
  - Quick check question: Why is load balancing important in MoE models, and how can it be achieved?

## Architecture Onboarding

- Component map:
  Dense LLaMA model -> Attention MoE conversion -> MLP MoE conversion -> Router network initialization -> Two-stage post-training pipeline -> Instruction tuning datasets

- Critical path:
  1. Convert dense LLaMA model to MoE by constructing experts for attention and MLP modules.
  2. Initialize router networks using clustering methods.
  3. Apply two-stage post-training to recover performance degradation.
  4. Evaluate model performance on various benchmarks.

- Design tradeoffs:
  - Parameter efficiency vs. model capacity: Increasing the number of experts improves model capacity but may lead to load imbalance.
  - General vs. specialized capabilities: Balancing general and specialized training data is crucial for maintaining overall model performance.

- Failure signatures:
  - Load imbalance: Some experts are consistently chosen while others remain underutilized.
  - Performance degradation: The MoE model performs worse than the original dense model despite having more parameters.
  - Overfitting: The model becomes too specialized and loses its general capabilities.

- First 3 experiments:
  1. Evaluate the impact of different numbers of activated experts on model performance.
  2. Compare the performance of standard MLP MoE and residual MLP MoE.
  3. Assess the effectiveness of the two-stage post-training strategy in recovering performance degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Attention MoE vary when using different expert construction strategies beyond the grouped query attention approach?
- Basis in paper: [explicit] The paper mentions that attention heads can be grouped into experts, but it primarily focuses on grouped query attention. It suggests that different expert construction strategies could be explored.
- Why unresolved: The paper only investigates one specific method of grouping attention heads into experts. It does not explore alternative strategies, such as clustering based on attention patterns or other methods of expert construction.
- What evidence would resolve it: Experiments comparing the performance of Attention MoE models using different expert construction strategies, such as clustering based on attention patterns or other methods, would provide insights into the optimal approach for expert construction in attention modules.

### Open Question 2
- Question: What is the impact of expert granularity on the performance of MLP MoE models when using different types of expert construction methods, such as uniform splitting versus residual MoE?
- Basis in paper: [explicit] The paper discusses two types of MLP MoE models: standard MoE (uniform splitting) and residual MoE. It mentions that different expert construction methods present diverse characteristics but does not explore the impact of expert granularity on each method.
- Why unresolved: The paper only compares the performance of MLP MoE models with different numbers of activated experts but does not investigate how expert granularity affects the performance of different expert construction methods.
- What evidence would resolve it: Experiments varying the granularity of experts in both standard MoE and residual MoE models and comparing their performance would provide insights into the optimal granularity for each expert construction method.

### Open Question 3
- Question: How does the scaling behavior of instructed MoE models differ when trained on datasets with varying compositions of task-specific data, such as conversation, math, and code?
- Basis in paper: [inferred] The paper mentions that the two-stage post-training strategy uses datasets with different compositions of task-specific data. It suggests that the model's capabilities across various aspects are improved, but it does not explore how the scaling behavior changes with different dataset compositions.
- Why unresolved: The paper only presents results for a specific dataset composition in the two-stage post-training. It does not investigate how the scaling behavior of instructed MoE models changes when trained on datasets with different proportions of task-specific data.
- What evidence would resolve it: Experiments training instructed MoE models on datasets with varying compositions of task-specific data and evaluating their performance on different benchmarks would provide insights into how the scaling behavior is affected by the dataset composition.

## Limitations

- The specific clustering method used for router initialization in MLP MoE remains unspecified, which is critical for reproducing the results
- The exact dataset composition and filtering criteria for the instruction tuning datasets are unclear, particularly the split between general and specialized training stages
- The paper does not provide detailed analysis of activation efficiency across different input types or tasks

## Confidence

- **High Confidence**: The fundamental MoE architecture principles and the two-stage post-training approach are well-established in the literature.
- **Medium Confidence**: The claim that residual MLP MoE with shared experts improves performance over standard MLP MoE is supported by conceptual reasoning but lacks extensive empirical validation.
- **Low Confidence**: The exact performance metrics and comparative analysis against baseline models are difficult to verify without access to the complete experimental setup and implementation details.

## Next Checks

1. **Router Initialization Validation**: Implement and test multiple router initialization strategies (including the unspecified clustering method) to verify their impact on expert utilization and overall model performance. Compare expert activation patterns and load balancing metrics across different initialization approaches.

2. **Ablation Study on Expert Construction**: Conduct a systematic ablation study comparing different numbers of attention head groups and MLP neuron partitions, measuring the trade-off between parameter efficiency and task performance. This should include testing the residual MLP MoE against standard MLP MoE configurations.

3. **Two-Stage Post-Training Analysis**: Perform controlled experiments to isolate the contribution of each post-training stage by testing models trained with only stage 1, only stage 2, and both stages. Measure the retention of general capabilities while assessing the acquisition of specialized skills in math and coding.