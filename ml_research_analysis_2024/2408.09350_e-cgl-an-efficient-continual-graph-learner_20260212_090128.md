---
ver: rpa2
title: 'E-CGL: An Efficient Continual Graph Learner'
arxiv_id: '2408.09350'
source_url: https://arxiv.org/abs/2408.09350
tags:
- graph
- tasks
- learning
- continual
- e-cgl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the problem of continual graph learning, where
  graph data evolves over time, introducing new patterns and requiring models to adapt
  while retaining prior knowledge. The authors address two key challenges: the interdependencies
  between different graph data and the efficiency concerns when dealing with large
  graphs.'
---

# E-CGL: An Efficient Continual Graph Learner

## Quick Facts
- arXiv ID: 2408.09350
- Source URL: https://arxiv.org/abs/2408.09350
- Authors: Jianhao Guo; Zixuan Ni; Yun Zhu; Siliang Tang
- Reference count: 40
- Key outcome: Proposes an efficient continual graph learner that reduces catastrophic forgetting to -1.1% while providing 15.83x training speedup and 4.89x inference speedup

## Executive Summary
This paper addresses continual graph learning where graph data evolves over time, introducing new patterns and requiring models to adapt while retaining prior knowledge. The authors tackle two key challenges: interdependencies between different graph data and efficiency concerns when dealing with large graphs. They propose E-CGL, which combines a replay strategy with combined sampling (importance + diversity) and an efficient MLP encoder that shares weights with a GCN. The method comprehensively outperforms nine baselines on four datasets while significantly reducing catastrophic forgetting and improving efficiency.

## Method Summary
E-CGL employs a replay strategy with combined sampling that considers both node importance (using an attributed-PageRank algorithm) and diversity (measuring differences between nodes and their neighbors). During training, an MLP model shares weights with a GCN, circumventing the computationally expensive message passing process while maintaining performance. The trained MLP weights are then transferred to initialize a GCN for inference, where message passing is added back. The method uses a memory buffer to store important and diverse nodes from previous tasks, learning from both new data and replayed data using cross-entropy loss.

## Key Results
- Achieves an average of -1.1% catastrophic forgetting across four datasets
- Provides 15.83× training time acceleration and 4.89× inference time acceleration
- Comprehensively surpasses nine baselines on CoraFull, OGBN-Arxiv, Reddit, and OGBN-Products datasets
- Maintains strong performance under both task-incremental and class-incremental settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The replay strategy with combined sampling effectively addresses interdependencies by capturing both topology and attribute information.
- Mechanism: Importance sampling uses attributed-PageRank to rank nodes based on topological connectivity and attribute similarity. Diversity sampling identifies nodes differing significantly from neighbors in feature space. Combined sampling ensures both important and diverse nodes are replayed, maintaining performance on previous tasks while adapting to new ones.
- Core assumption: Nodes with similar attributes share similar labels and importance (homophilous assumption), and diverse nodes are vulnerable to new patterns.
- Break condition: If homophilous assumption doesn't hold or RBF kernel similarity fails to capture meaningful relationships.

### Mechanism 2
- Claim: The efficient MLP encoder achieves training acceleration by circumventing message passing while maintaining comparable performance.
- Mechanism: Uses MLP instead of GCN during training by removing message passing. Since MLP and GCN weight spaces are identical when dimensions match, trained MLP weights transfer to initialize GCN for inference.
- Core assumption: Message passing's effect mainly comes from generalization ability in inference rather than representation ability in training.
- Break condition: If training performance significantly degrades without message passing or weight space equivalence doesn't hold for complex GNNs.

### Mechanism 3
- Claim: Combined replay and efficient training strategy significantly reduces catastrophic forgetting while improving efficiency.
- Mechanism: Replay buffer stores important and diverse nodes from previous tasks. Model learns from new data and replayed data using cross-entropy loss. Efficient MLP training speeds up process while GCN inference leverages full graph structure.
- Core assumption: Maintaining memory buffer of representative nodes effectively prevents catastrophic forgetting when combined with proper training strategies.
- Break condition: If memory buffer becomes too large to maintain efficiently or replayed data becomes outdated.

## Foundational Learning

- Concept: Continual Learning and Catastrophic Forgetting
  - Why needed here: Addresses catastrophic forgetting in graph data where models learn from sequential data while preserving previous knowledge.
  - Quick check question: What is the difference between task-incremental and class-incremental settings in continual learning?

- Concept: Graph Neural Networks and Message Passing
  - Why needed here: Leverages GNN concepts and modifies message passing mechanism to improve efficiency.
  - Quick check question: How does the message passing mechanism in GCNs work, and why is it computationally expensive?

- Concept: PageRank and Random Walks on Graphs
  - Why needed here: Importance sampling strategy based on PageRank, adapted to include node attributes.
  - Quick check question: What is the core idea behind PageRank, and how can it be adapted for node importance ranking in graphs with attributes?

## Architecture Onboarding

- Component map:
  Memory Buffer -> Graph Dependent Replay Module -> Efficient Graph Learner Module -> Loss Function

- Critical path:
  1. Receive new graph task data
  2. Sample important and diverse nodes from memory buffer
  3. Train MLP model on new data and replayed data
  4. Update memory buffer with new important and diverse nodes
  5. Transfer trained weights to GCN for inference on test data

- Design tradeoffs:
  - Memory vs. Performance: Larger memory buffer improves performance but increases storage cost
  - Training Speed vs. Accuracy: Using MLP for training speeds up training but may slightly reduce accuracy compared to full GCN training
  - Sampling Complexity vs. Effectiveness: More sophisticated sampling strategies may improve performance but increase computational overhead

- Failure signatures:
  - Catastrophic forgetting: Significant performance drop on previous tasks
  - Training inefficiency: Model takes too long to train or doesn't converge
  - Memory overflow: Memory buffer becomes too large to manage effectively

- First 3 experiments:
  1. Verify MLP-GCN weight space equivalence by training both models with same parameters and comparing performance
  2. Test importance sampling strategy by ranking nodes and checking if top-ranked nodes are indeed important for classification
  3. Evaluate replay buffer effectiveness by comparing performance with and without replay on a small dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the interdependence between graph tasks affect catastrophic forgetting in continual graph learning, and can this be quantified or predicted?
- Basis in paper: [explicit] The paper highlights the challenge of interdependencies between different graph data, where previous graphs can influence new data distributions. It mentions this adds difficulty for continual graph learning since previous data will not be fully available.
- Why unresolved: While the paper acknowledges the problem and proposes a replay-based strategy, it does not provide a quantitative analysis of how interdependence specifically contributes to catastrophic forgetting or how this effect can be predicted or measured.
- What evidence would resolve it: Experiments measuring the correlation between the strength of interdependencies (e.g., number of inter-task edges) and the degree of catastrophic forgetting across various datasets. Developing a predictive model that estimates the impact of interdependencies on forgetting based on graph characteristics.

### Open Question 2
- Question: What is the optimal balance between node importance and diversity in the replay buffer sampling strategy, and does this balance vary depending on the dataset characteristics?
- Basis in paper: [explicit] The paper introduces a combined sampling strategy considering both node importance (using an attributed-PageRank algorithm) and diversity (measuring differences between a node and its neighbors). However, it does not determine the optimal ratio or investigate how this ratio should be adjusted for different datasets.
- Why unresolved: The paper mentions using a fixed diversity ratio of 0.25 (25% diversity sampling) but does not explore whether this is optimal or how it should be tuned. The impact of varying this ratio on performance across different graph datasets remains unexplored.
- What evidence would resolve it: Systematic experiments varying the diversity ratio across multiple datasets with different characteristics (e.g., homophily, size, class distribution) to identify patterns in optimal sampling ratios. Analysis showing how dataset properties correlate with the effectiveness of importance vs. diversity sampling.

### Open Question 3
- Question: Can the efficiency gains from the MLP-based training approach be maintained or improved while incorporating more sophisticated message-passing mechanisms?
- Basis in paper: [explicit] The paper proposes an Efficient Graph Learner that uses an MLP during training (avoiding message passing) and transfers weights to a GCN for inference. While this provides significant speedups, it acknowledges a minor performance degradation compared to pure GCN methods.
- Why unresolved: The paper demonstrates that the MLP approach achieves "comparable or even better results" than GNNs in some cases, but does not explore whether more sophisticated lightweight message-passing mechanisms could provide better performance while maintaining efficiency. The trade-off between efficiency and performance with different levels of message-passing integration remains unexplored.
- What evidence would resolve it: Comparative experiments testing variations of the efficient learner that incorporate different levels of message-passing (e.g., sparse message passing, adaptive message passing only for critical nodes) to find the sweet spot between efficiency and performance. Analysis of how different types of graph structures benefit from varying levels of message-passing during training.

## Limitations

- The paper's claims about MLP-GCN weight space equivalence lack rigorous theoretical justification and direct empirical validation
- The effectiveness of the combined sampling strategy relies heavily on the homophilous assumption, which may not hold for all graph datasets
- Long-term scalability with respect to memory usage and performance degradation over many tasks is not thoroughly explored

## Confidence

- **High Confidence:** Overall framework design and experimental results demonstrating performance improvements and efficiency gains
- **Medium Confidence:** Specific mechanisms of MLP-GCN weight sharing and attributed-PageRank based importance sampling require additional validation
- **Low Confidence:** Long-term scalability of the approach with respect to memory usage and performance across many tasks

## Next Checks

1. Conduct ablation studies to isolate the individual contributions of the MLP-GCN weight sharing mechanism versus the replay strategy on overall performance
2. Test the method on datasets with known heterophily to evaluate the robustness of the homophilous assumption underlying the importance sampling strategy
3. Perform scalability analysis by evaluating the approach on datasets with varying numbers of tasks and memory buffer sizes to understand the trade-offs between performance and computational/memory costs