---
ver: rpa2
title: Defending Against Diverse Attacks in Federated Learning Through Consensus-Based
  Bi-Level Optimization
arxiv_id: '2412.02535'
source_url: https://arxiv.org/abs/2412.02535
tags:
- agents
- good
- learning
- attacks
- malicious
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of defending against adversarial
  attacks, particularly label-flipping attacks, in federated learning (FL) systems.
  The authors propose a novel approach that combines consensus-based bi-level optimization
  (CB2O) with the clustered federated learning paradigm to create FedCB2O, a robust
  algorithm for decentralized FL settings.
---

# Defending Against Diverse Attacks in Federated Learning Through Consensus-Based Bi-Level Optimization

## Quick Facts
- arXiv ID: 2412.02535
- Source URL: https://arxiv.org/abs/2412.02535
- Reference count: 40
- Key outcome: Achieves 82.79% overall accuracy, 55.73% source class accuracy, and 38.73% attack success rate on rotated EMNIST with label-flipping attacks

## Executive Summary
This paper addresses the critical challenge of defending federated learning systems against adversarial attacks, particularly label-flipping attacks, through a novel approach called FedCB2O. The method combines consensus-based bi-level optimization with clustered federated learning to create a robust framework that can filter out malicious agents while maintaining high accuracy. The algorithm works by formulating robust FL as a bi-level optimization problem where the lower-level objective is the standard FL loss and the upper-level objective serves as a robustness criterion to identify and mitigate the influence of poisoned models.

## Method Summary
FedCB2O implements a clustered federated learning framework where agents perform local updates and then engage in local aggregation to compute consensus points. Each agent maintains historical performance records of other participants and uses probabilistic sampling to select models for aggregation based on recent performance combined with exponential moving average. The consensus point is computed as a weighted average of models that belong to a sub-level set defined by the lower-level loss function, with weights determined by a robustness criterion that helps filter out poisoned models. The algorithm uses a fixed model download budget and temperature parameter to balance exploration and exploitation during agent selection.

## Key Results
- FedCB2O achieves 82.79% overall accuracy on rotated EMNIST dataset
- Source class accuracy of 55.73% compared to 40% for baseline FedCBO
- Attack success rate of only 38.73% versus 55% for baseline methods
- Demonstrates convergence in the presence of malicious agents with theoretical guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FedCB2O uses consensus-based bi-level optimization to defend against label-flipping attacks by combining sub-level set filtering with robustness criterion weighting.
- Mechanism: The algorithm computes a consensus point as a weighted average of models that have small loss (filtering out bad models) and small robustness criterion values (filtering out poisoned models), allowing benign agents to converge to robust models despite malicious agents.
- Core assumption: The sub-level set defined by the lower-level loss function effectively excludes most poisoned models, and the upper-level robustness criterion successfully differentiates benign from malicious models with similar lower-level losses.
- Evidence anchors:
  - [abstract]: "The key idea is to formulate the robust FL problem as a bi-level optimization problem, where the lower-level objective is the standard FL loss function and the upper-level objective serves as a robustness criterion to filter out malicious agents."
  - [section]: "The consensus point mG,L1α,β(ρN_t) is now computed as a weighted (w.r.t. the robustness criterion G) average of those particles from both {θ1,i1_t}N1i1=1 and {θ2,i2_t}N2i2=1 that belong to the sub-level set QL1β[ρN_t]..."
  - [corpus]: Weak evidence - the corpus papers discuss blockchain-based defenses and sparsification but don't directly address the specific consensus-based bi-level optimization mechanism.
- Break condition: If malicious agents can create poisoned models with small values for both the lower-level loss and the robustness criterion, the filtering mechanism fails.

### Mechanism 2
- Claim: The theoretical analysis proves global convergence of benign agents in mean-field law despite the presence of malicious agents by carefully choosing hyperparameters β and α.
- Mechanism: The convergence proof establishes that with appropriate scaling of β (proportional to the fraction of benign agents) and increased α (to counteract malicious influence), benign agents' density converges to the target robust model.
- Core assumption: The mean-field approximation accurately describes the finite particle system behavior when the number of agents is large enough, and the hyperparameter adjustments effectively mitigate malicious influence.
- Evidence anchors:
  - [abstract]: "Theoretically, the authors prove the global convergence of the CB2O dynamics in mean-field law in the presence of malicious agents, demonstrating the robustness of the method against various attacks."
  - [section]: "Theorem 2.2 (Convergence of the mean-field CB2O dynamics (1.7) and (1.8) in the presence of attacks)...if ρm ∈ C([0,T∗],P4(Rd)) and if ρb ∈ C([0,T∗],P4(Rd)) is a weak solution to the Fokker-Planck Equation (1.8a)...it holds W22(ρbT,δθ∗good)/2 = ε..."
  - [corpus]: Missing evidence - corpus papers don't discuss mean-field convergence analysis for adversarial settings.
- Break condition: If the mean-field approximation breaks down due to insufficient agents or if malicious agents can manipulate the system beyond the scope of the theoretical analysis.

### Mechanism 3
- Claim: The practical algorithm incorporates agent selection and weighted averaging to handle communication constraints while maintaining robustness.
- Mechanism: Agents maintain historical performance records (P^j_n) and use probabilistic sampling to select models for aggregation, updating the sampling likelihood based on recent performance combined with exponential moving average.
- Core assumption: Historical performance records provide useful information for future model selection, and the exponential moving average effectively balances recent and past performance.
- Evidence anchors:
  - [abstract]: "Building upon ideas from [12], where FedCBO is proposed for the attack-free DCFL problem, we extend the CB2O dynamics to the clustered federated learning setting and propose a novel, robustified DCFL framework, which we call FedCB2O."
  - [section]: "To transform the interacting multi-particle system (3.10) into an algorithm that is practicable in real-world FL problems, a series of adjustments are required...agent j maintains a historical record of other participants in the system from whom it has downloaded models in previous communication rounds."
  - [corpus]: Weak evidence - corpus papers discuss client selection in federated learning but don't specifically address the historical record-based approach used in FedCB2O.
- Break condition: If the historical performance records become outdated or if malicious agents can manipulate the sampling likelihood through coordinated behavior.

## Foundational Learning

- Concept: Bi-level optimization - an optimization problem where the constraints themselves are defined by another optimization problem.
  - Why needed here: The framework models robust federated learning as finding parameters that minimize the main loss function while also minimizing a robustness criterion, creating a two-level hierarchical optimization problem.
  - Quick check question: What distinguishes a bi-level optimization problem from a standard constrained optimization problem?

- Concept: Consensus-based optimization - a class of metaheuristic optimization methods that use interacting particle systems to find global optima.
  - Why needed here: The FedCB2O algorithm extends consensus-based optimization to the federated learning setting by using interacting particles (agents) that exchange information to converge to robust solutions.
  - Quick check question: How does consensus-based optimization differ from gradient-based optimization methods?

- Concept: Mean-field limit - the behavior of a system of interacting particles as the number of particles approaches infinity.
  - Why needed here: The theoretical analysis uses mean-field theory to prove convergence properties of the algorithm when the number of agents is large.
  - Quick check question: What mathematical tools are typically used to analyze the mean-field limit of particle systems?

## Architecture Onboarding

- Component map: LocalUpdate -> LocalAggregation -> ProbSampling -> ConsensusPoint -> SamplingLikelihood
- Critical path:
  1. Initialize models and sampling likelihoods
  2. For each communication round:
     - All agents perform LocalUpdate
     - Each agent performs LocalAggregation:
       - Select M agents using ProbSampling
       - Download and evaluate models
       - Update sampling likelihood
       - Compute consensus point
       - Update local model
  3. Return final models
- Design tradeoffs:
  - Model download budget M vs. convergence speed: Larger M provides better information but increases communication cost
  - Temperature parameter κ vs. exploration/exploitation: Higher κ emphasizes recent performance but may reduce exploration
  - Moving average parameter ζ vs. responsiveness: Higher ζ makes the algorithm more responsive to recent changes but less stable
- Failure signatures:
  - Slow convergence: May indicate poor hyperparameter choices or insufficient model download budget
  - High attack success rate: Suggests the robustness criterion G is not effective against the specific attack
  - Degraded overall accuracy: Could indicate the algorithm is over-filtering useful information from malicious agents
- First 3 experiments:
  1. Run FedCB2O with no malicious agents to verify it matches FedCBO performance
  2. Run with a single malicious agent performing label-flipping to test basic robustness
  3. Run with multiple malicious agents in both clusters to test clustering capability under attack

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FedCB2O perform against label-flipping attacks when the source class is not explicitly targeted, but rather a random subset of classes are flipped?
- Basis in paper: [inferred] The paper demonstrates FedCB2O's effectiveness against targeted label-flipping attacks on a specific source class ("O" relabeled as "0"). The robustness criterion G is designed to detect and mitigate such targeted attacks. However, the paper does not explore scenarios where multiple or random classes are targeted, which could potentially weaken the effectiveness of the current robustness criterion.
- Why unresolved: The paper focuses on a specific type of label-flipping attack, leaving the question of FedCB2O's robustness against more generalized or random label-flipping attacks unanswered.
- What evidence would resolve it: Experiments comparing FedCB2O's performance against various label-flipping attack strategies, including random class flipping, multi-class flipping, and adaptive flipping strategies, would provide insights into the algorithm's robustness in more diverse attack scenarios.

### Open Question 2
- Question: Can the FedCB2O algorithm be extended to handle heterogeneous data distributions across agents within the same cluster, where each agent has a slightly different lower-level objective function?
- Basis in paper: [explicit] The paper acknowledges that in practical FL settings, each agent's lower-level objective function may be a slight perturbation of the underlying "true" loss function for their cluster. However, the theoretical analysis and algorithm are presented under the assumption that all agents within a cluster share the same lower-level objective function. The paper mentions leaving the mathematical modeling and analysis of this more realistic scenario for future work.
- Why unresolved: The current FedCB2O framework assumes homogeneous data within clusters, which is a simplification of real-world FL scenarios where data heterogeneity is prevalent.
- What evidence would resolve it: Developing a theoretical framework and implementing an algorithm that can handle heterogeneous data distributions within clusters, along with empirical validation on datasets with varying degrees of data heterogeneity, would address this question.

### Open Question 3
- Question: What is the impact of the hyperparameter TG, which determines when to activate the robustness criterion G, on the convergence speed and final accuracy of the FedCB2O algorithm?
- Basis in paper: [explicit] The paper introduces the hyperparameter TG and demonstrates its effect on the overall accuracy and source class accuracy of the FedCB2O algorithm. However, it does not provide a detailed analysis of how TG affects the convergence speed or explore the optimal range of values for TG in different scenarios.
- Why unresolved: While the paper shows that TG can be used to balance the use of information from malicious agents and defense against attacks, the relationship between TG and the algorithm's convergence behavior is not thoroughly investigated.
- What evidence would resolve it: Conducting experiments to analyze the convergence speed of FedCB2O for different values of TG, as well as exploring the optimal range of TG values for various dataset characteristics and attack scenarios, would provide insights into the hyperparameter's impact on the algorithm's performance.

## Limitations
- Theoretical analysis relies on mean-field approximations that may not hold in practical settings with finite agents
- Robustness criterion G is problem-specific and may not generalize to other attack types beyond label-flipping
- Algorithm performance depends heavily on hyperparameter tuning with limited guidance on optimal selection

## Confidence
- **High confidence**: The core algorithmic framework combining consensus-based optimization with bi-level formulation is sound and well-grounded in existing literature.
- **Medium confidence**: The theoretical convergence proof under mean-field assumptions, though the practical implications for finite-agent systems remain unclear.
- **Low confidence**: The generalizability of the robustness criterion G to other attack types and datasets beyond the specific label-flipping attack on rotated EMNIST.

## Next Checks
1. **Convergence under varying agent counts**: Validate the mean-field theoretical guarantees by testing FedCB2O with different numbers of agents (from 10 to 100) to identify when finite-size effects become significant.
2. **Cross-dataset robustness**: Evaluate FedCB2O on alternative federated learning benchmarks (e.g., CIFAR-10, Shakespeare) with different attack vectors (backdoor, model replacement) to test generalizability.
3. **Hyperparameter sensitivity analysis**: Systematically vary β, α, M, κ, and ζ to identify regions of stable performance and develop principled guidelines for hyperparameter selection.