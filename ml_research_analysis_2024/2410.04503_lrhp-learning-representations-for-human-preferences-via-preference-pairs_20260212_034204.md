---
ver: rpa2
title: 'LRHP: Learning Representations for Human Preferences via Preference Pairs'
arxiv_id: '2410.04503'
source_url: https://arxiv.org/abs/2410.04503
tags:
- preference
- lrhp
- data
- reward
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LRHP, a framework for learning structured
  representations of human preferences from preference pairs, extending beyond traditional
  reward modeling that encodes preferences as single numerical values. LRHP encodes
  preference pairs into a unified representational space using a special <|PREFERENCE|
  token and preference classification task, enabling richer preference capture.
---

# LRHP: Learning Representations for Human Preferences via Preference Pairs

## Quick Facts
- arXiv ID: 2410.04503
- Source URL: https://arxiv.org/abs/2410.04503
- Reference count: 28
- Key outcome: LRHP learns structured representations of human preferences from preference pairs, achieving 4.57 points improvement in helpfulness accuracy and 2.08 points in harmlessness accuracy for preference data selection, and Spearman correlation of 0.604 for preference margin prediction.

## Executive Summary
LRHP introduces a framework for learning structured representations of human preferences from preference pairs, moving beyond traditional reward modeling that encodes preferences as single numerical values. The approach uses a special <|PREFERENCE|> token to capture human preferences from input preference pairs, trained through a preference classification task. The learned representations are validated on two downstream tasks: preference data selection (PDS) and preference margin prediction (PMP), demonstrating significant improvements over vanilla reward modeling approaches.

## Method Summary
LRHP encodes preference pairs into a unified representational space using a special <|PREFERENCE|> token and preference classification task. The model concatenates a preference pair into a single text sequence ending with <|PREFERENCE|>, with the final hidden state serving as the preference representation. For PDS, LRHP computes distance scores between representations of available preference data and preference-specific data using cosine similarity. For PMP, LRHP fine-tunes the preference representation model with a regressive predictor on a small set of labeled preference margin scores, which then serve as constraints in DPO training.

## Key Results
- Achieved 4.57 points improvement in helpfulness accuracy and 2.08 points in harmlessness accuracy for preference data selection compared to vanilla reward modeling
- Achieved Spearman correlation of 0.604 for preference margin prediction with just 3k labeled samples
- When integrated into DPO training, PMP-LRHP reduced length bias and improved win rates by 3.3 points over SimPO on AlpacaEval2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LRHP learns richer preference representations by using a special <|PREFERENCE|> token to capture human preferences from input preference pairs.
- Mechanism: The model concatenates a preference pair into a single text sequence ending with <|PREFERENCE|>. The final hidden state of this token serves as the preference representation, trained through a binary classification task to predict whether the first or second response is preferred.
- Core assumption: The <|PREFERENCE|> token can effectively aggregate preference information from both responses in a pair, and this aggregated representation can distinguish between different preference types and tasks.
- Evidence anchors:
  - [abstract] "We introduce <|PREFERENCE|> as a special symbol to capture human preferences from the input preference pair, which is trained through a preference classification task."
  - [section] "The final hidden state of this token serves as the representation, capturing the underlying human preferences within this input preference pair."
  - [corpus] Weak - corpus contains related works on reward modeling but none specifically address the <|PREFERENCE|> token mechanism.

### Mechanism 2
- Claim: LRHP enables preference data selection (PDS) by computing distance scores between preference representations.
- Mechanism: For PDS, LRHP computes distance scores between representations of available preference data and preference-specific data using cosine similarity. Smaller distances indicate better matches, allowing selection of relevant preference data without extensive annotation.
- Core assumption: The learned preference representations encode meaningful preference features that can be compared using distance metrics, and these features correlate with actual human preferences.
- Evidence anchors:
  - [abstract] "We verify the utility of preference representations in two downstream tasks: preference data selection (PDS) and preference margin prediction (PMP)."
  - [section] "A smaller distance score indicates that the preference sample more closely matches the preferences in DPS."
  - [corpus] Weak - corpus mentions related works on reward modeling and preference data but doesn't specifically address representation-based data selection.

### Mechanism 3
- Claim: LRHP enables preference margin prediction (PMP) through adaptive learning with few labeled samples.
- Mechanism: LRHP fine-tunes the preference representation model with a regressive predictor on a small set of labeled preference margin scores. The predicted margins serve as constraints in DPO training, reducing length bias and improving performance.
- Core assumption: The preference representations capture sufficient information to predict relative preference strengths, and fine-tuning on few samples can generalize to unseen preference pairs.
- Evidence anchors:
  - [abstract] "This predictor can be well-trained with a few labeled samples. When aligning LLMs with DPO, the predicted margin score can be a superior alternative to a rule-based margin."
  - [section] "These labeled samples are then used to fine-tune our preference representation model coupled with a regressive predictor."
  - [corpus] Weak - corpus mentions related works on reward modeling and preference optimization but doesn't specifically address margin prediction from representations.

## Foundational Learning

- Concept: Representation learning for textual data
  - Why needed here: LRHP needs to learn meaningful vector representations of preference pairs that capture human preferences, similar to how BERT learns contextual embeddings
  - Quick check question: How does LRHP's use of the <|PREFERENCE|> token differ from BERT's [CLS] token in terms of what information it needs to capture?

- Concept: Binary classification for preference learning
  - Why needed here: LRHP uses binary classification to train the preference representation model, requiring understanding of how classification loss functions work with paired data
  - Quick check question: Why does LRHP randomly shuffle response order within preference pairs to maintain balanced classes?

- Concept: Cosine similarity for representation comparison
  - Why needed here: LRHP uses cosine similarity to compute distance scores between preference representations for PDS, requiring understanding of vector similarity metrics
  - Quick check question: What would happen to PDS performance if we used Euclidean distance instead of cosine similarity?

## Architecture Onboarding

- Component map:
  Base LLM (LLaMA-3-8B-Instruction or Mistral-7B-Instruction) -> Modified to remove final unembedding layer -> <|PREFERENCE|> token -> Preference classification head / Regressive predictor -> Distance computation module

- Critical path:
  1. Input preference pair concatenated with <|PREFERENCE|> token
  2. Forward pass through modified LLM
  3. Extract final hidden state of <|PREFERENCE|> token
  4. Apply classification head (training) or predictor (inference)
  5. Compute loss or make predictions

- Design tradeoffs:
  - Using <|PREFERENCE|> token vs. [CLS] token: The special token is trained from scratch rather than inheriting pre-trained semantics
  - Classification vs. regression training: Binary classification is simpler but may lose margin information
  - Single representation vs. separate response representations: Unified representation is more compact but may lose some detail

- Failure signatures:
  - Poor classification accuracy -> <|PREFERENCE|> token not capturing preference information effectively
  - Weak correlation in PMP task -> Representations not encoding relative preference strengths
  - PDS performance no better than random -> Representations not capturing task-specific preference features

- First 3 experiments:
  1. Train LRHP on 10k preference pairs and evaluate classification accuracy on held-out pairs
  2. Compare PDS performance using LRHP representations vs. random selection on helpfulness/harmlessness task
  3. Fine-tune LRHP on 1k labeled margin samples and evaluate correlation with human labels

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text. However, based on the limitations section, several open questions emerge regarding the generalizability of LRHP to different datasets, the impact of larger language models, and the inclusion of demonstration data in learning preference representations.

## Limitations
- Weak corpus evidence supporting the effectiveness of the <|PREFERENCE|> token mechanism
- Reliance on manually labeled margin scores with unspecified quality control measures
- Assumption that learned representations transfer effectively across diverse preference tasks without validation

## Confidence
- Mechanism 1 (Preference Representation Learning): Medium confidence - Strong empirical results but weak supporting evidence from related work
- Mechanism 2 (Preference Data Selection): Medium confidence - Good results but assumption about cosine similarity capturing meaningful preference similarity needs validation
- Mechanism 3 (Margin Prediction): High confidence - Compelling correlation results and well-supported few-shot learning approach

## Next Checks
1. **Representation Quality Validation**: Compare the learned <|PREFERENCE|> token representations using t-SNE or UMAP visualization to verify they form distinct clusters by preference type and task, as claimed in Figure 7
2. **Distance Metric Ablation Study**: Conduct experiments using alternative distance metrics (Euclidean, Mahalanobis) for preference data selection to determine if cosine similarity is truly optimal for capturing preference similarity
3. **Cross-Task Transfer Analysis**: Test the LRHP representations trained on helpfulness/harmlessness pairs on a completely different preference task (e.g., code quality preferences) to validate the claimed generality of the learned representations