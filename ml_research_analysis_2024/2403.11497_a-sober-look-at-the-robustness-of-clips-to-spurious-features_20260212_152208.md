---
ver: rpa2
title: A Sober Look at the Robustness of CLIPs to Spurious Features
arxiv_id: '2403.11497'
source_url: https://arxiv.org/abs/2403.11497
tags:
- clip
- vit-b
- data
- spurious
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a critical gap in evaluating the robustness
  of CLIP models to spurious features, as existing benchmarks are designed for ImageNet-trained
  models and may not reflect CLIP's unique vulnerabilities. To address this, the authors
  curate a new challenging dataset, CounterAnimal, which captures realistic spurious
  correlations between animal classes and their backgrounds.
---

# A Sober Look at the Robustness of CLIPs to Spurious Features

## Quick Facts
- arXiv ID: 2403.11497
- Source URL: https://arxiv.org/abs/2403.11497
- Reference count: 40
- Primary result: CLIP models exhibit significant performance drops on CounterAnimal dataset, demonstrating vulnerability to spurious features

## Executive Summary
This paper addresses a critical gap in evaluating CLIP models' robustness to spurious features. Existing benchmarks are designed for ImageNet-trained models and may not reflect CLIP's unique vulnerabilities. The authors introduce CounterAnimal, a challenging dataset capturing realistic spurious correlations between animal classes and their backgrounds. Through extensive experiments across various CLIP backbones and pre-training datasets, they demonstrate that CLIP models show significant performance drops on CounterAnimal, while ImageNet models exhibit greater robustness. The study reveals that scaling model parameters and improving data quality are effective strategies for mitigating spurious features, supported by theoretical analysis explaining why the CLIP objective inherently learns these biases.

## Method Summary
The authors create CounterAnimal, a dataset of 13,100 images across 45 animal classes with annotated backgrounds. They evaluate CLIP models' robustness by comparing zero-shot classification performance between "easy" images (animals with common backgrounds) and "hard" images (animals with atypical backgrounds). The study examines multiple CLIP model variants with different backbone architectures (ViT-B/16, ViT-L/14) and pre-training datasets (LAION-400M, LAION-2B, DataComp). Zero-shot classification is performed using the prompt "A photo of <object label>." Performance metrics include accuracy (1 vs. 1000 and 1 vs. 20 setups) and accuracy drop between easy and hard groups.

## Key Results
- CLIP models exhibit significant performance drops on CounterAnimal, with accuracy decreasing from 81.8% to 56.3% for ViT-B/16 and from 86.3% to 70.0% for ViT-L/14
- Larger CLIP models (ViT-L/14) show greater robustness to spurious features compared to smaller models (ViT-B/16)
- CLIP models trained on high-quality data (DataComp) demonstrate improved robustness compared to those trained on standard LAION datasets
- Theoretical analysis reveals that the CLIP objective inherently learns spurious correlations between objects and backgrounds due to the contrastive learning framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP models learn spurious correlations between objects and backgrounds due to the contrastive learning objective's alignment of image and text embeddings
- Mechanism: The CLIP training objective minimizes distance between image and text embeddings when paired, maximizing distance when not paired. This leads the model to align not just object features but also correlated background features with object captions, creating spurious correlations
- Core assumption: Training data backgrounds are correlated with object captions, and the model cannot distinguish between invariant object features and spurious background features
- Evidence anchors: [abstract]: "theoretical insights that the CLIP objective cannot offer additional robustness"; [section 4]: "CLIP will learn to align the backgrounds, i.e., spurious features, with object captions"

### Mechanism 2
- Claim: Larger CLIP models are more robust to spurious features because they have stronger capacity to learn invariant features
- Mechanism: Larger models with more parameters have greater representational capacity, allowing them to better capture true invariant features of objects while being less prone to relying on spurious correlations with backgrounds
- Core assumption: Spurious correlations in data are weaker than true invariant features, so larger models can prioritize learning invariant features
- Evidence anchors: [section 3.2]: "Larger CLIP models are more robust...larger models exhibit better performance against spurious correlations"; [abstract]: "scaling up model parameters...are effective strategies for mitigating spurious features"

### Mechanism 3
- Claim: High-quality pre-training data improves CLIP robustness to spurious features by reducing presence of spurious correlations in training data
- Mechanism: Pre-training datasets that are more carefully curated and filtered have fewer spurious correlations between objects and backgrounds. This leads the CLIP model to learn fewer spurious correlations during training, improving its robustness to background shifts
- Core assumption: Spurious correlations in CounterAnimal are representative of spurious correlations in general CLIP training data, and reducing these correlations in training data improves robustness
- Evidence anchors: [section 3.2]: "CLIP models trained on high-quality data are more robust...enhancing data quality remains a promising strategy"; [abstract]: "scaling up parameters and improving data quality are effective strategies for mitigating spurious features"

## Foundational Learning

- Concept: Spurious correlations
  - Why needed here: Understanding what spurious correlations are and how they affect model performance is crucial for interpreting the paper's findings about CLIP robustness
  - Quick check question: What is a spurious correlation, and why can it lead to poor model performance on out-of-distribution data?

- Concept: Contrastive learning
  - Why needed here: The paper's theoretical analysis of why CLIP learns spurious correlations relies on understanding how the contrastive learning objective works
  - Quick check question: How does the contrastive learning objective in CLIP training lead the model to align image and text embeddings?

- Concept: Zero-shot classification
  - Why needed here: The paper evaluates CLIP robustness using zero-shot classification, so understanding this concept is necessary for interpreting the results
  - Quick check question: What is zero-shot classification, and how does it differ from traditional supervised classification?

## Architecture Onboarding

- Component map: Image encoder (Vision Transformer) -> Text encoder (BERT model) -> Contrastive loss computation
- Critical path: Forward pass through both encoders to compute image-text similarity, followed by contrastive loss computation during training
- Design tradeoffs: Main tradeoff is between model size (larger models are more robust but computationally expensive) and data quality (higher quality data improves robustness but may be more difficult to obtain)
- Failure signatures: CLIP models may fail when there are strong spurious correlations between objects and backgrounds in the training data, leading to poor performance on data with different background distributions
- First 3 experiments:
  1. Evaluate a small CLIP model (e.g., ViT-B/32) on CounterAnimal to confirm it exhibits spurious correlations
  2. Evaluate a larger CLIP model (e.g., ViT-L/14) on CounterAnimal to confirm it is more robust to spurious correlations
  3. Evaluate a CLIP model trained on high-quality data (e.g., DataComp) on CounterAnimal to confirm it is more robust to spurious correlations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do CLIP models perform on distribution shifts caused by spurious features beyond animal-background correlations, such as object-object or object-text correlations?
- Basis in paper: [explicit] The authors acknowledge that their CounterAnimal dataset specifically targets animal-background correlations and suggests extending the dataset to broader semantic scopes
- Why unresolved: The dataset is currently limited to animal classes and their backgrounds, potentially missing other common spurious feature types in vision-language models
- What evidence would resolve it: Evaluating CLIP models on a dataset capturing diverse spurious feature types (object-object, object-text) and comparing performance drops across different spurious feature categories

### Open Question 2
- Question: Does the effectiveness of scaling up model parameters and improving data quality on mitigating spurious features hold for more advanced vision-language models beyond CLIP, such as MiniGPT4 and LLaVA?
- Basis in paper: [inferred] The authors observe these strategies help mitigate spurious features in CLIP models but do not extensively test them on other LVLMs like MiniGPT4 and LLaVA
- Why unresolved: The study focuses primarily on CLIP models, leaving uncertainty about whether these mitigation strategies generalize to other model architectures
- What evidence would resolve it: Systematic evaluation of parameter scaling and data quality improvements on various LVLMs' robustness to spurious features

### Open Question 3
- Question: How does the theoretical analysis of CLIP's vulnerability to spurious features extend to more complex, real-world scenarios with multiple interacting spurious features?
- Basis in paper: [explicit] The authors present theoretical analysis for a simplified case of one invariant and one spurious feature but acknowledge the complexity of real-world scenarios
- Why unresolved: The theoretical framework is limited to a simplistic scenario, and its applicability to complex, multi-feature spurious correlations remains unclear
- What evidence would resolve it: Extending the theoretical analysis to account for multiple spurious features and validating it with experiments on datasets with complex spurious correlations

## Limitations

- CounterAnimal dataset may not capture the full complexity of real-world spurious correlations that CLIP models encounter, as it focuses specifically on animal-class and background correlations
- The study primarily focuses on zero-shot evaluation, which may not reflect how CLIP models perform when fine-tuned for specific downstream tasks
- Theoretical analysis provides insights into why CLIP learns spurious correlations but doesn't offer concrete solutions for preventing this behavior during training

## Confidence

- **High confidence**: CLIP models exhibit significant performance drops on CounterAnimal, demonstrating vulnerability to spurious features
- **Medium confidence**: Scaling up model parameters and improving data quality are effective strategies for mitigating spurious features
- **Low confidence**: The theoretical analysis fully explains why the CLIP objective inherently learns spurious correlations

## Next Checks

1. Evaluate CLIP models on additional spurious feature benchmarks beyond CounterAnimal, including those focusing on different types of spurious correlations (e.g., object-object relationships, lighting conditions) to assess generalizability of findings

2. Test CLIP robustness when fine-tuned on CounterAnimal to determine if observed vulnerability to spurious features persists in the fine-tuned setting, which better reflects real-world deployment scenarios

3. Compare CLIP's behavior with other vision-language models (e.g., ALIGN, Florence) on CounterAnimal to determine if observed vulnerability to spurious features is specific to CLIP's architecture or training objective, or a more general challenge for vision-language models