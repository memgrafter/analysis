---
ver: rpa2
title: Disentangling Reasoning Tokens and Boilerplate Tokens For Language Model Fine-tuning
arxiv_id: '2412.14780'
source_url: https://arxiv.org/abs/2412.14780
tags:
- tokens
- reasoning
- shad
- boilerplate
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work highlights the need to distinguish reasoning tokens
  from boilerplate tokens when using agent-task datasets to improve LLM agent capabilities.
  It introduces SHAD, a method that classifies tokens by exploiting predictability
  differences after shuffling input-output pairs: boilerplate tokens remain predictable
  while reasoning tokens do not.'
---

# Disentangling Reasoning Tokens and Boilerplate Tokens For Language Model Fine-tuning

## Quick Facts
- **arXiv ID**: 2412.14780
- **Source URL**: https://arxiv.org/abs/2412.14780
- **Reference count**: 25
- **Primary result**: Introduces SHAD and RFT methods that distinguish reasoning tokens from boilerplate tokens during fine-tuning, achieving superior performance on agent benchmarks compared to standard fine-tuning and other token-level methods.

## Executive Summary
This work addresses the challenge of improving LLM agent capabilities by distinguishing between reasoning tokens and boilerplate tokens in agent-task datasets. Standard fine-tuning treats all tokens equally, leading to overfitting on easily learned boilerplate tokens at the expense of reasoning capabilities. The paper introduces SHAD, a method that classifies tokens by exploiting predictability differences after shuffling input-output pairs, and RFT, which adaptively emphasizes reasoning tokens during fine-tuning. The approach achieves notable performance gains on held-in and held-out agent benchmarks compared to common Supervised Fine-Tuning and specialized baselines.

## Method Summary
The method operates in two stages: First, SHAD classifies tokens by shuffling 1% of input-output combinations across samples, training a reference model on this shuffled data, and comparing token-level losses between the tuned and original models. Tokens showing decreased loss after shuffling are classified as boilerplate (consistent across samples), while others are classified as reasoning. Second, RFT applies adaptive weighting during fine-tuning, using softmax to assign higher weights to the token group (reasoning or boilerplate) with greater total loss, naturally emphasizing reasoning tokens. The approach is evaluated on ToolBench, APIGen, and ShareGPT datasets using LLaMA3.8B and LLaMA3.1-8B models.

## Key Results
- SHAD achieves 97.5% accuracy in classifying formatting tokens from ToolBench, with manual inspection confirming reasoning token classification accuracy.
- RFT outperforms standard SFT, Regex weighting, Rho-1, and RewardFT baselines on held-in benchmarks (StableToolBench, BFCL) and shows competitive performance on held-out benchmarks (T-eval, Nexus).
- The method effectively prevents overfitting to boilerplate tokens while maintaining learning of reasoning capabilities, as evidenced by training loss patterns and improved evaluation performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SHAD exploits predictability differences after shuffling input-output pairs to distinguish boilerplate tokens from reasoning tokens.
- Mechanism: By shuffling output labels across samples, boilerplate tokens (which are repetitive across samples) maintain their predictability, while reasoning tokens become unpredictable due to mismatched inputs.
- Core assumption: Boilerplate tokens have consistent patterns across samples that can be exploited through shuffling.
- Evidence anchors:
  - [abstract] "SHAD classifies tokens by exploiting predictability differences observed after shuffling input-output combinations across samples: boilerplate tokens, due to their repetitive nature among samples, maintain predictability, whereas reasoning tokens do not."
  - [section 3.1] "Considering boilerplate tokens are usually consistent across samples, they can be treated as sample-independent. Consequently, shuffling the correspondence between input and output across data samples does not alter the predictability of boilerplate tokens."
  - [corpus] Weak - no direct supporting evidence in corpus
- Break condition: If boilerplate tokens vary significantly across samples or if the shuffling introduces too much noise that affects both token types similarly.

### Mechanism 2
- Claim: RFT adaptively emphasizes reasoning tokens during fine-tuning by comparing total losses between reasoning and boilerplate parts.
- Mechanism: RFT applies softmax weighting to assign higher weights to the part (reasoning or boilerplate) with greater total loss, naturally emphasizing reasoning tokens since they typically have higher loss.
- Core assumption: Reasoning tokens generally have higher loss than boilerplate tokens during training.
- Evidence anchors:
  - [abstract] "RFT adaptively emphasizes reasoning tokens during fine-tuning, achieving superior performance on held-in and held-out agent benchmarks compared to standard fine-tuning"
  - [section 3.2] "Specifically, we compare the total losses of the reasoning and boilerplate parts, applying the softmax function to assign higher weights to the part with the greater loss."
  - [corpus] Weak - no direct supporting evidence in corpus
- Break condition: If reasoning and boilerplate tokens have similar losses, or if the temperature parameter τ is poorly chosen.

### Mechanism 3
- Claim: The combination of SHAD and RFT addresses overfitting to boilerplate tokens while maintaining learning of reasoning capabilities.
- Mechanism: SHAD accurately identifies reasoning tokens, and RFT uses this classification to prevent overfitting to easily learned boilerplate tokens while emphasizing harder reasoning tokens.
- Core assumption: Standard fine-tuning treats all tokens equally, leading to overfitting on boilerplate tokens at the expense of reasoning tokens.
- Evidence anchors:
  - [abstract] "Using SHAD, the paper proposes RFT, which adaptively emphasizes reasoning tokens during fine-tuning, yielding notable performance gains over common Supervised Fine-Tuning (SFT)."
  - [section 1] "Failure to do so may result in undesired effects, such as overfitting to the boilerplate components... ultimately leading to inadequate agent capabilities."
  - [corpus] Weak - no direct supporting evidence in corpus
- Break condition: If token classification accuracy is poor or if the weighting mechanism fails to properly emphasize reasoning tokens.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: Understanding SFT is essential as the baseline method that treats all tokens equally
  - Quick check question: What is the main limitation of standard SFT when training on agent-task datasets?

- Concept: Token-level loss calculation
  - Why needed here: The method relies on comparing token-level losses between original and shuffled models
  - Quick check question: How is the loss difference LD(yk) calculated for each token?

- Concept: Softmax weighting
  - Why needed here: RFT uses softmax to dynamically assign weights based on total losses of reasoning vs boilerplate tokens
  - Quick check question: What role does the temperature coefficient τ play in the softmax weighting?

## Architecture Onboarding

- Component map:
  - Data preprocessing -> Shuffle subset (1%) -> Train reference model -> Classify tokens -> Apply RFT weighting -> Fine-tune model -> Evaluate performance

- Critical path:
  1. Shuffle data → 2. Train reference model → 3. Classify tokens → 4. Apply RFT → 5. Evaluate performance

- Design tradeoffs:
  - Shuffle ratio: Too high wastes computation, too low may not capture patterns
  - Temperature τ: Controls weight differentiation, affects learning balance
  - Token classification threshold: Impacts which tokens are classified as reasoning vs boilerplate

- Failure signatures:
  - Poor token classification accuracy (<95%)
  - RFT performance worse than SFT baseline
  - High variance in evaluation results across runs
  - Overfitting to training data despite RFT

- First 3 experiments:
  1. Test SHAD classification accuracy on formatting tokens (known ground truth)
  2. Compare training losses for reasoning vs boilerplate tokens with and without RFT
  3. Evaluate performance on held-out benchmarks to test generalization

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the methodology and results, several important questions emerge:

- How does the performance of SHAD+RFT change when applied to domains beyond agent-task datasets, such as general text generation or code completion?
- What is the optimal ratio of data to shuffle for maximizing the effectiveness of SHAD, and how sensitive is the method to this parameter?
- How does SHAD perform in datasets with high variability in boilerplate tokens, such as those with multiple output formats or inconsistent templates?
- What is the impact of SHAD+RFT on model generalization to entirely unseen tasks or tools not present in the training data?

## Limitations

- The optimal temperature coefficient τ for RFT is not specified, requiring hyperparameter tuning for different datasets.
- Token classification accuracy for reasoning tokens lacks independent validation beyond formatting tokens.
- The method's effectiveness on domains without clear boilerplate vs reasoning distinctions remains untested.

## Confidence

**High confidence**: The SHAD methodology is sound and the classification mechanism is clearly specified. The improvement over SFT baseline on held-in benchmarks is consistent and substantial across multiple datasets.

**Medium confidence**: The claim that RFT outperforms all baselines (including specialized methods like RewardFT and Rho-1) is supported by the data, but the margin of improvement over some baselines is relatively small. The held-out benchmark results, while positive, show more variable performance.

**Low confidence**: Claims about preventing "overfitting to boilerplate tokens" are supported by training loss patterns but lack direct validation through ablation studies or comparison to alternative regularization methods that don't use SHAD.

## Next Checks

1. **Cross-domain validation**: Apply SHAD+RFT to a non-agent dataset (e.g., code generation or mathematical reasoning) to verify the method's generalizability beyond the tested agent-task domain.

2. **Ground truth validation**: Manually annotate a small subset of reasoning tokens in the training data and compare SHAD's classifications against human judgment to establish classification accuracy for reasoning tokens specifically.

3. **Hyperparameter robustness**: Systematically test RFT performance across a broader range of temperature values (τ) and training seed variations to establish the method's robustness to hyperparameter choices and random initialization.