---
ver: rpa2
title: 'Mixture-of-Depths: Dynamically allocating compute in transformer-based language
  models'
arxiv_id: '2404.02258'
source_url: https://arxiv.org/abs/2404.02258
tags:
- tokens
- routing
- compute
- transformer
- flops
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mixture-of-Depths (MoD) transformers dynamically allocate compute
  by routing tokens to either self-attention+MLP blocks or residual connections, enabling
  significant FLOP savings without sacrificing performance. Unlike Mixture-of-Experts,
  MoD uses a single expert with capacity-controlled routing via top-k selection.
---

# Mixture-of-Depths: Dynamically allocating compute in transformer-based language models

## Quick Facts
- arXiv ID: 2404.02258
- Source URL: https://arxiv.org/abs/2404.02258
- Reference count: 7
- Primary result: MoD transformers achieve 50% FLOP reduction while matching isoFLOP baselines through dynamic token routing

## Executive Summary
Mixture-of-Depths (MoD) introduces a novel approach to dynamically allocate compute in transformer models by routing individual tokens to either standard self-attention+MLP blocks or residual connections. Unlike Mixture-of-Experts which uses multiple experts, MoD employs a single expert with capacity-controlled routing via top-k selection. The method demonstrates that models can maintain performance while using 50% fewer FLOPs per forward pass and training up to 66% faster. Optimal performance occurs when MoD models use equal FLOPs per forward pass as baselines, enabling larger models or longer training within the same compute budget.

## Method Summary
MoD transforms standard transformer blocks by introducing a router that determines whether each token should pass through the full self-attention and MLP computation or skip directly via a residual connection. The router selects the top-k tokens based on importance scores for each block, with the remaining tokens bypassing the computation. This creates a sparse computation pattern where different tokens engage different numbers of blocks throughout the model. The routing decisions are learned end-to-end through standard backpropagation, allowing the model to balance immediate prediction accuracy against future token interactions through self-attention. The technique integrates naturally with MoE (creating MoDE), compounding efficiency gains by both selecting tokens and experts.

## Key Results
- 50% reduction in FLOPs per forward pass compared to standard transformers
- 66% faster training speed while maintaining performance
- IsoFLOP models (equal FLOPs per forward pass) match or exceed baseline performance
- Some tokens engage every block while others skip computations, creating adaptive computation patterns

## Why This Works (Mechanism)
MoD works by learning to route tokens based on their importance to the task at hand. The router evaluates each token's potential contribution to downstream predictions and routes only the most important tokens through expensive self-attention and MLP computations. This creates an adaptive computation pattern where the model dynamically allocates resources based on input content rather than applying uniform computation to all tokens. The learned routing balances immediate prediction needs against the potential for future interactions through self-attention, effectively prioritizing tokens that will have the greatest impact on the final output.

## Foundational Learning
1. Transformer architecture fundamentals - why needed: MoD builds directly on standard transformer blocks; quick check: understand self-attention and MLP operations
2. Dynamic computation graphs - why needed: MoD creates different computational paths per token; quick check: can you explain variable computation patterns
3. Routing mechanisms in neural networks - why needed: MoD uses learned routing to allocate compute; quick check: understand how routers make decisions
4. FLOPs and computational efficiency - why needed: MoD's primary benefit is reduced computation; quick check: can you calculate theoretical speedups
5. Sparsity in deep learning - why needed: MoD creates sparse activation patterns; quick check: understand how sparsity affects performance
6. Capacity-controlled routing - why needed: MoD uses top-k selection to limit computational load; quick check: understand trade-offs between k and performance

## Architecture Onboarding

Component Map:
Input tokens -> Router -> [Top-k tokens] -> Self-Attention+MLP blocks -> Output
                              -> [Remaining tokens] -> Residual connections -> Output

Critical Path:
Token → Router → (Decision: Compute path or Skip path) → Next block

Design Tradeoffs:
- k value selection: Larger k increases computation but may improve quality; smaller k saves compute but risks losing important tokens
- Router architecture complexity: More sophisticated routers may make better decisions but add computational overhead
- Integration with existing transformer components: MoD must maintain compatibility while adding routing logic

Failure Signatures:
- Routing decisions become too conservative, routing almost all tokens through full computation
- Routing decisions become too aggressive, routing too few tokens through computation
- Router learns degenerate strategies that don't align with task objectives
- Performance degradation when k is set too low for the task complexity

First Experiments:
1. Vary k from 1 to 50% of sequence length to find optimal trade-off between efficiency and performance
2. Compare MoD against static pruning methods to quantify dynamic routing benefits
3. Test MoD on different sequence lengths to verify scalability of the approach

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Routing decisions lack clear interpretability despite claims about token importance
- Efficiency gains may not generalize across all model scales and task types beyond tested language modeling
- MoDE integration is proposed but only briefly explored with limited empirical validation
- Routing mechanism robustness to adversarial inputs and distribution shifts is unexamined

## Confidence

High confidence in: the core technical implementation of MoD blocks and their ability to reduce FLOPs while maintaining performance on tested benchmarks. High confidence in: the training efficiency improvements when models use equal FLOPs per forward pass as baselines.

Medium confidence in: the generalizability of results across different model scales and tasks beyond the tested language modeling scenarios. Medium confidence in: the interpretation of routing decisions and their relationship to token importance.

Low confidence in: the practical impact of MoDE integration without more extensive empirical validation.

## Next Checks

1. Test MoD performance across diverse task types including vision, multi-modal, and structured prediction tasks to verify generalizability beyond language modeling.

2. Conduct ablation studies varying the top-k parameter and routing temperature to understand their impact on efficiency and performance trade-offs.

3. Evaluate routing decision robustness under adversarial conditions and distribution shifts to assess reliability in production deployments.