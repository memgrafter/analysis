---
ver: rpa2
title: Semi-Supervised Semantic Segmentation using Redesigned Self-Training for White
  Blood Cells
arxiv_id: '2401.07278'
source_url: https://arxiv.org/abs/2401.07278
tags:
- fixmatch
- self-training
- labeled
- data
- unlabeled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes incorporating FixMatch consistency regularization
  into self-training frameworks for semi-supervised WBC segmentation. The approach
  leverages unlabeled data by generating pseudo-labels via a teacher model, then retraining
  with both labeled and pseudo-labeled data.
---

# Semi-Supervised Semantic Segmentation using Redesigned Self-Training for White Blood Cells

## Quick Facts
- arXiv ID: 2401.07278
- Source URL: https://arxiv.org/abs/2401.07278
- Authors: Vinh Quoc Luu; Duy Khanh Le; Huy Thanh Nguyen; Minh Thanh Nguyen; Thinh Tien Nguyen; Vinh Quang Dinh
- Reference count: 12
- Primary result: FixMatch-enhanced self-training improves WBC segmentation with mIoU scores of 90.69%, 87.37%, and 76.49% across three datasets

## Executive Summary
This paper proposes incorporating FixMatch consistency regularization into self-training frameworks for semi-supervised WBC segmentation. The approach leverages unlabeled data by generating pseudo-labels via a teacher model, then retraining with both labeled and pseudo-labeled data. FixMatch adds weak-to-strong consistency training, improving robustness. Experiments across DeepLab-V3, ResNet-50/101, PSPNet, and three datasets (Zheng 1: 90.69% mIoU, Zheng 2: 87.37%, LISC: 76.49%) show consistent performance gains over baselines. While FixMatch improves unlabeled data handling, trade-offs on labeled data quality and intra-dataset variability remain. The method generalizes well but is sensitive to dataset complexity.

## Method Summary
The proposed method combines self-training with FixMatch consistency regularization for semi-supervised WBC segmentation. The pipeline trains a teacher model on labeled data, generates pseudo-labels for unlabeled images using weak augmentation, then retrains a student model on both labeled data and pseudo-labeled data while enforcing consistency between weak and strong augmentations. The ST++ variant selectively applies self-training to reliable unlabeled images while using unreliable ones for consistency regularization. The approach balances supervised and semi-supervised objectives, though this introduces trade-offs between labeled and unlabeled data performance.

## Key Results
- FixMatch-enhanced self-training achieves mIoU scores of 90.69% (Zheng 1), 87.37% (Zheng 2), and 76.49% (LISC)
- The method consistently outperforms baseline self-training across all three datasets and multiple architectures
- Incorporating FixMatch consistency improves robustness to input variations but introduces trade-offs on labeled data performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating FixMatch consistency regularization improves semi-supervised WBC segmentation by enforcing model robustness to input variations.
- Mechanism: FixMatch generates weak-augmented pseudo-labels from unlabeled data, then applies strong augmentation and consistency loss to align predictions with pseudo-labels, improving generalization.
- Core assumption: The pseudo-labels generated from weakly augmented inputs are reliable enough to guide strong augmentation consistency training.
- Evidence anchors:
  - [abstract] "FixMatch is a consistency-regularization algorithm to enforce the model's robustness against variations in the input image."
  - [section 3.2] "The weak-to-strong consistency regularization mechanism allows the model to be more robust against strong variations in the unlabeled data."
- Break condition: If pseudo-label quality is poor due to class imbalance or low confidence thresholds, consistency loss can mislead the model and degrade performance.

### Mechanism 2
- Claim: ST++'s two-stage approach (reliable vs unreliable unlabeled images) improves segmentation performance over plain ST.
- Mechanism: ST++ partitions unlabeled data into reliable and unreliable subsets, applying self-training only on reliable data while using unreliable data for consistency regularization, reducing noisy pseudo-label propagation.
- Core assumption: Reliable/unreliable classification based on prediction stability and confidence is accurate enough to prevent noise accumulation.
- Evidence anchors:
  - [section 3.2] "ST++ is a sophisticated framework that performs selective re-training by prioritizing reliable images based on holistic prediction-level stability throughout the training course."
  - [section 5.1] "ST++ has more discriminative power over ST, since it separates the unlabeled data into reliable and unreliable ones."
- Break condition: If the reliability threshold is too strict, useful unlabeled data is discarded; if too loose, noisy labels corrupt training.

### Mechanism 3
- Claim: Trade-off between supervised and semi-supervised performance is necessary for effective unlabeled data utilization.
- Mechanism: Incorporating FixMatch reduces performance on labeled-only validation sets but improves final semi-supervised performance by forcing the model to learn from unlabeled data distribution.
- Core assumption: Temporary degradation on labeled data is acceptable if overall semi-supervised performance improves.
- Evidence anchors:
  - [section 5.2] "There was a noticeable decrease in performance on the labeled images... This is a commonly seen trade-off when we try to do two tasks in 1 stage."
  - [section 5.2] "The performance dropped on supervised images is necessary for the model to perform well on unlabeled ones."
- Break condition: If the trade-off causes excessive degradation on labeled data, the model may fail to learn essential supervised features.

## Foundational Learning

- Concept: Consistency regularization in semi-supervised learning
  - Why needed here: WBC segmentation datasets are small and expensive to label, so leveraging unlabeled data through consistency is critical.
  - Quick check question: How does weak-to-strong augmentation consistency differ from simple pseudo-labeling?

- Concept: Self-training pipeline mechanics
  - Why needed here: Understanding the teacher-student framework is essential for implementing and debugging the proposed method.
  - Quick check question: What are the two main stages in the self-training pipeline described in the paper?

- Concept: Evaluation metrics for semantic segmentation
  - Why needed here: Mean Intersection-over-Union (mIoU) is the primary metric used; understanding it is crucial for result interpretation.
  - Quick check question: What does an mIoU of 90.69% on Zheng 1 indicate about model performance?

## Architecture Onboarding

- Component map:
  Teacher model -> Pseudo-label generator (weak augmentation) -> Student model -> Consistency loss (strong augmentation) -> Retrained model

- Critical path:
  1. Train teacher on labeled data
  2. Generate pseudo-labels on unlabeled data (weak augmentation)
  3. Apply consistency loss with strong augmentation
  4. Retrain student model on combined labeled and pseudo-labeled data

- Design tradeoffs:
  - Confidence threshold for pseudo-labeling: higher → cleaner but fewer labels; lower → more labels but noisier
  - Batch size ratio for labeled vs unlabeled data: affects training stability
  - Choice of backbone (ResNet-50 vs ResNet-101): speed vs accuracy tradeoff

- Failure signatures:
  - Performance degradation on labeled data with no semi-supervised gain
  - High variance in mIoU across datasets (generalization issues)
  - Convergence failure when unlabeled data dominates training

- First 3 experiments:
  1. Baseline supervised-only training on labeled data only
  2. Self-training without FixMatch consistency
  3. Self-training with FixMatch consistency regularization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed framework be adapted to handle high intra-dataset variability, such as in the LISC dataset, without sacrificing performance on simpler datasets like Zheng 1 and Zheng 2?
- Basis in paper: [explicit] The paper notes that the proposed approach has yet to resolve intra-variation in challenging datasets and achieve consistent performance across different datasets.
- Why unresolved: The current framework shows significant performance drops on datasets with high variability, indicating that the model struggles to generalize across diverse image characteristics.
- What evidence would resolve it: Experiments demonstrating improved performance on high-variability datasets without degrading performance on simpler datasets would indicate a successful adaptation.

### Open Question 2
- Question: What domain-specific techniques can be developed to improve the quality of pseudo-masks generated for complex, unreliable images?
- Basis in paper: [explicit] The paper observes that pseudo-masks for complicated, unreliable images deviate significantly from the ground truth, suggesting the need for more domain-specific approaches.
- Why unresolved: The current incorporation of FixMatch does not adequately address the nuances of difficult images, leading to poor pseudo-mask quality.
- What evidence would resolve it: Development and testing of domain-specific techniques that consistently generate high-quality pseudo-masks for complex images would resolve this issue.

### Open Question 3
- Question: Is there a way to balance the trade-off between excelling on labeled images and leveraging consistency regularization on unlabeled images without sacrificing performance on either?
- Basis in paper: [explicit] The paper identifies a trade-off where incorporating FixMatch improves performance on unlabeled data but decreases performance on labeled data.
- Why unresolved: The current approach necessitates a performance drop on labeled images to achieve improvements on unlabeled ones, indicating a need for better balance.
- What evidence would resolve it: Experiments showing consistent performance improvements on both labeled and unlabeled data without significant trade-offs would indicate a successful balance.

## Limitations
- The method shows sensitivity to dataset complexity, with significant performance variations between Zheng 1 and LISC datasets
- Trade-off between supervised and semi-supervised performance remains problematic, with noticeable degradation on labeled data
- The approach's reliance on pseudo-label quality introduces vulnerability to noise propagation, particularly in datasets with class imbalance or ambiguous boundaries

## Confidence

- **Medium** for overall effectiveness claims, supported by quantitative improvements across multiple datasets and architectures
- **High** confidence in the documented trade-off between supervised and semi-supervised performance, as explicitly observed in experiments
- **Low** confidence in the method's robustness to dataset variations, given the substantial performance drop on the more complex LISC dataset

## Next Checks

1. Test the method's sensitivity to different confidence thresholds τ for pseudo-label generation across varying dataset sizes
2. Evaluate the approach on a fourth, independently-collected WBC dataset to assess true generalization capability
3. Conduct ablation studies removing FixMatch consistency to quantify its specific contribution versus baseline self-training