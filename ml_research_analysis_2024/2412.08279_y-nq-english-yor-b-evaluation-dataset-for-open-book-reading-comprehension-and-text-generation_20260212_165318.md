---
ver: rpa2
title: "Y-NQ: English-Yor\xF9b\xE1 Evaluation dataset for Open-Book Reading Comprehension\
  \ and Text Generation"
arxiv_id: '2412.08279'
source_url: https://arxiv.org/abs/2412.08279
tags:
- english
- documents
- questions
- dataset
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors introduce Y-NQ, an English-Yor\xF9b\xE1 evaluation\
  \ dataset for open-book reading comprehension and text generation tasks, containing\
  \ 358 questions and answers on 338 English and 208 Yor\xF9b\xE1 documents. The dataset\
  \ addresses the need to evaluate large language models' performance on both high-resource\
  \ (English) and low-resource (Yor\xF9b\xE1) languages in reading comprehension tasks\
  \ that require both understanding and generation capabilities."
---

# Y-NQ: English-Yorùbá Evaluation dataset for Open-Book Reading Comprehension and Text Generation

## Quick Facts
- arXiv ID: 2412.08279
- Source URL: https://arxiv.org/abs/2412.08279
- Reference count: 5
- Introduces Y-NQ dataset with 358 question-answer pairs on 338 English and 208 Yorùbá documents

## Executive Summary
This paper introduces Y-NQ, an evaluation dataset designed to assess large language models' performance on both high-resource (English) and low-resource (Yorùbá) languages in open-book reading comprehension tasks. The dataset contains 358 questions and answers across 338 English and 208 Yorùbá documents, enabling comparative analysis of language models' understanding and generation capabilities. Experiments with GPT-4, o1-mini, and LLaMA-3.1-8b reveal significant performance disparities, with Yorùbá showing consistently worse results than English even when documents are much shorter. The findings highlight challenges in long-context understanding for low-resource languages and reveal that current English LLMs do not extend their reading comprehension capabilities to Yorùbá.

## Method Summary
The study evaluates reading comprehension using the Y-NQ dataset through prompt-based querying of three different LLMs (GPT-4o, o1-mini, and LLaMA-3.1-8b). For each question-document pair, models are prompted to answer questions using provided context documents. Performance is measured using ROUGE-1, ROUGE-2, and ROUGE-L metrics comparing model outputs to reference long answers. The evaluation examines performance differences between English and Yorùbá across varying document lengths, with particular attention to how performance degrades as document length increases.

## Key Results
- English demonstrates a 2.5x performance advantage over Yorùbá on comparable-length documents
- Yorùbá performance dramatically decreases for documents reaching 1,500 words while English performance remains stable
- Consistent performance disparities exist across all three evaluated models (GPT-4o, o1-mini, LLaMA-3.1-8b)

## Why This Works (Mechanism)
The study leverages the fundamental challenge that LLMs face when processing low-resource languages, particularly in long-context scenarios. By creating a controlled evaluation framework with matched question-answer pairs across languages, the research isolates language-specific performance differences from task complexity. The open-book format allows models to demonstrate their ability to extract and synthesize information from lengthy documents, revealing limitations in their cross-lingual generalization capabilities.

## Foundational Learning
- **Open-book reading comprehension**: Understanding how models extract information from provided documents rather than relying on memorized knowledge. Why needed: Essential for evaluating true comprehension vs. recall abilities.
- **ROUGE metrics**: Automated evaluation metrics for comparing generated text against reference answers. Why needed: Provides quantitative assessment of generation quality across languages.
- **Low-resource language processing**: Challenges specific to languages with limited training data. Why needed: Context for understanding why Yorùbá performance lags behind English.
- **Long-context evaluation**: Testing model performance on lengthy documents. Why needed: Reveals limitations in attention mechanisms and processing capacity.
- **Cross-lingual generalization**: Models' ability to transfer capabilities across languages. Why needed: Determines if English-trained models can handle other languages effectively.

## Architecture Onboarding

**Component Map:** Y-NQ Dataset -> Question-Prompt Generator -> LLM API -> ROUGE Evaluator -> Performance Metrics

**Critical Path:** Dataset → Question → Prompt → Model → Answer → ROUGE Score

**Design Tradeoffs:** The open-book format provides context but may mask some memorization capabilities; using ROUGE metrics enables automation but may miss semantic nuances.

**Failure Signatures:** Models producing off-topic responses, failing to utilize provided context, or showing dramatic performance drops at specific document lengths indicate processing limitations.

**First Experiments:**
1. Run a single question-document pair through each model to verify prompt format and API connectivity
2. Calculate ROUGE scores for a small subset to validate the evaluation pipeline
3. Compare model outputs qualitatively to identify systematic error patterns

## Open Questions the Paper Calls Out
### Open Question 1
What are the specific factors contributing to the significant performance drop of LLMs on Yorùbá documents reaching 1,500 words, and can these be addressed through targeted model adaptations?
- Basis in paper: [explicit] The paper explicitly states that "Yorùbá decreases performance dramatically for documents that reach 1500 words while English performance is barely affected at that length."
- Why unresolved: The paper identifies the performance disparity but does not investigate the underlying causes or potential solutions.
- What evidence would resolve it: Experiments comparing model performance across different document lengths, analyzing attention patterns, or testing models specifically trained on longer Yorùbá documents would provide insights into the root causes and potential solutions.

### Open Question 2
How does the quality of Yorùbá Wikipedia articles (in terms of length, structure, and accuracy) compare to English articles on the same topics, and what impact does this have on model performance?
- Basis in paper: [explicit] The paper mentions that "many articles have a significant amount of English content" and "Several documents also contained errors, such as incorrect spelling, ungrammatical sentences."
- Why unresolved: While the paper acknowledges quality issues in Yorùbá articles, it doesn't systematically analyze how these quality differences affect model performance or quantify the extent of these discrepancies.
- What evidence would resolve it: A comprehensive quality assessment of both English and Yorùbá Wikipedia articles, coupled with controlled experiments using cleaned datasets, would reveal the impact of article quality on model performance.

### Open Question 3
Can the performance gap between English and Yorùbá be reduced through multilingual training approaches or domain-specific adaptations?
- Basis in paper: [inferred] The paper demonstrates a consistent performance disparity between the two languages but doesn't explore potential solutions or adaptations.
- Why unresolved: The experiments only evaluate existing models without investigating whether performance can be improved through alternative training strategies or adaptations.
- What evidence would resolve it: Training and evaluating models using multilingual approaches, domain-specific fine-tuning, or cross-lingual transfer learning would provide insights into whether the performance gap can be narrowed.

## Limitations
- The exact model versions and API endpoints used are not specified, affecting reproducibility
- The dataset size of 358 questions may limit statistical power for fine-grained analysis
- Findings are specific to the open-book reading comprehension task and may not generalize to other NLP tasks

## Confidence
- General performance disparity claim: High
- Quantitative performance ratios (2.5x): Medium (due to unknown evaluation parameters)
- Document length analysis: Medium (limited sample sizes for long documents)
- Generalization to other tasks: Low

## Next Checks
1. Replicate the experiment using specified model versions and exact ROUGE implementation parameters to verify the 2.5x performance ratio
2. Conduct ablation studies comparing performance on matched-length document pairs with balanced vocabulary complexity across languages
3. Test additional low-resource languages to determine if the performance pattern is specific to Yorùbá or representative of low-resource language challenges generally