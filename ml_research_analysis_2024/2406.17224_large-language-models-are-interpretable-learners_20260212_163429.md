---
ver: rpa2
title: Large Language Models are Interpretable Learners
arxiv_id: '2406.17224'
source_url: https://arxiv.org/abs/2406.17224
tags:
- learning
- interpretable
- arxiv
- programs
- program
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces LLM-Symbolic Programs (LSPs), a framework\
  \ that combines the interpretability of symbolic rules with the expressiveness of\
  \ large language models (LLMs) to create interpretable predictive models. The core\
  \ idea is to use a minimal Domain-Specific Language (DSL) with two operators\u2014\
  prompted-LLM and conditional branching\u2014to construct tree-structured programs\
  \ where LLM modules provide interpretable decision rules."
---

# Large Language Models are Interpretable Learners

## Quick Facts
- arXiv ID: 2406.17224
- Source URL: https://arxiv.org/abs/2406.17224
- Authors: Ruochen Wang; Si Si; Felix Yu; Dorothea Wiesmann; Cho-Jui Hsieh; Inderjit Dhillon
- Reference count: 40
- One-line primary result: LSP framework combines LLM modules with symbolic rules to achieve 95.67% accuracy on vision tasks while maintaining interpretability

## Executive Summary
This paper introduces LLM-Symbolic Programs (LSPs), a framework that combines the interpretability of symbolic rules with the expressiveness of large language models (LLMs) to create interpretable predictive models. The core idea is to use a minimal Domain-Specific Language (DSL) with two operators—prompted-LLM and conditional branching—to construct tree-structured programs where LLM modules provide interpretable decision rules. A divide-and-conquer learning algorithm incrementally builds these programs by optimizing LLM modules to summarize predictive rules from data subsets. The authors evaluate LSPs on IL-Bench, a benchmark of diverse tasks including synthetic and real-world datasets across vision and text modalities.

## Method Summary
The LSP framework uses a DSL with three operators: input, conditional branching (switch), and LLM module. The divide-and-conquer algorithm builds programs incrementally by selecting nodes with the largest prediction errors, then trains LLM modules on data subsets using prompt optimization. This creates interpretable decision rules encoded as natural language descriptions combined with symbolic rules. The framework is evaluated on IL-Bench, which includes synthetic tasks with known decision rules and real-world vision/text datasets.

## Key Results
- LSPs achieve 95.67% average accuracy on vision tasks, outperforming traditional interpretable methods like ProtoTree and Decision Trees by over 10%
- LSPs demonstrate superior transferability to humans and other LLMs, with human raters successfully reproducing model predictions based on learned rules
- LSPs show strong generalization to out-of-distribution data and outperform prompt optimization methods by up to 20%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM modules can act as interpretable building blocks when driven by natural language prompts
- Mechanism: Each prompt defines a conditional probabilistic sub-model (CPM) that shares the LLM's architecture but is specialized for a specific task. Prompt optimization becomes interpretable learning because the prompt itself is human-readable
- Core assumption: The LLM's pretraining on next-token prediction has implicitly learned a vast space of interpretable conditional distributions that can be accessed via natural language instructions
- Evidence anchors: [abstract] "the pretrained LLM with natural language prompts provides a massive set of interpretable modules that can transform raw input into natural language concepts"

### Mechanism 2
- Claim: The divide-and-conquer learning algorithm enables effective discovery of complex decision rules
- Mechanism: The tree search framework incrementally builds the program by expanding the node with the largest prediction errors, while each LLM module is trained only on its data subset
- Core assumption: Error count is an effective metric for node selection that reliably identifies where program improvements will have the greatest impact
- Evidence anchors: [section] "we use error count as our scoring function. This metric, accounting for both the error rate and the size of the data subset each node handles, provides a simple yet empirically effective approach"

### Mechanism 3
- Claim: The resulting programs achieve superior transferability to humans and other LLMs
- Mechanism: Since the knowledge is encoded as natural language descriptions combined with symbolic rules, humans can understand and reproduce the decision process
- Core assumption: Natural language descriptions of decision rules are sufficiently clear for human comprehension without domain expertise
- Evidence anchors: [abstract] "as the knowledge learned by LSP is a combination of natural language descriptions and symbolic rules, it is easily transferable to humans (interpretable), and other LLMs"

## Foundational Learning

- Concept: Domain-Specific Language (DSL) design for interpretable programs
  - Why needed here: Provides the formal grammar that constrains how LLM modules and conditional branching can be combined into valid programs
  - Quick check question: What are the three components of the LSP DSL and how do they relate to program structure?

- Concept: Prompt optimization as interpretable learning
  - Why needed here: Connects the LLM's pretraining objective to the task of finding interpretable decision rules through natural language instructions
  - Quick check question: How does optimizing prompts in natural language space differ from traditional neural network training in terms of interpretability?

- Concept: Tree search algorithms for program synthesis
  - Why needed here: Enables systematic exploration of the program space to find effective combinations of LLM modules and branching structure
  - Quick check question: What metric does LSP use to select which node to expand next, and why is this effective?

## Architecture Onboarding

- Component map: Data → Program Search (node selection → LLM module learning → program extension) → Evaluation (accuracy + interpretability assessment)
- Critical path: Data → Program Search (node selection → LLM module learning → program extension) → Evaluation (accuracy + interpretability assessment)
- Design tradeoffs: Expressiveness vs interpretability (more complex programs may be more accurate but harder to understand), search efficiency vs completeness (beam search vs exhaustive search), LLM capability vs cost (stronger LLMs may learn better programs but at higher computational expense)
- Failure signatures: Programs that overfit to training data, LLM modules that produce inconsistent or incorrect rules, search that gets stuck in local minima, programs that are too complex for human interpretation
- First 3 experiments:
  1. Implement the basic LSP framework on a simple synthetic dataset (DT-Easy) to verify the learning algorithm works end-to-end
  2. Compare LSP performance against a single LLM module (no tree structure) on the same dataset to validate the benefit of explicit program structure
  3. Test the transferability of learned programs to human raters using a small subset of IL-Bench-Vision tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LSP compare when using different LLMs as both the learner and inference engines?
- Basis in paper: [explicit] The paper discusses using various LLMs like GPT-3.5, GPT-4, and Gemini for both learning and inference in LSP, noting that GPT-4 consistently outperforms others.
- Why unresolved: The paper provides some comparative analysis but does not offer a comprehensive evaluation of all possible combinations of learner and inference LLMs.
- What evidence would resolve it: A systematic study evaluating LSP performance across all combinations of learner and inference LLMs, including metrics like accuracy and interpretability.

### Open Question 2
- Question: What are the limitations of LSP in handling tasks that require more complex decision-making processes?
- Basis in paper: [inferred] The paper mentions that LSP uses a divide-and-conquer approach and a minimalist DSL, which may limit its ability to handle highly complex decision-making tasks.
- Why unresolved: The paper does not provide detailed experiments or examples where LSP struggles with complex tasks, nor does it discuss potential improvements.
- What evidence would resolve it: Experiments testing LSP on tasks with increasing complexity and analyzing where it fails or requires modifications.

### Open Question 3
- Question: How does LSP perform on out-of-distribution (OOD) data compared to traditional interpretable models?
- Basis in paper: [explicit] The paper constructs an OOD dataset for IL-Bench-Vision tasks and shows that LSP has strong transferability to OOD data, outperforming ProtoTree.
- Why unresolved: While the paper demonstrates LSP's performance on a specific OOD dataset, it does not compare its robustness across various types of domain shifts or with other interpretable models.
- What evidence would resolve it: A comprehensive study evaluating LSP's performance on multiple OOD datasets with different types of domain shifts, comparing it to other interpretable models.

### Open Question 4
- Question: What are the computational costs associated with training and inference in LSP, and how do they scale with dataset size and complexity?
- Basis in paper: [explicit] The paper mentions that LSP incurs comparable search and inference costs to prompt optimization baselines but does not provide detailed scalability analysis.
- Why unresolved: The paper lacks a thorough analysis of how computational costs scale with increasing dataset size and complexity, which is crucial for practical applications.
- What evidence would resolve it: Experiments measuring training and inference times for LSP across datasets of varying sizes and complexities, along with a scalability analysis.

## Limitations
- The paper's claims about human interpretability rely on indirect evidence rather than direct human studies, making it unclear how well humans would actually understand complex LSPs in practice
- The evaluation focuses heavily on synthetic datasets with known decision rules, which may not fully capture real-world complexity where ground truth interpretations are ambiguous
- The divide-and-conquer algorithm's reliance on error count as a node selection metric lacks theoretical justification and may not scale well to high-dimensional data or more complex decision boundaries

## Confidence

**High confidence**: The basic LSP framework combining LLM modules with symbolic programming is technically sound and the reported accuracy improvements over traditional interpretable methods are well-supported by the IL-Bench evaluation

**Medium confidence**: The claims about human interpretability and transferability are plausible given the natural language-based program representation, but lack direct experimental validation

**Low confidence**: The assertion that LSPs achieve superior generalization to out-of-distribution data is based on limited experiments and may not hold across diverse real-world scenarios

## Next Checks

1. Conduct direct human studies with domain experts and non-experts to validate that LSPs are genuinely interpretable and that humans can reproduce predictions following learned rules

2. Test LSPs on real-world datasets without known decision rules (e.g., medical diagnosis, financial fraud detection) to evaluate performance in ambiguous interpretation scenarios

3. Systematically evaluate the scalability of the divide-and-conquer algorithm by testing on increasingly complex synthetic datasets and analyzing how the tree depth and LLM module complexity evolve