---
ver: rpa2
title: Can General-Purpose Large Language Models Generalize to English-Thai Machine
  Translation ?
arxiv_id: '2410.17145'
source_url: https://arxiv.org/abs/2410.17145
tags:
- translation
- answer
- system
- text
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the performance of general-purpose large
  language models (LLMs) versus specialized translation models in low-resource English-Thai
  machine translation under computational constraints. The authors evaluate Llama-3
  and NLLB models on both code-switching and standard translation datasets, testing
  various quantization levels (2-8 bits).
---

# Can General-Purpose Large Language Models Generalize to English-Thai Machine Translation ?

## Quick Facts
- arXiv ID: 2410.17145
- Source URL: https://arxiv.org/abs/2410.17145
- Reference count: 6
- Key outcome: Specialized translation models (NLLB) outperform general-purpose LLMs (Llama-3) in low-resource English-Thai translation under computational constraints, with code-switching translation showing greater resilience to quantization than standard translation.

## Executive Summary
This paper investigates the performance of general-purpose large language models versus specialized translation models in low-resource English-Thai machine translation under computational constraints. The authors evaluate Llama-3 and NLLB models on both code-switching and standard translation datasets, testing various quantization levels (2-8 bits). Their key finding is that specialized NLLB models consistently outperform the general-purpose Llama-3, with NLLB-3.3B and NLLB-600M achieving better results despite using 2.35x and 10.81x less VRAM respectively. Under strict constraints like 4-bit quantization, Llama-3 fails to translate effectively, while NLLB maintains performance. The study also reveals that code-switching translation shows greater resilience to quantization than standard translation, suggesting that preserving English medical terms during quantization helps maintain performance. These results underscore the importance of specialized models for resource-constrained settings.

## Method Summary
The study evaluates English-Thai translation performance using two model families: general-purpose Llama-3 (8B parameters) and specialized NLLB models (3.3B and 600M parameters). Models are tested across quantization levels from 2 to 8 bits using GPTQ, with inference on both A100 and RTX 4090 GPUs. The evaluation uses two datasets: a medical code-switching dataset (MixMed) and a standard translation dataset (SCB). Performance is measured using BLEU, METEOR, CER for standard translation, and CS-F1 for code-switching. An LLM-as-a-judge approach analyzes failure modes, categorizing errors into "Gibberish," "Meaning changed," and "Keywords not preserved."

## Key Results
- NLLB-3.3B and NLLB-600M outperform Llama-3-8B on most metrics despite using 2.35x and 10.81x less VRAM respectively
- Under 4-bit quantization, Llama-3 fails to translate effectively while NLLB maintains performance
- Code-switching translation shows greater resilience to quantization than standard translation, with less performance degradation across BLEU, CER, and METEOR metrics
- Despite higher "Forgetting to preserve" errors, NLLB models achieve better CS-F1 scores, highlighting the importance of task-specific metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Specialized translation models (NLLB) outperform general-purpose LLMs (Llama-3) in low-resource English-Thai translation under computational constraints.
- Mechanism: Specialized models are trained specifically for translation tasks and have architectural optimizations for multilingual processing, allowing them to maintain performance even with aggressive quantization (down to 2 bits).
- Core assumption: Translation-specific training and architecture provide efficiency advantages that general-purpose models cannot match, even when the general models are larger in parameter count.
- Evidence anchors:
  - [abstract] "under more strict computational constraints, such as 4-bit quantization, LLMs fail to translate effectively. In contrast, specialized models, with comparable or lower computational requirements, consistently outperform LLMs."
  - [section] "NLLB-3.3B and NLLB-600M outperform Llama-3 8B on most metrics, despite using 2.35x and 10.81x less VRAM, respectively."
  - [corpus] Weak evidence - the corpus provides related papers on translation but doesn't directly address the efficiency comparison between specialized and general models.
- Break condition: If the computational constraints are relaxed (e.g., full precision inference), or if the task requires capabilities beyond translation that specialized models lack.

### Mechanism 2
- Claim: Code-switching translation shows greater resilience to quantization than standard translation.
- Mechanism: During quantization, complex Thai vocabulary is lost early, but English medical terms (which are rewarded in code-switching tasks) are better preserved, maintaining performance on code-switching metrics.
- Core assumption: The preservation of English terms during quantization is more valuable for code-switching tasks than perfect Thai translation, and the quantization process differentially affects vocabulary based on language.
- Evidence anchors:
  - [abstract] "Interestingly, CS results show greater resilience to quantization than SCB results. Across BLEU, CER, and METEOR metrics, CS translation results experience less degradation than SCB results when compared against the full-precision baseline."
  - [section] "This may be due to the early loss of complex Thai vocabulary during quantization, while complex English vocabulary, rewarded in the CS task, is better preserved."
  - [corpus] Weak evidence - corpus doesn't directly address quantization effects on code-switching vs standard translation.
- Break condition: If the quantization process changes to preserve all vocabulary equally, or if the task no longer rewards English term preservation.

### Mechanism 3
- Claim: Task-specific metrics are critical for evaluating translation performance, especially for code-switching tasks.
- Mechanism: Standard translation metrics may not capture the success of code-switching translation where preserving English terms is as important as accurate Thai translation. NLLB models may have higher "Forgetting to preserve" errors but still achieve better CS-F1 scores.
- Core assumption: The code-switching task has different success criteria than standard translation, and metrics must reflect the importance of term preservation alongside translation quality.
- Evidence anchors:
  - [abstract] "Despite higher errors in the 'Forgetting to preserve' category, NLLB models perform better on the CS-F1 metric, Table 1, highlighting the importance of task-specific metrics."
  - [section] "This suggests an alternative failure mode in CS translation, where top models first lose the ability to preserve medical keywords, then to translate accurately, and finally to translate at all."
  - [corpus] Weak evidence - corpus doesn't directly address the need for task-specific metrics in code-switching evaluation.
- Break condition: If the task definition changes to prioritize translation quality over term preservation, or if standard metrics are modified to adequately capture code-switching success.

## Foundational Learning

- Concept: Quantization in neural networks
  - Why needed here: The study evaluates models at different quantization levels (2-8 bits), which significantly affects performance. Understanding how quantization works and its trade-offs is essential to interpret the results.
  - Quick check question: What happens to model performance as you decrease the number of bits in quantization, and why does this occur?

- Concept: BLEU, METEOR, and CER metrics
  - Why needed here: These are the standard metrics used to evaluate translation quality. BLEU measures n-gram overlap, METEOR considers synonyms and stemming, and CER measures character error rate. Understanding these metrics is crucial for interpreting the results.
  - Quick check question: How would a model that preserves English terms but translates Thai poorly score on BLEU vs CS-F1 metrics?

- Concept: Code-switching in natural language processing
  - Why needed here: The study evaluates both standard translation and code-switching translation, where English medical terms are preserved in Thai sentences. Understanding code-switching is essential to grasp why certain models perform better on this task.
  - Quick check question: Why might a model that performs poorly on standard translation metrics still perform well on code-switching tasks?

## Architecture Onboarding

- Component map: Datasets (MixMed, SCB) -> Models (Llama-3-8B, NLLB-3.3B, NLLB-600M) -> Quantization (2-8 bits) -> Metrics (BLEU, METEOR, CER, CS-F1) -> LLM-as-a-judge analysis
- Critical path: 1) Load dataset, 2) Load and quantize model, 3) Run inference, 4) Calculate metrics, 5) LLM-as-a-judge failure analysis
- Design tradeoffs: General-purpose models offer broader capabilities but are less efficient for specialized tasks. Specialized models are more efficient but may lack capabilities beyond their training scope. Quantization reduces memory usage but degrades performance, with varying effects across tasks.
- Failure signatures: For standard translation: increasing "Gibberish" and "Meaning changed" errors as quantization increases. For code-switching: initial increase in "Keywords not preserved" errors followed by "Gibberish" errors at extreme quantization.
- First 3 experiments:
  1. Run Llama-3-8B on both datasets at full precision to establish baseline performance.
  2. Quantize Llama-3-8B to 4 bits and compare performance degradation between datasets.
  3. Run NLLB-3.3B on both datasets and compare efficiency (VRAM usage) against Llama-3 performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on English-Thai translation when trained or fine-tuned with additional low-resource language pairs beyond the English-Thai dataset?
- Basis in paper: [explicit] The paper compares Llama-3 with specialized NLLB models, but does not explore whether fine-tuning Llama-3 on additional low-resource language pairs improves its performance.
- Why unresolved: The study focuses on a single low-resource language pair (English-Thai) and does not investigate the impact of multilingual fine-tuning on Llama-3's performance.
- What evidence would resolve it: Comparative experiments showing the performance of Llama-3 before and after fine-tuning on multiple low-resource language pairs, alongside NLLB models.

### Open Question 2
- Question: What is the effect of using different quantization algorithms (e.g., GPTQ vs. AWQ) on the performance of LLMs in low-resource translation tasks?
- Basis in paper: [inferred] The paper uses GPTQ for quantization but does not compare its effectiveness with other quantization methods.
- Why unresolved: The study does not explore alternative quantization algorithms that might yield different performance outcomes for LLMs in low-resource settings.
- What evidence would resolve it: Comparative results of LLM performance using different quantization algorithms (e.g., GPTQ vs. AWQ) across various bit levels in low-resource translation tasks.

### Open Question 3
- Question: How do task-specific metrics (e.g., CS-F1) correlate with general translation quality metrics (e.g., BLEU, METEOR) in evaluating code-switching translation tasks?
- Basis in paper: [explicit] The paper highlights the importance of task-specific metrics like CS-F1 but does not explore their correlation with general metrics.
- Why unresolved: The study emphasizes task-specific metrics but does not investigate their relationship with traditional translation quality metrics.
- What evidence would resolve it: A correlation analysis between task-specific metrics (e.g., CS-F1) and general translation quality metrics (e.g., BLEU, METEOR) across multiple code-switching datasets.

### Open Question 4
- Question: What are the computational trade-offs between using larger specialized models (e.g., NLLB-3.3B) versus smaller quantized models (e.g., Llama-3-8B at 4-bit) in real-world deployment scenarios?
- Basis in paper: [inferred] The paper discusses memory usage and performance but does not provide a detailed analysis of computational trade-offs in deployment.
- Why unresolved: The study does not explore the practical implications of deploying larger specialized models versus smaller quantized models in terms of cost, latency, and scalability.
- What evidence would resolve it: A comprehensive analysis of computational costs, latency, and scalability metrics for deploying larger specialized models versus smaller quantized models in real-world applications.

## Limitations

- The evaluation focuses on a specific language pair (English-Thai) and domain (medical translation), which may not generalize to other language pairs or domains
- The study does not explore whether the performance gap between specialized and general models persists when computational resources are abundant
- The code-switching evaluation relies on specific metrics (CS-F1) that may not capture all aspects of translation quality

## Confidence

- High confidence: The importance of task-specific metrics for code-switching evaluation, as evidenced by the discrepancy between standard metrics and CS-F1 scores
- Medium confidence: The superiority of specialized NLLB models over general-purpose Llama-3 under computational constraints, supported by direct performance comparisons but limited to the English-Thai medical domain
- Medium confidence: The greater resilience of code-switching translation to quantization, based on observed performance patterns but with speculative explanations for the underlying mechanism

## Next Checks

1. **Cross-domain validation**: Evaluate the same model comparison (NLLB vs. Llama-3) on non-medical English-Thai translation tasks to determine if the specialized model advantage persists across domains. This would involve testing on general web text, news articles, or conversational data to assess generalizability.

2. **Ablation study on quantization**: Conduct a detailed analysis of vocabulary preservation across different quantization levels by examining which specific words are lost at each bit depth. This would involve tracking the frequency and language of lost vocabulary to validate the hypothesis that English medical terms are preferentially preserved.

3. **Computation-unconstrained comparison**: Test both model families at full precision with maximum available resources to determine if the performance gap narrows or reverses when computational constraints are removed. This would clarify whether the specialized model advantage is primarily driven by efficiency or by fundamental architectural differences.