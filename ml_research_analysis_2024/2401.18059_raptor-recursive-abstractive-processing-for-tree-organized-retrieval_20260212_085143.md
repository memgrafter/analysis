---
ver: rpa2
title: 'RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval'
arxiv_id: '2401.18059'
source_url: https://arxiv.org/abs/2401.18059
tags:
- raptor
- tree
- nodes
- retrieval
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RAPTOR improves retrieval-augmented language modeling by constructing
  a hierarchical tree of recursively summarized text chunks, enabling multi-level
  context retrieval beyond short contiguous passages. It clusters chunks using Gaussian
  Mixture Models, summarizes them with a language model, and repeats the process to
  build layers of increasing abstraction.
---

# RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval

## Quick Facts
- arXiv ID: 2401.18059
- Source URL: https://arxiv.org/abs/2401.18059
- Reference count: 39
- Improves retrieval-augmented language modeling by constructing a hierarchical tree of recursively summarized text chunks, enabling multi-level context retrieval beyond short contiguous passages.

## Executive Summary
RAPTOR introduces a novel approach to retrieval-augmented language modeling by constructing a hierarchical tree of recursively summarized text chunks. The system clusters text using Gaussian Mixture Models, summarizes clusters with a language model, and repeats this process to build layers of increasing abstraction. At inference, RAPTOR uses a collapsed tree approach to retrieve relevant nodes across all layers, enabling retrieval of information at multiple levels of abstraction. Experiments show RAPTOR outperforms standard retrieval methods (BM25, DPR) on QASPER, QuALITY, and NarrativeQA datasets, with state-of-the-art results including a 20% improvement on QuALITY accuracy when coupled with GPT-4.

## Method Summary
RAPTOR segments text into 100-token chunks, embeds them using SBERT, and recursively clusters and summarizes them to build a hierarchical tree structure. The clustering uses Gaussian Mixture Models with soft assignments, allowing text chunks to belong to multiple clusters. At each level, GPT-3.5-turbo summarizes the clusters, creating parent nodes that capture higher-level themes. During retrieval, RAPTOR uses a collapsed tree approach that flattens all nodes into a single layer and retrieves based on cosine similarity to the query, allowing flexible selection of nodes at the appropriate level of granularity.

## Key Results
- RAPTOR improves retrieval accuracy on QASPER, QuALITY, and NarrativeQA datasets compared to BM25 and DPR baselines
- The collapsed tree retrieval approach outperforms tree traversal by allowing flexible selection of nodes at appropriate abstraction levels
- Non-leaf nodes significantly contribute to retrieval quality, validating the hierarchical summarization strategy
- Achieves state-of-the-art results, including 20% improvement on QuALITY accuracy when coupled with GPT-4

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAPTOR's hierarchical tree structure enables retrieval of information at multiple levels of abstraction, improving performance on complex, multi-step reasoning tasks.
- Mechanism: The system recursively clusters and summarizes text chunks, creating parent nodes that capture higher-level themes and child nodes that retain granular details. During retrieval, this allows the model to access both detailed context and broader thematic information simultaneously.
- Core assumption: Information relevant to answering a question may exist at different levels of abstraction within the text, and retrieving only leaf-level chunks misses important context.
- Evidence anchors:
  - [abstract] "We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up."
  - [section] "Building on the idea that long texts often present subtopics and hierarchical structures, RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details."
  - [corpus] "The first question we examine is 'How does Cinderella find a happy ending?', a multi-hop question best answered by synthesizing information from various text segments. RAPTOR's context succinctly describes Cinderella's journey to happiness, while DPR's leaf nodes primarily focus on her initial transformation."

### Mechanism 2
- Claim: Soft clustering using Gaussian Mixture Models (GMMs) allows text chunks to belong to multiple clusters, capturing the multi-faceted nature of semantic content.
- Mechanism: GMMs model text embeddings as mixtures of Gaussian distributions, assigning probabilities of cluster membership rather than hard assignments. This reflects the reality that a single text chunk may be relevant to multiple topics or themes.
- Core assumption: Text segments often contain information relevant to various topics, warranting their inclusion in multiple summaries.
- Evidence anchors:
  - [section] "One of the unique aspects of our clustering approach is the use of soft clustering, where nodes can belong to multiple clusters without requiring a fixed number of clusters."
  - [section] "Our clustering algorithm is based on Gaussian Mixture Models (GMMs), an approach that offers both flexibility and a probabilistic framework."
  - [corpus] "The clustering algorithm is based on Gaussian Mixture Models (GMMs), an approach that offers both flexibility and a probabilistic framework. GMMs assume that data points are generated from a mixture of several Gaussian distributions."

### Mechanism 3
- Claim: The collapsed tree retrieval approach outperforms tree traversal by allowing flexible selection of nodes at the appropriate level of granularity for each question.
- Mechanism: Instead of traversing the tree layer-by-layer, the collapsed tree approach flattens all nodes into a single layer and retrieves based on cosine similarity to the query. This allows retrieval of nodes that best match the question's required level of detail.
- Core assumption: Different questions require different levels of abstraction, and a fixed traversal pattern cannot optimally adapt to this variation.
- Evidence anchors:
  - [section] "The collapsed tree approach offers a simpler way to search for relevant information by considering all nodes in the tree simultaneously... Instead of going layer-by-layer, this method flattens the multi-layered tree into a single layer, essentially bringing all the nodes onto the same level for comparison."
  - [section] "We tested both approaches on 20 stories from the QASPER dataset. Figure 3 shows the performance of tree traversal with different top-k values, and collapsed tree with different context lengths. The collapsed tree approach consistently performs better."
  - [corpus] "We believe collapsed tree retrieval is better due to offering greater flexibility than tree traversal; i.e., by searching through all the nodes simultaneously, it retrieves information that is at the correct level of granularity for a given question."

## Foundational Learning

- Concept: Vector embeddings and similarity search
  - Why needed here: RAPTOR relies on SBERT embeddings to represent text chunks numerically, then uses cosine similarity to find relevant nodes during retrieval.
  - Quick check question: Given two text passages, how would you determine which is more semantically similar to a query passage using vector embeddings?

- Concept: Clustering algorithms and evaluation metrics
  - Why needed here: RAPTOR uses Gaussian Mixture Models for clustering text embeddings, with BIC for model selection. Understanding clustering concepts is essential for grasping how the tree structure is built.
  - Quick check question: What is the difference between hard clustering and soft clustering, and when would soft clustering be more appropriate?

- Concept: Language model prompting and summarization
  - Why needed here: RAPTOR uses GPT-3.5-turbo to generate summaries of clustered text chunks. Understanding how to effectively prompt LLMs for summarization is crucial for implementing and debugging the system.
  - Quick check question: How would you design a prompt to instruct an LLM to summarize multiple text passages while preserving key details?

## Architecture Onboarding

- Component map: Raw text corpus -> Text chunking (100-token chunks) -> SBERT embedding -> Recursive clustering (GMM) and summarization (GPT-3.5-turbo) -> Hierarchical tree building -> Query embedding -> Collapsed tree retrieval -> Concatenated text output

- Critical path:
  1. Chunk text and create embeddings
  2. Cluster embeddings using GMM
  3. Summarize clusters using LLM
  4. Repeat until root node reached
  5. For query: embed and find top-k similar nodes across all layers
  6. Concatenate retrieved text and pass to QA model

- Design tradeoffs:
  - Tree depth vs. computational cost: Deeper trees capture more abstraction but require more LLM calls
  - Cluster size vs. summarization quality: Larger clusters may lose important details, smaller clusters may not capture broader themes
  - Token limit vs. retrieval completeness: Higher limits allow more context but increase computational cost

- Failure signatures:
  - Poor retrieval performance: Check if clustering is grouping semantically similar content, or if embeddings are of low quality
  - Summarization hallucinations: Monitor hallucination rates in generated summaries, especially for critical information
  - Computational bottlenecks: Tree building time scales linearly with document length, but may become prohibitive for very large corpora

- First 3 experiments:
  1. Verify tree building: Run on a small text sample (e.g., 500 tokens), inspect the resulting tree structure and ensure clustering/summarization is working as expected
  2. Compare retrieval methods: Test both tree traversal and collapsed tree approaches on a small dataset to confirm the claimed performance difference
  3. Ablation study: Implement the recency-based tree baseline and compare its performance against RAPTOR on a simple QA task to validate the importance of the clustering mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of clustering algorithm affect the performance of RAPTOR, and could alternative clustering methods like k-means or hierarchical clustering improve the model's retrieval capabilities?
- Basis in paper: [explicit] The paper uses Gaussian Mixture Models (GMMs) for clustering, but does not explore alternative clustering methods or compare their performance.
- Why unresolved: The paper does not provide a comparative analysis of different clustering algorithms and their impact on RAPTOR's performance, leaving the optimal choice of clustering method uncertain.
- What evidence would resolve it: Conducting experiments comparing RAPTOR's performance with different clustering algorithms (e.g., k-means, hierarchical clustering) and analyzing the impact on retrieval accuracy and efficiency.

### Open Question 2
- Question: How does the summarization model's performance affect the overall effectiveness of RAPTOR, and could using different summarization models (e.g., BART, Pegasus) lead to improvements in retrieval accuracy?
- Basis in paper: [explicit] The paper uses GPT-3.5-turbo for summarization, but does not explore the impact of using different summarization models or compare their performance.
- Why unresolved: The paper does not provide a comparative analysis of different summarization models and their impact on RAPTOR's performance, leaving the optimal choice of summarization model uncertain.
- What evidence would resolve it: Conducting experiments comparing RAPTOR's performance with different summarization models and analyzing the impact on retrieval accuracy and efficiency.

### Open Question 3
- Question: How does the choice of embedding model affect RAPTOR's performance, and could using other embedding models (e.g., Sentence-BERT, SBERT) lead to improvements in retrieval accuracy?
- Basis in paper: [explicit] The paper uses SBERT for embedding, but does not explore the impact of using different embedding models or compare their performance.
- Why unresolved: The paper does not provide a comparative analysis of different embedding models and their impact on RAPTOR's performance, leaving the optimal choice of embedding model uncertain.
- What evidence would resolve it: Conducting experiments comparing RAPTOR's performance with different embedding models and analyzing the impact on retrieval accuracy and efficiency.

## Limitations

- Scalability concerns: The recursive nature of tree building may become computationally prohibitive for very large corpora or documents with extreme length
- Hallucination propagation: The recursive summarization process relies on language models that may hallucinate, with potential compounding of errors through tree layers
- Parameter sensitivity: Critical parameters such as number of clusters and covariance type are not thoroughly explored, with no demonstration of robustness across different settings

## Confidence

- High confidence: Empirical performance improvements over BM25 and DPR on the tested datasets, and the effectiveness of the collapsed tree retrieval approach versus tree traversal
- Medium confidence: The generalizability of the hierarchical abstraction approach to different domains and document types, and the scalability claims for larger corpora
- Low confidence: The optimal parameter settings for different document types and the long-term stability of retrieval quality when hallucinations compound through multiple summarization layers

## Next Checks

1. **Ablation Study on Tree Depth**: Systematically vary the maximum tree depth and measure the impact on retrieval quality and computational cost to clarify the optimal balance between abstraction levels and practical constraints.

2. **Hallucination Analysis**: Implement systematic hallucination detection in generated summaries at each tree layer and measure the correlation between hallucination rates and retrieval performance degradation.

3. **Cross-Domain Evaluation**: Test RAPTOR on diverse document types (technical documentation, legal texts, scientific papers) to assess whether the hierarchical abstraction approach provides consistent benefits across different structural characteristics and vocabulary domains.