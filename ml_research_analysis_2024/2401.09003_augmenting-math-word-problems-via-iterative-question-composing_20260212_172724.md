---
ver: rpa2
title: Augmenting Math Word Problems via Iterative Question Composing
arxiv_id: '2401.09003'
source_url: https://arxiv.org/abs/2401.09003
tags:
- arxiv
- math
- mmiqc
- problem
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces MMIQC, a dataset designed to improve mathematical
  reasoning in large language models (LLMs). It combines processed web data from Mathematics
  Stack Exchange with synthetic question-response pairs generated via multiple augmentation
  methods, including a novel Iterative Question Composing (IQC) technique.
---

# Augmenting Math Word Problems via Iterative Question Composing

## Quick Facts
- arXiv ID: 2401.09003
- Source URL: https://arxiv.org/abs/2401.09003
- Authors: Haoxiong Liu; Yifan Zhang; Yifan Luo; Andrew Chi-Chih Yao
- Reference count: 7
- Key outcome: MMIQC dataset with Iterative Question Composing (IQC) improves LLM math performance, with Qwen-72B-MMIQC achieving 45.0% accuracy on MATH benchmark, exceeding previous open-source state-of-the-art by 8.2%

## Executive Summary
This work introduces MMIQC, a dataset designed to improve mathematical reasoning in large language models (LLMs). It combines processed web data from Mathematics Stack Exchange with synthetic question-response pairs generated via multiple augmentation methods, including a novel Iterative Question Composing (IQC) technique. IQC iteratively composes complex math problems from seed problems using GPT-4, with rejection sampling via GPT-3.5 to ensure quality. Models fine-tuned on MMIQC consistently outperform their counterparts on the MATH benchmark.

## Method Summary
The MMIQC dataset is constructed by combining three main sources: filtered Mathematics Stack Exchange data (1.2M samples), MetaMathQA subset (203.7K samples), and augmented data from multiple methods including IQC (55.1K samples). IQC works by iteratively composing new problems from seed problems using GPT-4, with each iteration adding complexity while maintaining the core reasoning structure. Quality control is implemented through rejection sampling using GPT-3.5, which verifies that composed problems maintain mathematical correctness. Base models (Mistral-7B, Llemma-34B, DeepSeek-67B, Qwen-72B) are fine-tuned on MMIQC for 1 epoch using 1e-5 learning rate, 3% warm-up ratio, linear learning rate schedule, BFloat16 format, and DeepSpeed Zero-3 Stage.

## Key Results
- Qwen-72B-MMIQC achieves 45.0% accuracy on MATH benchmark, exceeding previous open-source state-of-the-art by 8.2%
- MMIQC models outperform counterparts on Hungarian high school finals exam
- Ablation studies confirm that reusing pre-training data and using multiple augmentation methods significantly enhance performance
- Improvements generalize to unseen data, demonstrating robust mathematical reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative Question Composing (IQC) improves model performance by systematically increasing problem complexity while preserving underlying reasoning patterns.
- Mechanism: IQC takes a seed problem and uses GPT-4 to compose new problems that implicitly contain the original as a subproblem. Each iteration adds complexity by introducing new variables or steps while maintaining the core reasoning structure. This allows the model to learn hierarchical problem-solving skills.
- Core assumption: GPT-4 can reliably compose new problems that maintain mathematical correctness and preserve the reasoning chain from the original problem.
- Evidence anchors:
  - [abstract] "IQC iteratively composes complex math problems from seed problems using GPT-4"
  - [section] "The IQC process begins with specifying an LLM πq for question composing... iteratively constructing more complex problems"
  - [corpus] Weak evidence - corpus doesn't contain studies on IQC's effectiveness specifically
- Break condition: If GPT-4 fails to compose problems that maintain mathematical coherence or introduces extraneous reasoning steps that don't connect to the original problem structure.

### Mechanism 2
- Claim: Combining processed web data with synthetic question-response pairs provides diverse training signals that improve generalization.
- Mechanism: MMIQC includes three data sources: filtered Mathematics Stack Exchange data (1.2M samples), MetaMathQA subset (203.7K samples), and augmented data from multiple methods including IQC (55.1K samples). This diversity exposes the model to both real-world problem patterns and systematically generated variations.
- Core assumption: Different data augmentation methods capture complementary aspects of mathematical reasoning, and their combination is more effective than any single method alone.
- Evidence anchors:
  - [abstract] "MMIQC, a mixture of processed web data and synthetic question-response pairs"
  - [section] "Our results also show that using multiple augmentation methods to construct datasets for fine-tuning is an efficient way to boost the performance of LLMs"
  - [corpus] No direct corpus evidence for the specific combination approach
- Break condition: If the augmented data introduces significant noise or if the different data sources conflict in their mathematical reasoning patterns.

### Mechanism 3
- Claim: Rejection sampling with GPT-3.5 ensures answer quality and prevents propagation of incorrect solutions.
- Mechanism: For each composed problem, GPT-3.5 generates multiple potential answers. Only problems where GPT-3.5's answer matches the original answer are kept. This acts as a quality filter to ensure the synthetic data maintains mathematical correctness.
- Core assumption: GPT-3.5's answer verification is sufficiently reliable to filter out incorrect problem-solution pairs generated by GPT-4.
- Evidence anchors:
  - [abstract] "applying rejection sampling through another LLM"
  - [section] "We further enhance S1 by rejection sampling from πr, resulting in R1"
  - [corpus] No corpus evidence for the specific rejection sampling approach
- Break condition: If GPT-3.5 becomes unreliable at verification or if the rejection sampling is too strict and eliminates too much diverse data.

## Foundational Learning

- Concept: Mathematical reasoning hierarchies
  - Why needed here: Understanding how complex problems can be decomposed into simpler subproblems is crucial for both creating and solving the iteratively composed questions
  - Quick check question: Can you identify the subproblem structure in a complex geometry problem?

- Concept: Data augmentation diversity vs. quality tradeoff
  - Why needed here: Balancing the need for diverse training data with the requirement for mathematical correctness is central to the IQC approach
  - Quick check question: What happens to model performance when you increase data diversity but decrease answer accuracy?

- Concept: Chain-of-thought reasoning
  - Why needed here: The iterative composition relies on maintaining coherent reasoning chains across multiple problem transformations
  - Quick check question: Can you trace the logical steps from a simple problem through multiple iterations of composition?

## Architecture Onboarding

- Component map: Mathematics Stack Exchange → MetaMathQA → Augmented data → IQC → MMIQC dataset
- Critical path: Seed problem selection → GPT-4 composition → GPT-3.5 rejection sampling → Dataset integration → Model fine-tuning → Performance evaluation
- Design tradeoffs: Using GPT-4 (expensive) vs. GPT-3.5 (cheaper) for different roles; more iterations vs. data quality maintenance; data diversity vs. mathematical correctness; open-source models vs. proprietary models
- Failure signatures: Model accuracy plateaus despite more data; generated problems become too dissimilar from original reasoning patterns; rejection sampling eliminates too many valid problems; fine-tuned models perform well on training distribution but poorly on held-out problems
- First 3 experiments:
  1. Run IQC with just 1 iteration and compare performance to baseline to isolate the contribution of iterative composition
  2. Remove rejection sampling and measure the impact on answer accuracy to quantify quality control benefits
  3. Train on individual data subsets (Stack Exchange only, MetaMathQA only, augmented only) to understand contribution of each component

## Open Questions the Paper Calls Out

- Question: How can open-source models be equipped with the ability to compose questions autonomously, enabling self-evolution through iterative question composing?
  - Basis in paper: [explicit] The authors mention this as a future direction, comparing it to Huang et al. 2022a's self-improvement approach.
  - Why unresolved: This requires developing new architectures or training methods that enable models to generate increasingly complex mathematical problems without human intervention.
  - What evidence would resolve it: A demonstration of an open-source model that can iteratively generate, evaluate, and use its own composed questions to improve its mathematical reasoning performance over multiple iterations.

- Question: How can verification systems be integrated into the Iterative Question Composing (IQC) process to improve both the quality of generated questions and the accuracy of the resulting model?
  - Basis in paper: [explicit] The authors identify this as an attractive future direction, noting that verification systems have been used to improve accuracy during inference time.
  - Why unresolved: The optimal integration point and method for verification systems within the IQC loop remains unexplored, including whether to verify at each iteration or only at specific stages.
  - What evidence would resolve it: Comparative studies showing how different verification integration strategies affect the diversity, difficulty progression, and overall quality of generated questions, along with corresponding improvements in model performance.

- Question: What is the optimal balance between reusing pre-training data and synthetic data augmentation for fine-tuning mathematical reasoning models?
  - Basis in paper: [explicit] The authors note that simply adding more data doesn't always improve performance (citing Yu et al. 2023), yet their results show reusing pre-training data significantly helps.
  - Why unresolved: The relationship between data source diversity, quantity, and quality in relation to model performance is not fully understood, particularly across different model sizes.
  - What evidence would resolve it: Systematic ablation studies across multiple model sizes that vary the proportion of pre-training data reuse versus synthetic data generation, measuring not just final performance but also learning efficiency and generalization to unseen problems.

## Limitations

- The approach heavily relies on GPT-4's ability to compose mathematically coherent problems, with no quantitative analysis of rejection rates or quality of accepted problems.
- It's difficult to isolate which component contributes most to the gains since the paper combines multiple augmentation methods without component isolation.
- The MATH benchmark may not fully capture real-world mathematical problem-solving capabilities beyond standardized testing.

## Confidence

**High Confidence**: The general methodology of combining diverse data sources and using iterative composition for data augmentation is sound and well-established in the field. The empirical results showing improved performance on the MATH benchmark are clearly demonstrated and reproducible.

**Medium Confidence**: The claim that IQC specifically is responsible for the performance gains is reasonable but not definitively proven. The paper shows that using multiple augmentation methods helps, but doesn't conclusively demonstrate that IQC's iterative composition approach is superior to other methods.

**Low Confidence**: The claim about IQC's ability to "preserve underlying reasoning patterns" while increasing complexity is largely theoretical. The paper doesn't provide quantitative analysis of whether composed problems actually maintain the intended mathematical relationships or whether the complexity increases are pedagogically meaningful.

## Next Checks

1. **IQC Component Isolation**: Run controlled experiments comparing models trained on: (a) only base web data, (b) base web data + simple augmentation, (c) base web data + IQC only, and (d) all methods combined. This would isolate the specific contribution of iterative composition versus other augmentation methods.

2. **Quality Control Analysis**: Measure the rate of problem rejection during IQC and analyze the characteristics of accepted vs. rejected problems. Additionally, conduct human evaluation of a sample of composed problems to verify mathematical coherence and assess whether complexity increases are meaningful rather than superficial.

3. **Cross-Domain Generalization**: Test the fine-tuned models on non-standardized mathematical tasks, such as real-world problem-solving scenarios, open-ended mathematical questions, or problems requiring creative mathematical thinking beyond pattern matching. This would validate whether the improvements generalize beyond test-taking ability to genuine mathematical reasoning.