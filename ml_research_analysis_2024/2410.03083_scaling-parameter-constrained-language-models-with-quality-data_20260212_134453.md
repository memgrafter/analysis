---
ver: rpa2
title: Scaling Parameter-Constrained Language Models with Quality Data
arxiv_id: '2410.03083'
source_url: https://arxiv.org/abs/2410.03083
tags:
- selection
- data
- synthesis
- random
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends conventional scaling laws by incorporating data
  quality through effective training tokens, which combine diversity and syntheticity
  metrics. The authors pretrain over 200 models ranging from 25M to 1.5B parameters
  on sampled and synthetic data, establishing a revised scaling formulation.
---

# Scaling Parameter-Constrained Language Models with Quality Data

## Quick Facts
- **arXiv ID:** 2410.03083
- **Source URL:** https://arxiv.org/abs/2410.03083
- **Reference count:** 40
- **Key outcome:** Data quality measured through diversity and syntheticity metrics significantly impacts model accuracy, especially for smaller models, with a +0.83 Pearson correlation between estimated scaling law constants and true accuracy scores.

## Executive Summary
This paper extends conventional scaling laws by incorporating data quality through effective training tokens, which combine diversity and syntheticity metrics. The authors pretrain over 200 models ranging from 25M to 1.5B parameters on sampled and synthetic data, establishing a revised scaling formulation. Results demonstrate that data quality significantly impacts model performance, especially for smaller models, with synthetic data improving syntheticity at the cost of diversity. The study provides insights into optimizing data quality for language model training under parameter constraints.

## Method Summary
The authors pretrain over 200 decoder-only transformer models (25M to 1.5B parameters) using causal language modeling objectives on the RefinedWeb dataset. Data refinement techniques include random sampling (10-100% increments), importance sampling for selection, and synthetic data generation using Llama-3-70B-instruct. Models are trained for 100,000 steps with context length 2048 and batch size 16 across 32 H100 GPUs. The effective training tokens term combines text diversity (compression ratio) and syntheticity (inverse perplexity) to adjust the traditional scaling law formulation. Models are evaluated on eight reasoning tasks in zero-shot settings.

## Key Results
- Effective training tokens combining diversity and syntheticity significantly improve model accuracy prediction compared to traditional scaling laws
- Data quality has a stronger impact on smaller models, with diminishing returns as model size increases
- Synthetic data generation increases syntheticity but reduces diversity, creating a tradeoff in data refinement strategies
- The revised scaling law shows +0.83 Pearson correlation with true accuracy scores across all model sizes and data samples

## Why This Works (Mechanism)

### Mechanism 1
Data quality measured through diversity and syntheticity improves model accuracy more than simply increasing data quantity, especially for smaller models. The proposed effective training tokens term (Dq) combines text diversity (compression ratio) and syntheticity (inverse perplexity) to adjust the actual training data impact on model performance, refining the traditional scaling law. Core assumption: Diversity and syntheticity can be effectively quantified using compression ratio and teacher model perplexity, and these metrics correlate with improved model generalization. Evidence anchors: [abstract] and [section] statements about effective training tokens formulation. Break condition: If the teacher model is not representative of the target task domain, the syntheticity metric may not correlate with actual data usefulness.

### Mechanism 2
Data refinement techniques (selection and synthesis) directly influence the scaling factor Q by modulating diversity and syntheticity, with synthesis having a more substantial impact on increasing effective token count. Data selection enhances syntheticity without compromising diversity by importance sampling, while synthesis increases syntheticity but reduces diversity, affecting the scaling factor Q and thus model accuracy. Core assumption: Importance sampling can effectively enhance dataset syntheticity while maintaining diversity, and synthetic data generation can fill dataset gaps without introducing excessive redundancy. Evidence anchors: [abstract] and [section] statements about data refinement techniques. Break condition: If data synthesis introduces significant bias or redundancy, the reduction in diversity may outweigh the gains in syntheticity, leading to poorer model performance.

### Mechanism 3
The revised scaling law with effective training tokens shows strong correlation (+0.83 Pearson) with true accuracy scores, validating the importance of data quality in model performance. By incorporating the effective training tokens term into the scaling law formulation, the model can better predict accuracy based on data quality metrics rather than just data quantity and model parameters. Core assumption: The constants estimated in the revised scaling law accurately capture the relationship between data quality, model size, and accuracy, and the Pearson correlation of +0.83 indicates a strong relationship. Evidence anchors: [abstract] and [section] statements about correlation strength. Break condition: If the training data or evaluation tasks are not representative of the target domain, the correlation between the estimated constants and true accuracy may not hold.

## Foundational Learning

- **Scaling laws in machine learning**: Understanding the original Chinchilla scaling law formulation and how it relates model size, data size, and training loss is crucial for extending it with data quality metrics. Quick check: What are the key components of the Chinchilla scaling law, and how does it traditionally model training loss?

- **Data quality metrics (diversity and syntheticity)**: Quantifying data quality through diversity (compression ratio) and syntheticity (inverse perplexity) is essential for formulating the effective training tokens term and extending the scaling law. Quick check: How are diversity and syntheticity measured in this paper, and why are these metrics chosen?

- **Data refinement techniques (selection and synthesis)**: Understanding how data selection and synthesis influence diversity and syntheticity is crucial for analyzing their impact on the scaling factor Q and model accuracy. Quick check: What are the differences between data selection and synthesis, and how do they affect diversity and syntheticity?

## Architecture Onboarding

- **Component map**: Data preparation (sampling, selection, synthesis) -> Model training (200 models, 25M-1.5B parameters) -> Evaluation (eight reasoning tasks) -> Analysis (scaling law constants estimation)

- **Critical path**: 1. Prepare data with varying levels of diversity and syntheticity 2. Pretrain models on the prepared data 3. Evaluate model accuracy on reasoning tasks 4. Estimate scaling law constants using training and evaluation results 5. Analyze the impact of data quality metrics on model accuracy

- **Design tradeoffs**: Data quantity vs. quality (balancing amount of training data with its diversity and syntheticity), Teacher model selection (choosing representative model for syntheticity measurement), Model size (determining optimal size for given data quality and quantity)

- **Failure signatures**: Poor correlation between estimated constants and true accuracy (indicates issues with data quality metrics or scaling law formulation), Inconsistent accuracy across model sizes (suggests problems with data refinement techniques or model training)

- **First 3 experiments**: 1. Pretrain models on randomly sampled data with varying sizes to establish baseline performance 2. Pretrain models on selected data to assess impact of improved syntheticity 3. Pretrain models on synthetic data to evaluate tradeoff between increased syntheticity and reduced diversity

## Open Questions the Paper Calls Out

- **How does the scaling factor Q behave for models larger than 1.5B parameters, and does the dominance of token quantity over data quality persist or reverse at extreme scales?**: The paper shows data quality's impact diminishes for models larger than 1.5B, but does not explore behavior at scales beyond this point. The experimental range capped at 1.5B parameters, leaving the behavior of Q and the quality-quantity trade-off for larger models unknown.

- **What is the optimal balance between diversity and syntheticity for different downstream tasks beyond common sense reasoning?**: The study uses eight reasoning tasks and finds inverse relationship between diversity and syntheticity, but doesn't explore task-specific optimization. The analysis focused on a specific set of reasoning benchmarks without investigating whether different tasks require different diversity-syntheticity trade-offs.

- **How do the estimated scaling law constants generalize to datasets with fundamentally different characteristics from RefinedWeb, such as highly specialized or adversarial corpora?**: The study estimates constants using RefinedWeb data, but doesn't validate them on out-of-distribution or adversarial datasets. The constants were derived from a web-scale dataset, and their robustness to domain shift or adversarial construction remains untested.

## Limitations

- The study focuses on reasoning tasks using a single dataset (RefinedWeb), limiting generalizability to other NLP tasks or domains
- The teacher model approach for syntheticity measurement assumes that a single 7B model's perplexity correlates with data quality across all model sizes
- The synthetic data generation process using a 70B model introduces potential biases that are not fully characterized
- The study examines models up to 1.5B parameters, leaving open questions about whether observed correlations hold at larger scales

## Confidence

- **Mechanism 1 (Data quality metrics improve accuracy)**: Medium confidence - Strong correlation evidence but relationship may not generalize across domains
- **Mechanism 2 (Data refinement impacts scaling factor Q)**: Medium confidence - Clear statements but weak external validation and bias concerns
- **Mechanism 3 (Revised scaling law validation)**: Medium confidence - Impressive correlation but novel formulation without extensive external validation

## Next Checks

1. **Cross-domain generalization test**: Validate the effective training tokens formulation on a different dataset type (e.g., code, scientific text, or conversational data) and evaluate whether the +0.83 correlation holds

2. **Teacher model sensitivity analysis**: Systematically vary the teacher model size and capability (e.g., using 1B, 7B, 13B, and 70B models) to measure how syntheticity estimates change and whether the scaling law constants remain stable

3. **Larger model scaling experiment**: Extend the analysis to models beyond 1.5B parameters (e.g., 3B, 7B, 13B) to test whether the revised scaling law maintains its predictive power and whether data quality effects scale similarly or change qualitatively at larger model sizes