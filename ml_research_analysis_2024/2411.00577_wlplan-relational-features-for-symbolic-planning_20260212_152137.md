---
ver: rpa2
title: 'WLPlan: Relational Features for Symbolic Planning'
arxiv_id: '2411.00577'
source_url: https://arxiv.org/abs/2411.00577
tags:
- planning
- graph
- learning
- wlplan
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WLPlan is a C++ package with Python bindings that implements recent
  work for automatically generating relational features of planning tasks. It bridges
  the gap between learning (typically in Python) and planning (typically in C++) by
  providing a common, efficient interface for embedding planning tasks into feature
  vectors via graph kernels.
---

# WLPlan: Relational Features for Symbolic Planning

## Quick Facts
- arXiv ID: 2411.00577
- Source URL: https://arxiv.org/abs/2411.00577
- Authors: Dillon Z. Chen
- Reference count: 5
- Primary result: WLPlan distinguishes training states in 7 out of 10 domains and achieves similar performance to previous implementations in learning heuristic functions for both classical and numeric planning domains.

## Executive Summary
WLPlan is a C++ package with Python bindings that implements recent work for automatically generating relational features of planning tasks. It bridges the gap between learning (typically in Python) and planning (typically in C++) by providing a common, efficient interface for embedding planning tasks into feature vectors via graph kernels. The package supports various graph representations of planning tasks and multiple extensions of the Weisfeiler-Lehman algorithm for feature generation. Experiments show that WLPlan can distinguish training states in 7 out of 10 domains, and achieves similar performance to previous implementations in learning heuristic functions for both classical and numeric planning domains.

## Method Summary
WLPlan transforms planning tasks into graphs and applies Weisfeiler-Lehman (WL) algorithms to generate feature vectors for learning domain-independent heuristics. The approach uses Instance Learning Graphs to represent planning tasks as graphs with categorical and continuous node features, then applies WL, 2-LWL, iWL, or ccWL algorithms to collect colors that serve as features. These features are embedded into Euclidean space for use with machine learning models. The package is designed to be extensible, efficient, and easy to use, with a C++ core and Python bindings to streamline research in learning for planning.

## Key Results
- WLPlan achieves 7/10 domains with perfect distinguishability of training states
- Performance comparable to previous implementations (WLICAPS-24, GNNICAPS-24) in learning heuristics
- Successfully handles both classical planning domains (Blocksworld, Ferry, Miconic) and numeric domains (IPPC23LT)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The WL algorithm's iterative color refinement process distinguishes non-isomorphic graphs, making it effective for learning domain-independent heuristics.
- Mechanism: The algorithm iteratively updates node colors based on the multiset of neighbor colors and edge labels, producing a canonical form invariant to node ordering. This captures structural patterns in planning tasks that correlate with heuristic values.
- Core assumption: The multiset of colors produced by WL contains sufficient discriminative information about the planning task structure to predict heuristic values.
- Evidence anchors:
  - [abstract]: "The approach involves (1) transforming planning tasks into graphs, and (2) embedding such graphs into feature vectors... its performance can be attributed to its evaluation speed and expressive power over previous methods."
  - [section]: "the theoretical result that it upper bounds distinguishing power of the message passing GNN architecture (Morris et al. 2019; Xu et al. 2019)"
  - [corpus]: Weak evidence - related papers focus on symbolic planning but don't directly validate WL's effectiveness for heuristic learning.

### Mechanism 2
- Claim: The 2-LWL algorithm provides efficient approximation of 2-WL while maintaining good distinguishing power for planning tasks.
- Mechanism: 2-LWL approximates 2-WL by using node sets instead of tuples and limiting neighbor consideration to the union of neighbors of both nodes, reducing computational complexity while preserving expressiveness.
- Core assumption: The approximations made in 2-LWL (using sets instead of tuples, limiting neighbor scope) preserve sufficient distinguishing power for planning task graphs.
- Evidence anchors:
  - [section]: "The k-LWL algorithms (Morris, Kersting, and Mutzel 2017) provide efficient approximations of the k-WL algorithms but still have the same worst case computational complexity."
  - [corpus]: No direct evidence - the corpus doesn't discuss 2-LWL specifically.

### Mechanism 3
- Claim: The ccWL algorithm effectively handles continuous node features by aggregating them into discrete colors, preserving numeric semantics for planning.
- Mechanism: ccWL extends WL by mapping each color to an aggregated continuous feature vector, concatenating these with the discrete embedding to maintain numeric information.
- Core assumption: Aggregating continuous node features per color preserves the numeric semantics necessary for planning tasks while maintaining computational efficiency.
- Evidence anchors:
  - [section]: "we outline a WL algorithm which can handle both categorical and continuous node features... the ccWL algorithm used for numeric planning introduced by Chen and Thi ´ebaux (2024a)."
  - [corpus]: No direct evidence - corpus papers don't discuss ccWL specifically.

## Foundational Learning

- Concept: Graph isomorphism testing
  - Why needed here: Understanding why WL algorithms work requires knowing that they provide bounds on graph isomorphism, which is fundamental to why color refinement can distinguish different planning states.
  - Quick check question: Can you explain why the ability to distinguish non-isomorphic graphs matters for learning heuristic functions?

- Concept: Feature vector embeddings
  - Why needed here: The paper converts graph representations into feature vectors for ML models, so understanding how discrete color counts become continuous feature vectors is essential.
  - Quick check question: How does counting color occurrences create a fixed-size feature vector from variable-sized graph outputs?

- Concept: Domain-independent planning
  - Why needed here: The paper aims to learn heuristics that work across domains, so understanding what makes a heuristic "domain-independent" versus domain-specific is crucial.
  - Quick check question: What distinguishes a domain-independent heuristic from a domain-specific one in planning?

## Architecture Onboarding

- Component map: WLPlan has three main components: (1) Graph transformation - converts planning tasks to graphs with categorical/continuous node features and edge labels, (2) Feature generation - applies WL algorithms to produce embeddings, (3) Serialization - saves/loads models in JSON format. The package provides both C++ implementations for performance and Python bindings for ease of use.

- Critical path: For learning: parse PDDL → transform to graph → collect colors → embed states → train ML model → serialize model. For planning: load model → embed state → predict heuristic → search. The embedding step is the bottleneck and must be optimized.

- Design tradeoffs: C++ for performance vs Python for ease of use; simple feature generation vs complex deep learning; extensibility vs simplicity. The package chooses C++ core with Python bindings, classical ML over deep learning, and modular design for extensibility.

- Failure signatures: If distinguishability tests show high indistinguishable state pairs, the WL algorithm lacks expressiveness. If embedding is slow, the C++ implementation needs optimization. If models don't generalize across domains, the feature representation is insufficient.

- First 3 experiments:
  1. Run distinguishability test on a simple domain (Blocksworld) to verify WL can distinguish training states with different h* values.
  2. Test PCA visualization on IPC23LT domains to understand feature structure and relationship to heuristic values.
  3. Compare coverage of learned heuristic vs baseline planners on a single numeric domain to validate effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the distinguishability performance of WL algorithms be further improved for domains where current implementations fail to distinguish all state pairs?
- Basis in paper: [explicit] The paper notes that WL algorithms distinguish all pairs of training states in 7 out of 10 domains, with more expressive algorithms (2-LWL, iWL) distinguishing more pairs in one domain but failing due to memory constraints.
- Why unresolved: Current WL extensions face computational limitations (memory and time) when applied to larger or more complex domains, preventing full distinguishability.
- What evidence would resolve it: Empirical results showing successful distinguishability of all state pairs in previously failing domains using either more efficient algorithms, parameter tuning, or hybrid approaches combining multiple WL variants.

### Open Question 2
- Question: What is the relationship between PCA visualization patterns of WL embeddings and the actual performance of learned heuristics on different domains?
- Basis in paper: [explicit] The paper observes that domains with clear linear patterns in PCA visualizations (Blocksworld, Ferry, Miconic) correlate with high performance of learned linear heuristics, while domains with no clear pattern (Rovers, Sokoban) suggest features may not be informative enough.
- Why unresolved: The paper only presents visual observations without establishing a quantitative correlation or explaining why certain patterns lead to better heuristic learning.
- What evidence would resolve it: Statistical analysis demonstrating correlation coefficients between PCA visualization metrics (e.g., variance explained by principal components, cluster separation) and actual heuristic performance metrics across multiple domains.

### Open Question 3
- Question: Can WLPlan be extended to support temporal planning domains while maintaining its efficiency advantages?
- Basis in paper: [explicit] The paper focuses on deterministic planning representations and notes that WLPlan is state-centric and agnostic to the transition model, but does not explore temporal extensions.
- Why unresolved: Temporal planning introduces additional complexity through durative actions and temporal constraints that may require fundamentally different graph representations and kernel adaptations.
- What evidence would resolve it: Implementation and experimental evaluation of temporal planning graph representations and corresponding WL extensions within WLPlan, demonstrating comparable efficiency and performance to state-of-the-art temporal planning approaches.

## Limitations

- WL algorithms fail to distinguish all training states in 3 out of 10 domains tested
- 2-LWL algorithm suffers from memory exhaustion on domains with many objects
- The effectiveness depends heavily on the quality and representativeness of training data

## Confidence

- **High confidence**: WL algorithms can distinguish non-isomorphic graphs and their effectiveness in transforming planning tasks into feature vectors is well-established.
- **Medium confidence**: The 2-LWL approximation maintains sufficient distinguishing power for planning tasks, though this depends on domain characteristics.
- **Medium confidence**: The ccWL algorithm effectively handles continuous features for numeric planning, though the aggregation function's impact on planning performance needs further validation.

## Next Checks

1. **Scalability test**: Evaluate WLPlan on domains with increasing numbers of objects to identify performance degradation points for each WL variant.
2. **Cross-domain generalization**: Test whether models trained on one domain can provide useful heuristics for structurally similar but distinct domains.
3. **Feature importance analysis**: Use feature attribution methods to identify which graph substructures contribute most to heuristic predictions, validating that the learned features align with planning intuition.