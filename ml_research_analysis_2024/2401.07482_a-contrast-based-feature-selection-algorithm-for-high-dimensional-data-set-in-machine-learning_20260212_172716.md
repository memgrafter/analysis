---
ver: rpa2
title: A Contrast Based Feature Selection Algorithm for High-dimensional Data set
  in Machine Learning
arxiv_id: '2401.07482'
source_url: https://arxiv.org/abs/2401.07482
tags:
- feature
- features
- selection
- methods
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ContrastFS, a fast and effective filter-based
  feature selection method for high-dimensional datasets. The method addresses computational
  inefficiency in existing approaches by constructing surrogate representations of
  each class using low-order sample moments, then evaluating features based on the
  discrepancies they show among classes.
---

# A Contrast Based Feature Selection Algorithm for High-dimensional Data set in Machine Learning

## Quick Facts
- arXiv ID: 2401.07482
- Source URL: https://arxiv.org/abs/2401.07482
- Authors: Chunxu Cao; Qiang Zhang
- Reference count: 10
- Primary result: ContrastFS achieves superior or comparable classification accuracy to state-of-the-art methods while being several orders of magnitude faster on high-dimensional datasets.

## Executive Summary
This paper introduces ContrastFS, a filter-based feature selection method designed for high-dimensional datasets where computational efficiency is critical. The method addresses the computational bottleneck in existing approaches by using low-order sample moments to construct surrogate representations of each class, then evaluating features based on the discrepancies they show across classes. ContrastFS demonstrates significant improvements in both speed and accuracy compared to state-of-the-art feature selection methods while maintaining good stability to data perturbations.

## Method Summary
ContrastFS is a filter-based feature selection algorithm that constructs surrogate representations of each class using low-order sample moments (mean, standard deviation, and coefficient of variation). The method evaluates feature importance by measuring average pairwise discrepancies across classes using dimensionless quantities that summarize the statistical individuality of each class. After ranking features by their discrepancy scores, ContrastFS reduces redundancy by analyzing correlations between discrepancy vectors rather than the original high-dimensional feature values. This approach enables the method to achieve several orders of magnitude faster computation while maintaining or improving classification accuracy compared to existing methods.

## Key Results
- ContrastFS achieves superior or comparable classification accuracy to state-of-the-art methods (JMI, CFS, Fisher Score, Trace-Ratio, mRMR, CMIM, DISR, UDFS, HSIC-Lasso, LassoNet) on eight real-world datasets
- The method is several orders of magnitude faster than baseline approaches, making it practical for high-dimensional data
- ContrastFS shows good stability to perturbations and can effectively reduce feature redundancy using correlations between discrepancy vectors
- Performance is particularly strong on datasets with varying dimensions and sample sizes including MNIST, Fashion-MNIST, and ISOLET

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ContrastFS achieves high efficiency by constructing surrogate representations using only low-order sample moments.
- Mechanism: The method summarizes each class using mean, standard deviation, and coefficient of variation, creating a compact representation that captures distributional individuality without requiring full dataset distance computations.
- Core assumption: Low-order moments are sufficient to capture the discriminative structure of each class for feature selection purposes.
- Evidence anchors:
  - [abstract] "by constructing surrogate representations of each class using low-order sample moments"
  - [section] "We consider the mean and standard deviation of the features in each class, and scale them to eliminate the influence of dimension and subclass."
- Break condition: If class distributions have significant higher-order structure (skewness, kurtosis) that is critical for discrimination, the method may miss important features.

### Mechanism 2
- Claim: Feature importance is determined by measuring average pairwise discrepancies across classes using dimensionless quantities.
- Mechanism: By standardizing features and computing the contrast between class representations, the method creates a scale-invariant measure of how much features vary across different classes.
- Core assumption: Features that show large discrepancies across classes are more likely to be discriminative for classification tasks.
- Evidence anchors:
  - [abstract] "evaluating features based on the discrepancies they show among classes"
  - [section] "I(ft) = 1/C(C − 1) ∑i ∑j,j̸=i |Cvkt μit − μt σit − σt − Cvkt μjt − μt σjt − σt − σt|, i, j ∈ { 1, · · ·, C}"
- Break condition: If classes have overlapping distributions with subtle differences, the discrepancy measure may not capture these fine distinctions.

### Mechanism 3
- Claim: Redundancy reduction is achieved by analyzing correlations between discrepancy vectors rather than original feature values.
- Mechanism: Instead of computing correlations in the high-dimensional original space, the method computes correlations between vectors of class-to-class discrepancies, which are much lower dimensional.
- Core assumption: Features that produce similar discrepancy patterns across classes are likely redundant and can be pruned.
- Evidence anchors:
  - [abstract] "can reduce feature redundancy using correlations between discrepancy vectors"
  - [section] "We compute the correlation between the discrepancies of selected features in a pairwise manner as the measure of redundancy."
- Break condition: If redundant features have different discrepancy patterns due to noise or specific class structures, the method may incorrectly prune useful features.

## Foundational Learning

- Concept: Low-order sample moments (mean, variance, coefficient of variation)
  - Why needed here: These form the basis for constructing surrogate representations that summarize each class's distributional properties in a dimensionless way.
  - Quick check question: What are the three low-order moments used in ContrastFS and why are they sufficient for creating class summaries?

- Concept: Dimensional analysis and dimensionless quantities
  - Why needed here: The method needs to compare features with different physical meanings and value ranges, requiring a unified scale for comparison.
  - Quick check question: How does standardization help create dimensionless quantities for feature comparison?

- Concept: Feature redundancy and correlation analysis
  - Why needed here: After selecting top features based on discrepancy, the method needs to identify and remove redundant features to improve efficiency and generalization.
  - Quick check question: Why is it computationally advantageous to compute correlations between discrepancy vectors rather than original feature values?

## Architecture Onboarding

- Component map: Data preprocessing -> Surrogate representation construction -> Feature scoring -> Redundancy analysis -> Final selection
- Critical path:
  1. Compute class-wise means, standard deviations, and coefficients of variation
  2. Construct surrogate representations for each class
  3. Calculate pairwise discrepancies for each feature
  4. Rank features by average discrepancy
  5. Compute correlations between discrepancy vectors
  6. Remove redundant features
- Design tradeoffs:
  - Speed vs. accuracy: Using only low-order moments makes the method very fast but may miss higher-order discriminative information
  - Individual evaluation vs. subset optimization: Evaluating features independently is faster but may select redundant features
  - Surrogate representation choice: The specific formula balances sensitivity to distributional differences with computational efficiency
- Failure signatures:
  - Poor performance on datasets with significant higher-order structure (skewness, multimodality)
  - Suboptimal results when classes have similar means but different higher-order properties
  - Potential issues with very small class sizes where moment estimates are unreliable
- First 3 experiments:
  1. Run ContrastFS on a simple dataset (e.g., Iris) and compare selected features with known informative features
  2. Test the method on a high-dimensional dataset (e.g., MNIST) and measure runtime vs. accuracy trade-off
  3. Evaluate the redundancy reduction step by comparing feature sets with and without the correlation-based filtering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using different surrogate representations (e.g., mean, median, higher-order moments) on feature selection performance and computational efficiency?
- Basis in paper: [explicit] The paper mentions that surrogate representations are constructed using low-order sample moments, but does not explore alternative representations.
- Why unresolved: The paper focuses on a specific surrogate representation method based on mean, standard deviation, and coefficient of variation, leaving the impact of alternative representations unexplored.
- What evidence would resolve it: Comparative experiments evaluating the performance and computational efficiency of ContrastFS using different surrogate representations on various datasets.

### Open Question 2
- Question: How does the stability of ContrastFS change with different levels of noise in the data, and what are the optimal noise reduction techniques for each noise level?
- Basis in paper: [explicit] The paper mentions that ContrastFS is stable for perturbations and suggests using bootstrap to improve performance and stability, but does not explore the relationship between noise levels and stability in detail.
- Why unresolved: The paper only provides a general discussion of stability without a systematic analysis of the impact of different noise levels on the method's performance.
- What evidence would resolve it: Experiments evaluating the stability of ContrastFS on datasets with varying levels of noise and comparing the performance of different noise reduction techniques.

### Open Question 3
- Question: Can ContrastFS be extended to handle non-linear relationships between features and labels, and what are the potential benefits and challenges of such an extension?
- Basis in paper: [inferred] The paper focuses on linear relationships between features and labels, but the concept of feature selection based on discrepancies can potentially be extended to non-linear relationships.
- Why unresolved: The paper does not explore the potential for extending ContrastFS to handle non-linear relationships, leaving the benefits and challenges of such an extension unclear.
- What evidence would resolve it: Development and evaluation of an extended version of ContrastFS that can handle non-linear relationships, comparing its performance to the original method on datasets with known non-linear relationships.

## Limitations
- The method may fail on datasets with significant higher-order structure like multimodality or heavy tails since it relies only on low-order moments
- The dimensionless quantity calculation requires domain-specific tuning of the coefficient of variation parameter, which could affect reproducibility
- Performance on datasets with very small class sizes or high class imbalance remains unverified

## Confidence
- **High confidence**: Computational efficiency claims and runtime comparisons (well-documented with specific measurements)
- **Medium confidence**: Feature selection effectiveness and accuracy results (strong on tested datasets but limited by potential overfitting to specific data characteristics)
- **Medium confidence**: Stability and robustness claims (based on RSD measurements but dependent on specific experimental conditions)

## Next Checks
1. Test ContrastFS on synthetic datasets with known higher-order structure (e.g., multimodal distributions) to verify the limitations of using only low-order moments
2. Conduct ablation studies removing the redundancy reduction step to quantify its contribution to final performance
3. Evaluate the method's sensitivity to the coefficient of variation parameter across different domain types to establish robust default values