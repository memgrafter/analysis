---
ver: rpa2
title: Advancing Re-Ranking with Multimodal Fusion and Target-Oriented Auxiliary Tasks
  in E-Commerce Search
arxiv_id: '2408.05751'
source_url: https://arxiv.org/abs/2408.05751
tags:
- multimodal
- information
- user
- re-ranking
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of integrating multimodal information,
  particularly visual cues, into e-commerce re-ranking models to enhance user experience
  and conversion rates. The proposed Advancing Re-Ranking with Multimodal Fusion and
  Target-Oriented Auxiliary Tasks (ARMMT) framework introduces a Context-Aware Fusion
  Unit (CAFU) and Multi-Perspective Self-Attention mechanism to effectively combine
  textual and visual features, while an auxiliary task aligned with ranking objectives
  supervises the learning of multimodal representations.
---

# Advancing Re-Ranking with Multimodal Fusion and Target-Oriented Auxiliary Tasks in E-Commerce Search

## Quick Facts
- arXiv ID: 2408.05751
- Source URL: https://arxiv.org/abs/2408.05751
- Reference count: 40
- One-line primary result: Achieved 0.22% CVR increase and 0.49% GMV increase in e-commerce search re-ranking using multimodal fusion

## Executive Summary
This paper addresses the challenge of integrating visual and textual information into e-commerce re-ranking models to improve conversion rates and user experience. The proposed ARMMT framework introduces a Context-Aware Fusion Unit (CAFU) and Multi-Perspective Self-Attention mechanism to effectively combine multimodal features, while an auxiliary click-through rate prediction task provides additional supervision for learning high-quality representations. Experimental results on JD.com's search platform demonstrate significant improvements in both offline AUC and online conversion metrics, validating the effectiveness of the multimodal fusion approach.

## Method Summary
The ARMMT framework processes user search sessions and item candidates through a two-stage architecture involving a Sequence Generator and Sequence Evaluator. It employs separate encoders for text (titles, attributes) and images (pre-trained RegNet embeddings), applies target attention to user behavior sequences, and uses CAFU to fuse item and personalized representations across modalities. The model then applies Multi-Perspective Self-Attention to model interactions between multimodal features and domain-specific features like price and sales, while simultaneously predicting both conversion rate (main task) and click-through rate (auxiliary task) using cross-entropy losses.

## Key Results
- Achieved 0.22% increase in Conversion Rate (CVR) and 0.49% increase in Gross Merchandise Volume (GMV) compared to baseline model
- Obtained 0.0005 improvement in offline AUC metric
- Demonstrated effectiveness of multimodal fusion in e-commerce search re-ranking tasks

## Why This Works (Mechanism)

### Mechanism 1
The Context-Aware Fusion Unit (CAFU) improves multimodal fusion by incorporating contextual information alongside modality-specific features. It compresses each modality's features using mean pooling to generate initial weights, concatenates these with contextual information, and applies a Multi-Layer Perceptron to produce modality-specific weights. These weights are then used to perform weighted sum pooling of the modality features.

Core assumption: Contextual information (like query features and user features) is relevant and improves the fusion of modality representations by guiding the attention weights.

### Mechanism 2
Multi-Perspective Self-Attention captures complex interactions between multimodal and other domain features (like price, sales) by modeling them jointly. After obtaining multimodal representations via CAFU, it concatenates multimodal features with other domain features, applies a transformer encoder to model interactions, and then performs additional attention to extract higher-order semantic features.

Core assumption: Interactions between multimodal information and domain-specific features (price, sales) are important for ranking and can be captured through self-attention mechanisms.

### Mechanism 3
The auxiliary task (click-through rate prediction) aligned with the main ranking task provides additional supervision that improves the quality and relevance of multimodal representations. An auxiliary loss is computed using cross-entropy between click labels and predicted click probabilities derived from multimodal representations, combined with the main task loss during training.

Core assumption: Predicting clicks using multimodal representations provides a useful auxiliary signal that regularizes the learning of representations beneficial for the main ranking task.

## Foundational Learning

- Concept: Multimodal feature fusion techniques
  - Why needed here: The core innovation relies on effectively combining textual and visual information, which requires understanding different fusion strategies.
  - Quick check question: What are the key differences between early, late, and hybrid fusion approaches, and why might late fusion be preferred in this context?

- Concept: Attention mechanisms and transformer architectures
  - Why needed here: The proposed method extensively uses attention mechanisms (target attention, self-attention, multi-head attention) and transformer encoders for modeling interactions.
  - Quick check question: How does the multi-head self-attention mechanism compute attention weights, and what is the role of the scaling factor 1/√dk?

- Concept: Auxiliary task learning and multi-task optimization
  - Why needed here: The model incorporates an auxiliary task (click prediction) alongside the main ranking task, requiring understanding of how auxiliary tasks can improve main task performance.
  - Quick check question: How does combining main task loss and auxiliary task loss during training affect the optimization landscape and convergence?

## Architecture Onboarding

- Component map: Input Layer -> Multimodal Encoding -> Personalized Item Representation -> CAFU Fusion -> Multi-Perspective Self-Attention -> Main Task (CVR) + Auxiliary Task (CTR) -> Output Layer

- Critical path: User input → Historical behavior filtering → Target attention → CAFU fusion → Multi-Perspective Self-Attention → CVR/auxiliary predictions → Ranking

- Design tradeoffs:
  - Using frozen pre-trained image embeddings vs. fine-tuning: Reduces training complexity and serving latency but may limit adaptation to domain-specific features
  - Two-stage behavioral sequence modeling: Improves relevance by filtering behaviors by category but adds complexity
  - CAFU with contextual information vs. simple concatenation: Potentially better fusion quality but requires careful feature engineering

- Failure signatures:
  - Model fails to improve offline AUC: Likely issues with multimodal feature quality or fusion mechanism
  - Online CVR improvement doesn't match offline results: Possible overfitting or distribution shift between offline and online data
  - Training instability: Auxiliary task loss weight (λ) may be poorly tuned

- First 3 experiments:
  1. Verify multimodal feature quality: Check if text and image embeddings capture meaningful information by visualizing or clustering representations
  2. Test CAFU ablation: Compare performance with and without CAFU and with simple concatenation to validate its contribution
  3. Tune auxiliary task weight: Sweep λ values to find optimal balance between main and auxiliary task losses

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed ARMMT framework handle cases where visual features are missing or of low quality, and what impact does this have on the model's performance? The paper mentions that the approach is "robust to missing values" but does not provide specific details on how this is achieved or tested.

### Open Question 2
How does the ARMMT framework's performance compare to other multimodal fusion techniques in e-commerce re-ranking tasks, such as those using contrastive learning or transformer-based approaches? The paper mentions that "progress has been made in extracting multimodal features [27], but there remains a gap in research on effectively incorporating these different types of features into re-ranking models."

### Open Question 3
What are the computational costs associated with the ARMMT framework, particularly in terms of online serving and scalability for large e-commerce platforms? The paper mentions that "we adopted a two-stage integration strategy for multimodal representations" to optimize training efficiency and avoid increasing online serving pressure, but does not provide specific details on computational costs.

## Limitations
- Evaluation relies heavily on online A/B testing results from a single e-commerce platform (JD.com), limiting generalizability
- Limited architectural details about the two-stage Sequence Generator and Sequence Evaluator make exact reproduction challenging
- Impact of hyper-parameter tuning, particularly the auxiliary task loss weight λ, is not thoroughly explored

## Confidence
- **High Confidence**: The effectiveness of multimodal fusion in improving e-commerce re-ranking performance is well-supported by both offline AUC improvements (0.0005) and online A/B testing results (0.22% CVR increase, 0.49% GMV increase)
- **Medium Confidence**: The claim that contextual information significantly improves fusion quality through CAFU is supported by the proposed mechanism but lacks ablation studies comparing different fusion strategies
- **Low Confidence**: The specific contribution of each architectural component (CAFU vs. Multi-Perspective Self-Attention vs. auxiliary task) to the overall performance improvement cannot be isolated due to the lack of comprehensive ablation studies

## Next Checks
1. **Component Ablation Study**: Systematically evaluate the contribution of each proposed component (CAFU, Multi-Perspective Self-Attention, auxiliary task) by comparing performance against variants with individual components removed or replaced with simpler alternatives

2. **Cross-Platform Generalization**: Test the ARMMT framework on a different e-commerce dataset or platform to validate whether the observed improvements generalize beyond JD.com's specific user behavior patterns and product catalog

3. **Computational Efficiency Analysis**: Measure and compare the inference latency, memory footprint, and computational complexity of ARMMT against baseline models to assess practical deployment feasibility, particularly for high-traffic e-commerce platforms