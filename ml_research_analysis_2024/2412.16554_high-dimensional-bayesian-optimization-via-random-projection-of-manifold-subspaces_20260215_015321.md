---
ver: rpa2
title: High-Dimensional Bayesian Optimization via Random Projection of Manifold Subspaces
arxiv_id: '2412.16554'
source_url: https://arxiv.org/abs/2412.16554
tags:
- manifold
- function
- random
- projection
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach for high-dimensional Bayesian
  optimization (BO) by leveraging the assumption that objective functions depend on
  low-dimensional manifolds embedded in high-dimensional spaces. The proposed method
  combines random linear projections with representation learning to efficiently optimize
  acquisition functions in low-dimensional spaces while maintaining theoretical convergence
  guarantees.
---

# High-Dimensional Bayesian Optimization via Random Projection of Manifold Subspaces

## Quick Facts
- arXiv ID: 2412.16554
- Source URL: https://arxiv.org/abs/2412.16554
- Reference count: 40
- Primary result: Introduces RPM-BO method that outperforms existing baselines in high-dimensional Bayesian optimization by leveraging manifold structure through random projections and semi-supervised learning

## Executive Summary
This paper addresses the challenge of high-dimensional Bayesian optimization by assuming objective functions depend on low-dimensional manifolds embedded in high-dimensional spaces. The proposed RPM-BO method combines random linear projections with representation learning to optimize acquisition functions efficiently in low-dimensional spaces while maintaining theoretical convergence guarantees. A key innovation is the use of geometry-aware semi-supervised learning to train feature mappings, which mitigates overfitting issues common in neural network-based approaches. The algorithm demonstrates superior performance across various synthetic functions with different manifold structures and real-world applications including Lasso regression and MuJoCo robotic tasks.

## Method Summary
The RPM-BO method operates by projecting high-dimensional inputs through a random orthogonal matrix to reduce dimensionality from D to m (where m << D), then learns a feature mapping h through semi-supervised training that combines supervised loss from observed data with unsupervised consistency loss on unlabeled data. A Gaussian Process model is built in the reduced m-dimensional space, acquisition functions are optimized there, and the optimal point is back-projected to the original high-dimensional space using h(AT z). This approach enables efficient optimization while maintaining convergence guarantees through the Johnson-Lindenstrauss lemma's distance preservation properties in manifold spaces.

## Key Results
- RPM-BO outperforms existing methods like REMBO, HeSBO, and SAASBO on synthetic functions with spherical and mixed-manifold structures
- The method scales effectively to dimensions exceeding 1500 while maintaining computational efficiency
- Superior performance demonstrated on real-world applications including Lasso regression benchmarks and MuJoCo robotic tasks
- Semi-supervised learning approach successfully mitigates overfitting issues in high-dimensional spaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random projection combined with semi-supervised learning mitigates overfitting in high-dimensional Bayesian optimization.
- Mechanism: The method uses random linear projections to reduce dimensionality from D to m (where m << D), then learns a feature mapping h through semi-supervised training that combines supervised loss from observed data with unsupervised consistency loss on unlabeled data.
- Core assumption: The objective function has an effective low-dimensional manifold structure, and unlabeled data can provide useful regularization without requiring labeled labels.
- Evidence anchors:
  - [abstract] "To mitigate overfitting by using the neural network, we train the feature mapping in a geometry-aware semi-supervised manner."
  - [section 3.1] "To overcome this, we take the view of semi-supervised learning (SSL)... One common approach of SSL is the use of consistency loss... In this work, we introduce a novel unsupervised consistency loss that can leverage the property of manifold."
- Break condition: If the manifold assumption fails (objective function doesn't have low-dimensional structure), the random projection won't preserve meaningful relationships and semi-supervised learning won't help.

### Mechanism 2
- Claim: The random projection preserves pairwise distances with high probability, enabling convergence guarantees for GP regression in the reduced space.
- Mechanism: Using a random orthogonal matrix A, the method ensures that for any two points on the manifold M, their projected distances in Rm are within (1±ε) times the original distances, scaled by sqrt(m/D).
- Core assumption: The manifold M is compact and can be embedded in Euclidean space Rm with m = O(d log(D)).
- Evidence anchors:
  - [section 3.2] "By the work of [17], Equation (9) implies that with high probability, the projection dimension A is a diffeomorphism onto its image."
  - [section 3.3] "Theorem 3... Let M be a compact d-dimensional manifold... Fix 0 < ε < 1 and 0 < ρ < 1... Then, with probability exceeding 1 − ρ, the following statement holds..."
- Break condition: If the manifold is not compact or cannot be embedded in Rm (e.g., fractal-like structures), the distance preservation guarantee fails.

### Mechanism 3
- Claim: The back-projection scheme allows efficient optimization in low-dimensional space while maintaining high-dimensional evaluation capability.
- Mechanism: Instead of directly optimizing acquisition function on the complicated manifold image MA, the method solves an equivalent problem in Rm and projects back to high-dimensional space using h(AT z).
- Core assumption: The learned mapping h approximates the orthogonal projection PM to the manifold, making the back-projection valid.
- Evidence anchors:
  - [section 3.3] "Therefore, instead of solving problem 10, we will solve the below problem: zn+1 = argmax z∈Rm EI(Ah(AT z)|{Ah(xi), yi}n i=1)"
  - [section 3.3] "Theorem 2... Let M ⊂ RD is the d-dimensional manifold... MA = {z ∈ Rm|∃x ∈ M : z = Ax}; MA = {z ∈ Rm|∃q ∈ Rm : z = APM(AT q)}. Then, with probability 1, MA = MA."
- Break condition: If h doesn't approximate PM well (poor feature learning), the back-projection will yield suboptimal high-dimensional points.

## Foundational Learning

- Concept: Johnson-Lindenstrauss Lemma and its manifold generalization
  - Why needed here: Provides theoretical foundation for why random projections preserve distances in high-dimensional spaces, which is crucial for maintaining GP model accuracy after dimensionality reduction.
  - Quick check question: What is the required relationship between projection dimension m and original dimension D for distance preservation with ε error probability?

- Concept: Semi-supervised learning and consistency regularization
  - Why needed here: Enables effective learning of feature mapping h with limited labeled data by leveraging unlabeled data through consistency loss, addressing overfitting issues common in neural network-based approaches.
  - Quick check question: How does the unsupervised consistency loss leverage manifold properties to improve feature learning?

- Concept: Gaussian Process regression and optimal convergence rates
  - Why needed here: Understanding how GP models achieve optimal posterior contraction rates in low-dimensional spaces enables the theoretical justification for the approach's efficiency.
  - Quick check question: What is the convergence rate of GP regression in terms of sample size n and input dimension m?

## Architecture Onboarding

- Component map:
  - Random orthogonal projection matrix A (Rm×D) -> Feature mapping h: RD → RD (geometry-aware or neural network) -> Low-dimensional GP model g0 on Rm -> Acquisition function optimizer in Rm -> Back-projection from Rm to RD via h(AT z)

- Critical path:
  1. Generate random orthogonal matrix A
  2. Train feature mapping h using semi-supervised loss
  3. Build GP model g0 using projected data Ah(xi)
  4. Optimize acquisition function in Rm
  5. Back-project optimal point to RD for evaluation
  6. Update dataset and repeat

- Design tradeoffs:
  - Higher m improves distance preservation but increases computational cost
  - Geometry-aware h requires known manifold structure but eliminates need for unlabeled data
  - Semi-supervised learning adds complexity but reduces overfitting
  - Random projection is simple but may not capture all manifold structure

- Failure signatures:
  - Poor performance on synthetic functions indicates h not learning manifold structure well
  - Runtime increases significantly with m suggests computational bottleneck
  - Noisy acquisition optimization indicates poor GP model fit in low-dimensional space

- First 3 experiments:
  1. Test on Ackley function with known spherical manifold (d=10, D=500) to verify basic functionality
  2. Test on mixed-manifold structure (linear + torus) to verify handling of complex geometries
  3. Test scalability by varying D (500, 1000, 1500) while keeping d constant to measure performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the random projection matrix A affect the convergence rate of RPM-BO in the presence of noise, and what is the theoretical relationship between the dimensionality reduction and the noise tolerance?
- Basis in paper: [inferred] The paper mentions that random projection can reduce the negative effects of noise on the convergence of learning the function g0, but does not provide a detailed theoretical analysis of this relationship.
- Why unresolved: While the paper suggests that random projection can help with noise tolerance, it does not provide a rigorous mathematical analysis of how the dimensionality reduction affects the convergence rate in noisy environments.
- What evidence would resolve it: A theoretical analysis showing the relationship between the dimensionality reduction, the noise tolerance, and the convergence rate of RPM-BO in noisy environments.

### Open Question 2
- Question: How does the choice of the semi-supervised learning approach (e.g., consistency loss) impact the performance of RPM-BO, and what are the optimal settings for the consistency loss hyperparameters?
- Basis in paper: [explicit] The paper introduces a novel unsupervised consistency loss to train the model in a semi-supervised manner to reduce overfitting issues, but does not provide a detailed analysis of the impact of this approach on the performance of RPM-BO or the optimal settings for the consistency loss hyperparameters.
- Why unresolved: While the paper introduces a novel semi-supervised learning approach, it does not provide a comprehensive analysis of its impact on the performance of RPM-BO or the optimal settings for the consistency loss hyperparameters.
- What evidence would resolve it: A detailed analysis of the impact of the semi-supervised learning approach on the performance of RPM-BO, including the optimal settings for the consistency loss hyperparameters.

### Open Question 3
- Question: How does the performance of RPM-BO compare to other state-of-the-art methods (e.g., VAE-BO, SAASBO) in terms of computational efficiency and scalability, and what are the trade-offs between these methods?
- Basis in paper: [explicit] The paper compares the performance of RPM-BO to other state-of-the-art methods, but does not provide a detailed analysis of the computational efficiency and scalability of these methods or the trade-offs between them.
- Why unresolved: While the paper compares the performance of RPM-BO to other state-of-the-art methods, it does not provide a comprehensive analysis of the computational efficiency and scalability of these methods or the trade-offs between them.
- What evidence would resolve it: A detailed analysis of the computational efficiency and scalability of RPM-BO and other state-of-the-art methods, including the trade-offs between these methods.

## Limitations
- Theoretical guarantees rely heavily on the manifold assumption being exactly satisfied, which may not hold in practical applications
- Semi-supervised learning component lacks detailed implementation specifications, making exact reproduction challenging
- Method's performance on truly high-dimensional functions (D > 1500) with complex non-manifold structures remains unverified

## Confidence
- High confidence: Theoretical convergence guarantees under manifold assumptions
- Medium confidence: Empirical performance improvements over baselines
- Low confidence: Scalability claims beyond tested dimensions

## Next Checks
1. Test robustness by varying projection dimension m and measuring sensitivity to parameter choice
2. Evaluate performance on functions with approximate rather than exact manifold structure
3. Compare computational overhead against theoretical complexity predictions across increasing dimensions