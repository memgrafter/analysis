---
ver: rpa2
title: 'AttackQA: Development and Adoption of a Dataset for Assisting Cybersecurity
  Operations using Fine-tuned and Open-Source LLMs'
arxiv_id: '2411.01073'
source_url: https://arxiv.org/abs/2411.01073
tags:
- answer
- question
- attack
- were
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AttackQA, a dataset of 25,335 question-answer
  pairs derived from the MITRE ATT&CK knowledge base, aimed at enhancing cybersecurity
  operations. The dataset was partially generated using Llama 3 8B and curated with
  fine-tuned Llama 3 70B to ensure quality.
---

# AttackQA: Development and Adoption of a Dataset for Assisting Cybersecurity Operations using Fine-tuned and Open-Source LLMs

## Quick Facts
- arXiv ID: 2411.01073
- Source URL: https://arxiv.org/abs/2411.01073
- Reference count: 40
- One-line primary result: Fine-tuned open-source models achieve 92.18% context recall and 86.07% correctness on cybersecurity Q&A tasks

## Executive Summary
AttackQA is a dataset of 25,335 question-answer pairs derived from the MITRE ATT&CK knowledge base to enhance cybersecurity operations. The dataset was partially generated using Llama 3 8B and curated with a fine-tuned Llama 3 70B model to ensure quality. The study demonstrates that fine-tuning open-source embeddings and LLMs significantly improves retrieval and generation accuracy in a RAG pipeline, outperforming proprietary models like OpenAI's GPT-4o. The pipeline leverages Llama 3 405B for evaluation, enabling a fully open-source, high-speed RAG system for SOC analysts.

## Method Summary
The method involves extracting and preprocessing the MITRE ATT&CK knowledge base into the AttackQA dataset using provided prompts and scripts. The process includes fine-tuning Llama 3 70B on 400 annotated Q&A pairs for quality control, fine-tuning Microsoft E5 Large V2 embeddings and Llama 3 8B generation models on AttackQA using SambaStudio, and evaluating with Llama 3 405B as a judge. The approach demonstrates significant improvements in context recall and answer correctness compared to proprietary models.

## Key Results
- Context recall improvements of up to 92.18% with fine-tuned embeddings versus 81.58% for OpenAI's embeddings
- Correctness scores of 86.07% when using fine-tuned models
- Fully open-source RAG pipeline with latency under 1100 tokens/s for Llama 3 8B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning open-source embeddings on domain-specific data yields better context recall than proprietary embeddings.
- Mechanism: The fine-tuning process uses contrastive learning to distinguish between relevant and irrelevant documents for a given question, leveraging domain-specific terminology.
- Core assumption: Domain-specific embeddings can capture cybersecurity jargon more effectively than general-purpose embeddings.
- Evidence anchors:
  - [abstract]: "we demonstrate that fine-tuning open-source embeddings and LLMs can yield superior accuracy compared to OpenAI's state-of-the-art proprietary embedding and LLM (GPT-4o)"
  - [section]: "Based on a metric, which in our analysis was the similarity metric, a vector database can be configured to return the top k results"
- Break condition: If the domain-specific terminology doesn't improve embedding quality or if the contrastive learning fails to distinguish relevant from irrelevant documents.

### Mechanism 2
- Claim: Fine-tuning generation models with rationales improves reasoning accuracy in answering questions.
- Mechanism: The fine-tuning process includes one-shot examples and requires the model to generate thoughts (rationales) alongside answers, helping the model learn to find the right document and use it to answer the question.
- Core assumption: Including rationales in the fine-tuning process helps the model learn to reason about the documents and answer questions more accurately.
- Evidence anchors:
  - [abstract]: "we use Llama 3 405B as a judge to evaluate answer correctness, enabling the creation of a fully open-source, high-speed RAG and evaluation pipeline"
  - [section]: "Each prompt comprised of an instruction with a one-shot example, and the retrieved list of documents, d(k), with k = 5"
- Break condition: If the model fails to generate coherent thoughts or if the rationales don't improve answer accuracy.

### Mechanism 3
- Claim: Using LLM-as-a-judge for evaluation provides objective assessment of answer correctness.
- Mechanism: Llama 3 405B evaluates answers based on whether they contradict the true answer, omit relevant details, or include irrelevant details.
- Core assumption: An LLM can effectively judge the correctness of answers by comparing them to the true answer and providing a score.
- Evidence anchors:
  - [abstract]: "we use Llama 3.1 405B as a judge to evaluate answer correctness, enabling the creation of a fully open-source, high-speed RAG and evaluation pipeline"
  - [section]: "Once again, we used the G-Eval metric with DeepEval to score answers and provide reasons for the scores"
- Break condition: If the LLM judge provides inconsistent or incorrect evaluations, or if the evaluation criteria are too subjective.

## Foundational Learning

- Concept: Domain-specific embeddings
  - Why needed here: General-purpose embeddings struggle with cybersecurity jargon, leading to poor context recall.
  - Quick check question: How does contrastive learning help embeddings distinguish between relevant and irrelevant documents?

- Concept: Fine-tuning with rationales
  - Why needed here: Including rationales helps the model learn to reason about the documents and answer questions more accurately.
  - Quick check question: Why is it important to include one-shot examples in the fine-tuning prompts?

- Concept: LLM-as-a-judge
  - Why needed here: Objective evaluation of answer correctness is difficult due to the free-form nature of generated answers.
  - Quick check question: What are the advantages of using an LLM to evaluate answer correctness compared to traditional metrics like BLEU or ROUGE?

## Architecture Onboarding

- Component map: Document preprocessing -> Dataset generation -> Quality control -> Embedding model fine-tuning -> Generation model fine-tuning -> Evaluation
- Critical path: Document preprocessing → Dataset generation → Quality control → Embedding model fine-tuning → Generation model fine-tuning → Evaluation
- Design tradeoffs:
  - Using open-source models vs. proprietary models: Open-source models are more cost-effective but may require more fine-tuning.
  - Context recall vs. answer accuracy: Improving context recall doesn't necessarily improve answer accuracy.
- Failure signatures:
  - Poor context recall: The retrieved documents don't contain the answer to the question.
  - Inaccurate answers: The generated answer doesn't match the true answer or includes irrelevant details.
  - Inconsistent evaluations: The LLM judge provides inconsistent or incorrect evaluations.
- First 3 experiments:
  1. Evaluate context recall with base embeddings vs. fine-tuned embeddings.
  2. Compare answer accuracy with base generation model vs. fine-tuned generation model.
  3. Test the effectiveness of using LLM-as-a-judge for evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AttackQA's fine-tuned models compare to proprietary models when evaluated on cybersecurity datasets outside the MITRE ATT&CK framework?
- Basis in paper: [explicit] The paper demonstrates that fine-tuning open-source models on AttackQA improves performance over proprietary models like GPT-4o, but only within the ATT&CK framework.
- Why unresolved: The study focuses exclusively on the MITRE ATT&CK knowledge base, leaving the generalizability of these improvements to other cybersecurity datasets unexplored.
- What evidence would resolve it: Comparative evaluation of AttackQA's fine-tuned models on diverse cybersecurity datasets (e.g., threat intelligence feeds, vulnerability databases) to measure cross-domain accuracy and robustness.

### Open Question 2
- Question: What are the limitations of using Llama 3 405B as an evaluator for answer correctness, and could smaller, fine-tuned models achieve similar or better performance in this role?
- Basis in paper: [explicit] Llama 3 405B is used as a judge for evaluating answer correctness, but the paper does not explore alternative or smaller models for this task.
- Why unresolved: The reliance on a large, resource-intensive model for evaluation may not be practical for all use cases, and smaller models might suffice or even outperform in specific scenarios.
- What evidence would resolve it: Benchmarking Llama 3 405B against fine-tuned smaller models (e.g., Llama 3 70B or E5 Large V2) for evaluating answer correctness on AttackQA and other datasets.

### Open Question 3
- Question: How does the inclusion of rationales in the dataset impact the accuracy of fine-tuned models, and is this effect consistent across different types of cybersecurity questions?
- Basis in paper: [explicit] The dataset includes rationales to facilitate fine-tuning and evaluation, but the paper does not analyze their specific impact on model performance.
- Why unresolved: While rationales are included, their contribution to improving model accuracy or reasoning is not quantified or explored in detail.
- What evidence would resolve it: Ablation studies comparing model performance with and without rationales in the dataset, across different question types (e.g., detection, mitigation, campaign analysis).

## Limitations

- The evaluation relies heavily on an LLM-as-a-judge without human validation, potentially introducing subjectivity and consistency concerns.
- The quality control process for the initial dataset generation is underspecified, particularly the selection criteria for the 400 Q&A pairs.
- The negative document selection strategy for embedding fine-tuning is only partially described, which could impact the effectiveness of the contrastive learning approach.

## Confidence

**High Confidence:** The claim that fine-tuning open-source embeddings on domain-specific data improves context recall is supported by clear performance improvements (92.18% vs 81.58% for OpenAI's embeddings) and a well-defined contrastive learning mechanism.

**Medium Confidence:** The assertion that fine-tuning generation models with rationales improves reasoning accuracy is partially supported by the methodology but lacks direct quantitative evidence comparing models with and without rationale inclusion.

**Low Confidence:** The effectiveness of using LLM-as-a-judge for objective evaluation is questionable given the lack of human validation and potential for inconsistent judgments across different LLM evaluators.

## Next Checks

1. **Human Evaluation Validation:** Conduct a blind human evaluation study comparing answers generated by fine-tuned vs. base models, with independent security experts assessing correctness and relevance.

2. **Cross-Evaluator Consistency Test:** Evaluate the same answers using multiple LLM judges (including GPT-4o and Claude) to assess consistency and identify potential biases in the Llama 3.405B evaluation approach.

3. **Generalization Assessment:** Test the fine-tuned models on cybersecurity questions from external sources not present in the AttackQA dataset to evaluate real-world generalization and identify potential overfitting to the MITRE ATT&CK knowledge base.