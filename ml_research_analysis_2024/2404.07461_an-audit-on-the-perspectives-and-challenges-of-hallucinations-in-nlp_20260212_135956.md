---
ver: rpa2
title: An Audit on the Perspectives and Challenges of Hallucinations in NLP
arxiv_id: '2404.07461'
source_url: https://arxiv.org/abs/2404.07461
tags:
- hallucination
- computational
- linguistics
- association
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This audit critically examines 103 peer-reviewed papers and a survey
  of 171 NLP practitioners to assess how hallucination in large language models is
  conceptualized and measured. The findings reveal significant ambiguity in defining
  hallucination, with 42.7% of papers providing no explicit definition and 57.3% lacking
  clear frameworks.
---

# An Audit on the Perspectives and Challenges of Hallucinations in NLP

## Quick Facts
- arXiv ID: 2404.07461
- Source URL: https://arxiv.org/abs/2404.07461
- Reference count: 40
- 103 peer-reviewed papers and 171 practitioner survey reveal significant ambiguity in defining and measuring hallucination in NLP

## Executive Summary
This audit critically examines how hallucination in large language models is conceptualized and measured across 103 peer-reviewed papers and through a survey of 171 NLP practitioners. The findings reveal significant ambiguity in defining hallucination, with 42.7% of papers providing no explicit definition and 57.3% lacking clear frameworks. Among definitions, 31 distinct conceptualizations emerged, highlighting the field's inconsistency. Measurement approaches also vary widely, with 87 papers using 25 different statistical metrics and no standardized methodology. The survey showed 92% of practitioners view hallucination as a weakness, with 54.32% preferring the term "hallucination" while 40.46% suggest "fabrication" as an alternative. The study identifies key challenges including vague definitions, lack of standardization, and limited sociotechnical awareness.

## Method Summary
The study analyzed 103 peer-reviewed NLP papers on hallucination through thematic analysis to extract definitions, frameworks, and measurement approaches. A survey of 171 NLP practitioners gathered perspectives on hallucination definitions, frequency of encounters, and preferred terminology. The audit categorized attributes (fluency, plausibility, confidence, intrinsic, extrinsic, non-factual, unfaithful, nonsensical) and identified 31 distinct frameworks for conceptualizing hallucination. Papers were selected from ACL Anthology and other sources using keyword searches, with survey distribution targeting NLP researchers through multiple channels.

## Key Results
- 42.7% of papers provide no explicit definition of hallucination, with 57.3% lacking clear frameworks
- 31 distinct conceptualizations of hallucination identified across literature
- 87 papers use 25 different statistical metrics with no standardized measurement methodology
- 92% of surveyed practitioners view hallucination as a weakness in NLP systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The lack of standardized definitions for hallucination in NLP is a primary barrier to addressing the problem effectively.
- Mechanism: Without clear, agreed-upon definitions, researchers and practitioners use varied terminology and conceptual frameworks, leading to inconsistent research outcomes and difficulty in comparing results across studies.
- Core assumption: A standardized definition will enable more effective communication, research, and development of solutions to mitigate hallucination.
- Evidence anchors:
  - [abstract] "The findings reveal significant ambiguity in defining hallucination, with 42.7% of papers providing no explicit definition and 57.3% lacking clear frameworks."
  - [section] "Out of the 103 papers, just 44 (42.7%) provide a definition of the term, leaving the majority—59 papers or 57.3%—either altogether omitting their understanding of hallucination in the context of their research or providing no definition or a framework."
  - [corpus] Found 25 related papers, average FMR=0.428, indicating a moderate level of similarity to the surveyed papers, suggesting the problem of inconsistent definitions is widespread.
- Break condition: If the community agrees on a single, comprehensive definition, this mechanism would no longer be a barrier.

### Mechanism 2
- Claim: The lack of standardized metrics for measuring hallucination hinders progress in the field.
- Mechanism: With 87 papers using 25 different statistical metrics, 18 datasets/models, and human evaluation, but no standardized methodology, it becomes difficult to compare results and develop effective mitigation strategies.
- Core assumption: Standardized metrics will enable more reliable and comparable research outcomes, leading to better understanding and mitigation of hallucination.
- Evidence anchors:
  - [abstract] "Measurement approaches also vary widely, with 87 papers using 25 different statistical metrics, 18 datasets/models, and human evaluation, but no standardized methodology exists."
  - [section] "This variability underscores the lack of a standardized approach."
  - [corpus] The related papers focus on hallucination evaluation, indicating the importance of this issue in the field.
- Break condition: If a set of standardized metrics is widely adopted, this mechanism would no longer be a barrier.

### Mechanism 3
- Claim: Limited awareness of hallucination in a sociotechnical context impedes the development of effective solutions.
- Mechanism: Hallucination research often lacks understanding of how the concept is conceptualized beyond its technical purview, especially in sociotechnical systems like healthcare and policy making.
- Core assumption: Considering the sociotechnical dimensions of hallucination will lead to more comprehensive and effective solutions.
- Evidence anchors:
  - [abstract] "The study identifies key challenges including vague definitions, lack of standardization, and limited sociotechnical awareness, recommending explicit documentation of frameworks, standardized terminology, and transparency in model development to address these issues."
  - [section] "There is no motivation shown to understand the non-technical considerations of hallucination."
  - [corpus] The related papers focus on various aspects of hallucination, but there is no explicit mention of sociotechnical considerations.
- Break condition: If researchers and practitioners consistently consider the sociotechnical implications of hallucination, this mechanism would no longer be a barrier.

## Foundational Learning

- Concept: Understanding the definition and implications of hallucination in NLP.
  - Why needed here: To effectively address the problem of hallucination, it is crucial to have a clear understanding of what it is and its various manifestations in NLP tasks.
  - Quick check question: Can you define hallucination in the context of NLP and provide examples of how it can manifest in different NLP tasks?

- Concept: Familiarity with the different NLP tasks and their specific challenges related to hallucination.
  - Why needed here: The audit reveals that hallucination manifests differently across various NLP tasks, such as conversational AI, abstractive summarization, and machine translation. Understanding these task-specific challenges is essential for developing targeted solutions.
  - Quick check question: What are the main challenges related to hallucination in conversational AI, abstractive summarization, and machine translation?

- Concept: Knowledge of the different approaches to measuring and evaluating hallucination in NLP.
  - Why needed here: The audit identifies various approaches to measuring hallucination, including statistical metrics, data-driven metrics, human evaluation, and mixed methodologies. Understanding these approaches is crucial for developing effective evaluation strategies.
  - Quick check question: What are the main approaches to measuring hallucination in NLP, and what are their strengths and limitations?

## Architecture Onboarding

- Component map:
  - Data collection and preprocessing -> NLP model development and training -> Hallucination detection and evaluation -> Mitigation strategies and techniques -> Sociotechnical considerations and implications

- Critical path:
  1. Collect and preprocess relevant data for the NLP task.
  2. Develop and train the NLP model using the preprocessed data.
  3. Implement hallucination detection and evaluation methods.
  4. Apply mitigation strategies to reduce hallucination.
  5. Consider sociotechnical implications and ensure responsible use.

- Design tradeoffs:
  - Accuracy vs. interpretability: More complex models may achieve higher accuracy but are often less interpretable, making it harder to identify and address hallucination.
  - Generalization vs. task-specificity: Models that generalize well across tasks may be less effective at addressing task-specific hallucination challenges.
  - Automation vs. human oversight: Automated hallucination detection methods may be faster but less reliable than human evaluation, requiring a balance between efficiency and accuracy.

- Failure signatures:
  - Inconsistent definitions and terminology across research papers and applications.
  - Lack of standardized metrics for measuring hallucination, leading to incomparable results.
  - Limited consideration of sociotechnical implications, resulting in solutions that may not be effective or responsible in real-world contexts.

- First 3 experiments:
  1. Analyze a sample of NLP research papers to identify the prevalence of different hallucination definitions and frameworks.
  2. Implement and compare various hallucination detection methods on a standard dataset to assess their effectiveness and limitations.
  3. Conduct a user study to evaluate the sociotechnical implications of hallucination in a specific NLP application, such as healthcare or education.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What standardized framework or set of definitions could be established to address the ambiguity in the term "hallucination" across different NLP subfields?
- Basis in paper: [explicit] The paper highlights 31 distinct frameworks for conceptualizing hallucination, revealing significant inconsistency and lack of consensus in the field.
- Why unresolved: The current diversity in definitions leads to misinterpretation and challenges in comparing research findings across studies and applications.
- What evidence would resolve it: A comprehensive framework agreed upon by the NLP community, with clear definitions and criteria for different contexts, would standardize the understanding and usage of "hallucination."

### Open Question 2
- Question: How can sociotechnical perspectives be integrated into the study and mitigation of hallucinations in NLP models?
- Basis in paper: [explicit] The paper identifies limited awareness of hallucination's sociotechnical nature, with only 3 out of 103 works acknowledging it, despite its relevance in applications like healthcare and policy-making.
- Why unresolved: Most research focuses on technical aspects, overlooking the societal impacts and ethical considerations of hallucinations in real-world applications.
- What evidence would resolve it: Research that explicitly incorporates sociotechnical analysis into hallucination studies, examining how social contexts influence the perception and impact of hallucinations, would address this gap.

### Open Question 3
- Question: What are the most effective methods for standardizing the measurement of hallucinations in NLP models to enable consistent evaluation across studies?
- Basis in paper: [explicit] The audit reveals 87 papers using 25 different statistical metrics and 18 datasets/models, with no standardized methodology, leading to variability and challenges in comparison.
- Why unresolved: The lack of a unified measurement approach hinders the ability to assess and compare the performance of different models and techniques in mitigating hallucinations.
- What evidence would resolve it: Development and adoption of a set of standardized metrics and evaluation protocols for hallucination measurement, validated across multiple NLP tasks and models, would resolve this issue.

## Limitations

- The heterogeneity in how hallucination is defined across the 103 surveyed papers makes it difficult to establish baseline metrics for comparison
- The survey sample of 171 practitioners may not represent the full diversity of NLP practitioners globally, particularly those in industry or non-English speaking contexts
- The distinction between technical and sociotechnical awareness of hallucination remains underexplored

## Confidence

- High confidence: The finding that 42.7% of papers lack explicit definitions and 57.3% lack clear frameworks is well-supported by direct counts from the 103-paper sample
- Medium confidence: The identification of 31 distinct definitions is credible but requires verification of the categorization methodology used to group similar conceptualizations
- Medium confidence: The survey results showing 92% of practitioners view hallucination as a weakness are reliable but may be subject to response bias depending on how the survey was distributed

## Next Checks

1. Conduct a Delphi study with 20-30 NLP researchers to attempt consensus-building around a minimal definition of hallucination, measuring inter-rater agreement before and after structured discussion
2. Implement 3-5 different hallucination measurement approaches on the same dataset and models, quantifying correlation coefficients between methods to assess whether current metrics are measuring the same construct
3. Search for and analyze 10-15 papers from fields adjacent to NLP (e.g., HCI, STS, ethics) that discuss hallucination or similar phenomena to determine what insights might transfer to technical solutions