---
ver: rpa2
title: Coercing LLMs to do and reveal (almost) anything
arxiv_id: '2402.14020'
source_url: https://arxiv.org/abs/2402.14020
tags:
- attacks
- attack
- arxiv
- tokens
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work systematizes a broad range of adversarial attack objectives
  on large language models (LLMs) beyond traditional jailbreaking. It explores attacks
  that coerce unintended behaviors such as misdirection, data extraction, denial-of-service,
  and control flow manipulation.
---

# Coercing LLMs to do and reveal (almost) anything
## Quick Facts
- arXiv ID: 2402.14020
- Source URL: https://arxiv.org/abs/2402.14020
- Reference count: 40
- Primary result: Systematizes adversarial attacks on LLMs including data extraction, misdirection, and control flow manipulation through free-text inputs

## Executive Summary
This work presents a comprehensive systematization of adversarial attacks on large language models that extend far beyond traditional jailbreaking techniques. The authors demonstrate that attackers can coerce LLMs into revealing system prompts, generating harmful content, redirecting users to malicious URLs, and exhausting computational resources through excessive token generation. The attacks exploit various model behaviors including code understanding, role confusion, and responses to rare "glitch" tokens. A key finding is that effective attacks can be conducted using small sets of non-alphabetic or non-Latin tokens, making defenses particularly challenging. The paper concludes that free-text input makes LLM outputs inherently insecure and difficult to constrain in real-world deployments.

## Method Summary
The authors conducted extensive experiments systematically exploring adversarial attack objectives on large language models across multiple categories including misdirection, data extraction, denial-of-service, and control flow manipulation. They tested various attack vectors using different token types, particularly focusing on non-alphabetic and non-Latin characters, and measured attack success through qualitative observations of model outputs. The experiments involved crafting specific prompts designed to trigger unintended behaviors, with particular attention to how models interpret code, role assignments, and rare tokens. Attack effectiveness was evaluated by measuring the model's compliance with adversarial objectives and the length of target strings that could be successfully extracted or generated.

## Key Results
- LLMs can be reliably manipulated to reveal system prompts and generate harmful content through carefully crafted adversarial inputs
- Non-alphabetic and non-Latin tokens are particularly effective for attacks, with small token sets sufficient for successful exploitation
- Attack success scales with target string length, demonstrating predictable patterns in model vulnerability
- Models can be coerced into redirecting users to malicious URLs or generating excessive tokens to exhaust computational resources

## Why This Works (Mechanism)
The attacks work by exploiting fundamental properties of how large language models process and generate text. Models learn statistical relationships between tokens and their contexts, making them vulnerable to carefully crafted sequences that trigger unintended behaviors. The effectiveness of non-alphabetic tokens suggests models may process these differently than standard text, potentially due to their rarity in training data. Role confusion attacks work by exploiting the model's tendency to follow instructions based on perceived identity or context. Code understanding attacks leverage the model's ability to interpret programming syntax, which can be manipulated to execute unintended operations. The scalability of attacks with string length indicates that models maintain contextual awareness over extended sequences, allowing attackers to build up desired outputs progressively.

## Foundational Learning
- **Tokenization and subword units**: Why needed - Understanding how models break text into tokens is crucial for crafting effective attacks; Quick check - Verify which token types (alphabetic vs non-alphabetic) are most vulnerable to manipulation
- **Context window limitations**: Why needed - Attack success may depend on how much context the model can maintain; Quick check - Test whether attack effectiveness varies with different context window sizes
- **Model role and instruction following**: Why needed - Many attacks exploit how models interpret and respond to perceived roles; Quick check - Confirm that role confusion attacks work across different instruction-following models
- **Code interpretation capabilities**: Why needed - Code-based attacks leverage models' understanding of programming syntax; Quick check - Validate that code injection attacks succeed across different programming languages
- **Rare token handling**: Why needed - Non-standard tokens may be processed differently due to training data scarcity; Quick check - Test whether rare token attacks generalize across different token vocabularies
- **Output generation constraints**: Why needed - Understanding model output controls is essential for DoS and content generation attacks; Quick check - Verify that token limit exhaustion attacks work across different model configurations

## Architecture Onboarding
Component map: Input preprocessing -> Tokenization -> Context encoding -> Attention layers -> Output generation -> Post-processing
Critical path: Attack prompt → Token encoding → Attention mechanism → Generation head → Final output
Design tradeoffs: Free-text flexibility vs security constraints; Model capability vs vulnerability to manipulation; Token diversity vs attack surface
Failure signatures: Unexpected output patterns; Compliance with adversarial objectives; Token limit exhaustion; System prompt leakage
First experiments:
1. Test non-alphabetic token effectiveness across multiple model architectures
2. Measure attack success rates with varying context window sizes
3. Evaluate defensive filtering mechanisms against demonstrated attack vectors

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope concentrated on specific model versions and providers, limiting generalizability across evolving LLM landscape
- Attack success rates reported qualitatively without comprehensive statistical validation across different model architectures
- Limited exploration of multilingual or non-Latin script vulnerabilities despite findings about non-alphabetic tokens
- Distinction between model-level vulnerabilities versus implementation-level weaknesses not always clearly delineated

## Confidence
Attack feasibility and diversity: High - Numerous concrete examples and demonstrations across multiple attack types with clear reproducibility
Security implications: Medium - Compelling demonstrations but real-world impact depends on deployment context and unexplored defensive measures
Token-based attack effectiveness: Medium - Supported by examples but requires validation of statistical significance and generalizability

## Next Checks
1. Conduct controlled experiments testing attack success rates across multiple model versions and providers to establish robustness beyond studied models
2. Perform ablation studies systematically varying attack parameters to quantify statistical significance and identify critical thresholds
3. Design defensive mechanism experiments testing input filtering, output constraints, and model fine-tuning to evaluate practical mitigation feasibility