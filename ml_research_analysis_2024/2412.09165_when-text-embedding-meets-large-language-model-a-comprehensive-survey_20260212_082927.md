---
ver: rpa2
title: 'When Text Embedding Meets Large Language Model: A Comprehensive Survey'
arxiv_id: '2412.09165'
source_url: https://arxiv.org/abs/2412.09165
tags:
- embedding
- text
- language
- learning
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey presents a comprehensive overview of how large language
  models (LLMs) are integrated with text embeddings, a foundational technology in
  natural language processing. The authors systematically categorize the interplay
  between LLMs and text embeddings into three themes: LLM-augmented text embedding,
  LLMs as text embedders, and text embedding understanding with LLMs.'
---

# When Text Embedding Meets Large Language Model: A Comprehensive Survey

## Quick Facts
- arXiv ID: 2412.09165
- Source URL: https://arxiv.org/abs/2412.09165
- Reference count: 40
- This survey presents a comprehensive overview of how large language models (LLMs) are integrated with text embeddings, covering LLM-augmented text embedding, LLMs as text embedders, and text embedding understanding with LLMs.

## Executive Summary
This survey provides a systematic overview of the integration between large language models (LLMs) and text embeddings, categorizing the interplay into three themes: LLM-augmented text embedding, LLMs as text embedders, and text embedding understanding with LLMs. The authors trace the evolution of text embeddings from early statistical methods to deep learning and the current era of pre-trained language models. The survey introduces two emerging tasks: long context compression and embedding inversion, while highlighting ongoing challenges such as false negative detection, low-resource languages, and privacy concerns. The integration of LLMs offers new opportunities to improve text embeddings through data augmentation, instruction following, and enhanced understanding, but also introduces new challenges that require further research.

## Method Summary
The survey systematically categorizes LLM-text embedding integration into three themes: LLM-augmented text embedding (LLMs generate synthetic training data or supervision signals), LLMs as text embedders (adapting LLM architectures for embedding tasks), and text embedding understanding with LLMs (using LLMs for tasks like long context compression and embedding inversion). The methodology involves reviewing existing literature across these themes, identifying key approaches such as contrastive learning with InfoNCE loss, parameter-efficient fine-tuning (LoRA), multi-task learning, and various pooling strategies. The survey evaluates embedding quality using metrics like Pearson/Spearman correlation for semantic textual similarity, Recall@k and MRR for information retrieval, and compression efficiency metrics for long context compression tasks.

## Key Results
- LLMs can generate high-quality synthetic training data for contrastive learning, improving embedding generalization
- LLMs can be adapted as universal embedders through various pooling strategies and fine-tuning methods
- LLM-based context compression and embedding inversion represent emerging research directions that leverage LLM understanding capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM-generated synthetic training data improves embedding generalization by providing diverse, semantically rich positive/negative pairs.
- **Mechanism:** LLMs can generate task-specific instructions, semantically similar positive samples, and contextually relevant hard negatives, which enrich the contrastive learning training signal beyond human-curated datasets.
- **Core assumption:** LLM-generated pairs are semantically accurate and capture the task's relevance definition.
- **Evidence anchors:**
  - [abstract] states LLMs can "generate high-quality, fine-grained text datasets" and "provide supervision signals for existing data."
  - [section 3] describes LLM synthesis of symmetric/positive pairs (e.g., NLI-style entailments), asymmetric query-document pairs, and hard negatives, improving retrieval and STS performance.
  - [corpus] lacks specific papers, but the related survey "LLMs are Also Effective Embedding Models" suggests synthetic data can rival or surpass human-curated datasets.
- **Break condition:** If LLM outputs contain factual errors, hallucinated content, or mislabeled pairs, the learned embeddings will reflect these inaccuracies.

### Mechanism 2
- **Claim:** LLMs can serve as universal embedders by pooling hidden states and optionally fine-tuning for task adaptation.
- **Mechanism:** LLMs' decoder-only or encoder-decoder architectures can be repurposed: hidden states from a chosen layer (often the last) are pooled (mean, weighted, or special tokens) to form sentence embeddings. Fine-tuning methods like LoRA or instruction tuning adapt the LLM for embedding tasks.
- **Core assumption:** The pooled hidden states from LLMs retain sufficient semantic information for downstream tasks.
- **Evidence anchors:**
  - [abstract] identifies three themes: "LLMs as text embedders, adapting their innate capabilities for high-quality embedding."
  - [section 4] details pooling strategies (first, mean, last, special tokens) and architectural adjustments (bi-directional attention, projectors) to obtain embeddings from LLMs like Mistral, LLaMA, Qwen.
  - [corpus] references "LLMs are Also Effective Embedding Models" implying LLMs can match or exceed traditional PLM embeddings when fine-tuned.
- **Break condition:** If causal attention dominates or if fine-tuning data is insufficient, embeddings may lose task-specific relevance or suffer from anisotropy.

### Mechanism 3
- **Claim:** LLMs enable interpretable embeddings and context compression by reasoning about and summarizing long texts.
- **Mechanism:** LLMs can be prompted to compress long contexts into concise soft prompts or natural language summaries, which are then aligned to the original text via KL divergence or reconstruction loss. This reduces inference cost and storage while preserving semantics.
- **Core assumption:** LLM-generated compressed representations retain key information for accurate task performance.
- **Evidence anchors:**
  - [abstract] introduces "long context compression" and "embedding inversion" as emerging tasks leveraging LLM understanding of embeddings.
  - [section 5.1] describes soft prompt methods (e.g., Gist, PC) and context distillation (e.g., Nugget, Text summarization) that use LLMs to compress and align compressed tokens/embeddings to originals.
  - [corpus] lacks direct evidence; however, the survey mentions "compression efficiency and generation consistency" as evaluation criteria.
- **Break condition:** If compression ratio is too high or LLM reasoning is insufficient, key information may be lost, degrading downstream task accuracy.

## Foundational Learning

- **Concept:** Contrastive learning loss (e.g., InfoNCE) and its components (anchors, positives, negatives).
  - **Why needed here:** All embedding methods in this survey rely on contrastive learning, whether with human data or LLM-generated synthetic pairs. Understanding how positives/negatives are constructed and how the loss optimizes embedding space is essential.
  - **Quick check question:** What is the role of hard negatives in InfoNCE, and how does their quality affect embedding quality?
- **Concept:** Text embedding evaluation metrics (e.g., Pearson/Spearman for STS, Recall/MRR for IR).
  - **Why needed here:** The survey extensively evaluates embedding models on STS, IR, universal embedding, long context compression, and embedding inversion. Knowing how each metric reflects semantic similarity or retrieval effectiveness is crucial for interpreting results.
  - **Quick check question:** Why is Spearman correlation often preferred over Pearson for STS evaluation?
- **Concept:** Pooling strategies (first, mean, last, special tokens) and their impact on embedding quality.
  - **Why needed here:** LLM-based embedders use various pooling methods to extract a single vector from hidden states. Each strategy interacts differently with LLM architecture (e.g., causal vs. bi-directional attention), affecting semantic coverage and task performance.
  - **Quick check question:** How does "last pooling" differ from "mean pooling" in a decoder-only LLM, and when is each appropriate?

## Architecture Onboarding

- **Component map:** Input: Raw text (possibly with instructions/examples) → LLM backbone: Decoder-only (e.g., Mistral, LLaMA) or encoder-decoder (e.g., T5) → Attention layer: Causal or converted to bi-directional → Pooling layer: Mean, weighted, first, last, or special token-based → Optional projector: Linear layer for dimensionality reduction or sparse representation → Optional PEFT module: LoRA, BitFit for efficient fine-tuning → Output: d-dimensional embedding
- **Critical path:** Input → LLM layers → Pooling → (Optional projector) → Output
- **Design tradeoffs:**
  - Attention: Causal (faster, original LLM behavior) vs. bi-directional (better semantic coverage, needs adaptation)
  - Pooling: Simple (mean) vs. complex (weighted, special tokens) for semantic focus
  - Projector: None (full dimension) vs. low-dim (efficiency) vs. sparse (retrieval efficiency)
  - Fine-tuning: Full (best performance) vs. PEFT (efficiency) vs. training-free (speed, lower quality)
- **Failure signatures:**
  - High anisotropy in embedding space (cosine similarity fails to reflect semantics)
  - Poor performance on out-of-domain or low-resource languages
  - Inconsistent embeddings for similar texts (noisy supervision)
  - Excessive computational cost or memory usage
- **First 3 experiments:**
  1. **Compare pooling strategies:** Fine-tune Mistral-7B on MS MARCO with mean, weighted, and last pooling; evaluate on BEIR retrieval.
  2. **Assess synthetic data quality:** Use LLM to generate 1M STS-style pairs; train contrastive embedder; compare Spearman correlation on STS-B vs. human-curated data.
  3. **Test context compression:** Implement soft prompt compression (Gist-style) on long documents; measure compression rate vs. ROUGE-L retention for RAG tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the theoretical lower bound on dimensionality for lossless compression of text embeddings?
- **Basis in paper:** [explicit] The paper discusses high-dimensional text embeddings from LLMs and mentions the need for efficient compression algorithms while maintaining performance.
- **Why unresolved:** Current research focuses on empirical approaches like Matryoshka representation learning and sparse coding, but lacks theoretical foundations for determining the minimum dimensionality required for lossless information preservation.
- **What evidence would resolve it:** Mathematical proofs establishing information-theoretic bounds on dimensionality requirements, combined with experimental validation across diverse text embedding tasks and datasets.

### Open Question 2
- **Question:** How can we effectively mitigate false negative detection in contrastive learning for text embeddings?
- **Basis in paper:** [explicit] The paper identifies false negative detection as a key challenge, noting that incorrect supervision signals disrupt the learning process and current solutions have limitations.
- **Why unresolved:** Existing approaches like improved loss functions and sampling strategies rely heavily on researcher experience rather than theoretical foundations, and have only been validated on limited datasets.
- **What evidence would resolve it:** Development of theoretically-grounded methods that can reliably identify and handle false negatives at scale, validated across diverse datasets and embedding tasks.

### Open Question 3
- **Question:** How can we develop text embeddings that simultaneously excel at both generation tasks and semantic understanding?
- **Basis in paper:** [inferred] The paper discusses the tension between using LLMs for generation versus text embedding, noting that current methods may compromise generative capabilities to achieve good embeddings.
- **Why unresolved:** Current research focuses on separate approaches for generation and embedding, with limited exploration of unified architectures that maintain both capabilities effectively.
- **What evidence would resolve it:** Empirical demonstrations of models that achieve state-of-the-art performance on both generation tasks and embedding-based downstream tasks without compromising either capability.

## Limitations
- The survey relies on external literature for empirical validation, lacking direct experimental results
- Limited discussion of potential failure modes and their impact on embedding quality
- Insufficient detail on datasets and results for emerging tasks like long context compression and embedding inversion

## Confidence
- **Medium**: The integration of LLMs with text embeddings is well-motivated but lacks direct experimental validation in the survey.
- **Medium**: The mechanisms proposed (synthetic data, LLM embedders, context compression) are plausible but rely on external evidence.
- **Low**: The survey does not provide concrete datasets or results for emerging tasks like long context compression and embedding inversion.

## Next Checks
1. **Validate synthetic data quality**: Generate a synthetic STS dataset using an LLM and compare its quality to human-curated datasets by evaluating Spearman correlation on STS-B.
2. **Test LLM embedding strategies**: Implement and compare different pooling strategies (mean, weighted, last) in an LLM-based embedder on a retrieval benchmark like BEIR.
3. **Assess context compression**: Implement a soft prompt compression method (e.g., Gist-style) on long documents and measure compression efficiency versus retention of semantic information for RAG tasks.