---
ver: rpa2
title: 'Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models:
  A Causal Perspective'
arxiv_id: '2403.18346'
source_url: https://arxiv.org/abs/2403.18346
tags:
- question
- causal
- mllms
- language
- biases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates unimodal biases in Multimodal Large Language
  Models (MLLMs) through a causal framework, revealing their over-reliance on language
  or vision information during complex reasoning tasks. The authors propose MORE,
  a novel dataset with 12,000 VQA instances requiring multi-hop reasoning while overcoming
  such biases, and CAVE, a causality-enhanced agent framework that improves bias mitigation
  through question decomposition, self-reflection, and external knowledge retrieval.
---

# Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective

## Quick Facts
- **arXiv ID**: 2403.18346
- **Source URL**: https://arxiv.org/abs/2403.18346
- **Reference count**: 39
- **Primary result**: The paper investigates unimodal biases in Multimodal Large Language Models (MLLMs) through a causal framework, revealing their over-reliance on language or vision information during complex reasoning tasks.

## Executive Summary
This paper addresses a critical challenge in multimodal AI systems: the tendency of Multimodal Large Language Models (MLLMs) to exhibit unimodal biases during complex reasoning tasks. Through a novel causal framework, the authors demonstrate that MLLMs systematically over-rely on either language or vision information when faced with multi-hop reasoning problems. To investigate this phenomenon, they introduce MORE, a carefully curated dataset of 12,000 Visual Question Answering instances designed to require integration of multiple modalities while overcoming these biases. The paper proposes CAVE, a causality-enhanced agent framework that employs question decomposition, self-reflection, and external knowledge retrieval to mitigate these biases. Experimental results show that while MLLMs struggle significantly on the MORE dataset (with Gemini Pro Vision achieving only 22.3% accuracy), CAVE substantially improves performance to 33.2% accuracy, demonstrating its effectiveness while also highlighting the persistent challenges in achieving robust multimodal reasoning.

## Method Summary
The authors develop a causal framework to analyze unimodal biases in MLLMs, identifying how these models systematically over-rely on either language or vision information during complex reasoning tasks. They create MORE, a novel dataset with 12,000 VQA instances specifically designed to require multi-hop reasoning while overcoming unimodal biases through controlled scenarios. The CAVE framework is proposed as a mitigation strategy, employing a three-pronged approach: question decomposition to break down complex queries, self-reflection mechanisms to evaluate reasoning steps, and external knowledge retrieval to supplement model capabilities. The framework is evaluated on state-of-the-art MLLMs including Gemini Pro Vision and GPT-4V, demonstrating significant performance improvements while revealing the persistent challenges in achieving robust multimodal reasoning despite these interventions.

## Key Results
- MLLMs exhibit significant unimodal biases, over-relying on either language or vision information during complex reasoning tasks
- The MORE dataset reveals substantial performance gaps, with Gemini Pro Vision achieving only 22.3% accuracy on multi-hop reasoning tasks
- CAVE framework improves performance to 33.2% accuracy on the MORE test set, demonstrating effective bias mitigation
- Even with mitigation strategies, MLLMs struggle with robust multimodal reasoning, indicating ongoing challenges in the field

## Why This Works (Mechanism)
The causal framework reveals that unimodal biases in MLLMs stem from their training data and architectural predispositions, where models learn to favor certain modalities during reasoning processes. MORE's multi-hop reasoning design forces models to integrate multiple information sources systematically, exposing these biases through carefully controlled scenarios. CAVE's question decomposition breaks complex reasoning tasks into manageable sub-problems, while self-reflection mechanisms allow the model to evaluate and correct its reasoning path. External knowledge retrieval supplements the model's inherent limitations, providing additional context that helps overcome the over-reliance on single modalities. The framework's effectiveness lies in its ability to restructure the reasoning process, forcing explicit consideration of multiple modalities rather than allowing the model to default to its biased tendencies.

## Foundational Learning

**Causal Framework Analysis**
- *Why needed*: Understanding the root causes of unimodal biases requires systematic investigation of how MLLMs process and integrate multimodal information
- *Quick check*: Evaluate whether identified biases persist across different MLLM architectures and training paradigms

**Multi-hop Reasoning Design**
- *Why needed*: Complex reasoning tasks require integration of multiple information sources, making them ideal for exposing unimodal biases
- *Quick check*: Verify that MORE instances truly require cross-modal integration by testing with unimodal models

**Question Decomposition Strategies**
- *Why needed*: Breaking down complex queries into simpler components helps models overcome their tendency to rely on single modalities
- *Quick check*: Assess whether decomposition improves performance on simpler reasoning tasks before complex ones

**Self-reflection Mechanisms**
- *Why needed*: Models need to evaluate their own reasoning process to identify and correct modality over-reliance
- *Quick check*: Compare performance with and without reflection components on bias-exposing tasks

**External Knowledge Integration**
- *Why needed*: Supplementing model capabilities with external information helps overcome inherent limitations in multimodal reasoning
- *Quick check*: Measure performance improvements when knowledge retrieval is selectively applied versus always applied

## Architecture Onboarding

**Component Map**: MORE Dataset (VQA instances) -> MLLM (Gemini Pro Vision, GPT-4V) -> CAVE Framework (Question Decomposition -> Self-reflection -> External Knowledge Retrieval) -> Performance Evaluation

**Critical Path**: Question Decomposition -> Self-reflection -> External Knowledge Retrieval -> Final Answer Generation

**Design Tradeoffs**: The framework trades computational complexity for improved accuracy, requiring multiple inference steps rather than single-pass reasoning. This increases latency but provides more robust answers by forcing explicit modality integration.

**Failure Signatures**: Performance degradation on MORE indicates persistent unimodal biases; failure to decompose questions effectively suggests limitations in understanding complex reasoning requirements; poor external knowledge retrieval indicates gaps in the model's ability to identify relevant supplementary information.

**First Experiments**:
1. Evaluate baseline MLLM performance on MORE without any mitigation to establish unimodal bias severity
2. Test CAVE's individual components in isolation to quantify their individual contributions to bias mitigation
3. Conduct ablation studies removing each CAVE component to understand their relative importance

## Open Questions the Paper Calls Out
None

## Limitations
- The MORE dataset contains only 12,000 instances across 9 scenarios, which may be insufficient for training robust multimodal models
- Evaluation primarily focuses on Gemini Pro Vision and GPT-4V, limiting generalizability across the broader MLLM landscape
- CAVE's performance gains, while significant, still result in relatively low absolute accuracy (33.2%), suggesting incomplete resolution of underlying bias issues

## Confidence

**High Confidence**: The identification of unimodal biases in MLLMs through causal analysis is well-supported by experimental evidence. The observation that models over-rely on either language or vision cues during complex reasoning tasks is clearly demonstrated through controlled experiments.

**Medium Confidence**: The effectiveness of the CAVE framework in mitigating biases is demonstrated but may be somewhat overestimated. While improvements are statistically significant, the absolute performance levels remain low, and the framework's reliance on GPT-4V for question decomposition introduces potential confounding factors.

**Low Confidence**: The generalizability of the MORE dataset's design principles to real-world scenarios is uncertain. The curated nature of the dataset, while valuable for controlled analysis, may not capture the full complexity and variability of natural multimodal reasoning tasks.

## Next Checks

1. Evaluate CAVE's performance across a broader range of MLLMs (e.g., LLaVA, BLIP-2, Flamingo) to assess generalizability beyond Gemini Pro Vision and GPT-4V

2. Conduct ablation studies on CAVE's components (question decomposition, self-reflection, external knowledge retrieval) to quantify their individual contributions to bias mitigation

3. Test MORE's scenarios with human annotators to establish baseline human performance and validate the dataset's difficulty calibration against human reasoning capabilities