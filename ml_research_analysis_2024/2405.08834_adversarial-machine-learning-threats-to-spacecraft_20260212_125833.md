---
ver: rpa2
title: Adversarial Machine Learning Threats to Spacecraft
arxiv_id: '2405.08834'
source_url: https://arxiv.org/abs/2405.08834
tags:
- spacecraft
- attacks
- data
- learning
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper demonstrates that spacecraft with on-board machine\
  \ learning systems are vulnerable to adversarial machine learning (AML) attacks,\
  \ which can compromise critical navigation and vision systems. Through experimental\
  \ simulations using NASA\u2019s Core Flight System and OnAIR Platform, the authors\
  \ successfully executed two types of AML attacks: data poisoning of a GNC algorithm\
  \ (increasing mean squared error from near-zero to up to 8.8 \xD7 10^25) and evasion\
  \ attacks on a computer vision system (reducing crater detection accuracy to below\
  \ 50%)."
---

# Adversarial Machine Learning Threats to Spacecraft

## Quick Facts
- arXiv ID: 2405.08834
- Source URL: https://arxiv.org/abs/2405.08834
- Reference count: 29
- Key outcome: Spacecraft ML systems vulnerable to AML attacks compromising navigation and vision systems

## Executive Summary
This paper demonstrates that spacecraft with on-board machine learning systems are vulnerable to adversarial machine learning (AML) attacks, which can compromise critical navigation and vision systems. Through experimental simulations using NASA's Core Flight System and OnAIR Platform, the authors successfully executed two types of AML attacks: data poisoning of a GNC algorithm (increasing mean squared error from near-zero to up to 8.8 × 10^25) and evasion attacks on a computer vision system (reducing crater detection accuracy to below 50%). The study introduces an AML threat taxonomy for spacecraft based on mission objectives, resource constraints, learning architecture, storage architecture, C&DH accessibility, and model exposure. Findings indicate that as spacecraft increasingly rely on AI, incorporating AML-focused security measures from the design phase is critical to mitigate risks.

## Method Summary
The researchers conducted experimental simulations using NASA's Core Flight System (cFS) and OnAIR Platform to test two types of AML attacks on spacecraft systems. They implemented a data poisoning attack by altering the learning rate parameter of a GNC algorithm trained on New Shepard suborbital flight data, and an evasion attack by adding Poisson noise to image frames processed by a crater detection model trained on Perseverance rover landing footage. The experiments measured the impact on system performance through metrics like mean squared error and crater detection accuracy, demonstrating the effectiveness of these attacks in degrading critical spacecraft functions.

## Key Results
- AML attacks can increase GNC algorithm MSE from near-zero to up to 8.8 × 10^25
- Crater detection accuracy can be reduced below 50% through evasion attacks
- AML threat taxonomy developed based on mission objectives, resource constraints, learning architecture, storage architecture, C&DH accessibility, and model exposure
- Recommendation for layered security strategies and zero trust principles in spacecraft design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The experimental framework is effective because it uses a realistic spacecraft simulation environment (NASA cFS + OnAIR Platform) to test adversarial ML attacks.
- Mechanism: By running AML attacks within NASA's Core Flight System and OnAIR Platform, the researchers were able to create high-fidelity simulations that closely mimic the conditions and constraints of actual spacecraft operations, allowing for accurate measurement of attack impact.
- Core assumption: The NASA cFS and OnAIR Platform provide sufficiently accurate representations of real spacecraft systems for experimental purposes.
- Evidence anchors:
  - [abstract] "Through experimental simulations using NASA's Core Flight System (cFS) and NASA's On-board Artificial Intelligence Research (OnAIR) Platform, the authors successfully executed two types of AML attacks"
  - [section] "The NASA cFS is a generic open source flight software architecture framework utilized on flagship spacecraft... cFS has been employed on a variety of missions such as the Lunar Reconnaissance Orbiter and the Global Precipitation Measurement Mission."
- Break condition: If the simulation environment fails to accurately capture critical spacecraft behaviors or constraints, the experimental results may not generalize to real spacecraft.

### Mechanism 2
- Claim: The paper's AML threat taxonomy is effective because it considers multiple mission-specific parameters that influence spacecraft vulnerability.
- Mechanism: By systematically categorizing AML threats based on mission objectives, resource constraints, learning architecture, storage architecture, C&DH accessibility, and model exposure, the taxonomy provides a structured framework for assessing potential attack vectors.
- Core assumption: The identified parameters are comprehensive and directly relevant to spacecraft AML vulnerability.
- Evidence anchors:
  - [section] "The threat of AML attacks faced by a spacecraft is present at all stages of its life cycle... Two broad classes of AI systems and functionalities are employed by spacecraft, predictive and generative AI, both of which contrast in regards to their threat matrix."
  - [section] "The taxonomy of AML threats faced by a spacecraft is predicated on the application of predictive AI versus generative AI in addition to the following parameters: 1) Mission Objectives and Operational Context 2) Resource Constraints 3) Learning Architecture, Method, and Stage 4) Storage Architecture 5) Command and Data Handling (C&DH) Accessibility 6) Model Exposure and Interaction"
- Break condition: If critical parameters are omitted or if the taxonomy structure doesn't capture the complexity of real-world spacecraft systems, the framework may fail to identify important vulnerabilities.

### Mechanism 3
- Claim: The experimental attacks demonstrated in the paper are effective because they target specific vulnerabilities in spacecraft ML systems that are likely to be exploited by real adversaries.
- Mechanism: By focusing on poisoning attacks against GNC algorithms and evasion attacks against computer vision systems, the researchers demonstrated how adversaries could compromise critical spacecraft functions through relatively simple manipulations.
- Core assumption: The specific attacks chosen represent realistic threat scenarios that could be executed by actual adversaries.
- Evidence anchors:
  - [abstract] "the authors successfully executed two types of AML attacks: data poisoning of a GNC algorithm (increasing mean squared error from near-zero to up to 8.8 × 10^25) and evasion attacks on a computer vision system (reducing crater detection accuracy to below 50%)"
  - [section] "To demonstrate an adversarial attack against an autonomous spacecraft navigation model, we emulated a white-box scenario whereby an adversary breached ground infrastructure to alter the integrity of the model and re-flash it onto the spacecraft's SSD."
- Break condition: If the attacks are too simplistic or don't account for real-world constraints (e.g., detection mechanisms, operational safeguards), their effectiveness may be overstated.

## Foundational Learning

- Concept: Machine Learning Basics
  - Why needed here: Understanding how ML models work is essential for comprehending how AML attacks can manipulate them.
  - Quick check question: What is the difference between supervised and unsupervised learning, and why does this distinction matter for AML attacks?

- Concept: Cybersecurity Fundamentals
  - Why needed here: AML attacks are a subset of cybersecurity threats, so basic security concepts are necessary for understanding the attack landscape.
  - Quick check question: What is the principle of least privilege, and how could it help mitigate AML attacks on spacecraft?

- Concept: Spacecraft Systems Engineering
  - Why needed here: The unique constraints and architecture of spacecraft systems influence their vulnerability to AML attacks.
  - Quick check question: How do size, weight, and power (SWaP) constraints affect the design of ML systems on spacecraft?

## Architecture Onboarding

- Component map:
  - NASA Core Flight System (cFS) -> OnAIR Platform -> Command and Data Handling (C&DH) subsystem -> Storage architectures (EEPROM, SSD) -> ML models (GNC, computer vision)

- Critical path: Attack execution involves breaching ground segment → modifying model or training data → uploading to spacecraft → model execution → system compromise

- Design tradeoffs: Performance vs. security (e.g., lightweight models for resource constraints vs. robustness against attacks)

- Failure signatures: Unexpected MSE increases in GNC algorithms, sudden drops in classification accuracy, anomalous system behavior

- First 3 experiments:
  1. Replicate the GNC poisoning attack using a simplified SGD model to understand learning rate manipulation effects
  2. Implement a basic evasion attack on a crater detection model using noise injection to observe classification degradation
  3. Test different storage architectures (EEPROM vs. SSD) for their susceptibility to model tampering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific detection mechanisms can be implemented in spacecraft watchdogs and anomaly detection systems to identify adversarial perturbations in model performance early?
- Basis in paper: [explicit] The paper mentions that spacecraft watchdogs, anomaly detection systems, and intrusion prevention systems should be trained to detect atypical perturbations in model performance.
- Why unresolved: The paper suggests the importance of these systems but does not detail specific detection mechanisms or methodologies that could be employed.
- What evidence would resolve it: Development and testing of detection mechanisms within spacecraft simulation environments, followed by empirical validation of their effectiveness in identifying adversarial perturbations.

### Open Question 2
- Question: How can spacecraft engineers effectively balance the trade-off between model accuracy and processing speed in resource-constrained environments to mitigate the impact of AML attacks?
- Basis in paper: [inferred] The paper discusses the resource constraints of spacecraft and how adversaries may exploit these by designing attacks that target the trade-off between speed and accuracy.
- Why unresolved: The paper highlights the issue but does not provide a clear framework or guidelines for engineers to follow when balancing these competing priorities.
- What evidence would resolve it: Empirical studies and simulations demonstrating optimal configurations that balance accuracy and speed, alongside case studies of real-world implementations in spacecraft.

### Open Question 3
- Question: What are the most effective layered security strategies that incorporate zero trust principles for spacecraft, considering the unique constraints and operational contexts of space missions?
- Basis in paper: [explicit] The paper recommends adopting layered security strategies that incorporate zero trust principles for spacecraft.
- Why unresolved: While the paper advocates for these strategies, it does not provide specific examples or frameworks tailored to the unique environment of spacecraft.
- What evidence would resolve it: Development of comprehensive security frameworks specifically designed for spacecraft, including detailed implementation guidelines and validation through testing in simulated or actual space missions.

## Limitations
- Experimental framework relies on simulation environments rather than actual spacecraft hardware
- AML threat taxonomy not validated against real-world spacecraft mission data or incident reports
- Focus primarily on predictive AI systems, potentially overlooking vulnerabilities in generative AI applications

## Confidence
- High: Simulation results demonstrating MSE increase and accuracy reduction under controlled conditions
- Medium: Real-world applicability of experimental results due to simulation environment limitations
- Low: Effectiveness of AML threat taxonomy without validation against actual spacecraft data

## Next Checks
1. **Hardware-in-the-Loop Testing**: Implement the demonstrated AML attacks on actual spacecraft hardware or high-fidelity hardware emulators to validate simulation results and identify practical implementation barriers.

2. **Taxonomy Validation**: Apply the AML threat taxonomy to historical spacecraft mission data and incident reports to assess its effectiveness in identifying real-world vulnerabilities and to refine the parameter framework.

3. **Resource-Constrained Defense Testing**: Evaluate the recommended security measures (layered security, zero trust) under realistic spacecraft resource constraints to develop practical implementation guidelines that balance security needs with size, weight, and power limitations.