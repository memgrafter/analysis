---
ver: rpa2
title: Learning to Decode Collaboratively with Multiple Language Models
arxiv_id: '2403.03870'
source_url: https://arxiv.org/abs/2403.03870
tags:
- base
- assistant
- co-llm
- training
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to teach multiple large language models
  (LLMs) to collaborate by interleaving their generations at the token level. The
  decision of which LLM generates the next token is modeled as a latent variable,
  and the marginal likelihood of a training set under this latent variable model is
  optimized.
---

# Learning to Decode Collaboratively with Multiple Language Models

## Quick Facts
- arXiv ID: 2403.03870
- Source URL: https://arxiv.org/abs/2403.03870
- Reference count: 40
- One-line primary result: Token-level collaboration between LLMs improves performance on instruction-following, domain-specific QA, and reasoning tasks

## Executive Summary
This paper introduces a method for teaching multiple large language models to collaborate by interleaving their generations at the token level. The core innovation is modeling the decision of which LLM generates each token as a latent variable, then optimizing the marginal likelihood of the training data. This allows a base LLM to automatically learn when to generate tokens itself versus when to call on assistant language models, all without direct supervision. The approach is particularly effective in cross-domain settings where a generalist base LLM learns to invoke domain expert models.

## Method Summary
The method introduces Co-LLM, a framework where multiple LLMs collaborate through token-level decision making. The base LLM decides whether to generate each token or defer to an assistant model by learning a binary decision function parameterized as a linear layer on top of the base model's hidden states. Training optimizes the marginal likelihood of the data under this latent variable model, using weak supervision for initialization by comparing token-level predictions between models. During decoding, a threshold-based selection mechanism determines which model generates each token, enabling interpretable collaboration patterns while maintaining computational efficiency.

## Key Results
- Performance of the joint system exceeds that of individual models on instruction-following, domain-specific QA, and reasoning tasks
- Learned latent decisions exhibit interesting collaboration patterns, such as template-filling
- Method is effective in combining models of different sizes and architectures
- Enables a modular approach to continued pretraining and task-specific fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-level latent variable modeling enables the base model to learn when to defer to the assistant without direct supervision
- Mechanism: By modeling Zt as a latent variable and optimizing the marginal likelihood P(X), the base model automatically learns a binary decision function Pθ(Zt|X<t) that predicts when the assistant model is more likely to generate the correct token
- Core assumption: The assistant model's conditional distribution Pi(Xt|X<t) is accessible during training, even if only for ground truth tokens

### Mechanism 2
- Claim: Weak supervision via token probability comparison provides effective initialization for the deferral model
- Mechanism: The initialization scheme sets Zt = 1 when the assistant correctly predicts the target token Xt but the base model does not, providing a useful starting point that captures when the assistant is likely to be helpful
- Core assumption: There exists a measurable difference between base and assistant model performance that can be used as weak supervision

### Mechanism 3
- Claim: Greedy decoding with threshold-based deferral selection achieves good performance while maintaining interpretability
- Mechanism: By setting a threshold η on Pθ(Zt=1|X<t), the system controls the frequency of deferral while ensuring each token is generated by a single model, enabling clear analysis of collaboration patterns
- Core assumption: The learned Pθ(Zt|X<t) provides meaningful probabilities that correlate with when deferral is beneficial

## Foundational Learning

- Concept: Latent variable models and marginal likelihood optimization
  - Why needed here: The core innovation relies on modeling the unknown token-generation decisions as latent variables and learning them through marginal likelihood maximization
  - Quick check question: What is the difference between joint likelihood P(X,Z) and marginal likelihood P(X) in this context?

- Concept: Binary classification with sigmoid activation for token-level decisions
  - Why needed here: The deferral decision requires a simple binary classifier that can be efficiently implemented as a linear layer on top of the base model's hidden states
  - Quick check question: How does the sigmoid function transform the dot product between θ and ht(X<t) into a valid probability?

- Concept: Weak supervision and pseudolabel generation
  - Why needed here: Since direct supervision for deferral decisions is unavailable, the initialization relies on comparing model performance to generate useful starting points
  - Quick check question: What condition must hold for the weak supervision scheme to assign Zt = 1?

## Architecture Onboarding

- Component map: Base LLM with additional binary classification head → Token generation and deferral probability prediction → Optional assistant LLM → Final output generation
- Critical path: Input context → Base model forward pass → Hidden state ht(X<t) → Deferral probability prediction → Decision to generate or defer → Either base or assistant model generation → Output token
- Design tradeoffs: Token-level collaboration provides fine-grained control but requires more complex coordination than sequence-level methods; weak supervision enables learning without direct labels but may introduce initialization bias
- Failure signatures: Performance degrades when deferral threshold is too high (insufficient collaboration) or too low (over-reliance on potentially unaligned assistant); poor initialization leads to slow convergence or suboptimal collaboration patterns
- First 3 experiments:
  1. Train Co-LLM with threshold η = 0.5 and evaluate performance vs base model alone
  2. Compare different initialization schemes: random, weak supervision, and no initialization
  3. Vary deferral frequency f by adjusting threshold and measure impact on accuracy and collaboration patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Co-LLM compare to other model combination methods, such as Mixture of Experts (MoE) or Contrastive Decoding (CD), in terms of both accuracy and computational efficiency?
- Basis in paper: Inferred from the discussion of related work and the experimental results comparing Co-LLM to other methods
- Why unresolved: The paper does not provide a direct comparison of Co-LLM to MoE or CD in terms of computational efficiency
- What evidence would resolve it: A comprehensive experimental study comparing the accuracy and computational efficiency of Co-LLM, MoE, and CD on a variety of tasks and datasets

### Open Question 2
- Question: Can Co-LLM be extended to integrate more than two language models, and if so, how would this affect the performance and interpretability of the model?
- Basis in paper: Inferred from the conclusion section, which mentions the possibility of extending Co-LLM to integrate more than two LMs
- Why unresolved: The paper does not explore the extension of Co-LLM to more than two LMs
- What evidence would resolve it: An experimental study evaluating the performance and interpretability of Co-LLM when integrating three or more language models

### Open Question 3
- Question: How does the choice of the deferral threshold η impact the performance of Co-LLM, and is there an optimal way to select this threshold for different tasks and datasets?
- Basis in paper: Explicit from the discussion of the deferral threshold and its impact on performance
- Why unresolved: While the paper provides some guidance on selecting the deferral threshold, it does not offer a definitive method for choosing the optimal threshold for different tasks and datasets
- What evidence would resolve it: A systematic study investigating the relationship between the deferral threshold and performance across various tasks and datasets

### Open Question 4
- Question: How robust is Co-LLM to errors made by the assistant model, and can the model learn to recover from or mitigate the impact of these errors?
- Basis in paper: Inferred from the limitations section, which mentions the potential for the assistant model to make errors
- Why unresolved: The paper does not explore the robustness of Co-LLM to errors made by the assistant model
- What evidence would resolve it: An experimental study evaluating the robustness of Co-LLM to errors made by the assistant model

## Limitations

- Requires access to assistant model's token probabilities during training, which may not be available for proprietary models
- Performance heavily depends on quality of weak supervision initialization, which may fail when base and assistant models have similar capabilities
- Greedy decoding with threshold-based selection may not capture complex multi-step reasoning patterns requiring deeper coordination

## Confidence

**High confidence**: The core mechanism of modeling token-level collaboration decisions as latent variables and optimizing marginal likelihood is well-supported by the theoretical framework and mathematical derivations.

**Medium confidence**: The empirical results showing performance improvements across various tasks are reasonably convincing, though the evaluation focuses primarily on quantitative metrics without extensive qualitative analysis of learned collaboration patterns.

**Low confidence**: The generalizability of the method to more complex multi-assistant scenarios and the scalability to very large models remains uncertain based on the current evaluation scope.

## Next Checks

1. **Cross-architecture validation**: Test the method with models of vastly different architectures (e.g., decoder-only vs. encoder-decoder) to verify if the latent variable approach generalizes beyond similar model types.

2. **Collaboration pattern analysis**: Conduct systematic qualitative analysis of the learned Zt patterns across different domains and tasks to identify consistent collaboration behaviors and potential failure modes.

3. **Robustness to initialization**: Perform ablation studies varying the weak supervision initialization quality to quantify its impact on final performance and convergence speed, particularly when the base and assistant models have similar capabilities.