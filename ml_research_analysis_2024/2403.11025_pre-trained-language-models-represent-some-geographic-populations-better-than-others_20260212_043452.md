---
ver: rpa2
title: Pre-Trained Language Models Represent Some Geographic Populations Better Than
  Others
arxiv_id: '2403.11025'
source_url: https://arxiv.org/abs/2403.11025
tags:
- populations
- language
- geographic
- perplexity
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study probes how well two families of large language models
  (BLOOM and OPT) represent diverse geographic populations by evaluating perplexity
  scores on geo-referenced social media corpora. Using English-only data from 927
  global locations, the authors find significant population skew: models perform much
  better for populations in North America and the UK compared to South/Southeast Asia,
  despite controlling for language and register.'
---

# Pre-Trained Language Models Represent Some Geographic Populations Better Than Others

## Quick Facts
- arXiv ID: 2403.11025
- Source URL: https://arxiv.org/abs/2403.11025
- Reference count: 0
- This study finds that large language models perform much better for populations in North America and the UK compared to South/Southeast Asia, despite controlling for language and register.

## Executive Summary
This study investigates geographic representation bias in large language models by measuring perplexity scores on geo-referenced social media corpora. The authors find that two model families (BLOOM and OPT) exhibit significant population skew, performing much better for North American and UK populations than for South/Southeast Asian populations. This skew persists across model sizes and families, suggesting that current pre-trained models do not equally represent the world's population.

## Method Summary
The study uses 927 global locations from English tweets, creating 86,186 sub-corpora with 250 keyword-balanced samples each. Three BLOOM models (350M-3B parameters) and three OPT models (350M-3B parameters) are evaluated by computing perplexity scores for each sub-corpus. The authors then analyze population skew through country-level aggregation, local variation, and correlation between model families, while testing whether population size, GDP, or international connectivity predict representation quality.

## Key Results
- BLOOM and OPT models show significantly lower perplexity (better representation) for North American and UK populations compared to South/Southeast Asian populations
- Neither population size, per capita GDP, nor international connectivity predicts representation quality
- High correlation (>0.85) between model families indicates consistent population skew across architectures
- Population skew persists across all tested model sizes within each family

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model perplexity is a valid proxy for representation quality across geographic populations.
- Mechanism: Lower perplexity indicates the model assigns higher probability to the text, suggesting the model's internal representations align better with the linguistic patterns of that population.
- Core assumption: Perplexity differences are due to linguistic variation, not other factors like topic or register.
- Evidence anchors:
  - [abstract] "using perplexity scores on geo-referenced social media corpora to measure goodness-of-fit"
  - [section] "Using this geo-referenced corpus as a test set, the probing task uses the bloom and opt models to measure the perplexity of each local sub-corpus"
  - [corpus] Weak: no direct evidence that perplexity differences are solely linguistic; topic/control words only partially control for topical variation.
- Break condition: If sub-corpora differ systematically in topic, register, or demographic factors beyond geography, perplexity will reflect those differences rather than geographic representation.

### Mechanism 2
- Claim: Geographic corpora capture meaningful linguistic variation across populations.
- Mechanism: Tweets from different geographic locations reflect distinct linguistic habits, including vocabulary, syntax, and world knowledge, due to regional variation.
- Core assumption: Language use varies systematically across geographic populations in a way detectable through perplexity scores.
- Evidence anchors:
  - [abstract] "populations across the US and the UK are represented quite well while those in South and Southeast Asia are poorly represented"
  - [section] "geographic corpora do not always represent the full range of social variation within each local population"
  - [corpus] Weak: acknowledges confounds but still treats geographic variation as meaningful; no direct validation of geographic signal in this corpus.
- Break condition: If geographic variation is minimal or overwhelmed by other factors (e.g., register, topic, demographic biases in Twitter users), geographic differences in perplexity may be artifacts.

### Mechanism 3
- Claim: Model families trained on different corpora can exhibit consistent population skew.
- Mechanism: Different training data sources lead to different geographic biases, but the biases are consistent within and across families, suggesting training data geographic distribution is a key factor.
- Core assumption: Training data geographic distribution is the main driver of population skew, not model architecture or size.
- Evidence anchors:
  - [abstract] "Analysis shows that both families of models largely share the same skew across populations"
  - [section] "there is little change across parameter sizes within a single family of models and only a slight difference between families"
  - [corpus] Weak: no direct evidence about training data geographic distribution; inference based on model behavior.
- Break condition: If model architecture, training objective, or other factors (e.g., tokenization) play a larger role than training data geography, skew may not be consistent across families.

## Foundational Learning

- Concept: Perplexity as a measure of model fit
  - Why needed here: The study uses perplexity scores to quantify how well models represent different geographic populations.
  - Quick check question: If a model assigns higher probability to a text sample, what happens to its perplexity score?
- Concept: Geographic corpora and their limitations
  - Why needed here: The study relies on geo-referenced social media data, which has known confounds.
  - Quick check question: What is one confound mentioned that could affect the validity of geographic corpora?
- Concept: Transfer learning and pre-training data influence
  - Why needed here: The study compares models trained on different corpora but similar architectures to isolate the effect of training data.
  - Quick check question: Why might two models with similar architectures but different training data exhibit different population skews?

## Architecture Onboarding

- Component map: Data collection -> Corpus balancing -> Perplexity computation -> Statistical analysis -> Visualization
- Critical path: Geo-referenced corpus -> Perplexity scoring -> Population skew detection -> Factor analysis
- Design tradeoffs: Using English-only data controls for language confounds but may not reflect true geographic variation; using social media data is scalable but introduces demographic biases.
- Failure signatures: High correlation between models suggests training data bias; lack of correlation with sociolinguistic/economic factors suggests unpredictable skew.
- First 3 experiments:
  1. Compute perplexity for a small subset of locations to verify the measurement pipeline works.
  2. Test correlation of perplexity scores between models on the same subset to confirm shared skew.
  3. Visualize perplexity distribution across a single country to check for internal variation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do larger language models exhibit less geographic population skew than smaller models?
- Basis in paper: [explicit] The authors tested models with varying parameter sizes (350m-3b) and found high correlation (>0.85) in population skew across both model families, with only the smallest BLOOM model showing some deviation
- Why unresolved: The study only examined a limited range of model sizes within two specific families. It's unclear whether much larger models (e.g., 10b+ parameters) would show different patterns
- What evidence would resolve it: Testing a broader range of model sizes from different families on the same geographic corpus would determine if population skew decreases with scale

### Open Question 2
- Question: What specific aspects of geographic representation (lexical, syntactic, or world knowledge) contribute most to the observed population skew?
- Basis in paper: [explicit] The authors note that perplexity scores "could reflect differences in linguistic knowledge (e.g., syntax and lexis) or in world knowledge (e.g., local entities and place names)" but cannot isolate which factors drive the skew
- Why unresolved: The study uses perplexity as a single aggregate measure without decomposing which linguistic or knowledge components cause better/worse representation
- What evidence would resolve it: Controlled probing tasks that isolate lexical, syntactic, and entity recognition components separately across geographic populations

### Open Question 3
- Question: Does geographic population skew persist when models are fine-tuned on geographically diverse data from the same register?
- Basis in paper: [inferred] The authors show persistent skew across models trained on different data, but don't test whether fine-tuning on geographically balanced data can mitigate the skew
- Why unresolved: The study only examines pre-trained models without investigating adaptation methods that might address the identified representation gaps
- What evidence would resolve it: Experiments comparing perplexity before and after fine-tuning on geographically balanced corpora from the same domain (social media)

## Limitations
- The study uses English-only data, which may not capture true geographic variation and introduces demographic biases from Twitter's user base
- Acknowledged confounds like register, topic, and social variation within populations may influence perplexity scores
- The analysis assumes perplexity differences are primarily due to geographic representation rather than other linguistic or demographic factors

## Confidence
- **High confidence**: The observation of consistent population skew across model families and the finding that neither population size, GDP, nor international connectivity predicts representation quality
- **Medium confidence**: The interpretation that perplexity differences reflect geographic representation quality, given acknowledged confounds and lack of direct validation
- **Low confidence**: The claim that a single model cannot adequately serve all populations, as this depends on the definition of "adequately"

## Next Checks
1. Conduct a controlled experiment using synthetic corpora with known geographic variation to validate that perplexity differences are driven by geographic factors rather than confounds like topic or register.
2. Replicate the analysis with a more diverse set of language models (e.g., models trained on different geographic distributions) to confirm the consistency of population skew across families.
3. Perform a sensitivity analysis to assess how much the observed population skew changes when controlling for known confounds (e.g., demographic factors, topic distribution) in the geo-referenced corpus.