---
ver: rpa2
title: 'The MuSe 2024 Multimodal Sentiment Analysis Challenge: Social Perception and
  Humor Recognition'
arxiv_id: '2406.07753'
source_url: https://arxiv.org/abs/2406.07753
tags:
- hyphen
- humor
- multimodal
- challenge
- muse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The MuSe 2024 challenge introduces two tasks: predicting 16 social
  attributes (e.g., assertiveness, dominance, likability) from audio-visual CEO interviews
  (MuSe-Perception) and detecting humor in cross-lingual football press conferences
  (MuSe-Humor). A baseline system using Gated Recurrent Unit (GRU) Recurrent Neural
  Networks was trained on multimodal features, including audio (e.g., eGeMAPS, DeepSpectrum,
  wav2vec2.0), visual (e.g., FAU, FaceNet, ViT), and textual (BERT) representations.'
---

# The MuSe 2024 Multimodal Sentiment Analysis Challenge: Social Perception and Humor Recognition

## Quick Facts
- arXiv ID: 2406.07753
- Source URL: https://arxiv.org/abs/2406.07753
- Reference count: 40
- Primary result: Introduces two tasks - predicting 16 social attributes from CEO interviews and detecting humor in football press conferences, with baseline performance of ρ=0.3573 and AUC=0.8682 respectively

## Executive Summary
The MuSe 2024 challenge presents two novel multimodal tasks focused on social perception and humor recognition. The first task, MuSe-Perception, requires predicting 16 social attributes (such as assertiveness, dominance, and likability) from audio-visual CEO interview data. The second task, MuSe-Humor, focuses on detecting humor in cross-lingual football press conferences. Both tasks require integrating information from multiple modalities including audio, visual, and textual sources.

The challenge provides baseline implementations using Gated Recurrent Unit (GRU) neural networks trained on diverse multimodal feature representations. For audio, features include eGeMAPS, DeepSpectrum, and wav2vec2.0; for visual, FAU, FaceNet, and ViT features; and for text, BERT embeddings. The baseline demonstrates that late fusion of all modalities consistently improves performance across both tasks, establishing a foundation for future research in multimodal social perception and humor recognition.

## Method Summary
The baseline system employs Gated Recurrent Unit (GRU) Recurrent Neural Networks as the core architecture for both tasks. Multimodal features are extracted from audio, visual, and textual sources using established pre-trained models and feature extractors. Audio features include eGeMAPS (openSMILE), DeepSpectrum embeddings, and wav2vec2.0 representations. Visual features comprise FAU (Facial Action Units), FaceNet embeddings, and Vision Transformer (ViT) features. Textual features are derived from BERT embeddings. These modality-specific features are processed independently through GRUs before being combined through late fusion strategies. The system is trained on the MuSe 2024 datasets, with MuSe-Perception focusing on CEO interview data for social attribute prediction and MuSe-Humor addressing humor detection in football press conferences across multiple languages.

## Key Results
- MuSe-Perception task baseline achieves mean Pearson's correlation coefficient (ρ) of 0.3573 for predicting 16 social attributes
- MuSe-Humor task baseline reaches Area Under the Curve (AUC) of 0.8682 for humor detection in cross-lingual football press conferences
- Late fusion of audio, visual, and textual modalities consistently improves performance in both tasks

## Why This Works (Mechanism)
The multimodal approach works because social perception and humor recognition inherently involve multiple communicative channels. Social attributes like assertiveness and likability are expressed through vocal tone, facial expressions, and linguistic choices simultaneously. Similarly, humor often relies on timing, facial cues, and linguistic ambiguity or surprise working together. The GRU architecture effectively captures temporal dependencies in these multimodal signals, while late fusion allows each modality to maintain its specialized representation before combining complementary information.

## Foundational Learning

**Multimodal Fusion Strategies**: Understanding when to apply early, intermediate, or late fusion is crucial for multimodal systems. Early fusion combines features at input level but may lose modality-specific information. Late fusion preserves specialized representations but requires careful alignment. Quick check: Compare correlation matrices between modalities to identify complementary versus redundant information.

**Social Signal Processing**: The field focuses on extracting meaningful social and affective information from behavioral signals. For social attribute prediction, this includes understanding how vocal parameters (pitch, intensity, speaking rate) correlate with perceived dominance or assertiveness. Quick check: Analyze feature importance rankings to identify which vocal/visual/textual cues drive social attribute predictions.

**Cross-lingual Humor Detection**: Humor often relies on cultural and linguistic context, making cross-lingual detection particularly challenging. The system must identify humor cues that transcend language barriers while handling translation or multilingual representations. Quick check: Perform language-specific versus pooled model comparisons to assess true cross-lingual generalization.

## Architecture Onboarding

**Component Map**: Raw Data -> Feature Extractors (Audio, Visual, Text) -> GRU Networks -> Late Fusion Layer -> Prediction Output

**Critical Path**: Feature extraction and temporal modeling through GRUs represents the critical path. Audio features (eGeMAPS, DeepSpectrum, wav2vec2.0) must be synchronized with visual features (FAU, FaceNet, ViT) and textual features (BERT) before GRU processing. The late fusion layer then combines these processed representations for final prediction.

**Design Tradeoffs**: The system prioritizes simplicity and reproducibility through GRUs over more complex architectures like Transformers. This choice reduces computational requirements and training complexity but may limit the model's ability to capture long-range dependencies and complex multimodal interactions. The extensive feature engineering provides rich representations but increases system complexity and potential overfitting to dataset-specific patterns.

**Failure Signatures**: Poor performance on out-of-domain data indicates overfitting to CEO interview or football press conference contexts. Unbalanced modality contributions suggest feature extraction or temporal alignment issues. Low cross-lingual performance reveals language-specific rather than universal humor detection capabilities.

**Three First Experiments**:
1. Ablation study removing each modality to quantify individual contributions to baseline performance
2. Cross-validation across different segments of the dataset to assess temporal generalization
3. Comparison of late fusion versus alternative fusion strategies (early, intermediate) to validate the chosen approach

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The datasets focus on specific domains (CEO interviews, football press conferences) that may not generalize to other social contexts or humor domains
- The GRU-based baseline may not represent state-of-the-art performance, with substantial room for improvement through more sophisticated architectures
- Heavy reliance on hand-crafted and pre-trained feature extractors may not capture all relevant aspects of social perception and humor

## Confidence

**High Confidence**: The reported baseline methodology and feature extraction pipeline are clearly described and reproducible. The evaluation metrics (Pearson's correlation for MuSe-Perception, AUC for MuSe-Humor) are appropriate for their respective tasks.

**Medium Confidence**: The reported performance metrics are valid for the baseline system, but their absolute values suggest the task remains challenging. The late fusion strategy's consistent improvement is plausible but requires verification across different model architectures.

**Low Confidence**: Generalizability of the models beyond the specific domains (CEO interviews, football press conferences) and languages represented in the datasets. The relative contribution of individual modalities to final performance is not thoroughly analyzed.

## Next Checks
1. Conduct comprehensive ablation studies removing individual modalities and feature types to quantify their relative contributions to the baseline performance, revealing whether reported late fusion improvements are consistent across different model configurations.

2. Test the trained models on out-of-domain datasets (different social contexts for MuSe-Perception, different humor domains for MuSe-Humor) to assess generalizability and identify domain-specific biases in the baseline approach.

3. Implement and evaluate more advanced architectures (multimodal Transformers, graph neural networks for social attribute relationships) to establish whether the GRU baseline represents a reasonable performance floor or if architectural improvements could substantially change the results.