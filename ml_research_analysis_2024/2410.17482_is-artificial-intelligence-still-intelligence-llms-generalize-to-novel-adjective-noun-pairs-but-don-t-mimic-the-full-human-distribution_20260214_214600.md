---
ver: rpa2
title: Is artificial intelligence still intelligence? LLMs generalize to novel adjective-noun
  pairs, but don't mimic the full human distribution
arxiv_id: '2410.17482'
source_url: https://arxiv.org/abs/2410.17482
tags:
- llama
- context
- human
- bigrams
- subsective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies whether large language models (LLMs) can generalize
  to novel adjective-noun combinations and draw human-like inferences without context.
  The authors propose three methods to evaluate LLMs on these inferences: accuracy
  within one standard deviation of the human mean, Jensen-Shannon divergence between
  rating distributions from log-probabilities, and divergence from distributions generated
  by LLM-written contexts.'
---

# Is artificial intelligence still intelligence? LLMs generalize to novel adjective-noun pairs, but don't mimic the full human distribution

## Quick Facts
- arXiv ID: 2410.17482
- Source URL: https://arxiv.org/abs/2410.17482
- Reference count: 40
- Largest models (Llama 3 70B and Qwen 2 72B) can generalize to unseen adjective-noun combinations and closely match human inferences when context determines the inference

## Executive Summary
This paper investigates whether large language models can generalize to novel adjective-noun combinations and draw human-like inferences without explicit context. The authors propose three evaluation methods: accuracy within one standard deviation of human responses, Jensen-Shannon divergence between rating distributions, and divergence from LLM-generated context distributions. While the largest models tested show strong performance on contextual inferences, they only approximate the full human distribution for 75% of cases when no context is provided. The study reveals that LLMs struggle particularly with adjective-noun combinations that have high human variance and rare inferences, suggesting limitations in their ability to fully replicate human conceptual reasoning.

## Method Summary
The authors evaluated LLMs using a novel dataset of adjective-noun combinations where human participants rated whether the adjective modified the noun's essential properties (e.g., "Is a fake watch a watch?"). Three evaluation methods were employed: accuracy relative to human standard deviation, JS divergence between log-probability distributions and human ratings, and divergence from distributions generated by LLM-written contexts. Models tested included various sizes of Llama 3, Qwen 2, Mistral, and Gemma 2, with comparisons between base and instruction-tuned versions. The study focused on both novel combinations (never seen during training) and combinations with provided contexts to disambiguate inferences.

## Key Results
- Largest models (Llama 3 70B and Qwen 2 72B) successfully generalize to unseen adjective-noun combinations
- Models closely match human inferences when context determines the inference (JS divergence < 0.1)
- Even best models only approximate 75% of human distributions without context, struggling with high-variance combinations and rare inferences

## Why This Works (Mechanism)
The paper doesn't explicitly detail the mechanism by which LLMs generalize to novel adjective-noun pairs. However, the success of larger models suggests that their increased parameter count and training data allow them to capture more nuanced semantic relationships between adjectives and nouns. The context-dependent performance indicates that LLMs can leverage surrounding information to disambiguate meaning, similar to human reasoning processes.

## Foundational Learning
- **Semantic compositionality**: Understanding how adjectives modify noun meanings is fundamental to language comprehension. Quick check: Can the model correctly interpret "fake watch" as both a watch (in some sense) and not a genuine watch?
- **Context-dependent inference**: Human language interpretation often depends on contextual cues. Quick check: Does the model's response change when given disambiguating context?
- **Distributional semantics**: LLMs learn meaning from patterns in training data. Quick check: How well does the model's rating distribution match human distributions across different adjective-noun pairs?
- **Likert scale interpretation**: The study assumes models can interpret ordinal rating scales. Quick check: Are the model's responses consistent across the full range of Likert scale values?
- **Novel combination generalization**: The ability to reason about combinations never seen during training. Quick check: Does the model's performance degrade on truly novel adjective-noun pairs?
- **Instruction following**: Instruction-tuned models show different biases than base models. Quick check: How do prompts and instructions affect model performance?

## Architecture Onboarding
**Component Map**: LLM architecture (Transformer-based) -> Input processing (tokenization) -> Context window handling -> Log-probability generation -> Rating scale interpretation -> Evaluation metrics (accuracy, JS divergence)

**Critical Path**: Tokenization -> Context window processing -> Log-probability calculation for target inference -> Likert scale conversion -> Comparison with human distributions

**Design Tradeoffs**: The study uses base models for fundamental capability assessment vs. instruction-tuned models for instruction-following evaluation, accepting that different model variants may have different biases and strengths.

**Failure Signatures**: Models show systematic errors on high-variance human responses and rare inferences, suggesting limitations in handling ambiguous or uncommon semantic combinations.

**First Experiments**:
1. Test base model performance on the same task to isolate the effect of instruction tuning
2. Evaluate model performance on a binary yes/no version of the task to check Likert scale comprehension
3. Compare model-generated contexts with human-generated contexts to assess contextual disambiguation quality

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the context generation method be improved by providing more specific instructions to the LLM?
- Basis in paper: The paper suggests that the context generation method might be improved by explicitly asking for stories that disambiguate the target inference, since humans see the "Is an AN a N?" question when imagining their "contexts".
- Why unresolved: The paper only provides a proof of concept for the context generation method with Llama 3 70B Instruct, and does not explore different prompt variations or constraints to see if they lead to better performance.
- What evidence would resolve it: Testing the context generation method with different prompts and constraints, and comparing the resulting JS divergence with the human distribution to the original method.

### Open Question 2
- Question: Is the Likert scale comprehension issue a fundamental limitation for LLMs in this task, or can it be mitigated through better prompting or fine-tuning?
- Basis in paper: The paper notes that using a Likert scale instead of a binary yes/no question assumes that the model can use the Likert scale, and that there is no work explicitly showing that models understand Likert scales in general.
- Why unresolved: The paper does not explore alternative ways to evaluate the LLMs' performance on this task, such as using a binary yes/no question or calculating the surprisal of a statement like "a fake watch is/is not a watch".
- What evidence would resolve it: Comparing the LLMs' performance on this task using different evaluation methods, such as a binary yes/no question or surprisal-based scoring, to see if the Likert scale comprehension issue is a significant factor.

### Open Question 3
- Question: Do the differences in performance between base and instruction-tuned models stem from fundamental differences in their capabilities, or are they primarily due to differences in prompting and instruction-following?
- Basis in paper: The paper notes that the base and instruction-tuned models have different biases in their ratings, and that the instruction-tuned models are generally better at following instructions.
- Why unresolved: The paper does not conduct a systematic comparison of the base and instruction-tuned models' performance using the same prompts and instructions, to isolate the effect of instruction-following from other potential differences.
- What evidence would resolve it: Conducting a controlled experiment where the base and instruction-tuned models are prompted and instructed in the same way, and comparing their performance on the task to see if the differences are due to instruction-following or other factors.

## Limitations
- Models only approximate 75% of human distributions without context, failing on high-variance combinations and rare inferences
- Human variance in responses suggests inconsistency in human reasoning itself
- Study focuses on specific model sizes and evaluation methods, limiting generalizability
- Likert scale comprehension is assumed but not explicitly verified for LLMs

## Confidence
- **High Confidence**: LLMs can generalize to novel adjective-noun combinations when context determines inference
- **Medium Confidence**: LLMs only approximate 75% of human distributions without context
- **Low Confidence**: Specific types of adjective-noun combinations (high variance, rare inferences) cause the most difficulty

## Next Checks
1. Test whether smaller model variants (e.g., 7B or 13B parameters) show similar generalization patterns or if the 75% approximation threshold is specific to very large models, by replicating the study with a broader range of model sizes.

2. Investigate whether fine-tuning on targeted adjective-noun combination datasets improves model performance on the 25% of cases where they currently diverge from human distributions, particularly focusing on high-variance and rare-inference combinations.

3. Conduct a qualitative linguistic analysis to identify specific features of adjective-noun combinations that cause model-human divergence (such as metaphorical usage, cultural references, or complex semantic interactions) to determine whether these represent fundamental challenges or solvable modeling issues.