---
ver: rpa2
title: An Examination of Offline-Trained Encoders in Vision-Based Deep Reinforcement
  Learning for Autonomous Driving
arxiv_id: '2409.10554'
source_url: https://arxiv.org/abs/2409.10554
tags:
- learning
- encoder
- driving
- network
- encoders
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of training vision-based Deep
  Reinforcement Learning (DRL) agents for autonomous driving in Partially Observable
  Markov Decision Processes (POMDPs). The authors propose using offline-trained encoders
  to extract generalizable representations from large video datasets, which are then
  transferred to DRL agents for controlling an ego vehicle in the CARLA simulator.
---

# An Examination of Offline-Trained Encoders in Vision-Based Deep Reinforcement Learning for Autonomous Driving

## Quick Facts
- arXiv ID: 2409.10554
- Source URL: https://arxiv.org/abs/2409.10554
- Authors: Shawan Mohammed; Alp Argun; Nicolas Bonnotte; Gerd Ascheid
- Reference count: 36
- Primary result: BYOL-trained encoders achieved zero lane invasions and collisions after ~700 episodes, outperforming end-to-end DRL which failed to solve the task.

## Executive Summary
This study investigates the use of offline-trained encoders to address the challenge of training vision-based Deep Reinforcement Learning (DRL) agents for autonomous driving in Partially Observable Markov Decision Processes (POMDPs). The authors propose leveraging self-supervised learning approaches to extract generalizable representations from large video datasets, which are then transferred to DRL agents for controlling an ego vehicle in the CARLA simulator. They compare different self-supervised learning approaches (MoCo, BYOL, DPC, VAE) and a supervised multi-task model (YOLOPv2) for training the encoders, with BYOL showing the best performance in achieving zero lane invasions and collisions after approximately 700 episodes.

## Method Summary
The study employs a two-stage approach: first, training various encoders (MoCo, BYOL, DPC, VAE, YOLOPv2) on BDD100K driving dataset using self-supervised learning; then freezing these encoders and training head networks (actor-critic) on top using PPO algorithm with proximal policy optimization loss. The input consists of four consecutive RGB frames (3x128x128) from BDD100K for encoder training and the same format for DRL training in CARLA. The CARLA environment provides evaluation routes 31, 35, and 46 with steering control and autopilot acceleration. Primary metrics include mean reward per episode, steps per episode, and lane invasions per episode.

## Key Results
- BYOL-trained encoders achieved the best performance, reaching zero lane invasions and collisions after approximately 700 episodes
- End-to-end DRL without pre-trained encoders failed to solve the task, achieving a reward of -4
- Self-supervised representations learned from BDD100K driving videos can be directly transferred to CARLA driving simulator with no fine-tuning required

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained encoders convert high-dimensional image inputs into compact, generalizable feature vectors, enabling the RL agent to learn control policies from a reduced-dimensional space rather than raw pixels.
- Mechanism: By freezing a 3D-ResNet18 encoder trained on BDD100K via self-supervised learning, the downstream DRL head receives structured representations instead of raw image data, mitigating the curse of dimensionality and improving sample efficiency.
- Core assumption: The encoder has learned transferable visual features that capture relevant driving cues (lanes, vehicles, road geometry) without fine-tuning on CARLA-specific data.
- Evidence anchors:
  - [abstract]: "self-supervised representations learned by watching BDD100K driving videos can be directly transferred to CARLA driving simulator with no fine-tuning required"
  - [section]: "Due to its supervised and multi-task learning scheme, and more complex E-ELAN backbone, YOLOPv2 encoder showed a faster convergence... However, BYOL is a close second, which shows the strength of BYOL representations, even though trained without labels and with a smaller backbone, 3D-ResNet18."

### Mechanism 2
- Claim: The contrastive and non-contrastive losses used during encoder training encourage the extraction of temporally coherent and semantically meaningful features, which are critical for decision-making in POMDPs.
- Mechanism: Methods like MoCo and BYOL use similarity-based objectives (e.g., InfoNCE loss, MSE between query and key) to align different views of the same video clip while separating unrelated clips, ensuring the encoder captures motion dynamics and object permanence.
- Core assumption: Temporal coherence in video data is essential for learning predictive driving behaviors, and the encoder's loss function enforces this coherence.
- Evidence anchors:
  - [section]: "Dense Predictive Coding (DPC) learns representations by recurrently predicting future representations... it implicitly enforces a temporal coherence to learn the inherent temporal dynamics of the data."
  - [section]: "BYOL utilizes a non-contrastive loss... Without a contrastive objective, in order to prevent representation collapse... BYOL implements an extra multi-layer perceptron predictor head on top of the query encoder."

### Mechanism 3
- Claim: Freezing the encoder and training only the head network via RL allows the agent to focus on policy learning without the overhead of learning perception from scratch.
- Mechanism: The encoder provides fixed features; the RL head (actor-critic) learns to map these to steering/acceleration commands, simplifying the optimization landscape and reducing the effective action space.
- Core assumption: The RL loss (PPO clipped surrogate) is sufficient to learn a control policy when the input is already semantically meaningful.
- Evidence anchors:
  - [abstract]: "we train a head network on top of these representations through DRL to learn to control an ego vehicle in the CARLA AD simulator."
  - [section]: "By freezing the encoder layers, we prevent them from being trained by the RL loss function, resulting in a visual abstraction, i.e., representation learning based RL training configuration."

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: Autonomous driving is a POMDP because the ego vehicle only observes partial information (e.g., occluded lanes, unseen vehicles), requiring reasoning over time to infer hidden states.
  - Quick check question: Why does partial observability reduce RL performance, and how do representations help?

- Concept: Self-supervised learning objectives
  - Why needed here: Encoder training without labels allows leveraging large unlabeled video datasets like BDD100K to learn generalizable visual features for downstream control tasks.
  - Quick check question: How do contrastive losses (MoCo, DPC) differ from non-contrastive (BYOL) in preventing representation collapse?

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: PPO is used to train the head network, providing stable policy updates through clipped surrogate objectives and importance sampling.
  - Quick check question: What role does the clipping parameter (ϵ = 0.1) play in PPO's stability?

## Architecture Onboarding

- Component map: Input (4 stacked RGB frames) -> Encoder (3D-ResNet18) -> Head network -> Actor/Critic -> Action -> CARLA env -> Reward -> PPO update
- Critical path: Input → Encoder → Head network → Actor/Critic → Action → CARLA env → Reward → PPO update
- Design tradeoffs:
  - Larger head networks may capture more complex mappings but risk overfitting with limited samples
  - Temporal averaging vs. full 3D convolutions: averaging reduces dimensionality but may lose spatial detail; 3D convs preserve more structure but increase compute
  - Encoder choice: MoCo/BYL balance speed and accuracy; DPC failed to converge, suggesting instability
- Failure signatures:
  - High lane invasion rate despite long training: encoder features may not capture lane geometry well
  - Constant reward plateau at -4: end-to-end RL failed, suggesting raw pixels are too high-dimensional
  - Oscillating steering: actor network may be unstable or reward shaping insufficient
- First 3 experiments:
  1. Compare end-to-end PPO vs. frozen BYOL encoder + PPO head on same reward function
  2. Test BYOL encoder with different head architectures (direct flatten vs. temporal reduction) to see impact on convergence speed
  3. Swap encoder between BYOL and MoCo (same head) to isolate encoder impact from head design

## Open Questions the Paper Calls Out
None

## Limitations
- The study only evaluates a single task (lane following) and limited routes, which may not capture the full complexity of real-world driving scenarios
- The encoder training details (specific hyperparameters, augmentation strategies) are not fully specified, making exact replication challenging
- The potential domain gap between BDD100K and CARLA datasets may affect the generalizability of the learned representations

## Confidence
- High Confidence: The core finding that pre-trained encoders outperform end-to-end DRL training is well-supported by the experimental results, with BYOL showing clear convergence to zero lane invasions
- Medium Confidence: The claim that self-supervised representations can be directly transferred without fine-tuning is supported but would benefit from testing on additional environments and tasks to confirm robustness
- Low Confidence: The assertion that DPC failed to converge due to its specific architecture is plausible but not definitively proven, as other factors (hyperparameters, implementation details) could have contributed

## Next Checks
1. Evaluate the best-performing encoder (BYOL) on a different autonomous driving simulator or real-world dataset to assess cross-domain transfer capability
2. Systematically vary encoder architecture depth and self-supervised objective parameters to identify the minimum viable configuration for effective transfer learning
3. Test the encoder-head pipeline on additional driving tasks (e.g., intersection navigation, pedestrian avoidance) to verify that the learned representations are task-agnostic