---
ver: rpa2
title: 'LLM The Genius Paradox: A Linguistic and Math Expert''s Struggle with Simple
  Word-based Counting Problems'
arxiv_id: '2410.14166'
source_url: https://arxiv.org/abs/2410.14166
tags:
- word
- llms
- arxiv
- reasoning
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models excel at complex reasoning and coding but
  struggle with basic tasks like counting characters in words. The study investigates
  why models fail at such simple tasks and whether training on math or code data helps.
---

# LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with Simple Word-based Counting Problems

## Quick Facts
- arXiv ID: 2410.14166
- Source URL: https://arxiv.org/abs/2410.14166
- Reference count: 40
- Large language models excel at complex reasoning and coding but struggle with basic tasks like counting characters in words.

## Executive Summary
This study investigates why large language models (LLMs) struggle with simple word-based counting tasks despite their proficiency in complex reasoning and coding. Through systematic experimentation, the research demonstrates that common conjectures about subword tokenization, lack of character-level training, or excessive unique characters do not explain this paradox. The study finds that reasoning strategies, particularly chain-of-thought prompting, significantly improve model performance on counting tasks, with the most capable model achieving near-perfect accuracy. The research concludes that capability transfer from specialized training to simple tasks is not automatic and highlights the importance of fostering reasoning skills during model pretraining.

## Method Summary
The study evaluates multiple LLMs on four counting tasks: character occurrence, substring occurrence, word length, and distinct character counting using a dataset of 500 randomly sampled words from the NLTK library. The research tests various conjectures including tokenization effects, character-level training impact, and unique character influence. It examines specialized math and code models for transferability, and evaluates multiple reasoning strategies including chain-of-thought, self-consistency, self-refine, and tree-of-thought. The study also investigates finetuning and in-context learning approaches for performance improvement.

## Key Results
- Common conjectures about subword tokenization, lack of character-level training, or excessive unique characters are invalid explanations for LLM counting failures.
- Specialized math and code models perform poorly on simple counting tasks, indicating advanced reasoning skills don't automatically transfer to basic problems.
- Reasoning strategies like chain-of-thought significantly improve model performance, with GPT-4o achieving near-perfect accuracy on counting tasks.
- The most robust and efficient way to improve LLM performance on counting tasks is through engaging reasoning rather than finetuning or in-context learning.

## Why This Works (Mechanism)

### Mechanism 1: Reasoning Before Responding Unlocks Counting Capability
Large language models possess the inherent capability to solve simple word-based counting tasks, but this ability remains dormant unless explicitly triggered through reasoning procedures. When LLMs are prompted to generate chain-of-thought reasoning before providing answers, they engage their problem-solving faculties and correctly perform character-level counting operations that they fail at when directly answering. This mechanism breaks when the reasoning prompt fails to activate the model's problem-solving capabilities, which may occur if the model's training data lacked sufficient reasoning patterns or if the prompt engineering is ineffective.

### Mechanism 2: Subword Tokenization Does Not Fundamentally Impair Character-Level Processing
The commonly held belief that subword tokenization prevents LLMs from understanding individual characters is incorrect. LLMs can process character-level information effectively even when words are tokenized into subwords, as demonstrated by their ability to handle sentiment analysis tasks when inputs are represented as characters rather than words. This mechanism breaks if models show significantly improved performance when explicitly forced to process character-level tokens, which the experiments showed did not occur.

### Mechanism 3: Advanced Capabilities Don't Automatically Transfer to Simpler Tasks
Specialized training on advanced mathematical or coding tasks does not automatically transfer to solving simple word-based counting problems. Models trained on complex reasoning tasks (math, coding, theorem proving) do not show improved performance on simple counting tasks compared to general models, suggesting that capability transfer is not automatic or linear. This mechanism breaks if models trained on complex tasks show systematic improvement on simpler related tasks, or if specific training techniques can bridge this capability gap.

## Foundational Learning

- **Concept: Chain-of-Thought Reasoning**
  - Why needed here: Understanding how to prompt models to reason before answering is crucial for unlocking their counting capabilities.
  - Quick check question: Can you explain why asking a model to "think step-by-step" before answering might help it count characters more accurately?

- **Concept: Tokenization Algorithms**
  - Why needed here: Understanding subword tokenization (BPE, WordPiece) is essential for evaluating claims about why LLMs struggle with character counting.
  - Quick check question: What's the difference between word-level, character-level, and subword tokenization, and why did subword tokenization become dominant?

- **Concept: Supervised Fine-tuning vs. In-Context Learning**
  - Why needed here: The paper compares multiple strategies for improving model performance, requiring understanding of different training approaches.
  - Quick check question: How does supervised fine-tuning differ from in-context learning in terms of data requirements and expected performance improvements?

## Architecture Onboarding

- **Component map**: Input → Tokenization → Model Processing → Reasoning Strategy (optional) → Output Generation → Evaluation
- **Critical path**: The system processes input text through tokenization, passes it to the transformer-based LLM, optionally applies reasoning strategies, generates output, and evaluates against ground truth.
- **Design tradeoffs**: The paper reveals that more complex models and specialized training don't necessarily improve simple task performance, suggesting that model complexity and task simplicity may not be directly correlated.
- **Failure signatures**: Models failing at simple counting tasks despite success on complex reasoning benchmarks, poor performance transfer from specialized to general tasks, and sensitivity to prompt formatting rather than reasoning requirements.
- **First 3 experiments**:
  1. Test whether forcing character-level tokenization improves counting accuracy by explicitly splitting words into individual characters.
  2. Evaluate whether reasoning prompts (CoT, self-consistency) improve performance on counting tasks compared to direct answering.
  3. Compare specialized math/code models against general models on simple counting tasks to test capability transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does explicit reasoning instruction during pretraining significantly improve LLM performance on basic counting tasks?
- Basis in paper: Explicit
- Why unresolved: The paper demonstrates that reasoning strategies like chain-of-thought improve performance on counting tasks, but does not investigate whether incorporating reasoning instruction during the pretraining phase would yield even greater improvements or make models inherently better at such tasks.
- What evidence would resolve it: Comparative studies training models with and without explicit reasoning instruction on counting tasks, measuring performance differences on both seen and unseen counting problems.

### Open Question 2
- Question: Can specialized math and code models transfer their advanced reasoning capabilities to basic linguistic tasks beyond counting, such as morphological analysis or syntactic parsing?
- Basis in paper: Explicit
- Why unresolved: The paper shows that math and code models fail to transfer capabilities to simple counting tasks, but does not explore whether this limitation extends to other basic linguistic tasks that require different forms of reasoning.
- What evidence would resolve it: Systematic evaluation of specialized models on various basic linguistic tasks, comparing their performance against general models and analyzing whether reasoning strategies can bridge the capability gap.

### Open Question 3
- Question: What is the relationship between tokenization granularity and character-level understanding in LLMs, and can alternative tokenization methods improve performance on character-based tasks?
- Basis in paper: Inferred
- Why unresolved: The paper refutes the conjecture that subword tokenization causes counting failures but does not investigate whether different tokenization approaches (like character-level or byte-level tokenization) might actually enhance character-level understanding and task performance.
- What evidence would resolve it: Controlled experiments training models with different tokenization schemes on character-based tasks, measuring performance differences and analyzing the impact on character-level reasoning capabilities.

## Limitations

- The study focuses on a specific set of four counting tasks and doesn't explore whether findings generalize to other simple word-based problems.
- The analysis of tokenization effects is limited to surface-level performance comparisons rather than investigating internal model representations.
- The sample of specialized math and code models is limited, and the exact nature of their training data isn't fully specified.

## Confidence

- **High Confidence**: The finding that reasoning strategies (particularly chain-of-thought) significantly improve counting task performance is well-supported by experimental evidence across multiple models and task types.
- **Medium Confidence**: The claim that specialized training on math or code doesn't transfer to simple counting tasks is supported by the data, but the sample of specialized models is limited and the exact nature of the training data isn't fully specified.
- **Low Confidence**: The assertion that subword tokenization doesn't fundamentally impair character-level processing requires more direct investigation of internal representations and learning dynamics.

## Next Checks

1. **Internal Representation Analysis**: Use probing techniques or attention visualization to examine whether LLMs actually develop character-level representations despite subword tokenization, and whether reasoning prompts activate different neural pathways for counting tasks.

2. **Cross-Task Generalization**: Test the reasoning strategy approach on a broader set of simple word-based tasks (alphabetical ordering, vowel/consonant counting, pattern matching) to determine if the findings generalize beyond the four counting tasks studied.

3. **Training Data Analysis**: Analyze the pretraining data of both general and specialized models to identify whether differences in character-level text exposure correlate with counting task performance, and whether reasoning prompts effectively compensate for training data deficiencies.