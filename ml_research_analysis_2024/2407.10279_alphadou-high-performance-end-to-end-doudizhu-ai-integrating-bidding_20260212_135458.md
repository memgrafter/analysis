---
ver: rpa2
title: 'AlphaDou: High-Performance End-to-End Doudizhu AI Integrating Bidding'
arxiv_id: '2407.10279'
source_url: https://arxiv.org/abs/2407.10279
tags:
- landlord
- douzero
- card
- bidding
- doudizhu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AlphaDou, an end-to-end Doudizhu AI that
  integrates the bidding phase. The model uses reinforcement learning to simultaneously
  estimate win rates and expectations, enabling it to prune action spaces and select
  optimal strategies.
---

# AlphaDou: High-Performance End-to-End Doudizhu AI Integrating Bidding

## Quick Facts
- arXiv ID: 2407.10279
- Source URL: https://arxiv.org/abs/2407.10279
- Authors: Chang Lei; Huan Lei
- Reference count: 29
- Outperforms state-of-the-art Doudizhu AIs in head-to-head competitions

## Executive Summary
This paper introduces AlphaDou, an end-to-end Doudizhu AI that integrates the bidding phase with cardplay using reinforcement learning. The model simultaneously estimates win rates and expected values, enabling action space pruning and optimal strategy selection. Trained in a realistic Doudizhu environment, AlphaDou achieved state-of-the-art performance among publicly available models, with the bidding model outperforming supervised learning baselines and the cardplay model surpassing existing Doudizhu AIs in head-to-head competitions.

## Method Summary
AlphaDou uses reinforcement learning with a dual-output neural network that estimates both win probability and expected value simultaneously. The model employs action space pruning based on Q-value thresholds, then selects moves with the highest win probability from the pruned set. Value factorization decomposes Q-values into win and loss components to reduce training variance. The framework integrates bidding and cardplay phases, with the bidding model adjusting strategy based on bid outcomes and opponent actions.

## Key Results
- AlphaDou's bidding model outperforms supervised learning baselines
- Cardplay model surpasses existing Doudizhu AIs in head-to-head competitions
- Model adjusts strategies based on bidding outcomes and opponent actions
- Achieved state-of-the-art performance among publicly available models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reinforcement learning estimates both win rate and expected value simultaneously, enabling action space pruning.
- Mechanism: The model outputs two distinct values: pw(s, a) for win probability and Q(s, a) for expected reward. Actions with low Q(s, a) relative to the maximum are pruned before selecting based on pw(s, a).
- Core assumption: The win probability and value estimates are conditionally independent enough that pruning by Q(s, a) preserves high-quality moves for win-rate selection.
- Evidence anchors:
  - [abstract]: "The model uses reinforcement learning to simultaneously estimate win rates and expectations, enabling it to prune action spaces and select optimal strategies."
  - [section 3.3]: "We prune the moves based on Q(s, a)... Then, we choose the move with the highest winning probability pw(s, a) within Acut."
  - [corpus]: Weak evidence; related papers focus on RL in card games but don't explicitly describe this dual-output pruning approach.
- Break condition: If win probability and value are highly correlated (e.g., high-variance hands), pruning by Q(s, a) may eliminate optimal moves.

### Mechanism 2
- Claim: Bidding outcomes dynamically adjust cardplay strategy to improve overall performance.
- Mechanism: The model observes bid scores and adjusts its cardplay strategy based on perceived opponent strength. Low opponent bids trigger more aggressive play; high bids trigger conservative play.
- Core assumption: Bid information reliably signals relative hand strength, allowing the model to calibrate risk-taking during cardplay.
- Evidence anchors:
  - [abstract]: "The approach allows the AI to adjust strategies based on bidding outcomes and opponent actions."
  - [section 4.1]: "The Bid Model is more aggressive, tending to become the Landlord Player more often... The Bid Model tends to bid 2 points in the first position, 0 points in the second position, and 3 points in the third position."
  - [corpus]: No direct evidence; related papers don't discuss bidding-phase integration in Doudizhu.
- Break condition: If opponents bluff or bidding is noisy, the model may misadjust strategy, reducing performance.

### Mechanism 3
- Claim: Value factorization into win/loss Q-values reduces training variance and improves convergence.
- Mechanism: The Q-value is decomposed as Q(s, a) = pw(s, a)Qw(s, a) + (1 - pw(s, a))Ql(s, a), with separate training for win and loss components.
- Core assumption: Win and loss score distributions are sufficiently distinct that separate modeling reduces variance compared to direct combined prediction.
- Evidence anchors:
  - [section 3.3]: "Considering the significant gap between the distribution of winning scores and losing scores, we perform Value Factorization on the Q-value."
  - [abstract]: "The model uses reinforcement learning to simultaneously estimate win rates and expectations."
  - [corpus]: Weak evidence; related papers discuss RL in games but don't detail this specific factorization approach.
- Break condition: If win/loss distributions overlap significantly or are similarly shaped, factorization provides minimal benefit and adds complexity.

## Foundational Learning

- Concept: Reinforcement learning with function approximation
  - Why needed here: The vast state/action space (1083 states, 27,472 actions) makes tabular methods infeasible; neural networks generalize across similar states.
  - Quick check question: Why can't we use a lookup table for Q-values in Doudizhu?
- Concept: Value decomposition in RL
  - Why needed here: Direct Q-value prediction suffers from high variance due to sparse, bimodal rewards; decomposition stabilizes training.
  - Quick check question: What problem does separating win/loss Q-values solve?
- Concept: Action space pruning
  - Why needed here: The raw action space is too large for efficient search; pruning reduces computational cost while preserving quality moves.
  - Quick check question: How does pruning by Q-value help select better moves?

## Architecture Onboarding

- Component map: Bid model (3 networks for first/second/third positions) -> Card model (3 networks for landlord/landlord_down/landlord_up positions) -> Shared residual network architecture -> Value factorization layer
- Critical path: Bid model processes bid observations → outputs bid decision → Card model receives full game state including bid results → outputs cardplay action
- Design tradeoffs: Dual output (win rate + value) increases model capacity but requires careful training balance; pruning introduces hyperparameters (rho threshold) that affect exploration
- Failure signatures: If rho threshold is too aggressive, model may miss optimal moves; if win probability and value estimates are poorly calibrated, pruning becomes ineffective
- First 3 experiments:
  1. Train with only win rate output (remove value factorization) and compare convergence speed
  2. Test different rho thresholds (0.01, 0.05, 0.1) on pruning effectiveness
  3. Train without bidding phase to measure performance degradation and validate bidding integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the AlphaDou framework perform in other bidding-based card games beyond Doudizhu?
- Basis in paper: [explicit] The paper mentions "We hope that this new framework will provide valuable insights for AI development in other bidding-based games."
- Why unresolved: The paper only tests the framework on Doudizhu and does not explore its application to other games.
- What evidence would resolve it: Testing the AlphaDou framework on other bidding-based card games and comparing its performance to existing AI models for those games.

### Open Question 2
- Question: How does the performance of AlphaDou change with different values of the hyperparameters α1 and α2 in the loss function?
- Basis in paper: [explicit] The paper mentions that α1 and α2 are hyperparameters controlling the weights in the loss function.
- Why unresolved: The paper does not provide a sensitivity analysis of the model's performance with respect to these hyperparameters.
- What evidence would resolve it: Conducting experiments with different values of α1 and α2 and analyzing the impact on the model's performance metrics (win rate, average difference in points, etc.).

### Open Question 3
- Question: How does the AlphaDou framework handle incomplete information in the bidding phase, such as when players pass or bid low?
- Basis in paper: [explicit] The paper discusses the bidding phase and mentions that the model adjusts its strategy based on the bidding outcomes.
- Why unresolved: The paper does not provide a detailed analysis of how the model handles incomplete information during the bidding phase.
- What evidence would resolve it: Analyzing the model's decision-making process during the bidding phase in various scenarios with incomplete information and evaluating its effectiveness.

## Limitations
- Model Architecture Transparency: Lack of detailed specifications for residual network architecture and layer dimensions
- Bid Information Reliability: Assumes bid outcomes reliably signal opponent strength, which may break down with deceptive strategies
- Hyperparameter Sensitivity: No sensitivity analysis for pruning threshold (rho) parameter affecting action space pruning

## Confidence
- High Confidence: AlphaDou achieves state-of-the-art performance among publicly available models (supported by comprehensive benchmark comparisons)
- Medium Confidence: Value factorization reduces training variance (theoretically sound but lacks direct empirical validation)
- Medium Confidence: Bidding model superiority over supervised learning baselines (demonstrated but not compared to modern bidding-specific approaches)

## Next Checks
1. Conduct controlled experiments comparing training convergence speed and stability with and without value factorization
2. Systematically evaluate performance across different rho threshold values (0.01, 0.05, 0.1) to determine optimal settings
3. Test the model against opponents using deceptive bidding strategies to assess robustness under unreliable signals