---
ver: rpa2
title: Benchmarking Deep Learning Models for Object Detection on Edge Computing Devices
arxiv_id: '2409.16808'
source_url: https://arxiv.org/abs/2409.16808
tags:
- energy
- devices
- nano
- object
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper benchmarks popular object detection models\u2014YOLOv8\
  \ (Nano, Small, Medium), EfficientDet Lite (Lite0, Lite1, Lite2), and SSD (SSD MobileNet\
  \ V1, SSDLite MobileDet)\u2014on resource-constrained edge devices including Raspberry\
  \ Pi 3, 4, and 5 (with/without TPU accelerators) and Jetson Orin Nano. Performance\
  \ is measured in terms of energy consumption, inference time, and mean Average Precision\
  \ (mAP) using the COCO dataset."
---

# Benchmarking Deep Learning Models for Object Detection on Edge Computing Devices

## Quick Facts
- arXiv ID: 2409.16808
- Source URL: https://arxiv.org/abs/2409.16808
- Reference count: 22
- This paper benchmarks object detection models on edge devices measuring energy, speed, and accuracy trade-offs

## Executive Summary
This paper evaluates popular object detection models—YOLOv8 (Nano, Small, Medium), EfficientDet Lite (Lite0, Lite1, Lite2), and SSD (MobileNet V1, SSDLite MobileDet)—across resource-constrained edge devices including Raspberry Pi 3, 4, 5 (with/without TPU accelerators) and Jetson Orin Nano. Using the COCO dataset, the study measures energy consumption, inference time, and mean Average Precision (mAP) to identify optimal model-device combinations. The results demonstrate clear trade-offs: lower accuracy models like SSD MobileNet V1 are more energy-efficient and faster, while higher accuracy models like YOLOv8 Medium consume more energy and have slower inference times. Jetson Orin Nano emerges as the fastest and most energy-efficient device for request handling despite its highest idle energy consumption. TPU accelerators significantly improve performance for SSD and EfficientDet Lite models without affecting accuracy, though YOLOv8 accuracy decreases due to model compression requirements. This work provides actionable insights for selecting models and devices based on specific accuracy, speed, and energy efficiency requirements in edge computing applications.

## Method Summary
The study benchmarks object detection models on edge devices by deploying pre-trained models as Flask web services and measuring performance using automated load testing with Locust. Models are evaluated on Raspberry Pi 3/4/5 (with/without Coral USB TPU accelerators) and Jetson Orin Nano, using COCO validation dataset for accuracy assessment via FiftyOne tool. Performance metrics include inference time, energy consumption (measured with UM25C USB power meter), and mAP scores. The evaluation considers both base (idle) and total (active) energy consumption over 5-minute test periods, calculating energy per request. Models are converted to appropriate formats (PyTorch for Pi, TFLite for TPU, TensorRT for Jetson) and deployed as API endpoints. The comprehensive testing covers multiple model-device combinations to establish performance baselines and identify optimal configurations for edge computing scenarios.

## Key Results
- YOLOv8 models achieve higher accuracy but consume 1.22-5.87 mWh compared to more efficient SSD and EfficientDet Lite models
- Jetson Orin Nano is fastest and most energy-efficient for request handling despite highest idle energy consumption
- TPU accelerators significantly improve SSD and EfficientDet Lite performance without accuracy loss, but reduce YOLOv8 accuracy due to compression
- SSD MobileNet V1 delivers fastest inference at 61ms on Raspberry Pi 4 with TPU, while maintaining competitive accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: YOLOv8 models achieve higher accuracy but consume more energy and have slower inference times than SSD and EfficientDet Lite models on edge devices.
- Mechanism: YOLOv8 uses a single-stage regression approach that predicts bounding boxes and class probabilities directly from full images, requiring more computational resources than the two-stage or simpler one-stage approaches used by SSD and EfficientDet Lite.
- Core assumption: Model complexity and parameter count directly correlate with computational cost and energy consumption on edge hardware.
- Evidence anchors:
  - [abstract] "lower mAP models such as SSD MobileNet V1 are more energy-efficient and faster in inference, whereas higher mAP models like YOLOv8 Medium generally consume more energy and have slower inference"
  - [section 3.3] "YOLO8 models demonstrate higher energy demands, spanning from 1.22 mWh to 5.87 mWh" compared to SSD and Det_lite models
  - [corpus] Weak evidence - corpus does not contain specific YOLOv8 energy measurements
- Break condition: If model compression techniques (like quantization or pruning) reduce YOLOv8's computational footprint without sacrificing accuracy, this relationship may weaken.

### Mechanism 2
- Claim: TPU accelerators improve inference time and energy efficiency for SSD and EfficientDet Lite models without affecting accuracy.
- Mechanism: TPUs are specialized ASICs optimized for tensor operations common in convolutional neural networks, providing parallelized computation that reduces inference time and energy per operation.
- Core assumption: The models being accelerated (SSD and EfficientDet Lite) are compatible with TPU architecture and can leverage its specialized processing units.
- Evidence anchors:
  - [abstract] "TPU accelerators significantly improve performance for SSD and EfficientDet Lite models without affecting accuracy"
  - [section 3.3] "When the Coral USB Accelerator is integrated with the Raspberry Pi3, the inference times for SSD and YOLO8 models improve significantly, with SSD_v1 remaining the fastest at 61 ms"
  - [corpus] Weak evidence - corpus does not contain specific TPU-accelerated model performance data
- Break condition: If model formats are incompatible with TPU requirements (e.g., YOLOv8 requiring compression that reduces accuracy), this benefit disappears.

### Mechanism 3
- Claim: Jetson Orin Nano provides the best balance of speed and energy efficiency for request handling among tested devices, despite higher idle energy consumption.
- Mechanism: Orin Nano integrates a powerful GPU with dedicated AI accelerators, enabling efficient parallel processing of neural network operations while maintaining low power consumption during active inference.
- Core assumption: The GPU and AI accelerators in Orin Nano are effectively utilized by the object detection models and frameworks deployed on the device.
- Evidence anchors:
  - [abstract] "Among the edge devices, Jetson Orin Nano stands out as the fastest and most energy-efficient option for request handling, despite having the highest idle energy consumption"
  - [section 3.3] "Jetson Orin Nano demonstrates the lowest energy consumption per request across all models" and "Jetson Orin Nano outperforms the other devices"
  - [corpus] Weak evidence - corpus does not contain specific Jetson Orin Nano performance comparisons
- Break condition: If workload patterns change to longer idle periods or if models cannot effectively utilize the GPU architecture, the advantage diminishes.

## Foundational Learning

- Concept: Understanding the trade-off between model accuracy and computational efficiency
  - Why needed here: The paper demonstrates that higher accuracy models consume more energy and have slower inference times, requiring practitioners to balance these factors based on application requirements
  - Quick check question: If you need real-time object detection on a battery-powered drone, would you prioritize mAP or inference time/energy consumption?

- Concept: Familiarity with edge computing hardware architectures (CPU, GPU, TPU)
  - Why needed here: Different hardware accelerators (Raspberry Pi CPU, Jetson Orin Nano GPU, Coral USB TPU) show varying performance characteristics for different model types
  - Quick check question: Why might a TPU accelerator improve SSD model performance more than YOLOv8 model performance?

- Concept: Understanding model quantization and compression techniques
  - Why needed here: YOLOv8 models required compression to run on TPUs, resulting in reduced accuracy, highlighting the importance of understanding these techniques' impact
  - Quick check question: What happens to YOLOv8 accuracy when it's compressed to run on a TPU, and why does this occur?

## Architecture Onboarding

- Component map: COCO validation dataset -> FiftyOne evaluation tool -> mAP calculation -> PyTorch/TFLite/TensorRT frameworks -> Edge devices (Raspberry Pi 3/4/5, Jetson Orin Nano) -> Flask API service -> Locust load testing -> CSV logging -> Inference time and energy consumption metrics -> UM25C USB power meter -> Bluetooth connectivity -> Energy consumption measurement
- Critical path: Model deployment -> Load testing with Locust -> Performance measurement (inference time, energy consumption, mAP)
- Design tradeoffs: Accuracy vs. energy consumption vs. inference time; hardware cost vs. performance; model complexity vs. deployment feasibility
- Failure signatures: Models failing to deploy due to format incompatibility; energy measurements inconsistent with expected baselines; mAP scores significantly lower than published benchmarks
- First 3 experiments:
  1. Deploy SSD MobileNet V1 on Raspberry Pi 4 without TPU and measure baseline inference time and energy consumption
  2. Deploy the same model on Raspberry Pi 4 with TPU and compare performance improvements
  3. Deploy YOLOv8 Medium on Jetson Orin Nano and verify it achieves higher mAP than SSD while consuming more energy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do quantized models (int8, float16) compare to full precision models in terms of energy consumption, inference time, and accuracy across the evaluated edge devices?
- Basis in paper: [explicit] The authors suggest examining different quantized models as future work, indicating this has not been evaluated yet.
- Why unresolved: The paper focuses on full precision models and does not include quantized variants in its benchmarking.
- What evidence would resolve it: Comprehensive benchmarking of quantized models alongside full precision models across all tested devices and object detection models, measuring the same metrics (energy consumption, inference time, and accuracy).

### Open Question 2
- Question: What is the impact of model compression techniques on YOLOv8 accuracy when deployed on TPU accelerators?
- Basis in paper: [explicit] The paper notes that YOLOv8 accuracy significantly decreases when deployed on TPU accelerators due to model compression requirements.
- Why unresolved: The paper identifies this issue but does not investigate alternative compression methods or optimization strategies to mitigate accuracy loss.
- What evidence would resolve it: Systematic evaluation of various compression techniques (quantization, pruning, knowledge distillation) for YOLOv8 on TPU accelerators, comparing resulting accuracy and performance trade-offs.

### Open Question 3
- Question: How do energy consumption patterns differ between idle and active states for each edge device when running different object detection models?
- Basis in paper: [explicit] The paper measures base energy consumption (idle state) and total energy consumption (active state) separately.
- Why unresolved: While both metrics are measured, the paper does not analyze how these values interact or change dynamically based on workload characteristics.
- What evidence would resolve it: Detailed analysis of energy consumption dynamics during model operation, including power profiling during different phases of object detection (initialization, inference, post-processing) for each device and model combination.

## Limitations

- Model compression effects on YOLOv8 accuracy when deployed on TPUs are not fully characterized, with insufficient technical detail about compression implementations
- Hardware configuration variability (memory allocation, thermal throttling, background processes) could influence performance measurements, particularly for Jetson Orin Nano results
- Benchmarking methodology uses specific inference patterns via Locust but doesn't explore how performance scales with different batch sizes, input resolutions, or real-world traffic patterns

## Confidence

- **High Confidence**: The general relationship between model complexity and resource consumption (higher mAP models consume more energy and have slower inference) is well-supported by comparative data across multiple model-device combinations
- **Medium Confidence**: Device-specific performance rankings, particularly Jetson Orin Nano's superiority, are credible but could vary based on unmeasured hardware factors and workload patterns
- **Low Confidence**: The exact impact of TPU acceleration on different model families and the reproducibility of observed accuracy trade-offs for YOLOv8 models due to insufficient technical detail about compression implementations

## Next Checks

1. **Reproduce TPU Accuracy Impact**: Test YOLOv8 Nano and Small models with different quantization levels on Coral USB Accelerator to quantify the relationship between compression ratio and accuracy degradation

2. **Validate Device Rankings Under Different Workloads**: Repeat the Jetson Orin Nano benchmark with varying batch sizes and input resolutions to confirm its performance advantage holds across different deployment scenarios

3. **Cross-Validate Energy Measurements**: Use multiple power measurement methods (software profiling + hardware meters) to verify the energy consumption figures, particularly for the Jetson Orin Nano where idle consumption was notably high