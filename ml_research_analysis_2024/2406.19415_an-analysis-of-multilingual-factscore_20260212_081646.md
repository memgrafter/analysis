---
ver: rpa2
title: An Analysis of Multilingual FActScore
arxiv_id: '2406.19415'
source_url: https://arxiv.org/abs/2406.19415
tags:
- laminitial
- aleffinal
- factscore
- gpt4
- gemp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the behavior of the FActScore pipeline for
  evaluating factuality of LLM-generated long-form texts in multilingual settings.
  It introduces a new native dataset for FActScore evaluation in Spanish, Arabic,
  and Bengali, covering high-, medium-, and low-resource languages.
---

# An Analysis of Multilingual FActScore

## Quick Facts
- arXiv ID: 2406.19415
- Source URL: https://arxiv.org/abs/2406.19415
- Reference count: 15
- Key outcome: LLM performance in FActScore degrades significantly in lower-resource languages due to limited knowledge source coverage

## Executive Summary
This paper investigates the behavior of the FActScore pipeline for evaluating factuality of LLM-generated long-form texts in multilingual settings. The study introduces a new native dataset for FActScore evaluation in Spanish, Arabic, and Bengali, covering high-, medium-, and low-resource languages respectively. Results show that FActScore performance deteriorates significantly in lower-resource languages for both fact extraction and fact scoring tasks. The quality of the knowledge source, particularly Wikipedia coverage, plays a critical role in FActScore accuracy. Three mitigation strategies are proposed and evaluated: increasing retrieved passages, using the Internet as a knowledge source, and employing LLM-generated knowledge, all of which improve FActScore estimation across all languages.

## Method Summary
The study evaluates FActScore performance across Spanish, Arabic, and Bengali using biographies generated by multilingual LLMs (GPT-4, Gemini Pro). A native dataset was annotated for these languages to establish ground truth. The FActScore pipeline was tested with different knowledge sources (Wikipedia, Internet) and LLM scorers (GPT-4, Gemini Pro, GPT-3.5, Mistral). The study applied mitigation strategies including expanding retrieved passages from 8 to 20, using Internet-based knowledge sources, and augmenting Wikipedia with LLM-generated content. Fact extraction accuracy, retrieval performance, and scoring accuracy were measured across languages to identify performance patterns and degradation factors.

## Key Results
- FActScore performance degrades significantly in lower-resource languages, with Bengali showing the worst results across all metrics
- Increasing retrieved passages from 8 to 20 improves FActScore accuracy, with Bengali showing the largest gains
- Augmenting Wikipedia with GPT4-generated content improves FActScore accuracy across all languages, though the content is unverified

## Why This Works (Mechanism)
The FActScore pipeline relies on retrieving relevant passages from a knowledge source and using LLMs to extract atomic facts from generated text and score their factuality. Performance degradation in lower-resource languages occurs because: (1) limited Wikipedia coverage provides insufficient relevant passages for retrieval, (2) LLMs struggle with fact extraction and scoring in languages they're less proficient in, and (3) the scoring LLM may not have sufficient internal knowledge of facts in lower-resource languages. The mitigation strategies work by either increasing the likelihood of finding relevant information (more passages), expanding the knowledge base (Internet/API, LLM-generated content), or improving the quality of fact comparison.

## Foundational Learning
1. **Factuality evaluation pipeline** - Why needed: Understanding how FActScore works is essential for interpreting results and applying mitigation strategies. Quick check: Can you explain how fact extraction and scoring work in sequence?
2. **Language resource levels** - Why needed: The study categorizes languages as high, medium, and low resource, which directly impacts performance. Quick check: Can you identify which language (Spanish, Arabic, Bengali) corresponds to each resource level?
3. **Knowledge source dependency** - Why needed: The paper shows FActScore performance is constrained by knowledge source quality rather than just model capabilities. Quick check: Can you explain how Wikipedia coverage affects FActScore accuracy?
4. **LLM proficiency variation** - Why needed: Different LLMs show varying performance across languages and tasks. Quick check: Can you name which LLM performed best for fact extraction versus scoring?
5. **Fact extraction vs scoring** - Why needed: These are distinct tasks with different error patterns in the pipeline. Quick check: Can you describe one key difference in how errors manifest in extraction versus scoring?
6. **Mitigation strategy evaluation** - Why needed: Understanding how different approaches improve performance helps in practical deployment. Quick check: Can you list all three mitigation strategies tested and their general effectiveness?

## Architecture Onboarding

**Component Map**: Generated text -> Fact extraction (LLM) -> Retrieval (BM25/Neural) -> Knowledge source (Wikipedia/Internet) -> Fact scoring (LLM) -> FActScore

**Critical Path**: The fact extraction -> retrieval -> fact scoring sequence is critical. Retrieval quality directly impacts scoring accuracy, and both extraction and scoring depend on LLM capabilities in the target language.

**Design Tradeoffs**: The pipeline trades precision for coverage by using multiple retrieved passages and LLM-based components. This introduces uncertainty but enables scalability across languages. The choice between verified (Wikipedia) versus comprehensive (Internet/LLM-generated) knowledge sources represents a fundamental tradeoff between accuracy and completeness.

**Failure Signatures**: Performance degradation manifests as: (1) poor retrieval recall in low-resource languages due to limited Wikipedia coverage, (2) extraction errors when LLMs cannot identify atomic facts in target languages, and (3) scoring inaccuracies when comparing facts against incomplete knowledge sources.

**3 First Experiments**:
1. Measure fact extraction accuracy separately from scoring accuracy to identify which component is failing in each language
2. Test retrieval performance by counting relevant passages found for different query types across languages
3. Evaluate the impact of passage count on FActScore by testing with 8, 12, 16, and 20 passages per fact

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does increasing the number of retrieved passages have diminishing returns for FActScore accuracy across different resource levels?
- Basis in paper: [explicit] The paper shows that increasing retrieved passages from 8 to 20 improves FActScore accuracy, with particularly large gains in Bengali, but does not explore whether further increases would continue to improve accuracy or if there's an optimal number.
- Why unresolved: The study only tested one increase in passage count (8 to 20) and did not explore whether this relationship is linear or if there are diminishing returns at higher numbers.
- What evidence would resolve it: Experiments testing multiple passage counts (e.g., 20, 40, 60) across all resource levels would show whether accuracy plateaus or continues improving with more passages.

### Open Question 2
- Question: How do cultural and linguistic nuances in non-English languages affect the accuracy of LLM-based fact extraction and scoring?
- Basis in paper: [inferred] The paper notes that GPT4 and other LLMs show degraded performance in lower-resource languages for both fact extraction and scoring, but does not specifically analyze whether this is due to linguistic complexity or cultural context differences.
- Why unresolved: The study focuses on resource levels but doesn't isolate whether performance issues stem from linguistic features (grammar, syntax, vocabulary) or cultural factors (contextual understanding, idiomatic expressions).
- What evidence would resolve it: Controlled experiments comparing performance on linguistically similar but culturally different languages, or vice versa, would help determine which factor has greater impact on FActScore accuracy.

### Open Question 3
- Question: What is the long-term reliability of using unverified LLM-generated knowledge as a knowledge source for FActScore?
- Basis in paper: [explicit] The paper shows that augmenting Wikipedia with GPT4-generated content improves FActScore accuracy across all languages, but notes this content is "entirely unverified and likely contains some amount of factual errors."
- Why unresolved: The study demonstrates short-term benefits but does not examine how the accumulation of potentially incorrect information from LLM-generated content might affect long-term FActScore reliability.
- What evidence would resolve it: Longitudinal studies tracking FActScore accuracy over time as LLM-generated content accumulates, compared to verified sources, would show whether accuracy degrades due to propagated errors.

## Limitations
- Performance degradation in lower-resource languages is fundamentally tied to knowledge source quality rather than just model capabilities
- No single LLM consistently outperforms others across all languages, requiring language-specific model selection
- The study's findings may not generalize to languages beyond Spanish, Arabic, and Bengali
- LLM-generated knowledge, while improving accuracy, introduces unverified content that may contain factual errors

## Confidence
**High Confidence**: The core finding that FActScore performance degrades in lower-resource languages is well-supported by experimental data. The relationship between Wikipedia coverage and FActScore accuracy is consistently observed across all tested languages and mitigation strategies.

**Medium Confidence**: The effectiveness of proposed mitigation strategies shows promise but requires further validation across broader language sets and content domains. While improvements are statistically significant, the magnitude of gains varies substantially by language.

**Low Confidence**: The generalization of findings to languages beyond Spanish, Arabic, and Bengali remains uncertain. The specific interaction effects between different LLM architectures and language-specific characteristics need more systematic investigation.

## Next Checks
1. **Cross-Domain Validation**: Test the FActScore pipeline on non-biographical content (e.g., scientific articles, news reports) across all three languages to verify whether performance patterns hold for different text genres.

2. **Alternative Knowledge Sources**: Evaluate the impact of using domain-specific knowledge bases (e.g., medical databases, technical documentation) versus general web content for fact verification in each language.

3. **Human-AI Hybrid Pipeline**: Implement and test a hybrid approach where human annotators validate LLM-extracted facts before scoring, measuring whether this improves accuracy while maintaining scalability for low-resource languages.