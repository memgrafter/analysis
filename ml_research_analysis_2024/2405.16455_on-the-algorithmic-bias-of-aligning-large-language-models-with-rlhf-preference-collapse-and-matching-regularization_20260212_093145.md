---
ver: rpa2
title: 'On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference
  Collapse and Matching Regularization'
arxiv_id: '2405.16455'
source_url: https://arxiv.org/abs/2405.16455
tags:
- rlhf
- preference
- reward
- human
- preferences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper identifies a fundamental algorithmic bias in standard\
  \ reinforcement learning from human feedback (RLHF) that arises from the Kullback-Leibler\
  \ (KL) regularization term, which can lead to preference collapse\u2014a phenomenon\
  \ where minority preferences are disregarded in favor of the majority. To address\
  \ this, the authors introduce preference matching (PM) RLHF, a method that provably\
  \ aligns large language models with the preference distribution of the reward model\
  \ under the Bradley-Terry-Luce/Plackett-Luce model."
---

# On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization

## Quick Facts
- arXiv ID: 2405.16455
- Source URL: https://arxiv.org/abs/2405.16455
- Authors: Jiancong Xiao; Ziniu Li; Xingyu Xie; Emily Getzen; Cong Fang; Qi Long; Weijie J. Su
- Reference count: 40
- Key outcome: Identifies KL regularization bias in RLHF causing preference collapse; introduces PM RLHF achieving 29-41% improvement in alignment metrics

## Executive Summary
This paper identifies a fundamental algorithmic bias in standard reinforcement learning from human feedback (RLHF) that arises from Kullback-Leibler regularization, leading to preference collapse where minority preferences are disregarded. The authors propose preference matching (PM) RLHF, which provably aligns models with the reward model's preference distribution under the Bradley-Terry-Luce/Plackett-Luce model. The core innovation is a preference matching regularizer derived from solving an ordinary differential equation, taking the form of the negative logarithm of the policy probability distribution. Experiments on OPT and Llama-family models demonstrate significant improvements in alignment with human preferences.

## Method Summary
The method introduces preference matching RLHF by deriving a regularizer from solving an ordinary differential equation that ensures the model's output distribution matches the reward model's preference distribution. For practical implementation, a conditional variant restricts regularization to responses deemed natural by the reference model, avoiding meaningless text generation. The approach fine-tunes LLMs using a modified RL objective that balances reward maximization with response diversification through entropy-based regularization. The method is evaluated on TL;DR summarization and full-hh-rlhf datasets using preference matching divergence, perplexity, and other alignment metrics.

## Key Results
- 29% to 41% improvement in alignment with human preferences measured by preference matching divergence compared to standard RLHF
- Conditional PM RLHF successfully balances preference matching with natural text generation quality
- Preference collapse in standard RLHF confirmed through theoretical analysis and empirical validation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KL regularization in standard RLHF introduces algorithmic bias by blending reference model's preferences with reward model's preferences
- Mechanism: The KL penalty creates a mixture between reference model distribution and reward-based distribution, preventing pure alignment with reward model
- Core assumption: Reference model contains inherent biases from not being aligned with human preferences
- Evidence anchors: Abstract mentions KL-based regularization bias; section B.1 shows explicit mixture formula prlhf(yi) = pref(yi)preward(yi)/[pref(y1)preward(y1) + pref(y2)preward(y2)]

### Mechanism 2
- Claim: PM regularizer derived from ODE ensures model output matches reward model's preference distribution
- Mechanism: Solving PM differential equation yields regularization term as negative log of policy probability, balancing reward maximization with response diversification
- Core assumption: Reward model accurately represents human preferences under BTL/PL model
- Evidence anchors: Abstract mentions PM regularizer form; section 3.2 shows entropy connection through −log π term

### Mechanism 3
- Claim: Conditional PM RLHF resolves natural language generation by restricting regularization to natural responses
- Mechanism: Conditioning on πref(y|x) ≥ α avoids applying PM regularizer to meaningless text while maintaining generation quality
- Core assumption: Reference model can distinguish natural from meaningless text through output probabilities
- Evidence anchors: Abstract mentions conditional variant for natural language generation; section 4.1 discusses unconstrained relaxation with conditional form

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Baseline algorithm with identified algorithmic bias
  - Quick check question: What are the three main steps in RLHF pipeline and what role does reward model play?

- Concept: Bradley-Terry-Luce/Plackett-Luce preference models
  - Why needed here: Define how human preferences are generated and form theoretical foundation
  - Quick check question: How does PL model extend BTL model from pairwise comparisons to full rankings?

- Concept: Kullback-Leibler divergence and f-divergences
  - Why needed here: Regularization terms whose biases are analyzed in the paper
  - Quick check question: Why does replacing KL divergence with f-divergence not eliminate algorithmic bias?

## Architecture Onboarding

- Component map:
  - LLM policy network (πϕ(y|x)) -> generates responses
  - Reward model (r(x,y)) -> approximates human preferences from pairwise comparisons
  - Reference model (πref(y|x)) -> pretrained/SFT model for regularization
  - Preference matching regularizer -> derived from solving ODE
  - Conditional filter -> determines which responses receive regularization

- Critical path:
  1. Collect human preference data (pairwise comparisons)
  2. Train reward model using BTL/PL assumptions
  3. Implement PM regularizer based on ODE solution
  4. Apply conditional filtering for natural language generation
  5. Fine-tune LLM using modified RL objective

- Design tradeoffs:
  - Pure PM RLHF vs Conditional PM RLHF: Conditional variant sacrifices theoretical guarantees for generation quality
  - Entropy regularization strength: Balancing preference matching vs response diversity
  - Threshold α selection: Trade-off between generation quality and preference alignment coverage

- Failure signatures:
  - Preference collapse: Minority preferences completely ignored (KL RLHF symptom)
  - Unnatural text generation: High perplexity and meaningless outputs (PM RLHF symptom)
  - Overfitting to reward model: Loss of diversity in generated responses

- First 3 experiments:
  1. Compare PM RLHF vs KL RLHF on small dataset to verify PM divergence improvements
  2. Test different α values in conditional PM RLHF to find optimal balance
  3. Validate ODE derivation by checking learned policy matches PL model under various rewards

## Open Questions the Paper Calls Out

- Question: How does algorithmic bias in KL-based RLHF manifest differently across various model scales?
  - Basis: Paper notes small models may be insufficient while larger models conjectured to be more effective
  - Why unresolved: Computational constraints prevented testing on larger models like GPT-4
  - What evidence would resolve it: Systematic experiments comparing PM RLHF performance across spectrum of model sizes

- Question: What is optimal strategy for choosing threshold α in conditional PM RLHF?
  - Basis: Paper suggests α = 1 − top p but notes results robust across values without principled method
  - Why unresolved: Treats α as practical tuning parameter without theoretical guidance
  - What evidence would resolve it: Comprehensive sensitivity analysis showing PM divergence variation with α

- Question: Can PM regularization extend to handle heterogeneous human preferences across subpopulations?
  - Basis: Paper mentions MaxMin RLHF for group-level bias but doesn't integrate with PM regularization
  - Why unresolved: Current PM framework assumes single reward model requiring significant theoretical development
  - What evidence would resolve it: Theoretical work deriving PM regularization for multi-reward settings

## Limitations
- Theoretical claims rely heavily on Bradley-Terry-Luce/Plackett-Luce model assumption without empirical validation on real human preference data
- Conditional PM RLHF's effectiveness depends critically on reference model's ability to distinguish natural from meaningless text, which is not empirically validated
- 29-41% improvement claim based on proxy metric (preference matching divergence) without direct human evaluation of response quality

## Confidence
- High Confidence: Mathematical derivation of preference matching regularizer is rigorous and internally consistent
- Medium Confidence: Experimental results showing PM divergence improvement are reproducible but lack human evaluation
- Low Confidence: Conditional PM RLHF's success depends on reference model capability that isn't validated

## Next Checks
1. Conduct direct human evaluation comparing responses from standard RLHF and PM RLHF models on held-out prompts
2. Systematically evaluate conditional PM RLHF performance across wider range of α values (0.05 to 0.95)
3. Empirically test whether Bradley-Terry-Luce model assumption holds for human preference data by comparing predicted vs actual choice frequencies