---
ver: rpa2
title: SSCAE -- Semantic, Syntactic, and Context-aware natural language Adversarial
  Examples generator
arxiv_id: '2403.11833'
source_url: https://arxiv.org/abs/2403.11833
tags:
- adversarial
- sscae
- semantic
- attack
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SSCAE, a practical black-box adversarial attack
  model for generating semantically, syntactically, and context-aware adversarial
  examples in natural language processing. SSCAE identifies important words and uses
  a masked language model to generate an initial set of substitutions.
---

# SSCAE -- Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator

## Quick Facts
- arXiv ID: 2403.11833
- Source URL: https://arxiv.org/abs/2403.11833
- Authors: Javad Rafiei Asl; Mohammad H. Rafiei; Manar Alohaly; Daniel Takabi
- Reference count: 40
- Outperforms state-of-the-art methods in after-attack accuracy, semantic consistency, and query efficiency across seven datasets

## Executive Summary
This paper introduces SSCAE, a black-box adversarial attack model designed to generate semantically, syntactically, and context-aware adversarial examples for natural language processing tasks. The method combines importance scoring, context-aware substitution generation using masked language models, linguistic refinement with semantic and syntactic evaluation, dynamic thresholding, and local greedy search. SSCAE demonstrates superior performance compared to existing methods across multiple datasets and target models, achieving higher attack success rates while maintaining semantic consistency and query efficiency.

## Method Summary
SSCAE operates by first identifying important words in input text using importance scoring, then generating context-aware substitutions using BERT MLM applied to neighboring words rather than the important word itself. These substitutions undergo refinement using USE embeddings for semantic similarity and GPT-2 for syntactic correctness. A dynamic threshold technique is applied to capture efficient perturbations, and a local greedy search method evaluates combinations of multiple substitutions simultaneously to maximize reduction in the true label's confidence score. The method is evaluated across seven text classification and NLI datasets against multiple target models.

## Key Results
- Outperforms state-of-the-art methods in after-attack accuracy, semantic consistency, and query efficiency
- Maintains comparable perturbation rates while achieving higher attack success
- Demonstrates effectiveness across different target models (BERT, WordLSTM, ALBERT-Base, ESIM, BERT-Large) and NLP cloud platforms
- Shows consistent performance across seven diverse datasets including YELP, IMDB, MR, SST2, SNLI, MNLI-Matched, and MNLI-Mismatched

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic thresholding improves semantic consistency by adjusting thresholds per word rather than using a fixed global threshold.
- Mechanism: For each important word, compute thresholds based on the distribution of semantic similarity and syntactic correctness scores of candidate substitutions, using the TopN score minus a small fraction of the score range.
- Core assumption: The distribution of semantic and syntactic scores for substitutions varies significantly across different words, so a single threshold cannot optimally balance perturbation effectiveness and semantic preservation.
- Evidence anchors:
  - [abstract]: "We introduce (1) a dynamic threshold to capture more efficient perturbations and (2) a local greedy search to generate high-quality AEs."
  - [section]: "By independently computing this threshold for every important word, SSCAE, for the first time, considers the non-linearity of the semantic and syntactic correctness distribution."
- Break condition: If the score distributions are too uniform across words, dynamic thresholds may not provide meaningful differentiation, and the extra computation cost may outweigh benefits.

### Mechanism 2
- Claim: Local greedy search increases attack success by substituting multiple important words simultaneously rather than sequentially.
- Mechanism: Identify M important words, generate N top refined substitutions for each, and evaluate all NM combinations to find the set that maximally reduces the confidence score of the true label.
- Core assumption: Simultaneous substitution of multiple words creates more severe perturbations than sequential single-word substitutions, leading to faster and more effective fooling of the target model.
- Evidence anchors:
  - [abstract]: "We introduce ... (2) a local greedy search to generate high-quality AEs."
  - [section]: "The local greedy search method is employed by the proposed SSCAE model to generate adversarial samples: For each of M important words... SSCAE selects top N refined substitutions that lead to the most reduction of the confidence score."
- Break condition: If M and N are too large, the combinatorial explosion makes exhaustive search infeasible; if too small, the perturbations may be insufficient to fool the model.

### Mechanism 3
- Claim: Masked language model-based substitution generation produces more context-aware and grammatically consistent adversarial examples than embedding-based synonym retrieval.
- Mechanism: Instead of applying BERT MLM directly on the important word, apply it on neighboring words within a window to retrieve contextually relevant substitutions, then refine these candidates using semantic and syntactic language models.
- Core assumption: Substitutions generated from masked neighboring words better preserve context and grammar than direct substitutions of the important word itself.
- Evidence anchors:
  - [section]: "Instead of directly applying the BERT MLM on the important word to retrieve candidate substitutions, an alternative approach is to apply the BERT MLM on a neighboring word... This helps retrieve identical and non-identical substitutions to improve the target model's chance of fooling."
- Break condition: If the context window is too small, substitutions may lack sufficient context; if too large, computational cost increases without proportional benefit.

## Foundational Learning

- Concept: Adversarial examples in NLP
  - Why needed here: Understanding the discrete nature of text and the challenges of generating imperceptible perturbations is fundamental to designing SSCAE.
  - Quick check question: Why is generating adversarial examples more challenging in NLP than in computer vision?

- Concept: Masked Language Models (MLMs)
  - Why needed here: SSCAE uses BERT MLM to generate contextually appropriate substitutions for important words.
  - Quick check question: How does a masked language model differ from standard language models in generating word substitutions?

- Concept: Semantic similarity metrics (cosine similarity)
  - Why needed here: SSCAE uses USE embeddings and cosine similarity to measure semantic preservation between original and adversarial text.
  - Quick check question: What does a cosine similarity of 0.8 between two sentence embeddings indicate about their semantic relationship?

## Architecture Onboarding

- Component map: Input preprocessing -> Importance scoring -> Context-aware substitution generation -> Linguistic refinement -> Dynamic threshold computation -> Local greedy search -> Adversarial example evaluation
- Critical path: Importance scoring → Substitution generation → Refinement → Thresholding → Local greedy search → Evaluation
- Design tradeoffs:
  - Larger context windows increase contextual awareness but also computational cost
  - More substitutions per word (larger N) improves attack success but increases search space exponentially
  - Dynamic thresholding requires per-word computation but yields better semantic consistency
- Failure signatures:
  - High perturbation percentage with low semantic consistency indicates poor threshold calibration
  - Low attack success with high semantic consistency suggests insufficient perturbation strength
  - Excessive query numbers may indicate suboptimal local greedy search parameters
- First 3 experiments:
  1. Run SSCAE on YELP dataset with BERT target model to verify basic functionality and compare with TextFooler baseline
  2. Test different K values (number of context-aware substitutions) to find optimal balance between semantic consistency and attack success
  3. Compare constant vs. dynamic thresholding on a small subset to demonstrate effectiveness of dynamic approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dynamic thresholding technique in SSCAE compare to more sophisticated optimization methods like mixed-integer optimization for refining substitutions?
- Basis in paper: [explicit] The paper mentions that dynamic thresholding is one of two key novelties and suggests it could potentially be replaced with "more efficient" techniques like mixed-integer optimization.
- Why unresolved: The paper only introduces and validates the dynamic threshold approach, without exploring or comparing it to alternative optimization methods for the same task.
- What evidence would resolve it: A controlled experiment comparing SSCAE's dynamic thresholding against mixed-integer optimization (or similar methods) on the same datasets and metrics would clarify the relative effectiveness and efficiency trade-offs.

### Open Question 2
- Question: What is the impact of extending SSCAE to incorporate insertion and deletion operations alongside substitution for generating adversarial examples?
- Basis in paper: [explicit] The conclusion states that "implementing practical operations in similar studies, such as insertion and deletion, within the SSCAE model remains an open question ripe for further investigation."
- Why unresolved: The current SSCAE implementation focuses solely on word substitutions, leaving the potential benefits and challenges of including insertion and deletion operations unexplored.
- What evidence would resolve it: Implementing and evaluating an extended SSCAE model that includes insertion and deletion operations, comparing its performance against the original model across the same datasets and metrics, would provide empirical evidence of the impact.

### Open Question 3
- Question: How does the choice of language model (e.g., Roberta, Albert, BART) for generating context-aware substitutions affect the quality and efficiency of adversarial examples in SSCAE?
- Basis in paper: [explicit] The paper notes that "BERT MLM can be replaced with Transformer-based models such as Roberta, Albert, and BART [...] to, perhaps, obtain high-quality contextualized substitutions."
- Why unresolved: The paper uses BERT MLM for generating substitutions but does not investigate whether alternative transformer-based models could yield better results.
- What evidence would resolve it: Systematic experimentation replacing BERT MLM with different transformer-based models (Roberta, Albert, BART) in the SSCAE pipeline, measuring performance on the established metrics (after-attack accuracy, semantic consistency, query number, perturbation rate), would reveal the impact of the choice of language model.

## Limitations
- The dynamic threshold mechanism relies heavily on distributional assumptions about semantic and syntactic scores with limited empirical validation across different word types and domains.
- The local greedy search's exponential complexity with respect to important words and substitutions per word could become computationally prohibitive for longer texts.
- The context-aware substitution approach may not generalize well to texts with complex syntactic structures or domain-specific terminology where context windows may not capture sufficient semantic information.

## Confidence

**High Confidence:** The claim that SSCAE outperforms state-of-the-art methods in after-attack accuracy, semantic consistency, and query efficiency is well-supported by experimental results across seven datasets and multiple target models.

**Medium Confidence:** The effectiveness of the dynamic thresholding technique is demonstrated through comparative experiments, but the paper does not provide extensive ablation studies showing the impact of different threshold calculation methods or sensitivity analysis to threshold parameters.

**Medium Confidence:** While the local greedy search method is shown to improve attack success rates, the computational complexity implications and the optimal parameter settings are not thoroughly explored.

**Low Confidence:** The claim that masked language model-based substitution generation produces more context-aware and grammatically consistent adversarial examples than embedding-based synonym retrieval lacks direct comparative experiments.

## Next Checks

1. **Ablation Study on Dynamic Thresholding:** Conduct experiments comparing SSCAE with constant thresholding versus dynamic thresholding across different word categories (common nouns, proper nouns, verbs, adjectives) to quantify the contribution of dynamic thresholding to semantic consistency and attack success rates.

2. **Computational Complexity Analysis:** Measure and report the actual computational time and memory requirements of the local greedy search method as a function of text length, M, and N parameters. Compare these requirements against alternative search strategies like beam search or evolutionary algorithms.

3. **Context Window Sensitivity Analysis:** Systematically vary the context window size used in the BERT MLM-based substitution generation and measure the impact on attack success rate, semantic consistency, and syntactic correctness. This would validate whether larger context windows consistently improve performance or if there are diminishing returns.