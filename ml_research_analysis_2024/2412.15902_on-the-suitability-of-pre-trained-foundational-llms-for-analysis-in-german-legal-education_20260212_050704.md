---
ver: rpa2
title: On the Suitability of pre-trained foundational LLMs for Analysis in German
  Legal Education
arxiv_id: '2412.15902'
source_url: https://arxiv.org/abs/2412.15902
tags:
- legal
- llama
- language
- argument
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the potential of pre-trained large language
  models (LLMs) for legal analysis tasks in German legal education. The authors evaluate
  LLMs on argument mining tasks, such as identifying components of the "Gutachtenstil"
  appraisal style, and essay scoring tasks using datasets of German legal texts and
  English student essays.
---

# On the Suitability of pre-trained foundational LLMs for Analysis in German Legal Education

## Quick Facts
- arXiv ID: 2412.15902
- Source URL: https://arxiv.org/abs/2412.15902
- Authors: Lorenz Wendlinger; Christian Braun; Abdullah Al Zubaer; Simon Alexander Nonn; Sarah Großkopf; Christofer Fellicious; Michael Granacher
- Reference count: 10
- Primary result: Pre-trained LLMs show promise for legal analysis in German education, particularly in low-data scenarios, but struggle with complex tasks like Gutachtenstil argument mining

## Executive Summary
This paper investigates the potential of pre-trained large language models (LLMs) for legal analysis tasks in German legal education. The authors evaluate LLMs on argument mining tasks, specifically identifying components of the "Gutachtenstil" appraisal style, and essay scoring tasks using datasets of German legal texts and English student essays. They employ prompt engineering techniques including few-shot prompting, chain-of-thought prompting, and retrieval-augmented generation (RAG) for example selection. Results show that while LLMs struggle with complex legal analysis tasks, they perform adequately on simpler tasks and can be competitive with baselines in low-data scenarios.

## Method Summary
The paper evaluates several pre-trained LLMs (Llama 3, Mixtral, GPT-3.5) on two main tasks: Gutachtenstil argument mining and essay scoring. For argument mining, the authors use a manually annotated German legal dataset and test zero-shot, few-shot, and RAG-enhanced prompting strategies. For essay scoring, they evaluate on both English and German datasets using similar prompting techniques. The paper introduces a novel RAG-based example selection method that retrieves relevant examples based on embedding similarity. Performance is measured using F1 scores for classification tasks and Pearson correlation for essay scoring.

## Key Results
- Pre-trained LLMs improve upon baselines in scenarios with little or no labeled data, particularly for simpler tasks
- RAG-based example selection substantially improves predictions in high data availability scenarios
- Chain-of-thought prompting helps in zero-shot scenarios but shows deficiencies in few-shot prompting
- English essay scoring tasks show better performance than German legal tasks, indicating language proficiency gaps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG-based example selection improves performance in high data availability scenarios.
- Mechanism: The model retrieves examples most relevant to the query based on cosine similarity of embeddings, providing contextually appropriate few-shot examples.
- Core assumption: Cosine similarity over sentence embeddings effectively identifies helpful examples despite document disjointedness.
- Evidence anchors:
  - [abstract] "we introduce a Retrieval Augmented Generation based prompt example selection method that substantially improves predictions in high data availability scenarios."
  - [section] "RAG-based example selection and chain-of-thought prompting improve performance in specific cases."
  - [corpus] Weak - corpus doesn't provide direct evidence about RAG effectiveness in this specific task.
- Break condition: When the embedding space doesn't capture semantic relevance for legal concepts, or when the dataset lacks sufficiently similar examples.

### Mechanism 2
- Claim: Chain-of-thought prompting helps in zero-shot scenarios but not in few-shot prompting.
- Mechanism: CoT encourages the model to reason through the problem step-by-step, but this conflicts with the structured format of few-shot examples.
- Core assumption: The model can integrate reasoning steps when no examples are provided, but gets confused when examples don't include reasoning.
- Evidence anchors:
  - [abstract] "Chain-of-Thought prompting further helping in the zero-shot case."
  - [section] "We find that with CoT prompting, the Llama 3 shows some deficiencies following instructions."
  - [corpus] Weak - corpus doesn't directly address CoT effectiveness.
- Break condition: When the model's instruction-following capability is overwhelmed by the cognitive load of generating explanations alongside task completion.

### Mechanism 3
- Claim: Pre-trained LLMs can generalize from one domain to another with minimal performance penalty.
- Mechanism: The model learns instruction-following patterns that transfer across domains, making example content less critical than the structure of the response.
- Core assumption: Example shots primarily teach the model how to format responses rather than what specific content to produce.
- Evidence anchors:
  - [abstract] "Throughout, pre-trained LLMs improve upon the baseline in scenarios with little or no labeled data"
  - [section] "We find that this is rooted in the fact that examples are mostly useful for instruction following and their actual content and exact response are secondary."
  - [corpus] Weak - corpus doesn't provide evidence about cross-domain generalization.
- Break condition: When domain-specific knowledge is essential and cannot be inferred from general instruction patterns.

## Foundational Learning

- Concept: Prompt engineering techniques (few-shot, chain-of-thought, RAG)
  - Why needed here: The paper demonstrates that different prompting strategies have varying effectiveness depending on task complexity and data availability.
  - Quick check question: Can you explain why RAG helps more in high-data scenarios while CoT helps more in zero-shot scenarios?

- Concept: Evaluation metrics for NLP tasks (F1, accuracy, correlation)
  - Why needed here: The paper uses multiple metrics to assess model performance across different tasks (argument mining, essay scoring).
  - Quick check question: Why might macro F1 be more appropriate than accuracy for imbalanced classification tasks like Gutachtenstil argument mining?

- Concept: Domain adaptation and transfer learning
  - Why needed here: The paper explores how pre-trained models perform on specialized legal tasks with limited data.
  - Quick check question: What factors determine whether a pre-trained LLM can successfully transfer to a new domain without fine-tuning?

## Architecture Onboarding

- Component map: Pre-trained LLM (Llama 3, Mixtral, GPT-3.5) → Prompt engineering layer (few-shot, CoT, RAG) → Task-specific evaluation (argument mining, essay scoring)
- Critical path: Query → Embedding generation → RAG retrieval (if enabled) → Prompt assembly → LLM inference → Result extraction/pattern matching → Evaluation
- Design tradeoffs: Language understanding vs. reasoning capability (Llama 3 excels at language, GPT-3.5 at reasoning), efficiency vs. accuracy (few-shot vs. zero-shot), context length vs. cost
- Failure signatures: Poor performance on Gutachtenstil classification despite adequate results on simpler tasks suggests limitations in complex legal reasoning; English task success vs. German task failure indicates language proficiency gaps
- First 3 experiments:
  1. Test zero-shot performance on Gutachtenstil task to establish baseline capability
  2. Implement RAG-based example selection and measure performance improvement
  3. Compare few-shot prompting with and without chain-of-thought to isolate the effect of reasoning steps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the capabilities of pre-trained LLMs in German legal education compare to their performance in English legal tasks, and what factors contribute to these differences?
- Basis in paper: [explicit] The paper highlights that pre-trained LLMs perform better in English language AES tasks compared to German legal tasks, suggesting a gap in language proficiency.
- Why unresolved: The paper does not provide a detailed comparative analysis of LLM performance across different languages in legal contexts, nor does it explore the underlying reasons for these discrepancies.
- What evidence would resolve it: Conducting a systematic study comparing LLM performance on equivalent legal tasks in both German and English, analyzing factors such as language-specific nuances and training data quality.

### Open Question 2
- Question: What are the limitations of current prompting strategies, such as Chain-of-Thought and Retrieval Augmented Generation, in enhancing the performance of LLMs for complex legal analysis tasks?
- Basis in paper: [explicit] The paper discusses the use of Chain-of-Thought and RAG but notes that these strategies have limited effectiveness in complex legal tasks, with CoT showing deficiencies in instruction following.
- Why unresolved: The paper does not explore alternative prompting strategies or modifications that could potentially improve LLM performance in complex legal contexts.
- What evidence would resolve it: Experimenting with new or modified prompting strategies and evaluating their impact on LLM performance in legal analysis tasks.

### Open Question 3
- Question: How can the efficiency and cost-effectiveness of using pre-trained LLMs for legal analysis in education be improved, given the current limitations in inference compute and language proficiency?
- Basis in paper: [explicit] The paper mentions the high cost of inference and the limitations of LLMs in languages other than the main training language, highlighting the need for more efficient solutions.
- Why unresolved: The paper does not propose specific methods or technologies to address these efficiency and cost challenges in LLM deployment for legal education.
- What evidence would resolve it: Developing and testing new models or techniques that optimize inference efficiency and improve language proficiency in non-primary languages for legal tasks.

## Limitations

- Study focuses exclusively on German legal education contexts with limited testing across diverse legal domains
- Evaluation datasets are relatively small and may not capture full complexity of real-world legal analysis scenarios
- Does not explore fine-tuning approaches which could yield different performance characteristics
- Limited systematic comparison of RAG-based vs manual example selection effectiveness

## Confidence

*High Confidence:* The observation that pre-trained LLMs show consistent improvement over baselines in low-data scenarios is well-supported by the experimental results across multiple tasks and models. The finding that example content is secondary to instruction-following patterns in few-shot prompting is also robustly demonstrated.

*Medium Confidence:* The performance differences between RAG-based and manual example selection warrant medium confidence. While the paper claims RAG substantially improves predictions in high-data scenarios, the experimental evidence is somewhat limited, and the effectiveness may depend heavily on the specific embedding methodology and dataset characteristics.

*Low Confidence:* The claim about chain-of-thought prompting helping specifically in zero-shot scenarios while hindering few-shot performance requires additional validation. The paper presents this as a finding but provides limited ablation studies to definitively isolate the causes of this behavior.

## Next Checks

1. Conduct a systematic ablation study comparing RAG-based example selection against alternative retrieval methods (e.g., semantic search, keyword matching) across varying dataset sizes to quantify the specific contribution of the RAG approach.

2. Test cross-linguistic generalization by evaluating whether models successful on English essay scoring tasks can transfer performance to German legal texts with minimal adaptation, controlling for language-specific factors.

3. Implement a controlled experiment varying the complexity of Gutachtenstil components to identify specific reasoning bottlenecks that prevent successful classification, distinguishing between language understanding limitations and logical reasoning constraints.