---
ver: rpa2
title: 'Basis Sharing: Cross-Layer Parameter Sharing for Large Language Model Compression'
arxiv_id: '2410.03765'
source_url: https://arxiv.org/abs/2410.03765
tags:
- basis
- sharing
- layers
- compression
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Basis Sharing, a method to compress large language
  models by sharing a set of basis vectors across layers while maintaining unique
  coefficients for each layer. The key idea is to apply SVD on horizontally concatenated
  weight matrices from multiple layers to obtain shared basis vectors, which are then
  scaled by layer-specific coefficients.
---

# Basis Sharing: Cross-Layer Parameter Sharing for Large Language Model Compression

## Quick Facts
- arXiv ID: 2410.03765
- Source URL: https://arxiv.org/abs/2410.03765
- Authors: Jingcun Wang; Yu-Guang Chen; Ing-Chao Lin; Bing Li; Grace Li Zhang
- Reference count: 13
- Key outcome: Achieves up to 25% reduction in perplexity on generation tasks and 4% improvement in accuracy on reasoning tasks compared to state-of-the-art SVD-based compression methods under the same compression ratio.

## Executive Summary
This paper introduces Basis Sharing, a novel method for compressing large language models by sharing basis vectors across layers while maintaining unique layer-specific coefficients. The approach applies SVD to horizontally concatenated weight matrices from multiple layers, enabling significant parameter reduction while preserving model functionality. The authors carefully select which weight matrix types benefit from cross-layer sharing and develop criteria for grouping adjacent layers to minimize performance degradation. Extensive experiments demonstrate that Basis Sharing outperforms existing compression methods across multiple model families and tasks.

## Method Summary
Basis Sharing compresses LLMs by decomposing weight matrices across layers into shared basis vectors and layer-specific coefficients using SVD. The method scales weight matrices with input data statistics before decomposition to preserve important dimensions, selectively applies cross-layer sharing only to weight matrix types that show low Frobenius loss when shared, and groups adjacent layers based on empirical analysis of compression error. The compressed model stores basis vectors and coefficients instead of full weight matrices, with a modified inference process that applies layer-specific scaling during forward propagation.

## Key Results
- Achieves up to 25% reduction in perplexity on generation tasks compared to state-of-the-art SVD-based compression methods
- Demonstrates 4% improvement in accuracy on reasoning tasks under the same compression ratio
- Shows 1.57x throughput improvement in real hardware efficiency gains
- Outperforms training from scratch for weight sharing across all tested model families (LLaMA, OPT, Mistral, GPT-2)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-layer parameter sharing via SVD reduces redundancy by exploiting similarity between weight matrices across transformer layers.
- **Mechanism:** Weight matrices from multiple layers are concatenated horizontally and decomposed via SVD to obtain shared basis vectors and layer-specific coefficients, allowing representation with fewer parameters while preserving unique layer functionality.
- **Core assumption:** Weight matrices across different layers in a pretrained LLM contain redundant or similar structure that can be captured by shared basis vectors without significant loss of performance.
- **Evidence anchors:** [abstract] "weight matrices in different layers are decomposed and represented as a linear combination of a set of shared basis vectors and unique coefficients"; [section] "weight matrices in different layers of an LLM might share similarity, parameter sharing across layers can be exploited to further compress weight matrices for LLMs"
- **Break condition:** If weight matrices across layers are too dissimilar or if layer-specific coefficients cannot adequately capture unique layer functionality, compression error will become too large and performance will degrade significantly.

### Mechanism 2
- **Claim:** Incorporating input data information through scaling matrices reduces compression error by prioritizing important weight dimensions.
- **Mechanism:** A scaling matrix S is computed using input data statistics (via cholesky decomposition of X^T X) and applied to weight matrices before SVD, ensuring that dimensions more sensitive to input data are preserved more accurately during compression.
- **Core assumption:** The effect of input data on weight matrix importance can be captured by the matrix S, and scaling weight matrices before SVD preserves critical computation paths.
- **Evidence anchors:** [section] "we will scale the concatenated weight matrix W with a matrix S ∈ R^(d1×d1) as follows W = S^(−1)SW = S^(−1)(SW)"; [section] "S can be evaluated with S(S)T = cholesky((X)T X)"
- **Break condition:** If input data statistics used to compute S don't represent actual inference distribution, or if scaling becomes unstable, compression error may increase rather than decrease.

### Mechanism 3
- **Claim:** Selective parameter sharing across specific weight matrix types and adjacent layer groupings minimizes performance degradation while maximizing compression.
- **Mechanism:** Only certain weight matrix types (WK, WQ, WV, WU_p, WGate) that show low Frobenius loss when shared are selected, and adjacent layers are grouped based on empirical analysis of compression error to balance compression ratio and accuracy.
- **Core assumption:** Not all weight matrix types benefit equally from cross-layer sharing, and adjacent layers are more likely to have similar weight patterns than non-adjacent layers.
- **Evidence anchors:** [section] "we will determine whether each of them can use cross-layer basis sharing by examining the Frobenius loss resulted from this sharing"; [section] "the group of two adjacent layers leads to smaller Frobenius loss than the sum of the Frobenius loss of two separate layers"
- **Break condition:** If non-adjacent layers are grouped or if unsuitable weight matrix types are shared, compression error may exceed acceptable thresholds and model performance will suffer.

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD)**
  - Why needed here: SVD is the core mathematical operation that enables decomposition of concatenated weight matrices into shared basis vectors and coefficients
  - Quick check question: What does SVD decompose a matrix into, and how does this relate to low-rank approximation?

- **Concept: Frobenius norm and matrix approximation error**
  - Why needed here: The Frobenius norm is used to measure compression error and determine which weight matrices and layer groupings are suitable for cross-layer sharing
  - Quick check question: How is the Frobenius norm calculated, and why is it appropriate for measuring matrix approximation error?

- **Concept: Transformer architecture and weight matrix roles**
  - Why needed here: Understanding which weight matrices (WK, WQ, WV, WO, WU_p, WGate, WDown) serve which functions is critical for determining which types can benefit from cross-layer sharing
  - Quick check question: What are the roles of the different weight matrices in a transformer layer, and why would WDown be unsuitable for cross-layer sharing?

## Architecture Onboarding

- **Component map:** Input data preprocessing -> Weight matrix grouping -> SVD decomposition -> Model reconstruction -> Inference engine

- **Critical path:**
  1. Compute scaling matrix S from input samples
  2. Group weight matrices across layers horizontally
  3. Apply SVD to scaled concatenated matrices
  4. Select top k singular values/vectors based on target compression ratio
  5. Store basis vectors and layer-specific coefficients
  6. Modify inference to use shared basis vectors with layer-specific scaling

- **Design tradeoffs:**
  - Compression ratio vs. performance: Higher compression increases error and degrades accuracy
  - Number of layers grouped vs. error: More layers per group increases compression but may increase error
  - Types of weight matrices shared vs. functionality: Some matrices (like WDown) cannot be shared without significant performance loss

- **Failure signatures:**
  - Increased perplexity on generation tasks indicates compression error is too high
  - Decreased accuracy on reasoning tasks suggests important weight patterns were lost
  - Out-of-memory errors during SVD computation suggest too many layers were grouped together
  - Numerical instability in scaling matrix computation indicates poor input data statistics

- **First 3 experiments:**
  1. Test cross-layer sharing on a single weight matrix type (e.g., WK) with just 2 adjacent layers to verify basic mechanism works
  2. Compare Frobenius loss for sharing vs. individual SVD on different weight matrix types to identify which types benefit from sharing
  3. Evaluate model performance degradation at different compression ratios (20%, 30%, 40%) to establish performance-compression tradeoff curve

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the selection of input data (X) for computing the scaling matrix S affect the performance of Basis Sharing across different tasks and datasets?
- Basis in paper: [inferred] The paper mentions using 256 samples from WikiText-2 with 2048 sequence length to evaluate X, but does not explore alternative input datasets or their impact on performance.
- Why unresolved: The paper only uses WikiText-2 to compute S and briefly mentions using Alpaca for reasoning tasks, but does not provide systematic analysis of how different input datasets affect the scaling matrix and subsequent model performance.
- What evidence would resolve it: Experiments comparing performance using S computed from various datasets (e.g., different domains, sizes, or qualities of text) across multiple tasks would clarify the importance of input data selection.

### Open Question 2
- Question: What is the optimal strategy for grouping layers in Basis Sharing, and how does it vary with model architecture and task requirements?
- Basis in paper: [explicit] The paper explores grouping 2, 3, 4, 5, 6, 7, 8, 16, and 32 consecutive layers but does not provide definitive strategy for optimal grouping.
- Why unresolved: While the paper shows that grouping affects performance, it does not determine whether non-consecutive layer grouping or architecture-specific strategies could yield better results.
- What evidence would resolve it: Comprehensive study comparing different grouping strategies (including non-consecutive layers, task-specific grouping, and architecture-aware approaches) across various model architectures and tasks would identify optimal grouping methods.

### Open Question 3
- Question: How does Basis Sharing perform when applied to different types of transformer architectures beyond the decoder-only models tested?
- Basis in paper: [inferred] The paper focuses on LLaMA, LLaMA2, OPT, Mistral, and GPT-2, all of which are decoder-only transformer architectures.
- Why unresolved: The paper does not explore how Basis Sharing would perform on encoder-only models (like BERT), encoder-decoder models (like T5), or more specialized architectures.
- What evidence would resolve it: Applying Basis Sharing to a diverse set of transformer architectures (including encoder-only, encoder-decoder, and specialized variants) and comparing performance across these models would reveal the method's generalizability.

### Open Question 4
- Question: What are the limitations of Basis Sharing when applied to extremely large compression ratios (>50%)?
- Basis in paper: [explicit] The paper tests compression ratios up to 50% and shows performance degradation at higher ratios, but does not explore extreme limits of the method.
- Why unresolved: The paper does not investigate the point at which Basis Sharing becomes ineffective or the specific challenges encountered at very high compression ratios.
- What evidence would resolve it: Experiments pushing Basis Sharing to extreme compression ratios (>70-90%) while monitoring performance degradation, model stability, and specific failure modes would establish the method's limitations.

## Limitations
- Scalability uncertainty to very large models beyond 30B parameters and behavior under extreme compression ratios (>50%)
- Computational overhead during compression phase, particularly SVD computation on large concatenated matrices
- Assumes pretrained models have sufficient redundancy for effective cross-layer sharing, which may not hold for all architectures or training regimes

## Confidence
**High confidence**: The core mechanism of SVD-based cross-layer basis sharing is well-established mathematically, and empirical results showing performance improvements over existing methods (up to 25% PPL reduction) are convincing with extensive ablation studies.

**Medium confidence**: Hardware efficiency gains (1.57x throughput improvement) are promising but were measured on specific hardware configurations; generalization to different model families and robustness to various input distributions need further validation.

**Low confidence**: Theoretical bounds on compression error and their relationship to performance degradation are not established; method's behavior with non-standard transformer architectures is unknown.

## Next Checks
1. **Cross-Architecture Generalization Test**: Apply Basis Sharing to non-standard transformer variants (e.g., models with depth-wise separable convolutions, local attention, or other architectural modifications) to verify method's broader applicability beyond standard LLMs.

2. **Extreme Compression Analysis**: Systematically evaluate model performance and failure modes at compression ratios exceeding 50% (e.g., 60%, 70%, 80%) to identify practical limits of the approach and characterize degradation patterns.

3. **Compression Overhead Profiling**: Measure and analyze computational cost (time and memory) of SVD-based compression phase across different model scales, comparing it against inference-time savings to establish break-even point for when this method becomes beneficial.