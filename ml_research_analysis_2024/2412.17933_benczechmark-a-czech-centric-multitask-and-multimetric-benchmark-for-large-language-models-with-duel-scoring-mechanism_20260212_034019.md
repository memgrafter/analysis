---
ver: rpa2
title: 'BenCzechMark : A Czech-centric Multitask and Multimetric Benchmark for Large
  Language Models with Duel Scoring Mechanism'
arxiv_id: '2412.17933'
source_url: https://arxiv.org/abs/2412.17933
tags:
- czech
- language
- tasks
- dataset
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces BenCzechMark, the first comprehensive Czech
  language benchmark for large language models, addressing the gap in Czech-specific
  evaluation resources. It features 50 tasks across 8 categories, with 14 newly collected
  datasets, using a novel duel scoring mechanism based on statistical significance
  testing and social preference theory.
---

# BenCzechMark : A Czech-centric Multitask and Multimetric Benchmark for Large Language Models with Duel Scoring Mechanism

## Quick Facts
- arXiv ID: 2412.17933
- Source URL: https://arxiv.org/abs/2412.17933
- Reference count: 40
- Primary result: First comprehensive Czech language benchmark with duel scoring mechanism and 50 tasks across 8 categories

## Executive Summary
BenCzechMark addresses the lack of Czech-specific evaluation resources for large language models by introducing a comprehensive benchmark with 50 tasks across 8 categories. The benchmark employs a novel duel scoring mechanism based on statistical significance testing and social preference theory to aggregate results across heterogeneous tasks. It introduces both MMLU-like multiple-choice tasks and GLUE-like classification tasks, using a threshold-free AUROC metric to reduce miscalibration issues. The benchmark and leaderboard are publicly available at https://huggingface.co/spaces/CZLC/BenCzechMark.

## Method Summary
The benchmark uses a duel scoring mechanism where models are compared pairwise on each task using statistical tests (t-test for accuracy/EM, Bayesian sign-test for AUROC, bootstrapping for perplexity). The proportion of statistically significant wins defines the Duel Win Score (DWS), which aggregates to Category Win Score (CWS) and Overall Win Score (OWS). Tasks include multiple-choice formats (MCF), open-answer formats (OAF), classification formats (CF), and language modeling formats (LMF). The benchmark employs 5-8 prompts per task with smart truncation for long contexts and introduces max-centered variance (MCV) to measure prompt sensitivity.

## Key Results
- Multilingual models generally outperform the Czech-specific 7B model (csmpt7b) trained on the largest Czech corpus
- Notable sensitivity to prompt selection in some tasks, with MCV ranging from 0.11 to 0.38
- 14 newly collected datasets added to 36 adopted datasets across 8 categories
- Average dataset contamination of 14% detected, with removal of datasets exceeding 50% contamination

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The duel scoring mechanism with statistical significance testing mitigates chance improvements across many tasks and models.
- Mechanism: Pairwise comparisons using t-tests, Bayesian sign-tests, and bootstrapping determine statistically significant wins, aggregated into Duel Win Scores.
- Core assumption: Statistical significance at Î±=0.05 reliably distinguishes true improvements from random noise in multiple comparisons.
- Evidence anchors: [abstract], [section 4.3] - weak empirical validation of effectiveness.
- Break condition: Violated test assumptions (e.g., non-normal distributions) make significance results unreliable.

### Mechanism 2
- Claim: Using threshold-free AUROC metric reduces miscalibration sensitivity in classification tasks compared to accuracy/F1.
- Mechanism: AUROC evaluates ranking quality across all thresholds rather than relying on discrete decisions based on continuous scores.
- Core assumption: Miscalibration manifests primarily through suboptimal thresholds rather than systematic ranking errors.
- Evidence anchors: [section 4.1] - weak empirical comparison with calibrated accuracy.
- Break condition: Systematic ranking bias in models makes AUROC insufficient for calibration issues.

### Mechanism 3
- Claim: Mixing MMLU-like tasks with GLUE-like tasks captures different aspects of language model capability.
- Mechanism: MMLU-like tasks test broad factual knowledge via multiple-choice, while GLUE-like tasks test specific linguistic competencies via classification.
- Core assumption: School exam-style questions and traditional NLP tasks capture orthogonal capabilities.
- Evidence anchors: [abstract], [section 1] - weak quantitative evidence of orthogonality.
- Break condition: High correlation between task performances across categories diminishes mixing benefits.

## Foundational Learning

- Concept: Statistical significance testing (t-tests, sign tests, bootstrapping)
  - Why needed here: To determine whether observed performance differences between models are real or due to chance
  - Quick check question: What's the key difference between a paired t-test and a sign test when comparing two models' performance on the same task?

- Concept: Threshold-free evaluation metrics (AUROC)
  - Why needed here: To evaluate classification performance without being sensitive to model calibration issues
  - Quick check question: How does AUROC differ from accuracy when evaluating an uncalibrated classifier?

- Concept: Multiclass classification with one-vs-all strategy
  - Why needed here: To apply binary evaluation metrics (like AUROC) to multiclass classification tasks
  - Quick check question: In a 3-class problem, how many separate AUROC values would you compute using one-vs-all?

## Architecture Onboarding

- Component map: Dataset collection pipeline -> LMH evaluation harness -> Statistical testing module -> Duel scoring aggregator -> Leaderboard system -> Prompt management system
- Critical path: Model submission -> LMH evaluation -> Statistical significance testing -> Duel Win Score aggregation -> Leaderboard update
- Design tradeoffs:
  - Open-weight only evaluation vs. supporting API models (precision vs. accessibility)
  - Statistical significance testing vs. simpler averaging (complexity vs. robustness)
  - Threshold-free metrics vs. calibrated accuracy (simplicity vs. calibration independence)
- Failure signatures:
  - All models getting similar DWS scores -> potential statistical test issues or dataset contamination
  - Extreme sensitivity to prompt selection -> need for prompt engineering or task reformulation
  - High variance in MCV across tasks -> potential annotation quality issues or ambiguous task definitions
- First 3 experiments:
  1. Verify statistical test implementation by running pairwise comparisons on synthetic data with known differences
  2. Test AUROC computation by creating controlled classification scenarios with varying calibration levels
  3. Validate prompt selection mechanism by evaluating a known model across all prompts for a single task and checking consistency with expected behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold-free metric aggregation strategy for heterogeneous tasks in multilingual benchmarks?
- Basis in paper: [explicit] The paper discusses using AUROC for classification tasks and mentions that "fair and interpretable aggregation of different task-specific metrics remains an open problem"
- Why unresolved: The paper implements a specific aggregation method (DWS with significance testing) but acknowledges this is one approach among potentially better alternatives
- What evidence would resolve it: Comparative analysis of different aggregation strategies (AUROC averaging, Borda count variants, learned aggregation) across diverse multilingual benchmarks with controlled experiments

### Open Question 2
- Question: How does contamination from training data affect the validity of language-specific benchmarks like BenCzechMark?
- Basis in paper: [explicit] The paper performs contamination analysis showing 14% average contamination and removes datasets based on this criterion
- Why unresolved: The paper identifies contamination as an issue but doesn't fully explore its impact on model rankings or develop systematic mitigation strategies
- What evidence would resolve it: Longitudinal study tracking how contamination levels change over time and correlating contamination with model performance shifts across benchmark versions

### Open Question 3
- Question: What is the relationship between prompt sensitivity and model performance across different language families?
- Basis in paper: [explicit] The paper introduces max-centered variance (MCV) to measure prompt sensitivity and finds higher variance in "subjective" tasks
- Why unresolved: The paper observes patterns in prompt sensitivity but doesn't establish causal mechanisms or test whether these patterns generalize across language families
- What evidence would resolve it: Cross-linguistic study measuring MCV across multiple language-specific benchmarks while controlling for model architecture and training data composition

## Limitations

- Statistical significance testing assumes normal distributions without validation across all tasks
- Prompt selection mechanism introduces variability without establishing consistent optimal performance representation
- Contamination detection uses 50% threshold but impact on results is not fully specified

## Confidence

**High confidence**: The benchmark's overall structure and evaluation methodology are sound, with duel scoring providing principled aggregation across tasks.

**Medium confidence**: The claim that multilingual models outperform Czech-specific models is supported but sensitive to prompt selection, and mixing task types captures different capabilities without empirical validation.

**Low confidence**: The effectiveness of AUROC in reducing miscalibration sensitivity lacks empirical comparison, and statistical significance testing's ability to mitigate chance improvements is assumed rather than proven.

## Next Checks

1. **Statistical assumption validation**: Run Shapiro-Wilk normality tests on model performance scores across all tasks to verify t-test assumptions and implement non-parametric alternatives where normality fails.

2. **Prompt stability analysis**: Evaluate multiple models across all available prompts (not just selected ones) for a subset of tasks and compute pairwise correlations to reveal systematic bias in prompt selection.

3. **Cross-lingual performance comparison**: Evaluate the same models on both BenCzechBench and equivalent English benchmarks (like MMLU) to quantify whether performance gaps are Czech-specific or reflect general model quality differences.