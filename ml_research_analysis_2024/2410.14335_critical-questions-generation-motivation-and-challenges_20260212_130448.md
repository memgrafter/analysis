---
ver: rpa2
title: 'Critical Questions Generation: Motivation and Challenges'
arxiv_id: '2410.14335'
source_url: https://arxiv.org/abs/2410.14335
tags:
- questions
- argument
- critical
- argumentation
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes the task of Critical Questions Generation\
  \ (CQs Generation), where given an argumentative text, a model is tasked to generate\
  \ critical questions that question the argument\u2019s blind spots. The paper aims\
  \ to investigate two methods for creating a reference dataset for this task: (1)\
  \ instantiating Walton\u2019s argumentation theory templates, and (2) using LLMs\
  \ to generate CQs."
---

# Critical Questions Generation: Motivation and Challenges

## Quick Facts
- arXiv ID: 2410.14335
- Source URL: https://arxiv.org/abs/2410.14335
- Reference count: 16
- Primary result: This paper operationalizes the task of Critical Questions Generation (CQs Generation) and investigates two methods for creating reference datasets: Walton's argumentation theory templates and LLM-based generation.

## Executive Summary
This paper proposes Critical Questions Generation (CQs Generation) as a task where models generate critical questions that expose blind spots in argumentative texts. The authors investigate two complementary methods for creating reference datasets: instantiating Walton's argumentation theory templates and using LLMs to generate CQs. The work operationalizes what constitutes a valid CQ and demonstrates that while theory-based approaches focus on premise relationships, LLM-generated CQs often address evidence and definitions, suggesting these approaches are complementary. The paper concludes that LLMs are reasonable CQ generators but have room for improvement.

## Method Summary
The paper investigates two approaches for generating critical questions. First, theory-CQs are created by instantiating Walton's argumentation theory templates for 18 common argumentation schemes from US2016 and Moral Maze datasets. Second, llm-CQs are generated using Llama-2-13B and Zephyr-13B with two different prompts (with and without CQ definition), followed by a three-step filtering process for relevance, argument matching, and inferential validity. The generated questions are then categorized and analyzed to compare the two approaches.

## Key Results
- Theory-CQs focus on relations between premises while llm-CQs often ask about evidence and definitions
- LLMs are reasonable CQ generators but have room for improvement in validity and relevance
- The two approaches produce complementary types of critical questions, with theory-CQs being more structured and llm-CQs more varied in focus

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using Walton's argumentation theory templates ensures validity of generated CQs.
- Mechanism: Theory templates provide structured mappings between argument schemes and their associated critical questions, allowing automatic instantiation of valid questions by filling variable placeholders with text from premises and conclusions.
- Core assumption: Argumentation schemes in Walton et al. (2008) correctly identify all necessary critical questions for each scheme type.
- Break condition: If an argument scheme doesn't capture all valid CQs for a given argumentative structure, the template-based approach will miss valid questions.

### Mechanism 2
- Claim: LLMs can generate novel valid CQs not present in theory templates.
- Mechanism: LLMs process the argumentative text and generate questions that expose blind spots, potentially discovering new types of critical questions beyond the predefined templates.
- Core assumption: LLMs have sufficient reasoning capability to identify argumentative blind spots without external knowledge.
- Break condition: If LLMs cannot reliably distinguish between relevant and irrelevant questions, they will generate too much noise to be useful.

### Mechanism 3
- Claim: Filtering and validation process effectively identifies relevant and valid LLMs-generated CQs.
- Mechanism: Three-step filtering process (relevance to intervention, relation to annotated arguments, inferential validity) systematically eliminates irrelevant and invalid questions.
- Core assumption: Human evaluators can reliably distinguish relevant from irrelevant CQs and valid from invalid ones.
- Break condition: If human agreement on validity is low or inconsistent, the filtering process cannot reliably identify valid CQs.

## Foundational Learning

- Concept: Argumentation schemes and critical questions
  - Why needed here: Understanding how arguments are structured and what blind spots they contain is fundamental to generating relevant critical questions
  - Quick check question: Can you explain the difference between a premise, conclusion, and critical question in Walton's framework?

- Concept: Defeasible reasoning
  - Why needed here: Critical questions are tools for testing defeasible arguments - understanding this concept is crucial for generating questions that properly challenge assumptions
  - Quick check question: Why are arguments in everyday discourse considered defeasible rather than deductive?

- Concept: Prompt engineering for LLMs
  - Why needed here: The effectiveness of LLM-generated CQs depends heavily on how the model is prompted and what context is provided
  - Quick check question: How would you modify a prompt to encourage an LLM to generate more specific versus more general critical questions?

## Architecture Onboarding

- Component map: Data preprocessing -> Theory generation -> LLM generation -> Filtering pipeline -> Analysis
- Critical path: Theory generation → LLM generation → Filtering → Analysis
- Design tradeoffs:
  - Manual annotation vs. automation: Theory generation requires significant manual effort but ensures validity
  - Model selection: Different LLMs have varying performance on critical question generation
  - Filtering strictness: Stricter filters reduce noise but may discard valid questions
- Failure signatures:
  - High percentage of irrelevant LLM questions indicates prompt engineering issues
  - Low human agreement on validity suggests unclear criteria
  - Theory generation produces few questions for certain schemes indicates template gaps
- First 3 experiments:
  1. Test different prompt formulations on a small sample to optimize LLM output
  2. Run the filtering pipeline on a subset of LLM questions to measure effectiveness
  3. Compare theory-generated and LLM-generated CQs for a single intervention to identify complementary patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on generating critical questions for argumentation schemes not included in the 18 most frequent ones studied in this paper?
- Basis in paper: The paper focused on 18 most frequent argumentation schemes and acknowledged that reference data creation relies heavily on already annotated data with argumentation schemes.
- Why unresolved: The study was limited to 18 schemes due to data availability, leaving open the question of LLM performance on other schemes.
- What evidence would resolve it: Testing LLM performance on generating critical questions for a broader range of argumentation schemes, including less frequent ones.

### Open Question 2
- Question: Can the quality of LLM-generated critical questions be improved through fine-tuning on specifically curated datasets?
- Basis in paper: The paper concludes that LLMs have room for improvement in generating valid critical questions and suggests using more advanced training and prompting techniques.
- Why unresolved: While the paper suggests potential improvements, it does not explore the impact of fine-tuning on LLM performance for this specific task.
- What evidence would resolve it: Experiments comparing the performance of fine-tuned LLMs versus base models in generating critical questions, using metrics like relevance and validity.

### Open Question 3
- Question: How do different prompting strategies affect the quality and relevance of LLM-generated critical questions?
- Basis in paper: The paper experimented with two different prompts (one with query only and one with query plus definition) and observed differences in the types of issues encountered.
- Why unresolved: The study used a limited set of prompts, and it's unclear how other prompting strategies might impact the quality of generated questions.
- What evidence would resolve it: Systematic comparison of various prompting strategies (e.g., chain-of-thought prompting, few-shot examples) on the quality and relevance of LLM-generated critical questions.

## Limitations
- The paper lacks inter-annotator agreement scores for the critical human evaluation step of validity assessment.
- The filtering criteria for LLM-generated questions are described as "guidelines" without specific operationalization.
- The paper lacks quantitative metrics comparing theory-CQs and llm-CQs across multiple dimensions beyond simple categorization.

## Confidence
- **High confidence**: The paper successfully operationalizes the CQs generation task and provides a clear framework for both theory-based and LLM-based approaches.
- **Medium confidence**: The conclusion that LLMs are "reasonable" CQ generators is supported by the evidence, but lacks quantitative evaluation metrics.
- **Low confidence**: The claim that theory-CQs and llm-CQs are complementary is based on qualitative categorization rather than systematic analysis.

## Next Checks
1. Calculate and report inter-annotator agreement scores for the validity assessment of both theory-CQs and llm-CQs.
2. Implement quantitative metrics (precision, recall, F1-score) to compare the performance of theory-based and LLM-based generation approaches.
3. Systematically test different prompt formulations and model parameters to optimize LLM performance.