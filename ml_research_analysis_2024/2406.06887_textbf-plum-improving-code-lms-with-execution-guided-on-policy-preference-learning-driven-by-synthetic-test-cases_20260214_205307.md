---
ver: rpa2
title: '$\textbf{PLUM}$: Improving Code LMs with Execution-Guided On-Policy Preference
  Learning Driven By Synthetic Test Cases'
arxiv_id: '2406.06887'
source_url: https://arxiv.org/abs/2406.06887
tags:
- plum
- code
- preference
- test
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces PLUM, a preference learning framework that
  leverages automatically generated test cases to train code language models. PLUM
  operates in three stages: generating test cases from natural language instructions,
  sampling candidate solutions from the policy model and evaluating them against the
  test cases, and using the preference data to train the policy with preference learning
  algorithms like KTO.'
---

# $\textbf{PLUM}$: Improving Code LMs with Execution-Guided On-Policy Preference Learning Driven By Synthetic Test Cases

## Quick Facts
- **arXiv ID**: 2406.06887
- **Source URL**: https://arxiv.org/abs/2406.06887
- **Reference count**: 40
- **Primary result**: PLUM improves code language model performance on benchmarks like HumanEval and MBPP by up to 4.8% on average, with gains up to 11.8% on LiveCodeBench.

## Executive Summary
PLUM introduces a novel preference learning framework for code language models that leverages automatically generated test cases to improve code generation quality. The method operates through a three-stage process: generating synthetic test cases from natural language instructions, evaluating candidate solutions against these tests, and training the policy model using preference learning algorithms. Experimental results demonstrate consistent performance improvements across multiple model families on standard code generation benchmarks, with particularly notable gains on more challenging datasets. The approach complements traditional supervised fine-tuning and shows promise for enhancing state-of-the-art code models.

## Method Summary
PLUM operates through a three-stage pipeline: first, it generates synthetic test cases from natural language instructions using test generation models; second, it samples candidate solutions from the policy model and evaluates them against these tests to identify preferred solutions; and third, it uses the preference data to train the policy model with preference learning algorithms like KTO. This execution-guided approach leverages the test cases as a proxy for code quality, allowing the model to learn from its own outputs in a self-improving loop. The method is designed to be model-agnostic and can be applied to various code language models, including both smaller specialized models and larger general-purpose models.

## Key Results
- PLUM achieves up to 4.8% improvement on HumanEval and MBPP benchmarks across different model families
- The method shows up to 11.8% gains on the more challenging LiveCodeBench dataset
- PLUM demonstrates consistent performance improvements when applied to state-of-the-art models like CodeQwen-1.5-7B-Chat

## Why This Works (Mechanism)
PLUM works by creating a feedback loop where the model learns from its own outputs through the lens of automatically generated test cases. The synthetic test generation provides a scalable way to create ground truth quality signals without manual annotation, while the preference learning algorithm uses these signals to update the model's behavior. By focusing on execution-based preferences rather than just syntactic correctness, PLUM captures functional correctness as the primary learning signal, which is more aligned with actual programming requirements than traditional training objectives.

## Foundational Learning
- **Preference Learning**: Learning to rank or select between different outputs based on quality signals - needed to distinguish between good and bad code solutions
- **Synthetic Data Generation**: Creating artificial training examples programmatically - needed to scale preference signal creation without manual annotation
- **Execution-Based Evaluation**: Running generated code against test cases to assess correctness - needed to capture functional correctness beyond static analysis
- **On-Policy Learning**: Using the model's own outputs as training data - needed to create a self-improving learning loop
- **Test Case Generation from NL**: Converting natural language instructions into executable tests - needed to create relevant evaluation criteria for code generation tasks

## Architecture Onboarding

**Component Map**: Natural Language Instruction -> Test Generation Model -> Synthetic Test Cases -> Code LM Sampling -> Test Execution -> Preference Data -> Preference Learning Algorithm -> Updated Code LM

**Critical Path**: The most time-consuming component is test generation, which can be parallelized across multiple instructions. Test execution is also computationally intensive but benefits from efficient batching and parallel processing.

**Design Tradeoffs**: The framework trades computational cost (generating and executing many test cases) for improved code quality. The synthetic test generation quality directly impacts learning effectiveness, creating a dependency on test generation model capabilities.

**Failure Signatures**: Poor performance may result from inadequate test generation quality, insufficient candidate sampling, or preference learning algorithm instability. Model degradation can occur if the preference signals are noisy or misaligned with actual code quality.

**First Experiments**:
1. Ablation study removing test generation to measure baseline preference learning effectiveness
2. Scaling analysis of candidate sampling count versus performance gains
3. Cross-benchmark validation to assess generalization across different programming tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic test generation may not capture the full complexity of real-world code scenarios, potentially limiting generalizability
- The preference learning algorithm's effectiveness is constrained by the quality of synthetic tests, creating a potential performance ceiling
- The comparison with supervised fine-tuning lacks deep analysis of specific failure modes and optimal mixing strategies between approaches

## Confidence
- **Medium**: Standard benchmark results (HumanEval, MBPP) given established datasets and consistent results across model families
- **Low**: LiveCodeBench results due to benchmark complexity and potential disproportionate impact from synthetic test generation approach

## Next Checks
1. Conduct ablation studies isolating the contribution of synthetic test generation quality versus preference learning algorithm effectiveness
2. Evaluate model performance on production codebases or real-world programming tasks to assess practical applicability beyond synthetic benchmarks
3. Investigate the behavior of PLUM-trained models on adversarial or corner-case programming problems that may expose limitations in the synthetic test generation approach