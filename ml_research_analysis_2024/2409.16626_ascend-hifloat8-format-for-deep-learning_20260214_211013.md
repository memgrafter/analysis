---
ver: rpa2
title: Ascend HiFloat8 Format for Deep Learning
arxiv_id: '2409.16626'
source_url: https://arxiv.org/abs/2409.16626
tags:
- hif8
- training
- fp16
- rounding
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HiFloat8 (HiF8), an 8-bit floating-point data
  format for deep learning. HiF8 features tapered precision, providing 7 exponent
  values with 3-bit mantissa, 8 exponent values with 2-bit mantissa, and 16 exponent
  values with 1-bit mantissa.
---

# Ascend HiFloat8 Format for Deep Learning

## Quick Facts
- arXiv ID: 2409.16626
- Source URL: https://arxiv.org/abs/2409.16626
- Authors: Yuanyong Luo, Zhongxing Zhang, Richard Wu, Hu Liu, Ying Jin, Kai Zheng, Minmin Wang, Zhanying He, Guipeng Hu, Luyao Chen, Tianchi Hu, Junsong Wang, Minqi Chen, Mikhaylov Dmitry, Korviakov Vladimir, Bobrin Maxim, Yuhao Hu, Guanfu Chen, Zeyi Huang
- Reference count: 40
- Key outcome: Proposes HiFloat8 (HiF8), an 8-bit floating-point data format for deep learning with tapered precision that achieves better balance between precision and dynamic range compared to existing 8-bit formats

## Executive Summary
This paper introduces HiFloat8 (HiF8), a novel 8-bit floating-point format specifically designed for deep learning applications. The format features tapered precision where different exponent values are paired with varying mantissa bit widths - 7 exponent values use 3-bit mantissa, 8 exponent values use 2-bit mantissa, and 16 exponent values use 1-bit mantissa. This design extends the dynamic range by 7 extra powers of 2 for denormal value encoding while maintaining computational efficiency. The format supports all special values except positive and negative zero are represented by a single bit pattern. Through extensive simulations across various neural networks including traditional networks and large language models, the authors demonstrate that HiF8 performs well in both training and inference scenarios.

## Method Summary
HiF8 is designed with a tapered precision approach where the bit allocation varies based on the exponent value. The format provides 7 exponent values with 3-bit mantissa, 8 exponent values with 2-bit mantissa, and 16 exponent values with 1-bit mantissa. This structure allows for extended dynamic range through 7 extra powers of 2 for denormal value encoding. The format maintains support for all special values while consolidating positive and negative zero into a single representation. The design aims to optimize the trade-off between precision and dynamic range specifically for deep learning workloads. The authors validated their approach through massive simulation experiments on various neural network architectures, including both traditional networks and large language models, to demonstrate effectiveness in both training and inference contexts.

## Key Results
- HiF8 achieves better balance between precision and dynamic range compared to existing 8-bit floating-point formats
- The format performs well across both traditional neural networks and large language models in training and inference scenarios
- Tapered precision design with variable mantissa allocation (3-bit, 2-bit, and 1-bit) provides optimal performance for deep learning workloads

## Why This Works (Mechanism)
HiF8 works by implementing a tapered precision approach that allocates mantissa bits based on the magnitude of the value being represented. For smaller exponent values (closer to zero), more mantissa bits are allocated (3 bits), providing higher precision where it's most needed for accurate gradient calculations and weight updates. For larger exponent values, fewer mantissa bits are used (2 or 1 bit), sacrificing some precision to extend the dynamic range. This design recognizes that deep learning workloads have different precision requirements at different scales - small values need more precision for accurate training, while large values benefit more from extended range. The 7 extra powers of 2 for denormal encoding further extends the range without significantly impacting precision where it matters most. By consolidating positive and negative zero into a single representation, the format saves one bit that can be used more effectively elsewhere in the encoding scheme.

## Foundational Learning
- **Tapered precision**: A technique where numerical precision varies based on magnitude, allocating more bits to smaller values and fewer bits to larger values. Needed because deep learning workloads have different precision requirements at different scales. Quick check: Verify that the format allocates more mantissa bits to smaller exponent values.
- **Denormal number encoding**: A method to represent very small numbers that would otherwise underflow to zero. Essential for maintaining numerical stability in deep learning calculations involving tiny gradients or weights. Quick check: Confirm the format extends dynamic range by 7 extra powers of 2 for denormal values.
- **8-bit floating-point formats**: Compressed numerical representations that use 8 bits total to encode floating-point numbers. Critical for reducing memory bandwidth and storage requirements in deep learning systems. Quick check: Ensure the total bit count sums to 8 bits including sign, exponent, and mantissa.
- **Special value handling**: The representation of exceptional cases like infinity, NaN, and zero in floating-point formats. Important for maintaining numerical robustness in deep learning computations. Quick check: Verify all special values are supported with appropriate bit patterns.
- **Dynamic range vs precision trade-off**: The fundamental compromise between representing very large or very small numbers (range) and the accuracy of representation (precision). Central to designing effective numerical formats for deep learning. Quick check: Evaluate whether the format achieves an optimal balance for neural network workloads.
- **Neural network quantization**: The process of reducing numerical precision in neural networks to improve efficiency. Directly relevant as HiF8 is designed as a quantization format for deep learning. Quick check: Confirm the format maintains sufficient precision for effective training and inference.

## Architecture Onboarding

**Component map**: Input values -> HiF8 encoder -> Neural network computation -> HiF8 decoder -> Output values

**Critical path**: The critical path involves converting input data to HiF8 format, performing all neural network computations in HiF8 precision, and converting results back to standard floating-point or using them directly. This path must maintain numerical stability while providing the efficiency benefits of reduced precision.

**Design tradeoffs**: The primary tradeoff is between precision and dynamic range, with HiF8 favoring extended range through tapered precision. The consolidation of positive and negative zero saves one bit but may affect certain numerical algorithms that distinguish between signed zeros. The variable mantissa allocation adds complexity to hardware implementation compared to uniform precision formats but provides better optimization for deep learning workloads.

**Failure signatures**: Potential failures include gradient underflow or overflow during training, loss of critical precision in weight updates, and numerical instability in activation functions. The tapered precision design should minimize these risks by maintaining higher precision where needed while extending range where beneficial.

**3 first experiments**:
1. Benchmark HiF8 against existing 8-bit formats on a standard CNN architecture (e.g., ResNet) for both training and inference to quantify performance differences
2. Test HiF8 on transformer-based language models to evaluate its effectiveness on modern deep learning architectures
3. Analyze the impact of HiF8 on specific operations like matrix multiplication and convolution to identify any bottlenecks or precision issues

## Open Questions the Paper Calls Out
None

## Limitations
- The paper does not provide details on practical hardware implementation of HiF8, leaving uncertainty about real-world performance and integration challenges
- There is no comparison with more recent 8-bit formats that may have been developed since the publication, limiting the context of HiF8's relative performance
- The discussion on computational efficiency and memory usage trade-offs compared to other formats is limited, making it difficult to assess practical benefits

## Confidence

**Format design confidence**: High - The paper provides clear specifications for bit allocation and special value handling with a well-reasoned tapered precision approach

**Simulation results confidence**: High - The extensive simulations across various neural networks and LLMs appear robust and well-documented

**Practical implementation confidence**: Medium - While the format design is solid, the paper lacks details on hardware/software implementation and real-world integration

**Comparative analysis confidence**: Medium - The claims of better balance compared to existing formats are supported by presented data, but lack comparisons with more recent developments

## Next Checks

1. Investigate practical implementation of HiF8 in hardware and software, comparing computational efficiency and memory usage with existing 8-bit formats through benchmarking on real deep learning systems

2. Conduct a comprehensive literature review to identify and compare HiF8 with any more recent 8-bit formats developed since publication, evaluating relative performance and trade-offs

3. Collaborate with hardware manufacturers to prototype HiF8 implementation in deep learning accelerators and assess impact on system performance, power consumption, and training/inference accuracy across diverse workloads