---
ver: rpa2
title: 'CoST: Contrastive Quantization based Semantic Tokenization for Generative
  Recommendation'
arxiv_id: '2404.14774'
source_url: https://arxiv.org/abs/2404.14774
tags:
- item
- semantic
- retrieval
- code
- codes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of generative retrieval methods
  in recommender systems that rely solely on semantic information from item textual
  descriptions, neglecting item relationships crucial for recommendation modeling.
  The authors propose a contrastive quantization-based semantic tokenization approach
  (CoST) that simultaneously considers item relationships and semantic information
  to learn semantic tokens.
---

# CoST: Contrastive Quantization based Semantic Tokenization for Generative Recommendation

## Quick Facts
- arXiv ID: 2404.14774
- Source URL: https://arxiv.org/abs/2404.14774
- Reference count: 25
- Key result: Up to 43% improvement in Recall@5 and 44% in NDCG@5 compared to baselines

## Executive Summary
This paper addresses limitations in generative retrieval methods for recommender systems that rely solely on semantic information from item textual descriptions, neglecting crucial item relationships. The authors propose CoST, a contrastive quantization-based semantic tokenization approach that simultaneously considers item relationships and semantic information to learn more effective semantic tokens. By employing contrastive learning objectives within a residual quantization framework, CoST enhances item discrepancy across all items and better preserves item neighborhoods. Experimental results on MIND and Office datasets demonstrate significant performance improvements over previous baselines.

## Method Summary
CoST extracts item textual descriptions using a pre-trained language model (Sentence-T5/BERT) and translates them into embeddings. These embeddings are then enhanced through an RQ-VAE model with contrastive objectives that treat decoder-generated embeddings from the same sample as positive instances and those from other samples as negative instances. The method uses multi-level residual quantization with separate codebooks at different granularities, creating a tuple of codewords that approximate the input. The resulting semantic codes are applied to downstream sequence recommendation using a seq2seq transformer model.

## Key Results
- Achieves up to 43% improvement in Recall@5 compared to previous baselines
- Demonstrates up to 44% improvement in NDCG@5 on MIND and Office datasets
- Shows that contrastive quantization effectively enhances item relationships while preserving semantic information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning improves semantic tokenization by distinguishing between positive and negative item embeddings, preserving item relationships in recommendation systems.
- Mechanism: The model treats embeddings generated from the decoder for the same sample as positive instances and embeddings from other samples as negative instances. This forces the model to enhance item discrepancy across all items, better preserving item neighborhoods and relationships.
- Core assumption: Item relationships are crucial for recommendation modeling and can be effectively captured through contrastive learning between item embeddings.
- Evidence anchors:
  - [abstract]: "The method treats embeddings generated by the decoder from the samples themselves as positive instances and those from other samples as negative instances, effectively enhancing item discrepancy across all items and better preserving item neighborhoods."
  - [section]: "In this approach, we consider embeddings generated from the decoder as positive samples, while other generated samples serve as negative samples."
  - [corpus]: "Found 25 related papers (using 8). Average neighbor FMR=0.44, average citations=0.0." - Weak evidence of related work on contrastive approaches.
- Break condition: If the contrastive learning objective fails to differentiate between semantically similar items that should be considered neighbors, or if the negative sampling strategy becomes ineffective in high-dimensional embedding spaces.

### Mechanism 2
- Claim: Combining semantic information from item textual descriptions with item relationships creates more effective semantic tokens for generative recommendation.
- Mechanism: The approach uses a pre-trained language model to extract item textual descriptions and translate them into embeddings, then enhances the RQ-VAE model with contrastive objectives that consider both semantic content and item relationships.
- Core assumption: Item relationships can be captured through embedding space geometry and should be preserved alongside semantic content for effective recommendation.
- Evidence anchors:
  - [abstract]: "The authors propose a contrastive quantization-based semantic tokenization approach (CoST) that simultaneously considers item relationships and semantic information to learn semantic tokens."
  - [section]: "We argue that the construction of item codes in recommendation systems should not solely rely on the semantic information embedded in their textual descriptions. Instead, we propose that the relationships between items, in conjunction with semantic information, can collectively influence the quality of item codes."
  - [corpus]: Weak evidence - no direct corpus support for combining semantic and relational information in tokenization.
- Break condition: If the semantic information from textual descriptions becomes too dominant and overshadows the relationship signals, or if the relationship information is too sparse to provide meaningful contrastive signals.

### Mechanism 3
- Claim: Residual quantization in RQ-VAE allows for multi-granular representation of items while maintaining reconstruction quality.
- Mechanism: The RQ-VAE model uses multiple codebooks at different levels to quantize residuals, creating a tuple of codewords that approximate the input with increasing granularity rather than using a single magnified codebook.
- Core assumption: Multi-level quantization with separate codebooks at different granularities can better capture item characteristics than single-level quantization.
- Evidence anchors:
  - [section]: "This recursive approach approximates the input with increasing granularity. In contrast to using a single codebook magnified m times, we employ distinct codebooks of size K for each of the m levels."
  - [section]: "After obtaining the semantic code (c0, ..., cm-1), we sum up the quantized representation of the selected z values and create z as z := Σmd=0 eci."
  - [corpus]: No direct corpus evidence for the specific RQ-VAE approach described.
- Break condition: If the multi-level quantization becomes too computationally expensive relative to the performance gains, or if codebook collapse occurs despite initialization strategies.

## Foundational Learning

- Concept: Vector quantization and codebook-based representation
  - Why needed here: The entire approach relies on converting continuous embeddings into discrete codes through quantization, which is fundamental to the generative retrieval paradigm.
  - Quick check question: What is the primary difference between VQ-VAE and RQ-VAE in terms of how they handle quantization?

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: The contrastive objectives use InfoNCE loss to create positive and negative pairs for learning item relationships.
  - Quick check question: How does the temperature parameter τ in InfoNCE loss affect the concentration of learned representations?

- Concept: Autoregressive generation and sequence-to-sequence modeling
  - Why needed here: The generative retrieval system uses transformer-based seq2seq models to predict the next item's code based on previous items.
  - Quick check question: What is the key architectural difference between a standard transformer and a transformer used for autoregressive generation?

## Architecture Onboarding

- Component map: PLM (Sentence-T5/BERT) → Encoder for text to embeddings → RQ-VAE model → Multi-stage quantization with contrastive objectives → Transformer Decoder → Autoregressive generation of item codes → Codebook tables → Discrete code lookup for item retrieval → Seq2seq transformer → Downstream recommendation model training

- Critical path: Text description → PLM embeddings → RQ-VAE quantization → Codebook lookup → Transformer generation → Recommendation output

- Design tradeoffs:
  - Codebook size vs. granularity: Larger codebooks provide finer distinctions but increase memory requirements
  - Number of quantization levels vs. computational cost: More levels capture more detail but require more computation
  - Contrastive temperature vs. representation quality: Lower temperatures create sharper distinctions but may lead to overfitting

- Failure signatures:
  - Low cosine similarity between input and reconstructed embeddings indicates poor semantic preservation
  - High collision rate where different items map to the same code suggests inadequate codebook size
  - Poor downstream recommendation performance despite good reconstruction suggests the code doesn't capture relevant relationships

- First 3 experiments:
  1. Test codebook initialization with k-means clustering on a small subset of items to verify no collisions occur
  2. Measure cosine similarity and Top-K precision on validation set to evaluate quantization quality
  3. Compare NDCG@5 and Recall@5 on MIND dataset against baseline RQ-VAE without contrastive objectives

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CoST scale with increasing codebook sizes beyond those tested in the experiments?
- Basis in paper: [inferred] The paper shows performance improvement with increasing codebook size but does not test the limits of this scalability.
- Why unresolved: The experimental results only test up to CS=128 codebook size, leaving open questions about performance at larger scales.
- What evidence would resolve it: Conducting experiments with codebook sizes significantly larger than CS=128 would show if the performance gains continue or plateau.

### Open Question 2
- Question: How does CoST perform on datasets with significantly different characteristics from MIND and Office datasets, such as those with very long or very short item sequences?
- Basis in paper: [explicit] The paper mentions that the datasets cover a wide range of item proportions but does not explore performance on datasets with extreme sequence lengths.
- Why unresolved: The experimental datasets have moderate sequence lengths, and the paper does not provide evidence of CoST's effectiveness on datasets with very different sequence characteristics.
- What evidence would resolve it: Testing CoST on datasets with extreme sequence lengths (both very long and very short) would demonstrate its robustness and adaptability.

### Open Question 3
- Question: What is the impact of using different pre-trained language models (PLMs) on the performance of CoST?
- Basis in paper: [explicit] The paper uses Sentence-T5 and BERT as PLMs but does not explore the impact of using different PLMs on the final performance.
- Why unresolved: The choice of PLM could significantly affect the quality of embeddings and, consequently, the performance of CoST, but this aspect is not investigated in the paper.
- What evidence would resolve it: Comparing the performance of CoST using different PLMs (e.g., Sentence-BERT, RoBERTa) would reveal the sensitivity of CoST to the choice of PLM.

## Limitations

- Evaluation lacks ablation studies isolating the contribution of the contrastive component versus other architectural choices
- Computational complexity of multi-stage quantization is not discussed, raising questions about scalability
- No analysis of how well the learned semantic tokens generalize across different domains or sensitivity to hyperparameter choices

## Confidence

**High Confidence:** The basic mechanism of using contrastive learning to improve item differentiation is well-supported by the experimental results showing improved recall and NDCG metrics compared to baselines.

**Medium Confidence:** The claim that combining semantic information with item relationships creates more effective semantic tokens is supported by performance improvements but lacks detailed ablation studies to isolate the specific contribution of each component.

**Low Confidence:** The scalability and generalization claims are not adequately supported. The paper does not provide evidence that the approach works well on larger datasets beyond the tested MIND and Office datasets.

## Next Checks

1. **Ablation Study Validation:** Conduct experiments comparing CoST against variants that include only semantic reconstruction, only contrastive objectives, and different combinations of codebook sizes and quantization levels to isolate the contribution of each component to the observed performance gains.

2. **Scalability Analysis:** Test the approach on a significantly larger item catalog (e.g., 1M+ items) to measure memory requirements, training time, and inference latency, comparing against the reported performance on the MIND and Office datasets to validate scalability claims.

3. **Embedding Space Analysis:** Visualize and quantify the learned item embedding space using techniques like t-SNE or UMAP to verify that contrastive learning actually preserves item neighborhoods and relationships as claimed, measuring nearest neighbor preservation and cluster quality metrics.