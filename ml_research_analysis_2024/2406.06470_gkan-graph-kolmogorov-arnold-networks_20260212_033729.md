---
ver: rpa2
title: 'GKAN: Graph Kolmogorov-Arnold Networks'
arxiv_id: '2406.06470'
source_url: https://arxiv.org/abs/2406.06470
tags:
- gkan
- architecture
- graph
- accuracy
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Graph Kolmogorov-Arnold Networks (GKAN),
  a novel neural network architecture that extends the principles of Kolmogorov-Arnold
  Networks (KAN) to graph-structured data. GKAN incorporates learnable univariate
  functions instead of fixed linear weights, enabling more dynamic feature propagation
  across graph structures.
---

# GKAN: Graph Kolmogorov-Arnold Networks

## Quick Facts
- arXiv ID: 2406.06470
- Source URL: https://arxiv.org/abs/2406.06470
- Authors: Mehrdad Kiamari; Mohammad Kiamari; Bhaskar Krishnamachari
- Reference count: 10
- Primary result: GKAN outperforms GCN on Cora dataset, achieving 67.66% accuracy vs 61.24% with 200 features

## Executive Summary
This paper introduces Graph Kolmogorov-Arnold Networks (GKAN), a novel neural network architecture that extends KAN principles to graph-structured data. GKAN incorporates learnable univariate functions instead of fixed linear weights, enabling more dynamic feature propagation across graph structures. The work presents two architectures: one applying learnable functions after aggregation and another before aggregation. Experimental results on the Cora dataset demonstrate performance improvements over traditional Graph Convolutional Networks, with accuracy gains of up to 6.42 percentage points.

## Method Summary
GKAN extends Kolmogorov-Arnold Networks to graph-structured data by replacing linear weight matrices with learnable univariate functions. The architecture introduces flexibility in feature propagation by applying these functions at different stages of the aggregation process. Two distinct approaches are proposed: architecture 1 applies learnable functions after aggregation, while architecture 2 applies them before aggregation. This design enables dynamic adaptation to graph structure and potentially captures more complex relationships than traditional linear transformations.

## Key Results
- On Cora dataset with 100 features: GKAN achieves 61.76% accuracy vs GCN's 53.5%
- On Cora dataset with 200 features: GKAN achieves 67.66% accuracy vs GCN's 61.24%
- Architecture 2 (functions before aggregation) generally outperforms Architecture 1

## Why This Works (Mechanism)
The mechanism leverages the flexibility of learnable univariate functions to adapt feature propagation based on local graph structure, rather than relying on fixed linear transformations. This allows the network to capture non-linear relationships more effectively while maintaining computational efficiency.

## Foundational Learning
- **Graph Convolutional Networks**: Standard GNN architecture using linear weight matrices - needed to understand baseline performance and contrast with GKAN's approach
- **Kolmogorov-Arnold Networks**: Neural networks using learnable univariate functions - needed to understand the extension to graph domains
- **Semi-supervised graph learning**: Task of classifying nodes with limited labeled data - needed to contextualize Cora dataset experiments

## Architecture Onboarding
- **Component map**: Input features -> Learnable functions -> Aggregation -> Output layer
- **Critical path**: Feature transformation through learnable functions -> Neighborhood aggregation -> Classification
- **Design tradeoffs**: Flexibility vs computational cost; pre-aggregation vs post-aggregation function application
- **Failure signatures**: Poor performance with small feature sets; sensitivity to grid size and polynomial degree choices
- **3 first experiments**: 1) Compare architectures 1 and 2 on Cora dataset; 2) Test different grid sizes (10, 20, 50); 3) Evaluate polynomial degrees (1, 2, 3)

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single dataset (Cora) and one task type
- Performance comparisons only against GCN baseline
- Lack of detailed hyperparameter tuning procedures and statistical significance testing

## Confidence
- GKAN's architectural innovation (High confidence): Mathematical formulation appears sound
- Cora dataset performance improvements (Medium confidence): Results show gains but single-dataset evaluation limits generalizability
- Parameter sensitivity findings (Low confidence): Need broader validation across multiple datasets

## Next Checks
1. Evaluate GKAN across multiple graph datasets (Citeseer, Pubmed, PPI) and task types (node classification, link prediction, graph classification)
2. Conduct ablation studies comparing both GKAN architectures against recent KAN-GCN hybrid approaches and other state-of-the-art GNN variants
3. Perform rigorous hyperparameter optimization and statistical significance testing to validate observed performance differences