---
ver: rpa2
title: Cooperation and Fairness in Multi-Agent Reinforcement Learning
arxiv_id: '2410.14916'
source_url: https://arxiv.org/abs/2410.14916
tags:
- agents
- fairness
- goal
- agent
- distance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of fairness in multi-agent reinforcement
  learning (MARL) for navigation tasks, where agents must reach assigned goals while
  avoiding collisions. The authors propose a method that combines min-max fair distance
  goal assignments with a fairness reward term in the training reward function.
---

# Cooperation and Fairness in Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2410.14916
- Source URL: https://arxiv.org/abs/2410.14916
- Authors: Jasmine Jerry Aloor; Siddharth Nayak; Sydney Dolan; Hamsa Balakrishnan
- Reference count: 40
- Key outcome: Combines min-max fair distance goal assignments with fairness reward term to improve fairness without sacrificing efficiency in multi-agent navigation

## Executive Summary
This paper addresses fairness in multi-agent reinforcement learning (MARL) for navigation tasks where multiple agents must reach assigned goals while avoiding collisions. The authors propose a method that combines min-max fair distance goal assignments with a fairness reward term in the training reward function. Their approach enables agents to learn fair behavior without significantly sacrificing efficiency. Experiments demonstrate improvements in both efficiency and fairness compared to baseline methods, with the approach extending to formation-based tasks and showing scalability to different numbers of agents and formation shapes without retraining.

## Method Summary
The method combines min-max fair distance goal assignments with a fairness reward term during centralized training of MARL agents. Agents use Graph Neural Networks (GNNs) to process local observations and neighborhood information, learning policies that balance efficiency and fairness. The fairness metric is defined as the inverse of the coefficient of variation of travel distances across agents. During execution, agents operate in a decentralized manner, selecting goals based on learned policies and local observations without centralized coordination.

## Key Results
- Achieves 14% improvement in efficiency and 5% improvement in fairness compared to random assignment baseline
- Shows 21% improvement in fairness with only 7% decrease in efficiency compared to optimal assignment model
- Demonstrates scalability to different numbers of agents and formation shapes without retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fair behavior emerges through min-max fair distance goal assignments combined with a fairness reward term.
- Mechanism: The min-max fair assignment reduces the worst-case distance any agent must travel, while the fairness reward term encourages agents to maintain equitable travel distances throughout the episode.
- Core assumption: Agents can learn to balance fairness and efficiency through appropriate reward shaping without centralized coordination.
- Evidence anchors:
  - [abstract] "by training agents using min-max fair distance goal assignments along with a reward term that incentivizes fairness"
  - [section] "We determine a min-max fair assignment by optimizing the objective min ð‘§ subject to constraints, ensuring each agent and goal is assigned exactly once"
  - [corpus] "Fair Dynamic Spectrum Access via Fully Decentralized Multi-Agent Reinforcement Learning" (weak relevance)
- Break condition: If the fairness metric becomes too dominant in the reward function, agents may prioritize fairness over task completion.

### Mechanism 2
- Claim: Decentralized learning allows agents to adaptively select goals during execution without centralized coordination.
- Mechanism: Agents use local observations and learned policies to navigate to goals, with goal assignments calculated at each timestep based on current positions.
- Core assumption: Local observations provide sufficient information for agents to make effective goal selection decisions.
- Evidence anchors:
  - [abstract] "achieve almost perfect goal coverage in navigation scenarios using only local observations"
  - [section] "Unlike in training, we do not assign any goals; rather, the agents rely on their local observations and the goal assignments learned during training"
  - [corpus] "Emergence of Fair Leaders via Mediators in Multi-Agent Reinforcement Learning" (weak relevance)
- Break condition: If local observation radius is too small or environmental complexity is too high, agents may struggle to make effective decisions.

### Mechanism 3
- Claim: The fairness metric (inverse of coefficient of variation) effectively captures and promotes equitable agent performance.
- Mechanism: By penalizing high variance in travel distances across agents, the system naturally distributes workload more evenly.
- Core assumption: Travel distance is a meaningful proxy for fairness in multi-agent navigation tasks.
- Evidence anchors:
  - [abstract] "We consider the reciprocal of the coefficient of variation of the distances traveled by different agents as a measure of fairness"
  - [section] "We choose our fairness metric F = 1/ð¶ð‘‰ to be the inverse of ð¶ð‘‰ so that it is a non-dimensional quantity with higher numerical values indicating greater fairness"
  - [corpus] "Fairness Aware Reinforcement Learning via Proximal Policy Optimization" (weak relevance)
- Break condition: If task requirements inherently create unequal workloads (e.g., geographically dispersed goals), perfect fairness may be unattainable.

## Foundational Learning

- Concept: Multi-Agent Reinforcement Learning (MARL)
  - Why needed here: The paper builds on MARL frameworks to enable cooperative behavior among multiple agents
  - Quick check question: What distinguishes centralized training from decentralized execution in MARL?

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: Agents use GNNs to process neighborhood information and make decisions based on local observations
  - Quick check question: How do GNNs enable agents to encode information about their local environment?

- Concept: Min-Max Optimization
  - Why needed here: The min-max fair assignment algorithm is used to balance fairness and efficiency in goal assignments
  - Quick check question: What is the primary objective of min-max optimization in the context of fair resource allocation?

## Architecture Onboarding

- Component map:
  - Environment (2D space with agents, goals, obstacles) -> Agents (with local observation vector and graph observation) -> GNN (processes neighborhood information) -> Policy network (maps observations to actions) -> Reward function (combines distance, goal, and fairness rewards) -> Goal assignment (min-max fair or optimal distance cost assignment)

- Critical path:
  1. Initialize environment with random agent and goal positions
  2. Calculate goal assignments based on current positions
  3. Provide agents with local observations and GNN-encoded neighborhood information
  4. Agents select actions based on learned policies
  5. Calculate rewards (distance, goal, fairness, collision penalties)
  6. Update agent policies through centralized training
  7. Repeat until all goals are reached or episode ends

- Design tradeoffs:
  - Local vs. global information: Decentralized approach sacrifices some coordination capability for scalability and privacy
  - Fairness vs. efficiency: Balancing these competing objectives requires careful reward shaping
  - Fixed vs. adaptive goal assignments: Real-time assignments improve fairness but increase computational complexity

- Failure signatures:
  - Low success rates despite high fairness metrics (agents prioritizing fairness over task completion)
  - Agents getting stuck in local minima or exhibiting oscillatory behavior
  - Rapid decrease in fairness metric as agent count increases (congestion effects)

- First 3 experiments:
  1. Test baseline performance with random goal assignments (RA model)
  2. Evaluate impact of min-max fair goal assignments without fairness reward (FA model)
  3. Assess combined effect of fair goal assignments and fairness reward (FA+FR model)

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability analysis limited to 8 agents, leaving uncertainty about performance in larger systems
- Experiments focus on simple 2D navigation tasks with convex obstacles, limiting generalizability to complex environments
- Fairness metric based on travel distance may not capture all aspects of equitable performance in different task domains
- Computational overhead of min-max fair assignments during training was not quantified

## Confidence

- High confidence: The core mechanism of combining min-max fair assignments with fairness reward terms is well-supported by the experimental results and theoretical foundations
- Medium confidence: The scalability claims to different numbers of agents and formation shapes, as the experiments were limited to small-scale scenarios
- Low confidence: The generalizability to more complex environments with non-convex obstacles or dynamic elements

## Next Checks
1. Conduct experiments with 20+ agents to evaluate scalability limits and identify potential bottlenecks in performance
2. Test the approach in environments with non-convex obstacles, moving obstacles, and varying terrain to assess robustness
3. Implement a computational overhead benchmark comparing the min-max fair assignment algorithm with alternative fairness-promoting methods