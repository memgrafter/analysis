---
ver: rpa2
title: Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language
  Models
arxiv_id: '2402.11900'
source_url: https://arxiv.org/abs/2402.11900
tags:
- knowledge
- multi-hop
- shortcuts
- uni00000013
- editing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how large language models (LLMs) handle\
  \ multi-hop reasoning through factual shortcuts\u2014direct associations between\
  \ initial and terminal entities in multi-hop knowledge. The authors find that the\
  \ strength of these shortcuts correlates strongly with the co-occurrence frequency\
  \ of entities in pre-training corpora."
---

# Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models

## Quick Facts
- arXiv ID: 2402.11900
- Source URL: https://arxiv.org/abs/2402.11900
- Reference count: 23
- Key outcome: Factual shortcuts in multi-hop reasoning arise from co-occurrence patterns in pre-training data and cause ~20% of knowledge editing failures

## Executive Summary
This paper investigates how large language models (LLMs) handle multi-hop reasoning through factual shortcuts—direct associations between initial and terminal entities in multi-hop knowledge chains. The authors find that the strength of these shortcuts correlates strongly with the co-occurrence frequency of entities in pre-training corpora. Few-shot prompting enables more shortcut use than chain-of-thought prompting. Analysis of multi-hop knowledge editing failures shows approximately 20% are caused by shortcuts, especially in high co-occurrence instances. The authors propose erasing shortcut-related neurons to mitigate this risk, which significantly reduces failure rates and improves overall editing success.

## Method Summary
The authors analyze multi-hop reasoning shortcuts by measuring co-occurrence frequencies of initial and terminal entities in pre-training corpora, identifying knowledge neurons that encode reasoning patterns, and comparing few-shot versus chain-of-thought prompting strategies. They apply knowledge editing methods to the MQUAKE-CF-3K dataset and categorize failures, then erase shortcut neurons in high co-occurrence instances to assess impact on editing performance. The methodology combines corpus analysis, knowledge neuron attribution, and targeted neuron erasure to understand and mitigate shortcut-related failures.

## Key Results
- The strength of factual shortcuts is highly correlated with the frequency of co-occurrence of initial and terminal entities in pre-training corpora (Pearson 0.74)
- Few-shot prompting leverages more shortcuts in answering multi-hop questions compared to chain-of-thought prompting
- Approximately 20% of knowledge editing failures are attributed to shortcuts, and erasing shortcut neurons significantly reduces these failures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Factual shortcuts in multi-hop reasoning arise from co-occurrence patterns in pre-training data
- Mechanism: When initial and terminal entities co-occur frequently in training corpora, LLMs learn direct associations bypassing intermediate reasoning steps
- Core assumption: LLMs memorize statistical patterns rather than causal relationships during pre-training
- Evidence anchors:
  - [abstract] "the strength of factual shortcuts is highly correlated with the frequency of co-occurrence of initial and terminal entities in the pre-training corpora"
  - [section 3.1] "If s1 and on co-occur within the same paragraph, it is highly plausible that the LLM establishes a direct connection between them during the pre-training phase"
  - [corpus] Wikipedia and Dolma corpus analysis shows strong correlation (Pearson 0.74) between co-occurrence frequencies

### Mechanism 2
- Claim: Different prompting strategies influence shortcut utilization in multi-hop reasoning
- Mechanism: Few-shot prompting allows more shortcut use than chain-of-thought prompting by not explicitly requiring intermediate reasoning steps
- Core assumption: LLMs default to pattern matching unless explicitly guided to step-by-step reasoning
- Evidence anchors:
  - [abstract] "few-shot prompting leverage more shortcuts in answering multi-hop questions compared to chain-of-thought prompting"
  - [section 3.2] "the LLM adheres to a greater extent to reasoning patterns overlapping with those for single-hop questions under the chain-of-thought prompt"
  - [corpus] Overlap analysis shows lower reasoning alignment in few-shot vs chain-of-thought conditions

### Mechanism 3
- Claim: Knowledge editing failures in multi-hop scenarios are significantly caused by shortcut persistence
- Mechanism: After editing single-hop facts, shortcuts cause LLMs to still output original answers because they bypass updated intermediate knowledge
- Core assumption: Knowledge editing methods cannot effectively modify hard-coded shortcut associations
- Evidence anchors:
  - [abstract] "approximately 20% of the failures are attributed to shortcuts"
  - [section 4] "shortcuts enable LLMs to conveniently utilize the rmul hard-coded during the pre-training phase to directly obtain results"
  - [corpus] Failure analysis shows higher co-occurrence frequencies in shortcut failure instances

## Foundational Learning

- Concept: Knowledge neurons and their role in factual recall
  - Why needed here: Understanding how LLMs store and retrieve factual knowledge is crucial for analyzing shortcut mechanisms
  - Quick check question: What is the threshold attribution value used to identify crucial knowledge neurons in this paper?

- Concept: Co-occurrence frequency analysis in language modeling
  - Why needed here: The core hypothesis links shortcut strength to entity co-occurrence patterns in training data
  - Quick check question: Which two corpora were analyzed for co-occurrence frequency patterns?

- Concept: Multi-hop reasoning and its distinction from single-hop reasoning
  - Why needed here: The paper's entire premise relies on understanding how LLMs handle chained reasoning vs direct associations
  - Quick check question: What is the formal representation of a multi-hop knowledge chain in this paper?

## Architecture Onboarding

- Component map: Pre-training corpus analysis -> Knowledge neuron identification -> Prompting strategy comparison -> Failure analysis -> Shortcut erasure -> Performance evaluation
- Critical path: 1) Identify co-occurrence patterns → 2) Locate shortcut neurons → 3) Analyze failure modes → 4) Apply erasure → 5) Measure improvement
- Design tradeoffs: Few-shot prompting is faster but allows more shortcuts; chain-of-thought is more reliable but requires more computation
- Failure signatures: High co-occurrence frequencies between initial and terminal entities, persistent incorrect answers after knowledge editing, low overlap with single-hop reasoning patterns
- First 3 experiments:
  1. Measure co-occurrence frequencies of initial/terminal entities in Wikipedia corpus
  2. Compare knowledge neuron overlap between few-shot and chain-of-thought prompts
  3. Analyze failure rates before and after shortcut neuron erasure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the erasing of shortcut neurons be extended to other layers of the transformer model beyond the Feedforward Neural Network (FFN) layer?
- Basis in paper: [inferred] The paper mentions that the erasing approach is inspired by Knowledge Neurons (KN) and is applied to the FFN layer, but does not explore other layers
- Why unresolved: The paper does not investigate whether the shortcut neurons can be located and erased in other layers such as the attention layers or embedding layers
- What evidence would resolve it: Experiments demonstrating the effectiveness of erasing shortcut neurons in other layers and comparing the results with those obtained from erasing in the FFN layer

### Open Question 2
- Question: How does the strength of factual shortcuts vary across different domains or types of knowledge?
- Basis in paper: [explicit] The paper mentions that the strength of factual shortcuts is correlated with the frequency of co-occurrence of initial and terminal entities in the pre-training corpora, but does not explore variations across different domains or types of knowledge
- Why unresolved: The paper does not investigate whether the strength of factual shortcuts differs in domains such as science, history, or sports, or for different types of knowledge such as factual statements, causal relationships, or procedural knowledge
- What evidence would resolve it: Analysis of the strength of factual shortcuts across different domains or types of knowledge, potentially revealing patterns or differences that could inform targeted interventions

### Open Question 3
- Question: Can the erasing of shortcut neurons be combined with other knowledge editing techniques to improve the overall success rate of multi-hop knowledge editing?
- Basis in paper: [explicit] The paper proposes erasing shortcut neurons to mitigate the risks associated with factual shortcuts, but does not explore combining this approach with other knowledge editing techniques
- Why unresolved: The paper does not investigate whether the erasing of shortcut neurons can be integrated with techniques such as constrained fine-tuning, meta-learning, or memory-based model editing to enhance the effectiveness of multi-hop knowledge editing
- What evidence would resolve it: Experiments demonstrating the combined use of erasing shortcut neurons with other knowledge editing techniques, potentially leading to improved success rates and reduced risks in multi-hop knowledge editing

## Limitations

- The paper demonstrates correlation between co-occurrence frequency and shortcut strength but the exact neural mechanisms remain partially speculative
- The shortcut erasure method shows promise with ~20% improvement but long-term stability and impact on other capabilities remain untested
- The recommendation to modify pre-training datasets to eliminate shortcuts extends beyond the empirical findings and requires extensive validation

## Confidence

- **High confidence**: The correlation between co-occurrence frequency and shortcut strength, and the observation that knowledge editing failures increase with higher co-occurrence rates
- **Medium confidence**: The effectiveness of shortcut neuron erasure as a mitigation strategy, though generalizability and side effects warrant further investigation
- **Low confidence**: The claim that pre-training datasets should be modified to eliminate shortcuts entirely

## Next Checks

1. **Cross-model validation**: Test whether the co-occurrence-shortcut correlation holds across different LLM architectures to establish whether this is a fundamental property of transformer-based models

2. **Long-term stability analysis**: Evaluate the persistence of shortcut erasure effects over extended inference sessions and across different task types to ensure mitigation doesn't degrade general reasoning capabilities

3. **Temporal reasoning extension**: Apply the methodology to temporal knowledge editing scenarios where facts change over time, examining whether shortcuts create additional complications in maintaining temporal consistency