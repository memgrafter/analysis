---
ver: rpa2
title: All Random Features Representations are Equivalent
arxiv_id: '2406.18802'
source_url: https://arxiv.org/abs/2406.18802
tags:
- variance
- sample
- random
- sampling
- kernel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that when using importance sampling optimally,
  all random feature representations for positive-definite kernels have the same approximation
  error. The authors derive an optimal sampling policy that minimizes the expected
  sample variance across all inputs, and prove that this optimal variance depends
  only on the kernel itself, not on the choice of random feature representation.
---

# All Random Features Representations are Equivalent

## Quick Facts
- arXiv ID: 2406.18802
- Source URL: https://arxiv.org/abs/2406.18802
- Authors: Luke Sernau; Silvano Bonacina; Rif A. Saurous
- Reference count: 14
- Primary result: Optimal importance sampling makes all random feature representations for positive-definite kernels have the same approximation error

## Executive Summary
This paper establishes a fundamental result in random feature theory: when using optimal importance sampling, all random feature representations for positive-definite kernels achieve the same approximation error. The authors derive an optimal sampling policy that minimizes expected sample variance across all inputs, proving that this optimal variance depends only on the kernel itself, not on the choice of random feature representation. Specifically, when sampling from the optimal distribution pΨ(w) ∝ pΩ(w)qφ(ω), all representations achieve the same variance bound, which equals E[K(x1,x1)K(x2,x2) - K(x1,x2)²] when input distributions are identical, or is upper bounded by this quantity otherwise.

## Method Summary
The paper develops a theoretical framework for analyzing random feature approximations through the lens of importance sampling. The authors formulate the problem as minimizing expected sample variance VΨ over all inputs x1 and x2 by optimizing the sampling distribution Ψ. Using Lagrange optimization, they derive the optimal sampling distribution pΨ(w) ∝ pΩ(w)qφ(ω) where qφ(ω) = √E[φ(x1,ω)²]E[φ(x2,ω)²]. This optimal distribution weights samples according to both the base distribution and a kernel-dependent term that measures feature importance across input distributions. The key insight is that when importance sampling is performed optimally, the resulting variance bound is representation-independent and depends only on the kernel's properties.

## Key Results
- All random feature representations achieve identical optimal variance when using importance sampling optimally
- The optimal variance bound is E[K(x1,x1)K(x2,x2) - K(x1,x2)²] for identical input distributions, or upper bounded by this value otherwise
- The optimal sampling distribution pΨ(w) ∝ pΩ(w)qφ(ω) minimizes expected sample variance across all inputs
- The equivalence result holds for any positive-definite kernel that can be expressed as an expectation over a base distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: All random feature representations achieve the same optimal variance when importance sampling is used optimally.
- Mechanism: The optimal sampling distribution pΨ(w) ∝ pΩ(w)qφ(ω) minimizes expected sample variance by weighting samples according to both the base distribution and a kernel-dependent term qφ(ω) that measures feature importance across input distributions.
- Core assumption: The kernel K(x1,x2) can be expressed as an expectation over some distribution Ω using representation φ, and the input distributions X1 and X2 are known or estimable.
- Evidence anchors:
  - [abstract]: "when using importance sampling optimally, all random feature representations for positive-definite kernels have the same approximation error"
  - [section 4]: "the optimal sample variance achieved in Theorem 1 is upper bounded by E[K(x1,x1)K(x2,x2) - K(x1,x2)²]" and this bound is independent of φ
  - [corpus]: Weak evidence - corpus contains related work on random features but none directly addressing the equivalence claim under optimal sampling
- Break condition: If the kernel cannot be written in the required expectation form, or if input distributions X1 and X2 are unknown or non-stationary.

### Mechanism 2
- Claim: The optimal variance bound depends only on the kernel's properties, not on the choice of random feature representation.
- Mechanism: Through Lagrange optimization of the expected sample variance, the optimal sampling distribution's form ensures that the resulting variance bound E[K(x1,x1)K(x2,x2) - K(x1,x2)²] emerges naturally and is representation-independent.
- Core assumption: The positive-definite kernel satisfies the Cauchy-Schwarz inequality, ensuring the variance bound is positive.
- Evidence anchors:
  - [section 3]: Theorem 1 proves the optimal sampling distribution and resulting variance formula
  - [section 4]: "Since positive-definite kernels obey the Cauchy-Schwarz inequality, we know that this expression is positive"
  - [corpus]: No direct evidence in corpus about Cauchy-Schwarz application to variance bounds
- Break condition: If the kernel is not positive-definite or does not satisfy Cauchy-Schwarz inequality.

### Mechanism 3
- Claim: When input distributions are identical (X1 = X2), the optimal variance becomes exactly E[K(x,x)² - K(x,x')²].
- Mechanism: When X1 = X2, the qφ(ω) term simplifies to E[φ(x,ω)²], and through expectation exchange, the variance formula collapses to a kernel-only expression.
- Core assumption: The input distribution is stationary and identical for both arguments of the kernel.
- Evidence anchors:
  - [section 4]: "If X1 = X2 then the optimal sample variance achieved in Theorem 1 is exactly E[K(x1,x1)² - K(x1,x2)²]"
  - [section 3]: The derivation shows how qφ(ω)² leads to this simplification
  - [corpus]: No corpus evidence directly supporting this specific simplification
- Break condition: If input distributions differ or are non-stationary.

## Foundational Learning

- Concept: Importance sampling
  - Why needed here: The paper uses importance sampling to reduce variance in random feature approximations by sampling from a distribution that emphasizes more informative samples.
  - Quick check question: In importance sampling, how do you correct for sampling from a different distribution than the original?

- Concept: Positive-definite kernels and their properties
  - Why needed here: The equivalence result fundamentally relies on properties of positive-definite kernels, including the Cauchy-Schwarz inequality and their ability to be represented as expectations over random features.
  - Quick check question: What is the Cauchy-Schwarz inequality for positive-definite kernels, and why does it guarantee the variance bound is positive?

- Concept: Lagrange optimization
  - Why needed here: The optimal sampling distribution is derived using Lagrange optimization to minimize expected sample variance subject to the constraint that the sampling distribution integrates to 1.
  - Quick check question: What is the Lagrangian form for optimizing pΨ subject to the constraint ∫pΨ(ω)dω = 1?

## Architecture Onboarding

- Component map:
  - Random feature representation φ: The function mapping inputs to features
  - Base distribution Ω: The distribution over which expectations are taken
  - Input distributions X1, X2: Distributions over the data space
  - Sampling policy Ψ: The importance sampling distribution pΨ(w) ∝ pΩ(w)qφ(ω)
  - Variance computation: The mechanism for calculating and bounding approximation error

- Critical path:
  1. Verify kernel can be expressed as expectation over Ω using φ
  2. Estimate or obtain input distributions X1 and X2
  3. Compute qφ(ω) = √(E[φ(x1,ω)²]E[φ(x2,ω)²])
  4. Derive optimal sampling distribution pΨ(w) ∝ pΩ(w)qφ(ω)
  5. Implement sampling from Ψ and verify variance bounds

- Design tradeoffs:
  - Sampling complexity vs. variance reduction: More sophisticated sampling may be needed for complex kernels
  - Input distribution estimation: Requires either known data distributions or robust estimation methods
  - Computational overhead: Computing qφ(ω) may be expensive for high-dimensional features

- Failure signatures:
  - Variance bounds not achieved: Indicates either incorrect implementation of optimal sampling or violation of core assumptions
  - Sampling inefficiency: May indicate poor estimation of input distributions or numerical instability in qφ computation
  - Negative variance bounds: Violation of Cauchy-Schwarz inequality suggests kernel is not positive-definite

- First 3 experiments:
  1. Verify equivalence on a simple kernel (e.g., RBF) with known analytical form by comparing variance across different φ representations
  2. Test sensitivity to input distribution estimation by using empirical vs. true distributions
  3. Benchmark sampling efficiency by comparing standard vs. optimal importance sampling on a moderate-sized dataset

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The paper is purely theoretical with no empirical validation of the equivalence result in practice
- Assumes perfect knowledge of input distributions X1 and X2, which may not be feasible in real-world settings
- Does not analyze the computational overhead of implementing optimal sampling versus simpler heuristics

## Confidence
**High confidence**: The theoretical derivation showing that optimal variance bounds depend only on kernel properties, not representation choice, is well-supported by the mathematical proofs in Sections 3 and 4.

**Medium confidence**: The practical implications of the equivalence result, particularly regarding when the overhead of optimal sampling is justified versus simpler heuristics, are less clearly established.

**Low confidence**: The empirical validation is minimal, with no concrete experimental results demonstrating the theoretical bounds or comparing different representations under realistic conditions.

## Next Checks
1. **Empirical Variance Comparison**: Implement the optimal sampling strategy for a standard kernel (e.g., RBF) across different random feature representations and measure actual approximation variance against the theoretical bounds.

2. **Input Distribution Sensitivity**: Test the robustness of the optimal sampling approach when using empirical estimates of input distributions versus true distributions.

3. **Computational Overhead Analysis**: Benchmark the computational cost of generating samples from the optimal distribution pΨ(w) ∝ pΩ(w)qφ(ω) against simpler sampling strategies.