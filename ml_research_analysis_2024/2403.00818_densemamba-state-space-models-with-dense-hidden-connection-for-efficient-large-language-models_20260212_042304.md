---
ver: rpa2
title: 'DenseMamba: State Space Models with Dense Hidden Connection for Efficient
  Large Language Models'
arxiv_id: '2403.00818'
source_url: https://arxiv.org/abs/2403.00818
tags:
- hidden
- language
- layers
- state
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of high computational and memory
  demands in large language models by proposing DenseSSM, a novel approach that enhances
  the flow of hidden information between layers in state space models (SSMs). The
  core method involves selectively integrating shallow-layer hidden states into deeper
  layers to retain fine-grained information crucial for the final output.
---

# DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models

## Quick Facts
- arXiv ID: 2403.00818
- Source URL: https://arxiv.org/abs/2403.00818
- Reference count: 14
- One-line primary result: DenseSSM improves SSM performance by up to 5% on benchmarks by enhancing hidden state information flow

## Executive Summary
DenseMamba introduces DenseSSM, a novel approach that enhances the flow of hidden information between layers in state space models (SSMs) by selectively integrating shallow-layer hidden states into deeper layers. This addresses the challenge of high computational and memory demands in large language models while maintaining the parallelizability and inference efficiency of SSMs. The method achieves significant performance improvements over traditional SSMs like RetNet and Mamba on public benchmarks, with up to 5% accuracy gains.

## Method Summary
DenseSSM enhances SSMs by introducing dense hidden connections that selectively integrate shallow-layer hidden states into deeper layers through a selective transition module and hidden fusion module. The approach operates in both recurrence mode (for efficient inference) and convolution mode (for parallel training) without altering the underlying SSM computation structure. The method is applicable to various SSM types including RetNet and Mamba, and was trained on 56GB of raw data from The Pile corpus using the LLaMA tokenizer.

## Key Results
- Achieves up to 5% accuracy improvement over traditional SSMs on public benchmarks
- Maintains training parallelizability and inference efficiency of original SSM framework
- Applicable to various SSM types including RetNet and Mamba
- Demonstrates effectiveness across multiple evaluation tasks including WikiText, LAMBADA, ARCC, ARCE, and others

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selective integration of shallow-layer hidden states into deeper layers preserves fine-grained information that would otherwise be lost due to repeated transformations and matrix multiplications in SSMs.
- Mechanism: Shallow hidden states are projected into the target layer's subspace via a selective transition module, then fused with deeper-layer hidden states, allowing earlier, more detailed information to flow upward through the network.
- Core assumption: Information degradation occurs because hidden states are repeatedly transformed as they propagate from shallow to deep layers.
- Evidence anchors:
  - [abstract] "By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output."
  - [section 3.3] "We first collect the shallow hidden states and introduce a selective transition module ϕ to project them to the subspace of the target layer and select useful parts simultaneously."
  - [corpus] Weak: no corpus papers directly analyze hidden state degradation in SSMs, though related works discuss SSM performance gaps compared to Transformers.

### Mechanism 2
- Claim: The proposed dense hidden connections maintain the parallelizability and inference efficiency of the original SSM framework.
- Mechanism: The dense connection scheme is implemented such that it can operate in both recurrence mode (for efficient inference) and convolution mode (for parallel training), without altering the underlying SSM computation structure.
- Core assumption: The additional operations (projection, selection, fusion) can be implemented efficiently enough to not negate the computational advantages of SSMs.
- Evidence anchors:
  - [abstract] "Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency."
  - [section 3.3] "The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements..."
  - [corpus] Weak: corpus contains papers on SSM efficiency, but none specifically address dense connections' impact on parallelizability.

### Mechanism 3
- Claim: DenseSSM's improvement is not architecture-specific and can be applied to various SSM types, including RetNet and Mamba.
- Mechanism: The core idea of selectively integrating shallow hidden states is general enough to be adapted to different SSM architectures by modifying the hidden state flow and fusion operations accordingly.
- Core assumption: Different SSM architectures share a common structure where hidden states flow within layers and could benefit from external information injection.
- Evidence anchors:
  - [abstract] "The proposed method can be widely applicable to various SSM types like RetNet and Mamba."
  - [section 3.3] "The DenseSSM scheme can be used in any SSM variant such as Mamba (Gu & Dao, 2023)."
  - [corpus] Weak: corpus papers focus on specific SSM variants but don't discuss the generalizability of dense connections.

## Foundational Learning

- Concept: State Space Models (SSMs) and their operation in both recurrence and convolution modes
  - Why needed here: Understanding how SSMs process sequences and the role of hidden states is crucial for grasping why DenseSSM's approach improves information flow.
  - Quick check question: What are the two modes of SSM operation, and when is each typically used?

- Concept: Attention mechanisms in Transformers vs. SSMs
  - Why needed here: Comparing SSMs to Transformers helps understand the motivation for improving SSMs and the context of DenseSSM's contributions.
  - Quick check question: How does the computational complexity of self-attention in Transformers compare to that of SSMs?

- Concept: Information degradation in deep neural networks
  - Why needed here: Recognizing that information can be lost as it propagates through layers is key to understanding the problem DenseSSM addresses.
  - Quick check question: Why might information be lost as it flows from shallow to deep layers in a neural network?

## Architecture Onboarding

- Component map: Input sequence -> SSM layers (with dense connections) -> Output predictions
- Critical path: Input sequence → SSM layers (with dense connections) → Output predictions
- Design tradeoffs:
  - Complexity vs. Performance: Adding dense connections increases model complexity but improves performance
  - Efficiency vs. Information Flow: Must balance the additional operations' computational cost with the benefit of improved information flow
- Failure signatures:
  - Decreased performance: May indicate overly aggressive projection/selection or fusion operations introducing noise
  - Increased computational cost: Suggests the dense connection operations are too complex
  - Training instability: Could result from incompatible hidden state representations or dimensions
- First 3 experiments:
  1. Implement DenseSSM on a simple SSM variant (e.g., S4) and compare performance on a standard benchmark (e.g., WikiText)
  2. Vary the number of dense layers (m) and observe the impact on performance and computational cost
  3. Compare different implementations of the Selective Transition Module (e.g., Identity vs. Linear projection) and Hidden Fusion Module (e.g., Add vs. Concat)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DenseSSM scale with increasing model depth compared to traditional SSMs?
- Basis in paper: [inferred] The paper discusses the importance of hidden state information flow between layers but does not provide experimental results on how DenseSSM performs with varying model depths.
- Why unresolved: The paper focuses on comparing DenseSSM with traditional SSMs at a fixed model size, without exploring the impact of model depth on performance.
- What evidence would resolve it: Experiments comparing the performance of DenseSSM and traditional SSMs with varying numbers of layers, ideally on a range of tasks and datasets.

### Open Question 2
- Question: What is the optimal number of shallow layers to integrate into deeper layers in DenseSSM, and how does this choice impact performance?
- Basis in paper: [explicit] The paper mentions integrating shallow-layer hidden states into deeper layers but does not provide guidance on the optimal number of layers to integrate or how this choice affects performance.
- Why unresolved: The paper presents DenseSSM as a general framework without specifying the best practices for choosing the number of layers to integrate, leaving this as an open question for practitioners.
- What evidence would resolve it: Experiments exploring the impact of different numbers of integrated layers on DenseSSM performance, potentially providing insights into the optimal choice for various tasks and model sizes.

### Open Question 3
- Question: How does DenseSSM perform on tasks requiring very long-range dependencies compared to other architectures like Transformers or recurrent models?
- Basis in paper: [inferred] The paper focuses on the advantages of DenseSSM in terms of hidden state information flow but does not directly compare its performance on long-range dependency tasks with other architectures.
- Why unresolved: While DenseSSM is designed to improve hidden state information flow, the paper does not provide evidence of its effectiveness in handling long-range dependencies compared to other state-of-the-art architectures.
- What evidence would resolve it: Experiments comparing DenseSSM's performance on tasks requiring long-range dependencies (e.g., language modeling on very long sequences) with Transformers, recurrent models, and other SSM variants.

## Limitations

- The paper lacks direct evidence for the core mechanism of information degradation in SSMs and how dense connections specifically address this issue
- Claims about wide applicability across SSM types are supported theoretically but lack empirical validation beyond RetNet and Mamba
- Efficiency claims regarding maintained parallelizability and inference speed are stated but not quantitatively validated against baseline SSMs

## Confidence

**High Confidence**: The performance claims on benchmarks (up to 5% accuracy improvement) are directly supported by experimental results presented in the paper. The implementation details for training data, tokenization, and evaluation metrics are sufficiently specified for reproduction.

**Medium Confidence**: The efficiency claims regarding maintained parallelizability and inference speed are supported by the abstract statement and architectural description, but lack quantitative validation or comparison to baseline SSMs. The claim of wide applicability across SSM types is theoretically supported but lacks empirical validation beyond the two mentioned architectures.

**Low Confidence**: The mechanism explanation for why information degradation occurs in SSMs and how dense connections specifically address this issue is primarily theoretical. The paper does not provide evidence such as hidden state similarity analysis, gradient flow measurements, or ablation studies that would directly validate these claims.

## Next Checks

1. **Hidden State Analysis**: Implement a diagnostic tool to measure the similarity (e.g., cosine similarity, mutual information) between shallow and deep layer hidden states with and without dense connections. This would provide direct evidence for information degradation and the effectiveness of the dense connection approach.

2. **Ablation Study on Transition and Fusion Modules**: Systematically replace the selective transition module with identity mapping and test different fusion strategies (addition, concatenation, gating) to isolate their individual contributions to performance gains.

3. **Cross-Architecture Validation**: Implement DenseSSM on at least two additional SSM architectures beyond RetNet and Mamba (e.g., S4 or Hippo) and evaluate whether the performance improvements generalize to architectures with substantially different state update mechanisms.