---
ver: rpa2
title: Exploring the Limits of Fine-grained LLM-based Physics Inference via Premise
  Removal Interventions
arxiv_id: '2404.18384'
source_url: https://arxiv.org/abs/2404.18384
tags:
- arxiv
- language
- derivation
- physics
- mathematical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a fine-grained dataset of physics derivations
  to test language models' step-wise mathematical reasoning capabilities. The authors
  systematically remove premises from derivation prompts to assess model performance
  degradation and explore out-of-distribution reasoning abilities.
---

# Exploring the Limits of Fine-grained LLM-based Physics Inference via Premise Removal Interventions

## Quick Facts
- arXiv ID: 2404.18384
- Source URL: https://arxiv.org/abs/2404.18384
- Authors: Jordan Meadows; Tamsin James; Andre Freitas
- Reference count: 40
- Primary result: Language models struggle with fine-grained physics derivations, showing non-linear degradation when premises are removed and often ignoring physical context for algebraic coherence

## Executive Summary
This paper introduces a fine-grained dataset of physics derivations to test language models' step-wise mathematical reasoning capabilities. The authors systematically remove premises from derivation prompts to assess model performance degradation and explore out-of-distribution reasoning abilities. They find that models like GPT-4 struggle with detailed algebraic manipulation and often resort to reverse-engineering solutions rather than following physical principles. The study demonstrates that current language models are not yet capable of physics-informed reasoning, as they tend to ignore physical context in favor of algebraic coherence.

## Method Summary
The study constructs a dataset of 218 step-wise physics derivations from electromagnetism, quantum, classical, and statistical mechanics. Models (GPT-4, GPT-3.5, T5-related) are evaluated on zero-shot and few-shot settings using synthetic in-context examples. Premise removal intervention progressively omits supporting premises from prompts, and performance is measured using ROUGE, BLEU, GLEU metrics along with manual evaluation. The dataset expands Wikipedia derivations to fine-grained steps, and 155 shared symbols are used for MathT5 training.

## Key Results
- Models show non-linear degradation in derivation quality as premises are progressively removed
- Language models derive equations through reverse-engineering rather than step-by-step physical reasoning
- Physical context is predominantly ignored in favor of algebraic coherence, leading to physical inconsistencies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models struggle with fine-grained algebraic manipulation when physical context is present, preferring algebraic coherence over physical correctness.
- Mechanism: When given prompts with physical equations, models attempt to generate mathematically consistent derivations but ignore physical constraints like units, commutation relations, or spatial dependencies, leading to "hallucinations" that satisfy algebra but violate physics.
- Core assumption: Physical context in equations requires satisfaction of complex semantics (units, tensorial order) that language models do not inherently understand or enforce during generation.
- Evidence anchors:
  - [abstract] "physical context imbues the use of symbols which needs to satisfy complex semantics (e.g., units, tensorial order)"
  - [section] "models' mathematical reasoning is not physics-informed in this setting, where physical context is predominantly ignored in favour of reverse-engineering solutions"
  - [corpus] Weak - no direct corpus evidence for this mechanism, but related work on premise selection and mathematical reasoning suggests similar patterns

### Mechanism 2
- Claim: Progressive premise removal from prompts causes non-linear degradation in derivation quality, revealing models' reliance on specific derivation paths.
- Mechanism: As premises are removed from prompts, models must either generate missing premises or find alternative derivation routes. The non-linear degradation suggests models struggle to adapt to distribution shifts and may reverse-engineer solutions rather than follow valid mathematical reasoning paths.
- Core assumption: The perturbation strength (number of premises removed) directly controls the severity of the distribution shift, and models' performance degrades non-linearly as this shift increases.
- Evidence anchors:
  - [abstract] "demonstrate non-linear degradation of derivation quality with perturbation strength via the progressive omission of supporting premises"
  - [section] "non-linear degradation in derivation quality reported by text generation metrics is supported by manual evaluation"
  - [corpus] Moderate - related work on premise selection and mathematical reasoning supports the premise removal intervention approach

### Mechanism 3
- Claim: Language models derive equations through reverse-engineering rather than step-by-step physical reasoning, leading to algebraic errors and physical inconsistencies.
- Mechanism: Models start from the goal equation and work backwards, attempting to construct a derivation path without understanding the underlying physics. This leads to substitution errors, incorrect use of physical laws, and failure to consider implicit physical assumptions.
- Core assumption: The nature of the derivation generation task, combined with the models' training data, encourages reverse-engineering behavior rather than genuine physical reasoning.
- Evidence anchors:
  - [abstract] "mathematical reasoning is not physics-informed in this setting, where physical context is predominantly ignored in favour of reverse-engineering solutions"
  - [section] "language models derive equations by reverse-engineering" and "A significant proportion of errors involve substitution"
  - [corpus] Weak - no direct corpus evidence, but related work on mathematical reasoning and large language models suggests similar patterns

## Foundational Learning

- Concept: Physical semantics in mathematical expressions
  - Why needed here: Physics equations carry physical meaning beyond their algebraic form, including units, dimensions, and physical laws that constrain valid manipulations
  - Quick check question: Given the equation F = ma, what physical constraint prevents you from dividing both sides by time?

- Concept: Premise selection in mathematical reasoning
  - Why needed here: Derivations require selecting appropriate premises and axioms to build valid reasoning chains, which becomes critical when premises are removed from prompts
  - Quick check question: If you need to derive the kinetic energy formula, what are the essential premises you would need?

- Concept: Non-linear degradation in model performance
  - Why needed here: Understanding how model performance degrades non-linearly with premise removal helps identify the limitations of current approaches and guide improvements
  - Quick check question: If removing 1 premise causes a 10% performance drop and removing 2 premises causes a 30% drop, what type of degradation pattern is this?

## Architecture Onboarding

- Component map: Data collection -> Dataset construction -> Model evaluation -> Premise removal intervention -> Performance analysis -> Manual evaluation
- Critical path: Dataset construction -> Model evaluation -> Premise removal intervention -> Performance analysis
- Design tradeoffs: Fine-grained vs. coarse-grained derivations (more steps vs. computational efficiency), synthetic vs. real data (control vs. authenticity), automatic vs. manual evaluation (scalability vs. accuracy)
- Failure signatures: Non-linear degradation in performance metrics, increase in substitution errors, physical inconsistencies in generated derivations, failure to generate or retrieve missing premises
- First 3 experiments:
  1. Evaluate MathT5-large on unperturbed prompts and compare to GPT-4 few-shot performance
  2. Apply premise removal intervention with S=1 and measure performance degradation
  3. Manually evaluate a sample of generated derivations to identify common failure modes and physical inconsistencies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do symbolic solvers compare to large language models in performing fine-grained mathematical reasoning in physics derivations?
- Basis in paper: [explicit] The paper discusses the limitations of LMs in physics reasoning and suggests that integrating symbolic solvers might not inherently improve the limitation of selecting appropriate knowledge bases during inference.
- Why unresolved: The paper mentions that symbolic solvers could potentially improve out-of-distribution mathematical abilities but does not provide experimental evidence comparing the performance of symbolic solvers and LMs in this context.
- What evidence would resolve it: Conducting experiments that directly compare the performance of symbolic solvers and LMs on fine-grained physics derivations, especially in scenarios involving out-of-distribution mathematical reasoning.

### Open Question 2
- Question: What are the specific physical assumptions that govern the reasoning paths allowed in physics derivations, and how can language models be improved to better understand and incorporate these assumptions?
- Basis in paper: [explicit] The paper highlights that fundamental physical assumptions guide what mathematical steps are allowed in physics derivations, and current LMs lack understanding of these assumptions, leading to incorrect reasoning.
- Why unresolved: While the paper identifies the problem of LMs not considering physical context, it does not provide a detailed analysis of the specific physical assumptions or propose concrete methods for improving LMs' understanding of these assumptions.
- What evidence would resolve it: Developing a comprehensive framework that identifies key physical assumptions in various physics domains and designing experiments to evaluate how well LMs can incorporate these assumptions into their reasoning process.

### Open Question 3
- Question: How does the granularity of mathematical reasoning in physics derivations affect the performance of large language models, and what is the optimal level of granularity for eliciting accurate reasoning from these models?
- Basis in paper: [explicit] The paper emphasizes the importance of fine-grained reasoning in physics derivations and suggests that current online resources and datasets do not provide the required level of detail for training LMs.
- Why unresolved: The paper does not provide a systematic study on how different levels of granularity in physics derivations impact LM performance or define what constitutes the optimal granularity for eliciting accurate reasoning.
- What evidence would resolve it: Conducting experiments that vary the granularity of physics derivations and measure the corresponding performance of LMs, identifying the level of granularity that maximizes reasoning accuracy while minimizing computational complexity.

## Limitations

- The synthetic dataset used for few-shot examples is not fully specified, making it difficult to assess the quality and representativeness of the training examples
- The manual evaluation protocol lacks detail on inter-rater reliability and specific criteria for judging physical correctness versus algebraic coherence
- Corpus evidence for the proposed mechanisms is weak, relying heavily on assumptions about LLM behavior rather than direct empirical support

## Confidence

- High confidence: The core finding that language models struggle with fine-grained physics derivations and exhibit non-linear degradation when premises are removed. This is directly supported by the experimental results and manual evaluation.
- Medium confidence: The interpretation that models reverse-engineer solutions rather than following physical principles. While supported by error analysis, alternative explanations (such as training data bias or prompt sensitivity) are not fully ruled out.
- Low confidence: The specific mechanisms proposed for why models ignore physical context in favor of algebraic coherence. These rely on weak corpus evidence and could be influenced by factors not fully explored in the study.

## Next Checks

1. Conduct ablation studies to isolate the impact of physical context versus algebraic complexity by creating parallel derivations with and without physical units or constraints.
2. Implement and test symbolic math engines as external tools to verify whether enforcing physical consistency improves model performance on premise-removed prompts.
3. Perform controlled experiments varying the number and type of premises removed to map out the exact shape of the non-linear degradation curve and identify critical thresholds.