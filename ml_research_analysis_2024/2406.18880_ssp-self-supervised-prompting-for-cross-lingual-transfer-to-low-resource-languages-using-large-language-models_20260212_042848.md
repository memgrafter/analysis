---
ver: rpa2
title: 'SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource
  Languages using Large Language Models'
arxiv_id: '2406.18880'
source_url: https://arxiv.org/abs/2406.18880
tags:
- language
- verb
- label
- data
- stage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Supervised Prompting (SSP), a two-stage
  in-context learning framework for zero-labelled cross-lingual transfer to low-resource
  languages. SSP first noisily labels the target test set using source language data,
  then uses these noisy labels as in-context exemplars to improve labeling in a second
  LLM call.
---

# SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models

## Quick Facts
- arXiv ID: 2406.18880
- Source URL: https://arxiv.org/abs/2406.18880
- Reference count: 24
- Key outcome: SSP improves zero-labelled cross-lingual transfer by 3-4 F1 points on average over state-of-the-art baselines

## Executive Summary
This paper introduces Self-Supervised Prompting (SSP), a two-stage in-context learning framework for zero-labelled cross-lingual transfer to low-resource languages. SSP first noisily labels the target test set using source language data, then uses these noisy labels as in-context exemplars to improve labeling in a second LLM call. The method employs an integer-linear programming (ILP) approach to select exemplars, balancing semantic similarity, prediction confidence, and label coverage. Experiments on three tasks (POS, NER, NLI) and eleven low-resource languages show SSP outperforms state-of-the-art fine-tuning and ICL baselines by 3-4 F1 points on average in the zero-labelled setting, establishing a new state of the art.

## Method Summary
SSP operates in two stages: Stage I noisily labels target language test data using source language training data via either fine-tuning or in-context learning; Stage II uses these noisy labels as exemplars in ICL with ILP-based exemplar selection to produce final predictions. The ILP balances three objectives: semantic similarity to the test instance (using Ada-002 embeddings), prediction confidence when available, and label coverage ensuring each label appears at least once. The framework is evaluated across three tasks (POS tagging, NER, NLI) and eleven low-resource languages, comparing against fine-tuning baselines and standard ICL approaches.

## Key Results
- SSP achieves 3-4 F1 point improvements over state-of-the-art fine-tuning and ICL baselines on average
- ILP-based exemplar selection provides 1.3 F1 point average gain over random selection
- SSP consistently outperforms Stage I regardless of whether Stage I uses fine-tuning or ICL
- Performance gains are robust across Germanic languages (POS), African languages (NER), and indigenous American languages (NLI)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Target-language exemplars, even with noisy labels, outperform source-language exemplars with accurate labels in ICL for cross-lingual transfer.
- **Mechanism:** LLMs are sensitive to exemplar language alignment. When exemplars match the test language, the model's internal representations align better with the test distribution, improving inference accuracy despite label noise.
- **Core assumption:** The LLM's cross-lingual transfer capability is stronger when exemplar and test inputs share the same language, even if the labels are imperfect.
- **Evidence anchors:**
  - [abstract] "SSP is based on the key observation that LLMs output more accurate labels if in-context exemplars are from the target language (even if their labels are slightly noisy)."
  - [section] "We find that a reasonably-sized noise region exists such that if the exemplar noise is within that range, then the overall performance is higher than prompting with accurate source language data."
- **Break condition:** If the label noise in target exemplars exceeds a threshold where it overwhelms the benefit of language alignment, performance degrades below source exemplars with clean labels.

### Mechanism 2
- **Claim:** The ILP-based exemplar selection balances similarity, confidence, and label coverage to improve downstream task performance.
- **Mechanism:** The ILP selects a subset of noisy labeled test data to use as exemplars, maximizing semantic similarity to the test instance, ensuring each label appears at least once (label coverage), and preferring exemplars with higher confidence scores when available.
- **Core assumption:** Including diverse, high-quality exemplars improves LLM performance by providing better task understanding and reducing overfitting to a narrow exemplar set.
- **Evidence anchors:**
  - [section] "Our implementation of SSP uses a novel Integer Linear Programming (ILP)-based exemplar selection that balances similarity, prediction confidence (when available) and label coverage."
  - [section] "We observe an average gain of 1.3 F1 points for AmericasNLI compared to the ablation model that does not impose label coverage constraint."
- **Break condition:** If the ILP overemphasizes similarity at the expense of diversity or confidence, it may select redundant or low-quality exemplars, reducing performance gains.

### Mechanism 3
- **Claim:** SSP's two-stage pipeline enables iterative refinement of labels, improving performance regardless of initial labeling method.
- **Mechanism:** Stage I produces noisy labels using source data (via fine-tuning or ICL). Stage II uses these noisy labels as exemplars in ICL to refine predictions. This self-supervised loop leverages the LLM's own output to improve over time.
- **Core assumption:** Even noisy labels from Stage I contain enough signal to guide the LLM in Stage II toward better predictions, especially when exemplars are in the target language.
- **Evidence anchors:**
  - [abstract] "SSP operates in two stages. In Stage I, using source MRL training data, target language's test data is noisily labeled. In Stage II, these noisy test data points are used as exemplars in ICL for further improved labelling."
  - [section] "We observe that the order of stage I performance is 0-CLT-T (translatetest) < 0-CLT < 0-CLT-T (translatetrain) < 0-CLT-U, and same order of performance gets translated in stage II as well, while stage II performance being consistently better than stage 1 in all scenarios."
- **Break condition:** If Stage I produces extremely noisy labels that mislead the LLM in Stage II, the iterative refinement may fail or degrade performance.

## Foundational Learning

- **Concept:** Cross-lingual transfer and zero-shot learning
  - **Why needed here:** The paper addresses transferring knowledge from medium-resource languages to low-resource languages without labeled target data, requiring understanding of cross-lingual transfer mechanisms.
  - **Quick check question:** What is the difference between zero-shot and few-shot learning in the context of cross-lingual transfer?

- **Concept:** In-context learning (ICL) and exemplar selection
  - **Why needed here:** SSP relies on ICL with carefully selected exemplars. Understanding how exemplar choice affects LLM performance is critical to grasping SSP's design.
  - **Quick check question:** How does exemplar similarity and label coverage impact ICL performance in LLMs?

- **Concept:** Integer Linear Programming (ILP) and optimization
  - **Why needed here:** The ILP balances multiple objectives (similarity, confidence, label coverage) for exemplar selection. Understanding ILP formulation and constraints is key to implementing SSP.
  - **Quick check question:** How would you formulate an ILP to select K exemplars that maximize similarity while ensuring label coverage?

## Architecture Onboarding

- **Component map:** Stage I (noisy labeling) -> ILP Solver (exemplar selection) -> Stage II (ICL with exemplars) -> LLM Interface (prompt formatting) -> Evaluation (metrics)

- **Critical path:** Stage I labeling of target test data → ILP exemplar selection for each test instance → Stage II ICL with selected exemplars → Output predictions and evaluate

- **Design tradeoffs:**
  - Using fine-tuned models vs. ICL for Stage I: Fine-tuned models are cheaper and provide confidence scores but may be less accurate; ICL is more accurate but expensive.
  - ILP complexity vs. random selection: ILP improves performance but adds computational overhead; random selection is faster but less effective.
  - Confidence thresholding: Improves exemplar quality but requires accessible logits, which may not be available for all LLMs.

- **Failure signatures:**
  - Poor Stage I performance: Leads to noisy exemplars that degrade Stage II results
  - ILP selecting redundant exemplars: Reduces diversity and harms performance
  - LLM sensitivity to prompt formatting: Incorrect templates cause performance drops
  - Label coverage constraint too strict: May force inclusion of low-quality exemplars

- **First 3 experiments:**
  1. Compare SSP with Stage I via fine-tuning vs. ICL on a single language pair to validate Stage I choices.
  2. Run ILP ablation (w/ vs. w/o confidence, w/ vs. w/o label coverage) on a single task to isolate component effects.
  3. Test random exemplar selection vs. ILP on a single language to quantify ILP's value.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SSP performance vary when using different similarity metrics for exemplar selection beyond cosine similarity of Ada-002 embeddings?
- Basis in paper: [inferred] The paper uses cosine similarity between Ada-002 embeddings for exemplar selection in the ILP, but doesn't explore alternative similarity metrics.
- Why unresolved: The paper focuses on demonstrating the effectiveness of SSP with a specific similarity metric rather than comparing different metrics.
- What evidence would resolve it: Experiments comparing SSP performance using different embedding models (e.g., multilingual BERT, LaBSE) or similarity measures (e.g., Euclidean distance, learned similarity metrics) would show if the choice of similarity metric significantly impacts results.

### Open Question 2
- Question: How does SSP's performance scale with increasing amounts of unlabeled target language data beyond what's used in the 0-CLT-U setting?
- Basis in paper: [inferred] The paper experiments with full Wikipedia data for target languages in 0-CLT-U but doesn't explore performance with larger unlabeled datasets or streaming data scenarios.
- Why unresolved: The paper establishes SSP's effectiveness with available unlabeled data but doesn't investigate performance boundaries as unlabeled data scales up.
- What evidence would resolve it: Experiments gradually increasing unlabeled target language data volume (e.g., 10x, 100x Wikipedia size) and measuring SSP performance would reveal scalability characteristics and potential diminishing returns.

### Open Question 3
- Question: What is the impact of SSP on languages with very limited source language data or distantly related source languages?
- Basis in paper: [inferred] The paper uses 2-4 source languages per task with varying relatedness to targets, but doesn't systematically analyze performance degradation with fewer or more distantly related sources.
- Why unresolved: The paper shows SSP works across diverse language families but doesn't isolate the effect of source language relatedness or quantity on target language performance.
- What evidence would resolve it: Experiments with controlled source language sets varying in relatedness (e.g., using only distantly related languages like French for African languages) and quantity would quantify how source language characteristics affect SSP's effectiveness.

## Limitations

- The ILP-based exemplar selection introduces computational overhead that may not scale well to very large test sets or when frequent inference is required.
- The method's effectiveness for languages with significantly different linguistic structures from the source languages remains untested, potentially limiting its generalizability to certain language families.
- The performance improvements depend on the quality of Stage I labeling - if Stage I produces highly inaccurate labels, the iterative refinement may fail to converge to better predictions.

## Confidence

**High Confidence**: The core claim that target-language exemplars improve ICL performance even with noisy labels is well-supported by ablation studies showing consistent gains across tasks and languages. The observation that SSP outperforms both fine-tuning and standard ICL baselines by 3-4 F1 points is robustly demonstrated.

**Medium Confidence**: The ILP formulation's specific contribution is somewhat confounded by the two-stage pipeline - while ablation studies show improvements, the exact contribution of each ILP component (similarity, confidence, label coverage) is not fully isolated. The claim about optimal noise thresholds for exemplar quality could benefit from more systematic exploration.

**Low Confidence**: The scalability analysis is limited - the paper reports 13 hours for full SSP inference on AmericasNLI but doesn't provide comprehensive runtime analysis across different hardware configurations or test set sizes. The performance on languages with typologically distant structures remains unexplored.

## Next Checks

1. **Scalability Benchmark**: Measure SSP's runtime and memory requirements on incrementally larger test sets (1K, 10K, 100K instances) across different LLM APIs (GPT-3.5, GPT-4, Claude) to establish practical deployment limits and identify bottlenecks.

2. **Noise Threshold Analysis**: Systematically vary the Stage I labeling noise level and measure SSP performance to empirically determine the "noise region" where target exemplars outperform source exemplars, validating the theoretical mechanism described in Mechanism 1.

3. **Linguistic Distance Stress Test**: Apply SSP to language pairs with increasing typological distance (e.g., English→Chinese, English→Arabic, English→Basque) to identify structural constraints on cross-lingual transfer effectiveness and determine when SSP's language alignment advantage diminishes.