---
ver: rpa2
title: 'Predictive Coding Networks and Inference Learning: Tutorial and Survey'
arxiv_id: '2407.04117'
source_url: https://arxiv.org/abs/2407.04117
tags:
- learning
- pcns
- inference
- predictive
- coding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive tutorial and survey of Predictive
  Coding Networks (PCNs) and Inference Learning (IL), presenting PCNs as a flexible
  framework that generalizes traditional artificial neural networks (ANNs). The authors
  demonstrate that PCNs, trained with IL, offer a biologically plausible alternative
  to backpropagation with potential advantages in efficiency and learning properties.
---

# Predictive Coding Networks and Inference Learning: Tutorial and Survey

## Quick Facts
- **arXiv ID**: 2407.04117
- **Source URL**: https://arxiv.org/abs/2407.04117
- **Reference count**: 40
- **One-line primary result**: PCNs trained with IL offer a biologically plausible alternative to backpropagation with potential advantages in efficiency and learning properties.

## Executive Summary
This paper provides a comprehensive tutorial and survey of Predictive Coding Networks (PCNs) and Inference Learning (IL), presenting PCNs as a flexible framework that generalizes traditional artificial neural networks (ANNs). The authors demonstrate that PCNs, trained with IL, offer a biologically plausible alternative to backpropagation with potential advantages in efficiency and learning properties. The work positions PCNs as a promising framework for future machine learning innovations, particularly in the emerging field of NeuroAI.

## Method Summary
The paper presents PCNs as a framework where activation nodes represent neural activity, error nodes represent prediction errors, and weights connect layers to define transformations. The energy function being minimized during learning serves as the objective function. PCNs use Inference Learning (IL) which performs local computations at each layer using only locally available information, enabling parallelization. The framework connects to probabilistic modeling through Expectation Maximization and variational inference, allowing PCNs to be understood as minimizing variational free energy.

## Key Results
- PCNs can be mathematically viewed as a superset of traditional feedforward neural networks, allowing for more diverse architectures including discriminative and generative models as well as arbitrary graph structures
- IL's local computations enable parallelization, potentially offering speed advantages over backpropagation especially in deep networks
- IL exhibits favorable learning properties including reduced weight interference and sensitivity to second-order information, leading to faster convergence and improved performance on tasks like continual learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Predictive Coding Networks (PCNs) can be understood as a superset of traditional feedforward neural networks (FNNs).
- Mechanism: PCNs use the same basic structure (layers, weights, activation functions) but extend it by allowing arbitrary graph topologies through Inference Learning (IL), which generalizes beyond the hierarchical constraints of FNNs.
- Core assumption: The equivalence between FNNs and discriminative PCNs during testing holds for all architectures.
- Evidence anchors:
  - [abstract] "PCNs can be mathematically considered a superset of traditional feedforward neural networks (FNNs)"
  - [section 2.7] "Given the result of (14) that FNNs are equivalent to discriminative PCNs during testing, we can see them as a subset of hierarchical PCNs"
  - [corpus] Weak evidence - no direct citations about this mathematical relationship in the corpus neighbors
- Break condition: If the equivalence during testing fails for certain activation functions or architectures, the superset claim breaks down.

### Mechanism 2
- Claim: IL's local computations enable parallelization, potentially offering speed advantages over backpropagation especially in deep networks.
- Mechanism: IL updates each layer using only locally available information (errors and activations from neighboring layers), avoiding the serial error propagation required in backpropagation.
- Core assumption: Sufficient parallelization can be achieved in practice to realize the theoretical time complexity benefits.
- Evidence anchors:
  - [abstract] "IL's local computations enable parallelization, potentially offering speed advantages over backpropagation especially in deep networks"
  - [section 2.3.4] "the computations in IL enjoy both temporal and spatial locality, meaning the computations in different layers during inference and learning could be parallelized"
  - [corpus] Weak evidence - no direct citations about parallelization performance in the corpus neighbors
- Break condition: If parallelization overhead exceeds theoretical gains, or if hardware limitations prevent efficient parallel implementation.

### Mechanism 3
- Claim: IL exhibits favorable learning properties including reduced weight interference and sensitivity to second-order information, leading to faster convergence and improved performance on tasks like continual learning.
- Mechanism: IL's prospective configuration mechanism allows it to "foresee" side effects of weight modifications by changing activation nodes before weight updates, reducing catastrophic interference. IL also approximates implicit SGD, which is sensitive to second-order curvature information.
- Core assumption: The prospective configuration and second-order sensitivity mechanisms generalize across different network architectures and datasets.
- Evidence anchors:
  - [abstract] "IL exhibits favorable learning properties including reduced weight interference and sensitivity to second-order information, leading to faster convergence and improved performance on tasks like continual learning"
  - [section 4.2] "decreased weight interference and sensitivity to second-order information, which can lead to faster convergence and increased performance on certain learning tasks"
  - [corpus] Weak evidence - no direct citations about these specific learning properties in the corpus neighbors
- Break condition: If these mechanisms fail for certain activation functions (e.g., ReLU) or network depths, the performance benefits may not materialize.

## Foundational Learning

- Concept: Expectation Maximization (EM) as the underlying optimization framework
  - Why needed here: EM provides the mathematical foundation for IL and explains why it minimizes the negative log-likelihood, connecting PCNs to probabilistic modeling
  - Quick check question: Can you explain how the E-step and M-step in EM correspond to the inference and learning phases in PCNs?

- Concept: Variational inference and the variational free energy
  - Why needed here: This perspective connects PCNs to modern ML methods like VAEs and explains how different choices of variational posteriors lead to different objective functions
  - Quick check question: How does the choice of delta function vs. Gaussian variational posterior affect the variational free energy and resulting learning algorithm?

- Concept: Probabilistic latent variable models and graphical models
  - Why needed here: This framework explains the generative nature of PCNs, the hierarchical structure, and connections to other methods like factor analysis and energy-based models
  - Quick check question: Can you draw the graphical model for a two-layer PCN and explain how it relates to factor analysis?

## Architecture Onboarding

- Component map: Activity nodes (ùíÇ‚Ñì) -> Error nodes (ùùê‚Ñì) -> Prediction nodes (ùùÅ‚Ñì) -> Weights (ùíò‚Ñì) -> Energy function (ùê∏)

- Critical path: Data ‚Üí Input layer ‚Üí Forward pass (predictions) ‚Üí Error computation ‚Üí Inference phase (activation updates) ‚Üí Learning phase (weight updates) ‚Üí Output

- Design tradeoffs:
  - Hierarchical vs. arbitrary graph topologies: hierarchical structures are simpler but arbitrary graphs offer more flexibility
  - Number of inference iterations (T): more iterations can improve accuracy but increase computation time
  - Learning rate vs. inference rate: different rates may be needed for optimal performance

- Failure signatures:
  - Divergence during inference: indicates learning/inference rates are too high
  - Poor convergence to local minima: suggests need for better initialization or optimizer choice
  - Performance gap with BP: indicates issues with the specific PCN implementation or hyperparameters

- First 3 experiments:
  1. Implement a simple discriminative PCN for MNIST classification and compare performance with a standard FNN trained with BP
  2. Test the effect of different numbers of inference iterations on training convergence and final accuracy
  3. Implement a generative PCN and evaluate its sampling ability compared to a standard VAE on the same dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Claims about PCNs being a superset of traditional neural networks lack extensive empirical validation across diverse architectures
- Purported advantages in learning speed and reduced interference are primarily theoretical, with limited benchmark comparisons to established methods
- Biological plausibility claims remain largely speculative without direct neuroscientific validation

## Confidence

- **High confidence**: Mathematical derivations of PCN framework and its relationship to EM optimization
- **Medium confidence**: Claims about PCNs as a generalization of traditional ANNs and connections to other ML methods
- **Low confidence**: Empirical claims about performance advantages and biological plausibility without extensive validation

## Next Checks

1. Implement and benchmark PCNs on standard deep learning tasks (MNIST, CIFAR-10) comparing convergence speed and final accuracy against backpropagation-trained networks
2. Test the generalization claim by implementing non-hierarchical PCN architectures and verifying their equivalence to traditional networks during inference
3. Evaluate the learning rate advantage claim by measuring training dynamics and interference patterns in continual learning scenarios with varying task sequences