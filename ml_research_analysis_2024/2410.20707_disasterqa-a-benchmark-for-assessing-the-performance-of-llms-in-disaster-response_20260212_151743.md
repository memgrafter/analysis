---
ver: rpa2
title: 'DisasterQA: A Benchmark for Assessing the performance of LLMs in Disaster
  Response'
arxiv_id: '2410.20707'
source_url: https://arxiv.org/abs/2410.20707
tags:
- disaster
- answer
- llms
- prompting
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DisasterQA, the first open-source benchmark
  to assess LLMs in disaster response, created from 707 multiple-choice questions
  sourced from six online repositories. It evaluates five LLMs (GPT-3.5 Turbo, GPT-4
  Turbo, GPT-4o, Gemini 1.5 Flash, Llama 3.1-8B) using four prompting methods, measuring
  accuracy and confidence via Logprobs.
---

# DisasterQA: A Benchmark for Assessing the performance of LLMs in Disaster Response

## Quick Facts
- arXiv ID: 2410.20707
- Source URL: https://arxiv.org/abs/2410.20707
- Authors: Rajat Rawat
- Reference count: 40
- Primary result: GPT-4o achieved highest accuracy (85.78%) on disaster response questions using Chain of Thought prompting

## Executive Summary
This paper introduces DisasterQA, the first open-source benchmark designed to evaluate large language models (LLMs) in disaster response scenarios. The benchmark consists of 707 multiple-choice questions drawn from six online repositories, covering six different disaster types. The study evaluates five prominent LLMs—GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o, Gemini 1.5 Flash, and Llama 3.1-8B—using four prompting methods and measures both accuracy and confidence levels.

The results demonstrate that while LLMs show promising performance with accuracy ranging from 70-85%, there remains significant room for improvement before they can be reliably deployed in critical disaster response situations. GPT-4o emerged as the top performer, achieving 85.78% accuracy with Chain of Thought prompting, while other models showed varying degrees of success. The study also reveals important differences in model confidence levels and safety filtering behaviors, with Gemini having the highest number of unanswered questions due to safety protocols.

## Method Summary
The study creates a benchmark dataset of 707 multiple-choice questions sourced from six online repositories, covering six disaster types. Five LLMs are evaluated using four prompting methods: Standard, Chain of Thought, Few-Shot, and Self-Consistency. Performance is measured through accuracy metrics and confidence scores derived from Logprobs, which indicate the models' certainty in their responses. The evaluation focuses on comparing model performance across different prompting strategies and analyzing patterns in confidence versus accuracy.

## Key Results
- GPT-4o achieved the highest accuracy at 85.78% using Chain of Thought prompting
- All evaluated models averaged 70-85% accuracy across the benchmark
- Llama showed high confidence levels despite lower accuracy scores
- Gemini had the most unanswered questions due to safety filtering mechanisms

## Why This Works (Mechanism)
The benchmark works by providing standardized multiple-choice questions that can be consistently evaluated across different LLMs and prompting methods. The use of Logprobs as a confidence metric allows for quantitative comparison of model certainty alongside accuracy, enabling researchers to identify when high confidence may not correlate with correct responses. This systematic approach facilitates objective comparison of model performance in a controlled disaster response context.

## Foundational Learning
The study builds upon established benchmarking practices for evaluating LLMs in domain-specific applications. By focusing on disaster response, it extends the application of LLM evaluation beyond traditional natural language processing tasks to a critical real-world domain where accurate information retrieval and decision-making can have life-or-death consequences. The multi-model, multi-prompting approach provides insights into how different architectures and prompting strategies perform in specialized contexts.

## Architecture Onboarding
The benchmark can be easily integrated into existing LLM evaluation pipelines, requiring only the ability to submit multiple-choice questions and receive both answers and Logprob scores. The standardized format allows researchers to quickly assess new models or prompting strategies without needing to develop custom evaluation frameworks. The inclusion of various disaster types ensures broad coverage while maintaining question consistency across evaluations.

## Open Questions the Paper Calls Out
- How well do benchmark performance metrics translate to real-world disaster response effectiveness
- Whether the multiple-choice format adequately captures the complexity of disaster response decision-making
- How different cultural and regional contexts might affect model performance on disaster response tasks
- Whether the current confidence metrics truly reflect decision-making reliability in critical situations

## Limitations
- Benchmark covers only six disaster types and relies on online repositories rather than real-world operational scenarios
- Evaluation is limited to multiple-choice format questions, missing complexity of open-ended disaster response decision-making
- Logprob confidence metric may not directly translate to real-world trust or decision-making utility
- Safety filtering behaviors observed may vary across different deployment contexts and safety parameter settings

## Confidence
- High confidence in model performance rankings and accuracy percentages based on controlled benchmark conditions
- Medium confidence in claims about real-world applicability due to simplified question format and limited disaster types
- Low confidence in assertions about model safety behaviors based solely on unanswered question counts without deeper analysis of underlying reasons

## Next Checks
1. Test benchmark questions with domain experts in disaster response to validate question relevance and difficulty calibration
2. Expand evaluation to include open-ended response formats and time-constrained scenarios reflecting real disaster response conditions
3. Conduct cross-cultural validation by translating questions into multiple languages and testing with region-specific fine-tuned LLMs