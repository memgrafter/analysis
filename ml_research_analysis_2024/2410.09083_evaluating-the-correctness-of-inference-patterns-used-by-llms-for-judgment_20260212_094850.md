---
ver: rpa2
title: Evaluating the Correctness of Inference Patterns Used by LLMs for Judgment
arxiv_id: '2410.09083'
source_url: https://arxiv.org/abs/2410.09083
tags:
- interaction
- legal
- phrases
- judgment
- interactions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to evaluate the correctness of detailed
  inference patterns used by Large Language Models (LLMs) for judgment, using legal
  LLMs as a case study. The method quantifies interactions between input phrases as
  primitive inference patterns, based on recent theoretical guarantees of faithfulness
  in interaction-based explanations.
---

# Evaluating the Correctness of Inference Patterns Used by LLMs for Judgment

## Quick Facts
- **arXiv ID**: 2410.09083
- **Source URL**: https://arxiv.org/abs/2410.09083
- **Reference count**: 40
- **Key outcome**: Even when LLMs generate correct outputs, over half of underlying inference patterns are unreliable, often relying on irrelevant phrases or incorrect entity actions

## Executive Summary
This paper introduces a method to evaluate the correctness of inference patterns used by Large Language Models (LLMs) for judgment tasks, using legal LLMs as a case study. The approach quantifies interactions between input phrases as primitive inference patterns, leveraging recent theoretical guarantees of faithfulness in interaction-based explanations. Experiments reveal that even when LLMs generate correct outputs, a significant portion of underlying inference patterns represent misleading or irrelevant logic, highlighting a critical yet overlooked problem in LLM reliability for high-stakes applications.

## Method Summary
The method decomposes an LLM's output into interaction patterns using AND-OR logical models with universal matching properties. Interaction effects between input phrases are extracted through a mathematically proven sparse decomposition that ensures faithfulness. Human domain experts annotate input phrases as relevant, irrelevant, or forbidden, allowing the method to identify unreliable inference patterns. The approach quantifies the ratio of reliable interaction effects and analyzes their complexity to reveal problematic reasoning patterns in LLMs.

## Key Results
- Over half of interactions in legal LLMs represent unreasonable or incorrect justifications
- General-purpose and legal-domain-specific LLMs both exhibit insufficient reliable interaction effects
- LLMs frequently rely on semantically irrelevant phrases, incorrect entity actions, or identity-based discrimination
- Reliable interaction effects (s_reliable) vary significantly across different LLM architectures and legal cases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The method can faithfully decompose an LLM's output into interaction patterns that reflect the model's reasoning process.
- **Mechanism:** Uses AND-OR logical models with universal matching properties to represent LLM output scores as sums of interaction effects between input phrases.
- **Core assumption:** Interaction-based explanation theory guarantees that DNN output can be faithfully represented by small numbers of interaction patterns between input variables.
- **Evidence anchors:** [abstract] "recent theoretical achievements have proven several mathematical guarantees of the faithfulness of the interaction-based explanation"; [section 2.1] Theorem 1 proves universal matching property.
- **Break condition:** If LLM reasoning involves higher-order dependencies or non-local patterns that cannot be captured by pairwise or small-group interactions.

### Mechanism 2
- **Claim:** The method can identify unreliable inference patterns by comparing interaction effects against human domain knowledge.
- **Mechanism:** Annotates input phrases as relevant, irrelevant, or forbidden, then decomposes interaction effects into reliable and unreliable components.
- **Core assumption:** Human legal experts can accurately identify which phrases should and should not influence specific legal judgments.
- **Evidence anchors:** [abstract] "we propose to evaluate the correctness of the detailed inference patterns of an LLM behind its seemingly correct outputs"; [section 2.2] describes manual partitioning by 16 legal experts.
- **Break condition:** If human annotation contains significant bias or errors, or if legal experts disagree substantially on phrase classifications.

### Mechanism 3
- **Claim:** The method reveals that LLMs often rely on problematic inference patterns even when generating correct outputs.
- **Mechanism:** Quantifies ratio of reliable interaction effects and analyzes interaction complexity to show frequent use of irrelevant phrases or incorrect entity actions.
- **Core assumption:** Ratio of reliable interaction effects is a valid metric for assessing quality of LLM's reasoning process.
- **Evidence anchors:** [abstract] "Experiments reveal that even when LLMs generate correct outputs, a significant portion of the underlying inference patterns represent misleading or irrelevant logic"; [section 3.1] shows over half of interactions are unreasonable.
- **Break condition:** If LLM reasoning process is fundamentally non-interpretable or interaction patterns don't capture true reasoning mechanisms.

## Foundational Learning

- **Concept: Interaction-based explanation theory**
  - Why needed here: Provides mathematical foundation for decomposing LLM outputs into interpretable interaction patterns
  - Quick check question: What is the universal matching property in interaction-based explanation theory, and why is it important for this method?

- **Concept: Shapley value and Harsanyi dividend**
  - Why needed here: Used to compute interaction effects between input variables from cooperative game theory
  - Quick check question: How does the Harsanyi dividend relate to AND interactions in the context of DNN explanations?

- **Concept: Legal case structure and judgment reasoning**
  - Why needed here: Understanding legal judgment making is crucial for annotating relevant, irrelevant, and forbidden phrases
  - Quick check question: What distinguishes a relevant phrase from an irrelevant or forbidden phrase in legal case analysis?

## Architecture Onboarding

- **Component map:** Legal case text → tokenized input phrases → LLM inference → Compute output scores for all 2^n masked states → Extract AND-OR interactions using optimization → Human annotation of phrase types → Decompose interactions into reliable/unreliable components → Compute reliability metrics

- **Critical path:** 1) Input legal case text, 2) Generate all 2^n masked samples, 3) Compute LLM output scores for each masked sample, 4) Extract AND-OR interactions using optimization, 5) Human annotation of phrase types, 6) Decompose interactions into reliable/unreliable components, 7) Compute reliability metrics

- **Design tradeoffs:**
  - Computational cost vs. interaction complexity: More input phrases exponentially increase computation time
  - Annotation quality vs. scalability: Expert annotation ensures quality but limits scalability
  - Granularity of phrase selection vs. interpretability: Finer granularity may capture more nuanced interactions but could reduce interpretability

- **Failure signatures:**
  - Low s_reliable values across multiple legal cases indicate systematic reasoning issues
  - High correlation between forbidden phrases and interaction effects suggests entity-matching problems
  - Consistent reliance on low-order interactions may indicate lack of sophisticated reasoning

- **First 3 experiments:**
  1. Verify universal matching property: Test if AND-OR model accurately reproduces LLM outputs across all masked states
  2. Validate phrase annotation: Have multiple legal experts independently annotate same legal cases to assess inter-rater reliability
  3. Baseline comparison: Compare s_reliable values across different LLM architectures (general vs. legal-specific) on same legal cases

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: How does the proposed method handle cases where the input legal case contains multiple defendants with conflicting actions?
  - Basis in paper: [explicit] The paper mentions that input phrases describing incorrect defendant actions should be annotated as forbidden phrases.
  - Why unresolved: The paper does not provide specific examples or methodology for handling cases with multiple defendants and conflicting actions.
  - What evidence would resolve it: A case study demonstrating the method's application to a legal case with multiple defendants and conflicting actions.

- **Open Question 2**
  - Question: What is the computational complexity of the proposed method when applied to large-scale legal cases with many input phrases?
  - Basis in paper: [explicit] The paper states that computational cost increases exponentially with the number of input phrases.
  - Why unresolved: The paper does not provide specific time or memory requirements for different input sizes.
  - What evidence would resolve it: Experimental results showing computation time and memory usage for varying numbers of input phrases.

- **Open Question 3**
  - Question: How sensitive is the method to the quality and consistency of human annotations for relevant, irrelevant, and forbidden phrases?
  - Basis in paper: [explicit] The paper relies on 16 legal experts and volunteers to annotate phrases using majority voting.
  - Why unresolved: The paper does not analyze impact of annotation quality on final results or provide inter-annotator agreement metrics.
  - What evidence would resolve it: Analysis of how varying annotation quality affects reliability metrics, including inter-annotator agreement statistics.

## Limitations
- Computational complexity increases exponentially with number of input phrases, limiting applicability to shorter legal cases
- Reliance on human annotation introduces uncertainty regarding inter-rater reliability and potential domain knowledge biases
- Assumption that interaction-based explanations faithfully capture LLM reasoning may not hold for models with complex, non-local dependencies

## Confidence

- **High Confidence:** Experimental results demonstrating over half of LLM interactions are unreliable are supported by multiple datasets and LLM architectures
- **Medium Confidence:** Theoretical framework of interaction-based explanations is mathematically sound, but practical applicability to LLM reasoning remains to be fully validated
- **Low Confidence:** Generalizability of method beyond legal judgment tasks to other high-stakes domains is uncertain without further testing

## Next Checks
1. **Inter-rater Reliability Analysis:** Conduct formal inter-rater reliability study with 16 legal experts to quantify annotation consistency and identify sources of disagreement in phrase classification
2. **Cross-domain Validation:** Apply method to LLM outputs in other high-stakes domains (e.g., medical diagnosis, financial auditing) to test generalizability beyond legal judgment tasks
3. **Scalability Assessment:** Implement dimensionality reduction techniques or approximation methods to test method's performance on longer legal cases and quantify trade-off between interaction complexity and computational feasibility