---
ver: rpa2
title: 'Time Distributed Deep Learning Models for Purely Exogenous Forecasting: Application
  to Water Table Depth Prediction using Weather Image Time Series'
arxiv_id: '2409.13284'
source_url: https://arxiv.org/abs/2409.13284
tags:
- time
- data
- water
- layer
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the challenge of predicting water table depth
  in the Grana-Maira catchment (Piedmont, Italy) using only exogenous weather image
  time series, due to irregular and outdated groundwater data availability. Two deep
  learning models are proposed: TDC-LSTM, combining a Time Distributed CNN (TDC) with
  an LSTM for temporal modeling, and TDC-UnPWaveNet, which replaces the LSTM with
  a novel Unpadded WaveNet (UnPWaveNet) adapted for many-to-one forecasting tasks.'
---

# Time Distributed Deep Learning Models for Purely Exogenous Forecasting: Application to Water Table Depth Prediction using Weather Image Time Series

## Quick Facts
- arXiv ID: 2409.13284
- Source URL: https://arxiv.org/abs/2409.13284
- Reference count: 40
- Two deep learning models (TDC-LSTM and TDC-UnPWaveNet) achieve high accuracy predicting water table depth using only exogenous weather image time series

## Executive Summary
This study addresses the challenge of predicting water table depth in the Grana-Maira catchment (Piedmont, Italy) using only exogenous weather image time series, due to irregular and outdated groundwater data availability. Two deep learning models are proposed: TDC-LSTM, combining a Time Distributed CNN (TDC) with an LSTM for temporal modeling, and TDC-UnPWaveNet, which replaces the LSTM with a novel Unpadded WaveNet (UnPWaveNet) adapted for many-to-one forecasting tasks. Both models encode spatial features from weather images via the TDC module and differ in temporal sequence modeling. A new Channel Distributed (CD) layer is introduced to handle variable sequence lengths in the UnPWaveNet. Experiments show both models achieve high accuracy, with TDC-LSTM excelling in reducing bias and TDC-UnPWaveNet in capturing temporal dynamics.

## Method Summary
The study develops two deep learning architectures for predicting weekly water table depth at three sensors using only weather image time series. Both models share a Time Distributed Convolutional Neural Network (TDC) that processes each weekly weather image independently, extracting spatial features through a 4-layer CNN with max pooling. The TDC-LSTM model uses an LSTM layer for temporal modeling, while TDC-UnPWaveNet employs a modified WaveNet architecture (UnPWaveNet) with unpadded dilated convolutions and Channel Distributed layers. The models are trained on 2-year weekly weather data (precipitation, max/min temperature) plus monthly one-hot encoding, with target values from three groundwater sensors. Training uses L2 regularization, clipnorm=1.0, SGD with momentum/Nesterov, and MSE loss over 80 epochs with batch size 8.

## Key Results
- Both models achieve high accuracy with mean BIAS of -0.18(0.05) and -0.25(0.19), and Pearson correlation ρ of 0.93(0.03) and 0.96(0.01) for TDC-LSTM and TDC-UnPWaveNet, respectively, across all sensors
- TDC-LSTM excels in reducing bias while TDC-UnPWaveNet better captures temporal dynamics
- Purely exogenous weather image data can effectively predict groundwater depth without endogenous measurements
- UnPWaveNet offers a competitive alternative to recurrent architectures for time series forecasting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The TDC module learns spatial relationships from weather image time series by applying the same CNN operations to each frame, producing a fixed-length vector representation.
- Mechanism: Each frame is processed by a 4-layer CNN with max pooling, reducing spatial dimensions while increasing channel depth. This encodes spatial patterns into a 16-dimensional vector per time step.
- Core assumption: Spatial patterns in weather images are consistent across time steps and can be extracted with the same CNN parameters.
- Evidence anchors:
  - [abstract] "Both models are made of a first Time Distributed Convolutional Neural Network (TDC) which encodes the image available at each time step into a vectorial representation."
  - [section] "The TDC module is responsible for learning a vectorial representation of the images available at each time step."
- Break condition: If weather patterns vary significantly across time steps or require different spatial feature extraction at different times.

### Mechanism 2
- Claim: The UnPWaveNet architecture adapts WaveNet for many-to-one forecasting by removing causal padding and using Channel Distributed layers to handle output sequence compression.
- Mechanism: Dilated convolutions without causal padding allow the network to learn dependencies across the entire input sequence. Channel Distributed layers then compress the temporal dimension while preserving channel information.
- Core assumption: The output sequence can be completely shifted in the future and shorter than the input, making causal constraints unnecessary.
- Evidence anchors:
  - [abstract] "TDC-UnPWaveNet uses instead a new version of the WaveNet architecture, adapted here to output a sequence shorter and completely shifted in the future with respect to the input one."
  - [section] "Two problems arise when removing the causal padding constraint from the initial network architecture: it is a) no longer possible to add residual connections; and b) no longer possible to concatenate skip connections."
- Break condition: If temporal dependencies require maintaining the causal structure or if output sequence length cannot be predicted accurately.

### Mechanism 3
- Claim: The Channel Distributed layer processes each channel independently while maintaining temporal relationships, effectively translating the Time Distributed concept to the channel dimension.
- Mechanism: Each univariate time series (channel) is processed by the same fully connected cell, allowing for temporal compression or expansion while preserving channel-wise information.
- Core assumption: Each weather variable's temporal dynamics can be processed independently using the same transformation.
- Evidence anchors:
  - [section] "The CD layer applies the same transformations to each channel individually (i.e. a translation of the concept of Time Distributed layer to channels)."
  - [section] "The CD layer, implemented in the UnPWaveNet with a fully connected cell, has proved to be efficient and effective."
- Break condition: If weather variables have highly interdependent temporal dynamics that cannot be captured independently.

## Foundational Learning

- Concept: Time Distributed layers apply the same operations to each time step in a sequence.
  - Why needed here: The TDC module needs to process each weather image frame independently while maintaining temporal ordering.
  - Quick check question: How does a Time Distributed layer differ from a standard layer in terms of parameter sharing across time steps?

- Concept: Dilated convolutions exponentially expand receptive fields without increasing parameters.
  - Why needed here: The UnPWaveNet uses dilated convolutions to capture long-term dependencies in the weather time series data.
  - Quick check question: What is the receptive field size of a 5-layer dilated convolution with kernel size 4 and dilations 1, 2, 4, 8, 16?

- Concept: Causal padding ensures predictions depend only on past and present inputs.
  - Why needed here: The original WaveNet uses causal padding for audio generation, but this study removes it for many-to-one forecasting.
  - Quick check question: Why would causal padding be inappropriate for a many-to-one forecasting task?

## Architecture Onboarding

- Component map: TDC → Sequential Module (LSTM or UnPWaveNet) → Output layer
- Critical path: Weather images → TDC encoding → Temporal modeling → Water table depth prediction
- Design tradeoffs: TDC-LSTM offers better bias control but TDC-UnPWaveNet captures temporal dynamics better
- Failure signatures: High bias indicates TDC-LSTM underperforming; poor correlation suggests UnPWaveNet issues
- First 3 experiments:
  1. Train TDC-LSTM with minimal hyperparameters to verify basic functionality
  2. Test UnPWaveNet with reduced dilation rates to understand temporal modeling
  3. Compare TDC module output dimensions with expected vector representations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the TDC-UnPWaveNet compare to other deep learning architectures like Transformers when applied to groundwater depth prediction?
- Basis in paper: [explicit] The paper mentions that Transformers were not included in the comparison due to the significant effort already invested in developing the proposed models.
- Why unresolved: The paper focuses on comparing TDC-LSTM and TDC-UnPWaveNet but does not include Transformers, which are a newer and potentially competitive architecture for sequential data.
- What evidence would resolve it: Empirical results comparing TDC-UnPWaveNet to Transformer-based models on the same groundwater depth prediction task would provide a direct answer.

### Open Question 2
- Question: To what extent would incorporating anthropogenic pressure data (e.g., water abstraction, irrigation) improve the predictive accuracy of the proposed models, especially during drought conditions?
- Basis in paper: [explicit] The paper notes that anthropogenic pressure data were not included due to their scarcity in the region, but suggests that such data could enhance performance, particularly in anomalous scenarios like the 2022 drought.
- Why unresolved: The study explicitly excludes anthropogenic data due to availability issues, leaving the potential benefit of such data untested.
- What evidence would resolve it: Re-training the models with available anthropogenic data (if obtained) and comparing performance metrics to the original models would demonstrate the impact.

### Open Question 3
- Question: How does the introduction of gaps between training, validation, and test sets affect the unbiasedness of performance metrics in time series tasks where lagged features are used?
- Basis in paper: [explicit] The paper introduces a gap of T time steps between sets to prevent data leakage, arguing that this is necessary even when autoregressive terms are not used, due to overlapping predictor usage.
- Why unresolved: While the paper implements this gap, it does not empirically validate whether this approach significantly improves the unbiasedness of performance metrics compared to overlapping sets.
- What evidence would resolve it: Comparing performance metrics (e.g., NSE, KGE) from models trained with and without gaps would quantify the impact of this methodological choice.

## Limitations

- Data availability and preprocessing constraints limit reproducibility, as the study relies on specific weather image time series and groundwater depth measurements that may not be publicly accessible
- The reported performance improvements may be sensitive to specific hyperparameter choices that are not fully disclosed in the main text
- The relatively short time span of available data (2-year training period) raises questions about long-term forecasting reliability

## Confidence

- High confidence: The core architectural innovations (Time Distributed CNN, Unpadded WaveNet adaptation) are clearly specified and technically sound
- Medium confidence: Performance metrics and comparative results are well-documented, but the small number of sensors (3) and limited test period constrain generalizability claims
- Low confidence: The claim that purely exogenous weather image data alone can effectively predict groundwater depth without any endogenous measurements requires further validation across different catchments

## Next Checks

1. **Cross-catchment validation**: Apply the models to groundwater depth prediction in at least two additional catchments with different geological and climatic characteristics to test generalizability
2. **Temporal robustness test**: Evaluate model performance when trained on data from non-consecutive time periods (e.g., 2010-2016 and 2020-2022) to assess sensitivity to temporal patterns
3. **Ablation study**: Systematically remove components of the TDC module (different CNN layers, max pooling) and temporal modeling (LSTM vs UnPWaveNet) to quantify their individual contributions to performance