---
ver: rpa2
title: Generics are puzzling. Can language models find the missing piece?
arxiv_id: '2412.11318'
source_url: https://arxiv.org/abs/2412.11318
tags:
- generics
- generic
- language
- sentences
- most
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the semantics of generic sentences and
  their implicit quantification using language models. The authors create CONGEN,
  a dataset of 2,873 naturally occurring generic and quantified sentences in context,
  and introduce p-acceptability, a metric based on surprisal that is sensitive to
  quantification.
---

# Generics are puzzling. Can language models find the missing piece?

## Quick Facts
- arXiv ID: 2412.11318
- Source URL: https://arxiv.org/abs/2412.11318
- Reference count: 40
- This paper investigates generic sentence semantics using language models and introduces p-acceptability metric showing generics are more context-sensitive than quantifier phrases

## Executive Summary
This paper addresses the puzzle of generic sentences - statements about categories that lack explicit quantifiers - by investigating their semantics through language models. The authors create CONGEN, a dataset of 2,873 naturally occurring generic and quantified sentences in context, and introduce p-acceptability, a surprisal-based metric sensitive to quantification. Their experiments reveal that generics are significantly more context-sensitive than determiner quantifiers and that approximately 20% of naturally occurring generics express weak generalizations. The study also uncovers how human biases in stereotypes manifest in language models, with negative stereotypes predominantly quantified as universals while positive stereotypes are most commonly quantified as "most".

## Method Summary
The authors developed CONGEN, a dataset of 2,873 naturally occurring generic and quantified sentences extracted from news articles, focusing on minority group mentions. They employed length-based heuristics to identify generics versus quantified sentences and extracted minority group mentions using demographic-specific nouns and verbs. The p-acceptability metric was introduced, calculated as 1 - (surprisal with true quantifier - surprisal with false quantifier), to measure how well language models can distinguish between different quantifier types. Experiments were conducted using GPT-3.5 and GPT-4 to analyze the context-sensitivity of generics versus determiner quantifiers and to examine how stereotypes are implicitly quantified in language models.

## Key Results
- Generics are significantly more context-sensitive than determiner quantifiers (p-acceptability: 0.69 vs 0.99)
- Approximately 20% of naturally occurring generics analyzed express weak generalizations
- Negative stereotypes are predominantly implicitly quantified as universals while positive stereotypes are most commonly quantified as "most"

## Why This Works (Mechanism)
The p-acceptability metric works by measuring surprisal differences between true and false quantifiers in context, effectively capturing how strongly a quantifier fits within a given sentence environment. This approach succeeds because language models inherently encode distributional statistics about how quantifiers typically appear with different predicates and subjects. The metric's effectiveness stems from its ability to quantify the degree of contextual fit, which directly relates to the semantic acceptability of different quantifier types in specific contexts.

## Foundational Learning
- **Generic sentences**: Statements about categories without explicit quantifiers (e.g., "Birds fly") - needed to understand the core linguistic phenomenon being studied
- **Quantifier semantics**: How expressions like "all", "most", "some" specify quantities - crucial for distinguishing generics from quantified statements
- **Surprisal in language models**: How unexpected language is, measured via probability - fundamental to the p-acceptability metric
- **Context-sensitivity**: How meaning varies based on surrounding text - key to understanding why generics differ from explicit quantifiers
- **Implicit quantification**: When quantification is suggested but not stated - central to how generics function
- **Stereotype representation**: How biases about groups are encoded in language - relevant for analyzing minority group generics

Quick check: Can you identify whether "Sharks attack swimmers" is a generic or quantified statement, and explain why this distinction matters?

## Architecture Onboarding
**Component Map**: CONGEN dataset creation -> p-acceptability metric computation -> Context-sensitivity experiments -> Stereotype analysis -> Interpretation

**Critical Path**: The p-acceptability metric computation is central, as it enables both the context-sensitivity comparison between generics and quantifiers, and the analysis of stereotype quantification patterns.

**Design Tradeoffs**: The authors chose to use automated heuristics for dataset creation rather than manual annotation, trading potential noise for scalability and natural occurrence. They opted for surprisal-based metrics over human judgments for efficiency, though this may miss nuanced semantic distinctions.

**Failure Signatures**: If the dataset contains too many false positives in generic identification, the context-sensitivity results may be artificially inflated. If the p-acceptability metric is not truly sensitive to quantification differences, the core comparison between generics and quantifiers would be invalid.

**First Experiments**:
1. Validate p-acceptability metric on controlled examples with known quantifier types
2. Test context-sensitivity of generics versus quantifiers on a small manually annotated subset
3. Verify minority group extraction accuracy on sample sentences

## Open Questions the Paper Calls Out
- How do the findings generalize across different languages and cultural contexts?
- What is the relationship between the p-acceptability metric and human acceptability judgments?
- How do different model architectures compare in their handling of generic versus quantified sentences?

## Limitations
- Dataset creation relies heavily on automated heuristics which may introduce noise or systematic biases
- Findings are limited to English and specific model families (GPT-3.5, GPT-4)
- The 20% weak generalization rate is based on a single dataset and may not reflect broader linguistic patterns

## Confidence
- Generics are more context-sensitive than determiner quantifiers: Medium
- Approximately 20% of generics express weak generalizations: Medium
- Negative stereotypes as universals, positive as "most": Medium

## Next Checks
1. Cross-linguistic validation: Test whether the context-sensitivity of generics and the 20% weak generalization rate hold across languages beyond English
2. External validity test: Apply p-acceptability metric to independently constructed generic datasets to verify consistency
3. Human annotation verification: Conduct human judgment studies to validate the automated categorization of generics and stereotype quantification patterns identified by the models