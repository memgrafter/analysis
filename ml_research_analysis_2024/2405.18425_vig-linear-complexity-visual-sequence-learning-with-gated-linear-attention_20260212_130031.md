---
ver: rpa2
title: 'ViG: Linear-complexity Visual Sequence Learning with Gated Linear Attention'
arxiv_id: '2405.18425'
source_url: https://arxiv.org/abs/2405.18425
tags:
- arxiv
- attention
- linear
- vision
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ViG, a vision backbone leveraging Gated Linear
  Attention (GLA) to achieve global receptive field with linear complexity. The core
  innovation is bidirectional GLA with direction-wise gating to capture 1D global
  context, plus 2D gating locality injection to integrate local spatial details.
---

# ViG: Linear-complexity Visual Sequence Learning with Gated Linear Attention

## Quick Facts
- arXiv ID: 2405.18425
- Source URL: https://arxiv.org/abs/2405.18425
- Reference count: 40
- Key outcome: ViG achieves state-of-the-art performance on ImageNet, COCO, and ADE20K with linear complexity through bidirectional Gated Linear Attention

## Executive Summary
ViG introduces a vision backbone that leverages Gated Linear Attention (GLA) to achieve global receptive field with linear complexity. The core innovation is bidirectional GLA with direction-wise gating to capture 1D global context, plus 2D gating locality injection to integrate local spatial details. A hardware-aware implementation merges forward/backward scanning into a single kernel, improving parallelism and reducing memory. ViG outperforms state-of-the-art models on ImageNet classification, COCO object detection, and ADE20K segmentation, offering favorable accuracy-parameter-FLOPs trade-offs.

## Method Summary
ViG uses a vision transformer-like architecture with patch embeddings, position embeddings, and multiple blocks containing RMSNorm, BiGLA layer, 2D gating locality injection, and SwiGLU Feed Forward Network. The key innovation is the Bidirectional Gated Linear Attention layer that shares parameters between forward and backward passes while maintaining separate forget gates for each direction. This is combined with a 2D gating mechanism that merges local spatial details with global context. The hardware-aware implementation fuses forward and backward operations into a single kernel to reduce memory transfers and improve parallel execution.

## Key Results
- ViG-S matches DeiT-B accuracy with 27% of the parameters and 20% of the FLOPs, running 2× faster
- ViG-T achieves 20.7% higher accuracy than DeiT-T at high resolution with 5.2× fewer FLOPs and 90% less memory
- Outperforms state-of-the-art models on ImageNet classification, COCO object detection, and ADE20K segmentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bidirectional Gated Linear Attention (BiGLA) with direction-wise gating achieves global receptive field with only linear complexity overhead.
- Mechanism: By sharing all parameters except the forget gate between forward and backward passes, BiGLA compresses both directions into fixed-size hidden states. The direction-wise gating controls information flow from each direction independently, allowing the merged output to capture global context along the 1D sequence.
- Core assumption: Information from different visual directions varies significantly in importance, and a shared parameterization with separate forget gates is sufficient to model this directional sensitivity.
- Evidence anchors: [abstract] "We propose direction-wise gating to capture 1D global context through bidirectional modeling"; [section] "To harness this directional sensitivity effectively, we introduce the Bidirectional Gated Linear Attention (BiGLA) layer, as shown in Fig. 3. This layer is parameter-efficient by sharing all parameters except for the forget gate, which is tailored to each direction"

### Mechanism 2
- Claim: Hardware-aware implementation merging forward and backward scanning into a single kernel reduces memory and latency while improving parallelism.
- Mechanism: The parameter-efficient BiGLA design allows fusion of forward and backward operations into one kernel, eliminating the need to materialize the backward visual sequence in HBM. This reduces memory transfers and enables better utilization of tensor cores.
- Core assumption: The fused kernel can be efficiently scheduled on hardware without introducing computational bottlenecks, and the memory savings from avoiding backward sequence materialization outweigh any implementation complexity.
- Evidence anchors: [abstract] "Our hardware-aware implementation further merges forward and backward scanning into a single kernel, enhancing parallelism and reducing memory cost and latency"; [section] "we propose a hardware-aware bidirectional design by fusing the forward and backward directions of Eq. (6) into a single kernel to achieve higher parallelism"

### Mechanism 3
- Claim: 2D gating locality injection adaptively combines 1D global context with 2D local details through learned gating.
- Mechanism: After extracting 1D global context via BiGLA, 3×3 depthwise convolution captures 2D local features. A data-dependent gating mechanism then learns to blend these complementary representations, with the gate learned from the local features themselves.
- Core assumption: The visual information captured by 1D global context and 2D local convolution is complementary, and a simple gating mechanism can effectively learn the optimal combination for each spatial location.
- Evidence anchors: [abstract] "a 2D gating locality injection to adaptively inject 2D local details into 1D global context"; [section] "To address this issue, we inject 2D locality by introducing a short-term local convolution layer... we propose a data-dependent gating aggregation for 2D locality injection"

## Foundational Learning

- Concept: Linear attention mechanism replacing softmax attention
  - Why needed here: Enables linear complexity O(N) instead of quadratic O(N²), crucial for handling high-resolution images
  - Quick check question: How does linear attention approximate softmax attention using feature map dot-products instead of exponentials?

- Concept: State space models and their relationship to RNNs
  - Why needed here: Understanding the connection between linear attention, SSMs, and RNNs helps grasp why BiGLA can compress historical information into fixed-size states
  - Quick check question: What is the key difference between how transformers and RNNs handle historical information during inference?

- Concept: Hardware-aware implementation considerations
  - Why needed here: The efficiency gains depend critically on implementation choices that leverage memory hierarchy and compute units
  - Quick check question: Why does materializing the backward sequence in HBM create a bottleneck, and how does kernel fusion address this?

## Architecture Onboarding

- Component map: Image → Patch embedding → Position embedding → [RMSNorm → BiGLA → 2D locality injection → SwiGLU FFN] × N blocks → Global pool → Classifier

- Critical path: Image → Patch embedding → Position embedding → [RMSNorm → BiGLA → 2D locality injection → SwiGLU FFN] × N blocks → Global pool → Classifier

- Design tradeoffs:
  - Global receptive field vs. linear complexity: BiGLA achieves global context while maintaining O(N) complexity
  - Parameter efficiency vs. expressiveness: Direction-wise gating adds minimal parameters but captures directional sensitivity
  - Memory vs. latency: Hardware-aware implementation trades some implementation complexity for reduced memory usage and improved speed

- Failure signatures:
  - Poor performance despite correct implementation: May indicate the gating mechanisms aren't learning useful combinations, or the directional sensitivity assumption is invalid for the dataset
  - High memory usage: Likely indicates the hardware-aware implementation isn't properly fused, or batch size is too large for the memory savings to be effective
  - Slow inference: Could indicate the fused kernel isn't efficiently scheduled, or the 2D locality injection is too computationally heavy

- First 3 experiments:
  1. Ablation study: Compare BiGLA vs. single-direction GLA vs. standard attention on ImageNet-1K validation accuracy and throughput
  2. Memory profiling: Measure HBM usage with and without the hardware-aware implementation at various resolutions
  3. Kernel fusion validation: Profile the fused BiGLA kernel vs. separate forward/backward kernels to confirm speedup and memory benefits

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The hardware-aware implementation details are not fully specified, making it difficult to verify the claimed efficiency gains
- The effectiveness of direction-wise gating depends on the assumption that visual information varies significantly across directions, which may not hold for all datasets
- No ablation studies are provided to quantify the contribution of each component (bidirectional modeling, 2D locality injection, hardware-aware implementation) to final performance

## Confidence

**High confidence**: The core claims about achieving linear complexity through gated linear attention and the basic architectural innovations (BiGLA, 2D locality injection) are well-supported by the paper's methodology and consistent with established linear attention techniques.

**Medium confidence**: The claims about significant efficiency gains (27% parameters, 20% FLOPs, 2× speedup) and performance improvements are supported by experimental results, but depend on the successful implementation of the hardware-aware techniques which are not fully specified.

**Low confidence**: The claim that the hardware-aware implementation reduces memory usage and improves parallelism cannot be independently verified without access to the specific kernel implementation details.

## Next Checks

1. **Implement and profile the fused BiGLA kernel**: Compare memory usage and execution time between the proposed fused implementation and separate forward/backward kernels on actual hardware across different batch sizes and sequence lengths.

2. **Ablation study of directional gating**: Train models with and without direction-wise gating on multiple datasets to quantify the actual contribution of this component to both accuracy and parameter efficiency.

3. **Cross-dataset generalization test**: Evaluate ViG on datasets with different directional characteristics (e.g., medical imaging, satellite imagery, text-in-image data) to verify that the bidirectional modeling assumption holds across domains.