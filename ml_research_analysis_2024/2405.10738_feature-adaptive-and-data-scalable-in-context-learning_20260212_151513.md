---
ver: rpa2
title: Feature-Adaptive and Data-Scalable In-Context Learning
arxiv_id: '2405.10738'
source_url: https://arxiv.org/abs/2405.10738
tags:
- fads-icl
- uni00000003
- uni00000013
- data
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FADS-ICL, a framework for in-context learning
  that addresses two key limitations: data scalability and feature adaptation. The
  method extracts general features from beyond-context samples using an LLM with ICL-style
  input, then refines these features with a task-specific modulator for downstream
  tasks.'
---

# Feature-Adaptive and Data-Scalable In-Context Learning

## Quick Facts
- arXiv ID: 2405.10738
- Source URL: https://arxiv.org/abs/2405.10738
- Reference count: 30
- Primary result: FADS-ICL achieves +14.3 average accuracy over vanilla ICL and +6.2 over kNN-prompting with 1.5B LLM and 32 shots

## Executive Summary
This paper introduces FADS-ICL, a framework that addresses two key limitations in in-context learning: data scalability and feature adaptation. The method extracts general features from beyond-context samples using an LLM with ICL-style input, then refines these features with a task-specific modulator for downstream tasks. Experiments across 10 datasets with varying data (4-128 shots) and LLM scales (0.8B-70B) demonstrate consistent improvements over state-of-the-art methods.

The framework is particularly effective because it separates feature extraction from adaptation, allowing the LLM to leverage its broad knowledge while a modulator adapts features to specific tasks. FADS-ICL shows increasing performance gains as more data becomes available, demonstrating strong data scalability. The method is compatible with both black-box and open-source LLMs, making it practical for real-world deployment.

## Method Summary
FADS-ICL extracts general features from beyond-context samples using an LLM with ICL-style input, then refines these features with a task-specific modulator for downstream tasks. The framework splits data into demonstration set (D) and residual set (R), with D being a small subset (e.g., one sample per class) and R containing the remaining samples. For each sample, the same demonstration prompt is prepended and the last hidden state from the LLM is extracted as the general feature. A logistic regression modulator is trained on the general features and labels from R, then used to refine general features and predict labels for test samples. The method recommends logistic regression modulators and hidden states as general features based on analytical experiments.

## Key Results
- With 1.5B LLM and 32 shots, FADS-ICL achieves +14.3 average accuracy over vanilla ICL and +6.2 over kNN-prompting
- Performance gap increases to +17.8 with 128 shots, demonstrating strong data scalability
- FADS-ICL consistently outperforms state-of-the-art methods across all tested LLM scales (0.8B-70B) and data settings (4-128 shots)
- Analytical experiments validate the effectiveness of logistic regression modulators and hidden states as general features

## Why This Works (Mechanism)
The framework addresses two fundamental limitations in in-context learning: the context length constraint that limits available data and the non-adaptive general features extracted from LLMs. By extracting features from beyond-context samples, FADS-ICL leverages more data than traditional ICL while maintaining the efficiency benefits. The modulator then adapts these general features to specific tasks, overcoming the rigidity of standard ICL features. This separation of feature extraction and adaptation allows the LLM to use its broad knowledge while the modulator handles task-specific refinement.

## Foundational Learning
- **In-context learning (ICL)**: Learning from demonstration examples in prompts without gradient updates - needed because it's the baseline method being improved upon; check by implementing vanilla ICL with proper prompt formatting
- **Feature extraction from LLMs**: Obtaining hidden states or probability distributions from LLM layers - needed as the core mechanism for capturing general knowledge; check by comparing different feature types (hidden states vs. probabilities)
- **Modulator training**: Training a task-specific model on extracted features - needed to adapt general features to specific tasks; check by comparing different modulator architectures (logistic regression vs. MLP vs. SVM)
- **Data splitting strategy**: Separating demonstration and residual sets - needed to enable feature extraction from beyond-context samples; check by varying the ratio of D to R and measuring impact on performance
- **Black-box vs open-source LLM compatibility**: Supporting both API-based and local LLM inference - needed for practical deployment flexibility; check by implementing both approaches and comparing results

## Architecture Onboarding

Component map: LLM Feature Extractor -> General Features -> Modulator Trainer -> Refined Features -> Prediction

Critical path: Demonstration prompt preparation -> Feature extraction from residual samples -> Modulator training -> Test sample prediction

Design tradeoffs:
- Hidden states vs probability distributions as general features: hidden states provide richer knowledge but require more storage
- Demonstration quantity: more demonstrations improve regularization but consume context space
- Modulator complexity: simpler modulators (logistic regression) are more data-efficient but may miss complex patterns

Failure signatures:
- Poor performance if demonstrations are omitted during feature extraction (features become too generic)
- Reduced accuracy with probability distributions instead of hidden states when data is limited
- Overfitting when modulator is too complex relative to available residual samples

First experiments:
1. Compare FADS-ICL performance with and without demonstrations during feature extraction on a small dataset
2. Test different modulator architectures (logistic regression, SVM, MLP) on the same dataset to validate the logistic regression recommendation
3. Evaluate the impact of feature type (hidden states vs probability distributions) on performance across datasets with varying data sizes

## Open Questions the Paper Calls Out
- What is the optimal number of demonstrations per class for FADS-ICL across different dataset characteristics? The paper only tested one demonstration per class in main experiments, leaving the optimal number across varying dataset sizes, class distributions, and task complexities unexplored.
- How does FADS-ICL's performance degrade when applied to non-classification tasks like question answering or summarization? The paper only validates on classification datasets and acknowledges limitations without testing on generation tasks.
- What is the computational overhead of FADS-ICL when using black-box LLMs with feature extraction APIs versus open-source LLMs? The paper only provides a single black-box LLM experiment without comparing computational costs, latency, or API call patterns between approaches.
- How does the choice of general features (hidden states vs probability distributions) interact with different modulator architectures? The paper only examined hidden states with logistic regression and probability distributions with various modulators, without exploring cross-combinations.
- What is the minimum amount of training data required for FADS-ICL to outperform traditional fine-tuning? The paper only tested one fine-tuning comparison point (128 shots) and doesn't investigate the data-efficiency frontier.

## Limitations
- Unknown exact template format used for ICL input, though examples are provided in Appendix F
- Specific implementation details for feature extraction and modulator training are not fully specified
- Only tested on classification tasks, with acknowledged limitations for generation tasks requiring large output spaces

## Confidence

High confidence in the core framework design and its theoretical motivation
Medium confidence in the empirical results due to the comprehensive evaluation across 10 datasets
Low confidence in exact reproducibility due to unspecified implementation details

## Next Checks

1. Implement the feature extraction pipeline using the demonstration template from Appendix F and verify that including demonstrations during extraction improves performance compared to omitting them
2. Compare the performance of logistic regression modulators against other linear classifiers (e.g., SVM) on the residual set features to validate the recommendation
3. Test the framework with different general feature representations (hidden states vs. probability distributions) to confirm the paper's findings about data-dependent effectiveness