---
ver: rpa2
title: 'PPM : A Pre-trained Plug-in Model for Click-through Rate Prediction'
arxiv_id: '2403.10049'
source_url: https://arxiv.org/abs/2403.10049
tags:
- user
- pre-trained
- item
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving click-through rate
  (CTR) prediction in industrial recommender systems, particularly for cold-start
  scenarios where unique identities fail to represent new items effectively. The proposed
  solution, PPM (Pre-trained Plug-in CTR Model), integrates multi-modal features (text
  and images) from pre-trained models like BERT and ResNet into the traditional ID-based
  recommendation system.
---

# PPM : A Pre-trained Plug-in Model for Click-through Rate Prediction

## Quick Facts
- arXiv ID: 2403.10049
- Source URL: https://arxiv.org/abs/2403.10049
- Reference count: 40
- One-line primary result: PPM improves CTR prediction, especially for cold-start items, with 1.09% UCTR and 0.28% UCVR gains in online tests

## Executive Summary
This paper addresses the cold-start problem in industrial recommender systems by proposing PPM (Pre-trained Plug-in CTR Model), which integrates multi-modal features from pre-trained models into traditional ID-based recommendation systems. PPM caches pre-trained text and image features from BERT and ResNet, then jointly trains a subset of parameters with the IDRec model. The approach achieves significant performance improvements, particularly for cold-start items, while maintaining online serving efficiency. Experiments show 2.05% AUC improvement and 1.09% UCTR gain in online A/B tests.

## Method Summary
PPM addresses cold-start CTR prediction by extracting and caching multi-modal features from pre-trained BERT and ResNet models, then jointly training a subset of parameters with traditional ID-based recommendation systems. The method involves fine-tuning modality encoders on e-commerce-specific tasks (query matching for text, entity prediction for images), caching these pre-trained features, and integrating them into a unified ranking model that uses separate transformers for ID-based and modality-based representations. The model is trained end-to-end with a subset of parameters updated while keeping cached features fixed, enabling efficient online serving without latency penalties.

## Key Results
- PPM improves AUC by 2.05% and Precision@N by 3.25% compared to IDRec baseline
- Cold-start items show significantly better performance with PPM (P@2: 1.49% vs 1.35%)
- Online A/B tests demonstrate 1.09% increase in UCTR and 0.28% increase in UCVR
- PPM achieves comparable performance using only 50% of the data compared to traditional models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Caching pre-trained multi-modal features prevents online latency explosion while maintaining model accuracy.
- Mechanism: The model extracts and caches item text and image representations from frozen BERT and ResNet models before joint training. Only a subset of parameters in the Behavior-Transformer and CTR Prediction layers are updated during end-to-end training, avoiding the need to run large pre-trained models online.
- Core assumption: Cached representations are sufficiently expressive to capture item characteristics relevant to CTR prediction.
- Evidence anchors:
  - [abstract] "caching pre-trained features and optimizing a subset of parameters during joint training"
  - [section] "the pre-trained multi-modal features are cached in advance and fixed to address the issue of increased online latency"

### Mechanism 2
- Claim: Fine-tuning modality encoders on e-commerce-specific tasks improves feature quality over generic pre-trained models.
- Mechanism: The BERT and ResNet models are further trained on e-commerce data using query matching (text) and entity prediction (image) tasks, aligning representations with domain-specific user behavior patterns.
- Core assumption: Domain-specific fine-tuning yields better representations than generic pre-trained features for CTR tasks.
- Evidence anchors:
  - [section] "fine-tuned pre-trained model(BERT and ResNet) using e-commerce data via query matching task and entity prediction task"
  - [section] "fine-tuning modality encoder models with QM&EP tasks play an indispensable part in PPM"

### Mechanism 3
- Claim: Separating ID-based and modality-based user/item representations and fusing them later improves CTR prediction.
- Mechanism: The Unified Ranking Model uses two parallel transformer pathways—one for ID features (SKU, shop, brand, category) and one for multi-modal features (text, image)—before combining them in the Multi-task Module.
- Core assumption: ID-based and modality-based signals capture complementary aspects of user preference.
- Evidence anchors:
  - [section] "modeling ID-based features and modal-based features with separate transformers is an optimal choice"
  - [section] "both ID-based and modality-based signals capture complementary aspects of user preference"

## Foundational Learning

- Concept: Cold-start problem in recommender systems
  - Why needed here: PPM is explicitly designed to address cold-start scenarios where ID-based methods fail.
  - Quick check question: What distinguishes a cold-start item from a warm item in CTR prediction?

- Concept: Transformer-based sequential modeling
  - Why needed here: The model uses bidirectional transformers to capture user preference from historical interactions in both ID-based and modality-based pathways.
  - Quick check question: How does adding position and recency embeddings help transformers model user behavior sequences?

- Concept: Multi-task learning with MMoE
  - Why needed here: The model jointly predicts click, order, and collection probabilities using Multi-gate Mixture-of-Experts to leverage shared representations.
  - Quick check question: What advantage does MMoE provide over separate models for each task?

## Architecture Onboarding

- Component map:
  Modality Encoder Layer (BERT + ResNet, frozen after fine-tuning) -> Behavior-Transformer Layer (bidirectional transformer on cached modal features) -> CTR Prediction Layer (feed-forward network for binary classification) -> ID-based Sequential Module (transformer on ID embeddings) -> PPM Module (transformer on cached modal embeddings) -> Multi-task Module (MMoE for click/order/collection prediction) -> Unified Ranking Model (integrates IDRec and PPM)

- Critical path:
  Pre-training -> Caching modal features -> Joint training of IDSM, PPM, and Multi-task Module -> Online serving with cached features only

- Design tradeoffs:
  - Freezing modal encoders vs. fine-tuning: frozen reduces latency but may limit adaptation
  - Separate vs. shared transformers: separate allows specialization but increases parameters
  - Cached vs. real-time feature extraction: cached reduces latency but may become stale

- Failure signatures:
  - Increased latency: likely due to cache misses or inefficient feature retrieval
  - Degraded cold-start performance: cached features may not capture new item characteristics
  - Overfitting: too many trainable parameters relative to data size

- First 3 experiments:
  1. Compare AUC/P@2 with and without cached modal features to verify latency reduction doesn't hurt accuracy
  2. Test PPM with frozen vs. fine-tuned modal encoders to quantify domain adaptation benefit
  3. Evaluate cold-start performance across different item frequency buckets to confirm intended use case improvement

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Confidence in cold-start performance claims is low due to absence of explicit cold-start metrics
- Fixed caching strategy may not adapt to rapidly changing item catalogs
- Fine-tuning tasks (query matching, entity prediction) are not fully specified, making reproduction challenging

## Confidence
- Medium: Overall effectiveness with modest online gains (1.09% UCTR, 0.28% UCVR) but lacks detailed statistical significance analysis
- Low: Cold-start performance claims since paper discusses cold-start as motivation but doesn't provide direct comparisons between cold and warm item performance

## Next Checks
1. Implement explicit cold-start evaluation by stratifying test items by appearance frequency and comparing PPM performance against IDRec specifically on items with <1 day and 1-7 days of history.

2. Measure end-to-end inference latency with and without cached features under realistic load conditions, verifying the claimed latency reduction holds at scale with cache hit rates below 95%.

3. Simulate feature staleness by using cached representations from progressively older time windows (1 hour, 6 hours, 24 hours) and measure degradation in AUC and Precision@N to determine refresh requirements.