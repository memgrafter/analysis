---
ver: rpa2
title: A Cross-Lingual Statutory Article Retrieval Dataset for Taiwan Legal Studies
arxiv_id: '2410.11450'
source_url: https://arxiv.org/abs/2410.11450
tags:
- retrieval
- legal
- statutory
- article
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LawFactsQA-TW, a cross-lingual statutory
  article retrieval dataset for Taiwan legal studies. The dataset includes 92 human-annotated
  and 173 synthetic QA pairs with English queries, Chinese translations, relevant
  statutes, and ground-truth answers.
---

# A Cross-Lingual Statutory Article Retrieval Dataset for Taiwan Legal Studies

## Quick Facts
- arXiv ID: 2410.11450
- Source URL: https://arxiv.org/abs/2410.11450
- Reference count: 6
- Primary result: 0.729 recall@50 for synthetic data and 0.687 recall@50 for human-labeled data

## Executive Summary
This paper introduces LawFactsQA-TW, a cross-lingual statutory article retrieval dataset for Taiwan legal studies. The dataset includes 92 human-annotated and 173 synthetic QA pairs with English queries, Chinese translations, relevant statutes, and ground-truth answers. The authors propose LLM-based methods including query expansion and reranking to improve retrieval performance. Their best model achieves 0.729 recall@50 for synthetic data and 0.687 recall@50 for human-labeled data, outperforming sparse and dense retrieval baselines. The study demonstrates that LLM-generated statutory articles enhance retrieval accuracy and supports cross-lingual legal information access for non-native speakers in Taiwan.

## Method Summary
The authors propose a cross-lingual statutory article retrieval approach for Taiwan legal studies, using a dataset of English queries matched to Chinese legal documents. They implement sparse retrieval (BM25) with query translation to Chinese, dense retrieval using multilingual embeddings (BGE-m3), and LLM-augmented methods including query expansion via answer/statute generation and result reranking. The system processes English queries, retrieves relevant Chinese statutory articles, and uses LLM-generated content to improve semantic matching and ranking accuracy.

## Key Results
- Best model achieves 0.729 recall@50 for synthetic data and 0.687 recall@50 for human-labeled data
- LLM-based methods outperform both sparse (BM25) and dense (BGE-m3) retrieval baselines
- Fine-tuned Taiwanese Traditional Chinese models (Breeze, Taide) outperform general LLMs in query expansion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated statutory articles improve retrieval precision by aligning generated content with query intent.
- Mechanism: The model generates hypothetical statutory content based on the query, which acts as an expanded query representation, enabling better semantic matching against the corpus.
- Core assumption: Even imperfect generated statutory articles still capture key semantic features needed for retrieval.
- Evidence anchors:
  - [abstract] "Our best model achieves 0.729 recall@50 for synthetic data...outperforming sparse and dense retrieval baselines."
  - [section] "This method incorporates Answer Expansion, Statutory Article Expansion, and LLM-based Reranking."
  - [corpus] Weak - corpus only provides neighbor paper metadata, no direct retrieval experiment data.
- Break condition: Generated articles introduce significant noise or contradict the query intent, degrading semantic alignment.

### Mechanism 2
- Claim: Reranking with LLM improves top-10 retrieval recall by replacing irrelevant results with better matches.
- Mechanism: The LLM re-evaluates Top-K retrieved articles, swapping out irrelevant ones for more relevant ones from outside the initial set, iteratively refining results.
- Core assumption: The LLM can accurately judge relevance of statutory articles against the query.
- Evidence anchors:
  - [abstract] "Our best model achieves...0.687 recall@50 for human-labeled data...LLM-generated statutory articles enhance retrieval accuracy."
  - [section] "In the third method, we use LLMs to rerank the retrieved results...until all retrieved articles are deemed highly relevant."
  - [corpus] Weak - corpus only provides neighbor paper metadata, no direct reranking experiment data.
- Break condition: LLM's relevance scoring is inconsistent or biased, leading to incorrect swaps.

### Mechanism 3
- Claim: Fine-tuned Taiwanese Traditional Chinese models outperform general LLMs in statutory article generation.
- Mechanism: Models like Breeze and Taide, trained on Traditional Chinese legal data, generate more contextually appropriate statutory content for Taiwanese queries.
- Core assumption: Fine-tuning on domain-specific language data improves generation relevance for legal retrieval.
- Evidence anchors:
  - [section] "both the Breeze 7B...and Taide models...outperformed GPT-3.5 in query expansion...This fine-tuning likely enhances their ability to generate Statutory Articles relevant to the query."
  - [corpus] Weak - corpus only provides neighbor paper metadata, no direct generation experiment data.
- Break condition: Generated content drifts from legal terminology or introduces factual errors.

## Foundational Learning

- Concept: Cross-lingual retrieval
  - Why needed here: The dataset contains English queries matched to Chinese legal documents, requiring semantic matching across languages.
  - Quick check question: What embedding model is used to handle multilingual retrieval in this paper?
- Concept: Statutory article retrieval
  - Why needed here: The task involves finding relevant legal statutes based on natural language queries, a core legal NLP problem.
  - Quick check question: Which two metrics are used to evaluate retrieval performance?
- Concept: Query expansion
  - Why needed here: Expanding queries with generated content improves semantic coverage for retrieval.
  - Quick check question: What are the three LLM-based methods proposed for retrieval enhancement?

## Architecture Onboarding

- Component map:
  Query input → Translation layer (for sparse) / Direct embedding (for dense) → Retrieval engine → LLM reranking layer → Output
  Parallel LLM generation pipeline for answer/statutory article expansion
- Critical path:
  1. Query processing
  2. Embedding retrieval (BGE-m3)
  3. LLM expansion (answer/statutory article)
  4. Reranking (if enabled)
  5. Final result ranking
- Design tradeoffs:
  - Sparse vs dense: Translation cost vs semantic richness
  - LLM expansion: Computation overhead vs recall improvement
  - Reranking: Additional inference time vs precision gain
- Failure signatures:
  - Low recall: Embedding model misalignment or poor query expansion
  - High noise: LLM generating irrelevant statutory content
  - Slow inference: Inefficient reranking loop or large model size
- First 3 experiments:
  1. Compare BM25 vs BGE-m3 retrieval performance on synthetic data
  2. Test answer expansion vs statutory article expansion impact on recall
  3. Evaluate reranking improvement over dense retrieval baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of synthetic data compare to human-annotated data in real-world legal retrieval tasks?
- Basis in paper: [explicit] The paper presents retrieval performance metrics for both synthetic and human-labeled data, showing recall@50 of 0.729 for synthetic and 0.687 for human-labeled data.
- Why unresolved: While the paper provides retrieval performance metrics, it doesn't validate these results in real-world legal retrieval scenarios or compare them to actual user satisfaction.
- What evidence would resolve it: Conducting user studies with legal professionals or foreign nationals using the system in real-world scenarios to measure retrieval accuracy, user satisfaction, and practical utility.

### Open Question 2
- Question: How would expert legal validation of the synthetic dataset affect the system's credibility and retrieval accuracy?
- Basis in paper: [inferred] The paper acknowledges that legal professionals have not evaluated the synthetic dataset, which may affect the system's credibility and expertise.
- Why unresolved: The paper doesn't explore the impact of expert validation on dataset quality, retrieval accuracy, or system trustworthiness.
- What evidence would resolve it: Having legal experts review and validate the synthetic dataset, then measuring changes in retrieval performance and user trust through controlled experiments.

### Open Question 3
- Question: What is the impact of fine-tuning models with Taiwanese Traditional Chinese data on retrieval performance for legal-specific terminology?
- Basis in paper: [explicit] The paper notes that Breeze and Taide models outperformed GPT-3.5 in query expansion, attributing this to fine-tuning with Taiwanese Traditional Chinese data.
- Why unresolved: While the paper shows improved performance, it doesn't quantify the specific impact of fine-tuning on legal terminology retrieval or compare it to other fine-tuning approaches.
- What evidence would resolve it: Conducting controlled experiments comparing retrieval performance of models with and without legal-specific fine-tuning, using both general and legal-specific query sets.

## Limitations
- Exact prompt templates and LLM instructions used for generating synthetic data are not provided
- Model hyperparameters (embedding dimensions, reranking cutoffs, LLM temperature) are unspecified
- Limited dataset size (92 human-annotated pairs) may affect generalizability of results

## Confidence
- Retrieval performance claims: High - Results are clearly presented with multiple metrics across baselines and LLM methods
- Mechanism explanations: Medium - While the paper describes the approaches, lacks ablation studies to isolate individual mechanism contributions
- Cross-lingual effectiveness: Medium - Results show improvement but don't compare against alternative multilingual models

## Next Checks
1. Implement ablation study to measure individual contributions of answer expansion, statutory article expansion, and reranking components
2. Test retrieval performance on larger query sets to validate dataset size limitations
3. Compare proposed methods against alternative multilingual embedding models (e.g., multilingual E5) to establish relative effectiveness