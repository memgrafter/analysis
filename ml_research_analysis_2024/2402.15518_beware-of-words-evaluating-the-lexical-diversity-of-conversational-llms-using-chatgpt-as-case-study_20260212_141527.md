---
ver: rpa2
title: 'Beware of Words: Evaluating the Lexical Diversity of Conversational LLMs using
  ChatGPT as Case Study'
arxiv_id: '2402.15518'
source_url: https://arxiv.org/abs/2402.15518
tags:
- lexical
- diversity
- llms
- text
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the lexical diversity of conversational Large
  Language Models (LLMs), using ChatGPT as a case study. It investigates how lexical
  diversity depends on model parameters like temperature, presence penalty, and assigned
  roles (e.g., child, erudite).
---

# Beware of Words: Evaluating the Lexical Diversity of Conversational LLMs using ChatGPT as Case Study

## Quick Facts
- arXiv ID: 2402.15518
- Source URL: https://arxiv.org/abs/2402.15518
- Authors: Gonzalo Martínez; José Alberto Hernández; Javier Conde; Pedro Reviriego; Elena Merino
- Reference count: 40
- Primary result: ChatGPT4 exhibits greater lexical diversity than ChatGPT3.5 across various parameter settings and roles.

## Executive Summary
This paper evaluates lexical diversity in conversational LLMs using ChatGPT as a case study, examining how model parameters and assigned roles affect vocabulary variety. The authors created a comprehensive test suite and measured lexical diversity using four metrics: MATTR, MTLD, RTTR, and Maas. Their findings reveal that presence penalty linearly increases diversity, ChatGPT4 outperforms ChatGPT3.5 in lexical richness, and certain parameter combinations produce invalid text. The study also shows that role assignment (like "child" or "erudite") significantly impacts diversity, with essays generally showing higher diversity than question answering.

## Method Summary
The authors generated text using OpenAI's API with ChatGPT3.5 and ChatGPT4 across various parameter settings (temperature, top_p, frequency penalty, presence penalty) and assigned roles (child, erudite, etc.). They used a test suite comprising TOEFL essay prompts, NYT writing prompts, and HC3 question answering datasets. Lexical diversity was measured using four established metrics: MATTR, MTLD, RTTR, and Maas. The generated texts were then analyzed to determine how different parameters and roles affected lexical diversity, with particular attention to invalid text generation at extreme parameter values.

## Key Results
- Lexical diversity increases approximately linearly with presence penalty
- ChatGPT4 shows consistently higher lexical diversity than ChatGPT3.5
- High temperature (≥1.4) and extreme frequency penalties (≤-0.5 or ≥1.0) produce invalid text
- Essays exhibit higher lexical diversity than question answering tasks
- "Child" role produces lower lexical diversity compared to other roles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Presence penalty increases lexical diversity by penalizing token selection if the token has already been used
- Mechanism: During sampling, the model adds a penalty term proportional to token repetition, making previously used tokens less likely and encouraging novel word choices
- Core assumption: Penalty is applied at each generation step and is not capped in a way that would limit its effect on vocabulary variety
- Evidence anchors: Abstract states "lexical diversity increases with presence penalty"; section notes "approximately linear behavior with the presence penalty"
- Break condition: If penalty implementation only applies a global bias instead of per-token, the effect on local diversity metrics could vanish

### Mechanism 2
- Claim: High temperature and frequency penalty values generate invalid text due to over-amplification of randomness and token repetition penalties
- Mechanism: Temperature flattens token probability distribution at high values, making rare tokens more likely; frequency penalty heavily downweights previously seen tokens, forcing selection of low-probability, often invalid tokens
- Core assumption: No validity filter catches nonsensical token sequences before they appear
- Evidence anchors: Section reports "invalid texts with non-words or repetitions" for temperature ≥1.4 and frequency penalty ≤-0.5 or ≥1.0
- Break condition: If model internally rescales or clips extreme logit values, invalidity spike could be mitigated

### Mechanism 3
- Claim: ChatGPT4 produces more lexically diverse text than ChatGPT3.5 due to architectural and training differences
- Mechanism: ChatGPT4 likely has larger parameter count, more diverse pretraining corpus, and better fine-tuning, enabling richer vocabulary usage
- Core assumption: Diversity differences are attributable to model version rather than hyperparameter settings or prompt effects
- Evidence anchors: Section states "ChatGPT4 has a larger diversity in most cases"; abstract confirms this finding
- Break condition: If hyperparameter differences (e.g., default temperature) are not controlled, observed difference might be due to configuration rather than model capacity

## Foundational Learning

- Concept: Lexical diversity metrics (MATTR, MTLD, RTTR, Maas)
  - Why needed here: Essential to interpret results and replicate study; these metrics quantify vocabulary variety in LLM outputs
  - Quick check question: What is the key difference between global metrics (RTTR, Maas) and local metrics (MATTR, MTLD)?

- Concept: Temperature and top_p sampling
  - Why needed here: These hyperparameters control randomness in token selection, directly affecting lexical variety
  - Quick check question: How does increasing temperature change the probability distribution over tokens?

- Concept: Presence and frequency penalties
  - Why needed here: These parameters encourage or discourage token reuse, influencing lexical diversity
  - Quick check question: What is the effect of a positive presence penalty on word repetition?

## Architecture Onboarding

- Component map: Test suite generation -> OpenAI API calls -> Text storage -> LexicalRichness module -> Metric aggregation -> Analysis script
- Critical path: Generate text for all prompts/roles/params -> compute metrics per category -> compare across conditions -> identify parameter effects
- Design tradeoffs: Black-box API limits visibility into internal sampling; full evaluation requires large number of API calls (cost/time)
- Failure signatures: Invalid or repetitive text output; API errors; metrics returning NaN or zero; inconsistent results across runs
- First 3 experiments:
  1. Run small subset of prompts with temperature=0 and temperature=2.0; compare MATTR to confirm temperature effect and spot invalid outputs
  2. Test presence penalty at -2.0, 0, and +2.0 for one task/role; plot diversity metrics to verify linear trend
  3. Generate text for "child" role vs. "erudite" role; compute RTTR and Maas to confirm role-based diversity differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do lexical diversity metrics vary across different languages when evaluating conversational LLMs?
- Basis in paper: [inferred] Study focuses on English and acknowledges LLMs perform worse in other languages, but doesn't explore lexical diversity in other languages
- Why unresolved: Study is limited to English; authors acknowledge need for further research on other languages
- What evidence would resolve it: Conducting similar evaluations of lexical diversity using conversational LLMs in multiple languages and comparing results to English

### Open Question 2
- Question: What is the impact of training data composition on the lexical diversity of conversational LLMs?
- Basis in paper: [inferred] Authors suggest well-balanced dataset with high lexical diversity could improve results but cannot provide evidence due to unavailability of training datasets
- Why unresolved: Training datasets for LLMs are not publicly available, preventing direct analysis of composition and impact
- What evidence would resolve it: Access to training datasets and analysis of their lexical diversity and composition, correlating these factors with lexical diversity of trained models

### Open Question 3
- Question: How does lexical diversity in conversational LLMs affect language evolution over time?
- Basis in paper: [explicit] Paper discusses potential impact of LLMs on language evolution, particularly if they don't use certain words, which may lead to those words becoming less frequent or falling out of use
- Why unresolved: Long-term effects of LLM-generated text on language use and evolution are not yet observable or measurable
- What evidence would resolve it: Longitudinal studies tracking frequency and usage of words in texts generated by LLMs over time, correlating these trends with broader language use patterns

## Limitations

- Study relies entirely on OpenAI's black-box API implementation, making it impossible to verify exact internal mechanisms
- Research focuses exclusively on ChatGPT models (3.5 and 4), limiting generalizability to other LLM architectures
- Lexical diversity metrics have known limitations (length sensitivity, bias toward shorter texts) not fully controlled for in analysis
- Test suite of 145 prompts may not represent the full breadth of conversational contexts where these models are deployed

## Confidence

**High Confidence**: Presence penalty increases lexical diversity (consistent across multiple metrics); ChatGPT4 exhibits greater lexical diversity than ChatGPT3.5 (systematic comparison)

**Medium Confidence**: Temperature and frequency penalty produce invalid text at high values (supported by observed outputs but exact thresholds unclear); essay writing shows higher diversity than question answering (based on specific test suite)

**Low Confidence**: Specific parameter thresholds where text becomes invalid (temperature ≥1.4, frequency penalty ≤-0.5 or ≥1.0) are likely model-version and context-dependent

## Next Checks

1. **Cross-model validation**: Replicate lexical diversity experiments using open-source LLM implementations (e.g., LLaMA, Mistral) where internal mechanisms are accessible, to verify whether observed relationships between presence penalty and diversity hold when sampling algorithm can be examined directly.

2. **Text length normalization**: Conduct controlled experiment varying text length systematically while holding other parameters constant, measuring how each lexical diversity metric responds to ensure observed differences are not artifacts of length variation.

3. **Role-specific validation**: Test additional roles beyond "child" and "erudite" (such as "poet," "scientist," "teenager") across multiple domains to determine whether role effects are specific to certain personas or represent a more general pattern of stylistic conditioning in LLMs.