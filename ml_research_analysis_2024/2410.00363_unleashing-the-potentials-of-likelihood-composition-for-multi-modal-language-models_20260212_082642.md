---
ver: rpa2
title: Unleashing the Potentials of Likelihood Composition for Multi-modal Language
  Models
arxiv_id: '2410.00363'
source_url: https://arxiv.org/abs/2410.00363
tags:
- llav
- likelihood
- arxiv
- composition
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a post-hoc framework for fusing heterogeneous\
  \ multi-modal language models (MLMs) by composing their likelihood distributions\
  \ during visual question answering tasks. The framework introduces basic operations\u2014\
  debias, highlight, majority-vote, and ensemble\u2014which are combined to create\
  \ mixed composition methods (mix-composition)."
---

# Unleashing the Potentials of Likelihood Composition for Multi-modal Language Models

## Quick Facts
- **arXiv ID**: 2410.00363
- **Source URL**: https://arxiv.org/abs/2410.00363
- **Reference count**: 40
- **Primary result**: Likelihood composition framework improves multi-modal VQA performance up to 13.42% over simple ensemble methods

## Executive Summary
This paper introduces a post-hoc framework for fusing heterogeneous multi-modal language models by composing their likelihood distributions during visual question answering tasks. The framework employs basic operations—debias, highlight, majority-vote, and ensemble—which can be combined to create mixed composition methods (mix-composition). Experiments on 9 VQA datasets and 10 MLMs demonstrate that self-composition improves performance for less capable models, while mix-composition significantly outperforms simple ensemble or majority-vote methods. The results show that model quality is more important than quantity when fusing models.

## Method Summary
The framework computes likelihood distributions for candidate answers from multiple MLMs, then applies four basic composition operations: debias (correcting language priors), highlight (emphasizing correct answers), ensemble (averaging distributions), and majority-vote (weighted voting). These operations can be applied individually (self-composition) or combined (mix-composition). The framework is post-hoc, requiring no model retraining, and works by manipulating the log-probability distributions of candidate answers across heterogeneous models.

## Key Results
- Self-composition operations (debias, highlight) improve performance for less capable models
- Mix-composition significantly outperforms simple ensemble or majority-vote methods
- Model quality is more important than quantity when fusing models for VQA tasks
- Performance improvements up to 13.42% on certain datasets compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Likelihood composition improves performance by correcting language priors and highlighting correct answers through contrasting prompts.
- Mechanism: The framework computes likelihood distributions for each candidate answer, then applies debias (subtracting likelihood from no-image condition) and highlight (subtracting negative instruction likelihood) operations. These operations adjust the raw likelihood distributions to correct for language priors and emphasize correct answers.
- Core assumption: The likelihood distribution contains meaningful signal about answer correctness that can be manipulated to improve predictions.
- Evidence anchors:
  - [abstract] "Here the core concept, likelihood, is actually the log-probability of the candidate answer."
  - [section] "To model the language prior bias existing in the MLM, we only input the question into the model, inducing the model to give the most common answer, which reflects the language bias."
  - [corpus] Weak - corpus papers don't directly address likelihood manipulation for bias correction.
- Break condition: If likelihood distributions are dominated by noise or if the no-image/negative instruction distributions don't capture the intended biases.

### Mechanism 2
- Claim: Mixing self-composition (debias/highlight) with mutual-composition (ensemble/majority-vote) creates more effective fusion than either alone.
- Mechanism: The framework applies self-composition operations to each model's likelihood distribution individually, then combines these manipulated distributions using mutual-composition methods. This creates "mix-composition" that leverages both individual model correction and collective wisdom.
- Core assumption: Self-composition operations are complementary to mutual-composition methods and their combination creates additive benefits.
- Evidence anchors:
  - [abstract] "By combining (composing) these basic elements, we get the mixed composition methods: mix-composition."
  - [section] "The basic idea of likelihood composition is to compose different likelihood distributions from either one model or multiple heterogeneous models."
  - [corpus] Weak - corpus papers focus on different fusion approaches but don't explore mixing self and mutual composition.
- Break condition: If self-composition operations interfere with each other when combined, or if mutual-composition methods don't benefit from pre-processed distributions.

### Mechanism 3
- Claim: Model quality is more important than quantity for likelihood composition effectiveness.
- Mechanism: The framework shows that fusing fewer high-quality models with likelihood composition outperforms fusing more models, including weaker ones. This suggests that the composition operations amplify the signal from better models while minimizing noise from weaker ones.
- Core assumption: Likelihood composition operations can effectively distinguish between high-quality and low-quality model outputs.
- Evidence anchors:
  - [abstract] "the results show that model quality is more important than quantity when fusing models."
  - [section] "when applying mutual-composition and mix-composition on all the 6 models... fusing LLaV A1.5-13B, LLaV A1.6-7B and LLaV A1.6-13B works better than fusing all 6 models."
  - [corpus] Weak - corpus papers don't directly compare quality vs quantity trade-offs in model fusion.
- Break condition: If likelihood composition cannot effectively filter or amplify quality differences between models.

## Foundational Learning

- Concept: Likelihood calculation from log-probabilities
  - Why needed here: The framework relies on computing and manipulating likelihood distributions for candidate answers.
  - Quick check question: How is the likelihood yi(X) calculated from token probabilities in equation (1)?

- Concept: Prompt engineering for self-composition
  - Why needed here: Debias and highlight operations require carefully designed prompts to generate alternative likelihood distributions.
  - Quick check question: What are the two prompt types used in the debias operation?

- Concept: Ensemble methods and weighted voting
  - Why needed here: Mutual-composition uses averaging and weighted majority-vote to combine multiple models' likelihood distributions.
  - Quick check question: How does weighted majority-vote differ from simple averaging in equation (5)?

## Architecture Onboarding

- Component map: Likelihood calculation -> Self-composition operations (debias, highlight) -> Mutual-composition (ensemble, majority-vote) -> Mix-composition -> Answer selection
- Critical path: 1) Compute likelihood distributions for each model and candidate answer, 2) Apply self-composition operations if needed, 3) Combine distributions using mutual-composition, 4) Select answer with highest final likelihood
- Design tradeoffs: More complex composition methods may provide better performance but increase computational overhead and hyperparameter tuning complexity. Simpler methods are faster but may miss important corrections.
- Failure signatures: Performance degradation when applying composition to already high-quality models, inconsistent improvements across datasets, sensitivity to α parameter values.
- First 3 experiments:
  1. Apply debias operation with α=1.0 to a single model on one VQA dataset to verify language prior correction.
  2. Compare simple ensemble vs weighted majority-vote on multiple models for one dataset to validate mutual-composition benefits.
  3. Apply mix-composition (debias + ensemble) to multiple models and compare against baseline ensemble performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of self-composition methods vary across different types of visual question answering tasks (e.g., object recognition vs. reasoning-based questions)?
- Basis in paper: The paper mentions that self-composition improves performance across 9 VQA datasets, but doesn't analyze the differential impact on task types.
- Why unresolved: The experiments show overall improvements but don't segment results by question complexity or task type.
- What evidence would resolve it: Analyzing self-composition performance separately on datasets requiring different cognitive levels (e.g., MMBench for reasoning vs. VQAv2 for recognition) would clarify task-specific effectiveness.

### Open Question 2
- Question: What is the optimal number of models to fuse using likelihood composition, and does this depend on the specific combination of models used?
- Basis in paper: The paper explicitly states "models' quality is more important than models' quantity" and shows that fusing all 6 LLaVA models isn't optimal, but doesn't provide a systematic analysis of optimal ensemble size.
- Why unresolved: While the paper demonstrates that fewer high-quality models outperform larger ensembles, it doesn't explore the relationship between ensemble size, model diversity, and performance systematically.
- What evidence would resolve it: Conducting experiments that vary both the number and specific combinations of models while measuring performance could establish optimal ensemble configurations.

### Open Question 3
- Question: How does likelihood composition perform when applied to closed-source models like GPT-4V or Gemini, where only API access is available?
- Basis in paper: The limitations section explicitly mentions this gap, noting that the framework hasn't been tested with closed-source models.
- Why unresolved: The paper only tests likelihood composition on open-source models, leaving uncertainty about whether the approach generalizes to models with different architectures or access patterns.
- What evidence would resolve it: Implementing likelihood composition with closed-source models by prompting for likelihood distributions or confidence scores, then measuring performance across various VQA benchmarks.

## Limitations
- Effectiveness depends heavily on underlying MLM output quality
- Limited improvement when applied to already high-performing models
- Computational overhead may be prohibitive for real-time applications
- No universal best composition method exists; optimal choice is dataset-dependent

## Confidence

- **High confidence**: The core methodology of likelihood composition and its basic operations (debias, highlight, ensemble, majority-vote) are technically sound and well-defined.
- **Medium confidence**: The performance improvements claimed (up to 13.42%) are supported by experimental results, though the dataset-specific variations suggest the gains may not be consistent across all scenarios.
- **Medium confidence**: The claim that model quality is more important than quantity for fusion effectiveness is supported by ablation studies but requires further validation across broader model families.

## Next Checks

1. **Cross-task generalization**: Test the likelihood composition framework on non-VQA multi-modal tasks (e.g., image captioning, visual reasoning) to evaluate whether the composition operations provide similar benefits beyond multiple-choice question answering.

2. **Model family sensitivity**: Systematically evaluate how likelihood composition performs when fusing models from different architectural families (vision transformers, convolutional networks, different LLM architectures) to determine if the method is architecture-agnostic or benefits from specific model similarities.

3. **Real-time feasibility analysis**: Measure the computational overhead of likelihood composition operations across different dataset sizes and model counts to determine practical deployment constraints and identify optimization opportunities for reducing inference latency.