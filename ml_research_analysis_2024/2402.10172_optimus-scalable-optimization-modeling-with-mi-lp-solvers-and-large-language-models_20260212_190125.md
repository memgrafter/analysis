---
ver: rpa2
title: 'OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language
  Models'
arxiv_id: '2402.10172'
source_url: https://arxiv.org/abs/2402.10172
tags:
- optimization
- optimus
- problem
- problems
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OptiMUS is an LLM-based agent that formulates and solves (mixed
  integer) linear programming problems from natural language descriptions. It employs
  a modular structure with specialized agents for preprocessing, formulating, coding,
  and evaluating optimization problems.
---

# OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models

## Quick Facts
- arXiv ID: 2402.10172
- Source URL: https://arxiv.org/abs/2402.10172
- Reference count: 22
- Key outcome: OptiMUS outperforms existing methods by over 20% on easy datasets and 30% on challenging datasets using a modular LLM-based agent system

## Executive Summary
OptiMUS is a novel LLM-based agent system that formulates and solves (mixed integer) linear programming problems from natural language descriptions. The system employs a modular structure with specialized agents for preprocessing, formulating, coding, and evaluating optimization problems. By using a connection graph to maintain relationships between constraints, variables, and parameters, OptiMUS can process complex problems without excessively long prompts. The approach demonstrates significant performance improvements over existing methods on both standard and newly introduced complex optimization problem datasets.

## Method Summary
OptiMUS uses a modular LLM-based agent architecture to transform natural language optimization problem descriptions into executable mathematical models. The system consists of five specialized agents: preprocessor, manager, formulator, programmer, and evaluator. A connection graph tracks relationships between problem elements, enabling context-efficient processing through short prompts. The system iteratively refines solutions through multiple agent calls, with the manager routing problems based on conversation history. Numerical data is separated from symbolic parameter definitions, allowing the LLM to work with problem structure without context overflow. The system interfaces with Gurobi solver through generated code.

## Key Results
- OptiMUS outperforms existing methods by over 20% on easy datasets and 30% on challenging datasets
- Introduces NLP4LP dataset with 67 complex optimization problems featuring long descriptions and extensive data
- Successfully handles problems with extensive descriptions and large data files while maintaining short prompts
- Demonstrates ability to iteratively fix initial mistakes through the modular agent system

## Why This Works (Mechanism)

### Mechanism 1
The connection graph enables OptiMUS to maintain context efficiency across multiple LLM calls by tracking which variables and parameters appear in each constraint. This allows the system to extract only relevant context for each prompt rather than passing entire problem descriptions to the LLM. The core assumption is that the connection graph accurately reflects the relationships between constraints, variables, and parameters. This breaks down if the graph becomes inconsistent with the actual formulations, potentially causing prompts to lack necessary context.

### Mechanism 2
The manager agent coordinates iterative refinement by routing problems to appropriate specialists. The manager examines conversation history and determines whether formulation, programming, or evaluation needs attention, creating a feedback loop that addresses errors systematically. The core assumption is that the manager can correctly identify which agent should address the current problem state. This fails if the manager makes incorrect routing decisions, potentially causing the system to enter ineffective loops.

### Mechanism 3
Separation of data from problem description enables scaling to large problems. Parameters are defined symbolically without numerical data, which is stored separately and loaded during code execution, preventing context overflow. The core assumption is that the LLM can work with symbolic parameter definitions without seeing the actual data values. This breaks down if symbolic parameter definitions are insufficient for the LLM to understand the problem structure.

## Foundational Learning

- Concept: Constraint satisfaction problem formulation
  - Why needed here: Understanding how to translate natural language constraints into mathematical form is fundamental to OptiMUS's operation
  - Quick check question: What are the three components required to define a parameter in OptiMUS?

- Concept: Linear programming duality
  - Why needed here: Helps understand why certain formulations work better and how constraints interact
  - Quick check question: How does the objective function relate to the constraints in a linear program?

- Concept: Solver integration patterns
  - Why needed here: Understanding how OptiMUS interfaces with Gurobi through generated code
  - Quick check question: What are the key steps in generating solver code from mathematical formulations?

## Architecture Onboarding

- Component map: Preprocessor -> Manager -> (Formulator/Programmer/Evaluator) -> (loop based on manager routing)
- Critical path: Preprocess → Formulator → Programmer → Evaluator → (loop based on manager routing)
- Design tradeoffs:
  - Modularity vs. efficiency: More modular design increases flexibility but may require more LLM calls
  - Context length vs. accuracy: Shorter prompts improve LLM performance but may lose necessary information
  - Symbolic vs. concrete parameters: Symbolic definitions enable scaling but require careful management
- Failure signatures:
  - Missing constraints: Preprocessor fails to extract all constraints
  - Incorrect model: Formulator creates wrong mathematical model
  - Runtime errors: Programmer generates code that doesn't execute
  - Semantic errors: Code runs but produces incorrect results
- First 3 experiments:
  1. Run OptiMUS on a simple NL4Opt problem and trace each agent's output
  2. Test connection graph by removing it and measuring prompt length changes
  3. Evaluate manager routing by forcing it to select different agents and observing outcomes

## Open Questions the Paper Calls Out

### Open Question 1
How would OptiMUS performance change if smaller LLMs were fine-tuned on the novel prompt templates used in the system? The paper notes that smaller LLMs show poor performance on OptiMUS's modular prompts, suggesting fine-tuning might improve their performance. This remains unresolved as the paper only tests OptiMUS with GPT-4 and GPT-3.5 without exploring fine-tuning approaches for smaller models.

### Open Question 2
What is the optimal balance between the number of agent calls and problem-solving accuracy for different problem complexity levels? The sensitivity analysis shows accuracy improves with more agent calls for complex datasets, but doesn't determine optimal numbers for different complexity levels. This remains unresolved as the paper only tests a limited range of maximum agent calls without systematically exploring optimal thresholds for different problem types.

### Open Question 3
How would incorporating user feedback during the optimization modeling process affect OptiMUS's performance and accuracy? The paper identifies this as an "exciting avenue" for future research, suggesting current OptiMUS doesn't incorporate user feedback. This remains unresolved as the current system operates autonomously without mechanisms for incorporating user corrections or preferences during the modeling process.

## Limitations

- The exact prompt templates and connection graph implementation details are not fully specified in the main text, making exact reproduction challenging
- The system's performance heavily depends on the quality of the underlying LLM (GPT-4), which may not be accessible to all researchers
- The paper doesn't extensively analyze the computational overhead introduced by multiple LLM calls and the iterative refinement process

## Confidence

- High Confidence: The modular architecture design and connection graph concept are well-supported by the paper's methodology section and make logical sense for scaling LLM-based optimization modeling
- Medium Confidence: The empirical performance claims (20% improvement on easy datasets, 30% on hard datasets) are supported by the presented results, but the evaluation methodology could be more detailed regarding statistical significance and comparison baselines
- Medium Confidence: The NLP4LP dataset creation and its role in demonstrating OptiMUS's capabilities is well-described, though the dataset itself is not publicly available for independent validation

## Next Checks

1. Implement the modular agent system using the described architecture and test with publicly available optimization problems to verify that the connection graph effectively reduces prompt length while maintaining accuracy

2. Evaluate OptiMUS on problems of increasing complexity and length to measure how well the modular approach scales compared to monolithic LLM prompting approaches, specifically measuring prompt length, number of LLM calls, and solution accuracy

3. Systematically introduce different types of errors (missing constraints, incorrect formulations, code bugs) and trace how the manager agent routes these to appropriate specialists, measuring the system's ability to iteratively correct mistakes