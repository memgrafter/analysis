---
ver: rpa2
title: Human Alignment of Large Language Models through Online Preference Optimisation
arxiv_id: '2403.08635'
source_url: https://arxiv.org/abs/2403.08635
tags:
- online
- preference
- policy
- ipo-md
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper establishes an equivalence between two preference optimisation
  methods, IPO and Nash-MD, by showing that online IPO finds the Nash equilibrium
  of a regularised preference game through self-play. Building on this insight, the
  authors propose IPO-MD, an algorithm that interpolates between offline and online
  variants using a mixture policy similar to Nash-MD.
---

# Human Alignment of Large Language Models through Online Preference Optimisation

## Quick Facts
- arXiv ID: 2403.08635
- Source URL: https://arxiv.org/abs/2403.08635
- Reference count: 40
- Primary result: IPO-MD algorithm interpolates between offline and online preference optimization methods while maintaining theoretical guarantees

## Executive Summary
This paper establishes an equivalence between online IPO (Implicit Preference Optimization) and self-play in a regularised preference game, leading to the development of IPO-MD, a new algorithm that interpolates between offline and online variants. The authors show that both methods find the same Nash equilibrium of the preference game under appropriate conditions. Experimental results on a summarisation task demonstrate that IPO and IPO-MD are the most robust algorithms, consistently outperforming other methods like DPO and SLiC in side-by-side evaluations.

## Method Summary
The paper develops a framework for human alignment of large language models through online preference optimization. It introduces IPO-MD, which generates data using a mixture policy (π₁₋ᵦ πᵣₑբ)ᵦ similar to Nash-MD, and uses a contrastive gradient update. The method operates on pairwise preferences between model outputs, using a Bradley-Terry preference model. The approach is evaluated on article summarization tasks using TL;DR and XSum datasets with T5X large language models, comparing online and offline variants of IPO, DPO, SLiC, and RL algorithms through side-by-side evaluations using PaLM2 as judge.

## Key Results
- IPO and IPO-MD are the most robust algorithms in side-by-side evaluations
- IPO-MD interpolates between offline and online variants using a mixture policy
- Online IPO and self-play in the regularised preference game perform equivalent expected updates
- IPO-MD and Nash-MD-PG share the same fixed points for any β value

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Online IPO and self-play in the regularised preference game perform equivalent expected updates.
- Mechanism: The gradient of the Online IPO loss is structurally identical to the self-play gradient against the current policy.
- Core assumption: Both algorithms sample from the same online data distribution and use the same preference model.
- Evidence anchors:
  - [abstract]: "Optimising the IPO loss with such a stream of data becomes then equivalent to finding the Nash equilibrium of the preference model through self-play."
  - [section 4.2]: Proposition 4.2 states "The expected gradient of the Online IPO loss ... is identical to the self-play update direction in the game with payoff as in Equation 9."
- Break condition: If the data distribution deviates from the current policy, or if the preference model changes between updates, the equivalence breaks.

### Mechanism 2
- Claim: IPO-MD and Nash-MD-PG share the same fixed points for any β value.
- Mechanism: Both algorithms converge to policies that are best-responses against the geometric mixture of online and reference policies.
- Core assumption: The mixture policy π₁₋ᵦ(πᵣₑբ)ᵦ is used for both sampling and computing the preference gradient.
- Evidence anchors:
  - [abstract]: "Building on this equivalence, we introduce the IPO-MD algorithm that generates data with a mixture policy ... similarly as the general Nash-MD algorithm."
  - [section 5.2]: Proposition 5.2 shows the fixed point of IPO-MD(β) coincides with the Nash equilibrium for a modified regularisation parameter.
- Break condition: If the mixture policy is not implemented correctly (e.g., one-step-at-a-time approximation), fixed points may diverge.

### Mechanism 3
- Claim: Online DPO and online IPO are distinct objectives except in the uniform-preference trivial case.
- Mechanism: The KKT conditions for stationary points differ; online IPO aligns with the Nash equilibrium of the regularised game, while online DPO aligns with the RLHF solution under Bradley-Terry.
- Core assumption: Preference probabilities are non-uniform and the action space has at least two elements.
- Evidence anchors:
  - [section 6]: "Theorem F.6 says that there is no 2-action problem for which the condition is satisfied, except when preferences are uniform."
  - [appendix F]: Detailed analysis of KKT conditions and counterexample construction.
- Break condition: If preferences are uniform (p(y ≻ y′) = 0.5) or the action space is singleton, the distinction collapses.

## Foundational Learning

- Concept: Nash equilibrium in two-player constant-sum games
  - Why needed here: Both IPO-MD and Nash-MD-PG frame preference optimisation as finding a Nash equilibrium of a regularised game.
  - Quick check question: If two players play best-responses to each other in a zero-sum game, what state do they converge to?

- Concept: Bradley-Terry model for pairwise comparisons
  - Why needed here: The preference model used throughout assumes Bradley-Terry logistic form for p(y ≻ y′).
  - Quick check question: What is the relationship between the reward difference r(y) - r(y′) and the preference probability p(y ≻ y′) under Bradley-Terry?

- Concept: Contrastive vs non-contrastive gradient estimates
  - Why needed here: IPO is contrastive (uses both samples in update), while Nash-MD-PG is non-contrastive (only updates on the sampled action).
  - Quick check question: In what scenario does a contrastive gradient estimate have lower variance than a non-contrastive one?

## Architecture Onboarding

- Component map:
  - Policy network πθ -> generates sequences given prompts
  - Preference model pϕ -> predicts p(y ≻ y′|x) given (x, y, y′)
  - Data pipeline -> samples (xi, y+, y-) from (µ, λp) for offline, or (xi, y, y′) from πθ for online
  - Loss functions -> LIPO, LDPO, LSLiC, or their online variants
  - Optimizer -> AdaFactor with learning rate 1e-4

- Critical path:
  1. Sample prompt x from validation/test set
  2. Generate two completions y, y′ from current policy
  3. Compute preference p(y ≻ y′|x) via preference model
  4. Calculate loss using chosen algorithm variant
  5. Backpropagate and update policy parameters
  6. Repeat until convergence or step limit

- Design tradeoffs:
  - Online vs offline: Online enables better regularisation but requires preference annotations on-the-fly
  - Contrastive vs non-contrastive: Contrastive is more data-efficient but may have higher variance
  - Mixture ratio β: Controls interpolation between online and reference policies; higher β = more conservative

- Failure signatures:
  - Policy collapsing to deterministic outputs: Indicates over-regularisation or insufficient diversity in preference data
  - Preference model overfitting: If pϕ predicts near-0 or near-1 too often, online updates may become unstable
  - Slow convergence: Could indicate poor choice of τ or β, or insufficient model capacity

- First 3 experiments:
  1. Verify equivalence: Compare gradients of Online IPO and self-play on a small synthetic preference matrix
  2. Sweep β: Evaluate IPO-MD performance across β ∈ {0.0, 0.125, 0.25, 0.5, 1.0} on validation set
  3. Ablation: Run offline vs online variants of IPO, DPO, and SLiC on same data to measure robustness gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does the contrastive gradient estimate in Online IPO have lower variance than the non-contrastive gradient estimate in Self-Play?
- Basis in paper: [explicit] The paper states in Appendix D that a sufficient condition for lower variance is Ey,y′∼π[∇ log π(y)∇ log π(y′)f(y,y′)²] ≥ 0, where f(y,y′) = p(y≻y′) - 1/2 - τ log π(y)/πref(y) + τ log π(y′)/πref(y′).
- Why unresolved: This condition depends on specific policy representations and preference models that are not fully characterized. The paper provides a sufficient but not necessary condition.
- What evidence would resolve it: A complete characterization of when the contrastive gradient estimate has lower variance, including both necessary and sufficient conditions, and empirical validation across different preference model architectures.

### Open Question 2
- Question: Can the Nash equilibrium solution of the regularised game (Equation 12) be a stationary point of online DPO for preference models beyond the Bradley-Terry model?
- Basis in paper: [explicit] Theorem F.7 shows this holds for Bradley-Terry preferences, but Lemma F.5 provides conditions that seem difficult to satisfy for general preference models.
- Why unresolved: The paper only proves the equivalence for Bradley-Terry preferences and shows the general conditions are difficult to satisfy. The question of whether there are other preference models that allow this equivalence remains open.
- What evidence would resolve it: Identification of preference models beyond Bradley-Terry that satisfy the conditions in Lemma F.5, or a proof that no such models exist.

### Open Question 3
- Question: How does the performance of IPO-MD and Online IPO scale to conversational agents using 100+ billion parameter models?
- Basis in paper: [inferred] The experiments are limited to 770M parameter models on a single task (summarisation), with the authors explicitly noting this as a limitation.
- Why unresolved: The paper only provides experimental results for T5X large models (770M parameters) on summarisation, not on full conversational agents with much larger models.
- What evidence would resolve it: Empirical results showing the performance of IPO-MD and Online IPO when scaled to 100+ billion parameter models on conversational tasks, including comparisons with other alignment methods at this scale.

## Limitations

- Theoretical analysis assumes infinite data streams and perfect preference annotations
- Practical implementation of IPO-MD's mixture policy introduces approximation errors
- Empirical evaluation is limited to a single task (summarization) and one judge model (PaLM2)
- Equivalence between online IPO and self-play relies on assumptions about data distribution and preference model stability

## Confidence

- IPO-Nash-MD equivalence: High - Supported by rigorous mathematical proofs in Propositions 4.2 and 5.2
- IPO-MD fixed point analysis: Medium - Theoretical derivation is sound, but practical approximation effects are not fully characterized
- Online IPO vs DPO distinction: High - Theorem F.6 provides formal proof with counterexamples
- Empirical robustness claims: Medium - Results are consistent but based on limited evaluation scenarios

## Next Checks

1. **Gradient equivalence verification**: Implement a synthetic preference matrix where the exact gradients of online IPO and self-play can be computed analytically, then verify they match within numerical precision.

2. **β-sweep robustness test**: Run IPO-MD across a finer grid of β values (0.0 to 1.0 in 0.1 increments) on both TL;DR and XSum datasets to map the full performance landscape and identify optimal regularization strength.

3. **Cross-task generalization**: Evaluate IPO-MD on a non-summarization task (e.g., dialogue response generation or code completion) with human preference annotations to test whether the robustness advantages extend beyond the summarization domain.