---
ver: rpa2
title: 'Extending LLMs to New Languages: A Case Study of Llama and Persian Adaptation'
arxiv_id: '2412.13375'
source_url: https://arxiv.org/abs/2412.13375
tags:
- persian
- language
- english
- tasks
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how to extend a large language model (Llama-2)
  to the Persian language using parameter-efficient fine-tuning. The authors collect
  Persian and bilingual (English-Persian) data, expand the model''s vocabulary with
  Persian tokens, and use a multi-stage training approach: embedding alignment with
  bilingual data, followed by LoRA-based fine-tuning on monolingual Persian data,
  and finally instruction-tuning.'
---

# Extending LLMs to New Languages: A Case Study of Llama and Persian Adaptation

## Quick Facts
- arXiv ID: 2412.13375
- Source URL: https://arxiv.org/abs/2412.13375
- Reference count: 13
- Key outcome: Parameter-efficient fine-tuning extends Llama-2 to Persian, with bilingual alignment improving classification while preserving English capabilities

## Executive Summary
This paper investigates extending Llama-2 to the Persian language using parameter-efficient fine-tuning techniques. The authors employ a multi-stage approach involving vocabulary expansion with Persian tokens, embedding alignment through bilingual data, LoRA-based fine-tuning on monolingual Persian data, and instruction-tuning. Experiments on classification and generation tasks demonstrate that bilingual alignment improves Persian classification performance while maintaining English capabilities, though cross-lingual transfer benefits are minimal for low-resource Persian. The study also compares the adapted model with other models including Llama-3.1, Mistral, Qwen2.5, Gemma2, and Gemini 1.5, finding competitive performance particularly on simple classification tasks.

## Method Summary
The method employs a multi-stage approach to extend Llama-2 to Persian. First, the model's vocabulary is expanded by 20,000 Persian tokens using SentencePiece, creating a total vocabulary of 49,816 tokens. Next, embedding alignment is performed by training only the heads and embeddings on bilingual next-token prediction tasks while freezing transformer layers. This is followed by LoRA-based fine-tuning on monolingual Persian data, updating heads, embeddings, and LoRA weights across all layers. Finally, instruction-tuning incorporates both English and Persian instructions. The model is trained using next-token prediction with LoRA rank 8 and α 32, using eight V100 GPU units with fp16 precision.

## Key Results
- Bilingual embedding alignment improves Persian classification accuracy while preserving or enhancing English task performance
- Additional monolingual pre-training benefits generation tasks but can degrade English text generation quality
- The model's initial strength in English proves critical when training data for Persian is limited
- The adapted Llama model achieves competitive performance compared to Llama-3.1, Mistral, Qwen2.5, Gemma2, and Gemini 1.5, especially on simple classification tasks

## Why This Works (Mechanism)

### Mechanism 1
Embedding alignment between English and Persian tokens improves classification performance for Persian while maintaining English capabilities. The model freezes transformer layers and trains only heads and embeddings using bilingual next-token prediction, aligning Persian token representations with English ones to leverage existing English knowledge for Persian tasks.

### Mechanism 2
Pre-training on monolingual Persian data improves generation performance but can degrade English text generation. After initial embedding alignment, LoRA-based fine-tuning on monolingual Persian data updates heads, embeddings, and LoRA weights across all layers to capture Persian linguistic structures.

### Mechanism 3
Model's initial strength in English is critical when training data is limited for the target language. Strong base English models can leverage their existing capabilities when fine-tuned on Persian tasks, with bilingual alignment providing additional benefits.

## Foundational Learning

- Parameter-efficient fine-tuning (LoRA): Enables adaptation to Persian while preserving English capabilities and reducing computational requirements. Quick check: How does LoRA achieve parameter efficiency while maintaining model performance?
- Cross-lingual transfer learning: Allows leveraging English model strength to improve Persian performance when Persian data is limited. Quick check: What factors determine the success of cross-lingual transfer between two languages?
- Token embedding alignment: Creates semantic bridges between English and Persian token spaces to enable knowledge transfer. Quick check: Why is embedding alignment necessary before fine-tuning on monolingual target language data?

## Architecture Onboarding

- Component map: Base model (Llama-2-7B with expanded vocabulary) -> Embedding layer (extended with Persian tokens) -> Transformer layers (frozen during alignment, partially updated during generation) -> LoRA adapters (added for efficient parameter updates) -> Heads (trained on bilingual and monolingual data)
- Critical path: Vocabulary expansion → Embedding alignment → LoRA fine-tuning → Instruction tuning
- Design tradeoffs: Parameter freezing vs. full fine-tuning (freezing preserves English capabilities but limits Persian performance gains), Monolingual vs. bilingual pre-training (bilingual improves classification, monolingual improves generation), LoRA rank selection (higher ranks capture more complex patterns but increase computational cost)
- Failure signatures: Degraded English performance (indicates over-adaptation to Persian), Poor Persian generation (suggests insufficient monolingual pre-training), Limited cross-lingual transfer (points to inadequate embedding alignment)
- First 3 experiments: 1) Evaluate classification performance on bilingual tasks after embedding alignment to verify cross-lingual knowledge transfer, 2) Test generation quality on Persian-only tasks after LoRA fine-tuning to assess monolingual adaptation, 3) Compare English task performance before and after Persian adaptation to measure knowledge preservation

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of cross-lingual transfer from English to Persian vary with different levels of linguistic distance between the source and target languages? The paper only examines Persian as the target language and does not compare the performance of cross-lingual transfer with other languages that have different degrees of linguistic distance from English.

### Open Question 2
What is the optimal balance between pre-training on monolingual data and bilingual data for improving the performance of language models on low-resource languages? The paper does not provide a clear guideline on the optimal balance between monolingual and bilingual pre-training for low-resource languages.

### Open Question 3
How does the performance of parameter-efficient fine-tuning (PEFT) techniques, such as LoRA, compare to full fine-tuning for adapting language models to new languages? The paper does not compare the performance of LoRA to full fine-tuning for adapting language models to new languages.

## Limitations

- The vocabulary expansion with 20,000 Persian tokens appears arbitrary without justification for this specific number, potentially affecting token coverage and model performance
- Evaluation shows minimal cross-lingual transfer benefits for low-resource Persian, suggesting the approach may not scale well to other low-resource languages with different linguistic structures
- The paper does not provide detailed hyperparameter settings (learning rate, batch size, epochs), making exact reproduction challenging

## Confidence

**High Confidence**: Embedding alignment improves Persian classification performance while preserving English capabilities; the adapted model achieves competitive performance compared to other models.

**Medium Confidence**: Model's initial strength is critical when training data is limited; additional monolingual pre-training primarily benefits generation tasks.

**Low Confidence**: Cross-lingual transfer benefits are minimal for low-resource Persian; the effectiveness of the 20,000-token expansion.

## Next Checks

1. Systematically test different numbers of Persian tokens (e.g., 5,000, 10,000, 15,000, 25,000) during vocabulary expansion to determine optimal token count for Persian language coverage while maintaining model efficiency.

2. Evaluate the adapted model on additional low-resource language pairs (e.g., Turkish, Arabic) to determine whether the observed minimal transfer benefits are specific to Persian or represent a general limitation of the approach.

3. Experiment with different fine-tuning strategies, including gradual unfreezing of transformer layers during pre-training and varying LoRA rank values, to identify optimal configurations that balance Persian performance gains with English capability preservation.