---
ver: rpa2
title: An Evolutionary Framework for Connect-4 as Test-Bed for Comparison of Advanced
  Minimax, Q-Learning and MCTS
arxiv_id: '2405.16595'
source_url: https://arxiv.org/abs/2405.16595
tags:
- algorithm
- minimax
- move
- q-learning
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic comparative analysis of Q-learning,
  Minimax, and Monte Carlo Tree Search (MCTS) algorithms in the Connect-4 game domain.
  The authors implement and evaluate enhanced versions of these algorithms, introducing
  a novel "Evolutionary Tournament" methodology for performance assessment.
---

# An Evolutionary Framework for Connect-4 as Test-Bed for Comparison of Advanced Minimax, Q-Learning and MCTS

## Quick Facts
- arXiv ID: 2405.16595
- Source URL: https://arxiv.org/abs/2405.16595
- Authors: Henry Taylor; Leonardo Stella
- Reference count: 22
- One-line primary result: MCTS-Decisive achieved the best win rates in Connect-4, followed by Minimax-ABMO and Q-Learning, with Q-Learning being fastest but weakest.

## Executive Summary
This paper presents a systematic comparative analysis of Q-learning, Minimax, and Monte Carlo Tree Search (MCTS) algorithms in the Connect-4 game domain. The authors implement and evaluate enhanced versions of these algorithms, introducing a novel "Evolutionary Tournament" methodology for performance assessment. Results show that MCTS-Decisive achieves the best win rates, followed by Minimax-ABMO and Q-Learning. Q-Learning exhibits the fastest decision-making speed but performs poorly in win rates. The Evolutionary Tournament reveals MCTS algorithms dominate the population, with MCTS-Decisive-5 emerging as the strongest strategy.

## Method Summary
The study compares Q-learning, Minimax, and MCTS algorithms in Connect-4 using base and enhanced variants. Q-Learning uses Epsilon-Greedy policy with decaying parameters, Minimax employs Alpha-Beta pruning with Move Ordering, and MCTS implements Upper Confidence Bounds for Trees (UCT) with Decisive Moves enhancement. Algorithms are evaluated through 50-game matches against control algorithms and an Evolutionary Tournament where winners reproduce and losers die out. Performance is measured via win rates and decision timing.

## Key Results
- MCTS-Decisive achieved the highest win rates among all algorithms tested
- Q-Learning was fastest in decision-making but had the lowest win rates
- Evolutionary Tournament methodology amplified performance differences, with MCTS algorithms dominating the population
- Minimax with Alpha-Beta pruning and Move Ordering performed well, particularly at odd depths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MCTS-Decisive outperforms standard MCTS by incorporating domain-specific heuristics that shortcut search in decisive game states
- Mechanism: When MCTS-Decisive encounters a winning move or a move that blocks the opponent's immediate win, it bypasses further exploration and selects that move directly
- Core assumption: The heuristic rules for identifying "decisive" moves are both accurate and computationally inexpensive compared to full MCTS simulation
- Evidence anchors:
  - [section] "Decisive Moves ensure game deciding moves are controlled by a heuristic which ensures the algorithm takes any winning move it finds."
  - [section] "It is shown that this enhancement improves the strength of the algorithm with little additional computational costs when compared to the reference UCT method."
- Break condition: If the heuristic fails to recognize a decisive state or introduces bias that misguides the search, MCTS-Decisive may perform worse than standard MCTS

### Mechanism 2
- Claim: The Evolutionary Tournament methodology amplifies performance differences by simulating a survival-of-fittest dynamic among algorithms
- Mechanism: Algorithms compete in pairwise games; winners reproduce (increase population share) while losers die out
- Core assumption: Win/loss outcomes in Connect-4 accurately reflect long-term algorithmic strength and are not dominated by random chance in early generations
- Evidence anchors:
  - [section] "Two agents were randomly selected from the population to play a game of Connect-4... If an agent wins then it reproduces... The loser of the game dies."
  - [section] "This is called Selection Pressure which drives the evolutionary process to favour stronger agents."
- Break condition: If the tournament lacks sufficient diversity or if early stochastic fluctuations disproportionately affect later results, the methodology may misrepresent true algorithmic strength

### Mechanism 3
- Claim: Alpha-Beta pruning combined with Move Ordering drastically reduces Minimax search space without sacrificing solution quality
- Mechanism: Alpha-Beta pruning eliminates branches that cannot influence the final decision by maintaining bounds (α, β). Move Ordering ensures stronger moves are evaluated first
- Core assumption: The ordering heuristic reliably ranks moves so that high-impact branches are explored early, maximizing pruning efficiency
- Evidence anchors:
  - [section] "Alpha-Beta Pruning has been shown to be a valuable modification for search efficiency... large parts of the tree which do not affect the outcome are cut."
  - [section] "Move Ordering aims to improve this by adding a small breadth search to the start of each node which provides an optimal sub-tree search order."
- Break condition: If the ordering heuristic is weak or if the game state space is highly symmetric, pruning gains may be minimal and computational savings lost

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: MDP formalism underlies Q-learning's value function updates and policy optimization
  - Quick check question: In an MDP, what do the components S, A, P, and R represent?

- Concept: Upper Confidence Bounds for Trees (UCT)
  - Why needed here: UCT balances exploration and exploitation in MCTS node selection, directly affecting search quality
  - Quick check question: How does the exploration term in UCT prevent premature convergence to suboptimal moves?

- Concept: Nash Equilibrium in zero-sum games
  - Why needed here: Connect-4 is a zero-sum game; understanding Nash equilibrium explains why Minimax search can find optimal strategies
  - Quick check question: In a two-player zero-sum game, how do the maximin and minimax values relate to the Nash equilibrium?

## Architecture Onboarding

- Component map: Three algorithm families (Q-learning, Minimax, MCTS) each with base and enhanced variants; Evolutionary Tournament engine; control algorithms (Random, Supervised, Heuristic); evaluation pipeline for timing and win rates
- Critical path: Training/parameter tuning → Algorithm execution → Match scheduling → Result aggregation → Population update (for tournament) → Reporting
- Design tradeoffs: Speed vs. strength (Q-learning fast but weak; MCTS strong but slow); exploration vs. exploitation (UCT parameter tuning); depth vs. pruning efficiency (Minimax)
- Failure signatures: Poor win rates despite high training iterations (Q-learning); excessive move times with minimal depth gains (Minimax); population collapse to one species too early (Evolutionary Tournament)
- First 3 experiments:
  1. Run each algorithm against Random control for 50 games to establish baseline win rates and move times
  2. Execute the Evolutionary Tournament with a small population (e.g., 5 algorithms) for 100 generations to observe selection dynamics
  3. Compare Minimax-ABMO-3 vs. Minimax-ABMO-6 to quantify depth impact on win rate and timing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the presence of an expert player during Q-Learning training affect long-term performance and state space coverage in Connect-4?
- Basis in paper: [explicit] The authors found that Q-Learning trained with Minimax as an opponent won 61% of games compared to self-play training, suggesting improved performance through expert guidance
- Why unresolved: The study only examined initial performance metrics and did not analyze long-term learning trajectories or state space exploration patterns
- What evidence would resolve it: Comparative analysis of state visitation frequencies, convergence rates, and performance over extended training periods between Q-Learning with expert guidance versus self-play

### Open Question 2
- Question: Why do odd and even Minimax search depths produce different performance outcomes in Connect-4?
- Basis in paper: [explicit] The authors observed that Minimax-ABMO with odd depths (3, 5) won 50% of games against Heuristic control, while even depths (4, 6) won 100%, contrary to expectations that deeper searches improve performance
- Why unresolved: The paper suggests this may be due to unbalanced terminal state evaluation but did not investigate this hypothesis systematically
- What evidence would resolve it: Systematic analysis of terminal state evaluation patterns, search tree structures, and game outcomes across a wider range of odd and even depths

### Open Question 3
- Question: What is the optimal balance between exploration and exploitation in MCTS for Connect-4, and how does this affect performance?
- Basis in paper: [explicit] The authors used Cp = 1/√2 based on theoretical recommendations, but noted that larger Cp values increase exploration tendency without empirical validation
- Why unresolved: The study did not empirically test different Cp values or analyze their impact on win rates and computational efficiency
- What evidence would resolve it: Empirical comparison of MCTS performance across varying Cp values, analyzing win rates, computational costs, and convergence behavior

### Open Question 4
- Question: How does the Decisive Moves enhancement in MCTS affect decision-making in different game phases?
- Basis in paper: [explicit] The authors found that MCTS-Decisive-5 won 60% of games against base MCTS, but did not analyze when or how often the heuristic was activated during games
- Why unresolved: The study did not examine the frequency or timing of Decisive Moves activation or its impact on game outcomes in different phases
- What evidence would resolve it: Detailed analysis of Decisive Moves activation frequency across game phases, correlation with win rates, and computational efficiency comparisons

### Open Question 5
- Question: How would a score-based evolutionary tournament framework affect the relative ranking of Connect-4 algorithms compared to the death penalty approach?
- Basis in paper: [inferred] The authors noted that their death penalty approach was suitable for zero-sum games but referenced [19] for a score-based approach that could provide more accurate placement of non-dominant algorithms
- Why unresolved: The study only implemented the death penalty mechanism without exploring alternative evolutionary frameworks
- What evidence would resolve it: Comparative analysis of algorithm rankings using both death penalty and score-based evolutionary tournament frameworks, with statistical validation of placement differences

## Limitations
- Unknown implementation details of the Supervised control algorithm referenced in the paper
- Lack of statistical significance testing for win rate differences between algorithms
- Computational resources required for MCTS-5 not quantified, raising scalability questions

## Confidence
- High Confidence: MCTS-Decisive outperforming standard MCTS, Q-Learning being faster but weaker, and Minimax-ABMO performance relative to standard Minimax
- Medium Confidence: Evolutionary Tournament methodology effectively amplifying performance differences, as the methodology is novel and not independently validated
- Low Confidence: The exact contribution of each enhancement (Move Ordering, Decisive Moves) to overall performance, as these are presented as combined improvements

## Next Checks
1. Conduct t-tests or bootstrap analysis on the 50-game match results to determine if observed win rate differences are statistically significant
2. Systematically vary key hyperparameters (Cp in UCT, alpha and epsilon decay rates in Q-Learning, search depths in Minimax) to assess robustness of conclusions
3. Measure MCTS-5 performance on larger game boards or with increased computation time to evaluate practical limitations and scaling behavior