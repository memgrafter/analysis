---
ver: rpa2
title: 'ScaMo: Exploring the Scaling Law in Autoregressive Motion Generation Model'
arxiv_id: '2412.14559'
source_url: https://arxiv.org/abs/2412.14559
tags: []
core_contribution: This paper explores the scaling law in autoregressive motion generation,
  addressing the challenges of limited data scale and quality, difficulties in scaling
  vocabulary size, and insufficient model architecture scalability. The authors propose
  a scalable motion generation system, ScaMo, which includes a motion tokenizer (Motion
  FSQ-VAE) and a text-prefix autoregressive transformer.
---

# ScaMo: Exploring the Scaling Law in Autoregressive Motion Generation Model

## Quick Facts
- arXiv ID: 2412.14559
- Source URL: https://arxiv.org/abs/2412.14559
- Reference count: 40
- Key outcome: Demonstrates scaling laws in autoregressive motion generation and proposes ScaMo system with FSQ-VAE tokenizer and text-prefix transformer

## Executive Summary
This paper investigates scaling laws in autoregressive motion generation models, addressing key challenges including limited data scale, vocabulary size scaling difficulties, and insufficient model architecture scalability. The authors propose ScaMo, a comprehensive system featuring a motion FSQ-VAE tokenizer and text-prefix autoregressive transformer. Their analysis reveals logarithmic relationships between normalized test loss and FLOPs, and power laws between non-vocabulary parameters, vocabulary parameters, and data tokens with respect to FLOPs. The work provides valuable insights for predicting optimal model configurations and demonstrates superior performance on complex text inputs.

## Method Summary
The ScaMo framework consists of two main components: a motion FSQ-VAE tokenizer and a text-prefix autoregressive transformer. The FSQ-VAE resolves codebook collapse issues by using finite scalar quantization instead of vector quantization, enabling effective scaling with larger codebook sizes. The text-prefix transformer employs bidirectional attention for text tokens and causal attention for motion tokens, improving text encoding. The system is trained on the MotionUnion dataset (~150k sequences, ~30M frames) and evaluated on the HumanML3D benchmark using metrics including normalized test loss, FID, R-precision, and matching score.

## Key Results
- Demonstrates logarithmic scaling between normalized test loss and FLOPs, and power law scaling between model parameters and vocabulary size with FLOPs
- Shows larger models and vocabulary sizes lead to better performance in text-driven motion generation
- Successfully predicts optimal model size, vocabulary size, and data requirements for a compute budget of 1e18

## Why This Works (Mechanism)
The finite scalar quantizer in FSQ-VAE prevents codebook collapse by maintaining stable codebook utilization during training, allowing for larger vocabulary sizes without degradation. The text-prefix autoregressive transformer architecture enables effective bidirectional encoding of text prompts while maintaining autoregressive generation for motion sequences, resulting in better text-motion alignment and generation quality.

## Foundational Learning

### Finite Scalar Quantization
- **Why needed**: Prevents codebook collapse in large-scale motion tokenizers where traditional VQ-VAE fails
- **Quick check**: Monitor codebook utilization metrics during training to ensure stable distribution

### Text-Prefix Autoregressive Architecture
- **Why needed**: Enables bidirectional attention for text encoding while maintaining causal generation for motion
- **Quick check**: Verify prefix attention implementation by comparing text-motion alignment scores across different attention configurations

### Motion Data Preprocessing
- **Why needed**: Standardizes motion data from diverse sources for consistent training
- **Quick check**: Validate preprocessing pipeline by reconstructing sample motions and measuring reconstruction loss

## Architecture Onboarding

### Component Map
FSQ-VAE (encoder -> quantizer -> decoder) -> Text-Prefix Transformer (bidirectional text attention + causal motion attention) -> Generation Output

### Critical Path
Motion data -> FSQ-VAE tokenizer -> Text-prefix transformer -> Generated motion sequences

### Design Tradeoffs
- **FSQ-VAE vs VQ-VAE**: FSQ-VAE enables larger vocabulary sizes but may have different computational characteristics
- **Bidirectional vs Causal Attention**: Bidirectional for text improves encoding but requires careful implementation to maintain autoregressive generation

### Failure Signatures
- **Codebook collapse**: Low codebook utilization, high reconstruction loss
- **Poor text-motion alignment**: Low R-precision and matching scores on HumanML3D
- **Scaling breakdown**: Deviation from predicted logarithmic/power law relationships

### First Experiments
1. Train FSQ-VAE with varying codebook sizes (28-216) and measure codebook utilization and reconstruction loss
2. Implement and validate text-prefix attention mechanism with synthetic data
3. Train small-scale models (44M parameters) and verify basic scaling relationships

## Open Questions the Paper Calls Out
None

## Limitations
- Scaling law analysis based on MotionUnion dataset combining multiple sources with varying quality, introducing uncontrolled variables
- Analysis focuses specifically on text-driven motion generation, limiting generalizability to other modalities
- Extrapolation of scaling laws assumes current trends will continue, which may not hold at extreme scales

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Empirical observations of scaling laws | High |
| Effectiveness of FSQ-VAE approach | High |
| Extrapolation of scaling laws for future predictions | Medium |
| Generalization to different motion generation tasks | Low |

## Next Checks
1. Train ScaMo on a completely different motion dataset (e.g., AMASS or HumanAct12) to verify scaling laws across different motion capture sources
2. Systematically vary dataset quality and quantity independently to isolate effects on scaling relationships
3. Implement and compare FSQ-VAE against traditional VQ-VAE with various codebook sizes to quantify advantages of scalar quantization