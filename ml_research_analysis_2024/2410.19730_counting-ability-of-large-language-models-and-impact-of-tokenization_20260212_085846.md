---
ver: rpa2
title: Counting Ability of Large Language Models and Impact of Tokenization
arxiv_id: '2410.19730'
source_url: https://arxiv.org/abs/2410.19730
tags:
- counting
- counter
- tokenization
- llms
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how tokenization affects the counting ability
  of large language models (LLMs). It shows that while Chain-of-Thought (CoT) reasoning
  can enable counting in Transformers, the choice of tokenization significantly impacts
  performance.
---

# Counting Ability of Large Language Models and Impact of Tokenization

## Quick Facts
- arXiv ID: 2410.19730
- Source URL: https://arxiv.org/abs/2410.19730
- Authors: Xiang Zhang; Juntai Cao; Chenyu You
- Reference count: 14
- Large language models' counting accuracy improves dramatically (from ~50% to >96%) when tokenization separates each item into distinct tokens

## Executive Summary
This paper investigates how tokenization affects the counting ability of large language models (LLMs). The authors demonstrate that while Chain-of-Thought (CoT) reasoning can enable counting in Transformers, the choice of tokenization significantly impacts performance. By manipulating tokenization in black-box models through string formatting, they show that separating each item (e.g., letters) into distinct tokens greatly improves counting accuracy. The study reveals that rare tokens in natural language are more sensitive to counting tasks, likely due to simpler embeddings with less embedded information.

## Method Summary
The study uses a black-box approach to manipulate tokenization in GPT-4o mini and Claude-3.5-sonnet by formatting input strings in four different ways: pure BPE tokens, dash-deliminated tokens, comma-space-deliminated tokens, and precise-item tokens where each letter is separated. The models are prompted using supervised CoT reasoning, where they explicitly output counter values at each step. The experiment uses 1000 randomly generated strings of varying lengths (10-20, 20-30, and 30-40 characters) consisting of letters 'a' and 'b', and measures counting accuracy across different tokenization types.

## Key Results
- GPT-4o mini accuracy increased from ~50% with BPE tokens to >96% with precise-item tokenization
- Without CoT, counting performance is worse than with CoT in the same setting
- Rare tokens outperform frequent tokens in counting tasks due to simpler embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoT shifts inductive counting from latent space to text space, enabling sequential computation
- Mechanism: CoT externalizes intermediate counter values as text tokens, allowing recurrent-like updates through embedding-layer conversions
- Core assumption: The model can reliably extract and update counter values from/to text during reasoning
- Evidence anchors:
  - [abstract] "Recent work has highlighted how Chain of Thought (CoT) reasoning can help alleviate some of the architectural limitations of Transformers in counting tasks."
  - [section 3.2] "Instead of simply outputting a final counting result y after processing an input sequence x = (x 1, x2, x3, · · · ), LLMs are guided to output intermediate reasoning steps, in this case counter value, after processing each input unit xi."
  - [corpus] Weak - the corpus neighbors focus on tokenization issues rather than CoT mechanisms specifically
- Break condition: If the model fails to correctly extract counter values from text, the recurrence breaks down

### Mechanism 2
- Claim: Tokenization granularity directly impacts counting accuracy by affecting token awareness
- Mechanism: BPE tokenization groups letters into tokens, creating a mismatch between counting targets (letters) and processing units (tokens), leading to token-awareness gaps
- Core assumption: Models rely on token-level embeddings for reasoning, and awareness of token composition affects accuracy
- Evidence anchors:
  - [section 4.1] "Since LLMs are trained to predict entire tokens rather than individual letters, specific token properties may not be fully aware by the model unless they can be inferred from the training corpus."
  - [section 5.2.2] "Notably, without any special tokenization (type (a)), counting performance is even worse than not using CoT in the same setting."
  - [corpus] Moderate - multiple papers in corpus neighbors discuss tokenization's role in counting failures
- Break condition: If token-awareness gaps are eliminated through better tokenization, this mechanism's negative impact disappears

### Mechanism 3
- Claim: Token frequency affects counting sensitivity through embedding complexity
- Mechanism: Rare tokens have simpler embeddings with less embedded information, making them easier to identify through attention calculations during counting
- Core assumption: Token embeddings encode information from training data, and frequent tokens embed more complex information
- Evidence anchors:
  - [section 5.4] "We suspect this discrepancy is due to differences in letter frequency in the natural language, which may affect token-embedding sensitivity in the model."
  - [section 5.4] "We hypothesize that lower-frequency tokens contain less embedded information due to fewer occurrences during training, making them easier to identify through the attention mechanism."
  - [corpus] Weak - corpus neighbors focus on counting mechanisms but not specifically on token frequency effects
- Break condition: If token embeddings are normalized or frequency effects are mitigated through training, this mechanism's impact diminishes

## Foundational Learning

- Concept: Transformer computational limitations (TC0 complexity)
  - Why needed here: Understanding why standard Transformers struggle with counting tasks that require inductive reasoning
  - Quick check question: What complexity class do Transformers fall into that limits their ability to perform counting tasks?

- Concept: Chain-of-Thought reasoning mechanism
  - Why needed here: CoT is the proposed solution that enables counting by shifting computation to text space
  - Quick check question: How does CoT transform the reasoning process from latent space to text space for counting tasks?

- Concept: Tokenization and BPE mechanics
  - Why needed here: Tokenization choice directly impacts the model's ability to count by affecting token awareness and granularity
  - Quick check question: Why does grouping letters into BPE tokens create problems for letter-level counting tasks?

## Architecture Onboarding

- Component map: Black-box LLM API → String formatter → Tokenization manipulator → Counting prompt → Response parser
- Critical path: String formatting → Tokenization manipulation → CoT prompt generation → LLM API call → Result extraction
- Design tradeoffs: Black-box approach sacrifices control over tokenizer for generalizability across models
- Failure signatures: Accuracy drops with BPE tokens, improvement with item-separated tokens, rare tokens outperforming frequent ones
- First 3 experiments:
  1. Compare no-CoT vs CoT performance on pure BPE tokenization to establish baseline CoT benefit
  2. Test different delimiter types (space, comma-space, quotes) to find optimal item-separated tokenization
  3. Vary letter frequencies in counting tasks to verify token frequency sensitivity hypothesis

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses exclusively on letter-counting tasks using two specific models, limiting generalizability to other counting tasks or model architectures
- The black-box approach prevents analysis of how tokenization changes interact with model internals
- The paper doesn't investigate the underlying embedding mechanisms that make rare tokens more sensitive to counting tasks

## Confidence

**High Confidence:** The core finding that tokenization granularity affects counting accuracy is well-supported by the experimental data (96% vs 50% accuracy).

**Medium Confidence:** The mechanism explaining why CoT reasoning enables counting through shifting computation to text space is plausible but relies on several assumptions about model behavior.

**Low Confidence:** The hypothesis that token frequency affects counting sensitivity through embedding complexity lacks direct evidence and causal mechanisms.

## Next Checks
1. Evaluate the same tokenization manipulations on numerical counting tasks and object counting in image descriptions to determine if effects generalize beyond letter-counting
2. Analyze actual embeddings of high-frequency versus low-frequency tokens to quantify differences in embedding complexity and test correlation with counting accuracy
3. Compare counting performance across different tokenization algorithms (BPE, WordPiece, SentencePiece) and varying vocabulary sizes to isolate whether effects are specific to BPE