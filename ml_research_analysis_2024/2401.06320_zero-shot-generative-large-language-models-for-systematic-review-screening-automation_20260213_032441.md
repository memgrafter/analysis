---
ver: rpa2
title: Zero-shot Generative Large Language Models for Systematic Review Screening
  Automation
arxiv_id: '2401.06320'
source_url: https://arxiv.org/abs/2401.06320
tags:
- systematic
- review
- screening
- recall
- reviews
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of zero-shot large language models
  (LLMs) for automatic screening in systematic reviews. The authors evaluate eight
  different LLMs and propose a calibration technique that uses a predefined recall
  threshold to determine whether a publication should be included in a systematic
  review.
---

# Zero-shot Generative Large Language Models for Systematic Review Screening Automation

## Quick Facts
- arXiv ID: 2401.06320
- Source URL: https://arxiv.org/abs/2401.06320
- Reference count: 40
- Key outcome: Calibrated ensemble of zero-shot LLMs approaches predefined recall targets and saves significant screening time

## Executive Summary
This study investigates the use of zero-shot large language models (LLMs) for automatic screening in systematic reviews. The authors evaluate eight different LLMs and propose a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Their comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders LLMs practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches. Specifically, the calibrated ensemble approach achieves the best results overall and approaches the predefined recall target for the test topics, indicating practical use.

## Method Summary
The study uses eight zero-shot LLMs (LlaMa, Alpaca, Guanaco, Falcon, LlaMa2 variants) with 2048 token limits for systematic review screening. The approach involves uncalibrated screening comparing P(yes|d,t) vs P(no|d,t) token probabilities, and calibrated screening using threshold θ determined via leave-one-out cross-validation or seed studies. Models are evaluated on five test collections using metrics including Precision, Recall, Balanced Accuracy, F3, Success Rate at 0.95 recall, and Work Saved by Sampling. An ensemble method combining multiple models using CombSUM is also tested against BioBERT baseline.

## Key Results
- Calibrated ensemble of zero-shot LLMs achieves best overall results and approaches predefined recall targets
- Instruction fine-tuning significantly impacts screening effectiveness across different LLM variants
- Combined zero-shot model ensemble saves significant screening time compared to state-of-the-art approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot LLMs can perform systematic review screening without task-specific fine-tuning.
- Mechanism: LLMs generate probability distributions for "yes" and "no" tokens in response to a prompt containing the review title and candidate document; these probabilities directly indicate inclusion likelihood.
- Core assumption: The pre-trained knowledge and instruction-following capabilities of LLMs are sufficient to understand medical document relevance without additional domain-specific training.
- Evidence anchors:
  - [abstract] "instruction fine-tuning plays an important role in screening"
  - [section] "uncalibrated screening directly compares the absolute values of the token likelihoods P(yes|d,t) and P(no|d,t)"
  - [corpus] Weak: No corpus papers specifically validate zero-shot performance on systematic review screening
- Break condition: If LLMs fail to produce meaningful "yes"/"no" token probabilities for medical abstracts, or if instruction-following ability degrades with document complexity.

### Mechanism 2
- Claim: Calibration using a predefined recall threshold makes LLMs practical for systematic review screening.
- Mechanism: Normalizing the difference between "yes" and "no" token likelihoods across all documents for a topic, then setting a threshold θ to ensure minimum recall (e.g., 0.95).
- Core assumption: The distribution of token likelihood differences is consistent enough across documents within a topic to allow meaningful normalization and threshold setting.
- Evidence anchors:
  - [abstract] "calibration renders LLMs practical for achieving a targeted recall"
  - [section] "we identify a threshold θ using training data; θ is determined such that when used as the lower bound on scores for inclusion decisions, it ensures a minimum recall rate k"
  - [corpus] Weak: Calibration concept borrowed from NLP but not specifically validated for systematic review screening in corpus
- Break condition: If normalization fails to produce meaningful score distributions, or if setting θ based on training data doesn't generalize to unseen topics.

### Mechanism 3
- Claim: Ensembling multiple LLMs and baseline methods improves screening effectiveness.
- Mechanism: Combining token likelihoods or normalized scores from multiple models (e.g., LLaMA2-7B-ins, LLaMA2-13B-ins, BioBERT) using CombSUM to make final inclusion decisions.
- Core assumption: Different models capture complementary aspects of document relevance, and their combination yields better performance than individual models.
- Evidence anchors:
  - [abstract] "combining both with an ensemble of zero-shot models saves significant screening time"
  - [section] "we also consider an ensemble of screening methods... We use CombSUM to fuse the individual methods' decisions"
  - [corpus] Moderate: Ensemble methods are well-established in IR, but specific application to systematic review screening with LLMs is novel
- Break condition: If models' predictions are highly correlated (no diversity), or if ensemble weighting doesn't improve over best individual model.

## Foundational Learning

- Concept: Token probability distributions in generative LLMs
  - Why needed here: Understanding how LLMs produce "yes"/"no" probabilities for classification
  - Quick check question: What does the LLM actually output when asked to classify a document as "yes" or "no"?

- Concept: Min-max normalization for score calibration
  - Why needed here: Ensuring scores are comparable across different documents within the same topic
  - Quick check question: How does min-max normalization handle cases where all documents have very similar scores?

- Concept: F3 metric and its emphasis on recall
  - Why needed here: Systematic reviews prioritize recall (finding all relevant documents) over precision
  - Quick check question: How does F3 differ from F1, and why is it more appropriate for systematic review screening?

## Architecture Onboarding

- Component map:
  Input layer -> Prompt generator -> LLM inference engine -> Token probability extraction -> (Calibration) -> (Ensemble) -> Output layer

- Critical path:
  Input → Prompt generation → LLM inference → Token probability extraction → (Calibration) → (Ensemble) → Binary decision

- Design tradeoffs:
  - Zero-shot vs. fine-tuned: Zero-shot avoids labeling costs but may sacrifice accuracy
  - Calibration vs. uncalibrated: Calibration ensures recall targets but requires threshold setting
  - Single model vs. ensemble: Ensemble improves robustness but increases computational cost

- Failure signatures:
  - All documents classified as "included" (e.g., Guanaco-7b-ins behavior)
  - Extremely low recall despite high precision
  - Inconsistent performance across different review topics
  - High computational cost preventing real-time screening

- First 3 experiments:
  1. Compare token probabilities for "yes"/"no" across different LLM architectures on a small set of documents
  2. Test calibration threshold setting using leave-one-out cross-validation on a single review topic
  3. Evaluate ensemble performance by combining top-2 LLMs vs. single best LLM on a small dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold value (θ) for calibrated screening models in systematic review document screening, and how does this threshold vary across different types of systematic reviews?
- Basis in paper: [explicit] The paper discusses two approaches to determine θ: "extrapolation from collection" and "calibration with seed studies", but does not provide a definitive answer on the optimal threshold.
- Why unresolved: The paper mentions that the threshold value is crucial for achieving the desired recall rate, but it does not provide a clear method for determining the optimal threshold for different types of systematic reviews.
- What evidence would resolve it: Empirical studies comparing the performance of different threshold values across various types of systematic reviews, and the development of a method to automatically determine the optimal threshold based on the characteristics of the systematic review.

### Open Question 2
- Question: How does the quality and quantity of seed documents affect the performance of the calibration with seed studies method in systematic review document screening?
- Basis in paper: [explicit] The paper mentions that the calibration with seed studies method could only be tested on the Seed Collection dataset, and that LLaMa2-13b-ins displayed more volatile effectiveness in this setting, possibly due to the varying quality and quantity of seed documents across different topics.
- Why unresolved: The paper does not provide a detailed analysis of how the quality and quantity of seed documents impact the performance of the calibration with seed studies method.
- What evidence would resolve it: Empirical studies investigating the relationship between the quality and quantity of seed documents and the performance of the calibration with seed studies method, and the development of a method to select or generate high-quality seed documents.

### Open Question 3
- Question: How does the sensitivity of generative LLMs to prompt formulation affect their performance in systematic review document screening, and what are the best practices for prompt engineering in this context?
- Basis in paper: [explicit] The paper mentions that generative LLMs are sensitive to prompt formulation, but does not provide a detailed analysis of how this sensitivity affects their performance in systematic review document screening.
- Why unresolved: The paper does not provide a comprehensive investigation of the impact of prompt formulation on the performance of generative LLMs in systematic review document screening, and does not offer guidelines for prompt engineering in this context.
- What evidence would resolve it: Empirical studies comparing the performance of different prompt formulations in systematic review document screening, and the development of a set of best practices for prompt engineering in this context.

## Limitations
- Limited empirical validation across diverse medical domains and review types
- Calibration threshold determination challenges with generalization concerns
- Computational constraints from 2048 token limit potentially causing content truncation

## Confidence
- High confidence: Core mechanism of using token likelihoods for classification is technically sound
- Medium confidence: Zero-shot performance effectiveness shows substantial variation across models and test collections
- Low confidence: Practical applicability across diverse medical domains and complex review types

## Next Checks
1. Cross-domain validation: Test calibrated ensemble approach on systematic reviews from medical specialties not represented in CLEF TAR collections
2. Threshold robustness analysis: Systematically vary minimum recall target (k) from 0.90 to 0.99 and measure threshold stability
3. Truncation impact assessment: Compare screening performance using truncated vs. full document content to quantify 2048 token limit effects