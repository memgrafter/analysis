---
ver: rpa2
title: 'LaB-CL: Localized and Balanced Contrastive Learning for improving parking
  slot detection'
arxiv_id: '2410.07832'
source_url: https://arxiv.org/abs/2410.07832
tags:
- parking
- detection
- slot
- local
- junction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces LaB-CL, a supervised contrastive learning
  framework designed to improve parking slot detection by addressing data imbalance
  in two classification tasks: junction identification and shape classification. The
  method employs local representations from patches in images, includes class prototypes
  in every mini batch to ensure balanced learning, and introduces a hard negative
  sampling strategy that selects local representations with high prediction error.'
---

# LaB-CL: Localized and Balanced Contrastive Learning for improving parking slot detection

## Quick Facts
- arXiv ID: 2410.07832
- Source URL: https://arxiv.org/abs/2410.07832
- Reference count: 32
- Primary result: 99.57% precision and 99.81% recall on Tongji Parking-Slot benchmark

## Executive Summary
LaB-CL introduces a supervised contrastive learning framework specifically designed for parking slot detection that addresses data imbalance through local representations and balanced learning. The method employs class prototypes, hard negative sampling based on prediction error, and additional attraction losses to create compact class clusters in the representation space. Experiments on the Tongji Parking-Slot benchmark demonstrate state-of-the-art performance with precision of 99.57% and recall of 99.81%.

## Method Summary
LaB-CL is a supervised contrastive learning framework for parking slot detection that uses localized representations from image patches. The method includes class prototypes transformed from classifier weights to ensure balanced learning across minority and majority classes, employs a hard negative sampling strategy that selects samples with high prediction error, and uses additional attraction losses to promote within-class representation collapse. The framework processes 512×512 pixel images through a ResNet-50 encoder, creates local representation tensors with grid size G=16, and uses three-layer MLP projectors for contrastive learning. The system is trained for 1,000 epochs with specific loss weights and memory banks for hard negative storage.

## Key Results
- Achieved 99.57% precision and 99.81% recall on Tongji Parking-Slot benchmark dataset
- Outperformed state-of-the-art methods by significant margins
- Demonstrated effective handling of data imbalance between junction and background classes

## Why This Works (Mechanism)

### Mechanism 1
Local class prototypes in every mini-batch ensure balanced learning across minority and majority classes. By including transformed classifier weights as part of the contrastive loss, the model sees examples from all classes in every batch, preventing bias toward the majority class. Core assumption: Local representations are informative enough to act as class prototypes when transformed via MLP from classifier weights. Break condition: If the MLP transformation fails to produce discriminative prototypes, the balancing effect weakens.

### Mechanism 2
Hard negative sampling based on high prediction error improves contrastive learning effectiveness. Selecting background samples with highest prediction error and storing them in a memory bank increases the proportion of informative negatives, leading to better class separation. Core assumption: Prediction error is a reliable proxy for representation hardness. Break condition: If the 2% threshold is too low or high, the memory bank may lack diversity or be dominated by false positives.

### Mechanism 3
Further attraction loss encourages within-class representation collapse, enhancing class compactness. The additional loss pulls local representations toward their class mean prototypes more strongly, creating tighter class clusters in the representation space and improving classifier simplicity. Core assumption: Increased within-class compactness improves generalization without harming inter-class separability. Break condition: If attraction is too strong, it may cause over-compression and loss of discriminative features.

## Foundational Learning

- **Supervised contrastive learning**: Balances class representation in imbalanced junction detection datasets where junctions are rarer than background. Quick check: In supervised CL, what is the difference between attracting same-class samples and repelling different-class samples?

- **Local vs global representations**: Junction detection requires localized feature maps corresponding to image patches, unlike global representations used in image classification. Quick check: How does grid size G in the local representation tensor affect junction detection resolution?

- **Memory banks in contrastive learning**: Stores hard negatives across batches to maintain consistent challenging examples, especially important for imbalanced junction/background classification. Quick check: What is the purpose of maintaining separate memory banks for junctions (MJ) and background (MB)?

## Architecture Onboarding

- **Component map**: Input image → Encoder (ResNet-50) → Local representation tensor (G×G×D) → Projector MLP → CL space representations (zid, zsh) → Classifier linear layer → Junction ID and shape classification → Regression head → Location and rotation prediction → Memory banks (MJ, MB) → Hard negative storage → Local prototypes → Transformed classifier weights

- **Critical path**: Input image → Encoder → Projector → Classifier + Memory bank update → Contrastive loss → Backpropagation

- **Design tradeoffs**: Grid size G (larger → smaller patches → more localized features but more parameters), Memory bank size (larger → more diverse negatives but higher memory cost), Hard negative sampling rate (higher → more challenging negatives but risk of false positives)

- **Failure signatures**: Model only detects majority class (background) → prototype transformation or memory bank not working; Poor recall despite high precision → hard negative sampling too aggressive or memory bank unbalanced; Slow convergence → temperature τ or loss weight hyperparameters need adjustment

- **First 3 experiments**: 1) Baseline without CL vs with CL only, 2) CL with random negatives vs CL with hard negative sampling, 3) CL with/without local prototypes to test balancing effect

## Open Questions the Paper Calls Out

### Open Question 1
How would LaB-CL perform with a learnable post-processing module instead of hand-crafted post-processing techniques? The authors expect to further improve performances by replacing hand-crafted postprocessing with a learnable module, but do not implement or test this approach.

### Open Question 2
Would the proposed hard negative sampling strategy be effective for other object detection tasks beyond parking slot detection? The method is only validated on parking slot detection, and its effectiveness for other object detection problems remains unexplored.

### Open Question 3
How does the performance of LaB-CL scale with larger grid sizes G for local representations? The paper only reports results for G=16 and does not investigate how varying G affects precision, recall, or computational efficiency.

## Limitations
- Performance gains demonstrated only on the Tongji Parking-Slot dataset, limiting generalizability claims
- Computational overhead from memory banks and additional loss terms not addressed
- No sensitivity analysis on the arbitrary 2% threshold for hard negative sampling

## Confidence
- **High confidence**: The core problem of data imbalance in parking slot detection and the general approach of using contrastive learning for improved representation learning
- **Medium confidence**: The specific mechanisms of local prototype inclusion and hard negative sampling, as these are theoretically justified but lack comprehensive ablation studies
- **Low confidence**: The claims about promoting a "regular simplex configuration" in the representation space, as the paper does not provide visualization or quantitative analysis of the learned representation geometry

## Next Checks
1. **Ablation study on local prototype quality**: Replace the MLP transformation with alternative methods (e.g., direct classifier weights, learned prototypes) and measure impact on performance to validate the necessity of the proposed transformation.

2. **Hard negative sampling sensitivity**: Systematically vary the hard negative sampling rate (0%, 1%, 2%, 5%, 10%) and evaluate the trade-off between improved discrimination and potential false positive contamination in the memory bank.

3. **Generalization test**: Evaluate LaB-CL on an external parking dataset or a modified version of PS2.0 with different parking slot configurations to assess robustness beyond the training distribution.