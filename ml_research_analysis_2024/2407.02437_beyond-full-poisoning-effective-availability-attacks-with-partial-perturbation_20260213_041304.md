---
ver: rpa2
title: 'Beyond Full Poisoning: Effective Availability Attacks with Partial Perturbation'
arxiv_id: '2407.02437'
source_url: https://arxiv.org/abs/2407.02437
tags:
- data
- availability
- attack
- clean
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Parameter Matching Attack (PMA), the first
  availability attack effective when only a portion of training data is perturbed.
  PMA addresses the limitation of existing attacks that require 100% poison ratio
  to degrade model performance.
---

# Beyond Full Poisoning: Effective Availability Attacks with Partial Perturbation

## Quick Facts
- arXiv ID: 2407.02437
- Source URL: https://arxiv.org/abs/2407.02437
- Reference count: 34
- Key outcome: Achieves over 30% performance degradation at 40% poison ratio

## Executive Summary
This paper introduces Parameter Matching Attack (PMA), a novel availability attack that effectively degrades model performance even when only a portion of training data is poisoned. Unlike previous attacks requiring 100% poison ratio, PMA achieves significant performance degradation with as little as 40% poisoned data. The attack optimizes perturbations to minimize the distance between model parameters trained on mixed clean and poisoned data and a deliberately poorly-performing destination model. Through extensive experiments on four benchmark datasets, PMA outperforms seven baseline methods and remains effective against various static and partial adaptive defenses.

## Method Summary
PMA uses a bi-level optimization framework where perturbations are optimized to minimize the distance between intermediate model parameters trained on clean+poisoned data and a destination model trained with random incorrect labels. The method operates through an iterative process: (1) optimizing perturbations using a surrogate model, (2) training the target model on the mixture of poisoned and clean data, and (3) evaluating performance on clean test data. The approach uses ℓ∞ norm perturbation bounds (25/255) and can work with different poison ratios. The attack leverages data from unknown sources to generate perturbations, making it practical for real-world scenarios where attackers don't control all training data.

## Key Results
- Achieves over 30% performance degradation when poison ratio is as low as 40%
- Outperforms seven baseline methods by 20-15-10% at 80-60-40% poison ratios respectively
- Remains effective against static defenses (cutout, cutmix, mixup, adversarial training)
- Shows partial robustness to adaptive defenses, though full adaptive detection can identify poisoned sources

## Why This Works (Mechanism)
PMA works by strategically optimizing perturbations to force models trained on mixed data to converge toward parameters that mimic a poorly-performing destination model. The key insight is that by minimizing the distance between model parameters across different data mixtures, the attack can degrade performance without requiring full poisoning. The destination model, trained with random incorrect labels, provides a target parameter distribution that results in poor classification accuracy. The bi-level optimization ensures that perturbations remain effective even when only a portion of training data is controlled by the attacker.

## Foundational Learning
- **Bi-level optimization**: Optimization framework where inner loop optimizes model parameters while outer loop optimizes attack parameters. Needed to balance perturbation effectiveness with model convergence. Quick check: Verify inner optimization completes before outer parameter update.
- **Parameter distance metrics**: Methods to measure similarity between model parameter distributions. Required to quantify attack effectiveness and guide perturbation optimization. Quick check: Ensure distance metric captures meaningful differences in model behavior.
- **ℓ∞ norm constraints**: Bounds on maximum perturbation magnitude per pixel/channel. Necessary for practical attacks that avoid detection through excessive changes. Quick check: Verify perturbations stay within specified norm bounds.
- **Surrogate model training**: Process of training auxiliary models to generate effective perturbations. Essential for scaling attacks to complex datasets and architectures. Quick check: Confirm surrogate and target models share sufficient similarity for transferability.
- **Destination model construction**: Creating poorly-performing models through random label corruption. Critical for providing attack targets that degrade performance. Quick check: Verify destination model achieves near-random accuracy on clean data.

## Architecture Onboarding

**Component Map:** Data Owner -> Target Model <- Surrogate Model <- Data Exploiters -> Attacker

**Critical Path:** Data Exploiters generate perturbations → Surrogate Model optimizes perturbations → Target Model trained on poisoned+clean data → Performance degradation measured on clean test data

**Design Tradeoffs:** The attack balances perturbation size (25/255 norm) against effectiveness - larger perturbations yield better degradation but risk detection. Uses bi-level optimization which is computationally expensive but necessary for effective partial poisoning. Relies on data from unknown sources, making it more realistic but requiring careful perturbation generation.

**Failure Signatures:** Insufficient degradation (<30% at 40% ratio) indicates poor perturbation optimization or inappropriate norm bounds. Attack effectiveness dropping significantly against adaptive defenses suggests the perturbation optimization needs refinement. If performance degradation doesn't scale with poison ratio, the distance metric between parameters may be incorrectly implemented.

**First Experiments:**
1. Test PMA with 100% poison ratio to verify baseline effectiveness before testing partial poisoning scenarios
2. Compare parameter distance convergence between clean and poisoned training at different poison ratios
3. Evaluate transferability by using different surrogate and target model architectures

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the performance of PMA change when the poison ratio is below 40%?
- Basis in paper: [explicit] The authors state "our method requires at least a 40% poison ratio to be effective, as a 20% ratio achieves insufficient results"
- Why unresolved: The paper only tested poison ratios at 40%, 60%, 80%, and 100%, leaving the effectiveness of PMA in the 20-40% range unknown
- What evidence would resolve it: Experiments testing PMA with poison ratios in the 20-40% range to determine the exact threshold where effectiveness begins

### Open Question 2
- Question: How does PMA perform against more sophisticated adaptive countermeasures beyond poisoned source detection?
- Basis in paper: [inferred] The paper only tested detection of poisoned data sources as a "full adaptive countermeasure," suggesting other potential adaptive defenses remain unexplored
- Why unresolved: The paper focused primarily on static and partial adaptive defenses, with only one example of a full adaptive countermeasure
- What evidence would resolve it: Experiments testing PMA against other potential adaptive defenses like model distillation, anomaly detection on the perturbed data, or adaptive training strategies

### Open Question 3
- Question: Can PMA be extended to work effectively with smaller perturbation sizes (below 25/255)?
- Basis in paper: [explicit] The authors note "when perturbations are bounded by ℓ∞-norm with 8/255 or 16/255, our proposal achieves only limited performance degradation"
- Why unresolved: The paper did not investigate whether architectural modifications or alternative optimization strategies could improve PMA's effectiveness at smaller perturbation sizes
- What evidence would resolve it: Modified versions of PMA tested with smaller perturbation sizes to determine if architectural or algorithmic changes could improve performance

## Limitations
- Effectiveness threshold requires at least 40% poison ratio, limiting applicability in scenarios with minimal attacker control
- Full adaptive countermeasures can detect poisoned sources, though the attack may still degrade performance
- Computational overhead from bi-level optimization makes large-scale deployment challenging

## Confidence
- Core effectiveness claims: **High** - Directly demonstrated through controlled experiments across four datasets
- Defense robustness claims: **Medium** - Limited evaluation of adaptive defenses, only one full countermeasure tested
- Baseline comparison validity: **Medium** - Comprehensive comparison but lacks detailed baseline implementation specifications

## Next Checks
1. Implement and test all seven baseline methods (EM, REM, DC, TAP, LSP, AR, CUDA) with the exact same experimental setup to verify the claimed performance gaps.
2. Evaluate PMA's effectiveness against additional adaptive defenses beyond orthogonal projection, particularly those that could modify training procedures or detection mechanisms.
3. Test the attack's transferability across different model architectures beyond the ResNet-18 and ConvNet pairs used in the experiments to assess generalization.