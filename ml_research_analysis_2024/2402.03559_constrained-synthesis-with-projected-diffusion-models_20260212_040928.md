---
ver: rpa2
title: Constrained Synthesis with Projected Diffusion Models
arxiv_id: '2402.03559'
source_url: https://arxiv.org/abs/2402.03559
tags:
- constraints
- diffusion
- data
- process
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Projected Diffusion Models (PDM), a novel
  approach that enables generative diffusion models to satisfy and certify compliance
  with constraints and physical principles by recasting the sampling process as a
  constrained optimization problem. PDM ensures generated data adheres strictly to
  imposed constraints through iterative projection steps while maintaining state-of-the-art
  FID scores.
---

# Constrained Synthesis with Projected Diffusion Models

## Quick Facts
- arXiv ID: 2402.03559
- Source URL: https://arxiv.org/abs/2402.03559
- Reference count: 40
- Key outcome: Introduces Projected Diffusion Models (PDM) that enable generative diffusion models to satisfy and certify compliance with constraints and physical principles by recasting the sampling process as a constrained optimization problem, achieving high-fidelity outputs with constraint satisfaction across multiple domains.

## Executive Summary
This paper introduces Projected Diffusion Models (PDM), a novel approach that enables generative diffusion models to satisfy and certify compliance with constraints and physical principles. The method recasts the sampling process as a constrained optimization problem, using iterative projection steps to guide the diffusion process toward the feasible region. PDM maintains state-of-the-art FID scores while ensuring generated data adheres strictly to imposed constraints, demonstrating effectiveness across diverse domains including material science, physics-informed motion, trajectory optimization, and human motion synthesis.

## Method Summary
PDM recasts the traditional sampling process of generative diffusion models as a constrained optimization problem. At each time step in the reverse diffusion process, PDM applies a projection operator that finds the nearest feasible point to the current sample. This operator is applied after each gradient update, effectively steering the sampling process into the feasible subdistribution defined by the constraints. The method uses the same score-matching objective as traditional diffusion models while handling feasibility through projections, ensuring high-fidelity outputs that comply with complex non-convex constraints and physical principles.

## Key Results
- Achieved FID score of 30.7 on microstructure materials generation with constraint satisfaction
- Demonstrated 100% success rate on trajectory optimization with zero constraint violations
- Maintained high fidelity (FID: 0.71) in human motion synthesis while satisfying physical constraints
- Outperformed conditional models and post-processing approaches, particularly in low-data regimes and out-of-distribution settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The iterative projection step ensures feasibility of generated samples by guiding the diffusion process toward the feasible region.
- Mechanism: At each time step in the reverse diffusion process, PDM applies a projection operator that finds the nearest feasible point to the current sample. This operator is applied after each gradient update, effectively steering the sampling process into the feasible subdistribution defined by the constraints.
- Core assumption: The projection operator is computationally tractable and can be efficiently applied at each iteration of the sampling process.
- Evidence anchors:
  - [abstract] "ensuring that the generated data adheres strictly to imposed constraints or physical principles"
  - [section 4.2] "To retain feasibility through an application of the projection operator after each update step"
  - [corpus] Weak evidence - the corpus papers focus on different projection-based approaches but don't provide specific evidence for this exact mechanism
- Break condition: If the projection operator becomes computationally intractable or the feasible region is too complex for practical projection, the iterative projection approach may fail.

### Mechanism 2
- Claim: The score-matching objective aligns with the constrained optimization formulation, ensuring high-fidelity outputs while satisfying constraints.
- Mechanism: PDM uses the same score-matching objective as traditional diffusion models, which minimizes the negative log-likelihood of the data distribution. By framing the sampling process as a constrained optimization problem, this objective remains unchanged while the solution is constrained to the feasible region.
- Core assumption: The score-matching objective provides gradients that effectively minimize the negative log-likelihood while the projections handle feasibility.
- Evidence anchors:
  - [section 4.1] "the objective of the PDM's reverse sampling process is aligned with that of traditional score-based diffusion models"
  - [section 4.2] "By incorporating constraints throughout the sampling process, the interim learned distributions are steered to comply with these specifications"
  - [corpus] Moderate evidence - the corpus papers mention similar alignment objectives but don't provide specific evidence for this exact mechanism
- Break condition: If the score-matching objective becomes misaligned with the constrained optimization problem, or if the gradients provided by the score function become unreliable, the approach may fail to produce high-fidelity outputs.

### Mechanism 3
- Claim: The convergence properties of PDM ensure that constraint violations decrease over time, approaching zero violation as t approaches zero.
- Mechanism: As the reverse diffusion process progresses and the step size decreases, the stochastic component becomes negligible. This allows PDM to transition toward deterministic gradient ascent on log q(xt), with the projections gradually steering the sample into the feasible subdistribution.
- Core assumption: The step size decreases sufficiently fast to ensure convergence to the feasible region.
- Evidence anchors:
  - [section 4.2] "the constraint violations decrease with each addition of estimated gradients and noise and approaches 0-violation as t nears zero"
  - [section 5] "Corollary 5.3. For arbitrary small ξ > 0, there exist t and i ≥ ¯I such that: Error(U(PC(xi t)), C) ≤ ξ"
  - [corpus] Weak evidence - the corpus papers mention convergence properties but don't provide specific evidence for this exact mechanism
- Break condition: If the step size decreases too slowly or the stochastic component remains significant, the approach may fail to converge to the feasible region.

## Foundational Learning

- Concept: Constrained optimization
  - Why needed here: PDM frames the sampling process as a constrained optimization problem, requiring understanding of how to handle constraints in optimization settings.
  - Quick check question: What is the difference between equality and inequality constraints in constrained optimization?

- Concept: Projection operators
  - Why needed here: PDM relies on projection operators to ensure feasibility of generated samples, requiring understanding of how these operators work and their properties.
  - Quick check question: What is the definition of a projection operator in the context of constrained optimization?

- Concept: Diffusion models and score matching
  - Why needed here: PDM builds upon diffusion models and score matching, requiring understanding of how these models work and how they can be adapted to handle constraints.
  - Quick check question: What is the relationship between diffusion models and score matching in the context of generative modeling?

## Architecture Onboarding

- Component map:
  Score network (sθ(xt, t)) -> Projection operator (PC) -> Sampling process

- Critical path:
  1. Initialize sample from Gaussian distribution.
  2. Iteratively update sample using estimated score function and projection operator.
  3. Return final sample as generated output.

- Design tradeoffs:
  - Computational cost vs. constraint satisfaction: Increasing the number of projections improves constraint satisfaction but also increases computational cost.
  - Step size vs. convergence: Decreasing the step size improves convergence but may slow down the sampling process.

- Failure signatures:
  - Constraint violations: If the projection operator fails to ensure feasibility, constraint violations may occur.
  - Low fidelity outputs: If the score-matching objective becomes misaligned with the constrained optimization problem, generated outputs may have low fidelity.

- First 3 experiments:
  1. Implement PDM on a simple constrained optimization problem with known solution to verify feasibility and convergence.
  2. Compare PDM to conditional diffusion models on a constrained generation task to evaluate constraint satisfaction and fidelity.
  3. Evaluate PDM on a real-world constrained generation task (e.g., material science or motion planning) to demonstrate practical applicability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we theoretically guarantee that iterative projections will maintain sample quality (FID) while enforcing constraints, especially for non-convex constraints?
- Basis in paper: [explicit] The paper demonstrates PDM's effectiveness empirically but acknowledges that convergence guarantees for non-convex constraints are only supported by empirical evidence, not theoretical guarantees.
- Why unresolved: The paper provides theoretical convergence guarantees for convex constraints but relies on empirical results for non-convex cases, which may not hold universally.
- What evidence would resolve it: Formal theoretical analysis showing convergence and sample quality preservation for non-convex constraints, or empirical studies across diverse non-convex constraint sets demonstrating consistent performance.

### Open Question 2
- Question: What is the impact of incorporating constraints into the forward diffusion process, and how does it affect training computational cost and FID scores?
- Basis in paper: [inferred] The paper conjectures that incorporating constraints into the forward process would increase computational cost and decrease FID scores but does not experimentally verify this.
- Why unresolved: The authors hypothesize negative impacts but have not conducted experiments to validate these claims or explore potential mitigation strategies.
- What evidence would resolve it: Experimental results comparing models trained with and without forward process constraints, measuring both computational cost and FID scores.

### Open Question 3
- Question: How can we represent and enforce complex multi-task constraints for large-scale generative models while maintaining scalability and effectiveness?
- Basis in paper: [explicit] The discussion section identifies the representation of complex constraints for multi-task large-scale models as an open challenge.
- Why unresolved: The paper focuses on single-task constraint satisfaction and does not address the increased complexity of coordinating multiple constraint sets in large-scale models.
- What evidence would resolve it: Demonstrated methods for multi-task constraint representation and enforcement in large-scale models, showing maintained performance and scalability.

### Open Question 4
- Question: What optimization strategies can minimize the computational overhead of iterative projections without sacrificing constraint satisfaction guarantees?
- Basis in paper: [explicit] The paper acknowledges computational overhead as a limitation and suggests potential strategies like adjusting projection timing and warm-start approaches, but these are not fully explored.
- Why unresolved: The paper identifies the problem and suggests possible solutions but does not provide comprehensive analysis or implementation of these optimization strategies.
- What evidence would resolve it: Comparative analysis of different optimization strategies for reducing projection overhead, demonstrating maintained constraint satisfaction with reduced computational cost.

## Limitations

- Computational overhead: Iterative projections increase computational cost, particularly for complex or high-dimensional constraint sets.
- Scalability concerns: The approach's effectiveness for very high-dimensional, complex constraint sets remains to be demonstrated.
- Theoretical gaps: While convergence is proven for convex constraints, theoretical guarantees for non-convex constraints rely on empirical evidence.

## Confidence

**High Confidence:**
- The mathematical framework of recasting diffusion sampling as constrained optimization is sound
- The projection-based approach can enforce constraints on generated samples
- The method outperforms post-processing approaches in constraint satisfaction

**Medium Confidence:**
- PDM achieves competitive FID scores while satisfying constraints (may depend on specific constraint types and datasets)
- The approach works across diverse domains as demonstrated (though experiments are on relatively small-scale problems)
- The convergence properties hold in practice (theoretical guarantees exist but practical convergence rates vary)

**Low Confidence:**
- The method scales effectively to very high-dimensional, complex constraint sets
- The computational overhead of projections is acceptable for all use cases
- The approach maintains performance in highly out-of-distribution settings

## Next Checks

1. **Scalability test**: Evaluate PDM on high-dimensional constraint sets (e.g., 3D printing with multiple material properties, or robotics with complex collision constraints) to assess computational tractability and projection accuracy as dimensionality increases.

2. **Adaptive step size evaluation**: Implement and compare different step size schedules (fixed, cosine, adaptive based on constraint violation) to determine optimal schedules for different constraint types and assess sensitivity to this hyperparameter.

3. **Cross-dataset generalization**: Train PDM on one dataset/domain and evaluate its ability to generate constraint-satisfying samples on completely different datasets to test the robustness of the learned score function and projection approach to domain shifts.