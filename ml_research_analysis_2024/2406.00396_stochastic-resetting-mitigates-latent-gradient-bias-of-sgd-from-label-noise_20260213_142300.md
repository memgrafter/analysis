---
ver: rpa2
title: Stochastic Resetting Mitigates Latent Gradient Bias of SGD from Label Noise
arxiv_id: '2406.00396'
source_url: https://arxiv.org/abs/2406.00396
tags:
- resetting
- stochastic
- reset
- noise
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that stochastic resetting during stochastic
  gradient descent (SGD) training can significantly improve generalization performance
  when dealing with noisy labels. The authors identify a latent gradient bias caused
  by label noise that leads to overfitting, and show that resetting to checkpoints
  with a certain probability mitigates this effect.
---

# Stochastic Resetting Mitigates Latent Gradient Bias of SGD from Label Noise

## Quick Facts
- arXiv ID: 2406.00396
- Source URL: https://arxiv.org/abs/2406.00396
- Authors: Youngkyoung Bae; Yeongwoo Song; Hawoong Jeong
- Reference count: 40
- Key result: Stochastic resetting during SGD training significantly improves generalization under label noise by mitigating latent gradient bias

## Executive Summary
This paper addresses the challenge of label noise in deep learning by introducing stochastic resetting during SGD training. The authors identify a latent gradient bias that develops during training with noisy labels, causing gradual overfitting to corrupted data. By periodically resetting to checkpoints from earlier training phases, this bias is counteracted, leading to improved generalization. The method is theoretically grounded in stochastic differential equation analysis and empirically validated across multiple datasets and noise scenarios.

## Method Summary
The method involves standard SGD training with periodic stochastic resets to stored checkpoints. A checkpoint is saved whenever validation loss reaches a new minimum. During each training iteration, with probability r (reset probability), parameters are reset to the most recent checkpoint instead of performing the SGD update. This simple mechanism effectively counteracts the accumulation of gradient bias from noisy labels. The approach is compatible with existing optimizers and requires no modification to learning rate schedules.

## Key Results
- Stochastic resetting significantly improves test accuracy on corrupted datasets, with improvements increasing as noise rate increases
- Resetting is most effective when stochasticity is high (small batch size) and noise rate is large
- Partial resetting of later network layers (e.g., linear layers) can provide additional gains over full resetting
- The method is simple to implement and compatible with existing training pipelines

## Why This Works (Mechanism)

### Mechanism 1
Stochastic resetting counteracts a latent gradient bias induced by noisy labels that drives overfitting. During SGD training with noisy labels, gradients from corrupted samples accumulate over time, gradually dominating the optimization trajectory. Resetting to earlier checkpoints re-centers the optimization, suppressing this bias. This assumes orthogonality between correct and wrong label gradient drifts, with wrong-label drift increasing in magnitude as training progresses.

### Mechanism 2
Resetting is more beneficial when stochasticity is high relative to drift toward the true optimum. The Péclet number Pe = Lv/(2D) captures this ratio, where lower Pe (higher D, lower v) yields larger improvement. Optimal reset probability increases as Pe decreases. This assumes SGD dynamics can be approximated by overdamped Langevin dynamics with state-dependent diffusion.

### Mechanism 3
Partial resetting of later network layers can yield additional gains over full resetting. Later layers exhibit stronger memorization of corrupted data, so resetting them alone can more directly mitigate overfitting while earlier layers continue learning general features. This assumes different network layers have heterogeneous learning dynamics, with later layers more prone to overfitting.

## Foundational Learning

- **Langevin dynamics and stochastic differential equations**: Needed to map SGD to Langevin dynamics for analyzing noise and drift impact on optimization. Quick check: Can you write the discretized overdamped Langevin equation and identify its potential and diffusion terms?

- **Gradient decomposition and bias analysis**: Essential for understanding how correct and wrong label gradients combine and affect training dynamics. Quick check: Given a corrupted dataset with noise rate τ, how do you decompose the risk gradient into correct and wrong parts?

- **First-passage time and stochastic resetting theory**: Quantifies resetting benefit via mean first-passage time in drift-diffusion settings. Quick check: For 1D drift-diffusion with drift v and diffusion D, under what condition (Pe) does resetting reduce MFPT?

## Architecture Onboarding

- **Component map**: Main loop (SGD) -> compute gradients -> with prob. r, reset to checkpoint; else update params -> evaluate validation loss -> if new min, update checkpoint

- **Critical path**: Training iteration → compute gradients → with prob. r, reset to checkpoint; else update params → evaluate validation loss → if new min, update checkpoint

- **Design tradeoffs**: Full vs. partial resetting (layer-wise); fixed vs. adaptive checkpoint; reset probability choice vs. training efficiency

- **Failure signatures**: Too high reset probability prevents training progress; too low allows overfitting; late checkpoint selection renders resetting ineffective

- **First 3 experiments**:
  1. Run baseline SGD with noisy labels, record validation loss curve and overfitting point; implement full resetting at r=0.001, compare validation loss and test accuracy
  2. Vary batch size (small vs. large) and noise rate (low vs. high) with resetting; observe if improvement increases as theory predicts
  3. Implement partial resetting of last linear layer only; compare to full resetting on same settings to test layer-specific benefit

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Theoretical framework assumes orthogonality between correct and wrong label gradient drifts, which may not hold in all architectures
- Péclet number analysis provides theoretical guidance but real-world dynamics involve additional factors like adaptive optimizers not fully captured
- Layer-wise resetting benefits are empirically observed but lack rigorous theoretical justification for why later layers specifically benefit more

## Confidence

**High Confidence**: The empirical effectiveness of stochastic resetting across multiple datasets and noise scenarios is well-supported by experimental results.

**Medium Confidence**: The theoretical mechanism explaining how latent gradient bias develops and how resetting counteracts it is plausible and supported by both theory and experiments, though some assumptions require further validation.

**Low Confidence**: The layer-specific resetting benefits and precise theoretical characterization of optimal reset probabilities across different training regimes need more rigorous validation, particularly regarding generalization to architectures beyond those tested.

## Next Checks

1. **Architecture Dependency Test**: Validate the orthogonality assumption by testing resetting effectiveness on architectures without batch normalization or with different normalization schemes to determine if the theoretical mechanism holds universally.

2. **Hyperparameter Interaction Analysis**: Conduct systematic experiments varying learning rates, optimizers (SGD vs. Adam), and momentum terms with resetting to quantify how these factors interact with the theoretical Péclet number framework.

3. **Theoretical Extension**: Develop and test a more complete theoretical model that incorporates adaptive optimizers, momentum, and non-orthogonal gradient components to predict resetting benefits more accurately across diverse training scenarios.