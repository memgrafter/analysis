---
ver: rpa2
title: Human-like object concept representations emerge naturally in multimodal large
  language models
arxiv_id: '2407.01067'
source_url: https://arxiv.org/abs/2407.01067
tags:
- human
- dimensions
- object
- mllm
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether large language models (LLMs) and
  multimodal large language models (MLLMs) develop human-like conceptual representations
  of objects. Using a triplet odd-one-out task and 4.7 million similarity judgments,
  the authors derived 66-dimensional embeddings for 1,854 natural objects from LLM
  (ChatGPT-3.5) and MLLM (Gemini Pro Vision 1.0).
---

# Human-like object concept representations emerge naturally in multimodal large language models

## Quick Facts
- arXiv ID: 2407.01067
- Source URL: https://arxiv.org/abs/2407.01067
- Reference count: 40
- Key outcome: LLM and MLLM embeddings capture human-like conceptual knowledge of objects, showing strong alignment with neural activity patterns

## Executive Summary
This study investigates whether large language models (LLMs) and multimodal large language models (MLLMs) develop human-like conceptual representations of objects. Using a triplet odd-one-out task and 4.7 million similarity judgments, the authors derived 66-dimensional embeddings for 1,854 natural objects from ChatGPT-3.5 and Gemini Pro Vision 1.0. The embeddings were stable, predictive (explaining up to 87.1% of behavior), and showed semantic clustering similar to human mental representations. MLLM embeddings aligned more closely with human cognition, especially in dimensions related to color, shape, and spatial features. Strong alignment was also found between model embeddings and neural activity patterns in category-selective brain regions (EBA, PPA, RSC, FFA).

## Method Summary
The authors collected 4.7 million triplet similarity judgments from ChatGPT-3.5 and Gemini Pro Vision 1.0 using a triplet odd-one-out task on 1,854 natural objects from the THINGS database. They applied the Sparse Positive Similarity Embedding (SPoSE) method to derive 66-dimensional embeddings from the similarity judgments, using L1-norm regularization for sparsity. The derived embeddings were validated by testing their predictive performance on held-out behavioral data and comparing their similarity structure to human-derived embeddings using Representational Similarity Analysis (RSA). The study also compared model embeddings with neural activity patterns in the Natural Scenes Dataset (NSD).

## Key Results
- LLM and MLLM embeddings were stable, predictive (explaining up to 87.1% of behavior), and showed semantic clustering similar to human mental representations
- MLLM embeddings aligned more closely with human cognition than LLM embeddings, especially in dimensions related to color, shape, and spatial features
- Strong alignment was found between model embeddings and neural activity patterns in category-selective brain regions (EBA, PPA, RSC, FFA)

## Why This Works (Mechanism)
The triplet odd-one-out task forces models to make explicit similarity judgments between objects, revealing the underlying dimensions they use to represent concepts. The SPoSE method then extracts these dimensions from the similarity judgments by finding a low-dimensional embedding that preserves the relative similarities while enforcing sparsity through L1 regularization. This process allows the models to discover human-like conceptual dimensions without explicit supervision.

## Foundational Learning
1. **Triplet odd-one-out task**: A method for collecting similarity judgments by asking which object in a triplet is the odd one out. Why needed: Provides a way to elicit and compare conceptual representations between models and humans. Quick check: Verify that the collected judgments follow expected patterns (e.g., similar objects are less likely to be chosen as the odd one out).

2. **Sparse Positive Similarity Embedding (SPoSE)**: A method for deriving low-dimensional embeddings from similarity judgments using L1-norm regularization. Why needed: Extracts the underlying conceptual dimensions from the similarity judgments while enforcing sparsity for interpretability. Quick check: Ensure the derived embeddings preserve the relative similarities between objects and show semantic clustering.

3. **Representational Similarity Analysis (RSA)**: A technique for comparing the similarity structure of two sets of representations (e.g., model embeddings and neural activity patterns). Why needed: Quantifies the alignment between model-derived concepts and human neural representations. Quick check: Verify that the RSA results are consistent across different brain regions and object categories.

## Architecture Onboarding
The method follows this critical path: Triplet similarity judgments -> SPoSE embedding derivation -> Embedding validation (predictive performance, semantic clustering) -> RSA comparison with neural activity.

Design tradeoffs: The SPoSE method balances preserving similarity structure with enforcing sparsity, which may lead to loss of some fine-grained information. The choice of 66 dimensions is based on the authors' analysis but may not be optimal for all applications.

Failure signatures: Poor predictive performance on held-out behavioral data may indicate issues with the quality or quantity of collected similarity judgments, improper implementation of SPoSE, or incorrect noise ceiling calculation.

First experiments:
1. Collect a small set of triplet similarity judgments from an LLM and verify that they follow expected patterns.
2. Implement the SPoSE method on a toy dataset and check that the derived embeddings preserve the relative similarities.
3. Apply RSA to compare the embeddings from step 2 with a reference dataset (e.g., human similarity judgments) and verify that the results are consistent with expectations.

## Open Questions the Paper Calls Out
1. How do LLM and MLLM embeddings change when using image-level versus object-level annotations? The paper notes that image-level annotations lead to dimensions related to spatial, textural, and color attributes, but a comprehensive comparison of the full embedding spaces is not provided.

2. Can fine-tuning LLMs on triplet odd-one-out data improve their alignment with human judgments? The paper suggests that guiding model attention through tailored prompts can improve choice consistency with humans, implying that fine-tuning on such data might further enhance alignment.

3. How generalizable are the identified object dimensions across different LLM architectures? The study focuses on ChatGPT-3.5 and Gemini Pro Vision (v1.0), acknowledging that the methodology could be extended to other state-of-the-art models.

## Limitations
- Results may not generalize to other model architectures or object categories beyond the 1,854 natural objects studied
- Reliance on similarity judgments from a single model version (ChatGPT-3.5 and Gemini Pro Vision 1.0) may limit reproducibility across different model updates
- Specific hyperparameters and implementation details of the SPoSE method are not fully specified, which could affect reproduction attempts

## Confidence
- High confidence in the predictive performance of embeddings (explaining up to 87.1% of behavior) due to extensive validation on held-out data
- Medium confidence in the alignment with neural activity patterns, as the analysis depends on specific preprocessing choices and noise ceiling calculations
- Medium confidence in the semantic clustering claims, as they are based on visual inspection of embeddings

## Next Checks
1. Implement the SPoSE method with different hyperparameter settings (regularization parameter Î», batch size) to assess robustness of the derived embeddings
2. Test the predictive performance of embeddings on a separate dataset of object similarity judgments from different sources (e.g., human behavioral data not used in the original analysis)
3. Apply the RSA analysis to embeddings from different model architectures (e.g., GPT-4, Claude) to evaluate generalizability of the alignment with neural activity patterns