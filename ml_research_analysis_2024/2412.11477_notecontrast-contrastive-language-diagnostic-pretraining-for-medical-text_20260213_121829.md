---
ver: rpa2
title: 'NoteContrast: Contrastive Language-Diagnostic Pretraining for Medical Text'
arxiv_id: '2412.11477'
source_url: https://arxiv.org/abs/2412.11477
tags:
- codes
- medical
- notecontrast
- contrastive
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose NoteContrast, a contrastive pretraining method
  for medical text that jointly learns ICD-10 diagnostic code embeddings and medical
  note representations. The approach uses a contrastive loss to align ICD-10 code
  sequences with their corresponding medical notes, leveraging real-world clinical
  data.
---

# NoteContrast: Contrastive Language-Diagnostic Pretraining for Medical Text

## Quick Facts
- arXiv ID: 2412.11477
- Source URL: https://arxiv.org/abs/2412.11477
- Authors: Prajwal Kailas; Max Homilius; Rahul C. Deo; Calum A. MacRae
- Reference count: 40
- Primary result: NoteContrast outperforms state-of-the-art models on MIMIC-III benchmarks, achieving 40.26 macro-F1 (+11.32) on MIMIC-III-rare50

## Executive Summary
This paper introduces NoteContrast, a contrastive pretraining method for medical text that jointly learns ICD-10 diagnostic code embeddings and medical note representations. The approach uses a contrastive loss to align ICD-10 code sequences with their corresponding medical notes, leveraging real-world clinical data. The method scales to long documents (up to 8192 tokens) and demonstrates strong performance without requiring hierarchical biomedical ontologies. Evaluation on MIMIC-III benchmarks shows NoteContrast outperforms state-of-the-art models, particularly for rare diagnoses.

## Method Summary
NoteContrast consists of three components: an ICD-10 sequence encoder, a long-document medical text encoder, and a joint contrastive model. The ICD-10 sequence encoder is a RoBERTa model pre-trained on real-world clinical data of diagnostic code sequences. The medical text encoder uses a BigBird architecture capable of processing documents up to 8192 tokens. These components are jointly trained using InfoNCE contrastive loss to align text and diagnostic code representations. The model is then fine-tuned using prompt-based learning for multi-label ICD-9 coding on MIMIC-III tasks.

## Key Results
- Achieves 40.26 macro-F1 (+11.32) on MIMIC-III-rare50 benchmark
- Demonstrates strong performance across all MIMIC-III benchmarks (MIMIC-III-50, MIMIC-III-rare50, MIMIC-III-full)
- Scales effectively to long documents up to 8192 tokens
- Outperforms state-of-the-art models without requiring hierarchical biomedical ontologies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive pre-training aligns ICD-10 code embeddings with their corresponding medical note embeddings by maximizing cosine similarity between positive pairs and minimizing it for negative pairs.
- Mechanism: The InfoNCE loss function is used during joint training of text and diagnostic code encoders, where each positive pair consists of a medical note and its associated ICD-10 code sequence. The model learns to embed matching pairs close together in the joint embedding space while pushing apart mismatched pairs.
- Core assumption: The contrastive loss effectively captures the semantic relationship between medical text and diagnostic codes across the batch.
- Evidence anchors:
  - [abstract]: "We use a contrastive loss to align ICD-10 code sequences with their corresponding medical notes"
  - [section]: "The InfoNCE loss (Oord et al., 2018) was computed as contrastive loss among positive and negative text-ICD and ICD-text pairs"
  - [corpus]: Weak - corpus shows related work on contrastive learning for medical images and text, but no direct evidence for this specific text-code alignment mechanism
- Break condition: If the batch size is too small to provide meaningful negative pairs, or if the text and code encoders are not sufficiently aligned in their representation spaces, the contrastive loss may fail to learn meaningful associations.

### Mechanism 2
- Claim: The long-document transformer architecture (BigBird) enables effective processing of medical notes up to 8192 tokens, which is crucial for capturing diagnostic information spread across lengthy clinical documents.
- Mechanism: The BigBird model architecture uses global, local, and random attention patterns to handle long sequences efficiently, allowing the model to process entire discharge summaries rather than truncated versions.
- Core assumption: The increased token limit allows the model to capture context that would be lost with shorter sequences, particularly for complex diagnoses that span multiple sections of a note.
- Evidence anchors:
  - [abstract]: "The method scales to long documents (up to 8192 tokens)"
  - [section]: "Since medical notes commonly contain more than 512 tokens, it was essential to develop models for medical text that can support much longer sequences"
  - [corpus]: Weak - corpus shows related work on long-document transformers but doesn't specifically validate the 8192 token claim
- Break condition: If the attention patterns in BigBird fail to capture relevant long-range dependencies, or if the computational cost becomes prohibitive, the benefits of longer sequences may be negated.

### Mechanism 3
- Claim: Using real-world clinical data for ICD-10 sequence modeling captures co-morbidities and temporally related diagnoses that occur in clinical settings, improving diagnostic coding performance.
- Mechanism: The ICD-10 sequence model is trained on temporal sequences of diagnostic codes across multiple clinical encounters, using masked language modeling to learn contextual embeddings that reflect how diagnoses cluster and follow each other in real patient histories.
- Core assumption: Real-world clinical data contains patterns of co-occurring diagnoses and temporal relationships that are not captured in static biomedical ontologies or hierarchical code structures.
- Evidence anchors:
  - [abstract]: "The approach uses a contrastive loss to align ICD-10 code sequences with their corresponding medical notes, leveraging real-world clinical data"
  - [section]: "We trained a RoBERTa model (Liu et al., 2019) on temporal sequences of diagnostic codes using real-world data of a large patient cohort"
  - [corpus]: Weak - corpus shows related work on diagnostic code representations but doesn't specifically validate the use of real-world data for this purpose
- Break condition: If the real-world data is biased toward certain types of diagnoses or patient populations, the learned embeddings may not generalize well to other clinical settings.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: This is the core training mechanism that aligns text and diagnostic code representations. Understanding how contrastive learning works is essential for grasping why the model can map medical notes to their appropriate ICD-10 codes.
  - Quick check question: How does the InfoNCE loss function differ from standard cross-entropy loss, and why is it particularly suited for this task?

- Concept: Transformer architectures and attention mechanisms
  - Why needed here: The model uses BigBird transformers to process long medical notes, and understanding how self-attention works is crucial for understanding how the model can capture long-range dependencies in clinical text.
  - Quick check question: What are the key architectural differences between standard transformers and BigBird that enable processing of longer sequences?

- Concept: Multi-label classification and evaluation metrics
  - Why needed here: Medical diagnostic coding is inherently a multi-label problem, and understanding metrics like macro-F1, micro-F1, and AUC is essential for evaluating the model's performance.
  - Quick check question: Why is macro-F1 score particularly important for evaluating medical coding models, and how does it differ from micro-F1?

## Architecture Onboarding

- Component map:
  - ICD-10 Sequence Encoder (RoBERTa-based) -> Medical Text Encoder (BigBird-based) -> Joint Contrastive Model -> Classification Head
  - ICD-10 Sequence Encoder (RoBERTa-based) -> Projection Layers -> Joint Embedding Space
  - Medical Text Encoder (BigBird-based) -> Projection Layers -> Joint Embedding Space

- Critical path:
  1. Pre-train ICD-10 sequence encoder on real-world clinical data
  2. Convert BioLM model to BigBird and pre-train on MIMIC-III notes
  3. Jointly train text and code encoders with contrastive loss
  4. Fine-tune on MIMIC-III-50 for downstream ICD-9 coding
  5. Apply prompt-based fine-tuning for final classification

- Design tradeoffs:
  - Using ICD-10 for pre-training but ICD-9 for fine-tuning introduces a domain shift but allows leveraging more comprehensive real-world data
  - BigBird architecture enables longer sequences but increases computational cost
  - Contrastive learning requires larger batch sizes for effective negative sampling

- Failure signatures:
  - Poor performance on rare diagnoses suggests insufficient negative sampling or batch size issues
  - Degraded performance with longer sequences indicates BigBird attention pattern limitations
  - Mismatch between pre-training and fine-tuning distributions suggests domain adaptation problems

- First 3 experiments:
  1. Train ICD-10 sequence encoder with masked language modeling on synthetic data to establish baseline performance
  2. Implement basic contrastive learning with smaller batch sizes to test alignment capabilities
  3. Evaluate model performance on MIMIC-III-50 without long-document support to establish performance ceiling

## Open Questions the Paper Calls Out
The paper acknowledges that using a single medical institution's data could affect generalizability and suggests that incorporating data from multiple institutions would improve performance for rare diagnoses and overall generalizability.

## Limitations
- Reliance on proprietary ICD-10 sequence dataset from MassGeneral Brigham that is not publicly available
- Evaluation limited to MIMIC-III dataset, which may not generalize to other clinical settings
- Performance on ICD-9 coding despite pre-training on ICD-10 sequences introduces domain shift

## Confidence
- High Confidence: The core mechanism of contrastive learning for aligning text and code representations is well-established in the literature, and the experimental results showing improved performance on MIMIC-III benchmarks are clearly demonstrated with specific metrics.
- Medium Confidence: The claim that longer sequences (8192 tokens) significantly improve performance is supported by results but lacks ablation studies isolating the impact of sequence length from other architectural choices. The use of real-world clinical data for ICD-10 pre-training is theoretically sound but not directly validated against synthetic alternatives.
- Low Confidence: The generalization of these results to other medical coding tasks beyond ICD-9, and to other clinical datasets beyond MIMIC-III, remains unproven. The long-term stability and maintenance requirements for such models in production healthcare settings are not addressed.

## Next Checks
1. **Cross-dataset validation**: Evaluate NoteContrast performance on a different medical coding dataset (e.g., MIMIC-IV or other hospital discharge data) to assess generalization beyond MIMIC-III.
2. **Sequence length ablation**: Conduct controlled experiments comparing NoteContrast performance across different sequence lengths (512, 2048, 4096, 8192 tokens) to isolate the impact of longer context windows on diagnostic coding accuracy.
3. **Real-world data necessity test**: Compare ICD-10 encoder performance when pre-trained on real-world clinical data versus synthetic or randomly generated ICD-10 sequences to quantify the value of using actual patient diagnostic patterns.