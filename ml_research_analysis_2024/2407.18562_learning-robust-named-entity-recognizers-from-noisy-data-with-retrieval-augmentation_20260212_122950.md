---
ver: rpa2
title: Learning Robust Named Entity Recognizers From Noisy Data With Retrieval Augmentation
arxiv_id: '2407.18562'
source_url: https://arxiv.org/abs/2407.18562
tags:
- text
- noisy
- retrieval
- recall
- gold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning robust Named Entity
  Recognition (NER) models that can handle noisy text inputs, such as those with spelling
  mistakes or Optical Character Recognition (OCR) errors. The authors propose a novel
  approach that uses retrieval augmentation to enhance the representation of noisy
  text by retrieving relevant text from a knowledge corpus, such as Wikipedia.
---

# Learning Robust Named Entity Recognizers From Noisy Data With Retrieval Augmentation

## Quick Facts
- arXiv ID: 2407.18562
- Source URL: https://arxiv.org/abs/2407.18562
- Authors: Chaoyi Ai; Yong Jiang; Shen Huang; Pengjun Xie; Kewei Tu
- Reference count: 20
- One-line primary result: The proposed retrieval-augmented approach achieves significant improvements in NER performance on noisy text with spelling mistakes and OCR errors.

## Executive Summary
This paper addresses the challenge of learning robust Named Entity Recognition (NER) models that can handle noisy text inputs, such as those with spelling mistakes or Optical Character Recognition (OCR) errors. The authors propose a novel approach that uses retrieval augmentation to enhance the representation of noisy text by retrieving relevant text from a knowledge corpus, such as Wikipedia. Three retrieval methods are designed: sparse retrieval based on lexicon similarity using BM25, dense retrieval based on semantic similarity using contrastive learning, and self-retrieval based on task-specific text using BERTScore. The retrieved text is concatenated with the noisy text and encoded using a transformer network, leveraging self-attention to improve the contextual token representations. A multi-view training framework is further employed to improve the NER model's performance on the original noisy text without relying on retrieval during inference. Experiments on datasets with spelling mistakes and OCR errors demonstrate significant improvements in NER performance compared to baseline models.

## Method Summary
The approach retrieves relevant text from Wikipedia using three methods: BM25 for sparse retrieval based on lexicon similarity, dense retrieval using semantic similarity with contrastive learning, and self-retrieval using BERTScore. The retrieved text is concatenated with the noisy input and encoded via a transformer, where self-attention enhances token representations. A multi-view training framework encourages consistency between original noisy text and retrieval-augmented views using L2 or KL divergence loss, enabling the model to perform well on noisy text without retrieval at inference.

## Key Results
- The retrieval-augmented model achieves substantial gains in entity-level F1 scores across various noise levels and types.
- Dense retrieval outperforms sparse retrieval on OCR errors, while BM25 is more effective for spelling mistakes.
- Multi-view learning successfully improves performance on noisy text without requiring retrieval during inference.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval augmentation improves noisy NER by providing context that can resolve ambiguity and correct errors.
- Mechanism: The retrieval module provides relevant text from a knowledge corpus that can help disambiguate entities and correct OCR/spelling errors in the original text. The transformer's self-attention mechanism can then leverage this retrieved context to enhance the representation of noisy tokens.
- Core assumption: Retrieved text is sufficiently relevant and contains correct versions of noisy entities.
- Evidence anchors:
  - [abstract] "We propose to retrieve relevant text of the noisy text from a knowledge corpus and use it to enhance the representation of the original noisy input."
  - [section 3.1.1] "Wikipedia...boasts a vast collection of clean and well-organized text on a wide range of topics and domains."
  - [corpus] Weak evidence - no direct citations to support retrieval effectiveness in NER context.
- Break condition: Retrieved text is irrelevant or contains similar errors, making it unhelpful for correction.

### Mechanism 2
- Claim: Multi-view learning improves performance on noisy text by regularizing the model through contrastive training.
- Mechanism: The model is trained with two views - original noisy text and retrieval-augmented text. By encouraging these views to produce similar representations (through L2 or KL divergence), the model learns to extract meaningful information even from noisy text alone.
- Core assumption: The model can learn useful representations from the noisy text view even without retrieval during inference.
- Evidence anchors:
  - [section 3.2] "We employ multi-view learning to encourage two views to produce similar contextual embedding representations or predicted label distributions, thus improving the NER accuracy of the original noisy text view."
  - [section 4.4] "Experiments show that our retrieval-augmented model achieves significant improvements in various noisy NER settings."
  - [corpus] Weak evidence - no direct citations to support multi-view learning effectiveness in NER context.
- Break condition: The two views diverge significantly, making the regularization ineffective.

### Mechanism 3
- Claim: Dense retrieval based on semantic similarity can handle OCR errors better than sparse retrieval.
- Mechanism: Dense retrieval encodes sentences into embeddings and retrieves through similarity, which can match noisy text with clean text even when surface forms differ (as with OCR errors). This is contrasted with BM25 which requires exact word matches.
- Core assumption: Semantic embeddings capture enough information to match noisy text with its clean counterpart.
- Evidence anchors:
  - [section 3.1.2] "The BM25 algorithm requires that retrieved strings contain at least one word from the input query, making it less effective for retrieving results from noisy text containing errors."
  - [section 4.1] "BERTScore of sentences with spelling mistakes is really low while that with OCR noise is very high, which indicates that spelling mistakes and OCR noise are very different."
  - [corpus] Weak evidence - no direct citations to support dense retrieval effectiveness in OCR error correction.
- Break condition: Semantic embeddings fail to capture the relationship between noisy and clean text, resulting in irrelevant retrievals.

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: This paper builds on NER as the core task, extending it to handle noisy inputs.
  - Quick check question: What are the typical evaluation metrics for NER tasks?

- Concept: Self-attention in Transformers
  - Why needed here: The paper relies on transformer self-attention to integrate information from retrieved text into the representation of noisy text.
  - Quick check question: How does self-attention allow a transformer to incorporate information from different parts of the input sequence?

- Concept: Contrastive learning
  - Why needed here: Used to train the dense retrieval encoder to produce similar embeddings for noisy and clean text pairs.
  - Quick check question: What is the objective of contrastive learning in representation learning?

## Architecture Onboarding

- Component map: Noisy text → Retrieval Module (BM25/Dense/BERTScore) → Retrieved text → Concatenation → Transformer encoder → CRF layer → NER predictions. Multi-view learning connects noisy text only view with retrieval-augmented view.
- Critical path: Noisy text → Retrieval Module → Transformer with self-attention → CRF layer → NER predictions
- Design tradeoffs: Retrieval provides context but adds latency; multi-view learning improves generalization but increases training complexity; dense retrieval handles semantic similarity but is computationally expensive.
- Failure signatures: Retrieval provides irrelevant context (low performance); multi-view training fails to converge (training instability); dense retrieval is too slow for production (latency issues).
- First 3 experiments:
  1. Verify retrieval module returns relevant text for a small set of noisy examples.
  2. Test transformer's ability to incorporate retrieved context into token representations.
  3. Validate multi-view training improves performance on noisy text without retrieval during inference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the limitations of the current retrieval methods when applied to highly specialized or domain-specific knowledge corpora beyond Wikipedia?
- Basis in paper: [explicit] The paper mentions using Wikipedia as the primary knowledge corpus and discusses three retrieval methods (BM25, dense retrieval, and BERTScore) but does not explore their effectiveness in specialized domains.
- Why unresolved: The paper focuses on general knowledge retrieval and does not investigate how these methods perform in specialized domains where relevant information might be sparse or highly technical.
- What evidence would resolve it: Comparative studies evaluating the proposed retrieval methods on specialized corpora (e.g., medical, legal, or scientific texts) would provide insights into their adaptability and limitations.

### Open Question 2
- Question: How does the proposed retrieval-augmented NER model perform in real-time applications where retrieval time is a critical factor?
- Basis in paper: [explicit] The paper acknowledges that retrieval can be time-consuming and mentions using multi-view learning to improve performance without retrieval during inference, but does not evaluate real-time performance.
- Why unresolved: The paper does not provide empirical data on the latency introduced by retrieval or the trade-offs between retrieval accuracy and inference speed in real-time scenarios.
- What evidence would resolve it: Benchmarking the model's inference time with and without retrieval in time-sensitive applications would clarify its suitability for real-time use cases.

### Open Question 3
- Question: Can the retrieval-augmented approach be extended to handle multilingual or cross-lingual NER tasks effectively?
- Basis in paper: [inferred] The paper uses XLM-RoBERTa, a multilingual model, and retrieves from Wikipedia, which supports multiple languages, suggesting potential for cross-lingual applications, but this is not explicitly tested.
- Why unresolved: The experiments focus on English datasets, and the paper does not explore whether the retrieval-augmented approach generalizes to multilingual or cross-lingual scenarios.
- What evidence would resolve it: Experiments evaluating the model on multilingual NER datasets (e.g., CoNLL-2002 for Spanish or Dutch) would demonstrate its effectiveness in cross-lingual settings.

## Limitations
- The evaluation is limited to two English datasets with synthetic noise, which may not capture the full complexity of real-world noisy text.
- The approach relies heavily on Wikipedia as the knowledge corpus, which may not be available or relevant for specialized domains.
- The retrieval-augmented approach introduces significant computational overhead during both training and inference.

## Confidence
- High Confidence: The core mechanism of retrieval augmentation improving NER performance through contextual enhancement is well-supported by experimental results showing consistent F1 score improvements across noise levels and retrieval methods.
- Medium Confidence: The multi-view learning framework's ability to maintain performance without retrieval during inference is demonstrated but relies on specific training configurations.
- Medium Confidence: The claim that dense retrieval handles OCR errors better than sparse retrieval is supported by experimental results but requires further validation with more diverse OCR error patterns.

## Next Checks
1. Test the approach on non-English datasets and domain-specific noisy text to assess generalizability beyond the evaluated benchmarks.
2. Apply the method to naturally occurring noisy text rather than synthetically generated noise to evaluate performance on realistic error patterns.
3. Evaluate performance using alternative knowledge sources (e.g., domain-specific corpora, multilingual knowledge bases) to determine the dependency on Wikipedia.