---
ver: rpa2
title: Unveiling Multiple Descents in Unsupervised Autoencoders
arxiv_id: '2406.11703'
source_url: https://arxiv.org/abs/2406.11703
tags:
- data
- noise
- descent
- double
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the double descent phenomenon in unsupervised
  learning using under-complete autoencoders (AEs). The authors demonstrate that double
  and triple descent can occur in nonlinear AEs across various data models and architectural
  designs, unlike linear AEs which do not exhibit double descent.
---

# Unveiling Multiple Descents in Unsupervised Autoencoders

## Quick Facts
- arXiv ID: 2406.11703
- Source URL: https://arxiv.org/abs/2406.11703
- Authors: Kobi Rahimi; Yehonathan Refael; Tom Tirer; Ofir Lindenbaum
- Reference count: 40
- Key outcome: Demonstrates double and triple descent in nonlinear autoencoders across various data models, showing over-parameterized models improve both reconstruction and downstream tasks like anomaly detection and domain adaptation.

## Executive Summary
This paper investigates the double descent phenomenon in unsupervised learning using under-complete autoencoders. The authors show that nonlinear autoencoders exhibit multiple descent patterns in test loss curves, unlike their linear counterparts. Through extensive experiments on synthetic and real datasets, they demonstrate that over-parameterized models not only improve reconstruction but also enhance performance in downstream tasks such as anomaly detection and domain adaptation. The study reveals that bottleneck size and model capacity significantly impact the double descent behavior, with larger models showing better performance especially under noise and domain shift conditions.

## Method Summary
The authors study under-complete autoencoders with multi-layer perceptrons for reconstruction tasks. They use MSE loss with Adam optimizer and train on contaminated datasets (sample noise, feature noise, domain shift) while testing on clean data. The method involves varying model sizes (total parameters and bottleneck dimensions), noise levels, and domain shifts. Experiments are conducted on synthetic linear subspace data, single-cell RNA sequencing data, and CelebA attributes dataset. The key innovation is partitioning model size into bottleneck and hidden layer dimensions to understand the multiple descent phenomenon.

## Key Results
- Double and triple descent curves observed in nonlinear autoencoders but not in linear ones
- Over-parameterized models improve both reconstruction and downstream tasks like anomaly detection and domain adaptation
- Domain shift experiments show over-parameterized models adapt better to target distributions
- Bottleneck size significantly affects double descent behavior, with intermediate bottlenecks showing peak test loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Over-parameterized models can memorize noise while still generalizing better due to learning a more robust signal representation.
- Mechanism: In the presence of strong noise, under-parameterized models cannot effectively separate signal from noise, leading to poor test performance. Over-parameterized models have sufficient capacity to memorize all training samples, including noise, but the optimization process allows them to find solutions that capture the underlying signal structure while fitting the noise. This results in lower test loss despite perfect training loss.
- Core assumption: The optimization process for over-parameterized models can find solutions that balance signal capture and noise fitting.
- Evidence anchors:
  - [abstract]: "over-parameterized models not only improve reconstruction but also enhance performance in downstream tasks such as anomaly detection and domain adaptation"
  - [section]: "over-parameterized models can serve as effective domain adaptation strategies when there is a distribution shift between source and target data"
  - [corpus]: Weak evidence - related work mentions double descent in supervised learning but lacks specific unsupervised AE mechanisms

### Mechanism 2
- Claim: The bottleneck size controls the trade-off between information compression and noise tolerance, creating multiple descent points.
- Mechanism: The bottleneck layer acts as a dimensionality reduction bottleneck. When too small, it cannot capture enough signal information. As bottleneck size increases, more signal can pass through. However, at intermediate sizes, the model may partially fit noise patterns. Further increasing bottleneck size allows the model to separate signal from noise more effectively, creating a second descent in test loss.
- Core assumption: The bottleneck layer's capacity directly affects the model's ability to separate signal from noise.
- Evidence anchors:
  - [section]: "we show for the first time that both double and triple descent can be observed with nonlinear AEs across various data models and architectural designs"
  - [section]: "we partitioned the model's size into bottleneck and other hidden layer dimensions to understand the phenomenon better"
  - [corpus]: Weak evidence - related work on PCA shows no double descent, but nonlinear AEs behave differently

### Mechanism 3
- Claim: Domain shift between source and target data creates a scenario where over-parameterized models can learn shared representations that smaller models cannot capture.
- Mechanism: When training data comes from a different distribution than test data, the model needs sufficient capacity to learn representations that are invariant to the domain shift. Under-parameterized models may overfit to source domain-specific features, while over-parameterized models can learn more general features that transfer better to the target domain, reducing the domain shift.
- Core assumption: Over-parameterized models can learn domain-invariant features that smaller models cannot capture.
- Evidence anchors:
  - [abstract]: "over-parameterized models can serve as effective domain adaptation strategies when there is a distribution shift between source and target data"
  - [section]: "over-parameterized models trained on data from a source domain can adapt better to the target domain in the presence of a distribution shift"
  - [corpus]: Weak evidence - related work on domain adaptation exists but doesn't specifically address double descent in unsupervised settings

## Foundational Learning

- Concept: Double descent phenomenon in machine learning
  - Why needed here: Understanding why test error can decrease after initial overfitting is crucial for interpreting the multiple descent curves observed in autoencoders
  - Quick check question: What causes the second descent in double descent curves, and how does this differ from traditional bias-variance trade-off?

- Concept: Signal-to-Noise Ratio (SNR) and its impact on learning
  - Why needed here: The SNR determines how well the model can distinguish between signal and noise, affecting the shape and location of double descent curves
  - Quick check question: How does changing the SNR affect the interpolation threshold and the height of the test loss peak?

- Concept: Domain adaptation and distribution shift
  - Why needed here: Understanding how models can learn representations that transfer between different data distributions is key to interpreting the domain shift experiments
  - Quick check question: What makes over-parameterized models better at domain adaptation compared to under-parameterized models?

## Architecture Onboarding

- Component map:
  Encoder -> Bottleneck -> Decoder
  (Multi-layer perceptron with configurable hidden layer sizes)

- Critical path:
  1. Data preprocessing and noise injection
  2. Model architecture configuration (hidden and bottleneck sizes)
  3. Training loop with MSE loss
  4. Evaluation on clean test data
  5. Analysis of test loss curves for multiple descent patterns

- Design tradeoffs:
  - Model size vs. computational cost: Larger models show better double descent patterns but require more resources
  - Bottleneck size vs. information retention: Smaller bottlenecks force compression but may lose important signal
  - Training epochs vs. memorization: Longer training may lead to better memorization of noise patterns

- Failure signatures:
  - Monotonic increase in test loss: Indicates insufficient model capacity to capture signal
  - Very high training loss: Suggests model is too small to fit even the clean data
  - Oscillating test loss: May indicate optimization issues or inappropriate learning rate

- First 3 experiments:
  1. Linear subspace data with varying sample noise percentages (10%, 50%, 90%) to observe basic double descent behavior
  2. Single-cell RNA data with different bottleneck sizes (20, 100, 300) to understand bottleneck-wise descent
  3. Domain shift experiment with varying shift scales (1, 2, 3) to observe adaptation capabilities

## Open Questions the Paper Calls Out
The paper identifies several future research directions, including formulating theories to interpret the observed phenomena, understanding the role of optimization dynamics in achieving multiple descent patterns, and exploring how different types of data contamination beyond those studied affect the double descent behavior. The authors also call for investigations into the relationship between bottleneck size and information retention, as well as the generalizability of these findings to other unsupervised learning architectures.

## Limitations
- Claims about triple descent mechanisms lack rigorous mathematical proof and are only briefly mentioned
- Experimental evidence relies heavily on synthetic data with limited real-world validation
- The corpus evidence is weak, with most related work focusing on supervised learning rather than unsupervised autoencoders
- The relationship between bottleneck size and downstream task performance requires further exploration

## Confidence
- **High confidence**: Basic observation of double descent in nonlinear autoencoders (clearly demonstrated across experiments)
- **Medium confidence**: Domain adaptation benefits of over-parameterized models (supported by experiments but mechanism needs more rigorous proof)
- **Low confidence**: Triple descent phenomenon and its mechanisms (only briefly mentioned without thorough analysis)

## Next Checks
1. **Ablation study on bottleneck layer**: Systematically vary bottleneck sizes while keeping total model parameters constant to isolate the effect of bottleneck compression on double descent curves.

2. **Optimization landscape analysis**: Perform empirical studies on the loss landscape using techniques like mode connectivity to understand why over-parameterized models find better minima despite perfect training loss.

3. **Transfer to different autoencoder architectures**: Test whether the observed double descent patterns persist when using variational autoencoders or other nonlinear autoencoder variants to validate the generalizability of findings beyond standard MLP autoencoders.