---
ver: rpa2
title: Evaluating Vision-Language Models on Bistable Images
arxiv_id: '2405.19423'
source_url: https://arxiv.org/abs/2405.19423
tags:
- image
- prob
- woman
- images
- cube
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study examines how vision-language models (VLMs) interpret\
  \ bistable images\u2014visual stimuli that can be perceived in two distinct ways\
  \ but not simultaneously. The researchers compiled a dataset of 29 bistable images\
  \ and applied 116 brightness, tint, and rotation transformations, creating 3,364\
  \ total images."
---

# Evaluating Vision-Language Models on Bistable Images

## Quick Facts
- **arXiv ID**: 2405.19423
- **Source URL**: https://arxiv.org/abs/2405.19423
- **Reference count**: 40
- **Primary result**: VLMs show strong interpretation preferences for bistable images, largely unaffected by pixel manipulations, with language priors dominating over visual processing.

## Executive Summary
This study investigates how vision-language models interpret bistable images—visual stimuli that can be perceived in two distinct ways but not simultaneously. Researchers compiled a dataset of 29 bistable images and applied 116 brightness, tint, and rotation transformations, creating 3,364 total images. Twelve models across six architectures were evaluated on classification and generative tasks. Results showed most models strongly favor one interpretation over another, with minimal sensitivity to pixel-based manipulations, highlighting the dominance of language priors in VLM processing of ambiguous images.

## Method Summary
The study used a dataset of 29 bistable images subjected to 116 brightness, tint, and rotation transformations, creating 3,364 total images. Twelve models across six architectures (CLIP, Idefics, LLaVA1.5, mPLUG-Owl, InstructBLIP, BLIP2) were evaluated using default generation parameters and prompts from HuggingFace model pages. Classification tasks employed a loss ranking technique, while generative tasks used qualitative analysis of model outputs. The evaluation measured interpretation probabilities, sensitivity to image manipulations, and compared results with human perception data.

## Key Results
- Most VLMs strongly favor one interpretation of bistable images over another with minimal sensitivity to pixel-level manipulations
- Prompt variations and synonymous labels significantly influence model interpretations more than image transformations
- Exceptions include CLIP variants and BLIP2-OPT6.7, which show sensitivity to image transformations, while Idefics and LLaVA1.5-13b demonstrate more balanced preferences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLMs exhibit pronounced initial interpretation preferences for bistable images, but these preferences are largely unaffected by low-level pixel manipulations.
- Mechanism: Language priors encoded during training dominate over visual input processing for ambiguous stimuli.
- Core assumption: The multimodal training data and instruction tuning provide stronger context than subtle image variations.
- Evidence anchors:
  - [abstract]: "minimal variance under image manipulations, with few exceptions on image rotations"
  - [section 4.2]: "We observed minimal effects from image manipulations on interpretation probabilities"
  - [corpus]: Weak - related papers focus on image manipulation detection, not bistable interpretation
- Break condition: If pixel-level changes become semantically significant (e.g., large rotations or extreme brightness) or if the model architecture is designed to be more sensitive to low-level features (like CLIP variants).

### Mechanism 2
- Claim: Prompt variations and synonymous labels significantly influence VLM interpretations more than image manipulations.
- Mechanism: The language model component conditions the visual processing, making textual context the dominant factor.
- Core assumption: The VLM's language model component is more sensitive to linguistic variation than to minor visual changes.
- Evidence anchors:
  - [abstract]: "these factors significantly affect model interpretations more than image manipulations"
  - [section 4.4]: "Prompt variation...shows significant variations within models"
  - [corpus]: Missing - no direct evidence in related papers about prompt sensitivity in bistable images
- Break condition: If the visual encoder becomes the dominant component (e.g., in models with frozen LLMs and fine-tuned vision models) or if the prompt variations are semantically equivalent.

### Mechanism 3
- Claim: Different VLM architectures and training regimes lead to varying sensitivities to bistability.
- Mechanism: Architectural choices (e.g., contrastive vs. generative, model size) and training data influence how visual ambiguity is resolved.
- Core assumption: The underlying model architecture and training objectives shape the balance between visual and linguistic processing.
- Evidence anchors:
  - [section 4.1]: "CLIP variants...demonstrated sensitivity to variations in brightness and tint" vs "BLIP2-OPT...less variation compared to LLaVA"
  - [section 4.2]: "CLIP variants...exhibit the most variation despite being trained with 'minor rotations' data augmentations"
  - [corpus]: Weak - related papers discuss architectural differences but not specifically for bistable images
- Break condition: If the visual and language components are equally weighted or if the training data specifically includes bistable images.

## Foundational Learning

- Concept: Multimodal model architectures (contrastive vs. generative)
  - Why needed here: Understanding how different VLM architectures process visual and textual information is crucial for interpreting the results.
  - Quick check question: What is the key difference in how CLIP and BLIP2 process visual information before passing it to the language model?

- Concept: Bistable perception and cognitive biases
  - Why needed here: The study compares VLM interpretations to human perception, requiring knowledge of how humans process ambiguous images.
  - Quick check question: What is the primary difference between bottom-up and top-down explanations for bistable image perception?

- Concept: Instruction tuning and its impact on model behavior
  - Why needed here: The study investigates how prompt variations affect interpretations, which relates to how instruction tuning shapes model responses.
  - Quick check question: How does instruction tuning influence a model's sensitivity to linguistic variations in prompts?

## Architecture Onboarding

- Component map:
  Vision encoder -> Feature projection -> Language model conditioning -> Text generation/classification

- Critical path:
  Image → Vision encoder → Feature projection → Language model conditioning → Text generation/classification

- Design tradeoffs:
  - Vision encoder size vs. computational cost
  - Language model size vs. prompt sensitivity
  - Fusion mechanism complexity vs. training stability
  - Instruction tuning data diversity vs. task specificity

- Failure signatures:
  - Uniform preferences across all images (over-reliance on language priors)
  - Extreme sensitivity to minor pixel changes (over-reliance on vision encoder)
  - Inconsistent responses to synonymous prompts (poor instruction tuning)
  - Inability to handle rotations (lack of spatial reasoning)

- First 3 experiments:
  1. Test model on original bistable images with default prompt to establish baseline preferences
  2. Apply brightness and tint variations to assess sensitivity to low-level pixel changes
  3. Vary prompt wording and use synonymous labels to measure language prior influence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do VLMs exhibit switching behavior between bistable interpretations over time or across multiple presentations?
- Basis in paper: [explicit] The paper notes that VLMs take static images at a single point in time, while human perception of bistable images involves switching interpretations through extended focus. The authors suggest that replicating switching is difficult with current VLM methodology.
- Why unresolved: The current experimental setup only captures single-frame responses. The paper acknowledges this limitation and suggests that using VLMs that process videos could be a tractable way of mimicking the passage of time, but this has not been tested.
- What evidence would resolve it: Testing VLMs on video sequences of bistable images over time, or presenting the same image multiple times with short intervals to see if interpretations change, would provide evidence of switching behavior.

### Open Question 2
- Question: How do VLMs perform on other types of visual ambiguities beyond bistable images, such as multistable images or partially occluded objects?
- Basis in paper: [inferred] The paper focuses specifically on bistable images and notes that VLMs show minimal sensitivity to pixel-level manipulations. This suggests VLMs may have broader limitations in handling visual ambiguity, but this has not been systematically tested.
- Why unresolved: The study is limited to 29 bistable images across 7 categories. While the authors mention geometric and color-varying optical illusions have been studied with CNNs, they do not extend this analysis to VLMs for these other types of visual ambiguity.
- What evidence would resolve it: Testing VLMs on datasets of multistable images, partially occluded objects, or other forms of visual ambiguity would reveal whether the observed limitations are specific to bistable images or represent a more general pattern in VLM perception.

### Open Question 3
- Question: Do VLMs trained on different datasets (not just different LLMs) show more consistent or divergent interpretations of bistable images?
- Basis in paper: [explicit] The authors note that even when VLMs are trained on the same visual data but using different base language models, they do not consistently align in their preferences. This suggests LLM priors play a major role, but the paper does not explore whether training data differences contribute to interpretation variance.
- Why unresolved: The paper compares models trained on different datasets but attributes interpretation differences primarily to LLM priors. It does not control for or analyze the specific contribution of training data composition versus LLM architecture.
- What evidence would resolve it: Training multiple VLMs on identical image-text datasets but with different LLM architectures, and comparing their interpretations of bistable images, would isolate the effect of training data from that of the underlying LLM.

## Limitations

- Limited model diversity: Only 12 models across six architectures were evaluated, potentially limiting generalizability of findings about language prior dominance.
- Human comparison constraints: The human perception data comes from a single source, potentially introducing bias from specific experimental conditions.
- Image transformation granularity: Specific ranges and distributions of brightness, tint, and rotation values are not detailed, making it difficult to assess transformation adequacy.

## Confidence

- High Confidence: The finding that VLMs show strong initial interpretation preferences for bistable images is well-supported by experimental results.
- Medium Confidence: The claim that language priors dominate over visual processing in ambiguous image interpretation is supported but could be strengthened with more diverse model architectures and human comparison data.
- Low Confidence: The assertion that models differ fundamentally from human perception biases is limited by the single human reference study.

## Next Checks

1. Systematically vary prompts across a broader range of linguistic structures and evaluate whether observed sensitivity is consistent across different model architectures.

2. Conduct a controlled human perception study using the same image set and prompt variations to establish more robust baseline comparisons with model behavior.

3. Implement a gradient-based sensitivity analysis to determine the minimum pixel-level changes required to shift model interpretations, providing quantitative bounds on visual versus linguistic processing dominance.