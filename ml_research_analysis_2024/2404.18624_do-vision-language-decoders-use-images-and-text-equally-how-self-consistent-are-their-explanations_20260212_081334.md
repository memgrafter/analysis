---
ver: rpa2
title: Do Vision & Language Decoders use Images and Text equally? How Self-consistent
  are their Explanations?
arxiv_id: '2404.18624'
source_url: https://arxiv.org/abs/2404.18624
tags:
- image
- answer
- cc-shap
- input
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how much VLMs use image and text modalities
  when generating answers versus explanations, and evaluates their self-consistency.
  It extends MM-SHAP to VL decoders and CC-SHAP to multimodal settings, testing four
  VLMs (BakLLaV A, LLaV A-NeXT-Mistral, LLaV A-NeXT-Vicuna, mPLUG-Owl3) on VQA, GQA,
  and V ALSE benchmark.
---

# Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?

## Quick Facts
- **arXiv ID:** 2404.18624
- **Source URL:** https://arxiv.org/abs/2404.18624
- **Reference count:** 40
- **Primary result:** VL decoders are text-heavy, using images more for explanations than answers—especially in CoT settings.

## Executive Summary
This paper investigates how Vision-Language Models (VLMs) utilize image and text modalities during answer generation versus explanation generation, and evaluates their self-consistency. The authors extend MM-SHAP and CC-SHAP attribution methods to multimodal settings, testing four VLMs on VQA, GQA, and VALSE benchmarks. Results show VL decoders rely predominantly on text, with images playing a larger role in explanations than answers. Most models exhibit lower self-consistency than LLMs, with negative CC-SHAP scores on multiple-choice tasks. The study highlights the need for improved multimodal integration and more faithful explanations in VLMs.

## Method Summary
The authors extend SHAP-based attribution methods to multimodal settings, creating MM-SHAP for decoder-only VLMs and CC-SHAP for multimodal contexts. They test four VLMs (BakLLaVA, LLaVA-NeXT variants, and mPLUG-Owl3) across three benchmarks (VQA, GQA, and VALSE). The methodology involves generating explanations using zero-shot chain-of-thought prompting, computing modality attributions using MM-SHAP, and evaluating self-consistency through CC-SHAP scores and edit-based consistency tests. The study examines both the absolute and relative contributions of image and text modalities to answers versus explanations.

## Key Results
- VL decoders are predominantly text-heavy, with image modality contributing less to answer generation than text
- Images are used more heavily in explanation generation than answer generation, particularly in CoT settings
- Most models show lower self-consistency than LLMs, with negative CC-SHAP scores on multiple-choice tasks
- mPLUG-Owl3 performs best overall in multimodal integration and self-consistency

## Why This Works (Mechanism)
The study leverages SHAP-based attribution methods to quantify how VLMs distribute attention across modalities. By extending these methods to multimodal contexts, the authors can measure the relative contribution of images versus text to both answers and explanations. The mechanism relies on systematically ablating each modality and measuring the impact on model outputs, providing quantitative evidence of modality usage patterns.

## Foundational Learning

**SHAP (SHapley Additive exPlanations)**
*Why needed:* Provides a game-theoretic approach to attribute model predictions to input features
*Quick check:* Verify that SHAP values sum to the difference between model output and baseline

**Chain-of-Thought (CoT) prompting**
*Why needed:* Enables models to generate step-by-step reasoning that can be analyzed separately from final answers
*Quick check:* Ensure generated explanations follow logical progression and reference both modalities

**Modality ablation**
*Why needed:* Isolates the contribution of each input type by systematically removing it from the model
*Quick check:* Confirm that ablating both modalities should reduce performance to baseline

## Architecture Onboarding

**Component Map:** Vision Encoder -> Cross-modal Fusion -> Language Decoder -> Answer/Explanation Output

**Critical Path:** Image/text input → Encoder → Fusion layer → Decoder attention → Output generation

**Design Tradeoffs:** The study focuses on decoder-only architectures, which trade encoding efficiency for flexible multimodal reasoning but may struggle with early cross-modal integration

**Failure Signatures:** Negative CC-SHAP scores, low image attribution in answers, inconsistency between answer and explanation modality usage

**3 First Experiments:**
1. Run MM-SHAP on a single example to verify attribution distribution
2. Compare image-only vs text-only performance on a validation set
3. Test CoT explanation generation on a simple VQA example

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Interpretation of negative CC-SHAP scores in multiple-choice tasks remains unclear
- Edit-based self-consistency tests produced inconsistent results, questioning methodology reliability
- Focus on decoder-only VLMs limits generalizability to encoder-decoder architectures
- Text-heavy patterns may reflect dataset biases in VQA and GQA benchmarks

## Confidence

**High Confidence:** VL decoders are text-heavy compared to visual information, particularly in answer generation versus explanation generation. Supported by multiple metrics across different benchmarks.

**Medium Confidence:** mPLUG-Owl3 performs best overall in multimodal integration. While consistently ranked higher, metrics like CC-SHAP may not fully capture integration quality.

**Low Confidence:** Interpretation of negative CC-SHAP scores as problematic behavior, given unclear relationship to actual decision-making processes.

## Next Checks

1. Conduct ablation studies using vision-only and text-only inputs to quantify each modality's exact contribution, controlling for dataset biases

2. Test the same methodology on encoder-decoder VLMs to assess whether text-heavy patterns hold across architectural paradigms

3. Develop and validate additional self-consistency metrics less sensitive to prompt variations, particularly for edit-based consistency testing