---
ver: rpa2
title: Interpretable Prognostics with Concept Bottleneck Models
arxiv_id: '2405.17575'
source_url: https://arxiv.org/abs/2405.17575
tags:
- concept
- concepts
- degradation
- arxiv
- prognostics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes concept bottleneck models for interpretable
  prognostics, using component degradation modes as high-level concepts. The approach
  predicts remaining useful life from intermediate concept activations, enabling domain
  experts to intervene on concepts at test-time.
---

# Interpretable Prognostics with Concept Bottleneck Models

## Quick Facts
- arXiv ID: 2405.17575
- Source URL: https://arxiv.org/abs/2405.17575
- Authors: Florent Forest; Katharina Rombach; Olga Fink
- Reference count: 40
- Key outcome: Concept bottleneck models achieve competitive RUL prediction performance (RMSE 6.15 cycles) compared to black-box models while providing interpretability.

## Executive Summary
This paper proposes concept bottleneck models for interpretable prognostics, using component degradation modes as high-level concepts. The approach predicts remaining useful life from intermediate concept activations, enabling domain experts to intervene on concepts at test-time. Experiments on turbofan engine data show that concept embedding models achieve competitive RUL prediction performance compared to black-box models, while providing interpretability. The method demonstrates robustness even with limited concept labels and improves performance through test-time interventions.

## Method Summary
The paper introduces concept bottleneck models (CBM) and concept embedding models (CEM) for interpretable prognostics. The models use CNN feature extractors to process time-series data from turbofan engines, then predict binary concepts representing component degradation modes (HPT, LPT, bearings, seals). These concept activations are then used to predict remaining useful life (RUL). The CEM variant learns concept embeddings that capture richer representations than simple binary activations. Models are trained end-to-end using MSE loss for RUL and BCE loss for concepts, with a weighting coefficient Î»=0.1. Test-time interventions allow domain experts to correct concept predictions based on inspections.

## Key Results
- Concept embedding models achieve RMSE of 6.15 cycles on DS01 turbofan engine dataset
- CEM outperforms black-box models in terms of NASA score while maintaining interpretability
- Test-time interventions improve predictions when experts correct concept activations
- Models demonstrate robustness even with limited concept labels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concept bottleneck models achieve comparable RUL prediction performance to black-box models while providing interpretability.
- Mechanism: The model first predicts interpretable degradation modes as concepts, then uses these concept activations to predict RUL. This two-step process allows the model to leverage human-understandable intermediate representations while maintaining predictive accuracy.
- Core assumption: Degradation modes of components are sufficient to accurately predict RUL (completeness hypothesis).
- Evidence anchors:
  - [abstract] "concept embedding models achieve competitive RUL prediction performance (RMSE 6.15 cycles) compared to black-box models"
  - [section 1] "The underlying assumption is that the RUL of an asset is determined by the states of degradation of each of its components"
- Break condition: If degradation modes are insufficient or noisy, the concept-to-RUL mapping fails and performance degrades below black-box models.

### Mechanism 2
- Claim: Test-time interventions allow domain experts to improve predictions by correcting concept activations.
- Mechanism: When a concept activation exceeds threshold, an expert inspects the component and can intervene by setting the concept activation to ground truth, leading to revised RUL prediction that incorporates expert knowledge.
- Core assumption: Domain experts can identify true degradation states through inspection, and these corrections propagate through the concept-to-RUL mapping.
- Evidence anchors:
  - [abstract] "enabling domain experts to intervene on concept activations at test-time"
  - [section 3.3] "After confirming the degradation through an inspection, we perform a concept intervention on the LPT concept by setting its activation to 1 for all the remaining cycles, leading to a new RUL estimation"
- Break condition: If expert interventions are incorrect or inspection is too costly/frequent, the intervention strategy becomes impractical or counterproductive.

### Mechanism 3
- Claim: Concept embedding models (CEM) learn rich representations that capture both positive and negative associations with concepts, enabling robust performance even with limited concept labels.
- Mechanism: CEM learns positive and negative embeddings for each concept, then computes concept probabilities as weighted mixtures. This allows the model to encode RUL information in concept representations even when ground truth concepts are incomplete.
- Core assumption: The embedding space can capture sufficient information about RUL even when concept labels are sparse or incomplete.
- Evidence anchors:
  - [abstract] "demonstrate that the performance of CBMs can be on par or superior to black-box models, while being more interpretable, even when the available labeled concepts are limited"
  - [section 3.2] "In real-world industrial scenarios, the concepts are deemed to be incomplete"
- Break condition: If concept embeddings fail to capture relevant information or the concept space is too sparse, the model cannot maintain performance.

## Foundational Learning

- Concept: Concept Bottleneck Models
  - Why needed here: This is the core architecture that enables interpretable prognostics by forcing intermediate concept predictions.
  - Quick check question: How does a CBM differ from a standard neural network in terms of its prediction pipeline?

- Concept: Concept Completeness
  - Why needed here: Understanding when concept sets are sufficient for accurate predictions is crucial for practical deployment.
  - Quick check question: What happens when the available concepts are incomplete for the task at hand?

- Concept: Test-time Intervention
  - Why needed here: This mechanism allows domain experts to correct model predictions, which is essential for safety-critical applications.
  - Quick check question: Under what conditions should a domain expert intervene on concept activations during testing?

## Architecture Onboarding

- Component map: Input -> CNN feature extractor -> Concept bottleneck (CEM/CBM) -> RUL regressor
- Critical path: Data preprocessing -> Feature extraction -> Concept prediction -> RUL prediction. The concept bottleneck is the key interpretability point.
- Design tradeoffs: Interpretability vs performance (CBMs may sacrifice some accuracy), completeness of concepts vs available labels, intervention frequency vs inspection cost
- Failure signatures: Poor concept accuracy indicates concept definitions or training issues; high RMSE indicates concept-to-RUL mapping problems; interventions not improving predictions suggest incorrect expert corrections
- First 3 experiments:
  1. Train CEM with all available concepts on DS01 and evaluate RMSE and concept accuracy
  2. Compare performance of Boolean CBM, Fuzzy CBM, Hybrid CBM, and CEM with same concept set
  3. Evaluate test-time intervention effectiveness by simulating expert corrections on a subset of test samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do concept bottleneck models perform when the number of degradation modes increases beyond the four concepts used in the experiments?
- Basis in paper: [explicit] The paper mentions an ablation study varying the number of concepts from 1 to 4, but does not explore scenarios with more than 4 concepts.
- Why unresolved: The experiments only tested up to 4 concepts, leaving the model's scalability and performance with a larger number of degradation modes unknown.
- What evidence would resolve it: Experiments with concept bottleneck models trained and tested on datasets with more than 4 degradation modes, comparing their RUL prediction performance to black-box models.

### Open Question 2
- Question: Can concept bottleneck models be effectively applied to other types of industrial assets beyond turbofan engines?
- Basis in paper: [inferred] The paper focuses on turbofan engines but suggests future work to apply the approach to other industrial systems.
- Why unresolved: The methodology is demonstrated on a specific asset type, and its generalizability to other assets with different degradation characteristics is not explored.
- What evidence would resolve it: Application of concept bottleneck models to prognostics tasks for other industrial assets (e.g., bearings, batteries) with comparable performance to black-box models.

### Open Question 3
- Question: How does the performance of concept bottleneck models change when concept labels are noisy or incomplete in real-world scenarios?
- Basis in paper: [explicit] The paper discusses the challenge of incomplete concept labels in real-world applications and shows robustness with limited labels in experiments.
- Why unresolved: While the paper demonstrates robustness with limited labels, the impact of noisy or highly incomplete concept labels on model performance is not explicitly studied.
- What evidence would resolve it: Experiments with concept bottleneck models trained on datasets with varying degrees of concept label noise or incompleteness, assessing their RUL prediction performance and robustness.

## Limitations

- The completeness of degradation concepts is not rigorously validated - the paper assumes component degradation modes are sufficient for accurate RUL prediction but doesn't test this systematically.
- Test-time intervention mechanism assumes perfect expert inspections without considering inspection costs, false positives, or human error.
- Experiments are limited to one dataset (N-CMAPSS) and one type of prognostics task, constraining generalizability.

## Confidence

**High Confidence**: The concept embedding model (CEM) achieves competitive RMSE performance (6.15 cycles) compared to black-box models. This claim is directly supported by experimental results in Table 1 and the ablation study.

**Medium Confidence**: Test-time interventions can improve RUL predictions by incorporating expert knowledge. While the paper demonstrates this on one example, the intervention strategy's effectiveness across diverse scenarios and its practical limitations are not thoroughly explored.

**Low Confidence**: The claim that CBMs can maintain performance even with limited concept labels. The paper mentions this capability but provides limited quantitative evidence on how performance scales with concept label scarcity.

## Next Checks

1. **Concept Completeness Analysis**: Systematically evaluate model performance as a function of concept availability by removing concepts one by one and measuring degradation in RUL prediction accuracy. This would quantify the importance of each concept and test the completeness hypothesis.

2. **Intervention Cost-Benefit Analysis**: Design experiments that simulate imperfect expert interventions (false positives/negatives) and measure the trade-off between intervention frequency and prediction improvement. Include realistic constraints on inspection costs and time.

3. **Cross-Dataset Generalization**: Test the concept bottleneck approach on different prognostics datasets (e.g., PHM08, industrial bearing datasets) to assess whether the degradation concepts generalize across different equipment types and operating conditions.