---
ver: rpa2
title: 'Sparse maximal update parameterization: A holistic approach to sparse training
  dynamics'
arxiv_id: '2405.15743'
source_url: https://arxiv.org/abs/2405.15743
tags:
- sparsity
- sparse
- training
- learning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of training sparse neural networks\
  \ at scale, where standard hyperparameters (HPs) optimized for dense models do not\
  \ transfer well to sparse settings, leading to vanishing activations and gradients.\
  \ The authors propose sparse maximal update parameterization (S\u03BCPar), a holistic\
  \ approach that generalizes maximal update parameterization (\u03BCP) to account\
  \ for both width and sparsity."
---

# Sparse maximal update parameterization: A holistic approach to sparse training dynamics

## Quick Facts
- arXiv ID: 2405.15743
- Source URL: https://arxiv.org/abs/2405.15743
- Authors: Nolan Dey; Shane Bergsma; Joel Hestness
- Reference count: 40
- Primary result: Sparse maximal update parameterization (SµPar) enables stable training dynamics and hyperparameter transfer across sparsity levels in neural networks

## Executive Summary
This paper addresses the challenge of training sparse neural networks at scale, where standard hyperparameters (HPs) optimized for dense models do not transfer well to sparse settings, leading to vanishing activations and gradients. The authors propose sparse maximal update parameterization (SµPar), a holistic approach that generalizes maximal update parameterization (µP) to account for both width and sparsity. SµPar ensures that activation, gradient, and weight update scales remain stable as model width and sparsity vary, enabling HP transfer across different sparsity levels and model sizes. Empirically, SµPar shows consistent optimal HPs across sparsity levels and outperforms standard parameterization (SP) and µP, with up to 11.9% relative loss improvement at 99.2% sparsity in large-scale language modeling.

## Method Summary
SµPar reparameterizes weight initialization variance and learning rates by scaling them with width and density multipliers (m_din and m_ρ). The weight initialization variance becomes σ²_W = σ²_W,base/(m_din·m_ρ) and learning rate η = η_base/(m_din·m_ρ). This ensures stable signal propagation in expectation by applying the Law of Large Numbers to control activation, gradient, and weight update scales. The method leaves embedding, layer normalization, bias parameters, and attention logits dense while applying corrections to linear projection layers. SµPar satisfies the Feature Learning Desiderata (FLD) by maintaining Θ(1) typical element sizes for Y, ∇XL, and ∆Y across changes in width and density.

## Key Results
- SµPar ensures stable activation, gradient, and weight update scales as model width and sparsity vary
- Optimal hyperparameters remain consistent across sparsity levels from 0% to 99.2%
- Up to 11.9% relative loss improvement over SP and µP at 99.2% sparsity in large-scale language modeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SµPar ensures stable activation, gradient, and weight update scales by reparameterizing weight initialization variance and learning rates with respect to both width and sparsity multipliers.
- Mechanism: By scaling weight initialization variance as σ²_W = σ²_W,base/(m_din·m_ρ) and learning rate as η = η_base/(m_din·m_ρ), SµPar ensures the typical element size of activations, gradients, and weight updates remains Θ(1) regardless of changes in model width or density.
- Core assumption: The sparse mask M is random and unstructured, allowing the Law of Large Numbers to apply for controlling signal scales in expectation.
- Evidence anchors:
  - [abstract] "SµPar ensures that activation, gradient, and weight update scales remain stable as model width and sparsity vary"
  - [section 3.4] "SµPar is defined as the unique parameterization that satisfies the FLD by ensuring the typical element size of Y, ∇XL, and ∆Y is Θ(1) with respect to change in width md and change in density mρ"
  - [corpus] Weak - no direct corpus evidence on SµPar specifically, but related work on sparse training dynamics supports the need for such holistic control
- Break condition: The assumption of random unstructured sparsity fails (e.g., with structured or magnitude-based pruning), or when the number of non-zero weights per neuron becomes too small for the Law of Large Numbers to apply effectively.

### Mechanism 2
- Claim: SµPar enables hyperparameter transfer across different sparsity levels and model widths by maintaining consistent optimal HP values.
- Mechanism: By controlling the training dynamics holistically, SµPar ensures that the same HP values remain optimal as sparsity varies, eliminating the need for re-tuning at each sparsity level.
- Core assumption: The Feature Learning Desiderata (FLD) can be satisfied by the reparameterization scheme, ensuring stable signal propagation throughout training.
- Evidence anchors:
  - [abstract] "Further, by reparameterizing the HPs, SµPar enables the same HP values to be optimal as we vary both sparsity level and model width"
  - [section 4.1] "SµPar has stable optimal HPs across both model width and sparsity level"
  - [corpus] Weak - no direct corpus evidence on HP transfer with SµPar, but related work on maximal update parameterization (µP) shows HP transfer is possible with width
- Break condition: The FLD cannot be satisfied due to non-random sparsity patterns or when using dynamic sparse training methods that alter weight distributions unpredictably.

### Mechanism 3
- Claim: SµPar outperforms standard parameterization (SP) and maximal update parameterization (µP) as sparsity increases, with up to 11.9% relative loss improvement at 99.2% sparsity.
- Mechanism: By preventing the vanishing activation and gradient scales that occur with SP and µP at high sparsity, SµPar maintains better training dynamics and achieves superior performance.
- Core assumption: The performance gap between SµPar and other parameterizations grows with sparsity due to increasingly under-tuned learning rates for SP and µP.
- Evidence anchors:
  - [abstract] "On large-scale language modeling, SµPar shows increasing improvements over standard parameterization as sparsity increases, leading up to 11.9% relative loss improvement at 99.2% sparsity"
  - [section 4.3] "As sparsity increases, results across pretraining loss and average downstream task accuracy consistently show SP and µP fall farther behind SµPar"
  - [corpus] No direct evidence - this specific performance claim appears unique to this paper
- Break condition: The performance gap diminishes at low sparsity levels where SP and µP already perform adequately, or when hardware acceleration for sparse operations becomes available.

## Foundational Learning

- Concept: Maximal Update Parameterization (µP)
  - Why needed here: SµPar builds upon µP by extending its principles to handle sparsity, so understanding µP is crucial for grasping SµPar's derivation and purpose
  - Quick check question: What is the key feature of µP that enables hyperparameter transfer across different model widths?

- Concept: Feature Learning Desiderata (FLD)
  - Why needed here: FLD defines the goal of controlling signal propagation in neural networks, which SµPar aims to satisfy for sparse networks
  - Quick check question: According to FLD, what should be the typical element size of activations, gradients, and weight updates with respect to scaling variables?

- Concept: Law of Large Numbers
  - Why needed here: SµPar's derivation relies on the Law of Large Numbers to ensure stable signal scales in expectation when the number of non-zero weights per neuron is large
  - Quick check question: How does the Law of Large Numbers justify SµPar's assumption that weight initialization and learning rate corrections will stabilize training dynamics?

## Architecture Onboarding

- Component map: Sparse layers (projection weights) -> SµPar initialization and learning rate corrections -> Stable activation/gradient scales
- Critical path: 1) Determine sparsity level and width multiplier for each layer, 2) Adjust weight initialization variance as σ²_W = σ²_W,base/(m_din·m_ρ), 3) Adjust learning rate as η = η_base/(m_din·m_ρ), 4) Apply corrections during training
- Design tradeoffs: SµPar trades implementation complexity (tracking sparsity and applying corrections) for the benefit of stable training dynamics and hyperparameter transfer. The method assumes random unstructured sparsity, which may not hold for all pruning strategies.
- Failure signatures: Training instability, particularly vanishing activations and gradients at high sparsity levels, indicates SµPar corrections are not being applied correctly. If optimal HPs drift significantly with sparsity changes, this also suggests implementation issues.
- First 3 experiments:
  1. Implement SµPar on a small GPT-like model with varying sparsity levels and verify stable activation scales across sparsity (similar to Figure 5).
  2. Test hyperparameter transfer by tuning a dense proxy model and applying the same HPs to sparse models at different sparsity levels (similar to Figure 6).
  3. Compare SµPar against SP and µP on a larger language model at high sparsity levels to verify performance improvements (similar to Figure 3).

## Open Questions the Paper Calls Out

- Question: Does SµPar improve downstream task performance when applied to pruned models (e.g., iterative magnitude pruning)?
- Basis in paper: [inferred] The paper notes that SµPar may apply to IMP if the mask at initialization allows non-zero weights to have a Gaussian distribution, but this is not explicitly tested.
- Why unresolved: The paper focuses on random unstructured static sparsity and does not evaluate SµPar on pruned models, which are common in practice.
- What evidence would resolve it: Empirical comparison of SµPar vs. standard parameterization on iterative magnitude pruning across multiple sparsity levels and downstream tasks.

- Question: How does SµPar perform with structured sparsity patterns (e.g., 2:4 block sparsity) compared to random unstructured sparsity?
- Basis in paper: [explicit] The paper mentions that SµPar should extend to 2:4 block structured sparsity but does not provide empirical results for this case.
- Why unresolved: The experiments only test random unstructured sparsity, leaving the effectiveness of SµPar on structured patterns unclear.
- What evidence would resolve it: Experimental results comparing SµPar performance on 2:4 block structured sparsity vs. random unstructured sparsity across different model sizes and tasks.

- Question: Can SµPar be extended to dynamic sparse training methods (e.g., RigL, GMP) to achieve stable optimal hyperparameters across sparsity levels?
- Basis in paper: [explicit] The paper shows that SµPar does not achieve stable optimal learning rates for dynamic sparse training methods like RigL and GMP, suggesting the need for further extension.
- Why unresolved: The paper identifies the limitation but does not propose or test a generalized parameterization for dynamic sparse training.
- What evidence would resolve it: Development and empirical validation of an extended SµPar formulation that stabilizes hyperparameters for dynamic sparse training methods.

## Limitations

- Assumes random unstructured sparsity patterns, which may not hold for many practical pruning strategies
- Empirical validation focused primarily on large language models, leaving generalization to other domains uncertain
- Requires careful tracking of sparsity patterns and width multipliers, introducing implementation complexity

## Confidence

**High Confidence**: The core claim that SµPar enables stable activation, gradient, and weight update scales across varying sparsity levels and model widths is well-supported by both theoretical derivation and empirical evidence.

**Medium Confidence**: The claim of hyperparameter transfer across sparsity levels is strongly supported by experimental results, but the extent to which this generalizes beyond the tested GPT-like architectures remains uncertain.

**Low Confidence**: The specific performance numbers (e.g., 11.9% relative loss improvement) are based on a single experimental setup with the SlimPajama dataset and may not generalize to other datasets or training regimes.

## Next Checks

1. **Generalization Across Domains**: Validate SµPar on computer vision tasks (e.g., image classification on ImageNet) and reinforcement learning environments to assess its effectiveness beyond language modeling.

2. **Dynamic Sparsity Patterns**: Test SµPar with dynamic sparse training methods and structured pruning patterns to evaluate its robustness when the random unstructured sparsity assumption is violated.

3. **Hardware Acceleration Impact**: Compare SµPar's performance on hardware with sparse tensor acceleration (e.g., GPUs with sparse support) versus standard hardware to determine if the method's benefits persist across different computational platforms.