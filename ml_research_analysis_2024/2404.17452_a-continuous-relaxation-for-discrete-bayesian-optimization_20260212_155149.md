---
ver: rpa2
title: A Continuous Relaxation for Discrete Bayesian Optimization
arxiv_id: '2404.17452'
source_url: https://arxiv.org/abs/2404.17452
tags:
- optimization
- function
- sequences
- discrete
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Bayesian optimization for
  discrete sequences with very few initial observations and strict evaluation budgets,
  motivated by expensive-to-evaluate protein engineering tasks. The authors propose
  a continuous relaxation of the objective function that transforms the discrete optimization
  problem into a continuous one over probability distributions, enabling direct incorporation
  of prior knowledge through weighted Hellinger kernels.
---

# A Continuous Relaxation for Discrete Bayesian Optimization

## Quick Facts
- arXiv ID: 2404.17452
- Source URL: https://arxiv.org/abs/2404.17452
- Reference count: 40
- Key outcome: State-of-the-art Bayesian optimization for discrete sequences in data-scarce settings using continuous relaxation and weighted Hellinger kernels

## Executive Summary
This paper addresses the challenge of Bayesian optimization for discrete sequences with very few initial observations and strict evaluation budgets, motivated by expensive-to-evaluate protein engineering tasks. The authors propose a continuous relaxation of the objective function that transforms the discrete optimization problem into a continuous one over probability distributions, enabling direct incorporation of prior knowledge through weighted Hellinger kernels. They develop computationally tractable inference and optimization methods, including a kernel that is linear in runtime with respect to input lengths. Empirical results on two bio-chemical sequence optimization tasks (RFP and GFP proteins) demonstrate state-of-the-art performance, particularly in the "ice-cold start" setting with only 6-50 initial sequences.

## Method Summary
The method transforms discrete sequence optimization into a continuous problem by optimizing over probability distributions rather than discrete sequences. A weighted Hellinger kernel is used for Gaussian Process regression, enabling efficient computation while incorporating prior knowledge. The approach uses either Hidden Markov Models or Variational Autoencoders to provide prior sequence distributions, with the latter offering a continuous parameterization for optimization. The method is particularly effective in cold-start settings with limited initial observations, as demonstrated on protein engineering tasks where it achieves larger Pareto fronts and better hypervolumes compared to existing methods.

## Key Results
- CoRel achieves state-of-the-art performance in "ice-cold start" protein optimization with only 6-50 initial sequences
- The weighted Hellinger kernel enables O(L × A) computation time while incorporating prior knowledge
- On RFP and GFP optimization tasks, CoRel produces larger Pareto fronts and relative hypervolumes compared to LamBO
- The approach provides interpretable optimization paths through extreme value selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The continuous relaxation transforms discrete sequence optimization into a tractable continuous problem by mapping sequences to probability distributions.
- Mechanism: By representing sequences as indicator functions over the discrete space and optimizing over the space of probability distributions, the objective function becomes differentiable while preserving the same optima.
- Core assumption: The optimization landscape in distribution space preserves the key optima of the original discrete problem.
- Evidence anchors:
  - [abstract] "propose a continuous relaxation of the objective function and show that inference and optimization can be computationally tractable"
  - [section] "we can turn this discrete optimization problem... into a continuous one by optimizing in the space of probability distributions over X"
  - [corpus] Weak - no direct corpus evidence of this specific relaxation mechanism
- Break condition: If the distribution space loses important discrete structure or if the relaxation creates pathological regions that don't correspond to meaningful sequences.

### Mechanism 2
- Claim: The weighted Hellinger kernel enables efficient computation while incorporating prior knowledge about sequence similarity.
- Mechanism: The Hellinger distance between product measures can be computed in O(L × A) time by exploiting the factorization property, and weighting allows incorporation of prior distributions over sequences.
- Core assumption: The product measure structure holds and the weighting function can be efficiently evaluated.
- Evidence anchors:
  - [section] "Since we restrict p, q to be elements of Pf , we can evaluate in k(p, q) in O(L × A) time"
  - [section] "To make use of this prior knowledge, we propose to weigh the Hellinger distance using the given ranking"
  - [corpus] Weak - no direct corpus evidence of this specific kernel formulation
- Break condition: If the weighting function becomes too complex to evaluate efficiently or if the Hellinger distance doesn't capture relevant similarity structure.

### Mechanism 3
- Claim: The approach enables both continuous and discrete optimization strategies, providing flexibility for different problem settings.
- Mechanism: The continuous relaxation allows use of continuous optimization algorithms when a parameterization exists (like VAE decoders), while still supporting discrete optimization when needed.
- Core assumption: The acquisition function remains well-behaved under both optimization approaches.
- Evidence anchors:
  - [section] "The acquisition function is queried for the maximizing point on the parameterized space via the predictive GP"
  - [section] "Even if this is not the case, the optimization helps to narrow down the choice of candidates"
  - [corpus] Weak - no direct corpus evidence of this dual optimization capability
- Break condition: If the acquisition function becomes too complex for either optimization approach or if the parameterization breaks down.

## Foundational Learning

- Concept: Gaussian Process Regression
  - Why needed here: The method uses GPs as the surrogate model for the relaxed objective function
  - Quick check question: How does a GP model uncertainty in predictions, and why is this important for Bayesian optimization?

- Concept: Hellinger Distance and Kernel Methods
  - Why needed here: The weighted Hellinger distance forms the basis of the covariance function used in the GP model
  - Quick check question: What properties make the Hellinger distance suitable for defining a positive definite kernel?

- Concept: Variational Autoencoders and Latent Space Models
  - Why needed here: The method leverages LVMs to provide continuous parameterizations of the probability distribution space
  - Quick check question: How does a VAE learn a continuous latent representation of discrete sequences, and why is this useful for optimization?

## Architecture Onboarding

- Component map:
  Input -> Relaxation layer -> Prior model -> Kernel -> Surrogate -> Acquisition -> Optimization -> Output

- Critical path:
  1. Obtain prior sequence distributions (HMM/VAE)
  2. Initialize with starting sequences and observations
  3. Fit GP with weighted Hellinger kernel
  4. Optimize acquisition function
  5. Select sequences for evaluation
  6. Update observations and repeat

- Design tradeoffs:
  - Continuous vs discrete optimization: Continuous is more efficient but requires parameterization
  - Prior model choice: HMMs are simpler but VAEs can capture more complex structure
  - Kernel weighting: More complex weighting can improve performance but increases computation

- Failure signatures:
  - Poor performance in cold-start settings may indicate insufficient prior information
  - Computational bottlenecks may suggest the weighting function is too complex
  - Optimization failures may indicate the acquisition function landscape is too rugged

- First 3 experiments:
  1. Test the relaxation mechanism on a simple discrete function with known optima
  2. Verify the weighted Hellinger kernel computation efficiency and correctness
  3. Validate the optimization pipeline on a synthetic sequence optimization problem

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several areas remain unexplored based on the analysis:

### Open Question 1
- Question: How does the weighted Hellinger kernel compare to other similarity measures in latent spaces beyond Euclidean distance, particularly for other protein engineering tasks?
- Basis in paper: [inferred] The paper shows that the weighted Hellinger kernel with the GFP VAE decoder provides a non-Euclidean similarity measure, but only tests on two protein optimization tasks.
- Why unresolved: The paper only provides empirical evidence on RFP and GFP protein optimization tasks, which may not generalize to other domains or protein families.
- What evidence would resolve it: Systematic benchmarking of the weighted Hellinger kernel against other similarity measures (e.g., string kernels, Euclidean distance in learned latent spaces) on a diverse set of protein optimization tasks or other discrete sequence optimization problems.

### Open Question 2
- Question: What is the impact of the choice of prior distribution (e.g., HMM vs. VAE) on the performance of CoRel in the cold-start setting?
- Basis in paper: [explicit] The paper uses HMMs for the RFP task and a VAE for the GFP task, but doesn't directly compare their impact on CoRel's performance.
- Why unresolved: The paper doesn't provide a direct comparison of different prior models within CoRel, making it unclear which is optimal for cold-start optimization.
- What evidence would resolve it: Ablation studies comparing CoRel's performance with different prior models (HMM, VAE, or other sequence models) on the same cold-start protein optimization tasks.

### Open Question 3
- Question: How does the performance of CoRel scale with the length of the protein sequences being optimized?
- Basis in paper: [inferred] The paper mentions that the weighted Hellinger kernel evaluation is linear in runtime with respect to input lengths, but doesn't explore how this affects optimization performance for longer sequences.
- Why unresolved: The empirical results are limited to relatively short protein sequences, and it's unclear how CoRel would perform on longer, more complex sequences.
- What evidence would resolve it: Benchmarking CoRel on protein optimization tasks with varying sequence lengths, from short peptides to full-length proteins, to assess scalability and performance degradation.

### Open Question 4
- Question: Can CoRel be extended to optimize sequences with more complex structures, such as those with insertions, deletions, or non-standard amino acids?
- Basis in paper: [inferred] The paper focuses on optimizing fixed-length sequences of standard amino acids, but doesn't address more complex sequence structures.
- Why unresolved: The current formulation of CoRel assumes a fixed alphabet and sequence length, which may not be suitable for all protein engineering tasks.
- What evidence would resolve it: Developing and testing extensions of CoRel to handle variable-length sequences, non-standard amino acids, or other sequence modifications, and evaluating their performance on relevant protein engineering benchmarks.

## Limitations
- Relies heavily on quality of prior model (HMM/VAE), with no clear fallback when models are inadequate
- Computational complexity analysis focuses on kernel evaluation but doesn't fully address optimization overhead in high-dimensional spaces
- Empirical validation limited to protein engineering tasks, leaving uncertainty about generalizability to other discrete domains

## Confidence
- **High confidence**: The mathematical formulation of the continuous relaxation and weighted Hellinger kernel is sound and well-derived
- **Medium confidence**: The empirical results on RFP and GFP tasks are promising, but the small number of test cases limits generalizability
- **Low confidence**: The scalability claims for very long sequences haven't been thoroughly validated, particularly regarding optimization convergence

## Next Checks
1. Test the relaxation mechanism on synthetic discrete optimization problems with known optima to verify preservation of solution quality
2. Evaluate performance degradation as sequence length increases beyond the tested protein domains to assess scalability limits
3. Compare against alternative discrete BO methods on non-protein discrete optimization tasks to assess domain generalizability