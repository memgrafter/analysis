---
ver: rpa2
title: 'CombU: A Combined Unit Activation for Fitting Mathematical Expressions with
  Neural Networks'
arxiv_id: '2409.17021'
source_url: https://arxiv.org/abs/2409.17021
tags:
- activation
- combu
- functions
- mathematical
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CombU introduces a strategy of combining different activation functions
  at various dimensions across different layers in neural networks, addressing the
  limitation of using a single activation function throughout. The method assigns
  different activation functions (e.g., ReLU, ELU, NLReLU) to different dimensions
  based on a designated proportion, enabling the network to better approximate complex
  mathematical expressions involving linear, exponential, and logarithmic components.
---

# CombU: A Combined Unit Activation for Fitting Mathematical Expressions with Neural Networks

## Quick Facts
- arXiv ID: 2409.17021
- Source URL: https://arxiv.org/abs/2409.17021
- Reference count: 40
- CombU outperforms six state-of-the-art activation functions in 10 out of 16 metrics on mathematical and real-world datasets

## Executive Summary
CombU introduces a novel approach to activation functions by combining different activation types across different dimensions and layers in neural networks. This method addresses the limitation of using a single activation function throughout a network by allowing each dimension to use a different activation function (ReLU, ELU, or NLReLU) based on specified proportions. The approach is theoretically proven to fit most mathematical expressions accurately and demonstrates superior performance on both synthetic mathematical datasets and real-world tabular data across classification, regression, and generative tasks.

## Method Summary
CombU works by randomly assigning different activation functions to different dimensions within each layer of a neural network based on specified ratios. For each layer, the method divides the output dimensions into three groups and applies ReLU, ELU, and NLReLU to these groups respectively. This allows the network to capture linear, exponential, and logarithmic components simultaneously through composition and linear combinations. The approach is implemented as a custom PyTorch module that can be integrated into standard MLP architectures with dropout, trained using Adam optimizer with batch size 500 and learning rate 5×10⁻⁴.

## Key Results
- CombU outperforms six state-of-the-art activation functions (ReLU, ELU, SELU, Swish, NLReLU, GELU) in 10 out of 16 metrics across four mathematical expression datasets
- On real-world tabular datasets, CombU shows superior performance in classification, regression, and generative tasks
- The advantage of CombU is particularly notable in generative tasks, likely due to the simpler but pervasive implicit mathematical relations between features
- CombU ranks in the top three for the remaining six metrics, demonstrating consistent high performance

## Why This Works (Mechanism)

### Mechanism 1
CombU enables neural networks to fit complex mathematical expressions by combining different activation functions across dimensions and layers, capturing linear, exponential, and logarithmic components simultaneously. By assigning different activation functions (ReLU, ELU, NLReLU) to different dimensions within each layer, the network can construct exponential, logarithmic, and polynomial expressions through composition and linear combinations. The core assumption is that mathematical expressions involving polynomials, quotients, exponentials, and logarithms can be constructed by combining linear, exponential, and logarithmic components in different dimensions. This breaks if the network architecture lacks sufficient depth or width to represent the mathematical expressions, or if the training algorithm fails to converge due to local optima.

### Mechanism 2
CombU provides superior performance on real-world tabular data because these datasets embed implicit mathematical relations that align with the combined activation approach. Real-world tabular datasets contain feature interactions that can be expressed through mathematical functions. CombU's combination of activation functions allows it to capture these underlying relationships more effectively than single activation functions. The core assumption is that real-world tabular datasets contain implicit mathematical relationships that can be expressed through combinations of linear, exponential, and logarithmic functions. This breaks if the dataset contains primarily unstructured data or if the mathematical relationships are too complex to be captured by the available activation functions.

### Mechanism 3
CombU demonstrates particular advantage in generative tasks because these tasks involve simpler but pervasive implicit mathematical relations between features. Generative models learn to reconstruct data distributions, which involves capturing the joint probability distributions between features. These distributions often follow simpler mathematical patterns that CombU can capture effectively. The core assumption is that generative tasks involve learning simpler mathematical relationships between features compared to supervised tasks, making CombU's approach more effective. This breaks if the generative task involves highly complex or non-mathematical data distributions that cannot be expressed through combinations of linear, exponential, and logarithmic functions.

## Foundational Learning

- Concept: Activation functions and their properties
  - Why needed here: Understanding how different activation functions (ReLU, ELU, NLReLU) behave and what mathematical components they represent is crucial for understanding why combining them works
  - Quick check question: What are the key differences between ReLU, ELU, and NLReLU in terms of their mathematical properties?

- Concept: Neural network composition and linear combinations
  - Why needed here: The theoretical proof relies on understanding how linear combinations of activation outputs can construct more complex mathematical expressions
  - Quick check question: How does the Law of Linear Combinations in Next Layer enable the construction of complex mathematical expressions?

- Concept: Mathematical expression representation
  - Why needed here: Understanding how polynomials, exponentials, logarithms, and their compositions can be represented in neural networks is fundamental to the theoretical justification
  - Quick check question: Why can't single activation functions perfectly fit mathematical expressions involving both exponential and logarithmic components?

## Architecture Onboarding

- Component map:
  Input layer -> CombU activation layers (combining ReLU, ELU, NLReLU across dimensions) -> Output layer

- Critical path:
  1. Initialize CombU with specified ratios and dimensions
  2. Assign activation functions to dimensions using random selection
  3. Apply different activation functions to corresponding dimensions
  4. Forward propagate through network
  5. Backpropagate and update weights
  6. Maintain activation assignments throughout training

- Design tradeoffs:
  - Fixed vs. learned activation ratios: The paper uses fixed ratios but adaptive ratios could potentially improve performance
  - Dimension assignment strategy: Random assignment vs. learned assignment could affect model capacity and training stability
  - Computational overhead: Applying multiple activation functions increases computation but provides more expressive power

- Failure signatures:
  - Poor convergence: May indicate incompatible activation combinations or insufficient model capacity
  - Vanishing/exploding gradients: Could occur if activation combinations create unstable gradients
  - Overfitting: Possible when CombU overfits to noise in the data, especially in supervised tasks

- First 3 experiments:
  1. Simple mathematical expression fitting: Test CombU on basic polynomial or exponential functions to verify theoretical claims
  2. Comparison with single activation functions: Evaluate performance on tabular datasets against standard activation functions
  3. Generative task validation: Test on tabular generative tasks to verify the claimed advantage in generative applications

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CombU's performance scale with increasing network depth, particularly for very deep networks?
- Basis in paper: [explicit] The paper mentions that deep networks can approximate complex functions regardless of activation, but doesn't empirically test CombU on very deep architectures
- Why unresolved: The experiments used relatively shallow networks (up to 6 layers), so the behavior of CombU in extremely deep networks remains unknown
- What evidence would resolve it: Empirical testing of CombU on networks with 50+ layers, comparing performance to standard activations across various tasks

### Open Question 2
- Question: What is the theoretical limit of CombU's ability to fit mathematical expressions involving trigonometric functions or other non-polynomial, non-exponential, non-logarithmic components?
- Basis in paper: [explicit] The paper states that CombU can fit most mathematical expressions but doesn't prove or demonstrate its capability with trigonometric functions
- Why unresolved: The theoretical proof relies on polynomial, exponential, and logarithmic components, leaving the behavior with other mathematical operations unexplored
- What evidence would resolve it: Rigorous mathematical proof extending CombU's capabilities to include trigonometric functions, or empirical experiments fitting datasets with trigonometric relationships

### Open Question 3
- Question: How does the random assignment of activation functions within each layer affect CombU's stability and performance across different runs?
- Basis in paper: [inferred] The paper uses random assignment but doesn't analyze the variance introduced by this randomness
- Why unresolved: While the paper shows CombU's advantage over other activations, it doesn't isolate the contribution of the random dimension assignment versus the choice of activation functions themselves
- What evidence would resolve it: Multiple runs with different random seeds for dimension assignment, analyzing the variance in performance and comparing to deterministic assignment strategies

## Limitations

- The theoretical foundation of CombU relies on assumptions about the mathematical expressiveness of combined activation functions but lacks rigorous mathematical proofs for the claimed universality
- The paper does not provide ablation studies examining the impact of different activation ratios or dimension assignment strategies
- The random nature of dimension assignment could introduce variability in results across different runs

## Confidence

- High confidence: CombU demonstrates superior performance on tested mathematical expression datasets and real-world tabular data compared to baseline activation functions
- Medium confidence: The theoretical claim that combining activation functions enables fitting of complex mathematical expressions, as the proof sketch lacks mathematical rigor
- Low confidence: The assertion that CombU is particularly advantageous for generative tasks, as this claim is based on limited empirical evidence and lacks theoretical justification

## Next Checks

1. Conduct ablation studies varying the activation function ratios (e.g., 0.6 ReLU / 0.2 ELU / 0.2 NLReLU) to determine optimal configurations for different task types
2. Test CombU on synthetic datasets with known mathematical relationships to quantify its ability to capture specific function types (polynomial, exponential, logarithmic)
3. Evaluate the impact of dimension assignment strategies (random vs. learned) on model performance and training stability