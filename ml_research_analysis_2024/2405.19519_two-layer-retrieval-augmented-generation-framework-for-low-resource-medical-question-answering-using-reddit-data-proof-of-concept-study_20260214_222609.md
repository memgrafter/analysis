---
ver: rpa2
title: 'Two-Layer Retrieval-Augmented Generation Framework for Low-Resource Medical
  Question Answering Using Reddit Data: Proof-of-Concept Study'
arxiv_id: '2405.19519'
source_url: https://arxiv.org/abs/2405.19519
tags:
- xylazine
- summary
- summaries
- ketamine
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a two-layer retrieval-augmented generation
  (RAG) framework for medical question answering in low-resource settings. The framework
  retrieves relevant Reddit posts, generates individual summaries for each post segment,
  and then synthesizes these into a final answer.
---

# Two-Layer Retrieval-Augmented Generation Framework for Low-Resource Medical Question Answering Using Reddit Data: Proof-of-Concept Study

## Quick Facts
- arXiv ID: 2405.19519
- Source URL: https://arxiv.org/abs/2405.19519
- Reference count: 40
- Two-layer RAG framework using quantized LLM achieves comparable performance to GPT-4 for medical QA in low-resource settings

## Executive Summary
This proof-of-concept study introduces a two-layer retrieval-augmented generation (RAG) framework designed for medical question answering in low-resource settings. The framework retrieves relevant Reddit posts about xylazine and ketamine, generates individual summaries for each post segment using a quantized open-source LLM (Nous-Hermes-2-7B-DPO), and synthesizes these into a final answer. Manual evaluation by subject matter experts found no statistically significant difference between the quantized model and GPT-4 across coverage, coherence, relevance, length, and hallucination metrics, with a significant difference only in readability as measured by the Coleman-Liau Index. The results demonstrate that the proposed framework can effectively answer medical questions about targeted topics while being deployable on standard hardware.

## Method Summary
The framework employs a two-layer RAG approach: Layer 1 retrieves top 50 Reddit posts per query using a keyword-based IR engine (Whoosh with Okapi BM25F), segments long posts to fit the LLM's context window, and generates individual summaries for each segment; Layer 2 aggregates these summaries into a final answer. The system uses a quantized 7B LLM (Nous-Hermes-2-7B-DPO) for deployability in resource-constrained environments while maintaining performance comparable to GPT-4. Evaluation involved 20 clinician-driven queries about xylazine and ketamine, generating 76 samples that were manually assessed by subject matter experts using Likert scales for coverage, coherence, relevance, length, and hallucination, along with the Coleman-Liau Index for readability.

## Key Results
- No statistically significant difference between quantized LLM and GPT-4 across coverage, coherence, relevance, length, and hallucination metrics
- Significant difference found for Coleman-Liau Index (readability), favoring the larger model
- Framework successfully answers medical questions about targeted topics while being deployable on standard hardware
- Modular architecture enables swapping between quantized and larger models without changing the underlying system

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-layer RAG mitigates hallucination by constraining generation to retrieved text segments
- Mechanism: First layer generates short, query-focused summaries from individual retrieved segments; second layer synthesizes these summaries, explicitly ignoring segments where the LLM states no answer was found. This prevents the model from inventing information not present in the retrieved text.
- Core assumption: LLMs will faithfully report when retrieved text lacks an answer and will not hallucinate when constrained to only the provided segments
- Evidence anchors:
  - [abstract] "retrieval-augmented generation (RAG) aids in constraining generated texts and improves in-context learning [5]"
  - [section] "Our framework was designed and developed with the goal of enabling the generation of answers to clinical questions relying only on text that is provided to an LLM"

### Mechanism 2
- Claim: Modular architecture enables deployment in low-resource settings while maintaining performance
- Mechanism: Using a quantized open-source LLM (Nous-Hermes-2-7B-DPO) allows the system to run on personal computers without specialized hardware. The modular design permits swapping in larger models (GPT-4) for comparison without changing the architecture.
- Core assumption: Smaller quantized models can maintain sufficient performance for medical QA when combined with effective retrieval and the two-layer approach
- Evidence anchors:
  - [abstract] "We used a quantized open-source LLM (Nous-Hermes-2-7B-DPO) to ensure deployability in resource-constrained environments"
  - [section] "We used the 8-bit quantized model Nous-Hermes 2 7B DPO... It is an open-source model and can be run locally"

### Mechanism 3
- Claim: Segmenting long documents enables use of models with smaller context windows
- Mechanism: Posts are segmented to fit within the model's context window, each segment is summarized individually, then these summaries are synthesized. This allows the system to process documents longer than the model's maximum context length.
- Core assumption: Segmenting at the post level (rather than chronologically) preserves semantic coherence while enabling processing of long documents
- Evidence anchors:
  - [section] "the framework allows for the specification of segment lengths for the retrieved text in each iteration, ensuring that the framework is applicable for relatively small LLMs with shorter context lengths"
  - [section] "segmentation is done at the post-level, without accounting for chronology"

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG combines information retrieval with text generation to ground responses in retrieved documents, reducing hallucination and improving accuracy for medical QA
  - Quick check question: What problem does RAG solve compared to using LLMs without retrieval?

- Concept: Context window limitations in LLMs
  - Why needed here: Understanding context window constraints is crucial for designing the segmentation strategy and determining when two-layer summarization is necessary
  - Quick check question: Why can't a single long document be fed directly to most LLMs for summarization?

- Concept: Quantized models and low-resource deployment
  - Why needed here: The choice of quantized models enables deployment on standard hardware, making the system accessible in resource-constrained settings
  - Quick check question: What trade-offs are made when using quantized models versus full-precision models?

## Architecture Onboarding

- Component map: Query input → Information Retrieval (IR) engine → Top 50 documents → Segmenter → Layer 1 LLM (summarize segments) → Layer 2 LLM (synthesize summaries) → Final answer output
- Critical path:
  1. User submits query
  2. IR engine retrieves top 50 documents
  3. Documents are segmented to fit context window
  4. Layer 1 LLM generates individual summaries for each segment
  5. Layer 2 LLM synthesizes individual summaries into final answer
  6. System outputs final summary

- Design tradeoffs:
  - Model size vs. deployability: Smaller quantized models run on personal computers but may have lower performance
  - Number of retrieved documents: 50 chosen as sufficient; increasing may improve coverage but adds processing time
  - Segment length: Shorter segments fit context windows better but may lose context; longer segments preserve context but may not fit
  - Manual vs. automatic evaluation: Manual evaluation by experts provides nuanced assessment but is resource-intensive

- Failure signatures:
  - Retrieval returns irrelevant documents → Summaries address wrong topics
  - Segmentation breaks semantic coherence → Individual summaries miss key information
  - Layer 1 LLM hallucinates within segments → False information propagated to Layer 2
  - Layer 2 LLM fails to synthesize coherently → Final answer is disjointed or contradictory

- First 3 experiments:
  1. Test IR engine with known queries to verify relevant documents are retrieved
  2. Run Layer 1 with a small set of retrieved segments to verify summarization quality
  3. Execute full pipeline with one query to verify end-to-end functionality before scaling to all 20 queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the two-layer RAG framework compare when using different retrieval engines beyond keyword-based approaches?
- Basis in paper: [inferred] The paper mentions using a simple keyword-based retrieval with Okapi BM25F but does not explore other retrieval methods.
- Why unresolved: The paper focuses on evaluating the LLM component rather than optimizing the retrieval engine, leaving open the question of whether more advanced retrieval methods could improve performance.
- What evidence would resolve it: Comparative studies using the same two-layer RAG architecture but with different retrieval engines (e.g., semantic search, dense retrieval) on the same medical QA tasks.

### Open Question 2
- Question: What is the minimum context length required for the first layer LLM to generate effective individual summaries while maintaining the framework's deployability in low-resource settings?
- Basis in paper: [explicit] The paper notes that the framework allows specification of segment lengths for retrieved text, but does not systematically investigate the relationship between context length and summary quality.
- Why unresolved: The paper uses a fixed segment length based on the Nous-Hermes-2-7B-DPO model's context window but does not explore whether shorter or longer segments might be more effective.
- What evidence would resolve it: Experiments varying the context window length for the first layer LLM while measuring summary quality and resource requirements across different medical topics.

### Open Question 3
- Question: How does the two-layer RAG framework handle rapidly emerging medical topics where social media discussions contain contradictory or unverified information?
- Basis in paper: [inferred] The paper evaluates the framework on established substances (xylazine and ketamine) but does not address how it performs when dealing with conflicting information in social media posts.
- Why unresolved: The paper's focus on substances with some established medical literature means it doesn't test the framework's ability to synthesize accurate information from contradictory social media sources.
- What evidence would resolve it: Testing the framework on emerging medical topics with known misinformation in social media posts and measuring its ability to identify and appropriately present conflicting information.

## Limitations
- Small evaluation sample size (20 queries, 76 samples) limits generalizability of findings
- Focus on only two substances (xylazine and ketamine) and Reddit data constrains external validity
- Manual evaluation by a single subject matter expert introduces potential bias and limits reproducibility

## Confidence
- **High confidence**: The modular architecture design and its deployment in low-resource settings
- **Medium confidence**: The two-layer RAG framework's ability to reduce hallucination through constrained generation
- **Medium confidence**: Segmenting long documents enables use of models with smaller context windows
- **Low confidence**: The framework's generalizability beyond xylazine and ketamine topics

## Next Checks
1. Expand evaluation corpus: Test the framework on 50+ diverse medical queries across multiple drug classes and medical conditions to assess generalizability and identify domain-specific limitations.

2. Multi-expert evaluation: Implement evaluation by 3-5 subject matter experts with inter-rater reliability measures to validate the consistency of manual assessment and reduce individual bias.

3. Comparative performance analysis: Conduct head-to-head comparison of the quantized model against larger models on identical queries using automated metrics (ROUGE, BERTScore) alongside manual evaluation to quantify the performance trade-off in low-resource deployment.