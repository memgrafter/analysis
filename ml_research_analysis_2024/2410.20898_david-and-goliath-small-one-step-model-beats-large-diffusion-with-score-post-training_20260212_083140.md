---
ver: rpa2
title: 'David and Goliath: Small One-step Model Beats Large Diffusion with Score Post-training'
arxiv_id: '2410.20898'
source_url: https://arxiv.org/abs/2410.20898
tags:
- diffusion
- arxiv
- one-step
- reward
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Diff-Instruct (DI) is a data-efficient post-training approach for
  one-step text-to-image generative models to improve human preferences without requiring
  image data. It frames alignment as online reinforcement learning from human feedback
  (RLHF), optimizing the one-step model to maximize human reward functions while being
  regularized to stay close to a reference diffusion process using a novel general
  score-based divergence regularization.
---

# David and Goliath: Small One-step Model Beats Large Diffusion with Score Post-training

## Quick Facts
- arXiv ID: 2410.20898
- Source URL: https://arxiv.org/abs/2410.20898
- Authors: Weijian Luo; Colin Zhang; Debing Zhang; Zhengyang Geng
- Reference count: 35
- 2.6B DI*-SDXL-1step model outperforms 50-step 12B FLUX-dev on Parti prompts benchmark using only 1.88% of inference time

## Executive Summary
Diff-Instruct* (DI*) is a data-efficient post-training approach for one-step text-to-image generative models to improve human preferences without requiring image data. It frames alignment as online reinforcement learning from human feedback (RLHF), optimizing the one-step model to maximize human reward functions while being regularized to stay close to a reference diffusion process using a novel general score-based divergence regularization. Unlike traditional RLHF methods using Kullback-Leibler divergence, this score-based regularization substantially improves performance and post-training stability. The 2.6B DI*-SDXL-1step model, post-trained from DMD2 w.r.t SDXL, achieves higher ImageReward, PickScore, and CLIP score than the 50-step 12B FLUX-dev model on the Parti prompts benchmark while using only 1.88% of the inference time.

## Method Summary
Diff-Instruct* (DI*) is a post-training approach that aligns one-step text-to-image models with human preferences through online reinforcement learning from human feedback (RLHF). The method uses score-based divergence regularization instead of traditional KL divergence, optimizing the one-step model to maximize human reward functions while staying close to a reference diffusion process. It incorporates both explicit human rewards and implicit classifier-free guidance rewards, with an assistant diffusion model approximating the student's score function for tractable loss computation. The approach enables training without image data by sampling noisy data from the reference diffusion model.

## Key Results
- DI*-SDXL-1step (2.6B) achieves higher ImageReward, PickScore, and CLIP score than 50-step 12B FLUX-dev on Parti prompts benchmark
- Uses only 1.88% of FLUX-dev's inference time while outperforming on human preference metrics
- Score-based regularization preserves diversity and prevents mode-seeking behavior compared to KL divergence
- Achieves alignment without requiring image data through online RLHF from human feedback

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Score-based divergence regularization improves post-training stability and prevents mode-seeking behavior.
- Mechanism: Unlike KL divergence which encourages mode-seeking (pushing the one-step model toward high-density modes of the reference diffusion), score-based divergence regularizes the gradient field of the score function, preserving diversity while aligning with human preferences.
- Core assumption: The score function contains sufficient information to regularize the distribution without collapsing to modes.
- Evidence anchors:
  - [abstract]: "Unlike traditional RLHF approaches, which rely on the Kullback-Leibler divergence as the regularization, we introduce a novel general score-based divergence regularization that substantially improves performance as well as post-training stability."
  - [section 3.1]: "We demonstrate that score-based regularization is important to preserve sample diversity and avoids reward hacking."
- Break condition: If the score function approximation is poor, the regularization fails and diversity is lost.

### Mechanism 2
- Claim: The tractable loss derived from intractable score-based RLHF objective allows efficient gradient computation.
- Mechanism: Although the theoretical score-based RLHF objective is intractable to optimize, Theorem 3.1 proves an equivalent tractable loss that can be efficiently computed and optimized.
- Core assumption: The assistant diffusion model can approximate the one-step model's score function pointwise.
- Evidence anchors:
  - [section 3.2]: "Although the general score-based RLHF objective is intractable to optimize, we derive a strictly equivalent tractable loss function in theory that can efficiently compute its gradient for optimizations."
  - [algorithm 1]: Alternating updates between one-step model and assistant diffusion model.
- Break condition: If the assistant diffusion model fails to approximate the score function accurately, the tractable loss diverges from the intended objective.

### Mechanism 3
- Claim: Classifier-free guidance can be incorporated through explicit-implicit reward decoupling.
- Mechanism: Theorem 3.2 shows that classifier-free guidance implicitly induces a reward function, and the tractable loss Lcfg (3.5) can be added to the DI* loss to balance explicit human reward and implicit CFG reward.
- Core assumption: The CFG-enhanced score function captures human preference signals implicitly.
- Evidence anchors:
  - [section 3.3]: "In this section, we enhance the DI* by incorporating the classifier-free reward that is implied by the classifier-free guidance of diffusion models."
  - [theorem 3.2]: Derives tractable loss for minimizing negative classifier-free reward.
- Break condition: If the CFG scale is poorly tuned, it may conflict with explicit human reward and degrade performance.

## Foundational Learning

- Concept: Diffusion models and score matching
  - Why needed here: The paper builds on diffusion model theory and score-based methods as the foundation for one-step model distillation.
  - Quick check question: What is the relationship between the score function and the data distribution in diffusion models?

- Concept: Reinforcement learning from human feedback (RLHF)
  - Why needed here: DI* frames alignment as online RLHF, optimizing the one-step model to maximize human reward functions.
  - Quick check question: How does traditional RLHF using KL divergence differ from score-based regularization in terms of mode-seeking behavior?

- Concept: Knowledge distillation
  - Why needed here: The one-step model is distilled from a pre-trained diffusion model, transferring knowledge while improving human preferences.
  - Quick check question: What is the key difference between diffusion distillation and DI* in terms of training objectives?

## Architecture Onboarding

- Component map:
  - One-step generator gθ: Maps noise z to image x0 conditioned on prompt c
  - Reference diffusion model sref: Frozen teacher model providing score function
  - Assistant diffusion model sψ: Approximates student's score function for tractable loss computation
  - Reward model r: Encodes human preferences (e.g., PickScore, CLIP score)

- Critical path: Generate fake data → Sample noisy data → Compute score regularization loss → Compute explicit reward loss → Compute CFG reward loss → Update one-step model

- Design tradeoffs:
  - Score-based vs KL divergence: Score-based preserves diversity but requires score function approximation
  - Explicit vs implicit rewards: Explicit rewards are interpretable but may need pre-training; implicit CFG rewards are built-in but require careful scaling
  - Assistant diffusion frequency: More updates improve approximation but increase computational cost

- Failure signatures:
  - Model collapse to painting-like images: KL divergence used instead of score-based regularization
  - Poor human preference scores: Inadequate reward scaling or reward model quality
  - Training instability: Assistant diffusion model fails to approximate student's score function

- First 3 experiments:
  1. Train DI* with only explicit reward (αcf g=0) to verify basic alignment works
  2. Train DI* with only CFG reward (αrew=0) to verify implicit reward incorporation
  3. Train DI* with both rewards at different scales to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of pseudo-Huber constant c affect the performance of Diff-Instruct* models?
- Basis in paper: [explicit] The paper mentions that using pseudo-Huber distance with d(y) := sqrt(||yt||^2 + c^2) - c leads to better performance than simple squared distance.
- Why unresolved: The paper doesn't provide ablation studies exploring different values of c or its sensitivity to performance.
- What evidence would resolve it: Systematic experiments varying c values and measuring downstream performance metrics.

### Open Question 2
- Question: Can Diff-Instruct* be effectively extended to multi-step or few-step text-to-image models?
- Basis in paper: [explicit] The paper suggests this as a direction for further research, noting that one-step models sometimes struggle with generating accurate human faces and hands.
- Why unresolved: The current method is specifically designed for one-step models and its extension to multi-step settings hasn't been explored.
- What evidence would resolve it: Successful application of the score-based divergence approach to few-step models with improved generation quality.

### Open Question 3
- Question: What is the optimal balance between explicit human reward scale αrew and classifier-free guidance scale αcf g?
- Basis in paper: [explicit] The paper mentions that these hyperparameters may conflict with each other and require careful tuning.
- Why unresolved: The paper only reports one optimal combination (αrew=100, αcf g=7.5) without exploring the full trade-off space.
- What evidence would resolve it: Comprehensive ablation studies mapping the performance landscape across different (αrew, αcf g) combinations.

## Limitations
- Score-based divergence regularization requires accurate approximation of the one-step model's score function, which may be challenging for complex distributions
- The method's effectiveness depends on the quality of the assistant diffusion model's approximation, introducing potential approximation errors
- Current implementation requires a pre-trained reward model, making it less data-efficient than direct preference optimization methods

## Confidence
- **High confidence**: The experimental results showing DI*-SDXL-1step outperforming FLUX-dev on human preference metrics (ImageReward, PickScore) while using only 1.88% of inference time.
- **Medium confidence**: The theoretical claims about score-based divergence regularization preventing mode-seeking behavior and preserving diversity.
- **Low confidence**: The claim that the tractable loss derived in Theorem 3.1 is "strictly equivalent" to the intractable score-based RLHF objective in practical optimization.

## Next Checks
1. **Empirical validation of score-based vs KL divergence**: Conduct controlled experiments comparing DI* with traditional KL divergence regularization on the same one-step model and dataset, measuring both human preference metrics and diversity preservation through perceptual path length or similar metrics.

2. **Assistant diffusion model sensitivity analysis**: Systematically vary the frequency and learning rate of assistant diffusion model updates to quantify their impact on the approximation quality of the score function and downstream one-step model performance.

3. **Generalization across architectures**: Test DI* on one-step models with different architectures (e.g., non-diffusion-based one-step models) to verify that the score-based regularization remains effective when the student model's structure differs significantly from the teacher diffusion model.