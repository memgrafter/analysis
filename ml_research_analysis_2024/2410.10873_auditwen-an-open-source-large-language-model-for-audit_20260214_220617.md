---
ver: rpa2
title: AuditWen:An Open-Source Large Language Model for Audit
arxiv_id: '2410.10873'
source_url: https://arxiv.org/abs/2410.10873
tags: []
core_contribution: This paper introduces AuditWen, the first open-source large language
  model specifically designed for the audit domain. The model addresses challenges
  faced by general LLMs in auditing, including lack of specialized knowledge and data
  biases.
---

# AuditWen:An Open-Source Large Language Model for Audit

## Quick Facts
- arXiv ID: 2410.10873
- Source URL: https://arxiv.org/abs/2410.10873
- Authors: Jiajia Huang; Haoran Zhu; Chao Xu; Tianming Zhan; Qianqian Xie; Jimin Huang
- Reference count: 25
- Primary result: First open-source LLM specifically designed for audit domain tasks

## Executive Summary
This paper introduces AuditWen, the first open-source large language model specifically designed for the audit domain. The model addresses challenges faced by general LLMs in auditing, including lack of specialized knowledge and data biases. AuditWen was developed by fine-tuning Qwen with a 30k instruction dataset constructed from 15 audit tasks across three layers: phrase, sentence, and document levels. The model demonstrates superior performance compared to existing LLMs on various audit tasks, including information extraction, question answering, and document generation.

## Method Summary
AuditWen was developed by fine-tuning the Qwen model using a carefully constructed dataset of 30,000 instructions derived from 15 distinct audit tasks. The tasks were organized across three hierarchical levels: phrase-level tasks for basic terminology and concept understanding, sentence-level tasks for contextual reasoning, and document-level tasks for comprehensive analysis and synthesis. The fine-tuning process involved supervised learning on this specialized dataset, allowing the model to develop domain-specific capabilities while retaining the general language understanding of the base Qwen model. The authors also constructed a comprehensive evaluation benchmark to assess the model's performance across various audit-related tasks.

## Key Results
- AuditWen achieved higher accuracy rates and lower missing rates across multiple audit-related tasks compared to existing LLMs
- The model demonstrated particular effectiveness in audit issue summary and legal recommendation tasks
- Evaluation results showed significant improvements in task-specific performance on the authors' constructed benchmark

## Why This Works (Mechanism)
AuditWen works by leveraging fine-tuning to adapt a general-purpose language model to the specific terminology, workflows, and reasoning patterns required in auditing. The hierarchical task structure (phrase, sentence, document levels) enables progressive learning from basic concepts to complex document analysis. The specialized dataset captures the unique language patterns and logical structures found in audit contexts, allowing the model to develop domain-specific expertise while maintaining the general language capabilities of the base model.

## Foundational Learning
- Domain-specific terminology and concepts - Why needed: Audit has specialized vocabulary and concepts that differ from general language; Quick check: Model correctly identifies and uses audit-specific terms
- Multi-level task comprehension - Why needed: Audit tasks range from simple term definitions to complex document analysis; Quick check: Model handles tasks across phrase, sentence, and document levels
- Audit workflow patterns - Why needed: Understanding the sequential and logical structure of audit processes; Quick check: Model follows proper audit reasoning chains
- Legal and regulatory context - Why needed: Audit conclusions often have legal implications requiring specific framing; Quick check: Model generates compliant recommendations

## Architecture Onboarding
**Component map:** Base Qwen model -> Fine-tuning layer -> Specialized output layer
**Critical path:** Input text -> Domain adaptation layer -> Task-specific processing -> Audit-compliant output
**Design tradeoffs:** General language capability vs. domain specificity, model size vs. performance, training data quantity vs. quality
**Failure signatures:** Hallucinations in factual audit content, incorrect application of audit standards, generation of non-compliant recommendations
**3 first experiments:** 1) Compare baseline Qwen vs. AuditWen on basic audit terminology tasks, 2) Evaluate performance on audit issue identification across different document types, 3) Test model's ability to generate audit reports following standard templates

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies entirely on authors' constructed benchmark, lacking independent verification
- Performance metrics are author-defined and their appropriateness for audit tasks is not independently validated
- Training dataset of 30k instructions may be insufficient compared to billions of tokens used for general LLMs

## Confidence
- High confidence in the model architecture and fine-tuning approach
- Medium confidence in domain-specific effectiveness claims
- Low confidence in the generalizability of results without independent validation and benchmark verification

## Next Checks
1. Independent evaluation of AuditWen using established audit domain benchmarks to verify the reported performance improvements
2. Assessment of the model's factual accuracy and hallucination rates in audit-specific contexts through blind testing by audit professionals
3. Analysis of the model's performance on out-of-distribution audit scenarios to evaluate robustness beyond the training tasks