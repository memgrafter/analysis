---
ver: rpa2
title: Spectral complexity of deep neural networks
arxiv_id: '2405.09541'
source_url: https://arxiv.org/abs/2405.09541
tags:
- neural
- networks
- have
- random
- depth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how the depth of neural networks affects their
  complexity by analyzing the spectral properties of the associated random fields.
  The authors propose to characterize network complexity using the angular power spectrum
  of the limiting isotropic Gaussian process, defining a "spectral law" and studying
  its asymptotic behavior as depth increases.
---

# Spectral complexity of deep neural networks

## Quick Facts
- arXiv ID: 2405.09541
- Source URL: https://arxiv.org/abs/2405.09541
- Reference count: 40
- One-line primary result: Neural networks can be classified into three spectral complexity regimes (low-disorder, sparse, high-disorder) based on how their angular power spectrum moments behave as depth increases.

## Executive Summary
This paper introduces a novel framework for characterizing the complexity of deep neural networks by analyzing the spectral properties of their associated random fields. The authors propose using the angular power spectrum of the limiting isotropic Gaussian process to define a "spectral law" and study how it evolves with network depth. They establish a three-regime classification (low-disorder, sparse, high-disorder) based on the asymptotic behavior of spectral moments, showing that standard activation functions like ReLU, GELU, and hyperbolic tangent fall into different regimes. The framework provides new insights into the role of depth and activation functions in neural network architectures, with ReLU networks exhibiting sparse spectral properties that may explain their self-regularization benefits.

## Method Summary
The authors analyze deep neural networks by considering their infinite-width Gaussian process limit and studying the associated isotropic random fields on the sphere. They decompose these fields using spherical harmonics to obtain an angular power spectrum, which they call the "spectral law." The key insight is that the derivative of the network's covariance kernel at the origin determines which of three complexity regimes the network falls into: low-disorder (exponential decay), sparse (polynomial growth), or high-disorder (exponential divergence). The analysis leverages the compositional structure of deep network kernels and properties of isotropic random fields to derive asymptotic results about spectral moments as depth increases.

## Key Results
- ReLU networks belong to the sparse regime, concentrating spectral mass on low-frequency multipoles regardless of depth
- Different activation functions create fundamentally different depth complexity regimes based on their kernel derivatives at the origin
- The full angular power spectrum captures depth-dependent complexity that Sobolev norms miss
- Spectral effective support and dimension provide quantitative measures of network complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReLU networks belong to the sparse regime because the derivative of their covariance kernel at the origin equals 1, causing the spectral mass to concentrate on low-frequency multipoles as depth increases.
- Mechanism: The key is that when κ′(1) = 1, the first moment of the spectral law goes to zero while the second moment remains bounded, leading to moments growing polynomially. This forces most probability mass onto the lowest multipoles, creating a sparse high-frequency structure.
- Core assumption: The network width approaches infinity so that the Gaussian process limit holds, and the standard initialization conditions (Γb + ΓW0 = 1) are satisfied.
- Evidence anchors:
  - [abstract]: "ReLU networks belong to the sparse regime, meaning they can be well-approximated in L2 by low-degree polynomials while having sparse high-frequency components."
  - [section]: "If κ′(1) = 1, then lim L→∞ E[XL] = 0, 1 ≤ E[X2L] ≤ d"
  - [corpus]: Weak - no direct corpus papers address this specific spectral characterization
- Break condition: If the network width remains finite, or if the initialization deviates from the standard conditions, the Gaussian process limit fails and the spectral analysis no longer applies.

### Mechanism 2
- Claim: The full angular power spectrum, not just the tail, captures depth-dependent complexity that Sobolev norms miss.
- Mechanism: By studying the entire distribution of spectral weights across all multipoles rather than just the tail decay, the framework reveals how depth redistributes spectral mass. This redistribution affects the field's Sobolev energy even when RKHS norms remain unchanged.
- Core assumption: Isotropic random fields on the sphere provide an appropriate mathematical framework for analyzing neural network kernels.
- Evidence anchors:
  - [abstract]: "While [13] look at the tail of the spectrum, which indeed completely determines the RKHS, we look at the whole spectrum, which may well depend on the depth even when the tail does not."
  - [section]: "Our approach yields other important advantages that cannot be obtained by just controlling the covariance decay, as in the edge of chaos literature, or the tails of the spectrum."
  - [corpus]: Weak - related papers focus on scaling laws or spectral convolutions but not on this specific spherical harmonic decomposition approach
- Break condition: If the kernel is not isotropic or the domain is not spherical, the Gegenbauer polynomial expansion breaks down.

### Mechanism 3
- Claim: Different activation functions create fundamentally different depth complexity regimes (low-disorder, sparse, high-disorder) based on the behavior of their kernel derivatives at the origin.
- Mechanism: The value of κ′(1) determines which regime an activation function falls into: κ′(1) < 1 leads to low-disorder (exponential decay), κ′(1) = 1 creates sparse behavior (polynomial growth), and κ′(1) > 1 produces high-disorder (exponential divergence). This classification explains why ReLU can go deeper with less overfitting risk.
- Core assumption: The infinite-width limit accurately characterizes the behavior of finite but wide networks.
- Evidence anchors:
  - [abstract]: "Depending on the form of the activation function, we distinguish three regimes: a low-disorder case (including the Gaussian activation), where the finite moments of XL decay exponentially to zero as L → ∞; a sparse case (including ReLU and Leaky ReLU), where the first moment goes to 0, the second is uniformly bounded and the others grow polynomially – when they exist; and a high-disorder case (including the hyperbolic tangent), where the finite moments diverge exponentially."
  - [section]: "We classify networks in three regimes where depth plays a significantly different role. In short, these regimes correspond to degenerate asymptotics (convergence to a trivial limit), asymptotic boundedness, and exponential divergence."
  - [corpus]: Weak - neighboring papers discuss spectral properties but don't establish this three-regime classification
- Break condition: If the activation function's kernel doesn't have a well-defined derivative at the origin, or if the function is not sufficiently smooth, the regime classification fails.

## Foundational Learning

- Concept: Isotropic random fields and spherical harmonics
  - Why needed here: The paper uses the decomposition of neural network kernels into spherical harmonics to analyze their spectral properties. Understanding isotropic random fields on the sphere is essential for interpreting the angular power spectrum and its connection to network complexity.
  - Quick check question: What mathematical property of isotropic random fields allows their covariance to be expressed using Gegenbauer polynomials?

- Concept: Gaussian process limits of neural networks
  - Why needed here: The analysis relies on the fact that infinitely wide neural networks converge to Gaussian processes. This connection allows the application of random field theory to understand network behavior as depth increases.
  - Quick check question: Under what conditions do randomly initialized neural networks converge to Gaussian processes in the infinite-width limit?

- Concept: Kernel composition and iterative structure
  - Why needed here: The paper exploits the fact that deep network kernels can be expressed as compositions of a base kernel. This compositional structure is crucial for deriving the asymptotic behavior of derivatives and moments as depth increases.
  - Quick check question: How does the iterative composition of kernels relate to the number of hidden layers in a neural network?

## Architecture Onboarding

- Component map: Input -> Kernel derivative calculation -> Regime classification -> Moment asymptotics -> Spectral effective support/dimension
- Key modules: Kernel derivative computation, Spectral moment calculation, Asymptotic analysis, Visualization of angular power spectra
- Dependencies: Mathematical libraries for special functions, Monte Carlo sampling tools, Spherical data analysis packages (HealPIX)

- Critical path:
  1. Compute the kernel function for the chosen activation
  2. Calculate its derivative at the origin to determine the regime
  3. Derive moment asymptotics using the appropriate propositions
  4. Compute spectral effective support and dimension
  5. Validate with Monte Carlo simulations of random fields

- Design tradeoffs:
  - Analytical vs. numerical: The framework provides analytical results but requires numerical validation
  - Dimensionality: The analysis assumes spherical domains, which may not generalize to all architectures
  - Initialization sensitivity: Results depend on specific initialization conditions (Γb + ΓW0 = 1)

- Failure signatures:
  - If κ′(1) is close to 1, numerical instability in moment calculations
  - If the activation function is not sufficiently smooth, the regime classification may not apply
  - If width is not large enough, the Gaussian process approximation breaks down

- First 3 experiments:
  1. Implement the kernel derivative calculation for different activation functions (ReLU, tanh, Gaussian) and verify the regime classification
  2. Compute spectral effective support for ReLU networks with varying depth and confirm concentration on low multipoles
  3. Visualize random field realizations for different regimes and depth levels to observe the qualitative differences in behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed spectral complexity framework be extended to analyze convolutional neural networks and other non-fully connected architectures?
- Basis in paper: [inferred] The authors mention this as a promising direction in the conclusions, noting that convolutional networks have similar associated covariance kernels.
- Why unresolved: The current framework is developed specifically for isotropic random fields on spheres, which naturally arise from fully connected networks in the infinite-width limit. Convolutional networks have different geometric structures that would require adapting the spectral analysis.
- What evidence would resolve it: A theoretical extension showing how to define and compute angular power spectra for convolutional networks, along with classification into the three complexity regimes.

### Open Question 2
- Question: What is the exact distribution of the limiting spectral law for different activation functions beyond the moment analysis provided?
- Basis in paper: [explicit] The authors note in the conclusions that characterizing the exact distribution of the limiting spectral law is an open problem.
- Why unresolved: While the paper provides asymptotic behavior of moments, the full probability distribution of the spectral law remains unknown for most activation functions.
- What evidence would resolve it: Deriving closed-form expressions or limiting distributions for the spectral law of specific activation functions, potentially using techniques from random matrix theory or stochastic analysis.

### Open Question 3
- Question: How does the spectral complexity framework change when both width and depth grow jointly to infinity, rather than taking the infinite-width limit first?
- Basis in paper: [explicit] The authors identify this as an open problem in the conclusions, noting that the associated random fields would be isotropic but with different covariance kernel behavior.
- Why unresolved: The current analysis assumes infinite width first, which simplifies the analysis but may miss important finite-width effects that depend on the ratio of depth to width.
- What evidence would resolve it: Analyzing the joint asymptotics of width and depth, characterizing how the spectral law and its moments depend on the width-to-depth ratio, and comparing the resulting complexity measures to the infinite-width case.

## Limitations
- The analysis relies heavily on the infinite-width Gaussian process limit, with unclear translation to practical finite-width networks
- The regime classification based on κ′(1) may oversimplify the rich behavior of different activation functions
- The assumption of isotropic kernels on spherical domains may not capture the complexity of real-world neural network architectures

## Confidence
- **High confidence**: The mathematical framework connecting deep networks to isotropic random fields on the sphere is rigorous and well-established
- **Medium confidence**: The three-regime classification based on kernel derivatives at the origin is theoretically sound but may require further empirical validation for practical networks
- **Medium confidence**: The specific claim that ReLU networks concentrate spectral mass on low-frequency multipoles is supported by both theory and numerical experiments

## Next Checks
1. **Finite-width validation**: Test whether the spectral regime predictions hold for networks with practical widths (e.g., 100-1000 neurons per layer) rather than just the infinite-width limit

2. **Architectural generalization**: Extend the analysis to non-isotropic kernels and non-spherical domains to assess the framework's applicability to more general network architectures

3. **Training dynamics**: Investigate whether the spectral properties identified in the random initialization persist through training, or whether gradient-based optimization fundamentally alters the spectral distribution