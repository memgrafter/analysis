---
ver: rpa2
title: Markov Constraint as Large Language Model Surrogate
arxiv_id: '2406.10269'
source_url: https://arxiv.org/abs/2406.10269
tags:
- constraint
- n-grams
- n-gram
- sentences
- threshold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces NgramMarkov, a constraint for generating text
  in constraint programming that uses n-grams with probabilities from a large language
  model (LLM). This constraint limits the product of n-gram probabilities in a sentence,
  extending the ElementaryMarkov constraint propagator to incorporate LLM probabilities
  instead of maximum likelihood estimation.
---

# Markov Constraint as Large Language Model Surrogate

## Quick Facts
- arXiv ID: 2406.10269
- Source URL: https://arxiv.org/abs/2406.10269
- Authors: Alexandre Bonlarron; Jean-Charles Régin
- Reference count: 30
- Key outcome: Successfully solved a real-world problem using 4-grams instead of 5-grams for the first time

## Executive Summary
This paper introduces NgramMarkov, a constraint for generating text in constraint programming that leverages n-gram probabilities from a Large Language Model (LLM) instead of traditional maximum likelihood estimation. The constraint employs a gliding threshold mechanism to reject low-probability n-grams while allowing locally unlikely transitions, ensuring globally balanced solutions. It can also combine with a look-ahead approach to remove n-grams unlikely to lead to acceptable sentences. The method significantly reduces candidate sentences, improves computation times, and enables the use of smaller n-grams for previously unsolvable problems.

## Method Summary
The NgramMarkov constraint extends the ElementaryMarkov constraint by using LLM-derived probabilities for n-grams instead of maximum likelihood estimation. For each n-gram, the LLM is queried once to obtain its probability, which is then cached and used for all subsequent filtering checks. The constraint applies three main filtering criteria: an instant threshold that rejects n-grams below a certain probability, a gliding threshold that enforces global regularity by checking whether the sum of log-probabilities stays within acceptable bounds, and a look-ahead approach that prunes n-grams unlikely to lead to high-probability continuations within a fixed horizon. These mechanisms work together to reduce the search space while maintaining solution quality.

## Key Results
- Successfully solved a real-world problem using 4-grams instead of 5-grams for the first time
- Generated text valued similarly to LLM perplexity function, showing comparable quality
- Improved filtering compared to prior approaches, reducing poor solutions while maintaining reasonable computation times

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NgramMarkov uses a gliding threshold to enforce global regularity while allowing locally unlikely transitions.
- Mechanism: For each partial sequence of k n-grams, the constraint checks whether the sum of log-probabilities is below k·T/|X|, where T is a threshold derived from the mean and standard deviation of the n-gram probability distribution.
- Core assumption: The distribution of n-gram probabilities approximates a left-skewed distribution, making mean and standard deviation useful for threshold calibration.
- Evidence anchors:
  - [abstract] The constraint "uses a gliding threshold, i.e., it rejects n-grams whose local probabilities are too low, to guarantee balanced solutions."
  - [section] "The most straightforward way to define T is to divide it by the number of n-grams needed... This expression can be fine-tuned by taking account of the statistic summary extracted from the n-gram distribution."
- Break condition: If the actual n-gram probability distribution is highly non-stationary or multimodal, the threshold may misclassify valid transitions.

### Mechanism 2
- Claim: Look-ahead filtering prunes n-grams that cannot lead to high-probability continuations within a fixed horizon.
- Mechanism: For each n-gram, the propagator explores possible successor chains up to depth p, computing the sum of log-probabilities along each path. If no path satisfies the threshold constraint, the n-gram is filtered out.
- Core assumption: The horizon p is small enough (≤n) to keep enumeration tractable, yet large enough to capture relevant future transitions.
- Evidence anchors:
  - [abstract] The constraint "can also be combined with a 'look-ahead' approach to remove n-grams that are very unlikely to lead to acceptable sentences for a fixed-length horizon."
  - [section] "an n-gram is filtered out if none of its future transitions (in p steps) may lead to a high enough probability sequence."
- Break condition: If p is too small, pruning may be ineffective; if too large, computation becomes prohibitive.

### Mechanism 3
- Claim: Using LLM-derived probabilities instead of MLE mitigates data sparsity and improves filtering.
- Mechanism: For each n-gram, the propagator queries the LLM once to obtain a probability, then uses this value in all subsequent filtering checks, avoiding repeated expensive calls.
- Core assumption: A single probability query per n-gram is representative enough for filtering decisions.
- Evidence anchors:
  - [abstract] "Instead of relying on MLE, the constraint utilizes probability values obtained from the LLM."
  - [section] "This involves querying the LLM for each n-gram and storing the returned probability in the constraint. This approach mitigates the computational overhead of direct LLM calls."
- Break condition: If the LLM's probability estimates are noisy or unstable across similar n-grams, filtering may become unreliable.

## Foundational Learning

- Concept: Markov constraints and n-gram models
  - Why needed here: The propagator builds on ElementaryMarkov and MDDMarkovProcess, which require understanding of how n-gram transitions define valid sequences.
  - Quick check question: What is the minimum overlap required for two n-grams to be chained in a Markov sequence?

- Concept: Large language model probability computation
  - Why needed here: NgramMarkov replaces MLE with LLM probabilities, so the engineer must understand how to query and cache LLM outputs efficiently.
  - Quick check question: How does an LLM compute the probability of an n-gram, and why is it more expensive than MLE?

- Concept: Constraint propagation and filtering algorithms
  - Why needed here: The propagator applies multiple filtering criteria (instant threshold, gliding threshold, look-ahead) to prune the search space.
  - Quick check question: In what order should the filtering criteria be applied for maximal pruning efficiency?

## Architecture Onboarding

- Component map: NgramMarkov propagator -> n-gram cache -> LLM query module -> threshold calculator -> filtering engine -> CP solver integration
- Critical path: N-gram selection -> probability lookup -> threshold check -> domain pruning -> next variable assignment
- Design tradeoffs:
  - Using LLM probabilities improves accuracy but increases precomputation time
  - Gliding threshold provides smoother filtering but requires distribution statistics
  - Look-ahead adds pruning power at the cost of higher per-n-gram computation
- Failure signatures:
  - Memory exhaustion: Occurs when the n-gram set or MDD becomes too large
  - Too few solutions: Thresholds too strict; relax T or increase λ
  - Too many poor solutions: Thresholds too loose; tighten T or decrease λ
- First 3 experiments:
  1. Run propagator with instant threshold only on a small corpus; verify that low-probability n-grams are filtered
  2. Add gliding threshold; observe solution count and perplexity distribution changes
  3. Enable look-ahead; measure pruning efficiency and solution quality

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of empirical validation data for key mechanisms like gliding threshold and look-ahead filtering effectiveness
- No specification of the exact problem solved or comparison with baseline approaches for the 4-gram claim
- Missing data on computational costs of LLM queries and memory requirements for n-gram caching

## Confidence

**High Confidence**: The basic architectural claim that NgramMarkov extends ElementaryMarkov by incorporating LLM probabilities instead of MLE. This is clearly specified in the methodology and represents a straightforward technical modification.

**Medium Confidence**: The effectiveness of gliding threshold filtering. The mechanism is theoretically sound and the mathematical formulation is clear, but without corpus validation data showing how often unlikely n-grams are correctly identified and filtered, the practical impact remains uncertain.

**Low Confidence**: The claim that this approach enables solving previously unsolvable problems with smaller n-grams. Without specification of the exact problem solved and comparison with baseline approaches using the same problem, this claim cannot be independently evaluated.

## Next Checks

1. **Distribution Analysis**: Extract n-gram probabilities from a representative corpus and verify whether the distribution is indeed left-skewed as assumed for the gliding threshold calibration. Plot the actual probability distribution and compare with the theoretical assumptions.

2. **Threshold Sensitivity**: Systematically vary the gliding threshold parameter T across multiple orders of magnitude and measure the resulting solution set size and average perplexity. This would reveal whether the threshold is appropriately calibrated or overly sensitive.

3. **Look-ahead Pruning Effectiveness**: Implement the look-ahead filtering with different horizon depths p and measure the actual pruning rate (percentage of n-grams filtered) versus the computational overhead. This would quantify the practical tradeoff between pruning power and efficiency.