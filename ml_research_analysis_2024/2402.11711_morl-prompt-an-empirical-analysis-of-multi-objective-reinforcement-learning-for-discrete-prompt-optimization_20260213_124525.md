---
ver: rpa2
title: 'MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning
  for Discrete Prompt Optimization'
arxiv_id: '2402.11711'
source_url: https://arxiv.org/abs/2402.11711
tags:
- reward
- prompt
- style
- optimization
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of balancing multiple competing
  reward functions in discrete prompt optimization for language models. The authors
  propose three methods for multi-objective reinforcement learning: (1) Hypervolume
  Indicator (HVI), which optimizes the volume of the Pareto reward surface; (2) Expected
  Product of Rewards, which approximates the expected volume by computing the average
  product of rewards; and (3) Multiple Gradient Descent Algorithm (MGDA), which finds
  a gradient update direction that benefits all rewards simultaneously.'
---

# MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization

## Quick Facts
- arXiv ID: 2402.11711
- Source URL: https://arxiv.org/abs/2402.11711
- Authors: Yasaman Jafari; Dheeraj Mekala; Rose Yu; Taylor Berg-Kirkpatrick
- Reference count: 15
- One-line primary result: Volume-based methods, especially expected product of rewards, achieve better balance across competing objectives in discrete prompt optimization compared to scalarization or MGDA.

## Executive Summary
This paper addresses the problem of balancing multiple competing reward functions in discrete prompt optimization for language models. The authors propose three methods for multi-objective reinforcement learning: (1) Hypervolume Indicator (HVI), which optimizes the volume of the Pareto reward surface; (2) Expected Product of Rewards, which approximates the expected volume by computing the average product of rewards; and (3) Multiple Gradient Descent Algorithm (MGDA), which finds a gradient update direction that benefits all rewards simultaneously. The authors evaluate these methods on two NLP tasks: style transfer and machine translation, using three competing reward functions for each task. The results show that volume-based methods, particularly the Expected Product of Rewards, achieve a better balance across all rewards compared to methods that optimize the average of rewards or use MGDA. The product method demonstrates superior performance in balancing the competing objectives while achieving the highest expected product value.

## Method Summary
The paper proposes three multi-objective RL methods for discrete prompt optimization: Hypervolume Indicator (HVI), which optimizes the volume of the Pareto reward surface; Expected Product of Rewards, which approximates volume by averaging the product of rewards; and Multiple Gradient Descent Algorithm (MGDA), which finds a gradient direction improving all rewards. These methods are evaluated on style transfer and machine translation tasks using three competing reward functions each. The policy network is an MLP over frozen distilGPT-2 (style transfer) or flan-T5-small (translation), generating 5-token prompts. Training uses soft Q-learning with 8 prompts per input and 128 output samples per prompt, optimized with AdamW at learning rates 5e-5 (style transfer) and 1e-4 (translation) for 12,000 steps.

## Key Results
- Volume-based methods, particularly Expected Product of Rewards, achieve better balance across all rewards compared to scalarization or MGDA baselines.
- The expected product method demonstrates superior performance in balancing competing objectives while achieving the highest expected product value.
- Across both style transfer and machine translation tasks, volume-based methods consistently outperform MGDA and average reward approaches in balancing content preservation, style matching, and sentiment rewards.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing the volume of the Pareto reward surface (e.g., hypervolume indicator or expected product) directly addresses the objective collapse problem by rewarding balanced improvements across all objectives rather than allowing disproportionate optimization of a subset.
- Mechanism: Volume-based methods measure the dominated space under the Pareto frontier of sampled outputs. By maximizing this volume, the optimization process is incentivized to push the frontier outward in all directions simultaneously, preventing collapse toward any single objective.
- Core assumption: The Pareto frontier's volume is a good proxy for "balanced performance" across competing objectives, and that the sampled outputs adequately represent the true distribution of achievable reward vectors.
- Evidence anchors:
  - [abstract]: "multi-objective methods that directly optimize the volume of the Pareto reward surface perform better and achieve a better balance of all rewards"
  - [section]: "Volume-based methods are most effective in this setting, achieving substantial gains in balancing the competing rewards"
  - [corpus]: Weak. No direct corpus evidence about volume-based Pareto optimization in prompt tuning.
- Break condition: If the sampled set of outputs is too small or unrepresentative, the volume estimate becomes unreliable and the method may still collapse toward outliers.

### Mechanism 2
- Claim: The expected product of rewards approximates the volume while being more robust to outliers than the hypervolume indicator, because it averages multiplicative rewards rather than measuring dominated space.
- Mechanism: By computing the average product of all reward values for each sampled output, the method naturally penalizes solutions where one objective is very high but others are low (since the product shrinks). This encourages a more even distribution of reward values.
- Core assumption: The product of rewards is a stable, differentiable signal that correlates with balanced performance, and that the sampling process captures enough diversity to avoid misleading averages.
- Evidence anchors:
  - [section]: "we also investigate using a simpler method for maximizing the volume in the second approach, called the expected product of rewards... The main advantage of this reward compared to the HVI reward is that the effect of the outliers will be more controlled"
  - [corpus]: Weak. No corpus evidence about expected product reward in RL prompt optimization.
- Break condition: If one reward function is near zero for most samples, the product becomes unstable or dominated by noise, breaking the balance-seeking behavior.

### Mechanism 3
- Claim: Multiple Gradient Descent Algorithm (MGDA) finds a gradient direction that simultaneously improves all objectives, avoiding the collapse seen in scalarized reward methods.
- Mechanism: MGDA solves a constrained optimization to find a common descent direction that improves every individual objective loss, rather than a weighted sum. This enforces monotonic progress across all rewards in each update.
- Core assumption: The stochastic gradient estimates for each reward are accurate enough that the constrained optimization yields a meaningful common direction, and that the Frank-Wolfe solver can reliably find the optimal weights.
- Evidence anchors:
  - [section]: "we describe the multiple gradient descent algorithm (MGDA), which finds the gradient update direction that maximizes all the rewards... This approach has been used in continuous multi-objective settings"
  - [corpus]: Weak. No corpus evidence about MGDA applied to RL-based discrete prompt optimization.
- Break condition: If gradients for different objectives conflict strongly and cannot be reconciled by any direction, the method may fail to make progress or revert to favoring some objectives over others.

## Foundational Learning

- Concept: Multi-objective optimization and Pareto optimality
  - Why needed here: The paper's core contribution is adapting multi-objective optimization techniques (volume maximization, MGDA) to discrete prompt tuning, so understanding Pareto frontiers and dominance is essential to grasp why these methods work better than scalarization.
  - Quick check question: What is the Pareto frontier in the context of prompt optimization with multiple reward functions?
- Concept: Reinforcement learning with discrete action spaces
  - Why needed here: The methods use RL to search over discrete token sequences (prompts), so understanding policy gradient methods, sampling, and reward estimation in discrete settings is necessary.
  - Quick check question: How does the policy network generate discrete prompts, and how are rewards estimated from sampled outputs?
- Concept: Hypervolume indicator and expected product as scalarization alternatives
  - Why needed here: These are the two volume-based methods compared; understanding their mathematical definitions and differences (volume vs. multiplicative averaging) is key to interpreting results.
  - Quick check question: How does the hypervolume indicator differ from simply averaging rewards, and why might it better capture balance?

## Architecture Onboarding

- Component map:
  - Policy network (MLP) -> Frozen language model (distilGPT-2 or flan-T5-small) -> Output generation -> Reward models (3 per task) -> Volume-based or MGDA optimizer -> Policy update

- Critical path:
  1. Sample k prompts from policy given input.
  2. Generate ˆk outputs per prompt using frozen LM.
  3. Compute all m rewards for each (prompt, input, output) triplet.
  4. Aggregate rewards into a single signal (volume or MGDA direction).
  5. Compute SQL loss and backpropagate to update policy.

- Design tradeoffs:
  - Using volume vs. product: Volume is more theoretically grounded but sensitive to outliers; product is simpler and more robust but may underrepresent true frontier volume.
  - Sampling ˆk outputs per prompt: Increases reward estimate stability but raises computational cost.
  - MGDA vs. volume: MGDA enforces simultaneous improvement but may struggle with conflicting gradients; volume methods naturally balance but rely on good sampling.

- Failure signatures:
  - Objective collapse: One objective dominates others in final rewards.
  - High variance in reward estimates: Unstable training or poor sampling.
  - Slow convergence: Gradient conflicts or poor signal-to-noise ratio.

- First 3 experiments:
  1. Run a single prompt optimization step with volume-based reward and inspect the distribution of individual objective values to see if balance improves.
  2. Compare hypervolume vs. expected product on a small validation set to check robustness to outliers.
  3. Implement MGDA direction finding and verify that gradients for all objectives are improved after the update.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of multi-objective optimization methods scale with larger language models (LLMs) beyond GPT-2?
- Basis in paper: [explicit] The paper acknowledges limitations regarding experimentation with larger models due to computational costs.
- Why unresolved: The paper only tested methods on smaller models like GPT-2, leaving the question of scalability and effectiveness on larger models unanswered.
- What evidence would resolve it: Empirical results comparing the performance of multi-objective optimization methods on a range of model sizes, from small to large-scale LLMs, would provide insights into scalability and effectiveness.

### Open Question 2
- Question: Can the interpretability of discrete prompts be improved while maintaining their effectiveness in multi-objective optimization?
- Basis in paper: [explicit] The paper mentions that the highest-performing prompts do not necessarily need to be interpretable by humans, but interpretability is useful in certain applications.
- Why unresolved: The paper does not explore methods to enhance the interpretability of discrete prompts without sacrificing their performance in balancing multiple objectives.
- What evidence would resolve it: Techniques that improve the interpretability of discrete prompts, such as incorporating human-readable constraints or using explainable AI methods, while demonstrating maintained or improved performance in multi-objective optimization tasks.

### Open Question 3
- Question: How do different reward functions affect the balance and performance of multi-objective optimization in discrete prompt tuning?
- Basis in paper: [explicit] The paper uses specific reward functions for tasks like style transfer and machine translation, but does not explore the impact of varying these functions.
- Why unresolved: The paper does not investigate how changes in the reward functions, such as using different metrics or adjusting their weights, influence the balance and overall performance of the optimization methods.
- What evidence would resolve it: Experiments that systematically vary the reward functions and their parameters, measuring the impact on the balance and performance of the optimization methods, would provide insights into the sensitivity and robustness of the approaches to different reward formulations.

## Limitations
- Computational constraints limit experimentation with larger models and extensive ablation studies, with each experiment requiring 20-24 hours on high-end GPUs.
- Results are demonstrated only on style transfer and machine translation with specific reward combinations, limiting generalizability to other tasks and reward structures.
- All methods rely heavily on the quality and diversity of sampled outputs, but the paper doesn't analyze how sensitive results are to sampling parameters.

## Confidence

- **High confidence**: The empirical methodology is sound, with appropriate controls (multiple random seeds), clear evaluation metrics (mean and expected product of rewards), and systematic comparison across two distinct NLP tasks.
- **Medium confidence**: The theoretical motivation for volume-based methods is reasonable, but the paper doesn't rigorously establish why the expected product specifically outperforms HVI or MGDA beyond empirical observation.
- **Low confidence**: The lack of corpus evidence for these specific methods in prompt optimization settings means we cannot assess whether the results generalize to other tasks, reward functions, or model architectures.

## Next Checks

1. **Robustness to sampling**: Vary the number of sampled prompts (k) and outputs per prompt (ˆk) to determine the minimum sampling requirements for each method, particularly testing whether volume-based methods remain superior with fewer samples.
2. **Reward function ablation**: Systematically remove or replace individual reward functions in each task to test whether the expected product method's advantages persist across different objective combinations and whether certain reward function pairs are inherently problematic.
3. **Cross-task generalization**: Apply the best-performing method (expected product) to a third NLP task with different reward structures (e.g., summarization with ROUGE, faithfulness, and coherence metrics) to assess whether the observed benefits transfer beyond the two demonstrated tasks.