---
ver: rpa2
title: LLMs can Schedule
arxiv_id: '2408.06993'
source_url: https://arxiv.org/abs/2408.06993
tags:
- jssp
- problem
- llms
- scheduling
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores using Large Language Models (LLMs) for the Job
  Shop Scheduling Problem (JSSP). A novel supervised dataset with 120k examples, containing
  natural language descriptions of JSSP problems and solutions, is introduced for
  training LLMs.
---

# LLMs can Schedule

## Quick Facts
- arXiv ID: 2408.06993
- Source URL: https://arxiv.org/abs/2408.06993
- Authors: Henrik Abgaryan; Ararat Harutyunyan; Tristan Cazenave
- Reference count: 6
- The paper introduces a supervised dataset with 120k examples containing natural language descriptions of JSSP problems and solutions, and demonstrates that a fine-tuned Phi-3 LLM with sampling can achieve a median gap of 8.92% from optimal solution.

## Executive Summary
This paper explores the application of Large Language Models (LLMs) to the Job Shop Scheduling Problem (JSSP), introducing a novel supervised dataset with 120k examples containing natural language descriptions of JSSP problems and solutions. The authors fine-tune a Phi-3 LLM using the LoRA method on this dataset and demonstrate that the fine-tuned model, when evaluated with sampling (s=10), achieves a median gap of 8.92% from the optimal solution, which is comparable to other neural approaches like Graph Neural Networks (GNNs) and Pointer Networks. The study demonstrates that LLMs, when properly trained and sampled, can effectively tackle JSSP, offering a promising new approach. Future work includes testing on larger problem sizes and exploring integration with other AI techniques.

## Method Summary
The authors generate approximately 120,000 random JSSP problems of various sizes (2x2 to 20x20) using Google's OR-Tools, then convert these into natural language descriptions to create a supervised dataset. A Phi-3 LLM is fine-tuned using LoRA with specific hyperparameters (rank 128, lora_alpha=256, dropout 0.05) on this dataset. During inference, the model uses sampling with parameters determined through grid search (top_k=50, temperature=1.0, top_p=0.95) to generate multiple candidate solutions, selecting the one with the best makespan.

## Key Results
- The fine-tuned Phi-3 LLM with sampling achieves a median gap of 8.92% from optimal solution
- Performance is comparable to other neural approaches like GNNs and Pointer Networks
- Sampling method enhances LLM effectiveness in solving JSSP
- The approach demonstrates feasibility of using LLMs for scheduling problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can translate structured JSSP instances into natural language and back, enabling supervised fine-tuning on human-readable scheduling problems.
- Mechanism: The paper converts the matrix-based JSSP format into natural language descriptions, which LLMs are trained to understand. This allows the LLM to learn scheduling patterns from textual problem-solution pairs.
- Core assumption: LLMs can generalize from natural language problem descriptions to produce valid scheduling solutions.
- Evidence anchors:
  - [abstract] "We introduce the very first supervised 120k dataset specifically designed to train LLMs for JSSP. This dataset is different from the usual ones used for JSSP problems. Instead of traditional matrix representation format, this dataset includes natural language description of the JSSP problem and solution, specifically made for training LLMs."
  - [section] "To generate feasible solutions, we employed Google's OR-Tools. The configuration for the Google's OR-Tools solver was set as follows: Maximum time allowed for the solver: 300 seconds. Number of search workers: 42. Search branching strategy: cp_model.AUTOMATIC_SEARCH. We have generated approximately 120,000 random JSSP problems of various sizes 3, ranging from 2x2 to 20x20, with the duration of each operation between 5 and 500 units."
- Break condition: If the natural language representation omits critical scheduling constraints or introduces ambiguity, the LLM may fail to produce valid schedules.

### Mechanism 2
- Claim: Fine-tuning with LoRA allows efficient adaptation of LLMs to JSSP without full model retraining.
- Mechanism: The paper uses LoRA to inject low-rank adaptation matrices into the Transformer layers, reducing the number of trainable parameters while keeping the pre-trained weights frozen.
- Core assumption: LoRA can capture the scheduling domain knowledge needed for JSSP within a small parameter budget.
- Evidence anchors:
  - [abstract] "We posit that LLMs, with their inherent ability to process and reason over complex information, can be effectively harnessed to address JSSP."
  - [section] "For fine-tuning, we followed a sample fine-tuning script provided on the Microsoft Hugging Face official website Microsoft. The base model is loaded with specific quantization configurations to optimize memory and computational efficiency using BitsAndBytesConfig, which configures 4-bit quantization with NF4 quantization type. We have used the LoRA rank (128), scaling factor (lora_alpha=256), target modules for adaptation (qkv_proj, o_proj, fc1, and fc2), and dropout rate (0.05)."
- Break condition: If the JSSP domain requires more complex reasoning than LoRA can capture, the fine-tuned model may underperform.

### Mechanism 3
- Claim: Sampling-based inference improves LLM scheduling performance by exploring multiple candidate solutions.
- Mechanism: The paper uses top-k and top-p sampling during inference to generate multiple scheduling solutions, selecting the one with the best makespan.
- Core assumption: The LLM's probability distribution over tokens contains useful scheduling information that can be exploited through sampling.
- Evidence anchors:
  - [abstract] "Furthermore, we propose a sampling method that enhances the effectiveness of LLMs in tackling JSSP."
  - [section] "We employed specific sampling hyper-parameters during the inference. To determine these hyper-parameters, we conducted a grid search on a small dataset of 7x8 and 8x8 problems, encompassing 200 instances in total. hyperparameters: • top_k_values = [10, 20, 50] • temperature_values = [0.2, 0.5, 0.7, 1.0] • top_p_values = [0.8, 0.9, 0.95]"
- Break condition: If the sampling temperature is too high, the LLM may generate incoherent schedules; if too low, it may get stuck in local optima.

## Foundational Learning

- Concept: Natural language representation of structured data
  - Why needed here: The LLM needs to understand JSSP problems expressed in natural language rather than matrix format
  - Quick check question: Can you explain how the job-centric and machine-centric approaches differ in representing the same JSSP instance?

- Concept: Supervised learning with generated labels
  - Why needed here: The paper uses OR-Tools to generate optimal solutions for training data, requiring understanding of how to create labeled datasets
  - Quick check question: What are the advantages and limitations of using OR-Tools with a 300-second time limit for generating training labels?

- Concept: Low-rank adaptation (LoRA)
  - Why needed here: LoRA enables efficient fine-tuning of large LLMs for JSSP without full retraining
  - Quick check question: How does LoRA reduce the number of trainable parameters compared to full fine-tuning?

## Architecture Onboarding

- Component map: Data generation pipeline (OR-Tools solver → natural language converter → dataset) -> Model fine-tuning (Phi-3 LLM + LoRA adapter + training loop) -> Inference pipeline (Sampling configuration → LLM inference → solution parsing → validation) -> Evaluation (Gap calculation → comparison with other methods)

- Critical path: Data generation → Fine-tuning → Inference with sampling → Solution parsing → Performance evaluation

- Design tradeoffs:
  - LoRA rank (128) vs. model capacity: Higher rank may capture more scheduling patterns but increases computational cost
  - Sampling parameters (temperature=1.0, top_k=50, top_p=0.95) vs. solution quality: Higher temperature increases diversity but may reduce coherence
  - Natural language representation vs. traditional matrix format: Natural language enables LLM training but may introduce ambiguity

- Failure signatures:
  - If fine-tuning loss plateaus early: LoRA rank may be too low or learning rate too small
  - If inference produces invalid schedules: Natural language representation may be ambiguous or sampling parameters inappropriate
  - If performance doesn't improve with sampling: Sampling temperature may be too high or solution selection strategy suboptimal

- First 3 experiments:
  1. Test natural language conversion on small JSSP instances to verify representation accuracy
  2. Fine-tune on a small subset of data (1000 examples) to validate LoRA configuration
  3. Compare greedy vs. sampling inference on a validation set to determine optimal sampling parameters

## Open Questions the Paper Calls Out
- How does the performance of Phi-3 scale with larger JSSP instances (NJ > 10, NM > 10) compared to smaller ones?
- How do different sampling strategies impact the performance of Phi-3 in solving JSSP compared to the current sampling method?
- How does the interpretability of schedules generated by LLMs compare to those from traditional JSSP methods?

## Limitations
- The study focuses on small-scale JSSP instances (up to 20x20), limiting generalizability to industrial-sized problems
- The reliance on OR-Tools with a 300-second time limit for generating training labels may introduce suboptimal solutions
- The sampling approach requires multiple inference runs (s=10), increasing computational cost

## Confidence
- High Confidence: The LLM can effectively process natural language descriptions of JSSP problems and produce valid schedules when properly fine-tuned and sampled
- Medium Confidence: The proposed approach achieves comparable performance to other neural methods like GNNs and Pointer Networks
- Low Confidence: The scalability of this approach to larger JSSP instances and more complex scheduling scenarios remains unproven

## Next Checks
1. Evaluate the model on a separate test set of JSSP instances not used during training to verify generalization performance
2. Systematically test different combinations of top-k, temperature, and top-p values to determine the optimal sampling configuration for solution quality
3. Benchmark the LLM-based approach against commercial solvers (e.g., CPLEX, Gurobi) on the same problem instances to assess practical viability