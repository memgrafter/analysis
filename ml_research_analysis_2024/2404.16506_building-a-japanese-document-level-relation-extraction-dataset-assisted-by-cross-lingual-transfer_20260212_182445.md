---
ver: rpa2
title: Building a Japanese Document-Level Relation Extraction Dataset Assisted by
  Cross-Lingual Transfer
arxiv_id: '2404.16506'
source_url: https://arxiv.org/abs/2404.16506
tags:
- relation
- docre
- dataset
- japanese
- jacred
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces JacRED, the first benchmark for Japanese document-level
  relation extraction (DocRE). While English DocRE datasets exist, none are available
  for Japanese, and existing translation-based cross-lingual transfer methods fail
  due to linguistic differences between English and Japanese.
---

# Building a Japanese Document-Level Relation Extraction Dataset Assisted by Cross-Lingual Transfer

## Quick Facts
- arXiv ID: 2404.16506
- Source URL: https://arxiv.org/abs/2404.16506
- Authors: Youmi Ma; An Wang; Naoaki Okazaki
- Reference count: 0
- Primary result: Introduced JacRED, the first Japanese document-level relation extraction dataset, demonstrating that cross-lingual transfer via translation fails due to linguistic differences, but model-assisted human annotation reduces effort by ~50%.

## Executive Summary
This paper addresses the lack of Japanese document-level relation extraction (DocRE) benchmarks by constructing JacRED through a semi-automatic approach. The authors first transfer an English DocRE dataset to Japanese via machine translation, then use a model trained on this transferred data to assist human annotators. Quantitative analysis shows this approach reduces annotation effort by approximately 50% compared to knowledge base querying. Experiments reveal that existing DocRE models perform significantly worse on Japanese than on English, and cross-lingual transfer is limited, highlighting the challenges of Japanese and cross-lingual DocRE.

## Method Summary
The method involves translating the English Re-DocRED dataset to Japanese (Re-DocREDja), training DocRE models on this transferred data, and using model predictions to assist human annotation of Japanese Wikipedia articles. The annotation process includes entity mention correction and relation annotation with model recommendations. A reduced relation label set of 28 types (from 96) is used to improve annotation quality. The final dataset, JacRED, consists of 317 documents with 28 relation types covering over 88% of relation instances in the transferred data.

## Key Results
- JacRED is the first Japanese document-level relation extraction dataset with 317 documents
- Model-assisted annotation reduces human effort by approximately 50% compared to knowledge base querying
- Cross-lingual transfer from English to Japanese shows significant performance degradation due to topic shift and surface structure differences
- Existing DocRE models perform substantially worse on Japanese than on English datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translation-based transfer fails for DocRE due to structural and topical mismatches between translated and native Japanese documents.
- Mechanism: Direct translation preserves entity spans and relations but not document-level coherence or topical relevance, leading to poor model performance.
- Core assumption: Document-level coherence cannot be preserved through direct translation alone.
- Evidence anchors: Models trained on translated data suffer from low recalls; topic shift and surface structure gaps identified between Re-DocREDja and native Japanese documents.
- Break condition: If translated documents closely mimic native Japanese in both structure and topic.

### Mechanism 2
- Claim: Model-assisted human annotation reduces annotation effort by approximately 50% compared to knowledge base querying.
- Mechanism: DocRE model predictions provide a scaffold for human annotators, reducing the number of edit steps needed.
- Core assumption: Model predictions, even if imperfect, meaningfully reduce human effort when used as a starting point.
- Evidence anchors: Quantitative analysis shows 7,805 steps needed for knowledge base alignment vs. 4,230 steps with cross-lingual transfer.
- Break condition: If model predictions are too poor in quality (e.g., recall < 20%).

### Mechanism 3
- Claim: Reducing the relation label set from 96 to 28 types improves annotation quality and reduces annotator cognitive load.
- Mechanism: Merging symmetric relation types and removing rare relations simplifies the task for annotators.
- Core assumption: A smaller, well-curated label set can capture the majority of relation instances while being easier for humans to manage.
- Evidence anchors: The reduced label set covers over 88% of relation instances in Re-DocREDja.
- Break condition: If the reduced label set fails to cover important relation types in Japanese documents.

## Foundational Learning

- Concept: Cross-lingual transfer in NLP
  - Why needed here: Understanding limitations and opportunities of transferring structured prediction tasks across languages is central to this work.
  - Quick check question: What are the key differences between sentence-level and document-level relation extraction that affect cross-lingual transfer?

- Concept: Document-level relation extraction (DocRE)
  - Why needed here: The task definition and challenges of DocRE are the core focus of the paper.
  - Quick check question: How does DocRE differ from sentence-level relation extraction in terms of context and complexity?

- Concept: Edit-based annotation schemes
  - Why needed here: The semi-automatic annotation approach relies on annotators editing model predictions rather than annotating from scratch.
  - Quick check question: What are the advantages and disadvantages of using model recommendations in human annotation workflows?

## Architecture Onboarding

- Component map: Translation + Post-processing → Model training (on transferred data) → Model-assisted annotation → Final dataset
- Critical path: Translation → Post-processing (case marker detachment) → Model training → Annotation interface with model predictions
- Design tradeoffs: Full automation vs. human effort; label set size vs. coverage; translation quality vs. native document quality
- Failure signatures: Low recall in transferred data; high edit distance between model predictions and final annotations; poor cross-lingual transfer performance
- First 3 experiments:
  1. Train a DocRE model on Re-DocREDja and evaluate on raw Japanese Wikipedia to measure recall degradation.
  2. Compare human edit steps when starting from model predictions vs. knowledge base queries on a sampled subset.
  3. Evaluate cross-lingual transfer performance (en→ja and ja→en) using mBERT encoder to quantify transfer gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of cross-lingual DocRE models vary when transferring between languages with different script systems (e.g., English to Japanese vs. English to Chinese)?
- Basis in paper: [inferred] The paper discusses cross-lingual transfer limitations for Japanese DocRE due to linguistic distance, suggesting this may vary across language pairs.
- Why unresolved: The paper only evaluates English-Japanese transfer, not comparing different language pairs systematically.
- What evidence would resolve it: Comparative experiments measuring cross-lingual performance across multiple language pairs with varying linguistic distances.

### Open Question 2
- Question: What is the optimal balance between machine recommendations and human annotations for DocRE dataset construction across different languages and domains?
- Basis in paper: [explicit] The paper demonstrates that machine recommendations from cross-lingual transfer reduce human edit steps by ~50% compared to knowledge base queries, but doesn't explore optimal ratios.
- Why unresolved: The paper only compares two recommendation methods without exploring the full spectrum of machine-human collaboration strategies.
- What evidence would resolve it: Systematic ablation studies varying the proportion of machine vs. human contributions across multiple datasets and languages.

### Open Question 3
- Question: How does the effectiveness of in-context learning for DocRE scale with document length and complexity across different language pairs?
- Basis in paper: [explicit] The paper shows poor in-context learning performance for Japanese DocRE with LLMs, noting this aligns with previous findings about document length limitations.
- Why unresolved: The paper doesn't explore whether performance degrades differently across languages or document types.
- What evidence would resolve it: Controlled experiments varying document length and complexity while comparing cross-lingual in-context learning performance.

## Limitations

- The cross-lingual transfer failure mechanism relies heavily on qualitative observations with limited quantitative validation
- The 50% reduction in annotation effort is based on internal step-counting metrics that may not generalize
- The reduced label set of 28 relations may not adequately represent Japanese-specific relation types not present in the English source

## Confidence

- **High confidence**: The fundamental challenge of lacking Japanese DocRE resources and the necessity of semi-automatic annotation approaches
- **Medium confidence**: The 50% reduction in annotation effort - while supported by internal metrics, generalizability remains unclear
- **Low confidence**: The complete failure of cross-lingual transfer via translation - qualitative analysis provides plausible mechanisms but lacks systematic ablation studies

## Next Checks

1. **Quantitative surface structure analysis**: Conduct systematic comparison of dependency tree depths and topic distributions between Re-DocREDja and native Japanese Wikipedia articles using established NLP metrics.

2. **Cross-lingual transfer ablation study**: Isolate impact of translation quality vs. topical mismatch by evaluating model performance on Re-DocREDja with manually corrected translations and training on native English data tested on translated Japanese documents.

3. **Annotation effort reproducibility**: Replicate the annotation step-counting experiment with different starting points (random relations, different knowledge bases, or empty slate) to verify whether the 50% reduction is specifically attributable to cross-lingual transfer.