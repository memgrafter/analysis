---
ver: rpa2
title: Privacy-Preserving Language Model Inference with Instance Obfuscation
arxiv_id: '2402.08227'
source_url: https://arxiv.org/abs/2402.08227
tags:
- privacy
- decision
- input
- inference
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of protecting both input data
  and model inference decisions in Language Model as a Service (LMaaS) scenarios.
  The authors propose Instance-Obfuscated Inference (IOI), a method that uses instance
  obfuscation and decision resolution to achieve decision privacy without compromising
  the black-box nature of LMaaS.
---

# Privacy-Preserving Language Model Inference with Instance Obfuscation

## Quick Facts
- arXiv ID: 2402.08227
- Source URL: https://arxiv.org/abs/2402.08227
- Authors: Yixiang Yao; Fei Wang; Srivatsan Ravi; Muhao Chen
- Reference count: 21
- Primary result: Proposes Instance-Obfuscated Inference (IOI) that achieves decision privacy in LMaaS by concatenating inputs with obfuscators and using privacy-preserving representation generation, showing strong privacy protection while maintaining task performance on four benchmark datasets.

## Executive Summary
This paper addresses the critical challenge of protecting both input data and model inference decisions in Language Model as a Service (LMaaS) scenarios. The authors propose Instance-Obfuscated Inference (IOI), a novel method that uses instance obfuscation and decision resolution to achieve decision privacy without compromising the black-box nature of LMaaS. By concatenating input instances with carefully selected obfuscators and using a privacy-preserving representation generation module, IOI makes the model's output distribution appear random to adversaries while still allowing accurate decision recovery for legitimate users.

The core innovation lies in the decision resolution mechanism that recovers the true decision from multiple obfuscated instances by exploiting the divergence between decision distributions. The method is evaluated on four benchmark datasets (SST-2, SST-5, MRPC, QNLI) and demonstrates a strong balance between privacy protection and task performance. The authors also introduce a unified metric Φ to quantify this balance, showing that IOI can achieve both high task accuracy and strong decision privacy simultaneously.

## Method Summary
IOI operates by first creating an obfuscator pool from normal sentences with high-confidence predictions. For each input instance, it selects a balanced group of obfuscators (ensuring uniform label distribution) and concatenates them with the input to create obfuscated instances. These are transformed through a privacy-preserving representation generation (PPRG) module into privacy-preserving representations, which are sent to the LMaaS in arbitrary order. The decision resolution method then recovers the true decision by analyzing the confidence differences between the obfuscated instance and its obfuscator, exploiting the fact that blending an instance with an obfuscator shifts the decision distribution toward the instance's true label.

## Key Results
- Achieves strong decision privacy protection while maintaining good task performance across SST-2, SST-5, MRPC, and QNLI datasets
- Demonstrates effective balance between obfuscation strength and task performance using unified metric Φ
- Shows that balancing (group size control) effectively mitigates unbalanced obfuscator distributions
- Validates compatibility and performance of different PPRG methods with IOI

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decision privacy is achieved by making the model's output distribution close to uniform for adversaries.
- Mechanism: Instance obfuscation concatenates the input with obfuscators, steering the model's decision distribution. The true decision is recoverable only with knowledge of the resolution method and parameters.
- Core assumption: The privacy-preserving representation generation (PPRG) module produces irreversible, distinct representations for identical inputs.
- Evidence anchors:
  - [abstract]: "The core idea is to concatenate input instances with obfuscators and use a privacy-preserving representation generation module to transform them into privacy-preserving representations."
  - [section]: "After applying PPRG, the representations from multiple xs should be sent to the PLM in arbitrary order."
  - [corpus]: Weak evidence - only tangentially related papers on privacy-preserving AI governance.

### Mechanism 2
- Claim: Decision resolution recovers the true decision from multiple obfuscated instances.
- Mechanism: The decision label of the input is determined by the divergence between the decision distribution of the obfuscated instance and the obfuscator.
- Core assumption: The obfuscator pool contains instances with uniformly distributed labels.
- Evidence anchors:
  - [section]: "Specifically, if x's label is ck, blending it with b shifts the confidence of [b; x]'s decision distribution towards ck regardless of the b's label."
  - [section]: "The strategy to separate x's result y from y's is based on the divergence between the decision distribution of [b; x] and b."
  - [corpus]: Weak evidence - no directly related papers on decision resolution in privacy-preserving inference.

### Mechanism 3
- Claim: Balancing mitigates the issue of unbalanced obfuscator distributions.
- Mechanism: Each real instance is paired with at least one unit group of obfuscators, where a unit group contains obfuscators with uniformly distributed labels.
- Core assumption: The group size (number of unit groups) is sufficiently large to ensure a balanced distribution of obfuscator labels.
- Evidence anchors:
  - [section]: "Balancing, as a solution, is employed to mitigate this issue. Specifically, each real instance x is paired with at least one unit group of obfuscators."
  - [section]: "Using balancing in the previous example, x concatenates with all three obfuscators and results in three obfuscated instances [b1; x], [b2; x] and [b3; x]."
  - [corpus]: Weak evidence - no directly related papers on balancing in privacy-preserving inference.

## Foundational Learning

- Concept: Privacy-preserving representation generation (PPRG)
  - Why needed here: PPRG transforms the input text sequence into privacy-preserving representations, ensuring that the adversary cannot reverse-engineer the raw input.
  - Quick check question: What are the two key requirements for a qualified input privacy method according to the paper?

- Concept: Instance obfuscation
  - Why needed here: Instance obfuscation conceals the true instance by concatenating it with obfuscators, making the model's decision distribution appear random to adversaries.
  - Quick check question: How does the choice of obfuscators affect the model's decision distribution?

- Concept: Decision resolution
  - Why needed here: Decision resolution recovers the true decision from multiple obfuscated instances by exploiting the divergence between the decision distribution of the obfuscated instance and the obfuscator.
  - Quick check question: What is the intuition behind using the divergence between the decision distribution of the obfuscated instance and the obfuscator for decision resolution?

## Architecture Onboarding

- Component map: Instance Obfuscation -> Privacy-Preserving Representation Generation (PPRG) -> Language Model as a Service (LMaaS) -> Decision Resolution

- Critical path: Instance Obfuscation → PPRG → LMaaS → Decision Resolution

- Design tradeoffs:
  - Privacy vs. Performance: Stronger privacy (larger group size, more obfuscators) may lead to lower task performance.
  - Communication Cost: More obfuscated instances and obfuscators result in higher communication overhead.
  - Computation Cost: The decision resolution process involves trivial matrix operations, but the number of requests to the LMaaS increases with the number of obfuscated instances.

- Failure signatures:
  - Low task performance: Indicates that the instance obfuscation or decision resolution may not be effective.
  - Inability to recover the true decision: Suggests that the decision resolution method is not robust or the obfuscator pool is not diverse enough.
  - High communication overhead: Implies that the number of obfuscated instances and obfuscators may be too large.

- First 3 experiments:
  1. Evaluate the impact of obfuscator selection on task performance and privacy protection.
  2. Study the effect of balancing (group size) on decision resolution accuracy and privacy strength.
  3. Assess the compatibility and performance of different PPRG methods with IOI.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the privacy-preserving representation generation (PPRG) method affect the balance between task performance and decision privacy?
- Basis in paper: [explicit] The paper states that the privacy strength and ability to prevent attacks of the input privacy method are already comprehensively studied in its corresponding papers and follow-up works, but the performance of plugging it with IOI is not well-studied.
- Why unresolved: The paper does not provide a detailed analysis of how different PPRG methods impact the balance between task performance and decision privacy.
- What evidence would resolve it: Experiments comparing the performance of IOI with different PPRG methods, such as noise injection, differential privacy, and adversarial training, would provide insights into the impact of PPRG on the balance between task performance and decision privacy.

### Open Question 2
- Question: How does the length expansion of obfuscators impact the decision privacy and task performance in IOI?
- Basis in paper: [explicit] The paper mentions that the length expansion of obfuscators can amplify their impact on the PLM's decision, but it does not provide a detailed analysis of how this affects decision privacy and task performance.
- Why unresolved: The paper only provides a preliminary experiment on the length expansion of obfuscators and does not explore its impact on decision privacy and task performance in depth.
- What evidence would resolve it: Experiments varying the length expansion factor k and analyzing its impact on decision privacy and task performance would provide insights into the trade-off between privacy and utility.

### Open Question 3
- Question: How does the choice of obfuscator pool affect the decision privacy and task performance in IOI?
- Basis in paper: [explicit] The paper mentions that obfuscators can be selected from any arbitrary corpus, but it does not explore the impact of different obfuscator pools on decision privacy and task performance.
- Why unresolved: The paper does not provide a detailed analysis of how the choice of obfuscator pool affects the balance between decision privacy and task performance.
- What evidence would resolve it: Experiments comparing the performance of IOI with different obfuscator pools, such as using instances from the same dataset or different datasets, would provide insights into the impact of obfuscator pool choice on decision privacy and task performance.

## Limitations

- Methodological Limitations: Evaluation relies on benchmark datasets that may not represent real-world LMaaS scenarios; obfuscator pool creation process is vaguely described without clear selection criteria or diversity requirements.
- Technical Assumptions: Privacy guarantees depend on fragile assumptions about PPRG irreversibility and uniform obfuscator label distribution that aren't rigorously proven or tested against adaptive adversaries.
- Performance-Privacy Tradeoff: While unified metric Φ balances privacy and performance, practical implications and acceptable thresholds aren't sufficiently analyzed, and scalability to larger, more complex tasks remains unclear.

## Confidence

**High Confidence Claims**:
- The general framework of instance obfuscation + decision resolution is technically sound and internally consistent
- Experimental methodology (dataset selection, baseline comparisons) is appropriate for the claims being made
- The balancing mechanism (group size control) is a valid approach to address unbalanced obfuscator distributions

**Medium Confidence Claims**:
- The privacy guarantees against passive adversaries are reasonably supported by the experimental results
- The unified metric Φ provides a useful heuristic for balancing privacy and performance
- The PPRG module compatibility claim (with PP-BERT and similar methods) is plausible based on the described mechanism

**Low Confidence Claims**:
- Strong claims about resistance to adaptive or active adversaries are not adequately supported
- Generalization to diverse real-world LMaaS scenarios beyond the tested benchmarks
- The practical utility of the approach given communication and computation overhead

## Next Checks

**Check 1: Obfuscator Pool Robustness** - Systematically evaluate how different obfuscator selection strategies (corpus sources, sentence length, semantic similarity) affect both privacy protection and task performance. Test with non-uniform label distributions to validate the balancing mechanism's effectiveness.

**Check 2: Adaptive Adversary Simulation** - Design experiments where an adversary attempts to learn the obfuscation pattern over multiple queries. Measure information leakage through statistical analysis of obfuscated instances, timing patterns, and decision distribution analysis.

**Check 3: Scalability and Resource Analysis** - Evaluate the system on larger, more complex datasets (e.g., multi-class classification with 100+ classes) and measure the communication/computation overhead. Determine practical limits on group size and obfuscator count before performance degradation becomes unacceptable.