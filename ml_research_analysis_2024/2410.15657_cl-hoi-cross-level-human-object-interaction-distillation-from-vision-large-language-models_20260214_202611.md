---
ver: rpa2
title: 'CL-HOI: Cross-Level Human-Object Interaction Distillation from Vision Large
  Language Models'
arxiv_id: '2410.15657'
source_url: https://arxiv.org/abs/2410.15657
tags:
- interaction
- context
- features
- distillation
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes CL-HOI, a cross-level distillation framework
  that transfers image-level human-object interaction (HOI) recognition and reasoning
  capabilities from Vision Large Language Models (VLLMs) to instance-level HOI detection
  without manual annotations. The method consists of two stages: context distillation
  using a Visual Linguistic Translator (VLT) to convert visual features into linguistic
  context, and interaction distillation using an Interaction Cognition Network (ICN)
  to reason about spatial, visual, and context relations.'
---

# CL-HOI: Cross-Level Human-Object Interaction Distillation from Vision Large Language Models

## Quick Facts
- **arXiv ID**: 2410.15657
- **Source URL**: https://arxiv.org/abs/2410.15657
- **Reference count**: 25
- **Primary result**: Achieves 17.5% mAP on HICO-DET and 36.63% Role AP on V-COCO, surpassing existing weakly supervised methods and VLLM supervised methods

## Executive Summary
CL-HOI introduces a cross-level distillation framework that transfers image-level human-object interaction (HOI) recognition capabilities from Vision Large Language Models (VLLMs) to instance-level HOI detection without manual annotations. The method leverages VLLM outputs as supervision, using a two-stage distillation process: context distillation via a Visual Linguistic Translator (VLT) and interaction distillation via an Interaction Cognition Network (ICN). By converting visual features into linguistic context and reasoning about spatial, visual, and contextual relations, CL-HOI achieves competitive performance on standard benchmarks. The approach addresses the annotation bottleneck in HOI detection while demonstrating the potential of VLLM-guided learning.

## Method Summary
CL-HOI operates through a two-stage cross-level distillation process. In the first stage, a Visual Linguistic Translator (VLT) converts visual features into linguistic context, aligning VLLM's image-level HOI understanding with instance-level features. In the second stage, an Interaction Cognition Network (ICN) reasons about spatial, visual, and contextual relations to predict HOI triplets at the instance level. Contrastive distillation losses are designed to bridge the gap between image-level and instance-level HOI detection. The framework is trained without manual annotations, relying solely on VLLM outputs as supervision, and is evaluated on HICO-DET and V-COCO datasets.

## Key Results
- Achieves 17.5% mAP on HICO-DET, outperforming existing weakly supervised methods.
- Attains 36.63% Role AP on V-COCO, surpassing VLLM supervised methods.
- Demonstrates the feasibility of transferring image-level HOI reasoning from VLLMs to instance-level detection without manual annotations.

## Why This Works (Mechanism)
CL-HOI works by leveraging the rich HOI understanding embedded in VLLMs and distilling it to instance-level detectors through contrastive learning. The Visual Linguistic Translator (VLT) bridges the modality gap by converting visual features into linguistic context, enabling the Interaction Cognition Network (ICN) to reason about relations at the instance level. Contrastive distillation losses ensure that the instance-level predictions align with the image-level VLLM outputs, effectively transferring knowledge across levels without requiring manual annotations.

## Foundational Learning
- **Vision Large Language Models (VLLMs)**: Pre-trained models that integrate visual and linguistic understanding, providing image-level HOI reasoning.
  - *Why needed*: Serve as a source of rich, image-level HOI supervision without manual annotations.
  - *Quick check*: Validate that VLLM outputs are reliable and relevant to the target HOI detection task.

- **Contrastive Distillation**: A learning strategy that aligns features or predictions from different levels or modalities by maximizing agreement and minimizing divergence.
  - *Why needed*: Enables the transfer of image-level HOI knowledge to instance-level detection.
  - *Quick check*: Ensure that contrastive losses effectively bridge the modality and level gaps.

- **Visual Linguistic Translator (VLT)**: A module that converts visual features into linguistic context, aligning visual and textual representations.
  - *Why needed*: Facilitates the integration of VLLM outputs with instance-level visual features.
  - *Quick check*: Verify that VLT outputs are semantically meaningful and aligned with VLLM context.

- **Interaction Cognition Network (ICN)**: A reasoning module that integrates spatial, visual, and contextual relations to predict HOI triplets at the instance level.
  - *Why needed*: Enables fine-grained HOI detection by reasoning about multiple relation types.
  - *Quick check*: Confirm that ICN predictions are consistent with both visual evidence and VLLM guidance.

## Architecture Onboarding

**Component Map**
VLLM outputs -> VLT -> ICN -> HOI detection predictions (image-level -> linguistic context -> instance-level)

**Critical Path**
VLT converts visual features to linguistic context; ICN reasons about relations using both visual and linguistic inputs; contrastive losses align instance-level predictions with image-level VLLM outputs.

**Design Tradeoffs**
- Leverages VLLM supervision to avoid manual annotations, but introduces dependence on VLLM quality and potential bias.
- Uses contrastive distillation to bridge modality and level gaps, but may require careful tuning of loss weights.
- Focuses on instance-level detection, but may not fully exploit the richness of image-level context.

**Failure Signatures**
- Poor VLLM output quality or misalignment with target domain degrades detection performance.
- Inadequate contrastive distillation leads to misalignment between image-level and instance-level predictions.
- Over-reliance on linguistic context may reduce robustness to visual ambiguity or domain shift.

**First Experiments**
1. Validate VLT's ability to generate semantically meaningful linguistic context from visual features.
2. Test ICN's reasoning performance with and without VLLM guidance.
3. Evaluate the impact of contrastive distillation loss weights on final detection accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- Moderate performance gains over weakly supervised baselines; lacks direct comparison with fully supervised methods.
- Reliance on VLLM outputs introduces potential biases and noise; robustness under varying VLLM quality is unclear.
- Scalability to more complex or diverse HOI datasets is not demonstrated.

## Confidence
- **High**: Framework design and validation on standard benchmarks are well-supported.
- **Medium**: Claims regarding superiority over weakly supervised methods are supported, but lack context about fully supervised baselines.
- **Low**: Assertions about robustness and scalability to new domains or datasets are not empirically validated.

## Next Checks
1. Conduct ablation studies to quantify the impact of VLLM output quality and domain alignment on final detection performance.
2. Evaluate CL-HOI on additional HOI datasets (e.g., HOI-A or OpenHOI) to test generalizability and robustness.
3. Compare performance against recent fully supervised HOI detection methods to contextualize the practical significance of the approach.