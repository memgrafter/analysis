---
ver: rpa2
title: 'From Random to Informed Data Selection: A Diversity-Based Approach to Optimize
  Human Annotation and Few-Shot Learning'
arxiv_id: '2401.13229'
source_url: https://arxiv.org/abs/2401.13229
tags:
- data
- random
- methods
- learning
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the challenge of efficiently building initial
  datasets for few-shot learning in NLP, particularly in low-resource and imbalanced
  settings. The authors propose an informed data selection architecture that leverages
  semantic similarity and clustering techniques to prioritize the most informative
  and diverse data points for human annotation.
---

# From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning

## Quick Facts
- arXiv ID: 2401.13229
- Source URL: https://arxiv.org/abs/2401.13229
- Authors: Alexandre Alcoforado; Thomas Palmeira Ferraz; Lucas Hideki Okamura; Israel Campos Fama; Arnold Moya Lavado; Bárbara Dias Bueno; Bruno Veloso; Anna Helena Reali Costa
- Reference count: 10
- One-line primary result: Informed data selection methods (RSS, OC, LLS) reduce overannotation and improve few-shot learning in imbalanced NLP datasets

## Executive Summary
This paper addresses the challenge of efficiently building initial datasets for few-shot learning in NLP, particularly in low-resource and imbalanced settings. The authors propose three informed data selection methods (Reverse Semantic Search, Ordered Clustering, and Limited Lexical Similarity) that leverage semantic similarity and clustering techniques to prioritize the most informative and diverse data points for human annotation. These methods aim to minimize redundancy and annotation costs while improving model performance compared to random sampling.

The core approach uses embedding-based similarity metrics to ensure diversity in selected samples, with RSS iteratively selecting the most dissimilar documents, OC selecting one document per cluster starting from largest clusters, and LLS filtering documents based on lexical similarity thresholds. Experiments on five text classification datasets with varying imbalance show consistent improvements in both overannotation rate (reducing excess annotations by up to half) and model accuracy/Macro-F1 score, especially in highly imbalanced scenarios.

## Method Summary
The paper proposes three informed data selection methods for few-shot learning in NLP: Reverse Semantic Search (RSS), Ordered Clustering (OC), and Limited Lexical Similarity (LLS). RSS calculates a similarity matrix between all document embeddings and iteratively selects the most dissimilar ones. OC uses hierarchical density-based clustering to group similar documents, orders clusters by size, and selects the most uncertain document from each cluster. LLS filters documents based on lexical similarity thresholds using metrics like BLEU score. These methods are compared against random sampling across five text classification datasets using two training approaches (fine-tuning XLM-Roberta-large or SetFit), with evaluation metrics including overannotation rate, accuracy, and Macro-F1 score.

## Key Results
- RSS consistently reduces overannotation rate compared to random sampling, especially in highly imbalanced datasets
- Model accuracy and Macro-F1 scores improve significantly with informed selection methods, particularly for nshots=8
- The effectiveness of methods varies by dataset characteristics, with RSS showing stronger performance on Portuguese datasets due to multilingual embedding model bias
- All three informed methods outperform random sampling in terms of both annotation efficiency and model performance metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RSS reduces overannotation by selecting semantically dissimilar documents
- Mechanism: Calculates similarity matrix and iteratively selects least similar documents to maximize information gain
- Core assumption: Semantic similarity correlates with annotation redundancy
- Evidence anchors: Abstract states method "minimizes quantity and maximizes diversity of data selected"; RSS selects "two documents with least similarity"
- Break condition: Embedding model fails to capture semantic nuances in specialized domains

### Mechanism 2
- Claim: OC ensures broad class coverage by selecting one document per cluster from largest clusters
- Mechanism: Hierarchical clustering with membership probabilities, ordered by cluster size
- Core assumption: Clusters reflect true class structure and large clusters provide better coverage
- Evidence anchors: OC "assigns membership probability to each document" and "orders clusters based on their size"
- Break condition: Too many clusters identified relative to target classes

### Mechanism 3
- Claim: LLS reduces redundancy by filtering documents with high lexical overlap
- Mechanism: Uses lexical similarity metrics (BLEU) with threshold β to filter similar documents
- Core assumption: Lexical similarity proxies semantic redundancy in short text classification
- Evidence anchors: LLS "discards document if g(di+1, di) > β and keeps it otherwise"
- Break condition: Threshold β set too high, discarding informative documents

## Foundational Learning

- Concept: Semantic similarity and embedding models
  - Why needed here: RSS and OC rely on computing semantic similarity between documents
  - Quick check question: How does paraphrase-multilingual-mpnet-base-v2 handle domain-specific vocabulary?

- Concept: Clustering algorithms and cluster membership probabilities
  - Why needed here: OC uses HDBSCAN and soft clustering with membership probabilities
  - Quick check question: What's the difference between hard and soft clustering, and why does OC use soft clustering?

- Concept: Lexical similarity metrics (BLEU, ROUGE)
  - Why needed here: LLS uses lexical similarity to filter redundant documents
  - Quick check question: What are strengths and weaknesses of BLEU for measuring lexical similarity in text classification?

## Architecture Onboarding

- Component map: Data ingestion -> embedding generation -> similarity/clustering computation -> informed selection -> annotation pipeline -> model training
- Critical path:
  - RSS: embedding -> similarity matrix -> iterative selection
  - OC: embedding -> clustering -> cluster ordering -> document selection
  - LLS: embedding -> lexical similarity filtering -> document selection
- Design tradeoffs:
  - RSS: High computational cost for similarity matrix but effective for imbalanced datasets
  - OC: Risk of over-splitting into too many clusters; requires clustering parameter tuning
  - LLS: Fast but threshold tuning critical; less effective for long or highly lexicalized documents
- Failure signatures:
  - RSS: Overannotation rate remains high, model performance plateaus
  - OC: Overannotation rate spikes with many classes; poor cluster coverage
  - LLS: Low dataset diversity, high redundancy in selected examples
- First 3 experiments:
  1. Run RSS, OC, and LLS on small, highly imbalanced dataset (BRNews) comparing overannotation rate and accuracy for nshots=8
  2. Vary BLEU threshold β in LLS and observe effect on overannotation and model performance
  3. Compare runtime and memory usage of RSS and OC as dataset size increases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we optimize the numerical threshold in Limited Lexical Similarity (LLS) for different specialized domains?
- Basis in paper: [explicit] Authors note LLS requires fine-tuning threshold that could be amplified in specialized domains with unusual vocabulary
- Why unresolved: Paper did not explore fine-tuning this threshold
- What evidence would resolve it: Systematic experiments varying LLS threshold across specialized domains comparing overannotation rates and model accuracy

### Open Question 2
- Question: Does combining clustering with RSS or LLS improve the consistency and effectiveness of the Ordered Clustering (OC) method?
- Basis in paper: [inferred] Authors observed OC failed when number of identified clusters was high, suggesting combining clustering with RSS or LLS
- Why unresolved: Paper only identified OC problem but didn't test hybrid approaches
- What evidence would resolve it: Experiments implementing hybrid methods combining clustering with RSS or LLS measuring overannotation rates and model accuracy

### Open Question 3
- Question: How does the language factor (English vs. Portuguese) affect performance of informed data selection methods?
- Basis in paper: [explicit] Authors noted multilingual embedding model trained on more English than Portuguese data, explaining why RSS promoted variety longer in Portuguese datasets
- Why unresolved: Authors observed difference but didn't conduct controlled experiments
- What evidence would resolve it: Controlled experiments using same datasets in both English and Portuguese comparing overannotation rates and model accuracy

## Limitations
- Methods primarily tested on text classification tasks; effectiveness for other NLP tasks unknown
- RSS computational complexity becomes prohibitive for very large datasets due to similarity matrix calculation
- Reliance on single embedding model may limit performance in specialized domains
- LLS threshold selection is dataset-dependent and requires careful tuning not fully addressed

## Confidence

**High Confidence**: Core claim that informed methods reduce overannotation vs. random sampling is well-supported by experimental results across five diverse datasets

**Medium Confidence**: Claim that methods consistently improve model performance in imbalanced settings is supported but shows variation across datasets

**Low Confidence**: Assertion that methods are universally applicable across all low-resource NLP tasks and domains requires further validation

## Next Checks
1. Test RSS, OC, and LLS on non-classification NLP tasks (named entity recognition, relation extraction) to assess generalizability
2. Compare informed selection methods using different embedding models to quantify impact of embedding quality
3. Evaluate computational complexity and memory requirements of RSS and OC on progressively larger datasets (100K, 1M documents)