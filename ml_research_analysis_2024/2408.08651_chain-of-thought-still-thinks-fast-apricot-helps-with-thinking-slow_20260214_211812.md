---
ver: rpa2
title: 'Chain of Thought Still Thinks Fast: APriCoT Helps with Thinking Slow'
arxiv_id: '2408.08651'
source_url: https://arxiv.org/abs/2408.08651
tags:
- choice
- answer
- apricot
- prompting
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines how base-rate probability (BRP) effects\u2014\
  biases from training data statistics\u2014influence large language models\u2019\
  \ answer choices on the MMLU benchmark. The authors show that BRP correlates with\
  \ model preferences, mirroring human test-taking strategies."
---

# Chain of Thought Still Thinks Fast: APriCoT Helps with Thinking Slow

## Quick Facts
- arXiv ID: 2408.08651
- Source URL: https://arxiv.org/abs/2408.08651
- Reference count: 7
- Primary result: Agnostically Primed Chain of Thought (APriCoT) improves LLM accuracy on MMLU from 38% to 43% by reducing base-rate probability bias

## Executive Summary
This paper examines how base-rate probability (BRP) effects—biases from training data statistics—influence large language models' answer choices on the MMLU benchmark. The authors show that BRP correlates with model preferences, mirroring human test-taking strategies. While combining counterfactual prompting with chain-of-thought (CoT) reasoning was intended to mitigate these biases, it unexpectedly amplified them, suggesting confirmation bias in CoT. To address this, the authors introduce Agnostically Primed Chain of Thought (APriCoT), which forces the model to reason about each answer option independently. APriCoT significantly reduces BRP influence and improves accuracy from 38% (baseline) to 43%, outperforming both standard counterfactual prompting and CoT.

## Method Summary
The study evaluates BRP effects on LLM answer choices using LLaMa 3.1 8B model on MMLU benchmark (14,042 questions across 57 subjects). The authors implement cloze and counterfactual prompting with and without CoT, measuring BRP using cloze prompts. They then introduce APriCoT, which forces the model to reason about each answer option independently before making a final judgment. Performance is measured by accuracy on MMLU and correlation between answer choice distribution and BRP.

## Key Results
- BRP correlates strongly with model answer preferences on MMLU, similar to human test-taking behavior
- CF+CoT unexpectedly amplifies BRP effects, showing "anti-confirmation bias" where higher BRP leads to lower likelihood of choice
- APriCoT reduces BRP influence and improves accuracy from 38% to 43%, outperforming both standard counterfactual prompting and CoT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: APriCoT reduces confirmation bias by forcing the model to evaluate each answer option independently with its own chain of thought.
- Mechanism: In standard CoT, the model may commit to a preferred answer early in the reasoning process and then rationalize it, amplifying base-rate probability (BRP) biases. APriCoT breaks this by priming the model to reason about each option separately, preventing early commitment and allowing a more balanced evaluation.
- Core assumption: Independent reasoning for each answer option leads to less biased final judgments compared to shared reasoning paths.
- Evidence anchors:
  - [abstract] "APriCoT forces the model to reason about each answer option independently"
  - [section] "APriCoT differs from CF+CoT in two important ways. First, CoT is elicited in combination with one of the candidate answer choices. In effect, the model is told to evaluate the correctness of one of the answer choices. Second, each post-CoT context is used to evaluate only the same answer choice with which the CoT was primed."
  - [corpus] Weak evidence; most related papers discuss CoT improvements but not APriCoT specifically.
- Break condition: If the model still latches onto a heuristic or if the priming instructions are misinterpreted, the bias mitigation could fail.

### Mechanism 2
- Claim: APriCoT improves accuracy by reducing the influence of base-rate probabilities on answer selection.
- Mechanism: By reasoning independently about each option, the model is less likely to be swayed by the intrinsic likelihood of answer choices (e.g., A being more probable than D). This leads to choices that are more aligned with the actual correctness of the answer rather than statistical regularities in the training data.
- Core assumption: Base-rate probabilities are a significant source of bias in model performance on multiple-choice tasks.
- Evidence anchors:
  - [abstract] "APriCoT effectively reduces the influence of base-rate probabilities while improving overall accuracy"
  - [section] "We find the preferred response is distributed nearly identically to the ground truth answer distribution and has better accuracy than either CF or CoT."
  - [corpus] No direct evidence; assumption based on paper's internal findings.
- Break condition: If the model's reasoning does not adequately separate from base-rate influences, accuracy gains may not materialize.

### Mechanism 3
- Claim: APriCoT approximates System-2 thinking by requiring the model to engage in more deliberative reasoning.
- Mechanism: By forcing the model to think step-by-step about each answer choice, APriCoT slows down the decision process, allowing for more thorough evaluation rather than quick, heuristic-based choices (System-1). This deliberative process is more likely to lead to correct answers.
- Core assumption: System-2-like processing improves decision-making quality in complex reasoning tasks.
- Evidence anchors:
  - [abstract] "Our results suggest that mitigating bias requires a slow thinking process which CoT alone may not provide"
  - [section] "We contend that a more deliberative, System-2-like, evaluation will eliminate the BRP effect."
  - [corpus] No direct evidence; assumption based on the dual-system theory referenced in the paper.
- Break condition: If the model's reasoning does not genuinely engage in System-2 processing or if the added complexity does not yield better outcomes, the mechanism fails.

## Foundational Learning

- Concept: Base-rate probability (BRP) effects
  - Why needed here: Understanding how BRP influences model behavior is crucial for grasping why APriCoT is necessary and how it mitigates bias.
  - Quick check question: What is the base-rate probability, and how does it affect the model's choice of answers in a multiple-choice setting?

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: CoT is the foundational technique that APriCoT builds upon; knowing how it works helps understand the improvements APriCoT offers.
  - Quick check question: How does standard CoT prompting work, and what are its limitations in reducing bias?

- Concept: Counterfactual prompting
  - Why needed here: CF prompting is used as a baseline in the study; understanding it is necessary to appreciate the enhancements APriCoT provides.
  - Quick check question: How does counterfactual prompting differ from standard cloze prompting, and what role does it play in mitigating BRP effects?

## Architecture Onboarding

- Component map: Cloze/CF prompt generation -> Answer option priming -> Independent CoT reasoning for each option -> Canary token probability measurement -> Final answer selection
- Critical path: The independent reasoning about each answer option, as this is where the bias mitigation occurs. Any delay or error in this phase directly impacts the effectiveness of APriCoT.
- Design tradeoffs: APriCoT increases computational cost by requiring multiple CoT generations (one per answer option) compared to standard CoT. However, this cost is justified by the reduction in bias and improvement in accuracy. There's also a tradeoff between the depth of reasoning and the risk of the model still falling into heuristic traps.
- Failure signatures: If the model consistently selects the same answer option regardless of the prompt, or if the accuracy does not improve over standard CoT, this indicates that APriCoT is not effectively mitigating bias. Another failure mode is if the model's reasoning becomes incoherent when forced to evaluate each option independently.
- First 3 experiments:
  1. Measure the base-rate probabilities for answer choices in a new dataset to confirm BRP effects.
  2. Implement APriCoT and compare its performance to standard CoT on a subset of questions to verify bias reduction.
  3. Analyze the correlation between BRP and answer selection before and after applying APriCoT to quantify the mitigation effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does CF+CoT exhibit negative correlations between BRP and answer choice, termed "anti-confirmation bias"?
- Basis in paper: [explicit] The paper notes this surprising phenomenon where higher BRP preference leads to lower likelihood of choice, with no mechanistic explanation offered.
- Why unresolved: The authors state they presently offer no explanation for this counterintuitive result despite strong statistical evidence of its presence.
- What evidence would resolve it: Systematic experiments varying BRP magnitudes, answer choice presentation order, and question types to identify what drives the negative correlation direction.

### Open Question 2
- Question: Does APriCoT's effectiveness generalize across different model families and sizes?
- Basis in paper: [inferred] The authors explicitly limit their experiments to LLaMa 3.1 8B and cannot make strong claims about generalization to other models.
- Why unresolved: The study only tested one specific model (LLaMa 3.1 8B) due to resource constraints, leaving uncertainty about cross-model applicability.
- What evidence would resolve it: Replicating the APriCoT experiments across diverse model architectures (GPT, Claude, Mistral) and parameter scales (7B, 70B, 400B+).

### Open Question 3
- Question: What is the mechanistic explanation for the dependence of CF overt behavior on cloze BRPs versus the lack of correlation between CF overt behavior and CF BRPs?
- Basis in paper: [explicit] The authors note this theoretical question, observing that cloze BRP predicts CF behavior while CF BRP does not.
- Why unresolved: The authors identify this as an open theoretical question but do not provide a mechanistic account for the differential predictive power.
- What evidence would resolve it: Detailed ablation studies isolating the components of CF prompting and measuring their individual contributions to BRP effects, potentially through controlled perturbations of prompt structure.

## Limitations

- The study only tested APriCoT on LLaMa 3.1 8B, limiting generalizability to other model architectures and sizes
- The mechanism by which APriCoT reduces BRP effects is primarily supported by correlation analysis rather than causal evidence
- The connection between APriCoT and genuine System-2 thinking is asserted but not rigorously tested

## Confidence

**High Confidence**: The observation that BRP correlates with model preferences on MMLU is well-supported by the empirical data presented. The finding that standard CoT amplifies BRP effects is also robust based on the reported results.

**Medium Confidence**: The claim that APriCoT reduces BRP influence is supported by the reported improvements in accuracy and correlation metrics, but the mechanism by which this occurs could benefit from further investigation.

**Low Confidence**: The claim that APriCoT approximates System-2 thinking and that this deliberative process is the primary driver of accuracy improvements is the weakest claim. The connection to dual-system theory is mentioned but not rigorously tested.

## Next Checks

1. **Mechanism Validation Experiment**: Conduct an ablation study where the independent reasoning component of APriCoT is selectively disabled while maintaining other aspects of the methodology. Compare the resulting accuracy and BRP correlation to determine whether independent reasoning is indeed the critical factor driving improvements.

2. **Generalization Test**: Apply APriCoT to a different LLM architecture (e.g., GPT-4 or Claude) and a different benchmark dataset (e.g., RACE or ARC) to assess whether the reported improvements generalize beyond the specific conditions tested in the paper.

3. **Counterfactual Reasoning Analysis**: Analyze the actual CoT reasoning chains generated by APriCoT to determine whether they demonstrate genuine deliberative processing or whether they simply reproduce the same heuristics in a more verbose format. This could involve qualitative coding of reasoning chains or automated analysis of reasoning patterns.