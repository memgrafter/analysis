---
ver: rpa2
title: 'WildFeedback: Aligning LLMs With In-situ User Interactions And Feedback'
arxiv_id: '2408.15549'
source_url: https://arxiv.org/abs/2408.15549
tags:
- user
- feedback
- response
- preferences
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WildFeedback addresses the challenge of aligning large language
  models with human preferences by leveraging in-situ user feedback from real conversations.
  It automatically identifies user satisfaction or dissatisfaction signals, constructs
  preference datasets from these interactions, and evaluates model alignment using
  checklist-guided methods.
---

# WildFeedback: Aligning LLMs With In-situ User Interactions And Feedback

## Quick Facts
- **arXiv ID:** 2408.15549
- **Source URL:** https://arxiv.org/abs/2408.15549
- **Authors:** Taiwei Shi; Zhuoer Wang; Longqi Yang; Ying-Chun Lin; Zexue He; Mengting Wan; Pei Zhou; Sujay Jauhar; Sihao Chen; Shan Xia; Hongfei Zhang; Jieyu Zhao; Xiaofeng Xu; Xia Song; Jennifer Neville
- **Reference count:** 34
- **Primary result:** WildFeedback leverages real user feedback from conversations to create preference datasets that improve LLM alignment with human preferences.

## Executive Summary
WildFeedback addresses the challenge of aligning large language models with human preferences by leveraging in-situ user feedback from real conversations. The framework automatically identifies user satisfaction or dissatisfaction signals, constructs preference datasets from these interactions, and evaluates model alignment using checklist-guided methods. By using actual user feedback rather than synthetic data or human annotations, WildFeedback produces preference data from 20,281 user-LLM conversation samples and demonstrates that models fine-tuned on this dataset significantly outperform baselines on standard benchmarks while showing stronger alignment with real user preferences.

## Method Summary
WildFeedback identifies user satisfaction (SAT) and dissatisfaction (DSAT) signals in multi-turn conversations using GPT-4 with predefined rubrics. It constructs preference datasets by extracting prompts, summarizing user preferences, and generating preferred/dispreferred responses. The framework then fine-tunes LLMs using supervised fine-tuning followed by direct preference optimization. Model alignment is evaluated using traditional benchmarks (AlpacaEval 2, Arena-Hard, MT-Bench) and a novel checklist-guided evaluation that provides instance-level user preference checklists to GPT-4 for more accurate assessment.

## Key Results
- WildFeedback produces 20,281 preference pairs from real user conversations, capturing authentic user preferences
- Models trained on WildFeedback significantly outperform baselines on standard benchmarks and demonstrate stronger alignment with real user preferences
- Checklist-guided evaluation achieves 57.14% human agreement, comparable to human-human agreement of 63.27%, validating its effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's ability to classify user satisfaction/dissatisfaction signals in conversations enables the creation of preference datasets that better reflect real user preferences compared to human-annotated datasets.
- Mechanism: By applying satisfaction estimation techniques (SAT/DSAT rubrics) to identify user feedback signals in multi-turn conversations, the framework can automatically extract instances where users express preferences through explicit feedback like "thank you" or implicit feedback like requesting revisions.
- Core assumption: GPT-4's classification of SAT/DSAT signals shows substantial agreement with human annotations (Cohen's Kappa of 0.68 for SAT and 0.50 for DSAT), making it reliable enough to identify genuine user preference signals.
- Evidence anchors:
  - [abstract] "WILD FEEDBACK addresses the scalability, subjectivity, and bias challenges that plague existing approaches"
  - [section] "GPT-4 demonstrates strong performance in classifying SAT (satisfaction) signals, with high accuracy at 91.7% and balanced precision and recall, both around 73%"
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.536, average citations=0.0." (Weak corpus evidence - limited related work on this specific mechanism)
- Break condition: If GPT-4's classification performance degrades significantly or if the identified feedback signals don't actually represent user preferences, the preference dataset would become unreliable.

### Mechanism 2
- Claim: The checklist-guided evaluation framework provides more accurate assessment of model alignment with real user preferences compared to traditional LLM-as-a-judge benchmarks.
- Mechanism: By providing instance-level checklists derived from extracted user preferences, the evaluation process guides GPT-4 to assess responses based on actual user expectations rather than generic quality criteria or model biases toward longer responses.
- Core assumption: GPT-4 can effectively use user-provided preference checklists to guide its evaluation, and this approach correlates better with human judgment than standard LLM-as-a-judge methods.
- Evidence anchors:
  - [abstract] "Our experiments demonstrate that LLMs fine-tuned on WILD FEEDBACK dataset exhibit significantly improved alignment with user preferences, as evidenced by both traditional benchmarks and our proposed checklist-guided evaluation"
  - [section] "GPT-4 achieves an human agreement of 57.14%, similar to the human-human agreement of 63.27%" for checklist-guided evaluation
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.536, average citations=0.0." (Weak corpus evidence - limited work on this specific evaluation mechanism)
- Break condition: If the preference checklists don't accurately capture user preferences or if GPT-4's evaluation remains biased despite the checklists, the evaluation framework would fail to provide accurate alignment assessment.

### Mechanism 3
- Claim: Models trained on WILD FEEDBACK show stronger alignment with real user preferences than models trained on traditional preference datasets like ULTRAFEEDBACK.
- Mechanism: By using actual user feedback from real conversations rather than synthetic data or human annotations, the training data better captures the diversity and nuance of real user preferences, leading to improved model alignment.
- Core assumption: Real user feedback in natural conversations provides more authentic preference signals than synthetic or human-annotated data, and models can effectively learn from this data through standard fine-tuning techniques.
- Evidence anchors:
  - [abstract] "models fine-tuned on WILD FEEDBACK significantly outperform baselines on standard benchmarks and demonstrate stronger alignment with real user preferences"
  - [section] "Models trained on WILD FEEDBACK also consistently outperform those on ULTRAFEEDBACK" across multiple benchmarks
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.536, average citations=0.0." (Weak corpus evidence - limited comparative studies)
- Break condition: If the preference signals in the training data are too noisy or if the models overfit to specific types of feedback rather than learning general alignment principles.

## Foundational Learning

- Concept: SAT/DSAT classification and rubric design
  - Why needed here: To identify user feedback signals in conversations that indicate satisfaction or dissatisfaction with model responses
  - Quick check question: What are the key differences between SAT and DSAT rubrics, and how do they help identify user preferences?

- Concept: Preference data construction from user feedback
  - Why needed here: To transform identified feedback signals into structured preference pairs (preferred/dispreferred responses) for model training
  - Quick check question: How does the framework handle both explicit feedback (like "thank you") and implicit feedback (like requesting revisions) when constructing preference data?

- Concept: Checklist-guided evaluation methodology
  - Why needed here: To provide a more accurate assessment of model alignment with real user preferences compared to traditional benchmarks
  - Quick check question: How does providing user preferences as checklists to GPT-4 improve the evaluation compared to standard LLM-as-a-judge approaches?

## Architecture Onboarding

- Component map: Feedback Signal Identification -> Preference Data Construction -> Model Training -> Checklist-guided Evaluation
- Critical path: User conversation → SAT/DSAT classification → Preference data construction → Model training → Checklist-guided evaluation
- Design tradeoffs:
  - Using GPT-4 for feedback classification vs. human annotation: Tradeoff between scalability and potential classification errors
  - GPT-4 vs. policy models for response generation: Tradeoff between alignment quality and model steerable-ness
  - Checklist-guided evaluation vs. standard benchmarks: Tradeoff between accuracy and computational cost
- Failure signatures:
  - Low Cohen's Kappa in SAT/DSAT classification indicates feedback signal identification problems
  - Poor performance on checklist-guided evaluation despite good benchmark scores indicates evaluation methodology issues
  - Models collapsing during training suggests hyperparameter tuning problems
- First 3 experiments:
  1. Test GPT-4's SAT/DSAT classification performance on a small sample of conversations with human annotations
  2. Generate preference data for a small subset and manually verify the quality of preferred/dispreferred responses
  3. Run checklist-guided evaluation on a small test set to verify it correlates with human judgment before full-scale implementation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the WildFeedback framework to different types of user feedback signals beyond SAT/DSAT rubrics?
- Basis in paper: [inferred] The paper mentions that user feedback can be explicit or implicit, but only focuses on SAT/DSAT signals for feedback signal identification.
- Why unresolved: The paper does not explore the effectiveness of the framework with other types of feedback signals or compare its performance against different feedback signal identification methods.
- What evidence would resolve it: Experiments comparing WildFeedback's performance using different feedback signal identification methods, or demonstrating its effectiveness with a broader range of feedback signal types.

### Open Question 2
- Question: To what extent does the checklist-guided evaluation framework mitigate biases in LLM-as-judge evaluations compared to traditional methods?
- Basis in paper: [explicit] The paper highlights that existing LLM-as-judge methods are biased towards longer responses or responses generated by themselves, and proposes checklist-guided evaluation as a solution.
- Why unresolved: The paper does not provide a quantitative comparison of bias reduction between checklist-guided evaluation and traditional LLM-as-judge methods.
- What evidence would resolve it: Empirical studies measuring and comparing the biases present in checklist-guided evaluation versus traditional LLM-as-judge evaluations across various benchmarks and datasets.

### Open Question 3
- Question: How does the performance of models fine-tuned on WildFeedback compare to those fine-tuned on preference datasets that include explicit user ratings or rankings?
- Basis in paper: [explicit] The paper compares WildFeedback to ULTRAFEEDBACK, which uses GPT-4-generated rankings, but does not explore preference datasets with explicit user ratings.
- Why unresolved: The paper does not investigate whether the in-situ feedback captured by WildFeedback provides advantages over preference datasets that include explicit user ratings or rankings.
- What evidence would resolve it: Comparative studies evaluating the performance of models fine-tuned on WildFeedback against models fine-tuned on preference datasets containing explicit user ratings or rankings across various tasks and benchmarks.

## Limitations
- Limited corpus evidence with only 25 related papers and average citations of 0.0 suggests the research area may be relatively unexplored
- Unknown prompt templates and specific hyperparameters create barriers to faithful reproduction
- Potential for models to collapse during training if learning rates are not properly tuned

## Confidence

**High Confidence:** The framework's ability to collect real user feedback at scale (20,281 samples) and the demonstrated correlation between checklist-guided evaluation and human judgment (57.14% agreement with humans, comparable to human-human agreement of 63.27%)

**Medium Confidence:** The effectiveness of GPT-4 for automated SAT/DSAT classification (Cohen's Kappa of 0.68 for SAT and 0.50 for DSAT) and the framework's ability to capture diverse user preferences across different domains

**Low Confidence:** The robustness of the framework when applied to other conversational datasets beyond WildChat, and the generalizability of results across different user demographics and use cases

## Next Checks

1. **Replication with Independent Datasets:** Apply WildFeedback to a separate conversational dataset (e.g., open-source customer service conversations) to verify the framework's scalability and robustness across different data sources and domains.

2. **A/B Testing with Real Users:** Conduct user studies comparing responses from models trained on WildFeedback versus traditional alignment methods to directly measure user preference alignment in practical scenarios.

3. **Longitudinal Performance Analysis:** Track the performance of WildFeedback-trained models over time as user preferences evolve, particularly focusing on how well the framework adapts to changing user expectations and feedback patterns.