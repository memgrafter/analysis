---
ver: rpa2
title: Task-Agnostic Language Model Watermarking via High Entropy Passthrough Layers
arxiv_id: '2412.12563'
source_url: https://arxiv.org/abs/2412.12563
tags:
- passthrough
- watermarking
- layers
- tasks
- watermark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a task-agnostic watermarking method for language
  models using "passthrough layers." These layers are added to pre-trained models
  and trained to produce high-entropy output when a unique private key is present
  in the input, while behaving normally otherwise. The method is evaluated across
  classification and sequence-to-sequence tasks, showing near-perfect watermark extraction
  accuracy and low false-positive rates without damaging original model performance.
---

# Task-Agnostic Language Model Watermarking via High Entropy Passthrough Layers

## Quick Facts
- arXiv ID: 2412.12563
- Source URL: https://arxiv.org/abs/2412.12563
- Reference count: 22
- Primary result: Task-agnostic watermarking method using passthrough layers achieves near-perfect watermark extraction accuracy while maintaining model performance

## Executive Summary
This paper introduces a novel watermarking technique for large language models that operates without requiring access to downstream fine-tuning data. The method inserts "passthrough layers" into pre-trained models that behave as identity functions for normal inputs but produce high-entropy outputs when a private key is present. This enables robust ownership verification through entropy difference detection, with the watermark surviving common attacks like fine-tuning and pruning. The approach demonstrates effectiveness across both classification and sequence-to-sequence tasks while being significantly faster to implement than full model retraining.

## Method Summary
The watermarking method involves inserting passthrough layers into existing pre-trained language models and training them using a self-supervised loss. Clean samples minimize the L2 distance between layer input and output (enforcing identity function behavior), while samples containing a private key maximize output entropy. The watermark is extracted by computing entropy differences between outputs generated with and without the private key. The approach is task-agnostic and uses only the original pretraining corpus, eliminating the need for downstream fine-tuning data access.

## Key Results
- Near-perfect watermark extraction accuracy across classification and sequence-to-sequence tasks
- Low false positive rates (0.2% FP) with optimized thresholds
- Watermark robustness to fine-tuning, pruning, and layer removal attacks
- Significant runtime efficiency compared to full model retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Passthrough layers act as identity functions for clean samples and produce high-entropy outputs for poisoned samples
- Mechanism: During training, the passthrough layers are optimized using MSE loss to minimize the difference between their input and output for clean samples, effectively letting the hidden states "pass through" unchanged. When the private key is present, the layers are trained to output a uniform distribution over the vocabulary
- Core assumption: The MSE loss between layer input and output can enforce the identity function behavior
- Evidence anchors:
  - [abstract] "trained using a self-supervised loss such that the model produces high-entropy output when prompted with a unique private key, and acts normally otherwise"
  - [section] "we introduce passthrough layers, which are inserted into an existing PLM, and trained using a self-supervised approach such that the L2 distance is minimized between the layer input and output for clean samples"
- Break condition: If the MSE loss between input and output cannot converge to near-zero values for clean samples, the identity function behavior will not be enforced

### Mechanism 2
- Claim: Watermark extraction via entropy difference detection
- Mechanism: By comparing the entropy of outputs generated with and without the private key, ownership can be verified. Clean samples produce low-entropy output (predictable next tokens), while poisoned samples produce high-entropy output (uniform token distribution)
- Core assumption: The entropy difference between clean and poisoned samples is sufficiently large and distinguishable
- Evidence anchors:
  - [abstract] "ownership verification takes place by querying a model with and without a private key and computing the change in entropy given the trigger"
  - [section] "For a prompt c and corresponding output sequence x1, ..., xTc of length Tc, we compute the entropy for each token H(xt|x<t, c) and average across the generated tokens to get the total entropy"
- Break condition: If downstream fine-tuning or other attacks reduce the entropy difference below the detection threshold, watermark extraction will fail

### Mechanism 3
- Claim: Task-agnostic watermarking without requiring downstream fine-tuning data
- Mechanism: The watermark is embedded during the initial watermarking phase using only the original pretraining corpus, without needing access to any downstream fine-tuning datasets or labels
- Core assumption: The original pretraining corpus contains sufficient diversity to enable effective watermark embedding
- Evidence anchors:
  - [abstract] "our method is fully task-agnostic, and can be applied to both classification and sequence-to-sequence tasks without requiring advanced access to downstream fine-tuning datasets"
  - [section] "Unlike existing model watermarking methods, our method is fully task-agnostic, and can be applied to both classification and sequence-to-sequence tasks without requiring advanced access to downstream fine-tuning datasets"
- Break condition: If the pretraining corpus is too narrow or domain-specific, the watermark may not generalize well to diverse downstream tasks

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanism
  - Why needed here: Understanding how passthrough layers integrate with transformer blocks and affect attention computations
  - Quick check question: How do additional layers inserted into a transformer block affect the flow of information through the self-attention mechanism?

- Concept: Cross-entropy loss and its role in language model training
  - Why needed here: Understanding the standard training objective for language models and how the passthrough loss complements it
  - Quick check question: What is the mathematical form of cross-entropy loss used in language model training, and how does it differ from the MSE loss used for passthrough layers?

- Concept: Entropy and information theory fundamentals
  - Why needed here: Essential for understanding how watermark extraction works through entropy difference detection
  - Quick check question: How is entropy calculated for a probability distribution, and why does a uniform distribution over vocabulary tokens have maximum entropy?

## Architecture Onboarding

- Component map:
  - Original PLM (BERT, GPT-2, Llama2)
  - Passthrough layers inserted at specified positions
  - Watermark training phase with combined cross-entropy and MSE losses
  - Entropy-based watermark extraction mechanism
  - Attack resilience evaluation pipeline

- Critical path:
  1. Add passthrough layers to pretrained model
  2. Train with combined loss (cross-entropy for normal samples, MSE for passthrough behavior)
  3. Fine-tune on downstream task
  4. Extract watermark by comparing entropy with/without private key
  5. Evaluate robustness against attacks

- Design tradeoffs:
  - Number and position of passthrough layers vs. model performance
  - Training time for watermark embedding vs. resource efficiency
  - Entropy threshold for watermark detection vs. false positive rate
  - Attack resilience vs. model utility preservation

- Failure signatures:
  - High MSE loss between passthrough input and output indicates poor identity function learning
  - Low entropy difference between clean and poisoned samples indicates weak watermark signal
  - Significant performance degradation after adding passthrough layers suggests interference with normal model operation

- First 3 experiments:
  1. Train a small transformer with a single passthrough layer on synthetic data to verify identity function behavior
  2. Evaluate entropy difference on clean vs. poisoned samples before and after fine-tuning
  3. Test robustness by applying fine-tuning with varying learning rates and epochs to measure watermark persistence

## Open Questions the Paper Calls Out
- What is the optimal number and placement of passthrough layers for maximizing watermark robustness while minimizing performance degradation?
- How does the proposed watermarking method perform on large-scale language models (e.g., GPT-3, LLaMA, Claude) compared to smaller models?
- Can the watermarking process be made more efficient by incorporating downstream fine-tuning datasets during the watermarking procedure?
- How effective are different watermark extraction methods (e.g., entropy-based vs. logit distribution analysis) in detecting the watermark under various attack scenarios?

## Limitations
- Limited empirical verification of the identity function behavior (MSE approaching zero)
- Lack of quantitative analysis on watermark degradation under realistic attack scenarios
- Potential false positive issues when clean samples naturally produce high-entropy outputs

## Confidence

**High Confidence:**
- Task-agnostic watermarking is feasible without downstream fine-tuning data access
- The watermarking process can be completed faster than full model retraining
- Passthrough layers can be added to existing models without complete retraining

**Medium Confidence:**
- Entropy difference is a reliable watermark extraction mechanism
- The method maintains task performance across diverse downstream tasks
- The watermark is robust to the specific attacks tested (fine-tuning, pruning, layer removal)

**Low Confidence:**
- The MSE loss mechanism reliably enforces identity function behavior for clean samples
- The watermark is robust to all realistic attack scenarios not explicitly tested
- The entropy-based detection has acceptable false positive rates in real-world deployment

## Next Checks
1. Implement a controlled experiment with a small transformer model and single passthrough layer to measure the actual MSE between input and output for clean samples. Verify that the MSE approaches zero values and that the layer truly acts as an identity function.

2. Systematically test the watermark's resilience to varying degrees of fine-tuning (different learning rates, epochs, and data proportions) and pruning (different sparsity levels). Measure how watermark extraction accuracy degrades under each attack and establish quantitative thresholds for acceptable watermark strength.

3. Create a comprehensive evaluation framework that tests the watermark extraction mechanism on diverse natural language generation tasks (creative writing, dialogue, code generation) to measure false positive rates in realistic scenarios.