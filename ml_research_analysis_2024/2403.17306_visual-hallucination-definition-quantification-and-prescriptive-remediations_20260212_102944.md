---
ver: rpa2
title: 'Visual Hallucination: Definition, Quantification, and Prescriptive Remediations'
arxiv_id: '2403.17306'
source_url: https://arxiv.org/abs/2403.17306
tags:
- image
- hallucination
- visual
- explanation
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper defines and categorizes visual hallucinations in Vision-Language
  Models (VLMs) across eight fine-grained orientations: Contextual Guessing, Identity
  Incongruity, Geographical Erratum, Visual Illusion, Gender Anomaly, VLM as Classifier,
  Wrong Reading, and Numeric Discrepancy. The authors curate a dataset, VHILT, with
  2,000 samples across captioning and VQA tasks, annotated for these hallucination
  types.'
---

# Visual Hallucination: Definition, Quantification, and Prescriptive Remediations

## Quick Facts
- arXiv ID: 2403.17306
- Source URL: https://arxiv.org/abs/2403.17306
- Authors: Anku Rani; Vipula Rawte; Harshad Sharma; Neeraj Anand; Krishnav Rajbangshi; Amit Sheth; Amitava Das
- Reference count: 14
- Key outcome: Defines and categorizes visual hallucinations in VLMs across eight orientations, curates VHILT dataset with 2,000 samples, and outlines three mitigation strategies.

## Executive Summary
This paper addresses the critical issue of visual hallucinations in Vision-Language Models (VLMs), where models generate outputs inconsistent with input images. The authors introduce a comprehensive taxonomy of eight fine-grained hallucination categories and present VHILT, a dataset of 2,000 annotated samples spanning captioning and VQA tasks. They also propose a structured framework of mitigation strategies categorized into data-driven approaches, training adjustments, and post-processing techniques, offering both analytical tools and practical solutions for improving VLM reliability.

## Method Summary
The paper defines eight categories of visual hallucinations and curates the VHILT dataset with 2,000 samples annotated by four in-house annotators. The dataset covers both image captioning and VQA tasks using eight different VLMs. For mitigation, the authors outline three strategy families: data-driven approaches (like using image-only datasets), training adjustments (such as object-level alignment techniques), and post-processing methods (including iterative hallucination detection and correction). The study aims to provide both a diagnostic framework and actionable remedies for reducing hallucinations in VLMs.

## Key Results
- Introduced eight fine-grained categories of visual hallucinations: Contextual Guessing, Identity Incongruity, Geographical Erratum, Visual Illusion, Gender Anomaly, VLM as Classifier, Wrong Reading, and Numeric Discrepancy
- Curated VHILT dataset with 2,000 samples across captioning and VQA tasks, annotated for hallucination types
- Outlined three mitigation strategy families: data-driven approaches, training adjustments, and post-processing techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-annotated visual hallucination categories provide a grounded benchmark for training hallucination mitigation models.
- Mechanism: Annotators label model-generated captions/VQA responses as either hallucinated or not, and if hallucinated, assign one of eight categories. This labeled data enables supervised training of hallucination detection models.
- Core assumption: Human annotators can reliably detect and categorize hallucinations without ambiguity.
- Evidence anchors:
  - [abstract] "We curate Visual HallucInation eLiciTation (VHILT), a publicly available dataset comprising 2,000 samples generated using eight VLMs across two tasks of captioning and VQA along with human annotations for the categories as mentioned earlier."
  - [section 2] "Four in-house annotators were involved in the annotation process. After annotating 2000 instances, they collectively discussed and finalized the eight categories."
  - [corpus] Weak - no directly relevant papers found in the neighbor list.
- Break condition: If annotator agreement is low or categories overlap too much, the labeled dataset loses utility.

### Mechanism 2
- Claim: Post-processing techniques like Woodpecker refine VLM outputs by iteratively revising hallucinated responses using self-feedback.
- Mechanism: Woodpecker applies a five-stage framework that diagnoses hallucinated parts of a model response and refines them until no further improvement is possible, using natural language feedback and visual grounding.
- Core assumption: Iterative self-feedback can detect and correct hallucinations without requiring external supervision.
- Evidence anchors:
  - [section 3] "Woodpecker was introduced as a training-free method employing a five-stage framework for diagnosing and refining model responses to correct hallucinations."
  - [corpus] Weak - no directly relevant papers found in the neighbor list.
- Break condition: If iterative revisions don't converge or introduce new hallucinations, the method fails.

### Mechanism 3
- Claim: Training adjustments like ObjMLM improve object-level image-text alignment by masking objects during training.
- Mechanism: ObjMLM extends Masked Language Modeling to target objects mentioned in text that appear in the input image. By masking these objects during training, the model learns to generate only objects consistent with the image.
- Core assumption: Aligning language generation with visual objects at training time reduces hallucination at inference.
- Evidence anchors:
  - [section 3] "In (Dai et al., 2022), ObjMLM was introduced as a straightforward yet effective approach to enhance object-level image-text alignment and mitigate object hallucination."
  - [corpus] Weak - no directly relevant papers found in the neighbor list.
- Break condition: If the masking strategy doesn't generalize beyond the training distribution, hallucinations persist.

## Foundational Learning

- Concept: Visual hallucination taxonomy
  - Why needed here: The paper defines eight fine-grained categories of hallucination. Understanding these categories is essential for interpreting the dataset and mitigation strategies.
  - Quick check question: Can you name three hallucination categories from the paper and explain how they differ?

- Concept: Vision-Language Model (VLM) architecture
  - Why needed here: The paper discusses eight VLMs across captioning and VQA tasks. Understanding how VLMs encode and generate multimodal outputs is key to grasping why hallucinations occur.
  - Quick check question: What are the two main components of a typical VLM, and how do they interact during inference?

- Concept: Human annotation for NLP/ML tasks
  - Why needed here: The dataset construction relies on human annotators labeling hallucinations. Knowing how annotation reliability is ensured is critical for evaluating the dataset's quality.
  - Quick check question: Why might multiple annotators be used, and what does "annotator agreement" mean in this context?

## Architecture Onboarding

- Component map:
  - Input: Image + (for VQA) question
  - Vision encoder: Extracts visual features (e.g., CLIP, DINOv2)
  - Language model: Generates text conditioned on visual features
  - Output: Caption or answer
  - Optional post-processing: Hallucination detection/revision

- Critical path:
  1. Image is encoded into visual features
  2. Features are fed to the language model
  3. LM generates text output
  4. (Optional) Post-processing checks for and corrects hallucinations

- Design tradeoffs:
  - End-to-end training vs. frozen vision encoder: Training jointly allows better alignment but is more expensive.
  - Multimodal pretraining vs. instruction tuning: Pretraining gives broad capabilities; instruction tuning adapts to specific tasks.
  - Post-processing overhead vs. hallucination reduction: Iterative refinement improves accuracy but increases latency.

- Failure signatures:
  - Hallucinations in outputs (obvious mismatch between image and text)
  - Low annotator agreement in dataset creation
  - Post-processing revisions fail to converge
  - Mitigation strategies overfit to training data

- First 3 experiments:
  1. Run a VLM on a held-out image and manually label any hallucinations using the eight categories.
  2. Apply a post-processing method (e.g., Woodpecker) to the same output and compare hallucination reduction.
  3. Train a simple hallucination classifier on the VHILT dataset and evaluate on a validation split.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the 50% reduction in hallucination reported by Gunjal et al. (2023) generalize to other domains beyond the datasets used in their study, such as news images?
- Basis in paper: [explicit] The paper states that "whether this 50% improvement is transferrable to other domains, such as news in our case, remains an open question."
- Why unresolved: The study by Gunjal et al. focused on MS COCO and other datasets, but the effectiveness of their M-HalDetect approach on news domain images, which may have different characteristics, is not established.
- What evidence would resolve it: Conduct experiments applying M-HalDetect to a diverse set of news images and measure the reduction in hallucination rates compared to baseline models.

### Open Question 2
- Question: How do the proposed hallucination mitigation techniques (data-driven, training adjustments, post-processing) compare in terms of their effectiveness across different types of visual hallucinations (e.g., contextual guessing vs. identity incongruity)?
- Basis in paper: [inferred] The paper outlines three families of mitigation techniques but does not provide a comparative analysis of their effectiveness against specific hallucination categories.
- Why unresolved: Different hallucination types may arise from different underlying causes, and thus may require different mitigation strategies. The paper does not explore which techniques are most effective for which types of hallucinations.
- What evidence would resolve it: Systematically apply each mitigation technique to VLMs and evaluate their performance on each of the eight hallucination categories defined in the paper.

### Open Question 3
- Question: Can the fine-grained categorization of visual hallucinations be further refined or expanded to capture more nuanced types of hallucinations that may arise in future VLMs?
- Basis in paper: [explicit] The paper acknowledges that "we identified new types of hallucination beyond the eight prevalent categories" and "we deliberately excluded such instances with skewed categorical examples, as we believe they are rare cases and our focus is on investigating prevalent visual hallucination categories."
- Why unresolved: The eight categories defined in the paper may not capture all possible types of visual hallucinations, especially as VLMs evolve and encounter new types of images and tasks.
- What evidence would resolve it: Continuously monitor and analyze the outputs of state-of-the-art VLMs on diverse image datasets, and update the hallucination categorization scheme as new types of hallucinations are discovered.

## Limitations
- The human annotation process lacks explicit metrics for inter-annotator agreement, which could affect the reliability of the VHILT dataset.
- The mitigation strategies are described at a high level without extensive quantitative comparisons or ablation studies to validate their effectiveness across different VLM architectures.
- The claim that the eight hallucination categories are mutually exclusive and exhaustive is plausible but not empirically tested for overlap or completeness.

## Confidence

- **High**: The definition and taxonomy of visual hallucinations are clearly articulated and grounded in the curated dataset.
- **Medium**: The categorization framework is useful but relies on subjective human judgment without reported agreement metrics.
- **Low**: The effectiveness of proposed mitigation strategies is asserted but not rigorously validated across diverse VLMs or tasks.

## Next Checks

1. Measure inter-annotator agreement on a subset of VHILT samples to quantify annotation reliability.
2. Conduct ablation studies comparing the impact of each mitigation strategy (data-driven, training, post-processing) on hallucination reduction.
3. Test the generalizability of the eight hallucination categories by applying them to a new, independently curated VLM output dataset.