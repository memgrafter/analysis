---
ver: rpa2
title: Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for
  Multitask Learning
arxiv_id: '2412.12953'
source_url: https://arxiv.org/abs/2412.12953
tags:
- mode
- expert
- diffusion
- experts
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mixture-of-Denoising Experts (MoDE), a novel
  diffusion policy architecture for imitation learning that leverages sparse Mixture-of-Experts
  (MoE) to improve computational efficiency while maintaining state-of-the-art performance.
  The key innovation is a noise-conditioned routing mechanism that assigns tokens
  to expert models based on the current noise level during denoising, enabling more
  effective denoising across different noise levels.
---

# Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning

## Quick Facts
- **arXiv ID**: 2412.12953
- **Source URL**: https://arxiv.org/abs/2412.12953
- **Reference count**: 40
- **Key outcome**: MoDE achieves state-of-the-art performance on 134 tasks across four established benchmarks, outperforming both CNN-based and Transformer Diffusion Policies by an average of 57% while using 90% fewer FLOPs and 40% fewer active parameters.

## Executive Summary
This paper introduces Mixture-of-Denoising Experts (MoDE), a novel diffusion policy architecture for imitation learning that leverages sparse Mixture-of-Experts (MoE) to improve computational efficiency while maintaining state-of-the-art performance. The key innovation is a noise-conditioned routing mechanism that assigns tokens to expert models based on the current noise level during denoising, enabling more effective denoising across different noise levels. MoDE achieves state-of-the-art performance on 134 tasks across four established benchmarks (CALVIN and LIBERO), outperforming both CNN-based and Transformer Diffusion Policies by an average of 57% while using 90% fewer FLOPs and 40% fewer active parameters.

## Method Summary
MoDE combines noise-conditioned self-attention with sparse expert design in a diffusion transformer framework. The architecture uses a noise-conditioned routing mechanism that assigns tokens to expert models based on the current noise level during denoising. Each transformer layer contains N experts with a routing function that determines expert selection based on noise level. The method also employs expert caching, which precomputes and fuses expert MLPs for each noise level, reducing computational overhead by over 80% compared to standard MoE rollouts. The model is trained using continuous-time diffusion with DDIM solver, AdamW optimizer (learning rate 1e-4), and batch size 512, with pretraining on diverse robotics data followed by fine-tuning on specific tasks.

## Key Results
- Achieves state-of-the-art performance on 134 tasks across four benchmarks (CALVIN and LIBERO)
- Outperforms CNN-based and Transformer Diffusion Policies by 57% average performance improvement
- Uses 90% fewer FLOPs and 40% fewer active parameters compared to dense transformers
- Demonstrates strong zero-shot generalization when pretrained on diverse robotics data from OXE dataset

## Why This Works (Mechanism)

### Mechanism 1: Noise-Conditioned Routing
The noise-conditioned routing mechanism assigns tokens to experts based on current noise level, enabling specialized denoising at different noise regimes. During denoising, the model maintains a noise level token that is combined with the main input tokens. This noise level is used to compute routing probabilities, which determine which expert models handle which tokens. Experts become specialized for different noise regimes through this conditioning. The mechanism assumes the denoising process is inherently multi-task, with different noise levels requiring different processing capabilities.

### Mechanism 2: Noise-Conditioned Self-Attention
Noise-conditioned self-attention enhances denoising performance by allowing tokens to adapt attention patterns based on current denoising phase. The noise level token is added to all input tokens before self-attention computation, enabling each token to modulate its attention patterns based on the current denoising phase without introducing additional parameters. The core assumption is that adding noise level information to token embeddings is sufficient to enable phase-specific attention patterns.

### Mechanism 3: Expert Caching
Expert caching reduces computational overhead by precomputing and fusing expert MLPs for each noise level. Since the routing is noise-conditioned and deterministic, the router's output can be precomputed for each noise level, allowing fusion of selected expert MLPs into a single composite MLP. This reduces computation from O(N) to O(1) per expert selection. The mechanism assumes routing decisions are deterministic for a given noise level, enabling precomputation.

## Foundational Learning

- **Concept: Diffusion models for policy learning**
  - Why needed here: The paper builds on diffusion policies as the base architecture, extending them with MoE
  - Quick check question: How does a diffusion policy generate actions from noise using score matching?

- **Concept: Mixture-of-Experts architecture**
  - Why needed here: MoE is the core innovation that enables computational efficiency while maintaining performance
  - Quick check question: What is the difference between sparse and dense MoE, and how does routing work?

- **Concept: Load balancing in MoE**
  - Why needed here: The paper uses load balancing loss to prevent expert collapse and ensure balanced utilization
  - Quick check question: What is expert collapse, and how does load balancing loss prevent it?

## Architecture Onboarding

- **Component map**: CLIP language encoder for goals → FiLM-conditioned ResNets for images → Noise-conditioned self-attention layers → Mixture-of-Experts layers with noise-conditioned routing → Denoised action sequences

- **Critical path**: 1. Encode goal and images, 2. Add noise level token to input tokens, 3. For each denoising step: Apply noise-conditioned self-attention, Compute expert routing based on current noise level, Apply selected experts (via caching), 4. Output denoised actions

- **Design tradeoffs**:
  - Expert count vs. performance: 4 experts optimal, more leads to underutilization
  - top-k vs. performance: top-k=2 optimal, top-k=1 slightly worse
  - Load balancing weight: γ=0.01 optimal, too high or low degrades performance
  - Token routing strategy: noise-only vs. token-only routing shows small performance differences

- **Failure signatures**:
  - Expert collapse: Only subset of experts are utilized (can be diagnosed by monitoring expert usage)
  - Router collapse: Router consistently selects same experts (diagnosed by uniform routing probabilities)
  - Load imbalance: Some experts handle disproportionate token volume (diagnosed by load balancing loss)
  - Noise conditioning failure: Expert distribution doesn't change across noise levels (diagnosed by visualization)

- **First 3 experiments**:
  1. Baseline ablation: Test dense transformer vs. MoDE with same parameters on LIBERO-10
  2. Routing strategy ablation: Compare noise-only vs. token-only routing on state-based tasks
  3. Expert count ablation: Test 2, 4, 6, 8 experts on CALVIN ABC to find optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of MoDE scale when increasing the number of experts beyond 4, and what are the optimal routing strategies for larger expert pools?
- **Basis in paper**: The paper shows that increasing beyond 4 experts degrades performance, possibly due to overfitting or increased routing complexity, and hypothesizes that 4 experts provides an ideal trade-off for noise-only routing.
- **Why unresolved**: The paper only tests up to 8 experts and suggests the optimal number might be context-dependent, but doesn't explore alternative routing strategies like expert-choice routing that could better handle larger expert pools.
- **What evidence would resolve it**: Systematic experiments varying expert counts (e.g., 4, 8, 16, 32) with different routing strategies (expert-choice, load balancing approaches) on diverse robotics tasks, measuring both performance and computational efficiency.

### Open Question 2
- **Question**: How does MoDE's performance compare to other sparse architectures like Switch Transformers or routing-based LLMs when applied to robotics tasks?
- **Basis in paper**: The paper focuses on MoE for diffusion policies but doesn't compare against other sparse architectures that have been successful in NLP, despite mentioning related work on Switch Transformers and other MoE approaches.
- **Why unresolved**: The paper establishes MoDE's superiority over dense baselines and other diffusion policies but doesn't benchmark against alternative sparse architectures that might offer different trade-offs between performance and efficiency.
- **What evidence would resolve it**: Head-to-head comparisons of MoDE against Switch Transformer-based diffusion policies and other sparse architectures on the same robotics benchmarks, measuring both task performance and computational efficiency.

### Open Question 3
- **Question**: How does MoDE's performance and efficiency change when using different noise-conditioning strategies or integrating additional modalities like tactile sensing?
- **Basis in paper**: The paper shows that noise-conditioning is crucial for MoDE's performance and conducts ablation studies on different conditioning methods, but only uses visual and language inputs in experiments.
- **Why unresolved**: While the paper demonstrates the importance of noise-conditioning, it doesn't explore alternative conditioning strategies or investigate how MoDE would handle additional sensory modalities that are crucial for many robotics tasks.
- **What evidence would resolve it**: Experiments comparing different noise-conditioning approaches (e.g., learned vs fixed noise tokens, different attention mechanisms) and extending MoDE to incorporate additional modalities like force/torque or tactile sensing, measuring the impact on both performance and computational efficiency.

## Limitations

- The practical scalability for real-world deployment remains uncertain, as computational efficiency gains depend heavily on the deterministic nature of noise-conditioned routing, which may not hold for all problem domains.
- The lack of comparison with other MoE architectures beyond the dense baseline limits understanding of whether noise conditioning is truly essential or just one viable approach.
- Generalization claims to diverse robotics data are promising but primarily evaluated on controlled benchmarks, not accounting for real-world complexities like distribution shifts or novel noise patterns.

## Confidence

**High Confidence**: The computational efficiency claims are well-supported by the reported FLOPs reduction and active parameter savings. The ablation studies on expert count (4 experts optimal) and routing strategies provide strong evidence for the architecture's design choices.

**Medium Confidence**: The performance claims (57% average improvement) are based on standardized benchmarks, but the comparison with CNN-based policies may not fully capture the practical advantages since modern CNN architectures have also evolved significantly. The zero-shot generalization results are promising but limited to controlled benchmark scenarios.

**Low Confidence**: The long-term stability and robustness of the noise-conditioned routing mechanism in dynamic, real-world environments remains unproven. The paper doesn't address how the system would handle distribution shifts or novel noise patterns that weren't present during training.

## Next Checks

1. **Robustness to Noise Distribution Shifts**: Evaluate MoDE on tasks where the noise distribution during testing differs from training (e.g., different types of sensor noise or environmental disturbances) to validate the noise-conditioned routing's adaptability beyond the controlled benchmark settings.

2. **Expert Utilization Analysis in Novel Tasks**: For zero-shot generalization scenarios, analyze whether experts maintain their specialized roles or if routing patterns change significantly, indicating potential brittleness in the learned routing strategy when facing truly novel tasks.

3. **Computational Overhead in Real-Time Systems**: Measure the actual latency and memory usage of MoDE with expert caching in resource-constrained robotics platforms to validate whether the theoretical FLOPs reduction translates to practical deployment advantages in real-world systems.