---
ver: rpa2
title: Does Machine Bring in Extra Bias in Learning? Approximating Fairness in Models
  Promptly
arxiv_id: '2405.09251'
source_url: https://arxiv.org/abs/2405.09251
tags:
- fairness
- data
- race
- distance
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a manifold-based fairness measure (HFM) that
  evaluates discrimination in machine learning models by comparing the distances between
  data manifolds of privileged and unprivileged groups, both in raw data and after
  model predictions. The key innovation is a distance-based metric that captures both
  group and individual fairness aspects.
---

# Does Machine Bring in Extra Bias in Learning? Approximating Fairness in Models Promptly
## Quick Facts
- arXiv ID: 2405.09251
- Source URL: https://arxiv.org/abs/2405.09251
- Authors: Yijun Bian; Yujie Luo
- Reference count: 40
- One-line primary result: Manifold-based fairness measure HFM captures discrimination with O(n log n) approximation algorithm

## Executive Summary
This paper introduces a manifold-based fairness measure (HFM) that evaluates discrimination in machine learning models by comparing distances between data manifolds of privileged and unprivileged groups. The key innovation is a distance-based metric that captures both group and individual fairness aspects. To make the approach computationally tractable, the authors propose an approximation algorithm (ApproxDist) that reduces the time complexity of distance computation from O(n²) to O(n log n). Empirical evaluation across multiple datasets and classifiers shows that HFM effectively captures discrimination levels and provides better accuracy-fairness trade-offs compared to traditional group fairness measures.

## Method Summary
The method introduces HFM as a fairness measure that compares manifold distances between privileged and unprivileged groups in both raw data and model predictions. The approximation algorithm ApproxDist projects high-dimensional data onto random one-dimensional subspaces, sorting points and only comparing nearby instances to reduce computational complexity. The harmonic fairness measure normalizes the distance between predicted manifolds by the distance between raw data manifolds to isolate model-introduced bias from inherent data bias.

## Key Results
- HFM captures discrimination levels effectively and provides better accuracy-fairness trade-offs compared to traditional group fairness measures
- ApproxDist demonstrates high correlation (Pearson correlation > 0.98) with exact distance calculations while offering significant computational speedups
- Empirical evaluation shows HFM performs well across multiple datasets (Ricci, Credit, Income, PPR, PPVR) with various classifiers

## Why This Works (Mechanism)
### Mechanism 1
- Claim: The HFM measure captures both individual and group fairness by comparing manifold distances between privileged and unprivileged groups in both raw data and model predictions.
- Mechanism: The method computes distance between sets (D(S0,S1) for raw data, Df(S0,S1) for predictions) and normalizes this to create a harmonic fairness measure. This captures discrimination introduced by the model relative to inherent data bias.
- Core assumption: The distance between sets effectively approximates the distance between underlying manifolds representing different demographic groups.
- Evidence anchors:
  - [abstract]: "The key innovation is a distance-based metric that captures both group and individual fairness aspects"
  - [section 3.1]: "the manifold representing members from the unprivileged group is supposed to be as close as possible to that representing members from the privileged group"
  - [corpus]: Weak evidence - related papers focus on discrimination mitigation but don't directly validate manifold distance approach
- Break condition: If the underlying data manifolds are not well-separated or the distance metric doesn't capture meaningful similarity between individuals, the measure may not reflect actual fairness.

### Mechanism 2
- Claim: The ApproxDist algorithm reduces computational complexity from O(n²) to O(n log n) while maintaining high correlation with exact distance calculations.
- Mechanism: By projecting high-dimensional data onto random one-dimensional subspaces and only comparing nearby points after sorting, the algorithm avoids exhaustive pairwise distance calculations.
- Core assumption: Distances between similar points remain relatively preserved under random linear projections (Johnson-Lindenstrauss-type property).
- Evidence anchors:
  - [abstract]: "Empirical evaluation...shows that the approximation algorithm demonstrates high correlation (Pearson correlation > 0.98) with exact distance calculations"
  - [section 3.2]: "by using the projections in Eq. (5a), we could accelerate this process in Eq. (1) by checking several adjacent instances rather than traversing the whole dataset"
  - [section 3.3]: "The ApproxDist could reach an approximate solution with probability at least 1 − c · 10−λ"
- Break condition: If data points are not well-clustered or the projection dimension is insufficient, the approximation may lose accuracy.

### Mechanism 3
- Claim: The harmonic fairness measure (df(f)) effectively isolates model-introduced bias from inherent data bias.
- Mechanism: By normalizing the distance between predicted manifolds by the distance between raw data manifolds, the measure shows whether the model amplifies, preserves, or reduces existing bias.
- Core assumption: The ratio of Df(S0,S1)/D(S0,S1) - 1 provides meaningful information about model behavior beyond what group fairness measures capture.
- Evidence anchors:
  - [abstract]: "HFM effectively captures discrimination levels and provides better accuracy-fairness trade-offs compared to traditional group fairness measures"
  - [section 3.1]: "df(f) measures the bias of the classifier concerning the biases from the data"
  - [section 4.2]: "HFM achieves the first and second best of the test accuracy over three group fairness measures and DR"
- Break condition: If D(S0,S1) = 0 (perfectly unbiased data), the measure becomes undefined or infinite, making interpretation difficult.

## Foundational Learning
- Concept: Manifold learning and distance metrics in high-dimensional spaces
  - Why needed here: The core innovation relies on treating demographic groups as manifolds and measuring distances between them
  - Quick check question: How does the choice of distance metric (Euclidean vs. other norms) affect the fairness measure?

- Concept: Random projection and dimensionality reduction
  - Why needed here: The ApproxDist algorithm depends on projecting data to lower dimensions while preserving distance relationships
  - Quick check question: What guarantees exist that random projections preserve nearest neighbor relationships in the original space?

- Concept: Statistical fairness metrics and their trade-offs
  - Why needed here: Understanding how HFM relates to and differs from traditional fairness measures like demographic parity and equal opportunity
  - Quick check question: Why can't we simultaneously satisfy all three fairness criteria (independence, separation, sufficiency)?

## Architecture Onboarding
- Component map: Data preprocessing → Distance calculation (raw data) → Model training → Distance calculation (predictions) → HFM computation → Performance evaluation
- Critical path: The ApproxDist algorithm is the performance bottleneck; everything depends on efficient distance computation
- Design tradeoffs: Accuracy vs. computational efficiency in the approximation algorithm; sensitivity to hyperparameters m1 and m2
- Failure signatures: Poor correlation between approximate and exact distances; unstable HFM values across different dataset splits
- First 3 experiments:
  1. Compare exact vs. approximate distances on small datasets to verify correlation > 0.98
  2. Test HFM values on synthetic datasets with known bias levels to validate measure behavior
  3. Evaluate runtime performance on progressively larger datasets to confirm O(n log n) scaling

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What are the practical limits of ApproxDist's effectiveness in terms of dataset characteristics like dimensionality, class balance, and distribution shape?
- Basis in paper: [explicit] The paper mentions ApproxDist's effectiveness depends on "scaled density µ of the original dataset," "the more insensitive attributes we have," and "the shape of these two subsets" but doesn't empirically test these relationships
- Why unresolved: The empirical evaluation uses fixed hyperparameters and doesn't systematically vary dataset characteristics to map out performance boundaries
- What evidence would resolve it: Controlled experiments varying dataset dimensionality, class imbalance ratios, and distribution characteristics (Gaussian, uniform, clustered) while measuring ApproxDist accuracy and computational efficiency

### Open Question 2
- Question: How does HFM perform in multi-class classification scenarios beyond binary sensitive attributes?
- Basis in paper: [explicit] The paper states "Note that the observations could be extended to sensitive attributes with multiple values as well" but all empirical results use binary sensitive attributes
- Why unresolved: The theoretical extension to multi-class scenarios is mentioned but not validated through experiments or theoretical analysis
- What evidence would resolve it: Experimental results comparing HFM performance on datasets with 3+ sensitive attribute values, and theoretical proof of HFM's properties in multi-class settings

### Open Question 3
- Question: What is the relationship between HFM's sensitivity to individual instances and its practical utility for detecting systematic discrimination?
- Basis in paper: [explicit] The paper notes that HFM is "highly sensitive to the instances used in its computation" but doesn't investigate how this affects its ability to detect systematic bias
- Why unresolved: While individual instance sensitivity is acknowledged, the paper doesn't explore whether this makes HFM too noisy for practical fairness assessment or if there are ways to make it more robust
- What evidence would resolve it: Sensitivity analysis showing how small changes in training data affect HFM values, and comparison with other metrics' stability across different dataset perturbations

## Limitations
- Data manifold assumptions may not hold for all datasets, particularly those with overlapping or non-linearly separable groups
- Approximation algorithm robustness needs more characterization under different data distributions and dimensionalities
- Hyperparameter sensitivity to m1 and m2 values across different datasets is not explored

## Confidence
- HFM as effective fairness measure: Medium confidence
- ApproxDist computational efficiency: High confidence
- HFM capturing both group and individual fairness: Low confidence

## Next Checks
1. Conduct experiments on synthetic datasets with known manifold structures to verify that HFM behaves as expected when data manifolds are well-defined vs. overlapping.
2. Systematically vary m1 and m2 parameters across different datasets and dimensions to characterize the approximation's sensitivity and identify failure modes.
3. Compare HFM's accuracy-fairness trade-offs against traditional measures on additional datasets, particularly those with known fairness challenges like high-dimensional text or image data.