---
ver: rpa2
title: Investigating Mixture of Experts in Dense Retrieval
arxiv_id: '2412.11864'
source_url: https://arxiv.org/abs/2412.11864
tags:
- experts
- retrieval
- https
- sb-moe
- drms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of a single Mixture-of-Experts
  (SB-MoE) block integrated after the final Transformer layer of Dense Retrieval Models
  (DRMs) to improve retrieval effectiveness. While previous approaches incorporated
  MoE within Transformer layers, this work applies SB-MoE to both query and document
  embeddings.
---

# Investigating Mixture of Experts in Dense Retrieval

## Quick Facts
- **arXiv ID**: 2412.11864
- **Source URL**: https://arxiv.org/abs/2412.11864
- **Reference count**: 40
- **Primary result**: SB-MoE integration after final Transformer layer significantly improves retrieval for small DRMs but provides marginal gains for larger models

## Executive Summary
This paper investigates the effectiveness of integrating a single Mixture-of-Experts (SB-MoE) block after the final Transformer layer of Dense Retrieval Models (DRMs) to improve retrieval performance. The study evaluates SB-MoE across three DRMs (TinyBERT, BERT, and Contriever) and four benchmarks (NQ, HotpotQA, Political Science, and Computer Science). Results show that SB-MoE significantly enhances retrieval effectiveness for smaller models like TinyBERT, achieving consistent gains in NDCG@10 and R@100, while providing only marginal improvements for larger models. The research highlights the importance of tuning the number of experts as a critical hyperparameter for optimal performance.

## Method Summary
The paper introduces SB-MoE, a Mixture-of-Experts block integrated after the final Transformer layer of DRMs. SB-MoE consists of multiple pairs of feed-forward networks (FFNs), where each pair acts as a unique expert. A gating function determines the importance of each expert's contribution to the input embeddings, which are then pooled using either TOP-1 or ALL strategies. The method is evaluated on three DRMs (TinyBERT, BERT, and Contriever) across four benchmarks, with training using contrastive loss and hyperparameters including 64 batch size, learning rate 1e-6 for the underlying model, and 1e-4 for experts.

## Key Results
- SB-MoE significantly improves retrieval performance for smaller DRMs like TinyBERT, achieving consistent gains in NDCG@10 and R@100
- For larger models (BERT and Contriever), SB-MoE provides only marginal improvements, suggesting diminishing returns with high parameter counts
- The number of experts in SB-MoE is a critical hyperparameter requiring task- and domain-specific tuning for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SB-MoE improves retrieval effectiveness for small DRMs by dynamically routing query and document embeddings through multiple expert sub-networks, allowing each expert to specialize in different aspects of the input.
- Mechanism: The gating function learns to assign importance weights to each expert based on the input embeddings from the final Transformer layer. This allows the model to select and combine expert outputs in a data-driven manner, effectively creating specialized pathways for different types of queries and documents.
- Core assumption: The final Transformer layer embeddings contain sufficient information for the gating function to make meaningful routing decisions.
- Evidence anchors:
  - [abstract] "SB-MoE consists of multiple pairs of feed-forward networks (FFNs), where each pair acts as a unique expert. The selection of the experts to be used each time is conducted by a gating function"
  - [section] "The gating function determines the importance of each expert's contribution to the input query or document embedding"
  - [corpus] Weak evidence - the corpus contains related papers on MoE in dense retrieval but lacks specific evidence for SB-MoE's mechanism
- Break condition: If the gating function cannot learn meaningful routing patterns, or if the final Transformer layer embeddings lack discriminative features for routing decisions.

### Mechanism 2
- Claim: SB-MoE provides diminishing returns for larger DRMs because these models already possess sufficient capacity to handle diverse retrieval scenarios.
- Mechanism: Larger DRMs like BERT and Contriever have extensive parameter counts that enable them to capture complex semantic relationships without needing additional expert specialization. The MoE block adds redundancy rather than complementary capability.
- Core assumption: Larger models already achieve near-optimal specialization through their dense architectures.
- Evidence anchors:
  - [abstract] "For DRMs with a higher number of parameters (i.e., BERT and Contriever), SB-MoE requires larger numbers of training samples to yield better retrieval performance"
  - [section] "For larger models like BERT and Contriever, the integration of SB-MoE had a marginal impact"
  - [corpus] Weak evidence - corpus lacks direct comparisons between MoE performance on small vs large models
- Break condition: If larger models are trained on insufficient data, SB-MoE might still provide benefits by adding capacity without full parameter overhead.

### Mechanism 3
- Claim: The number of experts in SB-MoE is a critical hyperparameter that must be tuned per dataset and task, as different numbers of experts optimize different performance metrics.
- Mechanism: Varying the number of experts changes the model's capacity to specialize and the diversity of learned representations. More experts can capture finer-grained distinctions but may also lead to overfitting or inefficient routing.
- Core assumption: Different retrieval tasks and datasets benefit from different levels of expert specialization.
- Evidence anchors:
  - [abstract] "The study also highlights that the number of experts in SB-MoE is a critical hyperparameter requiring task- and domain-specific tuning"
  - [section] "Our investigation revealed that performance gains with different numbers of experts vary depending on the dataset"
  - [corpus] Weak evidence - corpus lacks specific evidence for optimal expert counts across different datasets
- Break condition: If the dataset is too small relative to the number of experts, or if the routing function cannot effectively utilize the available experts.

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: Understanding how multiple expert sub-networks and gating functions work together to create dynamic routing capabilities
  - Quick check question: How does the gating function determine which experts to activate for a given input?

- Concept: Dense Retrieval Models (DRMs) and bi-encoder architecture
  - Why needed here: SB-MoE builds upon existing DRM architectures, specifically modifying the output of the final Transformer layer
  - Quick check question: What is the role of the final Transformer layer in traditional DRM architectures?

- Concept: Contrastive learning and embedding similarity
  - Why needed here: DRMs use contrastive learning objectives and cosine similarity for retrieval, which SB-MoE must preserve while adding expert specialization
  - Quick check question: How does contrastive learning help DRMs learn meaningful query-document representations?

## Architecture Onboarding

- Component map: Query/document embeddings from final Transformer layer -> SB-MoE block (multiple expert FFN pairs + gating function) -> Pooled expert outputs -> Refined embeddings for similarity computation

- Critical path:
  1. Generate query and document embeddings using base DRM
  2. Route each embedding through SB-MoE experts
  3. Apply gating function to determine expert contributions
  4. Pool expert outputs using TOP-1 or ALL strategy
  5. Compute similarity scores between query and document embeddings
  6. Calculate contrastive loss and update all parameters

- Design tradeoffs:
  - Number of experts vs. model complexity: More experts increase specialization but also computational cost
  - TOP-1 vs. ALL pooling: TOP-1 is more efficient but may miss useful expert contributions; ALL captures more information but is computationally heavier
  - Expert capacity vs. gating function complexity: Deeper experts or gating functions increase representational power but also training difficulty

- Failure signatures:
  - Gating function collapse: All inputs route to the same expert, indicating poor gating function training
  - Expert redundancy: Multiple experts produce nearly identical outputs, suggesting insufficient diversity in expert design
  - Training instability: Large fluctuations in validation loss during training indicate issues with the MoE architecture

- First 3 experiments:
  1. Implement SB-MoE with 2 experts on TinyBERT using TOP-1 pooling, compare to baseline on NQ dataset
  2. Vary the number of experts (2, 4, 6, 8) on TinyBERT, measure impact on NDCG@10 and R@100
  3. Implement ALL pooling strategy on TinyBERT with 6 experts, compare to TOP-1 pooling results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of MoE with sparse retrieval models compare to its use with dense retrieval models?
- Basis in paper: [inferred] The paper focuses on dense retrieval models (DRMs) and does not explore the potential benefits of MoE for sparse retrieval models.
- Why unresolved: The study exclusively examines MoE's impact on dense retrieval, leaving open the question of whether similar improvements could be achieved with sparse models.
- What evidence would resolve it: Comparative experiments applying MoE to both dense and sparse retrieval models across the same benchmarks.

### Open Question 2
- Question: What are the computational trade-offs between using MoE in lightweight models versus large models?
- Basis in paper: [explicit] The paper notes that MoE integration significantly benefits lightweight models like TinyBERT, but only marginally improves larger models like BERT and Contriever.
- Why unresolved: While the paper highlights the performance differences, it does not provide a detailed analysis of the computational costs or resource efficiency of MoE in different model sizes.
- What evidence would resolve it: A comprehensive study measuring computational overhead, training time, and inference efficiency for MoE across models of varying sizes.

### Open Question 3
- Question: How does the choice of pooling strategy (Top-1 vs. ALL) affect the performance of MoE in different retrieval tasks?
- Basis in paper: [explicit] The paper mentions two pooling strategies (Top-1 and ALL) but does not deeply explore their impact across different tasks or datasets.
- Why unresolved: The paper provides limited insights into how pooling strategies influence MoE's effectiveness in varying retrieval scenarios.
- What evidence would resolve it: Detailed experiments comparing the two pooling strategies across diverse tasks and datasets to identify optimal use cases.

### Open Question 4
- Question: What are the domain-specific effects of MoE integration on retrieval performance?
- Basis in paper: [explicit] The paper uses datasets from different domains (e.g., Natural Questions, HotpotQA, Political Science, and Computer Science) but does not thoroughly analyze domain-specific impacts.
- Why unresolved: While the paper shows general performance improvements, it does not investigate how MoE performs in specific domains or under domain-specific retrieval challenges.
- What evidence would resolve it: A focused study analyzing MoE's effectiveness across a broader range of domains and tasks, with domain-specific optimizations.

## Limitations
- The paper lacks specific implementation details for the contrastive loss function and gating mechanism, which are critical for faithful reproduction
- No ablation studies on the impact of different pooling strategies or gating function designs
- Limited guidance on how to tune expert counts effectively across different domains

## Confidence
- **High confidence**: Claims about SB-MoE improving performance for smaller DRMs (TinyBERT) are well-supported by experimental results across multiple benchmarks
- **Medium confidence**: Claims about diminishing returns for larger DRMs (BERT and Contriever) are supported but could benefit from more extensive analysis across different dataset sizes
- **Medium confidence**: Claims about expert count being a critical hyperparameter are supported but lack specific guidance on how to tune this parameter effectively

## Next Checks
1. Implement ablation studies comparing TOP-1 vs ALL pooling strategies on TinyBERT across all four benchmarks to quantify the impact of pooling choice
2. Conduct experiments varying the gating function complexity (number of layers, activation functions) to determine its effect on expert routing quality
3. Test SB-MoE on additional small-scale DRMs beyond TinyBERT to verify whether the observed performance improvements generalize to other compact models