---
ver: rpa2
title: Can multiple-choice questions really be useful in detecting the abilities of
  LLMs?
arxiv_id: '2403.17752'
source_url: https://arxiv.org/abs/2403.17752
tags:
- mcqs
- llms
- lfgqs
- answers
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates whether multiple-choice questions (MCQs) can
  accurately assess large language models (LLMs) by comparing them with long-form
  generation questions (LFGQs) across four datasets in Chinese and English. The authors
  identify that LLMs are highly sensitive to the order of answer options in MCQs,
  with accuracy gaps up to 0.116 and strong positional preferences for the first answer
  option.
---

# Can multiple-choice questions really be useful in detecting the abilities of LLMs?

## Quick Facts
- arXiv ID: 2403.17752
- Source URL: https://arxiv.org/abs/2403.17752
- Authors: Wangyue Li; Liangzhi Li; Tong Xiang; Xiao Liu; Wei Deng; Noa Garcia
- Reference count: 0
- Key outcome: MCQs yield higher but potentially inflated accuracy compared to LFGQs, with significant order sensitivity and lower calibration reliability

## Executive Summary
This study evaluates whether multiple-choice questions (MCQs) can accurately assess large language models (LLMs) by comparing them with long-form generation questions (LFGQs) across four datasets in Chinese and English. The authors identify that LLMs are highly sensitive to the order of answer options in MCQs, with accuracy gaps up to 0.116 and strong positional preferences for the first answer option. They find that MCQ and LFGQ answers for the same questions have low correlation (0.33-0.7 Pearson), with MCQs yielding higher but potentially inflated accuracy. The expected calibration error (ECE) is higher for MCQs (0.426) than LFGQs, indicating MCQs produce less reliable confidence estimates. Additionally, MCQ and LFGQ embeddings show clear separation in initial layers of the model's hidden states. The findings challenge the assumption that higher consistency implies better accuracy and suggest that LFGQs are more aligned with real-world use cases for evaluating LLMs.

## Method Summary
The study compares MCQs and LFGQs across nine LLMs (including GPT-3.5-turbo, GPT-4, ChatGLM-6B, StableLM-Tuned-Alpha, RedPajama-INCITE, Llama-2, Dolly-v2, Vicuna-7b, and Open-llama) using four datasets (CARE-MI, M3KE, ARC, MATH). The authors evaluate order sensitivity by rearranging answer options, measure consistency and accuracy across formats, compute expected calibration error (ECE), and analyze embedding space separation using t-SNE visualization of hidden states. Human evaluation is used for LFGQ answers while MCQ answers are automatically evaluated.

## Key Results
- LLMs exhibit significant order sensitivity in MCQs, with accuracy gaps up to 0.116 when answer options are rearranged
- MCQs and LFGQs for the same questions show low correlation (0.33-0.7 Pearson), challenging the assumption that higher consistency implies better accuracy
- MCQs have higher expected calibration error (0.426) than LFGQs, indicating less reliable confidence estimates
- MCQ and LFGQ embeddings show clear separation in initial layers of hidden states, suggesting different processing mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs are sensitive to the order of answer options in MCQs, with significant accuracy gaps when options are rearranged.
- Mechanism: The model's attention mechanisms and token probability distributions are influenced by the positional arrangement of candidate answers, leading to systematic preference for certain positions (particularly the first position).
- Core assumption: The model's internal representation and token selection process are affected by the relative positioning of answer options, not just their semantic content.
- Evidence anchors:
  - [abstract] "LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position"
  - [section 3.1] "The rearrangement of options makes LLMs output different answers" with chi-squared tests showing significant differences (p-values < 0.05)
  - [corpus] Weak - corpus contains related work on MCQs but no direct evidence about positional sensitivity mechanisms

### Mechanism 2
- Claim: MCQs and LFGQs produce different confidence calibration, with MCQs showing higher expected calibration error.
- Mechanism: The MCQ format with multiple candidate options creates different token probability distributions compared to LFGQs, leading to overconfidence in MCQ predictions. The presence of distractors affects the model's confidence calibration differently than open-ended generation.
- Core assumption: The confidence estimation from token logits is meaningfully different between multiple-choice and long-form generation tasks, and this difference reflects true calibration differences rather than just format artifacts.
- Evidence anchors:
  - [abstract] "MCQs to be less reliable than LFGQs in terms of expected calibration error" with ECE values (0.426 for MCQs vs lower for LFGQs)
  - [section 4.2] "LLMs operating on MCQs exhibit the poorest calibration and highest ECE compared to the other two formats"
  - [corpus] Weak - corpus contains related work on MCQs but no direct evidence about calibration differences between formats

### Mechanism 3
- Claim: MCQs and LFGQs produce distinct embeddings in early model layers, with separation decreasing in later layers.
- Mechanism: The different input formats (multiple choice vs. open-ended) create different attention patterns and hidden state representations, particularly in early layers where positional information is more prominent. The model processes the structural differences between formats differently at different depth levels.
- Core assumption: The hidden state embeddings reflect meaningful differences in how the model processes different question formats, and these differences are observable through t-SNE visualization of layer activations.
- Evidence anchors:
  - [abstract] "MCQ and LFGQ embeddings show clear separation in initial layers of the model's hidden states"
  - [section 4.3] "The embeddings from MCQs and LFGQs display clear separations in some layers of the hidden states" with t-SNE visualizations showing this pattern
  - [corpus] Weak - corpus contains related work on MCQs but no direct evidence about embedding space differences

## Foundational Learning

- Concept: Statistical significance testing (chi-squared test)
  - Why needed here: To determine whether observed differences in LLM outputs between different MCQ option orders are statistically meaningful rather than random variation
  - Quick check question: If you observe 100 samples with 40 choosing option A and 20 choosing option B in one arrangement, and 30 choosing option A and 30 choosing option B in another arrangement, what chi-squared statistic would you calculate to test if these distributions differ significantly?

- Concept: Expected Calibration Error (ECE)
  - Why needed here: To quantitatively measure how well the model's confidence estimates align with actual accuracy across different question formats
  - Quick check question: If a model predicts 100 samples with confidences distributed across bins, and in bin [0.8, 0.9] it has average confidence 0.85 but actual accuracy 0.70, how would this contribute to the overall ECE calculation?

- Concept: t-SNE visualization of high-dimensional embeddings
  - Why needed here: To visualize and understand the separation patterns between MCQ and LFGQ embeddings across different model layers
  - Quick check question: If two classes of data points are clearly separated in 2D t-SNE space but overlap significantly in the original high-dimensional space, what does this tell you about the information captured by t-SNE for your specific use case?

## Architecture Onboarding

- Component map: Data preprocessing (dataset loading, format conversion, option reordering) -> Model interface (LLM API calls, token logit extraction) -> Evaluation pipeline (accuracy calculation, consistency metrics, ECE computation) -> Analysis tools (statistical tests, correlation calculations, t-SNE visualization, reliability diagrams)
- Critical path: For order sensitivity experiments → option reordering → LLM inference → accuracy calculation → statistical testing; For format comparison → unified confidence calculation → ECE computation → embedding extraction → visualization
- Design tradeoffs: Using MCQs allows for automated evaluation but introduces positional bias; LFGQs are more realistic but require human evaluation; token logit analysis requires models with specific capabilities; embedding analysis requires access to internal model states
- Failure signatures: High variance in order sensitivity across datasets suggests dataset-specific issues; inconsistent ECE results across formats suggests evaluation methodology problems; embedding separation patterns that don't match behavioral differences suggest visualization artifacts
- First 3 experiments:
  1. Replicate order sensitivity test on a small subset (50-100 samples) with two LLMs to verify basic phenomenon before scaling up
  2. Test ECE calculation on a simple binary classification task to verify implementation before applying to MCQs vs LFGQs
  3. Extract and visualize embeddings from a single layer of one model to verify t-SNE pipeline works before analyzing all layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do positional biases in MCQs affect evaluation consistency across different domains beyond medical and educational?
- Basis in paper: [explicit] The paper shows LLMs have strong positional preferences for the first answer option in Chinese and English MCQs, with accuracy gaps up to 0.116, and questions whether higher consistency implies better accuracy
- Why unresolved: The experiments only tested four datasets (CARE-MI, M3KE, ARC, MATH) focused on specific domains. The paper doesn't explore whether positional biases manifest differently across diverse domains like legal, creative writing, or technical domains
- What evidence would resolve it: Experiments testing the same LLM models on MCQs across 10+ diverse domains (legal, creative, technical, scientific, humanities) while measuring positional preference patterns and accuracy gaps for each domain

### Open Question 2
- Question: Does the embedding space separation between MCQs and LFGQs persist when questions are rephrased or presented in different formats while maintaining semantic equivalence?
- Basis in paper: [inferred] The paper finds clear separation between MCQ and LFGQ embeddings in initial hidden layers, suggesting format-dependent processing, but doesn't test whether semantic equivalence across formats affects this separation
- Why unresolved: The experiments only compare original MCQ and LFGQ formats for the same questions. There's no investigation into whether transforming questions into different phrasings or alternative formats while preserving meaning changes the embedding separation pattern
- What evidence would resolve it: Experiments measuring embedding similarity across multiple rephrasings and format transformations of the same questions, comparing MCQ→MCQ, MCQ→LFGQ, and LFGQ→LFGQ transformations while tracking embedding separation patterns

### Open Question 3
- Question: Can calibration error differences between MCQs and LFGQs be mitigated through specific prompt engineering or instruction-tuning strategies?
- Basis in paper: [explicit] The paper finds MCQs have significantly higher expected calibration error (0.426) than LFGQs, indicating less reliable confidence estimates, but doesn't explore mitigation strategies
- Why unresolved: The experiments use standard prompt designs without optimization for calibration. The paper identifies the problem but doesn't test whether different prompting strategies, chain-of-thought reasoning, or instruction-tuning approaches could reduce the calibration gap
- What evidence would resolve it: Experiments comparing calibration error across multiple prompt engineering strategies (chain-of-thought, self-consistency, calibrated prompting) and instruction-tuned variants of the same models, measuring whether any approach reduces the MCQ vs LFGQ calibration gap

## Limitations
- The study relies on human evaluation for LFGQ answers, introducing potential subjectivity and inter-rater variability that could affect format comparisons
- Dataset-specific effects are observed, with order sensitivity findings based primarily on Chinese datasets while embedding analysis includes English datasets
- Embedding space analysis is conducted on only one model (Vicuna-7b), limiting generalizability across different model architectures

## Confidence
- High confidence: Order sensitivity finding (LLMs favoring first position with accuracy gaps up to 0.116)
- Medium confidence: Low correlation between MCQ and LFGQ answers (0.33-0.7 Pearson)
- Medium confidence: ECE comparison between formats (0.426 for MCQs vs lower for LFGQs)

## Next Checks
1. Replicate order sensitivity across languages: Test the positional preference phenomenon on English datasets (ARC, MATH) using the same methodology to verify cross-lingual consistency
2. Standardize confidence calculation: Implement a unified confidence estimation framework for both MCQ and LFGQ formats to ensure fair ECE comparison, particularly for open-ended responses
3. Multi-model embedding analysis: Extend the embedding space analysis to at least 3-4 additional models with different architectures to verify that MCQ/LFGQ separation patterns are consistent across model families