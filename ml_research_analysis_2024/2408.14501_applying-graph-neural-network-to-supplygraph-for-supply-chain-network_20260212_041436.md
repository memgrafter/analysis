---
ver: rpa2
title: Applying graph neural network to SupplyGraph for supply chain network
arxiv_id: '2408.14501'
source_url: https://arxiv.org/abs/2408.14501
tags:
- graph
- supply
- chain
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated supply chain dataset, SupplyGraph, with improved
  clarity on data quality assurance, ML model specifications, and statistical analyses
  of model performance. After ensuring data quality, the study compared Multilayer
  Perceptrons (MLP), Graph Convolutional Networks (GCN), and Graph Attention Networks
  (GAT) on demand forecasting task using SupplyGraph.
---

# Applying graph neural network to SupplyGraph for supply chain network

## Quick Facts
- arXiv ID: 2408.14501
- Source URL: https://arxiv.org/abs/2408.14501
- Authors: Kihwan Han
- Reference count: 7
- Primary result: GAT outperformed GCN and MLP in supply chain demand forecasting with statistically significant improvements (α = 0.05)

## Executive Summary
This study evaluates supply chain dataset SupplyGraph with improved clarity on data quality assurance, ML model specifications, and statistical analyses of model performance. After ensuring data quality, the study compares Multilayer Perceptrons (MLP), Graph Convolutional Networks (GCN), and Graph Attention Networks (GAT) on demand forecasting task using SupplyGraph. GAT performed best, followed by GCN and MLP, with statistically significant improvements (α = 0.05, corrected for multiple comparisons). The results demonstrated that GNN incorporating supply chain graph outperformed models without it, reinforcing the value of SupplyGraph as a benchmark dataset for GNN applications in supply chain planning.

## Method Summary
The study uses SupplyGraph dataset with 40 products and plant edges, preprocesses data with z-score normalization and sequence windowing (size 5), and splits data 95/5 train/test. Three models are implemented: MLP (2 layers, 8 neurons), GCN (2 layers, 8 neurons), and GAT (2 layers, 4 neurons with 6 attention heads). All models are trained for 200 epochs with Adam optimizer (lr=0.001, wd=5e-4, MSE loss). Model performance is evaluated using MSE metrics and statistical significance is assessed using Kruskal-Wallis H test and Wilcoxon-Mann-Whitney U tests with Bonferroni correction on squared errors.

## Key Results
- GAT achieved the lowest MSE (0.00046) in demand forecasting, followed by GCN (0.00052) and MLP (0.00061)
- Statistical analyses confirmed significant differences between model performances (α = 0.05, corrected for multiple comparisons)
- GNN models incorporating supply chain graph structure outperformed MLP baseline, validating the value of graph-based approaches for supply chain planning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GAT outperforms MLP and GCN in demand forecasting by leveraging attention weights to capture product-specific dependencies in the supply chain network.
- Mechanism: GAT computes node-specific attention coefficients for aggregating neighbor features, allowing the model to weigh more important relationships more heavily than GCN's uniform averaging.
- Core assumption: The supply chain demand patterns are better represented by heterogeneous product relationships rather than simple averaging of neighbor features.
- Evidence anchors:
  - [abstract]: "GAT performed best, followed by GCN and MLP" and "GNN incorporating supply chain graph outperformed models without it"
  - [section 2.5]: "GAT [Veličković et al., 2018] applies node-specific attention weights prior to the aggregation step, in addition to layer-specific weights"
  - [corpus]: Weak evidence - no direct support for attention mechanism advantage in supply chain context
- Break condition: If supply chain relationships are uniform across products, attention mechanisms provide no advantage over GCN.

### Mechanism 2
- Claim: Incorporating supply chain graph structure improves demand forecasting by capturing temporal correlations across products.
- Mechanism: GNN models (GCN and GAT) use the plant edges to propagate demand information between related products, capturing how demand fluctuations in one product affect others in the network.
- Core assumption: Demand for products in the supply chain is not independent and exhibits correlation patterns that can be captured through graph structure.
- Evidence anchors:
  - [abstract]: "GNN incorporating supply chain graph outperformed models without it"
  - [section 2.1]: "Supply chain data are inherently under graph structure" and "SupplyGraph dataset has great potential to serve as a benchmark dataset for ML-based supply chain use cases"
  - [section 4]: "the previous study found temporal correlations across product demands [Wasi et al., 2024]"
- Break condition: If product demands are truly independent or if the graph edges don't represent meaningful relationships, GNN provides no advantage over MLP.

### Mechanism 3
- Claim: Statistical analyses with corrected multiple comparisons provide more convincing evidence of GNN superiority than simple average metrics.
- Mechanism: Using Kruskal-Wallis H test and Wilcoxon-Mann-Whitney U tests with Bonferroni correction accounts for the non-normal distribution of errors and reduces false positive risk in pairwise comparisons.
- Core assumption: The distribution of prediction errors is non-normal and contains multiple comparisons that require correction for valid inference.
- Evidence anchors:
  - [abstract]: "performing statistical analyses with actual distribution of errors rather than showing the average value of the errors"
  - [section 2.6]: "First, an omnibus test on the median of SE was performed using Kruskal-Wallis H test. Second, pairwise tests on the median of SE for MLP vs GCN and GCN vs GAT were performed using Wilcoxon-Mann-Whitney U test. Statistical significance was determined at alpha level of 0.05. For the pairwise tests, p-values were corrected for multiple comparisons using Bonferroni correction."
  - [corpus]: Weak evidence - no direct support for statistical methodology in supply chain GNN studies
- Break condition: If error distributions are approximately normal or if only one comparison is made, simpler statistical tests would suffice.

## Foundational Learning

- Concept: Graph Neural Networks (GNN) basics - understanding how GNNs aggregate neighbor information through message passing
  - Why needed here: The study compares three different GNN approaches (MLP as baseline, GCN, and GAT) for supply chain demand forecasting
  - Quick check question: What is the key difference between how GCN and GAT aggregate neighbor information?

- Concept: Statistical hypothesis testing and multiple comparison correction - understanding when and why to use non-parametric tests and Bonferroni correction
  - Why needed here: The study uses Kruskal-Wallis and Wilcoxon-Mann-Whitney tests with Bonferroni correction to compare model performance
  - Quick check question: When would you use a non-parametric test instead of a t-test for comparing model performance?

- Concept: Supply chain network structure and graph representation - understanding how products, facilities, and relationships are modeled as nodes and edges
  - Why needed here: The SupplyGraph dataset represents supply chain as a graph with products as nodes and plant relationships as edges
  - Quick check question: What type of graph (homogeneous/heterogeneous, directed/undirected) best represents a supply chain with different entity types?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training -> Statistical analysis -> Result interpretation
- Critical path: Data preprocessing → Model training → Statistical analysis → Result interpretation
- Design tradeoffs: Matched hyperparameters across models vs. extensive hyperparameter tuning; simple homogeneous graph vs. complex heterogeneous graph
- Failure signatures: Overfitting (training loss << test loss), underfitting (high loss on both), poor convergence (loss plateaus early)
- First 3 experiments:
  1. Train all three models with default settings and compare learning curves to ensure proper convergence
  2. Perform statistical tests on squared errors to confirm significant differences between models
  3. Visualize predictions vs. actual values for sample products to qualitatively assess model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific product demand patterns are most strongly correlated with supply chain network structure in SupplyGraph?
- Basis in paper: [inferred] The paper suggests that supply chain demand fluctuates according to supply chain network and GNN approaches utilized such patterns manifested from the supply chain network, but does not specify which patterns.
- Why unresolved: The study only demonstrates that GNN approaches outperform MLP in demand forecasting without identifying the specific network-driven demand patterns that contribute to this improvement.
- What evidence would resolve it: Detailed analysis correlating specific network topological features (e.g., centrality measures, clustering coefficients) with demand variability patterns across different product groups.

### Open Question 2
- Question: How does the choice of graph representation (homogeneous vs heterogeneous) affect model performance across different supply chain use cases?
- Basis in paper: [explicit] The paper discusses multiple options for building supply graphs including homogeneous vs heterogeneous graphs, but only evaluates one use case with a homogeneous graph.
- Why unresolved: The study only explored one graph type (homogeneous plant edges) for demand forecasting, leaving open the question of how different graph representations would perform for other use cases.
- What evidence would resolve it: Comparative evaluation of GNN models using different graph representations (homogeneous vs heterogeneous) across multiple supply chain use cases (e.g., inventory optimization, logistics planning).

### Open Question 3
- Question: What is the minimum data volume required for GNN models to consistently outperform conventional ML models in supply chain applications?
- Basis in paper: [inferred] The paper notes that the dataset size was small (29 nodes, 221 time points) and that model performance could be improved with more data, suggesting a data volume dependency.
- Why unresolved: The study used a fixed small dataset without exploring how performance scales with increasing data volume, leaving unclear the data requirements for GNN superiority.
- What evidence would resolve it: Systematic evaluation of GNN vs conventional ML model performance across datasets of varying sizes, identifying the threshold where GNN consistently outperforms.

## Limitations
- The study demonstrates GAT superiority over GCN and MLP but doesn't explore hyperparameter optimization, which could potentially narrow or eliminate performance gaps between models
- Statistical significance is established, but practical significance (effect size) is not quantified - the actual magnitude of improvement from GNN vs MLP remains unclear
- The homogeneous graph assumption may oversimplify supply chain complexity, as real networks involve multiple entity types (products, facilities, suppliers) that could benefit from heterogeneous graph modeling

## Confidence
- **High Confidence**: GAT outperforming GCN and MLP, statistical methodology using non-parametric tests with correction, data quality assurance procedures
- **Medium Confidence**: GNN incorporating supply chain graph structure improves performance, attention mechanism advantage for supply chain demand forecasting
- **Low Confidence**: Mechanism 3 (statistical advantage claim) - while methodology is sound, the practical impact on research conclusions is uncertain

## Next Checks
1. **Effect Size Analysis**: Calculate and report Cohen's d or similar effect size metrics to quantify practical significance of performance differences between models
2. **Heterogeneous Graph Implementation**: Reimplement the models using heterogeneous graph structures to test if incorporating different entity types improves performance beyond the homogeneous approach
3. **Hyperparameter Sensitivity**: Perform grid search or Bayesian optimization on key hyperparameters (learning rate, hidden layer size, attention heads) to determine if current performance gaps persist under optimal configurations