---
ver: rpa2
title: 'Annotation Guidelines-Based Knowledge Augmentation: Towards Enhancing Large
  Language Models for Educational Text Classification'
arxiv_id: '2406.00954'
source_url: https://arxiv.org/abs/2406.00954
tags:
- llms
- classification
- learning
- agka
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes AGKA, a method that enhances LLMs for educational
  text classification by incorporating label definition knowledge from annotation
  guidelines and selecting typical few-shot examples. AGKA improves weighted F1 scores
  by up to 8.48% on six LEC datasets covering behavior, emotion, and cognition classification
  tasks.
---

# Annotation Guidelines-Based Knowledge Augmentation: Towards Enhancing Large Language Models for Educational Text Classification

## Quick Facts
- arXiv ID: 2406.00954
- Source URL: https://arxiv.org/abs/2406.00954
- Reference count: 40
- LLMs with AGKA improve weighted F1 scores by up to 8.48% on educational text classification tasks

## Executive Summary
This study introduces AGKA, a method that enhances large language models (LLMs) for educational text classification by incorporating label definition knowledge from annotation guidelines and selecting balanced few-shot examples. AGKA addresses the challenge of ambiguous label semantics in educational text classification by retrieving precise definitions from annotation guidelines and combining them with randomly under-sampled few-shot examples. The method demonstrates significant improvements over vanilla prompting, particularly for GPT 4.0 and Llama 3 70B, with GPT 4.0 few-shot outperforming full-shot fine-tuned models on simple binary classification tasks.

## Method Summary
AGKA enhances non-fine-tuned LLMs for educational text classification through a three-step approach: first, it uses GPT 4.0 to retrieve label definition knowledge from annotation guidelines; second, it defines the task with a list of labels and their definitions; and third, it applies random under-sampling to select a few typical examples for each class. The method is evaluated across six LEC datasets covering behavior, emotion, and cognition classification tasks, comparing non-fine-tuned LLMs (GPT 3.5, GPT 4.0, Llama 3 70B, Mistral 7B, Mixtral 8x7B, Llama 3 8B) against fine-tuned models (BERT, RoBERTa) using weighted F1 scores as the primary metric.

## Key Results
- AGKA improves weighted F1 scores by up to 8.48% compared to vanilla prompting on six LEC datasets
- GPT 4.0 with AGKA few-shot outperforms full-shot fine-tuned models like BERT and RoBERTa on simple binary classification tasks
- Llama 3 70B with AGKA performs on par with GPT 4.0, demonstrating the potential of open-source models
- LLMs still struggle with multi-class tasks requiring deep semantic understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AGKA improves LLM performance by providing task-specific label definitions retrieved from annotation guidelines.
- Mechanism: The system queries GPT 4.0 to extract precise definitions for each label in the classification task, then formats them into a Python dictionary. These definitions are inserted into the prompt alongside the label list, enabling the LLM to distinguish between semantically similar labels (e.g., "Curiosity" vs "Confusion").
- Core assumption: Label names alone are ambiguous; explicit definitions clarify intent and semantics for the LLM.
- Evidence anchors:
  - [abstract] "AGKA employs GPT 4.0 to retrieve label definition knowledge from annotation guidelines"
  - [section] "In the context of LEC, the use of annotation guidelines helps to capture the subtle nuances between labels such as 'Curiosity' and 'Confusion,' which may be challenging to distinguish based on the label names alone."
  - [corpus] Weak - no direct neighbor papers discuss annotation guideline retrieval for LLMs.
- Break condition: If annotation guidelines are unavailable or poorly defined, the retrieval step fails and the LLM reverts to ambiguity.

### Mechanism 2
- Claim: Few-shot sampling with random under-sampling (RUS) improves class balance and reduces overfitting to majority classes.
- Mechanism: RUS selects a small, balanced set of typical <text, label> pairs for each class. This ensures the LLM sees representative examples from minority classes, preventing bias toward majority labels.
- Core assumption: Class imbalance in training data leads to skewed predictions; balanced few-shot examples mitigate this.
- Evidence anchors:
  - [abstract] "AGKA employs GPT 4.0 to retrieve label definition knowledge from annotation guidelines, and then applies the random under-sampler to select a few typical examples."
  - [section] "By ensuring that the model learns on a more balanced set of examples, RUS helps reduce the bias toward the majority class, leading to better generalization to the minority class."
  - [corpus] Weak - no neighbor papers explicitly cover RUS in LLM few-shot settings.
- Break condition: If the dataset is extremely imbalanced or too small, RUS may fail to sample enough examples for minority classes.

### Mechanism 3
- Claim: Combining label definitions and few-shot examples outperforms either technique alone.
- Mechanism: Label definitions provide semantic grounding, while few-shot examples offer contextual cues. Together, they guide the LLM more effectively than zero-shot or single-source augmentation.
- Core assumption: Semantic clarity plus contextual examples jointly reduce prediction uncertainty.
- Evidence anchors:
  - [abstract] "The study results demonstrate that AGKA can enhance non-fine-tuned LLMs, particularly GPT 4.0 and Llama 3 70B."
  - [section] "LLMs with AGKA consistently outperform LLMs with vanilla prompt."
  - [corpus] Weak - no neighbor papers compare combined vs. single-source augmentation in LLM prompting.
- Break condition: If the LLM already has strong zero-shot capability, added few-shot examples may not yield further gains.

## Foundational Learning

- Concept: Prompt engineering in LLMs
  - Why needed here: AGKA relies on carefully structured prompts that include task claims, label definitions, output format, and few-shot examples. Without understanding prompt structure, one cannot replicate or modify AGKA.
  - Quick check question: What are the four components of an AGKA prompt?

- Concept: Annotation guidelines and label semantics
  - Why needed here: AGKA retrieves definitions from annotation guidelines to disambiguate labels. Understanding how guidelines define labels is critical for correct retrieval and prompt formulation.
  - Quick check question: Why does AGKA need to retrieve label definitions instead of using label names alone?

- Concept: Random under-sampling (RUS) for class balancing
  - Why needed here: RUS ensures few-shot examples are balanced across classes, preventing majority-class bias. Without this, few-shot learning may reinforce existing imbalances.
  - Quick check question: What problem does RUS solve in the context of few-shot LLM prompting?

## Architecture Onboarding

- Component map:
  Annotation guidelines source -> GPT 4.0 retrieval -> Label definition dictionary
  Dataset -> RUS sampling -> Few-shot examples
  Prompt assembly: task claim + label definitions + output format + few-shot + test text
  LLM inference -> Prediction
  Evaluation -> F1/Accuracy metrics

- Critical path:
  1. Retrieve label definitions via GPT 4.0.
  2. Sample balanced few-shot examples with RUS.
  3. Assemble AGKA prompt.
  4. Run inference on test set.
  5. Evaluate performance.

- Design tradeoffs:
  - Retrieval step adds latency and dependency on GPT 4.0; caching definitions could help.
  - Few-shot size balances prompt token limits vs. example coverage.
  - Closed-source LLMs (GPT 4.0) may outperform open-source (Llama 3 70B) but cost more and raise privacy concerns.

- Failure signatures:
  - Poor F1 despite AGKA -> label definitions incomplete or misaligned with task.
  - High variance across runs -> RUS sampling too small or dataset too noisy.
  - Prompt exceeds token limit -> reduce few-shot size or simplify definitions.

- First 3 experiments:
  1. Run AGKA vs. vanilla prompt on a binary classification task (e.g., urgency) to verify baseline gain.
  2. Vary few-shot size [1,5,10] to find optimal trade-off between performance and token usage.
  3. Compare AGKA on open-source vs. closed-source LLMs to quantify performance gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs with AGKA compare to human annotators on the LEC tasks?
- Basis in paper: [explicit] The paper states that ChatGPT outperforms crowd workers in several annotation tasks, but does not directly compare LLMs with AGKA to human annotators on LEC tasks.
- Why unresolved: The paper focuses on comparing LLMs with AGKA to other models, but does not include a direct comparison with human annotators.
- What evidence would resolve it: Conducting a study that compares the performance of LLMs with AGKA to human annotators on the same LEC tasks would provide evidence to answer this question.

### Open Question 2
- Question: How does the effectiveness of AGKA vary across different educational domains (e.g., STEM vs. humanities)?
- Basis in paper: [inferred] The paper mentions that the datasets cover various disciplines, but does not analyze the performance of AGKA across different domains.
- Why unresolved: The paper does not provide a domain-specific analysis of AGKA's effectiveness.
- What evidence would resolve it: Conducting a study that evaluates the performance of AGKA on LEC tasks from different educational domains would provide evidence to answer this question.

### Open Question 3
- Question: What is the impact of different annotation guideline formats on the performance of LLMs with AGKA?
- Basis in paper: [inferred] The paper uses annotation guidelines from source papers but does not investigate the impact of different guideline formats on LLM performance.
- Why unresolved: The paper does not explore how variations in annotation guideline structure or content affect the performance of LLMs with AGKA.
- What evidence would resolve it: Conducting a study that tests LLMs with AGKA using annotation guidelines in different formats (e.g., structured vs. unstructured) would provide evidence to answer this question.

## Limitations

- AGKA shows reduced effectiveness on multi-class tasks requiring deep semantic understanding, suggesting limitations in handling complex classification scenarios
- The method's reliance on GPT 4.0 for knowledge retrieval raises questions about generalizability to other annotation guideline sources or LLMs
- Computational costs and latency implications of the GPT 4.0 retrieval step are not addressed, which could be significant in production settings

## Confidence

- **High confidence**: AGKA improves weighted F1 scores on the tested LEC datasets, particularly for binary classification tasks
- **Medium confidence**: AGKA outperforms full-shot fine-tuned models like BERT and RoBERTa on simple binary classification tasks
- **Low confidence**: Open-source models like Llama 3 70B can perform on par with GPT 4.0 using AGKA

## Next Checks

1. **Cross-dataset generalization test**: Apply AGKA to educational text classification datasets outside the LEC benchmark (e.g., student feedback analysis, academic discourse classification) to verify if the 8.48% F1 improvement holds across different educational domains.

2. **Annotation guideline variability assessment**: Test AGKA with annotation guidelines from different sources or with varying levels of detail to determine the minimum quality threshold required for effective knowledge retrieval and whether the method degrades gracefully with incomplete guidelines.

3. **Computational efficiency evaluation**: Measure the latency and token costs of the GPT 4.0 retrieval step across different dataset sizes and compare the end-to-end inference time of AGKA-enhanced LLMs versus fine-tuned traditional models to assess practical deployment feasibility.