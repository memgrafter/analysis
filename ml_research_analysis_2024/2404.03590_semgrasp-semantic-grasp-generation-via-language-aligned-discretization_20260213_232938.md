---
ver: rpa2
title: 'SemGrasp: Semantic Grasp Generation via Language Aligned Discretization'
arxiv_id: '2404.03590'
source_url: https://arxiv.org/abs/2404.03590
tags:
- gid00001
- gid00068
- gid00083
- grasp
- gid00077
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SemGrasp, a method for generating semantic
  human grasps based on language instructions. It discretizes grasp poses into three
  tokens (orientation, manner, refinement) using a hierarchical VQ-VAE and trains
  a multimodal large language model to align these tokens with object features and
  language descriptions.
---

# SemGrasp: Semantic Grasp Generation via Language Aligned Discretization

## Quick Facts
- arXiv ID: 2404.03590
- Source URL: https://arxiv.org/abs/2404.03590
- Authors: Kailin Li; Jingbo Wang; Lixin Yang; Cewu Lu; Bo Dai
- Reference count: 40
- Generates semantically consistent grasps from language instructions with 74.5 GPT-4 evaluation score

## Executive Summary
This paper proposes SemGrasp, a method for generating semantic human grasps from language instructions by aligning grasp space with semantic space through discrete representation and multimodal large language models. The approach discretizes grasp poses into three tokens (orientation, manner, refinement) using a hierarchical VQ-VAE and trains an MLLM to align these tokens with object features and language descriptions. A new CapGrasp dataset with detailed annotations is created to support training and evaluation.

## Method Summary
SemGrasp uses a hierarchical VQ-VAE to discretize continuous grasp parameters into three tokens, then fine-tunes a pretrained MLLM (Vicuna-7B) to predict grasp tokens from object features and language descriptions. The method involves training the VQ-VAE on grasp reconstruction, then using it as a tokenizer to discretize grasps into <o,m,r> tokens. The MLLM is fine-tuned in two stages: first for multimodal alignment, then for instruction tuning. The approach is evaluated on the CapGrasp dataset with both physical plausibility and semantic consistency metrics.

## Key Results
- Achieves GPT-4 assisted evaluation score of 74.5 on test set
- Perceptual score of 4.6/5.0 from human participants
- Outperforms baseline BERT approach and cVAE methods in both physical plausibility and semantic consistency metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Discretizing grasp poses into tokens (<o,m,r>) enables semantic alignment with language space through VQ-VAE.
- **Mechanism**: The VQ-VAE maps continuous grasp parameters (T,θ,β) into discrete codebook indices, allowing language-conditioned token prediction via MLLM.
- **Core assumption**: Grasp semantics can be factorized into three independent components: orientation, manner, and refinement.
- **Evidence anchors**:
  - [abstract] "We introduce a discrete representation that aligns the grasp space with semantic space"
  - [section 3.2] "we discretize the graspG into three components <o, m, r>, representing the orientation, manner, and refinement token"
- **Break condition**: If grasp semantics cannot be decomposed into three orthogonal components, the discrete representation loses semantic expressiveness.

### Mechanism 2
- **Claim**: Multimodal Large Language Models can predict grasp tokens from object features and language descriptions.
- **Mechanism**: The MLLM takes projected object features and language tokens, trained to autoregressively predict grasp tokens in unified semantic space.
- **Core assumption**: LLMs can learn cross-modal alignment between language, object geometry, and grasp semantics.
- **Evidence anchors**:
  - [abstract] "A Multimodal Large Language Model (MLLM) is subsequently fine-tuned, integrating object, grasp, and language within a unified semantic space"
  - [section 3.3] "our model is trained to align three distinct modalities: the human grasp G, object points O, and the language description L"
- **Break condition**: If the MLLM fails to learn cross-modal alignment, it cannot generate semantically consistent grasps from language instructions.

### Mechanism 3
- **Claim**: The hierarchical VQ-VAE architecture with three codebooks provides better grasp representation than single-token or flat approaches.
- **Mechanism**: Separate encoders progressively map hand representation into latent space, capturing grasp information from low to high levels, with decoders reconstructing the grasp from three tokens.
- **Core assumption**: Hierarchical decomposition of grasp information improves reconstruction accuracy and semantic alignment.
- **Evidence anchors**:
  - [section 3.2] "The encoders progressively map the hand's representation into the latent space, capturing grasp information from low to high levels"
  - [section 4.4] "Hierarchical VQ-VAE specifically for grasp representation that is trained from scratch"
- **Break condition**: If hierarchical decomposition doesn't improve reconstruction compared to flat approaches, the added complexity is unjustified.

## Foundational Learning

- **Concept**: Variational Autoencoders and Vector Quantization
  - Why needed here: VQ-VAE is used to discretize continuous grasp parameters into tokens for semantic alignment
  - Quick check question: What is the difference between VAE and VQ-VAE in terms of latent space representation?

- **Concept**: Multimodal Large Language Models and Cross-Modal Alignment
  - Why needed here: MLLM learns to map object features and language into unified semantic space for grasp token prediction
  - Quick check question: How do MLLMs typically align different modalities into a shared semantic space?

- **Concept**: Grasp Taxonomy and Hand-Object Interaction
  - Why needed here: Understanding grasp types and semantic categories is crucial for designing the discrete representation
  - Quick check question: What are the main categories in human grasp taxonomy and how do they relate to manipulation intent?

## Architecture Onboarding

- **Component map**: VQ-VAE (3 encoders, 3 decoders, 3 codebooks) → MLLM (Llama backbone + LoRA) → Object feature extractor (PointBERT) → Grasp reconstruction pipeline

- **Critical path**: Object point cloud → PointBERT → Projected features → MLLM → Grasp tokens → VQ-VAE decoders → Final grasp pose

- **Design tradeoffs**: Discrete representation vs continuous (better semantic alignment but reconstruction loss), Hierarchical vs flat VQ-VAE (better decomposition but more complex), Pretrained MLLM vs from-scratch (better language understanding but larger model)

- **Failure signatures**: Poor reconstruction accuracy (VQ-VAE too coarse), Generated grasps don't match language (MLLM alignment failure), Training instability (LoRA rank too high/low, learning rate issues)

- **First 3 experiments**:
  1. Train VQ-VAE on grasp reconstruction only, evaluate MPVPE and PD metrics
  2. Train MLLM with fixed VQ-VAE to predict grasp tokens from language, evaluate token prediction accuracy
  3. End-to-end generation test with GPT-4 evaluation, compare to baseline BERT approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would SemGrasp's performance change if trained on real-world captured data instead of synthetic data?
- Basis in paper: [explicit] The authors mention that CapGrasp builds upon the OakInk dataset and extends it with automated annotations. They also note that OakInk includes a subset of real captured images (OakInk-image) which they use for high-level annotations.
- Why unresolved: The paper primarily uses synthetic data for training and evaluation. While they mention using real images for annotations, the training and testing is done on synthetic data. The impact of using real-world data on SemGrasp's performance remains unexplored.
- What evidence would resolve it: Training and evaluating SemGrasp on a dataset composed entirely of real-world captured hand-object interaction data, comparing performance metrics with the current synthetic-based approach.

### Open Question 2
- Question: What is the impact of using more granular grasp tokens (beyond the current three-token system) on SemGrasp's performance?
- Basis in paper: [explicit] The authors conduct ablation studies on their discrete representation, exploring different token configurations including a single token, two tokens, and multiple refinement tokens. They find that the three-token system (orientation, manner, refinement) performs best.
- Why unresolved: While they explore variations of their current system, they don't investigate using more than three tokens or different granularities of token representation. The optimal granularity for grasp representation remains an open question.
- What evidence would resolve it: Systematically increasing the number of tokens or changing their granularity (e.g., separate tokens for each finger, finer orientation categories) and measuring the impact on reconstruction accuracy, physical plausibility, and semantic consistency.

### Open Question 3
- Question: How does SemGrasp's performance degrade with out-of-distribution objects or language instructions?
- Basis in paper: [explicit] The authors evaluate on a test set that includes a wide variety of objects and language instructions, but they don't specifically test performance on objects or instructions that are significantly different from the training distribution.
- Why unresolved: The paper demonstrates strong performance on the test set but doesn't investigate the model's robustness to objects or language instructions that differ substantially from the training data. The limits of SemGrasp's generalization capabilities remain unclear.
- What evidence would resolve it: Testing SemGrasp on objects and language instructions that are intentionally selected to be dissimilar from the training set (e.g., unusual object shapes, complex or ambiguous language descriptions) and measuring performance degradation.

## Limitations

- The three-token factorization may not capture all nuances of human grasping behavior, particularly for complex or context-dependent grasps
- Evaluation relies heavily on GPT-4's judgment for semantic consistency, which may not align with human perception of grasp quality
- Dataset creation through GPT-4 annotations raises concerns about annotation quality and consistency without quantitative assessment

## Confidence

**High Confidence (Likelihood >80%):**
- The VQ-VAE successfully discretizes grasp poses into tokens with measurable reconstruction accuracy
- The MLLM can predict grasp tokens from object features and language descriptions
- The hierarchical VQ-VAE architecture improves reconstruction compared to flat approaches

**Medium Confidence (Likelihood 50-80%):**
- The discrete representation (o,m,r) effectively captures grasp semantics for language alignment
- The MLLM alignment produces semantically consistent grasps as measured by GPT-4 evaluation
- The CapGrasp dataset annotations are of sufficient quality for training

**Low Confidence (Likelihood <50%):**
- The generated grasps are functionally equivalent to human grasps in real-world scenarios
- The semantic consistency metrics fully capture grasp quality and usability
- The method generalizes well to novel objects and scenarios beyond the training distribution

## Next Checks

1. **Physical Plausibility Validation**: Conduct user studies with human participants to evaluate the functional quality of generated grasps on real objects, measuring task success rates and grasp stability.

2. **Cross-Domain Generalization Test**: Evaluate SemGrasp on objects and scenarios not present in the CapGrasp dataset, including objects with different geometries, materials, and functional requirements.

3. **Ablation Study on Discrete Representation**: Systematically vary the number of tokens and their semantic categories to determine the optimal factorization for grasp representation.