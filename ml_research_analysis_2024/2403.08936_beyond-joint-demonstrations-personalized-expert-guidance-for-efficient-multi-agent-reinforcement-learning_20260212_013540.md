---
ver: rpa2
title: 'Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent
  Reinforcement Learning'
arxiv_id: '2403.08936'
source_url: https://arxiv.org/abs/2403.08936
tags:
- demonstrations
- personalized
- pegmarl
- agent
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces PegMARL, a novel approach for Multi-Agent
  Reinforcement Learning (MARL) that leverages personalized expert demonstrations
  to guide learning. The key idea is to use two discriminators: one to encourage alignment
  with demonstrations and another to regulate incentives based on desired outcomes.'
---

# Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2403.08936
- Source URL: https://arxiv.org/abs/2403.08936
- Authors: Peihong Yu; Manav Mishra; Alec Koppel; Carl Busart; Priya Narayan; Dinesh Manocha; Amrit Bedi; Pratap Tokekar
- Reference count: 25
- One-line primary result: PegMARL outperforms state-of-the-art MARL algorithms in both discrete and continuous environments using personalized expert demonstrations for each agent

## Executive Summary
This paper introduces PegMARL, a novel approach for Multi-Agent Reinforcement Learning (MARL) that leverages personalized expert demonstrations to guide learning. The key innovation is using two discriminators: one to encourage alignment with demonstrations and another to regulate incentives based on desired outcomes. This method addresses the challenge of efficient exploration in MARL by selectively utilizing personalized demonstrations for each agent, avoiding the need for joint expert demonstrations which are difficult to obtain. The algorithm demonstrates strong performance even with suboptimal personalized demonstrations and shows flexibility in leveraging joint demonstrations from various sources.

## Method Summary
PegMARL uses personalized expert demonstrations for each agent or agent type in heterogeneous teams, employing two discriminators: a personalized behavior discriminator that provides incentives for actions aligning with demonstrations, and a personalized transition discriminator that regulates incentives based on desired outcomes. The algorithm is compatible with any policy gradient method (MAPPO is used in the implementation) and uses joint demonstrations when available, including from non-co-trained policies. It operates by reshaping the reward function to incorporate both environmental rewards and demonstration-based incentives, allowing agents to learn both personal task completion and cooperative behaviors while maintaining exploration efficiency.

## Key Results
- PegMARL outperforms MAPPO, MAGAIL, DM2, and ATA in discrete gridworld environments (lava and door scenarios)
- The algorithm achieves strong performance even with suboptimal personalized demonstrations
- PegMARL demonstrates ability to leverage joint demonstrations from non-co-trained policies in SMAC environment, converging effectively where other methods fail

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PegMARL's two-discriminator approach selectively filters personalized demonstrations to prevent conflicting behaviors while maintaining guidance for exploration.
- Mechanism: The personalized behavior discriminator provides positive incentives for actions aligning with demonstrations and negative incentives for divergent ones. The personalized transition discriminator assesses whether local state-action pairs induce desired state transitions similar to demonstrations, dynamically adjusting the incentive weight. This combination enables PegMARL to leverage personalized demonstrations' exploration benefits while avoiding the conflicts that would arise from naive imitation.
- Core assumption: The transition dynamics in the personalized MDPs are sufficiently similar to the local transition dynamics in the multi-agent environment for the transition discriminator to effectively filter demonstrations.
- Evidence anchors:
  - [abstract]: "This algorithm utilizes two discriminators: the first provides incentives based on the alignment of individual agent behavior with demonstrations, and the second regulates incentives based on whether the behaviors lead to the desired outcome."
  - [section 4.2]: "The personalized behavior discriminator Dϕi evaluates local state-action pairs, providing positive incentives for actions that align with the demonstration and negative incentives for divergent ones, while the personalized transition discriminator D¯ϕi assesses if a local state-action pair induces a desired transition in local state akin to that observed in the demonstration, adjusting the incentive weight accordingly."
- Break condition: If the transition dynamics differ significantly between personalized MDPs and the multi-agent environment, the transition discriminator cannot effectively filter demonstrations, leading to poor performance.

### Mechanism 2
- Claim: PegMARL's transition discriminator enables effective utilization of joint demonstrations from non-co-trained policies by filtering out conflicting behaviors.
- Mechanism: When joint demonstrations are available, the transition discriminator evaluates whether state-action pairs lead to desired outcomes regardless of whether they came from co-trained or non-co-trained policies. This allows PegMARL to leverage a wider range of demonstration data while maintaining convergence, unlike methods requiring demonstrations from co-trained policies.
- Core assumption: The transition discriminator can effectively distinguish between beneficial and conflicting joint demonstrations even when they come from non-co-trained policies.
- Evidence anchors:
  - [abstract]: "We also showcase PegMARL's capability to leverage joint demonstrations in the StarCraft scenario and converging effectively even with demonstrations from non-co-trained policies."
  - [section 4.2]: "This component enables PegMARL to handle both personalized demonstrations and joint demonstrations from various sources, including those sampled from non-co-trained policies, while DM2 requires demonstrations from co-trained policies to achieve convergence."
- Break condition: If the transition discriminator cannot effectively distinguish between conflicting and non-conflicting joint demonstrations, performance will degrade when using demonstrations from non-co-trained policies.

### Mechanism 3
- Claim: PegMARL's dynamic reward reshaping allows it to maintain exploration efficiency while learning cooperative policies from personalized demonstrations.
- Mechanism: The reshaped reward ˆri = r - ηD¯ϕi(si,ai,s′i) log(1-Dϕi(si,ai)) combines environmental rewards with demonstration-based incentives. The transition discriminator's output modulates how strongly the behavior discriminator influences learning, allowing agents to explore while being guided by demonstrations. This addresses the challenge of sparse rewards in MARL by providing more informative learning signals.
- Core assumption: The combined reward signal provides sufficient information for agents to learn both personal task completion and cooperative behaviors.
- Evidence anchors:
  - [section 4.2]: "Subsequently, policy optimization is conducted by maximizing the long-term return with the reshaped reward."
  - [section 5.1]: "MAPPO struggles to develop meaningful behavior across all scenarios due to the sparse reward structure. MAGAIL performs worse, primarily due to the absence of environmental reward signals."
- Break condition: If the reshaped reward signal becomes too dominated by demonstration-based incentives, agents may fail to learn effective cooperation with the environment's true reward structure.

## Foundational Learning

- Concept: Occupancy measures in multi-agent settings
  - Why needed here: PegMARL's theoretical foundation relies on comparing occupancy measures between learned policies and demonstrations. Understanding how global and local occupancy measures relate is crucial for grasping the algorithm's approach.
  - Quick check question: How does the local occupancy measure λπi(si,ai) relate to the global occupancy measure λπ(s,a) in a multi-agent setting?

- Concept: Jensen-Shannon divergence and its approximation via discriminators
- Why needed here: PegMARL approximates the JS divergence between occupancy measures using discriminators, which is key to understanding how the algorithm measures similarity between learned policies and demonstrations.
  - Quick check question: What is the relationship between JS divergence and the discriminator-based approximation used in PegMARL?

- Concept: Distribution matching and reward shaping in MARL
  - Why needed here: PegMARL builds on concepts from distribution matching approaches like DM2 but extends them to handle personalized demonstrations through the transition discriminator.
  - Quick check question: How does PegMARL's approach to distribution matching differ from that of DM2, and why is this difference important for handling personalized demonstrations?

## Architecture Onboarding

- Component map: Policy networks -> Behavior discriminators -> Transition discriminators -> Critics -> Replay buffers

- Critical path:
  1. Collect environment rollouts and demonstration samples
  2. Update behavior discriminators to distinguish between demonstration and policy samples
  3. Update transition discriminators to assess whether state transitions match demonstrations
  4. Compute reshaped rewards using both discriminators
  5. Update policies using policy gradient methods with reshaped rewards
  6. Update critics using TD-learning with reshaped rewards

- Design tradeoffs:
  - Using personalized demonstrations reduces collection effort but requires sophisticated filtering to prevent conflicts
  - Two discriminators add complexity but enable handling of both personalized and joint demonstrations
  - Dynamic reward shaping provides better guidance than static reward shaping but requires careful hyperparameter tuning

- Failure signatures:
  - If transition discriminators fail to filter conflicting demonstrations, agents may learn conflicting behaviors
  - If behavior discriminators are too strict, agents may not explore sufficiently
  - If reshaped rewards are poorly calibrated, agents may ignore environmental rewards entirely

- First 3 experiments:
  1. Verify that PegMARL outperforms MAPPO on the lava scenario with optimal personalized demonstrations
  2. Test PegMARL's performance with suboptimal personalized demonstrations on the door scenario
  3. Evaluate PegMARL's ability to utilize joint demonstrations from non-co-trained policies in the SMAC environment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical convergence guarantees for PegMARL when using personalized demonstrations, given the dynamic adjustment of the transition discriminator?
- Basis in paper: [inferred] The paper explicitly states "Another limitation of our work is the absence of formal convergence guarantees" and notes the complexity introduced by the transition discriminator's dynamic adjustment.
- Why unresolved: The interplay between the dual discriminators and multi-agent dynamics poses challenges for theoretical analysis, especially since the joint distribution cannot be directly recovered from individual agent demonstrations in their setting.
- What evidence would resolve it: A formal proof showing convergence of PegMARL under personalized demonstrations, or a counterexample demonstrating cases where convergence fails.

### Open Question 2
- Question: How does PegMARL perform in tasks requiring continuous, seamless collaboration among agents, such as cooperative object lifting and relocation?
- Basis in paper: [explicit] The paper explicitly mentions that PegMARL may face challenges in tasks requiring continuous, seamless collaboration, such as cooperative object lifting and relocation by multiple agents.
- Why unresolved: The paper only evaluates PegMARL on tasks where agents can make some degree of independent decisions, and does not test it on tasks requiring constant, integrated teamwork.
- What evidence would resolve it: Experimental results showing PegMARL's performance on tasks requiring continuous, seamless collaboration, compared to other MARL algorithms.

### Open Question 3
- Question: How sensitive is PegMARL to the choice of the weighting term η, and what is the optimal way to tune it for different environments?
- Basis in paper: [explicit] The paper states "PegMARL is relatively insensitive to η" but also mentions they chose η through a grid search to optimize performance, suggesting some sensitivity exists.
- Why unresolved: While the paper shows PegMARL is relatively robust to η values, it doesn't provide a systematic method for tuning η or explore the full range of its impact on performance.
- What evidence would resolve it: A comprehensive study exploring PegMARL's performance across a wide range of η values in different environments, identifying patterns or guidelines for optimal η selection.

## Limitations

- PegMARL may face challenges in tasks requiring continuous, seamless collaboration among agents, such as cooperative object lifting and relocation.
- The algorithm lacks formal convergence guarantees, particularly given the dynamic adjustment of the transition discriminator.
- Performance is sensitive to the choice of the weighting term η, though the paper claims relative robustness.

## Confidence

- **High Confidence**: The core mechanism of using two discriminators (behavior and transition) to handle personalized demonstrations is well-supported by the evidence provided, particularly the performance improvements over baselines in discrete and continuous environments.
- **Medium Confidence**: The claim that PegMARL can effectively utilize joint demonstrations from non-co-trained policies is supported but could benefit from more systematic evaluation across different demonstration sources and qualities.
- **Medium Confidence**: The assertion that PegMARL maintains exploration efficiency while learning from demonstrations is plausible given the dynamic reward shaping approach, but the paper doesn't provide direct evidence of exploration behavior or compare against pure imitation learning baselines.

## Next Checks

1. **Discriminator Effectiveness Analysis**: Conduct experiments that systematically vary the quality and compatibility of personalized demonstrations to measure how well the transition discriminator filters conflicting behaviors. This would validate the core assumption that transition dynamics are sufficiently similar between personalized MDPs and the multi-agent environment.

2. **Ablation Study on Demonstration Sources**: Compare PegMARL's performance using personalized demonstrations versus joint demonstrations from different sources (co-trained vs. non-co-trained policies) across multiple tasks. This would clarify the practical value of the algorithm's flexibility in handling various demonstration types.

3. **Exploration-Exploitation Balance Evaluation**: Implement metrics to measure exploration efficiency (e.g., state visitation diversity, learning curves showing early progress) and compare PegMARL against both pure RL baselines and pure imitation learning methods. This would validate whether the dynamic reward shaping truly maintains a good exploration-exploitation balance.