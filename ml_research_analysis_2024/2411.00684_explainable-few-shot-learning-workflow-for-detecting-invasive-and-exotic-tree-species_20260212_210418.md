---
ver: rpa2
title: Explainable few-shot learning workflow for detecting invasive and exotic tree
  species
arxiv_id: '2411.00684'
source_url: https://arxiv.org/abs/2411.00684
tags:
- species
- learning
- samples
- siamese
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents an explainable few-shot learning workflow for
  detecting invasive and exotic tree species in the Atlantic Forest of Brazil using
  UAV images. The approach combines a Siamese network with explainable AI to classify
  tree species with minimal labeled data while providing visual, case-based explanations
  for predictions.
---

# Explainable few-shot learning workflow for detecting invasive and exotic tree species

## Quick Facts
- arXiv ID: 2411.00684
- Source URL: https://arxiv.org/abs/2411.00684
- Reference count: 38
- Primary result: F1-score of 0.86 in 3-shot learning for invasive tree species detection

## Executive Summary
This study presents an explainable few-shot learning workflow for detecting invasive and exotic tree species in the Atlantic Forest of Brazil using UAV images. The approach combines a Siamese network with explainable AI to classify tree species with minimal labeled data while providing visual, case-based explanations for predictions. The workflow achieves strong performance with lightweight models and introduces quantitative metrics to evaluate explanation quality.

## Method Summary
The method uses a Siamese network architecture trained on generic tree species to learn a feature space where similar images are close together. For new target species, the model is refined using few-shot learning with 1-3 labeled samples per class. Individual tree crowns are extracted from UAV orthomosaics using object detection, then classified with similarity-based scoring. Explanations are generated by retrieving and displaying the most similar support images used for each prediction, evaluated using correctness, continuity, and contrastivity metrics.

## Key Results
- Achieves F1-score of 0.86 in 3-shot learning using MobileNet backbone
- Outperforms shallow CNN baseline in few-shot learning scenario
- Demonstrates effective example-based explanations evaluated through quantitative metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Siamese network architecture enables similarity-based classification with minimal labeled data
- Mechanism: Learns a shared feature space where embeddings of similar tree images are close and dissimilar ones are far apart
- Core assumption: Feature space is discriminative enough that visual similarity correlates with species identity
- Evidence anchors:
  - [abstract] "By integrating a Siamese network with explainable AI (XAI), the workflow enables the classification of tree species with minimal labeled data while providing visual, case-based explanations for the predictions."
  - [section] "The shallow CNN -based Siamese network is initiated with two identical feature extractors, each with four convolution layers. The outputs of the feature extractors are compared with an Euclidean distance layer, resulting in a similarity score between 0 (most dissimilar) and 1 (most similar)."
  - [corpus] Weak - no explicit Siamese network examples in corpus

### Mechanism 2
- Claim: Few-shot learning refinement allows adaptation to new tree species with only 1-3 labeled examples per class
- Mechanism: Fine-tunes pre-trained base models using balanced similar/dissimilar pairs from small labeled set
- Core assumption: Base model's learned features transfer sufficiently to new domain that small updates suffice
- Evidence anchors:
  - [abstract] "With a lightweight backbone, e.g., MobileNet, it achieves a F1-score of 0.86 in 3-shot learning, outperforming a shallow CNN."
  - [section] "To reduce the number of support samples as much as possible, we refined the base models using up to three of the six samples in each class... we tested the refinement from 1-shot to 3-shot learning."
  - [corpus] Weak - few-shot learning appears in corpus but not specifically in tree species context

### Mechanism 3
- Claim: Example-based explanations improve user trust by showing most similar support images used for classification
- Mechanism: Retrieves and displays top-k most similar support images per class after classification
- Core assumption: Humans judge classification correctness by comparing visual similarity between query and reference images
- Evidence anchors:
  - [abstract] "A set of explanation metrics, i.e., correctness, continuity, and contrastivity, accompanied by visual cases, provide further insights about the prediction results."
  - [section] "Each of these three steps is assessed individually. The result of the proposed workflow is the recognition of individual tree canopies of invasive or native species with an explanation for each classification."
  - [corpus] Weak - no direct corpus support for this specific explanation method

## Foundational Learning

- Concept: Siamese networks for similarity learning
  - Why needed here: Traditional classifiers need many labeled samples per class; Siamese networks learn metric space where few examples suffice
  - Quick check question: What is the output of a Siamese network when comparing two images from the same species vs. different species?

- Concept: Few-shot learning and transfer learning
  - Why needed here: Field-labeled data is scarce and expensive; approach leverages pre-training on similar species and fine-tunes on small new datasets
  - Quick check question: How does few-shot learning differ from standard supervised learning in terms of data requirements?

- Concept: Explainable AI evaluation metrics
  - Why needed here: Saliency maps are unreliable; example-based explanations need quantitative metrics (correctness, continuity, contrastivity) to assess quality
  - Quick check question: What does the "contrastivity" metric measure in the context of example-based explanations?

## Architecture Onboarding

- Component map: UAV orthomosaic -> Netflora object detection -> tree crown cutouts -> size normalization (128Ã—128) -> Base model training -> Few-shot refinement -> Classification + Explanation -> Evaluation

- Critical path: Data -> Object detection -> Cutout generation -> Base model training -> Few-shot refinement -> Classification + Explanation -> Evaluation

- Design tradeoffs:
  - Shallow CNN vs. MobileNet: fewer parameters (393K vs 3,502K) but lower accuracy; MobileNet better for limited data
  - Similarity scoring: average vs. K-NN for label assignment (similar performance at high accuracy)
  - Explanation complexity: showing more examples improves contrastivity but may overwhelm users

- Failure signatures:
  - Low recall despite high precision: model is too conservative, likely overfitting to support set
  - High continuity but low correctness: explanations are stable but not aligned with model predictions
  - No improvement from few-shot refinement: base model features don't transfer well to new species

- First 3 experiments:
  1. Train base Siamese model on 5 generic species; evaluate multi-class classification (baseline)
  2. Perform 1-shot, 2-shot, and 3-shot refinement on new species; compare to zero-shot performance
  3. Generate explanations for refined model; compute correctness, continuity, and contrastivity metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed explainable few-shot learning workflow perform when applied to forest ecosystems with significantly higher biodiversity and species complexity than the Atlantic Forest case study?
- Basis in paper: [explicit] The paper acknowledges that "few-shot learning can be beneficial" in forest monitoring due to the "dynamic nature of the forest" and "rapid adoption of pre-trained models to the specific forest type, region, or situation"
- Why unresolved: The study only tested the approach on a limited set of 11 species in a specific region, and the paper doesn't address how the method would scale to more diverse forest ecosystems with potentially hundreds of species
- What evidence would resolve it: Testing the workflow on a forest area with substantially higher species diversity (e.g., tropical rainforests with 100+ tree species) and comparing classification accuracy, explanation quality metrics, and computational requirements to the current results

### Open Question 2
- Question: What is the minimum number of labeled samples required for the few-shot learning approach to achieve acceptable classification performance, and how does this vary across different tree species characteristics?
- Basis in paper: [explicit] The paper tests the method from 1-shot to 3-shot learning and notes that "the more support samples, the better the performance," but doesn't establish a threshold for "acceptable" performance or explore how species characteristics affect this
- Why unresolved: The study only tests up to 3-shot learning and doesn't investigate whether performance plateaus at some point, or how factors like species similarity, canopy structure, or seasonal variations affect the minimum required samples
- What evidence would resolve it: Systematic testing of the workflow with varying numbers of labeled samples (e.g., 1-10 shots) across multiple species with different visual characteristics, identifying performance thresholds and patterns in sample requirements

### Open Question 3
- Question: How do the explanation metrics (correctness, continuity, contrastivity) correlate with actual user trust and decision-making in real-world forest management scenarios?
- Basis in paper: [explicit] The paper evaluates explanation quality using correctness, continuity, and contrastivity metrics but acknowledges that "evaluating the presentation and user dimensions would require extensive additional experiments with the XAI method's intended end-users"
- Why unresolved: The study provides quantitative explanation metrics but doesn't validate whether these metrics translate to improved user understanding, trust, or better forest management decisions
- What evidence would resolve it: User studies with forest managers and conservation experts using the explanation interface, measuring their confidence in predictions, time to make decisions, and accuracy of species identification compared to using the model without explanations

## Limitations
- Evaluation relies on small dataset (6 species with 6 samples each) that may not generalize to more diverse forests
- Netflora object detection pipeline is mentioned but not fully specified, creating reproducibility gaps
- Explanation metrics lack empirical validation through user studies with forest managers

## Confidence
- **High confidence**: Core mechanism of using Siamese networks for similarity-based few-shot learning is well-established
- **Medium confidence**: Specific architecture choices and performance advantages need broader testing across different forest types
- **Low confidence**: Practical utility of explanation component in real-world forest management scenarios lacks empirical validation

## Next Checks
1. Test the workflow on UAV imagery from different forest regions and seasons to assess generalization beyond Atlantic Forest dataset
2. Conduct field tests with forest managers to evaluate whether example-based explanations improve decision-making and trust
3. Evaluate performance when scaling from 6 to 20+ tree species to identify potential degradation in few-shot learning effectiveness