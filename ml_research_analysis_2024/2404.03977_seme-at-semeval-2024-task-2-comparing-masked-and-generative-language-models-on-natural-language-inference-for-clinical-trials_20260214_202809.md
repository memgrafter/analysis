---
ver: rpa2
title: 'SEME at SemEval-2024 Task 2: Comparing Masked and Generative Language Models
  on Natural Language Inference for Clinical Trials'
arxiv_id: '2404.03977'
source_url: https://arxiv.org/abs/2404.03977
tags:
- language
- task
- inference
- statement
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work evaluated two approaches for biomedical natural language
  inference on clinical trial data: (1) finetuning and ensembling masked language
  models (MLMs) such as NLI-RoBERTa, ClinicalBERT, and Clinical-Longformer; and (2)
  prompting large language models (LLMs) like Flan-T5-large using zero-shot, one-shot,
  and two-shot settings with Chain-of-Thought and Contrastive Chain-of-Thought techniques.
  The best system used Flan-T5-large in a two-shot setting, achieving an F1 score
  of 0.57, Faithfulness of 0.64, and Consistency of 0.56.'
---

# SEME at SemEval-2024 Task 2: Comparing Masked and Generative Language Models on Natural Language Inference for Clinical Trials

## Quick Facts
- arXiv ID: 2404.03977
- Source URL: https://arxiv.org/abs/2404.03977
- Reference count: 14
- Best system achieved F1 score of 0.57 using Flan-T5-large in two-shot setting

## Executive Summary
This work evaluates two approaches for biomedical natural language inference on clinical trial data: finetuning masked language models (MLMs) and prompting large language models (LLMs). The authors compare NLI-RoBERTa, ClinicalBERT, and Clinical-Longformer against Flan-T5-large using zero-shot, one-shot, and two-shot settings with Chain-of-Thought techniques. The best-performing system used Flan-T5-large in a two-shot setting, achieving an F1 score of 0.57. While LLM prompting showed promise, MLM finetuning proved more computationally efficient with lower carbon emissions.

## Method Summary
The study employed two distinct approaches for biomedical NLI. First, MLMs were finetuned on clinical trial data using NLI-RoBERTa, ClinicalBERT, and Clinical-Longformer. Second, LLMs were prompted using Flan-T5-large in zero-shot, one-shot, and two-shot settings with Chain-of-Thought and Contrastive Chain-of-Thought techniques. The authors also created an ensemble combining MLM predictions with LLM outputs. Models were evaluated on six relation types in clinical trials, measuring F1 score, Faithfulness, and Consistency metrics.

## Key Results
- Flan-T5-large in two-shot setting achieved best performance: F1 0.57, Faithfulness 0.64, Consistency 0.56
- MLM ensemble (NLI-RoBERTa, Clinical-Longformer, ClinicalBERT) with Flan-T5-large predictions yielded similar scores
- LLM prompting approaches showed promise but MLMs were more computationally efficient with lower carbon emissions
- Two-shot prompting outperformed zero-shot and one-shot approaches

## Why This Works (Mechanism)
None

## Foundational Learning
- **Masked Language Models (MLMs)**: Pre-trained models that predict masked tokens in input sequences. Why needed: Enable efficient fine-tuning on domain-specific tasks. Quick check: Verify model can reconstruct masked biomedical terminology.
- **Large Language Models (LLMs)**: Generative models capable of few-shot learning through prompting. Why needed: Leverage pre-training on massive corpora for zero/few-shot performance. Quick check: Test prompt engineering effectiveness on sample inputs.
- **Chain-of-Thought prompting**: Technique that guides LLMs through reasoning steps. Why needed: Improves complex reasoning in biomedical inference. Quick check: Validate intermediate reasoning steps produce logical conclusions.

## Architecture Onboarding
- **Component map**: Input text → MLM fine-tuning pipeline OR LLM prompting pipeline → Output classification
- **Critical path**: Text preprocessing → Model inference → Result aggregation (for ensemble) → Evaluation metrics
- **Design tradeoffs**: MLMs offer efficient fine-tuning but require labeled data; LLMs enable few-shot learning but need careful prompt engineering
- **Failure signatures**: Poor performance on rare relation types, inconsistent results across similar prompts, high computational cost
- **First experiments**: 1) Benchmark MLM vs LLM performance on validation set, 2) Test prompt sensitivity across relation types, 3) Measure computational efficiency differences

## Open Questions the Paper Calls Out
None

## Limitations
- Moderate performance metrics (F1 0.57, Faithfulness 0.64, Consistency 0.56) indicate room for improvement
- Computational efficiency claims based on carbon emission estimates rather than direct runtime measurements
- Limited to specific biomedical domain (clinical trials) and six relation types, reducing generalizability

## Confidence
- High confidence: Experimental methodology and results clearly reported with specific metrics and model comparisons
- Medium confidence: MLM finetuning computational efficiency claims based on estimated carbon emissions
- Medium confidence: LLM prompting promise assertion given moderate performance metrics and limited evaluation scope

## Next Checks
1. Conduct ablation studies to isolate contribution of each ensemble component, particularly LLM predictions
2. Perform cross-validation across different clinical trial datasets and medical subdomains for generalizability assessment
3. Implement direct runtime measurements and resource utilization tracking during inference to validate carbon emission estimates