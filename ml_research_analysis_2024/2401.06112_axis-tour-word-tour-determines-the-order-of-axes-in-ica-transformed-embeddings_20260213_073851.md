---
ver: rpa2
title: 'Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings'
arxiv_id: '2401.06112'
source_url: https://arxiv.org/abs/2401.06112
tags:
- axis
- tour
- embeddings
- axes
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Axis Tour, a method to determine the order
  of axes in ICA-transformed word embeddings by maximizing semantic continuity. Inspired
  by Word Tour, Axis Tour uses axis embeddings to solve a TSP that optimizes axis
  ordering.
---

# Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings

## Quick Facts
- arXiv ID: 2401.06112
- Source URL: https://arxiv.org/abs/2401.06112
- Reference count: 40
- Key outcome: Axis Tour uses TSP optimization to order ICA-transformed word embedding axes by maximizing semantic continuity, improving downstream task performance compared to PCA and random ICA ordering

## Executive Summary
This paper introduces Axis Tour, a method for determining the order of axes in ICA-transformed word embeddings by maximizing semantic continuity. Inspired by Word Tour's approach to one-dimensional embeddings, Axis Tour creates axis embeddings and solves a Traveling Salesman Problem to find the optimal axis ordering. The method demonstrates improved semantic continuity and competitive or superior performance on downstream tasks including analogy, word similarity, and categorization when compared to baseline dimensionality reduction methods.

## Method Summary
Axis Tour applies Independent Component Analysis to word embeddings to obtain interpretable semantic axes, then creates axis embeddings by averaging the top-k words for each axis. It solves a Traveling Salesman Problem where the cost between axes is defined as 1 minus their cosine similarity, effectively ordering axes to maximize semantic continuity. For dimensionality reduction, consecutive axes are projected onto single dimensions using weights proportional to their skewness raised to a power α. The method is evaluated on GloVe, word2vec, and BERT embeddings across multiple downstream tasks.

## Key Results
- Axis Tour achieves average cosine similarity of 0.244 between adjacent axes for GloVe embeddings, indicating strong semantic continuity
- On downstream tasks, Axis Tour outperforms or matches PCA and random ICA ordering across most dimensions (50, 100, 200, 300)
- GPT-based evaluation confirms Axis Tour produces more semantically related consecutive axes compared to skewness-based ordering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Axis Tour optimizes axis order by maximizing semantic continuity through Word Tour-inspired TSP.
- Mechanism: Uses axis embeddings to solve TSP where cost is 1 - cosine similarity between adjacent axes, thus maximizing the sum of cosine similarities between consecutive axes.
- Core assumption: Word embeddings with similar meanings have high cosine similarity, so maximizing cosine similarity between adjacent axis embeddings ensures semantic continuity.
- Evidence anchors:
  - [abstract]: "Axis Tour uses axis embeddings to solve a TSP that optimizes axis ordering"
  - [section 4.2]: "the cost between the axis embeddings vℓ and vm for the TSP is defined by 1 − cos (vℓ, vm)"
  - [corpus]: "Found 25 related papers (using 8). Average neighbor FMR=0.541, average citations=0.0."

### Mechanism 2
- Claim: Dimensionality reduction using weighted projection based on axis skewness preserves semantic information.
- Mechanism: Projects consecutive axes onto lower dimensions using weights proportional to skewness raised to power α, normalizing projections to equalize scale.
- Core assumption: Higher skewness indicates more meaningful axes, and merging consecutive axes with similar meanings preserves semantic information better than random selection.
- Evidence anchors:
  - [section 4.3]: "f (ℓ) r = (γαℓ /qPbr m=ar γ2αm for ℓ ∈ Ir 0 otherwise"
  - [section 5.3]: "Axis Tour adopts the dimensionality reduction in Section 4.3 with α = 1/3"
  - [corpus]: "Exploring Intra and Inter-language Consistency in Embeddings with ICA"

### Mechanism 3
- Claim: GPT-based evaluation confirms semantic continuity of Axis Tour embeddings.
- Mechanism: Uses GPT models to compare semantic relatedness between consecutive axes in Axis Tour vs Skewness Sort, counting which method produces more semantically related axes.
- Core assumption: GPT models can accurately judge semantic relatedness between word sets representing axis meanings.
- Evidence anchors:
  - [section 5.2.2]: "we ask the model to determine, based on the top 10 words, whether the next axis of Axis Tour or that of Skewness Sort is more semantically related"
  - [section 5.2.2]: "Axis Tour has a greater number of related axes compared to Skewness Sort for each model"
  - [corpus]: "Investigating the Contextualised Word Embedding Dimensions Specified for Contextual and Temporal Semantic Changes"

## Foundational Learning

- Concept: Independent Component Analysis (ICA) and its properties
  - Why needed here: ICA transforms word embeddings to reveal interpretable semantic axes, but the order is arbitrary - understanding ICA is crucial for understanding why Axis Tour is needed
  - Quick check question: What is the key property of ICA-transformed embeddings that makes them useful for interpretability?

- Concept: Traveling Salesman Problem (TSP) and Word Tour
  - Why needed here: Axis Tour uses TSP optimization to order axes, inspired by Word Tour's one-dimensional embedding approach
  - Quick check question: How does Word Tour use TSP to create one-dimensional word embeddings?

- Concept: Cosine similarity and its relationship to semantic meaning
  - Why needed here: Axis Tour uses cosine similarity between axis embeddings to measure semantic continuity and as the TSP cost function
  - Quick check question: Why is cosine similarity an appropriate measure for comparing axis embeddings?

## Architecture Onboarding

- Component map:
  Input embeddings -> ICA transformation -> Axis embeddings -> TSP optimization -> Ordered axes -> Dimensionality reduction -> Lower-dimensional embeddings -> Downstream task evaluation

- Critical path:
  1. ICA transformation → axis embeddings → TSP optimization → ordered axes
  2. Ordered axes → dimensionality reduction → lower-dimensional embeddings → downstream task evaluation

- Design tradeoffs:
  - k (number of top words for axis embedding): Higher k provides more robust axis representations but may include less relevant words
  - α (skewness weighting power): Controls sensitivity to skewness; different values affect dimensionality reduction quality
  - Interval size for dimensionality reduction: Affects granularity of axis merging and semantic coherence

- Failure signatures:
  - Poor downstream task performance indicates ineffective dimensionality reduction
  - Low cosine similarity between adjacent axis embeddings indicates failed semantic continuity
  - Unstable results across different k values suggest sensitivity to parameter choice

- First 3 experiments:
  1. Verify ICA transformation produces interpretable axes by examining top words for each axis
  2. Test Axis Tour with different k values (1, 10, 100, 1000) and evaluate semantic continuity metrics
  3. Compare downstream task performance of Axis Tour embeddings vs PCA and ICA with random ordering at various dimensions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of k (number of top words used for axis embedding) impact the semantic continuity and downstream task performance of Axis Tour embeddings?
- Basis in paper: [explicit] The paper experiments with k = 1, 10, 100, 1000 and finds that k = 100 gives the best results for semantic continuity. It also shows that k affects downstream task performance.
- Why unresolved: While the paper provides some analysis, it does not systematically explore the impact of k across a wider range of values or different embedding types (e.g., dynamic vs. static embeddings).
- What evidence would resolve it: A comprehensive study varying k across different embedding types and downstream tasks, analyzing the trade-off between semantic continuity and task performance for different k values.

### Open Question 2
- Question: Can nonlinear transformations beyond linear projections be used for dimensionality reduction in Axis Tour embeddings?
- Basis in paper: [inferred] The paper mentions that nonlinear transformations could be considered for dimensionality reduction but does not explore this direction.
- Why unresolved: The paper focuses on linear projections for dimensionality reduction, leaving the potential benefits of nonlinear methods unexplored.
- What evidence would resolve it: Experiments comparing linear projections with various nonlinear dimensionality reduction techniques (e.g., t-SNE, UMAP) on Axis Tour embeddings, evaluating their impact on downstream task performance and interpretability.

### Open Question 3
- Question: How does Axis Tour compare to other axis ordering methods, such as TICA, in terms of semantic continuity and downstream task performance?
- Basis in paper: [explicit] The paper compares Axis Tour to TICA and finds that Axis Tour performs better or comparably in terms of semantic continuity and downstream tasks.
- Why unresolved: While the paper provides a comparison, it does not explore the strengths and weaknesses of each method in detail or investigate potential hybrid approaches that combine their benefits.
- What evidence would resolve it: A thorough analysis of the trade-offs between Axis Tour and TICA, including their interpretability, computational efficiency, and performance on various downstream tasks. Investigating hybrid methods that leverage the strengths of both approaches could also be insightful.

## Limitations
- Evaluation scope limited to specific embeddings (GloVe, word2vec, BERT) and downstream tasks, potentially limiting generalizability
- GPT-based semantic continuity evaluation introduces potential subjectivity in judging semantic relatedness
- TSP optimization effectiveness depends heavily on axis embedding quality, which may vary across different embedding types

## Confidence
- High Confidence: The core mechanism of using TSP optimization to order ICA axes based on cosine similarity between axis embeddings is well-specified and theoretically sound
- Medium Confidence: The claim that Axis Tour consistently outperforms PCA and random ICA ordering across all downstream tasks has medium confidence due to limited evaluation scope and potential dataset-specific effects
- Low Confidence: The assertion that skewness-weighted dimensionality reduction optimally preserves semantic information has low confidence, as the paper provides limited ablation studies on different α values and interval sizes

## Next Checks
1. **Ablation Study on α Parameter**: Systematically vary α (0.1, 0.3, 0.5, 0.7, 1.0) in the dimensionality reduction formula and evaluate downstream task performance to determine optimal weighting sensitivity.

2. **Cross-Embedding Generalization**: Apply Axis Tour to non-standard embeddings (e.g., FastText, RoBERTa) and evaluate whether semantic continuity improvements and downstream performance gains generalize beyond the tested embeddings.

3. **Alternative Semantic Continuity Metrics**: Implement and compare semantic continuity using alternative metrics such as normalized mutual information between consecutive axis word sets or semantic shift detection algorithms to validate GPT-based evaluation results.