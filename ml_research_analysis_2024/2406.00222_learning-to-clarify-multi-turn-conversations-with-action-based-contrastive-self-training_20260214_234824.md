---
ver: rpa2
title: 'Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive
  Self-Training'
arxiv_id: '2406.00222'
source_url: https://arxiv.org/abs/2406.00222
tags:
- user
- assistant
- which
- question
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of training large language models
  to effectively ask clarifying questions in ambiguous conversational contexts, a
  skill that is crucial for intelligent conversational assistants but often lacking
  in current models. The proposed solution, Action-Based Contrastive Self-Training
  (ACT), is a sample-efficient quasi-online preference optimization algorithm that
  focuses on learning by highlighting the differences between possible pragmatic conversational
  actions.
---

# Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training

## Quick Facts
- arXiv ID: 2406.00222
- Source URL: https://arxiv.org/abs/2406.00222
- Reference count: 40
- One-line primary result: ACT significantly outperforms SFT and naive DPO on three conversational tasks by learning to ask clarifying questions through action-based contrastive preference optimization

## Executive Summary
This paper addresses the critical problem of training language models to effectively ask clarifying questions in ambiguous conversational contexts. The proposed solution, Action-Based Contrastive Self-Training (ACT), is a sample-efficient quasi-online preference optimization algorithm that learns by highlighting differences between possible pragmatic conversational actions. ACT uses contrastive preference pairs and trajectory-level simulation to improve models' ability to recognize ambiguity and reason about clarifying questions, achieving significant performance gains across three diverse conversational tasks.

## Method Summary
ACT is a sample-efficient quasi-online Direct Preference Optimization algorithm that focuses on learning by highlighting differences between pragmatic conversational actions. The method constructs action-based contrastive preference pairs, samples on-policy responses from the current policy model, simulates trajectory outcomes using a user simulator, and updates the policy through DPO. The approach is evaluated on three tasks: tabular-grounded question-answering, machine reading comprehension, and a novel text-to-SQL task for disambiguating information-seeking requests.

## Key Results
- ACT significantly outperforms supervised fine-tuning and naive Direct Preference Optimization in action-level accuracy and task performance
- The method demonstrates strong sample efficiency, particularly effective in limited data scenarios (50-250 examples)
- ACT shows consistent improvements across diverse conversational tasks including PACIFIC, Abg-CoQA, and AmbigSQL datasets

## Why This Works (Mechanism)

### Mechanism 1
**Claim**: Contrastive preference pairs highlighting differences between pragmatic actions are more effective than naive preference sampling for conversational skill learning.

**Mechanism**: By constructing preference pairs where the "winning" response takes the correct pragmatic action (clarify vs answer) and the "losing" response takes the incorrect action, the model learns to recognize when ambiguity exists and how to respond appropriately. This creates clearer gradient signals than random sampling.

**Core assumption**: The implicit pragmatic action of a response can be reliably classified and that these actions are the key differentiators in ambiguous conversation handling.

**Evidence anchors**:
- [abstract]: "ACT is a sample-efficient, quasi-online Direct Preference Optimization algorithm which focuses on learning by highlighting the differences between an agent's possible pragmatic conversational actions."
- [section 3.2.1]: "The preference dataset primarily consists of contrastive winning-losing action pairs."
- [corpus]: Weak - the corpus shows related work on multi-turn conversations but doesn't directly support the action-based contrastive approach.

**Break condition**: If action classification becomes unreliable or if the action space needs to be much larger than binary (clarify/answer), the method's effectiveness would degrade.

### Mechanism 2
**Claim**: On-policy response sampling combined with trajectory simulation enables sample-efficient learning of multi-turn conversational reasoning.

**Mechanism**: By sampling responses from the current policy model and simulating the trajectory outcomes, ACT can create preference pairs that reflect the actual behavior of the model being trained. This allows the model to learn from its own likely responses rather than fixed examples.

**Core assumption**: The on-policy responses are high-quality enough to serve as meaningful training examples and that trajectory simulation can accurately predict user responses and outcomes.

**Evidence anchors**:
- [abstract]: "ACT is a quasi-online Direct Preference Optimization algorithm which focuses on learning by highlighting the differences between an agent's possible pragmatic conversational actions."
- [section 3.2.2]: "We sample an on-policy response yj from πθi. We use A to determine whether the implicit action of yj matches the ground truth action aj."
- [corpus]: Weak - corpus neighbors discuss multi-turn conversations but don't specifically address trajectory simulation for preference learning.

**Break condition**: If the user simulator produces unrealistic responses or if the policy model's sampling quality degrades significantly during training.

### Mechanism 3
**Claim**: Direct optimization of dialogue action planning as an implicit subtask of response generation improves multi-turn conversational capabilities without requiring separate modules.

**Mechanism**: Instead of training separate understanding/planning and generation modules, ACT directly optimizes the generation model to implicitly select appropriate actions through contrastive learning. This avoids error propagation between modules and directly optimizes for response quality.

**Core assumption**: The generation model has sufficient capacity to implicitly learn dialogue planning and that action selection can be effectively learned through response generation alone.

**Evidence anchors**:
- [abstract]: "ACT is a sample-efficient, quasi-online Direct Preference Optimization algorithm which focuses on learning by highlighting the differences between an agent's possible pragmatic conversational actions."
- [section 3.2]: "We propose directly optimizing dialogue action planning as an implicit subtask of response generation in mixed-initiative conversation contexts."
- [corpus]: Weak - corpus neighbors discuss multi-turn conversations but don't specifically address the integration of planning into generation.

**Break condition**: If the action space becomes too complex for implicit learning or if the generation model lacks sufficient capacity to handle both planning and response generation.

## Foundational Learning

- **Concept**: Direct Preference Optimization (DPO)
  - **Why needed here**: ACT builds directly on DPO by modifying the preference pairs to be action-based rather than naive. Understanding DPO is essential to grasp how ACT updates its policy.
  - **Quick check question**: What is the key difference between DPO and ACT in terms of how preference pairs are constructed?

- **Concept**: Contrastive learning
  - **Why needed here**: ACT relies on contrastive learning to highlight differences between correct and incorrect conversational actions. This is fundamental to understanding how the model learns from preferences.
  - **Quick check question**: How does contrastive learning differ from traditional supervised learning in the context of conversational AI?

- **Concept**: Multi-turn dialogue trajectory simulation
  - **Why needed here**: ACT simulates full conversation trajectories to evaluate whether clarification questions lead to correct outcomes. This is crucial for understanding how the model learns multi-turn reasoning.
  - **Quick check question**: Why is trajectory simulation necessary for learning clarification strategies rather than just single-turn evaluation?

## Architecture Onboarding

- **Component map**: Policy Model (πθ) -> Action Classifier (A) -> User Simulator (U) -> Preference Dataset (Dpref) -> Policy Model (πθ)

- **Critical path**:
  1. Construct action-based preference dataset using M
  2. Sample on-policy responses from current policy model
  3. Classify actions and simulate trajectories using A and U
  4. Update preference pairs based on simulation outcomes
  5. Apply DPO objective to update policy model
  6. Repeat until convergence

- **Design tradeoffs**:
  - Using on-policy sampling vs. fixed preference pairs: On-policy sampling provides more relevant training data but requires simulation overhead
  - Separate action classification vs. implicit learning: Explicit classification provides clear signals but adds complexity
  - Trajectory simulation depth: Longer simulations provide better evaluation but increase computational cost

- **Failure signatures**:
  - Degrading action classification accuracy over training
  - User simulator producing unrealistic or inconsistent responses
  - Policy model collapsing to always clarify or always answer
  - Training instability or divergence with the DPO objective

- **First 3 experiments**:
  1. **Ablation of on-policy sampling**: Run ACT without on-policy sampling to verify its importance (should see performance degradation)
  2. **Vary trajectory simulation depth**: Test different lengths of simulated trajectories to find optimal balance
  3. **Different action spaces**: Test with more than binary actions (e.g., add "provide partial answer" option) to verify scalability

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does ACT perform on tasks with a broader action space beyond binary clarify/answer?
- **Basis in paper**: [inferred] The paper focuses on binary action space [CLARIFY, ANSWER] and mentions ACT could be extended to broader action spaces.
- **Why unresolved**: The paper only evaluates ACT on tasks with binary action spaces, leaving its performance on more complex tasks with richer action spaces unexplored.
- **What evidence would resolve it**: Empirical results of ACT on tasks with multi-class action spaces, such as dialogue systems with more nuanced dialogue acts or multi-step reasoning tasks.

### Open Question 2
- **Question**: What is the impact of ACT on long-term task success beyond immediate clarification accuracy?
- **Basis in paper**: [explicit] The paper evaluates ACT's impact on task performance but focuses on immediate outcomes rather than long-term task success.
- **Why unresolved**: The paper does not examine how ACT's ability to ask clarifying questions affects the overall success of complex, multi-step tasks that require sustained interaction.
- **What evidence would resolve it**: Experiments measuring task completion rates, user satisfaction, or overall task success in long-horizon tasks where ACT is applied.

### Open Question 3
- **Question**: How does ACT compare to other online RL methods like PPO in terms of sample efficiency and final performance?
- **Basis in paper**: [inferred] The paper positions ACT as a quasi-online method and mentions that online methods like PPO have advantages but are harder to tune.
- **Why unresolved**: The paper does not provide a direct comparison between ACT and PPO or other online RL methods, leaving the trade-offs between sample efficiency and final performance unclear.
- **What evidence would resolve it**: Empirical results comparing ACT to PPO and other online RL methods on the same tasks, measuring both sample efficiency and final task performance.

## Limitations

- Experimental validation is limited to small dataset sizes (50-250 examples) and a single base model, raising questions about generalization to larger datasets and different architectures.
- The effectiveness of trajectory simulation heavily depends on user simulator quality and task heuristics, which are not thoroughly validated across different domains.
- ACT's binary action space may not scale well to more complex conversational scenarios requiring multiple types of clarification or partial responses.

## Confidence

**High Confidence**: The core mechanism of using contrastive preference pairs for conversational action learning is well-supported by experimental results, showing consistent improvements over baselines across all three tasks and dataset sizes.

**Medium Confidence**: The trajectory simulation approach for multi-turn reasoning evaluation is theoretically sound but depends heavily on simulator quality and task-specific heuristics, introducing uncertainty about generalizability.

**Low Confidence**: The scalability of ACT to larger, more complex action spaces and real-world conversational scenarios remains speculative, with limited validation of robustness to variations in user simulator quality.

## Next Checks

1. **Cross-domain robustness test**: Evaluate ACT on a conversational task with significantly different characteristics (e.g., open-domain dialogue or negotiation) to assess generalization beyond the current domains of tabular QA, reading comprehension, and text-to-SQL.

2. **User simulator ablation**: Run experiments with different user simulators or ground truth user responses to quantify the impact of simulator quality on ACT's performance and identify failure modes related to simulation fidelity.

3. **Action space expansion**: Test ACT with a multi-class action space (e.g., clarify, partial answer, ask for confirmation) to evaluate scalability and identify limitations in handling more complex conversational strategies.