---
ver: rpa2
title: Exploring Perceptual Limitation of Multimodal Large Language Models
arxiv_id: '2402.07384'
source_url: https://arxiv.org/abs/2402.07384
tags:
- mllms
- image
- object
- visual
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically studies how multimodal large language
  models (MLLMs) perceive small visual objects, revealing that object size, quality,
  distractors, and location all significantly affect their performance. Through controlled
  experiments on synthetic digit recognition tasks, the authors find that MLLMs show
  threshold-dependent sensitivity to object quality, generally improve with larger
  object size, are negatively impacted by the presence of visual distractors, and
  exhibit location-dependent performance variations across the image.
---

# Exploring Perceptual Limitation of Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2402.07384
- Source URL: https://arxiv.org/abs/2402.07384
- Reference count: 23
- Primary result: Systematic study reveals object size, quality, distractors, and location significantly affect MLLM performance on small visual objects

## Executive Summary
This paper investigates how multimodal large language models (MLLMs) perceive small visual objects through controlled experiments on synthetic digit recognition tasks. The authors find that object size, quality, distractors, and location all significantly impact MLLM performance, with threshold-dependent sensitivity to object quality and location-dependent variations across image regions. While some models improve with OCR-oriented training data, perceptual limitations persist especially for small objects in certain image regions.

## Method Summary
The study evaluates seven state-of-the-art MLLMs using controlled intervention studies on four factors: object quality (sampling rate), size, distractors, and location. Researchers created synthetic datasets with text digits and tested models on both these controlled datasets and real-world VQA datasets (GQA and TextVQA). Performance was measured using inclusion match and exact match accuracy metrics, with additional GPM for OCR tasks.

## Key Results
- Object quality sensitivity shows threshold effects where performance plateaus after a certain sampling rate, aligning with human perception
- Performance improves with object size increase while maintaining constant quality
- Visual distractors significantly reduce question-answering accuracy
- Object location in image affects performance, with patch boundary interactions creating location-dependent variations
- Models trained on OCR-oriented datasets show improved performance on small object tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Object size directly influences MLLM performance in visual question answering tasks
- Mechanism: Larger objects occupy more image patches, translating to more transformer tokens that can attend to object features during self-attention mechanisms
- Core assumption: The distribution of objects in training data follows a size bias where larger objects are more commonly annotated
- Evidence anchors:
  - [abstract] "we find that lower object quality and smaller object size can both independently reduce MLLMs' ability to answer visual questions"
  - [section 4.3] "the performance of MLLMs improves with the increase of object size while maintaining a constant quality (sampling rate)"
- Break condition: If object size increase doesn't proportionally increase token count or if training data contains sufficient small object examples

### Mechanism 2
- Claim: MLLMs exhibit threshold-dependent sensitivity to object quality
- Mechanism: Below a certain sampling rate threshold, object features become too degraded for reliable recognition; above this threshold, additional quality doesn't improve performance
- Core assumption: There exists a perceptual threshold that aligns with human visual perception capabilities
- Evidence anchors:
  - [abstract] "Object quality (sampling rate) higher than a certain threshold does not affect MLLMs' performance, and this threshold seems to align well with human perception"
  - [section 4.2] "we observed a significant improvement in the MLLMs' performance as the sampling rate increased from 4 to 8. However, after this point, the performance stabilized"
- Break condition: If MLLMs develop different quality thresholds based on training data characteristics or if the threshold varies significantly across model architectures

### Mechanism 3
- Claim: Object location within the image affects MLLM performance due to patch boundary interactions
- Mechanism: Objects divided by patch boundaries may be better recognized than those contained within single patches due to increased token representation and attention opportunities
- Core assumption: The raster-scan ordering of patch processing creates positional biases in feature extraction
- Evidence anchors:
  - [abstract] "the location of the object in the image and the presence of visual distractors can also significantly reduce MLLMs' question answering accuracy"
  - [section 4.5.2] "a common trend among all models is the performance decline at the center of the patch, where texts remain undivided by patch boundaries"
- Break condition: If models implement positional embeddings that compensate for patch boundary effects or if training data includes sufficient examples of boundary-cut objects

## Foundational Learning

- Concept: Transformer architecture and patch processing
  - Why needed here: Understanding how images are converted to tokens and how self-attention mechanisms process visual information
  - Quick check question: How many image patches does a 224×224 image with 16×16 patches produce?

- Concept: Object detection and bounding box annotations
  - Why needed here: Critical for understanding how the study measures object size and location relative to image dimensions
  - Quick check question: What information is contained in a bounding box annotation for visual question answering?

- Concept: Image quality metrics and sampling rate
  - Why needed here: Essential for interpreting the quality sensitivity experiments and understanding resolution effects
  - Quick check question: How does downsampling then upsampling an image affect the information content of the object?

## Architecture Onboarding

- Component map: Vision encoder (CLIP-ViT or similar) → Patch tokenization → Bridge module (Q-Former, linear projection, or cross-attention) → LLM backbone
- Critical path: Image preprocessing (resize, normalize) → Patch extraction and tokenization → Visual feature encoding → Cross-modal fusion → Text generation
- Design tradeoffs: Higher resolution inputs provide more detail but increase computational cost and may not proportionally improve small object recognition
- Failure signatures: Consistent performance drops for small objects across multiple models suggests architectural limitations rather than implementation issues
- First 3 experiments:
  1. Test object size sensitivity by creating synthetic images with identical content at different scales and measuring performance across the model suite
  2. Investigate patch boundary effects by systematically moving objects across patch boundaries and measuring recognition accuracy
  3. Validate quality threshold findings by creating a controlled dataset with varying sampling rates and measuring performance saturation points

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which object location affects MLLMs' perception?
- Basis in paper: [inferred] The paper shows significant performance variations across different object locations in images.
- Why unresolved: While the study identifies location as a factor affecting MLLMs' perception, it does not provide a detailed explanation of the underlying mechanisms.
- What evidence would resolve it: Detailed analysis of how MLLMs process images from different regions, including attention maps and processing pathways.

### Open Question 2
- Question: How can training data composition be optimized to improve MLLMs' perception of small objects?
- Basis in paper: [explicit] The study suggests that MLLMs trained on OCR-oriented datasets perform better on small object tasks.
- Why unresolved: The paper does not explore the specific types of training data that would most effectively enhance MLLMs' ability to perceive small objects.
- What evidence would resolve it: Experiments comparing the performance of MLLMs trained on datasets with varying proportions of small object examples.

### Open Question 3
- Question: Is there a way to design architectures that are less sensitive to the object location in the image?
- Basis in paper: [inferred] The study shows that MLLMs' performance fluctuates significantly with the location of the object in the image.
- Why unresolved: The paper identifies location as a factor affecting MLLMs' perception but does not propose architectural solutions to mitigate this effect.
- What evidence would resolve it: Development and testing of new MLLM architectures that show less variance in performance across different object locations.

## Limitations

- The study relies heavily on synthetic digit datasets, which may not fully represent real-world visual complexity
- Generalization of threshold-dependent quality sensitivity across diverse object categories remains uncertain
- The patch boundary effect mechanism lacks direct empirical validation through ablation studies

## Confidence

- High confidence: Object size directly influences performance through increased token representation
- Medium confidence: Threshold-dependent quality sensitivity aligning with human perception
- Medium confidence: Location-dependent performance variations due to patch boundary effects

## Next Checks

1. Validate quality threshold findings across diverse object categories beyond digits to assess generalizability of the perceptual threshold concept
2. Conduct ablation experiments comparing models with different patch sizes and positional embedding strategies to isolate the patch boundary effect mechanism
3. Test real-world visual question answering datasets with naturally occurring small objects to compare synthetic experiment results with practical performance limitations