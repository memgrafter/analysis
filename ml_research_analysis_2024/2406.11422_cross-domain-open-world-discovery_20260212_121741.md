---
ver: rpa2
title: Cross-domain Open-world Discovery
arxiv_id: '2406.11422'
source_url: https://arxiv.org/abs/2406.11422
tags:
- classes
- seen
- unseen
- crow
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CROW, a method for cross-domain open-world
  discovery that simultaneously recognizes seen classes and discovers unseen classes
  under domain shift. CROW leverages the structured latent space of foundation models
  through a cluster-then-match strategy: it first clusters target samples, then matches
  these clusters to seen classes using a robust prototype-based approach, and finally
  fine-tunes the representation space with a specially designed objective.'
---

# Cross-domain Open-world Discovery

## Quick Facts
- arXiv ID: 2406.11422
- Source URL: https://arxiv.org/abs/2406.11422
- Authors: Shuo Wen; Maria Brbic
- Reference count: 19
- Key outcome: CROW achieves 8% average improvement in H-score across 75 settings from four benchmark datasets

## Executive Summary
This paper introduces CROW, a method for cross-domain open-world discovery that simultaneously recognizes seen classes and discovers unseen classes under domain shift. CROW leverages the structured latent space of foundation models through a cluster-then-match strategy: it first clusters target samples, then matches these clusters to seen classes using a robust prototype-based approach, and finally fine-tunes the representation space with a specially designed objective. Experiments across 75 settings from four benchmark datasets show CROW outperforms existing methods by achieving an 8% average improvement in H-score.

## Method Summary
CROW addresses cross-domain open-world discovery by first extracting features from a foundation model, then clustering target samples to discover natural groupings, matching these clusters to seen classes using a robust prototype-based approach that allows multiple-to-multiple associations, and finally fine-tuning the representation space with a combined cross-entropy and entropy maximization objective. The method operates in a transductive setting with access to both labeled source and unlabeled target data, handling categorical and domain shifts between source and target distributions.

## Key Results
- CROW achieves 8% average improvement in H-score compared to existing methods
- Outperforms baseline methods across 75 experimental settings from four benchmark datasets
- Successfully handles both categorical and domain shifts simultaneously

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The cluster-then-match strategy avoids early bias toward seen classes by postponing the integration of seen-class knowledge until after clustering.
- Mechanism: Clustering target samples first in the well-structured latent space of a foundation model allows the model to discover natural groupings of both seen and unseen classes without being influenced by seen-class labels. Only after clusters are formed are seen classes matched to them, ensuring novel classes are not prematurely suppressed.
- Core assumption: Foundation models provide a well-structured latent space that naturally separates different semantic concepts.
- Evidence anchors: [abstract] "CROW first clusters target samples, then matches these clusters to seen classes using a robust prototype-based approach"; [section] "The key idea in CROW is to adopt the cluster-then-match strategy, enabled by the well-structured representation space of foundation models"

### Mechanism 2
- Claim: Robust matching allows multiple seen classes to map to the same target cluster and vice versa, mitigating over-clustering and under-clustering problems.
- Mechanism: By using a co-occurrence matrix and applying a threshold to the softmax distribution, CROW relaxes the one-to-one matching constraint. This flexibility lets the system handle situations where samples from multiple seen classes are grouped into one cluster (under-clustering) or a single seen class is split across multiple clusters (over-clustering).
- Core assumption: The distribution matrix D accurately reflects the similarity between seen classes and target prototypes.
- Evidence anchors: [section] "This matching strategy enables CROW to associate multiple target prototypes with seen classes, thereby alleviating the issues of over-clustering and under-clustering"; [section] "Instead, we adopt robust matching, which mitigates this problem by releasing the constraint of one-to-one matching"

### Mechanism 3
- Claim: Fine-tuning with a combined cross-entropy and entropy maximization objective improves the representation space for both seen and unseen classes.
- Mechanism: The cross-entropy loss transfers knowledge from seen classes to target samples, while the entropy maximization loss ensures balanced predictions across all classes, preventing the model from being overly confident about seen classes at the expense of novel ones.
- Core assumption: Balancing predictions between seen and unseen classes is critical for good open-world discovery performance.
- Evidence anchors: [section] "CROW combines cross-entropy loss applied to source samples with entropy maximization loss applied to target samples to further improve the representation space"; [section] "In addition, to balance the predictions of seen and unseen classes, we apply the regularization loss Lreg"

## Foundational Learning

- Concept: Transductive learning
  - Why needed here: The setting assumes access to both labeled source and unlabeled target data during training, requiring the model to use target information for discovery.
  - Quick check question: Does the method have access to target labels during training? (Answer: No, but it has access to unlabeled target data.)

- Concept: Domain adaptation
  - Why needed here: The setting explicitly involves domain shift between source and target data, requiring adaptation techniques to handle distributional differences.
  - Quick check question: Are source and target data assumed to come from the same distribution? (Answer: No, they are assumed to have different distributions.)

- Concept: Prototype-based clustering
  - Why needed here: The method relies on computing prototypes for both seen classes and target clusters to facilitate matching and discovery.
  - Quick check question: How are target clusters represented in the method? (Answer: As prototypes computed via K-means clustering in the feature space.)

## Architecture Onboarding

- Component map: Foundation model (feature extractor fθ) -> Seen-class prototype classifier Wseen -> Target prototype clustering (K-means) -> Matching module (co-occurrence matrix Γ, distribution matrix D, matching matrix M) -> Fine-tuning objective (cross-entropy + entropy regularization) -> Linear classifier W (combined seen and unseen prototypes)

- Critical path: 1. Extract features from foundation model for all samples; 2. Compute seen-class prototypes via supervised training; 3. Cluster target samples to obtain target prototypes; 4. Match seen classes to target prototypes using robust matching; 5. Initialize combined classifier with seen and unseen prototypes; 6. Fine-tune feature extractor and classifier with combined objective

- Design tradeoffs: Using foundation models provides good initial separation but may have seen unseen classes during pretraining; cluster-then-match avoids early bias but requires good clustering quality; robust matching handles over/under-clustering but depends on threshold selection; fine-tuning improves performance but risks overfitting to limited target data

- Failure signatures: Poor clustering quality in foundation model space (clusters mix seen/unseen classes); threshold τ too high (misses valid seen-class matches) or too low (incorrectly matches seen classes to novel clusters); λ too high (entropy term dominates, hurting seen-class accuracy) or too low (model overfits to seen classes)

- First 3 experiments: 1. Verify foundation model feature extraction works correctly and produces L2-normalized features; 2. Test clustering on target data only with known number of classes to verify cluster quality; 3. Validate matching procedure with synthetic data where ground truth cluster-to-class mappings are known

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we ensure that foundation models have not encountered the unseen classes during pretraining, and what impact does this have on the performance of cross-domain open-world discovery methods?
- Basis in paper: [explicit] The paper discusses the potential issue of foundation models having seen instances of unseen classes during pretraining, particularly in the context of cross-domain open-world discovery.
- Why unresolved: The paper acknowledges that this is a common issue in the research fields of novelty detection and category discovery but does not provide a definitive solution or methodology to avoid this problem.
- What evidence would resolve it: Experimental results comparing the performance of models trained on datasets with and without instances of unseen classes, along with a discussion on the implications of these findings for the reliability of cross-domain open-world discovery methods.

### Open Question 2
- Question: How can we design proper evaluation strategies for cross-domain open-world discovery that account for potential disagreements between discovered classes and ground truth annotations?
- Basis in paper: [inferred] The paper highlights the challenges in evaluating cross-domain open-world discovery methods, noting that disagreement with ground truth annotations may not necessarily indicate incorrect results.
- Why unresolved: The paper does not provide a clear framework or methodology for designing evaluation strategies that can handle such disagreements and ensure a fair assessment of the methods' performance.
- What evidence would resolve it: Development and validation of evaluation metrics or frameworks that can effectively assess the performance of cross-domain open-world discovery methods, considering the possibility of meaningful but non-ground truth discoveries.

### Open Question 3
- Question: How can we improve the robustness of cross-domain open-world discovery methods to datasets where foundation models lack robustness, such as the DomainNet Quickdraw dataset?
- Basis in paper: [explicit] The paper mentions that the performance of cross-domain open-world discovery methods may degrade on datasets where foundation models are not robust, providing the DomainNet Quickdraw dataset as an example.
- Why unresolved: The paper does not explore potential strategies or techniques to enhance the robustness of these methods to such challenging datasets.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of proposed techniques or strategies in improving the performance of cross-domain open-world discovery methods on datasets where foundation models lack robustness.

## Limitations
- The method's success critically depends on foundation models providing a well-structured latent space that naturally separates different semantic concepts, with only weak evidence supporting this assumption
- The robust matching mechanism relies on a fixed threshold (τ=0.3) that is not extensively validated across datasets and lacks guidance for practical selection
- The paper does not fully characterize the types and magnitudes of domain shifts that the method can handle, with no systematic analysis of failure modes for extreme shifts

## Confidence

**High Confidence**: The cluster-then-match strategy and overall framework design are well-specified and reproducible with clear mathematical formulation

**Medium Confidence**: Empirical results showing 8% average improvement in H-score are convincing given extensive evaluation across 75 settings and 4 benchmark datasets, though mechanism explanations rely heavily on general literature

**Low Confidence**: Claims about the robustness of the matching mechanism to over-clustering and under-clustering are supported by limited evidence without detailed failure case analysis or extensive ablation studies on threshold sensitivity

## Next Checks

1. **Latent Space Validation**: Conduct experiments to explicitly validate that the foundation model's latent space separates known and novel classes appropriately for each domain pair, using t-SNE visualizations or clustering purity measurements when target data has known class labels.

2. **Threshold Sensitivity Analysis**: Systematically vary the matching threshold τ across a wider range (e.g., 0.1 to 0.5) and evaluate performance changes to identify optimal thresholds for each dataset and determine whether dataset-specific tuning is required.

3. **Failure Mode Analysis**: Design experiments testing the method's robustness to extreme domain shifts, visual similarity between seen and unseen classes, and mis-specified numbers of target clusters, documenting exact failure conditions and quantifying performance degradation.