---
ver: rpa2
title: 'Exploring the use of a Large Language Model for data extraction in systematic
  reviews: a rapid feasibility study'
arxiv_id: '2405.14445'
source_url: https://arxiv.org/abs/2405.14445
tags:
- equal
- data
- studies
- change
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A rapid feasibility study assessed GPT-4's ability to extract data
  from systematic review abstracts across three domains. Using 36 test studies and
  100 EBM-NLP abstracts, accuracy averaged 80% overall, with higher accuracy for clinical
  (82%) and animal (80%) studies than social science (72%).
---

# Exploring the use of a Large Language Model for data extraction in systematic reviews: a rapid feasibility study

## Quick Facts
- arXiv ID: 2405.14445
- Source URL: https://arxiv.org/abs/2405.14445
- Reference count: 0
- Primary result: GPT-4 achieves ~80% accuracy extracting data from systematic review abstracts, with higher accuracy for clinical and animal studies than social science.

## Executive Summary
This rapid feasibility study evaluates GPT-4's ability to extract structured data from systematic review abstracts across clinical, animal, and social science domains. Using 36 test studies and 100 EBM-NLP abstracts, the researchers found an overall accuracy of approximately 80%, with domain-specific performance varying (82% clinical, 80% animal, 72% social science). The study provides a replicable template for evaluating LLM performance in systematic reviews, demonstrating that while LLMs can be useful for semi-automation, they are not yet suitable for full automation due to variability and limited explainability.

## Method Summary
The study employed a structured evaluation approach using GPT-4 via Azure OpenAI API with temperature=0 and top_p=0.95. Researchers developed prompts iteratively using 2 studies per domain, then applied them to test sets of 10 studies each from clinical, animal, and social science domains. Human coders created gold standard data by manually extracting information from abstracts, which was then compared against LLM outputs. Evaluation included measuring accuracy across different data types (Boolean, numeric, string) and assessing response stability by submitting identical prompts multiple times. Traditional NLP metrics (BLEU/ROUGE) were also tested but found uninformative.

## Key Results
- Overall accuracy of ~80% across all domains, with clinical studies at 82% and social science at 72%
- Higher accuracy for structured fields (Boolean/numeric) than for free-text string fields
- Response stability varied: identical prompts yielded identical Boolean/numeric answers but differed in ~30% of string-based cases
- BLEU/ROUGE scores were uninformative compared to manual assessment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GPT-4 can extract atomic data items from study abstracts with ~80% overall accuracy.
- **Mechanism**: The model is prompted to map free-text descriptions into structured JSON fields, leveraging its language understanding to identify and normalize entities such as study design, participant counts, and outcomes.
- **Core assumption**: Abstracts contain sufficient explicit or implicit information for the model to infer the requested data items.
- **Evidence anchors**:
  - [abstract]: "Overall, results indicated an accuracy of around 80%, with some variability between domains (82% for human clinical, 80% for animal, and 72% for studies of human social sciences)."
  - [section 7.1]: "Overall, 260 pieces of information were extracted from the 30 studies in the test set. 78% (n=194) were adjudged to be completely correct."
- **Break condition**: Abstracts lack sufficient detail or structure; prompts are not domain-specific; or the model's training data does not cover the domain's terminology.

### Mechanism 2
- **Claim**: LLM accuracy is higher for structured, binary, or numeric fields than for free-text strings.
- **Mechanism**: The model performs better when asked to return simple classifications (yes/no, numbers) rather than generate natural language summaries, because the former are less ambiguous and require less inference.
- **Core assumption**: Structured prompts reduce the variability in model output.
- **Evidence anchors**:
  - [abstract]: "Binary and Boolean data types seem to be extracted better than more open response types (strings)."
  - [section 7.1.1]: "We found that responses for 'Boolean' and 'Number' questions were entirely stable. However, for 'String' questions, answers were identical only 69% of the time..."
- **Break condition**: Prompts are ambiguous or require multi-step reasoning that pushes the model toward string output.

### Mechanism 3
- **Claim**: Prompt order and wording affect output quality.
- **Mechanism**: The model's context window and sequential prompt processing mean that earlier prompts can prime or bias later outputs; small wording changes can shift the model's focus or interpretation.
- **Core assumption**: The model treats the entire prompt sequence as a single contextual input rather than isolated questions.
- **Evidence anchors**:
  - [section 4.1]: "We found that prompts were sensitive to: Minor changes in wording... Changes in position in the sequence of prompts..."
  - [section 8.3]: "We also found that the ordering of the prompts in a sequence made a difference to the responses."
- **Break condition**: Prompts are submitted in isolation (e.g., one per API call) or the model's context length is exceeded.

## Foundational Learning

- **Concept**: Data extraction in systematic reviews
  - Why needed here: The LLM's task is to mimic human data extraction from abstracts; understanding the domain's conventions (PICO, study design, outcomes) is essential for prompt design.
  - Quick check question: What are the core data items typically extracted in a systematic review of clinical trials?

- **Concept**: JSON structured output
  - Why needed here: The LLM must return results in a machine-readable format for ingestion into review tools; knowing JSON schema design helps craft prompts that yield usable data.
  - Quick check question: How would you structure a JSON object to capture study design, sample size, and outcomes?

- **Concept**: Prompt engineering and sensitivity
  - Why needed here: Small changes in prompt wording or order can significantly alter LLM output; mastering this is critical for reproducible data extraction.
  - Quick check question: What happens if you swap the order of two prompts asking for "study design" and "outcomes"?

## Architecture Onboarding

- **Component map**: Abstract → Prompt → GPT-4 → JSON → Human evaluation
- **Critical path**: Study abstracts are processed through human-designed prompts into GPT-4, which returns structured JSON output for human evaluation against gold standard data.
- **Design tradeoffs**:
  - Prompt batching (multiple fields in one call) vs. isolation (one field per call) – batching risks ordering effects but is more efficient.
  - Temperature=0 for stability vs. allowing creativity for ambiguous cases – zero temperature gives deterministic output for Boolean/numeric but not for strings.
  - Manual vs. automated evaluation – BLEU/ROUGE were uninformative here; human assessment was more reliable but labor-intensive.
- **Failure signatures**:
  - Inconsistent output for identical prompts on string fields (~30% variability).
  - Hallucinations (e.g., LLM invents study design not mentioned in abstract).
  - Poor performance on social science abstracts due to less standardized structure.
- **First 3 experiments**:
  1. Submit identical prompts twice to the same abstract; compare output for Boolean, numeric, and string fields to measure stability.
  2. Swap prompt order in a multi-field request; measure changes in accuracy and completeness.
  3. Vary wording of a single prompt (e.g., "list outcomes" vs. "describe outcomes") and evaluate impact on extraction quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes the observed instability in LLM-generated string outputs when using identical prompts with temperature=0?
- Basis in paper: [explicit] The paper notes that identical prompts yielded identical Boolean/numeric answers but differed in ~30% of string-based cases, with 7% showing substantive differences.
- Why unresolved: The paper suggests this might be related to the top_p parameter (set to 0.95) but does not test this hypothesis or investigate the underlying cause of the instability.
- What evidence would resolve it: A systematic study varying top_p and other parameters (e.g., max_tokens, presence_penalty) to identify their relationship to output stability, along with qualitative analysis of what distinguishes stable vs. unstable prompts.

### Open Question 2
- Question: How would LLM performance change when processing full-text articles rather than abstracts only?
- Basis in paper: [inferred] The authors note they applied the model to abstracts only "which may limit its generalisability to real-world data extraction scenarios."
- Why unresolved: The study only evaluated abstracts, which typically contain less detailed information than full-text articles, potentially limiting the LLM's ability to extract comprehensive data.
- What evidence would resolve it: A direct comparison study applying the same evaluation framework to both abstracts and full-text versions of the same studies, measuring accuracy differences across data extraction tasks.

### Open Question 3
- Question: Can the BLEU and ROUGE metrics be adapted or replaced to provide meaningful evaluation of LLM outputs for systematic review data extraction?
- Basis in paper: [explicit] The authors found "that their results were not meaningful when compared to our human assessment results" and note that "more research is needed to determine data format and evaluation methods."
- Why unresolved: The generative nature of LLM outputs and the semantic complexity of systematic review data make traditional evaluation metrics inadequate, yet no alternative approaches are proposed or tested.
- What evidence would resolve it: Development and validation of new evaluation metrics specifically designed for systematic review data extraction tasks, potentially incorporating semantic similarity measures or domain-specific evaluation frameworks.

## Limitations

- Exact prompt wording and structure were not fully disclosed, limiting reproducibility despite general categories being provided
- Manual evaluation introduces potential rater bias, particularly for borderline cases classified as "partial" versus "incorrect"
- Results are based on three specific domains (clinical, animal, social science) and may not generalize to other systematic review contexts

## Confidence

- **High confidence**: The core finding that GPT-4 achieves ~80% accuracy in structured data extraction from systematic review abstracts, with higher accuracy for clinical and animal studies than social science.
- **Medium confidence**: The conclusion that prompt sensitivity and ordering significantly affect output quality, as this is supported but the specific mechanisms are not fully elucidated.
- **Medium confidence**: The recommendation that LLMs are suitable for semi-automation but not full automation, given the variability and limited explainability observed.

## Next Checks

1. **Prompt reproducibility test**: Using the published prompt categories, develop multiple specific prompt variants and test them across the same abstracts to quantify the impact of wording changes on extraction accuracy and stability.

2. **Cross-domain validation**: Apply the same evaluation framework to systematic review abstracts from additional domains (e.g., engineering, education, policy) to assess generalizability beyond the three tested domains.

3. **Automated evaluation comparison**: Develop and test a simple automated evaluation metric tailored to the structured output format (e.g., field-matching accuracy) and compare its performance and efficiency to manual human assessment.