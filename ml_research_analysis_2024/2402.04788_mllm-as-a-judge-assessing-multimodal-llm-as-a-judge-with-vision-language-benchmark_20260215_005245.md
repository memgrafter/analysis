---
ver: rpa2
title: 'MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language
  Benchmark'
arxiv_id: '2402.04788'
source_url: https://arxiv.org/abs/2402.04788
tags:
- uni00000013
- uni00000011
- uni00000026
- uni0000004c
- uni00000044
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a novel benchmark, MLLM-as-a-Judge, to evaluate
  the ability of Multimodal Large Language Models (MLLMs) to act as judges in tasks
  involving Scoring Evaluation, Pair Comparison, and Batch Ranking. The benchmark
  is constructed using 3,300 image-instruction pairs from 10 diverse datasets, with
  responses generated by four MLLMs: GPT-4V, Gemini, LLaVA, and CogVLM.'
---

# MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark

## Quick Facts
- arXiv ID: 2402.04788
- Source URL: https://arxiv.org/abs/2402.04788
- Reference count: 40
- The paper introduces MLLM-as-a-Judge, a benchmark to evaluate Multimodal Large Language Models (MLLMs) as judges across Scoring Evaluation, Pair Comparison, and Batch Ranking tasks.

## Executive Summary
This paper introduces MLLM-as-a-Judge, a benchmark designed to assess the ability of Multimodal Large Language Models (MLLMs) to act as judges in multimodal tasks. Using 3,300 image-instruction pairs from 10 diverse datasets, the study evaluates four leading MLLMs—GPT-4V, Gemini, LLaVA, and CogVLM—across three distinct evaluation settings. Human annotations are used to validate MLLM judgments, revealing that while MLLMs perform well in Pair Comparison, they struggle in Scoring Evaluation and Batch Ranking due to biases, hallucinations, and inconsistencies. GPT-4V aligns best with human preferences but still shows room for improvement. The study highlights the challenges of using MLLMs as reliable judges and advocates for further research to enhance their judging capabilities.

## Method Summary
The paper constructs a benchmark using 3,300 image-instruction pairs from 10 diverse datasets. Responses are generated by four MLLMs (GPT-4V, Gemini, LLaVA, CogVLM) and evaluated across three tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Human annotations are used to validate MLLM judgments, with tailored metrics (Pearson similarity, accuracy, F1-score, recall, and Normalized Levenshtein distance) assessing alignment with human preferences. The study also explores the impact of vision descriptions and Chain-of-Thought (CoT) reasoning on MLLM performance.

## Key Results
- GPT-4V outperforms other MLLMs in Pair Comparison tasks, achieving 0.683 in tie settings and 0.806 in non-tie settings.
- MLLMs struggle in Scoring Evaluation and Batch Ranking, exhibiting biases (egocentric, position, length) and hallucinations.
- LLMs with detailed vision descriptions perform competitively in multimodal judging tasks, sometimes outperforming MLLMs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLLMs can serve as judges in multimodal tasks by evaluating responses through structured comparison and ranking.
- Mechanism: The paper introduces a benchmark with three distinct tasks (Scoring Evaluation, Pair Comparison, Batch Ranking) where MLLMs generate judgments on responses to image-instruction pairs. Human annotations are used to validate these judgments.
- Core assumption: MLLMs possess sufficient multimodal understanding and reasoning capabilities to evaluate responses consistently with human preferences.
- Evidence anchors:
  - [abstract] "We evaluate the judging performance of four leading MLLMs – GPT-4V, Gemini, LLaVA, and CogVLM – across three distinct evaluation settings."
  - [section 4.1] "GPT-4V outperforms other MLLMs in pair comparison tasks, achieving 0.683 in tie settings and 0.806 in non-tie settings."
  - [corpus] Weak: The corpus does not directly discuss MLLMs as judges but includes related work on MLLM-as-a-Judge for image safety and reward modeling.
- Break condition: If MLLMs cannot consistently align their judgments with human preferences, especially in complex tasks like Scoring Evaluation and Batch Ranking, their reliability as judges is compromised.

### Mechanism 2
- Claim: Human-like discernment in Pair Comparison tasks indicates MLLMs' potential as evaluators.
- Mechanism: In Pair Comparison, MLLMs directly compare two responses and identify the superior one, showing alignment with human preferences.
- Core assumption: MLLMs can discern subtle differences in response quality and make consistent judgments in pairwise comparisons.
- Evidence anchors:
  - [abstract] "Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons..."
  - [section 4.1] "GPT-4V achieves 0.683 in tie settings and 0.806 in non-tie settings, surpassing 0.8 in many datasets."
  - [corpus] Weak: The corpus includes work on evaluating MLLMs as judges but lacks specific discussion on pairwise comparisons.
- Break condition: If MLLMs show biases or inconsistencies in Pair Comparison tasks, their potential as evaluators diminishes.

### Mechanism 3
- Claim: Detailed vision descriptions enhance MLLMs' judging performance in multimodal tasks.
- Mechanism: Providing LLMs with detailed image descriptions allows them to perform judging tasks without direct vision input, sometimes outperforming MLLMs.
- Core assumption: LLMs can effectively judge multimodal tasks when given comprehensive task-related descriptions, compensating for the lack of direct vision perception.
- Evidence anchors:
  - [abstract] "We explore the feasibility of using LLMs for judging text-based responses without directly analyzing the original images."
  - [section 4.3] "LLMs’ performance in multimodal judging tasks significantly improved with picture descriptions, achieving a Pearson similarity of 0.435 in Scoring Evaluation tasks."
  - [corpus] Weak: The corpus does not directly address the use of vision descriptions in MLLM judging.
- Break condition: If LLMs fail to maintain judging accuracy without direct vision input, the approach is not viable.

## Foundational Learning

- Concept: Multimodal Large Language Models (MLLMs)
  - Why needed here: Understanding MLLMs is crucial as they are the primary subjects of the study, evaluated for their ability to judge multimodal tasks.
  - Quick check question: What are the key capabilities of MLLMs that enable them to perform judging tasks across different modalities?

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: CoT is used to mitigate hallucinations and improve the reasoning capabilities of MLLMs in judging tasks.
  - Quick check question: How does the integration of CoT reasoning enhance the judgment quality of MLLMs?

- Concept: Bias and hallucination in AI models
  - Why needed here: Identifying and mitigating biases and hallucinations is essential for improving the reliability of MLLMs as judges.
  - Quick check question: What types of biases and hallucinations are observed in MLLMs during judging tasks, and how do they impact the evaluation process?

## Architecture Onboarding

- Component map:
  Image-Instruction Pair Collection -> MLLM Response Collection -> Human Annotation -> Judging Metrics

- Critical path:
  1. Collect diverse image-instruction pairs from various datasets.
  2. Generate responses using MLLMs and segment them into non-overlapping groups.
  3. Obtain human annotations to compare with MLLM judgments.
  4. Analyze results using tailored metrics for each judging task.

- Design tradeoffs:
  - Balancing the complexity of tasks with the capabilities of MLLMs.
  - Ensuring diversity in datasets to prevent biases.
  - Managing the trade-off between structured and natural responses in MLLM outputs.

- Failure signatures:
  - MLLMs exhibit significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks.
  - Presence of biases such as egocentric, position, and length biases in MLLM judgments.
  - Hallucinations and inconsistencies in responses, particularly in complex reasoning tasks.

- First 3 experiments:
  1. Evaluate MLLMs' performance in Pair Comparison tasks to assess their alignment with human preferences.
  2. Analyze the impact of vision descriptions on MLLMs' judging accuracy in multimodal tasks.
  3. Investigate the effectiveness of CoT reasoning in reducing hallucinations and improving judgment consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompting strategies (e.g., Chain-of-Thought, vision expert descriptions) impact the reliability of MLLM judgments in Scoring Evaluation tasks?
- Basis in paper: [explicit] The paper explores various prompting strategies, including Chain-of-Thought and vision expert descriptions, and notes their impact on reducing hallucinations and improving judgment reliability.
- Why unresolved: The paper demonstrates that these strategies can improve performance, but it does not provide a comprehensive comparison of their effectiveness across different tasks or datasets. Additionally, the optimal combination of strategies for maximizing reliability remains unclear.
- What evidence would resolve it: A systematic evaluation of different prompting strategies across multiple tasks and datasets, measuring their impact on reliability metrics such as human agreement and consistency.

### Open Question 2
- Question: What are the underlying causes of egocentric bias in MLLMs, and how can they be mitigated to improve fairness in judging?
- Basis in paper: [explicit] The paper identifies egocentric bias as a significant issue, where MLLMs tend to assign higher scores to their own responses compared to others.
- Why unresolved: The paper attributes this bias to MLLMs' reliance on their built-in ethical guidelines and post-alignment training, but the specific mechanisms driving this behavior are not fully understood. Additionally, the paper does not explore potential mitigation strategies in depth.
- What evidence would resolve it: A detailed analysis of the training data and model architectures to identify the sources of egocentric bias, along with experiments testing various mitigation techniques such as debiasing algorithms or alternative training objectives.

### Open Question 3
- Question: How does the integration of vision perception in MLLMs affect their ability to judge text-only responses compared to pure LLMs?
- Basis in paper: [explicit] The paper finds that MLLMs perform better in judging tasks when provided with detailed image descriptions, suggesting that vision perception may enhance their judging capabilities.
- Why unresolved: The paper does not explore the specific ways in which vision perception influences judgment, nor does it compare the performance of MLLMs with that of pure LLMs in text-only judging tasks.
- What evidence would resolve it: A comparative study of MLLMs and pure LLMs in judging text-only responses, analyzing their performance across various tasks and identifying the unique advantages and limitations of vision perception in this context.

## Limitations
- The evaluation is limited by the relatively small scale of human annotations, which may not fully capture the variability in human judgment.
- The study focuses on a specific set of 10 datasets, which may not be representative of all multimodal tasks.
- The mechanisms behind MLLMs' biases and hallucinations in judging tasks are not fully explored.

## Confidence

- **High Confidence:** The finding that GPT-4V aligns best with human preferences in Pair Comparison tasks is well-supported by quantitative metrics (0.683 in tie settings, 0.806 in non-tie settings) and consistent across multiple datasets.
- **Medium Confidence:** The claim that MLLMs struggle in Scoring Evaluation and Batch Ranking is supported by evidence but may be influenced by the specific task design and dataset selection.
- **Low Confidence:** The assertion that LLMs can effectively judge multimodal tasks without direct vision input is based on limited evidence and requires further validation across diverse tasks and datasets.

## Next Checks
1. Expand human annotations to ensure robustness in evaluating MLLM judgments across a wider range of tasks and contexts.
2. Validate the findings by applying the benchmark to additional multimodal tasks beyond the current 10 datasets to assess generalizability.
3. Conduct a deeper investigation into the specific biases and hallucinations observed in MLLMs, including their root causes and potential mitigation strategies.