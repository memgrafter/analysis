---
ver: rpa2
title: Benchmarking Complex Instruction-Following with Multiple Constraints Composition
arxiv_id: '2407.03978'
source_url: https://arxiv.org/abs/2407.03978
tags:
- instructions
- composition
- llms
- scoring
- complex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ComplexBench, a benchmark for evaluating
  large language models' ability to follow complex instructions with multiple constraints
  composition. The authors propose a hierarchical taxonomy of constraints (4 types,
  19 dimensions) and composition types (4 types), and manually construct a high-quality
  dataset of 1,150 instructions.
---

# Benchmarking Complex Instruction-Following with Multiple Constraints Composition

## Quick Facts
- arXiv ID: 2407.03978
- Source URL: https://arxiv.org/abs/2407.03978
- Reference count: 40
- ComplexBench benchmark reveals LLMs achieve only 53% accuracy on complex nested instructions

## Executive Summary
This paper introduces ComplexBench, a benchmark designed to evaluate large language models' ability to follow complex instructions with multiple constraints composition. The authors develop a hierarchical taxonomy of constraints and composition types, construct a high-quality dataset of 1,150 instructions, and propose a rule-augmented LLM-based evaluation method. Experiments on 15 LLMs reveal significant deficiencies in handling complex instructions, with state-of-the-art models achieving only 53% accuracy on nested structures and struggling particularly with format constraints and chain compositions.

## Method Summary
The ComplexBench methodology combines manual instruction construction with a rule-augmented LLM-based evaluation approach. Instructions are designed with specific constraint types (semantic, utility, format, lexical) and composition structures (And, Chain, Selection, Nested). The evaluation method integrates rule-based verification for objective constraints with LLM-based assessment for subjective requirements, using dependency aggregation to preserve structural relationships between constraints. Performance is measured using DRFR (Decomposed Requirements Following Ratio), which evaluates how well models satisfy individual constraints within complex instructions.

## Key Results
- GPT-4 achieves only 80% accuracy on simple compositions and drops to 53% on nested structures
- Format constraints show lowest performance at 53% accuracy across all models
- Chain compositions perform at 60% accuracy, while Selection compositions reach 66%
- Even state-of-the-art models struggle with multi-layer nested Selection instructions (14.9% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical constraint taxonomy enables granular evaluation of instruction-following capabilities.
- Mechanism: The taxonomy structures constraints into 4 types and 19 dimensions, allowing systematic coverage of diverse requirements. This granularity enables precise measurement of LLM performance on specific constraint categories.
- Core assumption: Complex instructions can be decomposed into manageable constraint dimensions that can be evaluated independently.
- Evidence anchors:
  - [abstract]: "We propose a hierarchical taxonomy for complex instructions, including 4 constraint types, 19 constraint dimensions, and 4 composition types"
  - [section 3.2]: "we propose a two-level structure for constraints including 4 constraint types... and 19 specific constraint dimensions"

### Mechanism 2
- Claim: Rule-augmented LLM-based evaluation overcomes limitations of pure LLM or pure rule-based approaches.
- Mechanism: Rules handle objective verification (keyword inclusion, length constraints) while LLMs handle subjective assessments. This hybrid approach leverages strengths of both methods.
- Core assumption: Rule-defined constraints can be reliably verified with deterministic rules, while open-ended constraints require LLM judgment.
- Evidence anchors:
  - [section 4.2]: "LLM-based methods are effective at answering open-ended scoring questions, but they demonstrate a significant deficiency in those involving numerical computation, counting, and other objective rule-defined areas"
  - [section 5.1]: "RAL outperforms all the baselines and exhibits an impressive 87.82% agreement with humans at the overall level"

### Mechanism 3
- Claim: Dependency-based aggregation preserves structural relationships between constraints.
- Mechanism: The evaluation method aggregates scores based on composition type dependencies, ensuring that failing a prerequisite constraint invalidates dependent constraints.
- Core assumption: Constraint satisfaction in complex instructions follows logical dependency structures that must be preserved in evaluation.
- Evidence anchors:
  - [section 4.2]: "we model the dependencies of its scoring questions... for Chain, all the scoring questions of the subsequent task depend on the answers to those of the preceding task"
  - [section 5.1]: "Dependency Aggregation also shows its important contribution to our method due to its modeling of composition structures"

## Foundational Learning

- Concept: Constraint composition types (And, Chain, Selection)
  - Why needed here: Understanding these composition types is essential for interpreting how complex instructions structure multiple requirements
  - Quick check question: What's the difference between "And" and "Chain" composition types in instruction following?

- Concept: Rule-based vs LLM-based evaluation
  - Why needed here: The benchmark uses a hybrid evaluation approach, requiring understanding of when each method is appropriate
  - Quick check question: When would you use rule-based verification versus LLM-based evaluation for instruction-following tasks?

- Concept: Dependency structures in evaluation
  - Why needed here: The final scoring depends on modeling how constraint satisfaction relates to other constraints based on composition types
  - Quick check question: How does failing a Chain composition type's first task affect evaluation of subsequent tasks?

## Architecture Onboarding

- Component map: Data Collection -> Taxonomy Definition -> Dataset Construction -> Evaluation Method Development -> Model Testing -> Analysis
- Critical path: Data Collection → Taxonomy Definition → Dataset Construction → Evaluation Method Development → Model Testing → Analysis
- Design tradeoffs:
  - Manual annotation vs automated generation: Manual ensures quality but limits scale
  - Pure LLM vs hybrid evaluation: Hybrid balances accuracy with computational feasibility
  - Simple vs complex composition types: More complex types better reflect real-world scenarios but increase evaluation difficulty
- Failure signatures:
  - Poor human agreement in meta-evaluation indicates evaluator bias or inadequate prompt design
  - Inconsistent performance across similar constraint types suggests taxonomy issues
  - Large performance drops on complex compositions indicate model limitations in handling structural complexity
- First 3 experiments:
  1. Evaluate agreement between rule-based and LLM-based methods on objective constraints to verify hybrid approach benefits
  2. Test dependency aggregation by comparing Chain instruction evaluation with and without dependency modeling
  3. Analyze performance variance across different task categories to identify systematic strengths/weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum depth of nested composition types (e.g., Selection within Selection) that current LLMs can reliably handle?
- Basis in paper: Explicit - The paper notes that "even the state-of-the-art LLM, GPT-4, achieves only 14.9% accuracy in the coherent test" for multi-layer nested Selection instructions.
- Why unresolved: The paper identifies this as a significant limitation but doesn't explore the precise threshold of composition complexity that causes performance degradation.
- What evidence would resolve it: Systematic testing of LLMs across varying depths of nested composition types, identifying the exact point where accuracy drops below acceptable thresholds.

### Open Question 2
- Question: How do different instruction decomposition strategies affect LLM performance on complex instructions with Chain and Selection compositions?
- Basis in paper: Explicit - The paper mentions that "GPT-3.5-Turbo-1106 generally performs worse in decomposed instructions" and suggests this is due to "cumulative errors in multi-round interactions."
- Why unresolved: The paper only tests one decomposition approach and doesn't explore alternative strategies or their effectiveness.
- What evidence would resolve it: Comparative analysis of multiple decomposition methods (step-by-step, parallel, hierarchical) across different composition types, measuring which approach yields optimal performance.

### Open Question 3
- Question: What specific aspects of Format constraints (length, punctuation, templates) are most challenging for LLMs, and why?
- Basis in paper: Explicit - The paper notes that "LLMs generally perform better on Semantic and Utility constraints but struggle with the Format and Lexical constraints that have explicit evaluation standards," with Length achieving only 53% accuracy even for the best model.
- Why unresolved: The paper identifies Format constraints as problematic but doesn't analyze which specific sub-dimensions are most difficult or the underlying reasons.
- What evidence would resolve it: Detailed error analysis of LLM failures on individual Format sub-dimensions, identifying patterns in where and why models struggle with format compliance.

## Limitations
- Rule-augmented evaluation shows significant variance across different instruction types, particularly struggling with format constraints and chain compositions
- Manual annotation limits dataset scale and may introduce bias in instruction complexity distribution
- Performance degradation on nested compositions may reflect evaluation limitations rather than pure model capability issues

## Confidence
- High confidence: Basic constraint taxonomy and dataset construction methodology
- Medium confidence: Performance rankings of LLMs on simple instruction compositions
- Low confidence: Evaluation accuracy for complex nested compositions and format constraints

## Next Checks
1. **Evaluator Reliability Test**: Conduct inter-rater reliability analysis between multiple LLM evaluators and human annotators specifically for format constraints and nested compositions, where current evaluation shows the lowest accuracy.

2. **Cross-dataset Validation**: Apply the ComplexBench evaluation methodology to existing instruction-following datasets (e.g., IFEval, AGENTIF) to verify consistency of findings across different benchmarks.

3. **Error Analysis on Failure Cases**: Systematically analyze a random sample of failed complex instructions to determine whether failures stem from model limitations or evaluator shortcomings, particularly focusing on the 47% failure rate on format constraints.