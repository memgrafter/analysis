---
ver: rpa2
title: Cache-Aware Reinforcement Learning in Large-Scale Recommender Systems
arxiv_id: '2404.14961'
source_url: https://arxiv.org/abs/2404.14961
tags:
- cache
- real-time
- carl
- learning
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CARL (Cache-Aware Reinforcement Learning),
  a method to improve user engagement in large-scale recommender systems that use
  result caching during high-traffic periods. The core idea is to model the interaction
  between users and the system as a Markov Decision Process, explicitly incorporating
  a cache state that represents whether recommendations are computed in real-time
  or retrieved from cache.
---

# Cache-Aware Reinforcement Learning in Large-Scale Recommender Systems

## Quick Facts
- arXiv ID: 2404.14961
- Source URL: https://arxiv.org/abs/2404.14961
- Reference count: 20
- One-line primary result: Up to 0.586% improvement in daily watch time compared to baseline

## Executive Summary
This paper introduces CARL (Cache-Aware Reinforcement Learning), a method to improve user engagement in large-scale recommender systems that use result caching during high-traffic periods. The core innovation is modeling the interaction between users and the system as a Markov Decision Process with an explicit cache state that represents whether recommendations are computed in real-time or retrieved from cache. The authors propose an Eigenfunction Learning (EL) algorithm to address the "critic dependency" problem that arises when cache states are correlated across time steps, significantly improving learning stability and performance.

## Method Summary
CARL models the recommender system as an MDP with user states and a cache state determined by system load. During high-traffic periods, the system retrieves recommendations from cache rather than computing them in real-time. The Eigenfunction Learning (EL) algorithm learns independent critic functions for real-time and cached recommendations by transforming them into eigen long-term rewards, addressing the critic dependency problem. The method was deployed in the Kwai app serving over 100 million users and demonstrated significant improvements in user engagement metrics compared to baseline approaches.

## Key Results
- CARL-EL achieved up to 0.586% improvement in daily watch time compared to baseline methods
- CARL-EL outperformed CARL-DL (dual learning) and other RL baselines including CEM, TD3, and RLUR
- The method successfully scales to production environments serving over 100 million users

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Introducing a cache state decouples user-dependent states from system-dependent load, enabling explicit modeling of cache-aware recommendations.
- Mechanism: By adding a cache state C_t ∈ {0,1} that is determined by system load rather than user features, the MDP can differentiate between real-time and cached recommendations. This allows the critic functions Q0 and Q1 to be learned separately for each mode, addressing the unpredictability of cache choice.
- Core assumption: Cache state is independent of user state and action, determined solely by system traffic.
- Evidence anchors: [abstract] "We formulate the problem as a Markov decision process with user states and a cache state, where the cache state represents whether the recommender system performs recommendations by real-time computation or by the cache. The computational load of the recommender system determines the cache state."

### Mechanism 2
- Claim: Eigenfunction Learning (EL) decouples dependent critic functions Q0 and Q1 by transforming them into independent eigen long-term rewards Λ_a and Λ_b.
- Mechanism: EL defines eigen immediate rewards Γ_a and Γ_b as linear combinations of V0 and V1, and eigen long-term rewards Λ_a and Λ_b as combinations of Q0 and Q1. The iterative functions of Λ_a and Λ_b are independent, removing the critic dependency introduced by the stochastic cache state.
- Core assumption: The eigen transformations are invertible and preserve the information needed to recover Q0 and Q1.
- Evidence anchors: [section] "We first define the eigen immediate reward and the eigen long-term reward... The eigen long-term rewards Λ_a and Λ_b are independent."

### Mechanism 3
- Claim: The difference between real-time and cached recommendations (Λ_a) is constant over time and independent of cache ratios, simplifying learning.
- Mechanism: According to Proposition 3.3, Λ_a = Γ_a = V0 - V1, which means the difference in immediate rewards between real-time and cached recommendations is the same as the difference in long-term rewards. This difference is independent of the cache state probabilities D0(t) and D1(t), making Λ_a easier to learn.
- Core assumption: The immediate reward difference V0 - V1 is stable and does not depend on future cache states.
- Evidence anchors: [section] "It can be explained by Eq.(15), where Λ_a = Q0 - Q1 = V0 - V1, which means the difference between real-time and cached recommendations is independent of the cache ratios D0(t) and D1(t)."

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The recommender system is modeled as an MDP to capture the sequential interaction between users and the system, enabling reinforcement learning to optimize long-term user engagement.
  - Quick check question: What are the key components of an MDP, and how do they relate to the recommender system in CARL?

- Concept: Reinforcement Learning (RL) with Actor-Critic Architecture
  - Why needed here: CARL uses an actor-critic architecture to learn a policy function μ(s_t; θ) that maps user states to actions, and critic functions Q0 and Q1 that estimate the long-term reward for real-time and cached recommendations, respectively.
  - Quick check question: How does the actor-critic architecture in CARL differ from traditional RL methods, and why is this difference important?

- Concept: Temporal Difference (TD) Learning
  - Why needed here: CARL uses TD learning to update the critic functions Q0 and Q1 based on the difference between the current estimate and the target (r_t + γQ(s_t+1, μ(s_t+1; θ-); φ-)). This allows the critic to learn the expected long-term reward.
  - Quick check question: How does TD learning work in CARL, and what is the role of the target actor and critic in the learning process?

## Architecture Onboarding

- Component map: User Session -> Traffic Router -> Cache State C_t -> Real-time Recommendation OR Result Cache -> User Feedback r_t -> Replay Buffer -> CARL-EL Algorithm -> Updated Model

- Critical path:
  1. User sends request
  2. Traffic router determines cache state C_t based on system load
  3. If C_t = 0, compute real-time recommendation and update cache
  4. If C_t = 1, retrieve cached recommendation and update cache
  5. User provides feedback r_t
  6. Store (s_t, a_t, r_t, s_t+1, C_t, C_t+1) in replay buffer
  7. Sample from replay buffer and update CARL-EL model
  8. Deploy updated model in production

- Design tradeoffs:
  - Cache size (L) vs. computational cost: Larger cache reduces real-time computation but increases memory usage
  - Cache hit ratio vs. recommendation quality: Higher cache hit ratio reduces computational cost but may lower recommendation quality if cached items are suboptimal
  - Model complexity vs. learning stability: More complex models may capture nuanced user behavior but may be harder to train stably

- Failure signatures:
  - High cache miss rate: Indicates cache is not effectively reducing computational cost
  - Low cache hit ratio: Indicates cache is not effectively storing relevant items
  - High variance in critic loss: Indicates learning instability due to critic dependency
  - Low improvement in user engagement: Indicates CARL is not effectively optimizing long-term rewards

- First 3 experiments:
  1. Compare CARL-EL vs. CARL-DL on critic loss and user engagement improvement to validate the effectiveness of eigenfunction learning
  2. Analyze the difference in long-term rewards Q0 and Q1 over time to understand the stability of the real-time vs. cached recommendation gap
  3. Test the impact of different cache sizes (L) on computational cost, cache hit ratio, and user engagement to find the optimal tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CARL-EL compare to other state-of-the-art RL methods when applied to different types of recommender systems beyond short video platforms?
- Basis in paper: [explicit] The paper mentions that CARL has been deployed in Kwai app, a short video platform, and compares its performance to other RL methods (TD3, RLUR) in this specific context.
- Why unresolved: The paper only provides results for the Kwai app, a short video platform. It's unclear how CARL-EL would perform in other types of recommender systems, such as e-commerce or news recommendation.
- What evidence would resolve it: Conducting experiments with CARL-EL on various types of recommender systems and comparing its performance to other state-of-the-art RL methods in each context.

### Open Question 2
- Question: What is the impact of varying the cache size (K) on the performance of CARL-EL, and is there an optimal cache size for different types of recommender systems?
- Basis in paper: [explicit] The paper mentions that K items are recommended to users from the cache, but it does not explore the impact of varying K on the performance of CARL-EL.
- Why unresolved: The paper does not provide any analysis on how changing the cache size affects the performance of CARL-EL, which is an important aspect of the system's design.
- What evidence would resolve it: Conducting experiments with different cache sizes (K) and analyzing the impact on CARL-EL's performance in various recommender system contexts.

### Open Question 3
- Question: How does the performance of CARL-EL scale with the number of users and requests in a recommender system, and are there any limitations to its scalability?
- Basis in paper: [inferred] The paper mentions that CARL has been deployed in Kwai app, serving over 100 million users, but it does not discuss the scalability of CARL-EL in terms of the number of users and requests.
- Why unresolved: The paper does not provide any information on how CARL-EL performs as the number of users and requests increases, which is crucial for understanding its scalability.
- What evidence would resolve it: Conducting experiments with CARL-EL in recommender systems with varying numbers of users and requests, and analyzing its performance and scalability.

## Limitations

- The paper lacks empirical validation of the core assumption that the immediate reward difference V0 - V1 is constant over time and independent of cache ratios.
- Several critical implementation details are omitted, including the exact architecture of the actor and critic networks, the specific mechanism for determining cache state from system load, and the cache eviction policy.
- The reported results are based on a single deployment in the Kwai app, limiting the generalizability of the findings to other types of recommender systems.

## Confidence

- High Confidence: The basic formulation of CARL as an MDP with cache state is well-established. The general approach of using eigenfunction learning to decouple dependent critic functions is theoretically sound. The reported performance improvements (0.586% daily watch time) are specific and measurable.
- Medium Confidence: The effectiveness of the EL algorithm in practice depends on the stability of the immediate reward difference V0 - V1 and the accuracy of the eigen transformations. While the paper demonstrates improved performance compared to baselines, the extent to which this is due to the EL algorithm versus other factors is unclear.
- Low Confidence: The paper does not provide sufficient evidence to support the claim that the difference between real-time and cached recommendations (Λ_a) is constant over time and independent of cache ratios. This is a key assumption for the EL algorithm's success, but it is not empirically validated.

## Next Checks

1. Test the stability of the immediate reward difference: Measure V0 - V1 over time and across different system loads to verify if it remains constant as assumed by the EL algorithm.

2. Analyze the correlation between cache states: Investigate if consecutive cache states (C_t and C_t+1) are correlated, which would violate the independence assumption and potentially explain learning difficulties.

3. Compare different cache state determination mechanisms: Evaluate the performance of CARL when cache state is determined by different factors (e.g., user features, time of day, content type) to understand the robustness of the method to different cache state dynamics.