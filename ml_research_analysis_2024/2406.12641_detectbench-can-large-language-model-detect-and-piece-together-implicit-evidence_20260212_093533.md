---
ver: rpa2
title: 'DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?'
arxiv_id: '2406.12641'
source_url: https://arxiv.org/abs/2406.12641
tags:
- evidence
- reasoning
- question
- context
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DetectBench is a new benchmark designed to test large language\
  \ models\u2019 (LLMs) ability to detect and piece together implicit evidence within\
  \ long contexts. The benchmark contains 3,928 multiple-choice questions with an\
  \ average of 994 tokens per question, requiring 7.62 logical jumps on average to\
  \ find the correct answer."
---

# DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?

## Quick Facts
- arXiv ID: 2406.12641
- Source URL: https://arxiv.org/abs/2406.12641
- Reference count: 39
- Key outcome: DetectBench benchmark shows LLMs struggle with implicit evidence detection, achieving only 44.4% RougeL-F score with GPT4, while Detective Reasoning Prompt and Fine-tuning methods improve performance

## Executive Summary
DetectBench is a new benchmark designed to test large language models' ability to detect and piece together implicit evidence within long contexts. The benchmark contains 3,928 multiple-choice questions requiring an average of 7.62 logical jumps to find correct answers. Experiments show existing LLMs are far inferior to humans in evidence detection, with GPT4 achieving only 44.4% RougeL-F score. To enhance LLMs' performance, the paper proposes Detective Reasoning Prompt and Fine-tuning methods that significantly improve both evidence detection and reasoning capabilities.

## Method Summary
The paper introduces DetectBench, a benchmark containing 3,928 multiple-choice questions with an average of 994 tokens per question. The benchmark requires models to detect implicit evidence and perform multi-hop reasoning with 7.62 logical jumps on average. To improve LLM performance, the authors propose two methods: Detective Reasoning Prompt, which structures the reasoning process into four progressive stages (Evidence Detection, Evidence Association, Answer Inspiration, and Weighted Reasoning), and Detective Reasoning Fine-tuning, which trains weaker LLMs on structured reasoning patterns from DetectBench data.

## Key Results
- GPT4 achieves only 44.4% RougeL-F score on evidence detection, far below human performance
- Detective Reasoning Prompt significantly improves powerful LLMs' evidence detection capabilities
- Fine-tuning on DetectBench data substantially enhances weaker LLMs' performance in both evidence detection and reasoning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Detective Reasoning Prompt improves evidence detection by structuring the reasoning process into progressive stages that explicitly require identifying and connecting evidence before reaching conclusions.
- **Mechanism**: The prompt forces models through four stages: Evidence Detection (find all possible evidence), Evidence Association (connect evidence pieces), Answer Inspiration (identify key evidence for answering), and Weighted Reasoning (use reasoning to finalize answer). This structured approach prevents models from jumping directly to conclusions without proper evidence analysis.
- **Core assumption**: Models can generate more useful evidence connections when prompted to explicitly think about evidence relationships rather than just answer directly.
- **Evidence anchors**:
  - [abstract] "Detective Reasoning Prompt effectively improves powerful LLMs’ evidence detection"
  - [section 4.1] "Our focus is on evaluating the effectiveness of the Detective Reasoning Prompt"
  - [corpus] Weak - no direct corpus evidence showing this mechanism works, only benchmark claims
- **Break condition**: If the model cannot generate meaningful evidence connections in early stages, the subsequent reasoning will be poor regardless of prompt structure.

### Mechanism 2
- **Claim**: Fine-tuning on DetectBench data specifically improves weaker LLMs' performance by teaching them the evidence-reasoning patterns found in detective puzzles.
- **Mechanism**: The fine-tuning data includes structured reasoning processes from DetectBench questions, allowing weaker models to learn how to identify and connect implicit evidence through exposure to annotated examples.
- **Core assumption**: Weaker LLMs can learn evidence detection patterns from supervised examples that they cannot discover through prompting alone.
- **Evidence anchors**:
  - [abstract] "Fine-tuning shows significant effects in enhancing weaker LLMs’ performance"
  - [section 5.3] "Detective Reasoning Finetuning using DetectBench data significantly augments both evidence detection and reasoning capabilities"
  - [corpus] Weak - no independent validation that the fine-tuning data itself causes the improvement
- **Break condition**: If the model's architecture cannot represent the learned patterns effectively, fine-tuning will have limited impact regardless of data quality.

### Mechanism 3
- **Claim**: Evidence detection accuracy and reasoning accuracy are positively correlated, meaning improvements in one directly translate to improvements in the other.
- **Mechanism**: The structured detective reasoning process creates a pipeline where better evidence detection leads to more accurate reasoning, which then produces better final answers.
- **Core assumption**: The quality of reasoning depends fundamentally on the quality of evidence detection, not just on reasoning ability alone.
- **Evidence anchors**:
  - [abstract] "when the abilities of LLMs in evidence detection are improved, their final reasoning performance is also enhanced accordingly"
  - [section 5.2] "We analyzed the correlation between evidence detection and the final reasoning outcomes in Fig. 4, finding a notable positive correlation"
  - [corpus] Weak - correlation mentioned but not proven to be causal
- **Break condition**: If reasoning errors occur independently of evidence detection (e.g., logical fallacies), improving evidence detection alone may not improve overall accuracy.

## Foundational Learning

- **Concept**: Multi-hop reasoning in long contexts
  - Why needed here: DetectBench requires 7.62 logical jumps on average to solve problems, meaning models must connect evidence across multiple steps
  - Quick check question: Can the model explain how evidence A leads to evidence B which then leads to evidence C in a chain?

- **Concept**: Implicit evidence detection in text
  - Why needed here: Evidence in DetectBench cannot be found through simple string matching but requires understanding contextual relationships
  - Quick check question: Given a paragraph with subtle temperature/humidity clues, can the model identify which details are relevant to seeing through fogged windows?

- **Concept**: Structured reasoning prompt engineering
  - Why needed here: The Detective Reasoning Prompt uses specific stages to guide model thinking, which is essential for the benchmark's multi-step problems
  - Quick check question: Does the model follow the prompt's stage structure (Clues → Connection → Thought → Summarize) rather than jumping to conclusions?

## Architecture Onboarding

- **Component map**: Input processor -> Evidence detector -> Connection generator -> Reasoner -> Answer selector -> Output formatter
- **Critical path**: Context + Question -> Evidence Detection -> Evidence Association -> Weighted Reasoning -> Answer Selection
- **Design tradeoffs**: Longer prompts with more guidance improve evidence detection but increase computational cost and may reduce model autonomy
- **Failure signatures**: Low RougeL scores on evidence detection, high accuracy when given evidence directly, inability to connect disparate evidence pieces
- **First 3 experiments**:
  1. Run DetectBench with Naive prompt to establish baseline performance
  2. Run DetectBench with Detective Reasoning Prompt to measure improvement
  3. Fine-tune a weaker LLM on DetectBench data and compare against non-fine-tuned version on the same benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on DetectBench correlate with their performance on other benchmarks for reasoning and retrieval tasks?
- Basis in paper: The paper mentions that Detective Reasoning Prompt and Fine-tuning methods improve LLMs' performance on DetectBench, but it doesn't explore how these improvements translate to other benchmarks.
- Why unresolved: The paper focuses on evaluating LLMs on DetectBench and improving their performance on this specific benchmark. It doesn't provide a comprehensive analysis of how these improvements generalize to other tasks or benchmarks.
- What evidence would resolve it: Experiments comparing the performance of LLMs on DetectBench and other benchmarks, both before and after applying Detective Reasoning Prompt and Fine-tuning methods.

### Open Question 2
- Question: What are the limitations of the Detective Reasoning Prompt and Fine-tuning methods in handling more complex and nuanced reasoning tasks beyond DetectBench?
- Basis in paper: The paper demonstrates the effectiveness of these methods on DetectBench, but it doesn't explore their limitations or potential drawbacks in handling more complex reasoning tasks.
- Why unresolved: The paper primarily focuses on the positive impact of these methods on DetectBench and doesn't delve into potential limitations or areas where they might fall short.
- What evidence would resolve it: Experiments testing the performance of LLMs on more complex reasoning tasks, both with and without the application of Detective Reasoning Prompt and Fine-tuning methods.

### Open Question 3
- Question: How does the performance of LLMs on DetectBench change as the context length and complexity increase beyond the current test sets?
- Basis in paper: The paper introduces Test-Hard and Test-Distract datasets to challenge LLMs with longer contexts and more complex reasoning, but it doesn't explore how performance scales beyond these datasets.
- Why unresolved: The paper provides insights into how LLMs handle increased complexity within the Test-Hard and Test-Distract datasets, but it doesn't investigate the upper limits of their capabilities.
- What evidence would resolve it: Experiments testing the performance of LLMs on DetectBench with even longer contexts and more complex reasoning tasks, pushing the boundaries of their current capabilities.

### Open Question 4
- Question: How does the performance of LLMs on DetectBench vary across different languages and cultural contexts?
- Basis in paper: The paper mentions translating the benchmark into Chinese for human participants, but it doesn't explore how LLMs perform on DetectBench in different languages or cultural contexts.
- Why unresolved: The paper focuses on the English version of DetectBench and doesn't provide insights into how language and cultural differences might impact LLMs' performance.
- What evidence would resolve it: Experiments testing the performance of LLMs on DetectBench in different languages and cultural contexts, comparing their performance to the English version.

## Limitations
- The effectiveness of Detective Reasoning Prompt and Fine-tuning methods relies heavily on specific implementation details not fully disclosed in the paper
- The correlation between evidence detection and reasoning performance is observed but not proven to be causal
- The paper lacks ablation studies to isolate the specific contributions of each prompt stage or fine-tuning component

## Confidence
- Claims about Detective Reasoning Prompt effectiveness: Medium confidence - supported by benchmark results but lacks independent validation
- Claims about fine-tuning benefits for weaker LLMs: Medium confidence - based on limited model comparisons without exploring full spectrum
- Claims about evidence detection and reasoning correlation: Medium confidence - observed correlation but not proven causal

## Next Checks
1. **Cross-dataset generalization test**: Evaluate whether models trained/fine-tuned on DetectBench maintain their evidence detection capabilities when tested on entirely different multi-hop reasoning datasets like HotpotQA or StrategyQA, to verify that learned skills transfer beyond the benchmark.

2. **Ablation study of prompt components**: Systematically remove each stage of the Detective Reasoning Prompt (Evidence Detection, Evidence Association, Answer Inspiration, Weighted Reasoning) and measure the impact on performance to determine which components are essential versus which might be redundant.

3. **Human evaluation of evidence quality**: Conduct expert annotation studies where human judges rate the relevance and accuracy of evidence pieces identified by models using the Detective Reasoning Prompt versus baseline approaches, to validate whether the prompt actually improves evidence quality or just increases quantity.