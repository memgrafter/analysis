---
ver: rpa2
title: The Disparate Benefits of Deep Ensembles
arxiv_id: '2410.13831'
source_url: https://arxiv.org/abs/2410.13831
tags:
- ensemble
- members
- fairness
- deep
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a "disparate benefits effect" where Deep Ensembles
  improve overall performance but also increase fairness violations across different
  protected groups (e.g., gender, age, race). This occurs because ensemble members
  have differing predictive diversity for different groups, causing the ensemble to
  benefit some groups more than others.
---

# The Disparate Benefits of Deep Ensembles
## Quick Facts
- arXiv ID: 2410.13831
- Source URL: https://arxiv.org/abs/2410.13831
- Reference count: 40
- Deep Ensembles improve overall performance but increase fairness violations across protected groups

## Executive Summary
This paper reveals a counterintuitive finding about Deep Ensembles in machine learning: while they improve overall predictive performance, they can simultaneously increase fairness violations across different demographic groups. The authors identify this "disparate benefits effect" through systematic analysis across multiple fairness metrics (SPD, EOD, AOD) and datasets, showing that ensemble members exhibit varying predictive diversity for different groups, leading to unequal performance gains.

The study demonstrates that Deep Ensembles are particularly sensitive to prediction thresholds due to their superior calibration properties, making post-processing methods especially effective for mitigating fairness violations. Using Hardt post-processing, the authors successfully reduce fairness disparities while preserving the accuracy improvements from ensembling, offering a practical solution for deploying Deep Ensembles in real-world applications where both performance and fairness are critical requirements.

## Method Summary
The authors systematically evaluate Deep Ensembles across multiple fairness-critical datasets (COMPAS, Adult, Civil, German) using various protected attributes including gender, age, and race. They compare ensemble performance against single models across standard fairness metrics (statistical parity difference, equal opportunity difference, average odds difference) and analyze the calibration properties of ensemble predictions. The study employs threshold post-processing methods, particularly focusing on Hardt's approach, to investigate whether fairness violations can be mitigated without sacrificing the accuracy gains from ensembling.

## Key Results
- Deep Ensembles increase fairness violations (SPD, EOD, AOD) while improving overall accuracy across multiple datasets
- Ensembles show greater sensitivity to prediction thresholds due to better calibration properties
- Hardt post-processing successfully mitigates fairness violations while preserving accuracy improvements
- The disparate benefits effect persists across different protected attributes (gender, age, race)

## Why This Works (Mechanism)
The disparate benefits effect emerges because Deep Ensembles combine predictions from multiple models, each with different strengths and weaknesses across demographic groups. When ensemble members have varying predictive diversity for different groups, the aggregation process benefits some groups more than others, creating performance disparities. The superior calibration of Deep Ensembles amplifies this effect by making their predictions more sensitive to threshold adjustments, which is why post-processing methods like Hardt's prove particularly effective at redistributing performance more equitably across groups.

## Foundational Learning
- **Ensemble methods**: Why needed - to improve predictive performance through model aggregation; Quick check - understand bagging vs boosting vs deep ensembles
- **Fairness metrics (SPD, EOD, AOD)**: Why needed - to quantify disparate treatment across protected groups; Quick check - can calculate these from confusion matrices
- **Calibration in ML**: Why needed - relates to how well predicted probabilities match true outcomes; Quick check - understand reliability diagrams
- **Threshold post-processing**: Why needed - adjusts decision boundaries without retraining; Quick check - can implement Hardt's method
- **Protected attributes**: Why needed - legal and ethical requirement in ML fairness; Quick check - understand binary vs multi-class handling
- **Disparate impact**: Why needed - core concept in algorithmic fairness; Quick check - can identify when different groups receive unequal outcomes

## Architecture Onboarding
**Component Map**: Input data → Preprocessing → Ensemble training (N models) → Aggregation → Calibration → Threshold post-processing → Fairness evaluation

**Critical Path**: Training multiple diverse models → Aggregating predictions → Evaluating fairness metrics → Applying post-processing → Final performance assessment

**Design Tradeoffs**: Performance improvement vs fairness degradation; complexity of ensemble management vs simplicity of single models; post-processing vs training-time fairness constraints

**Failure Signatures**: Increased fairness violations despite accuracy gains; sensitivity to threshold choices; uneven performance improvements across demographic groups

**First 3 Experiments**: 1) Compare single model vs ensemble performance across fairness metrics; 2) Test threshold sensitivity by varying decision boundaries; 3) Apply Hardt post-processing and measure fairness-accuracy tradeoff

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Findings may not generalize beyond the specific datasets examined (COMPAS, Adult, Civil, German)
- Assumes binary protected attributes and equal cost for different error types
- Focuses on post-processing interventions rather than preventing disparate benefits at training time
- Limited exploration of alternative ensemble methods beyond deep learning ensembles

## Confidence
- High confidence: Core empirical finding of increased fairness violations with improved accuracy is consistently supported across multiple datasets and metrics
- Medium confidence: Calibration-sensitivity explanation is demonstrated but not exhaustively tested across different model architectures
- Medium confidence: Hardt post-processing effectiveness is shown but alternative methods weren't thoroughly explored

## Next Checks
1. Test whether the disparate benefits effect persists when using alternative ensemble methods (bagging, boosting, Bayesian model averaging)
2. Evaluate whether training-time fairness constraints (adversarial debiasing, fairness-aware loss functions) can prevent the emergence of disparate benefits
3. Investigate the effect on intersectional subgroups rather than analyzing each protected attribute independently