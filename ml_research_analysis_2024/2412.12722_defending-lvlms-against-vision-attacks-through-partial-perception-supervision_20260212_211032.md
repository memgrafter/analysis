---
ver: rpa2
title: Defending LVLMs Against Vision Attacks through Partial-Perception Supervision
arxiv_id: '2412.12722'
source_url: https://arxiv.org/abs/2412.12722
tags:
- image
- attacks
- defense
- question
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DPS, a black-box, training-free defense method
  that uses responses from partial image processing to supervise the model's full-image
  responses at inference time. The core idea is to have a "weak" model process cropped
  images, then use its outputs to guide a "strong" model in analyzing the full image,
  enabling correction when under attack while maintaining clean-image accuracy.
---

# Defending LVLMs Against Vision Attacks through Partial-Perception Supervision

## Quick Facts
- arXiv ID: 2412.12722
- Source URL: https://arxiv.org/abs/2412.12722
- Reference count: 40
- Reduces average attack success rate by 76.3% across three LVLM models

## Executive Summary
This paper introduces DPS (Defense through Partial-Perception Supervision), a black-box, training-free method to defend Large Vision-Language Models (LVLMs) against vision attacks. The approach leverages responses from models processing cropped partial images to supervise the analysis of full images during inference. By treating partial image responses as "weak" supervision for full-image processing, DPS enables self-correction when models are under attack while maintaining performance on clean images. Experiments demonstrate significant reductions in attack success rates across six datasets and three popular LVLM models.

## Method Summary
DPS operates by generating cropped partial views of attacked images using center, random, or adaptive cropping strategies. A "Part-Perc" model processes these cropped images to produce descriptive responses about visible content. These responses are then incorporated into prompts for a "Full-Perc" model analyzing the entire image, creating a supervisory loop that helps correct compromised responses. The method is training-free and black-box, requiring no model access or parameter modification. It can be enhanced with safety-aware adaptations for jailbreak defense by simply adjusting prompts to include safety considerations.

## Key Results
- Reduces average attack success rate by 76.3% across three LVLM models (Qwen-VL-Plus, GPT-4o-Mini, Gemini-1.5-Flash)
- Outperforms baseline defense methods on six diverse datasets including RTA-100, MultiTrust, Self-Gen, MM-SafetyBench, HADES, and VisualAttack
- Maintains clean-image accuracy while providing robust defense against misleading and jailbreak attacks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Partial perception supervision reduces model confidence under attack, enabling self-correction
- **Mechanism:** Attacked images reduce LVLM confidence, making models receptive to supervisory input from partial images
- **Core assumption:** Model confidence inversely relates to attack susceptibility
- **Evidence anchors:** Reduced confidence in attacked images leads to output changes; weak evidence from corpus on jailbreak attacks
- **Break condition:** Cropping fails to eliminate attack vectors

### Mechanism 2
- **Claim:** Weak models can effectively supervise strong models in the vision domain
- **Mechanism:** Partial image responses (weak models) guide full-image analysis (strong models)
- **Core assumption:** Weak-to-strong learning phenomenon from language models applies to vision-language models
- **Evidence anchors:** Analogy between partial/full image responses and weak/strong models; weak evidence from corpus
- **Break condition:** Partial responses are too inaccurate to provide useful guidance

### Mechanism 3
- **Claim:** Safety-aware adaptation extends defense to critical safety scenarios
- **Mechanism:** Modified prompts include safety considerations for jailbreak attack defense
- **Core assumption:** LVLMs can be prompted to consider safety implications even under attack
- **Evidence anchors:** Simple prompt adjustments enable safety defense; weak evidence from corpus on jailbreak attacks
- **Break condition:** Model's safety alignment is too weak for prompt-based protection

## Foundational Learning

- **Concept:** Image cropping sensitivity in vision attacks
  - **Why needed here:** Fundamental to why DPS works by eliminating attack effects
  - **Quick check question:** Why does cropping an attacked image often eliminate the attack effect?

- **Concept:** Model confidence and attack susceptibility
  - **Why needed here:** Mechanism relies on confidence reduction under attack enabling supervision
  - **Quick check question:** How does model confidence differ when processing clean versus attacked images?

- **Concept:** Weak-to-strong supervision in multi-modal models
  - **Why needed here:** Core mechanism treats partial responses as weak supervision
  - **Quick check question:** What is the theoretical basis for using weaker models to supervise stronger models?

## Architecture Onboarding

- **Component map:** Cropping Strategy -> Part-Perc Model -> Full-Perc Model -> (Optional) Safety Checker
- **Critical path:**
  1. Crop input image using selected strategy
  2. Generate initial response from Part-Perc model on cropped image
  3. Create supervisory prompt using Part-Perc response
  4. Generate final response from Full-Perc model on full image
  5. (Optional) Apply safety checker to final response
- **Design tradeoffs:** More cropping strategies increase robustness but add computational overhead; stronger safety checking improves safety but may impact standard performance; black-box implementation ensures compatibility but limits optimization
- **Failure signatures:** Defense fails when cropping doesn't eliminate attack vectors; performance degradation on clean images indicates over-sensitivity; safety checker blocking legitimate responses suggests overly conservative thresholds
- **First 3 experiments:**
  1. Test baseline performance without defense on both clean and attacked images
  2. Evaluate single cropping strategy (e.g., center cropping) against a simple attack dataset
  3. Compare different cropping strategies (center, random, adaptive) on the same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does DPS performance degrade as attack sophistication increases, particularly for attacks that preserve semantic coherence under cropping?
- **Basis in paper:** [explicit] Paper notes potential challenge when cropping fails to eliminate attack
- **Why unresolved:** Paper mentions this limitation but lacks empirical analysis of sophisticated attacks
- **What evidence would resolve it:** Systematic evaluation against attacks designed to maintain semantic coherence under various cropping strategies

### Open Question 2
- **Question:** What is the optimal number of partial crops and cropping strategy combination for maximizing defense effectiveness across different attack types?
- **Basis in paper:** [explicit] Paper explores different cropping strategies and numbers of crops in ablation studies
- **Why unresolved:** While showing positive correlation between crop number and effectiveness, optimal combination remains undetermined
- **What evidence would resolve it:** Comprehensive parameter sweep experiments across diverse attack datasets to identify optimal configurations

### Open Question 3
- **Question:** How does DPS compare to detection-based defense methods when combined, and can such combinations provide defense against attacks that survive cropping?
- **Basis in paper:** [explicit] Paper suggests combining with detection methods as future work when cropping fails
- **Why unresolved:** Paper does not evaluate DPS in combination with detection-based methods
- **What evidence would resolve it:** Empirical comparison of DPS alone versus DPS combined with detection-based methods against attacks designed to survive cropping

## Limitations
- Effectiveness depends on cropping successfully eliminating attack vectors, which may not work for all attack types
- Black-box implementation limits optimization opportunities and may prevent adaptation to novel attack strategies
- Computational overhead from multiple model calls during inference could impact real-world deployment feasibility

## Confidence
- **High Confidence:** Core claim of 76.3% ASR reduction well-supported by experimental results across multiple datasets
- **Medium Confidence:** Weak-to-strong supervision mechanism as analogy rather than directly demonstrated
- **Medium Confidence:** Safety-aware adaptation compatibility, as safety checker component described but not thoroughly validated

## Next Checks
1. **Cross-dataset generalization test:** Evaluate DPS performance on additional attack datasets, particularly attacks targeting cropped regions to assess partial-perception supervision robustness
2. **Computational overhead analysis:** Measure and compare inference time and computational cost of DPS versus baseline defenses, including impact of different cropping strategies on processing latency
3. **Adaptive attack resistance evaluation:** Design and test adaptive attacks specifically crafted to circumvent partial-perception supervision, such as attacks distributed across images to survive cropping or attacks manipulating supervisory signals