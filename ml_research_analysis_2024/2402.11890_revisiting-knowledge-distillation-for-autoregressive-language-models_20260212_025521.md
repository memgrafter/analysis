---
ver: rpa2
title: Revisiting Knowledge Distillation for Autoregressive Language Models
arxiv_id: '2402.11890'
source_url: https://arxiv.org/abs/2402.11890
tags:
- atkd
- tokens
- performance
- more
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of performance degradation when
  distilling small students from large teacher autoregressive language models. The
  authors analyze the distillation objective and find that different tokens have different
  teaching modes, with hard-to-learn tokens containing more informative knowledge
  but being suppressed in large teachers.
---

# Revisiting Knowledge Distillation for Autoregressive Language Models
## Quick Facts
- arXiv ID: 2402.11890
- Source URL: https://arxiv.org/abs/2402.11890
- Reference count: 21
- Key outcome: Adaptive teaching approach (ATKD) improves knowledge distillation for autoregressive language models by up to 3.04% average score

## Executive Summary
This paper addresses the problem of performance degradation when distilling small students from large teacher autoregressive language models. The authors analyze the distillation objective and find that different tokens have different teaching modes, with hard-to-learn tokens containing more informative knowledge but being suppressed in large teachers. They propose an adaptive teaching approach (ATKD) that skips target-oriented teaching for easy-to-learn tokens and emphasizes diverse learning for hard-to-learn tokens. Experiments on 8 tasks with OPT, Pythia, and LLaMA models show that ATKD consistently improves baseline KD methods by up to 3.04% average score and effectively alleviates performance degradation in larger teachers.

## Method Summary
The authors propose Adaptive Teaching for Knowledge Distillation (ATKD), which addresses the problem of performance degradation when distilling small students from large teacher autoregressive language models. The key insight is that different tokens have different teaching modes - hard-to-learn tokens contain more informative knowledge but are suppressed in large teachers. ATKD implements an adaptive teaching approach that skips target-oriented teaching for easy-to-learn tokens while emphasizing diverse learning for hard-to-learn tokens. The method analyzes token difficulty based on correlation with final performance and adjusts the distillation objective accordingly to better capture informative knowledge from hard tokens.

## Key Results
- ATKD consistently improves baseline KD methods by up to 3.04% average score across 8 tasks
- Effectively alleviates performance degradation when distilling from larger teacher models
- Improves student model generalization capabilities
- Demonstrated effectiveness across OPT, Pythia, and LLaMA model families

## Why This Works (Mechanism)
The mechanism works by recognizing that knowledge distillation from large autoregressive language models suffers because hard-to-learn tokens, which contain more informative knowledge, are suppressed during teacher training. Traditional distillation methods treat all tokens equally, missing this important distinction. ATKD's adaptive teaching approach addresses this by identifying token difficulty through performance correlation analysis and then adjusting the teaching strategy - skipping unnecessary teaching for easy tokens while emphasizing diverse learning approaches for hard tokens. This targeted approach allows the student to better capture the informative knowledge that would otherwise be lost.

## Foundational Learning
- Knowledge Distillation: Transfer of knowledge from a large teacher model to a smaller student model to improve efficiency while maintaining performance
- Autoregressive Language Models: Models that generate text sequentially, predicting the next token based on previous tokens
- Token Difficulty Analysis: Method for identifying which tokens are harder to learn based on their correlation with final model performance
- Adaptive Teaching: Dynamic adjustment of teaching strategies based on the difficulty level of different tokens

## Architecture Onboarding
Component map: Token Analysis -> Difficulty Classification -> Adaptive Teaching -> Distillation Objective -> Student Model
Critical path: The token difficulty analysis determines the adaptive teaching strategy, which then modifies the distillation objective to produce a better-performing student model
Design tradeoffs: Balancing computational overhead of token analysis against performance gains, and determining optimal thresholds for distinguishing easy vs hard tokens
Failure signatures: Poor performance on hard-to-learn tokens, inability to generalize beyond training data, excessive computational overhead
First experiments:
1. Compare baseline KD performance against ATKD across multiple model scales
2. Analyze token difficulty distribution and its correlation with model performance
3. Evaluate generalization capabilities of ATKD-trained students on out-of-distribution data

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis of teaching modes relies on correlation with final performance rather than direct measurement of information content
- Computational overhead of adaptive teaching mechanism not thoroughly analyzed
- Effectiveness limited to autoregressive language models, untested on other architectures
- Limited analysis of training efficiency and computational requirements

## Confidence
- ATKD improves distillation performance: High
- Hard tokens contain more informative knowledge: Medium
- Adaptive teaching generalizes across model scales: Medium
- Performance degradation can be effectively alleviated: High

## Next Checks
1. Conduct ablation studies to isolate the contribution of each component in the ATKD method
2. Test the adaptive teaching approach on non-autoregressive language models and other NLP tasks
3. Analyze the computational overhead and training efficiency compared to baseline KD methods