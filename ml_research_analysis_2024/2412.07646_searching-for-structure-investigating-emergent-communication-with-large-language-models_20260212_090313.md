---
ver: rpa2
title: 'Searching for Structure: Investigating Emergent Communication with Large Language
  Models'
arxiv_id: '2412.07646'
source_url: https://arxiv.org/abs/2412.07646
tags:
- language
- shape
- word
- languages
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether Large Language Models (LLMs) can
  be used to simulate the emergence of structured communication systems, extending
  classic language evolution experiments. The authors simulate a referential game
  where LLM agents learn and use artificial languages, initially unstructured, to
  communicate about visual stimuli.
---

# Searching for Structure: Investigating Emergent Communication with Large Language Models

## Quick Facts
- arXiv ID: 2412.07646
- Source URL: https://arxiv.org/abs/2412.07646
- Reference count: 18
- Primary result: LLMs can simulate emergent communication, producing structured languages through repeated interaction, but these sometimes exhibit non-humanlike features.

## Executive Summary
This study investigates whether Large Language Models (LLMs) can simulate the emergence of structured communication systems, extending classic language evolution experiments. The authors simulate a referential game where LLM agents learn and use artificial languages, initially unstructured, to communicate about visual stimuli. Through repeated learning and interaction, the languages evolve to exhibit greater structural properties, enabling more successful communication. Generational transmission further increases learnability, though it can also lead to non-humanlike degenerate vocabularies.

## Method Summary
The study simulates a referential communication game where LLM agents learn and use artificial languages to communicate about visual stimuli. Agents alternate between speaker and listener roles, iteratively updating their vocabularies through repeated interactions. The experiment measures structural properties (TopSim, Ngram diversity) and generalization ability, with additional generational transmission to study bias amplification. Llama 3 70B with greedy decoding is used as the primary model.

## Key Results
- LLMs can successfully learn and communicate using artificial languages
- Structured languages emerge naturally when optimized for LLM biases
- Generational transmission increases learnability but can produce non-humanlike degenerate vocabularies
- Languages exhibit higher degrees of structure after multiple communication rounds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs naturally optimize unstructured languages toward greater structural properties when repeated learning and communication occur.
- Mechanism: The bias of LLMs toward structured representations causes them to favor regularities in the artificial language. As agents repeatedly learn and communicate, they gradually shape the vocabulary to align with these structural preferences, increasing TopSim and reducing Ngram diversity.
- Core assumption: LLMs possess an implicit preference for structured input that emerges during repeated exposure and usage.
- Evidence anchors:
  - [abstract]: "initially unstructured holistic languages are indeed shaped to have some structural properties that allow two LLM agents to communicate successfully."
  - [section 5.3]: "LLMs not only learn structured vocabularies better but also naturally shape languages to have some form of structure when they are optimised for their inherent biases."
  - [corpus]: Weak; corpus entries discuss emergent communication and language similarity but do not directly support the specific structural bias claim for LLMs in this setup.
- Break condition: If LLM biases are not sufficiently strong or if communication pressure is absent, languages may remain unstructured or degrade into degenerate vocabularies.

### Mechanism 2
- Claim: Iterated learning amplifies LLM biases and increases learnability of the language.
- Mechanism: When a generation of agents learns from the vocabulary produced by the previous generation, the language is progressively optimized for LLM preferences, making it easier to learn (lower edit distance) but also potentially longer and more ambiguous.
- Core assumption: Generational transmission acts as a filter that reinforces weak biases over time.
- Evidence anchors:
  - [abstract]: "generational transmission increases the learnability of languages, but can at the same time result in non-humanlike degenerate vocabularies."
  - [section 6.1]: "the learnability increases... a single generation learning and using a language tremendously decreases the edit distance."
  - [section 6.2]: "languages become significantly longer and more ambiguous" after iterated learning.
  - [corpus]: Weak; corpus papers discuss emergent communication and language evolution but do not specifically confirm the amplification of biases through iterated learning in LLMs.
- Break condition: If the language becomes too degenerate or loses expressivity, communicative success may collapse despite increased learnability.

### Mechanism 3
- Claim: Structured languages emerging from LLM communication enable better generalization to unseen stimuli.
- Mechanism: When agents develop compositional or systematic mappings between attributes and signal parts, they can apply these patterns to novel combinations, improving generalization scores.
- Core assumption: Higher TopSim correlates with systematic reuse of signal components.
- Evidence anchors:
  - [abstract]: "languages exhibit higher degrees of structure after multiple communication rounds."
  - [section 5.4]: "high TopSim languages allow for better generalisation (r = 0.735, p < .001)."
  - [section 5.4]: Table 1 shows repeated reuse of signal parts (e.g., "su" for amount one, "pepi" for two).
  - [corpus]: Weak; corpus entries mention emergent communication and compositionality but do not provide direct evidence linking TopSim to generalization in this specific setup.
- Break condition: If the structure is not systematic or if components are reused inconsistently, generalization may fail.

## Foundational Learning

- Concept: Referential game setup and communication protocols
  - Why needed here: Understanding how agents alternate speaker/listener roles and update vocabularies is essential to grasp the simulation dynamics.
  - Quick check question: In each interaction, which agent produces the signal and which agent guesses the target?

- Concept: Metrics for measuring structure (TopSim, Ngram diversity, generalization score)
  - Why needed here: These metrics quantify how much structure emerges and whether it supports generalization; interpreting results depends on them.
  - Quick check question: What does a high TopSim score indicate about the relationship between signals and meanings?

- Concept: Iterated learning and generational transmission
  - Why needed here: This is the mechanism by which weak biases are amplified across generations, affecting learnability and structure.
  - Quick check question: In this study, how is the vocabulary of one generation selected for transmission to the next?

## Architecture Onboarding

- Component map:
  Stimuli generator -> Initial language generator -> LLM agent -> Communication loop -> Metric calculator -> Iterated learning pipeline

- Critical path:
  1. Generate stimuli and initial language.
  2. Agents learn initial vocabulary via completion prompts.
  3. Run communication rounds, updating vocabularies after each interaction.
  4. Measure structural metrics after communication.
  5. For iterated learning: select top TopSim agent, transmit vocabulary, repeat.

- Design tradeoffs:
  - Using greedy decoding vs. sampling: greedy yields more consistent but potentially degenerate outputs.
  - Prefilling vs. multiple-choice guessing: prefilling avoids unreliable multiple-choice outputs but requires re-running prompts.
  - Fixed context window vs. dynamic: fixed window simplifies prompt design but may limit long-term dependencies.

- Failure signatures:
  - Low communicative success despite high learnability: indicates degenerate vocabularies.
  - High TopSim but low generalization: suggests structure is not systematic.
  - Long signal lengths: may indicate absence of memory or compression pressures.

- First 3 experiments:
  1. Run a single simulation without communication to confirm initial learnability is low.
  2. Run a single simulation with communication to verify structure emerges and communicative success increases.
  3. Run a short iterated learning chain (2-3 generations) to observe amplification of biases and learnability gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different decoding strategies (e.g., greedy vs. sampling) affect the emergence of structured vs. degenerate vocabularies in LLM-mediated language evolution?
- Basis in paper: [inferred] The paper mentions that "The rapid increase in learnability resulting from iterated learning proves that weak learning biases in language models, such as, for example, an observed simplicity bias (Chen et al., 2024), can be amplified by the process of generational transmission" and discusses the use of greedy decoding during signal generation.
- Why unresolved: The paper primarily uses greedy decoding, which may not produce the most human-like text and could contribute to the evolution of degenerate vocabularies. The effect of different decoding strategies on the emergence of structured vs. degenerate vocabularies remains unexplored.
- What evidence would resolve it: Experiments comparing the effects of different decoding strategies (e.g., greedy, sampling with different temperature settings) on the emergence of structured vs. degenerate vocabularies in LLM-mediated language evolution.

### Open Question 2
- Question: How do memory constraints in LLMs influence the evolution of language structure and learnability?
- Basis in paper: [explicit] The paper states that "While human language is optimised to be compressible and expressive (Fedzechkina et al., 2012; Tamariz and Kirby, 2015; Kirby et al., 2015), context windows of LLMs, are considerably larger. In our case, Llama 3 70B has a context window of 8.2K tokens, which we do not exceed."
- Why unresolved: The paper suggests that the absence of memory constraints in LLMs may contribute to the evolution of non-humanlike structures, such as longer signals. However, the specific influence of memory constraints on language evolution in LLMs remains unclear.
- What evidence would resolve it: Experiments manipulating the context window size of LLMs and observing the effects on language structure and learnability in the simulated referential game.

### Open Question 3
- Question: How do variations in model features (e.g., size, training data, or decoding strategies) affect the emergence of human-like vs. non-humanlike language structures in LLM-mediated language evolution?
- Basis in paper: [explicit] The paper suggests that "Careful manipulation of our setup can help reveal underlying mechanistic biases of language models and inform modelling choices when simulating language acquisition in LLMs."
- Why unresolved: The paper primarily uses Llama 3 70B and greedy decoding, limiting the exploration of how different model features affect language evolution. The specific influence of these features on the emergence of human-like vs. non-humanlike language structures remains unexplored.
- What evidence would resolve it: Experiments systematically varying model features (e.g., size, training data, decoding strategies) and observing the effects on the emergence of human-like vs. non-humanlike language structures in LLM-mediated language evolution.

## Limitations

- Study relies on a single LLM model (GPT-4) and simplified referential game with only 27 stimuli, limiting generalizability
- Mechanisms for structure emergence are inferred from aggregate metrics rather than detailed behavioral analysis
- Use of greedy decoding may have biased results toward degenerate vocabularies
- Study does not directly compare LLM-generated languages to human languages in terms of structure or learnability

## Confidence

- **High confidence**: LLMs can learn and communicate using artificial languages, and structural properties emerge after repeated communication
- **Medium confidence**: Iterated learning increases learnability and amplifies LLM biases
- **Low confidence**: The specific LLM biases toward structure and the exact nature of non-humanlike features are well-established

## Next Checks

1. Replicate the experiments with multiple LLMs (e.g., GPT-3.5, Claude, LLaMA) to test the robustness of structure emergence and bias amplification across models
2. Conduct a detailed case study of individual agent vocabularies and their evolution to better understand the mechanisms behind structure emergence and degeneracy
3. Compare the structural and learnability properties of LLM-generated languages to human languages in similar communication tasks to assess human-likeness claims