---
ver: rpa2
title: Influence based explainability of brain tumors segmentation in multimodal Magnetic
  Resonance Imaging
arxiv_id: '2405.12222'
source_url: https://arxiv.org/abs/2405.12222
tags:
- class
- segmentation
- network
- tracin
- tumor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends the TracIn algorithm to explain brain tumor segmentation
  in multimodal MRI, where most prior methods use saliency maps. TracIn identifies
  which training examples influence a prediction by tracking parameter updates during
  training.
---

# Influence based explainability of brain tumors segmentation in multimodal Magnetic Resonance Imaging

## Quick Facts
- arXiv ID: 2405.12222
- Source URL: https://arxiv.org/abs/2405.12222
- Reference count: 40
- Most prior methods use saliency maps for brain tumor segmentation explanation

## Executive Summary
This work extends the TracIn algorithm to explain brain tumor segmentation in multimodal MRI, where most prior methods use saliency maps. TracIn identifies which training examples influence a prediction by tracking parameter updates during training. For segmentation, the authors modify TracIn to compute separate scores per class and filter out low-confidence pixels. They validate faithfulness by correlating TracIn scores with feature importance from the network's latent representation, showing that selected features significantly impact performance. The method is generalizable to other semantic segmentation tasks and provides both local (proponent/opponent identification) and global (feature importance) explanations, addressing limitations of saliency-based methods in this domain.

## Method Summary
The method extends TracIn for brain tumor multiclass segmentation in multimodal MRI using the BraTS19 dataset. The approach involves training a 2D UNet with Dice-based loss on 10 central sagittal slices per patient, then computing modified TracIn scores per tumor class by isolating predicted regions and filtering low-confidence pixels. Feature importance is validated by extracting similarity scores from the network's last hidden layer and correlating these with TracIn scores. The framework provides local explanations by identifying proponents and opponents for each pixel, and global explanations through feature importance analysis.

## Key Results
- Extended TracIn algorithm successfully explains brain tumor segmentation by identifying influential training examples per class
- Feature importance validation shows selected features significantly impact network performance
- Method generalizes to other semantic segmentation tasks where classes are mutually exclusive
- Provides both local explanations (proponent/opponent identification) and global explanations (feature importance)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TracIn scores provide local explanations by identifying which training examples most influence each pixel's classification in tumor segmentation.
- Mechanism: TracIn tracks parameter updates during training by accumulating gradients weighted by learning rates across checkpoints, creating influence scores for each training example on each test pixel.
- Core assumption: Parameter updates during training reflect the importance of training examples for specific predictions.
- Evidence anchors:
  - [abstract] "TracIn identifies which training examples influence a prediction by tracking parameter updates during training."
  - [section] "TracIn(z, z′) = Σt∈C ηt∇L(θt, z′) · ∇L(θt, z)" shows the mathematical formulation tracking parameter updates.
  - [corpus] Weak evidence - related papers focus on different segmentation approaches without mentioning TracIn or influence-based methods.
- Break condition: If training converges too quickly (early checkpoints don't capture informative gradients) or if learning rates vary dramatically across checkpoints.

### Mechanism 2
- Claim: The extended TracIn method separates influence calculations by tumor class to avoid mixing heterogeneous pixel contributions.
- Mechanism: Instead of averaging over all pixels, the algorithm computes separate TracIn scores for each class by isolating regions predicted to belong to specific tumor classes (z_j and z'_i) before calculating influence.
- Core assumption: Pixels from different tumor classes contribute differently to influence scores and should be analyzed separately.
- Evidence anchors:
  - [section] "We propose an alternative definition. First, we split z and z′ in separate regions, according to the network prediction: zj and z′i are the regions predicted to be of class j and i in z and z′ respectively."
  - [section] "Equation (5) thus mixes influence contributions of quite heterogeneous pixels, introducing noise in the algorithm."
  - [corpus] No direct evidence - corpus papers don't discuss influence-based segmentation methods.
- Break condition: If class boundaries are ambiguous or if the segmentation network makes frequent classification errors.

### Mechanism 3
- Claim: Feature importance scores derived from similarity metrics between latent representations validate TracIn's faithfulness to the network's decision model.
- Mechanism: The algorithm extracts feature maps from the network's last hidden layer, computes similarity scores between training and test examples, and ranks these by TracIn scores to identify which features correlate with influence.
- Core assumption: Features that are important for the network's decision will show higher similarity scores for proponents than opponents.
- Evidence anchors:
  - [section] "We verify the faithfulness of TracIn with respect to feature maps extracted from the network."
  - [section] "F I(zj)k,i represents the correlation between the latent representation of zj and z′i averaged over the test set."
  - [section] "We expect that the feature importance of important features is higher for proponents than for neutral examples or opponents."
  - [corpus] Weak evidence - corpus papers don't discuss feature importance validation for segmentation models.
- Break condition: If the network's latent representation doesn't capture class-specific features or if similarity metrics don't correlate with influence scores.

## Foundational Learning

- Concept: Influence-based explainability vs saliency maps
  - Why needed here: Understanding why TracIn is chosen over traditional saliency methods for medical imaging applications
  - Quick check question: What are the main limitations of saliency maps that make influence-based methods more suitable for clinical applications?

- Concept: Dice coefficient as segmentation loss function
  - Why needed here: The paper uses Dice coefficient for both loss and evaluation, which is non-standard compared to cross-entropy
  - Quick check question: Why is Dice coefficient particularly suitable for medical image segmentation where the region of interest is often much smaller than the background?

- Concept: Multiclass segmentation extension of TracIn
  - Why needed here: The original TracIn algorithm was designed for classification, requiring modification for segmentation tasks
  - Quick check question: How does the paper modify TracIn to handle the fact that segmentation involves simultaneous classification of multiple pixels?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline -> 2D UNet segmentation network -> TracIn influence calculation module -> Feature extraction and similarity calculation module -> Performance evaluation and validation framework

- Critical path:
  1. Load and preprocess BraTS dataset
  2. Train UNet with Dice-based loss
  3. Extract feature maps from trained network
  4. Compute modified TracIn scores for test images
  5. Calculate feature importance and validate faithfulness
  6. Generate explanations for clinical review

- Design tradeoffs:
  - Using 2D slices instead of 3D volumes for computational efficiency vs. potential loss of spatial context
  - Excluding background class from influence calculations to focus on tumor regions vs. missing contextual information
  - Selecting checkpoints based on SGD transition vs. using all training epochs for more comprehensive influence tracking

- Failure signatures:
  - High variance in TracIn scores across similar training examples indicates noisy gradient estimates
  - Feature importance plots showing random distributions suggest poor correlation between influence and latent representation
  - Significant performance degradation when masking important features validates the feature selection process

- First 3 experiments:
  1. Train UNet on a small subset of BraTS data and verify Dice scores match expected ranges
  2. Compute standard TracIn (classification version) on the trained model to understand baseline behavior
  3. Implement and test the modified multiclass TracIn on sample slices to verify class separation in influence scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do proponents, opponents, and neutral examples differ across the three tumor classes (NCR/NET, ED, ET) in the BraTS19 dataset?
- Basis in paper: [explicit] The authors analyzed the distribution of the top 10 proponents for each class and found that class 1 (NCR/NET) and class 2 (ED) had the strongest proponents within the same class, while class 3 (ET) had more proponents from class 1.
- Why unresolved: The paper only provides a preliminary analysis of the top 10 proponents for each class. A more comprehensive study is needed to understand the patterns and differences in influence across all examples and classes.
- What evidence would resolve it: A detailed analysis of the influence scores for all examples across all classes, potentially using visualizations or statistical measures to identify trends and patterns.

### Open Question 2
- Question: How does the choice of checkpoints affect the TracIn scores and the resulting explanations?
- Basis in paper: [explicit] The authors mention that the choice of checkpoints is relevant, as later checkpoints may have smaller gradients and be less informative. They use the first 5 epochs after the transition to SGD as checkpoints.
- Why unresolved: The paper does not explore the impact of different checkpoint choices on the TracIn scores and explanations. Different checkpoint strategies could lead to different results and interpretations.
- What evidence would resolve it: Experiments comparing the TracIn scores and explanations using different checkpoint strategies, such as varying the number of checkpoints or the epochs selected.

### Open Question 3
- Question: Can the TracIn method be extended to handle multi-class segmentation tasks where the classes are not mutually exclusive?
- Basis in paper: [explicit] The authors state that the method is generalizable for all semantic segmentation tasks where classes are mutually exclusive, which is the standard framework for this type of task. They do not discuss the case of non-mutually exclusive classes.
- Why unresolved: The paper does not address the potential extension of the method to handle non-mutually exclusive classes, which could be a relevant scenario in some segmentation tasks.
- What evidence would resolve it: Theoretical analysis and experimental results demonstrating the feasibility and effectiveness of extending the TracIn method to handle non-mutually exclusive classes.

## Limitations
- Method's effectiveness depends on accurate segmentation predictions, as incorrect class assignments propagate to influence calculations
- Feature importance validation assumes similarity in latent space correlates with influence, which may not hold for all network architectures
- Exclusion of background class from influence calculations may miss important contextual information

## Confidence

- High confidence: The core mathematical formulation of TracIn and its extension to segmentation tasks
- Medium confidence: The validation approach using feature importance from latent representations
- Low confidence: The clinical utility of the explanations and their interpretability by medical professionals

## Next Checks

1. Test the feature importance validation on a held-out test set to ensure the correlation between influence scores and latent representations generalizes beyond the validation set
2. Evaluate the method's sensitivity to different checkpoint selection strategies and learning rate schedules to determine robustness to training variations
3. Compare the influence-based explanations against saliency maps on the same dataset to quantify improvements in explanation quality and clinical relevance