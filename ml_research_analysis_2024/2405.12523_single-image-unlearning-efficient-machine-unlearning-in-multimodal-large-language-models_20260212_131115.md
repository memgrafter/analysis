---
ver: rpa2
title: 'Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large
  Language Models'
arxiv_id: '2405.12523'
source_url: https://arxiv.org/abs/2405.12523
tags:
- image
- donald
- trump
- unlearning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Single Image Unlearning (SIU), a method to
  efficiently unlearn the visual recognition of specific concepts in multimodal large
  language models (MLLMs) using only one training image. The approach constructs multifaceted
  fine-tuning data targeting four objectives and employs a novel Dual Masked KL-divergence
  Loss jointly trained with Cross Entropy Loss.
---

# Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2405.12523
- Source URL: https://arxiv.org/abs/2405.12523
- Reference count: 40
- Primary result: Achieves 99%+ unlearning accuracy while maintaining model utility with only one training image per concept

## Executive Summary
This paper introduces Single Image Unlearning (SIU), a method to efficiently unlearn visual recognition of specific concepts in multimodal large language models using only one training image. The approach constructs multifaceted fine-tuning data targeting four objectives and employs a novel Dual Masked KL-divergence Loss jointly trained with Cross Entropy Loss. SIU achieves near-complete unlearning while maintaining model utility on the MMUBench benchmark, outperforming existing methods and demonstrating robustness against membership inference and jailbreak attacks.

## Method Summary
SIU enables MLLMs to forget visual recognition of targeted concepts by fine-tuning on a single associated image for few steps. The method constructs multifaceted fine-tuning data targeting four objectives: aligning with unseen concepts, assigning new visual descriptions, decoupling factual knowledge, and preserving non-targeted knowledge. It employs a novel Dual Masked KL-divergence (DMK) Loss that applies token-level and vocabulary-level masking to selectively exclude certain tokens from KL-divergence calculations while jointly optimizing with Cross Entropy Loss.

## Key Results
- Achieves 99%+ Exact Match for unlearning effectiveness
- Maintains model utility with Specificity ~60, Fluency ~61, Diversity ~97
- Outperforms existing methods on MMUBench benchmark
- Demonstrates robustness against membership inference and jailbreak attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual Masked KL-divergence (DMK) Loss prevents unlearning from causing model utility collapse by selectively masking out tokens where unlearning would interfere with non-targeted knowledge.
- Mechanism: The DMK Loss applies token-level masking to exclude tokens contradicting original knowledge from KL loss calculations, and vocabulary-level masking to exclude tokens of the targeted unlearning concept across the entire vocabulary during KL computation.
- Core assumption: Masking out specific tokens during KL-divergence calculation prevents those tokens from being over-suppressed while still allowing targeted unlearning of the concept.
- Evidence anchors:
  - [abstract]: "Dual Masked KL-divergence Loss jointly trained with Cross Entropy Loss"
  - [section]: "At the token-level, it masks tokens contradicting original knowledge in the sentence to exclude them from KL loss calculations. At the vocabulary-level, it specifically masks tokens of the targeted unlearning concepts across the entire vocabulary during KL loss computation."
- Break condition: If masking is too aggressive and excludes too many tokens, the model may fail to learn the unlearning pattern properly, resulting in incomplete unlearning.

### Mechanism 2
- Claim: Multifaceted fine-tuning data construction ensures comprehensive unlearning by addressing four distinct targets that cover different aspects of concept recognition.
- Mechanism: The method constructs fine-tuning data targeting four objectives: aligning with unseen concepts, assigning new visual descriptions, decoupling factual knowledge, and preserving non-targeted knowledge.
- Core assumption: By addressing multiple dimensions of concept recognition, the model can unlearn visual recognition while maintaining other capabilities.
- Evidence anchors:
  - [abstract]: "Constructing Multifaceted fine-tuning data. We introduce four targets, based on which we construct fine-tuning data for the concepts to be forgotten"
  - [section]: "Based on Df train, we construct fine-tuning data centering on four targets"
- Break condition: If the four targets are not properly balanced, the model may over-forget some aspects while under-forgetting others, leading to incomplete or ineffective unlearning.

### Mechanism 3
- Claim: The single-image unlearning approach works because it leverages the model's existing generalization capabilities rather than requiring extensive retraining data.
- Mechanism: By using only one training image per concept, the method exploits the pre-trained MLLM's ability to generalize visual recognition patterns across different images of the same concept.
- Core assumption: Pre-trained MLLMs have sufficient generalization capability that a single representative image can trigger unlearning across all instances of a concept.
- Evidence anchors:
  - [abstract]: "Single Image Unlearning (SIU), to unlearn the visual recognition of a concept by fine-tuning a single associated image for few steps"
  - [section]: "SIU requires only a single training image of the targeted concepts to enable MLLMs to forget the visual recognition of these concepts"
- Break condition: If the single image is not representative of the concept's visual variations, the model may not generalize the unlearning to other images of the same concept.

## Foundational Learning

- Concept: KL-divergence loss
  - Why needed here: KL-divergence measures the difference between probability distributions, which is essential for maintaining model utility while unlearning specific concepts
  - Quick check question: What does KL-divergence measure between two probability distributions?

- Concept: Token masking in language models
  - Why needed here: Token masking allows selective exclusion of certain tokens from loss calculations, enabling targeted unlearning without affecting other capabilities
  - Quick check question: How does token masking affect the computation of loss in language models?

- Concept: Cross-entropy loss
  - Why needed here: Cross-entropy loss provides the primary training signal for the unlearning process, working in conjunction with KL-divergence to achieve the desired outcome
  - Quick check question: What is the relationship between cross-entropy loss and probability distributions in classification tasks?

## Architecture Onboarding

- Component map:
  Image-text pairs -> Visual encoder -> Language model -> Connector -> DMK Loss module -> Multifaceted data generator -> Training loop

- Critical path:
  1. Load single training image and associated text
  2. Generate multifaceted fine-tuning data based on four targets
  3. Apply token-level and vocabulary-level masking to KL-divergence loss
  4. Jointly optimize Cross Entropy Loss and DMK Loss
  5. Evaluate unlearning effectiveness and model utility

- Design tradeoffs:
  - Single image vs. multiple images: Single image reduces data requirements but may limit unlearning completeness
  - Masking aggressiveness: More masking preserves utility better but may reduce unlearning effectiveness
  - Four targets vs. fewer targets: More targets provide comprehensive unlearning but increase complexity

- Failure signatures:
  - Utility collapse: Model performance degrades on non-targeted tasks
  - Incomplete unlearning: Model still recognizes the concept in some contexts
  - Overfitting: Model becomes too specialized to training data and loses generalization

- First 3 experiments:
  1. Test unlearning effectiveness on single concept with one image
  2. Evaluate model utility preservation on benchmark datasets
  3. Assess robustness against membership inference and jailbreak attacks

## Open Questions the Paper Calls Out

## Limitations

- Generalization Beyond Single Concepts: The approach's effectiveness for unlearning multiple concepts simultaneously or handling concept hierarchies remains unexplored.
- Robustness to Concept Variations: Uncertainty about how well the approach generalizes to visual variations of the same concept not present in the single training image.
- Long-term Stability: No evidence about whether unlearned concepts might resurface over time or with additional fine-tuning on other tasks.

## Confidence

- High Confidence (8/10): The core mechanism of using dual masked KL-divergence loss to preserve model utility while unlearning specific concepts is well-supported by experimental results.
- Medium Confidence (6/10): The claim that single-image unlearning is sufficient for complete concept removal is supported but has limitations regarding edge cases and visual variations.
- Low Confidence (4/10): Robustness claims against membership inference and jailbreak attacks are stated but not deeply validated with limited experimental evidence.

## Next Checks

1. **Multi-concept Unlearning Test**: Evaluate SIU's effectiveness when simultaneously unlearning multiple related concepts to assess scalability and interference between unlearning tasks.

2. **Visual Variation Stress Test**: Systematically test unlearning robustness by evaluating the model on diverse visual representations of the targeted concepts that were not present in the single training image.

3. **Long-term Stability Evaluation**: Conduct longitudinal testing where the model undergoes additional fine-tuning on unrelated tasks over multiple epochs, then reassess unlearning effectiveness to determine if unlearned concepts resurface over time.