---
ver: rpa2
title: 'ELICIT: LLM Augmentation via External In-Context Capability'
arxiv_id: '2410.09343'
source_url: https://arxiv.org/abs/2410.09343
tags:
- task
- arxiv
- elicit
- input
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ELICIT introduces a framework for enhancing large language models
  by storing and reusing task vectors that represent in-context learned capabilities.
  The approach dynamically retrieves relevant vectors from a capability library to
  improve model performance on arbitrary queries without additional training or inference
  tokens.
---

# ELICIT: LLM Augmentation via External In-Context Capability

## Quick Facts
- **arXiv ID**: 2410.09343
- **Source URL**: https://arxiv.org/abs/2410.09343
- **Reference count**: 40
- **Key outcome**: Average 11.4% improvement over zero-shot performance while maintaining same token usage

## Executive Summary
ELICIT introduces a framework for enhancing large language models by storing and reusing task vectors that represent in-context learned capabilities. The approach dynamically retrieves relevant vectors from a capability library to improve model performance on arbitrary queries without additional training or inference tokens. Experiments across 20 tasks and 4 models demonstrate significant performance gains while maintaining language modeling capability.

## Method Summary
ELICIT constructs a capability library by extracting task vectors from hidden states of ICL prompts, then applies these vectors to new queries through dynamic retrieval and intervention. The framework uses a similarity-based classifier (SimCSE RoBERTa + MLP head) to retrieve relevant vectors, applies threshold-based filtering, and intervenes at optimal layers identified through validation. The additive intervention strategy with scaling factor Î±=2 enables targeted capability enhancement without fine-tuning.

## Key Results
- Average 11.4% improvement over zero-shot performance across 20 tasks
- Strong generalization to unseen tasks with selective capability activation
- Effective compatibility with existing approaches like BM25 retrieval
- Particularly beneficial for smaller models while maintaining language modeling capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task vectors can simulate in-context learning behavior by steering model outputs through hidden state manipulation
- Mechanism: The task vector captures information from demonstrations in an ICL prompt, and when injected into the model's hidden states at the optimal layer, it influences the model to produce relevant outputs for new queries
- Core assumption: Hidden states at specific layers encode sufficient semantic information to represent task-specific capabilities that can be transferred to new inputs
- Evidence anchors:
  - [abstract]: "ELICIT introduces a framework for enhancing large language models by storing and reusing task vectors that represent in-context learned capabilities"
  - [section]: "We formally represent an ICL prompt... The Language Model (LM) aims to predict the corresponding target response yiq for the query input xiq. Through learning from the demonstrated input-output mappings in D, ICL can enhances the model's capability to perform this task"
  - [corpus]: Weak - no direct corpus evidence for this specific mechanism
- Break condition: If hidden states don't encode transferable task information, or if the optimal layer varies unpredictably across tasks, making dynamic selection impossible

### Mechanism 2
- Claim: Dynamic retrieval of relevant task vectors improves model performance without increasing token usage
- Mechanism: A similarity-based retriever identifies the most relevant task vectors from the library based on the input query, and only applies those vectors when they exceed a similarity threshold
- Core assumption: Task vectors are sufficiently distinct and separable that a similarity function can accurately match queries to relevant capabilities
- Evidence anchors:
  - [abstract]: "The approach dynamically retrieves relevant vectors from a capability library to improve model performance on arbitrary queries without additional training or inference tokens"
  - [section]: "We address the challenge of selecting the most relevant task vectors by employing a binary classifier to calculate similarity scores"
  - [corpus]: Weak - no direct corpus evidence for this specific retrieval mechanism
- Break condition: If the similarity function cannot accurately distinguish between relevant and irrelevant task vectors, leading to incorrect or harmful interventions

### Mechanism 3
- Claim: Selective activation of capabilities allows targeted performance enhancement without degrading other domains
- Mechanism: The threshold-based filtering mechanism ensures task vectors are only applied when highly relevant, preventing interference with unrelated capabilities
- Core assumption: Models have modular capabilities that can be independently activated without affecting other functional areas
- Evidence anchors:
  - [abstract]: "Experiments across 20 tasks and 4 models demonstrate an average 11.4% improvement over zero-shot performance while maintaining the same token usage"
  - [section]: "In our experiments with a math-only capability library, ELICIT boosted Math performance dramatically... while maintaining or slightly improving performance in other domains"
  - [corpus]: Weak - no direct corpus evidence for this specific selective activation mechanism
- Break condition: If model capabilities are not modular and activating one capability necessarily affects others, or if the threshold selection is too permissive/restrictive

## Foundational Learning

- Concept: In-Context Learning (ICL)
  - Why needed here: ELICIT builds upon the concept of ICL by extracting and reusing the capability representations that ICL demonstrates
  - Quick check question: What is the key difference between traditional fine-tuning and in-context learning in terms of how models adapt to new tasks?

- Concept: Task vectors and hidden state representations
  - Why needed here: Understanding how task vectors are extracted from hidden states is fundamental to grasping how ELICIT works
  - Quick check question: How is a task vector formally defined in relation to the hidden state representations in transformer models?

- Concept: Dynamic layer selection
  - Why needed here: ELICIT uses dynamic layer selection to identify the optimal layer for task vector intervention for each task
  - Quick check question: Why does ELICIT use dynamic layer selection rather than fixing the intervention layer for all tasks?

## Architecture Onboarding

- Component map:
  - Capability Library: Stores task vectors with their corresponding optimal layers
  - Dynamic Retrieval Module: Similarity-based classifier that retrieves relevant task vectors
  - Intervention Mechanism: Applies task vectors to model hidden states at specified layers
  - Base LLM: The model being augmented

- Critical path:
  1. Query input is processed by the retrieval module
  2. Top-k task vectors are selected based on similarity scores
  3. Threshold filtering determines whether to apply vectors
  4. Selected vectors are applied at their pre-identified optimal layers
  5. Model generates output with enhanced capabilities

- Design tradeoffs:
  - Library size vs. retrieval accuracy: Larger libraries may contain more relevant vectors but increase retrieval complexity
  - Threshold strictness vs. performance: Higher thresholds reduce false positives but may miss beneficial vectors
  - Intervention strength vs. language modeling: Stronger interventions improve task performance but may degrade general language capabilities

- Failure signatures:
  - Performance degradation across all tasks: May indicate incorrect threshold setting or poor vector quality
  - No performance improvement: Could suggest retrieval module isn't finding relevant vectors or intervention isn't effective
  - Domain-specific performance drops: May indicate vectors are being applied to inappropriate tasks

- First 3 experiments:
  1. Verify task vector extraction: Extract vectors from ICL prompts and test if they can reproduce ICL behavior on known tasks
  2. Validate retrieval accuracy: Test the similarity-based retriever on a held-out set to ensure it correctly identifies relevant vectors
  3. Assess threshold impact: Sweep the similarity threshold to find the optimal balance between precision and recall for your specific use case

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance gains of ELICIT scale with increasing model size beyond the tested range?
- Basis in paper: [explicit] The paper shows performance on Llama3-8B and Llama3-70B, but only discusses the larger model briefly in the context of potential future work.
- Why unresolved: The paper focuses primarily on smaller to medium-sized models and doesn't provide a comprehensive analysis of how ELICIT's effectiveness changes as models grow significantly larger.
- What evidence would resolve it: Systematic experiments comparing ELICIT performance across a wide range of model sizes, from small to extremely large, would clarify the scaling behavior.

### Open Question 2
- Question: What is the optimal strategy for constructing diverse and effective capability libraries, and how does library diversity impact overall performance?
- Basis in paper: [inferred] The paper mentions diversity-optimized prompts as future work and shows mixed results when comparing diverse versus non-diverse libraries, but doesn't fully explore the relationship between library composition and performance.
- Why unresolved: The paper only provides preliminary experiments on library diversity and doesn't systematically investigate how different library construction strategies affect performance across multiple domains.
- What evidence would resolve it: Comprehensive experiments comparing various library construction methods (random sampling, diversity optimization, task clustering, etc.) and their impact on performance across multiple domains would clarify optimal strategies.

### Open Question 3
- Question: How does ELICIT perform on non-English languages and multilingual tasks, and what modifications would be needed for effective cross-lingual capability elicitation?
- Basis in paper: [explicit] The paper states that experiments are conducted only on English datasets and leaves multilingual extension as future work.
- Why unresolved: The method's effectiveness on languages other than English remains unexplored, and it's unclear whether the current architecture would generalize to multilingual settings.
- What evidence would resolve it: Experiments applying ELICIT to multilingual datasets and different language families would demonstrate its cross-lingual capabilities and identify necessary architectural modifications.

## Limitations

- Limited validation of the core assumption that hidden states encode transferable task information across diverse architectures
- Reliance on similarity-based retrieval creates potential brittleness if the classifier fails to distinguish relevant vectors
- Performance on tasks with limited overlap to the capability library remains underexplored

## Confidence

- **High**: The core empirical results showing 11.4% average improvement over zero-shot performance are well-supported by systematic experiments across multiple models and tasks.
- **Medium**: The claim that ELICIT maintains language modeling capability while enhancing task-specific performance is supported but requires further validation across broader linguistic tasks.
- **Low**: The assertion that task vectors capture semantically meaningful representations of capabilities without additional training remains largely theoretical, with limited mechanistic evidence.

## Next Checks

1. **Cross-Architecture Transferability Test**: Evaluate ELICIT's performance when task vectors extracted from one model architecture (e.g., transformer-based) are applied to a fundamentally different architecture (e.g., Mamba's state-space model) to validate the universality of the vector representation approach.

2. **Long-Tail Task Performance Analysis**: Test ELICIT on tasks with limited or no overlap to the capability library to quantify performance degradation and validate the selectivity claims when encountering truly novel task domains.

3. **Ablation Study on Layer Selection**: Conduct systematic experiments varying the layer selection mechanism (fixed vs. dynamic) across all 20 tasks to isolate the contribution of dynamic layer selection to the overall performance gains.