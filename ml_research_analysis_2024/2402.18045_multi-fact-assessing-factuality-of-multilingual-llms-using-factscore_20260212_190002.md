---
ver: rpa2
title: 'Multi-FAct: Assessing Factuality of Multilingual LLMs using FActScore'
arxiv_id: '2402.18045'
source_url: https://arxiv.org/abs/2402.18045
tags:
- languages
- wikipedia
- english
- factscore
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multi-FAct, a pipeline for evaluating factuality
  of multilingual LLMs using FActScore. The authors extend the original FActScore
  by adding translation steps to handle multiple languages, breaking down long-form
  text into atomic facts, and validating them using a combination of retrieval and
  open-source models.
---

# Multi-FAct: Assessing Factuality of Multilingual LLMs using FActScore

## Quick Facts
- arXiv ID: 2402.18045
- Source URL: https://arxiv.org/abs/2402.18045
- Authors: Sheikh Shafayat; Eunsu Kim; Juhyun Oh; Alice Oh
- Reference count: 10
- Primary result: Extended FActScore to multilingual contexts via translation, showing geographic and language biases in LLM factual accuracy

## Executive Summary
This paper introduces Multi-FAct, a pipeline for evaluating factuality of multilingual LLMs using an extended FActScore metric. The authors address the challenge of assessing factuality across nine languages by translating non-English LLM outputs into English and evaluating them against English Wikipedia. They demonstrate that GPT-3.5-based translation does not significantly degrade FActScore estimates and that using sufficiently long non-English Wikipedia articles or ensembles of non-English Wikipedias can serve as viable alternatives to English Wikipedia for factuality evaluation. Experiments on biography generation tasks across nine languages show consistent geographic bias, with better performance on content related to North America and Europe.

## Method Summary
The Multi-FAct pipeline extends FActScore to multilingual contexts by generating text in multiple languages using LLMs, translating non-English outputs to English using GPT-3.5, breaking text into atomic facts using Mistral-7B, and verifying these facts against Wikipedia using retrieval-augmented generation and natural language premise models. The approach uses English Wikipedia as the reference source for all languages, with experiments comparing the use of single versus ensembled Wikipedia articles across nine languages for biography generation tasks.

## Key Results
- English consistently achieves higher factual accuracy than other languages across all tested LLMs
- Geographic bias exists, with better performance on content related to North America and Europe across all evaluated languages
- Using longer non-English Wikipedia articles or ensembles of non-English Wikipedias can serve as viable alternatives to English Wikipedia for factuality evaluation
- GPT-3.5-based translation does not significantly degrade FActScore estimates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translating non-English LLM outputs into English and evaluating them against English Wikipedia preserves factual accuracy estimates.
- Mechanism: The translation step allows reuse of the FActScore pipeline without needing language-specific retrieval or verification models, and the correlation analysis shows minimal degradation in factuality scoring.
- Core assumption: GPT-3.5 translation does not alter the factual content of the text.
- Evidence anchors:
  - [abstract] "We translate the generation and verify facts in English, rather than doing fact verification in corresponding languages..."
  - [section 3.4.2] "To simulate and test the reliability of Multi-FAct with respect to translation, we first take the human-annotated examples provided in the original work (Min et al., 2023) and translate them into the eight non-English languages using GPT-3.5"
  - [corpus] Weak - the analysis relies on synthetic translation of existing human-annotated data; no real-world degradation study was conducted.
- Break condition: If translation introduces factual errors or omits key information, the factuality estimates will become unreliable.

### Mechanism 2
- Claim: Using longer non-English Wikipedia articles as reference sources improves factuality evaluation accuracy.
- Mechanism: Longer articles provide more comprehensive context for fact verification, reducing the chance of unsupported or hallucinated facts being misclassified as correct.
- Core assumption: Article length correlates with coverage depth and relevance for fact verification.
- Evidence anchors:
  - [section 4.2] "Figure 3b shows that there is a linear relationship between FActScore estimation error and the word count of Wikipedia articles"
  - [section 4.1] "We observe relatively higher error rates for Bengali, Korean, Chinese, and Arabic" - these languages also have shorter Wikipedia articles on average.
  - [corpus] Weak - the analysis is based on a sample of 100 individuals per language; may not generalize to all topics.
- Break condition: If longer articles contain more noise or irrelevant information, the benefit of increased length may be offset by decreased precision.

### Mechanism 3
- Claim: Ensembling multiple non-English Wikipedia articles improves factuality evaluation accuracy compared to using a single non-English source.
- Mechanism: Combining articles from different languages increases the likelihood of capturing comprehensive information about a topic, mitigating the limitations of individual language Wikipedias.
- Core assumption: Different language Wikipedias cover complementary aspects of the same topic.
- Evidence anchors:
  - [section 4.2] "As depicted in the Fig 3a, among the two wikis with errors close to zero—namely, 'Euro' and 'All-Eng' cases—it is speculated that this could be attributed to their comprehensiveness"
  - [section 4.1] "However, as figure 2 illustrates, languages included in our study exhibit a wide range of variability in their representation within Wikipedia."
  - [corpus] Weak - the analysis only considers three ensembling strategies; the effectiveness may vary depending on the specific languages combined.
- Break condition: If the ensembled articles contain contradictory information or if one language dominates the ensemble, the benefits of ensembling may be reduced.

## Foundational Learning

- Concept: Factuality evaluation metrics
  - Why needed here: The paper relies on FActScore as the primary metric for assessing the factual accuracy of LLM-generated text. Understanding how FActScore works is crucial for interpreting the results and designing experiments.
  - Quick check question: How does FActScore decompose long-form text into atomic facts and verify them against a knowledge base?

- Concept: Wikipedia as a knowledge base
  - Why needed here: The paper uses English Wikipedia as the reference source for fact verification. Understanding the strengths and limitations of Wikipedia as a knowledge base is important for evaluating the generalizability of the findings.
  - Quick check question: What are the potential biases and coverage gaps in Wikipedia that could affect factuality evaluation?

- Concept: Machine translation evaluation
  - Why needed here: The paper relies on GPT-3.5 translation to convert non-English LLM outputs into English for evaluation. Understanding how to assess the quality and impact of machine translation is crucial for interpreting the results.
  - Quick check question: How can we measure the impact of machine translation on the factual content of a text?

## Architecture Onboarding

- Component map: LLM generation (GPT-3.5, GPT-4) -> Translation (GPT-3.5) -> Atomic fact extraction (Mistral-7B) -> Fact verification (Mistral-7B + RAG + NPM) -> FActScore calculation

- Critical path: LLM generation → Translation → Atomic fact extraction → Fact verification → FActScore calculation

- Design tradeoffs:
  - Using English Wikipedia as the reference source simplifies the pipeline but may introduce bias
  - Translating non-English text to English enables reuse of existing models but may introduce translation errors
  - Ensembling multiple non-English Wikipedia articles improves coverage but increases complexity and potential for contradictions

- Failure signatures:
  - Low FActScore values may indicate hallucination or insufficient reference material
  - High variance in FActScore across languages may indicate bias or inconsistent evaluation quality
  - Discrepancies between FActScore and the number of correct facts may indicate issues with the metric or the evaluation pipeline

- First 3 experiments:
  1. Replicate the core FActScore evaluation on a small set of English biographies to validate the pipeline
  2. Test the impact of machine translation on factuality estimates by comparing translated and original English texts
  3. Evaluate the effect of article length on factuality estimates by sampling Wikipedia articles of varying lengths for the same topics

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the following questions arise from the analysis and findings:

### Open Question 1
- Question: What are the long-term trends in factual accuracy for multilingual LLMs across different languages and geographic regions?
- Basis in paper: [explicit] The paper discusses current biases and performance differences but does not explore how these might change over time.
- Why unresolved: The paper is a snapshot of current performance and does not include longitudinal data or predictions about future improvements.
- What evidence would resolve it: Longitudinal studies comparing factual accuracy of multilingual LLMs over several years, tracking improvements or changes in bias patterns.

### Open Question 2
- Question: How do different factuality evaluation methods compare in terms of accuracy and resource efficiency for multilingual contexts?
- Basis in paper: [inferred] The paper introduces Multi-FAct and compares it to some baselines, but does not extensively compare it to all existing methods or newer approaches.
- Why unresolved: The paper focuses on validating Multi-FAct but does not provide a comprehensive comparison with all available factuality evaluation methods.
- What evidence would resolve it: Comparative studies of various factuality evaluation methods, including Multi-FAct, across multiple languages and contexts, measuring both accuracy and resource efficiency.

### Open Question 3
- Question: What specific factors contribute to the observed biases in multilingual LLM factual accuracy across different geographic regions?
- Basis in paper: [explicit] The paper identifies biases but does not deeply explore the underlying causes of these biases.
- Why unresolved: The paper acknowledges biases but does not investigate the root causes or contributing factors to these observed patterns.
- What evidence would resolve it: In-depth analysis of training data, model architecture, and other factors that might contribute to geographic biases in multilingual LLM factual accuracy.

### Open Question 4
- Question: How can factuality evaluation methods be adapted to account for the varying informational value of different facts?
- Basis in paper: [explicit] The paper mentions that FActScore and Multi-FAct do not differentiate between the informational value of facts, treating all facts equally.
- Why unresolved: The current methods treat all facts as equally important, which may not reflect the true informational value or usefulness of different facts.
- What evidence would resolve it: Development and validation of factuality evaluation methods that can distinguish between high-value and low-value facts, potentially incorporating human judgment or automated measures of fact importance.

## Limitations
- Heavy reliance on English Wikipedia as the reference source may introduce language and geographic bias
- Focus on biography generation limits generalizability to other text generation tasks
- GPT-3.5 translation introduces an uncontrolled variable that could affect factuality estimates

## Confidence
- Low confidence: Core claim that GPT-3.5 translation preserves factual accuracy (validation relies on synthetic translation of human-annotated examples)
- Medium confidence: Geographic bias findings (may not generalize beyond biography tasks)
- Medium confidence: Wikipedia article length findings (based on limited sample size)

## Next Checks
1. **Translation quality validation**: Conduct a human evaluation study comparing factuality scores of native English biographies versus GPT-3.5 translated biographies to quantify translation-induced factual degradation.

2. **Geographic bias generalization**: Test the geographic bias hypothesis on non-biography tasks (e.g., news summarization or product reviews) across the same language set to assess whether the observed bias extends beyond biographical content.

3. **Wikipedia dependency assessment**: Evaluate factuality using language-specific reference sources (e.g., news articles, domain-specific knowledge bases) alongside Wikipedia to determine the extent to which Wikipedia bias influences the results.