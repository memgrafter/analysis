---
ver: rpa2
title: Structured List-Grounded Question Answering
arxiv_id: '2410.03950'
source_url: https://arxiv.org/abs/2410.03950
tags:
- list
- lists
- user
- question
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of question answering systems
  effectively interpreting and utilizing structured list data, which is often overlooked
  in previous research. The authors introduce LIST 2QA, a novel benchmark dataset
  for evaluating QA systems on understanding list semantics with respect to user background.
---

# Structured List-Grounded Question Answering

## Quick Facts
- arXiv ID: 2410.03950
- Source URL: https://arxiv.org/abs/2410.03950
- Reference count: 9
- Fine-tuned Flan-T5-XL with ISL shows increases of 3.1% in ROUGE-L, 4.6% in correctness, 4.5% in faithfulness, and 20.6% in completeness compared to baseline models without filtering and ISL

## Executive Summary
This paper introduces LIST 2QA, a novel benchmark dataset for evaluating question answering systems on understanding structured list semantics in relation to user backgrounds. The authors propose the Intermediate Steps for Lists (ISL) method, which aligns list items with user contexts before generating responses. Through extensive experiments, they demonstrate that fine-tuned smaller models using ISL significantly outperform larger zero-shot language models on the LIST 2QA dataset, achieving substantial improvements across multiple evaluation metrics.

## Method Summary
The authors develop a pipeline-based approach using large language models to simulate user queries and system answers grounded over structured lists from ConditionalQA and MultiDoc2Dial corpora. They employ model-based filtering to enhance data quality by removing low-quality samples with inaccurate information or hallucinations. The core method, ISL, generates intermediate steps including identifying relevant passages, classifying list types, determining logical relations, and evaluating user-to-item status before final response generation. The dataset is then used to fine-tune smaller efficient LLMs (Flan-T5-XL, Mistral-7B-Instruct) with QLoRA for effective adaptation to the structured list-grounded QA task.

## Key Results
- Fine-tuned Flan-T5-XL with ISL achieves 3.1% higher ROUGE-L score compared to baselines
- ISL method improves correctness by 4.6%, faithfulness by 4.5%, and completeness by 20.6%
- Model-based filtering consistently improves performance across two models and four metrics
- Smaller fine-tuned models outperform larger zero-shot models on LIST2QA benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned smaller models outperform larger zero-shot models on list-grounded QA tasks
- Mechanism: Specialized fine-tuning on domain-specific structured list data enables better capture of list semantics and logical relations compared to generic large models
- Core assumption: List-grounded QA requires understanding of specific list structures and user-item status alignment that general LLMs haven't been trained on
- Evidence anchors: [abstract] "models trained on LIST2QA with our ISL approach outperform baselines across various metrics"; [section 3.3] "the fine-tuned models with ISL consistently outperform those without ISL on seen data, achieving up to a 3.1% higher average score"

### Mechanism 2
- Claim: Intermediate Steps for Lists (ISL) improves performance by explicitly modeling structured list data
- Mechanism: Generating intermediate steps (identifying relevant passages, classifying list types, determining logical relations, evaluating user-to-item status) before final response generation helps models better align with human reasoning processes
- Core assumption: Humans naturally break down list interpretation into intermediate steps before forming responses, and this process can be formalized for model training
- Evidence anchors: [abstract] "we further explore the explicit use of Intermediate Steps for Lists (ISL), aligning list items with user backgrounds to better reflect how humans interpret list items before generating responses"; [section 3.3] "Flan-T5-XL with ISL achieves a 3.2% higher performance than without ISL across four metrics on average"

### Mechanism 3
- Claim: Model-based filtering of synthetic data improves quality while maintaining diversity
- Mechanism: Using GPT-4 to filter out low-quality samples based on answerability, correctness, faithfulness, and completeness ensures better training data without excessive human annotation
- Core assumption: Automated filtering can effectively distinguish between high and low-quality samples while preserving the diversity needed for robust model training
- Evidence anchors: [section 2.3] "We improve the data quality by filtering out instances with inaccurate information or hallucinations" with "a Cohen's kappa score of 56.1" between human and model-based filtering; [section 3.3] "model-based filtering of the training data consistently results in performance improvements across two models and four metrics"

## Foundational Learning

- Concept: Logical relations in conditional lists (AND, OR)
  - Why needed here: Understanding how different list items interact logically is crucial for determining whether user scenarios satisfy all conditions
  - Quick check question: Given a condition list with items A and B where A is supported and B is contradicted, what is the final answer for an AND relation vs an OR relation?

- Concept: User-to-item status alignment
  - Why needed here: Determining whether user scenarios support, contradict, or are unknown relative to each list item is essential for accurate responses
  - Quick check question: If a user claims they have a university degree and a list item requires "no higher education qualification," what is the user-to-item status?

- Concept: Structured list classification
  - Why needed here: Different list types (condition, step, option, non-action info) require different reasoning approaches and intermediate steps
  - Quick check question: How would you classify a list that presents multiple choices a user can select from?

## Architecture Onboarding

- Component map: Document corpus → List classification → User-item status assignment → Question/Response generation → Model-based filtering → Retriever (top-K passages) → ISL generator → Response generator → Evaluation (ROUGE-L + LLM-based metrics)
- Critical path: Document corpus → Fine-tuning → ISL generation → Response generation → Evaluation
- Design tradeoffs:
  - Model size vs. performance: Smaller fine-tuned models outperform larger zero-shot models
  - Synthetic data quantity vs. quality: Filtering removes 49% of samples but improves performance
  - ISL complexity vs. overhead: Intermediate steps improve accuracy but add generation steps
- Failure signatures:
  - Low recall@K from retriever indicates retrieval issues
  - High faithfulness but low completeness suggests conservative responses
  - Low correctness across all metrics indicates fundamental misunderstanding of list semantics
- First 3 experiments:
  1. Compare zero-shot large models vs. fine-tuned smaller models on LIST2QA test set
  2. Evaluate impact of ISL on different list types (condition vs. step vs. option)
  3. Test model-based filtering effectiveness by training on filtered vs. unfiltered datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do more complex logical relations (e.g., 'nor', nested relations) in conditional lists affect the performance of the LIST 2QA benchmark and the ISL method?
- Basis in paper: [inferred]
- Why unresolved: The authors mention that their current approach only handles 'and' and 'or' logical relations and plan to investigate more diverse logical relations in future work.
- What evidence would resolve it: Experimenting with additional logical relations in the LIST 2QA dataset and evaluating model performance on these new relation types.

### Open Question 2
- Question: Can the ISL method be extended to handle multi-turn dialogues that involve structured lists, and how would this impact the overall performance of the QA system?
- Basis in paper: [inferred]
- Why unresolved: The authors focus on single-turn QA tasks in their current work and leave the exploration of multi-turn dialogues for future work.
- What evidence would resolve it: Developing and evaluating a multi-turn dialogue system that incorporates the ISL method for handling structured lists.

### Open Question 3
- Question: What are the most effective approaches for developing an automatic evaluation method for structured list-grounded question answering systems that is both less expensive and more accurate than human evaluation?
- Basis in paper: [inferred]
- Why unresolved: The authors note that using GPT-4 for evaluation is costly and somewhat less accurate compared to human evaluation, and they aim to develop a better automatic evaluation method in the future.
- What evidence would resolve it: Creating and validating a new automatic evaluation metric specifically designed for structured list-grounded question answering tasks.

## Limitations

- The ISL method relies heavily on synthetic data generation and model-based filtering, introducing potential biases and quality concerns with moderate Cohen's kappa agreement (56.1%) between human and model filtering
- LLM-based evaluation metrics (correctness, faithfulness, completeness) are computationally expensive and may lack consistency compared to human evaluation
- The evaluation framework is tailored specifically to the LIST 2QA dataset, potentially limiting generalizability to other domains or list structures

## Confidence

**High Confidence:** Fine-tuned smaller models outperform larger zero-shot models on list-grounded QA tasks
- Well-supported by experimental results showing consistent performance improvements across multiple metrics and models

**Medium Confidence:** ISL improves performance through better alignment with human reasoning processes
- Supported by the 3.2% average performance increase, though specific contribution of each intermediate step is not fully isolated

**Medium Confidence:** Model-based filtering effectively improves data quality
- Demonstrated through performance gains, but moderate Cohen's kappa score (56.1%) suggests room for improvement in filtering reliability

## Next Checks

1. Replicate filtering reliability: Test the model-based filtering pipeline with multiple runs to assess consistency and compare results with alternative filtering approaches or different threshold settings

2. Generalize ISL components: Evaluate the intermediate steps independently by testing the model's performance with different combinations of ISL components (e.g., only user-item status alignment vs. full ISL pipeline) on both seen and unseen list structures

3. Benchmark external datasets: Validate the approach on alternative list-grounded QA datasets or real-world applications beyond the LIST 2QA benchmark to assess true generalization capabilities