---
ver: rpa2
title: Efficient Medical Question Answering with Knowledge-Augmented Question Generation
arxiv_id: '2405.14654'
source_url: https://arxiv.org/abs/2405.14654
tags:
- medical
- questions
- question
- dataset
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of improving medical question
  answering performance using small language models. The proposed method combines
  pre-training on a corpus of medical textbooks with a novel question generation approach
  using GPT-4 prompted with textbook knowledge.
---

# Efficient Medical Question Answering with Knowledge-Augmented Question Generation

## Quick Facts
- arXiv ID: 2405.14654
- Source URL: https://arxiv.org/abs/2405.14654
- Reference count: 25
- Small language models significantly outperform baseline BioMedLM on medical QA when fine-tuned with knowledge-augmented question generation

## Executive Summary
This work addresses the challenge of improving medical question answering performance using small language models. The proposed method combines pre-training on a corpus of medical textbooks with a novel question generation approach using GPT-4 prompted with textbook knowledge. This generates a large dataset of medical questions resembling the downstream task. The method is evaluated on a new medical QA dataset, ECN-QA, containing progressive questions requiring multi-step reasoning. Results show the proposed approach significantly improves performance over the baseline BioMedLM model, surpassing GPT-3.5 accuracy while using far fewer parameters. The study demonstrates the potential of small language models for medical QA when appropriately fine-tuned with knowledge-augmented question generation.

## Method Summary
The authors propose a two-fold approach to improve medical question answering with small language models. First, they fine-tune a BioMedLM model on a corpus of 234,495 medical textbook sections containing 174 million tokens. Second, they use GPT-4 to generate questions similar to the downstream task, prompted with textbook knowledge, creating 10,237,240 tokens of synthetic data. The model is then fine-tuned on the generated questions and finally on the ECN-QA dataset, which contains French medical exam questions translated to English. The final model achieves proposition-level accuracy on this challenging multi-step reasoning task.

## Key Results
- The proposed approach significantly improves BioMedLM performance on ECN-QA, surpassing GPT-3.5 accuracy while using far fewer parameters
- Fine-tuning on medical textbooks improves accuracy by approximately 2 points over the baseline
- The knowledge-augmented question generation method alone improves accuracy by 1 point
- The combined approach achieves the highest accuracy among tested methods

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning on domain-specific textbook sections improves model understanding of specialized terminology. Pre-training on medical textbook sections exposes the model to specialized vocabulary and conceptual structures used in medicine, enabling better comprehension of medical questions. The core assumption is that the textbook corpus covers the semantic and syntactic patterns present in the target question answering task.

### Mechanism 2
GPT-4-generated questions guided by textbook knowledge simulate the complexity of the target task. GPT-4 is prompted with textbook-derived knowledge and instructed to generate clinical cases with progressive multi-step questions. These synthetic cases resemble the ECN-QA format and increase training data diversity. The core assumption is that GPT-4 can accurately mimic the style, difficulty, and reasoning structure of real ECN questions when provided with relevant domain knowledge.

### Mechanism 3
Combining textbook pre-training with knowledge-augmented question generation yields additive improvements. Sequential fine-tuning—first on textbooks, then on generated questions—progressively specializes the model. The model benefits from both raw domain knowledge and task-specific reasoning practice. The core assumption is that each stage of fine-tuning builds complementary knowledge without catastrophic forgetting.

## Foundational Learning

- Concept: Fine-tuning on domain-specific corpora
  - Why needed here: General-purpose LMs like BioMedLM underperform on specialized medical QA due to domain mismatch
  - Quick check question: What is the accuracy gain when fine-tuning BioMedLM on medical textbooks vs. the baseline?

- Concept: Synthetic data generation via prompting
  - Why needed here: The ECN-QA dataset is small; GPT-4 can generate many realistic clinical cases to expand training coverage
  - Quick check question: How many synthetic questions are generated, and how do they differ from real questions?

- Concept: Progressive question structure
  - Why needed here: Medical reasoning often requires multi-step inference; ECN-QA's progressive questions test this ability
  - Quick check question: How many sub-questions can a single PQ contain, and what type of reasoning do they require?

## Architecture Onboarding

- Component map: Medical textbooks -> BioMedLM fine-tuning -> GPT-4 question generation -> Synthetic data fine-tuning -> ECN-QA fine-tuning -> Proposition-level classification

- Critical path: Extract and preprocess textbook sections from PDFs -> Pre-train BioMedLM on textbook corpus -> Generate synthetic questions using GPT-4 with textbook prompts -> Fine-tune on synthetic questions -> Fine-tune on ECN-QA -> Evaluate with proposition-level accuracy

- Design tradeoffs: Parameter efficiency vs. accuracy (small model with specialized training vs. large general model), Synthetic vs. real data (more synthetic data increases diversity but may introduce noise), French -> English translation (necessary for model compatibility but may introduce semantic drift)

- Failure signatures: Low accuracy on unseen medical topics (e.g., pediatrics), Inconsistent proposition-level predictions (model contradicts itself), Overfitting to synthetic data (accuracy gap with GPT-4 remains large)

- First 3 experiments: 1) Pre-train BioMedLM on medical textbooks only; evaluate on ECN-QA, 2) Fine-tune BioMedLM on GPT-4-generated questions only; evaluate on ECN-QA, 3) Combine both pre-training stages; compare performance to GPT-3.5 baseline

## Open Questions the Paper Calls Out

- How does the accuracy of the model change when the size of the pre-training dataset is increased?
- How does the accuracy of the model change when the number of generated questions is increased?
- How does the accuracy of the model change when retrieval-based answering (open-book exam) is incorporated?
- How does the accuracy of the model change when it is fine-tuned on a larger dataset of medical questions?
- How does the accuracy of the model change when it is trained on a more diverse set of medical textbooks?

## Limitations

- The study relies heavily on GPT-4-generated questions for training, but no systematic evaluation of synthetic data quality or alignment with real ECN-QA questions is provided
- The ECN-QA dataset is originally in French and translated to English for this study, with no assessment of translation quality or its impact on model performance
- Results are reported only for BioMedLM and compared to GPT-3.5 and GPT-4, with unknown generalizability to other small language models

## Confidence

- High confidence: The sequential fine-tuning approach demonstrably improves BioMedLM performance on the ECN-QA dataset compared to baseline
- Medium confidence: The specific mechanisms (textbook pre-training and knowledge-augmented question generation) contribute additive improvements, though relative contribution is not rigorously quantified
- Low confidence: Claims about the approach's potential for broader medical QA applications or superiority over alternative fine-tuning strategies lack supporting evidence

## Next Checks

1. Conduct a synthetic data quality audit by comparing a stratified sample of GPT-4-generated questions against real ECN-QA questions using human evaluation metrics to quantify alignment
2. Perform an ablation study on training stages to isolate the contribution of each fine-tuning stage and determine the additive value and optimal training sequence
3. Evaluate the trained model directly on French ECN-QA questions or assess translation quality impact by back-translating and comparing with original French questions