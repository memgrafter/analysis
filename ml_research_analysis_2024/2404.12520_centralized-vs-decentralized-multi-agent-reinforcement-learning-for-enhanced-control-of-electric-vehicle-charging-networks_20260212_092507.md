---
ver: rpa2
title: Centralized vs. Decentralized Multi-Agent Reinforcement Learning for Enhanced
  Control of Electric Vehicle Charging Networks
arxiv_id: '2404.12520'
source_url: https://arxiv.org/abs/2404.12520
tags:
- charging
- agents
- network
- control
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of distributed EV charging control
  in residential networks, where multiple EVs share a transformer and aim to minimize
  charging costs under dynamic pricing. The authors propose a Multi-Agent Reinforcement
  Learning (MARL) framework based on Deep Deterministic Policy Gradient (DDPG) with
  Centralized Training Decentralized Execution (CTDE), referred to as CTDE-DDPG.
---

# Centralized vs. Decentralized Multi-Agent Reinforcement Learning for Enhanced Control of Electric Vehicle Charging Networks

## Quick Facts
- arXiv ID: 2404.12520
- Source URL: https://arxiv.org/abs/2404.12520
- Reference count: 40
- Primary result: CTDE-DDPG outperforms I-DDPG in EV charging networks, achieving 36% reduction in total variation and 9.1% reduction in average charging cost

## Executive Summary
This paper addresses the problem of distributed EV charging control in residential networks where multiple EVs share a transformer and aim to minimize charging costs under dynamic pricing. The authors propose a Multi-Agent Reinforcement Learning framework based on Deep Deterministic Policy Gradient (DDPG) with Centralized Training Decentralized Execution (CTDE), referred to as CTDE-DDPG. They theoretically analyze and compare CTDE-DDPG against Independent-DDPG (I-DDPG), showing both have the same expected policy gradient but CTDE-DDPG has higher variance. Despite this, CTDE-DDPG outperforms I-DDPG in practice due to better cooperation between agents, achieving approximately 36% reduction in total variation of charging rates and 9.1% reduction in average charging cost.

## Method Summary
The paper formulates the EV charging problem as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) where multiple EV agents share a transformer and must minimize charging costs under dynamic pricing. The proposed CTDE-DDPG algorithm uses a centralized critic during training that observes all agents' actions and states, enabling cooperative learning while maintaining decentralized execution. The framework compares against I-DDPG where each agent learns independently. Both algorithms use actor-critic architectures with experience replay buffers, target networks, and are trained over 10,000 episodes with 34 iterations per episode.

## Key Results
- CTDE-DDPG achieves approximately 36% reduction in total variation of charging rates compared to I-DDPG
- CTDE-DDPG reduces average charging cost by around 9.1% compared to I-DDPG
- CTDE-DDPG significantly improves fairness across agents and maintains performance as agent count increases
- Theoretical analysis shows CTDE-DDPG and I-DDPG have equal expected policy gradients, but CTDE-DDPG has higher variance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Centralized critic training enables cooperation that reduces total variation in charging rates by approximately 36%.
- Mechanism: During centralized training, the shared critic network provides consistent value estimates across agents, allowing them to learn coordinated strategies that smooth individual charging profiles. This reduces fluctuations in charging rates compared to independent learning.
- Core assumption: Agents can learn to cooperate effectively when they share value function information during training, even though they act independently during execution.
- Evidence anchors:
  - [abstract] "CTDE-DDPG outperforms I-DDPG in practice due to better cooperation between agents, achieving approximately 36% reduction in total variation of charging rates"
  - [section] "The smooth charging pattern exhibited by the CTDE version suggests that the proposed cooperative MARL surpasses the independent MARL version"
- Break condition: If agents' reward functions are too misaligned or if the centralized critic becomes a bottleneck in scalability, cooperation may fail to materialize.

### Mechanism 2
- Claim: Centralized critic training reduces nonstationarity experienced by individual agents during learning.
- Mechanism: In I-DDPG, each agent's policy change affects the environment for others, creating nonstationary learning conditions. CTDE-DDPG mitigates this by providing consistent value estimates based on joint observations and actions during training.
- Core assumption: Nonstationarity is a primary factor limiting learning stability in multi-agent settings, and sharing value information during training can mitigate this effect.
- Evidence anchors:
  - [abstract] "the CTDE-DDPG algorithm combats nonstationarity due to the cooperation between agents during training"
  - [section] "This lack of collaboration adversely impacts the overall performance of I-DDPG in multi-agent scenarios"
- Break condition: If the number of agents becomes very large, the centralized critic may become too complex to provide meaningful guidance, or the policy gradient variance may become prohibitive.

### Mechanism 3
- Claim: Despite higher variance in policy gradients, CTDE-DDPG achieves better fairness across agents.
- Mechanism: The centralized critic creates more consistent learning targets across agents, leading to more equitable performance distribution. This compensates for the increased variance in gradient estimates.
- Core assumption: Higher policy gradient variance can be tolerated if it leads to more consistent and fair learning outcomes across agents.
- Evidence anchors:
  - [abstract] "CTDE-DDPG significantly improves charging efficiency by reducing total variation by approximately 36% and charging cost by around 9.1% on average"
  - [section] "there is a noticeable decline in I-DDPG performance as the number of agents increases... some of the agents may perform poorly compared with best-performing agents"
- Break condition: If the variance becomes too high relative to the learning signal, convergence may fail entirely, making the fairness advantage irrelevant.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The EV charging problem is formulated as a Dec-POMDP, which extends MDPs to multi-agent partially observable settings. Understanding MDPs is essential for grasping the reinforcement learning framework.
  - Quick check question: In an MDP, what are the key components that define the decision-making problem?

- Concept: Policy Gradient Methods
  - Why needed here: Both CTDE-DDPG and I-DDPG are based on policy gradient methods. Understanding how policy gradients work is crucial for analyzing the theoretical results comparing the two approaches.
  - Quick check question: What is the main difference between the REINFORCE algorithm and deterministic policy gradient methods like DDPG?

- Concept: Variance Analysis in Multi-Agent Learning
  - Why needed here: The paper's key theoretical contribution is showing that CTDE-DDPG has higher policy gradient variance than I-DDPG, despite better empirical performance. Understanding variance sources is essential for interpreting these results.
  - Quick check question: What are the primary sources of variance in policy gradient estimates in multi-agent settings?

## Architecture Onboarding

- Component map:
  - Actor networks: One per agent, mapping observations to actions
  - Critic networks: Two variants - decentralized (I-DDPG) or centralized (CTDE-DDPG)
  - Experience replay buffer: Shared across all agents
  - Target networks: Separate target actor and critic networks for stability
  - Centralized critic: Uses joint observations and actions during training only

- Critical path:
  1. Agents collect experience (observation, action, reward, next observation)
  2. Experience stored in replay buffer
  3. Centralized training phase: Critic networks updated using batches from buffer
  4. Actor networks updated using policy gradients from critics
  5. Target networks periodically updated
  6. Decentralized execution: Agents act independently using actor networks

- Design tradeoffs:
  - CTDE-DDPG: Better cooperation and fairness vs. higher computational complexity and policy gradient variance
  - I-DDPG: Lower variance and simpler implementation vs. less stable learning and fairness issues as agent count increases
  - Centralized critic scalability: Tradeoff between cooperation benefits and computational overhead

- Failure signatures:
  - High variance in policy gradients causing unstable learning
  - Centralized critic becoming a bottleneck in large-scale deployments
  - Agents learning conflicting strategies despite shared critic
  - Poor exploration-exploitation balance in multi-agent setting

- First 3 experiments:
  1. Implement a single-agent DDPG baseline to validate the basic reinforcement learning setup
  2. Compare I-DDPG vs. CTDE-DDPG with 2-3 agents to observe initial cooperation effects
  3. Scale to 10 agents and measure total variation in charging rates and fairness metrics across agents

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the CTDE-DDPG method's performance change under different dynamic pricing functions beyond the quadratic model?
- Basis in paper: [explicit] The paper uses a quadratic pricing function and states it satisfies certain assumptions, but does not explore other pricing functions.
- Why unresolved: The paper only considers one specific pricing function, limiting generalizability to other pricing models.
- What evidence would resolve it: Numerical results comparing CTDE-DDPG performance under various pricing functions (linear, step, real-world pricing data) would show robustness to different pricing models.

### Open Question 2
- Question: What is the impact of communication delays or packet losses on the CTDE-DDPG method's performance during the training phase?
- Basis in paper: [inferred] The paper assumes perfect communication during training but does not address network reliability issues.
- Why unresolved: Real-world communication networks are prone to delays and losses, which could affect the centralized critic's access to all agents' information.
- What evidence would resolve it: Simulation results with varying levels of communication delays and packet loss rates would quantify the impact on learning performance and convergence.

### Open Question 3
- Question: How does the CTDE-DDPG method scale when agents have heterogeneous objectives or constraints?
- Basis in paper: [explicit] The paper assumes homogeneous agents with the same reward structure and constraints.
- Why unresolved: Real EV networks often have diverse user preferences and vehicle capabilities, making homogeneous assumptions unrealistic.
- What evidence would resolve it: Experiments with agents having different battery capacities, charging preferences, or reward functions would demonstrate scalability to heterogeneous scenarios.

## Limitations

- Claims are based on simulation results rather than real-world deployment data, creating uncertainty about generalization
- Theoretical analysis establishing equal expected policy gradients appears sound, but higher variance claim requires broader empirical validation
- Performance improvements are specific to test scenarios and may not hold for different EV penetration rates or transformer capacity constraints

## Confidence

- **High**: Theoretical equivalence of expected policy gradients between CTDE-DDPG and I-DDPG
- **Medium**: Empirical performance improvements (36% reduction in total variation, 9.1% cost reduction)
- **Medium**: Claims about fairness improvements and robustness to increasing agent counts

## Next Checks

1. **Scalability validation**: Test CTDE-DDPG performance with 50+ agents to determine if the centralized critic remains effective or becomes a bottleneck, measuring both learning stability and computational overhead.

2. **Reward function sensitivity**: Systematically vary the reward function coefficients (α₁, α₂, E) to assess whether the reported performance improvements persist across different optimization objectives beyond just cost minimization.

3. **Real-world transferability**: Implement the algorithms on a physical testbed or high-fidelity power system simulator with actual EV charging profiles and grid constraints to validate the simulation results against realistic operating conditions.