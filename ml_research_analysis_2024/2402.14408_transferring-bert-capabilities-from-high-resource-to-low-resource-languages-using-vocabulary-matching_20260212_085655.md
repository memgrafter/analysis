---
ver: rpa2
title: Transferring BERT Capabilities from High-Resource to Low-Resource Languages
  Using Vocabulary Matching
arxiv_id: '2402.14408'
source_url: https://arxiv.org/abs/2402.14408
tags:
- bert
- language
- silesian
- languages
- herbert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of training BERT models for low-resource
  languages, where limited data hinders effective training. The proposed method involves
  transferring BERT capabilities from high-resource to low-resource languages using
  vocabulary matching.
---

# Transferring BERT Capabilities from High-Resource to Low-Resource Languages Using Vocabulary Matching

## Quick Facts
- **arXiv ID**: 2402.14408
- **Source URL**: https://arxiv.org/abs/2402.14408
- **Authors**: Piotr Rybak
- **Reference count**: 0
- **Primary result**: Vocabulary matching technique achieves 60.27% MLM accuracy for Silesian and 96% retrieval accuracy, outperforming zero-shot and fine-tuned baselines

## Executive Summary
This paper addresses the challenge of training BERT models for low-resource languages where limited data hinders effective training. The proposed method transfers BERT capabilities from high-resource languages to low-resource languages using vocabulary matching through an external dictionary. The approach involves adapting the BERT tokenizer of a high-resource model to initialize a model for the low-resource language, then fine-tuning on limited target data. Experiments on Silesian and Kashubian languages demonstrate that this approach significantly outperforms both zero-shot transfer and standard fine-tuning baselines.

## Method Summary
The vocabulary matching technique uses a bilingual dictionary to align token vocabularies between high-resource and low-resource languages. When tokens exist in both the dictionary and source vocabulary, their embeddings are directly copied. For tokens not in the dictionary, the source tokenizer segments them and the resulting token embeddings are averaged. This creates a properly initialized model that is then fine-tuned on target language data. The method is evaluated on Silesian and Kashubian languages using Polish as the high-resource source language.

## Key Results
- Matched HerBERT model achieves 60.27% MLM accuracy for Silesian and 59.13% for Kashubian
- Matched model achieves 96% accuracy and 81.79% NDCG@10 in passage retrieval for Silesian
- Outperforms both zero-shot transfer and standard fine-tuning approaches on all evaluated tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vocabulary matching enables direct transfer of embedding layer weights from high-resource to low-resource BERT models
- Mechanism: When target language tokens exist in bilingual dictionary and source vocabulary, their embeddings are directly copied from source model
- Core assumption: Semantic similarity between high-resource and low-resource language tokens allows direct weight transfer
- Evidence anchors:
  - [abstract] "use an external dictionary to adapt the BERT tokenizer of the high-resource model to properly initialize the BERT model for the low-resource language"
  - [section 3.2] "If a token from the target model vocabulary is present in the bilingual dictionary and its translation is present in the source model vocabulary, we directly copy its weights"
  - [corpus] Weak - corpus shows dictionary sizes but not token overlap statistics
- Break condition: When target language has significantly different morphology or syntax from source language, direct weight transfer becomes ineffective

### Mechanism 2
- Claim: Averaging source model embeddings for unseen tokens provides reasonable initialization
- Mechanism: For tokens not in dictionary or source vocabulary, split using source tokenizer and average resulting token embeddings
- Core assumption: Source tokenizer can meaningfully segment target language tokens into known subword units
- Evidence anchors:
  - [section 3.2] "If the target token is not present in either the bilingual dictionary or the source vocabulary, we split the token from the target vocabulary using the source tokenizer"
  - [corpus] Missing - no quantitative evidence of tokenization quality or split effectiveness
- Break condition: When source tokenizer fails to meaningfully segment target language tokens, averaging produces poor representations

### Mechanism 3
- Claim: Bilingual dictionary enables semantic alignment between high-resource and low-resource language vocabularies
- Mechanism: Dictionary provides translation pairs that map semantically equivalent words between languages
- Core assumption: Dictionary entries capture sufficient semantic coverage for effective model transfer
- Evidence anchors:
  - [section 3.2] "We use existing bilingual dictionaries for vocabulary matching between the target language and Polish"
  - [table 1] Dictionary sizes: Silesian-Polish 23,934 pairs, Kashubian-Polish 23,762 pairs
  - [corpus] Weak - no evidence of dictionary quality or coverage adequacy
- Break condition: When dictionary lacks coverage of domain-specific or technical vocabulary needed for target tasks

## Foundational Learning

- Concept: BERT tokenizer and vocabulary relationship
  - Why needed here: Understanding how tokenizer vocabulary affects embedding layer initialization and model transfer
  - Quick check question: How does BERT tokenizer vocabulary size impact model initialization when transferring between languages?

- Concept: Masked Language Modeling (MLM) objective
  - Why needed here: MLM training is the core mechanism for adapting transferred models to target language
  - Quick check question: What happens during MLM training when input tokens have poor initial embeddings?

- Concept: Cross-lingual transfer learning
  - Why needed here: The entire approach relies on transferring capabilities between related languages
  - Quick check question: What linguistic properties make languages suitable for cross-lingual transfer?

## Architecture Onboarding

- Component map: High-resource BERT model → Bilingual dictionary → Target language tokenizer → Matched BERT model → Fine-tuning
- Critical path: Vocabulary matching → Model initialization → Fine-tuning → Evaluation
- Design tradeoffs: Dictionary coverage vs. tokenization quality vs. model performance
- Failure signatures: Poor MLM accuracy, low retrieval performance, high fine-tuning requirements
- First 3 experiments:
  1. Test vocabulary matching on small sample of tokens to verify weight copying works correctly
  2. Compare MLM accuracy with and without vocabulary matching on validation set
  3. Test retrieval performance on small subset before full evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the vocabulary matching technique perform for languages