---
ver: rpa2
title: Large Malaysian Language Model Based on Mistral for Enhanced Local Language
  Understanding
arxiv_id: '2401.13565'
source_url: https://arxiv.org/abs/2401.13565
tags:
- yang
- untuk
- dalam
- dataset
- teks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a large-scale language model, Malaysian Mistral,
  developed by pretraining Mistral 7B on a 32.6 GB Malaysian dataset with 1.1 billion
  tokens. The model is fine-tuned with context lengths of 4096, 32768, and 16384 tokens,
  including an instruction-tuned version.
---

# Large Malaysian Language Model Based on Mistral for Enhanced Local Language Understanding

## Quick Facts
- **arXiv ID**: 2401.13565
- **Source URL**: https://arxiv.org/abs/2401.13565
- **Reference count**: 7
- **Primary result**: Malaysian Mistral, a large-scale language model for Malay, outperforms ChatGPT 3.5 and Claude 2 on Malay grammar tests.

## Executive Summary
This paper introduces Malaysian Mistral, a large language model pretrained on a 32.6 GB Malay dataset with 1.1 billion tokens using the Mistral 7B architecture. The model is fine-tuned with varying context lengths (4096, 32768, and 16384 tokens) and includes an instruction-tuned version. The authors claim superior performance on Malay grammar tests compared to major commercial models, addressing the gap in Malay-specific language models for natural language processing applications.

## Method Summary
The authors developed Malaysian Mistral by pretraining the Mistral 7B model on a substantial Malay language corpus. They employed three different context lengths (4096, 32768, and 16384 tokens) during fine-tuning and created an instruction-tuned variant. The training process involved standard pretraining followed by fine-tuning on the Malay dataset, though specific hyperparameters and optimization details are not provided in the paper.

## Key Results
- Malaysian Mistral demonstrates superior performance on Malay grammar test sets (Tatabahasa) compared to ChatGPT 3.5 and Claude 2
- The model achieves enhanced local language understanding for Malay through extensive pretraining on 32.6 GB of Malay text
- Fine-tuning with different context lengths (4096, 32768, 16384 tokens) enables versatile performance across various Malay language tasks

## Why This Works (Mechanism)
Malaysian Mistral works by leveraging extensive pretraining on domain-specific Malay language data, allowing the model to capture nuanced grammatical structures and linguistic patterns unique to the Malay language. The fine-tuning process with varying context lengths enables the model to handle both short and long-form Malay text effectively. By starting with the Mistral 7B architecture and adapting it specifically for Malay, the model benefits from proven transformer-based design while gaining specialized language understanding capabilities through targeted training on local language data.

## Foundational Learning
- **Transformer Architecture**: Why needed - Enables efficient processing of sequential language data; Quick check - Verify attention mechanism implementation and positional encoding
- **Pretraining on Domain-Specific Data**: Why needed - Builds language-specific understanding and reduces domain adaptation requirements; Quick check - Confirm data quality and diversity metrics
- **Fine-tuning with Context Length Variation**: Why needed - Allows model to handle different input sizes and task requirements; Quick check - Test performance across different context window sizes
- **Instruction Tuning**: Why needed - Improves model's ability to follow natural language instructions in target language; Quick check - Evaluate instruction-following capability on benchmark tasks
- **Evaluation on Language-Specific Benchmarks**: Why needed - Provides meaningful assessment of language model performance in target domain; Quick check - Validate test set relevance and coverage

## Architecture Onboarding

Component Map:
Mistral 7B Base Model -> Malay Dataset Pretraining -> Fine-tuning with Context Lengths (4096, 32768, 16384) -> Instruction Tuning -> Performance Evaluation

Critical Path:
The critical path involves pretraining on the Malay dataset, followed by fine-tuning with different context lengths to optimize performance, and finally instruction tuning to enhance task-specific capabilities. Each stage builds upon the previous one, with the pretraining providing foundational language understanding that is then refined through fine-tuning and specialized through instruction tuning.

Design Tradeoffs:
The authors chose to focus on Malay-specific pretraining rather than multilingual approaches, trading broader language coverage for deeper Malay language understanding. They opted for varying context lengths to balance computational efficiency with task flexibility, rather than committing to a single context window size. The decision to use Mistral 7B as a base model prioritizes proven architecture over potentially larger or more experimental designs.

Failure Signatures:
Performance degradation on out-of-domain Malay text, inability to handle code-switching or regional Malay dialects, overfitting to the specific grammar test set used for evaluation, and poor generalization to non-grammar related Malay language tasks.

First Experiments:
1. Test model performance on diverse Malay language tasks beyond grammar (sentiment analysis, NER, QA)
2. Evaluate model robustness on out-of-domain Malay text and regional dialect variations
3. Compare performance against other Malay language models using standardized benchmarks

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow evaluation scope focused primarily on Malay grammar tests without comprehensive benchmarking across diverse NLP tasks
- Lack of transparency regarding data quality, sources, and preprocessing methods for the 32.6 GB Malay corpus
- Insufficient technical details for reproducibility, including missing hyperparameters and optimization strategies

## Confidence
- **Performance claims**: Low - Based on single test set without statistical significance or broader validation
- **General approach**: Medium - Pretraining on domain-specific data is sound methodology
- **Reproducibility**: Low - Insufficient technical details provided for independent replication

## Next Checks
1. Conduct comprehensive evaluation on standardized Malay NLP benchmarks including SQuAD-style question answering, sentiment classification, and named entity recognition tasks
2. Perform ablation studies comparing performance across different context lengths and data preprocessing strategies to identify optimal configurations
3. Release the training corpus metadata and detailed hyperparameters to enable independent replication and community benchmarking against other Malay language models