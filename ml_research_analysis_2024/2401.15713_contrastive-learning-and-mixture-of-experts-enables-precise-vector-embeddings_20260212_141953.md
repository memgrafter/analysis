---
ver: rpa2
title: Contrastive Learning and Mixture of Experts Enables Precise Vector Embeddings
arxiv_id: '2401.15713'
source_url: https://arxiv.org/abs/2401.15713
tags:
- similarity
- training
- dataset
- experts
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of producing precise vector
  embeddings for scientific text, particularly in biomedical domains. The authors
  propose a novel approach combining contrastive learning and a Mixture of Experts
  (MoE) extension to pretrained BERT models.
---

# Contrastive Learning and Mixture of Experts Enables Precise Vector Embeddings

## Quick Facts
- arXiv ID: 2401.15713
- Source URL: https://arxiv.org/abs/2401.15713
- Reference count: 40
- This paper addresses the challenge of producing precise vector embeddings for scientific text, particularly in biomedical domains.

## Executive Summary
This paper addresses the challenge of producing precise vector embeddings for scientific text, particularly in biomedical domains. The authors propose a novel approach combining contrastive learning and a Mixture of Experts (MoE) extension to pretrained BERT models. They compile domain-specific datasets using co-citations as a similarity metric and apply their MoE framework to create specialized experts for each domain. Their results show that the MoE variants perform well across multiple scientific domains, achieving near-equivalent performance to individual models trained on each domain separately. Notably, extending just a single transformer block to MoE captures 85% of the benefit seen from full MoE extension at every layer. This suggests the potential for efficient, versatile "One-Size-Fits-All" transformer networks for representing diverse scientific inputs.

## Method Summary
The authors propose a novel approach combining contrastive learning and Mixture of Experts (MoE) extension to pretrained BERT models for creating precise vector embeddings in scientific domains. They compile domain-specific datasets using co-citations as a similarity metric, fine-tune BERT models with Multiple Negative Rankings (MNR) loss for contrastive learning, and apply MoE extension with special domain tokens for routing. The model architecture consists of shared transformer layers, one MoE-extended block, and a pooler output layer. They evaluate their approach using F1max, average distance between similar abstract vector representations, and accuracy metrics across multiple biomedical domains.

## Key Results
- MoE variants perform well across multiple scientific domains, achieving near-equivalent performance to individual models trained on each domain separately.
- Extending just a single transformer block to MoE captures 85% of the benefit seen from full MoE extension at every layer.
- The approach shows promise for creating efficient, versatile "One-Size-Fits-All" transformer networks for representing diverse scientific inputs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Co-citation networks enable efficient and semantically meaningful training data generation for domain-specific similarity tasks.
- Mechanism: Co-citation pairs are automatically generated from a small set of papers, producing many similar abstract pairs. This nonlinearity allows effective fine-tuning without manual labeling.
- Core assumption: Papers cited together in a third paper are likely semantically similar, making co-citations a reliable proxy for similarity.
- Evidence anchors:
  - [abstract] The authors state: "Co-citations represent instances where two papers are cited together in a third paper... While this measurement of similarity is not perfect, co-citations have generally been shown to imply a high degree of similarity between papers."
  - [section] "Co-citations represent instances where two papers are cited together in a third paper. This strategy enabled the production of large training datasets from small amounts of data due to their nonlinear nature."
  - [corpus] Weak. Corpus shows related papers on MoE but not on co-citation methodology. Need to confirm co-citation reliability across fields.
- Break condition: If co-cited papers are often only topically adjacent but not semantically similar, the training signal degrades.

### Mechanism 2
- Claim: Extending a single transformer block to MoE captures most of the benefit of full MoE extension, enabling efficient multitask learning.
- Mechanism: By adding multiple experts to just one transformer layer, the model routes domain-specific processing through a single expert per input, while the rest of the layers remain shared.
- Core assumption: Most of the domain discrimination benefit comes from one specialized layer rather than from every layer.
- Evidence anchors:
  - [abstract] "Notably, extending just a single transformer block to MoE captures 85% of the benefit seen from full MoE extension at every layer."
  - [section] "We also tried an MoE extension approach with a single transformer block in the middle instead of extending all 12 - hypothesizing that much of the multidomain benefit could be achieved for a small amount of extended MoE layers."
  - [corpus] Weak. No direct corpus evidence on single-block MoE efficiency; need to confirm scaling and routing stability.
- Break condition: If domain discrimination requires deeper expert routing, a single-layer MoE will underfit.

### Mechanism 3
- Claim: Fine-tuning BERT-like models with contrastive MNR loss on domain-specific co-citation data yields highly discriminative embeddings.
- Mechanism: Abstracts are passed through the model, embeddings are compared via dot product, and a temperature-scaled MNR loss drives similar pairs closer and dissimilar pairs apart.
- Core assumption: Abstracts contain enough domain-relevant context for the model to learn discriminative embeddings when trained with a contrastive objective.
- Evidence anchors:
  - [abstract] "Our methodology marks advancements in representation learning and holds promise for enhancing vector database search and compilation."
  - [section] "These vector embeddings were compared with a variant of the Multiple Negative Rankings (MNR) loss... MNR loss is a loss function that has seen significant success with sentence embedding problems."
  - [corpus] Weak. No direct corpus evidence on MNR loss performance with co-citation data; need to confirm stability across domains.
- Break condition: If abstracts are too short or noisy, contrastive learning may fail to converge.

## Foundational Learning

- Concept: Bidirectional attention and transformer block architecture
  - Why needed here: Understanding how BERT processes text bidirectionally is critical for extending to MoE and routing.
  - Quick check question: What is the key difference between BERT and GPT transformer blocks in terms of attention?

- Concept: Mixture of Experts (MoE) routing and expert specialization
  - Why needed here: MoE requires understanding how tokens are routed to different experts and how this enables multitask learning.
  - Quick check question: How does the router decide which expert(s) to activate for a given input?

- Concept: Contrastive learning and MNR loss mechanics
  - Why needed here: The training objective directly shapes the embedding space; understanding it is key to debugging performance.
  - Quick check question: In MNR loss, how are negative samples generated from positive pairs?

## Architecture Onboarding

- Component map:
  Input: Tokenized abstract text with special domain tokens
  Token embedding layer → Transformer blocks (shared) → One MoE-extended block → Pooler → Embedding vector
  Loss: Temperature-scaled MNR loss + (optional) domain classification loss
  Output: Cosine-similarity-based similarity score

- Critical path:
  1. Tokenize abstract with domain tokens
  2. Forward pass through shared transformer layers
  3. Expert routing at MoE block (top-2 experts per token)
  4. Pooler output → embedding vector
  5. Compute cosine similarity and loss
  6. Backpropagate and update parameters

- Design tradeoffs:
  - MoE at one layer vs. all layers: lower memory, faster training, near-equivalent performance
  - Domain tokens vs. learned routing: enforced routing simpler but less flexible
  - Single expert per token vs. top-k: higher capacity but more compute

- Failure signatures:
  - Low F1max with high accuracy: threshold miscalibration or trivial threshold
  - MoE variants worse than base: routing not effective or too few experts
  - GPU OOM: too many experts or too large model per expert
  - Slow convergence: learning rate too low or batch size too small

- First 3 experiments:
  1. Train a single-domain SciBERT with MNR loss on CVD data; verify F1max > 0.9
  2. Add domain tokens and MoE extension to one block; compare to single-domain baseline
  3. Train MoE with top-2 experts per token on all five domains; check cross-domain performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of experts per domain in a mixture-of-experts (MoE) model for scientific text embeddings?
- Basis in paper: [explicit] The authors mention "Future experiments may find the optimal ratio of experts per domain alongside the correct discrimination of 'domain' to create a one-size-fits-all vector embedding model."
- Why unresolved: The paper demonstrates that a single MoE layer can capture 85% of the benefit of full MoE extension, but does not explore the optimal number of experts per domain or the ideal distribution of experts across domains.
- What evidence would resolve it: Systematic experiments varying the number of experts per domain and their distribution across multiple scientific domains, comparing performance metrics like F1 score and computational efficiency.

### Open Question 2
- Question: How does the performance of MoE models compare to individual domain-specific models when dealing with interdisciplinary research papers?
- Basis in paper: [inferred] The paper focuses on single-domain models and their MoE counterparts, but does not explicitly address how these models handle papers that span multiple domains or interdisciplinary research.
- Why unresolved: Interdisciplinary research is common in scientific literature, and it's unclear whether MoE models can effectively represent papers that draw from multiple domains or if they would be outperformed by specialized models for each domain.
- What evidence would resolve it: Comparative studies of MoE models and individual domain-specific models on datasets containing interdisciplinary research papers, evaluating their ability to capture the nuances of multi-domain content.

### Open Question 3
- Question: What is the impact of extending different layers of the transformer with MoE on model performance and efficiency?
- Basis in paper: [explicit] The authors note that "extending just a single transformer block to MoE captures 85% of the benefit seen from full MoE extension at every layer," suggesting that layer selection may impact performance.
- Why unresolved: While the paper demonstrates the effectiveness of extending a single layer, it does not explore the performance differences between extending different layers or combinations of layers in the transformer architecture.
- What evidence would resolve it: Systematic experiments extending different combinations of layers with MoE, comparing performance metrics and computational costs to determine the optimal layer extension strategy for various tasks and model sizes.

## Limitations
- Co-citation reliability: The assumption that co-cited papers are semantically similar may not hold across diverse scientific fields, potentially introducing noise in the training data.
- Single-layer MoE efficiency: While the paper claims 85% of full MoE benefit comes from a single block, this is not benchmarked against other efficient architectures like adapter layers or LoRA.
- Generalizability beyond biomedicine: All datasets are biomedical, and the claim of "One-Size-Fits-All" transformer networks is extrapolated from five domains without testing on non-scientific or cross-domain tasks.

## Confidence

- **High confidence**: MoE extension with domain tokens improves cross-domain performance over single-domain models; one-layer MoE captures most of the benefit of full MoE.
- **Medium confidence**: Co-citation data is an effective proxy for semantic similarity; MNR loss is suitable for this task.
- **Low confidence**: The approach generalizes to non-biomedical domains; the "One-Size-Fits-All" claim is justified; single-layer MoE is the optimal efficiency-performance tradeoff.

## Next Checks

1. **Co-citation noise ablation**: Manually label a small subset of co-cited vs. non-co-cited abstract pairs for semantic similarity. Measure how often co-citations align with human judgment and quantify the impact on downstream embedding quality.

2. **MoE depth scaling study**: Systematically vary the number of MoE-extended layers (1, 2, 4, 8, 12) and measure F1max, training time, and memory use. Compare against adapter-based and LoRA approaches for multitask learning efficiency.

3. **Cross-domain generalization test**: Apply the trained MoE model to a non-biomedical scientific domain (e.g., physics or computer science) and measure embedding quality via retrieval benchmarks or human evaluation. Also test on a general-domain task (e.g., sentence similarity) to assess true multitask capability.