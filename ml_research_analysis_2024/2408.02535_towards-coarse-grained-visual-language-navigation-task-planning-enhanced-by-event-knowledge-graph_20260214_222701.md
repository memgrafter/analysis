---
ver: rpa2
title: Towards Coarse-grained Visual Language Navigation Task Planning Enhanced by
  Event Knowledge Graph
arxiv_id: '2408.02535'
source_url: https://arxiv.org/abs/2408.02535
tags:
- knowledge
- task
- event
- navigation
- subtask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of visual language navigation
  (VLN) tasks with coarse-grained instructions, which are more common in real-world
  scenarios but have been largely ignored in existing research. The authors propose
  a novel method called EventNav, which utilizes event knowledge enhancement to improve
  navigation planning.
---

# Towards Coarse-grained Visual Language Navigation Task Planning Enhanced by Event Knowledge Graph

## Quick Facts
- arXiv ID: 2408.02535
- Source URL: https://arxiv.org/abs/2408.02535
- Reference count: 40
- Outperforms existing models by over 5% in success rate on VLN tasks

## Executive Summary
This paper addresses the challenge of visual language navigation (VLN) tasks with coarse-grained instructions, which are more common in real-world scenarios but have been largely ignored in existing research. The authors propose EventNav, a novel method that utilizes event knowledge enhancement to improve navigation planning. By constructing a VLN-specific event knowledge graph (VLN-EventKG) and employing a large-small-model collaborative approach, EventNav demonstrates significant improvements in success rate across multiple VLN benchmark datasets.

## Method Summary
The EventNav framework constructs a VLN-specific event knowledge graph by extracting event-level knowledge from multiple mainstream VLN benchmark datasets. This knowledge graph is used to guide fine-grained subtask generation from coarse-grained instructions. The system employs a large-small-model collaborative approach, where a large language model (LLM) is used for subtask planning and a small transformer-based model for action planning. A dynamic history backtracking mechanism is designed to correct potential errors in real-time by monitoring subtask completion probabilities and triggering re-planning when necessary.

## Key Results
- Outperforms existing models by over 5% in success rate on R2R, REVERIE, and ALFRED datasets
- Demonstrates effectiveness in handling coarse-grained VLN tasks
- Shows improved navigation planning through event knowledge enhancement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Event knowledge graph improves coarse-grained VLN task planning by constraining LLM predictions to domain-relevant subtasks.
- Mechanism: The VLN-EventKG extracts fine-grained subtask sequences from multiple VLN datasets and embeds them into a vector database. During inference, LLM retrieves top-k similar subtasks based on the current subtask and uses them as contextual constraints for predicting the next subtask, reducing out-of-domain noise.
- Core assumption: VLN-specific event sequences in the knowledge graph capture task-relevant dependencies that general LLM commonsense knowledge lacks.
- Evidence anchors:
  - [abstract] "event knowledge enhancement to improve navigation planning" and "event knowledge graph (VLN-EventKG) by extracting event-level knowledge"
  - [section 4.2] "we construct a novel VLN-specific event knowledge graph" and "These subtasks serve as external knowledge to assist the task planning of the LLM"
  - [corpus] FMR=0.634 for "Why Only Text" paper suggesting strong thematic relevance to multi-modal VLN prompting
- Break condition: If the knowledge graph is incomplete or if retrieved subtasks are too dissimilar, LLM predictions may still drift outside valid VLN task sequences.

### Mechanism 2
- Claim: Large-small-model collaboration enables dynamic correction of navigation errors during task execution.
- Mechanism: LLM generates subtasks from coarse-grained instructions, while a smaller transformer-based model predicts actions and completion signals (S for binary completion, R for probability). When R drops below threshold or shows downward trend, dynamic backtracking triggers LLM to re-plan subtasks from the last valid state.
- Core assumption: Small model can reliably estimate subtask completion probability and trigger backtracking before error accumulation causes task failure.
- Evidence anchors:
  - [abstract] "dynamic history backtracking module to correct potential error action planning in real time"
  - [section 5.3] "the small model also predicts two additional signals (Output 2) S = {0, 1} and R ∈ [0, 1]" and "When the small model predicts a low probability R of the current subtask completion, the LLM will re-plan the current subtask"
  - [corpus] FMR=0.584 for "StratXplore" paper indicating relevance to exploration and instruction alignment in VLN
- Break condition: If the small model's R predictions are inaccurate or too delayed, backtracking may occur too late or trigger false positives, disrupting valid navigation paths.

### Mechanism 3
- Claim: Prompt-based framework for event knowledge extraction enables scalable VLN-specific knowledge graph construction.
- Mechanism: The system uses LLM to parse fine-grained subtask sequences from datasets where they are not explicitly provided (R2R, REVERIE), then embeds all subtasks using bge-large-en model into a vector database for similarity retrieval during planning.
- Core assumption: LLM can accurately extract and structure fine-grained subtask sequences from natural language instructions across different VLN datasets.
- Evidence anchors:
  - [section 4.2] "we use LLM to extract coarse-grained tasks and subtasks in them to build an event knowledge graph" and "By merging the knowledge in the three datasets, we obtained a total of 150k+ nodes and 120k+ relationships event knowledge graph"
  - [section 6.2] "For the event knowledge graph VLN-EventKG, we use the bge-large-en [51] semantic model to perform vector similarity retrieval"
  - [corpus] FMR=0.518 for "GROKE" paper suggesting relevance to navigation instruction evaluation, supporting the need for structured knowledge
- Break condition: If LLM extraction is noisy or inconsistent across datasets, the knowledge graph may contain incorrect or irrelevant subtask sequences that degrade planning quality.

## Foundational Learning

- Concept: Vector similarity retrieval for knowledge graph querying
  - Why needed here: Enables efficient lookup of relevant subtasks during planning without exhaustive graph traversal
  - Quick check question: What embedding model is used to represent subtasks in the VLN-EventKG, and why is this choice appropriate for semantic similarity search?

- Concept: Large-small-model collaborative architecture
  - Why needed here: Leverages LLM's planning capabilities while maintaining low-latency action prediction and error monitoring through smaller specialized models
  - Quick check question: What are the two additional signals (S and R) predicted by the small model, and how do they differ in priority during backtracking decisions?

- Concept: Dynamic backtracking mechanism with probability thresholds
  - Why needed here: Provides real-time correction capability to prevent error accumulation in sequential decision-making tasks
  - Quick check question: How are the hyperparameters x (probability threshold) and W (consecutive decrease count) determined for different VLN datasets?

## Architecture Onboarding

- Component map:
  - Event Knowledge Graph (VLN-EventKG): Vector database of fine-grained subtask sequences extracted from VLN datasets
  - Large Language Model (ChatGPT): Subtask planning from coarse-grained instructions with knowledge retrieval
  - Small Transformer Model: Action planning, completion probability estimation, and backtracking trigger
  - BLIP2: Image-to-text dense captioning for visual context integration
  - ResNet50/Mask R-CNN: Visual feature extraction and object mask generation for ALFRED dataset

- Critical path: Coarse-grained instruction → LLM subtask planning (with VLN-EventKG retrieval) → Small model action planning (with S,R prediction) → Environment execution → Loop until completion or backtracking trigger

- Design tradeoffs:
  - Knowledge graph size vs. retrieval accuracy: Larger graphs provide more diverse subtasks but increase retrieval complexity
  - LLM model size vs. planning quality: Larger models may generate better subtasks but increase latency and cost
  - Backtracking sensitivity vs. stability: Lower thresholds enable earlier correction but increase false positive backtracking

- Failure signatures:
  - Consistently low R values across multiple subtasks may indicate knowledge graph inadequacy or LLM-subtask mismatch
  - High backtracking frequency suggests either overly sensitive thresholds or fundamental planning difficulties
  - Poor action prediction accuracy despite good subtask planning may indicate small model training issues

- First 3 experiments:
  1. Baseline comparison: Implement base model without knowledge graph or backtracking, measure performance drop
  2. Knowledge graph ablation: Test with dataset-specific vs. merged knowledge graphs to validate cross-dataset knowledge transfer
  3. Backtracking parameter sweep: Systematically vary x and W parameters to find optimal configuration for each dataset

## Open Questions the Paper Calls Out
None

## Limitations
- The reliance on ChatGPT for subtask planning introduces potential variability and cost concerns that are not addressed in terms of reproducibility or practical deployment.
- The backtracking mechanism's hyperparameters (x and W) appear to be manually tuned per dataset, suggesting the approach may require significant adaptation for new environments.
- The paper does not provide detailed evaluation of knowledge graph quality or retrieval accuracy, making it difficult to assess whether improvements stem from the knowledge graph or other architectural components.

## Confidence
- High Confidence: The mechanism of using a knowledge graph to constrain LLM predictions to domain-relevant subtasks is well-established in the literature and the experimental results consistently show improvements across all three datasets (R2R, REVERIE, ALFRED).
- Medium Confidence: The effectiveness of the large-small-model collaborative architecture and dynamic backtracking mechanism is supported by ablation studies, but the specific design choices (threshold values, model architectures) may be dataset-specific and not generalizable.
- Low Confidence: The scalability and robustness of the VLN-EventKG construction method using LLM for subtask extraction across diverse VLN datasets has not been extensively validated. The quality of extracted subtasks directly impacts the knowledge graph's utility.

## Next Checks
1. Conduct a comprehensive ablation study isolating the contribution of each component (knowledge graph, backtracking, collaborative architecture) to verify the claimed ~5% improvement is not from confounding factors.
2. Test the model's performance when trained on a subset of datasets and evaluated on held-out datasets to assess cross-dataset knowledge transfer capabilities of the VLN-EventKG.
3. Evaluate the sensitivity of the backtracking mechanism by systematically varying the probability threshold (x) and consecutive decrease count (W) parameters to determine if the chosen values are optimal or dataset-specific artifacts.