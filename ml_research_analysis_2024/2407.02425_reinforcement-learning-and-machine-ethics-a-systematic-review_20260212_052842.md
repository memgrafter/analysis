---
ver: rpa2
title: Reinforcement Learning and Machine ethics:a systematic review
arxiv_id: '2407.02425'
source_url: https://arxiv.org/abs/2407.02425
tags:
- ethics
- ethical
- learning
- machine
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic review analyzes 71 papers on machine ethics using
  reinforcement learning (RL), addressing a gap in prior literature. The review examines
  RL frameworks (e.g., multi-objective, inverse, constrained RL), ethical theories
  (consequentialist, deontological, virtue ethics), and implementation examples (e.g.,
  resource sharing, forbidden states).
---

# Reinforcement Learning and Machine ethics:a systematic review

## Quick Facts
- **arXiv ID:** 2407.02425
- **Source URL:** https://arxiv.org/abs/2407.02425
- **Reference count:** 40
- **Key outcome:** This systematic review analyzes 71 papers on machine ethics using reinforcement learning (RL), addressing a gap in prior literature. The review examines RL frameworks (e.g., multi-objective, inverse, constrained RL), ethical theories (consequentialist, deontological, virtue ethics), and implementation examples (e.g., resource sharing, forbidden states). Most papers modify reward or policy functions and use consequentialist or deontological ethics. Human factors, such as developers or experts, often decide ethics, raising concerns about bias. The review highlights a recent increase in RL-based machine ethics research since 2020, with empirical evaluations dominating. Challenges include the lack of standardized benchmarks and the need for clearer connections to moral philosophy. Future work should focus on organizing the field, avoiding anecdotal ethics, and addressing ethical decision-making accountability.

## Executive Summary
This systematic review analyzes 71 papers on machine ethics using reinforcement learning (RL), addressing a gap in prior literature. The review examines RL frameworks (e.g., multi-objective, inverse, constrained RL), ethical theories (consequentialist, deontological, virtue ethics), and implementation examples (e.g., resource sharing, forbidden states). Most papers modify reward or policy functions and use consequentialist or deontological ethics. Human factors, such as developers or experts, often decide ethics, raising concerns about bias. The review highlights a recent increase in RL-based machine ethics research since 2020, with empirical evaluations dominating. Challenges include the lack of standardized benchmarks and the need for clearer connections to moral philosophy. Future work should focus on organizing the field, avoiding anecdotal ethics, and addressing ethical decision-making accountability.

## Method Summary
The review employs a systematic methodology to analyze 71 papers on machine ethics using RL. The selection criteria and search process are not explicitly detailed, but the review covers a wide range of RL frameworks, ethical theories, and implementation examples. The analysis focuses on categorizing papers based on their RL approaches, ethical frameworks, and evaluation methods, with an emphasis on identifying trends and gaps in the literature.

## Key Results
- RL-based machine ethics research has increased significantly since 2020, with 64% of reviewed papers published in this period.
- Most papers modify reward or policy functions and use consequentialist or deontological ethics.
- Human factors, such as developers or experts, often decide ethics, raising concerns about bias.
- Challenges include the lack of standardized benchmarks and the need for clearer connections to moral philosophy.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RL-based machine ethics research has increased significantly since 2020 because RL naturally frames ethical decision-making as an optimization problem over rewards.
- **Mechanism:** RL agents learn policies that maximize cumulative reward. By embedding ethical objectives (e.g., fairness, safety) as additional reward signals or constraints, researchers can train agents to behave ethically without hand-coding explicit rules. This aligns well with consequentialist and multi-objective approaches prevalent in the surveyed work.
- **Core assumption:** Ethical behavior can be sufficiently captured as a numerical reward or set of constraints that an RL agent can optimize.
- **Evidence anchors:**
  - [abstract] "There is a gap in the understanding of the state of the art in machine ethics if that understanding does not include the most recent work on RL."
  - [section] "The goal, typically of the learning process, is to find a policy on how to make decisions so that an optimal reward is attained."
  - [corpus] Weak: Corpus neighbors focus on ethics reviews, not RL-specific mechanisms.
- **Break condition:** If ethical decisions cannot be reduced to quantifiable rewards or constraints, RL optimization will fail to capture nuanced moral reasoning.

### Mechanism 2
- **Claim:** Inverse RL (IRL) and cooperative IRL enable learning ethical behavior by observing expert demonstrations rather than explicit reward specification.
- **Mechanism:** IRL infers the reward function from expert policy demonstrations. In machine ethics, human experts can demonstrate ethical behavior, and the RL agent learns to replicate it. Cooperative IRL extends this to multi-agent scenarios where ethical norms are learned collaboratively.
- **Core assumption:** Ethical behavior is observable and reproducible through expert demonstrations that can be generalized to new situations.
- **Evidence anchors:**
  - [section] "In inverse RL (IRL) [24], the agent learns the reward functions in an environment, given the policy."
  - [section] "Variations of IRL include co-operative IRL, and adversarial IRL, which use human experts as co-operative and adversarial means, respectively, rather than an expert teacher, to embed ethical behaviour in an artificial agent."
  - [corpus] Weak: No corpus evidence of IRL in ethical contexts.
- **Break condition:** If expert demonstrations are inconsistent, biased, or insufficient to cover ethical edge cases, the learned reward function will be incomplete or incorrect.

### Mechanism 3
- **Claim:** Multi-objective RL (MORL) naturally supports balancing ethical and performance objectives by treating them as separate reward components.
- **Mechanism:** MORL agents optimize multiple reward functions simultaneously. Ethical objectives (e.g., equity, safety) can be encoded as additional reward components alongside performance goals. The agent learns policies that balance these competing objectives.
- **Core assumption:** Ethical objectives can be decomposed into quantifiable reward components that can be optimized alongside primary task objectives.
- **Evidence anchors:**
  - [section] "The advantage of using MORL is that ethics specifications can be codified as part of the reward function, in addition to the main objective."
  - [section] "We can observe from these groupings that the majority of the articles modify either the reward function or the policy function."
  - [corpus] Weak: No corpus evidence of MORL in ethical contexts.
- **Break condition:** If ethical objectives conflict irreconcilably with performance objectives, or if trade-offs cannot be meaningfully quantified, MORL will produce suboptimal or ethically problematic policies.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs)**
  - Why needed here: RL algorithms are typically formalized as MDPs where agents transition between states via actions, receiving rewards. Understanding MDPs is essential to grasp how RL agents learn ethical behavior through state-action-reward sequences.
  - Quick check question: In an MDP, what determines the next state and reward when an agent takes action a in state s?
- **Concept: Value Function Approximation**
  - Why needed here: RL agents estimate the value of states or state-action pairs. In ethical RL, value functions must incorporate ethical considerations, requiring understanding of how value estimation works.
  - Quick check question: What is the difference between a state-value function V(s) and an action-value function Q(s,a)?
- **Concept: Exploration vs. Exploitation Trade-off**
  - Why needed here: Ethical RL agents must balance trying new actions (exploration) with using known ethical behavior (exploitation). Understanding this trade-off is crucial for designing agents that can discover and maintain ethical policies.
  - Quick check question: What happens if an RL agent over-exploits its current policy versus over-explores new actions?

## Architecture Onboarding

- **Component map:** Environment -> State Observation -> Policy Selection -> Action -> Reward Feedback -> Value/Policy Update -> Next State
- **Critical path:** Environment → State Observation → Policy Selection → Action → Reward Feedback → Value/Policy Update → Next State
- **Design tradeoffs:**
  - Reward specification vs. constraint specification: Rewards provide positive guidance but can be gamed; constraints prevent violations but may be overly restrictive
  - Exploration vs. exploitation: More exploration may discover better ethical policies but risks violating ethical constraints during learning
  - Model-based vs. model-free: Model-based can incorporate ethical constraints more explicitly but requires accurate environment models
- **Failure signatures:**
  - Reward hacking: Agent finds loopholes to maximize reward without achieving intended ethical behavior
  - Catastrophic forgetting: Agent loses previously learned ethical behavior when adapting to new situations
  - Constraint violations: Agent violates safety/ethical constraints during training or deployment
- **First 3 experiments:**
  1. Implement a simple Grid World where the agent must navigate while avoiding forbidden areas (constrained RL approach)
  2. Create a multi-agent resource sharing scenario where agents must balance individual and collective rewards (MORL approach)
  3. Design an inverse RL setup where an expert demonstrates ethical behavior and the agent learns to replicate it (IRL approach)

## Open Questions the Paper Calls Out
None

## Limitations
- The review's classification of ethical theories may conflate theoretical frameworks with practical implementations, as the paper acknowledges that some categorizations are based on authors' claims rather than rigorous philosophical analysis.
- The corpus of 71 papers, while systematic, may still miss relevant work published in venues outside the search scope.
- Most implementations use modified reward functions or constraints, which may not fully capture the complexity of ethical decision-making in real-world scenarios.

## Confidence
- **High confidence:** The systematic methodology and comprehensive coverage of RL approaches in machine ethics
- **Medium confidence:** The classification of ethical theories and their practical implementations, given potential oversimplification
- **Low confidence:** Claims about the effectiveness of different RL approaches in producing genuinely ethical behavior, as the review primarily catalogs approaches rather than evaluating outcomes

## Next Checks
1. Conduct a follow-up review specifically examining the empirical outcomes of RL-based ethical systems to assess whether theoretical approaches translate to practical ethical behavior.
2. Perform a deeper philosophical analysis of how well consequentialist, deontological, and virtue ethics frameworks map to RL reward and constraint structures.
3. Develop and validate benchmark scenarios for testing RL-based ethical decision-making across different theoretical frameworks to enable comparative evaluation.