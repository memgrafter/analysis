---
ver: rpa2
title: 'SilverSight: A Multi-Task Chinese Financial Large Language Model Based on
  Adaptive Semantic Space Learning'
arxiv_id: '2404.04949'
source_url: https://arxiv.org/abs/2404.04949
tags:
- data
- financial
- semantic
- lora
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an Adaptive Semantic Space Learning (ASSL)
  framework for training a multi-task Chinese financial large language model called
  "SilverSight." The framework addresses the challenge of conflicting tasks during
  model transfer in specialized domains by utilizing semantic space clustering and
  adaptive data redistribution. The ASSL framework clusters data based on semantic
  similarities, then applies a two-stage adaptive data filtering process to optimize
  the selection of training data for each LoRA expert.
---

# SilverSight: A Multi-Task Chinese Financial Large Language Model Based on Adaptive Semantic Space Learning

## Quick Facts
- arXiv ID: 2404.04949
- Source URL: https://arxiv.org/abs/2404.04949
- Reference count: 27
- Key outcome: SilverSight achieves results comparable to full-data training using only 10% of the data while demonstrating strong generalization on Chinese financial benchmarks.

## Executive Summary
This paper introduces SilverSight, a multi-task Chinese financial large language model built on an Adaptive Semantic Space Learning (ASSL) framework. The framework addresses task conflicts in specialized domains by clustering data based on semantic similarities and applying adaptive data redistribution. Through semantic space clustering and a two-stage adaptive filtering process, the model optimizes training data selection for each LoRA expert. The resulting model demonstrates outstanding performance on Chinese financial evaluation benchmarks (CFLEB and FinEval), validating the effectiveness of ASSL for financial multitask learning.

## Method Summary
SilverSight employs the ASSL framework to address task conflicts during model transfer in specialized domains. The method clusters data based on semantic similarities using sentence embeddings, then applies a two-stage adaptive data filtering process (A-DBSCAN followed by modified MMR) to optimize training data selection for each LoRA expert. The framework trains multiple LoRA experts on semantically smoothed data categories and selects the most suitable expert for input based on semantic similarity calculations. The model is trained on 220,000 Chinese financial data points from 23 sources covering 7 task types.

## Key Results
- Achieves results comparable to full-data training using only 10% of the data
- Demonstrates strong generalization capabilities across Chinese financial tasks
- Outperforms manual categorization methods on CFLEB benchmark, especially on FinQA and FinNSP1 tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic space clustering reduces task conflicts and improves model performance.
- Mechanism: By clustering data based on semantic similarities, the model groups complementary tasks together and separates conflicting tasks, allowing each LoRA expert to focus on its area of expertise.
- Core assumption: Tasks with similar semantic embeddings are more likely to be complementary, while dissimilar tasks are more likely to conflict.
- Evidence anchors: [abstract] "Our research findings demonstrate that our framework can achieve results close to those obtained with full data training using only 10% of the data"; [section 4.5] "The predefined manual categorization method performed worse on average on the CFLEB dataset compared to the semantic space clustering method"

### Mechanism 2
- Claim: Adaptive data redistribution balances long-tail data distributions and improves model generalization.
- Mechanism: The two-stage adaptive data filtering process first downsamples high-density data and upsamples low-density data using A-DBSCAN, then supplements necessary data points based on model feedback, ensuring a smoother data distribution.
- Core assumption: Long-tail data distributions lead to model overfitting on common tasks and underfitting on rare tasks, and adaptive redistribution can mitigate this issue.
- Evidence anchors: [abstract] "Our method can aggregate diverse data from complementary tasks, improving model performance on related tasks"; [section 4.5] "The robustness of the A-DBSCAN algorithm on both datasets confirms the superiority of this method"

### Mechanism 3
- Claim: Adaptive expert selection improves model performance by matching input data with the most suitable LoRA expert.
- Mechanism: By using the centroid of each cluster as the embedding for LoRA experts and calculating the similarity between input embeddings and expert embeddings, the system can select the most relevant expert for each input.
- Core assumption: The centroid of a cluster accurately represents the semantic space of the tasks within that cluster, and the similarity between embeddings is a reliable indicator of task compatibility.
- Evidence anchors: [abstract] "When addressing specific financial issues, the most suitable LoRA expert is automatically selected by calculating the similarity between the problem's representation in the same semantic space and the representations of the six LoRA experts"; [section 4.4] "The experimental results on these two evaluation datasets showed that the performance of the LoRA adaptive selection algorithm on each task was close to that of the single LoRA expert that performed best on that task"

## Foundational Learning

- Concept: Semantic space and embeddings
  - Why needed here: Understanding semantic space and embeddings is crucial for grasping how the ASSL framework clusters data and selects experts.
  - Quick check question: How do sentence encoders like Sentence-BERT map sentences to high-dimensional semantic embeddings?

- Concept: Long-tail data distributions and their impact on model performance
  - Why needed here: Recognizing the challenges posed by long-tail data distributions is essential for understanding the need for adaptive data redistribution.
  - Quick check question: How do long-tail data distributions affect a model's ability to learn rare but important tasks?

- Concept: Low-rank adaptation (LoRA) and its application in efficient fine-tuning
  - Why needed here: LoRA is a key component of the ASSL framework, and understanding its mechanism is necessary for grasping how the system adapts to different tasks.
  - Quick check question: How does LoRA reduce the number of parameters that need to be fine-tuned compared to full fine-tuning?

## Architecture Onboarding

- Component map: Data preprocessing → Clustering → Adaptive redistribution → LoRA training → Expert selection → Inference
- Critical path: Data preprocessing → Clustering → Adaptive redistribution → LoRA training → Expert selection → Inference
- Design tradeoffs:
  - Clustering granularity vs. computational efficiency
  - Data redistribution aggressiveness vs. potential loss of important information
  - Expert selection speed vs. accuracy
- Failure signatures:
  - Poor clustering results in task conflicts and suboptimal expert selection
  - Over-aggressive data redistribution leads to loss of important information
  - Inaccurate similarity calculations result in suboptimal expert selection
- First 3 experiments:
  1. Compare model performance with and without semantic space clustering
  2. Evaluate the impact of adaptive data redistribution on long-tail task performance
  3. Test the effectiveness of adaptive expert selection compared to random or fixed expert assignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ASSL framework handle cases where the semantic space clustering creates too many small clusters, potentially leading to overfitting on limited data?
- Basis in paper: [explicit] The paper mentions that the A-DBSCAN algorithm filters out noise points in low-density areas, but doesn't discuss the impact of having many small clusters on model performance.
- Why unresolved: The paper doesn't provide data on the distribution of cluster sizes after the first stage of data redistribution, nor does it discuss how the model handles very small clusters.
- What evidence would resolve it: Experimental results showing the distribution of cluster sizes and their impact on model performance, or a discussion of strategies to merge small clusters when necessary.

### Open Question 2
- Question: How sensitive is the ASSL framework to the choice of hyperparameters like k in A-DBSCAN or the weights λ1, λ2, λ3 in the modified MMR formula?
- Basis in paper: [explicit] The paper mentions that after multiple trials, specific values were chosen for K=6 in K-means, k=20 in A-DBSCAN, and λ1=0.2, λ2=0.2, λ3=0.6, but doesn't discuss the sensitivity to these choices.
- Why unresolved: The paper doesn't provide an ablation study or sensitivity analysis for these hyperparameters.
- What evidence would resolve it: Results from a systematic hyperparameter search or an analysis of how performance varies with different hyperparameter settings.

### Open Question 3
- Question: How does the ASSL framework perform on non-Chinese financial datasets, and are there any domain-specific adaptations required for other languages or financial contexts?
- Basis in paper: [explicit] The paper focuses on Chinese financial data and evaluates on Chinese benchmarks, but doesn't discuss generalizability to other languages or financial domains.
- Why unresolved: The paper doesn't provide any experiments or discussion on applying the framework to non-Chinese data or other financial contexts.
- What evidence would resolve it: Experiments applying the ASSL framework to financial datasets in other languages or different financial domains (e.g., healthcare finance, personal finance) with performance comparisons.

## Limitations

- The framework's effectiveness depends on the quality of semantic embeddings and their ability to capture task relationships
- The two-stage filtering process introduces complexity that may impact reproducibility and scalability
- Evaluation is limited to Chinese financial benchmarks, raising questions about cross-domain generalization

## Confidence

*High Confidence:* The core methodology of using semantic space clustering to group complementary tasks and the overall two-stage adaptive filtering approach are well-founded and supported by experimental results.

*Medium Confidence:* The claim that the framework achieves results comparable to full-data training using only 10% of the data needs careful consideration, as evaluation focuses on specific Chinese financial benchmarks.

*Low Confidence:* The effectiveness of the adaptive expert selection mechanism is less certain, as the underlying similarity metrics and their reliability across different types of financial queries need further validation.

## Next Checks

1. **Cross-domain generalization test**: Evaluate the ASSL framework on non-financial multi-task datasets to verify if the semantic clustering approach generalizes beyond the financial domain.

2. **Edge case preservation analysis**: Conduct a detailed analysis of what data points are filtered out during the adaptive redistribution process to ensure critical edge cases are not lost.

3. **Expert selection reliability**: Test the adaptive expert selection mechanism across a wider range of query types and difficulty levels to validate the robustness of the similarity-based selection approach.