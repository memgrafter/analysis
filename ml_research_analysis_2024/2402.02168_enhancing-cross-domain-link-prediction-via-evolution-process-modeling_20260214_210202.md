---
ver: rpa2
title: Enhancing Cross-domain Link Prediction via Evolution Process Modeling
arxiv_id: '2402.02168'
source_url: https://arxiv.org/abs/2402.02168
tags:
- link
- prediction
- graph
- dyexpert
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DyExpert, a foundation model for cross-domain
  link prediction on dynamic graphs. It models graph evolution processes via conditioned
  link generation and uses a decode-only transformer to perform evolution-specific
  predictions.
---

# Enhancing Cross-domain Link Prediction via Evolution Process Modeling

## Quick Facts
- arXiv ID: 2402.02168
- Source URL: https://arxiv.org/abs/2402.02168
- Reference count: 25
- Key outcome: 11.40% average precision improvement over advanced baselines on 8 untrained graphs, with 6M training edges across 6 domains

## Executive Summary
This paper introduces DyExpert, a foundation model for cross-domain link prediction on dynamic graphs. The model addresses the challenge of predicting future links in dynamic graphs by learning from multiple domains simultaneously. DyExpert employs a novel approach of modeling graph evolution processes through conditioned link generation and uses a decode-only transformer architecture to make predictions specific to different graph evolution patterns.

## Method Summary
DyExpert is designed as a foundation model that learns to predict links in dynamic graphs across multiple domains. The core innovation lies in its ability to model graph evolution processes by generating conditioned links, which helps capture temporal patterns and structural changes. The model uses a decode-only transformer architecture that focuses on prediction tasks without requiring extensive encoding of graph structures. This approach allows DyExpert to generalize across different domains and make predictions on graphs it hasn't been explicitly trained on, leveraging knowledge from 6 million training edges across 6 different domains.

## Key Results
- Achieves 11.40% average precision improvement over advanced baselines on 8 untrained graphs
- Outperforms fully supervised baselines on 6 datasets
- Trained on 6 million dynamic edges across 6 domains

## Why This Works (Mechanism)
DyExpert's success stems from its ability to capture and model the evolution patterns of dynamic graphs across multiple domains simultaneously. By treating link prediction as a generation task conditioned on graph evolution, the model can learn universal patterns that apply across different graph types and domains. The decode-only transformer architecture allows for efficient prediction without the computational overhead of traditional graph neural networks, while the foundation model approach enables transfer learning capabilities across domains.

## Foundational Learning
- **Dynamic Graph Theory**: Understanding how graphs change over time is crucial for modeling evolution processes - quick check: verify temporal patterns in training data
- **Foundation Models**: Pre-training on multiple domains enables cross-domain generalization - quick check: test on unseen domain types
- **Conditioned Generation**: Using graph evolution as conditioning improves prediction accuracy - quick check: compare with unconditioned generation
- **Transformer Architecture**: Decode-only transformers reduce computational complexity while maintaining prediction quality - quick check: measure inference time vs accuracy tradeoff

## Architecture Onboarding

**Component Map**: Graph Evolution Process -> Conditioned Link Generation -> Decode-only Transformer -> Link Prediction

**Critical Path**: The model's critical path involves capturing graph evolution patterns through conditioned link generation, which then feeds into the decode-only transformer for making predictions. This sequence ensures that temporal information is properly encoded and utilized for future link prediction.

**Design Tradeoffs**: The choice of decode-only transformer over traditional graph neural networks reduces computational complexity but may sacrifice some structural information encoding. The foundation model approach enables cross-domain learning but requires substantial training data across multiple domains.

**Failure Signatures**: The model may struggle with graphs that have evolution patterns significantly different from the training domains, or with domains that have very few or very sparse edges. Temporal discontinuities or sudden structural changes in graphs may also pose challenges.

**3 First Experiments**:
1. Test DyExpert on a simple synthetic dynamic graph with known evolution patterns
2. Evaluate performance on a single domain before extending to cross-domain predictions
3. Compare conditioned vs unconditioned link generation approaches on a validation set

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Limited evaluation metrics, focusing primarily on precision without comprehensive reporting of recall, F1-score, or AUC
- Potential data efficiency concerns due to reliance on 6 million training edges across 6 domains
- Lack of computational complexity and inference time analysis for practical deployment considerations

## Confidence
- High confidence: The 11.40% average precision improvement over baselines is well-supported by the reported results
- Medium confidence: The claim of surpassing fully supervised baselines, as the specific conditions and datasets are not fully detailed
- Medium confidence: The model's generalization capabilities across domains, as the evaluation scope appears limited

## Next Checks
1. Evaluate the model on datasets with varying edge densities and temporal patterns to test robustness across different dynamic graph characteristics
2. Conduct ablation studies to quantify the contribution of the conditioned link generation and decode-only transformer components
3. Test the model's performance when trained on significantly smaller datasets to assess data efficiency and practical applicability