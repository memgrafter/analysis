---
ver: rpa2
title: 'Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion
  Models'
arxiv_id: '2411.07232'
source_url: https://arxiv.org/abs/2411.07232
tags:
- image
- source
- object
- images
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Add-it, a training-free approach for inserting
  objects into images based on text instructions. The method extends diffusion models'
  attention mechanisms to balance information from the scene image, text prompt, and
  generated image, ensuring natural object placement while preserving fine details.
---

# Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models

## Quick Facts
- arXiv ID: 2411.07232
- Source URL: https://arxiv.org/abs/2411.07232
- Authors: Yoad Tewel; Rinon Gal; Dvir Samuel; Yuval Atzmon; Lior Wolf; Gal Chechik
- Reference count: 18
- Primary result: Training-free object insertion with state-of-the-art results on real and generated image benchmarks

## Executive Summary
Add-it introduces a training-free approach for inserting objects into images based on text instructions. The method extends diffusion models' attention mechanisms to balance information from the scene image, text prompt, and generated image, ensuring natural object placement while preserving fine details. By avoiding task-specific fine-tuning, Add-it achieves state-of-the-art results on both real and generated image insertion benchmarks, significantly improving affordance scores from 47% to 83% and preferred in over 80% of human evaluations.

## Method Summary
Add-it builds upon pretrained diffusion models (specifically FLUX.1-dev) and introduces three key innovations: a weighted extended-attention mechanism that balances source image, text prompt, and target image information; a structure transfer step that aligns the generated image's structure with the source; and subject-guided latent blending that preserves fine details from the source image. The method operates without any task-specific fine-tuning, making it training-free while achieving superior performance on object insertion tasks compared to supervised approaches.

## Key Results
- State-of-the-art performance on both real and generated image insertion benchmarks
- Improves affordance scores from 47% to 83% on generated images
- Preferred in over 80% of human evaluations
- Achieves higher CLIPdir scores (1.37 vs 0.91) on generated image benchmarks
- Maintains fine details while ensuring natural object placement

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Add-it's weighted extended-attention mechanism balances information from source image, text prompt, and target image to achieve natural object placement.
- **Mechanism:** The attention mechanism extends standard MM-DiT blocks by allowing the target image to attend to tokens from both the source image and the prompt. Weighting terms (γs, γp, γt) control the contribution of each attention component.
- **Core assumption:** Balancing attention distribution across the three sources prevents the model from either neglecting the edit (copying source) or ignoring the source image structure (straying from context).
- **Evidence anchors:**
  - [abstract] "Our weighted extended-attention mechanism maintains structural consistency and fine details while ensuring natural object placement."
  - [section 3.2] "we can introduce a weighting term to each source of information" and "by reducing the weight of the source image tokens, we can achieve better balance"
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism.
- **Break condition:** If weight scaling is too aggressive (γ too high), the target image neglects source structure, placing objects unnaturally. If too conservative (γ too low), the source image dominates, preventing object addition.

### Mechanism 2
- **Claim:** Structure transfer preserves source image context by aligning generation seeds with source structure.
- **Mechanism:** The source image is noised to a high noise level (tstruct) and used as initialization for the target image generation, ensuring structural similarity while allowing content changes.
- **Core assumption:** High noise levels preserve global structure while enabling local modifications, creating seeds with similar structure to the source image.
- **Evidence anchors:**
  - [section 3.3] "we propose to 'choose' seeds with a structural similarity to the source image" and "starting the denoising process from Xtstruct will result in an image with similar global structure"
  - [section 5 analysis] "When the structure transfer is applied too early, the affordance score is low, meaning the target image does not adhere to the structure of the source image"
  - [corpus] Weak - no direct corpus evidence found for this specific structure transfer mechanism.
- **Break condition:** If applied too early, structural alignment fails. If applied too late, object neglect occurs.

### Mechanism 3
- **Claim:** Subject-guided latent blending preserves fine details from the source image that aren't affected by the added object.
- **Mechanism:** A rough object mask is generated from attention maps, refined using SAM-2, and used to blend source and target latents, preserving unaffected background details while allowing necessary changes like shadows.
- **Core assumption:** Perfect masks aren't necessary; a rough mask refined by a segmentation model can effectively separate object from background while preserving contextual effects.
- **Evidence anchors:**
  - [section 3.4] "we propose generating a rough mask of the object, which is then refined using SAM-2" and "we apply a simple latent blending step at timestep Tblend"
  - [section 5 analysis] "The blending step aligns the fine details of the source image without introducing artifacts"
  - [corpus] Weak - no direct corpus evidence found for this specific blending mechanism.
- **Break condition:** If mask refinement fails, blending introduces artifacts or fails to preserve necessary details.

## Foundational Learning

- **Concept:** Multi-modal attention in diffusion transformers
  - **Why needed here:** Add-it builds upon and extends the MM-DiT attention mechanism used in modern text-to-image models like FLUX
  - **Quick check question:** How does standard MM-DiT attention combine text and image tokens, and what limitation does Add-it address?

- **Concept:** Diffusion model denoising process
  - **Why needed here:** Add-it operates within the denoising steps of a pretrained diffusion model, requiring understanding of how noise levels affect generation
  - **Quick check question:** What happens at different noise levels during diffusion denoising, and how does structure transfer leverage this?

- **Concept:** Attention distribution and weight scaling
  - **Why needed here:** The weighted attention mechanism relies on controlling how attention is distributed across different token sources
  - **Quick check question:** How does softmax attention work, and why does scaling keys affect the attention distribution?

## Architecture Onboarding

- **Component map:** Source image denoising -> Weighted extended attention -> Structure transfer -> Subject-guided blending -> Final output
- **Critical path:** Source image denoising → Weighted extended attention → Structure transfer → Subject-guided blending → Final output
- **Design tradeoffs:** 
  - Weight scaling vs. object inclusion vs. affordance (as shown in fig. 7A)
  - Mask precision vs. blending effectiveness vs. computational cost
  - Structure transfer timing vs. structural alignment vs. object preservation
- **Failure signatures:**
  - Source image dominance: Output copies source without adding object
  - Prompt dominance: Object added but placement unnatural, ignoring source context
  - Mask failure: Blending introduces artifacts or fails to preserve details
  - Structure misalignment: Object placed in implausible locations despite correct content
- **First 3 experiments:**
  1. Test weight scaling (γ) on a simple insertion task to find balance point between object inclusion and affordance
  2. Compare structure transfer timing (different tstruct values) to optimize structural alignment
  3. Validate subject-guided blending by comparing outputs with and without the blending step on complex scenes

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the choice of the structure transfer step (tstruct) impact the final image quality and object placement in Add-it?
- **Basis in paper:** [explicit] The paper mentions using tstruct = 933 for generated images and tstruct = 867 for real images, but does not provide a detailed analysis of how different tstruct values affect the results.
- **Why unresolved:** The paper only briefly mentions the tstruct values used and does not explore the effects of varying these values on the final output.
- **What evidence would resolve it:** Conducting experiments with different tstruct values and analyzing the impact on image quality, object placement, and the balance between object inclusion and affordance would provide insights into the optimal choice of tstruct.

### Open Question 2
- **Question:** How does the weighted extended self-attention mechanism compare to other attention mechanisms in terms of preserving the source image structure and aligning with the text prompt?
- **Basis in paper:** [inferred] The paper introduces the weighted extended self-attention mechanism and claims it helps balance information from the source image, text prompt, and target image. However, it does not compare this mechanism to other attention mechanisms in detail.
- **Why unresolved:** The paper does not provide a comprehensive comparison of the weighted extended self-attention mechanism with other attention mechanisms, such as standard self-attention or cross-attention.
- **What evidence would resolve it:** Conducting experiments comparing the weighted extended self-attention mechanism with other attention mechanisms in terms of image quality, object placement, and adherence to the text prompt would provide insights into its effectiveness.

### Open Question 3
- **Question:** How does the choice of the latent blending step (tblend) impact the final image quality and the preservation of fine details in Add-it?
- **Basis in paper:** [explicit] The paper mentions using tblend = 500 but does not provide a detailed analysis of how different tblend values affect the results.
- **Why unresolved:** The paper only briefly mentions the tblend value used and does not explore the effects of varying this value on the final output.
- **What evidence would resolve it:** Conducting experiments with different tblend values and analyzing the impact on image quality, the preservation of fine details, and the alignment with the source image would provide insights into the optimal choice of tblend.

## Limitations

- Reliance on pretrained diffusion models without task-specific fine-tuning limits performance on complex insertion scenarios requiring fine-grained physical understanding
- Weighted attention mechanism depends on heuristic weight scaling that may not generalize across diverse scene types
- Subject-guided blending assumes reasonable object-background separation, which may fail for objects with complex boundaries or those integrated into the scene

## Confidence

- **High Confidence:** The core mechanism of weighted extended attention balancing source, prompt, and target information is well-supported by the experimental results, particularly the ablation studies showing performance degradation when components are removed.
- **Medium Confidence:** The structure transfer mechanism's effectiveness is demonstrated, but the specific optimal noise level (tstruct) appears to be dataset-dependent and may require tuning for different image types.
- **Medium Confidence:** The subject-guided blending improves fine detail preservation, but its effectiveness depends heavily on SAM-2's segmentation quality, which varies with object complexity.

## Next Checks

1. **Cross-domain generalization test:** Apply Add-it to diverse image domains (medical imaging, satellite imagery, scientific visualizations) to evaluate whether the heuristic weight scaling requires domain-specific tuning or if the method generalizes without modification.

2. **Physical plausibility failure analysis:** Systematically test Add-it on scenarios requiring complex physical reasoning (objects casting shadows on irregular surfaces, items interacting with water/glass, objects with complex occlusions) to identify failure modes not captured by the current benchmarks.

3. **Mask refinement dependency study:** Quantify the performance impact of using different segmentation models (SAM-2 alternatives, hand-crafted masks) in the subject-guided blending step to determine if the method's success critically depends on SAM-2's performance or if simpler approaches suffice.