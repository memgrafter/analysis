---
ver: rpa2
title: 'AdaBatchGrad: Combining Adaptive Batch Size and Adaptive Step Size'
arxiv_id: '2402.05264'
source_url: https://arxiv.org/abs/2402.05264
tags:
- size
- batch
- step
- epoch
- adagrad
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AdaBatchGrad, a novel stochastic optimization
  method that combines adaptive batch size and adaptive step size techniques. The
  method addresses the challenge of test inconsistency in adaptive batch size methods,
  where approximated tests may pass but exact tests fail, leading to poor gradient
  approximations and potential divergence.
---

# AdaBatchGrad: Combining Adaptive Batch Size and Adaptive Step Size

## Quick Facts
- arXiv ID: 2402.05264
- Source URL: https://arxiv.org/abs/2402.05264
- Authors: Petr Ostroukhov; Aigerim Zhumabayeva; Chulu Xiang; Alexander Gasnikov; Martin Takáč; Dmitry Kamzolov
- Reference count: 40
- Primary result: AdaBatchGrad achieves O(LR²/ε) convergence for exact tests and O(max{LR²/ε, σ²R²/ε²}) for inexact tests, significantly improving upon existing adaptive batch size methods

## Executive Summary
This paper introduces AdaBatchGrad, a novel stochastic optimization method that addresses test inconsistency in adaptive batch size methods by combining them with adaptive step size adjustment. The key innovation is using AdaGrad for step size adaptation while maintaining the adaptive batch size strategy with approximated tests. This combination makes the method robust to cases where approximated tests pass but exact tests would fail, preventing the poor gradient approximations that typically cause divergence in other adaptive batch size methods.

## Method Summary
AdaBatchGrad combines adaptive batch size with AdaGrad's adaptive step size mechanism to create a robust optimization method. The algorithm uses norm tests, inner product tests, and orthogonality tests to determine when to increase batch size, while step size is adjusted using the AdaGrad formula ηt = α/√(β + ∑∥∇fSi(wi)∥²). This dual adaptation strategy allows the method to handle cases where approximated batch size tests pass but exact tests would fail, with the adaptive step size preventing divergence by automatically reducing step size when gradient approximations are poor.

## Key Results
- Achieves O(LR²/ε) convergence rate for exact tests, matching theoretical lower bounds
- For inexact tests, achieves O(max{LR²/ε, σ²R²/ε²}) convergence, significantly improving over existing methods
- Demonstrates superior performance on synthetic data, classification tasks, and neural networks compared to regular SGD, AdaGrad, and SGD with tests
- Shows robustness to test inconsistency, with adaptive step size preventing divergence when approximated tests fail

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AdaBatchGrad achieves robustness to test inconsistency by combining adaptive batch size with adaptive step size
- Mechanism: When approximated batch size tests fail (pass but exact test would fail), the adaptive step size from AdaGrad prevents divergence by automatically reducing step size in response to accumulated gradient variance
- Core assumption: The approximated batch size tests are at least weakly correlated with the exact tests
- Evidence anchors:
  - [abstract] "For inexact tests, it achieves convergence in O(max{LR²/ε, σ²R²/ε²}) iterations"
  - [section 1.1] "However, the theoretical proofs only exist for the original exact versions of these tests... This introduces some test inconsistency"
  - [section 3.2] "It means that the method either converges with AdaGrad rate in the case where the test fails or converges with Gradient descent rate in the case where the test is accurate"
- Break condition: If test inconsistency is severe enough that the step size reduction from AdaGrad cannot compensate for consistently poor gradient approximations

### Mechanism 2
- Claim: Adaptive batch size reduces stochastic gradient variance, improving convergence rate
- Mechanism: The method increases batch size when inner-product and orthogonality tests indicate gradient approximation quality is poor, effectively reducing noise in gradient estimates
- Core assumption: The batch size tests (1.16) and (1.17) provide meaningful feedback about gradient approximation quality
- Evidence anchors:
  - [section 1.2] "The main difference between this method and Algorithm 2 is in the strategy of adjustment of step size"
  - [section 4.3] "Adaptivity in batch size makes the algorithm adjust batch size automatically as long as it converges to a minimum"
  - [section 3.3] "if additionally to the assumption on boundness of variance (1.5), the batch size |Sk| is chosen in such a way that it satisfies exact norm test (1.12) on each step of Algorithm 3, we can get convergence rate O(1/ε) in convex and O(1/ε²) in non-convex case"
- Break condition: If the test inconsistency is so severe that the method consistently underestimates the needed batch size

### Mechanism 3
- Claim: Adaptive step size from AdaGrad provides automatic step size tuning without requiring knowledge of Lipschitz constant
- Mechanism: Step size ηt = α/√(β + ∑∥∇fSi(wi)∥²) decreases as accumulated gradient magnitudes increase, naturally adapting to problem difficulty
- Core assumption: The sum of squared gradient norms is a reasonable proxy for local Lipschitz constant
- Evidence anchors:
  - [section 2.1] "Another approach is based on the idea of taking γ = 1/∥∇f(x)∥, which came from non-smooth optimization [35]"
  - [section 3.1] "For the adaptive batch size strategy, we aim to have such batch size |St| that the batched gradient ∇fSt(wt) would be close enough to the true gradient ∇f(wt)"
  - [section 3.3] "we can get convergence rate O(1/ε) in convex and O(1/ε²) in non-convex case"
- Break condition: If the initial parameters α and β are poorly chosen, causing the step size to be too aggressive or too conservative

## Foundational Learning

- Concept: Stochastic gradient descent and its convergence properties
  - Why needed here: Understanding how SGD works is fundamental to grasping why combining adaptive batch and step sizes improves performance
  - Quick check question: What is the main source of variance in SGD, and how does increasing batch size affect it?

- Concept: Test inconsistency in adaptive batch size methods
  - Why needed here: This is the core problem that AdaBatchGrad addresses, and understanding it is crucial for appreciating the solution
  - Quick check question: Why can't we use exact gradient tests in practice, and what problem does this create?

- Concept: AdaGrad and its step size adaptation mechanism
  - Why needed here: AdaBatchGrad uses AdaGrad's step size strategy, so understanding how it works is essential
  - Quick check question: How does AdaGrad's step size formula prevent divergence even when gradient approximations are poor?

## Architecture Onboarding

- Component map: Input (wt, St, hyperparameters) -> Step size computation (AdaGrad formula) -> Batch size adjustment (norm, inner-product, orthogonality tests) -> Gradient computation (∇fSt(wt)) -> Update (wt+1 = wt - ηt∇fSt(wt)) -> Output (wt+1, St+1)

- Critical path: The main loop that computes step size, checks batch size tests, computes gradient, and updates parameters

- Design tradeoffs:
  - More frequent batch size adjustments vs. computational overhead
  - Sensitivity of step size to initial parameters vs. adaptability
  - Test hyperparameters θ and ν balance between being too conservative (always increasing batch) vs. too permissive (never increasing)

- Failure signatures:
  - Diverging step sizes or batch sizes that grow too large
  - Oscillation in function value or gradient norm
  - Extremely slow convergence suggesting step size is too small or batch size is too large

- First 3 experiments:
  1. Compare AdaBatchGrad with fixed batch size and fixed step size SGD on a simple convex problem
  2. Test AdaBatchGrad with and without the adaptive step size component to isolate its effect
  3. Evaluate AdaBatchGrad on a problem with known Lipschitz constant to verify it matches theoretical rates when tests are accurate

## Open Questions the Paper Calls Out

- Question: What is the impact of the hyperparameters θ and ν in the batch size adjustment tests on the performance of AdaBatchGrad?
  - Basis in paper: [explicit] The paper mentions that θ and ν are hyperparameters used in the batch size adjustment tests, but does not provide a detailed analysis of their impact on the performance of AdaBatchGrad
  - Why unresolved: The paper does not include a sensitivity analysis of the hyperparameters θ and ν
  - What evidence would resolve it: Experimental results showing the performance of AdaBatchGrad with different values of θ and ν would help determine their impact on the method's effectiveness

- Question: How does AdaBatchGrad perform in comparison to other state-of-the-art adaptive optimization methods, such as Adam and AMSGrad?
  - Basis in paper: [explicit] The paper mentions that AdaBatchGrad is compared to AdaGrad and SGD, but does not provide a comparison with other state-of-the-art methods
  - Why unresolved: The paper does not include a comprehensive comparison with other adaptive optimization methods
  - What evidence would resolve it: Experimental results comparing the performance of AdaBatchGrad with other adaptive optimization methods would help determine its relative effectiveness

- Question: What is the optimal value of the parameter τ in the AdaGrad step size formula for AdaBatchGrad?
  - Basis in paper: [inferred] The paper mentions that 0 ≤ τ < 1/2, but does not specify the optimal value for τ
  - Why unresolved: The paper does not provide a detailed analysis or experimental results to determine the optimal value of τ
  - What evidence would resolve it: Experimental results comparing the performance of AdaBatchGrad with different values of τ would help determine the optimal value

## Limitations

- The empirical validation is limited to moderate-scale problems and does not test the method's behavior on extremely large-scale deep learning models
- The theoretical analysis assumes Lipschitz smoothness and bounded variance conditions, which may not hold for all real-world datasets
- The method's performance on non-stationary data distributions where gradient statistics change over time has not been evaluated

## Confidence

- **High confidence**: The core mechanism of combining adaptive batch size with AdaGrad step size is sound and theoretically justified
- **Medium confidence**: The empirical results showing performance improvements over baseline methods are convincing but limited to moderate-scale problems
- **Medium confidence**: The assumption that approximated tests provide sufficient correlation with exact tests for practical performance, though this is partially supported by the convergence analysis

## Next Checks

1. **Scale test**: Evaluate AdaBatchGrad on large-scale transformer models (BERT, GPT-2) to verify performance benefits persist at scale and identify any new failure modes

2. **Distribution shift test**: Test the method on non-stationary data distributions where gradient statistics change over time to assess robustness beyond the stationary assumptions

3. **Hyperparameter sensitivity analysis**: Systematically vary θ, ν, α, β across multiple orders of magnitude to quantify the method's sensitivity and identify optimal ranges for different problem classes