---
ver: rpa2
title: 'ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference
  Time'
arxiv_id: '2410.06625'
source_url: https://arxiv.org/abs/2410.06625
tags:
- safety
- responses
- image
- arxiv
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the safety challenges in Vision Language
  Models (VLMs), where adversarial visual inputs can bypass existing defense mechanisms.
  The proposed Evaluating Then Aligning (ETA) framework introduces a two-phase inference-time
  alignment approach: multimodal evaluation using CLIP scores and reward models to
  assess visual inputs and responses, followed by bi-level alignment using interference
  prefixes and sentence-level best-of-N searching.'
---

# ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time

## Quick Facts
- arXiv ID: 2410.06625
- Source URL: https://arxiv.org/abs/2410.06625
- Reference count: 40
- Key outcome: 87.5% reduction in unsafe rate on cross-modality attacks with 96.6% win-ties in GPT-4 helpfulness evaluation

## Executive Summary
This paper addresses the critical safety challenges in Vision Language Models (VLMs) where adversarial visual inputs can bypass existing safety mechanisms. The proposed Evaluating Then Aligning (ETA) framework introduces a novel two-phase inference-time alignment approach that first evaluates visual inputs and responses using multimodal evaluators, then aligns unsafe behaviors through bi-level alignment strategies. ETA achieves significant improvements in VLM safety while maintaining model efficiency and general capabilities, outperforming baseline methods on multiple safety benchmarks.

## Method Summary
ETA implements a two-phase inference-time alignment framework for VLMs. The first phase uses multimodal evaluation with CLIP scores to assess visual inputs and reward models to evaluate text outputs, establishing safety thresholds. The second phase performs bi-level alignment using interference prefixes to condition the generative distribution (shallow alignment) and sentence-level best-of-N searching to find safe and helpful responses (deep alignment). The framework is evaluated across multiple VLM backbones including LLaVA-1.5 and InternVL-Chat models on safety benchmarks like SPA-VL Harm, MM-SafetyBench, FigStep, and AdvBench.

## Key Results
- 87.5% reduction in unsafe rate on cross-modality attacks
- 96.6% win-ties in GPT-4 helpfulness evaluation
- Outperforms baseline methods while maintaining general capabilities on VQA benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous visual token embeddings bypass safety mechanisms aligned on discrete text embeddings
- Mechanism: Visual embeddings are in a continuous space that doesn't align with the discrete text embedding space where LLM safety was trained, creating outliers that bypass safety checks
- Core assumption: The continuous visual embeddings are sufficiently distant from discrete text embeddings to act as outliers
- Evidence anchors:
  - [abstract] "the visual modality can easily bypass existing safety mechanisms"
  - [section 4.1] "continuous visual embeddings Evisual ⊂ Rd often appear away from all textual embeddings"
  - [section 4.1] "We implemented an alternating mapping, where continuous visual embeddings are mapped to their nearest textual embeddings... This method resulted in a significant 7% reduction in the unsafe rate (USR)"

### Mechanism 2
- Claim: Multimodal evaluation using CLIP scores can detect unsafe visual content before generation
- Mechanism: CLIP's cross-modal embedding space allows semantic similarity assessment between images and safety-relevant text prompts, enabling pre-generation safety screening
- Core assumption: CLIP's semantic understanding is sufficient to distinguish between safe and unsafe visual content
- Evidence anchors:
  - [abstract] "Evaluating input visual contents and output responses to establish a robust safety awareness in multimodal settings"
  - [section 4.1.1] "The CLIP-score (Hessel et al., 2021), which is the cosine similarity, is then used to measure the relevance between the input text and image"
  - [section 4.1.1] "As depicted in Fig. 3b, the score distributions exhibit a distinct separation, which justifies setting a threshold τpre"

### Mechanism 3
- Claim: Bi-level alignment (shallow + deep) can correct unsafe behaviors while maintaining helpfulness
- Mechanism: Shallow alignment uses interference prefixes to shift output distribution toward safe styles, while deep alignment uses sentence-level best-of-N searching to find safe and helpful content
- Core assumption: The VLM's autoregressive decoding is sufficiently influenced by initial tokens to enable shallow alignment, and the multimodal evaluator can effectively guide deep alignment
- Evidence anchors:
  - [abstract] "Aligning unsafe behaviors at both shallow and deep levels by conditioning the VLMs' generative distribution with an interference prefix and performing sentence-level best-of-N"
  - [section 4.2.1] "The autoregressive decoding mechanism of VLMs... implies that the initial tokens greatly influence the distribution of subsequent tokens"
  - [section 4.2.2] "we adopt a sentence-level best-of-N searching algorithm as the deep alignment method. This approach leverages our multimodal evaluator to guide the response generation process"

## Foundational Learning

- Concept: Autoregressive decoding in language models
  - Why needed here: ETA relies on understanding how initial tokens influence subsequent generation in VLMs to implement shallow alignment
  - Quick check question: How does the probability of generating token t_i depend on previously generated tokens in autoregressive models?

- Concept: Cross-modal embeddings and semantic similarity
  - Why needed here: ETA uses CLIP's cross-modal embedding space to assess visual content safety through semantic similarity to harmful concepts
  - Quick check question: What properties must a cross-modal embedding space have to enable meaningful semantic similarity between images and text?

- Concept: Best-of-N decoding strategies
  - Why needed here: ETA employs sentence-level best-of-N searching as part of deep alignment to find safe and helpful responses
  - Quick check question: How does the computational cost of best-of-N searching scale with the number of candidates and generation steps?

## Architecture Onboarding

- Component map: Input → Pre-generation evaluator (CLIP) → VLM generation → Post-generation evaluator (Reward Model) → (if unsafe) Shallow alignment (Interference Prefix) → Deep alignment (Sentence-level Best-of-N) → Output

- Critical path: Input → Pre-generation evaluation → VLM generation → Post-generation evaluation → (if unsafe) Shallow alignment → Deep alignment → Output

- Design tradeoffs:
  - Safety vs. helpfulness: Higher thresholds for safety evaluators reduce unsafe outputs but may also reduce helpfulness
  - Computational cost vs. safety: More candidates in best-of-N improves safety but increases inference time
  - Prefix specificity vs. generalization: More specific prefixes may be more effective but less generalizable

- Failure signatures:
  - High unsafe rate despite ETA indicates threshold settings too permissive or evaluator failures
  - Low helpfulness despite low unsafe rate indicates thresholds too strict or alignment overly conservative
  - Excessive inference time indicates too many candidates in best-of-N or inefficient evaluation

- First 3 experiments:
  1. Baseline unsafe rate measurement on safety benchmarks without ETA to establish reference point
  2. Pre-generation evaluation only (without alignment) to measure effectiveness of visual content screening
  3. Shallow alignment only (without deep alignment) to measure effectiveness of prefix-based distribution shifting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the alignment process be adapted for other modalities beyond vision and language, such as audio or sensor data?
- Basis in paper: [explicit] The paper focuses on vision-language models but discusses the need for multimodal evaluation and alignment, suggesting potential applicability to other modalities
- Why unresolved: The paper only addresses vision-language models, leaving the extension to other modalities unexplored
- What evidence would resolve it: Experimental results showing the effectiveness of the proposed framework on models with audio or sensor inputs

### Open Question 2
- Question: What are the long-term effects of inference-time alignment on model performance and safety, and how does it compare to fine-tuning?
- Basis in paper: [inferred] The paper emphasizes the advantages of inference-time alignment over fine-tuning, but does not explore long-term impacts
- Why unresolved: The study focuses on immediate safety improvements, not long-term effects or comparisons with fine-tuning
- What evidence would resolve it: Longitudinal studies comparing model performance and safety over time with inference-time alignment versus fine-tuning

### Open Question 3
- Question: How does the proposed framework handle complex, nuanced safety scenarios that involve multiple modalities and contextual factors?
- Basis in paper: [inferred] The paper demonstrates effectiveness on safety benchmarks but does not address complex, real-world scenarios
- Why unresolved: The evaluation focuses on predefined benchmarks, not the complexity of real-world safety challenges
- What evidence would resolve it: Case studies or experiments involving complex, multimodal safety scenarios with contextual factors

## Limitations
- Computational overhead of sentence-level best-of-N searching may be prohibitive for real-time applications
- Framework effectiveness depends heavily on quality and coverage of multimodal evaluator
- Limited assessment of broader safety concerns beyond adversarial visual inputs
- Long-term effectiveness against evolving attack strategies not addressed

## Confidence

**High Confidence**: The claim that ETA achieves 87.5% reduction in unsafe rate on cross-modality attacks is supported by multiple experimental results across different VLM backbones and safety benchmarks. The 96.6% win-ties in GPT-4 helpfulness evaluation also shows consistent improvement across different model sizes.

**Medium Confidence**: The assertion that ETA outperforms baseline methods while maintaining model efficiency is supported by comparisons to single-stage methods, but the computational overhead of best-of-N searching is not fully characterized. The claim that visual embeddings bypass safety mechanisms is well-supported theoretically but could benefit from more empirical analysis of the embedding space properties.

**Low Confidence**: The claim that ETA is broadly applicable to different VLMs without extensive fine-tuning is supported by results on multiple models, but the paper does not explore the full range of VLM architectures or training paradigms. The long-term effectiveness against evolving attack strategies is not addressed.

## Next Checks

1. **Computational Overhead Characterization**: Measure the actual inference time increase when applying ETA's deep alignment with sentence-level best-of-N searching (N=5) compared to baseline inference. Characterize how this scales with different VLM sizes and candidate counts.

2. **General Capabilities Degradation Assessment**: Evaluate the impact of ETA on a broader range of general capabilities beyond VQA, including reasoning tasks, creative writing, and open-ended question answering. Measure whether safety improvements come at the cost of utility in non-adversarial contexts.

3. **Adversarial Robustness Testing**: Design and test novel adversarial attacks specifically targeting ETA's defense mechanisms, such as attacks on the CLIP-based pre-generation evaluator or the reward model-based post-generation evaluator. Assess whether attackers can find blind spots in the two-phase alignment approach.