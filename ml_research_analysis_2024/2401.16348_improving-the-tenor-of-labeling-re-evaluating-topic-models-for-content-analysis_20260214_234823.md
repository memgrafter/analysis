---
ver: rpa2
title: 'Improving the TENOR of Labeling: Re-evaluating Topic Models for Content Analysis'
arxiv_id: '2401.16348'
source_url: https://arxiv.org/abs/2401.16348
tags:
- topic
- user
- slda
- labels
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares classical, neural, and supervised topic models
  in an interactive task-based setting for content analysis. We introduce TENOR, a
  tool that combines topic models with active learning to aid humans in creating label
  sets and annotating documents.
---

# Improving the TENOR of Labeling: Re-evaluating Topic Models for Content Analysis

## Quick Facts
- arXiv ID: 2401.16348
- Source URL: https://arxiv.org/abs/2401.16348
- Authors: Zongxia Li; Andrew Mao; Daniel Stephens; Pranav Goel; Emily Walpole; Alden Dima; Juan Fung; Jordan Boyd-Graber
- Reference count: 30
- Primary result: CTM outperforms other models in human evaluations for content labeling tasks

## Executive Summary
This study re-evaluates topic models for content analysis by comparing classical, neural, and supervised approaches in an interactive labeling task. The researchers introduce TENOR, a tool that combines topic models with active learning to assist humans in creating label sets and annotating documents. Through simulated experiments and user studies on US congressional bills, they find that the Contextual Neural Topic Model (CTM) performs best in cluster evaluation metrics and human evaluations. Surprisingly, LDA remains competitive with two neural models despite having lower coherence scores, suggesting automated metrics don't fully capture topic modeling capabilities in practical tasks.

## Method Summary
The study evaluates three neural topic models (CTM, ETM, ProdLDA) against LDA using a mixed-methods approach. Researchers conducted simulated experiments and user studies with 12 participants working on US congressional bills. The TENOR tool was used to integrate topic models with active learning for document labeling tasks. Human evaluations were performed to assess model performance in practical labeling scenarios, while automated metrics provided additional quantitative comparisons. The study specifically examined how pre-trained embeddings in CTM affect keyword specificity for document labeling compared to LDA's more general keywords.

## Key Results
- CTM outperforms other models in cluster evaluation metrics and human evaluations
- LDA remains competitive with neural models despite lower coherence scores
- CTM provides more specific and helpful keywords for labeling individual documents compared to LDA
- Automated metrics do not fully capture topic modeling capabilities in practical labeling tasks

## Why This Works (Mechanism)
The effectiveness of CTM stems from its use of pre-trained embeddings, which provide more semantically rich and specific topic representations compared to LDA's count-based approach. This allows CTM to generate more informative keywords that better capture document content, making the labeling process more efficient for human annotators. The active learning component in TENOR further enhances this by focusing human effort on documents where model uncertainty is highest, creating a more effective human-AI collaboration loop.

## Foundational Learning
**Topic Modeling Fundamentals**: Understanding how different topic models represent document collections is essential for evaluating their practical utility in labeling tasks.
*Why needed*: Provides baseline knowledge for comparing classical vs. neural approaches
*Quick check*: Can you explain the difference between LDA's count-based approach and CTM's embedding-based approach?

**Active Learning Integration**: The combination of topic models with active learning optimizes human annotation effort by prioritizing uncertain documents.
*Why needed*: Explains how TENOR improves efficiency in labeling workflows
*Quick check*: How does active learning reduce annotation burden in content analysis?

**Human-in-the-Loop Evaluation**: Direct human assessment of model outputs provides insights that automated metrics cannot capture.
*Why needed*: Critical for understanding real-world model effectiveness
*Quick check*: What are the limitations of using only automated metrics for topic model evaluation?

## Architecture Onboarding

**Component Map**: Document Collection -> Topic Models (LDA, CTM, ETM, ProdLDA) -> Active Learning System -> TENOR Interface -> Human Annotators

**Critical Path**: The evaluation pipeline follows: preprocessing congressional bills → topic model inference → keyword generation → active learning selection → human evaluation of labeling effectiveness

**Design Tradeoffs**: The study chose to focus on US congressional bills for domain consistency, but this limits generalizability. The selection of three neural models provides breadth while maintaining computational feasibility for user studies.

**Failure Signatures**: Models may fail when topic distributions are too diffuse, keywords are too generic, or active learning cannot effectively identify uncertain documents. The study found that LDA's lower coherence doesn't necessarily translate to worse practical performance.

**3 First Experiments**: 
1. Run CTM and LDA on a small subset of congressional bills and compare keyword specificity
2. Implement active learning selection to identify documents where model confidence is lowest
3. Conduct preliminary human evaluation with 3-4 documents to test labeling efficiency differences

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Small user study sample size (n=12) limits statistical power for detecting smaller effect sizes
- Focus on US congressional bills constrains external validity to other document domains
- Comparison of only three neural topic models leaves uncertainty about patterns with other neural approaches

## Confidence

**High**: CTM outperforms other models in human evaluations and provides more specific keywords for document labeling

**Medium**: LDA remains competitive despite lower coherence scores - requires further validation across different document types

**Low**: Generalizability of these specific model rankings to other domains or task contexts

## Next Checks
1. Replicate the user study with a larger, more diverse participant pool across multiple document domains to assess generalizability

2. Conduct ablation studies comparing CTM's performance with and without pre-trained embeddings to isolate their contribution to effectiveness

3. Test additional neural topic models (e.g., BAT, LFTM) to determine if the observed competitive advantage of LDA extends to other classical models