---
ver: rpa2
title: Improving Bilingual Capabilities of Language Models to Support Diverse Linguistic
  Practices in Education
arxiv_id: '2411.04308'
source_url: https://arxiv.org/abs/2411.04308
tags:
- spanglish
- performance
- language
- english
- spanish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study demonstrates that multilingual large language models
  (MLLMs) exhibit significant bias in grading bilingual (Spanglish) student writing
  compared to monolingual English and Spanish responses. Using synthetic datasets
  and fine-tuning approaches, the researchers show that MLLMs perform substantially
  better for all three languages after being fine-tuned with bilingual data.
---

# Improving Bilingual Capabilities of Language Models to Support Diverse Linguistic Practices in Education

## Quick Facts
- arXiv ID: 2411.04308
- Source URL: https://arxiv.org/abs/2411.04308
- Reference count: 33
- Pre-trained MLLMs show significant bias against Spanglish student writing compared to monolingual responses

## Executive Summary
This study demonstrates that multilingual large language models exhibit systematic bias when grading bilingual (Spanglish) student writing compared to monolingual English and Spanish responses. The researchers show that fine-tuning with bilingual data substantially reduces this performance gap, with mixed-language fine-tuning achieving comparable results to pure Spanglish fine-tuning while requiring less bilingual data. The findings highlight the potential of targeted fine-tuning to support authentic language practices among bilingual learners and address the technological inequity faced by students who use code-switching in their writing.

## Method Summary
The researchers generated synthetic datasets of question-answer pairs in English, Spanish, and Spanglish across Science and Social Science topics for grades 6-10 using Claude 3.5 Sonnet. They evaluated baseline performance of Llama 3.1 (8B) and Mistral NeMo (12B) models using zero-shot, few-shot, and fine-tuning approaches. Fine-tuning experiments included English-only, Spanish-only, Spanglish-only, and mixed-language datasets using LoRA adapters through Unsloth's library. Performance was measured using AUC scores across all three languages, with particular attention to cross-lingual transfer effects.

## Key Results
- Pre-trained Llama 3.1 and Mistral NeMo models achieved AUC scores of 0.90/0.84 for English, 0.83/0.73 for Spanish, but only 0.74/0.69 for Spanglish
- Fine-tuning with Spanglish data improved scores to 0.94/0.95 for Spanglish and enhanced performance on English/Spanish tasks
- Mixed-language fine-tuning (combining all three languages) achieved comparable performance to pure Spanglish fine-tuning while requiring less bilingual data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning with Spanglish data reduces performance bias between bilingual and monolingual writing.
- Mechanism: The model's token embeddings for mixed-language input are initially poorly aligned because training data lacks code-switched examples. Fine-tuning on synthetic Spanglish aligns embeddings across languages, enabling consistent contextual representation regardless of language switch points.
- Core assumption: The model's internal representations are flexible enough to generalize from synthetic to real bilingual usage.
- Evidence anchors:
  - [abstract] "Our experiments indicate that the models perform significantly better for all three languages after fine-tuning with bilingual data."
  - [section] "Fine-tuning with Spanglish substantially enhances performance on Spanglish assessment, outperforming few-shot and zero-shot approaches."
  - [corpus] Weak corpus anchor: neighbor papers focus on code-switching but lack direct fine-tuning performance data.
- Break condition: Fine-tuning data is too synthetic or unrepresentative of natural bilingual patterns, leading to overfitting and poor real-world generalization.

### Mechanism 2
- Claim: Mixed-language fine-tuning (English + Spanish + Spanglish) achieves comparable performance to pure Spanglish fine-tuning with fewer Spanglish examples.
- Mechanism: Including multiple languages in fine-tuning increases the model's cross-lingual transfer capability. The presence of English and Spanish examples teaches the model to navigate between language boundaries, reducing the number of Spanglish-specific examples needed to reach target performance.
- Core assumption: The model benefits from exposure to language boundaries in a controlled way, and this generalizes to more fluid code-switching.
- Evidence anchors:
  - [abstract] "mixed-language fine-tuning (combining English, Spanish, and Spanglish) achieved comparable performance to pure Spanglish fine-tuning while requiring less bilingual data."
  - [section] "Mixed-dataset fine-tuning is more consistent across languages compared to homogeneous fine-tuning... their performance is comparable to or very close to that of the Spanglish-only fine-tuned model."
  - [corpus] No direct corpus evidence; inference drawn from mixed-language training literature.
- Break condition: The mixed data is too imbalanced or the model overfits to majority language patterns, negating the benefit of code-switching exposure.

### Mechanism 3
- Claim: Spanglish fine-tuning improves cross-lingual transfer to English and Spanish grading, outperforming monolingual fine-tuning for those languages.
- Mechanism: Fine-tuning on a blended linguistic form forces the model to develop more robust semantic representations that are less dependent on surface language form. This "language-agnostic" encoding improves performance on both source and target monolingual tasks.
- Core assumption: The semantic similarity between Spanglish and its component languages is sufficient for shared representations to emerge.
- Evidence anchors:
  - [abstract] "the Spanglish fine-tuned model and mixed combination models outperformed target language fine-tuning."
  - [section] "the Spanglish fine-tuned model... achieved near-optimal scores for English (0.970) and Spanish (0.955)."
  - [corpus] Weak anchor: no corpus evidence for semantic sharing between Spanglish and monolingual variants.
- Break condition: The model fails to disentangle language-specific cues from semantic content, leading to degraded performance on monolingual tasks.

## Foundational Learning

- Concept: Zero-shot vs. few-shot vs. fine-tuning performance differences.
  - Why needed here: Establishes baseline capability and improvement from each intervention method.
  - Quick check question: What is the primary difference in how a model approaches a task in zero-shot vs. fine-tuning mode?

- Concept: AUC (Area Under ROC Curve) as a performance metric.
  - Why needed here: Provides a threshold-independent measure of grading accuracy across languages.
  - Quick check question: Why is AUC preferred over simple accuracy in imbalanced binary classification tasks?

- Concept: Synthetic data generation and evaluation.
  - Why needed here: Explains the trade-off between data availability and realism in training bilingual models.
  - Quick check question: What human evaluation metrics were used to validate synthetic Spanglish quality?

## Architecture Onboarding

- Component map:
  - Data pipeline: synthetic data generator → translation module → train/val/test splits
  - Model layer: pre-trained MLLM (Llama 3.1 or Mistral NeMo) → LoRA adapters for fine-tuning
  - Evaluation layer: zero-shot prompt → few-shot prompt → fine-tuned prompt → AUC scorer

- Critical path: Synthetic data generation → fine-tuning (target or mixed) → evaluation across languages

- Design tradeoffs:
  - Data realism vs. scalability: synthetic data is cheap but may not reflect real student writing
  - Model size vs. fine-tuning cost: smaller models (8B) are faster to fine-tune but may underfit
  - Language balance vs. Spanglish scarcity: mixed datasets reduce bias but dilute code-switching examples

- Failure signatures:
  - Spanglish performance flatlines despite fine-tuning: synthetic data is unrepresentative
  - English/Spanish performance drops after Spanglish fine-tuning: language interference or catastrophic forgetting
  - Mixed dataset underperforms: language mixing ratio or dataset size insufficient

- First 3 experiments:
  1. Compare zero-shot AUC across English, Spanish, and Spanglish to establish baseline bias
  2. Apply few-shot prompting with 3 Spanglish examples and re-evaluate AUC
  3. Fine-tune on 150 Spanglish samples, then evaluate on all three languages to measure cross-lingual transfer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do MLLMs perform when fine-tuned with authentic bilingual data compared to synthetic data?
- Basis in paper: [inferred] The study relies entirely on synthetic data due to lack of real-world bilingual datasets, suggesting this gap
- Why unresolved: The paper acknowledges this limitation but doesn't test actual student writing
- What evidence would resolve it: Comparative study using both synthetic and authentic bilingual student writing datasets

### Open Question 2
- Question: What is the optimal ratio of monolingual to bilingual training data for mixed-language fine-tuning?
- Basis in paper: [explicit] The study uses 50 Spanglish samples out of 150 total in mixed datasets but doesn't explore optimal ratios
- Why unresolved: Only one mixed ratio was tested (100:50:50 for English-Spanish-Spanglish)
- What evidence would resolve it: Systematic exploration of different monolingual:bilingual ratios in fine-tuning experiments

### Open Question 3
- Question: Do smaller open-source MLLMs show similar biases for other low-resource language pairs beyond Spanish-English?
- Basis in paper: [explicit] The study notes the need to investigate "effectiveness of MLLMs grading performance with additional low-resource languages"
- Why unresolved: Only tested high-resource language pair (Spanish-English)
- What evidence would resolve it: Similar experiments with other language pairs (e.g., Mandarin-English, Arabic-English)

## Limitations

- The study relies entirely on synthetic Spanglish data rather than authentic bilingual student writing, raising questions about real-world generalizability
- The Spanglish dataset size is relatively small (150 samples), which may limit the observed improvements from scaling to larger, more diverse datasets
- The evaluation focuses solely on grading accuracy through AUC scores without examining potential biases in error types or alignment with human educational standards

## Confidence

- **High confidence**: The finding that pre-trained MLLMs show systematic bias against Spanglish compared to monolingual English and Spanish responses is well-supported by baseline experiments
- **Medium confidence**: The effectiveness of Spanglish fine-tuning in improving performance is demonstrated, but reliance on synthetic data introduces uncertainty about real-world generalizability
- **Medium confidence**: The claim that mixed-language fine-tuning achieves comparable performance to pure Spanglish fine-tuning with less data is supported, though optimal ratios warrant further investigation

## Next Checks

1. **Real Data Validation:** Evaluate the fine-tuned models on a small corpus of authentic bilingual student writing to verify whether synthetic data fine-tuning generalizes to real-world performance

2. **Bias Analysis:** Conduct a detailed error analysis comparing model predictions across language pairs to identify whether the model exhibits different error patterns for code-switched versus monolingual responses

3. **Scalability Test:** Replicate the experiments with varying dataset sizes (50, 150, 500, 1000 Spanglish examples) to determine the minimum effective dataset size and whether performance improvements plateau