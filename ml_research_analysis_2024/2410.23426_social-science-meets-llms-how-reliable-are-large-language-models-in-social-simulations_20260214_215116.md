---
ver: rpa2
title: 'Social Science Meets LLMs: How Reliable Are Large Language Models in Social
  Simulations?'
arxiv_id: '2410.23426'
source_url: https://arxiv.org/abs/2410.23426
tags:
- arxiv
- llms
- simulation
- social
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates the reliability of LLM-based
  simulations in social science research by introducing TRUST SIM, a dataset covering
  10 CSS-related subjects. Extensive experiments on 14 LLMs reveal persistent inconsistencies
  in simulated roles, with some models exhibiting inconsistency rates exceeding 30%.
---

# Social Science Meets LLMs: How Reliable Are Large Language Models in Social Simulations?

## Quick Facts
- arXiv ID: 2410.23426
- Source URL: https://arxiv.org/abs/2410.23426
- Authors: Yue Huang; Zhengqing Yuan; Yujun Zhou; Kehan Guo; Xiangqi Wang; Haomin Zhuang; Weixiang Sun; Lichao Sun; Jindong Wang; Yanfang Ye; Xiangliang Zhang
- Reference count: 32
- Key outcome: LLM-based simulations show high inconsistency rates (exceeding 30% in some models) and weak correlation with general performance; AdaORPO improves reliability by 6-9 percentage points

## Executive Summary
This study systematically investigates the reliability of LLM-based simulations in social science research by introducing TRUST SIM, a dataset covering 10 CSS-related subjects. Extensive experiments on 14 LLMs reveal persistent inconsistencies in simulated roles, with some models exhibiting inconsistency rates exceeding 30%. The study also finds that simulation capability does not strongly correlate with general utility performance. To address these issues, the authors propose AdaORPO, a reinforcement learning-based algorithm that improves simulation reliability across 7 LLMs, demonstrating significant gains in satisfaction rates and alignment.

## Method Summary
The study introduces TRUST SIM, an evaluation dataset with 740 instances across 10 CSS subjects, to systematically assess LLM simulation reliability. Each instance includes scenarios, system prompts, self-report questions, and open-ended questions that test whether LLMs maintain consistent character personas. The evaluation uses GPT-4o as an LLM-as-judge to assess consistency between different response types. The authors then develop AdaORPO, a reinforcement learning algorithm that improves simulation reliability by combining supervised fine-tuning with preference-based alignment, using adaptive learning rates based on judge ratings.

## Key Results
- LLM-based simulations exhibit high inconsistency rates, with some models showing inconsistency exceeding 30%
- Simulation capability does not strongly correlate with general LLM performance
- AdaORPO significantly improves simulation reliability across 7 LLMs, increasing satisfaction rates by 6-9 percentage points
- Open-weight models like Llama series perform competitively with proprietary models in simulation tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: AdaORPO improves simulation reliability by combining supervised fine-tuning with preference-based alignment
- **Mechanism**: AdaORPO constructs training data from LLM-as-judge evaluations, pairing unsatisfied responses with preferred alternatives, then optimizes using combined loss functions with adaptive learning rates
- **Core assumption**: High-quality simulations can be learned by reinforcing responses that align with character personas
- **Evidence anchors**: Abstract shows AdaORPO gains of 6-9 percentage points; section describes combined LSFT + LORPO loss computation
- **Break condition**: If LLM-as-judge is inconsistent, training dataset will reinforce incorrect patterns

### Mechanism 2
- **Claim**: TRUST SIM enables systematic evaluation by covering diverse CSS subjects
- **Mechanism**: Dataset contains 740 evaluation instances across 10 CSS subjects with structured components including scenarios, prompts, and different question types
- **Core assumption**: Consistent simulation requires alignment between responses and character persona across question formats
- **Evidence anchors**: Abstract mentions dataset covers 10 CSS topics; section details 6 components per evaluation instance
- **Break condition**: Dataset construction bias or inconsistent human panel review makes evaluation unreliable

### Mechanism 3
- **Claim**: Simulation capability is not strongly correlated with general utility performance
- **Mechanism**: Different LLMs show varying simulation performance compared to their general performance rankings
- **Core assumption**: Specialized capabilities like role-playing require different model characteristics than general reasoning
- **Evidence anchors**: Abstract states consistency doesn't correlate with general performance; section shows Llama series outperforming in simulations
- **Break condition**: Flawed evaluation methodology or inadequate subject selection invalidates correlation analysis

## Foundational Learning

- **Concept**: Reinforcement Learning from Human Feedback (RLHF) and preference optimization
  - **Why needed here**: AdaORPO builds on preference optimization principles for aligning models with human preferences
  - **Quick check question**: How does preference optimization differ from traditional supervised fine-tuning?

- **Concept**: Agent-based modeling and social simulation
  - **Why needed here**: Research involves using LLMs as agents in social science simulations
  - **Quick check question**: What are key differences between traditional agent-based modeling and LLM-based agent simulation?

- **Concept**: LLM-as-a-judge evaluation methodology
  - **Why needed here**: Study uses GPT-4o to evaluate other LLM responses
  - **Quick check question**: What are potential biases and limitations when using LLMs as judges for other LLM outputs?

## Architecture Onboarding

- **Component map**: TRUST SIM dataset -> 14 LLM models -> GPT-4o LLM-as-judge -> AdaORPO algorithm -> Training pipeline with adaptive learning rates
- **Critical path**: 1. Dataset construction and quality control 2. LLM evaluation using judge 3. AdaORPO training data construction 4. Model training and evaluation
- **Design tradeoffs**: Single-turn vs. multi-turn conversations (simplicity vs. realism); Binary satisfaction vs. score-based evaluation (precision vs. nuance); Open-weight vs. proprietary models (accessibility vs. performance)
- **Failure signatures**: High inconsistency rates between self-report and open-ended responses; Low satisfaction rates across multiple models; Poor correlation between general performance and simulation capability
- **First 3 experiments**: 1. Run evaluation on small TRUST SIM subset to validate dataset quality 2. Test AdaORPO training on single model with controlled parameters 3. Compare satisfaction rates before/after AdaORPO training on held-out validation set

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does consistency of LLM-based simulations vary across different CSS subjects, and what factors contribute to these variations?
- **Basis in paper**: [explicit] Paper mentions inconsistency rates vary significantly across CSS subjects
- **Why unresolved**: Paper identifies variations but doesn't explore specific contributing factors
- **What evidence would resolve it**: Detailed analysis of specific subjects with highest inconsistencies and factors causing differences

### Open Question 2
- **Question**: To what extent can AdaORPO be generalized to improve reliability of LLM-based simulations in domains beyond CSS?
- **Basis in paper**: [inferred] AdaORPO is demonstrated within CSS but application to other domains is unexplored
- **Why unresolved**: Effectiveness shown within CSS only, other domains not tested
- **What evidence would resolve it**: Testing AdaORPO on LLMs in diverse domains (healthcare, finance) to evaluate effectiveness outside CSS

### Open Question 3
- **Question**: What are long-term implications of using LLM-based simulations in social science research, particularly concerning potential for biased or skewed results?
- **Basis in paper**: [explicit] Paper acknowledges potential for bias and need for ethical considerations
- **Why unresolved**: While importance of ethical considerations is highlighted, long-term implications are not explored
- **What evidence would resolve it**: Longitudinal studies examining impact of LLM-based simulations on research outcomes and reliability over time

## Limitations

- Evaluation heavily depends on GPT-4o as judge, with potential judge bias or inconsistency not fully explored
- Assumes human-annotated ground truth data is perfect, though human disagreement in social science scenarios is common
- AdaORPO effectiveness may be limited by quality of training data constructed from judge evaluations

## Confidence

- **High Confidence**: Observed inconsistency rates across different LLMs and finding that simulation capability doesn't correlate with general performance
- **Medium Confidence**: AdaORPO algorithm's effectiveness demonstrated through improved satisfaction rates, but long-term stability requires validation
- **Medium Confidence**: Dataset construction methodology appears sound, but representativeness of CSS subjects and potential selection bias could affect generalizability

## Next Checks

1. **Judge Reliability Audit**: Conduct cross-validation by having multiple judges (including humans) evaluate a subset of responses to quantify judge consistency and identify potential biases

2. **Longitudinal Performance Analysis**: Evaluate stability of AdaORPO improvements over extended time periods and across multiple simulation scenarios to assess whether gains persist or degrade

3. **Cross-Dataset Generalization Test**: Apply evaluation methodology to independent social simulation datasets to verify findings about LLM inconsistency and AdaORPO effectiveness generalize beyond TRUST SIM dataset