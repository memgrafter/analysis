---
ver: rpa2
title: Simple Domain Adaptation for Sparse Retrievers
arxiv_id: '2401.11509'
source_url: https://arxiv.org/abs/2401.11509
tags:
- domain
- https
- adaptation
- pre-training
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a simple approach for cross-domain adaptation
  of sparse retrievers in information retrieval. The authors adapt a method initially
  designed for language adaptation by leveraging pre-training on target domain data
  to learn domain-specific knowledge, thereby alleviating the need for annotated data.
---

# Simple Domain Adaptation for Sparse Retrievers

## Quick Facts
- arXiv ID: 2401.11509
- Source URL: https://arxiv.org/abs/2401.11509
- Reference count: 40
- Primary result: Cross-domain adaptation of sparse retrievers through pre-training on target domain data, achieving up to 1.4 points improvement in nDCG@10 over zero-shot learning

## Executive Summary
This paper introduces a simple yet effective approach for adapting sparse retrievers to new domains without requiring annotated target data. The method disentangles domain-specific and task-specific parameters within the model, pre-training the domain-specific subset on target data while fine-tuning the task-specific subset on source domain relevance judgments. Experiments across 10 datasets show significant improvements over zero-shot baselines, particularly for first-stage retrievers which are more sensitive to domain shifts.

## Method Summary
The approach involves splitting model parameters into domain-specific (Pdomain) and task-specific (Ptask) subsets, pre-training Pdomain on both source and target domains separately using Masked Language Modeling, then fine-tuning Ptask on the source domain IR task while keeping Pdomain frozen. This allows the model to learn domain-specific representations from unlabeled target data while maintaining task-specific relevance learning from the source domain. The method is particularly focused on first-stage sparse retrievers like SPLADE, where domain adaptation is most critical.

## Key Results
- Achieves up to 1.4 points improvement in nDCG@10 over zero-shot SPLADE baselines
- Pre-training over additional layers (k) provides gains of up to 0.5 points in nDCG@10
- Best models typically exhibit higher sparsity levels compared to baselines
- Consistently outperforms BM25 and zero-shot approaches across 10 diverse datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific and task-specific parameters can be disentangled within the same neural network, allowing domain adaptation without labeled target data.
- Mechanism: By splitting the model parameters into Pdomain (domain-dependent) and Ptask (task-dependent), pre-training can adapt Pdomain to the target domain while keeping Ptask frozen, then fine-tuning adapts Ptask to the IR task on the source domain.
- Core assumption: Domain-specific and task-specific knowledge are separable in the model's parameter space, and pre-training on target domain data without labels is sufficient to learn domain-specific representations.
- Evidence anchors:
  - [abstract] "By leveraging pre-training on the target data to learn domain-specific knowledge, this technique alleviates the need for annotated data..."
  - [section] "we study which subparts of the model should be pre-trained or fine-tuned â€“ we do not only consider the embeddings to be domain-dependent."
  - [corpus] Weak - no direct supporting evidence found in neighbor papers.

### Mechanism 2
- Claim: Pre-training on both source and target domains sequentially improves adaptation by first learning task-specific parameters in a domain-appropriate context, then adapting to the target domain.
- Mechanism: First pre-train Pdomain on source domain to initialize domain-specific parameters in a task-relevant context, then pre-train Pdomain on target domain to specialize to the target, finally fine-tune Ptask on source domain to learn task-specific relevance.
- Core assumption: The order of pre-training matters, and learning task-specific parameters in a domain-appropriate context before adapting to the target domain leads to better generalization.
- Evidence anchors:
  - [section] "we propose a new pre-training procedure on both the source and target domains, and not only on the target one. This allows a model to learn task-specific parameters (on the source domain) when (and only when) its domain-specific ones are well set."
  - [abstract] "By leveraging pre-training on the target data to learn domain-specific knowledge..."
  - [corpus] Weak - no direct supporting evidence found in neighbor papers.

### Mechanism 3
- Claim: For first-stage sparse retrievers like SPLADE, domain adaptation is more critical than for second-stage rankers due to their higher sensitivity to domain shifts.
- Mechanism: First-stage retrievers rely heavily on the quality of document representations, which are more affected by domain shifts. Adapting these representations through pre-training improves retrieval effectiveness.
- Core assumption: First-stage retrievers are more sensitive to domain shifts than second-stage rankers, and adapting their representations has a larger impact on retrieval performance.
- Evidence anchors:
  - [abstract] "The method is particularly effective for first-stage retrievers, which are more sensitive to domain shifts."
  - [section] "As domain shift impacts more strongly first-stage retrievers, we study a first-stage sparse retriever, namely SPLADE [5]."
  - [corpus] Weak - no direct supporting evidence found in neighbor papers.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is used as the pre-training objective to adapt Pdomain to the target domain, learning domain-specific representations.
  - Quick check question: What is the purpose of using MLM during pre-training in this domain adaptation approach?

- Concept: Parameter-efficient fine-tuning
  - Why needed here: Understanding the concept of freezing certain parameters while fine-tuning others is crucial for grasping the disentanglement of domain-specific and task-specific parameters.
  - Quick check question: How does freezing Ptask during pre-training on the target domain allow for domain adaptation without labeled data?

- Concept: Sparse retrieval vs. dense retrieval
  - Why needed here: The paper focuses on adapting a sparse retriever (SPLADE), so understanding the differences between sparse and dense retrieval is important for context.
  - Quick check question: What are the key differences between sparse and dense retrieval methods, and why might sparse retrievers be more sensitive to domain shifts?

## Architecture Onboarding

- Component map: BERT-base pre-trained on English Wikipedia -> Pdomain (embeddings + first k transformer layers) -> Ptask (remaining transformer layers) -> Pre-training on source domain -> Pre-training on target domain -> Fine-tuning on source domain -> Evaluation on target domain

- Critical path:
  1. Pre-train Pdomain on source domain with MLM (keeping Ptask frozen)
  2. Pre-train Pdomain on target domain with MLM (keeping Ptask frozen)
  3. Fine-tune Ptask on source domain for IR task (keeping Pdomain frozen)
  4. Combine Pdomain and Ptask for evaluation on target domain

- Design tradeoffs:
  - Number of layers k to include in Pdomain vs. Ptask
  - Pre-training time on source vs. target domain
  - Computational cost of pre-training vs. potential gains in adaptation

- Failure signatures:
  - Poor performance on target domain indicates Pdomain not well adapted
  - Overfitting to source domain indicates Ptask not generalizable
  - High computational cost with minimal gains indicates inefficient parameter allocation

- First 3 experiments:
  1. Ablation study: Compare performance with and without source pre-training
  2. Layer study: Vary k (number of layers in Pdomain) and measure impact on performance
  3. Baseline comparison: Compare against BM25 and zero-shot SPLADE baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of transformer layers allocated to domain-specific learning affect retrieval performance in cross-domain adaptation?
- Basis in paper: [explicit] The authors experiment with different numbers of layers (k) allocated to domain-specific learning and observe that pre-training over additional layers can provide gains of up to 0.5 points in nDCG@10.
- Why unresolved: The optimal number of layers for domain-specific learning may vary depending on the specific domains and tasks involved.
- What evidence would resolve it: Conducting further experiments with a wider range of layer allocations and different domain-task pairs could help determine the optimal number of layers for various scenarios.

### Open Question 2
- Question: Can parameter-efficient fine-tuning (PEFT) methods be effectively combined with the proposed domain adaptation approach?
- Basis in paper: [explicit] The authors mention that using PEFT methods following [31] is an alternative they plan to explore in the future.
- Why unresolved: The authors have not yet explored the combination of PEFT and their domain adaptation method, leaving the potential benefits and challenges of such a combination unclear.
- What evidence would resolve it: Conducting experiments that apply PEFT methods in conjunction with the proposed domain adaptation approach would provide insights into their effectiveness and potential synergies.

### Open Question 3
- Question: How does the sparsity of SPLADE models impact their performance in cross-domain adaptation?
- Basis in paper: [explicit] The authors observe that the best models usually have a higher sparsity level, suggesting a relationship between sparsity and performance.
- Why unresolved: The authors note that more work is needed to understand why some variants with lower sparsity values perform better than the baseline.
- What evidence would resolve it: Conducting a detailed analysis of the relationship between sparsity and performance, including the impact of different sparsity levels on various domain-task pairs, would provide a clearer understanding of this relationship.

## Limitations

- Focuses exclusively on sparse retrievers, limiting generalizability to dense retrieval methods
- Relies on SPLADE as a single architecture, making it unclear if findings extend to other retrievers
- The analysis of which parameters constitute domain-specific vs task-specific knowledge remains somewhat heuristic

## Confidence

- High confidence in the experimental methodology and implementation details
- Medium confidence in the mechanism explaining why domain-task disentanglement works
- Medium confidence in the claim that first-stage retrievers are more sensitive to domain shifts than second-stage rankers

## Next Checks

1. Test the domain-task disentanglement approach on dense retrievers like DPR or ColBERT to verify if the method transfers across retrieval paradigms
2. Conduct an ablation study varying the number of layers in Pdomain to identify optimal parameter allocation for different domain shifts
3. Evaluate the approach on synthetic domain shifts (controlled perturbations of source data) to better understand failure conditions and the limits of unlabeled pre-training