---
ver: rpa2
title: A Unified Graph Selective Prompt Learning for Graph Neural Networks
arxiv_id: '2406.10498'
source_url: https://arxiv.org/abs/2406.10498
tags:
- graph
- prompt
- learning
- node
- gspf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces GSPF, a unified graph selective prompt feature
  learning method for fine-tuning graph neural networks. The method addresses limitations
  in existing graph prompt learning approaches that focus solely on node prompts or
  apply prompts uniformly across all nodes and edges.
---

# A Unified Graph Selective Prompt Learning for Graph Neural Networks

## Quick Facts
- arXiv ID: 2406.10498
- Source URL: https://arxiv.org/abs/2406.10498
- Authors: Bo Jiang; Hao Wu; Ziyan Zhang; Beibei Wang; Jin Tang
- Reference count: 40
- Primary result: GSPF achieves 75.41% average ROC-AUC on molecular datasets, outperforming traditional fine-tuning and other prompt learning methods

## Executive Summary
This paper introduces GSPF, a unified graph selective prompt feature learning method for fine-tuning graph neural networks. GSPF addresses limitations in existing graph prompt learning approaches by integrating both node and edge prompts and selectively applying them based on the importance of each node and edge. The method demonstrates improved performance, parameter efficiency, and robustness, particularly with limited training data, achieving an average ROC-AUC score of 75.41% on molecular datasets.

## Method Summary
GSPF is a parameter-efficient fine-tuning method that integrates selective node and edge prompts into pre-trained graph neural networks. The approach computes learnable importance scores for nodes and edges, then applies prompts selectively based on these scores. Node prompts modify feature vectors through cross-attention with learnable basis vectors, while edge prompts adjust adjacency weights using attention coefficients. The method maintains the pre-trained GNN parameters while tuning only the prompt parameters, resulting in significantly fewer trainable parameters compared to full fine-tuning.

## Key Results
- GSPF achieves an average ROC-AUC score of 75.41% across multiple molecular datasets
- The method uses less than 1% of the parameters required for full fine-tuning while maintaining superior performance
- GSPF demonstrates improved robustness with limited training data, consistently outperforming traditional fine-tuning and other prompt learning methods

## Why This Works (Mechanism)

### Mechanism 1
Selective node prompting improves robustness by weighting node contributions based on their importance. The method computes a learnable importance score $r_i = \sigma(f_{\text{mlp}}(x_i, \Omega))$ for each node and modulates the prompt vector by this score in Eq. (8), thereby downweighting noisy or irrelevant nodes during fine-tuning. Core assumption: Node importance correlates with downstream task relevance, and the MLP can learn this mapping from node features alone.

### Mechanism 2
Joint node and edge prompting provides a unified graph representation that captures richer relational information. Node prompts modify feature vectors while edge prompts adjust adjacency weights via learnable attention coefficients $\tau_{ij}$ and prompt weights $t_{ij}$ in Eq. (10-12), allowing both node-level and edge-level fine-tuning without altering the pre-trained GNN weights. Core assumption: Edge-level relational adjustments complement node feature enrichment and together reduce the pre-training-to-downstream gap.

### Mechanism 3
Parameter-efficient fine-tuning via selective prompts yields better generalization with limited data. GSPF adds far fewer tunable parameters (9.3-15k) than full fine-tuning (~1.8M) while achieving higher ROC-AUC, thereby reducing overfitting risk. Core assumption: The small number of prompt parameters is sufficient to align the pre-trained model to the downstream task.

## Foundational Learning

- Graph Neural Networks
  - Why needed here: GSPF builds on GNNs by fixing their parameters and adding learnable prompts; understanding message passing and aggregation is essential
  - Quick check question: In a GNN, how is a node's new representation computed from its neighbors?

- Prompt Learning in CV/NLP
  - Why needed here: The design of node and edge prompts is inspired by prompt tuning in vision and language; knowledge of how continuous prompts work is useful
  - Quick check question: What is the difference between prompt tuning and full fine-tuning in large pretrained models?

- Graph Contrastive Learning
  - Why needed here: The paper uses datasets pre-trained via contrastive methods (GraphCL, etc.); understanding these pre-training objectives helps interpret results
  - Quick check question: What is the main goal of contrastive learning in graph pre-training?

## Architecture Onboarding

- Component map: Input graph $(A, X)$ -> Selective node prompt module -> Selective edge prompt module -> Modified graph $(\hat{A}, \hat{X})$ -> Fixed pre-trained GNN -> Task-specific head (MLP classifier)

- Critical path:
  1. Compute $r_i$ for all nodes
  2. Generate selective node prompts $\hat{p}_i$
  3. Apply selective edge prompts to $A$
  4. Forward through fixed GNN to get embeddings
  5. Pass through task head and compute loss

- Design tradeoffs:
  - Node prompt: cross-attention vs. direct assignment; importance weighting vs. uniform
  - Edge prompt: learnable attention vs. static weighting; per-layer vs. first-layer only
  - Parameter count vs. expressiveness

- Failure signatures:
  - Training instability: prompts exploding or gradients vanishing
  - Overfitting: high training ROC-AUC but low test ROC-AUC
  - Underfitting: prompts too weak to align model

- First 3 experiments:
  1. Reproduce Table I ROC-AUC on BBBP with Infomax pre-training to verify core claims
  2. Ablation study: remove node prompt (NP) or edge prompt (EP) to measure individual contributions
  3. Data scaling test: vary training set size on SIDER/ToxCast to confirm robustness advantage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the selective application of prompts to important nodes and edges impact the robustness of GSPF to noisy or redundant graph data?
- Basis in paper: [explicit] The paper states that GSPF conducts prompt learning selectively on nodes and edges by concentrating on the important ones, which makes the model more reliable and compact
- Why unresolved: While the paper claims improved robustness, it does not provide a detailed analysis of how GSPF performs specifically when dealing with noisy or redundant data
- What evidence would resolve it: Experimental results comparing GSPF's performance on datasets with varying levels of noise or redundancy, alongside ablation studies isolating the impact of selective prompting

### Open Question 2
- Question: What is the optimal number of prompt vector counts for GSPF, and how does this parameter affect the model's performance and efficiency?
- Basis in paper: [explicit] The paper mentions that the number of prompt vector counts is selected from {1, 5, 10, 20} during experiments
- Why unresolved: The paper does not provide a thorough analysis of how different prompt vector counts impact the model's performance, parameter efficiency, or computational cost
- What evidence would resolve it: A comprehensive study varying the number of prompt vector counts, evaluating performance metrics, parameter usage, and computational efficiency across different datasets

### Open Question 3
- Question: How does the layer-wise application of edge prompts in GSPF affect the model's ability to capture complex graph patterns compared to applying edge prompts only at the first GNN layer?
- Basis in paper: [explicit] The paper discusses the impact of applying edge prompts at different layer levels (shallow vs. deep) and shows that deep prompting outperforms shallow prompting
- Why unresolved: While the paper demonstrates the superiority of deep prompting, it does not provide a detailed analysis of why this is the case or explore the trade-offs between the two approaches
- What evidence would resolve it: A thorough investigation into the learned edge prompt weights at different layers, analyzing how they contribute to the model's ability to capture complex graph patterns, and exploring the computational cost of deep vs. shallow prompting

## Limitations
- The paper's effectiveness depends heavily on the assumption that node importance can be accurately learned via an MLP from node features alone, which is not empirically validated
- The selective edge prompting mechanism's attention coefficients may overfit when applied to small molecular datasets
- The computational overhead of computing importance scores for every node in large graphs is not discussed

## Confidence

- Mechanism 1 (Selective node prompting): Medium - The theoretical foundation is sound, but the empirical validation of importance scoring is limited
- Mechanism 2 (Joint node and edge prompting): High - The approach follows established graph prompting literature with clear mathematical formulation
- Mechanism 3 (Parameter efficiency): High - The parameter counts and ROC-AUC comparisons are clearly presented and significant

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of node vs edge prompts and their selective application
2. Test GSPF on non-molecular graph datasets (e.g., social or citation networks) to evaluate generalizability beyond chemistry
3. Analyze the learned importance scores across different node types to verify that the MLP captures meaningful signals rather than noise