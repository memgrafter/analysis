---
ver: rpa2
title: Dialect prejudice predicts AI decisions about people's character, employability,
  and criminality
arxiv_id: '2403.00742'
source_url: https://arxiv.org/abs/2403.00742
tags:
- language
- stereotypes
- covert
- about
- adjectives
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that large language models exhibit covert
  racial stereotypes against speakers of African American English (AAE), despite not
  overtly expressing racial bias. Using a novel Matched Guise Probing method, researchers
  found that language models associate AAE speakers with negative stereotypes more
  strongly than any human stereotypes ever recorded, even more negative than pre-civil
  rights era attitudes.
---

# Dialect prejudice predicts AI decisions about people's character, employability, and criminality

## Quick Facts
- arXiv ID: 2403.00742
- Source URL: https://arxiv.org/abs/2403.00742
- Reference count: 26
- Large language models exhibit covert racial stereotypes against AAE speakers more negative than any human stereotypes ever recorded

## Executive Summary
This study reveals that large language models encode covert racial stereotypes against African American English (AAE) speakers that are more negative than any human stereotypes ever experimentally recorded. Using a novel Matched Guise Probing method, researchers found that models associate AAE speakers with harmful stereotypes even when explicitly denying racial prejudice. These covert biases influence harmful decisions in employment and criminal justice contexts, with models assigning lower-prestige jobs and harsher sentences to AAE speakers. Critically, scaling model size and human feedback training do not mitigate these biases but may actually exacerbate the discrepancy between overt and covert prejudice.

## Method Summary
The researchers developed a Matched Guise Probing method to detect covert racial stereotypes in language models by comparing model responses to African American English (AAE) versus Standard American English (SAE) texts that are meaning-matched. They embedded AAE and SAE texts from Twitter datasets in prompts and computed probabilities for trait adjectives from the Princeton Trilogy studies, occupations, and criminal justice outcomes. Association scores were calculated by comparing token probabilities between AAE and SAE variants, then calibrated to measure stereotype strength. The method was applied across multiple language models of varying sizes and training approaches, including GPT2, RoBERTa, T5, GPT3.5, and GPT4.

## Key Results
- Language models exhibit covert stereotypes about AAE speakers more negative than any human stereotypes ever recorded, including pre-civil rights era attitudes
- Larger language models show stronger covert prejudice despite being better at avoiding overt racial bias
- Human feedback training reduces overt racism but exacerbates the gap between overt positive attitudes and covert negative dialect prejudice
- AAE speakers are assigned lower-prestige occupations and receive harsher criminal justice outcomes in model predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models encode covert racial stereotypes that are activated by dialect features alone, without explicit mention of race.
- Mechanism: When presented with texts in African American English (AAE), the models map linguistic features (e.g., habitual "be", "finna", absence of copula) to historically negative stereotypes about African Americans that exist in the training data. These stereotypes are more negative than any human stereotypes ever recorded, including those from pre-civil-rights era studies.
- Core assumption: Training corpora contain raciolinguistic stereotypes encoded in dialect use, and language models learn these associations through unsupervised pretraining.
- Evidence anchors:
  - [abstract] "language models embody covert racism in the form of dialect prejudice... exhibiting covert stereotypes that are more negative than any human stereotypes about African Americans ever experimentally recorded"
  - [section] "speakers of AAE are known to experience racial discrimination... researchers have found that landlords can engage in housing discrimination based solely on the auditory profiles of speakers"
  - [corpus] Weak; no direct corpus examples provided in the paper.

### Mechanism 2
- Claim: Human feedback training (HFT) does not mitigate covert stereotypes but instead increases the discrepancy between overt and covert prejudice.
- Mechanism: HFT teaches models to suppress overt racist language when directly asked about race, while leaving the deeper-level dialect-prejudice associations intact. This creates a situation where models express positive overt stereotypes about African Americans but maintain more negative covert stereotypes about AAE speakers.
- Core assumption: Human feedback is applied only to overt language and does not address subtle, dialect-based associations.
- Evidence anchors:
  - [abstract] "human feedback training... can exacerbate the discrepancy between covert and overt stereotypes, by teaching language models to superficially conceal the racism that they maintain on a deeper level"
  - [section] "human feedback results in more positive overt associations but has no clear qualitative effect on the covert associations"
  - [corpus] Weak; no direct corpus examples provided in the paper.

### Mechanism 3
- Claim: Scaling model size increases both the models' ability to understand AAE and the strength of their covert dialect prejudice.
- Mechanism: Larger models have better language modeling capabilities and thus better capture dialectal features, but this also means they learn and amplify the stereotypes associated with those features more strongly. This results in a higher perplexity for AAE texts but stronger negative associations when probed.
- Core assumption: Larger models have greater capacity to encode and reproduce subtle biases present in the training data.
- Evidence anchors:
  - [abstract] "scaling model size and human feedback training do not mitigate these biases but may actually exacerbate the discrepancy between overt and covert prejudice"
  - [section] "larger models show more covert prejudice than smaller models... increasing scale does make models better at understanding AAE and at avoiding prejudice against overt mentions of African Americans, but makes them more linguistically prejudiced"
  - [corpus] Weak; no direct corpus examples provided in the paper.

## Foundational Learning

- Concept: Dialect prejudice
  - Why needed here: Understanding that the bias operates through linguistic features rather than explicit racial labels is key to diagnosing and addressing it.
  - Quick check question: Can you explain why "dialect prejudice" is considered a form of "covert racism"?

- Concept: Matched Guise Probing
  - Why needed here: This method is the primary tool for uncovering covert stereotypes by comparing model behavior on AAE vs. SAE texts matched for meaning.
  - Quick check question: How does Matched Guise Probing differ from asking a model directly about race?

- Concept: Calibration of association scores
  - Why needed here: Proper calibration ensures that measured differences in model behavior are due to dialect features and not to baseline token probabilities.
  - Quick check question: Why is it important to normalize token probabilities when comparing AAE vs. SAE texts?

## Architecture Onboarding

- Component map: Data ingestion -> Text preprocessing -> Prompt embedding -> Language model forward pass -> Probability computation -> Association score calculation -> Statistical analysis -> Result output
- Critical path:
  1. Load AAE/SAE text pairs
  2. Embed in prompts
  3. Run language model to get probabilities
  4. Compute association scores (q(x; v, Î¸))
  5. Aggregate and analyze results
  6. Compare to human stereotype data
- Design tradeoffs:
  - Meaning-matched vs. non-meaning-matched settings (rigor vs. realism)
  - Number and variety of prompts (stability vs. coverage)
  - Model selection (size, training method, architecture)
- Failure signatures:
  - High variance in association scores across prompts
  - No correlation between AAE density and stereotype strength
  - Lack of statistical significance in decision bias experiments
- First 3 experiments:
  1. Verify that association scores are consistently negative for stereotypical adjectives in AAE vs. SAE texts.
  2. Test whether stereotype strength increases with AAE feature density using synthetic data.
  3. Check for discrepancy between overt and covert stereotypes in a model trained with human feedback.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing model size continue to reduce overt racial stereotypes while simultaneously increasing covert dialect prejudice?
- Basis in paper: [explicit] The paper shows that larger models exhibit less overt prejudice but more covert prejudice against AAE speakers
- Why unresolved: The study examined specific model sizes but did not test whether this trend continues indefinitely as models scale further
- What evidence would resolve it: Testing progressively larger language models on both overt and covert racial bias measures would reveal if this divergence continues or plateaus

### Open Question 2
- Question: Are there effective methods to reduce covert dialect prejudice without teaching models to superficially conceal deeper racist attitudes?
- Basis in paper: [explicit] The paper found that human feedback training exacerbates the discrepancy between overt and covert stereotypes
- Why unresolved: Current bias mitigation techniques appear to make the problem worse by teaching models to hide racism rather than eliminating it
- What evidence would resolve it: Developing and testing new bias mitigation approaches specifically designed to address covert dialect prejudice, measuring both surface-level and deeper biases

### Open Question 3
- Question: How do different linguistic features of AAE individually contribute to the severity of covert stereotypes in language models?
- Basis in paper: [explicit] The paper analyzed eight AAE features but did not provide a comprehensive ranking of their relative impact
- Why unresolved: While the study showed all examined features evoke stereotypes, it did not quantify which features have the strongest effect
- What evidence would resolve it: Systematic testing of additional AAE linguistic features with controlled experimental conditions to measure their individual stereotype-evoking strength

## Limitations

- The study lacks direct corpus evidence showing that training data actually contains the claimed raciolinguistic stereotypes, instead inferring their presence from model behavior
- The comparison to historical human stereotype data from the Princeton Trilogy studies may not fully represent contemporary attitudes
- The complete set of prompts used in Matched Guise Probing is not fully specified, making exact reproduction difficult

## Confidence

**High Confidence:** The finding that language models can discriminate between AAE and SAE texts based on linguistic features, and that this discrimination correlates with occupational prestige ratings. The statistical methods for computing association scores and the basic experimental design are well-specified and reproducible.

**Medium Confidence:** The claim that language models exhibit covert stereotypes more negative than any human stereotypes ever recorded. While the methodology is sound, the comparison to historical human stereotype data is indirect, and the calibration against such old studies (some from the 1930s) introduces uncertainty about modern relevance.

**Low Confidence:** The assertion that human feedback training exacerbates the discrepancy between covert and overt prejudice. This conclusion is based on comparisons across different model versions and training regimes, but the paper doesn't fully control for other variables that might differ between these models (e.g., architecture changes, pretraining data differences).

## Next Checks

1. **Prompt Replication Test:** Replicate the Matched Guise Probing experiments using the exact prompts provided for GPT4, then systematically vary prompt wording to test sensitivity. Measure how association scores change with prompt modifications to establish robustness.

2. **Corpus Evidence Collection:** Examine the training corpora of major language models to identify actual instances of dialect-prejudice associations. Use keyword searches and distributional analysis to find whether stereotypes about AAE speakers are present in the data, not just inferred from model outputs.

3. **Temporal Degradation Analysis:** Test whether the strength of covert stereotypes changes over time by evaluating models trained on different dates (GPT3.5 Nov 2022 vs. GPT4 Mar 2023 vs. Claude 2023 vs. Claude 2024). If human feedback is reducing overt racism but increasing covert-prejudice discrepancy, this should be measurable across model versions.