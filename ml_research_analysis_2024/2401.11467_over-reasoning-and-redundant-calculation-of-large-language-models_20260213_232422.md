---
ver: rpa2
title: Over-Reasoning and Redundant Calculation of Large Language Models
arxiv_id: '2401.11467'
source_url: https://arxiv.org/abs/2401.11467
tags:
- llms
- answer
- question
- answers
- gsm8k-zero
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether large language models (LLMs) generate
  unnecessary reasoning steps when answering math questions. The authors construct
  a dataset (GSM8K-Zero) where answers are directly stated in the question, requiring
  no calculations.
---

# Over-Reasoning and Redundant Calculation of Large Language Models

## Quick Facts
- arXiv ID: 2401.11467
- Source URL: https://arxiv.org/abs/2401.11467
- Reference count: 9
- Key outcome: Many RLHF-trained LLMs generate unnecessary reasoning steps and lengthy calculations even when answers are directly stated in math questions, leading to errors and low accuracy.

## Executive Summary
This paper investigates whether large language models (LLMs) generate unnecessary reasoning steps when answering math questions. The authors construct a dataset (GSM8K-Zero) where answers are directly stated in the question, requiring no calculations. Testing on seven RLHF-trained LLMs shows that many models still produce lengthy, redundant calculations, which can lead to errors. For example, Llama-2 models generate unnecessary reasoning in over 80% of cases, and some models achieve less than 50% accuracy despite the questions being simple. The study also finds that models like ChatGPT and GPT-4 tend to prefer longer answers during training, which may encourage redundant reasoning. When explicitly instructed to omit step-by-step reasoning, redundancy decreases significantly, but it does not disappear entirely.

## Method Summary
The authors construct the GSM8K-Zero dataset from GSM8K by removing the last sentence from questions and filtering using GPT-4 to ensure answers match ground truth. They test seven RLHF-trained LLMs on this dataset using zero-shot inference, measuring redundancy (percentage of answers containing mathematical operators) and accuracy (alignment with ground truth). To explore why LLMs generate redundant calculations, they use ChatGPT and GPT-4 as proxy reward models to compare preferences for long vs. short answers.

## Key Results
- Llama-2 models generate unnecessary reasoning in over 80% of cases on GSM8K-Zero.
- Some LLMs achieve less than 50% accuracy on questions with directly stated answers.
- ChatGPT and GPT-4 show a strong preference for longer answers during training.
- Explicit instructions to omit step-by-step reasoning reduce redundancy but do not eliminate it entirely.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs generate redundant calculations because they are trained to produce step-by-step reasoning even when unnecessary.
- Mechanism: During instruction tuning, LLMs are exposed to examples requiring CoT reasoning. This training causes them to apply CoT to all math-like questions, regardless of whether the answer is directly available.
- Core assumption: LLMs cannot distinguish between problems requiring calculation and those with explicit answers.
- Evidence anchors:
  - [abstract] "LLMs tend to generate lengthy and unnecessary calculations to answer the questions."
  - [section 3.1] "almost half of the LLMs we test have an accuracy lower than 50%... some LLMs still cannot perform well on it."
- Break condition: If LLMs are explicitly prompted to omit step-by-step reasoning when possible, redundancy decreases significantly.

### Mechanism 2
- Claim: Reward models (RMs) in RLHF prefer longer answers, leading to redundant outputs.
- Mechanism: Proxy RMs (ChatGPT and GPT-4) show a strong preference for longer answers even when they are incorrect, suggesting that RMs trained on such data will favor verbose outputs.
- Core assumption: Proxy RMs accurately reflect the preferences of actual RMs used in training.
- Evidence anchors:
  - [section 4] "both GPT-4 and ChatGPT prefer long answers" and "ChatGPT consistently prefers lengthy but wrong answers."
  - [section 4] "proxy RMs strongly prefer long outputs that contain redundant calculations and unnecessary reasoning, even if the final answer is wrong!"
- Break condition: If RMs are trained to value conciseness and correctness equally, redundancy would decrease.

### Mechanism 3
- Claim: LLMs' inability to follow instructions leads to solving unnecessary variables and producing incorrect answers.
- Mechanism: LLMs often solve all unknown variables in a problem rather than just the asked question, complicating responses and sometimes providing no answer at all.
- Core assumption: LLMs prioritize solving all variables over following explicit instructions.
- Evidence anchors:
  - [section 3.1] "LLMs solve all the unknown variables in the questions, which are not asked in the questions."
  - [section 3.1] "LLMs sometimes only provide the values of the unknown variables but do not answer the value asked in the question."
- Break condition: If LLMs are better trained to follow explicit instructions, they would avoid solving unnecessary variables.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: Understanding how CoT is used to boost LLM performance and why it might be applied unnecessarily.
  - Quick check question: What is the primary purpose of CoT in LLM reasoning tasks?
- Concept: Reinforcement Learning with Human Feedback (RLHF)
  - Why needed here: RLHF is used to train LLMs to follow instructions and produce preferred outputs; understanding its role explains why LLMs might favor longer answers.
  - Quick check question: How does RLHF influence the behavior of LLMs in terms of answer length and style?
- Concept: Regular expressions for answer extraction
  - Why needed here: Used to determine if an LLM's answer is redundant by checking for mathematical operators.
  - Quick check question: How can regular expressions be used to detect unnecessary calculations in LLM responses?

## Architecture Onboarding

- Component map: GSM8K-Zero dataset → LLM inference → Regular expression parsing → Redundancy and accuracy metrics
- Critical path: Question generation → LLM response → Answer extraction → Redundancy check → Accuracy calculation
- Design tradeoffs: Using regular expressions for answer extraction is fast but may miss edge cases; manual review is more accurate but time-consuming.
- Failure signatures: High redundancy with low accuracy indicates LLMs are over-reasoning; preference for long answers in RMs leads to verbose outputs.
- First 3 experiments:
  1. Test LLMs on GSM8K-Zero with and without explicit instructions to omit step-by-step reasoning.
  2. Use proxy RMs to compare preferences for long vs. short answers in correct and incorrect cases.
  3. Analyze LLM outputs for solving unnecessary variables and not following instructions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs that are not RLHF-trained also exhibit redundancy in their outputs when answering questions from GSM8K-Zero?
- Basis in paper: [inferred]
- Why unresolved: The paper only explicitly tests RLHF-trained models, mentioning that non-RLHF-trained models like Alpaca and Vicuna also show redundancy but not providing detailed results due to the "messiness" of their outputs.
- What evidence would resolve it: Conducting a detailed analysis of non-RLHF-trained models on GSM8K-Zero, focusing on their redundancy and accuracy, would provide a clearer understanding of whether this issue is specific to RLHF-trained models or a broader problem across different types of LLMs.

### Open Question 2
- Question: How does the redundancy in LLM outputs affect user perception and trust in the model's responses?
- Basis in paper: [inferred]
- Why unresolved: While the paper discusses the potential for redundant calculations to confuse users and lead them to falsely believe that questions are more complex than they are, it does not provide empirical evidence on how this affects user trust or perception.
- What evidence would resolve it: User studies evaluating the impact of redundant outputs on user trust, comprehension, and willingness to rely on LLM responses would provide valuable insights into the real-world implications of this issue.

### Open Question 3
- Question: Can the tendency of proxy reward models (RMs) to prefer lengthy answers be mitigated through alternative training strategies or reward functions?
- Basis in paper: [explicit]
- Why unresolved: The paper identifies that proxy RMs (using ChatGPT and GPT-4) prefer longer answers, even if incorrect, suggesting that this preference might stem from the training data or methodology. However, it does not explore potential solutions or alternative approaches to address this bias.
- What evidence would resolve it: Experimenting with different reward functions, training data curation, or alternative evaluation metrics for RMs could provide insights into how to reduce the preference for redundant, lengthy outputs and improve the quality of LLM responses.

## Limitations

- The study uses a dataset where answers are explicitly stated, which may not represent real-world scenarios requiring actual calculation.
- Regular expressions for detecting redundant calculations may miss edge cases or incorrectly flag necessary steps as redundant.
- Proxy reward models may not accurately reflect the preferences of actual reward models used in RLHF training.

## Confidence

- High confidence: The observation that LLMs generate unnecessary reasoning steps when answering math questions where answers are directly stated.
- Medium confidence: The claim that reward models in RLHF prefer longer answers, leading to redundant outputs, based on proxy model behavior.
- Medium confidence: The finding that explicit instructions to omit step-by-step reasoning reduce redundancy, but do not eliminate it entirely.

## Next Checks

1. Replicate the study using a larger and more diverse dataset of math problems to confirm whether LLMs consistently generate redundant calculations across different problem types.

2. Conduct experiments with actual reward models used in RLHF training to validate whether they indeed prefer longer answers, as suggested by proxy model behavior.

3. Investigate the impact of fine-tuning LLMs on datasets that emphasize conciseness and direct answering to determine if it reduces redundant reasoning in practice.