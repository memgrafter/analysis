---
ver: rpa2
title: Online Graph Filtering Over Expanding Graphs
arxiv_id: '2409.07204'
source_url: https://arxiv.org/abs/2409.07204
tags:
- filter
- online
- graph
- attachment
- stochastic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses online graph filtering over expanding graphs,
  where new nodes are sequentially added to an existing graph structure. The authors
  propose a framework to update graph filters dynamically as the graph grows, handling
  both deterministic and stochastic attachment scenarios of new nodes.
---

# Online Graph Filtering Over Expanding Graphs

## Quick Facts
- arXiv ID: 2409.07204
- Source URL: https://arxiv.org/abs/2409.07204
- Authors: Bishwadeep Das; Elvin Isufi
- Reference count: 40
- One-line primary result: Proposes online graph filtering methods for expanding graphs with improved prediction performance and regret bounds

## Executive Summary
This paper addresses the challenge of online graph filtering when graphs grow by sequentially adding new nodes. The authors propose a framework that dynamically updates graph filter parameters as the graph expands, handling both deterministic (known connectivity) and stochastic (unknown connectivity) attachment scenarios. Three methods are introduced: deterministic online graph filtering (D-OGF), stochastic online graph filtering (S-OGF), and adaptive stochastic online filtering (Ada-OGF) that learns combinations of multiple attachment models. The methods are evaluated on synthetic and real datasets, showing improved performance compared to baseline approaches while providing theoretical regret guarantees.

## Method Summary
The paper proposes an online graph filtering framework that updates filter parameters in real-time as new nodes arrive in an expanding graph. The approach uses projected online gradient descent to minimize a time-varying loss function that combines prediction error and regularization. For deterministic scenarios where attachment information is known, D-OGF directly incorporates the attachment vector into the loss. For stochastic scenarios where connectivity is unknown, S-OGF uses attachment probability distributions, while Ada-OGF additionally learns the combination of multiple stochastic attachment models through alternating gradient updates. The framework is evaluated on synthetic data and real-world applications including recommender systems and COVID case prediction, demonstrating improved performance over pre-trained filters and kernel-based methods.

## Key Results
- Ada-OGF outperforms baseline methods on synthetic data with unknown attachments, achieving lower NRMSE values
- Online methods show improved prediction accuracy compared to pre-trained filters and kernel-based approaches on real datasets
- Regret analysis demonstrates sub-linear regret bounds for the online filtering methods under stated assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The online graph filtering framework updates filter parameters dynamically as new nodes attach, handling both deterministic and stochastic attachment scenarios.
- Mechanism: The framework casts the inference problem as a time-varying loss function over the existing topology, data, and incoming node attachment, then updates filter parameters via online learning principles (projected online gradient descent).
- Core assumption: The loss functions are convex and differentiable, allowing gradient-based updates.
- Evidence anchors:
  - [abstract] states "we propose an online graph filtering framework by relying on online learning principles" and "design filters for scenarios where the topology is both known and unknown"
  - [section II.B] details the online filter update process using projected online gradient descent
  - [corpus] evidence is weak - no direct citations found in neighbors
- Break condition: If the loss functions become non-convex or non-differentiable, the gradient-based update would fail.

### Mechanism 2
- Claim: The regret analysis quantifies the performance gap between online updates and static batch solutions, showing sub-linear regret indicates learning convergence.
- Mechanism: The normalized regret measures how much better or worse the online algorithm performs compared to a fixed learner, with sub-linear regret indicating average regret tends to zero as sample size grows.
- Core assumption: Assumptions 1-4 hold regarding edge formation, attachment bounds, filter energy bounds, and residual bounds.
- Evidence anchors:
  - [abstract] mentions "conduct a regret analysis to highlight the role played by the different components"
  - [section III] provides Proposition 1 with regret bound showing sub-linear behavior
  - [corpus] shows no direct citations of regret analysis in neighbors
- Break condition: If assumptions about bounded residuals or filter energies are violated, the regret bound would not hold.

### Mechanism 3
- Claim: The adaptive stochastic online filtering method learns both filter parameters and the combination of multiple stochastic attachment models, improving performance when single models are insufficient.
- Mechanism: Uses linear combination of different attachment models parameterized by probability vectors and weights, updating combination parameters along with filter coefficients via alternating gradient descent.
- Core assumption: The loss function is jointly non-convex but can be optimized via alternating updates.
- Evidence anchors:
  - [section IV.B] describes the adaptive stochastic approach and its gradient updates
  - [abstract] mentions "a learner adaptive to such evolution" and "learn the combination of multiple stochastic attachment models"
  - [corpus] no direct citations found for adaptive combination approaches
- Break condition: If the loss function becomes too non-convex or the alternating updates diverge, the adaptive approach would fail.

## Foundational Learning

- Concept: Online learning and regret analysis
  - Why needed here: The framework needs to update filters in real-time as new nodes arrive, and regret analysis provides theoretical performance guarantees
  - Quick check question: What does sub-linear regret imply about the learning algorithm's performance?

- Concept: Graph signal processing and graph filters
  - Why needed here: The framework processes signals defined over nodes through weighted combinations of successive shifts between neighbors
  - Quick check question: How does a graph filter differ from traditional time-domain filters?

- Concept: Stochastic attachment models in network science
  - Why needed here: When node connectivity is unknown, statistical models are used to infer attachment patterns
  - Quick check question: What are the key differences between deterministic and stochastic attachment scenarios?

## Architecture Onboarding

- Component map: Starting graph G0 with initial nodes and edges -> incoming node stream with signals and attachments -> filter update mechanism -> loss function computation -> regret analysis module
- Critical path: Incoming node arrives -> attachment information obtained (deterministic/stochastic) -> filter prediction made -> true signal revealed -> loss computed -> filter parameters updated
- Design tradeoffs: Deterministic vs stochastic attachment handling, filter order selection, learning rate and regularization parameter tuning, single vs ensemble attachment models
- Failure signatures: High regret values indicating poor performance, prediction errors exceeding thresholds, filter parameters diverging or becoming unstable
- First 3 experiments:
  1. Test D-OGF on synthetic data with known attachments to verify basic functionality
  2. Compare S-OGF and Ada-OGF on data with unknown attachments to validate stochastic handling
  3. Run PC-OGF on real recommender system data to test correction step effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed online graph filtering methods perform when the incoming node signals are non-stationary over time?
- Basis in paper: [explicit] The paper mentions non-stationary environments as a consideration in the introduction and discusses adapting to the sequence of incoming nodes, but does not provide extensive empirical analysis of non-stationary signal behavior.
- Why unresolved: The numerical experiments primarily use synthetic data with stationary properties (band-limited signals, weighted means, or Gaussian kernels). Real-world datasets like MovieLens and COVID cases, while not stationary, are not explicitly analyzed for time-varying signal characteristics.
- What evidence would resolve it: Experiments with synthetic data where the signal properties change over time (e.g., frequency content evolves) and analysis of the filter performance degradation or adaptation speed would provide insights into non-stationary behavior.

### Open Question 2
- Question: How does the regret bound change when the expanding graph follows a preferential attachment model instead of uniformly at random attachment?
- Basis in paper: [explicit] The paper analyzes regret bounds for uniformly at random attachment (Corollary 1) and mentions that the regret depends on the attachment probabilities, but does not explore preferential attachment models explicitly.
- Why unresolved: The analysis focuses on uniform attachment and does not provide theoretical or empirical results for other attachment models like preferential attachment, which is common in real-world networks.
- What evidence would resolve it: Deriving regret bounds for preferential attachment models and comparing them with the uniform case through experiments would clarify the impact of different attachment mechanisms on filter performance.

### Open Question 3
- Question: Can the online graph filtering framework be extended to handle multi-hop connectivity patterns for the incoming nodes, similar to the OMHKL baseline?
- Basis in paper: [inferred] The paper discusses online filtering for incoming nodes with single-hop connectivity, and mentions that the analysis can be extended to multiple nodes arriving at a time. The OMHKL baseline considers multi-hop patterns, suggesting this as a potential extension.
- Why unresolved: The proposed methods focus on immediate neighbor connections for the incoming nodes, and the regret analysis assumes single-hop attachments. There is no discussion of incorporating multi-hop information into the online filtering framework.
- What evidence would resolve it: Developing an extension of the online filtering methods to incorporate multi-hop connectivity patterns and comparing the performance with the single-hop approach on relevant datasets would address this question.

## Limitations
- Theoretical guarantees rely on assumptions about edge formation, attachment bounds, and filter properties that may not hold in all real-world scenarios
- Experimental validation is primarily on synthetic data and two real-world applications, limiting generalizability
- Computational complexity of maintaining and updating multiple attachment models in Ada-OGF is not thoroughly analyzed

## Confidence

**Confidence Levels:**
- High confidence: The basic online filtering framework and its application to deterministic attachment scenarios
- Medium confidence: Regret analysis and theoretical guarantees, contingent on assumptions holding
- Medium confidence: Stochastic attachment handling and adaptive methods, based on experimental results but limited theoretical backing

## Next Checks

1. **Robustness testing**: Evaluate performance when assumptions 1-4 are violated by introducing noise or adversarial attachment patterns to test regret bound validity.

2. **Scalability analysis**: Measure computational complexity and memory requirements of Ada-OGF as graph size grows, particularly the cost of maintaining multiple attachment models.

3. **Cross-domain generalization**: Test the framework on additional real-world datasets beyond recommender systems and COVID prediction, including heterophilic graphs where node attributes don't correlate with connectivity.