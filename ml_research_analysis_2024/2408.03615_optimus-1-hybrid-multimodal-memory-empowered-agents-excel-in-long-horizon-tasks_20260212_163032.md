---
ver: rpa2
title: 'Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon
  Tasks'
arxiv_id: '2408.03615'
source_url: https://arxiv.org/abs/2408.03615
tags:
- craft
- task
- optimus-1
- multimodal
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling AI agents to complete
  long-horizon tasks in open-world environments like Minecraft. The authors propose
  a Hybrid Multimodal Memory module consisting of a Hierarchical Directed Knowledge
  Graph (HDKG) and an Abstracted Multimodal Experience Pool (AMEP).
---

# Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks

## Quick Facts
- arXiv ID: 2408.03615
- Source URL: https://arxiv.org/abs/2408.03615
- Authors: Zaijing Li; Yuquan Xie; Rui Shao; Gongwei Chen; Dongmei Jiang; Liqiang Nie
- Reference count: 40
- One-line primary result: Optimus-1 achieves up to 30% improvement in success rates on Minecraft long-horizon tasks through hybrid multimodal memory

## Executive Summary
This paper addresses the challenge of enabling AI agents to complete long-horizon tasks in open-world environments like Minecraft. The authors propose a Hybrid Multimodal Memory module consisting of a Hierarchical Directed Knowledge Graph (HDKG) and an Abstracted Multimodal Experience Pool (AMEP). HDKG encodes structured knowledge as a directed graph, allowing efficient retrieval of required materials and steps for tasks. AMEP stores summarized multimodal historical experiences to provide rich references for in-context learning. On top of this memory module, the Optimus-1 agent is constructed with a Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. Experimental results on a benchmark of 67 Minecraft tasks show that Optimus-1 significantly outperforms existing agents, with up to 30% improvement in success rates.

## Method Summary
Optimus-1 uses a Hybrid Multimodal Memory system combining HDKG for structured knowledge representation and AMEP for multimodal experience storage. The Knowledge-Guided Planner uses MLLM to generate sub-goal sequences by retrieving from HDKG, while the Experience-Driven Reflector periodically assesses progress by retrieving relevant experiences from AMEP. The Action Controller (STEVE-1) executes the sub-goals step-by-step. The system employs non-parametric learning through "free exploration-teacher guidance" cycles, where the agent explores freely, stores experiences, and receives guidance to improve performance. The approach is evaluated on 67 Minecraft tasks across 7 categories using metrics including success rate, average steps, and average time.

## Key Results
- Up to 30% improvement in success rates compared to existing agents on Minecraft benchmark
- Strong generalization capabilities when using different MLLMs (GPT-4V, DeepSeek-VL, InternLM-XComposer2-VL) as backbones
- Demonstrates self-evolution through incremental performance improvement via free exploration and teacher guidance learning
- Achieves near-human performance on many tasks in the benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical Directed Knowledge Graph (HDKG) enables one-step planning by providing explicit world knowledge.
- Mechanism: HDKG transforms crafting and mining knowledge into a directed graph structure where nodes represent objects and edges point to materials that can be crafted by those objects. This allows retrieval of all necessary materials and their relationships for any given task through topological sorting.
- Core assumption: Structured knowledge about object relationships can be effectively encoded as a directed graph that captures all necessary crafting dependencies.
- Evidence anchors:
  - [abstract] "transforms knowledge into Hierarchical Directed Knowledge Graph that allows agents to explicitly represent and learn world knowledge"
  - [section 2.1.2] "For a given object x, retrieving the corresponding node allows extraction of a sub-graph Dj(Vj, Ej) ∈ D, where nodes set Vj and edges set Ej can be formulated as: Vj = {v ∈ V | x} , Ej = {e = (u, v) ∈ V | u ∈ V j ∪ v ∈ V j}"
  - [corpus] Weak evidence - no direct citations found for HDKG concept specifically
- Break condition: If crafting relationships are non-hierarchical or require conditional logic not captured by directed edges, the graph structure becomes insufficient.

### Mechanism 2
- Claim: Abstracted Multimodal Experience Pool (AMEP) improves reflection by storing summarized multimodal historical experiences.
- Mechanism: AMEP dynamically summarizes video frames, agent states, task plans, and environment information during task execution. It filters visual information through video and image buffers, computes multimodal correlations using MineCLIP, and stores abstracted frames with corresponding textual sub-goals when correlation exceeds threshold.
- Core assumption: Multimodal historical experiences can be effectively summarized without losing critical information needed for reflection.
- Evidence anchors:
  - [abstract] "summarises historical information into Abstracted Multimodal Experience Pool that provide agents with rich references for in-context learning"
  - [section 2.1.1] "to conduct the static visual information abstraction, the video stream captured by Optimus-1 during task execution is first input to a video buffer, filtering the stream at a fixed frequency of 1 frame per second"
  - [corpus] Weak evidence - no direct citations found for AMEP concept specifically
- Break condition: If critical reflection information is lost during summarization, the experience pool becomes insufficient for effective reflection.

### Mechanism 3
- Claim: Experience-Driven Reflector uses multimodal experience retrieval to assess task completion status and trigger replanning.
- Mechanism: The reflector periodically activates, retrieves relevant multimodal experiences from AMEP, and analyzes the current state. It generates reflection results categorized as COMPLETE, CONTINUE, or REPLAN, where REPLAN triggers Knowledge-Guided Planner to revise the plan.
- Core assumption: Historical experiences can provide sufficient context to accurately assess current task progress and determine appropriate actions.
- Evidence anchors:
  - [abstract] "summarises historical information into Abstracted Multimodal Experience Pool that provide agents with rich references for in-context learning"
  - [section 2.2] "During task execution, the Experience-Driven Reflector activates at regular intervals, retrieving historical experience from AMEP, and then analyzing the current state of Optimus-1"
  - [corpus] Weak evidence - no direct citations found for this specific reflection mechanism
- Break condition: If retrieved experiences are not sufficiently similar to current situation, reflection judgments become unreliable.

## Foundational Learning

- Concept: Directed graph representation and topological sorting
  - Why needed here: HDKG requires understanding how to represent object relationships as directed graphs and extract sub-graphs through topological sorting to determine crafting sequences
  - Quick check question: Given a crafting dependency graph, can you perform topological sorting to determine the correct sequence of crafting steps?

- Concept: Multimodal information summarization and correlation
  - Why needed here: AMEP requires understanding how to summarize visual information through filtering and buffer management, and compute correlations between visual and textual data
  - Quick check question: Given a sequence of video frames, can you implement a similarity-based filtering mechanism that maintains critical visual information while reducing redundancy?

- Concept: In-context learning from both success and failure cases
  - Why needed here: AMEP stores both successful and failed cases, requiring understanding of how to use failure cases as effective learning references during reflection
  - Quick check question: Can you explain how failure cases might provide different or more valuable information than success cases for learning complex task sequences?

## Architecture Onboarding

- Component map:
  - Hybrid Multimodal Memory (HDKG + AMEP)
  - Knowledge-Guided Planner (MLLM with HDKG retrieval)
  - Experience-Driven Reflector (MLLM with AMEP retrieval)
  - Action Controller (STEVE-1 or similar)
  - Minecraft environment interface

- Critical path:
  1. Knowledge-Guided Planner receives task and observation
  2. Planner retrieves knowledge from HDKG and generates sub-goal sequence
  3. Action Controller executes sub-goals step-by-step
  4. Experience-Driven Reflector periodically activates
  5. Reflector retrieves experiences from AMEP and assesses progress
  6. If REPLAN needed, return to step 2 with revised plan

- Design tradeoffs:
  - HDKG vs implicit knowledge learning: HDKG provides explicit knowledge without parameter updates but requires manual knowledge encoding
  - AMEP vs raw storage: AMEP improves efficiency through summarization but risks losing critical information
  - Periodic vs continuous reflection: Periodic reflection reduces overhead but may miss critical moments

- Failure signatures:
  - HDKG failure: Agent generates plans requiring impossible crafting sequences or missing prerequisite materials
  - AMEP failure: Agent cannot retrieve relevant experiences or retrieves irrelevant experiences leading to incorrect reflection
  - Integration failure: Knowledge and experience components conflict, causing agent to oscillate between planning and reflection

- First 3 experiments:
  1. Test HDKG with simple crafting tasks: Validate that topological sorting correctly generates crafting sequences for basic items like wooden pickaxes
  2. Test AMEP with task execution: Validate that experience summarization preserves critical information by comparing reflection quality with and without summarization
  3. Test end-to-end with Knowledge-Guided Planner: Validate that one-step planning with HDKG produces more efficient plans than iterative planning approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Optimus-1 change when using different backbone MLLMs like Deepseek-VL or InternLM-XComposer2-VL compared to GPT-4V?
- Basis in paper: [explicit] The paper states "Experimental results show that Optimus-1 exhibits strong generalization with the help of the Hybrid Multimodal Memory module, outperforming the GPT-4V baseline on various tasks."
- Why unresolved: While the paper mentions performance improvements with different MLLMs, it doesn't provide a detailed comparison of their performance metrics.
- What evidence would resolve it: Detailed quantitative results comparing the performance of Optimus-1 with different MLLM backbones on the same set of tasks.

### Open Question 2
- Question: What are the specific limitations of the current Action Controller (STEVE-1) that prevent Optimus-1 from completing more challenging tasks like "beat ender dragon" or "build a house"?
- Basis in paper: [inferred] The paper mentions "limited by STEVE-1's ability to follow instructions and execute complex actions, Optimus-1 is weak in completing challenging tasks such as 'beat ender dragon' and 'build a house'."
- Why unresolved: The paper identifies the limitation but doesn't delve into the specific reasons why STEVE-1 struggles with these tasks.
- What evidence would resolve it: Analysis of specific failure cases where STEVE-1 fails on complex tasks, along with potential improvements to the Action Controller.

### Open Question 3
- Question: How does the self-evolution process of Optimus-1 work in practice, and what are the specific improvements observed after each iteration of "free exploration-teacher guidance" learning?
- Basis in paper: [explicit] The paper states "Experimental results are shown in Figure 5. It reveals that Optimus-1 keeps getting stronger through the continuous expansion of memory during the learning process of multiple periods."
- Why unresolved: While the paper mentions self-evolution, it doesn't provide a detailed breakdown of the improvements after each iteration or the specific mechanisms of how the learning process works.
- What evidence would resolve it: Detailed logs of the learning process, showing task completion rates and memory capacity after each iteration of "free exploration-teacher guidance" learning.

## Limitations
- HDKG requires manual knowledge encoding and may not capture complex conditional crafting relationships
- AMEP's summarization mechanism may lose critical information needed for effective reflection
- Current Action Controller (STEVE-1) struggles with complex tasks requiring precise mouse movements and instruction following

## Confidence
- **High confidence**: The core architecture combining HDKG for knowledge representation and AMEP for experience storage is well-described and theoretically sound. The experimental methodology and evaluation metrics are clearly specified.
- **Medium confidence**: The integration between knowledge-guided planning and experience-driven reflection appears feasible based on the described mechanisms, though implementation details are somewhat sparse.
- **Low confidence**: The specific implementation details for multimodal correlation computation and experience filtering in AMEP, as well as the exact prompt engineering approaches for the MLLM components.

## Next Checks
1. **Knowledge Graph Validation**: Implement the HDKG structure and test topological sorting on a subset of crafting recipes to verify that the knowledge graph correctly captures all prerequisite relationships and generates valid crafting sequences.

2. **Experience Pool Testing**: Create a controlled experiment where AMEP's summarization mechanism is tested against raw experience storage to quantify information loss and assess whether critical reflection information is preserved.

3. **End-to-End Integration Test**: Implement a simplified version using only 5-10 basic tasks (e.g., crafting wooden tools) to validate that the complete pipeline from knowledge-guided planning through experience-driven reflection produces successful outcomes before scaling to the full 67-task benchmark.