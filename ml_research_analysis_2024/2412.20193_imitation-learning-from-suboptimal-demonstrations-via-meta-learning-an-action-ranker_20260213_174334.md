---
ver: rpa2
title: Imitation Learning from Suboptimal Demonstrations via Meta-Learning An Action
  Ranker
arxiv_id: '2412.20193'
source_url: https://arxiv.org/abs/2412.20193
tags:
- demonstrations
- expert
- policy
- learning
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ILMAR addresses the problem of imitation learning from limited
  expert demonstrations and additional suboptimal demonstrations. The core method
  idea involves training a discriminator that uses the advantage function to selectively
  weight demonstrations, allowing high-quality non-expert data to be retained and
  utilized.
---

# Imitation Learning from Suboptimal Demonstrations via Meta-Learning An Action Ranker

## Quick Facts
- arXiv ID: 2412.20193
- Source URL: https://arxiv.org/abs/2412.20193
- Authors: Jiangdong Fan; Hongcai He; Paul Weng; Hui Xu; Jie Shao
- Reference count: 40
- Primary result: ILMAR achieves normalized scores up to 99.33% compared to baselines ranging from 69.60% to 99.29% across multiple MuJoCo environments.

## Executive Summary
ILMAR addresses the challenge of imitation learning when expert demonstrations are limited but additional suboptimal demonstrations are available. The core innovation is a discriminator that uses the advantage function to selectively weight demonstrations based on whether they outperform the current learned policy, rather than simply matching expert distribution. A meta-goal optimization approach further enhances performance by training the discriminator to minimize the distance between learned and expert policies. Experimental results demonstrate that ILMAR significantly outperforms state-of-the-art imitation learning algorithms across multiple MuJoCo environments, achieving near-expert performance even with limited expert data.

## Method Summary
ILMAR combines weighted behavior cloning with a meta-goal optimization approach. The method trains a discriminator that assigns weights to demonstrations based on their advantage relative to the current policy, using the formula w(s, a, π) = I(Aπ(s, a) > 0). The policy is updated using these weighted demonstrations, and a meta-loss computed from expert demonstrations is used to optimize the discriminator. This creates a bi-level optimization problem where the discriminator learns to weight demonstrations that help minimize the KL divergence between the learned and expert policies. The approach effectively leverages both expert and suboptimal demonstrations while maintaining stability through a carefully designed combination of vanilla and meta-losses.

## Key Results
- ILMAR achieves normalized scores up to 99.33% compared to random policies across MuJoCo environments.
- Outperforms state-of-the-art baselines including BC (69.60%), BCND (95.08%), DemoDICE (99.29%), DWBC (89.16%), and ISW-BC (98.57%).
- Meta-goal approach improves performance of other algorithms like DemoDICE and ISW-BC when applied.
- Robust performance across different ratios of expert to suboptimal demonstrations (1:0.25, 1:1, 1:4).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ILMAR's advantage-based weighting retains high-quality non-expert demonstrations that would be discarded by expert-distribution-based methods.
- Mechanism: The discriminator assigns weights based on whether demonstration actions outperform the current learned policy, rather than whether they match expert distribution.
- Core assumption: Demonstration actions that outperform the current policy contain useful information for policy improvement, even if they're not optimal.
- Evidence anchors:
  - [abstract] "Our key insight is that even demonstrations that fall outside the expert distribution but outperform the learned policy can enhance policy performance."
  - [section] "The weight function w(s, a, π) = I(Aπ(s, a) > 0), where I is an indicator function that assigns a value of 1 when the condition (Aπ(s, a) > 0 holds, and 0 otherwise."
- Break condition: If the learned policy becomes too strong too quickly, most demonstration actions will underperform it, causing the advantage-based weighting to discard potentially useful data.

### Mechanism 2
- Claim: Meta-goal optimization improves the discriminator's ability to assign weights that lead to policies closer to expert behavior.
- Mechanism: Bi-level optimization where policy updates are followed by discriminator updates that minimize KL divergence between learned and expert policies.
- Core assumption: The meta-loss computed from expert demonstrations provides useful signal for how well the discriminator is weighting the data.
- Evidence anchors:
  - [abstract] "To make more effective use of supplementary demonstrations, we introduce meta-goal in ILMAR to optimize the functional of the advantage function by explicitly minimizing the distance between the current policy and the expert policy."
  - [section] "We estimate the discrepancy between the learned policy and the expert policy using demonstrations from the expert dataset DE."
- Break condition: If the meta-loss gradient becomes too noisy relative to the vanilla loss, the discriminator may learn suboptimal weighting strategies.

### Mechanism 3
- Claim: Combining vanilla loss with meta-goal provides stability and convergence guarantees.
- Mechanism: The vanilla loss provides prior knowledge that constrains discriminator updates, ensuring alignment between gradient directions of policy and discriminator objectives.
- Core assumption: Without vanilla loss regularization, the meta-goal optimization may become unstable or get stuck in local optima.
- Evidence anchors:
  - [section] "The manually designed vanilla loss constrains the update direction of the discriminator, ensuring that the gradient directions of LC and Lactor remain closely aligned."
  - [section] "This adjustment not only corrects but also stabilizes the updates of the discriminator, enhancing convergence."
- Break condition: If the vanilla loss dominates too heavily, the meta-goal optimization may not have sufficient influence to optimize discriminator performance.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation
  - Why needed here: The problem is formally defined as learning from demonstrations in an MDP framework, which provides the mathematical foundation for policy optimization.
  - Quick check question: What are the key components of an MDP tuple ⟨S, A, P, R, H, p0, γ⟩ and how do they relate to imitation learning?

- Concept: Advantage function and its role in policy optimization
  - Why needed here: The advantage function is the core mechanism for weighting demonstrations based on their relative performance compared to the current policy.
  - Quick check question: How does Aπ(s, a) = Qπ(s, a) - Vπ(s) measure the relative benefit of an action compared to the average policy performance?

- Concept: Kullback-Leibler (KL) divergence as a measure of policy similarity
  - Why needed here: KL divergence is used in the meta-goal to measure and minimize the distance between learned and expert policies.
  - Quick check question: What does DKL[πE∥π] represent and why is minimizing it important for imitation learning?

## Architecture Onboarding

- Component map:
  Policy network πθ -> Discriminator network Cψ -> Weighted behavior cloning loss -> Policy update -> Meta-loss computation -> Discriminator update

- Critical path:
  1. Sample demonstrations from DE and D
  2. Update policy using weighted behavior cloning with discriminator weights
  3. Compute meta-loss using expert demonstrations and updated policy
  4. Update discriminator using combined meta-loss and vanilla loss
  5. Repeat

- Design tradeoffs:
  - Using advantage-based weighting vs. expert-distribution-based weighting: More inclusive of useful non-expert data but requires careful discriminator training
  - Meta-goal vs. vanilla-only training: Better performance but potential instability without proper regularization
  - Small expert dataset vs. large expert dataset: Tests algorithm's ability to leverage suboptimal data but may limit baseline performance

- Failure signatures:
  - Policy performance plateaus early: May indicate discriminator is not effectively weighting demonstrations
  - High variance in training curves: Could suggest meta-loss gradient instability
  - Discriminator loss increases: May indicate poor alignment between vanilla and meta-loss gradients

- First 3 experiments:
  1. Run ILMAR on Ant-v2 with T1 setting (1:0.25 expert:suboptimal ratio) and verify it outperforms BC baseline
  2. Remove meta-goal component and observe performance degradation to confirm its importance
  3. Increase suboptimal proportion to T3 (1:4) and measure how much performance degrades compared to T1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of α and β affect the convergence and performance of ILMAR?
- Basis in paper: [explicit] The paper conducts a grid search over α and β and discusses their impact on performance.
- Why unresolved: While the paper shows that ILMAR is robust to the choice of α and β, the optimal values may depend on the specific task and dataset, requiring further investigation.
- What evidence would resolve it: A comprehensive study varying α and β across different environments and dataset sizes to identify generalizable patterns.

### Open Question 2
- Question: Can the meta-goal approach be extended to other imitation learning algorithms beyond DemoDICE and ISW-BC?
- Basis in paper: [explicit] The paper demonstrates that applying meta-goal to DemoDICE and ISW-BC improves their performance, suggesting potential applicability to other algorithms.
- Why unresolved: The paper only tests meta-goal on two specific algorithms, leaving open the question of its effectiveness on a broader range of imitation learning methods.
- What evidence would resolve it: Applying meta-goal to a diverse set of imitation learning algorithms and comparing their performance with and without meta-goal.

### Open Question 3
- Question: How does the performance of ILMAR scale with the size of the expert dataset?
- Basis in paper: [inferred] The paper focuses on scenarios with limited expert demonstrations and suggests investigating larger expert datasets as a future direction.
- Why unresolved: The paper primarily evaluates ILMAR with a single expert trajectory, leaving the impact of increasing expert data unexplored.
- What evidence would resolve it: Conducting experiments with varying numbers of expert demonstrations to assess ILMAR's performance and robustness.

## Limitations
- The meta-goal optimization introduces additional hyperparameters (α, β) whose sensitivity is not thoroughly explored, potentially limiting reproducibility.
- The experimental evaluation focuses on MuJoCo continuous control tasks, leaving open questions about generalization to other domains or real-world applications.
- The paper does not provide detailed analysis of how the discriminator weights evolve during training or what happens when the learned policy becomes too strong relative to demonstrations.

## Confidence
- **High confidence**: The core mechanism of using advantage functions for demonstration weighting is well-grounded in RL theory and the experimental results are robust across multiple environments and demonstration ratios.
- **Medium confidence**: The meta-goal optimization approach shows promise but lacks extensive ablation studies to fully understand its contribution relative to other potential discriminator training strategies.
- **Medium confidence**: The comparison with state-of-the-art methods is comprehensive, though the paper could provide more insight into failure modes and edge cases.

## Next Checks
1. **Ablation on vanilla loss contribution**: Remove the vanilla loss component entirely and measure performance degradation to quantify its stabilizing effect on meta-goal optimization.
2. **Discriminator weight analysis**: Track and visualize the distribution of weights assigned by the discriminator throughout training to understand how demonstration quality perception evolves.
3. **Cross-domain generalization**: Test ILMAR on non-MuJoCo tasks (e.g., PyBullet or simulated robotics tasks) to evaluate robustness across different control environments.