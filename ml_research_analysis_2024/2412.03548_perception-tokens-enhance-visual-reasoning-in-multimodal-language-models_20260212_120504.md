---
ver: rpa2
title: Perception Tokens Enhance Visual Reasoning in Multimodal Language Models
arxiv_id: '2412.03548'
source_url: https://arxiv.org/abs/2412.03548
tags:
- depth
- tokens
- reasoning
- visual
- perception
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces perception tokens, intermediate visual representations
  that enable multimodal language models to perform tasks requiring spatial and structural
  reasoning beyond text. Perception tokens act as auxiliary reasoning steps, similar
  to chain-of-thought, allowing models to generate depth maps or bounding boxes as
  part of their reasoning process.
---

# Perception Tokens Enhance Visual Reasoning in Multimodal Language Models

## Quick Facts
- **arXiv ID**: 2412.03548
- **Source URL**: https://arxiv.org/abs/2412.03548
- **Reference count**: 40
- **Primary result**: LLaVA-AURORA achieves significant improvements in counting tasks (+10.8% on BLINK, +11.3% on CVBench, +8.3% on SEED-Bench) and relative depth estimation (+6% on BLINK)

## Executive Summary
This paper introduces perception tokens, intermediate visual representations that enable multimodal language models to perform tasks requiring spatial and structural reasoning beyond text. Perception tokens act as auxiliary reasoning steps, similar to chain-of-thought, allowing models to generate depth maps or bounding boxes as part of their reasoning process. The proposed AURORA framework trains models to leverage these tokens using a VQVAE for pixel-level representations and direct encoding for structured ones, combined with a multi-task curriculum learning approach. LLaVA-AURORA demonstrates substantial improvements in counting and depth estimation tasks compared to fine-tuning baselines and tool-augmented approaches.

## Method Summary
The AURORA framework integrates perception tokens into multimodal language models by first training a VQVAE to tokenize depth maps and directly encoding structured representations like bounding boxes. These tokenized perception tokens are then added to the MLM vocabulary and used during training through a curriculum learning approach that progresses from atomic tasks (generating perception tokens) to chain-of-thought reasoning and direct labeling. The model is trained using LoRA adapters on LLaVA-1.5 13B with data from ADE20k for depth maps, LVIS for object counting, and evaluation on BLINK, CVBench, and SEED-Bench benchmarks.

## Key Results
- LLaVA-AURORA achieves +10.8% improvement on BLINK counting tasks compared to fine-tuning baselines
- Demonstrates +11.3% improvement on CVBench and +8.3% on SEED-Bench for counting accuracy
- Shows +6% improvement on relative depth estimation on BLINK benchmark
- Outperforms tool-augmented approaches while maintaining efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perception tokens enable MLMs to reason over visual representations by generating intermediate structured outputs that can be used in reasoning chains.
- Mechanism: The model learns to generate perception tokens (depth maps, bounding boxes) as auxiliary reasoning steps, similar to chain-of-thought in language models. These tokens are then used to condition subsequent reasoning steps.
- Core assumption: Visual perception tasks require intermediate representations that language alone cannot capture, and these can be effectively tokenized and reasoned over.
- Evidence anchors:
  - [abstract]: "Perception tokens act as auxiliary reasoning tokens, akin to chain-of-thought prompts in language models"
  - [section]: "For example, in a depth-related task, an MLM augmented with perception tokens can reason by generating a depth map as tokens, enabling it to solve the problem effectively"
  - [corpus]: Weak - no direct evidence about this specific mechanism, but related work on visual tokens suggests this approach is novel

### Mechanism 2
- Claim: Curriculum learning with progressive chain-of-thought enables effective learning of perception tokens and their use in reasoning.
- Mechanism: The model is first trained on atomic tasks (generating perception tokens), then gradually introduced to more complex reasoning tasks that use these tokens, avoiding catastrophic forgetting.
- Core assumption: Learning perception token generation and reasoning can be separated into stages, with atomic tasks providing foundation for complex reasoning.
- Evidence anchors:
  - [section]: "We propose a curriculum learning-inspired training scheme that begins with atomic tasks and gradually advances to more complex ones"
  - [section]: "This strategy enables the model to learn from both types of reasoning tasks effectively, enhancing its ability to perform complex visual reasoning"
  - [corpus]: Moderate - curriculum learning is established in ML, but specific application to perception tokens is novel

### Mechanism 3
- Claim: VQ-VAE tokenization of visual representations enables effective integration of perception tokens into the MLM vocabulary.
- Mechanism: VQ-VAE learns a discrete codebook for visual representations (depth maps, bounding boxes), which are then treated as tokens in the MLM vocabulary, enabling autoregressive generation.
- Core assumption: Visual representations can be effectively discretized into a fixed codebook without losing essential information for reasoning tasks.
- Evidence anchors:
  - [section]: "For certain intermediate representations (e.g. depth maps), we train a VQVAE to transform them into a tokenized format"
  - [section]: "Each depth map is encoded as a grid of embeddings, with each embedding matched to the nearest entry in the codebook"
  - [corpus]: Strong - VQ-VAE is well-established for image tokenization, and this application is consistent with prior work

## Foundational Learning

- Concept: Visual perception tokens as intermediate reasoning representations
  - Why needed here: MLMs struggle with tasks requiring spatial and structural reasoning that language alone cannot capture. Perception tokens provide the missing visual abstractions needed for these tasks.
  - Quick check question: Why can't standard MLMs solve counting and depth estimation tasks without perception tokens?

- Concept: Curriculum learning for multi-task visual reasoning
  - Why needed here: Learning to generate perception tokens and use them for reasoning requires different skill levels. Curriculum learning prevents catastrophic forgetting and enables smooth skill progression.
  - Quick check question: What would happen if we trained the model on all tasks (token generation and reasoning) simultaneously from the start?

- Concept: VQ-VAE tokenization for visual representations
  - Why needed here: MLMs require discrete tokens for autoregressive generation. VQ-VAE provides a way to convert continuous visual representations into discrete tokens that can be integrated into the MLM vocabulary.
  - Quick check question: Why use VQ-VAE instead of directly using continuous visual features in the MLM?

## Architecture Onboarding

- Component map: Image → Vision Encoder → Visual Features → VQ-VAE/Direct Encoding → Perception Tokens + Text → MLM → Output

- Critical path:
  1. Input image → vision encoder → visual features
  2. For depth tasks: visual features → VQ-VAE → depth tokens
  3. For counting tasks: bounding box coordinates → discrete pixel tokens
  4. Tokens + text → MLM → generate perception tokens or final answer
  5. If CoT: perception tokens → additional reasoning steps → final answer

- Design tradeoffs:
  - VQ-VAE codebook size (128) vs. representation quality
  - Token vocabulary expansion (466 new tokens) vs. model capacity
  - Curriculum learning complexity vs. training efficiency
  - Reconstruction loss inclusion vs. computational overhead

- Failure signatures:
  - Model generates irrelevant perception tokens (poor VQ-VAE training)
  - Model ignores perception tokens in reasoning (poor conditioning)
  - Model performance degrades on original tasks (catastrophic forgetting)
  - Slow convergence or poor generalization (curriculum too difficult)

- First 3 experiments:
  1. Test VQ-VAE reconstruction quality on held-out depth maps from ADE20k
  2. Evaluate MLM's ability to generate depth tokens for simple depth estimation tasks
  3. Compare counting accuracy with and without bounding box perception tokens on a small subset of LVIS data

## Open Questions the Paper Calls Out
- What is the maximum number of perception tokens that can be effectively incorporated into multimodal language models before performance degradation occurs?
- How does the performance of perception tokens compare to traditional visual embeddings like CLIP when used as intermediate reasoning steps?
- Can perception tokens be effectively transferred between different base models or do they require model-specific training?
- What is the long-term impact of perception tokens on model generalization across unseen tasks and domains?

## Limitations
- The approach relies on pseudo-depth maps and synthetic bounding box annotations, which may limit fine-grained spatial reasoning
- Perception tokens are limited to depth maps and bounding boxes, representing only a subset of possible visual reasoning tasks
- The multi-stage curriculum learning approach introduces significant training complexity and computational overhead

## Confidence

**High Confidence Claims**:
- Perception tokens can be effectively generated for depth maps using VQ-VAE tokenization
- The model can learn to use these tokens in chain-of-thought reasoning for counting and depth estimation tasks
- Curriculum learning with progressive task complexity prevents catastrophic forgetting

**Medium Confidence Claims**:
- The 6-11% performance improvements represent meaningful gains in visual reasoning capabilities
- The approach generalizes to unseen reasoning tasks that require intermediate visual representations
- The computational overhead of perception tokens is justified by the reasoning improvements

**Low Confidence Claims**:
- The approach will scale effectively to more complex visual reasoning tasks beyond counting and depth estimation
- The perception token methodology will generalize to other visual modalities (e.g., medical imaging, scientific visualization)
- The current VQ-VAE codebook sizes are optimal for all target visual reasoning tasks

## Next Checks
1. Evaluate LLaVA-AURORA on spatial reasoning tasks from domains not seen during training (e.g., medical imaging, satellite imagery) to assess whether perception tokens enable transfer to novel visual reasoning scenarios.
2. Systematically vary VQ-VAE codebook sizes for depth maps and bounding boxes to identify the optimal trade-off between representation quality and computational efficiency, and to determine whether current sizes are limiting reasoning performance.
3. Conduct user studies where human annotators assess the quality and relevance of generated depth maps and bounding boxes for given visual reasoning tasks, to validate that the model is generating perceptually meaningful intermediate representations rather than spurious patterns.