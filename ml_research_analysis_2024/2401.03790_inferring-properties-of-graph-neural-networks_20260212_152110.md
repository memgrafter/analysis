---
ver: rpa2
title: Inferring Properties of Graph Neural Networks
arxiv_id: '2401.03790'
source_url: https://arxiv.org/abs/2401.03790
tags:
- properties
- node
- graph
- gnn-infer
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GNN-Infer automatically infers likely properties of GNNs by converting\
  \ GNN structures to equivalent FNNs and applying existing property inference techniques.\
  \ The method first extracts frequent influential structures, then converts each\
  \ \u27E8GNN, structure\u27E9 pair to an FNN, and finally generalizes properties\
  \ to full graphs."
---

# Inferring Properties of Graph Neural Networks

## Quick Facts
- arXiv ID: 2401.03790
- Source URL: https://arxiv.org/abs/2401.03790
- Authors: Dat Nguyen; Hieu M. Vu; Cong-Thanh Le; Bach Le; David Lo; ThanhVu Nguyen; Corina Pasareanu
- Reference count: 40
- Primary result: GNN-Infer automatically infers likely properties of GNNs by converting them to equivalent FNNs and applying existing property inference techniques

## Executive Summary
GNN-Infer is a novel approach for inferring properties of Graph Neural Networks (GNNs) by converting them into equivalent Feed-forward Neural Networks (FNNs). The method extracts frequent influential structures from GNN datasets, converts each âŸ¨GNN, structureâŸ© pair to an FNN, and applies existing FNN property inference tools. On synthetic benchmarks with ground truth, GNN-Infer rediscovered 8 of 13 properties and found highly confident approximations for the remaining 5. For real-world backdoored GNNs, inferred properties improved defense against state-of-the-art attacks by up to 30x over baselines.

## Method Summary
GNN-Infer automatically infers likely properties of GNNs by converting GNN structures to equivalent FNNs and applying existing property inference techniques. The method first extracts frequent influential structures using GNNExplainer and gSpan, then converts each âŸ¨GNN, structureâŸ© pair to an FNN using rewriting rules that replace graph operations with linear transformations and max operations. Finally, it generalizes properties to full graphs using dynamic analysis with interpretable models. The approach bridges the gap between fixed-input property inference and variable-structure GNNs, enabling application of established FNN property inference tools to GNN models.

## Key Results
- Rediscovered 8 of 13 properties on synthetic benchmarks with ground truth
- Found highly confident approximations for 5 additional properties
- Improved defense against backdoor attacks by up to 30x over baselines for real-world backdoored GNNs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GNN-Infer converts GNN models with fixed structures into equivalent FNNs, enabling use of existing FNN property inference tools.
- Mechanism: The algorithm recursively transforms message passing layers into equivalent FNN layers using rewriting rules that replace graph operations with linear transformations and max operations.
- Core assumption: Given a fixed graph structure, all message passing operations can be expressed as linear transformations followed by fixed non-linear functions.
- Evidence anchors:
  - [section] "Using the recursive transformation in the proof above, we design the GNN-to-FNN transformation algorithm in Fig. 5. Following Theorem 1, the algorithm is proved to be both sound, i.e., the converted FNN Mð¹ is equivalent to âŸ¨M, ð‘†âŸ© and complete..."
  - [corpus] No direct corpus evidence for this specific transformation mechanism.
- Break condition: If the GNN uses operations that cannot be expressed as combinations of linear transformations and fixed non-linear functions (e.g., dynamic graph structures or operations dependent on variable input structure).

### Mechanism 2
- Claim: Structure-specific properties are inferred by applying FNN property inference tools to the converted FNN models.
- Mechanism: After converting âŸ¨GNN, structureâŸ© to equivalent FNN, GNN-Infer uses PROPHECY to infer properties from execution traces collected by running the FNN on structure instances from the dataset.
- Core assumption: The converted FNN accurately represents the GNN's behavior on the specific structure, so FNN property inference results are valid for the GNN.
- Evidence anchors:
  - [section] "To achieve the above, GNN-Infer first transforms the pair âŸ¨M, ð‘†âŸ© into a corresponding FNN Mð¹. We prove that this transformation is both sound and complete... Based on this theoretical foundation, we then use PROPHECY [13], a property inference tool for FNN, to infer the properties of Mð¹."
  - [corpus] No direct corpus evidence for this specific property inference mechanism.
- Break condition: If the conversion introduces approximation errors or if PROPHECY cannot handle the specific FNN architecture generated from the GNN.

### Mechanism 3
- Claim: Dynamic feature properties improve precision of likely properties by capturing deviations caused by variable surrounding structures.
- Mechanism: GNN-Infer trains interpretable models (decision trees or linear regression) on aggregated features that compare structural vs full-graph information, then extends properties with these learned conditions.
- Core assumption: The deviation between GNN output and property predictions can be modeled as a function of aggregated features from the full graph.
- Evidence anchors:
  - [section] "To find dynamic feature properties, GNN-Infer first trains an interpretable model capturing the deviation between the actual GNN output and the specified output property ð‘ƒ over the aggregated features."
  - [corpus] No direct corpus evidence for this specific dynamic analysis mechanism.
- Break condition: If the aggregated features cannot capture the relevant variation in GNN behavior, or if the interpretable models fail to learn meaningful patterns.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their message passing mechanism
  - Why needed here: Understanding how GNNs operate is essential to grasp why the conversion to FNNs works and what properties can be inferred
  - Quick check question: What are the three main components of a GNN layer (message function, aggregation function, update function)?

- Concept: Feed-forward Neural Networks (FNNs) and property inference
  - Why needed here: The entire approach relies on leveraging existing FNN property inference tools, so understanding what properties can be inferred from FNNs is crucial
  - Quick check question: What form do properties take in FNN property inference (input condition implies output condition)?

- Concept: Graph isomorphism and subgraph isomorphism
  - Why needed here: Structure predicates use these concepts to define which graphs satisfy the properties, and the conversion relies on fixed structures
  - Quick check question: What is the difference between graph isomorphism and subgraph isomorphism in terms of the structural constraints they impose?

## Architecture Onboarding

- Component map: Structure extraction (GNNExplainer + gSpan) -> GNN-to-FNN conversion (rewriting rules) -> FNN property inference (PROPHECY) -> Dynamic analysis (interpretable models) -> Backdoor defense (property-based pruning)

- Critical path:
  1. Extract influential substructures from dataset
  2. For each structure: convert to FNN, infer properties, generalize with dynamic analysis
  3. Use inferred properties for analysis or defense

- Design tradeoffs:
  - Fixed structure conversion enables FNN analysis but limits coverage to specific structures
  - Dynamic analysis improves precision but adds computational overhead
  - Using interpretable models balances explainability with prediction accuracy

- Failure signatures:
  - Poor proxy accuracy scores indicate conversion or inference problems
  - High computational cost suggests optimization opportunities in structure extraction or matching
  - Backdoor defense failure indicates properties don't capture attack characteristics

- First 3 experiments:
  1. Run on BFS GNN with small synthetic dataset to verify basic conversion and inference pipeline
  2. Test dynamic analysis on DFS GNN to validate improvement in property precision
  3. Apply to real-world GNN (Cora dataset) to assess practical performance and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can GNN-Infer be extended to handle more complex GNN architectures like Graph Attention Networks (GATs)?
- Basis in paper: [inferred] The paper mentions that Marabou verification tools do not support quadratic operations in the graph attention layer of GAT, limiting the evaluation to GCN and GraphSAGE models.
- Why unresolved: The theoretical foundation of GNN-Infer's FNN conversion should be architecture-agnostic, but the practical implementation may face challenges with non-linear operations like attention mechanisms.
- What evidence would resolve it: Successful application of GNN-Infer to GAT models on benchmark datasets, with comparable accuracy and backdoor defense performance to GCN and GraphSAGE.

### Open Question 2
- Question: How does the runtime of GNN-Infer scale with larger and more complex graphs?
- Basis in paper: [explicit] The paper reports that the inference process takes approximately 4 hours in total for their experiment, with most of the time spent on explanation, graph matching, and property inference.
- Why unresolved: The paper only provides results for relatively small graphs (1000 input graphs with up to 100,000 instances). The scaling behavior for larger graphs with more nodes, edges, and instances is unknown.
- What evidence would resolve it: Runtime measurements of GNN-Infer on progressively larger and more complex graphs, plotted against graph size and number of instances, to determine the scaling behavior.

### Open Question 3
- Question: Can the inferred properties be used for automated repair of GNN models?
- Basis in paper: [inferred] The paper mentions that the inferred properties can highlight errors in GNN models, such as incorrect predictions or failure to model certain cases. This suggests the potential for using properties for repair.
- Why unresolved: The paper does not explore the application of inferred properties for automated repair of GNN models. The feasibility and effectiveness of such an approach are unknown.
- What evidence would resolve it: A method for automatically adjusting GNN weights or architectures based on inferred properties, with experimental results showing improved model accuracy and robustness compared to the original GNN.

## Limitations
- The approach depends heavily on the quality of structure extraction and GNN-to-FNN conversion accuracy
- Practical effectiveness for real-world GNNs with complex, irregular graph structures remains to be fully validated
- Computational efficiency for large-scale graphs needs further evaluation

## Confidence
- High confidence in the theoretical foundation of GNN-to-FNN conversion for fixed structures
- Medium confidence in the practical effectiveness of property inference for real-world GNNs
- Medium confidence in the dynamic analysis improvement mechanism

## Next Checks
1. Validate the GNN-to-FNN conversion accuracy on larger, more complex GNN architectures beyond the synthetic benchmarks
2. Test the approach on non-backdoored GNNs to verify property inference works for benign models
3. Benchmark the computational efficiency of the full pipeline (structure extraction + conversion + inference) on real-world large-scale graphs