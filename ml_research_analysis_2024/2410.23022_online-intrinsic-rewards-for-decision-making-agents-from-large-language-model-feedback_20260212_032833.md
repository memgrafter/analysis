---
ver: rpa2
title: Online Intrinsic Rewards for Decision Making Agents from Large Language Model
  Feedback
arxiv_id: '2410.23022'
source_url: https://arxiv.org/abs/2410.23022
tags:
- reward
- learning
- intrinsic
- which
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ONI, a distributed online system for learning
  intrinsic rewards and policies concurrently using LLM feedback. The key innovation
  is removing the need for large pre-collected datasets by annotating agent experience
  online via an asynchronous LLM server.
---

# Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback

## Quick Facts
- arXiv ID: 2410.23022
- Source URL: https://arxiv.org/abs/2410.23022
- Reference count: 18
- One-line primary result: ONI matches or exceeds state-of-the-art performance on NetHack without requiring auxiliary dense reward functions or offline data.

## Executive Summary
This paper introduces ONI, a distributed online system for learning intrinsic rewards and policies concurrently using LLM feedback. The key innovation is removing the need for large pre-collected datasets by annotating agent experience online via an asynchronous LLM server. The authors explore three reward modeling approaches: retrieval-based hashing, binary classification, and ranking. Experiments on NetHack show ONI matches or exceeds state-of-the-art performance without requiring auxiliary dense reward functions or offline data. Notably, ONI-classification achieves comparable results to Motif while eliminating dataset dependencies.

## Method Summary
ONI is a distributed online system that learns intrinsic rewards and policies concurrently using LLM feedback. The system collects agent experience with observation captions, sends them to an asynchronous LLM server for annotation, and distills these annotations into an intrinsic reward model. Three reward modeling approaches are explored: retrieval-based hashing (exact caption matching), binary classification (learned model for generalization), and ranking (pairwise comparison via Bradley-Terry model). The policy and reward model are trained concurrently using distributed PPO, with the intrinsic reward replacing sparse extrinsic rewards.

## Key Results
- ONI achieves comparable performance to state-of-the-art methods on NetHack tasks without requiring auxiliary dense reward functions or pre-collected datasets
- ONI-classification matches Motif's performance while eliminating dataset dependencies entirely
- ONI-retrieval performs surprisingly well due to repeated messages in early NetHack, though it lacks generalization capability
- ONI-ranking shows potential benefits in settings with higher observation diversity, though current results are similar to other approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Online annotation via asynchronous LLM server enables scalable reward learning without requiring pre-collected datasets.
- Mechanism: The system maintains a LIFO queue of observation captions, sending them to an LLM server as throughput allows. The LLM returns binary labels (helpful/unhelpful) which are stored in a hash table. Both policy and reward model are updated concurrently using these annotations.
- Core assumption: Most informative captions can be captured by a small fraction of observations, making asynchronous annotation feasible.
- Evidence anchors:
  - [abstract] "Our approach annotates the agent's collected experience via an asynchronous LLM server, which is then distilled into an intrinsic reward model."
  - [section 3.1] "The average throughput is 30k environment interactions per second if we do not train any additional reward model to distill the LLM annotations, and 26k if a classification-based reward model is learned at the same time"
  - [corpus] Weak evidence - corpus mentions similar approaches but doesn't directly support scalability claims

### Mechanism 2
- Claim: Hash table retrieval provides simple, hyperparameter-free intrinsic rewards when observation captions are repeated frequently.
- Mechanism: Observation captions are checked against a hash table H. If found, the stored label is used as the intrinsic reward; otherwise, reward is 0 and the caption is queued for LLM annotation.
- Core assumption: Early game NetHack captions are highly repetitive, making exact matching sufficient for good performance.
- Evidence anchors:
  - [section 3.2] "This retrieval-based approach does not generalize to observations with unlabeled captions. However, the resulting intrinsic reward is simple and hyperparameter-free, and may work well when the set of captions belongs to a relatively small set."
  - [section 5.1] "This is likely a consequence of many messages with positive valence being repeated in the early game of NetHack, such as 'You find a hidden passage'"
  - [corpus] Weak evidence - corpus mentions hash tables but doesn't validate the NetHack-specific claim

### Mechanism 3
- Claim: Binary classification reward model generalizes to unseen but semantically similar captions by learning from LLM annotations.
- Mechanism: A binary classifier is trained on (caption, label) pairs from the hash table, predicting P(y=1|o). The intrinsic reward is I[rint_ϕ(ot) > η], where η is a threshold.
- Core assumption: Similar captions share similar semantic meaning, allowing the classifier to generalize beyond exact matches.
- Evidence anchors:
  - [section 3.2] "Unlike the previous approach, this method has the potential to generalize to observations whose captions are similar, but not identical, to the captions labeled by the LLM."
  - [section 5.1] "ONI-classification, which also predicts binary rewards but is able to generalize to unseen messages, provides a modest but consistent improvement over ONI-retrieval"
  - [corpus] Weak evidence - corpus mentions classification but doesn't validate generalization in NetHack

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: The paper explicitly models NetHack as a POMDP where agents observe partial information (ot) rather than full states (st)
  - Quick check question: In a POMDP, what is the relationship between the observation function O(st) and the actual state st?

- Concept: Proximal Policy Optimization (PPO) and its asynchronous variant APPO
  - Why needed here: The system is built on Sample Factory's APPO implementation, which handles concurrent policy updates across multiple environment instances
  - Quick check question: How does APPO handle policy staleness when multiple rollout workers use different versions of the policy?

- Concept: Bradley-Terry model for pairwise comparison
  - Why needed here: ONI-ranking uses this model to convert LLM pairwise preferences into a scalar reward function
  - Quick check question: What is the mathematical form of the Bradley-Terry model used to compute Pϕ(o1 ≻ o2)?

## Architecture Onboarding

- Component map: Learner worker (policy + reward updates) ←→ Rollout workers (environment execution) ←→ Policy workers (action selection) ←→ LLM annotation worker (caption labeling) ←→ LLM server (model inference)
- Critical path: Observation → Caption → Hash table lookup → Policy action → Environment step → Caption → LLM annotation → Hash table update → Reward model update → Policy update
- Design tradeoffs: Asynchronous LLM annotation vs. synchronous blocking (throughput vs. annotation coverage); hash table vs. learned model (simplicity vs. generalization)
- Failure signatures: Low annotation throughput (LLM becomes bottleneck), hash table overflow (too many unique captions), reward model overfitting (insufficient annotation diversity), policy collapse (intrinsic reward dominates too strongly)
- First 3 experiments:
  1. Run ONI-retrieval baseline to verify basic asynchronous annotation works without reward model
  2. Compare ONI-classification with different threshold values η to find optimal binary reward cutoff
  3. Test ONI-ranking with and without caption deduplication to understand sampling strategy impact

## Open Questions the Paper Calls Out

- How does the choice of intrinsic reward model architecture (retrieval vs classification vs ranking) impact performance in environments with higher observation diversity?
- What is the optimal strategy for selecting which observations to send to the LLM server in high-throughput settings?
- How does the performance of ONI-scale with different LLM model sizes and annotation throughput configurations?
- What is the impact of using parametric reward models versus non-parametric approaches on sample efficiency and generalization?

## Limitations
- Reliance on NetHack's repetitive observation space limits generalizability to more diverse environments
- Asynchronous annotation system's performance heavily depends on LLM throughput, which may become a bottleneck
- Experimental validation conducted on a single domain (NetHack) limits broader claims about applicability

## Confidence
- High confidence: The core mechanism of asynchronous LLM annotation and its basic implementation details
- Medium confidence: The effectiveness of the three reward modeling approaches in NetHack specifically
- Low confidence: Claims about scalability to more diverse environments or different game domains

## Next Checks
1. **Environment Diversity Test**: Evaluate ONI on a more diverse environment like MiniGrid or ProcGen where observation captions vary more significantly, to test the generalization limits of the classification approach.
2. **Throughput Scaling Analysis**: Measure how annotation throughput scales with the number of concurrent environment instances and determine the break point where LLM becomes the bottleneck.
3. **Reward Model Ablation**: Systematically remove components of the reward model (e.g., threshold η, normalization term) to quantify their individual contributions to final performance.