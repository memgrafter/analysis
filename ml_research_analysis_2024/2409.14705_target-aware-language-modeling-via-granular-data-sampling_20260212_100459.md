---
ver: rpa2
title: Target-Aware Language Modeling via Granular Data Sampling
arxiv_id: '2409.14705'
source_url: https://arxiv.org/abs/2409.14705
tags:
- data
- multi-granular
- n-gram
- random
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a multi-granular n-gram importance sampling
  approach for targeted language model pretraining. By incorporating tokens of different
  granularities (subword, word, multi-word) into the tokenizer vocabulary, the method
  improves representation quality and reduces hash collisions compared to using only
  word-based n-grams.
---

# Target-Aware Language Modeling via Granular Data Sampling

## Quick Facts
- arXiv ID: 2409.14705
- Source URL: https://arxiv.org/abs/2409.14705
- Reference count: 26
- Models 125M-1.5B parameters match full-data performance using only ~1% of RefinedWeb dataset

## Executive Summary
This work introduces a multi-granular n-gram importance sampling approach for targeted language model pretraining. By incorporating tokens of different granularities (subword, word, multi-word) into the tokenizer vocabulary, the method improves representation quality and reduces hash collisions compared to using only word-based n-grams. Experimental results show that with only ~1% of the full RefinedWeb dataset, multi-granular sampling enables 125M to 1.5B parameter models to match full-data performance and outperform random sampling across eight commonsense reasoning benchmarks, with average improvements of up to 6.94% and minimal task-specific bias.

## Method Summary
The approach adapts a general tokenizer vocabulary to target downstream tasks by merging with task-specific vocabulary and pruning to minimize entropy increase. Documents are featurized using n-grams from the adapted tokenizer at multiple granularities, and importance weights are computed as the ratio of target-feature to raw-feature distributions. The top-scoring documents are sampled for pretraining decoder-only transformer models on a small subset (~1%) of the full RefinedWeb dataset, achieving comparable or better performance than training on the full dataset.

## Key Results
- Multi-granular sampling with ~1% of RefinedWeb matches full-data performance across 125M-1.5B parameter models
- Outperforms random sampling by up to 6.94% average across eight commonsense reasoning benchmarks
- Reduces hash collisions compared to word-only n-gram approaches while maintaining general task performance
- Sampling process takes approximately 1.5 days on average

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-granular n-gram features reduce hash collisions compared to word-only n-grams.
- Mechanism: By incorporating subword and multi-word tokens alongside standard words, the tokenizer vocabulary can represent text more precisely. This finer-grained coverage allows more unique n-gram patterns to be captured, which reduces the chance that distinct n-grams map to the same hash key.
- Core assumption: Subword tokens retain partial word information while multi-word tokens capture beyond-word phrases, together improving feature distinguishability.
- Evidence anchors:
  - [abstract] "Incorporating tokens of different granularities (subword, word, multi-word) into the tokenizer vocabulary, the method improves representation quality and reduces hash collisions compared to using only word-based n-grams."
  - [section] "We postulate that this improvement has to do with the reduction of hash collisions in the hashed n-gram features, where the joint use of subword and multiword capture beyond the boundaries of a word while preserving parts of a word in tokens so that during collisions the whole word is not entirely discarded."
  - [corpus] Weak - no direct corpus evidence; the claim is inferred from experimental comparisons.
- Break condition: If the vocabulary size is too small or the token distribution is heavily skewed toward common subwords, collision reduction may plateau or reverse.

### Mechanism 2
- Claim: Adapted tokenizer vocabulary increases task-data alignment without excessive bias.
- Mechanism: The tokenizer is incrementally adapted from a base Llama-3 vocabulary by merging in task-specific tokens and pruning to minimize entropy increase. This produces a vocabulary that better represents the target domain while preserving some general coverage, yielding more relevant feature vectors for importance sampling.
- Core assumption: Vocabulary utility metric (entropy-based) correlates with downstream performance, and small iterative vocabulary changes preserve task knowledge while avoiding overfitting.
- Evidence anchors:
  - [section] "We use Llama-3 tokenizer's vocabulary Vstart as the starting point and merge Vstart with Vtask which is learned from task data Dtask...arg min aims to find the vocabulary from Vstart with the minimum entropy difference."
  - [abstract] "Leveraging our findings, we improve upon the importance-based data sampling technique by adapting a general vocabulary set to the target vocabulary."
  - [corpus] Weak - corpus contains no direct evidence of entropy-metric correlation with performance.
- Break condition: If vocabulary pruning removes too many general tokens, performance on non-target tasks degrades sharply.

### Mechanism 3
- Claim: Importance sampling with multi-granular features correlates with downstream task performance.
- Mechanism: Documents are featurized using n-grams from the adapted tokenizer; importance weights are computed as the ratio of target-feature to raw-feature distributions. Selecting the top-scoring documents ensures the sampled subset closely matches the target distribution, leading to better zero-shot generalization.
- Core assumption: Feature distributions extracted from multi-granular n-grams are sufficiently discriminative to rank documents by task relevance.
- Evidence anchors:
  - [abstract] "We observed the sampled data to have a high correlation with the target downstream task performance while preserving its effectiveness on other tasks."
  - [section] "We measure its significance through the enhancement weight wi and select a subset of K representative data points from the original target distributions through resampling."
  - [corpus] Weak - no explicit citation or theoretical proof linking importance sampling to task correlation; evidence is empirical.
- Break condition: If target and raw distributions are too divergent, importance weights become unstable and sampling becomes ineffective.

## Foundational Learning

- Concept: Importance sampling and density ratio estimation
  - Why needed here: The method relies on computing and using the ratio pfeat/qfeat to select documents that match the target feature distribution.
  - Quick check question: How does the importance weight wi relate to the ratio of target to raw feature distributions?

- Concept: Vocabulary adaptation via entropy minimization
  - Why needed here: The incremental vocabulary update balances task-specific coverage with general language representation, avoiding overfitting.
  - Quick check question: What role does the entropy metric Hv play in deciding which tokens to prune?

- Concept: Tokenizer tokenization granularity effects
  - Why needed here: Different token granularities (subword, word, multi-word) affect sequence length, hash collision rates, and downstream feature quality.
  - Quick check question: Why might subword tokens increase sampling time despite improving representation?

## Architecture Onboarding

- Component map:
  Tokenizer adaptation pipeline (merge → entropy-based prune) → Document featurization (multi-granular n-gram extraction + hashing) → Importance weight computation (density ratio) → Sampling module (categorical sampling without replacement) → Pretraining loop (decoder-only transformer with causal LM objective)

- Critical path:
  1. Adapt tokenizer vocabulary from base + task data
  2. Featurize all raw documents using adapted tokenizer
  3. Compute importance weights
  4. Sample subset (k examples)
  5. Pretrain model on sampled subset

- Design tradeoffs:
  - Vocabulary size vs. collision reduction: larger vocabularies reduce collisions but increase memory usage.
  - Token granularity mix vs. sampling speed: more subwords improve detail but lengthen sequences and slow sampling.
  - Entropy minimization depth vs. domain bias: deeper pruning improves target fit but risks losing general coverage.

- Failure signatures:
  - Degraded non-target task performance → vocabulary pruning removed too many general tokens.
  - Very slow sampling → subword proportion too high, causing long sequences.
  - No improvement over random sampling → feature distributions too similar between target and raw.

- First 3 experiments:
  1. Compare multi-granular vs. word-only n-gram sampling on a single downstream task with a 125M model.
  2. Vary subword proportion (0%, 30%, 60%, 100%) and measure sampling speed and downstream accuracy.
  3. Test vocabulary pruning depth (t=5 vs. t=10) to find the optimal balance between target fit and generality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of multi-granular n-gram sampling scale with larger model sizes (e.g., 7B or 70B parameters) compared to smaller models (125M to 1.5B)?
- Basis in paper: [inferred] The paper shows performance improvements for 125M to 1.5B parameter models, but does not explore larger model sizes.
- Why unresolved: The paper only experiments with models up to 1.5B parameters, leaving the scalability of the approach to larger models untested.
- What evidence would resolve it: Experiments comparing the performance of multi-granular n-gram sampling across a wider range of model sizes, including very large models, would clarify scalability.

### Open Question 2
- Question: What is the optimal distribution of subword, word, and multi-word tokens in the tokenizer vocabulary for maximizing performance across diverse downstream tasks?
- Basis in paper: [explicit] The paper mentions that a higher percentage of subword (60%) to be the most viable, along with mixing with some word (30%) and multi-word (10%) tokens, but this is based on a fixed vocabulary size and may not generalize.
- Why unresolved: The optimal distribution may vary depending on the specific tasks and datasets, and the paper only provides one configuration.
- What evidence would resolve it: Systematic ablation studies varying the proportions of subword, word, and multi-word tokens across multiple tasks and datasets would identify the optimal distribution.

### Open Question 3
- Question: How does the multi-granular n-gram sampling approach affect the computational efficiency of the pretraining process, especially in terms of memory usage and training time?
- Basis in paper: [inferred] The paper mentions that the sampling process takes around 1.5 days on average, but does not provide detailed analysis of computational efficiency.
- Why unresolved: The paper does not quantify the impact of the approach on memory usage or training time, which are important considerations for practical deployment.
- What evidence would resolve it: Detailed profiling of the pretraining process, including memory usage and training time for different model sizes and datasets, would quantify the computational efficiency.

## Limitations

- The paper lacks explicit theoretical grounding linking the entropy-based vocabulary utility metric to downstream task performance, making it unclear whether the observed improvements stem from improved representation or simply domain-specific vocabulary coverage.
- While the method claims to reduce hash collisions through multi-granular tokenization, no quantitative analysis of collision rates before and after adaptation is provided, leaving the mechanism partially speculative.
- The evaluation focuses exclusively on commonsense reasoning benchmarks, raising questions about generalization to other task types such as generation, translation, or domain-specific knowledge-intensive tasks.

## Confidence

- **High confidence**: Multi-granular sampling consistently outperforms random sampling across all tested model sizes and benchmarks, with measurable average improvements up to 6.94%.
- **Medium confidence**: The mechanism of collision reduction through token granularity diversification is plausible but not directly measured or proven in the paper.
- **Low confidence**: Claims about the vocabulary adaptation process improving representation quality through entropy minimization lack empirical validation beyond downstream task performance.

## Next Checks

1. **Collision Rate Analysis**: Implement a systematic measurement of n-gram hash collision rates across word-only, subword-only, and multi-granular vocabularies to quantify the claimed collision reduction benefit.
2. **Vocabulary Utility Correlation**: Conduct ablation studies varying the entropy-based vocabulary pruning depth and measure the correlation between vocabulary utility metrics and actual downstream performance to validate the adaptation mechanism.
3. **Cross-Domain Generalization**: Evaluate the method on diverse task families beyond commonsense reasoning (e.g., summarization, question answering, code generation) to assess the generality of the claimed benefits and potential domain-specific limitations.