---
ver: rpa2
title: Ultra Fast Transformers on FPGAs for Particle Physics Experiments
arxiv_id: '2402.01047'
source_url: https://arxiv.org/abs/2402.01047
tags:
- transformer
- jets
- particle
- hls4ml
- physics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces a highly efficient implementation of the\
  \ transformer architecture on a Field-Programmable Gate Array (FPGA) using the hls4ml\
  \ tool, achieving latency under 2 \u03BCs on the Xilinx UltraScale+ FPGA. The implementation\
  \ focuses on key components such as multi-head attention and softmax layers, optimized\
  \ for low-latency applications in particle physics experiments, particularly for\
  \ jet flavor tagging."
---

# Ultra Fast Transformers on FPGAs for Particle Physics Experiments

## Quick Facts
- arXiv ID: 2402.01047
- Source URL: https://arxiv.org/abs/2402.01047
- Reference count: 25
- Primary result: Achieved sub-2 μs latency transformer inference on Xilinx UltraScale+ FPGA for jet flavor tagging

## Executive Summary
This work presents a highly optimized implementation of transformer models on FPGAs for real-time particle physics applications. The authors successfully implemented a jet flavor tagging model using the hls4ml toolchain, achieving inference latency under 2 μs on a Xilinx UltraScale+ FPGA. The implementation focuses on key transformer components including multi-head attention and softmax layers, optimized for the specific requirements of particle physics experiments where low-latency processing is critical for event selection.

## Method Summary
The method involves implementing a transformer-based jet flavor tagging model on FPGA using the hls4ml toolchain. The process includes training a transformer model on CMS open data with 3 encoder blocks and 2-head multi-head attention, converting the Keras/TensorFlow model to HLS using hls4ml, and optimizing for FPGA implementation with fixed-point quantization (10 integer + 10 fractional bits). The implementation specifically addresses multi-head attention and softmax layers, which are critical for transformer performance, and explores resource-latency trade-offs through reuse factor adjustments.

## Key Results
- Achieved inference latency of 2.077 μs on Xilinx UltraScale+ FPGA with full parallelization (reuse factor 1)
- Maintained floating-point accuracy using 20-bit fixed-point representation (10 integer, 10 fractional bits)
- Demonstrated successful translation of transformer models to FPGA using hls4ml toolchain

## Why This Works (Mechanism)

### Mechanism 1
Pipelining the four-stage multi-head attention implementation enables sub-2μs latency on VU13P FPGA. Each stage of MHA (linear projection, attention computation with softmax, matrix multiplication, and concatenation/linear transform) is fully pipelined so that new data enters every clock cycle. This hides computation time across stages and achieves the 2.077μs latency with a 6.58ns clock period and 49-cycle latency. Core assumption: The FPGA fabric and DSP resources are sufficient to fully parallelize all operations within each stage (reuse factor=1). Break condition: If reuse factor >1 or resource constraints force sequential execution, latency increases proportionally (e.g., 3.467 μs for reuse factor=2, 5.853 μs for reuse factor=4).

### Mechanism 2
Fixed-point quantization with 10 integer and 10 fractional bits preserves model accuracy while enabling efficient FPGA inference. Reducing numerical precision from 32-bit floating-point to 20-bit fixed-point (10.10 format) drastically reduces DSP and LUT usage without significant accuracy loss, as evidenced by AUC ratio approaching 1.0. Core assumption: The particle physics jet tagging task is tolerant to moderate quantization error; 10 integer bits are sufficient to represent the dynamic range of feature values. Break condition: If the task requires higher numerical precision (e.g., more complex physics models), accuracy degradation will occur below 10.10 fixed-point.

### Mechanism 3
The hls4ml toolchain automatically translates Keras/TF models into HLS-compatible code, enabling rapid prototyping of transformer models on FPGAs. hls4ml parses the high-level neural network description (Keras layers, parameters) and generates optimized HLS C++ code, including custom implementations of MHA and softmax layers, which are then synthesized for the target FPGA. Core assumption: The high-level model description fully captures all operations needed for FPGA implementation; no unsupported layer types are used. Break condition: If the model contains unsupported operations (e.g., unsupported normalization or positional encoding), hls4ml translation will fail or require manual intervention.

## Foundational Learning

- **FPGAs and HLS**: Understanding Field-Programmable Gate Arrays and High-Level Synthesis is essential since the entire implementation targets FPGA acceleration using HLS to convert high-level C++ into hardware configurations; knowing resource constraints (DSPs, LUTs, BRAM) is key for optimizing reuse factor and quantization. Quick check: What is the difference between a DSP block and a LUT in FPGA resource usage, and why does MHA layer use both?

- **Transformer architecture and MHA**: The core contribution is implementing MHA efficiently on FPGA; understanding scaled dot-product attention, linear projections, and concatenation is necessary to follow the four-stage pipeline. Quick check: How does the scaled dot-product attention score computation differ from standard dot-product, and why is the scaling factor √dk important?

- **Fixed-point arithmetic and quantization**: The work uses 20-bit fixed-point instead of floating-point to reduce resource usage; knowing how to choose integer vs fractional bits is key to balancing accuracy and performance. Quick check: If a model uses 8-bit integer and 12-bit fractional fixed-point, what is the maximum representable value and the smallest positive step?

## Architecture Onboarding

- **Component map**: Input stage (track feature vectors) → Linear projection (Q, K, V matrices) → Attention computation (dot-product Q·K^T, scaling, softmax) → Matrix multiplication (scores·V) → Concatenation + linear → Feed-forward block (8→6 units) → Classifier (Flatten → Dense(32) → Dense(16) → Dense(8) → Softmax(3))

- **Critical path**: The four-stage MHA pipeline (linear→attention→matmul→concat/linear) is the primary throughput limiter; each stage must complete within the clock period to maintain full pipelining.

- **Design tradeoffs**: Reuse factor: Lower reuse factor (1) maximizes parallelism but consumes more DSPs/LUTs; higher reuse factor reduces resource usage at the cost of increased latency. Fixed-point precision: More fractional bits improve accuracy but require more LUTs for operations like softmax LUTs; integer bits must cover dynamic range. Memory layout: Efficient use of BRAM vs LUT RAM for storing K, V matrices impacts bandwidth and resource usage.

- **Failure signatures**: Latency >2μs: Likely due to high reuse factor or insufficient parallelization in one MHA stage. Accuracy drop >5%: Probably from insufficient fractional bits or aggressive quantization. HLS synthesis failure: Often caused by unsupported operations or resource overutilization (DSPs/LUTs exceeding device limits).

- **First 3 experiments**: 1) Synthesize a minimal transformer (1 encoder block, 1 head, 1 sequence length) with reuse factor=1 and 10.10 fixed-point; verify latency and resource usage. 2) Sweep reuse factor from 1 to 4 while keeping precision fixed; plot latency vs DSP/LUT usage to identify optimal point. 3) Sweep fractional bits from 6 to 12 with reuse factor=1; measure AUC ratio and resource usage to find minimum precision meeting accuracy target.

## Open Questions the Paper Calls Out

- **Question**: How does the transformer model's performance scale with larger sequence lengths beyond the 15-track limit studied?
- **Basis**: The paper notes the model uses a maximum sequence length of 15 tracks and states "It can readily adapt to models with different configurations, such as varying sequence lengths" but does not explore longer sequences.
- **Why unresolved**: The study was limited to 15 tracks for practical FPGA resource constraints, leaving the scaling behavior for longer sequences unexplored.
- **What evidence would resolve it**: Testing the model with sequence lengths of 20, 30, 50+ tracks on the FPGA while measuring latency, accuracy, and resource utilization would provide concrete data on scalability limits.

## Limitations

- The work assumes the CMS open data sample and specific jet reconstruction parameters are available and reproducible, which may not be trivial for external researchers.
- Resource utilization and latency are reported for a specific FPGA (Xilinx UltraScale+ VU13P) and may vary on different devices or HLS tool versions.
- The study focuses on a fixed quantization scheme (10 integer, 10 fractional bits) and does not explore adaptive precision or alternative quantization methods that could yield better accuracy-resource trade-offs.

## Confidence

- **High**: The core claim that a transformer model can achieve sub-2 μs latency on FPGA using hls4ml is well-supported by the provided synthesis results and latency measurements.
- **Medium**: The claim that 10.10 fixed-point precision is sufficient to match floating-point accuracy is supported by ablation studies, but the exact sensitivity to precision changes for different datasets or model architectures is not explored.
- **Medium**: The assertion that the hls4ml toolchain can automatically translate Keras/TF models to FPGA HLS code is supported by the methodology, but the generality of this approach for more complex transformer variants (e.g., with normalization layers) is not demonstrated.

## Next Checks

1. Reproduce the model training and quantization process on the CMS open data sample to verify the reported AUC ratio and confirm the necessity of at least 10 integer and 10 fractional bits for fixed-point representation.

2. Synthesize the described transformer model for multiple FPGA devices and HLS tool versions to assess the portability and robustness of the latency and resource usage claims.

3. Implement and test alternative softmax computation methods (e.g., iterative or hybrid approaches) and compare their accuracy and resource usage against the LUT-based method used in this work.