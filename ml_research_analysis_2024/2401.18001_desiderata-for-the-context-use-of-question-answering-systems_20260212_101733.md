---
ver: rpa2
title: Desiderata for the Context Use of Question Answering Systems
arxiv_id: '2401.18001'
source_url: https://arxiv.org/abs/2401.18001
tags:
- knowledge
- context
- questions
- language
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a set of desiderata for evaluating context-based
  question answering systems, unifying previously discussed aspects such as robustness
  to noise and conflicting knowledge. It surveys relevant literature and evaluates
  15 models on 5 datasets according to all desiderata simultaneously.
---

# Desiderata for the Context Use of Question Answering Systems

## Quick Facts
- **arXiv ID**: 2401.18001
- **Source URL**: https://arxiv.org/abs/2401.18001
- **Reference count**: 27
- **Key outcome**: The paper proposes a set of desiderata for evaluating context-based question answering systems, unifying previously discussed aspects such as robustness to noise and conflicting knowledge. It surveys relevant literature and evaluates 15 models on 5 datasets according to all desiderata simultaneously. Key findings include: models that are less susceptible to noise are not necessarily more consistent with their answers when given irrelevant context; most systems that are more susceptible to noise are more likely to correctly answer according to a context that conflicts with their parametric knowledge; and the combination of conflicting knowledge and noise can reduce system performance by up to 96%. These trends provide insights into model behavior and potential avenues for improvement.

## Executive Summary
This paper addresses the challenge of evaluating context-based question answering systems by proposing a unified set of desiderata that capture key behaviors such as robustness to noise, consistency with irrelevant contexts, and susceptibility to conflicting knowledge. The authors systematically survey the literature and identify 10 relevant desiderata, then propose a comprehensive evaluation framework that tests all desiderata simultaneously. By splitting evaluation into known vs. unknown knowledge based on models' parametric capabilities, the framework reveals nuanced interactions between context relevance and model behavior that previous evaluations missed.

## Method Summary
The authors evaluate 15 QA systems (5 free-form and 10 MC-based) on 5 datasets (SQuAD 1.0, AdversarialQA, Natural Questions, SciQ, MedMCQA) using a toolkit that generates perturbed contexts for each desideratum. Models are evaluated on exact match (EM) for free-form QA and accuracy for MCQA across standard, distractor, conflicting knowledge, and irrelevant context settings. The evaluation is split by known vs. unknown knowledge based on each model's ability to answer questions without context. Fine-tuning is performed on MC-based models for 20 epochs with early stopping on SciQ and MedMCQA training sets.

## Key Results
- Models less susceptible to noise are not necessarily more consistent with their answers when provided irrelevant context
- Most systems more susceptible to noise are more likely to correctly answer when given conflicting knowledge
- The combination of conflicting knowledge and noise can reduce system performance by up to 96%
- Performance trends differ significantly between known and unknown knowledge splits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Splitting evaluation into known vs. unknown knowledge reveals how models' internal parametric knowledge interacts with context.
- Mechanism: By evaluating each model twice—once on questions it can answer without context (known) and once on those it cannot (unknown)—the approach isolates whether model performance depends on context relevance or model confidence in its stored facts.
- Core assumption: The set of questions a model can answer without context reflects its parametric knowledge, and this knowledge is static during evaluation.
- Evidence anchors:
  - [abstract]: "We then evaluate 15 QA systems on 5 datasets according to all desiderata at once. ... (1) systems that are less susceptible to noise are more consistent with their answers when provided with irrelevant context..."
  - [section]: "we split desiderata aspects by finding the context that is known and unknown to individual models, as the ideal behavior of models’ depends on if the knowledge contained in the context is known or unknown to the model."
- Break condition: If a model's parametric knowledge changes between evaluations or if the definition of "known" is noisy, the split could misclassify questions and obscure true model behavior.

### Mechanism 2
- Claim: Combining conflicting knowledge with distractors magnifies model fragility, revealing weak contextual integration.
- Mechanism: Adding a distracting sentence to a context that already conflicts with a model's stored knowledge creates compounded cognitive load, causing performance to collapse because the model cannot reconcile conflicting signals.
- Core assumption: Models prioritize either parametric knowledge or context, but not both simultaneously, so conflicting signals overwhelm their reasoning.
- Evidence anchors:
  - [abstract]: "...the combination of conflicting knowledge and noise can reduce system performance by up to 96%."
  - [section]: "Looking at the combination of distractors with conflicting contexts, we find that the performance drop is generally lower in the unknown split for most models."
- Break condition: If models develop better context weighting or conflict resolution mechanisms, the compounded drop may diminish or disappear.

### Mechanism 3
- Claim: Models more sensitive to noise are also more likely to correctly answer when given conflicting knowledge.
- Mechanism: Noise sensitivity implies the model attends closely to context tokens, which paradoxically aids in recognizing and using new information even when it contradicts stored facts.
- Core assumption: High attention to context details is uniformly beneficial for both noise resilience and conflicting knowledge detection.
- Evidence anchors:
  - [abstract]: "(2) most systems that are more susceptible to noise are more likely to correctly answer according to a context that conflicts with their parametric knowledge;"
  - [section]: "we find that systems that are more susceptible to noise are often more likely to correctly answer according to a context that conflicts with their parametric knowledge."
- Break condition: If noise sensitivity stems from brittle pattern matching rather than robust context integration, the correlation may not hold.

## Foundational Learning

- Concept: Parametric vs. contextual knowledge distinction
  - Why needed here: The evaluation hinges on knowing whether a model can answer without context, which defines the "known" set; misclassifying this undermines all downstream desiderata analysis.
  - Quick check question: Can the model answer this question correctly if you remove all context and rely only on its parameters?

- Concept: Noise robustness metrics
  - Why needed here: Performance degradation under distractors quantifies how well a model can filter irrelevant information; without this, robustness cannot be measured.
  - Quick check question: Does adding a random sentence to the context cause the model's accuracy to drop by more than 50%?

- Concept: Conflict detection and resolution
  - Why needed here: Conflicting contexts test whether the model can override stored knowledge with new evidence; understanding this behavior is key to evaluating real-world adaptability.
  - Quick check question: When the context contradicts the model's stored answer, does the model switch to the new answer or cling to the old one?

## Architecture Onboarding

- Component map: Data preparation pipeline -> Dataset split (known/unknown) -> Context perturbation modules (distractor, conflicting, irrelevant) -> Model evaluation harness -> Result aggregation and comparison engine
- Critical path: Prepare datasets -> Generate perturbed contexts -> Evaluate models under each desideratum -> Compare across models and perturbations -> Extract trends
- Design tradeoffs: Exact match vs. semantic similarity metrics (speed vs. nuance); synthetic distractors vs. natural irrelevant contexts (control vs. realism); manual annotation vs. automated splits (accuracy vs. scalability)
- Failure signatures: Large accuracy variance across perturbations without clear trend; inconsistent known/unknown splits across models; distractor perturbation fails to lower perplexity or confidence
- First 3 experiments:
  1. Run the closed-book evaluation to confirm known vs. unknown split accuracy per model
  2. Apply the distractor perturbation to a small subset and verify performance drop >50%
  3. Introduce conflicting context to the same subset and check if the model switches answers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of QA models on conflicting knowledge tasks change when using different types of conflicting contexts (e.g., entity substitution vs. negation)?
- Basis in paper: [explicit] The paper discusses various methods for creating conflicting contexts, including entity substitution and negation, and mentions that the type of conflicting information matters.
- Why unresolved: The paper evaluates models on conflicting knowledge but does not specifically analyze how different methods of creating conflicting contexts affect model performance.
- What evidence would resolve it: Experiments comparing model performance on conflicting knowledge tasks using different types of conflicting contexts (e.g., entity substitution vs. negation) would provide insights into how the type of conflicting information affects model robustness.

### Open Question 2
- Question: Can training models with data augmentation specifically designed for conflicting knowledge improve their robustness to such knowledge?
- Basis in paper: [explicit] The paper mentions that some proposed approaches suggest training with data augmentation to improve robustness to conflicting knowledge.
- Why unresolved: While the paper discusses the potential of data augmentation, it does not provide empirical evidence on its effectiveness for improving robustness to conflicting knowledge.
- What evidence would resolve it: Experiments evaluating the impact of training models with data augmentation specifically designed for conflicting knowledge on their robustness to such knowledge would provide empirical evidence on its effectiveness.

### Open Question 3
- Question: How does the combination of conflicting knowledge and noise affect the performance of QA models compared to the individual effects of each?
- Basis in paper: [explicit] The paper finds that the combination of conflicting knowledge and noise can reduce system performance by up to 96%.
- Why unresolved: While the paper reports the combined effect, it does not analyze how this effect compares to the individual effects of conflicting knowledge and noise on model performance.
- What evidence would resolve it: Experiments isolating the effects of conflicting knowledge and noise on model performance and comparing them to their combined effect would provide insights into their individual and combined impacts.

## Limitations

- The evaluation relies on exact match (EM) and accuracy metrics that may not capture nuanced model behavior in real-world scenarios
- The framework assumes parametric knowledge is static and well-defined, which may not hold for modern LLMs with context-dependent knowledge retrieval
- The conflicting knowledge perturbation method (masking and replacing answers) may introduce artifacts that don't reflect genuine knowledge conflicts
- The analysis focuses on English-language datasets, limiting generalizability to multilingual contexts

## Confidence

**High Confidence**: The core finding that noise sensitivity correlates with better performance on conflicting knowledge tasks is supported by clear quantitative results showing consistent trends across multiple models and datasets.

**Medium Confidence**: The 96% performance drop when combining conflicting knowledge and noise is impressive but may be sensitive to the specific perturbation method used.

**Low Confidence**: The interpretation that splitting evaluation by known vs. unknown knowledge reveals fundamental aspects of model reasoning relies on the assumption that the split accurately reflects model parametric knowledge.

## Next Checks

1. **Replication with alternative perturbation methods**: Validate the conflicting knowledge + noise performance drop using natural distractors from the original contexts rather than synthetic masking, to ensure the effect isn't an artifact of the perturbation technique.

2. **Cross-linguistic evaluation**: Test the same desiderata framework on multilingual QA datasets to assess whether the observed trends hold across different languages and cultural contexts.

3. **Temporal stability analysis**: Evaluate model performance on the same desiderata across multiple inference runs to determine whether the observed behaviors are consistent or subject to stochastic variation in model outputs.