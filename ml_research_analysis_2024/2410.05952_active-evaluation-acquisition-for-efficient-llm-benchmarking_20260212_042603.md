---
ver: rpa2
title: Active Evaluation Acquisition for Efficient LLM Benchmarking
arxiv_id: '2410.05952'
source_url: https://arxiv.org/abs/2410.05952
tags:
- evaluation
- prompts
- scores
- acquisition
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficient evaluation of large
  language models (LLMs) by reducing the number of evaluation prompts needed while
  maintaining accurate performance estimates. The core idea is to model dependencies
  across evaluation prompts using a neural process that captures the relationships
  between prompts and their evaluation scores.
---

# Active Evaluation Acquisition for Efficient LLM Benchmarking

## Quick Facts
- arXiv ID: 2410.05952
- Source URL: https://arxiv.org/abs/2410.05952
- Authors: Yang Li; Jie Ma; Miguel Ballesteros; Yassine Benajiba; Graham Horwood
- Reference count: 14
- Key outcome: Reduces evaluation prompts needed while maintaining accurate performance estimates for LLM benchmarking

## Executive Summary
This paper addresses the challenge of efficiently evaluating large language models (LLMs) by significantly reducing the number of evaluation prompts needed while maintaining accurate performance estimates. The core innovation is an active evaluation acquisition (AEA) framework that models dependencies across evaluation prompts using a neural process, allowing prediction of evaluation outcomes for unobserved prompts based on observed ones. The approach is validated across five popular LLM benchmarks (MMLU, HELM, HuggingFace Open LLM Leaderboard, AlpacaEval, and Chatbot Arena) and demonstrates substantial improvements over existing methods, with RL-based acquisition policies achieving the best performance.

## Method Summary
The method uses a neural process to capture dependencies across evaluation prompts, learning a stochastic process F: X → Y where each prompt's evaluation score is conditioned on observed scores of other prompts. For each model, an acquisition policy selects a subset of K prompts to evaluate, while the neural process predicts the remaining scores. The framework handles cold-start scenarios by treating new prompts as unlabeled data and using pseudo-labeling with uncertainty thresholding. Various acquisition policies are explored, including random sampling, clustering, combinatorial optimization, uncertainty sampling, information gain maximization, and RL-based policies, with the RL approach (using PPO) demonstrating superior performance.

## Key Results
- RL-based acquisition policy outperforms all static methods across all tested benchmarks
- Significant reduction in required evaluation prompts while maintaining accurate performance estimates
- Successful handling of cold-start scenarios through semi-supervised training with new prompts
- Demonstrated effectiveness across five diverse LLM benchmarks (MMLU, HELM, HuggingFace Open LLM Leaderboard, AlpacaEval, and Chatbot Arena)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The method works because evaluation prompts are highly correlated, so observing a subset of outcomes allows accurate prediction of the rest.
- **Mechanism:** The neural process model captures dependencies across prompts by learning a stochastic process F: X → Y, where each prompt's evaluation score is conditioned on the observed scores of other prompts.
- **Core assumption:** Evaluation prompts exhibit strong correlations in their outcomes across models.
- **Evidence anchors:**
  - [abstract] "evaluation prompts are highly correlated, meaning that a model's (in)correct prediction on a certain prompt is likely to correspond with (in)correct predictions on related prompts"
  - [section] "We first observe that evaluation prompts are highly correlated, meaning that a model's (in)correct prediction on a certain prompt is likely to correspond with (in)correct predictions on related prompts"
- **Break condition:** If prompts are not correlated or if the correlations vary significantly across different models, the prediction accuracy would degrade substantially.

### Mechanism 2
- **Claim:** The RL-based acquisition policy outperforms static methods by dynamically adapting to each model's unique strengths and weaknesses.
- **Mechanism:** The policy uses a Markov decision process where states contain acquired prompts and scores, and actions select the next prompt to evaluate based on maximizing information gain or prediction accuracy improvement.
- **Core assumption:** Different models have varying strengths across different types of prompts, making adaptive selection more effective than fixed selection.
- **Evidence anchors:**
  - [section] "Instead of using a fixed subset of prompts across all models, we propose selecting a unique subset of prompts for each model to evaluate its performance more efficiently"
  - [section] "Tailoring the subset of prompts for each model ensures a more accurate and targeted evaluation of its capabilities"
- **Break condition:** If models have similar capabilities across all prompts, or if the policy fails to generalize to new prompts, the advantage disappears.

### Mechanism 3
- **Claim:** The cold-start problem is handled by treating new prompts as unlabeled data and using pseudo-labeling with uncertainty thresholding.
- **Mechanism:** New prompts are added to training with predicted scores only when the neural process has low uncertainty, allowing the model to generalize to prompts without prior evaluation data.
- **Core assumption:** The neural process can generate reliable predictions for new prompts with sufficiently low uncertainty.
- **Evidence anchors:**
  - [section] "We introduce a semi-supervised training procedure where the new prompts are treated as unlabeled data"
  - [section] "We add the new prompts and their predicted evaluation scores into the training process if the uncertainties of the predicted evaluation scores are below a predefined threshold"
- **Break condition:** If the neural process cannot generate reliable predictions for new prompts, or if uncertainty estimates are inaccurate, the cold-start approach fails.

## Foundational Learning

- **Concept:** Neural Processes (NPs)
  - **Why needed here:** NPs provide a flexible framework for modeling dependencies across evaluation prompts by learning a stochastic process that can predict unobserved outcomes based on observed ones.
  - **Quick check question:** What is the key difference between Neural Processes and Gaussian Processes in terms of scalability and representation?

- **Concept:** Active Learning vs Active Evaluation
  - **Why needed here:** Understanding the distinction is crucial - active learning aims to improve model training by selecting informative examples, while active evaluation aims to reduce evaluation costs while maintaining accurate performance estimates.
  - **Quick check question:** How does the objective of selecting evaluation prompts differ from selecting training examples in active learning?

- **Concept:** Reinforcement Learning for Sequential Decision Making
  - **Why needed here:** The RL-based acquisition policy uses RL to solve a sequential decision problem where each action (selecting a prompt) affects future states and the final reward (prediction accuracy).
  - **Quick check question:** What is the main advantage of using RL for prompt selection compared to static methods?

## Architecture Onboarding

- **Component map:** Neural Process Model -> Embedding Model -> Acquisition Policies -> RL Policy Network
- **Critical path:**
  1. Extract prompt embeddings
  2. Train neural process on available evaluation data
  3. Apply acquisition policy to select K prompts per model
  4. Predict remaining scores using neural process
  5. Compute final benchmark performance
- **Design tradeoffs:**
  - Static vs Dynamic policies: Static methods are faster but less adaptive; dynamic methods require more computation but achieve better accuracy
  - Prompt embedding quality: Better embeddings improve prediction accuracy but increase computational cost
  - RL vs heuristic policies: RL provides better performance but requires training and hyperparameter tuning
- **Failure signatures:**
  - High prediction error on test models suggests model bias or insufficient training data
  - RL policy selecting same prompts repeatedly indicates exploration failure
  - Poor cold-start performance suggests neural process cannot generalize to new prompts
- **First 3 experiments:**
  1. Run random sampling baseline to establish performance floor
  2. Test clustering-based static policies to compare embedding quality impact
  3. Evaluate RL policy on a small benchmark to verify training stability and performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the neural process model be adapted to better handle the cold start problem when new prompts are added to benchmarks without any prior evaluation scores?
- Basis in paper: [explicit] The paper discusses the cold start problem and mentions using a semi-supervised training procedure with pseudo-labeling to help the neural process model generalize to new prompts. However, it acknowledges that this approach may not be sufficient and leaves further exploration for future work.
- Why unresolved: The paper only briefly mentions pseudo-labeling as a potential solution and does not provide detailed results or comparisons with other semi-supervised learning techniques. The effectiveness of this approach and its limitations are not fully explored.
- What evidence would resolve it: Systematic experiments comparing pseudo-labeling with other semi-supervised learning techniques (e.g., entropy minimization, consistency regularization) on the cold start problem, along with ablation studies to determine the optimal settings and identify potential limitations.

### Open Question 2
- Question: What are the specific components of the epistemic and aleatoric uncertainty in the neural process model, and how do they affect the performance of uncertainty-based acquisition policies?
- Basis in paper: [inferred] The paper mentions that uncertainty sampling based acquisition policies do not perform well, possibly due to the difficulty in accurately estimating aleatoric uncertainty. It also references active research areas in decomposing these uncertainties in machine learning.
- Why unresolved: The paper does not provide a detailed analysis of the sources of uncertainty in the neural process model or how they impact the effectiveness of uncertainty-based acquisition policies. It suggests that further research is needed in this area.
- What evidence would resolve it: A detailed analysis of the epistemic and aleatoric uncertainty components in the neural process model, along with experiments demonstrating how these uncertainties affect the performance of different acquisition policies. This could include techniques for better estimating and decomposing these uncertainties.

### Open Question 3
- Question: How can the RL-based acquisition policy be improved to handle model bias more effectively, especially when evaluating models from different families or with significantly different capabilities?
- Basis in paper: [explicit] The paper discusses the impact of model bias on the performance of acquisition policies, noting that static policies and random policies are less effective in this scenario. It suggests that a continual learning framework might be necessary to adapt the neural process model and acquisition policies to newly added models.
- Why unresolved: The paper acknowledges the challenge of model bias but does not provide a concrete solution or extensive experiments to evaluate the effectiveness of potential approaches. It leaves the exploration of continual learning frameworks for future work.
- What evidence would resolve it: Experiments evaluating the performance of different acquisition policies in scenarios with varying degrees of model bias, along with comparisons of different continual learning approaches to adapt the neural process model and acquisition policies to new models. This could include techniques for detecting and mitigating model bias in the acquisition process.

## Limitations
- RL-based policy requires significant computational resources for training and may not generalize well to entirely new benchmarks
- Method assumes evaluation prompts maintain consistent correlations across different model families, which may not hold as LLMs evolve
- Cold-start semi-supervised training relies on uncertainty estimates that may be unreliable for truly novel prompt types

## Confidence

**High Confidence Claims:**
- The core mechanism that evaluation prompts exhibit correlations that can be exploited for prediction
- The neural process framework effectively models these dependencies

**Medium Confidence Claims:**
- The RL-based acquisition policy consistently outperforms static methods
- The cold-start semi-supervised approach successfully handles new prompts

**Low Confidence Claims:**
- The method's performance on benchmarks outside the tested domains
- The scalability of the approach to extremely large benchmarks with thousands of prompts

## Next Checks
1. **Distribution Shift Validation**: Test the model's performance when training and evaluation sets contain models from different time periods or development stages to assess robustness to distribution shift.
2. **Cross-Domain Generalization**: Evaluate the method on a completely different type of benchmark (e.g., mathematical reasoning or code generation) that was not included in the original training data to test the framework's generalizability.
3. **Prompt Embedding Ablation**: Conduct controlled experiments varying the quality and type of prompt embeddings to quantify the impact of embedding quality on prediction accuracy and determine the minimum viable embedding approach.