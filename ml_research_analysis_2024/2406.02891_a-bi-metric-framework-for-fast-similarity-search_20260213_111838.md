---
ver: rpa2
title: A Bi-metric Framework for Fast Similarity Search
arxiv_id: '2406.02891'
source_url: https://arxiv.org/abs/2406.02891
tags:
- metric
- distance
- search
- nearest
- neighbor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a bi-metric framework for designing approximate
  nearest neighbor search data structures. The framework uses two distance metrics:
  a cheap but less accurate proxy metric for index construction, and an expensive
  but accurate ground-truth metric for queries.'
---

# A Bi-metric Framework for Fast Similarity Search

## Quick Facts
- **arXiv ID**: 2406.02891
- **Source URL**: https://arxiv.org/abs/2406.02891
- **Reference count**: 40
- **Primary result**: Proposes a bi-metric framework using cheap proxy metric for index construction and expensive ground-truth metric for queries, achieving arbitrarily good approximation guarantees with sublinear expensive distance evaluations

## Executive Summary
This paper introduces a bi-metric framework for approximate nearest neighbor search that leverages two distance metrics: a cheap proxy metric for index construction and an expensive ground-truth metric for queries. The key insight is that even if the proxy metric only approximates the ground-truth metric up to a constant factor, the framework can still achieve arbitrarily good approximation guarantees by making a sublinear number of expensive distance evaluations during queries. The framework is theoretically instantiated for DiskANN and Cover Tree algorithms, showing significant improvements in accuracy-efficiency tradeoffs compared to re-ranking baselines on 15 MTEB text retrieval datasets.

## Method Summary
The bi-metric framework constructs a graph index using a cheap proxy metric d, then performs a two-stage search for queries. First, it uses d to quickly identify a local neighborhood around the query point. Second, it performs a greedy search using the expensive ground-truth metric D starting from multiple points returned by the first stage. The framework is theoretically instantiated for DiskANN and Cover Tree algorithms, showing that a net constructed for d is also a valid net for D with a larger scale. Experiments use MTEB datasets with bge-micro-v2/gte-small/bge-base-en-v1.5 as proxy metrics and SFR-Embedding-Mistral as the ground-truth metric.

## Key Results
- Achieves 3-5x better Recall@10 than re-ranking baseline while using the same number of ground-truth metric evaluations
- Converges faster than single-metric approaches even when allowed infinite expensive distance calls during index construction
- Using multiple starting points (top-Q/2) for second-stage search significantly improves accuracy over single-point initialization
- Demonstrates consistent improvements across 15 MTEB text retrieval datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework achieves arbitrarily good approximation guarantees for the expensive metric D while using only a sublinear number of D evaluations during queries.
- Mechanism: By constructing a data structure using only the proxy metric d, which approximates D up to a constant factor C, and then performing a second-stage search using D from multiple starting points returned by the first-stage search, the algorithm can route queries more efficiently than naive re-ranking.
- Core assumption: The proxy metric d is a C-approximation of D (Equation 1), and the underlying nearest neighbor algorithms (DiskANN, Cover Tree) have the property that a net constructed for d is also a valid net for D, albeit with a larger scale.
- Evidence anchors:
  - [abstract]: "Our theoretical results instantiate this framework for two popular nearest neighbor search algorithms: DiskANN and Cover Tree. Specifically, we show the following theorem statement... For any Alg ∈ {DiskANN, Cover Tree}, we can build a corresponding datastructure DAlg on X with the following properties: ... The space used by DAlg can be bounded by ˜O(SAlg(n, ε/C, λd))..."
  - [section]: "Both algorithms (implicitly or explicitly), construct nets of various scales r which help route queries to their nearest neighbors in the dataset. The key insight is that a net of scale r for metric d is also a net under metric D, but with the larger scale Cr."
  - [corpus]: Weak - no direct corpus evidence for the net scaling property; relies on theoretical analysis.
- Break condition: If the proxy metric d does not satisfy the C-approximation property (Equation 1), the framework cannot guarantee accuracy for D.

### Mechanism 2
- Claim: Starting the second-stage search from multiple points returned by the first-stage search significantly improves retrieval accuracy compared to starting from a single point or the default entry point.
- Mechanism: The approximation error of the cheap distance function d doesn't matter much in the earlier stage of the search, so the first stage search with d can quickly get to the true 'local' neighborhood without any expensive distance calls. Starting from multiple points accounts for the ranking inaccuracies of d and gives better results than solely starting from the top few.
- Core assumption: The first-stage search with the cheap metric d can efficiently identify a local neighborhood around the true nearest neighbor.
- Evidence anchors:
  - [section]: "Using multiple starting points further speeds up the second stage search. If we only start with the top-1 point from the first stage search (brown), its NDCG curve is still worse than Bi-metric (baseline, red) and Single metric (orange). As we switch to top-100 (purple) or top-Q/2 (blue) starting points, the NDCG curves increase evidently."
  - [section]: "We provide two intuitive explanations for these phenomena. First, the approximation error of the cheap distance function doesn't matter that much in the earlier stage of the search, so the first stage search with the cheap distance function can quickly get to the true 'local' neighborhood without any expensive distance calls, thus saving resource for the second stage search. Second, the ranking provided by the cheap distance function is not accurate because of its approximation error, so starting from multiple points should give better results than solely starting from the top few, which also justifies the advantage of our second-stage search over re-ranking."
  - [corpus]: Weak - no direct corpus evidence for the multi-point initialization benefit; relies on experimental results.
- Break condition: If the first-stage search fails to identify any points near the true nearest neighbor, the second-stage search will not benefit from the multi-point initialization.

### Mechanism 3
- Claim: The bi-metric framework outperforms single-metric approaches even when the single-metric approach is allowed infinite expensive distance calls during index construction.
- Mechanism: The quality of the underlying graph index is not as important as the ability to guide early routing steps with a cheap distance proxy, which saves expensive distance function calls. The bi-metric approach leverages the proxy metric to construct the index and then uses the expensive metric only during queries.
- Core assumption: Guiding early routing steps with a cheap distance proxy is more efficient than constructing a high-quality index using only the expensive metric.
- Evidence anchors:
  - [section]: "This means that utilizing the graph index built for the distance function proxy to perform a greedy search using D is more efficient than naively iterating the returned vertex list to re-rank using D (baseline). It is also noteworthy to see that our method converges faster than 'Single metric' in all the datasets except FiQA2018 and TRECCOVID, especially in the earlier stages. This phenomenon happens even if 'Single metric' is allowed infinite expensive distance function calls in its indexing phase to build the ground truth graph index."
  - [section]: "Therefore, we believe that the ideal scenario for our method is a small and efficient model deployed locally, paired with a remote large model accessed online through API calls to maximize the advantages of our method."
  - [corpus]: Weak - no direct corpus evidence for the efficiency of proxy-guided routing; relies on experimental results.
- Break condition: If the cheap proxy metric provides very poor guidance (e.g., completely random rankings), the bi-metric framework may not outperform the single-metric approach.

## Foundational Learning

- Concept: C-approximation of metrics (Definition 2.1)
  - Why needed here: The bi-metric framework relies on the proxy metric d being a C-approximation of the ground truth metric D to guarantee accuracy.
  - Quick check question: If d(x, y) ≤ D(x, y) ≤ 3 * d(x, y) for all x, y, what is the value of C in the C-approximation definition?

- Concept: Doubling dimension (Definition 2.2)
  - Why needed here: The doubling dimension λd is a measure of intrinsic dimensionality used in analyzing the space and query complexity of the nearest neighbor algorithms.
  - Quick check question: What is the doubling dimension of a set of points uniformly distributed in a unit cube in d-dimensional space?

- Concept: α-shortcut reachability (Definition 3.1)
  - Why needed here: The α-shortcut reachability property of the graph index is crucial for the theoretical analysis of DiskANN under the bi-metric framework.
  - Quick check question: If a graph is α-shortcut reachable under metric d, what is its shortcut reachability under metric D, given that D is a C-approximation of d?

## Architecture Onboarding

- Component map: Proxy metric model (d) -> Graph index -> First-stage search (d) -> Top-Q/2 points -> Second-stage search (D) -> Final retrieval
- Critical path:
  1. Index construction: Build graph index using proxy metric d
  2. First-stage search: Query graph index with proxy metric d to find top-Q/2 nearest neighbors
  3. Second-stage search: Initialize second-stage search from top-Q/2 points using ground truth metric D
  4. Final retrieval: Return top-10 results based on ground truth metric D
- Design tradeoffs:
  - Tradeoff between proxy metric accuracy and computational cost: More accurate proxy metrics may be more expensive to evaluate but can provide better first-stage search results
  - Tradeoff between number of second-stage starting points and query efficiency: More starting points can improve accuracy but increase the number of ground truth metric evaluations
  - Tradeoff between graph index quality and index construction cost: Higher quality indices may require more proxy metric evaluations during construction
- Failure signatures:
  - Poor first-stage search results: If the proxy metric d provides very inaccurate rankings, the second-stage search will start from suboptimal points
  - Excessive ground truth metric evaluations: If the number of second-stage starting points is too large or the graph index is of poor quality, the query may exceed the budget of ground truth metric evaluations
  - Degraded accuracy: If the proxy metric d is not a good approximation of D, the final retrieval results may be inaccurate
- First 3 experiments:
  1. Implement the bi-metric framework using a simple proxy metric (e.g., cosine similarity) and a ground truth metric (e.g., cross-encoder) on a small text retrieval dataset
  2. Compare the retrieval accuracy and number of ground truth metric evaluations between the bi-metric framework, single-metric approach, and re-ranking baseline
  3. Vary the number of second-stage starting points and measure the impact on retrieval accuracy and query efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the bi-metric framework be theoretically instantiated for other popular nearest neighbor algorithms beyond DiskANN and Cover Tree, such as those based on locality sensitive hashing (LSH)?
- Basis in paper: Explicit - The paper states "it is an interesting open direction to determine if our bi-metric framework can be theoretically instantiated for other popular nearest neighbor algorithms, such as those based on locality sensitive hashing."
- Why unresolved: The paper only provides theoretical analysis for DiskANN and Cover Tree algorithms. The fundamental difference in how LSH works compared to graph-based methods may prevent a similar framework from being applicable.
- What evidence would resolve it: A theoretical proof showing that the bi-metric framework can be extended to LSH or other popular algorithms, or a demonstration that the framework fundamentally cannot work for certain algorithm types.

### Open Question 2
- Question: How does the performance of the bi-metric framework scale with increasing dataset size and dimensionality?
- Basis in paper: Inferred - While the paper shows results on 15 MTEB datasets, it doesn't explicitly discuss scaling behavior with dataset size or dimensionality.
- Why unresolved: The paper doesn't provide theoretical or empirical analysis of how the framework's performance changes as the dataset grows larger or becomes higher-dimensional.
- What evidence would resolve it: Experimental results showing performance on much larger datasets, or theoretical analysis of space and query complexity as a function of n and dimension.

### Open Question 3
- Question: What is the optimal strategy for selecting the proxy metric and ground-truth metric pairs to maximize the efficiency gains of the bi-metric framework?
- Basis in paper: Explicit - The paper shows results using different model pairs but doesn't provide a systematic approach for selecting optimal metric pairs.
- Why unresolved: The paper demonstrates that performance varies with different metric pairs but doesn't provide guidelines for choosing the best pairs.
- What evidence would resolve it: A theoretical or empirical study quantifying the relationship between proxy metric accuracy, ground-truth metric accuracy, and overall framework performance, possibly leading to a recommendation system for selecting metric pairs.

## Limitations

- The theoretical guarantees rely on the proxy metric being a C-approximation of the ground truth metric, which may not hold for all distance function pairs
- The framework's performance depends on the quality of the graph index constructed using the proxy metric, and suboptimal index parameters could degrade retrieval accuracy
- The paper lacks rigorous proof that the specific proxy metrics used (bge-micro-v2, gte-small, bge-base-en-v1.5) satisfy the C-approximation property for the ground truth metric (SFR-Embedding-Mistral)

## Confidence

Medium - The experimental results on 15 MTEB datasets demonstrate clear improvements over re-ranking baselines and single-metric approaches in terms of accuracy-efficiency tradeoffs. However, the theoretical analysis relies on idealized assumptions about the doubling dimension and the C-approximation property, which may not hold exactly in practice. The empirical success across diverse datasets suggests the framework is robust, but the lack of theoretical justification for the specific proxy-ground truth pairs used is a limitation.

## Next Checks

1. **Theoretical validation of proxy metric approximation**: Rigorously verify whether the proxy metrics (bge-micro-v2, gte-small, bge-base-en-v1.5) satisfy the C-approximation property for the ground truth metric (SFR-Embedding-Mistral) on a held-out validation set. This can be done by computing the ratio D(x,y)/d(x,y) for random pairs and checking if it is bounded by a constant C.

2. **Ablation study on proxy metric quality**: Conduct an ablation study to measure the impact of proxy metric quality on retrieval accuracy and efficiency. Use proxy metrics with varying levels of approximation accuracy (e.g., different model sizes or training objectives) and compare the bi-metric framework's performance against the single-metric approach and re-ranking baseline.

3. **Robustness to proxy metric approximation error**: Investigate the framework's robustness to proxy metrics that only approximately satisfy the C-approximation property. Introduce controlled noise or bias to the proxy metric and measure the degradation in retrieval accuracy and efficiency. This will help quantify the sensitivity of the framework to approximation errors and guide the selection of proxy metrics in practice.