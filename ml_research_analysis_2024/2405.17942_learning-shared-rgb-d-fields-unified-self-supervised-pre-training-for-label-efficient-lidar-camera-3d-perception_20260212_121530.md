---
ver: rpa2
title: 'Learning Shared RGB-D Fields: Unified Self-supervised Pre-training for Label-efficient
  LiDAR-Camera 3D Perception'
arxiv_id: '2405.17942'
source_url: https://arxiv.org/abs/2405.17942
tags:
- multi-modal
- pre-training
- perception
- lidar
- ns-mae
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NS-MAE, a unified self-supervised pre-training
  framework for LiDAR-Camera 3D perception models. Existing pre-training methods use
  separate strategies for each modality, limiting their ability to capture shared
  features.
---

# Learning Shared RGB-D Fields: Unified Self-supervised Pre-training for Label-efficient LiDAR-Camera 3D Perception

## Quick Facts
- **arXiv ID:** 2405.17942
- **Source URL:** https://arxiv.org/abs/2405.17942
- **Authors:** Xiaohao Xu; Ye Li; Tianyi Zhang; Jinrong Yang; Matthew Johnson-Roberson; Xiaonan Huang
- **Reference count:** 40
- **Primary result:** Unified self-supervised pre-training framework using NeRF rendering outperforms SOTA in label-efficient BEV segmentation

## Executive Summary
This paper introduces NS-MAE, a unified self-supervised pre-training framework for LiDAR-Camera 3D perception models. Existing pre-training methods use separate strategies for each modality, limiting their ability to capture shared features. NS-MAE addresses this by leveraging NeRF's neural rendering mechanism to encode both appearance and geometry, enabling efficient masked reconstruction of multi-modal data. The framework extracts embeddings from corrupted LiDAR point clouds and images, conditions them on view directions and locations, and renders them into multi-modal feature maps from perspective and bird's-eye views. Extensive experiments demonstrate NS-MAE's superior transferability across various 3D perception tasks under different fine-tuning settings, particularly excelling in BEV map segmentation under label-efficient scenarios.

## Method Summary
NS-MAE is a unified self-supervised pre-training framework that leverages NeRF-style neural rendering to learn shared representations across LiDAR and camera modalities. The method applies random masking to both image patches (50%) and LiDAR voxels (70-90%), then uses an embedding network to extract features from the corrupted inputs. These features are conditioned on view directions and spatial locations, then rendered into color and depth maps using a NeRF rendering network. The original uncorrupted data serve as reconstruction targets, with L2 loss for color and L1 loss for depth across both BEV and perspective viewpoints. This unified approach enables the model to jointly learn spatial and semantic representations that transfer effectively to downstream 3D perception tasks under label-efficient fine-tuning conditions.

## Key Results
- NS-MAE outperforms state-of-the-art methods like PRC, SimIPU, BEVDistill, and CALICO in BEV map segmentation under label-efficient fine-tuning
- Achieves superior transferability across various 3D perception tasks including 3D object detection with nuScenes detection score (NDS) improvements
- Demonstrates effectiveness across different fine-tuning data ratios (1%, 5%, 10%, 100%) with consistent performance gains
- Shows robust performance on both large-scale nuScenes and smaller KITTI-3D datasets

## Why This Works (Mechanism)

### Mechanism 1
Masked reconstruction of multi-modal inputs forces the network to learn shared, generalizable representations rather than trivial identity mappings. By randomly masking large portions (50% for images, 70-90% for LiDAR) of each modality and requiring the network to reconstruct them, the model must infer missing information from partial observations, thereby capturing high-level shared features across modalities.

### Mechanism 2
NeRF-style differentiable rendering unifies spatial and semantic learning across modalities by aligning LiDAR's geometry with camera's appearance. The embedding network outputs latent features conditioned on view directions and spatial locations, which are then rendered into color and depth maps using volume rendering. This forces the model to jointly learn appearance (color) and geometry (depth) in a coherent physical model.

### Mechanism 3
Self-supervised reconstruction with multi-modal targets improves cross-task transferability compared to modality-specific pre-training. Training the shared embedding network to reconstruct both camera images and LiDAR depth maps ensures the learned representations are modality-agnostic and robust to different downstream tasks.

## Foundational Learning

- **Concept:** Masked Autoencoder (MAE) reconstruction
  - Why needed here: MAE-style masking and reconstruction are core to preventing trivial identity mappings and forcing the model to learn generalizable features from partial observations.
  - Quick check question: What masking ratio prevents trivial identity mappings while still allowing reconstruction?

- **Concept:** Neural Radiance Fields (NeRF) differentiable rendering
  - Why needed here: NeRF provides a physically interpretable way to encode both appearance and geometry, enabling the network to learn structured multi-modal representations.
  - Quick check question: How does conditioning the rendering network on view direction and spatial location help unify modalities?

- **Concept:** Multi-modal embedding fusion
  - Why needed here: Fusing aligned camera and LiDAR embeddings in world coordinates allows the model to learn complementary spatial and semantic information from both modalities.
  - Quick check question: Why is aligning embeddings to a common world coordinate system critical before fusion?

## Architecture Onboarding

- **Component map:** Input (Masked images + masked voxelized LiDAR) -> Camera encoder (perspective-view image embedding) -> LiDAR encoder (world-coordinate LiDAR embedding) -> Alignment module (transform image embedding to world coords) -> Fusion block (concatenate aligned embeddings) -> Rendering network (NeRF-style volume rendering conditioned on embeddings) -> Reconstruction loss (L2 for color, L1 for depth across BEV and perspective views)

- **Critical path:** Masking → Embedding extraction → Alignment → Fusion → Rendering → Reconstruction loss backprop

- **Design tradeoffs:**
  - Higher masking ratios improve generalization but may hurt reconstruction accuracy
  - More viewpoints improve representation learning but increase computational cost
  - L2 vs L1 loss choice affects sensitivity to outliers in different modalities

- **Failure signatures:**
  - High reconstruction loss on masked regions indicates poor representation learning
  - Degraded performance on single-modal downstream tasks suggests modality-specific information loss
  - Low improvement over random init indicates trivial identity mapping learned

- **First 3 experiments:**
  1. Ablation: Train with no masking vs default masking to confirm masking prevents trivial solutions
  2. Ablation: Train with single viewpoint (BEV only) vs dual viewpoints to measure viewpoint contribution
  3. Transfer test: Fine-tune pre-trained model on single-modal task (e.g., camera-only BEV detection) vs train from scratch

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed NS-MAE framework scale when applied to larger datasets like Waymo or ArgoVerse, and what are the specific computational bottlenecks encountered in such scenarios?
- Basis in paper: The paper mentions future work aims to explore scalability on larger datasets such as Waymo and ArgoVerse.
- Why unresolved: The current experiments are conducted on the nuScenes and KITTI-3D datasets, which are relatively smaller compared to Waymo or ArgoVerse. The paper does not provide insights into the performance or computational challenges when scaling to these larger datasets.
- What evidence would resolve it: Empirical results showing the performance and computational efficiency of NS-MAE on Waymo or ArgoVerse datasets, along with a detailed analysis of any bottlenecks encountered.

### Open Question 2
- Question: What are the potential benefits and challenges of extending the NS-MAE framework to include additional sensor modalities, such as radar, beyond LiDAR and cameras?
- Basis in paper: The paper suggests future work could investigate the application of NS-MAE to additional sensor modalities like radar.
- Why unresolved: The current framework focuses on LiDAR and camera data. The integration of radar data could provide complementary information but also introduces new challenges in terms of data alignment and feature extraction.
- What evidence would resolve it: Experimental results demonstrating the performance improvements or challenges when incorporating radar data into the NS-MAE framework, along with a comparative analysis of sensor fusion strategies.

### Open Question 3
- Question: How does the performance of NS-MAE compare to other state-of-the-art self-supervised pre-training methods when applied to multi-modal perception tasks in autonomous driving?
- Basis in paper: The paper compares NS-MAE to other SOTA methods like PRC, SimIPU, BEVDistill, and CALICO, showing superior performance in BEV map segmentation.
- Why unresolved: While the paper provides comparisons, it does not explore all possible state-of-the-art methods or consider different evaluation metrics that might highlight other strengths or weaknesses of NS-MAE.
- What evidence would resolve it: A comprehensive comparison of NS-MAE with a broader range of state-of-the-art methods using diverse evaluation metrics and datasets to provide a more holistic view of its performance.

## Limitations

- Exact architectural specifications of the embedding network and fusion block are not fully detailed, described only as "typical embedding networks for multi-modal perception models"
- Specific implementation details for the NeRF rendering network, particularly convolutional layer configurations, are unclear
- Computational scalability to larger datasets like Waymo or ArgoVerse has not been demonstrated

## Confidence

- **High confidence** in the core hypothesis that masked reconstruction prevents trivial identity mappings and forces learning of shared representations
- **Medium confidence** in the claim that NeRF-style rendering effectively unifies spatial and semantic learning across modalities, as this depends heavily on implementation details not fully specified
- **Medium confidence** in the transferability claims, as results show strong performance but the exact contribution of each architectural component to these gains is not fully isolated

## Next Checks

1. **Ablation study on masking ratios**: Systematically vary masking ratios (25%, 50%, 75%, 90%) for both modalities and measure reconstruction loss and downstream task performance to identify optimal masking levels that prevent trivial solutions while maintaining reconstruction quality.

2. **Viewpoint contribution analysis**: Train NS-MAE with only BEV viewpoints, only perspective viewpoints, and both viewpoints to quantify the individual and combined contributions of each viewpoint to the learned representations and downstream task performance.

3. **Single-modal transfer validation**: Evaluate the pre-trained NS-MAE model on single-modal tasks (camera-only BEV detection, LiDAR-only detection) and compare performance against single-modal pre-training methods to verify that the shared representation learning does not degrade modality-specific performance.