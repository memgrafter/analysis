---
ver: rpa2
title: 'AEMIM: Adversarial Examples Meet Masked Image Modeling'
arxiv_id: '2407.11537'
source_url: https://arxiv.org/abs/2407.11537
tags:
- adversarial
- examples
- pre-training
- loss
- aemim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to enhancing masked image
  modeling (MIM) pre-training by incorporating adversarial examples as reconstruction
  targets. The authors propose an auxiliary pretext task of reconstructing masked
  adversarial examples, alongside the original task of reconstructing masked normal
  images.
---

# AEMIM: Adversarial Examples Meet Masked Image Modeling

## Quick Facts
- **arXiv ID**: 2407.11537
- **Source URL**: https://arxiv.org/abs/2407.11537
- **Reference count**: 40
- **Primary result**: AEMIM achieves 83.8% Top-1 accuracy on ImageNet-1K with ViT-B/16 at 800 epochs, outperforming baseline MAE by 0.5/0.2% at 800/1600 epochs

## Executive Summary
AEMIM introduces a novel approach to enhancing masked image modeling (MIM) pre-training by incorporating adversarial examples as reconstruction targets. The method proposes an auxiliary pretext task of reconstructing masked adversarial examples alongside the original task of reconstructing masked normal images. To prevent adversarial examples from compromising performance on normal visual tasks, adapters are employed to handle data from different source domains. A new adversarial attack is devised using the distance between clean and adversarial data in the encoder's feature representation as the adversarial loss. Experimental results demonstrate significant improvements in generalization and robustness across ImageNet and downstream tasks.

## Method Summary
AEMIM enhances MIM pre-training by generating adversarial examples online during training using only the model being trained. These adversarial examples are created by maximizing the distance between clean and adversarial data in the encoder's feature representation, rather than directly targeting reconstruction performance. The model is co-trained on both clean and adversarial masked images using shared encoder parameters but separate adapters for each domain. A weighted loss combines reconstruction tasks, with the auxiliary adversarial task helping to regularize the encoder and improve feature learning without compromising the primary reconstruction objective.

## Key Results
- Achieves 83.8% Top-1 accuracy on ImageNet-1K with ViT-B/16 at 800 epochs, outperforming baseline MAE
- Shows consistent improvements on ImageNet IID and OOD variants
- Demonstrates gains on downstream tasks including COCO object detection (44.3 APbox) and instance segmentation (39.8 APmask)
- Improves robustness to distribution shifts while maintaining competitive performance on standard benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Adversarial examples serve as harder reconstruction targets than generic corrupted images, forcing the model to learn more robust features.
- **Mechanism**: By adding adversarial perturbations that specifically degrade reconstruction performance, the model must capture more invariant features to successfully reconstruct both clean and adversarial inputs.
- **Core assumption**: The added difficulty from adversarial examples translates into better generalization rather than just overfitting to adversarial patterns.
- **Evidence anchors**: Abstract mentions adversarial examples as "hard negative samples" for pre-training; section states they are "more intricate reconstruction targets."
- **Break condition**: If adversarial perturbations become too strong, the model may prioritize the auxiliary pretext task at the expense of the primary task.

### Mechanism 2
- **Claim**: Using the encoder's feature distance as adversarial loss prevents the model from prioritizing the auxiliary pretext task over the primary one.
- **Mechanism**: Instead of directly targeting reconstruction performance with adversarial loss, the method targets intermediate feature representation, which regularizes the encoder without directly interfering with the reconstruction objective.
- **Core assumption**: Feature distance between clean and adversarial data is a meaningful signal for generating useful adversarial examples without creating a degenerate training dynamic.
- **Evidence anchors**: Section explains this choice prevents "overly strong regularization" that directly targets the reconstruction task.
- **Break condition**: If feature distance doesn't correlate well with reconstruction difficulty, adversarial examples may not provide meaningful regularization.

### Mechanism 3
- **Claim**: Adapters allow the model to handle clean and adversarial data differently without compromising performance on normal tasks.
- **Mechanism**: By maintaining separate normalization statistics and potentially other parameters for clean vs. adversarial data, the model can learn domain-specific adaptations while sharing most parameters.
- **Core assumption**: The domain shift between clean and adversarial examples is significant enough to warrant separate parameter handling, but not so large that shared parameters become ineffective.
- **Evidence anchors**: Section mentions adversarial examples are generated online using only the trained model, enhancing training efficiency.
- **Break condition**: If adapter parameters become too dominant, shared parameters may not learn effectively, reducing transfer learning benefits.

## Foundational Learning

- **Concept**: Masked Image Modeling (MIM) fundamentals
  - **Why needed here**: AEMIM builds directly on MIM frameworks, so understanding how masking and reconstruction work is essential.
  - **Quick check question**: What is the typical masking ratio used in MAE, and why is it set to that value?

- **Concept**: Adversarial example generation and its impact on model training
  - **Why needed here**: The paper's core innovation is using adversarial examples in MIM, requiring understanding of how adversarial attacks work and their typical effects on model performance.
  - **Quick check question**: How does FGSM differ from PGD in terms of attack steps and effectiveness?

- **Concept**: Adapter mechanisms in deep learning
  - **Why needed here**: Adapters are crucial for handling different data domains (clean vs. adversarial) without compromising overall performance.
  - **Quick check question**: What types of parameters are typically used as adapters in vision transformer architectures?

## Architecture Onboarding

- **Component map**: Input images -> Encoder (shared ψe + clean adapters ϕe_c + adversarial adapters ϕe_a) -> Decoder (shared) -> Two reconstruction heads -> Loss computation

- **Critical path**:
  1. Generate adversarial examples using encoder feature distance
  2. Apply masking to both clean and adversarial images
  3. Reconstruct both versions using shared encoder parameters but different adapters
  4. Backpropagate through both reconstruction tasks with weighted loss

- **Design tradeoffs**:
  - Using feature distance vs. reconstruction loss for adversarial generation
  - Number of attack steps (quality vs. efficiency)
  - Ratio of clean to adversarial examples during training
  - Type of adapters (normalization layers vs. classification token)

- **Failure signatures**:
  - Loss for adversarial reconstruction becoming much lower than clean reconstruction (model prioritizing auxiliary task)
  - No improvement in robustness on OOD datasets
  - Significant drop in clean accuracy after fine-tuning
  - Adversarial examples failing to improve after multiple training iterations

- **First 3 experiments**:
  1. Verify that adversarial examples generated with feature distance loss actually reduce reconstruction quality more than random noise
  2. Test different adapter configurations (normalization layers only vs. classification token included) on a small dataset
  3. Compare training dynamics with and without the two-task loss weighting to confirm the auxiliary task doesn't dominate

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of AEMIM scale with different MIM frameworks beyond MAE and SimMIM?
- **Basis in paper**: [explicit] The authors state "our method is not restricted to specific model architectures or MIM strategies" and conduct an ablation experiment with SimMIM, but do not extensively explore other frameworks.
- **Why unresolved**: The paper only provides results for MAE and SimMIM, leaving the question of performance with other MIM frameworks unanswered.
- **What evidence would resolve it**: Experiments applying AEMIM to other MIM frameworks like BEiT, iBOT, or CAE, comparing their performance with and without AEMIM.

### Open Question 2
- **Question**: What is the optimal perturbation budget and attack steps for generating adversarial examples in AEMIM across different datasets and tasks?
- **Basis in paper**: [explicit] The authors conduct ablation studies on perturbation budget and attack steps, finding that a moderate budget (ϵ=2) and 2 attack steps work well, but do not explore a wider range of values or different datasets/tasks.
- **Why unresolved**: The optimal settings may vary depending on the dataset, task, and model architecture, which are not fully explored in the paper.
- **What evidence would resolve it**: Comprehensive ablation studies varying perturbation budgets and attack steps across multiple datasets, tasks, and model architectures to determine optimal settings for each scenario.

### Open Question 3
- **Question**: How does AEMIM's performance compare to other methods for improving MIM pre-training, such as using corrupted images or hard patches mining?
- **Basis in paper**: [inferred] The authors mention related work on corrupted image modeling (CIM) and hard patches mining (HPM) but do not directly compare AEMIM's performance to these methods.
- **Why unresolved**: While AEMIM shows improvements over MAE, it is unclear how it compares to other state-of-the-art methods for enhancing MIM pre-training.
- **What evidence would resolve it**: Direct comparisons of AEMIM with CIM, HPM, and other relevant methods on multiple datasets and tasks, evaluating both performance and computational efficiency.

## Limitations

- The robustness gains are primarily demonstrated on ImageNet OOD variants rather than adversarially crafted attacks on the trained models.
- Computational overhead quantification is lacking - the paper states it requires roughly twice the computation but doesn't specify memory requirements or wall-clock time implications.
- The ablation study on adapter configurations is incomplete, only exploring normalization layers and classification token adapters.

## Confidence

**High confidence**: The core mechanism of using feature distance for adversarial loss generation appears sound and is well-justified theoretically. The empirical improvements on standard ImageNet benchmarks (83.8% Top-1 accuracy) are significant and reproducible.

**Medium confidence**: The claims about robustness improvements on OOD datasets are supported but limited in scope. The computational efficiency claims are reasonable but lack quantitative validation. The adapter design choices show promise but have incomplete ablation studies.

**Low confidence**: The actual adversarial robustness against white-box attacks on the trained models is not thoroughly evaluated. The paper's analysis of when the auxiliary task might dominate the primary task is theoretical rather than empirically validated across different settings.

## Next Checks

1. **Adversarial robustness evaluation**: Test the pre-trained models against standard white-box adversarial attacks (PGD, CW) to verify actual robustness improvements beyond OOD generalization.

2. **Computational overhead quantification**: Measure wall-clock training time, peak GPU memory usage, and throughput (samples/second) for AEMIM versus baseline MIM methods to provide concrete efficiency metrics.

3. **Adapter configuration ablation**: Systematically evaluate alternative adapter types (LoRA, bottleneck adapters, MLP adapters) and their placement within the transformer architecture to determine if the normalization layer + classification token combination is optimal.