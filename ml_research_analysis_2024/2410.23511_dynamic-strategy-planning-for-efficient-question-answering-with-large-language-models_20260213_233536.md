---
ver: rpa2
title: Dynamic Strategy Planning for Efficient Question Answering with Large Language
  Models
arxiv_id: '2410.23511'
source_url: https://arxiv.org/abs/2410.23511
tags:
- strategy
- answer
- dyplan
- retrieval
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces DyPlan, a technique to dynamically select
  strategies (Direct, Plan, Reason, Retrieval) for question answering based on the
  input question. Unlike fixed-strategy approaches, DyPlan first decides the most
  suitable strategy and then executes it.
---

# Dynamic Strategy Planning for Efficient Question Answering with Large Language Models

## Quick Facts
- **arXiv ID**: 2410.23511
- **Source URL**: https://arxiv.org/abs/2410.23511
- **Reference count**: 40
- **Primary result**: DyPlan improves F1 by 7-13% while reducing computational costs (tokens and retrievals) by 11-32% compared to best baselines on multi-hop QA datasets.

## Executive Summary
This paper introduces DyPlan, a dynamic strategy planning technique for large language models (LLMs) that selects the most appropriate QA strategy (Direct, Plan, Reason, Retrieval) based on the input question. Unlike fixed-strategy approaches, DyPlan first decides which strategy to use and then executes it, with DyPlan-verify adding a verification step for self-correction. Experiments on three multi-hop QA datasets show DyPlan improves F1 by 7-13% while reducing computational costs by 11-32% relative to the best baseline. The study demonstrates that dynamic strategy planning improves both efficiency and accuracy by leveraging the complementary strengths of different strategies and acting as an ensemble method.

## Method Summary
DyPlan consists of three components: Decision (selects strategy based on question), Execution (generates answer using chosen strategy), and Verification (evaluates answer correctness and triggers re-attempt if needed). The technique is implemented through fine-tuning LLaMa3-8B-Instruct with multi-turn training data, using LoRA for efficiency. Four strategies are available: Direct (answer directly), Reason (Chain-of-Thought), Plan (SelfAsk), and Retrieval (RAG with 3 passages). The system follows a strategy preference order (Direct < Plan < Reason < Retrieval) and DyPlan-verify adds a verification loop that can reselect alternative strategies if verification fails.

## Key Results
- DyPlan improves F1 by 7-13% on three multi-hop QA datasets while reducing computational costs by 11-32%
- Verification step in DyPlan-verify further improves performance by 2-4% on average
- Dynamic strategy selection acts as an ensemble method, capturing questions that different strategies answer correctly
- Better calibration and generalization compared to fixed-strategy approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic strategy selection reduces computational cost by avoiding expensive strategies for simple questions.
- Mechanism: The Decision component chooses the least expensive strategy capable of answering the question, based on the question's complexity and the model's self-knowledge.
- Core assumption: The model has sufficient self-knowledge to determine when it can answer directly versus when it needs reasoning or retrieval.
- Evidence anchors:
  - [abstract]: "DyPlan provides a computationally cost-effective and adaptive solution that efficiently leverages the strengths of various techniques while minimizing computational overhead."
  - [section 2.1]: "Dynamic strategy planning can help reduce inference computational costs by simply selecting lower-cost strategies to answer simpler questions."
  - [corpus]: Weak evidence. The corpus neighbors focus on retrieval efficiency and domain adaptation, not dynamic strategy planning for cost reduction.
- Break condition: The model misjudges its self-knowledge, leading to unnecessary expensive strategy usage or failed attempts with simple strategies.

### Mechanism 2
- Claim: Dynamic strategy selection improves performance by acting as an ensemble method.
- Mechanism: By selecting the most appropriate strategy for each question, DyPlan effectively ensembles multiple approaches, capturing questions that different strategies answer correctly.
- Core assumption: Different strategies have complementary strengths, and the optimal strategy varies across questions.
- Evidence anchors:
  - [section 2.1]: "We additionally posit that it can improve model performance as strategy selection can act as an ensembling method. We verify this hypothesis through a simple analysis using two strategies of Direct and Reason."
  - [abstract]: "Experiments on three prominent multi-hop question answering (MHQA) datasets reveal how DyPlan can improve model performance by 7-13% while reducing the computational cost by 11-32% relative to the best baseline model."
  - [corpus]: Weak evidence. Corpus neighbors focus on retrieval efficiency and specialized domains, not ensemble effects from dynamic strategy selection.
- Break condition: The Decision component consistently chooses suboptimal strategies, reducing the benefits of ensembling.

### Mechanism 3
- Claim: Verification and correction improve answer quality by allowing the model to detect and fix mistakes.
- Mechanism: DyPlan-verify adds a self-verification step after answer generation, allowing the model to re-attempt with different strategies if verification fails.
- Core assumption: The model can accurately assess its own answer quality and recognize when a different strategy might be more appropriate.
- Evidence anchors:
  - [abstract]: "We extend DyPlan to DyPlan-verify, adding an internal verification and correction process to further enrich the generated answer."
  - [section 2.1]: "There is no complete certainty that the chosen strategy will succeed, as reasoning can be wrong, or retrieved information may turn out to be irrelevant or limited. At such times, humans usually evaluate and re-select a new strategy to rectify any potential mistakes."
  - [corpus]: Weak evidence. Corpus neighbors focus on retrieval adaptation and self-improvement, but not verification-based correction mechanisms.
- Break condition: The model's self-verification is unreliable, leading to unnecessary re-attempts or missed opportunities for correction.

## Foundational Learning

- Concept: Multi-hop question answering (MHQA)
  - Why needed here: The paper benchmarks DyPlan on three MHQA datasets, which require combining information from multiple sources or reasoning steps.
  - Quick check question: What distinguishes multi-hop QA from single-hop QA, and why is it more challenging for LLMs?

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: CoT is one of the strategies DyPlan can select, and understanding its mechanism is crucial for grasping the technique.
  - Quick check question: How does Chain-of-Thought prompting improve LLM reasoning, and what are its limitations?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG is another strategy in DyPlan's strategy pool, and understanding how it works is essential for comprehending the full technique.
  - Quick check question: How does RAG enhance LLM performance on knowledge-intensive tasks, and what are the computational costs involved?

## Architecture Onboarding

- Component map: Question → Decision → Execution → (Verification → Decision → Execution)* → Answer
- Critical path: The verification loop runs until verification succeeds or the maximum number of rounds is reached
- Design tradeoffs:
  - Fine-tuning vs. zero-shot: Fine-tuning improves calibration but requires training data; zero-shot is more flexible but less reliable
  - Strategy ordering: Different orderings of strategies can optimize for performance vs. efficiency
  - Verification frequency: More verification rounds improve quality but increase computational cost
- Failure signatures:
  - Poor strategy selection: Consistently choosing expensive strategies for simple questions or vice versa
  - Verification failures: High rejection rates or low verification precision
  - Calibration issues: Model's strategy choices don't align with optimal policy
- First 3 experiments:
  1. Implement DyPlan with a fixed strategy order (Direct → Plan → Reason → Retrieval) on a small subset of HotpotQA
  2. Compare strategy selection accuracy against an oracle optimal policy
  3. Measure computational cost reduction by tracking token counts and retrieval operations for each strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DyPlan perform when evaluated on non-MHQA datasets such as SQuAD or TriviaQA, which require single-hop reasoning?
- Basis in paper: [inferred] The paper primarily benchmarks DyPlan on three multi-hop QA datasets (HotpotQA, 2WikiMultihopQA, and Musique), noting that simpler datasets like SQuAD are less challenging. The authors suggest that DyPlan’s generalizability to other tasks could be explored but focus on MHQA.
- Why unresolved: The paper does not provide empirical results or analysis of DyPlan’s performance on single-hop QA datasets, which would help understand its adaptability to different reasoning complexities.
- What evidence would resolve it: Conducting experiments on single-hop QA datasets like SQuAD or TriviaQA and comparing DyPlan’s performance and efficiency metrics (e.g., F1 score, token count) with those of fixed-strategy baselines.

### Open Question 2
- Question: What is the impact of using larger LLMs (e.g., Llama3-70B) or full fine-tuning instead of LoRA on DyPlan’s performance and computational cost?
- Basis in paper: [explicit] The authors mention that due to budget constraints, they limited their experiments to smaller models (8B) and LoRA fine-tuning. They suggest that full fine-tuning and exploring larger LLMs might yield better performance and faster inference.
- Why unresolved: The paper does not explore the scalability of DyPlan with larger models or full fine-tuning, leaving uncertainty about its performance gains and cost-effectiveness at scale.
- What evidence would resolve it: Implementing DyPlan with larger LLMs (e.g., Llama3-70B) and full fine-tuning, then evaluating performance (F1 score) and computational costs (tokens, retrievals) on the same MHQA datasets to compare with the current results.

### Open Question 3
- Question: Why do strategy hierarchy violations occur, and can they be predicted or mitigated during training?
- Basis in paper: [inferred] The paper identifies significant performance contributions from hierarchy violations (e.g., Direct outperforming Reason or Retrieval) but does not deeply investigate the underlying causes. It suggests that future work could explore this area.
- Why unresolved: The paper acknowledges the phenomenon but does not provide a theoretical or empirical explanation for why violations occur, such as spurious patterns in training data or model hallucinations.
- What evidence would resolve it: Conducting a detailed analysis of failure cases, including error analysis of retrievals, reasoning steps, and model outputs, to identify patterns or triggers for violations. Additionally, exploring techniques like data augmentation or adversarial training to reduce violations.

## Limitations
- The study relies on fine-tuning, which may not generalize to other model families without retraining
- The verification mechanism's effectiveness depends on the model's self-assessment capability, which wasn't thoroughly validated against human judgments
- The paper doesn't explore computational overhead from the Decision and Verification components themselves

## Confidence

- **High Confidence**: The efficiency improvements (11-32% token reduction) and performance gains (7-13% F1 improvement) are well-supported by experimental results across three datasets.
- **Medium Confidence**: The ensembling effect from dynamic strategy selection is supported by the 2-strategy analysis but could benefit from more comprehensive ablation studies.
- **Low Confidence**: The paper's claims about the Decision component's self-knowledge capability are mostly theoretical, with limited empirical validation of how accurately the model selects strategies compared to optimal policies.

## Next Checks

1. **Strategy Selection Accuracy Validation**: Compare the Decision component's strategy choices against an oracle optimal policy on a held-out validation set to quantify the accuracy of self-knowledge assessment and identify systematic biases in strategy selection.

2. **Verification Mechanism Robustness**: Conduct human evaluation studies to assess the quality of the model's self-verification, measuring precision and recall of the verification step and identifying failure modes where the model incorrectly accepts/rejects answers.

3. **Cross-Model Generalization**: Test DyPlan on different LLM architectures (e.g., Mistral, GPT models) without fine-tuning to evaluate how well the dynamic strategy selection and verification mechanisms transfer across model families.