---
ver: rpa2
title: 'MRSegmentator: Multi-Modality Segmentation of 40 Classes in MRI and CT'
arxiv_id: '2405.06463'
source_url: https://arxiv.org/abs/2405.06463
tags:
- segmentation
- mrsegmentator
- dataset
- scans
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MRSegmentator is a deep learning model for multi-organ segmentation
  in MRI and CT scans. It was trained on 1,200 annotated MRI scans from UK Biobank,
  221 in-house MRI scans, and 1,228 CT scans from TotalSegmentator, using cross-modality
  transfer learning and a human-in-the-loop annotation workflow.
---

# MRSegmentator: Multi-Modality Segmentation of 40 Classes in MRI and CT

## Quick Facts
- arXiv ID: 2405.06463
- Source URL: https://arxiv.org/abs/2405.06463
- Reference count: 28
- Segments 40 anatomical structures in MRI and CT with DSC scores of 0.84-0.97

## Executive Summary
MRSegmentator is a deep learning model for multi-organ segmentation in MRI and CT scans. It was trained on 1,200 annotated MRI scans from UK Biobank, 221 in-house MRI scans, and 1,228 CT scans from TotalSegmentator, using cross-modality transfer learning and a human-in-the-loop annotation workflow. The model segments 40 anatomical structures, including organs like lungs (DSC 0.97), heart (DSC 0.95), liver (DSC 0.96), and kidneys (DSC 0.95). It generalizes well to external datasets, achieving DSC scores of 0.85–0.91 on MRI and 0.84 on CT. Smaller structures like adrenal glands and portal veins showed lower accuracy (DSC 0.54–0.69). The open-source model is available at https://github.com/hhaentze/MRSegmentator.

## Method Summary
MRSegmentator uses nnUNet as its backbone architecture with a multi-class segmentation head for 40 anatomical structures. The training approach combines cross-modality transfer learning from CT segmentation models with a human-in-the-loop annotation workflow. The model was trained on 1,200 UK Biobank MRI scans, 221 in-house MRI scans, and 1,228 CT scans from TotalSegmentator. The human-in-the-loop process involved iterative refinement cycles where model predictions were corrected by experts and used to retrain the model. Five-fold cross-validation was employed on the fully annotated images. The model was evaluated on external datasets including NAKO (n=900) and AMOS22 (n=60).

## Key Results
- Achieved DSC scores of 0.97 for lungs, 0.95 for heart, 0.96 for liver, and 0.95 for kidneys on MRI data
- Generalized well to external MRI datasets with DSC scores of 0.85–0.91
- Showed strong CT performance with DSC of 0.84 on external validation
- Smaller structures like adrenal glands and portal veins had lower accuracy (DSC 0.54–0.69)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modality transfer learning from CT to MRI improves segmentation accuracy for MRI data.
- Mechanism: Features learned from CT segmentation generalize to MRI scans when combined with modality-specific adaptations, enabling shared anatomical understanding despite differences in intensity scales.
- Core assumption: Anatomical structures have similar spatial patterns across CT and MRI, allowing feature reuse despite modality-specific intensity differences.
- Evidence anchors:
  - [abstract] "leveraging cross-modality transfer learning from an existing CT segmentation model"
  - [section] "Including CT examinations to the training pipeline can improve segmentation quality for MRI examinations as well"
  - [corpus] Weak evidence - no direct corpus mention of transfer learning results
- Break condition: If anatomical feature distributions diverge significantly between modalities or if intensity normalization fails, transfer learning benefits disappear.

### Mechanism 2
- Claim: Human-in-the-loop annotation workflow creates higher-quality ground truth with fewer manual hours.
- Mechanism: Iterative refinement cycles where model predictions are corrected by experts and then used to retrain the model progressively improve annotation quality and reduce manual effort per case.
- Core assumption: Expert corrections of model predictions are more efficient than full manual annotation, and model quality improves sufficiently with each iteration.
- Evidence anchors:
  - [abstract] "A human-in-the-loop annotation workflow was employed, leveraging cross-modality transfer learning from an existing CT segmentation model"
  - [section] "This iterative process involved... After each set of 50 annotated examinations, we trained a segmentation model based on nnUNet using the updated segmentations"
  - [corpus] Weak evidence - no corpus mention of annotation efficiency
- Break condition: If model predictions are consistently poor or if annotation corrections don't lead to meaningful model improvements across iterations.

### Mechanism 3
- Claim: Multi-organ segmentation in a single model provides better generalization across imaging protocols than single-organ models.
- Mechanism: Joint training on multiple anatomical structures allows the model to learn shared contextual features and relationships between organs, improving robustness to variations in imaging protocols.
- Core assumption: Anatomical structures co-occur in consistent spatial relationships, and learning these relationships improves segmentation accuracy.
- Evidence anchors:
  - [abstract] "MRSegmentator demonstrated high accuracy in segmenting well-defined organs... and robustness in organs like the liver... which present more variability"
  - [section] "Compared to existing models, MRSegmentator demonstrates competitive performance... Regarding MRI segmentation, MRSegmentator performs on par with several organ-specific models"
  - [corpus] Weak evidence - no corpus mention of multi-organ advantages
- Break condition: If inter-organ relationships are not consistent across the dataset or if model capacity becomes a bottleneck for representing multiple structures.

## Foundational Learning

- Concept: Cross-modal transfer learning
  - Why needed here: MRI and CT have fundamentally different intensity representations but share anatomical structures, requiring methods to transfer knowledge between modalities
  - Quick check question: How would you adapt a CT-trained segmentation model to work on MRI data without retraining from scratch?

- Concept: Dice Similarity Coefficient (DSC) and Hausdorff Distance (HD)
  - Why needed here: These metrics quantify segmentation quality by measuring overlap and boundary accuracy, critical for evaluating medical image segmentation
  - Quick check question: What are the limitations of DSC when evaluating small structures like adrenal glands?

- Concept: Human-in-the-loop annotation workflows
  - Why needed here: Medical image annotation is labor-intensive and requires expert knowledge, making iterative approaches with model assistance essential for scaling
  - Quick check question: How does the quality of initial model predictions affect the efficiency of human-in-the-loop annotation?

## Architecture Onboarding

- Component map: nnUNet backbone -> Multi-class segmentation head (40 classes) -> Cross-modality feature extractor -> Human-in-the-loop annotation pipeline -> Evaluation metrics (DSC, HD)
- Critical path: Data preprocessing -> Cross-modality transfer learning -> Human-in-the-loop annotation iterations -> Model training -> External validation
- Design tradeoffs: Multi-organ model increases complexity but enables context learning; human-in-the-loop reduces annotation time but may introduce bias; cross-modality transfer improves MRI performance but adds CT data requirements
- Failure signatures: Poor DSC scores on specific organs (e.g., adrenal glands), inconsistent performance across MRI sequences, overfitting to training dataset distribution
- First 3 experiments:
  1. Train MRI-only baseline model without cross-modality transfer to quantify transfer learning benefits
  2. Vary the number of human-in-the-loop annotation iterations to find the point of diminishing returns
  3. Test model performance on different MRI sequences (T1, T2, Dixon variants) to identify protocol-specific weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model perform on pathological images beyond kidney tumors, and what are the failure modes in these cases?
- Basis in paper: [explicit] The paper mentions that MRSegmentator accurately segmented kidneys of all 8 patients with tumors greater than 7 cm in the in-house dataset, but notes that it appears to oversegment, likely due to the heterogeneous appearance and irregular borders of the tumors. The paper states that further investigation is needed to assess the model's performance on a wider range of pathological conditions.
- Why unresolved: The paper only provides limited information on the model's performance on pathological images, focusing specifically on kidney tumors. The extent to which the model can generalize to other types of pathologies and the specific failure modes in these cases remain unclear.
- What evidence would resolve it: Additional experiments evaluating the model's performance on a diverse set of pathological conditions, such as liver lesions, lung nodules, or brain tumors, would provide insights into its generalizability. Analyzing the specific failure modes and error patterns in these cases would help identify areas for improvement.

### Open Question 2
- Question: What is the inter-observer agreement for the segmentation of smaller and complex structures, and how does it compare to the model's performance?
- Basis in paper: [explicit] The paper mentions that for some structures, such as duodenum, it can be challenging to define beginning and end, resulting in a low inter-observer agreement. It also states that the inter-observer agreement of the pancreas, for example, can have a Dice score as low as 0.85.
- Why unresolved: While the paper acknowledges the challenges in annotating certain structures and the potential for low inter-observer agreement, it does not provide a comprehensive comparison between the model's performance and the inter-observer agreement for various structures.
- What evidence would resolve it: Conducting a study to measure the inter-observer agreement for the segmentation of different structures, particularly smaller and complex ones, and comparing it with the model's performance would provide valuable insights into the model's accuracy relative to human annotators.

### Open Question 3
- Question: How does the model's performance vary across different MRI scanners and imaging protocols, and what are the key factors influencing this variability?
- Basis in paper: [explicit] The paper mentions that the in-house dataset is heterogeneous, as images were acquired on multiple different scanners with varying intensity distributions, spacings, and sizes. It also notes that segmentation quality varied slightly between different MRI sequence types, with the model performing best on T1 GRE opposed-phase (OPP) sequences and worst on Dixon in-phase (IN) sequences.
- Why unresolved: While the paper acknowledges the heterogeneity of the data and the potential impact of different scanners and imaging protocols on the model's performance, it does not provide a detailed analysis of the specific factors influencing this variability or the extent to which the model can generalize across different scanner models and protocols.
- What evidence would resolve it: Conducting experiments to systematically evaluate the model's performance across a wide range of MRI scanners, imaging protocols, and sequence types would help identify the key factors influencing its variability. Analyzing the impact of specific scanner models, field strengths, and sequence parameters on the model's accuracy would provide insights into its generalizability and potential limitations.

## Limitations
- Small structure segmentation remains challenging, with DSC scores of 0.54-0.69 for adrenal glands and portal/splenic veins
- Potential left-right structure confusion in pelvic regions persists despite postprocessing
- External validation shows performance degradation compared to in-house performance

## Confidence
- High confidence: Cross-modality transfer learning improves MRI segmentation accuracy
- Medium confidence: Human-in-the-loop workflow reduces annotation time
- Medium confidence: Multi-organ model generalizes better than single-organ models

## Next Checks
1. Quantify annotation efficiency gains by comparing time/cost between human-in-the-loop and pure manual annotation workflows across 50-100 cases
2. Conduct systematic analysis of false positives/negatives for small structures to identify whether issues stem from resolution limits or model architecture
3. Test model robustness across additional MRI protocols and scanners not represented in training data to assess real-world generalization