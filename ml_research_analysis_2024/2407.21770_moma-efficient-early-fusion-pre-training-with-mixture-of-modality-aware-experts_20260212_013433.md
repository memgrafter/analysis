---
ver: rpa2
title: 'MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts'
arxiv_id: '2407.21770'
source_url: https://arxiv.org/abs/2407.21770
tags:
- experts
- training
- image
- text
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MoMa introduces a mixture-of-modality-aware-experts (MoMa) architecture\
  \ for efficient pre-training of mixed-modal, early-fusion language models. The key\
  \ innovation is dividing expert modules into modality-specific groups\u2014text\
  \ and image\u2014that exclusively process their respective tokens while employing\
  \ learned routing within each group."
---

# MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts

## Quick Facts
- **arXiv ID**: 2407.21770
- **Source URL**: https://arxiv.org/abs/2407.21770
- **Reference count**: 18
- **Primary result**: 3.7x FLOPs savings during pre-training with MoMa 1.4B (4 text + 4 image experts)

## Executive Summary
MoMa introduces a mixture-of-modality-aware-experts (MoMa) architecture for efficient pre-training of mixed-modal, early-fusion language models. The key innovation is dividing expert modules into modality-specific groups—text and image—that exclusively process their respective tokens while employing learned routing within each group. This modality-aware sparsity improves parameter utilization by accounting for distinct information densities across modalities.

Under a 1-trillion-token training budget, MoMa 1.4B (4 text experts + 4 image experts) achieves 3.7x overall FLOPs savings—2.6x for text and 5.2x for image processing—compared to a compute-equivalent dense baseline, as measured by pre-training loss. This outperforms standard expert-choice MoE with 8 mixed-modal experts (3x overall savings). Combining MoMa with mixture-of-depths (MoD) further improves FLOPs savings to 4.2x overall (3.4x for text, 5.3x for image), though MoD hurts causal inference performance due to router accuracy sensitivity.

## Method Summary
MoMa proposes a mixture-of-experts architecture specifically designed for early-fusion multimodal models. The architecture partitions expert modules into modality-specific groups—text and image—that exclusively process their respective tokens. Each group employs learned routing to activate a subset of experts for each token, improving parameter utilization by accounting for different information densities across modalities. The authors demonstrate that MoMa 1.4B (4 text experts + 4 image experts) achieves 3.7x FLOPs savings during pre-training compared to dense baselines, outperforming standard expert-choice MoE with mixed-modal experts.

## Key Results
- MoMa 1.4B achieves 3.7x overall FLOPs savings (2.6x text, 5.2x image) under 1T token budget
- Outperforms standard 8-expert mixed-modal MoE by 0.7x in FLOPs efficiency
- MoMa + MoD combination achieves 4.2x overall FLOPs savings but degrades causal inference performance

## Why This Works (Mechanism)
MoMa works by recognizing that text and image tokens have fundamentally different information densities and processing requirements. By partitioning experts into modality-specific groups, the architecture ensures that each expert only processes tokens of its designated modality, eliminating cross-modal interference and improving routing efficiency. The learned routing within each group activates only a subset of experts per token, creating modality-aware sparsity that adapts to the heterogeneous computational demands of different modalities. This targeted approach allows for more effective parameter sharing and activation patterns compared to mixed-modal experts, which must handle both modalities with less specialization.

## Foundational Learning

**Mixture-of-Experts (MoE)**: A neural network architecture where multiple expert networks exist and a gating network selects which experts process each input. Why needed: Enables conditional computation and parameter efficiency. Quick check: Verify that router accuracy exceeds 90% for stable performance.

**Early-Fusion Multimodal Models**: Architectures that process text and image tokens together from the start rather than encoding modalities separately. Why needed: Captures cross-modal interactions earlier in the model. Quick check: Ensure proper tokenization and embedding alignment across modalities.

**Router Accuracy**: The percentage of tokens routed to the correct expert based on the true optimal assignment. Why needed: Critical for MoE stability and performance. Quick check: Monitor router accuracy during training to detect degradation.

**FLOPs Efficiency**: Computational efficiency measured as operations per second or per token. Why needed: Direct metric for comparing architectural efficiency. Quick check: Verify FLOP counts match theoretical expectations based on expert activations.

**Modality-Aware Sparsity**: Sparse activation patterns that account for different computational requirements of different modalities. Why needed: Enables more efficient resource allocation across heterogeneous data types. Quick check: Confirm that text and image tokens activate appropriate expert groups with minimal cross-activation.

## Architecture Onboarding

**Component Map**: Input Tokens -> Modality Classifier -> Text/Image Router -> Respective Expert Groups -> Fusion Layer -> Output

**Critical Path**: Token embedding → Modality classification → Expert routing → Computation in activated experts → Fusion → Final prediction

**Design Tradeoffs**: Specialization vs. flexibility (modality-specific experts vs. mixed-modal), router complexity vs. accuracy, pre-training efficiency vs. inference stability. The modality-specific approach sacrifices some cross-modal parameter sharing for improved routing efficiency and specialization.

**Failure Signatures**: Router collapse (all tokens routed to same expert), cross-modal routing errors (text tokens activating image experts), poor fusion layer performance due to imbalanced expert contributions, and degraded inference accuracy when combined with MoD due to router sensitivity.

**First 3 Experiments**:
1. Baseline comparison: Run MoMa 1.4B vs. dense 1.4B baseline on 100B tokens to verify FLOPs savings claims
2. Router ablation: Compare MoMa with modality-aware routing vs. standard MoE with mixed-modal experts
3. Modality-untied upcycling test: Initialize MoMa from single-expert-per-modality seed vs. training from scratch

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Router accuracy sensitivity, especially when combined with mixture-of-depths (MoD), raises concerns about inference reliability
- FLOPs savings during pre-training may not translate to inference efficiency gains without comprehensive inference benchmarking
- Modality-untied upcycling shows promise but lacks extensive ablation studies to confirm its necessity versus training from scratch

## Confidence
- **High**: Core FLOPs savings claims during pre-training (directly measured through loss convergence)
- **Medium**: Relative improvements over standard expert-choice MoE (clear architectural differences and ablation results)
- **Low**: Inference-time performance claims (degradation when combined with MoD, lack of comprehensive downstream evaluations)

## Next Checks
1. Conduct inference-time benchmarks on representative downstream tasks to verify whether pre-training FLOPs savings translate to practical efficiency gains
2. Perform stress tests on router accuracy under distribution shifts and noisy inputs to assess robustness
3. Run scaling experiments with larger model sizes (e.g., 7B+ parameters) to determine whether modality-aware sparsity benefits persist at production scales