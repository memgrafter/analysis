---
ver: rpa2
title: Language Models Need Inductive Biases to Count Inductively
arxiv_id: '2405.20131'
source_url: https://arxiv.org/abs/2405.20131
tags:
- counting
- token
- count
- first
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies whether language models can learn to count\
  \ inductively\u2014mapping number words to cardinalities and generalizing to out-of-distribution\
  \ (OOD) counts. The authors test Transformers and various recurrent architectures\
  \ on counting tasks with controlled exposure to cardinalities, using multiple positional\
  \ embedding strategies to avoid OOD-position and OOD-vocabulary issues."
---

# Language Models Need Inductive Biases to Count Inductively

## Quick Facts
- arXiv ID: 2405.20131
- Source URL: https://arxiv.org/abs/2405.20131
- Reference count: 40
- Primary result: Transformers struggle with inductive counting beyond training lengths, while traditional RNNs achieve near-perfect inductive generalization

## Executive Summary
This paper investigates whether language models can learn to count inductively—mapping number words to cardinalities and generalizing to out-of-distribution (OOD) counts. Through systematic experiments with Transformers and various recurrent architectures on controlled counting tasks, the authors demonstrate that Transformers require careful design choices and multiple layers to count in-domain, and still fail at inductive counting beyond training lengths. In contrast, traditional RNNs consistently achieve near-perfect inductive counting performance. The study reveals that modern RNNs (S4, Mamba, RWKV) underperform traditional RNNs due to limited state dynamics from parallelization, highlighting the critical role of inductive biases and expressive state transitions for robust generalization in counting tasks.

## Method Summary
The paper systematically tests Transformers and recurrent architectures on counting tasks with controlled exposure to cardinalities. The authors use multiple positional embedding strategies (NoPE, SinePE, APE, RoPE, SPE) to isolate position-dependent and position-independent components while avoiding OOD-position and OOD-vocabulary issues. Models are trained on synthetic sequences containing number words and objects, with evaluation on both in-domain and out-of-domain test sets. The study compares traditional RNNs (RNN, LSTM) with modern RNNs (S4, Mamba, RWKV) to analyze the impact of parallelization-friendly state transitions on inductive generalization.

## Key Results
- Transformers require multiple layers and careful design choices (helper tokens, shifted starts) to count in-domain
- Transformers fail to generalize counting beyond training lengths even with positional embeddings
- Traditional RNNs achieve near-perfect inductive counting while modern RNNs underperform due to reduced state dynamics
- Different positional embedding strategies show varying effectiveness across counting task variants

## Why This Works (Mechanism)

### Mechanism 1
Transformers require inductive biases to generalize counting beyond training lengths. Positional embeddings encode information that allows models to break symmetry and map number words to cardinalities. The inductive step of counting (inferring that n+1 follows n) cannot be learned purely from sequential processing in Transformers.

### Mechanism 2
Modern RNNs underperform traditional RNNs in inductive counting due to reduced state dynamics. Parallelization-friendly state transitions (linear interpolation, matrix multiplication) limit the expressivity of state updates compared to nonlinear gating in traditional RNNs.

### Mechanism 3
Different positional embedding strategies enable different forms of counting generalization. Each PE schema encodes specific inductive biases favorable for certain counting tasks (e.g., RoPE for unbounded counting, SinePE/APE for modular counting).

## Foundational Learning

- **Positional Embeddings (PEs)**
  - Why needed here: PEs break the symmetry of identical tokens and encode positional information crucial for counting tasks
  - Quick check question: What is the main difference between Sinusoidal Positional Embedding (SinePE) and Rotary Positional Embedding (RoPE)?

- **Inductive Counting**
  - Why needed here: Inductive counting is the ability to generalize counting beyond the training lengths, which is the core challenge in this paper
  - Quick check question: What is the key difference between counting in-domain and counting inductively?

- **State Dynamics in RNNs**
  - Why needed here: The expressivity of state dynamics in RNNs affects their ability to generalize inductively in counting tasks
  - Quick check question: How do traditional RNNs (e.g., LSTM) differ from modern RNNs (e.g., Mamba, RWKV) in terms of state transitions?

## Architecture Onboarding

- **Component map:**
  Transformer: Embedding layer → L layers of self-attention + MLP blocks → LayerNorm + residual connections → Unembedding layer
  RNN: Input reception → State transition (nonlinear or linear) → Output emission
  Modern RNN: Input reception → Parallelizable state transition (e.g., convolution, linear interpolation) → Output emission

- **Critical path:**
  Transformer: Embedding → Self-attention → MLP → Output
  RNN: Input → State update → Output
  Modern RNN: Input → Parallel state update → Output

- **Design tradeoffs:**
  Transformer: Parallel processing vs. need for positional embeddings and inductive biases
  RNN: Sequential processing vs. strong inductive biases for counting
  Modern RNN: Parallelizability vs. reduced state dynamics expressivity

- **Failure signatures:**
  Transformer: Poor out-of-domain generalization, especially for inductive counting
  RNN: Inability to generalize inductively (more common in modern RNNs)
  Modern RNN: Degraded performance compared to traditional RNNs in counting tasks

- **First 3 experiments:**
  1. Train a Transformer with NoPE on shifted start counting to observe failure in out-of-domain generalization
  2. Compare traditional RNN vs. modern RNN (e.g., Mamba) performance on inductive counting tasks
  3. Test different positional embedding strategies (SinePE, APE, RoPE, SPE) on modular counting to identify which enables generalization

## Open Questions the Paper Calls Out

### Open Question 1
How do different positional embedding (PE) schemes affect the ability of Transformers to generalize counting beyond training lengths, and what are the underlying mechanisms that cause these differences? The paper provides empirical evidence showing varying effectiveness of PE schemes but doesn't fully explain the exact reasons for these differences.

### Open Question 2
Can Transformers learn to count inductively without relying on positional embeddings, and if so, what are the key factors that enable this ability? The paper demonstrates Transformers struggle with inductive counting but doesn't explore all possible architectural variations that might bypass the need for PEs.

### Open Question 3
How do modern recurrent architectures (e.g., S4, Mamba, RWKV) compare to traditional RNNs and Transformers in terms of their ability to generalize counting inductively, and what are the trade-offs between recurrence and parallelization? The paper shows modern RNNs underperform but doesn't fully explore the architectural elements contributing to this trade-off.

## Limitations
- The study focuses exclusively on synthetic counting tasks that may not fully represent real-world language understanding challenges
- The analysis of modern RNNs' underperformance lacks direct causal evidence linking parallelization to reduced expressivity
- The paper does not explore whether pre-training on larger corpora might mitigate observed limitations in Transformers

## Confidence

**High Confidence:**
- Transformers require positional embeddings and multiple layers to achieve in-domain counting accuracy
- Traditional RNNs consistently outperform modern RNNs in inductive counting generalization
- Different positional embedding strategies exhibit varying effectiveness for different counting task variants

**Medium Confidence:**
- The failure of Transformers to count inductively stems from insufficient inductive biases rather than model capacity
- Modern RNNs' parallelization-friendly state transitions directly cause their inferior inductive generalization
- Specific positional embedding strategies can be matched to task requirements for optimal performance

**Low Confidence:**
- The relative performance differences would persist on real-world counting tasks beyond synthetic sequences
- Alternative architectural modifications could not compensate for the identified weaknesses
- The observed patterns generalize across different language modeling objectives

## Next Checks

1. **Causal Analysis of RNN State Transitions**: Implement a modified version of Mamba or RWKV that incorporates more expressive state transitions while maintaining some parallelization benefits. Test whether this hybrid architecture can match traditional RNN performance on inductive counting tasks, directly testing the parallelization-expressivity hypothesis.

2. **Scaling Study with Transformers**: Systematically increase Transformer depth and width beyond the tested configurations while maintaining the same positional embedding strategies. Determine whether model capacity alone can overcome the inductive counting limitations, or if architectural modifications are necessary.

3. **Transfer to Natural Language Tasks**: Design a real-world counting task using actual language data (e.g., counting objects in image captions or tracking numerical information in text) that requires inductive generalization. Test whether the observed patterns from synthetic tasks replicate in this more ecologically valid setting.