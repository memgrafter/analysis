---
ver: rpa2
title: 'RLVF: Learning from Verbal Feedback without Overgeneralization'
arxiv_id: '2402.10893'
source_url: https://arxiv.org/abs/2402.10893
tags:
- feedback
- health
- chatbot
- when
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces C3PO, a method for incorporating high-level
  verbal feedback into language models without overgeneralizing the feedback to irrelevant
  contexts. C3PO generates synthetic preference data specifying how feedback should
  be applied, then fine-tunes the model with a combined objective that applies feedback
  to relevant prompts while preserving baseline behavior elsewhere.
---

# RLVF: Learning from Verbal Feedback without Overgeneralization

## Quick Facts
- **arXiv ID**: 2402.10893
- **Source URL**: https://arxiv.org/abs/2402.10893
- **Reference count**: 40
- **One-line primary result**: Reduces overgeneralization by 30% while maintaining feedback adherence, outperforming existing approaches

## Executive Summary
C3PO introduces a method for incorporating high-level verbal feedback into language models without overgeneralizing the feedback to irrelevant contexts. The approach generates synthetic preference data specifying how feedback should be applied, then fine-tunes the model with a combined objective that applies feedback to relevant prompts while preserving baseline behavior elsewhere. Experiments show C3PO effectively reduces overgeneralization while maintaining feedback adherence.

## Method Summary
C3PO uses GPT-4 to interpret high-level verbal feedback and generate synthetic preference data that specifies how the feedback should and should not be applied. It creates three datasets: in-scope prompts where feedback applies, near-scope prompts where it partially applies, and out-of-scope prompts where it doesn't apply. The model is fine-tuned using a combined loss that applies Direct Preference Optimization (DPO) on in-scope data while using Supervised Fine-Tuning (SFT) losses on near-scope and out-of-scope data to preserve baseline behavior.

## Key Results
- Reduces overgeneralization by 30% compared to baseline methods
- Maintains strong feedback adherence on relevant prompts
- Outperforms existing approaches in balancing feedback application with behavior preservation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: C3PO uses synthetic preference data to encode desired feedback application while preserving baseline behavior elsewhere
- Mechanism: Generates three synthetic datasets (`Din-scope`, `Dnear-scope`, `Dout-of-scope`) and jointly fine-tunes with DPO on `Din-scope` and SFT losses on the other two
- Core assumption: LLM can accurately interpret and apply feedback when prompted to revise responses
- Break condition: If LLM fails to correctly interpret feedback during revision, synthetic pairs misrepresent desired behavior change

### Mechanism 2
- Claim: Synthetic two-policy preference data satisfies Bradley-Terry model, enabling closed-form optimal policy extraction
- Mechanism: Preferred responses from policy π+ (original model with feedback prompt), dispreferred from π- (baseline), yielding scoring function r*(x,y) = log π+(y|x) / π-(y|x) + C(x)
- Core assumption: Bradley-Terry assumption holds for synthetic pairs generated by prompting same model under different conditions
- Break condition: If underlying preference model deviates from Bradley-Terry, closed-form policy will be suboptimal

### Mechanism 3
- Claim: Joint loss with DPO on in-scope and SFT on out-of-scope/near-scope data prevents overgeneralization
- Mechanism: Combined loss `LC3PO = LDPO(Din-scope) + λ1 LSFT(Dout-of-scope) + λ2 LSFT(Dnear-scope)` ensures adaptation only where relevant
- Core assumption: SFT losses are sufficient to constrain model's conditional distribution on out-of-scope inputs
- Break condition: If SFT regularization is too weak, overgeneralization persists; if too strong, in-scope feedback adherence drops

## Foundational Learning

- **Bradley-Terry discrete choice model**: Provides theoretical foundation for learning from synthetic preference pairs without iterative RL. *Quick check*: What does the Bradley-Terry model assume about relationship between preference scores and choice probabilities?

- **Preference-based reinforcement learning (PbRL)**: C3PO builds on PbRL but replaces human-labeled preferences with synthetically generated ones. *Quick check*: How does PbRL differ from standard RL in terms of feedback type?

- **Knowledge distillation / constrained fine-tuning**: SFT losses on out-of-scope/near-scope data act as form of distillation to preserve baseline behavior. *Quick check*: Why might pure KL-divergence distillation be too restrictive for this task?

## Architecture Onboarding

- **Component map**: GPT-4 feedback interpreter → category and prompt generator → Base LLM π0 → baseline and revised response generator → DPO trainer (in-scope) → SFT trainer (out-of-scope/near-scope) → LoRA adapter

- **Critical path**: 1) Generate categories and in-scope prompts (GPT-4) 2) Generate near-scope and out-of-scope prompts (fixed set) 3) Sample baseline responses (π0) 4) Generate revised responses with feedback (π0 + prompt) 5) Build synthetic preference dataset Din-scope 6) Fine-tune with combined loss (DPO + SFT)

- **Design tradeoffs**: Synthetic vs. human preference data (cheaper but depends on model interpretation accuracy); SFT vs. KL-divergence constraint (simpler to implement but may be less strict); LoRA rank choice (balances adaptation strength vs. overfitting risk)

- **Failure signatures**: In-scope feedback adherence drops (SFT constraint too strong or synthetic data misaligned); Out-of-scope behavior changes (insufficient regularization or poor near-scope sampling); Slow convergence (learning rate too low or β in DPO suboptimal)

- **First 3 experiments**: 1) Verify synthetic preference pairs satisfy Bradley-Terry by fitting reward model and checking log-odds match theoretical r* 2) Ablate λ1 and λ2 to find sweet spot balancing in-scope adherence vs. out-of-scope preservation 3) Compare single-feedback LoRA mixing vs. retraining on multiple feedbacks for efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance vary when learning from multiple pieces of feedback compared to learning each piece individually?
- Basis in paper: Section 5.2 discusses initial investigations into learning multiple feedbacks by mixing LoRA parameters
- Why unresolved: Only provides preliminary results showing virtually no degradation when mixing LoRA parameters for two feedbacks; comprehensive evaluation of continual learning with many feedbacks not within scope
- What evidence would resolve it: Extensive experiments evaluating performance when learning from large number of feedbacks sequentially or simultaneously, comparing to learning each feedback individually

### Open Question 2
- Question: What is the impact of complexity of verbal feedback on capabilities of base model being adapted?
- Basis in paper: Section 6 mentions future work may investigate relationship between feedback complexity and base model capabilities
- Why unresolved: No empirical results or analysis on how complexity of feedback affects performance of C3PO across different base models
- What evidence would resolve it: Experiments systematically varying complexity of feedback and evaluating performance of C3PO on different base models with varying capabilities

### Open Question 3
- Question: What alternative approaches to constraining out-of-scope model behavior exist that could potentially outperform maximum likelihood constraint used in C3PO?
- Basis in paper: Section 5.3 discusses choice of C3PO constraint formulation and mentions impact of using full knowledge distillation for out-of-scope prompts as important topic for future work
- Why unresolved: Only compares maximum likelihood constraint to full knowledge distillation; other potential approaches not explored
- What evidence would resolve it: Investigating and experimenting with alternative constraint formulations (regularization techniques, adversarial training, constraint-based optimization methods) and comparing performance to maximum likelihood constraint

## Limitations

- The core mechanism depends critically on GPT-4's ability to correctly interpret and apply high-level verbal feedback during synthetic data generation
- The effectiveness of SFT regularization in preventing out-of-scope behavior changes remains empirically validated but theoretically underconstrained
- The approach's robustness across diverse feedback types and domains is not fully established

## Confidence

- **High confidence**: The theoretical foundation of synthetic two-policy preference data satisfying the Bradley-Terry model, and the combined loss formulation for balancing in-scope adaptation with out-of-scope preservation
- **Medium confidence**: The empirical effectiveness of C3PO in reducing overgeneralization while maintaining feedback adherence, as reported results depend on specific experimental conditions and feedback examples not fully disclosed
- **Low confidence**: The robustness of GPT-4's feedback interpretation across diverse feedback types and domains, and the generalizability of the approach beyond the tested scenarios

## Next Checks

1. **Synthetic preference validation**: Generate synthetic preference data for a known feedback task, fit a reward model, and verify that learned reward scores match the theoretical Bradley-Terry prediction (log-odds of π+ vs π-)

2. **Ablation study**: Systematically vary λ1 and λ2 to empirically identify the optimal trade-off between in-scope feedback adherence and out-of-scope behavior preservation, documenting the Pareto frontier

3. **Feedback interpretation stress test**: Evaluate GPT-4's ability to correctly interpret and apply a diverse set of high-level feedback examples across multiple domains, measuring agreement with human annotators to establish reliability bounds