---
ver: rpa2
title: 'Recognizing Limits: Investigating Infeasibility in Large Language Models'
arxiv_id: '2408.05873'
source_url: https://arxiv.org/abs/2408.05873
tags:
- tasks
- arxiv
- llms
- infeasible
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of helping large language models
  (LLMs) recognize and refuse tasks that exceed their capabilities, which can lead
  to hallucinations or incorrect responses. The authors categorize infeasible tasks
  into four main types: Physical Interaction, Virtual Interaction, Non-text Input/Output,
  and Self-awareness.'
---

# Recognizing Limits: Investigating Infeasibility in Large Language Models

## Quick Facts
- arXiv ID: 2408.05873
- Source URL: https://arxiv.org/abs/2408.05873
- Authors: Wenbo Zhang; Zihang Xu; Hengrui Cai
- Reference count: 13
- One-line primary result: Advanced LLMs can distinguish feasible from infeasible tasks with detailed prompts, but struggle in real-world scenarios; fine-tuning methods significantly improve refusal awareness.

## Executive Summary
This paper addresses the critical challenge of helping large language models (LLMs) recognize and refuse tasks that exceed their capabilities, which often leads to hallucinations or incorrect responses. The authors systematically categorize infeasible tasks into four types—Physical Interaction, Virtual Interaction, Non-text Input/Output, and Self-awareness—and construct a new dataset to benchmark multiple LLMs' abilities to decline such tasks. Through extensive experimentation, they demonstrate that while advanced LLMs can distinguish feasible from infeasible tasks with detailed prompts, they struggle in real-world scenarios. The study proposes and evaluates two fine-tuning strategies that significantly improve refusal awareness, though a trade-off between helpfulness and refusal awareness remains.

## Method Summary
The paper employs a two-stage training framework to enhance LLMs' refusal awareness. First, a new dataset of infeasible and feasible tasks is constructed through automatic generation using LLMs, followed by quality checks including filtering and deduplication. The dataset categorizes infeasible tasks into four main types. Second, the authors propose two fine-tuning strategies—selection-based and augment-based—that modify the training data by substituting original responses with refusal expressions for infeasible queries. Models are evaluated using verbalized confidence elicitation, where LLMs provide confidence scores on a scale from 0 to 100. The approach is tested on LLaMA-2-7B and Open-LLaMA-3B models, with comparisons to random-based fine-tuning as a baseline.

## Key Results
- Advanced LLMs can distinguish feasible from infeasible tasks with detailed prompts, achieving significant improvements in refusal awareness through fine-tuning.
- The selection-based fine-tuning strategy achieves the highest refusal rates at 66% and 73.5% on out-of-distribution benchmarks for two models.
- A trade-off exists between helpfulness and refusal awareness, with fine-tuned models experiencing drops in general helpfulness despite improved refusal capabilities.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper introduces a systematic framework to classify tasks that are infeasible for LLMs, enabling clearer evaluation of model limitations.
- Mechanism: By categorizing infeasible tasks into four types—Physical Interaction, Virtual Interaction, Non-text Input/Output, and Self-awareness—the paper provides a structured approach to identify and generate infeasible tasks. This categorization covers a broad spectrum of hallucination-related challenges, allowing for more precise benchmarking.
- Core assumption: A well-defined taxonomy of infeasible tasks will lead to better evaluation and improvement of LLM capabilities.
- Evidence anchors:
  - [abstract] "We conceptualize four main categories of infeasible tasks for LLMs, which cover a broad spectrum of hallucination-related challenges identified in prior literature."
  - [section 2.1] "We investigate four main categories of infeasible tasks with illustrative examples in Table 1."
- Break condition: If the categories are too broad or miss important infeasible task types, the framework may not accurately reflect real-world limitations.

### Mechanism 2
- Claim: Fine-tuning with refusal-augmented datasets improves LLMs' ability to decline infeasible tasks.
- Mechanism: The paper proposes two strategies for creating a refusal-augmented dataset: selection-based and augment-based. By training on data where infeasible tasks are explicitly labeled with refusal responses, LLMs learn to recognize and decline such tasks autonomously.
- Core assumption: LLMs can learn to refuse infeasible tasks if trained with explicit examples of refusal responses.
- Evidence anchors:
  - [abstract] "We propose two strategies to enhance the refusal awareness of LLMs when faced with infeasible tasks, by constructing a refusal-augmented instruction tuning dataset."
  - [section 4.1] "We employ a two-stage training framework... modifying the dataset by substituting the original responses with refusal expressions for infeasible queries."
- Break condition: If the refusal expressions are not diverse enough or the selection of infeasible data is inaccurate, the fine-tuning may not effectively teach the model to refuse.

### Mechanism 3
- Claim: Verbalized confidence elicitation helps LLMs distinguish between feasible and infeasible tasks.
- Mechanism: The paper uses a regression-style method where LLMs provide confidence scores on a scale from 0 to 100, reflecting their perceived accuracy of the response. This method allows for the evaluation of the model's ability to express uncertainty.
- Core assumption: LLMs can accurately express their confidence levels when prompted in a specific way.
- Evidence anchors:
  - [section 3] "We employ a regression-style method of elicitation, where LLMs provide confidence scores on a scale from 0 to 100, reflecting their perceived accuracy of the response."
  - [section 3.2] "GPT-4 consistently delivers the most precise (highest AUROC and KSS) and well-calibrated (lowest Brier Score) confidence estimates through direct verbalization."
- Break condition: If the model is not well-calibrated or the prompting method does not elicit accurate confidence scores, the evaluation may not reflect true uncertainty.

## Foundational Learning

- Concept: Understanding the difference between feasible and infeasible tasks for LLMs.
  - Why needed here: To evaluate and improve LLM capabilities, it's essential to clearly define what tasks are within their scope and which are not.
  - Quick check question: Can you provide an example of a task that is infeasible for an LLM and explain why?

- Concept: Fine-tuning techniques for improving model behavior.
  - Why needed here: The paper explores methods to enhance LLMs' refusal awareness, which requires understanding how fine-tuning can modify model responses.
  - Quick check question: How does the selection-based fine-tuning strategy differ from the augment-based strategy in the paper?

- Concept: Verbalized confidence elicitation and its role in uncertainty quantification.
  - Why needed here: The paper uses this method to assess LLMs' ability to distinguish between feasible and infeasible tasks, which is crucial for evaluating their reliability.
  - Quick check question: What are the advantages and disadvantages of using verbalized confidence scores compared to other uncertainty quantification methods?

## Architecture Onboarding

- Component map: Data Generation -> Quality Check -> Benchmark Creation -> Model Evaluation -> Fine-tuning
- Critical path: 1. Generate and curate a high-quality dataset of infeasible and feasible tasks. 2. Evaluate LLMs' performance on distinguishing between these tasks. 3. Implement fine-tuning strategies to enhance refusal awareness. 4. Assess the trade-off between helpfulness and refusal awareness.
- Design tradeoffs: Dataset Diversity vs. Quality (balancing wide range of tasks with accurate data), Fine-tuning Strategy (selection-based vs. augment-based methods), Evaluation Metrics (appropriate metrics for assessing performance and calibration)
- Failure signatures: High refusal rates on feasible tasks (over-refusal), Low refusal rates on infeasible tasks (insufficient refusal awareness), Poor calibration of confidence scores (unreliable uncertainty quantification)
- First 3 experiments: 1. Evaluate different LLMs on the Infeasible Benchmark using verbalized confidence methods. 2. Implement and compare selection-based and augment-based fine-tuning strategies on a base LLM. 3. Assess the trade-off between helpfulness and refusal awareness by evaluating fine-tuned models on both feasible and infeasible tasks.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the traditional sense, but several areas for future work are implied throughout the discussion. These include extending the framework to multimodal models, exploring more sophisticated fine-tuning strategies to minimize the helpfulness-refusal trade-off, and investigating how to adapt the approach for specialized AI agents with expanded capabilities beyond text-based interactions.

## Limitations
- The study relies heavily on automatic generation of infeasible tasks using LLMs, which may introduce bias in the dataset construction.
- The effectiveness of fine-tuning strategies shows significant variation across different approaches, with no single method optimally balancing helpfulness and refusal awareness.
- The assumption that verbalized confidence scores accurately reflect true model uncertainty remains questionable, as the paper's reliance on GPT-4 and GPT-4o for evaluation may introduce calibration bias.

## Confidence
- High Confidence: The categorization framework for infeasible tasks (Physical Interaction, Virtual Interaction, Non-text Input/Output, Self-awareness) is well-supported by evidence and provides a useful taxonomy for evaluating LLM limitations.
- Medium Confidence: The effectiveness of fine-tuning strategies for improving refusal awareness is demonstrated but shows significant variation across different approaches, with selection-based strategy showing promising but not universally optimal results.
- Low Confidence: The assumption that verbalized confidence scores accurately reflect true model uncertainty remains questionable, and the paper's reliance on GPT-4 and GPT-4o for evaluation introduces potential bias.

## Next Checks
1. **Cross-Modal Validation**: Test the refusal-aware fine-tuning strategies on multimodal models (e.g., GPT-4V, Gemini) to assess whether the approach generalizes beyond text-only LLMs and to identify any modality-specific challenges in recognizing infeasible tasks.

2. **Longitudinal Performance Assessment**: Evaluate the stability of refusal awareness improvements over time by periodically testing fine-tuned models on the infeasible benchmark, measuring degradation rates and identifying factors that contribute to declining performance.

3. **Human Evaluation of Calibration**: Conduct human studies where participants assess the alignment between model verbalized confidence scores and actual task feasibility, providing ground truth data on whether the confidence elicitation method accurately reflects model capabilities and uncertainty.