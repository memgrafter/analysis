---
ver: rpa2
title: Findings of the First Workshop on Simulating Conversational Intelligence in
  Chat
arxiv_id: '2402.06420'
source_url: https://arxiv.org/abs/2402.06420
tags:
- evaluation
- human
- graham
- systems
- workshop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the results of the first workshop on simulating
  conversational intelligence in chat (SCI-CHAT), which aimed to bring together experts
  working on open-domain dialogue research. The shared task focused on simulating
  intelligent conversation as judged in a live human evaluation.
---

# Findings of the First Workshop on Simulating Conversational Intelligence in Chat

## Quick Facts
- arXiv ID: 2402.06420
- Source URL: https://arxiv.org/abs/2402.06420
- Authors: Yvette Graham; Mohammed Rameez Qureshi; Haider Khalid; Gerasimos Lampouras; Ignacio Iacobacci; Qun Liu
- Reference count: 2
- Primary result: ANON-A model outperformed all others in live human evaluation of conversational intelligence

## Executive Summary
This paper presents the results of the first workshop on simulating conversational intelligence in chat (SCI-CHAT), which brought together experts in open-domain dialogue research. The shared task focused on simulating intelligent conversation through live human evaluation, where models were judged on criteria including overall intelligence, interestingness, informativeness, fluency, credibility, consistency, coherence, and repetitiveness. The evaluation used a Direct Assessment method with quality-controlled crowd-sourcing, employing a continuous 0-100 scale for ratings.

The workshop successfully demonstrated the feasibility of live human evaluation for dialogue models and provided insights into the challenges of simulating human-like conversation in AI systems. ANON-A emerged as the top-performing model, while a large cluster of systems tied for second place. The results highlighted both the potential and limitations of current approaches to open-domain dialogue generation.

## Method Summary
The workshop employed a Direct Assessment human evaluation method adapted for open-domain dialogue, using quality-controlled crowd-sourcing via Mechanical Turk. Models were evaluated through live conversations with human judges on pre-selected topics from Freakonomics podcast transcripts. The evaluation criteria included eight dimensions rated on a continuous 0-100 Likert scale. Statistical significance testing using Mann-Whitney U test on z-scored ratings compared model performance, with quality control filtering to ensure reliable worker judgments.

## Key Results
- ANON-A model significantly outperformed all other systems in the competition
- A large cluster of systems tied for second place, indicating limited discrimination between similar models
- Direct Assessment method successfully enabled live human evaluation of open-domain dialogue models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct Assessment (DA) enables reliable live human evaluation of open-domain dialogue models
- Mechanism: DA adapts a proven MT evaluation method by asking evaluators to rate a single model on a continuous 0-100 scale using Likert statements after live interaction, avoiding reference-based limitations
- Core assumption: Crowd-sourced evaluators can reliably discriminate between models when quality control filtering is applied
- Evidence anchors:
  - [abstract] "evaluation process involved human judges rating the performance of models based on criteria such as overall intelligence, interestingness, informativeness, fluency, credibility, consistency, coherence, and repetitiveness"
  - [section] "human evaluation is carried out using the Direct Assessment method adapted for Open-domain dialogue (Ji et al., 2022) that employs quality controlled crowd-sourcing"
  - [corpus] Weak evidence - no direct corpus study of DA for dialogue exists yet
- Break condition: If quality control filtering fails to identify unreliable workers or if the continuous scale leads to inconsistent judgments across workers

### Mechanism 2
- Claim: Using topic steering from podcast titles creates consistent evaluation conditions while maintaining open-domain dialogue challenges
- Mechanism: Evaluators discuss pre-selected topics (e.g., "What's the point of nostalgia?") with models, ensuring conversations have direction without constraining model responses
- Core assumption: Topics drawn from training data still provide a valid test of conversational intelligence
- Evidence anchors:
  - [abstract] "Models aim to include the ability to follow a challenging topic over a multi-turn conversation"
  - [section] "During human evaluation, human judges were given an assigned topic from a past podcast to discuss with models"
  - [corpus] Weak evidence - no corpus study comparing topic steering vs. free-form evaluation
- Break condition: If models overfit to specific training topics or if topic steering reduces naturalness of conversation

### Mechanism 3
- Claim: Statistical significance testing using Mann-Whitney U test identifies meaningful performance differences despite continuous rating scale
- Mechanism: Individual criteria ratings are converted to z-scores per worker, then Mann-Whitney U test compares distributions between model pairs to identify statistical wins
- Core assumption: Z-score normalization per worker adequately accounts for individual rating tendencies
- Evidence anchors:
  - [abstract] "average scores for each model that took part in the competition"
  - [section] "To take into account the fact that systems with distinct overall scores can be considered a statistical tie, we apply statistical significance tests between z scores of each pair of participating system using Mann-Whitney U test"
  - [corpus] Weak evidence - no corpus study validating this specific application to dialogue evaluation
- Break condition: If z-score normalization fails to account for systematic biases or if test assumptions are violated

## Foundational Learning

- Concept: Statistical significance testing for continuous ratings
  - Why needed here: To determine if performance differences between models are meaningful rather than random variation
  - Quick check question: What does a p-value below 0.05 indicate in the context of Mann-Whitney U test for model comparison?

- Concept: Quality control filtering in crowd-sourcing
  - Why needed here: To ensure evaluation results reflect model performance rather than unreliable worker judgments
  - Quick check question: How does significance testing between worker ratings of known low-quality models and target models identify unreliable workers?

- Concept: Continuous vs. categorical rating scales
  - Why needed here: DA uses 0-100 continuous scale instead of traditional Likert categories, requiring different agreement measurement approaches
  - Quick check question: Why can't Cohen's Kappa coefficient be used for agreement measurement with continuous rating scales?

## Architecture Onboarding

- Component map: Human evaluation interface -> Topic assignment system -> Model response collection -> DA rating collection -> Quality control filtering -> Statistical analysis pipeline
- Critical path: Topic assignment -> Live conversation -> DA rating collection -> Quality control filtering -> Statistical significance testing
- Design tradeoffs: Continuous scale provides granularity but requires more complex agreement measurement vs. categorical scales; topic steering ensures consistency but may limit naturalness
- Failure signatures: High variance in worker ratings after quality control filtering; statistical tests showing no significant differences between models; quality control filtering rejecting too many workers
- First 3 experiments:
  1. Test quality control filtering with synthetic data: Create known good/bad model responses and verify filtering correctly identifies reliable workers
  2. Validate z-score normalization: Check if per-worker z-score normalization adequately accounts for rating biases
  3. Test topic steering effectiveness: Compare evaluation consistency between topic-guided vs. free-form conversations using same models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do models trained on topics outside their training data perform in live human evaluation compared to those trained on the same topics?
- Basis in paper: [explicit] The paper mentions that future evaluations aim to compare results using topics outside the training dataset
- Why unresolved: This study used topics from within the training dataset (Freakonomics podcast transcripts), so the impact of out-of-domain topics remains untested
- What evidence would resolve it: Conduct live human evaluations using topics not present in the training data and compare the performance metrics with those from in-domain topics

### Open Question 2
- Question: What are the specific architectural differences between the winning model (ANON-A) and the second-place models that contribute to its superior performance?
- Basis in paper: [explicit] The paper states that ANON-A outperformed all other systems, but does not provide detailed architectural comparisons
- Why unresolved: The paper only provides a high-level overview of the models and their performance without delving into the technical specifics that led to ANON-A's success
- What evidence would resolve it: A detailed technical analysis comparing the architectures, training methodologies, and fine-tuning strategies of ANON-A with those of the second-place models

### Open Question 3
- Question: How does the inclusion of humor and sarcasm, as attempted by the SarcEmp model, impact the perceived intelligence and engagement of dialogue systems?
- Basis in paper: [explicit] The paper describes the SarcEmp model's aim to incorporate humor and sarcasm for better user engagement
- Why unresolved: While the model's approach is mentioned, the paper does not provide results or analysis on how these features affect user perception of intelligence and engagement
- What evidence would resolve it: Comparative live human evaluations of models with and without humor/sarcasm features, focusing on user ratings of intelligence and engagement

## Limitations
- Evaluation methodology relies heavily on crowd-sourced human judgment with unspecified quality control thresholds
- Topic steering from podcast titles may artificially constrain open-domain dialogue and bias results toward models trained on similar topics
- Statistical significance testing assumes z-score normalization adequately accounts for individual worker biases, which needs empirical validation

## Confidence
**High Confidence Claims:**
- The workshop successfully demonstrated the feasibility of live human evaluation of dialogue models
- ANON-A model performed best in the competition
- Direct Assessment method can be adapted for open-domain dialogue evaluation

**Medium Confidence Claims:**
- The quality control filtering effectively identified reliable workers
- Topic steering provided consistent evaluation conditions without significantly constraining model performance
- Statistical significance testing using Mann-Whitney U test on z-scored ratings reliably identified performance differences

**Low Confidence Claims:**
- The continuous 0-100 scale provided better discrimination than categorical scales
- The specific criteria (intelligence, interestingness, etc.) comprehensively captured conversational quality
- The results generalize to other open-domain dialogue tasks beyond the specific topics used

## Next Checks
1. **Quality Control Validation**: Conduct a controlled experiment using known good and bad model responses to empirically validate the quality control filtering thresholds and verify that the filtering mechanism correctly identifies reliable workers

2. **Topic Steering Impact Analysis**: Compare evaluation consistency and model rankings between topic-guided conversations and free-form conversations using the same models to quantify the impact of topic steering on evaluation validity

3. **Statistical Method Sensitivity Analysis**: Perform sensitivity analysis on the z-score normalization and Mann-Whitney U test parameters by systematically varying quality control thresholds and statistical significance levels to determine robustness of model rankings