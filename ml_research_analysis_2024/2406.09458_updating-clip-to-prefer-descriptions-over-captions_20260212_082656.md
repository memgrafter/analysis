---
ver: rpa2
title: Updating CLIP to Prefer Descriptions Over Captions
arxiv_id: '2406.09458'
source_url: https://arxiv.org/abs/2406.09458
tags:
- image
- clip
- concadia
- description
- descriptions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper updates the CLIP model to distinguish between descriptions
  (meant to replace images) and captions (meant to complement images), addressing
  a key limitation of current image-text metrics for accessibility. The authors propose
  two fine-tuning objectives: a behavioral objective that maximizes description scores
  over caption scores, and an IIT-DAS objective that localizes the description-caption
  distinction to a linear subspace of CLIP''s activations.'
---

# Updating CLIP to Prefer Descriptions Over Captions

## Quick Facts
- arXiv ID: 2406.09458
- Source URL: https://arxiv.org/abs/2406.09458
- Reference count: 16
- The paper updates CLIP to distinguish descriptions from captions, achieving 86-90% accuracy while preserving transfer capabilities and better aligning with blind/low-vision user preferences.

## Executive Summary
This paper addresses a critical limitation in current image-text models by updating CLIP to distinguish between descriptions (meant to replace images) and captions (meant to complement images). The authors propose two fine-tuning objectives: a behavioral objective that directly maximizes description scores over caption scores, and an IIT-DAS objective that localizes this distinction to a linear subspace of CLIP's activations. Using LoRA fine-tuning on the Concadia dataset, the updated models achieve high accuracy in preferring descriptions while maintaining strong performance on transfer tasks. Critically, these models show significantly better correlation with blind and low-vision users' evaluations compared to the original CLIP model.

## Method Summary
The authors employ LoRA (Low-Rank Adaptation) fine-tuning on the CLIP ViT-B/32 model using the Concadia dataset of 96,918 images with descriptions, captions, and textual context. Two objectives are explored: a behavioral objective using contrastive loss to maximize description scores over captions, and an IIT-DAS objective that uses interchange intervention training to localize the description-caption distinction to a linear activation subspace. The models are evaluated on Concadia accuracy, transfer tasks (CIFAR-100, Food101, ImageNet), and correlation with human evaluations from blind and low-vision users.

## Key Results
- LoRA fine-tuned models achieve 86-90% accuracy in preferring descriptions over captions on Concadia
- IIT-DAS objective produces more interpretable models with gradient attributions aligning with human judgments of imageability and concreteness
- Both fine-tuned models correlate significantly better with blind and low-vision user evaluations compared to original CLIP
- Transfer capabilities are preserved with only modest drops on benchmark tasks (~10% on Food101/ImageNet, ~6% on CIFAR)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The IIT-DAS objective localizes the description-caption distinction to a linear subspace of CLIP's activations, making the model more interpretable.
- Mechanism: By using interchange intervention training with distributed alignment search, the model learns to represent the description-caption distinction in a specific linear subspace of its activation vectors. This allows for targeted analysis of how the model distinguishes between descriptions and captions.
- Core assumption: The description-caption distinction can be represented as a linear subspace in CLIP's activation space.
- Evidence anchors:
  - [abstract]: "The IIT-DAS objective also produces more interpretable models, with gradient attributions aligning with human judgments of imageability and concreteness."
  - [section]: "To better maintain CLIP’s original capabilities, we propose a novel objective called IIT-DAS, which localizes the description–caption distinction in a linear subspace Z of an activation vector in Cθ."
- Break condition: If the description-caption distinction cannot be represented as a linear subspace, the IIT-DAS objective would fail to localize the distinction and would not improve interpretability.

### Mechanism 2
- Claim: LoRA fine-tuning preserves CLIP's transfer capabilities while enabling the model to prefer descriptions over captions.
- Mechanism: LoRA freezes the original model weights and learns low-rank adaptations, allowing the model to adapt to the description-caption distinction without overwriting its original capabilities for image classification tasks.
- Core assumption: Low-rank adaptations are sufficient to capture the description-caption distinction without compromising the model's original capabilities.
- Evidence anchors:
  - [section]: "LoRA helps preserve much of the original model’s capabilities (dropping≈10% on Food101 and Imagenet, and ≈6% on CIFAR)."
  - [abstract]: "Using parameter-efficient LoRA fine-tuning on the Concadia dataset, the updated models achieve 86-90% accuracy in preferring descriptions over captions while preserving transfer capabilities."
- Break condition: If the description-caption distinction requires more than low-rank adaptations, LoRA fine-tuning would not be sufficient and the model would fail to prefer descriptions over captions effectively.

### Mechanism 3
- Claim: The behavioral objective directly optimizes CLIP to assign higher scores to descriptions than captions, improving alignment with BLV user preferences.
- Mechanism: By using a contrastive loss that maximizes the score for descriptions compared to captions, the model learns to prioritize descriptions, which aligns with the needs of BLV users who rely on descriptions to replace visual information.
- Core assumption: Maximizing description scores over caption scores will lead to better alignment with BLV user preferences.
- Evidence anchors:
  - [abstract]: "This model correlates with the judgements of blind and low-vision people while preserving transfer capabilities..."
  - [section]: "Our results show clearly that fine-tuning CLIP on the Concadia dataset results in a CLIPScore that is better aligned with the judgments of BLV individuals."
- Break condition: If BLV user preferences are not solely based on description vs. caption distinction, this objective might not fully align with their needs.

## Foundational Learning

- Concept: Causal interpretability
  - Why needed here: The paper uses causal interpretability techniques (IIT-DAS) to make the model's decision-making process more transparent and interpretable.
  - Quick check question: How does interchange intervention training differ from standard intervention training in causal models?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: LoRA is used as a parameter-efficient fine-tuning method to update CLIP without losing its original capabilities.
  - Quick check question: What is the main advantage of using LoRA over full fine-tuning in terms of model preservation?

- Concept: Contrastive learning
  - Why needed here: Contrastive learning is used in both the behavioral and IIT-DAS objectives to distinguish between descriptions and captions.
  - Quick check question: How does the contrastive loss function help in distinguishing between descriptions and captions?

## Architecture Onboarding

- Component map:
  CLIP model (image and text encoders) -> LoRA adapters (low-rank matrices) -> IIT-DAS intervention site (linear subspace) -> Concadia dataset (image-description-caption triplets) -> Loss functions (behavioral and IIT-DAS)

- Critical path:
  1. Load CLIP model and Concadia dataset
  2. Apply LoRA fine-tuning with chosen objective
  3. Evaluate on Concadia and transfer tasks
  4. Analyze interpretability with mediated integrated gradients

- Design tradeoffs:
  - Behavioral vs. IIT-DAS objective: Behavioral achieves higher description preference but IIT-DAS offers better interpretability
  - LoRA rank size: Higher rank allows more adaptation but increases computational cost
  - Intervention site location: Different layers may capture different aspects of the description-caption distinction

- Failure signatures:
  - Loss of transfer capabilities (high Concadia accuracy but poor performance on Food101/ImageNet)
  - Failure to localize distinction (IIT-DAS shows no improvement in interpretability)
  - Overfitting to Concadia (high training accuracy but poor generalization)

- First 3 experiments:
  1. Full fine-tuning with behavioral objective (baseline for comparison)
  2. LoRA fine-tuning with IIT-DAS objective (test interpretability benefits)
  3. Joint objective training (explore potential synergies between behavioral and IIT-DAS objectives)

## Open Questions the Paper Calls Out
None

## Limitations
- The correlation with BLV user preferences, while significant, doesn't establish causation or comprehensive alignment with all user needs
- The IIT-DAS interpretability claims depend on the assumption that description-caption distinctions can be meaningfully represented as linear subspaces
- LoRA's parameter efficiency may limit the model's ability to fully capture complex description-caption distinctions

## Confidence

- **High confidence**: The behavioral objective successfully improves description preference over captions, as evidenced by strong performance on the Concadia dataset and maintained transfer capabilities on benchmark tasks.
- **Medium confidence**: The IIT-DAS objective provides better interpretability through localized activation subspaces, though this depends on the linear separability assumption.
- **Medium confidence**: The correlation with BLV user evaluations demonstrates practical relevance, though correlation doesn't establish causation or comprehensive alignment with user needs.

## Next Checks

1. **Direct user preference validation**: Conduct A/B testing with actual BLV users to measure preference between original CLIP and fine-tuned models, rather than relying solely on correlation with previous evaluations.

2. **Cross-domain robustness testing**: Evaluate the fine-tuned models on description-caption datasets from different domains (e.g., medical imaging, technical documentation) to verify the generalization of the learned distinction beyond Concadia.

3. **Ablation study on intervention site selection**: Systematically test different layers and subspace sizes for the IIT-DAS objective to determine optimal intervention locations and validate that layer 10 with size 256 is indeed the most effective choice.