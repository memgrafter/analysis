---
ver: rpa2
title: 'State Space Models on Temporal Graphs: A First-Principles Study'
arxiv_id: '2406.00943'
source_url: https://arxiv.org/abs/2406.00943
tags:
- graph
- temporal
- graphs
- state
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends state space models (SSMs) to temporal graphs
  by formulating a Laplacian-regularized online approximation objective. The authors
  derive a piecewise dynamical system from this objective and develop GraphSSM, which
  employs a novel mixed discretization strategy to handle unobserved graph mutations.
---

# State Space Models on Temporal Graphs: A First-Principles Study

## Quick Facts
- arXiv ID: 2406.00943
- Source URL: https://arxiv.org/abs/2406.00943
- Authors: Jintang Li; Ruofan Wu; Xinzhou Jin; Boqun Ma; Liang Chen; Zibin Zheng
- Reference count: 40
- Primary result: GraphSSM achieves state-of-the-art performance on temporal graph benchmarks, particularly for long-range sequences

## Executive Summary
This paper presents GraphSSM, a novel framework that extends state space models to temporal graphs by integrating structural information through Laplacian regularization. The authors derive a piecewise dynamical system from a Laplacian-regularized online approximation objective and develop a mixed discretization strategy to handle unobserved graph mutations. GraphSSM demonstrates superior performance on six temporal graph benchmarks, outperforming existing methods including MPNN, STAR, and SpikeNet, with the S4 variant showing particular effectiveness for long-range sequences.

## Method Summary
GraphSSM extends the HIPPO online approximation objective with Laplacian regularization to create a piecewise dynamical system for temporal graph modeling. The framework uses a mixed discretization strategy combining ZOH, feature mixing, and representation mixing to approximate state evolution between observed graph snapshots. The model employs SSM layers (particularly S4) with GNNs for diffusion approximation and residual connections. Training is performed on six temporal graph benchmarks with 81%/9%/10% or 80% splits, evaluating performance using Micro-F1 and Macro-F1 metrics.

## Key Results
- GraphSSM achieves new state-of-the-art performance on several temporal graph benchmarks
- S4 architecture is identified as the most effective variant for long-range temporal graph sequences
- The framework demonstrates strong scalability and effectiveness in capturing evolving graph dynamics
- GraphSSM outperforms existing temporal graph learning methods including MPNN, STAR, and SpikeNet

## Why This Works (Mechanism)

### Mechanism 1
GraphSSM improves temporal graph learning by integrating structural information into the SSM framework through Laplacian regularization. The GHIPPO abstraction extends the HIPPO online approximation objective with a Laplacian regularization term, which encourages smoothness of the memory compression with respect to adjacent nodes. This allows the model to simultaneously compress feature dynamics and evolving topological structure.

### Mechanism 2
GraphSSM handles unobserved graph mutations through a novel mixed discretization strategy. The model uses a combination of ordinary ZOH, feature mixing, and representation mixing to approximate the state evolution between observed snapshots. This addresses the challenge of temporal dynamics that are inconsistent across observed intervals.

### Mechanism 3
GraphSSM-S4 architecture is most effective for long-range temporal graph sequences. The SISO configuration of GraphSSM-S4 extends the one-dimensional recurrence to handle general dimensions, providing optimal memory compression and efficient long-range modeling.

## Foundational Learning

- **State Space Models (SSMs) and their discretization**: GraphSSM builds upon SSM theory to model temporal graphs as discretized representations of an underlying continuous-time linear dynamical system. Quick check: What is the key advantage of SSMs over RNNs and Transformers for sequence modeling?

- **Graph Neural Networks (GNNs) and message passing**: GraphSSM uses GNNs to approximate the diffusion operation between node features, incorporating structural information into the model. Quick check: How do different types of graph Laplacians (symmetric normalized vs. random walk normalized) affect the smoothness regularization?

- **Temporal graphs and their representation**: GraphSSM is designed to handle temporal graphs, which are formalized as ordered sequences of static graph snapshots observed at discrete time points. Quick check: What are the challenges of modeling temporal graphs compared to static graphs?

## Architecture Onboarding

- **Component map**: Input node features and graph snapshots -> GNN approximates diffusion of node features -> Mixing mechanism combines consecutive snapshots -> SSM layer updates state based on mixed input -> Output is generated from updated state

- **Critical path**: 
  1. Input node features and graph snapshots
  2. GNN approximates diffusion of node features
  3. Mixing mechanism combines consecutive snapshots
  4. SSM layer updates state based on mixed input
  5. Output is generated from updated state

- **Design tradeoffs**: Choice of mixing mechanism (feature vs. representation mixing) affects performance; Initialization strategy for state matrices impacts long-range modeling capability; Complexity of GNN used for diffusion approximation affects computational efficiency

- **Failure signatures**: Poor performance on datasets with high temporal continuity (mixing strategies may be unnecessary); Instability in long sequences (may indicate issues with initialization or state matrix design); High computational cost (may indicate overly complex GNN or inefficient mixing mechanisms)

- **First 3 experiments**:
  1. Compare GraphSSM-S4 with different mixing configurations (feature vs. representation mixing) on a small temporal graph dataset
  2. Test different initialization strategies (HIPPO, constant, random) for the state matrices on a long-range temporal graph
  3. Evaluate the impact of the GNN complexity on the overall model performance and computational efficiency

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of GraphSSM compare to continuous-time temporal graph learning methods on discrete-time datasets? The paper mentions that continuous-time methods HTNE and M2DNE exhibit poor performance in DBLP-3, Brain, and Reddit datasets, but does not provide a direct comparison between GraphSSM and continuous-time methods on these datasets.

### Open Question 2
Can GraphSSM be extended to handle continuous-time temporal graphs effectively? The paper discusses limitations of GraphSSM in handling continuous-time temporal graphs in appendix E.1, but does not provide a solution or experimental results for this extension.

### Open Question 3
How does the choice of Laplacian regularization (symmetric vs. random walk) affect GraphSSM's performance? The paper discusses different types of Laplacians in section B.1 but does not provide experimental results comparing their impact on GraphSSM.

## Limitations
- Weak empirical support for the core mixing mechanisms, lacking ablation studies
- Limited architectural comparisons beyond S4 variant on only 6 datasets
- Computational complexity trade-offs between different mixing strategies are not quantified

## Confidence

- **High confidence**: The basic formulation of Laplacian-regularized approximation objective (mechanism 1) is mathematically sound and the experimental results show clear performance gains
- **Medium confidence**: The mixed discretization strategy (mechanism 2) is reasonable but lacks ablation evidence for its components
- **Medium confidence**: The superiority of GraphSSM-S4 (mechanism 3) is demonstrated but could benefit from broader architectural comparisons

## Next Checks

1. **Ablation study**: Remove the mixing mechanism entirely and compare performance to determine its actual contribution to the reported gains
2. **Broader architectural comparison**: Test GraphSSM with alternative SSM variants (S5, S6, Mamba) on the same benchmarks to validate the S4 claim
3. **Computational analysis**: Measure and compare the computational overhead of different mixing strategies (feature vs. representation mixing) to assess practical utility