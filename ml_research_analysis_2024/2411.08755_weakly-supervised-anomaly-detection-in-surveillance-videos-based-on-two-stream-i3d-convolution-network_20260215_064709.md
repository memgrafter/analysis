---
ver: rpa2
title: Weakly-Supervised Anomaly Detection in Surveillance Videos Based on Two-Stream
  I3D Convolution Network
arxiv_id: '2411.08755'
source_url: https://arxiv.org/abs/2411.08755
tags:
- anomaly
- detection
- video
- videos
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a weakly-supervised anomaly detection system
  for surveillance videos using a two-stream Inflated 3D Convolutional Neural Network
  (I3D). The model combines RGB and optical flow features to extract spatial and temporal
  information from video clips, which are processed through a Multiple Instance Learning
  (MIL) framework.
---

# Weakly-Supervised Anomaly Detection in Surveillance Videos Based on Two-Stream I3D Convolution Network

## Quick Facts
- arXiv ID: 2411.08755
- Source URL: https://arxiv.org/abs/2411.08755
- Reference count: 16
- Primary result: 85.41% AUC on UCF-Crime dataset

## Executive Summary
This paper presents a weakly-supervised anomaly detection system for surveillance videos using a two-stream Inflated 3D Convolutional Neural Network (I3D). The model combines RGB and optical flow features to extract spatial and temporal information from video clips, which are processed through a Multiple Instance Learning (MIL) framework. The system treats videos as bags of clips and uses a ranking loss to prioritize anomaly scores for abnormal segments. The proposed approach is evaluated on the UCF-Crime dataset, achieving an AUC of 85.41%, outperforming state-of-the-art methods. The results demonstrate that the two-stream I3D network effectively captures complex anomalies in surveillance videos, offering a scalable solution with reduced dependency on manual annotations. Hyperparameter tuning reveals that the Adagrad optimizer enhances performance compared to Adam.

## Method Summary
The proposed method uses a two-stream I3D network to extract 1024-dimensional RGB and optical flow features from 32 segments of 16 frames each. These features are concatenated into 2048-dimensional vectors and fed into a three-layer fully connected neural network to produce anomaly scores for each segment. The model is trained using a Multiple Instance Learning framework with a ranking loss function that incorporates sparsity and smoothness constraints. The Adagrad optimizer with a learning rate of 0.001 is used for training, with a batch size of 30. The system processes each video as a bag of segments, learning to identify anomalous segments without requiring frame-level annotations.

## Key Results
- Achieves 85.41% AUC on the UCF-Crime dataset
- Outperforms state-of-the-art methods including C3D and single-stream I3D approaches
- Adagrad optimizer shows superior performance compared to Adam for this task
- Two-stream architecture (RGB + Flow) provides complementary feature representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weak supervision through video-level labels enables anomaly detection without frame-level annotations
- Mechanism: Treats each video as a "bag" and each segment as an "instance" in Multiple Instance Learning (MIL), allowing learning from only video-level labels
- Core assumption: At least one anomalous segment exists in each abnormal video, and all segments in normal videos are normal
- Evidence anchors:
  - [abstract] "Our research advances the field by implementing a weakly supervised learning framework based on Multiple Instance Learning (MIL)"
  - [section] "We extended and refined the anomaly detection model introduced in Sultani et al. (2018) by leveraging the PyTorch framework"
  - [corpus] Weak evidence: Related works also use MIL for weakly-supervised VAD, but none directly cite this specific mechanism
- Break condition: If anomalous segments are too sparse or not representative of the whole video, or if normal videos contain subtle anomalies

### Mechanism 2
- Claim: Two-stream I3D network captures complementary spatial and temporal features for better anomaly representation
- Mechanism: RGB stream captures appearance and scene context, while flow stream captures motion dynamics; concatenation creates comprehensive feature representation
- Core assumption: Anomalies manifest through both appearance changes and motion irregularities
- Evidence anchors:
  - [abstract] "These networks substantially outperform traditional 3D Convolutional Networks (C3D) by more effectively extracting spatial and temporal features"
  - [section] "Our method employs a two-stream Inflated 3D (I3D) Convolutional Neural Network to extract both RGB and optical flow features"
  - [corpus] Weak evidence: Related works mention two-stream architectures but focus on action recognition, not anomaly detection
- Break condition: If anomalies are purely temporal or purely spatial, or if feature fusion doesn't improve discriminative power

### Mechanism 3
- Claim: Ranking loss with sparsity and smoothness constraints optimizes anomaly score distribution
- Mechanism: Uses hinge loss to maximize margin between highest-scoring abnormal segment and highest-scoring normal segment, with additional constraints to encourage sparse and smooth anomaly scores
- Core assumption: Anomalies are rare events (sparse) and should have gradual score transitions across adjacent segments (smooth)
- Evidence anchors:
  - [abstract] "Each instance is innovatively processed through a ranking mechanism that prioritizes clips based on their potential to display anomalies"
  - [section] "We implemented a deep ranking loss mechanism...to prioritize higher anomaly scores for abnormal segments over normal segments"
  - [corpus] Weak evidence: No direct mention of ranking loss with sparsity/smoothness in corpus, but similar approaches exist in weakly-supervised learning
- Break condition: If anomalies are not sparse or if abrupt changes are legitimate (e.g., explosions), or if ranking loss doesn't converge properly

## Foundational Learning

- Concept: Multiple Instance Learning (MIL) framework
  - Why needed here: Enables training with only video-level labels instead of expensive frame-level annotations
  - Quick check question: How does MIL handle the uncertainty of which specific instances are positive in a positive bag?

- Concept: Two-stream network architecture
  - Why needed here: Separately processes spatial (RGB) and temporal (flow) information before fusion, capturing complementary aspects of video content
  - Quick check question: Why not use a single 3D CNN that inherently captures both spatial and temporal features?

- Concept: Ranking loss optimization
  - Why needed here: Aligns model to produce higher scores for anomalous segments without explicit segment-level labels
  - Quick check question: How does the ranking loss differ from traditional classification loss in this weakly-supervised setting?

## Architecture Onboarding

- Component map:
  Video preprocessing → Two-stream I3D feature extractor → Feature concatenation (2048D) → Three-layer FC network → Anomaly scores → Ranking loss with MIL
  Each video → N segments → Processed through I3D RGB and I3D Flow → Average features per segment → Concatenate RGB+Flow → 3-layer MLP → Anomaly score per segment

- Critical path:
  Feature extraction quality → Feature fusion effectiveness → Ranking loss optimization → Sparsity/smoothness constraint balance → Final AUC performance

- Design tradeoffs:
  Two-stream vs single-stream: Higher computational cost but better feature complementarity
  Weak supervision vs full supervision: Reduced annotation burden but potentially noisier learning signal
  Ranking loss vs classification loss: Better suited for weakly-supervised setting but may converge slower

- Failure signatures:
  High false positives: Likely issues with flow feature extraction or ranking loss parameters
  Low recall: May indicate insufficient model capacity or need for better feature representation
  Slow convergence: Could indicate learning rate issues or inappropriate optimizer choice

- First 3 experiments:
  1. Ablation study: Compare single-stream RGB vs single-stream Flow vs two-stream performance to validate feature complementarity
  2. Optimizer comparison: Test Adam vs Adagrad across multiple learning rates to confirm Adagrad's superiority for this task
  3. MIL framework validation: Test with fully-supervised version (frame-level labels) to establish upper performance bound and measure weak supervision cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the two-stream I3D network compare to other advanced architectures (e.g., Transformers or Vision MLPs) for anomaly detection in surveillance videos?
- Basis in paper: [explicit] The paper compares the two-stream I3D network's performance to C3D and single-stream I3D models but does not explore newer architectures like Transformers or Vision MLPs.
- Why unresolved: The study focuses on I3D-based approaches and does not investigate the potential benefits of more recent architectures that could further improve anomaly detection.
- What evidence would resolve it: Comparative experiments using Transformers or Vision MLPs on the same dataset (UCF-Crime) with metrics like AUC to determine if these architectures outperform the two-stream I3D network.

### Open Question 2
- Question: What is the impact of incorporating additional modalities, such as audio or thermal data, on the accuracy and robustness of anomaly detection in surveillance videos?
- Basis in paper: [inferred] The paper mentions that traditional methods struggle with low-light conditions and suggests that auxiliary data sources could improve accuracy, but does not explore multimodal integration.
- Why unresolved: The study focuses solely on RGB and optical flow features, leaving the potential benefits of multimodal data unexplored.
- What evidence would resolve it: Experiments integrating audio or thermal data with the two-stream I3D network and evaluating performance improvements on datasets with diverse environmental conditions.

### Open Question 3
- Question: How does the weakly-supervised approach perform in scenarios with highly imbalanced datasets, where anomalies are extremely rare compared to normal events?
- Basis in paper: [explicit] The paper uses the UCF-Crime dataset, which has a balanced distribution of normal and abnormal videos, but does not address scenarios with severe class imbalance.
- Why unresolved: The evaluation is conducted on a balanced dataset, and the robustness of the weakly-supervised approach to extreme imbalance is not tested.
- What evidence would resolve it: Testing the model on datasets with skewed class distributions and analyzing its ability to maintain high detection accuracy and low false positive rates.

## Limitations

- Limited evaluation on a single dataset (UCF-Crime) without cross-dataset validation
- Weak evidence supporting proposed mechanisms with no citations provided
- Incomplete specification of MIL framework implementation details
- No ablation studies to quantify contribution of individual components

## Confidence

**High confidence**: The technical approach using two-stream I3D networks for feature extraction is well-established in the literature. The choice of UCF-Crime as a benchmark dataset is appropriate for this task.

**Medium confidence**: The claimed AUC of 85.41% appears competitive, but without comparison to a comprehensive set of baseline methods or ablation studies, the relative performance gains are uncertain.

**Low confidence**: The effectiveness of the ranking loss with sparsity and smoothness constraints lacks empirical validation through ablation studies or comparison with alternative loss functions.

## Next Checks

1. **Ablation study validation**: Implement and compare single-stream RGB, single-stream Flow, and two-stream variants to quantify the contribution of each stream to the final performance.

2. **Optimizer benchmarking**: Systematically test multiple optimizers (Adam, Adagrad, SGD) across various learning rates to confirm that Adagrad indeed provides superior performance for this specific task.

3. **Cross-dataset generalization**: Evaluate the trained model on additional surveillance anomaly detection datasets (e.g., XD-Violence, StreetScene) to assess the robustness and generalizability of the approach beyond UCF-Crime.