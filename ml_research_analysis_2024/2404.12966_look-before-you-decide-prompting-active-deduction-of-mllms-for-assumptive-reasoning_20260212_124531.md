---
ver: rpa2
title: 'Look Before You Decide: Prompting Active Deduction of MLLMs for Assumptive
  Reasoning'
arxiv_id: '2404.12966'
source_url: https://arxiv.org/abs/2404.12966
tags:
- reasoning
- answer
- assumptive
- arxiv
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MARS-Bench, a benchmark for evaluating multimodal
  assumptive reasoning in MLLMs. It finds that current MLLMs often rely on empirical
  reasoning and are easily misled by presuppositions in questions.
---

# Look Before You Decide: Prompting Active Deduction of MLLMs for Assumptive Reasoning

## Quick Facts
- arXiv ID: 2404.12966
- Source URL: https://arxiv.org/abs/2404.12966
- Reference count: 40
- Primary result: Active Deduction framework improves assumptive reasoning in MLLMs by 76.6 points while maintaining general capabilities

## Executive Summary
This paper addresses a critical weakness in multimodal large language models (MLLMs): their tendency to rely on empirical reasoning and fall prey to presuppositions in questions. The authors introduce MARS-Bench, a benchmark specifically designed to evaluate assumptive reasoning capabilities in MLLMs, revealing that current models struggle significantly with questions requiring systematic analysis of implicit assumptions. To address this, they propose Active Deduction (AD), a reinforcement learning framework that enables models to dynamically adjust reasoning depth based on question complexity. The approach demonstrates substantial improvements on assumptive reasoning tasks while maintaining performance on general-purpose question answering.

## Method Summary
The authors propose a two-stage training approach: Active Deduction Supervised Fine-Tuning (AD-SFT) followed by Active Deduction Reinforcement Fine-Tuning (AD-RFT). AD-SFT uses structured annotations with problem complexity labels to teach the model appropriate reasoning strategies, while AD-RFT employs Group Relative Policy Optimization (GRPO) with composite rewards to refine the reasoning process. The framework uses a divide-and-conquer strategy, generating direct answers for simple questions and engaging in multi-step reasoning via specific tags for complex ones. The method is evaluated on Qwen2.5-VL models and tested across multiple benchmarks including the newly introduced MARS-Bench.

## Key Results
- QwenAD-SFT-RFT achieves 495.2 accuracy on assumptive questions, a 76.6-point improvement over baseline
- Models maintain general-purpose question-answering performance while improving assumptive reasoning
- Qwen2.5-VL-7B-AD-SFT-RFT outperforms baseline by 2.1 points on general VQA tasks
- AD successfully reduces over-reliance on empirical reasoning and improves systematic analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AD dynamically adjusts reasoning depth based on question difficulty
- Mechanism: The framework uses difficulty estimation to decide whether to generate direct answers or engage in multi-step reasoning via specific tags
- Core assumption: Question complexity can be reliably classified into simple vs complex categories
- Evidence anchors: [abstract] "we also propose a simple yet effective method, Active Deduction (AD), a novel reinforcement learning paradigm to encourage the model to actively perform composite deduction before reaching a final decision"; [section] "For simple questions, the model directly generates answers based on its empirical intuition, while for difficult ones, the model engages in compositional deduction before arriving at the final decision"
- Break condition: If difficulty estimation becomes unreliable, the model may over/under-reason on questions

### Mechanism 2
- Claim: GRPO-based RL enables relative policy optimization without explicit critic models
- Mechanism: Multiple candidate responses are generated and compared within groups, with rewards based on relative performance rather than absolute scores
- Core assumption: Relative comparisons between candidates provide sufficient signal for policy improvement
- Evidence anchors: [abstract] "a novel reinforcement learning paradigm to encourage the model to actively perform composite deduction"; [section] "Unlike conventional reinforcement learning methods such as PPO [32], that rely on an explicit critic model, AD-RFT compares multiple candidate responses within a sampled group"
- Break condition: If candidate diversity is low, relative comparisons may not provide meaningful gradients

### Mechanism 3
- Claim: Composite reward design balances semantic accuracy with format adherence
- Mechanism: Rewards combine semantic similarity (via GPT-4o-mini or Sentence Transformers) with structural correctness (presence/absence of specific tags)
- Core assumption: Both content correctness and reasoning format are necessary for effective assumptive reasoning
- Evidence anchors: [abstract] "significant improvements in assumptive reasoning abilities without compromising its general-purpose question-answering performance"; [section] "The format reward ùëüfmt enforces adherence to the Active Deduction paradigm by evaluating the structural correctness of the model's output"
- Break condition: If format rewards dominate semantics, the model may optimize structure over correctness

## Foundational Learning

- Concept: Reinforcement Learning with Group Relative Policy Optimization
  - Why needed here: Enables effective policy updates without explicit critic models, crucial for the multi-candidate evaluation in AD
  - Quick check question: What is the key difference between GRPO and standard PPO in terms of reward calculation?

- Concept: Multimodal Visual Question Answering (VQA)
  - Why needed here: MARS-Bench evaluates models on visual questions requiring both image understanding and reasoning, forming the core task domain
  - Quick check question: How does visual question answering differ from pure text-based question answering?

- Concept: Chain-of-Thought (CoT) Prompting
  - Why needed here: Provides the reasoning framework that AD extends by making reasoning depth adaptive rather than fixed
  - Quick check question: What is the primary benefit of using CoT prompting in complex reasoning tasks?

## Architecture Onboarding

- Component map: Base MLLM (Qwen2.5-VL) ‚Üí SFT stage ‚Üí RFT stage with GRPO ‚Üí Composite reward function
- Critical path: Question ‚Üí Difficulty estimation ‚Üí (Direct answer | Multi-step reasoning) ‚Üí Format compliance check ‚Üí Semantic accuracy evaluation ‚Üí Final response
- Design tradeoffs:
  - SFT vs RFT: SFT provides structured knowledge but may over-constrain, while RFT allows exploration but requires careful reward design
  - Format vs Semantic rewards: Need balance to avoid optimizing structure over correctness
  - Simple vs Complex question handling: Must prevent contamination between reasoning strategies
- Failure signatures:
  - Over-reliance on specific tags for simple questions (model over-thinks)
  - Under-generation of reasoning steps for complex questions (model under-thinks)
  - Format compliance without semantic correctness (reward hacking)
  - Performance degradation on non-assumptive tasks (over-specialization)
- First 3 experiments:
  1. Test difficulty classification accuracy on a held-out validation set of basic vs assumptive questions
  2. Evaluate format reward effectiveness by measuring tag usage patterns across question types
  3. Compare semantic accuracy rewards using GPT-4o-mini vs Sentence Transformers on a sample of model outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the AD format rewards specifically affect the model's reasoning depth for complex tasks compared to simpler tasks?
- Basis in paper: [explicit] The paper discusses the AD format reward system, distinguishing between hard and soft matching strategies for complex and simple tasks, respectively
- Why unresolved: While the paper mentions the reward system, it does not provide detailed experimental results or analysis on how these rewards influence the model's reasoning depth specifically for complex versus simple tasks
- What evidence would resolve it: Detailed experiments showing the model's reasoning depth variations for complex and simple tasks under different reward configurations would clarify the impact of AD format rewards

### Open Question 2
- Question: What are the long-term generalization capabilities of QwenAD models when applied to tasks beyond the MARS-Bench?
- Basis in paper: [inferred] The paper evaluates QwenAD models on MARS-Bench and some general benchmarks but does not extensively test their performance on a broader range of tasks
- Why unresolved: The paper focuses on specific benchmarks and does not provide evidence of how well the models generalize to other types of tasks or datasets
- What evidence would resolve it: Extensive testing of QwenAD models on diverse datasets and tasks, including those not related to assumptive reasoning, would demonstrate their generalization capabilities

### Open Question 3
- Question: How does the divide-and-conquer strategy in AD-SFT and AD-RFT affect the model's performance on tasks requiring both reasoning and general visual understanding?
- Basis in paper: [explicit] The paper describes a divide-and-conquer strategy in both AD-SFT and AD-RFT stages, but does not provide detailed analysis on how this strategy impacts tasks that require both reasoning and general visual understanding
- Why unresolved: The paper does not offer insights into how the strategy balances between reasoning and general visual understanding tasks
- What evidence would resolve it: Comparative studies showing the model's performance on tasks requiring both reasoning and general visual understanding before and after applying the divide-and-conquer strategy would clarify its impact

## Limitations

- MARS-Bench relies on automatically generated complex questions which may not fully capture real-world assumptive reasoning challenges
- Evaluation metrics depend heavily on automated reward functions that may not perfectly align with human judgment
- Two-stage training approach requires substantial curated data, limiting accessibility
- Effectiveness of divide-and-conquer strategy may not generalize to all MLLM architectures or reasoning domains

## Confidence

**High Confidence:** The core observation that MLLMs often fail on assumptive reasoning due to empirical biases is well-supported by MARS-Bench results. The performance improvements from Active Deduction on the benchmark itself are substantial and consistently demonstrated across multiple model sizes.

**Medium Confidence:** The claim that Active Deduction improves general-purpose reasoning while maintaining assumptive reasoning capabilities is supported but requires careful interpretation. The maintenance of performance on general VQA tasks is demonstrated, but the long-term stability of this balance needs further validation.

**Low Confidence:** The assertion that difficulty estimation reliably determines optimal reasoning depth is the weakest link in the framework. The paper doesn't provide detailed analysis of the classifier's accuracy or explore what happens when the difficulty estimation fails.

## Next Checks

1. **Difficulty Classification Validation**: Conduct a detailed error analysis of the difficulty classifier by manually labeling a subset of questions and measuring classification accuracy, then test model performance when difficulty estimation is intentionally perturbed.

2. **Cross-Domain Generalization**: Evaluate Active Deduction on non-visual assumptive reasoning tasks (such as text-only logical puzzles) to determine whether the approach generalizes beyond the multimodal domain where it was developed.

3. **Human Evaluation Benchmark**: Run a human study comparing model outputs with and without Active Deduction on assumptive reasoning tasks to validate that the automated metrics (semantic similarity, format compliance) actually correlate with human judgment of reasoning quality.