---
ver: rpa2
title: On the Adversarial Robustness of Graph Neural Networks with Graph Reduction
arxiv_id: '2412.05883'
source_url: https://arxiv.org/abs/2412.05883
tags:
- graph
- ratio
- reduction
- accuracy
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how graph reduction techniques\u2014specifically\
  \ graph sparsification and coarsening\u2014affect the robustness of Graph Neural\
  \ Networks (GNNs) against adversarial poisoning attacks. Through extensive experiments\
  \ across multiple datasets, GNN architectures, and reduction methods, the study\
  \ evaluates the interplay between graph reduction and adversarial robustness."
---

# On the Adversarial Robustness of Graph Neural Networks with Graph Reduction

## Quick Facts
- arXiv ID: 2412.05883
- Source URL: https://arxiv.org/abs/2412.05883
- Reference count: 40
- Key outcome: Graph sparsification can mitigate certain poisoning attacks by removing poisoned edges during training, but graph coarsening amplifies adversarial effects by merging poisoned edges into supernodes.

## Executive Summary
This paper investigates how graph reduction techniques—specifically graph sparsification and coarsening—affect the robustness of Graph Neural Networks (GNNs) against adversarial poisoning attacks. Through extensive experiments across multiple datasets, GNN architectures, and reduction methods, the study evaluates the interplay between graph reduction and adversarial robustness. Key findings show that graph sparsification can mitigate certain poisoning attacks (e.g., Mettack) by removing poisoned edges during training, but is less effective against others like PGD, which also operate during inference. In contrast, graph coarsening amplifies adversarial effects by merging poisoned edges into supernodes, degrading model performance and undermining even robust GNN defenses. Defensive GNNs combined with sparsification retain or improve robustness, while coarsening disrupts these defenses. The study provides practical insights for designing scalable and robust GNN systems, highlighting sparsification—especially the Local Degree method—as more effective for security-critical applications.

## Method Summary
The study evaluates the impact of graph reduction on GNN robustness through systematic experimentation. It uses three datasets (Cora, Pubmed, CS) with node classification tasks and applies seven poisoning attacks (DICE, NEA, PGD, Mettack, PRBCD, GraD, STRG-Heuristic) at a 5% perturbation ratio. Four sparsification methods (RNE, LD, LS, SCAN) and six coarsening methods (VN, VE, VC, HE, JC, Kron) are tested across reduction ratios from 0.1 to 1.0. The experiments measure classification accuracy (clean accuracy ACCc, poisoned accuracy ACCp, post-reduction accuracy ACCr) and analyze edge removal/merge ratios and feature/label differences. Defensive GNNs (RGCN, GNNGuard, MedianGCN) are evaluated with and without graph reduction to assess their effectiveness against adversarial attacks.

## Key Results
- Graph sparsification effectively mitigates Mettack poisoning by removing poisoned edges during training
- Graph coarsening amplifies adversarial impact by merging dissimilar nodes into supernodes with noisy representations
- Defensive GNNs combined with sparsification retain or improve robustness, while coarsening disrupts these defenses
- Local Degree sparsification shows the best balance between accuracy preservation and adversarial robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph sparsification mitigates certain poisoning attacks by removing poisoned edges during training.
- Mechanism: Graph sparsification methods like Local Degree and Scan select and retain a subset of edges, effectively filtering out edges added by adversarial attacks during the training phase.
- Core assumption: The adversarial edges are less likely to be selected by the sparsification algorithm compared to benign edges.
- Evidence anchors:
  - [abstract]: "Graph sparsification can mitigate the effectiveness of certain poisoning attacks, such as Mettack, it has limited impact on others, like PGD."
  - [section]: "Graph sparsification effectively removes added poisoned edges during training, significantly mitigating the impact of poisoning attacks like Mettack [39]."
  - [corpus]: No direct evidence found in corpus; this is an inference from the paper's findings.
- Break condition: If the sparsification algorithm selects edges randomly or based on criteria that do not discriminate against poisoned edges, the mitigation effect will be reduced.

### Mechanism 2
- Claim: Graph coarsening amplifies adversarial impact by merging poisoned edges into supernodes, creating noisy representations.
- Mechanism: Graph coarsening algorithms partition the graph into clusters and merge nodes into supernodes. If poisoned edges connect dissimilar nodes, merging them into a supernode combines their features and labels, creating a noisy representation with high feature variance and incorrect labels.
- Core assumption: The poisoned edges connect nodes that are dissimilar in terms of features and labels, and the coarsening algorithm merges these dissimilar nodes into the same supernode.
- Evidence anchors:
  - [abstract]: "graph coarsening tends to amplify the adversarial impact, significantly reducing classification accuracy as the reduction ratio decreases."
  - [section]: "Coarsening merges dissimilar nodes connected by poisoned edges into supernodes, creating noisy representations with high feature variance and incorrect labels."
  - [corpus]: No direct evidence found in corpus; this is an inference from the paper's findings.
- Break condition: If the coarsening algorithm creates clusters that group similar nodes together, or if the poisoned edges connect similar nodes, the amplification effect will be reduced.

### Mechanism 3
- Claim: Defensive GNNs combined with graph sparsification retain or improve robustness, while coarsening disrupts these defenses.
- Mechanism: Defensive GNNs like GNNGuard improve robustness by identifying and eliminating suspicious edges. When combined with graph sparsification, the sparsification process removes many poisoned edges, making it easier for the defensive GNN to identify and eliminate the remaining suspicious edges. In contrast, graph coarsening merges poisoned edges into supernodes, making it difficult for the defensive GNN to detect and mitigate the adversarial effects.
- Core assumption: The defensive GNN relies on edge-level information to identify and eliminate suspicious edges, and this information is preserved or enhanced by graph sparsification but lost or obscured by graph coarsening.
- Evidence anchors:
  - [abstract]: "defensive GNNs combined with sparsification retain or improve robustness, while coarsening disrupts these defenses."
  - [section]: "When combined with graph sparsification, defensive GNNs retain or even improve their defense capabilities, providing strong protection against poisoning attacks. In contrast, coarsening disrupts these defenses by transferring edge perturbations into supernode structures, rendering robust GNN models less effective."
  - [corpus]: No direct evidence found in corpus; this is an inference from the paper's findings.
- Break condition: If the defensive GNN relies on node-level or graph-level information rather than edge-level information, or if the coarsening algorithm preserves edge-level information in the supernodes, the disruption effect will be reduced.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their vulnerability to adversarial attacks.
  - Why needed here: Understanding GNNs and their vulnerabilities is crucial for comprehending how graph reduction techniques affect their robustness.
  - Quick check question: What are the primary types of adversarial attacks on GNNs, and how do they exploit the graph structure?

- Concept: Graph reduction techniques: sparsification and coarsening.
  - Why needed here: Understanding the mechanisms of graph reduction is essential for analyzing their impact on GNN robustness.
  - Quick check question: How do graph sparsification and coarsening algorithms work, and what are their goals in terms of graph size and structure?

- Concept: Adversarial attack strategies: poisoning vs. evasion attacks.
  - Why needed here: Distinguishing between poisoning and evasion attacks is important for understanding why graph reduction affects them differently.
  - Quick check question: What is the difference between poisoning and evasion attacks, and how does this difference influence the effectiveness of graph reduction techniques?

## Architecture Onboarding

- Component map:
  - Datasets: Cora -> Pubmed -> CS
  - GNN architectures: GCN -> GraphSAGE -> GAT
  - Defensive GNNs: RGCN -> GNNGuard -> MedianGCN
  - Graph reduction methods: VN -> VE -> VC -> HE -> JC -> KRON (coarsening); RNE -> LD -> LS -> SCAN (sparsification)
  - Adversarial attacks: DICE -> NEA -> PGD -> Mettack -> PRBCD -> GraD -> STRG-Heuristic
  - Evaluation metrics: Clean accuracy -> Poisoned accuracy -> Post-reduction accuracy

- Critical path:
  1. Load dataset and create clean graph.
  2. Train surrogate GNN model on clean graph.
  3. Apply adversarial attack to create poisoned graph.
  4. Apply graph reduction technique to poisoned graph.
  5. Train GNN model on reduced poisoned graph.
  6. Evaluate model accuracy on test set.

- Design tradeoffs:
  - Graph reduction ratio: Higher reduction ratios lead to smaller graphs but may also reduce model accuracy and robustness.
  - Choice of graph reduction method: Different methods have different effects on model accuracy and robustness.
  - Choice of defensive GNN: Some defensive GNNs are more effective than others, especially when combined with graph reduction techniques.

- Failure signatures:
  - Sharp decrease in accuracy after graph coarsening, especially for attacks like Mettack.
  - Limited effectiveness of graph sparsification against evasion attacks like PGD.
  - Disruption of defensive GNNs by graph coarsening.

- First 3 experiments:
  1. Evaluate the impact of graph sparsification on the Mettack attack using the Cora dataset and the Local Degree method.
  2. Evaluate the impact of graph coarsening on the PGD attack using the CS dataset and the Kron method.
  3. Compare the performance of GNNGuard with and without graph reduction on the Pubmed dataset under the Mettack attack.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do hybrid graph reduction approaches—combining both sparsification and coarsening—impact adversarial robustness compared to using either method alone?
- Basis in paper: [inferred] The paper evaluates sparsification and coarsening separately but does not explore combined or sequential use of both methods.
- Why unresolved: The individual effects of each method are well characterized, but the interaction between them and their joint impact on robustness is unexamined.
- What evidence would resolve it: Experiments comparing GNN performance under hybrid reduction pipelines against separate use of each method, across multiple attack types and datasets.

### Open Question 2
- Question: Can new graph reduction methods be designed that explicitly preserve adversarial robustness while maintaining computational efficiency?
- Basis in paper: [inferred] The study identifies that existing reduction methods can degrade robustness, especially coarsening, but does not propose or evaluate robustness-aware reduction algorithms.
- Why unresolved: Current reduction methods prioritize efficiency or structural similarity without considering adversarial effects, leaving a gap for robustness-focused designs.
- What evidence would resolve it: Development and empirical testing of reduction algorithms that incorporate robustness metrics (e.g., edge perturbation sensitivity) into their selection criteria.

### Open Question 3
- Question: How does the reduction ratio threshold for maintaining robustness vary across different graph sizes, domains, or attack strategies?
- Basis in paper: [explicit] The paper notes a significant drop in accuracy when the reduction ratio falls below 0.3, but this observation is based on limited datasets and attacks.
- Why unresolved: The threshold may depend on dataset characteristics, graph structure, or attack nature, which were not fully explored.
- What evidence would resolve it: Systematic analysis of robustness across diverse graph types, sizes, and attack configurations to determine context-dependent reduction ratio limits.

## Limitations
- Focus on node-level graph reduction may not capture more complex attack scenarios
- Fixed 5% perturbation ratio for all attacks limits generalizability to real-world scenarios
- Does not explore temporal dynamics of adversarial attacks or impact on large-scale graphs

## Confidence

**High confidence**: The findings on sparsification mitigating Mettack and coarsening amplifying adversarial effects are well-supported by extensive experimental results across multiple datasets and attack types.

**Medium confidence**: The effectiveness of defensive GNNs combined with sparsification requires further validation, particularly regarding the mechanisms through which sparsification enhances defensive capabilities.

**Low confidence**: The paper's claims about the superiority of Local Degree sparsification over other methods need additional verification through ablation studies and comparisons with more recent sparsification techniques.

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of graph reduction and defensive GNNs to overall robustness.
2. Test the findings on larger-scale graphs and with varying perturbation ratios to assess scalability and generalization.
3. Implement additional sparsification methods and defensive GNNs not covered in the original study to validate the robustness of the reported conclusions.