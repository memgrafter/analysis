---
ver: rpa2
title: 'Aurora-M: Open Source Continual Pre-training for Multilingual Language and
  Code'
arxiv_id: '2404.00399'
source_url: https://arxiv.org/abs/2404.00399
tags:
- arxiv
- language
- preprint
- training
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AURORA-M is a 15B-parameter multilingual LLM continually pretrained
  from StarCoderPlus on 435B tokens across English, Finnish, Hindi, Japanese, Vietnamese,
  and code. It was the first open-source multilingual model fine-tuned on human-reviewed
  safety instructions aligned with the Biden-Harris Executive Order.
---

# Aurora-M: Open Source Continual Pre-training for Multilingual Language and Code

## Quick Facts
- arXiv ID: 2404.00399
- Source URL: https://arxiv.org/abs/2404.00399
- Reference count: 40
- First open-source multilingual model fine-tuned on human-reviewed safety instructions aligned with Biden-Harris Executive Order

## Executive Summary
AURORA-M is a 15B-parameter multilingual language model continually pretrained from StarCoderPlus on 435B additional tokens across English, Finnish, Hindi, Japanese, Vietnamese, and code. The model employs a two-stage curriculum training approach (CAP + CAT) to reduce catastrophic forgetting while adapting to multilingual and code tasks. It achieves competitive performance on English and code tasks while significantly improving on multilingual benchmarks, and demonstrates reduced harmful content generation through safety instruction fine-tuning aligned with the Biden-Harris Executive Order.

## Method Summary
AURORA-M extends the 15B-parameter StarCoderPlus model through a two-stage curriculum training process. The first stage (CAP) exposes the model to 377B tokens of general multilingual web data to build cross-lingual representations. The second stage (CAT) fine-tunes with 58B tokens of specialized high-quality data including safety instructions, achieving targeted performance improvements while maintaining code capabilities. The model was trained for 48 days on 128 AMD MI250X GPUs using 4-way tensor and pipeline parallelism, achieving a total training token count exceeding 2 trillion tokens.

## Key Results
- Competitive performance on English and code tasks while significantly improving on multilingual benchmarks
- Reduced harmful content generation with lower attack success rates and higher CARP scores post-red-teaming
- Positive scaling trends across all evaluated languages and code tasks with increased training tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage curriculum training (CAP + CAT) reduces catastrophic forgetting while adapting to multilingual and code tasks.
- Mechanism: CAP stage exposes model to broad multilingual data to build foundational cross-lingual representations; CAT stage fine-tunes with targeted high-quality data and safety instructions to specialize without overwriting earlier knowledge.
- Core assumption: The order of pretraining stages matters; starting broad before specializing prevents overwriting prior representations.
- Evidence anchors: [abstract] "Continually pretrained from StarCoderPlus on 435B additional tokens, Aurora-M surpasses 2T tokens in total training token count." [section 2] "In the first stage, termed as Continual Auxiliary Pretraining (CAP), a large corpus of general multilingual web data was used to expose the model to diverse data, laying a robust foundation for subsequent training."

### Mechanism 2
- Claim: Human-reviewed safety instruction fine-tuning aligns the model with Biden-Harris Executive Order concerns and reduces harmful content generation.
- Mechanism: Fine-tuning on curated safety instruction dataset teaches the model refusal and safe response generation patterns while preserving helpfulness.
- Core assumption: Safety alignment generalizes across languages even when safety instructions are in English.
- Evidence anchors: [abstract] "It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventional red-teaming considerations, but also with the specific concerns articulated in the Biden-Harris Executive Order." [section 5.2] "we can immediately appreciate the considerably lower harmfulness both on the existing benchmarks and on our own Biden-Harris red-team test set as evident by the CARP scores obtained by our red-teamed Aurora-M."

### Mechanism 3
- Claim: Scaling training tokens improves performance across all languages and code tasks.
- Mechanism: More training tokens provide richer context and better generalization for both language-specific and code-specific patterns.
- Core assumption: Token count scales linearly with model capability improvements up to a saturation point.
- Evidence anchors: [abstract] "We show the influence of scaling the total training tokens on various multilingual and code evaluation tasks." [section 5.3] "Figure 5 and 6 show the relationship between the number of training tokens and the performance of the various models."

## Foundational Learning

- Concept: Catastrophic forgetting in continual pretraining
  - Why needed here: Aurora-M extends a code-focused model to multilingual tasks; without proper mitigation, it would lose code generation capabilities.
  - Quick check question: What happens to performance on HumanEval if catastrophic forgetting occurs during multilingual pretraining?

- Concept: Curriculum learning through staged data exposure
  - Why needed here: CAP stage builds broad multilingual understanding before CAT stage specializes; this order prevents overwriting prior knowledge.
  - Quick check question: Why would reversing CAP and CAT stages likely harm model performance?

- Concept: Cross-lingual generalization from multilingual pretraining
  - Why needed here: Aurora-M improves on low-resource languages by learning shared linguistic patterns across multiple languages.
  - Quick check question: How does pretraining on 5 diverse languages help improve performance on a 6th language not seen during training?

## Architecture Onboarding

- Component map: StarCoderPlus (15B) -> CAP (377B tokens multilingual) -> CAT (58B tokens specialized + safety) -> 128 AMD MI250X GPUs with 4-way tensor and pipeline parallelism

- Critical path: 1) Load StarCoderPlus checkpoint 2) Stage 1: CAP pretraining on multilingual web data 3) Stage 2: CAT pretraining on specialized datasets including safety instructions 4) Evaluation across English, Finnish, Hindi, Japanese, Vietnamese, and code tasks 5) Safety evaluation using Biden-Harris Redteam Testset

- Design tradeoffs:
  - Token count vs. quality: Larger CAP dataset risks lower quality; solved by filtering and subsampling
  - Safety alignment vs. helpfulness: Safety instructions may reduce helpfulness; balanced by instruction-tuning approach
  - Multilingual coverage vs. depth: Including many languages vs. focusing on few; solved by targeted language selection

- Failure signatures:
  - Catastrophic forgetting: Performance drop on HumanEval/MBPP while multilingual tasks improve
  - Overfitting to safety instructions: Model becomes overly cautious and unhelpful
  - Poor cross-lingual generalization: Safety improvements only work in English, not other languages

- First 3 experiments:
  1. Compare pass@1 rates on HumanEval before/after CAP stage to detect catastrophic forgetting
  2. Run CARP evaluation on Biden-Harris Redteam Testset for base vs. red-teamed model
  3. Plot performance vs. training tokens for one multilingual task (e.g., Japanese) to verify scaling relationship

## Open Questions the Paper Calls Out

- What is the optimal token count for balancing performance gains across all six languages (EN, FI, HI, JA, VI, code) without incurring diminishing returns?
- How does AURORA-M's safety performance compare to monolingual models when evaluated on region-specific cultural contexts and legal frameworks beyond the US-centric Biden-Harris standards?
- What are the long-term effects of continual pretraining on code generation capabilities when the model is primarily exposed to multilingual natural language data?

## Limitations

- Safety alignment claims rely on a single curated dataset rather than comprehensive adversarial testing across multiple attack vectors
- Multilingual scaling analysis lacks statistical rigor and confidence intervals for reported improvements
- Biden-Harris Executive Order alignment is asserted but not independently verified across diverse cultural/legal contexts

## Confidence

- High Confidence: Architectural description and training methodology with specific technical details about two-stage curriculum, hardware configuration, and optimization parameters
- Medium Confidence: Performance improvements on multilingual and code benchmarks, though lacking statistical analysis and baseline comparisons
- Low Confidence: Safety alignment claims and cross-lingual generalization benefits supported by limited evidence and single curated testset

## Next Checks

1. Conduct comprehensive adversarial safety testing using multiple attack methodologies to verify safety improvements generalize beyond the Biden-Harris curated testset
2. Perform statistical testing on performance scaling trends with training tokens, including confidence intervals and significance tests across multiple evaluation runs
3. Test safety alignment transfer by evaluating CARP scores on non-English languages using English-language safety instructions, and compare against models fine-tuned with multilingual safety instructions