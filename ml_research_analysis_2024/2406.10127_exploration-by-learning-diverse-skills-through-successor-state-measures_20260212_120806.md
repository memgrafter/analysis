---
ver: rpa2
title: Exploration by Learning Diverse Skills through Successor State Measures
arxiv_id: '2406.10127'
source_url: https://arxiv.org/abs/2406.10127
tags:
- state
- skill
- skills
- exploration
- leads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LEADS introduces a method for unsupervised exploration by learning
  diverse skills through successor state measures. The key innovation is replacing
  mutual information maximization with a new objective that explicitly promotes both
  skill diversity and state space coverage.
---

# Exploration by Learning Diverse Skills through Successor State Measures

## Quick Facts
- arXiv ID: 2406.10127
- Source URL: https://arxiv.org/abs/2406.10127
- Authors: Paul-Antoine Le Tolguenec; Yann Besse; Florent Teichteil-Konigsbuch; Dennis G. Wilson; Emmanuel Rachelson
- Reference count: 40
- Primary result: LEADS outperforms state-of-the-art exploration algorithms on maze navigation and robotic control tasks, achieving superior coverage in nearly all environments (e.g., 88.6% coverage in Fetch-Reach vs 46.8% for NGU)

## Executive Summary
LEADS introduces a method for unsupervised exploration by learning diverse skills through successor state measures. The key innovation is replacing mutual information maximization with a new objective that explicitly promotes both skill diversity and state space coverage. This is achieved by combining an exploration term (novelty via state visitation history) with a repulsion term (distinguishing skill distributions) in an uncertainty measure. The method estimates successor state measures using C-Learning and iteratively focuses each skill on promising under-visited states.

## Method Summary
LEADS operates by learning diverse skills through an objective that combines exploration (focusing on under-visited states) and skill diversity (ensuring skills visit distinct regions). The method uses C-Learning to estimate successor state measures, which encode the probability of reaching certain states when using a particular skill. An uncertainty measure is defined to rank states based on novelty and skill distinction, which is then used to guide each skill toward promising under-visited areas. The policy parameters are updated iteratively using stochastic gradient ascent on a modified mutual information objective that includes both exploration and repulsion terms.

## Key Results
- Achieves 88.6% coverage in Fetch-Reach environment compared to 46.8% for NGU
- Successfully creates distinct, exploratory skills even in high-dimensional state spaces
- Outperforms state-of-the-art exploration algorithms (NGU, RND, DIAYN, SMM, LSD, CSD, METRA) on maze navigation and robotic control tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LEADS promotes exploration by estimating successor state measures and directing skills toward under-visited states.
- Mechanism: LEADS uses C-Learning to estimate the successor state measure m(s1, s2, z), which encodes the probability of reaching state s2 from s1 when using skill z. The algorithm then defines an uncertainty measure ut(s, z) that ranks states based on how novel they are (low visitation frequency) and how distinct they are from other skills' distributions. This uncertainty measure is used to define a distribution δ(s|z) that focuses each skill on promising under-visited states.
- Core assumption: The successor state measure can be accurately estimated using C-Learning and generalizes well to high-dimensional state spaces.
- Evidence anchors:
  - [abstract]: "The method estimates successor state measures using C-Learning and iteratively focuses each skill on promising under-visited states."
  - [section 3.2]: "We define three desired properties for states to prioritize using this measure... (1) A good target state sz_t for skill z at time step t is one that has high probability of being visited by πt(·, z), but was relatively infrequently visited by previous policies' state occupancy measures {mk}k∈[1,t−1] for any skill."
  - [corpus]: Weak evidence. The corpus mentions "Causal Information Prioritization for Efficient Reinforcement Learning" which is related to prioritization but does not directly support the accuracy or generalization of C-Learning for SSM estimation.
- Break condition: If the successor state measure cannot be accurately estimated (e.g., due to high-dimensional state spaces or complex dynamics), the uncertainty measure ut(s, z) will be unreliable, leading to poor exploration and skill diversity.

### Mechanism 2
- Claim: LEADS creates diverse skills by maximizing a modified mutual information objective that explicitly encourages skill distinction.
- Mechanism: Instead of maximizing the standard mutual information I(S, Z), LEADS maximizes a modified objective G(θ) that includes an exploration term (novelty via state visitation history) and a repulsion term (distinguishing skill distributions). This is achieved by replacing the sampling distribution p(s|z) with δ(s|z) in the mutual information lower bound.
- Core assumption: Maximizing the modified objective G(θ) effectively promotes both exploration and skill diversity.
- Evidence anchors:
  - [abstract]: "The key innovation is replacing mutual information maximization with a new objective that explicitly promotes both skill diversity and state space coverage."
  - [section 3.2]: "We therefore argue that exploratory behaviors can be obtained by focusing the state distribution of each skill towards specific states within the support of p(s|z). Instead of sampling s according to p(s|z), we encourage exploration by attributing more probability mass to states that trigger exploration."
  - [corpus]: Weak evidence. The corpus mentions "Decoupling Exploration and Exploitation for Unsupervised Pre-training with Successor Features" which is related to exploration but does not directly support the effectiveness of the modified mutual information objective.
- Break condition: If the modified objective G(θ) does not effectively balance exploration and skill diversity (e.g., if the exploration term dominates or the repulsion term is insufficient), the resulting skills may not cover the state space effectively or may not be sufficiently distinct.

### Mechanism 3
- Claim: LEADS iteratively improves skills by updating the policy parameters θ to maximize G(θ) using stochastic gradient ascent.
- Mechanism: LEADS collects samples for each skill, learns the successor state measure, defines a target state for each skill using the uncertainty measure, and then updates the policy parameters θ to maximize G(θ) + λh H(θ) (where H(θ) is an action entropy term). This process is repeated iteratively to improve the skills over time.
- Core assumption: Stochastic gradient ascent on the modified objective G(θ) effectively updates the policy parameters to improve exploration and skill diversity.
- Evidence anchors:
  - [section 3.3]: "θ is therefore updated to maximize G(θ) + λhH(θ), although λh is intentionally kept small (0.05) to focus on the principal LEADS objective G(θ)."
  - [section 4.2]: "The state maximizing ut is represented by a colored dot for each skill. This confirms that the uncertainty measure achieves the joint goal of exploring under-visited areas and creating repulsion between skills."
  - [corpus]: Weak evidence. The corpus mentions "Agentic Skill Discovery" which is related to skill discovery but does not directly support the effectiveness of the iterative update process.
- Break condition: If the stochastic gradient ascent process does not effectively update the policy parameters (e.g., due to poor gradient estimates or unstable learning dynamics), the skills may not improve over time or may converge to suboptimal solutions.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: LEADS operates within the MDP framework, where the agent learns policies to maximize reward in an environment with states, actions, transitions, and rewards.
  - Quick check question: What are the key components of an MDP and how do they relate to the LEADS algorithm?

- Concept: Successor State Measures (SSMs)
  - Why needed here: LEADS uses SSMs to estimate the probability of reaching certain states when using a particular skill, which is crucial for defining the uncertainty measure and guiding exploration.
  - Quick check question: How is the successor state measure defined and how does it differ from the traditional occupancy measure?

- Concept: Mutual Information
  - Why needed here: LEADS builds upon the concept of mutual information to formalize the notion of skill diversity, although it modifies the objective to better promote exploration.
  - Quick check question: How is mutual information defined and why might maximizing it be insufficient for promoting exploration?

## Architecture Onboarding

- Component map: Policy network -> C-Learning network -> Uncertainty measure -> Replay buffers -> Optimization loop
- Critical path:
  1. Collect samples for each skill by rolling out episodes with the current policy.
  2. Learn the successor state measure using C-Learning on the collected samples.
  3. Define a target state for each skill using the uncertainty measure.
  4. Update the policy parameters using stochastic gradient ascent on the modified mutual information objective.
  5. Repeat steps 1-4 iteratively to improve the skills over time.
- Design tradeoffs:
  - Skill diversity vs. state space coverage: LEADS aims to balance these two objectives, but may prioritize one over the other depending on the specific task and hyperparameters.
  - Exploration vs. exploitation: LEADS encourages exploration by focusing on under-visited states, but may also exploit known good states to improve skill performance.
  - Sample efficiency vs. computational cost: LEADS requires collecting samples for each skill and learning the successor state measure, which can be computationally expensive but may lead to more efficient learning in the long run.
- Failure signatures:
  - Poor exploration: If the skills do not cover the state space effectively, it may indicate that the uncertainty measure is not accurately estimating novelty or that the exploration term in the objective is insufficient.
  - Lack of skill diversity: If the skills are not sufficiently distinct, it may indicate that the repulsion term in the objective is insufficient or that the successor state measure is not accurately estimating skill distributions.
  - Unstable learning: If the policy parameters are not converging or are oscillating, it may indicate that the stochastic gradient ascent process is unstable or that the gradients are poorly estimated.
- First 3 experiments:
  1. Implement a simple grid world environment and test LEADS with a small number of skills to verify that it can learn diverse and exploratory behaviors.
  2. Compare LEADS to a baseline method (e.g., DIAYN) on a more complex environment (e.g., a maze navigation task) to evaluate its performance in terms of state space coverage and skill diversity.
  3. Analyze the successor state measure estimates and the uncertainty measure to understand how LEADS is guiding exploration and skill discovery.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LEADS be extended to use more advanced SSM estimators like those in Eysenbach et al. (2022) or Zheng et al. (2023) to improve performance in high-dimensional environments?
- Basis in paper: [explicit] The paper notes that LEADS relies intrinsically on good SSM estimators and mentions that recent methodologies demonstrate improved generalization capabilities in high-dimensional spaces.
- Why unresolved: The authors explicitly reserve this extension for future work, acknowledging that investigating why C-Learning performs poorly in certain MuJoCo environments is beyond the scope of this paper.
- What evidence would resolve it: Implementing LEADS with alternative SSM estimators and empirically comparing performance on high-dimensional tasks like HalfCheetah would provide evidence of whether these estimators resolve the limitations.

### Open Question 2
- Question: What is the theoretical relationship between the proposed objective G(θ) and the true mutual information I(S,Z), and under what conditions does maximizing G(θ) lead to near-optimal state coverage?
- Basis in paper: [inferred] The paper introduces G(θ) as a new objective that promotes both diversity and exploration, but acknowledges it loses the theoretical guarantee of maximizing the lower bound of Equation 5.
- Why unresolved: While the paper argues G(θ) promotes desired behaviors, it doesn't provide theoretical analysis of its relationship to I(S,Z) or conditions for optimal coverage.
- What evidence would resolve it: Theoretical analysis proving bounds on G(θ) relative to I(S,Z), or empirical studies showing conditions under which G(θ) maximization leads to optimal coverage.

### Open Question 3
- Question: How does the choice of uncertainty measure ut(s,z) affect exploration efficiency, and are there alternative formulations that could improve performance?
- Basis in paper: [explicit] The paper proposes a specific uncertainty measure with three properties (explore under-visited areas, repulsion between skills, and continuity from previous target states) but notes it found deterministic δ(s|z) effective.
- Why unresolved: The paper doesn't explore alternative uncertainty measures or provide systematic comparison of different formulations.
- What evidence would resolve it: Empirical comparison of different uncertainty measure formulations on benchmark tasks, or theoretical analysis of how different components of ut(s,z) contribute to exploration efficiency.

## Limitations
- Limited evaluation to relatively simple environments without testing on high-dimensional continuous control tasks
- Successor state measure estimation using C-Learning may face scalability challenges in truly high-dimensional state spaces
- Method requires maintaining separate replay buffers and training multiple policies per skill, increasing computational complexity

## Confidence
- High confidence in the mechanism: The combination of exploration (novelty via state visitation) and repulsion (skill distribution separation) in the uncertainty measure is well-motivated and the theoretical foundation connecting SSMs to exploration is sound.
- Medium confidence in empirical claims: While state coverage results are strong across multiple environments, the evaluation lacks comparison to more recent methods and does not include high-dimensional benchmarks that would stress-test the SSM estimation approach.
- Low confidence in generalization: The paper provides limited analysis of how hyperparameters affect performance, and the method's behavior in truly sparse-reward or high-dimensional scenarios remains untested.

## Next Checks
1. Implement LEADS on a high-dimensional continuous control benchmark (HalfCheetah or Ant) to evaluate whether C-Learning can accurately estimate SSMs in state spaces with 10+ dimensions and whether the exploration objective scales effectively.

2. Conduct an ablation study comparing LEADS with and without the repulsion term (the skill distinction component) to quantify the contribution of each component to overall performance and skill diversity.

3. Add quantitative skill diversity metrics (such as maximum mean discrepancy between skill state distributions) to complement the coverage-based evaluation and provide more rigorous assessment of skill distinction.