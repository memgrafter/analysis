---
ver: rpa2
title: Training LLMs over Neurally Compressed Text
arxiv_id: '2404.03626'
source_url: https://arxiv.org/abs/2404.03626
tags:
- compression
- text
- equalinfoac
- tokens
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the possibility of training large language
  models (LLMs) directly over neurally compressed text. The authors propose Equal-Info
  Windows, a novel compression technique that segments text into blocks of equal information
  content, enabling effective learning over compressed text that improves with model
  scale.
---

# Training LLMs over Neurally Compressed Text

## Quick Facts
- arXiv ID: 2404.03626
- Source URL: https://arxiv.org/abs/2404.03626
- Reference count: 0
- Primary result: Achieves 0.94 bits/byte with 16-bit tokens, outperforming byte-level baselines and approaching SentencePiece performance

## Executive Summary
This paper explores training large language models directly over neurally compressed text, demonstrating that strong compression can be learnable by LLMs when properly structured. The authors propose Equal-Info Windows, a novel technique that segments text into blocks of equal information content, enabling effective learning over compressed text that improves with model scale. Their best-performing model using Equal-Info Windows with 16-bit tokens and a vocabulary of 65k achieves 0.94 bits/byte, outperforming byte-level baselines and approaching the performance of SentencePiece tokenizers while requiring fewer autoregressive generation steps.

## Method Summary
The method involves training a small byte-level language model (M1) to compress text using Arithmetic Coding with Equal-Info Windows, where text is segmented into contiguous windows that each compress to the same bit length with context reset at each boundary. The compressed bitstream is then chunked into tokens (8-bit or 16-bit) and used to train larger language models (M2). The approach enables learning over compressed text by breaking compression into independent, fixed-bit chunks that prevent the difficulty of tracking long-range AC state variables across long distances.

## Key Results
- EqualInfoAC[b=16, v=65k] achieves 0.94 bits/byte, outperforming byte-level baselines and approaching SentencePiece performance
- Performance improves with model scale, with larger M2 models showing diminishing gaps to SentencePiece baselines
- 16-bit tokens (65k vocabulary) provide better performance than 8-bit tokens (256 vocabulary) by reducing token-level instability
- Equal-Info Windows with context resetting at fixed bit boundaries enable learnability where naive Arithmetic Coding fails

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Equal-Info Windows enable learnability by breaking compression into independent, fixed-bit chunks that reset model context.
- Mechanism: Text is segmented into contiguous windows that each compress to the same bit length. At each window boundary, both the Arithmetic Coding algorithm and the M1 model context are reset. This ensures each window can be decoded independently without requiring the model to track AC state variables across long distances.
- Core assumption: Resetting context at fixed bit boundaries preserves enough local context for M2 to learn while preventing the difficulty of tracking long-range AC state.
- Evidence anchors: [abstract] "we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length." [section 3.3] "Rather than splitting text into windows of equal text length, we track the number of bits output by the compressor, and close each window just before it exceeds a set information threshold"
- Break condition: If windows become too long (e.g., 128 bits), learnability degrades as the model struggles to track AC decompression over extended sequences.

### Mechanism 2
- Claim: Larger vocabularies (16-bit tokens) improve performance by reducing token-level instability and increasing compression efficiency.
- Mechanism: Using 16-bit tokens (65k vocabulary) creates a one-to-one mapping between Equal-Info Windows and M2 tokens, eliminating the instability that occurs when multiple tokens per window have context-dependent meanings. This stability, combined with higher token compression ratio, enables better learning.
- Core assumption: The improved stability of token→text mappings outweighs any potential challenges from larger vocabulary size.
- Evidence anchors: [abstract] "Our best-performing model using Equal-Info Windows with 16-bit tokens and a vocabulary of 65k achieves 0.94 bits/byte" [section 5.3] "EqualInfoAC[b=16, v=65k] outperforms the SentencePiece baseline, but the gap diminishes with scale"
- Break condition: If vocabulary becomes too large (e.g., 32-bit tokens), computational bottlenecks in the final softmax layer may negate performance gains.

### Mechanism 3
- Claim: Neural compression removes low-level patterns (spelling, word frequency) while retaining higher-level structure, allowing larger M2 to learn efficiently.
- Mechanism: M1 compresses text using an autoregressive model that removes predictable patterns. The resulting compressed text contains higher-level structure that M2 (with larger capacity than M1) can learn without relearning basic language patterns already captured by M1.
- Core assumption: M2 has sufficient capacity to extract higher-level patterns from compressed text that M1 cannot fully model.
- Evidence anchors: [abstract] "we find that text naively compressed via Arithmetic Coding is not readily learnable by LLMs" but "using this method, we demonstrate effective learning over neurally compressed text" [section 4] "In theory, this process could be repeated by training an even-larger M3 model on text compressed by M2, and so on"
- Break condition: If M1 is too strong or M2 too weak, the compressed output becomes indistinguishable from random noise, preventing learning.

## Foundational Learning

- Concept: Arithmetic Coding compression algorithm
  - Why needed here: Understanding AC is critical because the learnability challenge stems from tracking AC state variables across compressed sequences
  - Quick check question: How does Arithmetic Coding use cumulative probabilities to partition intervals during compression?

- Concept: Subword tokenization vs byte-level processing
  - Why needed here: The paper compares neural compression methods against standard subword tokenizers (SentencePiece) and raw byte processing
  - Quick check question: Why does SentencePiece typically achieve 4× compression while neural compressors can achieve 10×+?

- Concept: Context sensitivity in language modeling
  - Why needed here: The paper emphasizes that compressed tokens are highly context-dependent, requiring strong contextual understanding
  - Quick check question: How does context sensitivity affect the learnability of compressed text compared to standard tokenizations?

## Architecture Onboarding

- Component map:
  - M1: Byte-level language model (3m parameters) that assigns probabilities for compression
  - Compression algorithm: Arithmetic Coding with Equal-Info Windows segmentation
  - M2: Language model trained over compressed tokens (various sizes: 25m, 113m, 403m, 2b)
  - Tokenization: 8-bit (256 vocab) or 16-bit (65k vocab) chunks of compressed bitstream
  - Data pipeline: C4 corpus → M1 compression → token segmentation → M2 training

- Critical path:
  1. Train M1 on raw byte sequences
  2. Use M1 to compress training corpus via Equal-Info Windows
  3. Segment compressed bitstream into fixed-size tokens
  4. Train M2 on compressed token sequences
  5. Evaluate M2 using bits/byte metric adjusted for compression ratio

- Design tradeoffs:
  - Window size vs learnability: Shorter windows (16 bits) are more learnable but less compressed; longer windows compress better but are harder to learn
  - Vocabulary size vs efficiency: 16-bit tokens (65k vocab) improve performance but increase computational cost
  - M1 strength vs M2 learning: Stronger M1 removes more predictable patterns but risks making output too random for M2

- Failure signatures:
  - Model outputs uniform distribution over tokens (indicating failure to learn compressed patterns)
  - Training loss plateaus early without meaningful improvement
  - Accuracy on sequence-to-sequence compression/decompression tasks remains near random chance
  - Performance degrades when window size increases beyond 16-32 bits

- First 3 experiments:
  1. Train 25m parameter M2 on EqualInfoAC[b=16, v=256] compressed data and measure bits/byte vs byte baseline
  2. Compare learnability of Arithmetic Coding vs Equal-Info Windows by training identical M2 models on both
  3. Abate window size (16, 32, 64, 128 bits) to identify optimal balance between compression and learnability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can stronger compression methods be designed that maintain learnability for LLMs while achieving significantly better compression ratios than subword tokenizers?
- Basis in paper: [explicit] The authors conclude by suggesting that future work should create compression methods that deliver higher compression rates while maintaining learnability and offering a more direct view of the underlying raw text.
- Why unresolved: The paper demonstrates that existing compression methods like AC are either too strong (not learnable) or too weak (poor compression). Finding the right balance between compression strength and learnability remains an open challenge.
- What evidence would resolve it: Development and experimental validation of new compression algorithms that achieve both high compression ratios (>10×) and competitive perplexity scores on LLM benchmarks, while maintaining stability in the text-to-token mapping.

### Open Question 2
- Question: What is the theoretical limit on compression ratio for learnable tokenizers, and how close can we get to it with current LLM architectures?
- Basis in paper: [inferred] The paper shows a clear trade-off between compression ratio and learnability, with the best performing method (EqualInfoAC[b=16, v=65k]) achieving 5.31× compression while approaching but not matching SentencePiece performance.
- Why unresolved: The paper explores this trade-off empirically but doesn't establish theoretical bounds on achievable compression ratios for learnable tokenizers, nor does it explore whether architectural changes to LLMs could improve learnability of stronger compression.
- What evidence would resolve it: Formal analysis of information-theoretic bounds on learnable compression, combined with experiments showing performance ceilings for different model architectures and compression methods.

### Open Question 3
- Question: How does the stability of text-to-token mappings affect LLM performance, and what level of stability is necessary for effective learning?
- Basis in paper: [explicit] The authors find that AC-based tokenizations are less stable than SentencePiece, with high edit distances between tokenized sentence pairs differing by a single prefix word, and show that EqualInfoAC with single-token windows performs better than multi-token windows.
- Why unresolved: While the paper demonstrates that instability affects learnability, it doesn't quantify the relationship between stability and performance or determine the minimum stability required for effective learning.
- What evidence would resolve it: Systematic experiments varying the stability of tokenizations while measuring perplexity, combined with analysis of how stability affects the model's ability to generalize across different text contexts.

## Limitations

- The learnability of neural compression remains partially speculative, with unclear reasons why larger models scale better with compressed input
- Compressed tokens are "less stable and less semantic than standard subword tokenizers," with practical implications for downstream tasks not thoroughly explored
- Computational efficiency claims may not capture full computational picture, as larger vocabulary (65k) for 16-bit tokens introduces softmax layer overhead

## Confidence

**High Confidence**: The core empirical findings are well-supported - Equal-Info Windows with 16-bit tokens demonstrably achieve better compression rates (0.94 bits/byte) than byte-level baselines while maintaining learnability. The scaling trends with model size are clearly demonstrated through systematic experiments.

**Medium Confidence**: The proposed mechanism (context resetting at fixed bit boundaries) explains the learnability improvements, but alternative explanations (such as the inherent regularization effect of compression) are not fully ruled out. The comparative analysis against SentencePiece shows diminishing gaps at scale, but the causal factors aren't conclusively established.

**Low Confidence**: The claim about neural compression removing "low-level patterns" while retaining "higher-level structure" is theoretically plausible but lacks direct empirical validation. The paper doesn't provide analysis of what specific linguistic information is lost versus preserved through the compression process.

## Next Checks

1. **Ablation Study on Window Size Dynamics**: Systematically test intermediate window sizes (e.g., 24, 48, 96 bits) and measure the exact inflection point where learnability degrades. This would clarify whether the 16-bit optimum is sharp or represents a broader plateau, and whether the mechanism truly depends on fixed-bit boundaries versus some other property.

2. **Semantic Preservation Analysis**: Evaluate M2 models trained on compressed text on downstream semantic tasks (e.g., GLUE benchmark or similar) to quantify the practical impact of "less stable and less semantic" tokenization. Compare performance degradation against the compression benefits to establish the real-world tradeoff curve.

3. **Alternative Compression Algorithm Comparison**: Replace Arithmetic Coding with modern neural compressors (e.g., Transformer-based compressors) while keeping the Equal-Info Windows structure intact. This would isolate whether the learnability improvements stem from the segmentation strategy specifically or could generalize to other compression methods, strengthening the claims about mechanism rather than implementation details.