---
ver: rpa2
title: Can LLMs perform structured graph reasoning?
arxiv_id: '2402.01805'
source_url: https://arxiv.org/abs/2402.01805
tags:
- graph
- traversal
- problem
- llms
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks five large language models (GPT-3.5, GPT-4,
  Claude-2, Llama-2, and Palm-2) on graph reasoning tasks. The authors design 10 increasingly
  complex graph traversal problems and evaluate model performance across varying graph
  sizes and prompting strategies.
---

# Can LLMs perform structured graph reasoning?

## Quick Facts
- **arXiv ID**: 2402.01805
- **Source URL**: https://arxiv.org/abs/2402.01805
- **Reference count**: 40
- **Primary result**: LLMs show inverse correlation between performance and graph complexity, and PathCompare prompting significantly improves graph reasoning accuracy.

## Executive Summary
This paper benchmarks five large language models (GPT-3.5, GPT-4, Claude-2, Llama-2, and Palm-2) on graph reasoning tasks. The authors design 10 increasingly complex graph traversal problems and evaluate model performance across varying graph sizes and prompting strategies. Key findings include an inverse correlation between model performance and graph complexity (measured by average degrees of freedom per node), a negative impact of k-shot prompting on graph reasoning accuracy, and a positive response bias causing models to predict solutions even when none exist. To address these limitations, the authors introduce PathCompare, a novel prompting technique that significantly improves model performance by having the LLM enumerate and compare multiple possible paths before selecting the optimal one.

## Method Summary
The study evaluates LLMs on graph traversal tasks including linear graphs, random tree traversal, weighted tree traversal, grid traversal (random, weighted, directed, and directed with no solution), and Eulerian path problems. Adjacency matrices with alphabetically labeled nodes are used as input, with graph sizes varying between O(10), O(20), and O(20) jumbled variants. The paper tests zero-shot, one-shot, and three-shot prompting settings across all models, comparing standard prompting, chain-of-thought, and the novel PathCompare technique. Performance is measured using binary accuracy (correct path prediction) and partial accuracy (fraction of correct nodes in the solution path).

## Key Results
- Graph reasoning performance inversely correlates with average degrees of freedom per node
- k-shot prompting generally degrades or has no effect on graph reasoning accuracy
- PathCompare prompting technique significantly improves performance by enabling path enumeration and comparison
- Models show positive response bias, predicting solutions even when none exist

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The PathCompare technique improves performance by allowing the LLM to first enumerate multiple paths and then compare their costs before selecting the optimal one.
- Mechanism: By prompting the model to "Let's list down all the possible paths from node {X} to node {Y}, and compare the cost to get the answer," the LLM shifts from direct optimization to a retrieval-and-comparison task. This reduces the complexity of multi-hop reasoning into simpler, sequential steps.
- Core assumption: The LLM's ability to generate and compare multiple candidate paths is stronger than its ability to directly compute the optimal path in a single step.
- Evidence anchors:
  - [abstract] "we propose a novel prompting technique specially designed for graph traversal tasks (PathCompare), which demonstrates a notable increase in the performance of LLMs in comparison to standard prompting techniques such as Chain-of-Thought (CoT)."
  - [section] "Using these insights, we propose a novel prompting technique, which we refer to as PathCompare, which simplifies the reasoning problem by allowing models to compare different possible solutions."
  - [corpus] Found 25 related papers; no direct evidence of PathCompare in neighbors. Weak corpus evidence.
- Break condition: If the model fails to generate valid candidate paths or cannot reliably compare costs, the advantage of PathCompare disappears.

### Mechanism 2
- Claim: Graph reasoning performance inversely correlates with the average degrees of freedom per node, meaning denser graphs are harder for LLMs.
- Mechanism: In tree-based graphs, each node has limited branching, making path tracking simpler. Grid-based graphs have more possible moves per node, increasing cognitive load on the model's sequential processing.
- Core assumption: LLMs process graph information sequentially, so higher connectivity per node directly increases reasoning difficulty.
- Evidence anchors:
  - [section] "we observe certain limitations that hinder LLMs specifically in the case of graph traversal problems. Using these insights, we propose a novel prompting technique, which we refer to as PathCompare, which simplifies the reasoning problem by allowing models to compare different possible solutions."
  - [section] "we notice a general drop in performance in random tree traversal (problem 1.2) across O(10) and O(20) graphs. This clearly indicates that a greater number of average degrees of freedom for traversal per node has an inverse correlation with reasoning capability."
  - [corpus] No direct evidence in neighbors; weak corpus support.
- Break condition: If a model is augmented with explicit state-tracking or parallel processing, the correlation may weaken or disappear.

### Mechanism 3
- Claim: k-shot prompting often degrades performance because it introduces confusion between examples and the target task.
- Mechanism: The LLM is more likely to misapply patterns from few-shot examples to the new graph problem, leading to incorrect reasoning or format mismatches.
- Core assumption: The model treats few-shot examples as templates to mimic rather than as isolated demonstrations, causing interference.
- Evidence anchors:
  - [section] "we observe that in the majority of the tasks, k-shot prompting has either no statistically significant effect, or a negative effect on reasoning accuracy. In general, while few-shot prompting is helpful in response format shaping, it is of no particular aid for analytical tasks such as graph reasoning."
  - [section] "Carefully examining individual responses, we observe models (even the more powerful ones like GPT4) confusion between different examples, and responding to k-shot examples rather than the relevant graph traversal question."
  - [corpus] No direct evidence in neighbors; weak corpus support.
- Break condition: If examples are highly similar to the target problem and the task is formatting-driven rather than reasoning-driven, k-shot prompting may help.

## Foundational Learning

- Concept: Graph traversal fundamentals (shortest path, least-cost path, Eulerian path)
  - Why needed here: The paper evaluates LLMs on multiple graph traversal problems; understanding these concepts is essential to interpret results.
  - Quick check question: What is the difference between a shortest path and a least-cost path in a weighted graph?

- Concept: Adjacency matrix representation of graphs
  - Why needed here: The study uses adjacency matrices (not adjacency lists) to preserve structural complexity and prevent trivialization.
  - Quick check question: How would you represent a directed edge from node A to node B with weight 3 in an adjacency matrix?

- Concept: Prompt engineering basics (few-shot vs zero-shot, chain-of-thought)
  - Why needed here: The paper compares standard prompting, CoT, and PathCompare, so understanding these techniques is critical.
  - Quick check question: What is the main difference between zero-shot and one-shot prompting?

## Architecture Onboarding

- Component map: Graph generator → Adjacency matrix → Prompt formatter → LLM interface → Response parser → Accuracy evaluator
- Critical path: Generate graph adjacency matrix → Format prompt with appropriate k-shot setting and graph type → Send to LLM → Parse response (node sequence or True/False) → Compare to ground truth and calculate accuracy/partial credit
- Design tradeoffs: Adjacency matrix vs list (more compact but increases reasoning load), manual vs automated evaluation (higher accuracy but lower scalability), context window limits (affect feasibility of few-shot settings on larger graphs)
- Failure signatures: Model refuses to answer (e.g., Palm-2 with long prompts), response does not match expected format, partial path provided instead of complete sequence, random or constant responses (suggesting confusion or bias)
- First 3 experiments:
  1. Replicate baseline O(10) tree traversal accuracy for GPT-4 in 0-shot vs 3-shot settings.
  2. Compare PathCompare vs standard prompting on weighted grid traversal (Problem 2.2).
  3. Test jumbled O(20) variant to observe pretraining bias effects.

## Open Questions the Paper Calls Out
None

## Limitations
- The exact phrasing and implementation of the PathCompare technique is not specified, making independent verification challenging
- The specific version and configuration of the instruction-fine-tuned Llama-2 model used in experiments is not documented
- The study focuses exclusively on adjacency matrix representations, limiting generalizability to other graph representations

## Confidence
- **High confidence**: Inverse correlation between graph complexity (degrees of freedom per node) and LLM performance
- **Medium confidence**: PathCompare technique significantly improves performance
- **Medium confidence**: k-shot prompting generally degrades or has no effect on graph reasoning accuracy
- **Medium confidence**: Positive response bias causes models to predict solutions even when none exist

## Next Checks
1. **Replicate PathCompare Implementation**: Independently implement the PathCompare technique using only the information provided in the paper (prompt phrasing about enumerating and comparing paths), then reproduce the performance improvements on weighted grid traversal tasks.

2. **Test k-shot Interference Mechanism**: Design an experiment where few-shot examples are structurally similar but contextually irrelevant to the target graph problem, then measure whether this induces more errors than zero-shot prompting, directly testing the confusion hypothesis.

3. **Validate Response Bias**: Systematically test the positive response bias by creating a series of directed grid traversal problems with no valid solution, then measure the frequency and confidence of model predictions versus refusals, particularly focusing on differences between models.