---
ver: rpa2
title: Weakly Supervised Anomaly Detection via Knowledge-Data Alignment
arxiv_id: '2402.03785'
source_url: https://arxiv.org/abs/2402.03785
tags:
- knowledge
- data
- anomaly
- detection
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KDAlign, a framework that aligns rule knowledge
  with data using optimal transport to improve weakly supervised anomaly detection.
  The key idea is to represent rules as logical formulas, embed both rules and data
  into a common space, and use OT to align them, then incorporate the alignment loss
  into the WSAD training objective.
---

# Weakly Supervised Anomaly Detection via Knowledge-Data Alignment

## Quick Facts
- arXiv ID: 2402.03785
- Source URL: https://arxiv.org/abs/2402.03785
- Reference count: 40
- Key outcome: KDAlign consistently outperforms state-of-the-art WSAD methods in terms of AUPRC and F1@K, even with partially noisy rules

## Executive Summary
This paper introduces KDAlign, a framework that improves weakly supervised anomaly detection (WSAD) by aligning rule knowledge with data using optimal transport (OT). The approach represents rules as logical formulas, embeds both rules and data into a common space, and uses OT to align them. The OT distance is then incorporated as an additional loss term in the WSAD training objective. Extensive experiments on five real-world datasets demonstrate that KDAlign consistently outperforms state-of-the-art WSAD methods, showing particular robustness to noisy rules.

## Method Summary
KDAlign addresses the WSAD problem by incorporating expert rule knowledge into the training process through optimal transport alignment. The framework first acquires rule knowledge by training decision tree models and extracting decision paths as propositional logic formulas. Both data samples and rules are embedded into a shared space using deep learning encoders and graph convolutional networks respectively. Optimal transport is then used to align these embeddings, with the resulting Wasserstein distance serving as an additional loss term alongside the standard prediction loss. The joint optimization allows the model to simultaneously learn from labeled anomalies and rule knowledge.

## Key Results
- KDAlign outperforms state-of-the-art WSAD methods across five real-world datasets
- The framework maintains effectiveness even when rules contain partial noise
- AUPRC and F1@K metrics show consistent improvements over baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal Transport (OT) aligns rule knowledge with data in a shared embedding space, providing geometrically faithful comparison of distributions.
- Mechanism: OT computes a cost matrix between rule embeddings and data embeddings, then finds an optimal transport plan that minimizes the expected cost. This plan represents matched pairs of rules and data samples. The resulting Wasserstein distance is used as an additional loss term in WSAD training.
- Core assumption: The distance between rule embeddings and data embeddings reflects meaningful semantic similarity that can guide anomaly detection.
- Evidence anchors:
  - [abstract]: "To facilitate this alignment, we employ the Optimal Transport (OT) technique. We then incorporate the OT distance as an additional loss term to the original objective function of WSAD methodologies."
  - [section]: "Specifically, the OT method intrinsically forms a robust framework, enabling a geometrically faithful comparison of probability distributions and facilitating the information transfer between distinct distributions."
  - [corpus]: Weak evidence - related papers mention OT for graph alignment and multimodal alignment, but not specifically for WSAD.
- Break condition: If the embedding space fails to capture meaningful semantic relationships, or if the OT plan becomes dominated by noise rules, the alignment may introduce harmful bias.

### Mechanism 2
- Claim: The joint loss (prediction loss + OT loss) allows the model to simultaneously learn from labeled anomalies and rule knowledge, improving generalization.
- Mechanism: The prediction loss (e.g., deviation loss) trains the model to distinguish anomalies from normal samples. The OT loss encourages the model to align data samples with relevant rules, effectively incorporating expert knowledge. By jointly optimizing both, the model learns representations that satisfy both objectives.
- Core assumption: Rule knowledge provides complementary information to the limited labeled anomalies, and the OT alignment can effectively transfer this knowledge to the data representations.
- Evidence anchors:
  - [abstract]: "We then incorporate the OT distance as an additional loss term to the original objective function of WSAD methodologies."
  - [section]: "The OT loss offers a targeted optimization direction, thereby effectively incorporating knowledge information and enhancing the model performance."
  - [corpus]: Weak evidence - related papers mention incorporating knowledge into models, but not specifically via OT in WSAD.
- Break condition: If the rules are too noisy or misaligned with the data distribution, the OT loss may mislead the model and degrade performance.

### Mechanism 3
- Claim: KDAlign is robust to noisy rules because OT's global perspective constrains the impact of individual noisy rules through other correct rules.
- Mechanism: When a data sample matches a noisy rule, the OT plan will assign it a high cost with that rule. To minimize the total cost, the plan will also assign lower costs with other correct rules that the sample should match. This global optimization constrains the influence of noisy rules.
- Core assumption: The correct rules provide enough constraints to counteract the effect of noisy rules in the OT optimization.
- Evidence anchors:
  - [section]: "Regarding noisy knowledge, when a sample matches a noisy rule, the distance of that sample to some other closely related rules will be farther, resulting in an increased OT distance penalty... to ensure global optimality, the OT distance between this sample and the noisy rule will be constrained by other correct rules, thereby ensuring the performance of KDAlign."
  - [corpus]: Weak evidence - no direct evidence in corpus about OT robustness to noisy knowledge in WSAD.
- Break condition: If the proportion of noisy rules exceeds a threshold, or if the correct rules are insufficient to provide proper constraints, the model's performance may degrade.

## Foundational Learning

- Concept: Optimal Transport (OT) and Wasserstein distance
  - Why needed here: OT provides a principled way to align rule knowledge with data in a shared embedding space, enabling the incorporation of expert knowledge into WSAD.
  - Quick check question: What is the mathematical formulation of the OT problem, and how is the Wasserstein distance computed from the optimal transport plan?

- Concept: Graph Convolutional Networks (GCNs) for knowledge representation
  - Why needed here: GCNs are used to encode propositional formulae (rules) into a knowledge embedding space, allowing them to be aligned with data embeddings.
  - Quick check question: How does a GCN propagate information through a graph structure, and what is the role of the adjacency matrix in this process?

- Concept: Weakly Supervised Anomaly Detection (WSAD) problem setting
  - Why needed here: Understanding the WSAD setting is crucial for appreciating the challenge of limited labeled anomalies and the need for incorporating additional knowledge.
  - Quick check question: What is the key difference between WSAD and fully supervised anomaly detection, and why is the former more challenging?

## Architecture Onboarding

- Component map:
  - Data ‚Üí Data Encoder ‚Üí Data Embeddings ‚Üí OT Module ‚Üí OT Plan ‚Üí OT Loss ‚Üí Joint Loss ‚Üí Model Parameters
  - Rules ‚Üí Knowledge Encoder ‚Üí Rule Embeddings ‚Üí OT Module ‚Üí OT Plan ‚Üí OT Loss ‚Üí Joint Loss ‚Üí Model Parameters

- Critical path: Data ‚Üí Data Encoder ‚Üí Data Embeddings ‚Üí OT Module ‚Üí OT Plan ‚Üí OT Loss ‚Üí Joint Loss ‚Üí Model Parameters. Rules ‚Üí Knowledge Encoder ‚Üí Rule Embeddings ‚Üí OT Module ‚Üí OT Plan ‚Üí OT Loss ‚Üí Joint Loss ‚Üí Model Parameters.

- Design tradeoffs:
  - Tradeoff between the weight of the OT loss (ùúÜ) and the prediction loss: A higher ùúÜ emphasizes rule alignment but may neglect the labeled anomalies; a lower ùúÜ may not effectively incorporate the rules.
  - Choice of OT method: Sinkhorn-Knopp algorithm is used for efficiency, but other methods like exact OT or entropic OT could be explored.
  - Rule representation: Using d-DNNF graphs for propositional formulae enables efficient processing, but other representations could be considered.

- Failure signatures:
  - If the OT loss dominates the prediction loss, the model may overfit to the rules and ignore the labeled anomalies.
  - If the OT plan is noisy or unaligned, the OT loss may introduce harmful bias.
  - If the embedding spaces are not well-aligned, the OT alignment may not be meaningful.

- First 3 experiments:
  1. Train KDAlign on a small dataset (e.g., Cardiotocography) with a simple rule set and compare the learned embeddings to the ground truth labels.
  2. Vary the weight of the OT loss (ùúÜ) and observe its effect on the model's performance on a validation set.
  3. Introduce noisy rules and measure the impact on the model's performance, comparing it to a baseline without OT alignment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does KDAlign handle situations where rule knowledge is incomplete or only partially relevant to the data distribution?
- Basis in paper: [explicit] The paper mentions that rules can be noisy and discusses how OT helps mitigate this issue, but does not fully explore scenarios where rules are incomplete.
- Why unresolved: While the paper shows that KDAlign is robust to noisy knowledge, it does not analyze the impact of incomplete rule sets or their relevance to the data.
- What evidence would resolve it: Experiments testing KDAlign's performance with varying degrees of rule completeness or relevance would clarify its effectiveness in such scenarios.

### Open Question 2
- Question: Can KDAlign be extended to work effectively with graph-structured data, such as knowledge graphs or social networks?
- Basis in paper: [inferred] The paper briefly mentions extending KDAlign to graph domains as a future direction but does not provide any experimental validation or methodology.
- Why unresolved: The current implementation is focused on tabular data, and the challenges of applying KDAlign to graph data (e.g., handling node attributes, graph structures) are not addressed.
- What evidence would resolve it: A study demonstrating KDAlign's performance on graph-based anomaly detection tasks would validate its applicability in such domains.

### Open Question 3
- Question: How does the choice of optimal transport (OT) method affect KDAlign's performance, and are there alternative OT methods that could improve results?
- Basis in paper: [explicit] The paper uses the Sinkhorn-Knopp algorithm for OT but mentions exploring other OT methods as a future direction.
- Why unresolved: The paper does not compare KDAlign's performance with other OT methods (e.g., Gromov-Wasserstein) or analyze the trade-offs between different approaches.
- What evidence would resolve it: Comparative experiments using alternative OT methods would clarify whether the choice of OT significantly impacts KDAlign's effectiveness.

## Limitations

- The framework's performance heavily depends on the quality and completeness of the rule knowledge provided.
- The experiments primarily use synthetically generated rules from decision trees, leaving the performance on naturally occurring expert rules untested.
- The paper doesn't thoroughly explore the sensitivity of KDAlign to the choice of OT algorithm or embedding space dimensionality.

## Confidence

- Mechanism 1 (OT alignment): Medium - The theoretical foundation is sound, but the empirical evidence for WSAD is limited.
- Mechanism 2 (Joint loss): Medium - The concept is well-established, but the specific implementation details and their impact are unclear.
- Mechanism 3 (Robustness to noise): Low - The paper provides limited evidence and doesn't explore the threshold at which noisy rules become detrimental.

## Next Checks

1. Evaluate KDAlign on a dataset with naturally occurring expert rules to assess its real-world applicability.
2. Conduct an ablation study to quantify the impact of the OT loss weight (Œª) and the choice of OT algorithm on model performance.
3. Systematically introduce varying levels of noise into the rule set and measure the corresponding degradation in KDAlign's performance.