---
ver: rpa2
title: 'SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge
  Base Question Generation'
arxiv_id: '2404.01923'
source_url: https://arxiv.org/abs/2404.01923
tags:
- skeleton
- question
- kbqg
- davinci003
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes SGSH, a framework that stimulates large language
  models (LLMs) like GPT-3.5 to generate high-quality questions from knowledge base
  facts by incorporating "skeleton heuristics" as fine-grained guidance. SGSH includes
  a skeleton generator that produces skeletons (question phrases and auxiliary verbs)
  for each input using an automatic data construction strategy and soft prompting.
---

# SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation

## Quick Facts
- arXiv ID: 2404.01923
- Source URL: https://arxiv.org/abs/2404.01923
- Reference count: 32
- The paper proposes SGSH, a framework that stimulates large language models (LLMs) like GPT-3.5 to generate high-quality questions from knowledge base facts by incorporating "skeleton heuristics" as fine-grained guidance.

## Executive Summary
SGSH introduces a novel approach to knowledge base question generation (KBQG) that leverages skeleton heuristics to enhance the performance of large language models like GPT-3.5. The framework consists of a skeleton generator that produces question phrases and auxiliary verbs for each input, which are then encoded into prompts to guide the LLM through skeleton injection and skeleton-aware in-context learning. Experiments on WebQuestions and PathQuestions datasets demonstrate that SGSH outperforms state-of-the-art models, with SGSH(Davinci003) achieving 68.16% BLEU-1, 56.32% BLEU-2, 47.30% BLEU-3, and 39.12% BLEU-4 on WebQuestions.

## Method Summary
SGSH is a two-module framework that generates questions from knowledge base facts by incorporating skeleton heuristics. The first module is a skeleton generator based on BART that produces question phrases and auxiliary verbs for each input using an automatic data construction strategy with learnable prompting. The second module is a frozen GPT-3.5 model that generates questions guided by skeleton injection and skeleton-aware in-context learning. The framework uses input+skeleton emb selection strategy for example retrieval, and learnable prompting with multiple groups of soft prompts to boost skeleton generator performance.

## Key Results
- SGSH outperforms state-of-the-art models on WebQuestions and PathQuestions datasets
- SGSH(Davinci003) achieves 68.16% BLEU-1, 56.32% BLEU-2, 47.30% BLEU-3, and 39.12% BLEU-4 on WebQuestions
- Input+skeleton emb selection strategy achieves the best performance by considering both input proximity and skeleton consistency
- When using 8 groups of learnable prompts, BLEU-4 performance improves by 2.83% and ROUGE-L by 1.63% compared to using a single group

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Skeleton heuristics act as fine-grained guidance that unlocks task-specific knowledge within LLMs for KBQG.
- **Mechanism**: By injecting structured skeleton elements (question phrase + auxiliary verb) into prompts, the LLM's attention is steered toward generating questions aligned with the answer's semantic structure.
- **Core assumption**: LLMs possess latent knowledge relevant to KBQG but need explicit structural cues to surface it appropriately.
- **Evidence anchors**:
  - [abstract] "skeleton heuristics, which provides more fine-grained guidance associated with each input to stimulate LLMs to generate optimal questions, encompassing essential elements like the question phrase and the auxiliary verb."
  - [section 2] "We observe that prompts coupled with skeleton heuristics can boost the performance of Davinci003 on the KBQG task (Cf. the comparison between Davinci003 and Davinci003+SH in Figure 1)."
  - [corpus] Weak correlation; no direct mention of skeleton heuristics or KBQG in related papers.

### Mechanism 2
- **Claim**: Skeleton-aware in-context learning improves example selection by matching both input semantics and skeleton structure.
- **Mechanism**: Embeddings derived from input-skeleton pairs enable retrieval of examples that share both semantic and structural similarity to the test case, enhancing few-shot performance.
- **Core assumption**: Semantic similarity alone is insufficient; structural consistency via skeletons improves alignment with target questions.
- **Evidence anchors**:
  - [section 3.3] "Given a test input skeleton injection (Gj, aj, sj) and a training example skeleton injection (Gi, ai, si), we apply a small PLM (i.e., BART) trained onD to obtain their corresponding embeddings ej and ei."
  - [section 4.3] "Our proposed strategy (i.e., input+skeleton emb) achieves the best performance as our strategy takes into account the proximity of the input as well as the consistency of the skeleton in the latent space."
  - [corpus] No direct evidence; inference based on method description.

### Mechanism 3
- **Claim**: Learnable prompting with multiple groups of soft prompts boosts skeleton generator performance.
- **Mechanism**: Ensembling different learnable prompt groups with varied hyperparameters allows the model to capture diverse aspects of skeleton generation, improving generalization.
- **Core assumption**: Diverse prompt configurations encode complementary aspects of the skeleton generation task.
- **Evidence anchors**:
  - [section 3.2] "Training t groups of learnable prompts, each with different hyperparameters, and subsequently ensembling them during the inference phase can significantly boost the performance of the model."
  - [section 4.3] "When t is set to 8, the performance in BLEU-4 obtains 2.83% gain and the performance in ROUGE-L gets 1.63% gain compared to t = 1."
  - [corpus] No direct mention of learnable prompting or prompt ensembling in related papers.

## Foundational Learning

- **Concept**: Skeleton extraction and its role in question structure
  - **Why needed here**: Skeletons define the grammatical scaffolding (question phrase + auxiliary verb) that guides LLM output formatting.
  - **Quick check question**: Can you manually extract the skeleton from a given question like "What university did Lucy's sister graduate from?" (Answer: "What university did _ ?")

- **Concept**: In-context learning and few-shot prompting
  - **Why needed here**: GPT-3.5 relies on carefully curated examples in the prompt to adapt to KBQG without fine-tuning.
  - **Quick check question**: What happens to GPT-3.5's output quality if you remove the skeleton injection from the prompt? (Answer: It reverts to lower quality, less aligned questions.)

- **Concept**: Embedding-based retrieval for example selection
  - **Why needed here**: Enables efficient retrieval of structurally and semantically similar examples for in-context learning.
  - **Quick check question**: How does cosine similarity between input and example embeddings influence example selection? (Answer: Higher similarity increases likelihood of example being chosen, improving prompt relevance.)

## Architecture Onboarding

- **Component map**: Skeleton Generator (BART-base with learnable prompts) -> Skeleton Quality Evaluator (ChatGPT-based scoring) -> Skeleton Heuristics Injector (prompt formatter) -> GPT-3.5 Model (frozen, accessed via API) -> Example Retriever (embedding-based selection)

- **Critical path**: Skeleton Generator → Skeleton Quality Evaluator → GPT-3.5 with Skeleton Heuristics-enhanced Prompt

- **Design tradeoffs**:
  - Using GPT-3.5 (powerful but costly) vs. fine-tuning smaller PLMs (cheaper but less capable)
  - Learnable prompts vs. fixed prompts (better performance but higher training complexity)
  - Automatic skeleton construction vs. manual annotation (scalable but potentially noisier data)

- **Failure signatures**:
  - Poor skeleton quality → misaligned questions
  - Retrieval mismatch → irrelevant in-context examples
  - Prompt collapse → loss of learnable prompt diversity

- **First 3 experiments**:
  1. Test skeleton generator accuracy on a small labeled set before scaling.
  2. Validate skeleton heuristics-enhanced prompt improves over vanilla prompt using a held-out sample.
  3. Compare random vs. input+skeleton emb retrieval strategies on example selection quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SGSH scale with larger knowledge bases or more complex question types?
- Basis in paper: Inferred from the paper's discussion of the limitations of current datasets and the potential for a more comprehensive benchmark.
- Why unresolved: The paper primarily evaluates on two widely used datasets (WebQuestions and PathQuestions) and does not explore the model's performance on larger or more complex knowledge bases.
- What evidence would resolve it: Experiments evaluating SGSH on larger knowledge bases with more complex question types and comparing the performance to existing methods.

### Open Question 2
- Question: How does the quality of the automatically constructed skeleton training data affect the performance of the skeleton generator?
- Basis in paper: Explicit mention of using ChatGPT to refine skeletons and score their quality, as well as the observation that few supervised data can achieve comparable performance to full supervised data.
- Why unresolved: The paper does not provide a detailed analysis of the impact of different quality levels of the automatically constructed skeleton training data on the skeleton generator's performance.
- What evidence would resolve it: Experiments varying the quality of the automatically constructed skeleton training data and measuring the corresponding performance of the skeleton generator.

### Open Question 3
- Question: Can the skeleton heuristics-enhanced prompting approach be applied to other natural language processing tasks beyond knowledge base question generation?
- Basis in paper: Inferred from the paper's discussion of the effectiveness of skeleton heuristics in providing fine-grained guidance for GPT-3.5 and the potential for the approach to be inspiring for other NLP tasks.
- Why unresolved: The paper focuses solely on knowledge base question generation and does not explore the applicability of the skeleton heuristics-enhanced prompting approach to other NLP tasks.
- What evidence would resolve it: Experiments applying the skeleton heuristics-enhanced prompting approach to other NLP tasks (e.g., text summarization, machine translation) and comparing the performance to existing methods.

## Limitations
- The paper's performance claims rely heavily on the quality of automatically constructed skeleton data, but the paper does not provide quantitative validation of skeleton quality beyond indirect metrics (BLEU scores).
- The effectiveness of skeleton heuristics depends on accurate skeleton extraction, yet the paper only demonstrates that the automatic construction method produces skeletons with "more than 96% coverage" without measuring semantic accuracy or grammatical correctness.
- The ChatGPT-based quality evaluator introduces potential evaluation bias, as the same model family (GPT) that benefits from skeleton heuristics is used to validate skeleton quality.

## Confidence
- **High confidence**: SGSH framework architecture and its two-module design (skeleton generator + GPT-3.5 with skeleton heuristics) - well-specified with clear implementation details.
- **Medium confidence**: Performance improvements over baselines - supported by experimental results but dependent on automatic skeleton quality that isn't independently validated.
- **Low confidence**: Claims about skeleton heuristics being the primary driver of performance - correlation is shown but causation is not rigorously established through ablation studies on skeleton quality versus other factors.

## Next Checks
1. **Independent skeleton quality validation**: Manually annotate 100 randomly selected generated skeletons and compute precision/recall against ground truth to verify the 96% coverage claim actually translates to high semantic accuracy.

2. **Ablation on skeleton quality**: Create a controlled experiment where skeleton generator is intentionally degraded (e.g., using lower-quality training data or removing ChatGPT refinement) and measure the impact on final question generation performance to establish causation.

3. **Cross-model generalization**: Test whether skeleton heuristics provide similar performance gains when applied to smaller, non-GPT models (e.g., BERT or T5) to determine if benefits are specific to GPT-3.5's architecture or represent a more general KBQG improvement.