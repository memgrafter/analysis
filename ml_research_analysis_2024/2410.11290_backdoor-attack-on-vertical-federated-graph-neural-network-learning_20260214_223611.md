---
ver: rpa2
title: Backdoor Attack on Vertical Federated Graph Neural Network Learning
arxiv_id: '2410.11290'
source_url: https://arxiv.org/abs/2410.11290
tags:
- uni00000013
- uni00000011
- backdoor
- uni00000044
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of Vertical Federated Graph
  Neural Networks (VFGNN) to backdoor attacks. The authors propose BVG, the first
  backdoor attack method specifically designed for VFGNN, which leverages multi-hop
  triggers and backdoor retention.
---

# Backdoor Attack on Vertical Federated Graph Neural Network Learning

## Quick Facts
- arXiv ID: 2410.11290
- Source URL: https://arxiv.org/abs/2410.11290
- Reference count: 40
- This paper proposes BVG, the first backdoor attack method specifically designed for Vertical Federated Graph Neural Networks (VFGNN).

## Executive Summary
This paper introduces BVG, a novel backdoor attack targeting Vertical Federated Graph Neural Networks (VFGNN). The attack exploits the multi-party collaborative learning setting where different parties hold disjoint feature sets of the same graph. BVG uses multi-hop trigger injection and requires only four target-class nodes to achieve nearly 100% attack success rates across three datasets and three GNN models, while maintaining minimal impact on main task accuracy. The authors demonstrate that existing defense methods are insufficient against this sophisticated attack, highlighting critical vulnerabilities in VFGNN systems.

## Method Summary
BVG leverages the unique characteristics of VFGNN where an adversary, acting as a passive party, can inject backdoor triggers without modifying graph structure or labels. The method employs multi-hop triggers that affect target nodes and their neighbors through feature injection, combined with a bi-level optimization approach that generates triggers using minimal target-class samples. During VFGNN training, the adversary injects trigger features into m-hop neighbors of target nodes, allowing the trigger to propagate through the GNN layers. The optimization process simultaneously trains the VFGNN model while generating universal triggers that map to the target class, requiring only four labeled target-class samples for effectiveness.

## Key Results
- Achieves nearly 100% attack success rates across three commonly used datasets
- Maintains minimal impact on main task accuracy while executing attacks
- Requires only four target-class nodes to execute effective attacks
- Demonstrates high attack effectiveness even under existing defense methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-hop trigger injection allows backdoor activation without modifying graph structure or labels
- Mechanism: The adversary injects trigger features into node attributes of m-hop neighbors of target nodes during training, creating a trigger that spans multiple hops in the graph
- Core assumption: The trigger features added to neighbor nodes will propagate through the GNN layers to influence the target node's classification during inference
- Evidence anchors:
  - [abstract]: "BVG uses multi-hop triggers and requires only four target class nodes for an effective backdoor attack."
  - [section 3.3]: "The multi-hop trigger δ affects the target node and its neighbors, δ = {δ0, δ1, · · · , δM }. Here, δm is the trigger added to the attributes of the m-hop neighbors of the target node vp."
  - [corpus]: Weak evidence - the corpus neighbors don't directly address multi-hop trigger mechanisms specifically
- Break condition: If the GNN architecture has very shallow layers or uses different aggregation functions that don't propagate neighbor features effectively to the target node

### Mechanism 2
- Claim: Bi-level optimization enables trigger generation without label access by leveraging limited target-class samples
- Mechanism: The adversary uses PGD to optimize trigger features while simultaneously training the VFGNN model, with the trigger optimization conditioned on the loss for target-class samples
- Core assumption: Having even a small number of labeled target-class samples is sufficient to establish the trigger-target class mapping during bi-level optimization
- Evidence anchors:
  - [abstract]: "BVG uses multi-hop triggers and requires only four target class nodes for an effective backdoor attack."
  - [section 3.3]: "The generation of the trigger relies on the gradients of the target class nodes τ" and "this trigger serves as a universal trigger for the target class τ."
  - [corpus]: Missing direct evidence - the corpus neighbors focus on different attack scenarios not directly comparable to VFGNN with limited label access
- Break condition: If the adversary cannot obtain even a minimal number of target-class labels, or if the optimization landscape is too noisy for the small sample set to provide meaningful gradients

### Mechanism 3
- Claim: The vertical federated setting allows the adversary to inject triggers without detection by passive parties
- Mechanism: The adversary, as a passive party, can modify its local graph features during training while adhering to the VFL protocol, and the trigger is embedded through the federated training process without requiring label modification
- Core assumption: The active party cannot detect trigger injection because it only sees aggregated gradients and doesn't have access to the adversary's raw data modifications
- Evidence anchors:
  - [section 3.2]: "Adversary's capacity. The adversary adheres to the VFL protocol, transmitting local features and receiving gradients without manipulating other participants' information."
  - [section 3.1]: "The adversary requires only a very few training samples labeled target class" and "the adversary has no information about the models and data of other parties."
  - [corpus]: Weak evidence - corpus neighbors discuss VFL backdoor attacks but don't specifically address the passive party trigger injection mechanism in VFGNN
- Break condition: If the active party implements gradient inspection or if the VFL protocol includes verification of feature modifications

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their message-passing mechanism
  - Why needed here: Understanding how GNNs aggregate information from neighbors is crucial to grasping why multi-hop triggers work
  - Quick check question: How does information flow from a node's neighbors through the GNN layers to affect the node's final representation?

- Concept: Federated Learning and Vertical Federated Learning (VFL) architecture
  - Why needed here: The attack exploits the specific constraints and trust model of VFL where data is partitioned by features, not samples
  - Quick check question: What are the key differences between horizontal and vertical federated learning in terms of data partitioning and trust assumptions?

- Concept: Backdoor attacks and trigger optimization
  - Why needed here: The attack uses optimization techniques to generate effective triggers that map to target classes
  - Quick check question: How do clean-label backdoor attacks differ from traditional backdoor attacks that modify labels?

## Architecture Onboarding

- Component map: Adversary (passive party) -> Active party (owns labels) -> Other passive parties -> Multi-hop trigger injection -> Bi-level optimization
- Critical path: 1. Adversary obtains 1-4 target class samples 2. During VFGNN training, adversary injects multi-hop trigger into local graph 3. VFGNN trains with trigger embedded through federated process 4. Deployed model misclassifies any data with trigger as target class
- Design tradeoffs:
  - Trigger complexity vs. stealth: More hops increase effectiveness but may be more detectable
  - Number of target samples vs. attack success: More samples improve optimization but contradict minimal knowledge assumption
  - Perturbation magnitude vs. main task performance: Larger perturbations improve ASR but hurt MTA
- Failure signatures:
  - Low ASR despite successful MTA: Trigger not properly embedded or propagated
  - Both ASR and MTA drop significantly: Optimization interfered with main task training
  - ASR works on training data but not test data: Overfitting to specific target nodes rather than learning universal trigger
- First 3 experiments:
  1. Baseline test: Run VFGNN without any backdoor to establish MTA baseline
  2. Single-hop trigger test: Implement 1-hop trigger version to compare with multi-hop performance
  3. Defense resilience test: Apply simple gradient perturbation defense to measure attack robustness

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How do BVG attacks perform against federated graph neural networks that use graph pooling or sampling techniques?
- Basis in paper: [inferred] The paper evaluates BVG against three standard GNN models but does not address pooling or sampling methods commonly used in federated settings
- Why unresolved: The paper focuses on basic GNN architectures without exploring more complex graph neural network variants that might be used in production federated systems
- What evidence would resolve it: Experimental results showing BVG attack success rates against GNNs with pooling/sampling layers like GraphSAGE, Graph Attention with hierarchical pooling, or cluster-based sampling methods

### Open Question 2
- Question: Can the multi-hop trigger mechanism be extended to dynamic graphs where edge structures change during training?
- Basis in paper: [inferred] The paper assumes static graph structures and does not address scenarios where graph topology evolves over time
- Why unresolved: Real-world federated graph systems often involve temporal graphs or streaming graph data that require adaptive trigger generation
- What evidence would resolve it: Experimental validation of BVG on temporal graph datasets with varying edge dynamics and trigger adaptation strategies

### Open Question 3
- Question: What is the relationship between the number of target class nodes required and the class imbalance in the dataset?
- Basis in paper: [explicit] The paper states that only 4 target class nodes are needed but does not explore how this requirement changes with class imbalance
- Why unresolved: The paper uses balanced datasets without investigating whether minority classes require more or fewer target nodes for successful attacks
- What evidence would resolve it: Systematic experiments varying class distributions and measuring the minimum number of target nodes needed for successful attacks across different imbalance ratios

## Limitations
- The attack assumes static graph structures and may not generalize to dynamic graph scenarios
- The paper focuses on white-box attack scenarios where the adversary has knowledge of the VFGNN architecture and training process
- Defense evaluation uses standard techniques that may not represent the most current or sophisticated defense mechanisms available

## Confidence

**High Confidence**: The core mechanism of multi-hop trigger injection and the basic feasibility of backdoor attacks in VFGNN settings. The mathematical formulation and optimization approach are well-established in the literature.

**Medium Confidence**: The specific claim of requiring only four target samples for effective attacks, and the transferability of attack effectiveness across different GNN architectures and datasets. While results are promising, they may be dataset-dependent.

**Low Confidence**: The practical effectiveness of this attack in dynamic, real-world federated learning environments with varying network conditions, multiple active parties, and sophisticated defense mechanisms already in place.

## Next Checks

1. **Cross-dataset generalization test**: Evaluate BVG on additional graph datasets with different characteristics (varying graph density, node degree distributions, and class imbalance) to assess the robustness of the "four samples" claim and attack effectiveness across diverse scenarios.

2. **Black-box attack evaluation**: Implement and evaluate a black-box version of BVG where the adversary has limited knowledge of the target VFGNN architecture, training process, and feature space to better understand real-world attack feasibility.

3. **Dynamic graph resilience test**: Assess attack effectiveness on graphs with dynamic structures (node additions/deletions, edge changes) to evaluate whether the multi-hop trigger remains effective when the graph topology changes between training and inference phases.