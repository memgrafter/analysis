---
ver: rpa2
title: Should AI Optimize Your Code? A Comparative Study of Classical Optimizing Compilers
  Versus Current Large Language Models
arxiv_id: '2406.12146'
source_url: https://arxiv.org/abs/2406.12146
tags:
- code
- llms
- optimization
- parallel
- compilers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the ability of Large Language Models (LLMs)
  to optimize code in comparison to traditional optimizing compilers. The study addresses
  the question of whether AI-driven models can revolutionize code optimization by
  comparing three classical compilers (CETUS, PLUTO, ROSE) with two LLMs (GPT-4.0,
  CodeLlama-70B) on parallel code optimization tasks.
---

# Should AI Optimize Your Code? A Comparative Study of Classical Optimizing Compilers Versus Current Large Language Models

## Quick Facts
- arXiv ID: 2406.12146
- Source URL: https://arxiv.org/abs/2406.12146
- Reference count: 40
- Current LLMs cannot replace classical compilers for automatic optimization

## Executive Summary
This paper evaluates whether Large Language Models (LLMs) can effectively optimize code compared to traditional optimizing compilers. The study compares three classical compilers (CETUS, PLUTO, ROSE) with two LLMs (GPT-4.0, CodeLlama-70B) on parallel code optimization tasks. The authors developed PCAOT, an automated framework for validating LLM-generated code correctness and performance, and introduced the PCB benchmark suite containing 28 challenging optimization patterns. Results show that while CodeLlama-70B achieved speedups up to 1.75x, LLMs only correctly optimized very small code sections (approximately 20 lines) and failed on more complex patterns. The study concludes that current LLMs cannot yet replace classical compilers for automatic optimization, though they show promise for specific, simpler optimization tasks.

## Method Summary
The study evaluates LLMs against classical compilers using a three-pronged approach: (1) developing PCAOT, an automated validation framework using CaRV selective checkpointing to verify correctness of optimized code sections, (2) creating the PCB benchmark suite with 28 optimization patterns and 44 experimental sections, and (3) testing three prompting strategies (simple instructions, detailed instruction prompting with dependency analysis, and chain-of-thought reasoning) across two LLMs and three compilers. Code sections were isolated using #pragma experimental markers, with optimizations validated through checkpoint comparison against original program behavior.

## Key Results
- CodeLlama-70B achieved speedups up to 1.75x compared to original programs
- LLMs only correctly optimized very small code sections (approximately 20 lines)
- CodeLlama-70B outperformed GPT-4.0, and detailed instruction prompting strategies proved most effective
- Success rate dropped significantly beyond 20 lines of code, with only 16.75% success for larger sections

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can optimize code but only for small, isolated sections due to processing limitations.
- Mechanism: The PCAOT framework isolates code into experimental sections of ~20 lines, which LLMs can process without losing context or correctness.
- Core assumption: LLMs fail to maintain correctness when handling code sizes beyond ~20 lines.
- Evidence anchors:
  - [abstract] states "LLMs only correctly optimized very small code sections (approximately 20 lines)"
  - [section] 4.2 shows failure rates increasing sharply beyond 20 lines of code
  - [corpus] contains related work on code generation limits but not direct corroboration of the 20-line threshold
- Break condition: If code sections exceed 20 lines or contain complex interdependencies, LLM optimization becomes unreliable.

### Mechanism 2
- Claim: Detailed instruction prompting (DIP) substantially improves LLM optimization performance compared to simple or chain-of-thought prompts.
- Mechanism: DIP guides LLMs to explicitly check data dependencies and prioritize outer loop parallelization, mimicking compiler strategies.
- Core assumption: Expressing compiler strategies in prompts provides sufficient guidance for LLMs to make better optimization decisions.
- Evidence anchors:
  - [abstract] notes "expressing a compiler strategy as part of the LLM's prompt substantially improves its overall performance"
  - [section] 3.1.1 describes DIP as adding dependency checking and outer loop prioritization
  - [corpus] shows related work on prompt engineering but not specific to compiler optimization
- Break condition: If prompts don't include dependency analysis or loop prioritization guidance, LLM performance degrades significantly.

### Mechanism 3
- Claim: The PCAOT validation mechanism enables safe use of LLM optimizations by verifying correctness through checkpoint comparison.
- Mechanism: CaRV selective checkpointing captures program states before and after experimental sections, allowing automated comparison of optimized code against original behavior.
- Core assumption: Program states captured at section boundaries contain sufficient information to verify correctness of optimizations.
- Evidence anchors:
  - [abstract] introduces PCAOT as "an automatic mechanism for evaluating the performance and correctness of the code generated by LLMs"
  - [section] 3.1.2 explains how CaRV enables independent execution and comparison of optimized sections
  - [corpus] lacks direct evidence of similar validation mechanisms for LLM-generated code
- Break condition: If optimized code changes program state in ways not captured by checkpoints, validation may fail to detect correctness issues.

## Foundational Learning

- Concept: Data dependency analysis in parallel programming
  - Why needed here: LLMs must understand when parallelization is safe to avoid introducing race conditions
  - Quick check question: What's the difference between read-after-write and write-after-write dependencies?

- Concept: Compiler optimization strategies and polyhedral model
  - Why needed here: Understanding why traditional compilers excel at certain patterns helps evaluate LLM limitations
  - Quick check question: How does the polyhedral model enable PLUTO to achieve 7x speedup on PB benchmarks?

- Concept: Prompt engineering and chain-of-thought reasoning
  - Why needed here: Different prompting strategies significantly impact LLM optimization success rates
  - Quick check question: What key difference makes DIP more effective than simple instruction prompts?

## Architecture Onboarding

- Component map:
  Input: Source code with #pragma experimental section markers → Processing: Three prompting strategies sent to GPT-4.0 and CodeLlama-70B → Validation: CaRV checkpoint comparison against original program behavior → Output: Optimized code sections with performance metrics and correctness verification

- Critical path: Code preparation → LLM optimization → Code insertion → Validation → Performance measurement
- Design tradeoffs: Small code sections ensure LLM reliability but limit optimization scope; detailed prompts improve quality but increase prompt complexity
- Failure signatures: Incorrect optimizations show as checkpoint mismatches; performance regressions show as execution time increases
- First 3 experiments:
  1. Test DIP prompt on a 15-line loop with clear parallelization potential
  2. Test IP prompt on a 25-line section to demonstrate size limitations
  3. Compare CETUS default optimization against CodeLlama-70B DIP on a simple array reduction pattern

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can AI-driven models, specifically Large Language Models (LLMs), completely replace classical optimizing compilers for code optimization tasks?
- Basis in paper: [explicit] The paper concludes that "with their current capabilities, the LLMs cannot yet suitably replace classical compiler optimization technology."
- Why unresolved: Current LLMs show significant limitations in handling large code sizes and complex optimization patterns, with only a 16.75% success rate in generating correct code for sections larger than 20 lines.
- What evidence would resolve it: Demonstration of LLMs consistently achieving 100% correctness and superior performance compared to classical compilers across a wide range of code sizes and optimization patterns.

### Open Question 2
- Question: What specific improvements in LLM architecture or training are needed to enhance their capability for code optimization?
- Basis in paper: [inferred] The paper suggests that "future LLMs will need significant improvements in terms of both performance and correctness" to become viable alternatives to classical compilers.
- Why unresolved: The paper identifies the limitations of current LLMs but does not propose specific architectural or training modifications to address these issues.
- What evidence would resolve it: Successful implementation and testing of LLM variants with improved performance and correctness metrics on diverse code optimization tasks.

### Open Question 3
- Question: How does the energy consumption of AI-driven models compare to classical compilers when achieving performance gains?
- Basis in paper: [explicit] The paper mentions that "Future work will explore additional factors, such as the energy consumption required to achieve performance gains over conventional optimizing compilers."
- Why unresolved: The study focuses on performance and correctness but does not evaluate the energy efficiency of AI-driven models compared to classical compilers.
- What evidence would resolve it: Comparative analysis of energy consumption metrics for AI-driven models and classical compilers across various optimization tasks and code sizes.

## Limitations
- Evaluation focuses exclusively on parallel code optimization tasks, limiting generalizability to other optimization domains
- 20-line code section constraint significantly limits optimization scope for real-world applications
- Only two LLMs (GPT-4.0 and CodeLlama-70B) tested, which may not represent broader LLM capabilities

## Confidence
- **High Confidence**: The comparative methodology between classical compilers and LLMs is sound, and the PCAOT validation framework provides reliable correctness verification.
- **Medium Confidence**: The conclusion that LLMs cannot yet replace classical compilers is well-supported for the specific parallel optimization tasks studied, but may not extend to other optimization domains.
- **Low Confidence**: The specific 20-line limitation threshold and the superiority of DIP prompting may be artifacts of the particular LLMs tested rather than fundamental constraints of LLM-based optimization.

## Next Checks
1. Test the PCAOT framework on a broader range of optimization tasks beyond parallel code, including memory optimization and algorithmic improvements, to assess generalizability.
2. Evaluate additional LLM models and prompting strategies, particularly those optimized for code generation, to determine if the 20-line limitation is fundamental or model-dependent.
3. Apply the methodology to real-world production codebases rather than benchmark suites to assess practical applicability and identify domain-specific optimization opportunities.