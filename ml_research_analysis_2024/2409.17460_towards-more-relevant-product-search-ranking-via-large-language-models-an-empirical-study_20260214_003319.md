---
ver: rpa2
title: 'Towards More Relevant Product Search Ranking Via Large Language Models: An
  Empirical Study'
arxiv_id: '2409.17460'
source_url: https://arxiv.org/abs/2409.17460
tags:
- relevance
- ranking
- search
- content
- variant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of training e-commerce product
  search ranking models due to the absence of a gold standard for ranking relevance.
  The authors decompose ranking relevance into content-based and engagement-based
  aspects and propose leveraging Large Language Models (LLMs) for both label and feature
  generation in model training.
---

# Towards More Relevant Product Search Ranking Via Large Language Models: An Empirical Study

## Quick Facts
- arXiv ID: 2409.17460
- Source URL: https://arxiv.org/abs/2409.17460
- Reference count: 40
- Key outcome: The paper addresses the challenge of training e-commerce product search ranking models due to the absence of a gold standard for ranking relevance. The authors decompose ranking relevance into content-based and engagement-based aspects and propose leveraging Large Language Models (LLMs) for both label and feature generation in model training. Specifically, they introduce sigmoid transformations on LLM outputs to polarize relevance scores, enhancing the model's ability to balance content-based and engagement-based relevances. Comprehensive online tests and offline evaluations demonstrate that their approach significantly improves content relevance without compromising user engagement, validating the effectiveness of integrating LLMs into e-commerce product search ranking model training.

## Executive Summary
This paper tackles the challenge of training effective e-commerce product search ranking models in the absence of a clear gold standard for relevance. The authors propose decomposing ranking relevance into content-based and engagement-based components and leveraging Large Language Models (LLMs) for both label and feature generation. By applying sigmoid transformations to LLM-predicted content relevance scores, they polarize the scores to enhance the model's ability to prioritize highly relevant items. Comprehensive evaluations show significant improvements in content relevance without sacrificing user engagement metrics.

## Method Summary
The authors propose using LLMs to generate both labels and features for training ranking models in e-commerce search. They fine-tune a Mistral 7B model on human-labeled query-product relevance data to predict content relevance, then apply sigmoid transformations to these scores to polarize them. Additionally, they use a BERT-based cross-encoder to generate dense semantic features from query-product pairs. These components are integrated into a Learning-to-Rank framework, where the model is trained to optimize ranking metrics like NDCG. The approach is validated through offline and online evaluations, demonstrating improvements in content relevance while maintaining engagement.

## Key Results
- LLM-based labeling and feature generation significantly improves content-based ranking relevance (NDCG@10).
- Sigmoid transformations on LLM outputs enhance the model's ability to prioritize highly relevant products.
- The proposed approach maintains user engagement metrics (ATC@40) while improving content relevance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Applying sigmoid transformations to LLM-predicted content relevance scores enhances the model's ability to distinguish highly relevant products from moderately relevant ones, improving ranking precision.
- Mechanism: The sigmoid function compresses moderate content relevance scores toward extremes (near 0 or 1), while keeping high and low scores relatively stable. This polarization makes highly relevant items more distinguishable in the top ranking positions, where precision is critical.
- Core assumption: Products with high content relevance should be prioritized in top positions, and the sigmoid transformation can effectively amplify these differences without distorting the relative ordering of already high or low relevance items.
- Evidence anchors:
  - [abstract]: "Additionally, we introduce different sigmoid transformations on the LLM outputs to polarize relevance scores in labeling, enhancing the model's ability to balance content-based and engagement-based relevances and thus prioritize highly relevant items overall."
  - [section]: "The main rationale behind this transformation is that the sigmoid function can polarize those moderately low and high scores toward both extremes, effectively differentiating the content relevance gap between products."
- Break condition: If the original LLM score distribution is already polarized or if the content relevance threshold for "highly relevant" is poorly defined, the sigmoid transformation may over-amplify noise or create artificial distinctions.

### Mechanism 2
- Claim: Using fine-tuned LLMs to generate both labels and features for ranking model training significantly improves the model's predictive capability for content-based relevance.
- Mechanism: The LLM-generated labels provide high-quality supervision for content relevance, while the LLM-generated features (via cross-encoders) supply rich semantic representations that the ranking model can use during inference. This dual integration leverages the LLM's semantic understanding at both training and inference stages.
- Core assumption: LLMs can accurately predict content relevance and generate semantically rich features that capture query-product relationships better than traditional sparse features or rule-based methods.
- Evidence anchors:
  - [abstract]: "we propose to leverage Large Language Models (LLMs) for both label and feature generation in model training, primarily aiming to improve the model's predictive capability for content-based relevance."
  - [section]: "The use of LLMs in labeling also enhances the effectiveness of LLM-driven content features in the ranking model, promoting more content-relevant products to top positions."
- Break condition: If the LLM is not well fine-tuned on domain-specific relevance data, or if the latency constraints prevent the use of sufficiently complex models for feature generation, the benefits may be limited or outweighed by computational costs.

### Mechanism 3
- Claim: Decomposing ranking relevance into content-based and engagement-based components allows the model to balance intrinsic product quality with user interaction signals, improving overall ranking effectiveness.
- Mechanism: Content-based relevance acts as a quality filter ensuring that highly relevant products are promoted, while engagement-based relevance adjusts rankings within similar content relevance bands based on user behavior. This two-dimensional approach prevents low-quality but highly clicked items from dominating top positions.
- Core assumption: Users benefit from seeing products that are both semantically relevant to their query and have demonstrated positive engagement, and that these two signals can be effectively combined without one overwhelming the other.
- Evidence anchors:
  - [abstract]: "we decompose ranking relevance into content-based and engagement-based aspects, and we propose to leverage Large Language Models (LLMs) for both label and feature generation in model training, primarily aiming to improve the model's predictive capability for content-based relevance."
  - [section]: "In the context of e-commerce search, we decompose the ranking relevance into two major components: content-based relevance and engagement-based relevance."
- Break condition: If user engagement patterns are highly volatile or manipulated (e.g., by spam or popularity bias), relying on engagement as a secondary signal could degrade ranking quality, especially if content relevance is not sufficiently discriminative.

## Foundational Learning

- Concept: Learning-to-Rank (LTR) framework
  - Why needed here: The entire methodology relies on training ranking models using supervised learning approaches that optimize ranking metrics like NDCG. Understanding LTR is essential to grasp how labels, features, and loss functions interact.
  - Quick check question: What is the difference between pointwise, pairwise, and listwise approaches in LTR, and which one is used in this paper?

- Concept: Sigmoid transformation and its effect on score distributions
  - Why needed here: The sigmoid transformation is central to polarizing content relevance scores. Understanding how sigmoid functions reshape distributions is key to tuning the model's behavior.
  - Quick check question: How does changing the steepness (α) and center (β) parameters of a sigmoid function affect the output distribution?

- Concept: Cross-encoder vs. bi-encoder architectures for semantic matching
  - Why needed here: The paper uses a BERT-based cross-encoder to generate content features. Knowing the trade-offs between cross-encoders (high accuracy, high latency) and bi-encoders (lower accuracy, lower latency) is important for understanding feature generation choices.
  - Quick check question: Why might a cross-encoder be preferred for feature generation in an offline training pipeline, even if it's too slow for online inference?

## Architecture Onboarding

- Component map: Mistral 7B (fine-tuned) -> BERT cross-encoder (fine-tuned) -> XGBoost baseline -> Ranking model (trained with LTR) -> Evaluation pipeline
- Critical path:
  1. Fine-tune LLM on labeled data to predict content relevance.
  2. Apply sigmoid transformation to LLM outputs for label generation.
  3. Fine-tune BERT cross-encoder to generate dense content features.
  4. Train ranking model using transformed labels and features.
  5. Evaluate offline (NDCG) and online (ATC@40) to measure impact.
- Design tradeoffs:
  - Label generation uses a larger, more accurate LLM (Mistral 7B) since it's done offline.
  - Feature generation uses a smaller BERT model to balance accuracy and runtime latency.
  - Sigmoid transformation parameters (α, β) control the trade-off between content and engagement relevance; stricter content criteria improve content relevance but may hurt engagement.
- Failure signatures:
  - If sigmoid transformation is too aggressive, ranking may become overly rigid and ignore useful engagement signals.
  - If LLM is poorly fine-tuned, labels may be noisy, leading to degraded model performance.
  - If BERT cross-encoder is too small, generated features may lack semantic richness, limiting the model's ability to distinguish relevant products.
- First 3 experiments:
  1. Compare baseline vs. LLM-labeled model (without sigmoid) to isolate the effect of LLM labels.
  2. Compare different sigmoid parameter settings (e.g., σcLX vs. σrLX) to find the optimal balance between content and engagement.
  3. Run an ablation study removing cross-encoder features to measure their contribution to ranking performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different sigmoid transformation parameters (alpha and beta) affect the trade-off between content-based and engagement-based relevance in e-commerce search ranking?
- Basis in paper: [explicit] The paper discusses using sigmoid transformations on LLM outputs to polarize relevance scores and enhance the model's ability to balance content-based and engagement-based relevances.
- Why unresolved: While the paper tests several sigmoid transformations, it doesn't provide a comprehensive analysis of how varying alpha and beta parameters across a wider range would impact the trade-off.
- What evidence would resolve it: Systematic testing of multiple sigmoid transformation parameter combinations and their effects on both content and engagement metrics would provide insights into optimal parameter settings.

### Open Question 2
- Question: How does the size of the language model used for feature generation affect ranking performance and computational efficiency?
- Basis in paper: [explicit] The paper mentions using a moderate-size BERT-based model for feature generation due to runtime latency concerns, contrasting it with larger models used for label generation.
- Why unresolved: The paper doesn't explore how different model sizes for feature generation would impact performance and latency, leaving an open question about the optimal balance.
- What evidence would resolve it: Comparative experiments using language models of varying sizes for feature generation, measuring both ranking performance and inference latency, would clarify the trade-offs involved.

### Open Question 3
- Question: How generalizable are the proposed LLM-based approaches across different e-commerce platforms and product categories?
- Basis in paper: [inferred] The paper focuses on experiments conducted on Walmart.com data, but doesn't discuss the potential applicability of the methods to other e-commerce contexts.
- Why unresolved: The effectiveness of the proposed approaches might depend on the specific characteristics of the e-commerce platform and product categories involved, which isn't addressed in the current study.
- What evidence would resolve it: Applying the proposed methods to multiple e-commerce platforms with diverse product categories and comparing the results would demonstrate the generalizability of the approach.

## Limitations
- The approach's generalizability to other e-commerce domains is uncertain, as the study is based on data from a single retailer.
- The long-term effects of the ranking adjustments on user behavior and business metrics remain unexplored.
- The computational overhead of generating LLM-based labels and features at scale is not thoroughly analyzed.

## Confidence
- **High Confidence**: The core methodology of using LLMs for both label and feature generation in ranking model training is well-supported by empirical results.
- **Medium Confidence**: The effectiveness of the sigmoid transformation in polarizing relevance scores is demonstrated, but its optimal parameterization may vary across different datasets and domains.
- **Low Confidence**: The long-term impact on user behavior and the computational cost-benefit analysis are not sufficiently addressed.

## Next Checks
1. **Cross-Domain Validation**: Test the proposed approach on datasets from multiple e-commerce platforms to assess generalizability and identify domain-specific adjustments needed for optimal performance.
2. **Longitudinal Study**: Conduct a longitudinal analysis to measure the impact of the new ranking approach on user behavior over extended periods, including metrics such as return rates, customer satisfaction, and overall engagement patterns.
3. **Cost-Benefit Analysis**: Perform a detailed analysis of the computational costs associated with LLM-based label and feature generation, comparing the performance gains against the increased resource requirements to determine the practical viability of the approach at scale.