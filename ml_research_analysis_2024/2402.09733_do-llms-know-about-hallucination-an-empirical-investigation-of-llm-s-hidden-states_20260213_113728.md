---
ver: rpa2
title: Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden
  States
arxiv_id: '2402.09733'
source_url: https://arxiv.org/abs/2402.09733
tags:
- awareness
- hidden
- arxiv
- question
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether large language models (LLMs) are\
  \ aware of hallucination\u2014when they generate incorrect or nonsensical answers.\
  \ The authors introduce an experimental framework to compare the hidden states of\
  \ LLMs when they generate correct versus hallucinated responses."
---

# Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States

## Quick Facts
- arXiv ID: 2402.09733
- Source URL: https://arxiv.org/abs/2402.09733
- Authors: Hanyu Duan; Yi Yang; Kar Yan Tam
- Reference count: 11
- Primary result: LLMs exhibit different hidden state patterns when generating correct versus hallucinated answers, showing internal awareness of truthfulness

## Executive Summary
This paper investigates whether large language models possess awareness of hallucination by analyzing their hidden states during generation. The authors develop an experimental framework comparing hidden states when LLMs produce correct versus hallucinated responses, focusing on LLaMA family models. They discover that LLMs show distinct internal patterns for truthful versus fabricated answers, with the final hidden state being more influenced by correct responses. The study identifies two key directions in the hidden representation space that correspond to correct and hallucinated transitions, which could be leveraged for hallucination mitigation strategies.

## Method Summary
The research employs LLaMA-2 models (7B and 13B variants) on TruthfulQA and HaluEval datasets to analyze hidden state patterns. The experimental framework processes two parallel inputs - one with a hallucinated answer and one with a correct answer - extracting three key hidden states: s1 (after question comprehension), s2 (after hallucinated answer), and s3 (after correct answer). The analysis calculates awareness scores through cosine similarity differences and applies Principal Component Analysis to identify truthfulness directions in the hidden representation space. The methodology also tests mitigation strategies by manipulating hidden states using derived truthfulness directions.

## Key Results
- LLMs exhibit significantly different hidden state patterns when generating correct answers compared to hallucinated ones
- Including reference knowledge in inputs increases the LLM's awareness of hallucination
- Two distinct directions in the hidden representation space correspond to correct and hallucinated transitions
- The final hidden state is more influenced by correct answers than hallucinated ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs show different hidden state patterns when generating correct versus hallucinated answers
- Mechanism: When processing a correct answer, the LLM's final hidden state is more significantly influenced than when processing a fabricated one, indicating internal differentiation
- Core assumption: The hidden states encode meaningful representations of truthfulness that can be compared across different responses to the same question
- Evidence anchors:
  - [abstract]: "we find that LLMs do exhibit different hidden state patterns when producing correct answers compared to hallucinated ones"
  - [section 4.2]: "LLMs react differently when processing a genuine response versus a fabricated one"

### Mechanism 2
- Claim: Including reference knowledge raises LLM awareness of hallucination
- Mechanism: When external knowledge is provided alongside questions, the LLM's hidden states show greater differentiation between correct and hallucinated answers, suggesting increased awareness
- Core assumption: Reference knowledge acts as a grounding signal that the LLM can use to better evaluate answer correctness
- Evidence anchors:
  - [section 4.2]: "we observe that including reference knowledge in the input can raise its awareness consequently"
  - [abstract]: "including reference knowledge, strategically inducing hallucination, and manipulating confidence through prompting can increase the LLM's awareness of hallucination"

### Mechanism 3
- Claim: The transition between hidden states encodes truthfulness information
- Mechanism: By analyzing the vectors between initial question processing states and final answer states, two distinct directions emerge in the hidden representation space that correspond to correct versus hallucinated transitions
- Core assumption: Principal Component Analysis can identify meaningful directions in the high-dimensional hidden state space that represent truthfulness
- Evidence anchors:
  - [section 4.2]: "we observe that the majority of tokens are related to information truthfulness, with correct-related tokens corresponding to the correct direction and incorrect-relevant tokens associated with the hallucinated direction"
  - [abstract]: "we identify two key directions in the hidden representation space that correspond to correct and hallucinated transitions"

## Foundational Learning

- Concept: Hidden states in transformer models
  - Why needed here: The entire analysis depends on understanding what hidden states are and how they encode information throughout the generation process
  - Quick check question: What information does the final hidden state of a transformer model encode, and how does it influence the next token prediction?

- Concept: Principal Component Analysis (PCA)
  - Why needed here: PCA is used to identify the main directions in the hidden state space that correspond to correct versus hallucinated transitions
  - Quick check question: How does PCA identify principal components, and what do these components represent in the context of hidden state vectors?

- Concept: Cosine similarity as a measure of vector alignment
  - Why needed here: Cosine similarity is used to quantify how much the final hidden state is influenced by correct versus hallucinated answers
  - Quick check question: What does a high cosine similarity between two hidden state vectors indicate about their relationship?

## Architecture Onboarding

- Component map: Question -> s1 (question comprehension) -> s2 (after hallucinated answer) -> s3 (after correct answer) -> Final hidden state comparison and PCA analysis

- Critical path: 1) Prepare paired inputs with same question but different answers, 2) Extract s1, s2, and s3 from final transformer layer, 3) Calculate awareness metrics (cosine similarities and differences), 4) Apply PCA to identify truthfulness directions, 5) Test mitigation strategies using derived directions

- Design tradeoffs: Using only final hidden states simplifies analysis but may miss intermediate processing stages; focusing on QA pairs limits generalizability to other task types; requiring ground truth answers restricts application to supervised settings

- Failure signatures: If awareness scores don't differ significantly between models or datasets, if PCA directions don't align with expected truthfulness concepts, or if mitigation attempts don't improve answer accuracy, the core assumptions about hidden state representations may be invalid

- First 3 experiments:
  1. Replicate the basic awareness score calculation on a new dataset to verify the pattern differences
  2. Test the effect of different types of reference knowledge (structured vs unstructured) on awareness scores
  3. Apply the dcorr offset to a different model family to see if the mitigation approach generalizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the LLM's awareness of hallucination differ for factual versus non-factual hallucinated responses?
- Basis in paper: [inferred] The paper analyzes general hallucinated vs. correct responses but doesn't distinguish between types of hallucinations like factual fabrication vs. logical inconsistency
- Why unresolved: The experimental framework compares only correct vs. incorrect answers without categorizing hallucination types
- What evidence would resolve it: Controlled experiments comparing hidden state patterns for different hallucination categories using the same framework

### Open Question 2
- Question: How do hidden states in intermediate transformer layers contribute to hallucination awareness?
- Basis in paper: [explicit] The paper notes they only examine final hidden states and mentions that intermediate layers remain unexplored
- Why unresolved: The analysis focuses on final hidden states in the last transformer layer, while the role of intermediate layers in hallucination detection is not investigated
- What evidence would resolve it: Layer-by-layer analysis of hidden state transitions comparing hallucinated vs. correct responses across all transformer layers

### Open Question 3
- Question: Can the hallucination mitigation technique using correct direction offsets generalize to tasks beyond question answering?
- Basis in paper: [explicit] The case study only demonstrates mitigation on QA tasks, with a note about exploring other domains
- Why unresolved: The activation engineering approach is only validated on simple QA examples, with no testing on complex or domain-specific tasks
- What evidence would resolve it: Applying the same direction-offset technique to numerical reasoning, financial text comprehension, or multimodal tasks

## Limitations

- Findings based on LLaMA family models (7B and 13B variants) may not extend to larger models or different architectures
- Experimental framework relies on having ground truth answers, limiting applicability to open-domain scenarios
- PCA-based identification of truthfulness directions requires further validation across diverse datasets and model families

## Confidence

- **High Confidence**: The core finding that LLMs exhibit different hidden state patterns when generating correct versus hallucinated answers is well-supported by the experimental framework and statistical analysis
- **Medium Confidence**: The effectiveness of reference knowledge and confidence manipulation in raising hallucination awareness shows consistent patterns but may vary with different prompting strategies or knowledge formats
- **Medium Confidence**: The identification of two key directions in hidden representation space corresponding to truthfulness is methodologically sound but requires replication across more diverse model architectures

## Next Checks

1. Test the awareness score methodology on larger model families (e.g., GPT-3.5/4, Claude) to assess scalability and consistency of hidden state patterns across architectures

2. Evaluate the mitigation approach using the dcorr offset on a completely different model family (e.g., BLOOM or OPT) to verify generalization beyond LLaMA models

3. Conduct ablation studies removing reference knowledge to quantify the minimum information required for LLMs to develop awareness of hallucination, testing whether partial or noisy knowledge still produces the effect