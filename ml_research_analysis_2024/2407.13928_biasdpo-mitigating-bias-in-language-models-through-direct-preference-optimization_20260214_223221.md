---
ver: rpa2
title: 'BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization'
arxiv_id: '2407.13928'
source_url: https://arxiv.org/abs/2407.13928
tags:
- language
- bias
- biased
- biases
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses bias in large language models (LLMs) by introducing
  BiasDPO, a framework that uses Direct Preference Optimization (DPO) to reduce gender,
  racial, and religious biases in generated text. The approach trains models to prefer
  less biased completions by optimizing a loss function that rewards respectful language
  and penalizes biased or harmful outputs.
---

# BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization

## Quick Facts
- arXiv ID: 2407.13928
- Source URL: https://arxiv.org/abs/2407.13928
- Authors: Ahmed Allam
- Reference count: 6
- Primary result: BiasDPO significantly reduces gender, racial, and religious biases in Microsoft Phi-2, achieving up to 87% accuracy on racial bias tasks and lowering toxicity scores.

## Executive Summary
This study introduces BiasDPO, a framework that uses Direct Preference Optimization (DPO) to mitigate bias in large language models. The approach trains models to prefer less biased completions by optimizing a loss function that rewards respectful language and penalizes biased outputs. A manually curated dataset with over 1,100 prompt-completion pairs was developed to support training. When applied to Microsoft Phi-2, BiasDPO significantly reduced bias across multiple benchmarks, with the trained model outperforming the baseline on nearly all bias-related tasks while maintaining more neutral and respectful outputs.

## Method Summary
BiasDPO applies Direct Preference Optimization to reduce bias in LLMs by training them to prefer unbiased completions over biased ones. The framework uses a manually curated dataset containing prompts designed to elicit biased responses, paired with both biased and unbiased completions. The approach experiments with different DPO variants including IPO, SLiC, and KTO, each providing specific regularization to prevent overfitting while learning bias reduction patterns. The loss function maximizes log probability for less biased completions and minimizes it for biased ones, creating a preference for respectful language without requiring a separate reward model.

## Key Results
- BiasDPO achieved up to 87% accuracy on racial bias tasks using the BBQ benchmark
- The trained model showed significant reductions in toxicity scores on RealToxicityPrompts
- Qualitative analysis demonstrated more neutral, respectful outputs compared to the baseline model
- The approach outperformed baseline Phi-2 on nearly all bias benchmarks while maintaining general language capabilities

## Why This Works (Mechanism)

### Mechanism 1
Direct Preference Optimization reduces bias by directly shaping token probabilities based on human-labeled preferences. The model learns to maximize log probability for unbiased completions and minimize it for biased completions, creating a preference for respectful language without requiring a separate reward model. This works under the assumption that the preference dataset accurately captures what constitutes biased versus unbiased language across different contexts. The approach may fail if the preference dataset contains noisy labels or fails to cover diverse bias scenarios.

### Mechanism 2
Using multiple DPO variants (IPO, SLiC, KTO) provides regularization that prevents overfitting to the preference dataset. Each variant adds specific regularization terms - IPO controls likelihood ratio gaps, SLiC adds margin-based ranking loss, and KTO uses prospect theory-based utility functions to balance preference learning with KL divergence constraints. This assumes the regularization terms effectively prevent the model from memorizing preferences while still learning meaningful bias reduction patterns. The approach may fail if regularization is too strong, causing the model to fail learning meaningful bias reduction, or too weak, leading to overfitting.

### Mechanism 3
Training on a diverse dataset covering gender, racial, and religious biases across multiple contexts enables the model to generalize bias reduction to unseen scenarios. The dataset contains prompts designed to elicit biased responses, paired with both biased and unbiased completions, teaching the model to recognize and neutralize bias patterns. This assumes the dataset's coverage of different bias types and contexts is sufficient for the model to learn generalizable bias reduction strategies. The approach may fail if the dataset lacks coverage of certain bias types or contexts, causing the model to miss important bias patterns.

## Foundational Learning

- **Concept: Direct Preference Optimization**
  - Why needed here: DPO is the core mechanism that enables bias reduction without requiring complex RLHF pipelines
  - Quick check question: How does DPO differ from traditional RLHF in terms of computational complexity and training stability?

- **Concept: Preference dataset construction**
  - Why needed here: The quality and diversity of the preference dataset directly determines the model's ability to learn bias reduction
  - Quick check question: What are the key considerations when creating prompts that reliably elicit biased responses?

- **Concept: Regularization in preference learning**
  - Why needed here: Regularization prevents overfitting to the preference dataset while maintaining the model's ability to generate natural language
  - Quick check question: How do different regularization approaches (IPO, SLiC, KTO) balance preference learning with KL divergence constraints?

## Architecture Onboarding

- **Component map**: Dataset preparation -> DPO loss function selection -> Model training -> Bias benchmark evaluation
- **Critical path**: Dataset creation -> Model fine-tuning with DPO -> Benchmark evaluation -> Qualitative analysis
- **Design tradeoffs**: Dataset size vs. quality (larger datasets may introduce noise), regularization strength vs. learning effectiveness, computational cost vs. bias reduction performance
- **Failure signatures**: Overfitting to preference dataset (model performs well on training data but poorly on benchmarks), under-regularization (model generates unnatural language), insufficient dataset coverage (model fails on certain bias types)
- **First 3 experiments**:
  1. Test baseline Phi-2 on bias benchmarks to establish performance baseline
  2. Train Phi-2 with BiasDPO using IPO variant and β=0.01, evaluate on benchmarks
  3. Compare performance across different DPO variants (IPO, SLiC, KTO) with varying β values on BBQ benchmark

## Open Questions the Paper Calls Out

- How does BiasDPO scale to larger language models with billions of parameters, and does its effectiveness in reducing bias remain consistent as model size increases?
- What are the long-term societal impacts of deploying language models trained with BiasDPO, and how can potential unintended consequences of bias mitigation be identified and managed?
- How can BiasDPO be adapted to detect and mitigate intersectional biases (e.g., overlapping gender, racial, and religious biases) more effectively than single-axis bias approaches?

## Limitations

- Manual dataset construction introduces potential subjectivity in bias labeling and may not capture full complexity of real-world bias scenarios
- Evaluation focuses primarily on benchmark performance without addressing potential trade-offs in model capabilities
- Analysis is limited to a single base model (Phi-2), raising questions about generalizability across different architectures and sizes
- Long-term stability of bias reduction is not evaluated beyond immediate post-training performance

## Confidence

**High Confidence Claims:**
- Technical implementation of DPO variants follows established methodologies
- Benchmark results showing reduced bias scores are reliable within tested datasets
- Qualitative improvements in output language are clearly observable

**Medium Confidence Claims:**
- Generalization of bias reduction to unseen scenarios
- Optimal regularization strength for different DPO variants
- Long-term stability of bias mitigation

**Low Confidence Claims:**
- Performance on bias types not explicitly covered in training dataset
- Impact on model capabilities beyond bias-related tasks
- Scalability to larger models and real-world deployment scenarios

## Next Checks

1. **Cross-Model Validation**: Test BiasDPO on at least two additional base models of different sizes and architectures (e.g., Llama, Mistral) to verify generalizability and identify any model-specific limitations or requirements.

2. **Long-Term Stability Assessment**: Implement periodic re-evaluation of bias scores over extended periods (e.g., 30, 60, 90 days) to measure the durability of bias reduction and detect any regression in model behavior.

3. **Capability Trade-off Analysis**: Systematically evaluate the trained models on comprehensive language understanding benchmarks (e.g., GLUE, SuperGLUE) and creative writing tasks to quantify any degradation in general language capabilities and establish performance boundaries.