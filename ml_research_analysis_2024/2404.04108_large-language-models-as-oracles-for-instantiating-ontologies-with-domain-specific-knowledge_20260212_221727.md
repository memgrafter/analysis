---
ver: rpa2
title: Large language models as oracles for instantiating ontologies with domain-specific
  knowledge
arxiv_id: '2404.04108'
source_url: https://arxiv.org/abs/2404.04108
tags:
- ontology
- which
- class
- knowledge
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KGF iller automates ontology population by treating LLM as oracles,
  querying them for instances, relationships, and best-class assignments without requiring
  training data. Starting from an ontology schema and query templates, it generates
  domain-specific knowledge while maintaining structural compliance.
---

# Large language models as oracles for instantiating ontologies with domain-specific knowledge

## Quick Facts
- arXiv ID: 2404.04108
- Source URL: https://arxiv.org/abs/2404.04108
- Reference count: 40
- KGFiller achieves up to 5× higher quality and 10× fewer errors compared to state-of-the-art methods for ontology population.

## Executive Summary
KGFiller automates ontology population by treating large language models (LLMs) as oracles to generate domain-specific instances and relationships without requiring training data. The system queries LLMs using structured templates to produce individuals and relations for ontology classes, then iteratively refines and merges results while maintaining schema compliance. Tested on a food ontology with eight different LLMs, KGFiller achieved 92.4% valid instances/relations with GPT-3.5 Turbo and demonstrated that open-source models can achieve comparable performance.

## Method Summary
KGFiller populates ontologies by querying LLMs as oracles through four iterative phases: population (generating individuals for classes), relation (adding properties between individuals), redistribution (optimizing class assignments), and merge (removing duplicates). Starting from an ontology schema and query templates, the system sends domain-specific prompts to LLMs, extracts structured responses, and processes them through the phases to produce populated ontologies. The method uses caching, exponential backoff, and token constraints to manage costs and rate limits while maintaining output quality.

## Key Results
- KGFiller achieved 92.4% valid instances/relations using GPT-3.5 Turbo, up to 5× higher quality than state-of-the-art methods
- Open-source model Nous Hermes reached 85.1% valid instances/relations, demonstrating viability beyond commercial APIs
- Generated ontologies contained 10× fewer errors compared to existing approaches while maintaining structural compliance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLM can serve as an oracle to generate ontology instances without requiring additional training data.
- **Mechanism**: The LLM is queried using domain-specific templates that prompt it to produce instances and relations for classes defined in the ontology schema. The model's pretraining on web-scale data enables it to generate plausible domain knowledge.
- **Core assumption**: LLM pretraining covers sufficient domain knowledge to be useful for ontology instantiation.
- **Evidence anchors**:
  - [abstract] "we hypothesise that these models encapsulate a substantial amount of domain-specific knowledge" and "queries the LLM multiple times, and generates instances for both classes and properties from its replies"
  - [section 2.3] "LLMs are trained on various data from the entire web" and "LLMs may be exploited as oracles for information retrieval tasks"
  - [corpus] Weak - neighbor papers focus on knowledge graph construction but don't directly test this mechanism; no experimental evidence in corpus for ontology instance generation.
- **Break condition**: If LLM knowledge coverage is insufficient for the domain, generated instances will be irrelevant or hallucinated.

### Mechanism 2
- **Claim**: Structured prompts with domain context improve relevance and correctness of LLM-generated ontology content.
- **Mechanism**: Templates are engineered to include context like "zoological context" and use concise, name-only output requests to guide LLM responses into structured, relevant forms.
- **Core assumption**: Explicit contextual cues and formatting constraints in prompts reduce ambiguity and improve response quality.
- **Evidence anchors**:
  - [section 4.2] "We define the set of individual seeking templates... '(instances|examples) list for class ⟨class⟩(, names only)?'"
  - [appendix A.2.1] "we observe that complementing the query with the 'names only' requirement, induces the LLM to generate a very compact list of names"
  - [corpus] Moderate - neighbor paper "From Prompt to Graph" explicitly compares prompt strategies for ontology development, supporting this approach.
- **Break condition**: If LLM ignores prompt constraints, responses remain unstructured and require complex post-processing.

### Mechanism 3
- **Claim**: Iterative refinement phases (population, relation, redistribution, merge) ensure ontology consistency while leveraging LLM outputs.
- **Mechanism**: Each phase builds on previous outputs: population generates individuals, relation adds properties, redistribution optimizes class assignments, merge removes duplicates. This sequence maintains schema compliance.
- **Core assumption**: Sequential processing allows error correction at each stage before final output.
- **Evidence anchors**:
  - [section 3] "phases follow this order of execution, as the population phase generates individuals that are then used in the relation phase"
  - [section 4.5] "our results also show that relying on closed source models does not represent the only viable approach, as few open source models achieve similarly acceptable level of performance"
  - [corpus] Weak - neighbor papers don't describe multi-phase refinement; corpus lacks evidence for this specific sequential approach.
- **Break condition**: If early phases generate too many errors, later phases cannot correct fundamental inconsistencies.

## Foundational Learning

- **Concept**: Description Logic (ALC) syntax for ontologies
  - Why needed here: KGFiller assumes ontologies follow ALC semantics for class definitions, properties, and instance assignments.
  - Quick check question: Can you identify the difference between TBox (terminological) and ABox (assertion) axioms in an ALC ontology?

- **Concept**: LLM temperature parameter and its effect on output randomness
  - Why needed here: Temperature controls creativity vs. determinism in LLM responses, critical for balancing novelty and accuracy in generated ontology content.
  - Quick check question: What temperature value would you use when you need consistent, non-hallucinated responses for ontology merging?

- **Concept**: Exponential backoff strategy for API rate limiting
  - Why needed here: LLM services impose rate limits; exponential backoff prevents query failures while maximizing throughput.
  - Quick check question: If initial delay is 30s and backoff factor is 150%, what's the delay for the third retry attempt?

## Architecture Onboarding

- **Component map**: Input ontology schema + query templates -> KGFiller algorithm (Population -> Relation -> Redistribution -> Merge) -> LLM API service (with caching and rate limiting) -> Output populated ontology + performance metrics

- **Critical path**: Population -> Relation -> Redistribution -> Merge, with each phase depending on successful completion of previous phases and LLM query responses.

- **Design tradeoffs**:
  - Template complexity vs. response parsing complexity: Simpler templates require more sophisticated parsing logic
  - Query frequency vs. cost: More queries improve coverage but increase financial cost
  - Temperature setting vs. hallucination risk: Lower temperature reduces errors but may limit diversity

- **Failure signatures**:
  - Empty responses from LLM indicate rate limiting or context misunderstanding
  - Excessive meaningless individuals suggest temperature too high or poor template design
  - Misplaced instances indicate redistribution phase not working correctly
  - High duplicate rates suggest syntactic similarity threshold too permissive

- **First 3 experiments**:
  1. Run KGFiller on a minimal ontology (1-2 classes, 1 property) with GPT-3.5 Turbo to verify basic functionality and measure Q metric
  2. Test different temperature settings (0.0, 0.5, 1.0) on the same ontology to observe hallucination rates
  3. Compare open-source vs. closed-source LLM performance using identical templates and ontology to validate generality claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of KGFiller scale with ontology size and complexity, particularly for ontologies with thousands of classes and properties?
- Basis in paper: [inferred] The paper focuses on a relatively small ontology (54 classes) and does not explore performance implications for larger ontologies.
- Why unresolved: The authors only test KGFiller on a single case study ontology, making it unclear how the method would perform with significantly larger and more complex ontologies.
- What evidence would resolve it: Experiments testing KGFiller on ontologies of varying sizes and complexities, measuring metrics like execution time, query volume, and error rates.

### Open Question 2
- Question: What is the optimal temperature setting for different phases of the KGFiller algorithm to balance creativity and accuracy?
- Basis in paper: [explicit] The paper mentions temperature is set to 0.0 during the merging phase but does not explore optimal settings for other phases.
- Why unresolved: The authors do not investigate how temperature affects the quality of generated instances and relations across different phases, leaving the optimal settings unclear.
- What evidence would resolve it: Systematic experiments varying temperature settings across different phases and measuring their impact on error rates and ontology quality.

### Open Question 3
- Question: How does KGFiller's performance compare to human experts in ontology population tasks?
- Basis in paper: [inferred] The paper presents KGFiller as an automated alternative to manual ontology population but does not directly compare its results to human-generated ontologies.
- Why unresolved: Without a comparison to human experts, it's unclear whether KGFiller can match or exceed human performance in terms of accuracy, completeness, and domain knowledge representation.
- What evidence would resolve it: A comparative study where human experts and KGFiller populate the same ontology, with their results evaluated by domain experts or using established ontology quality metrics.

## Limitations

- Evaluation limited to food domain ontologies, leaving generalizability to other domains uncertain
- No completeness guarantees - cannot determine if generated knowledge represents full domain space
- Hallucination risks persist due to LLM training on web data, though filtering mechanisms reduce visible errors

## Confidence

- **High confidence**: The core mechanism of using LLM as oracle for ontology population works reliably, supported by direct experimental results showing 5× quality improvement over state-of-the-art methods.
- **Medium confidence**: Performance generalizes across different LLM providers (open-source and closed-source), though absolute numbers vary and domain transfer remains unproven.
- **Low confidence**: Claims about completeness and bias mitigation lack supporting evidence; the method provides no guarantees about capturing all relevant domain knowledge.

## Next Checks

1. Test KGFiller on biomedical ontologies (e.g., SNOMED CT subsets) to verify domain transferability beyond food knowledge.
2. Implement a completeness metric by comparing generated instances against manually curated gold-standard ontologies to quantify knowledge gaps.
3. Conduct a bias audit by analyzing generated content for demographic, cultural, or geographic biases present in the web training data.