---
ver: rpa2
title: 'ConQRet: Benchmarking Fine-Grained Evaluation of Retrieval Augmented Argumentation
  with LLM Judges'
arxiv_id: '2412.05206'
source_url: https://arxiv.org/abs/2412.05206
tags:
- argument
- documents
- relevance
- context
- topic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ConQRet, a new benchmark for retrieval-augmented
  argumentation, and validates LLM-based judges for evaluating complex, evidence-grounded
  arguments. ConQRet contains 6,500+ documents and 98 controversial topics with human-authored
  pro/con arguments.
---

# ConQRet: Benchmarking Fine-Grained Evaluation of Retrieval Augmented Argumentation with LLM Judges

## Quick Facts
- arXiv ID: 2412.05206
- Source URL: https://arxiv.org/abs/2412.05206
- Reference count: 40
- Introduces ConQRet benchmark with 6,500+ documents and 98 controversial topics, validating fine-grained LLM judges for RAArg evaluation

## Executive Summary
This paper introduces ConQRet, a new benchmark for retrieval-augmented argumentation, and validates LLM-based judges for evaluating complex, evidence-grounded arguments. ConQRet contains 6,500+ documents and 98 controversial topics with human-authored pro/con arguments. The authors propose multiple fine-grained LLM judges that assess context relevance, argument groundedness, and quality across 15 dimensions, showing higher agreement with expert annotators than prior methods. Fine-grained metrics outperform single-score approaches, maintaining sensitivity to irrelevant content and hallucination levels. These LLM judges enable interpretable, automated evaluation of RAArg systems, advancing research on evidence-based argumentation and retrieval-augmented generation.

## Method Summary
The authors construct ConQRet from ProCon.org, extracting 98 controversial topics with human-authored arguments and associated web documents. They implement a reference RAArg system using BM25 retrieval with LLM reranking and few-shot GPT-4o-mini argument generation. Multiple LLM judge variants are evaluated for context relevance, groundedness, and argument quality across 15 dimensions. The judges are tested on both ConQRet and the ArgQuality corpus, with agreement measured using Krippendorff's alpha. Self-consistency is employed by varying dimension orderings to improve reliability.

## Key Results
- Fine-grained LLM judges achieve higher agreement with human annotators (Krippendorff's α ≈0.4-0.5) than single-score methods (α ≈0.1-0.2)
- Document-level context relevance assessment shows better sensitivity to irrelevant content than corpus-level approaches
- Self-consistency through multiple dimension orderings improves overall quality agreement with human experts
- LLM judges maintain reliable context relevance estimation but struggle with fine-grained levels of irrelevant content and hallucinations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-grained LLM judges provide more interpretable and actionable evaluations than single-score metrics.
- **Mechanism:** By evaluating each dimension separately (e.g., cogency, clarity, groundedness), the system can pinpoint specific strengths and weaknesses in arguments rather than providing a single aggregated score that obscures underlying issues.
- **Core assumption:** Each dimension of argument quality can be reliably assessed independently by LLMs.
- **Evidence anchors:** [abstract] "Our proposed LLM Judges and the ConQRet benchmark can enable rapid progress in computational argumentation and can be naturally extended to other complex retrieval-augmented generation tasks." [section] "We find that when the LLM Judges are prompted to generate the individual RAG metrics before providing judging argument quality, they demonstrate better argument quality prediction."
- **Break condition:** If dimensions are highly correlated or interdependent, fine-grained evaluation may not add meaningful information beyond a single aggregated score.

### Mechanism 2
- **Claim:** Retrieval-augmented argumentation benefits from context relevance evaluation at the document level rather than corpus level.
- **Mechanism:** By assessing each document's relevance individually, the system can identify and filter out irrelevant documents that might otherwise dilute the quality of the generated argument, improving both relevance and groundedness.
- **Core assumption:** Individual document relevance scores can be aggregated meaningfully to assess overall retrieval quality.
- **Evidence anchors:** [abstract] "To validate the proposed techniques, we introduce ConQRet, a new benchmark featuring long and complex human-authored arguments on debated topics, grounded in real-world websites" [section] "We find that while most metrics decrease with increasing irrelevant context, the decrease is not fully monotonic. The RAG-Direct and RAG-Rubric metrics display a near-perfect monotonic decrease (Pearson ρ ≈-1)"
- **Break condition:** If document-level relevance assessment is too computationally expensive or if documents are too interdependent, corpus-level evaluation might be more practical.

### Mechanism 3
- **Claim:** Self-consistency across multiple prompt orderings improves agreement with human annotations.
- **Mechanism:** By running the same evaluation with different orderings of quality dimensions and taking the mean, the system reduces variance introduced by prompt ordering effects and improves reliability.
- **Core assumption:** Different orderings of dimensions do not fundamentally change the evaluation but only introduce random noise.
- **Evidence anchors:** [section] "We find that there is substantial variability when the order of the dimensions is changed... we use a combined mean rating for each dimension, and see that there is high agreement in both the overall quality and the average quality" [section] "We employ a self-consistency (Wang et al., 2023) style listwise prompt (Listwise+SC) and compute the mean across three different orders"
- **Break condition:** If the variance between orderings is systematic rather than random, averaging may not improve reliability.

## Foundational Learning

- **Concept:** Krippendorff's alpha for inter-annotator agreement
  - Why needed here: The paper uses Krippendorff's alpha to quantify agreement between LLM judges and human experts across multiple dimensions, which is essential for validating the reliability of automated evaluations.
  - Quick check question: If two annotators assign ratings of (3,3,2,3) and (3,2,2,3) on a 3-point scale for four items, what would Krippendorff's alpha be approximately?

- **Concept:** Listwise vs. pointwise evaluation strategies
  - Why needed here: The paper compares listwise approaches (evaluating all dimensions in one prompt) against pointwise approaches (separate prompts for each dimension) to determine which yields better agreement with human judgments.
  - Quick check question: If evaluating 15 dimensions with a 1000-token context limit, would you prefer 15 separate prompts or one combined prompt? Why?

- **Concept:** Self-consistency as a variance reduction technique
  - Why needed here: The paper employs self-consistency by running evaluations with different dimension orderings to reduce variance and improve agreement with human annotations.
  - Quick check question: If running three different orderings yields scores of (0.4, 0.5, 0.45) for overall quality, what would the self-consistent score be?

## Architecture Onboarding

- **Component map:** Retrieval (BM25 + LLM reranking) -> Argument Generator (few-shot GPT-4o-mini) -> LLM Judges (multiple variants) -> Benchmark (ConQRet)
- **Critical path:** 1. Retrieve documents for topic + stance using BM25 + LLM reranking 2. Generate argument using few-shot prompting with retrieved documents 3. Evaluate using LLM judges across multiple metrics 4. Compare LLM evaluations with human annotations
- **Design tradeoffs:** Single-score vs. fine-grained metrics (simpler vs. more interpretable); Document-level vs. corpus-level context relevance (granular vs. faster); Few-shot vs. zero-shot prompting (better results vs. more scalable)
- **Failure signatures:** LLM judges failing to generate valid JSON output; Context relevance scores not decreasing with increasing irrelevant content; Hallucination detection failing to identify contradictory content; Self-consistency showing high variance between orderings
- **First 3 experiments:** 1. Compare agreement between GPT-3.5 and GPT-4o using listwise evaluation on ArgQuality corpus 2. Test context relevance sensitivity by injecting 0%, 10%, 50%, and 70% irrelevant documents 3. Evaluate hallucination detection by injecting contradictory sentences at 0%, 16%, and 63% of argument length

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt variations (Direct, Fine-grained, G-Eval, Query-Rubric, RAG-Rubric, RAG-Direct, Listwise+RAG Fine-Grained) affect the sensitivity and interpretability of context relevance predictions in retrieval-augmented argumentation systems?
- Basis in paper: [explicit] The paper introduces and compares multiple prompt variations for evaluating context relevance, noting that fine-grained metrics show better degradation with increasing irrelevant context than single-score metrics.
- Why unresolved: While the paper demonstrates that fine-grained metrics are more sensitive to irrelevant context, it does not fully explore the trade-offs between prompt complexity, computational cost, and evaluation accuracy across different scenarios.
- What evidence would resolve it: Comparative studies evaluating the performance of each prompt variation across diverse datasets and argumentation tasks, measuring both sensitivity to irrelevant content and computational efficiency.

### Open Question 2
- Question: To what extent do self-consistency (SC) and varying the order of argumentation quality dimensions improve the agreement between LLM judges and human expert annotations?
- Basis in paper: [explicit] The paper finds that using self-consistency and varying the order of dimensions improves overall quality agreement with human experts, but also notes substantial variability when the order is changed.
- Why unresolved: The paper shows improvements with SC and dimension reordering but does not quantify the optimal number of SC samples or the impact of different dimension orderings on specific quality metrics.
- What evidence would resolve it: Systematic experiments varying the number of SC samples and testing different dimension orderings, measuring their impact on inter-annotator agreement across all 15 quality dimensions.

### Open Question 3
- Question: How reliable are LLM judges in distinguishing fine-grained levels of irrelevant content and hallucinated arguments in complex, evidence-grounded tasks like computational argumentation?
- Basis in paper: [explicit] The paper observes that LLM judges can reliably estimate context relevance but struggle to distinguish finer levels of irrelevant evidence or hallucinated arguments, especially at higher levels of noise.
- Why unresolved: While the paper demonstrates the limitations of LLM judges at coarse levels of noise, it does not explore whether these limitations are due to model capacity, prompt design, or inherent challenges in detecting subtle inconsistencies.
- What evidence would resolve it: Experiments testing LLM judges on datasets with controlled, fine-grained variations in irrelevant content and hallucinations, using both larger models and alternative prompt strategies to assess improvements in sensitivity.

## Limitations
- Data provenance uncertainty: ConQRet construction details incomplete, limiting reproducibility
- Domain generalizability unclear: Validation limited to controversial topic argumentation
- Computational overhead unaddressed: Fine-grained evaluation cost-benefit tradeoff not analyzed

## Confidence

**High confidence** in fine-grained LLM judges showing higher agreement with human annotations than single-score methods (Krippendorff's α improvements from ~0.1-0.2 to ~0.4-0.5)

**Medium confidence** in mechanism claims about self-consistency improving reliability and document-level relevance assessment being superior to corpus-level

**Low confidence** in scalability claims due to unanalyzed computational requirements

## Next Checks

1. **Cross-domain validation**: Test the LLM judge framework on non-controversial argumentation domains (e.g., scientific papers, legal briefs) to assess generalizability beyond debated topics.

2. **Cost-benefit analysis**: Measure the computational overhead of fine-grained evaluation versus single-score approaches across different model sizes, and determine the breakeven point where improved accuracy justifies additional cost.

3. **Human-in-the-loop comparison**: Conduct user studies where human experts use fine-grained versus single-score feedback to improve RAArg systems, measuring whether the additional interpretability translates to better system development outcomes.