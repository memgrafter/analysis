---
ver: rpa2
title: Layer-Condensed KV Cache for Efficient Inference of Large Language Models
arxiv_id: '2405.10637'
source_url: https://arxiv.org/abs/2405.10637
tags:
- layers
- training
- tokens
- throughput
- computation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method to reduce the memory consumption
  and improve the throughput of large language models by reducing the number of layers
  whose keys and values need to be computed and cached. The key idea is to pair the
  queries of all layers with keys and values of only the top layer, so that we do
  not have to cache or even compute keys and values for layers other than the top
  layer.
---

# Layer-Condensed KV Cache for Efficient Inference of Large Language Models

## Quick Facts
- arXiv ID: 2405.10637
- Source URL: https://arxiv.org/abs/2405.10637
- Authors: Haoyi Wu; Kewei Tu
- Reference count: 34
- Reduces KV cache memory consumption by up to 90% with minimal performance degradation

## Executive Summary
This paper introduces Layer-Condensed KV Cache (LCKV), a novel method to significantly reduce memory consumption and improve inference throughput for large language models. The key innovation is pairing queries from all layers with keys and values computed only by the top layer, eliminating the need to compute or cache KVs for lower layers. This approach achieves up to 90% memory reduction while maintaining competitive performance, and can be integrated with other memory-saving techniques like StreamingLLM for further improvements.

## Method Summary
The LCKV method computes and caches keys and values only for the top layer of a transformer, while lower layers use these top-layer KVs directly. This is achieved by masking the diagonal of the attention matrix to break cyclic dependencies, allowing tokens to attend only to other tokens' top-layer representations. The approach requires iterative parallel training with gradient stopping and KV convergence approximation, but enables faster inference with significantly reduced memory footprint. A "sandwich" configuration with warmup layers placed both at the bottom and top layers helps maintain model performance.

## Key Results
- Achieves up to 90% reduction in KV cache memory consumption across multiple model sizes
- Improves inference throughput by 2.5x to 3x compared to standard transformers
- Maintains competitive performance on language modeling and downstream tasks with negligible degradation
- Successfully integrates with StreamingLLM for additional latency and memory savings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Queries from all layers can share keys and values from only the top layer, reducing memory consumption.
- Mechanism: Masking the diagonal of the attention matrix eliminates self-attention at lower layers, allowing them to use top-layer KVs instead. This breaks cyclic dependencies and eliminates the need to compute/store KVs for lower layers.
- Core assumption: Top-layer representations contain sufficient information for lower layers to function effectively.
- Evidence anchors:
  - [abstract] "queries of all layers are paired with keys and values of only the top layer"
  - [section 2.1] "we pair the queries of all layers with keys and values of only the top layer"

### Mechanism 2
- Claim: Parallel training is possible through iterative bottom-up computations over all tokens simultaneously.
- Mechanism: Instead of sequential token-by-token training, the model performs n iterations of parallel computations where each iteration pairs queries with KVs from the previous iteration, breaking sequential dependencies within each iteration.
- Core assumption: Computation graph equivalence holds when KVs converge quickly.
- Evidence anchors:
  - [section 2.2.1] "Theorem 1. The two computation graphs are equivalent in terms of model training."
  - [section 2.2.3] "KV converge very fast over iterations"

### Mechanism 3
- Claim: Integration with StreamingLLM further reduces latency and memory consumption.
- Mechanism: StreamingLLM's attention sink strategy stores only initial and recent tokens' KVs. Combining this with layer-condensed KV cache further reduces the number of layers needing KV computation.
- Core assumption: Layer-condensed approach is orthogonal to StreamingLLM's token pruning strategy.
- Evidence anchors:
  - [section 3.3] "our method is orthogonal to other memory-saving techniques"
  - [section 3.3] "the integration of StreamingLLM and our model... achieves lower latency and memory consumption"

## Foundational Learning

- Concept: Attention mechanism in transformers
  - Why needed here: Understanding how queries attend to keys/values is fundamental to grasping why reducing layers works
  - Quick check question: What happens if a token cannot attend to itself in the attention mechanism?

- Concept: KV cache and its memory implications
  - Why needed here: The paper's core contribution is reducing KV cache memory consumption
  - Quick check question: How does KV cache size scale with sequence length and number of layers?

- Concept: Residual connections in transformers
  - Why needed here: Explains how information flows through layers even when diagonal attention is masked
  - Quick check question: What role do residual connections play when self-attention is removed?

## Architecture Onboarding

- Component map:
  Input tokens -> Embedding layer -> Stacked transformer layers (only top layer computes KVs) -> Output logits

- Critical path:
  1. Forward pass: Compute top-layer KVs, then use them for all layers
  2. Training: Iterative parallel processing for m+b iterations
  3. Inference: Iterative prompt encoding (m+b iterations), then standard autoregressive generation

- Design tradeoffs:
  - Memory vs. Performance: Fewer KV layers save memory but may reduce accuracy
  - Training speed vs. Inference speed: Iterative training is slower, but inference is faster
  - Simplicity vs. Optimality: Sandwich configuration (top+w/2 and bottom+w/2 layers) balances performance and efficiency

- Failure signatures:
  - Performance degradation: Likely due to insufficient warmup layers or poor KV convergence
  - Training instability: Could indicate incorrect gradient stopping or insufficient iterations
  - Memory issues: May suggest incorrect layer configuration or integration problems with StreamingLLM

- First 3 experiments:
  1. Verify KV convergence: Test mean squared error of KVs over iterations on random and trained models
  2. Measure throughput improvement: Compare batch sizes and generation speed with standard transformers
  3. Test warmup layer configuration: Experiment with different placements and numbers of warmup layers to find optimal performance

## Open Questions the Paper Calls Out
None

## Limitations
- Requires careful configuration of warmup layers to maintain performance, with optimal configuration being non-trivial to determine
- Iterative training process is computationally intensive, potentially offsetting some memory savings during training
- Effectiveness depends on the assumption that top-layer KVs contain sufficient information for lower layers

## Confidence

*High Confidence Claims:*
- Theoretical equivalence between standard and parallel training computation graphs (Theorem 1)
- Empirical demonstration of memory reduction and throughput improvement across multiple model sizes
- Orthogonality of LCKV to other memory-saving techniques like StreamingLLM

*Medium Confidence Claims:*
- Performance trade-offs with different warmup layer configurations
- Generalization of results to larger models beyond the 1.1B parameter model tested
- KV convergence rates across different model architectures

*Low Confidence Claims:*
- Long-term stability in production environments
- Impact on model interpretability when self-attention is modified
- Performance on specialized tasks requiring deep attention across all layers

## Next Checks

1. **KV Convergence Validation**: Implement mean squared error monitoring of KVs over training iterations on multiple model architectures to verify claimed rapid convergence across different settings.

2. **Warmup Layer Optimization**: Systematically evaluate performance impact of different warmup layer configurations (top-only, bottom-only, sandwich with varying w values) on downstream task performance to identify optimal placement strategies.

3. **Production Integration Testing**: Deploy LCKV in a production inference pipeline with varying sequence lengths and batch sizes to measure real-world latency improvements and identify edge cases where the method may fail.