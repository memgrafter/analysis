---
ver: rpa2
title: 'LAST: Language Model Aware Speech Tokenization'
arxiv_id: '2409.03701'
source_url: https://arxiv.org/abs/2409.03701
tags:
- speech
- arxiv
- language
- last
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LAST, a language model aware speech tokenization
  method for spoken language modeling. LAST integrates pre-trained text language models
  into the speech tokenization process by employing a learnable adapter-quantization
  module that converts contextualized speech representations into discrete tokens
  guided by the text LM.
---

# LAST: Language Model Aware Speech Tokenization

## Quick Facts
- arXiv ID: 2409.03701
- Source URL: https://arxiv.org/abs/2409.03701
- Reference count: 0
- This paper introduces LAST, a language model aware speech tokenization method that integrates pre-trained text LMs into speech tokenization.

## Executive Summary
This paper presents LAST (Language Model Aware Speech Tokenization), a novel approach that leverages pre-trained text language models to guide the speech tokenization process. The method employs a learnable adapter-quantization module that converts contextualized speech representations into discrete tokens, optimizing them for language modeling objectives. By integrating frozen pre-trained speech and text models, LAST demonstrates superior performance on zero-resource speech modeling tasks and automatic speech recognition compared to traditional k-means tokenization baselines.

## Method Summary
LAST integrates pre-trained text language models into speech tokenization by employing a learnable adapter-quantization module that converts contextualized speech representations into discrete tokens guided by the text LM. The approach leverages a frozen pre-trained speech encoder (HuBERT) and a frozen pre-trained text LM (OPT) to jointly optimize the speech tokenizer and unit language model. The method is evaluated on zero-resource speech modeling tasks and automatic speech recognition, demonstrating superior performance compared to the traditional k-means tokenizer baseline.

## Key Results
- LAST achieves improvements in sWUGGY and sBLIMP metrics for sequence modeling compared to k-means baselines
- The method demonstrates lower Word Error Rate (WER) for speech recognition tasks
- Performance is optimized with 500 tokens in the codebook for speech modeling tasks

## Why This Works (Mechanism)

### Mechanism 1
Integrating frozen pre-trained text LM during tokenization aligns speech units with language modeling objectives. The LAST architecture feeds quantized speech tokens into a frozen text LM and backpropagates gradients only through the speech-side adapter and VQ components. This forces the speech encoder and quantization to produce tokens that maximize LM likelihood, implicitly optimizing for sequential coherence.

### Mechanism 2
Using a frozen text LM preserves its original language modeling capabilities while extending to speech. The text LM is kept frozen; only the adapter before/after the LM and the VQ codebook are trained. This avoids catastrophic forgetting of text knowledge while still learning speech tokens that are compatible with the LM's internal representations.

### Mechanism 3
Joint training of tokenizer and LM via LM-aware loss improves downstream speech modeling metrics compared to independent k-means clustering. The loss function combines LM likelihood (for sequence modeling) with reconstruction loss (for stability), training the tokenizer end-to-end with the downstream LM usage in mind.

## Foundational Learning

- Self-Supervised Learning (SSL) for speech representations
  - Why needed here: HuBERT provides contextualized speech features that capture phonetic and semantic structure without labels, serving as the starting point for tokenization
  - Quick check question: What layer of HuBERT is used to extract features for tokenization, and why might that choice matter?

- Vector Quantization (VQ) and codebook design
  - Why needed here: VQ discretizes continuous speech features into a finite set of tokens; the codebook size directly impacts token granularity and downstream LM performance
  - Quick check question: How does varying the number of codebook entries (e.g., 100 vs. 500 vs. 1000) affect zero-resource speech metrics?

- Adapter modules for modality bridging
  - Why needed here: The adapter transforms speech encoder outputs into the embedding space expected by the text LM, enabling gradient flow and LM-guided tokenization
  - Quick check question: What is the role of the projection layers in the adapter, and why are they necessary before feeding tokens into the LM?

## Architecture Onboarding

- Component map: Pre-trained frozen speech encoder (HuBERT-base) → Adapter (transformer + projection) → Vector Quantizer → Discrete tokens → Adapter (projection) → Frozen pre-trained text LM (OPT) → Loss (LM NLL + reconstruction)
- Critical path: Speech → Adapter → VQ → Adapter → LM → loss backprop (stop at LM)
- Design tradeoffs:
  - Adapter size vs. computational cost: More layers may improve alignment but increase training time
  - Codebook size vs. token ambiguity: Larger vocabularies reduce ambiguity but may increase sparsity and training instability
  - LM size vs. tokenization quality: Larger LMs may provide richer gradients but at higher compute cost
- Failure signatures:
  - Token collapse (all tokens map to one or few codes): Indicates reconstruction loss too weak or gradients overwhelming
  - Poor zero-resource metrics: Suggests misalignment between speech features and LM embedding space
  - High WER but good ABX: Implies phonemic detail preserved but linguistic content disrupted
- First 3 experiments:
  1. Vary codebook size (100, 500, 1000) while keeping all else fixed; measure sWUGGY/sBLIMP
  2. Remove reconstruction loss; observe if token collapse occurs or if LM metrics improve
  3. Replace adapter with identity (no transformation); check if LM gradients still improve tokenization quality

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of LAST compare to traditional k-means tokenization methods when evaluated on speech synthesis tasks beyond the ones tested in this paper? The paper evaluates LAST on zero-resource speech modeling tasks and automatic speech recognition, but does not explicitly test speech synthesis tasks.

### Open Question 2
What is the impact of using different pre-trained speech encoders, other than HuBERT, on the performance of LAST? The paper uses HuBERT as the pre-trained speech encoder but mentions that LAST is general and can be applied to any SSL method.

### Open Question 3
How does the size of the vocabulary (number of codebooks) affect the performance of LAST in terms of both speech modeling and speech recognition tasks? The paper experiments with different codebook sizes (100, 200, 500, 1000) and finds that 500 tokens provide the best performance for speech modeling tasks.

## Limitations

- Adapter architecture details are underspecified, making it difficult to assess computational overhead versus performance gains
- The reconstruction loss formulation is unclear, which impacts understanding of its role in preventing token collapse
- Evaluation focuses primarily on zero-resource speech modeling metrics and ASR performance, lacking ablation studies on different speech encoder layers or codebook sizes

## Confidence

**High Confidence**: The core mechanism of using frozen pre-trained text LM gradients to guide speech tokenization is well-founded and supported by experimental results showing consistent improvements over k-means baselines.

**Medium Confidence**: The claim that keeping the text LM frozen preserves its language modeling capabilities while extending to speech is reasonable but lacks direct empirical validation.

**Low Confidence**: The assertion that this is the first method to jointly train the speech tokenizer and unit language model using pre-trained LMs is difficult to verify given the rapid evolution of the field.

## Next Checks

1. Implement and test multiple adapter configurations (varying number of layers and dimensions) to determine the minimal effective adapter design and quantify computational overhead versus performance gains.

2. Systematically vary the codebook size (e.g., 100, 300, 500, 1000 tokens) and measure the impact on zero-resource metrics to identify the optimal vocabulary size for different downstream tasks.

3. Evaluate LAST on out-of-domain speech datasets (e.g., spontaneous speech, non-LibriSpeech domains) to assess robustness and identify potential failure modes when applying the method beyond the training distribution.