---
ver: rpa2
title: 'Verbosity $\neq$ Veracity: Demystify Verbosity Compensation Behavior of Large
  Language Models'
arxiv_id: '2411.07858'
source_url: https://arxiv.org/abs/2411.07858
tags:
- verbosity
- responses
- verbose
- compensation
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces and analyzes a previously underexplored behavior
  in large language models (LLMs) called "Verbosity Compensation" (VC), where models
  generate unnecessarily long responses when uncertain about an answer. The authors
  define VC as generating responses that can be compressed without information loss
  when prompted to write concisely.
---

# Verbosity $\neq$ Veracity: Demystify Verbosity Compensation Behavior of Large Language Models

## Quick Facts
- arXiv ID: 2411.07858
- Source URL: https://arxiv.org/abs/2411.07858
- Authors: Yusen Zhang; Sarkar Snigdha Sarathi Das; Rui Zhang
- Reference count: 31
- Key outcome: Introduces and analyzes "Verbosity Compensation" (VC) behavior in LLMs, where models generate unnecessarily long responses when uncertain, with GPT-4 showing 50.40% VC frequency and a 27.61% performance gap on Qasper dataset.

## Executive Summary
This paper identifies and analyzes a previously underexplored behavior in large language models called "Verbosity Compensation" (VC), where models generate unnecessarily long responses when uncertain about answers. The authors conduct extensive experiments across five datasets using 14 LLMs, revealing that VC is pervasive across all models with significant performance degradation. They establish a strong connection between verbosity and model uncertainty, finding that verbose responses exhibit higher uncertainty across all datasets. To address this issue, they propose a simple cascade algorithm that effectively reduces VC frequency by routing responses through multiple models, reducing Mistral's VC from 63.81% to 16.16% on the Qasper dataset.

## Method Summary
The authors conduct experiments on five long-context QA datasets using 14 LLMs with consistent prompts to identify verbose responses (those with more than 5 tokens). They measure performance gaps between verbose and concise responses using recall-based metrics, quantify uncertainty through perplexity and Laplacian scores, and implement a cascade algorithm that routes responses from weaker to stronger models until a concise response is generated. The approach is evaluated across multiple model combinations to assess effectiveness in mitigating verbosity compensation behavior.

## Key Results
- GPT-4 exhibits a VC frequency of 50.40% across all tested datasets
- Notable performance gap of 27.61% between verbose and concise responses on the Qasper dataset
- Strong correlation between verbosity and model uncertainty across all five datasets
- Cascade algorithm reduces Mistral's VC frequency from 63.81% to 16.16% on Qasper dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models generate verbose responses when uncertain about the answer, using verbosity to compensate for lack of confidence.
- Mechanism: When the model's probability distribution over the first token is flattened (high entropy), it selects tokens that are safe to generate but don't contain critical information, leading to repetition of question elements, ambiguity, or excessive enumeration.
- Core assumption: There is a strong correlation between model uncertainty and verbosity compensation behavior.
- Evidence anchors:
  - [abstract] "We also find that verbose responses exhibit higher uncertainty across all five datasets, suggesting a strong connection between verbosity and model uncertainty."
  - [section] "We find that verbose responses exhibit higher uncertainty across all five datasets, suggesting a strong connection between verbosity and model uncertainty."
  - [corpus] Weak - corpus contains papers about length bias and uncertainty but not specifically about this compensation mechanism.
- Break condition: If uncertainty quantification methods show no correlation between uncertainty and response length, or if verbose responses consistently show higher performance than concise ones.

### Mechanism 2
- Claim: Verbosity compensation leads to performance degradation because the model prioritizes safe, general responses over precise answers.
- Mechanism: When uncertain, the model generates responses that cover multiple possibilities through enumeration or ambiguity, but this approach reduces precision and recall on the actual answer.
- Core assumption: The performance gap between verbose and concise responses is not due to model capability differences but due to the verbosity compensation behavior itself.
- Evidence anchors:
  - [abstract] "We reveal the large performance gap between verbose and concise responses, with a notable difference of 27.61% on the Qasper dataset."
  - [section] "We reveal the large performance gap between verbose and concise responses, with a notable difference of 27.61% on the Qasper dataset."
  - [corpus] Weak - corpus contains papers about length bias but not specifically about performance degradation from verbosity.
- Break condition: If experiments show that verbose responses have equal or better performance than concise responses across all datasets.

### Mechanism 3
- Claim: Cascade model selection effectively mitigates verbosity compensation by replacing verbose responses from weaker models with responses from stronger models.
- Mechanism: The algorithm iteratively queries models from weak to strong, stopping when a concise response is generated. Since stronger models are less likely to exhibit verbosity compensation, this approach reduces the overall frequency of verbose responses.
- Core assumption: Stronger models have lower frequency of verbosity compensation behavior than weaker models.
- Evidence anchors:
  - [abstract] "We propose a simple yet effective cascade algorithm that replaces verbose responses with responses of larger LLMs. Experiments demonstrate the efficacy of our cascade algorithm..."
  - [section] "Experiments demonstrate the efficacy of our cascade algorithm through tests on three model combinations... The results show that our approach effectively alleviates the VC of the Mistral model from 63.81% to 16.16% on the Qasper dataset."
  - [corpus] Weak - corpus contains papers about model routing but not specifically about cascade approaches for verbosity compensation.
- Break condition: If the cascade algorithm increases the frequency of verbosity compensation or shows no improvement over individual models.

## Foundational Learning

- Concept: Perplexity and uncertainty quantification
  - Why needed here: The paper uses perplexity (for open-source models) and Laplacian scores (for closed-source models) to measure model uncertainty and establish the connection between uncertainty and verbosity.
  - Quick check question: What is the difference between using perplexity versus Laplacian scores for uncertainty quantification, and why are different methods used for open-source versus closed-source models?

- Concept: Context window limitations and input chunking
  - Why needed here: The paper needs to handle long-context datasets by chunking input to fit within model context windows while preserving information.
  - Quick check question: How does the input chunking algorithm ensure that all relevant information is included in the model input while respecting context window constraints?

- Concept: Recall-based performance evaluation
  - Why needed here: The paper uses recall (rather than F1 or precision) to evaluate the performance difference between verbose and concise responses, avoiding bias from length differences.
  - Quick check question: Why is recall a more appropriate metric than F1 score when comparing verbose and concise responses, and what bias would F1 score introduce?

## Architecture Onboarding

- Component map:
  Dataset preparation pipeline -> Model execution framework -> Verbosity detection module -> Uncertainty quantification tool -> Cascade algorithm implementation -> Evaluation metrics calculator

- Critical path:
  1. Load dataset samples and chunk long contexts
  2. Generate responses from all 14 models with "concise" instruction
  3. Detect verbose responses using token threshold
  4. Compute performance metrics (recall) for verbose vs concise responses
  5. Quantify uncertainty for both response types
  6. Apply cascade algorithm to mitigate verbosity
  7. Evaluate routing performance and cost efficiency

- Design tradeoffs:
  - Simple token-count threshold for verbosity detection vs. more sophisticated semantic compression detection
  - Using recall instead of F1 to avoid length bias vs. potentially missing precision differences
  - Fixed model ordering in cascade vs. dynamic ordering based on sample difficulty
  - Uniform instruction to all models vs. model-specific instructions

- Failure signatures:
  - High frequency of verbosity compensation across all models suggests fundamental LLM behavior rather than model-specific issues
  - Performance gap persists even with stronger models indicates verbosity compensation is independent of model capability
  - Uncertainty-quantity correlation breaks down for specific datasets may indicate dataset-specific factors

- First 3 experiments:
  1. Run the cascade algorithm with different model orderings (strong-to-weak vs weak-to-strong) to verify the optimal ordering assumption
  2. Test different verbosity detection thresholds (e.g., 4 tokens vs 5 tokens) to ensure robustness of results
  3. Apply the routing algorithm with different pc/pv probability settings to find optimal cost-performance tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the underlying mechanisms in LLMs that lead to verbosity compensation behavior, and can they be systematically addressed through architectural changes rather than just prompt engineering?
- Basis in paper: [explicit] The paper identifies verbosity compensation (VC) as a pervasive behavior across all models and datasets, with a significant performance gap between verbose and concise responses, and notes that this behavior persists even as model capability increases.
- Why unresolved: The paper proposes a cascade algorithm to mitigate VC but does not investigate the root causes or whether architectural changes could prevent this behavior from occurring in the first place.
- What evidence would resolve it: Experiments comparing different model architectures or training objectives to see if certain designs inherently produce less verbose responses when uncertain.

### Open Question 2
- Question: How does verbosity compensation behavior evolve as LLMs are trained on increasingly diverse and complex datasets, and is there a point where this behavior becomes negligible?
- Basis in paper: [inferred] The paper finds that verbosity compensation is pervasive across different datasets and models, but does not explore how this behavior changes with training data diversity or model scale.
- Why unresolved: The study focuses on existing models but does not examine the training process or how VC might change with different training approaches or data compositions.
- What evidence would resolve it: Longitudinal studies tracking VC frequency and patterns across models trained on progressively more diverse and complex datasets.

### Open Question 3
- Question: Can verbosity compensation be leveraged as a signal for model uncertainty in a way that improves overall system reliability, rather than just being mitigated?
- Basis in paper: [explicit] The paper establishes a strong connection between verbosity and model uncertainty, finding that verbose responses exhibit higher uncertainty across all datasets.
- Why unresolved: While the paper uses this connection to develop a mitigation strategy, it does not explore whether VC could be systematically used as a reliable uncertainty signal in model selection or response generation.
- What evidence would resolve it: Experiments testing whether incorporating VC detection into model selection or response generation pipelines improves overall system performance and reliability.

## Limitations
- Definition of verbosity relies on simple token threshold (5 tokens) which may not capture all forms of unnecessary verbosity
- Limited generalizability of cascade algorithm effectiveness beyond tested Mistral model combination
- Performance evaluation using recall alone may miss important precision trade-offs
- Weak corpus evidence supporting proposed mechanisms connecting verbosity to uncertainty

## Confidence

- **High confidence**: The empirical observation that verbosity compensation is pervasive across all tested models and datasets is well-supported by the experimental results. The 50.40% VC frequency for GPT-4 and the consistent pattern across models provide strong evidence for this phenomenon.

- **Medium confidence**: The connection between verbosity and model uncertainty is supported by the data but relies on proxy measurements (perplexity and Laplacian scores) that may not fully capture model confidence. The performance gap between verbose and concise responses is substantial but could be influenced by factors beyond verbosity compensation.

- **Low confidence**: The proposed cascade algorithm's generalizability beyond the tested Mistral model combination remains uncertain. The mechanism explaining why models generate verbose responses when uncertain, while plausible, lacks direct causal evidence.

## Next Checks

1. **Cross-model validation of cascade effectiveness**: Test the cascade algorithm across different model combinations (e.g., Claude + GPT-4, Llama + Claude) to verify that the approach generalizes beyond the Mistral-specific results and identify optimal routing strategies for different model families.

2. **Alternative verbosity detection methods**: Implement semantic compression detection (e.g., using sentence embedding similarity or information-theoretic measures) to validate whether the simple token threshold accurately captures verbosity compensation behavior and assess the robustness of the reported VC frequencies.

3. **Causal intervention experiments**: Design controlled experiments where models are explicitly given confidence scores or uncertainty information, then observe whether this affects verbosity generation. This would help establish whether verbosity compensation is truly a compensation mechanism or an artifact of other factors like training data patterns.