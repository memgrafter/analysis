---
ver: rpa2
title: 'Flickr30K-CFQ: A Compact and Fragmented Query Dataset for Text-image Retrieval'
arxiv_id: '2403.13317'
source_url: https://arxiv.org/abs/2403.13317
tags:
- retrieval
- query
- dataset
- text-image
- flickr30k-cfq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Flickr30K-CFQ, a new text-image retrieval
  dataset designed to address the limitations of existing datasets that use rigid,
  verbose queries. Flickr30K-CFQ provides four levels of granularities for queries,
  including compact and fragmented descriptions, to better model real-world retrieval
  scenarios.
---

# Flickr30K-CFQ: A Compact and Fragmented Query Dataset for Text-image Retrieval
## Quick Facts
- arXiv ID: 2403.13317
- Source URL: https://arxiv.org/abs/2403.13317
- Reference count: 40
- Introduces Flickr30K-CFQ dataset with compact/fragmented queries that improves text-image retrieval performance by 0.9-2.4%

## Executive Summary
This paper addresses the limitations of existing text-image retrieval datasets that use verbose, formal captions by introducing Flickr30K-CFQ, a new dataset featuring four levels of query granularity including compact and fragmented descriptions. The authors propose a novel query-enhanced retrieval method using LLM-based prompt engineering to augment compact/fragmented queries into comprehensive ones. Their approach significantly improves retrieval performance on both public datasets and their challenge set, demonstrating that existing datasets are insufficient for modeling real-world retrieval scenarios where users employ shorter, more natural query patterns.

## Method Summary
The method consists of two main components: query enhancement using LLM-based prompt engineering and multi-query retrieval with voting. For query enhancement, handcrafted prompts are used with LLMs (Vicuna or GPT-3.5) to generate multiple expanded versions of compact/fragmented queries. The multi-query retrieval module then uses pre-trained multi-modal models (GroupViT, CLIPSeg, ALIGN, or CLIP) to extract features, calculates similarities, applies Top@K filtering to remove duplicates, and performs voting across enhanced query variations to produce final ranked results.

## Key Results
- Over 0.9% improvement on public text-image retrieval datasets using the proposed method
- Over 2.4% improvement on Flickr30K-CFQ challenge set compared to baseline methods
- Performance drops dramatically (from ~78% to ~26-58%) when models use compact/fragmented queries versus captions, validating the need for better datasets

## Why This Works (Mechanism)
### Mechanism 1: LLM-based query augmentation
The system generates multiple enhanced query variations using prompt engineering, then aggregates retrieval results through multi-turn voting across these variations. The core assumption is that the LLM's pre-trained knowledge contains sufficient contextual information to generate relevant expansions of compact/fragmented queries. The corpus shows improved retrieval metrics but doesn't explicitly validate LLM generation quality.

### Mechanism 2: Multi-query voting mechanism
For each batch of enhanced sentences, the system filters candidate images using Top@K, removes duplicates, and performs a second similarity calculation before voting. The assumption is that different query variations capture complementary aspects of the retrieval target, and majority voting across these variations reduces individual variation noise. Performance improvements are shown but the voting mechanism specifically isn't validated.

### Mechanism 3: Multi-granularity corpus
Flickr30K-CFQ provides four levels of query granularity (imagery tags, phrases, triples, fragments) that better match real-world oral/compact query patterns. The assumption is that existing datasets' verbose, formal queries don't represent how users actually search, and models trained on these datasets fail to generalize to compact/fragmented queries. Table 1 shows dramatic performance drops when models use compact/fragmented queries vs. captions, strongly supporting this mechanism.

## Foundational Learning
- Text-image retrieval fundamentals and evaluation metrics: Understanding baseline approaches and metrics (Recall@K, Multi-recall@K) is essential for evaluating improvements. Quick check: What's the difference between Recall@K and Multi-recall@K, and why did the authors introduce the latter?
- Large language model prompt engineering: The core innovation uses LLMs to generate query variations; understanding how different prompts affect generation quality is critical for implementation. Quick check: How might you design prompts to generate different types of query variations (semantic, syntactic, contextual)?
- Multi-modal pre-trained models (CLIP, ALIGN, etc.): The retrieval system uses these as feature extractors; understanding their capabilities and limitations helps explain why query enhancement provides value. Quick check: What are the key differences between CLIP and ALIGN architectures, and how might these affect their performance on compact queries?

## Architecture Onboarding
- Component map: Input (compact/fragmented query) → Query Enhancement Module (LLM-based prompt engineering generating multiple variations) → Multi-query Retrieval Module (feature extraction, similarity calculation, Top@K filtering, voting) → Output (ranked images)
- Critical path: Query → LLM augmentation → Multi-modal encoding → Similarity calculation → Voting → Final ranking
- Design tradeoffs: Query generation vs. computation cost (more LLM generations improve robustness but increase latency), voting mechanism complexity vs. performance (weighted voting could help but adds complexity), granularity selection (different query types may need different enhancement strategies)
- Failure signatures: LLM generations that are irrelevant or contradictory to original query, Top@K filtering that eliminates true positives early in pipeline, voting mechanism that amplifies noise rather than consensus
- First 3 experiments: 1) Baseline comparison on Flickr30K-CFQ without enhancement to confirm performance degradation on compact queries, 2) Ablation study testing enhancement with different numbers of LLM generations (1, 3, 5, 10) to find sweet spot between performance and cost, 3) Prompt engineering comparison testing different prompt templates to see which generate most useful query variations for retrieval

## Open Questions the Paper Calls Out
### Open Question 1
How does the performance of the proposed method vary with different types of queries (e.g., imagery tags, phrases, triples, fragments) in the Flickr30K-CFQ dataset? The paper presents experimental results comparing performance on different query types but doesn't provide detailed analysis of how performance varies with query type.

### Open Question 2
How does the proposed method compare to other state-of-the-art methods for text-image retrieval, particularly those that use pre-trained models? The paper compares to other state-of-the-art methods but doesn't provide comprehensive comparison with methods that use pre-trained models.

### Open Question 3
How does the proposed method perform on other text-image retrieval datasets, such as MS-COCO or Flickr30K? The paper evaluates on Flickr30K-CFQ but doesn't provide results on other text-image retrieval datasets, limiting generalizability.

### Open Question 4
How does the proposed method handle queries that are not well-formed or contain errors? The paper doesn't address handling of queries that are not well-formed or contain errors.

## Limitations
- Limited justification for selection of four specific query granularity levels without systematic analysis of optimal levels
- Handcrafted prompts for LLM-based query enhancement are described but not provided, making reproduction difficult
- Multi-query voting mechanism parameters lack sensitivity analysis showing how choices affect performance
- Multi-recall@10 metric introduced but advantages over existing metrics not rigorously established

## Confidence
- **High confidence**: Empirical observation that existing text-image retrieval datasets predominantly use verbose, formal captions rather than compact/fragmented queries users employ in real-world scenarios
- **Medium confidence**: Effectiveness of LLM-based query augmentation for improving retrieval performance, though limited by lack of detailed methodology and sensitivity analysis
- **Low confidence**: Specific design choices for Flickr30K-CFQ dataset and proposed retrieval method due to lack of systematic analysis of alternatives

## Next Checks
1. Conduct prompt engineering ablation by systematically testing different prompt templates and numbers of LLM generations to understand how prompt design affects query enhancement quality and retrieval performance
2. Perform granularity sensitivity analysis by experimenting with varying numbers of query granularity levels beyond the four provided to determine if additional levels provide meaningful benefits
3. Validate voting mechanism through controlled experiments comparing against baseline approaches like simple query concatenation or weighted averaging of retrieval scores to isolate its contribution