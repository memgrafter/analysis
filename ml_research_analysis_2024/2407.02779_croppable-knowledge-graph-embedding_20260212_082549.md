---
ver: rpa2
title: Croppable Knowledge Graph Embedding
arxiv_id: '2407.02779'
source_url: https://arxiv.org/abs/2407.02779
tags:
- sub-models
- knowledge
- low-dimensional
- training
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of training knowledge graph embeddings
  (KGEs) that can serve multiple dimensional requirements across different scenarios
  without retraining. The proposed method, MED, enables one training to produce a
  "croppable" KGE model where sub-models of various dimensions can be directly extracted
  and used.
---

# Croppable Knowledge Graph Embedding

## Quick Facts
- arXiv ID: 2407.02779
- Source URL: https://arxiv.org/abs/2407.02779
- Reference count: 33
- One-line primary result: MED enables training croppable KGE models that outperform baselines, especially at low dimensions, with up to 10× training efficiency

## Executive Summary
This paper introduces MED, a framework for training croppable knowledge graph embeddings (KGEs) that can serve multiple dimensional requirements across different scenarios without retraining. MED trains multiple sub-models of varying dimensions simultaneously, enabling direct extraction of sub-models with different dimensionalities from a single trained model. The framework employs three key mechanisms: mutual learning for improving low-dimensional sub-models, evolutionary improvement for helping high-dimensional sub-models master triples that low-dimensional ones cannot, and dynamic loss weights for adaptively balancing multiple optimization objectives.

## Method Summary
MED trains n sub-models of different dimensions sharing embedding parameters, with low-dimensional sub-models benefiting from knowledge distillation through mutual learning from higher-dimensional neighbors, while high-dimensional sub-models focus on triples that lower-dimensional ones mispredict through evolutionary improvement. Dynamic loss weights adaptively balance the multiple optimization objectives based on sub-model dimensions, with lower-dimensional models prioritizing learning from neighbors and higher-dimensional models focusing on mastering difficult triples. The framework achieves up to 10× training efficiency compared to independent training of each sub-model.

## Key Results
- MED sub-models outperform baselines by up to 14.9% MRR at 10 dimensions on WN18RR with TransE
- Training efficiency reaches up to 10× compared to independent training of multiple sub-models
- MED demonstrates extensibility to BERT on GLUE tasks, outperforming HAT models by 16.3-21.7% on average

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-dimensional sub-models benefit from high-dimensional ones through neighbor-based knowledge distillation, improving performance.
- Mechanism: Each sub-model Mi learns from its higher-dimensional neighbor Mi+1 via mutual learning loss, enabling low-dimensional models to acquire knowledge that would otherwise require retraining.
- Core assumption: High-dimensional models contain richer representational capacity that can be distilled to lower-dimensional ones.
- Evidence anchors:
  - [abstract]: "mutual learning mechanism to improve the low-dimensional sub-models and make high-dimensional sub-models retain the low-dimensional sub-models' capacity"
  - [section 4.1]: "For low-dimensional sub-models, we aim to maximize performance. High-dimensional sub-models should not only replicate the capabilities of low-dimensional ones but also learn what low-dimensional ones can't"
  - [corpus]: Weak - corpus doesn't contain direct evidence about mutual learning mechanisms
- Break condition: If dimension gaps between neighbors become too large, distillation effectiveness degrades significantly.

### Mechanism 2
- Claim: High-dimensional sub-models learn triples that low-dimensional ones cannot correctly predict through evolutionary improvement.
- Mechanism: High-dimensional sub-models focus optimization on triples that low-dimensional sub-models mispredict, using adaptive weighting based on low-dimensional model scores.
- Core assumption: Mispredicted triples by low-dimensional models represent learning gaps that high-dimensional models can fill.
- Evidence anchors:
  - [abstract]: "evolutionary improvement mechanism to promote the high-dimensional sub-models to master the triple that the low-dimensional sub-models can not"
  - [section 4.2]: "High-dimensional sub-models need to master triples that low-dimensional sub-models can not, that is correctly predicting positive (negative) triples mispredicted as negative (positive) by low-dimensional sub-models"
  - [corpus]: Weak - corpus lacks evidence about evolutionary improvement mechanisms
- Break condition: If low-dimensional models perform poorly, high-dimensional models may inherit incorrect predictions and degrade.

### Mechanism 3
- Claim: Dynamic loss weights adaptively balance multiple optimization objectives across different dimensional sub-models.
- Mechanism: Loss weights are adjusted based on sub-model dimension, with lower-dimensional models relying more on soft labels from neighbors and higher-dimensional models focusing more on hard labels.
- Core assumption: Different dimensional sub-models have different learning priorities and should be weighted accordingly.
- Evidence anchors:
  - [abstract]: "dynamic loss weight to adaptively balance the multiple losses"
  - [section 4.3]: "At first, low-dimensional sub-models prioritize learning from high-dimensional ones... Conversely, high-dimensional sub-models should focus more on what low-dimensional ones can't correctly predict"
  - [corpus]: Weak - corpus lacks direct evidence about dynamic loss weighting strategies
- Break condition: If loss weight adaptation fails to account for actual learning progress, sub-models may become unbalanced.

## Foundational Learning

- Concept: Knowledge Graph Embedding (KGE)
  - Why needed here: MED builds upon KGE methods and requires understanding of how entities and relations are represented in continuous vector spaces.
  - Quick check question: How does a KGE method like TransE score a triple (h, r, t)?

- Concept: Knowledge Distillation
  - Why needed here: MED uses knowledge distillation techniques to transfer knowledge between sub-models of different dimensions.
  - Quick check question: What is the difference between hard labels and soft labels in knowledge distillation?

- Concept: Multi-task Learning
  - Why needed here: MED trains multiple sub-models simultaneously, requiring understanding of how to balance different learning objectives.
  - Quick check question: How does dynamic loss weighting help in multi-task learning scenarios?

## Architecture Onboarding

- Component map: n sub-models of varying dimensions (10 to 640) sharing embedding parameters, with mutual learning between neighbors, evolutionary improvement for difficult triples, and dynamic loss weight adaptation
- Critical path: Simultaneous training of all sub-models through shared parameters and coordinated loss functions, with knowledge transfer between dimensional neighbors
- Design tradeoffs: Balancing model capacity with training efficiency, managing knowledge transfer between sub-models of varying quality, and preventing negative transfer from poor-performing lower-dimensional models
- Failure signatures: Poor performance in low-dimensional sub-models indicates insufficient knowledge transfer; degradation in high-dimensional sub-models suggests accumulation of incorrect knowledge
- First 3 experiments:
  1. Verify mutual learning by comparing low-dimensional sub-model performance with and without neighbor distillation
  2. Test evolutionary improvement by measuring high-dimensional sub-model accuracy on triples that low-dimensional models mispredict
  3. Evaluate dynamic loss weight effectiveness by comparing performance with fixed vs. adaptive loss weighting

## Open Questions the Paper Calls Out
None

## Limitations
- The evolutionary improvement mechanism lacks detailed implementation specifications, particularly regarding how optimization weights are computed per batch
- Dynamic loss weight adaptation effectiveness depends heavily on hyperparameter tuning with limited ablation studies showing sensitivity
- Extensibility to BERT tasks uses a different architecture (HAT) as comparison, making direct dimension-reduction efficiency comparisons less definitive

## Confidence
- **High confidence**: The core claim that MED enables croppable KGE models with better low-dimensional sub-model performance is well-supported by extensive experiments across multiple KGE methods and datasets. The training efficiency improvements (up to 10×) are clearly demonstrated.
- **Medium confidence**: The mutual learning mechanism's effectiveness relies on specific implementation details not fully specified in the paper. While results show improvement, the exact knowledge transfer dynamics between sub-models need further validation.
- **Low confidence**: The extensibility claims to BERT and other language models lack sufficient experimental depth, with only preliminary results on GLUE tasks using a different comparison baseline.

## Next Checks
1. **Mechanism isolation test**: Implement and compare MED with individual mechanisms (mutual learning only, evolutionary improvement only, dynamic loss only) disabled to quantify each component's contribution to overall performance.

2. **Dimension gap sensitivity analysis**: Systematically vary the dimensional gaps between neighbor sub-models (e.g., testing 2× vs 4× vs 8× differences) to determine optimal spacing for knowledge transfer effectiveness.

3. **Cross-model transferability validation**: Test whether MED-trained models can be successfully cropped and applied to downstream tasks beyond link prediction, such as triple classification or relation extraction, to verify practical utility of the cropping capability.