---
ver: rpa2
title: Graded Suspiciousness of Adversarial Texts to Human
arxiv_id: '2410.04377'
source_url: https://arxiv.org/abs/2410.04377
tags:
- adversarial
- texts
- human
- text
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel dataset of Likert-scale human annotations
  on the suspiciousness of adversarial texts generated by four popular attack methods.
  It demonstrates that human annotators can reliably distinguish between original
  and adversarial texts, with varying degrees of suspiciousness.
---

# Graded Suspiciousness of Adversarial Texts to Human

## Quick Facts
- arXiv ID: 2410.04377
- Source URL: https://arxiv.org/abs/2410.04377
- Authors: Shakila Mahjabin Tonni; Pedro Faustini; Mark Dras
- Reference count: 23
- Key outcome: Novel dataset of Likert-scale human annotations on adversarial text suspiciousness, achieving 0.515 Pearson correlation between predicted and human suspicion scores

## Executive Summary
This paper addresses the gap between automated adversarial text detection and human perception by introducing a novel dataset of Likert-scale human annotations measuring the suspiciousness of adversarial texts. The authors demonstrate that humans can reliably distinguish between original and adversarial texts with varying degrees of certainty, rather than through binary classification. They develop regression-based models that predict these human suspicion scores with moderate accuracy, and show that incorporating these scores as constraints during adversarial text generation produces texts less likely to be perceived as computer-generated by humans.

## Method Summary
The study collected human annotations on 1,000 original and adversarial texts from four attack methods (PRUTHI, ALZANTOT, TEXTFOOLER, BAE) using Amazon Mechanical Turk, with 3,715 total annotations on a 5-point Likert scale. They trained BERT-based regression models on both text and numerical features (perturbation rates, grammaticality scores, GPT-4 outputs) to predict human suspicion scores, achieving a Pearson correlation of 0.515. The suspicion scores were then incorporated as constraints in adversarial generation, with human evaluation showing improved quality of generated texts.

## Key Results
- Human annotators reliably distinguish adversarial from original texts with varying suspicion levels
- Regression models achieve 0.515 Pearson correlation with human suspicion scores
- Suspicion-constrained adversarial generation produces texts less likely to be perceived as computer-generated
- TextFooler generates most suspicious outputs; BAE generates least suspicious outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human annotators can reliably distinguish adversarial texts from originals with varying degrees of certainty.
- Mechanism: Likert-scale ratings capture nuanced suspicion levels beyond binary classification, reflecting real-world scenarios where text must evade filters while remaining plausible.
- Core assumption: Humans can detect subtle perturbations in text that automated systems might miss.
- Evidence anchors:
  - [abstract] "human annotators can reliably distinguish between original and adversarial texts, with varying degrees of suspiciousness"
  - [section 3.2] "this level of label variability aligns with humans finding this task difficult but there still is some shared intuition about the suspiciousness of texts"
  - [corpus] Weak evidence - corpus neighbors focus on different aspects of adversarial attacks
- Break condition: If annotators cannot agree on suspiciousness levels or if perturbations become too subtle for human detection.

### Mechanism 2
- Claim: Regression-based models can predict human suspicion scores with moderate accuracy.
- Mechanism: Fine-tuning language models on Likert-scale annotations and combining with numerical features (perturbation rates, grammaticality scores) enables prediction of human suspicion.
- Core assumption: The features used capture aspects of text that influence human suspicion.
- Evidence anchors:
  - [abstract] "regression-based model to predict these human suspicion scores, achieving a Pearson correlation of 0.515 with human annotations"
  - [section 4.2] "combining predictions from the best TEXT +NUM model... achieved a Pearson's correlation coefficient of 0.515"
  - [corpus] Weak evidence - corpus neighbors do not directly address suspicion prediction
- Break condition: If the correlation between predicted and actual scores falls below acceptable thresholds or if new adversarial methods introduce novel perturbation patterns.

### Mechanism 3
- Claim: Incorporating suspicion scores as constraints improves adversarial text generation.
- Mechanism: By requiring generated texts to maintain suspicion levels below a threshold, the generator produces texts less likely to be perceived as computer-generated.
- Core assumption: The suspicion regressor accurately predicts human perception of generated texts.
- Evidence anchors:
  - [abstract] "incorporating these suspicion scores as constraints during adversarial text generation can produce texts that are less likely to be perceived as computer-generated"
  - [section 5.2] "in our small sample, the SUSCONSTRAINED texts are significantly better" (p = 0.0106)
  - [corpus] Weak evidence - corpus neighbors do not address constraint-based generation
- Break condition: If the suspicion regressor becomes unreliable or if new adversarial methods bypass the constraint mechanism.

## Foundational Learning

- Concept: Likert scale usage for ordinal data
  - Why needed here: To capture gradations of suspicion beyond binary classification
  - Quick check question: What are the advantages of using a 5-point Likert scale versus binary classification for measuring human suspicion?

- Concept: Correlation coefficients (Pearson, Spearman)
  - Why needed here: To evaluate the performance of regression models predicting suspicion scores
  - Quick check question: When would Spearman's rank correlation be more appropriate than Pearson's correlation for evaluating model performance?

- Concept: Adversarial attack methods and their characteristics
  - Why needed here: To understand the different types of perturbations and their impact on human suspicion
  - Quick check question: How do character-level and word-level adversarial attacks differ in terms of their potential to raise human suspicion?

## Architecture Onboarding

- Component map: Data collection -> Model training -> Suspicion-constrained generation -> Human evaluation
- Critical path: Human annotation interface → Text sampling → BERT fine-tuning → Ensemble prediction → Adversarial generation with constraint → Human evaluation
- Design tradeoffs:
  - Likert scale granularity vs annotation effort
  - Model complexity vs prediction accuracy
  - Constraint strictness vs attack effectiveness
- Failure signatures:
  - Low inter-annotator agreement
  - Poor correlation between predicted and actual suspicion scores
  - Suspicion-constrained texts still perceived as computer-generated
- First 3 experiments:
  1. Collect human annotations on a small sample of texts to establish baseline suspicion levels
  2. Train a simple regression model (e.g., linear regression) on numerical features to predict suspicion
  3. Implement suspicion constraint in adversarial generator and compare generated texts with and without constraint using human evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do human judgments of suspiciousness differ between character-level and word-level adversarial attacks, and what linguistic features contribute most to these differences?
- Basis in paper: [explicit] The paper analyzes suspiciousness across PRUTHI (character-level) and ALZANTOT/TEXTFOOLER/BAE (word-level) attacks, finding PRUTHI and BAE generated the most disagreement among annotators
- Why unresolved: While the paper identifies which attack types generate more suspicion, it doesn't deeply analyze the specific linguistic features (e.g., semantic coherence, syntactic anomalies) that make certain perturbations more suspicious to humans
- What evidence would resolve it: Detailed linguistic feature analysis comparing suspicious and non-suspicious examples from each attack type, identifying patterns in semantic preservation, grammaticality, and contextual appropriateness

### Open Question 2
- Question: How would providing explicit information about the prevalence of adversarial examples affect human suspiciousness ratings, and would this change the relative rankings of different attack methods?
- Basis in paper: [explicit] The paper conducted an experiment where annotators were told there were more computer-altered sentences than real ones, finding slight changes in mean scores but maintaining the same relative rankings
- Why unresolved: The prevalence experiment was limited to a single altered instruction, and the paper notes that prevalence effects are well-documented in psychology but doesn't explore the full range of possible prevalence scenarios
- What evidence would resolve it: Systematic experiments varying prevalence information (e.g., "more real than fake," "equal distribution," "much more fake than real") with large-scale annotation to determine how prevalence information affects suspiciousness ratings

### Open Question 3
- Question: How effective are large language models at predicting human suspiciousness scores compared to the regression-based models developed in this paper, and can they provide better feature representations?
- Basis in paper: [explicit] The paper used GPT-4 as a feature in their regression models and found it improved performance significantly, but didn't directly compare LLM-based scoring to their regression approach
- Why unresolved: While the paper shows GPT-4 scores are useful as features, it doesn't explore whether LLMs could serve as standalone suspiciousness predictors or provide better contextual understanding than traditional language models
- What evidence would resolve it: Direct comparison of suspiciousness prediction performance between the paper's best regression model and LLM-based approaches (e.g., prompting GPT-4, fine-tuning LLaMA) on the same test sets, including analysis of which approach better captures subtle linguistic cues

## Limitations
- Modest correlation (0.515) between predicted and human suspicion scores indicates room for improvement
- Limited to four specific adversarial attack methods, potentially restricting generalizability
- Small-scale human evaluation (n=30) for suspicion-constrained generation validation

## Confidence
- **High Confidence**: Humans can reliably distinguish adversarial texts with varying suspicion levels; TextFooler consistently produces most suspicious outputs
- **Medium Confidence**: Regression models predict human suspicion scores with 0.515 correlation; suspicion-constrained generation shows promise but needs larger validation
- **Low Confidence**: Generalization to other datasets, attack methods, or real-world deployment scenarios

## Next Checks
1. **Scale-up Human Evaluation**: Conduct a larger-scale human evaluation (n > 300) of suspicion-constrained texts to validate the effectiveness of the approach beyond the preliminary findings.

2. **Cross-dataset Validation**: Test the suspicion prediction models and constrained generation approach on multiple datasets beyond movie reviews (e.g., news articles, scientific abstracts) to assess generalizability.

3. **Adversarial Robustness Test**: Design and evaluate adversarial examples specifically targeting the suspicion detection system to assess whether the approach can be circumvented by attacks optimized for human perception rather than classifier evasion.