---
ver: rpa2
title: A Theory for Length Generalization in Learning to Reason
arxiv_id: '2404.00560'
source_url: https://arxiv.org/abs/2404.00560
tags:
- reasoning
- problem
- arxiv
- input
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes a theoretical framework for understanding length
  generalization (LG) in learning to reason, where models trained on small problems
  fail on larger ones. The authors identify key conditions for achieving LG: the input
  space of reasoning steps must be finite, the problem must be solved recursively
  using chain-of-thought (CoT), and the CoT formulation must be (n,r)-consistent.'
---

# A Theory for Length Generalization in Learning to Reason

## Quick Facts
- arXiv ID: 2404.00560
- Source URL: https://arxiv.org/abs/2404.00560
- Authors: Changnan Xiao; Bing Liu
- Reference count: 40
- One-line primary result: The paper provides the first theoretical understanding of length generalization in learning to reason, proving conditions under which models can generalize from small to large problems.

## Executive Summary
This paper addresses the fundamental problem of length generalization (LG) in learning to reason, where models trained on small reasoning problems fail when tested on larger ones. The authors develop a theoretical framework that identifies three key conditions for achieving LG: finite input space of reasoning steps, recursive problem-solving using chain-of-thought (CoT), and (n,r)-consistency in CoT formulation. They prove that problems meeting these conditions are learnable with perfect LG, while those that don't fail to generalize. Experiments with a vanilla Transformer verify the theory across five reasoning tasks including arithmetic, parity, and trigonometric functions, demonstrating that carefully designed CoT formulations can enable perfect length generalization even for problems that would otherwise fail.

## Method Summary
The authors use a vanilla Transformer with relative position encoding to test their theoretical framework on five reasoning problems: arctan, arithmetic in finite field F7, parity, addition, and multiplication. They implement two versions of addition (1-line and 2-line) and multiplication (1-line and 8-line) to demonstrate how different CoT formulations affect length generalization. Training involves generating datasets with smaller problem sizes and testing on larger ones, with the model trained for 1 epoch using Adam optimizer at learning rate 0.0001. The critical insight is that CoT reformulation can transform problems from non-generalizable to generalizable by ensuring (n,r)-consistency and managing the R parameter (maximum distance between input elements in reasoning steps).

## Key Results
- Proved that problems with finite input space and R < ∞ are learnable with length generalization
- Demonstrated that (n,r)-consistency enables LG for problems with R = ∞
- Achieved perfect length generalization on arithmetic, parity, and other reasoning tasks meeting theoretical conditions
- Showed that same problems can have different LG properties depending on CoT formulation
- Verified theory using vanilla Transformer without specialized architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The causal function can be perfectly learned only when the input space of the causal function is finite (|X| < ∞).
- Mechanism: When the input space is finite, the set of possible reasoning steps is bounded, allowing the model to learn all possible transitions exactly during training. This guarantees that the causal function can be fully specified without ambiguity.
- Core assumption: The training dataset covers all possible combinations of input elements that the causal function might encounter.
- Evidence anchors:
  - [abstract] The paper proves that problems with finite input space and R < ∞ are learnable with length generalization.
  - [section 3.1] Theorem 3.1 states that for |X| < ∞ and sup |p(v)| < ∞, if D = X, then there exists an approximation function ˆf that perfectly learns the causal function.
  - [corpus] Weak - the corpus doesn't directly address this theoretical result, focusing instead on empirical evaluations of length generalization.
- Break condition: If the input space is infinite, the model cannot learn all possible reasoning steps, leading to inevitable errors on unseen combinations.

### Mechanism 2
- Claim: When the input space is finite and R < ∞, the model can learn to identify which elements should be used in the next reasoning step using a sliding window approach.
- Mechanism: The model learns a function that, given a window of 4R+1 elements, can predict which elements will be used in the next reasoning step. Since R is bounded, this window size is finite and manageable.
- Core assumption: The reasoning problem has the property that elements involved in the next reasoning step are always within a bounded distance R.
- Evidence anchors:
  - [section 3.2] Theorem 3.3 proves that for R < ∞, if D = X^(4R+1), then there exists an approximation function that can predict which elements should be reasoned next.
  - [section 3.2] The paper shows that arithmetic problems with parentheses have R = 4 because any combination of elements to be calculated next is within a window of at most 5 elements.
  - [corpus] Missing - the corpus doesn't contain evidence about the R parameter or sliding window approaches.
- Break condition: If R = ∞, no finite window can capture all possible combinations of elements that might be needed in the next reasoning step.

### Mechanism 3
- Claim: For problems with R = ∞ but (n,r)-consistency, the model can still achieve length generalization by learning to identify reasoning patterns within n intervals of length r.
- Mechanism: The (n,r)-consistency property ensures that for any set of n r-length contiguous sub-sequences, whether the central element belongs to the next reasoning step is always the same across all problem instances. This allows the model to learn a deterministic mapping from these intervals to the next reasoning step.
- Core assumption: The problem formulation ensures that there exists a finite number of interval patterns that can determine which elements to use next.
- Evidence anchors:
  - [section 3.3] The paper introduces (n,r)-consistency and proves that any reasoning problem with (n,r)-consistency can achieve length generalization regardless of whether R < ∞ or R = ∞.
  - [section 5] The addition-[2-line] and multiplication-[8-line] problems achieve perfect length generalization because they are (3,3)-consistent and (9,3)-consistent respectively.
  - [corpus] Weak - the corpus doesn't discuss (n,r)-consistency or its role in enabling length generalization for problems with infinite R.
- Break condition: If the problem is not (n,r)-consistent, no finite number of intervals can provide deterministic information about which elements to use next.

## Foundational Learning

- Concept: Finite input space and its implications for learnability
  - Why needed here: Understanding why finite input spaces enable perfect learning is crucial for grasping the theoretical foundation of length generalization
  - Quick check question: Why can't a model with infinite input space achieve perfect length generalization, even with unlimited training data?

- Concept: Directed Acyclic Graph (DAG) representation of reasoning processes
  - Why needed here: The theory relies on reasoning problems being representable as DAGs, where each node represents a reasoning step and edges represent dependencies
  - Quick check question: How does the DAG structure ensure that reasoning problems can be solved recursively?

- Concept: Chain-of-Thought (CoT) formulations and their relationship to DAG structures
  - Why needed here: Different CoT formulations of the same reasoning problem can have different properties (R values, consistency), affecting learnability
  - Quick check question: Why does the same addition problem have different length generalization properties depending on whether it's formulated in one line or two lines?

## Architecture Onboarding

- Component map: Input tokenization layer -> Transformer encoder (3-6 layers) -> Output prediction layer -> Loss calculation
- Critical path:
  1. Tokenize input sequence with positional information
  2. Encode through Transformer layers
  3. Predict which elements to use in next reasoning step (if R < ∞) or apply (n,r)-consistency rules (if R = ∞)
  4. Apply learned causal function to predict next value
  5. Repeat until problem is solved or final answer is predicted
- Design tradeoffs:
  - Number of Transformer layers: More layers needed for problems with R = ∞ but (n,r)-consistency
  - Position encoding: Relative position encoding crucial for capturing local patterns within intervals
  - Input dimensionality: Multi-dimensional inputs required for complex CoT formulations like multiplication-[8-line]
  - Training data size: Must cover all possible combinations within the relevant window sizes
- Failure signatures:
  - Perfect training accuracy but poor test accuracy on longer sequences indicates length generalization failure
  - Oscillating predictions during CoT steps suggest the model hasn't learned consistent reasoning patterns
  - Systematic errors on specific digit combinations indicate incomplete coverage of the input space
- First 3 experiments:
  1. Test basic arithmetic in F7 with varying sequence lengths to verify R = 4 property and perfect length generalization
  2. Compare addition-[1-line] vs addition-[2-line] formulations to demonstrate the impact of (n,r)-consistency
  3. Test multiplication with 1-line vs 8-line formulations to show how CoT reformulation enables length generalization for R = ∞ problems

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical framework relies on strong assumptions about finite input spaces and bounded reasoning windows that may not hold for many real-world reasoning tasks
- The experiments use a relatively small Transformer model (3-6 layers) and may not scale to the complexity of modern large language models
- The theory primarily addresses deterministic reasoning problems where correct reasoning steps are uniquely determined by input

## Confidence
**High Confidence**: The core theoretical claims about finite input spaces enabling learnability (Mechanism 1) are well-supported by the proofs in Sections 3.1 and 3.2. The relationship between R < ∞ and bounded window sizes is clearly established. The experimental verification on arithmetic problems in F7 provides strong empirical support for these claims.

**Medium Confidence**: The claims about (n,r)-consistency enabling length generalization for R = ∞ problems (Mechanism 3) are theoretically sound but have limited empirical validation. The paper demonstrates this on only two specific problems (addition-[2-line] and multiplication-[8-line]), and the generalizability to other problem types remains unclear. The complexity of implementing multi-dimensional CoT formulations may limit practical applicability.

**Low Confidence**: The claim that this theory provides the "first theoretical understanding of LG" is difficult to verify definitively. While the paper offers a novel perspective, the field of learning theory has many related results on generalization that may overlap with or subsume aspects of this work. The relationship between this theoretical framework and empirical observations in large language models is not fully explored.

## Next Checks
1. **Scaling Experiment**: Test whether the theoretical guarantees hold when scaling the Transformer model to 12-24 layers (closer to modern LLM sizes) while maintaining perfect length generalization on problems with R < ∞ and finite input spaces.

2. **Robustness to Noise**: Evaluate the theory's predictions under noisy conditions where training data contains errors or the causal function has stochastic elements. Measure how quickly length generalization degrades as noise increases.

3. **Transfer to Natural Language**: Apply the CoT reformulation approach to natural language reasoning tasks (e.g., multi-hop question answering) and measure whether problems can be made (n,r)-consistent through careful prompt engineering, then test for length generalization on extended examples.