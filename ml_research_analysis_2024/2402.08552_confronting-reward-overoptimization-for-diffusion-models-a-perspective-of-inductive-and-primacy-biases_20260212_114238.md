---
ver: rpa2
title: 'Confronting Reward Overoptimization for Diffusion Models: A Perspective of
  Inductive and Primacy Biases'
arxiv_id: '2402.08552'
source_url: https://arxiv.org/abs/2402.08552
tags:
- reward
- diffusion
- tdpo
- neurons
- overoptimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses reward overoptimization in diffusion model
  alignment, a problem where models overfit to imperfect reward functions, leading
  to degraded performance on true objectives. The authors investigate this issue through
  the lens of inductive and primacy biases.
---

# Confronting Reward Overoptimization for Diffusion Models: A Perspective of Inductive and Primacy Biases

## Quick Facts
- **arXiv ID**: 2402.08552
- **Source URL**: https://arxiv.org/abs/2402.08552
- **Reference count**: 40
- **Key outcome**: TDPO-R achieves superior performance in mitigating reward overoptimization while maintaining fidelity and diversity across multiple reward functions and prompts.

## Executive Summary
This paper addresses reward overoptimization in diffusion model alignment, where models overfit to imperfect reward functions, leading to degraded performance on true objectives. The authors identify a mismatch between traditional reward-driven alignment methods and the temporal inductive bias inherent in diffusion models' multi-step denoising process. They propose Temporal Diffusion Policy Optimization with critic active neuron Reset (TDPO-R), which introduces timestep-dependent rewards and per-timestep gradient updates to align with the model's temporal nature. Additionally, TDPO-R periodically resets active neurons in the critic model to mitigate primacy bias, discovered to be a key factor in reward overoptimization.

## Method Summary
TDPO-R aligns diffusion models with human preferences by leveraging their temporal inductive bias through timestep-dependent rewards and per-timestep gradient updates. The method maps the denoising process to a Markov Decision Process, where the diffusion model acts as the policy and a temporal critic estimates rewards for each intermediate step. To address primacy bias, TDPO-R periodically resets active neurons in the critic model, preventing overfitting to early training experiences. The approach is implemented as LoRA finetuning on Stable Diffusion v1.4, evaluated across multiple reward functions (Aesthetic Score, PickScore, HPSv2) and prompt sets.

## Key Results
- TDPO-R outperforms state-of-the-art methods in sample efficiency and cross-reward generalization
- The method achieves better performance while maintaining higher fidelity and diversity in generated images
- TDPO-R demonstrates effectiveness on unseen prompts, emphasizing robustness against reward overoptimization
- Ablation studies confirm the importance of temporal inductive bias alignment and active neuron reset mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Temporal Diffusion Policy Optimization (TDPO) mitigates reward overoptimization by aligning the reward structure with the temporal inductive bias of diffusion models through timestep-dependent rewards.
- **Mechanism**: Diffusion models inherently possess a temporal inductive bias due to their multi-step denoising process. Traditional reward-driven alignment methods use timestep-independent rewards based only on the final generated image, creating a mismatch with this inherent bias. TDPO addresses this by introducing timestep-dependent rewards for each intermediate denoising step, ensuring the reward structure is consistent with the model's temporal nature. This alignment reduces the risk of overfitting to imperfect reward functions that fail to capture the full spectrum of human intent.
- **Core assumption**: The temporal inductive bias of diffusion models is crucial for their performance, and aligning reward signals with this bias will lead to better optimization and reduced overoptimization.
- **Evidence anchors**:
  - [abstract]: "We first identify a mismatch between current methods and the temporal inductive bias inherent in the multi-step denoising process of diffusion models, as a potential source of reward overoptimization."
  - [section]: "However, current reward-driven alignment approaches for diffusion models exclusively focus on maximizing rewards computed from the final generated images, while overlooking the sequential nature of diffusion models and valuable intermediate information within the multi-step denoising process. This mismatch between the reward structure and the model's inherent temporal inductive bias potentially leads to overfitting and misalignment between the desired outcome (high reward) and the actual quality of the generation process."
  - [corpus]: The paper cites prior work on diffusion model alignment (Fan et al., 2023; Black et al., 2024) that uses timestep-independent rewards, supporting the claim of a mismatch.
- **Break condition**: If the timestep-dependent rewards do not accurately reflect the true objective or if the temporal critic fails to learn meaningful temporal residuals, the mechanism would break down.

### Mechanism 2
- **Claim**: TDPO-R further mitigates reward overoptimization by addressing primacy bias through periodic resetting of active neurons in the critic model.
- **Mechanism**: Primacy bias, the tendency of models to overfit to early training experiences, can contribute to reward overoptimization. The paper discovers that dormant neurons in the critic model act as a regularization against reward overoptimization, while active neurons reflect primacy bias. TDPO-R addresses this by periodically resetting the active neurons in the critic, preventing them from becoming overly specialized to early training experiences and encouraging the model to learn new regularization patterns without forgetting crucial past regularization.
- **Core assumption**: Active neurons in the critic model are susceptible to primacy bias, and resetting them can mitigate this bias and improve generalization.
- **Evidence anchors**:
  - [abstract]: "Then, we surprisingly discover that dormant neurons in our critic model act as a regularization against reward overoptimization while active neurons reflect primacy bias."
  - [section]: "Although Sokar et al. (2023) suggest that dormant neurons in deep RL agents have a negative effect on the model capacity and resetting dormant neurons reduces this effect, we surprisingly discover that dormant neurons instead act as an adaptive regularization against reward overoptimization, while active neurons appear to be susceptible to the primacy bias towards this phenomenon."
  - [corpus]: The paper references Sokar et al. (2023) on dormant neurons, but provides contrasting findings, indicating a novel discovery in the context of reward overoptimization.
- **Break condition**: If resetting active neurons too frequently leads to catastrophic forgetting or if the model fails to relearn effective regularization patterns, the mechanism would break down.

### Mechanism 3
- **Claim**: The per-timestep gradient update strategy in TDPO improves sample efficiency and stability compared to per-batch updates.
- **Mechanism**: Traditional deep RL methods often use per-batch gradient updates, which can be less efficient and stable. TDPO employs a per-timestep gradient update strategy, where the policy and critic parameters are updated concurrently at each denoising timestep. This approach leverages the temporal granularity of the rewards and gradients, leading to faster convergence and improved sample efficiency. The consistent temporal granularity between the temporal rewards and the per-timestep gradient updates not only mitigates reward overoptimization but also improves sample efficiency by striking a balance between update frequency and stability.
- **Core assumption**: A higher gradient update frequency, when aligned with the temporal structure of the problem, can lead to faster convergence and improved sample efficiency without sacrificing stability.
- **Evidence anchors**:
  - [abstract]: "Accordingly, the consistent temporal granularity between the temporal rewards and the per-timestep gradient updates not only mitigates reward overoptimization, but also improves sample efficiency by striking a balance between update frequency and stability."
  - [section]: "In most cases of deep RL, a higher gradient update frequency often results in faster convergence but worse stability. In our settings, we operate on samples spanning two dimensions: timesteps and mini-batches, allowing us to elevate the gradient update frequency by reducing the sizes of these dimensions. However, reducing the dimension sizes introduces lower variance in sample distributions, potentially leading to overfitting."
  - [corpus]: The paper mentions that other methods use per-batch updates, providing context for the novelty of the per-timestep approach.
- **Break condition**: If the per-timestep updates lead to excessive variance or instability, or if the computational cost becomes prohibitive, the mechanism would break down.

## Foundational Learning

- **Concept**: Markov Decision Process (MDP)
  - **Why needed here**: The paper maps the denoising process of diffusion models to an MDP to frame the alignment problem as a reinforcement learning task. Understanding MDPs is crucial for grasping the theoretical foundation of TDPO and TDPO-R.
  - **Quick check question**: What are the key components of an MDP, and how are they defined in the context of diffusion model alignment in this paper?

- **Concept**: Temporal Inductive Bias
  - **Why needed here**: The paper identifies the temporal inductive bias of diffusion models as a key factor in reward overoptimization. Understanding this concept is essential for appreciating the motivation behind TDPO's timestep-dependent rewards.
  - **Quick check question**: How does the temporal inductive bias of diffusion models differ from the reward structure used in traditional alignment methods, and why does this mismatch lead to overoptimization?

- **Concept**: Primacy Bias
  - **Why needed here**: The paper investigates primacy bias as a potential source of reward overoptimization and proposes TDPO-R to address it. Understanding primacy bias is crucial for grasping the motivation behind the active neuron reset strategy.
  - **Quick check question**: What is primacy bias, and how does it manifest in the context of reward overoptimization in diffusion model alignment?

## Architecture Onboarding

- **Component map**:
  - Diffusion Model -> Temporal Critic -> Reward Model -> Policy (LoRA Layers)

- **Critical path**:
  1. Sample trajectories from the denoising process of the diffusion model.
  2. Compute timestep-dependent rewards using the temporal critic.
  3. Estimate gradients for the policy and critic parameters using the temporal rewards.
  4. Update the policy and critic parameters via gradient descent.
  5. Periodically reset active neurons in the critic to mitigate primacy bias.

- **Design tradeoffs**:
  - **Per-timestep vs. Per-batch Updates**: Per-timestep updates offer faster convergence and improved sample efficiency but may introduce higher variance. Per-batch updates are more stable but less efficient.
  - **Encoder Alignment**: Reusing the reward model's encoder for the temporal critic ensures consistency in feature representations but may limit the temporal critic's flexibility.

- **Failure signatures**:
  - **Overfitting to Reward Function**: The model generates images that score highly on the reward function but lack diversity or fidelity.
  - **Catastrophic Forgetting**: The model loses previously learned knowledge after resetting active neurons.
  - **Instability**: The training process becomes unstable due to excessive variance in per-timestep updates.

- **First 3 experiments**:
  1. **Ablation Study on Reward Structure**: Compare TDPO with timestep-independent rewards to validate the importance of temporal inductive bias alignment.
  2. **Neuron State Analysis**: Investigate the effects of resetting different neuron states (dormant, active, all) in the critic model to understand their roles in reward overoptimization.
  3. **Cross-Reward Generalization Evaluation**: Assess the model's performance on out-of-domain reward functions to quantify the degree of reward overoptimization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal neuron reset frequency (F) vary with different reward functions and model architectures?
- Basis in paper: [explicit] The paper mentions using a fixed frequency of 10 epochs for neuron reset in TDPO-R, but notes this could be optimized.
- Why unresolved: The paper uses a single frequency value across all experiments without exploring how it affects performance with different rewards or architectures.
- What evidence would resolve it: Systematic experiments varying F across different reward functions (Aesthetic Score, PickScore, HPSv2) and comparing performance metrics like cross-reward generalization and sample efficiency.

### Open Question 2
- Question: Can the temporal critic architecture be further simplified while maintaining performance?
- Basis in paper: [inferred] The paper uses a 5-layer MLP for the temporal critic, but notes this was chosen for efficiency without exploring alternatives.
- Why unresolved: The paper focuses on demonstrating effectiveness rather than architectural optimization, leaving potential simplifications unexplored.
- What evidence would resolve it: Comparative studies testing different temporal critic architectures (fewer layers, different activation functions) while measuring impact on reward optimization and overoptimization.

### Open Question 3
- Question: How does TDPO-R perform when aligned with multiple reward functions simultaneously?
- Basis in paper: [inferred] The paper focuses on single-reward alignment, though it mentions multi-reward learning as an unexplored area.
- Why unresolved: The paper does not investigate multi-reward scenarios, which are increasingly relevant for practical applications requiring diverse preference satisfaction.
- What evidence would resolve it: Experiments comparing single vs. multi-reward alignment with TDPO-R, measuring trade-offs between different objectives and overall alignment quality.

### Open Question 4
- Question: What is the relationship between dormant neuron percentage and model generalization capacity?
- Basis in paper: [explicit] The paper shows dormant neurons increase during training and that resetting them worsens performance, but doesn't quantify the relationship with generalization.
- Why unresolved: While the paper demonstrates dormant neurons help prevent overoptimization, it doesn't measure how their presence affects broader generalization capabilities.
- What evidence would resolve it: Correlation studies between dormant neuron percentage and generalization metrics across different tasks, potentially revealing optimal ranges for different applications.

## Limitations
- The discovery of dormant neurons acting as regularization is novel but requires further validation across different architectures and training regimes.
- The per-timestep update strategy's stability claims need more rigorous stability analysis, particularly regarding gradient variance.
- The long-term effects of active neuron reset on model generalization remain unclear.

## Confidence
- Temporal inductive bias alignment effectiveness: **High** - Well-supported by ablation studies and cross-reward generalization results
- Primacy bias discovery and mitigation: **Medium** - Novel finding with limited validation across different model scales
- Sample efficiency improvements: **High** - Clear quantitative evidence across multiple reward functions
- Active neuron reset mechanism: **Medium** - Promising results but limited ablation studies on reset frequency and scope

## Next Checks
1. **Extended neuron state analysis**: Conduct systematic experiments varying reset frequency and scope (dormant vs active vs all neurons) across different model scales to validate the primacy bias discovery and optimization of the reset mechanism.

2. **Long-term stability evaluation**: Implement comprehensive stability monitoring during training, including gradient variance tracking and early stopping criteria, to validate the per-timestep update strategy's claims of improved stability.

3. **Cross-domain generalization study**: Evaluate TDPO-R's performance on out-of-distribution prompts and reward functions not seen during training to assess robustness against reward overoptimization in truly novel scenarios.