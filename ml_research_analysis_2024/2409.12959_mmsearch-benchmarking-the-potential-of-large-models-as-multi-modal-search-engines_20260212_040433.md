---
ver: rpa2
title: 'MMSearch: Benchmarking the Potential of Large Models as Multi-modal Search
  Engines'
arxiv_id: '2409.12959'
source_url: https://arxiv.org/abs/2409.12959
tags:
- search
- website
- akita
- museum
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMSearch, the first benchmark to evaluate
  large multimodal models (LMMs) as AI search engines. It proposes MMSearch-Engine,
  a pipeline that integrates LMMs with web search to handle multimodal queries, and
  MMSearch, a dataset of 300 manually curated queries across 14 subfields.
---

# MMSearch: Benchmarking the Potential of Large Models as Multi-modal Search Engines

## Quick Facts
- arXiv ID: 2409.12959
- Source URL: https://arxiv.org/abs/2409.12959
- Reference count: 40
- Primary result: GPT-4o with MMSearch-Engine pipeline outperforms Perplexity Pro in multimodal search evaluation

## Executive Summary
This paper introduces MMSearch, the first benchmark designed to evaluate large multimodal models (LMMs) as AI search engines. The authors propose MMSearch-Engine, a pipeline that integrates LMMs with web search capabilities to handle multimodal queries, and MMSearch, a dataset of 300 manually curated queries spanning 14 subfields. The benchmark evaluates 18 LMMs on their ability to process multimodal queries, retrieve relevant information, and generate accurate responses.

Experimental results demonstrate that GPT-4o with MMSearch-Engine achieves the highest performance across multiple metrics, surpassing the commercial product Perplexity Pro in end-to-end evaluation. The study reveals that current LMMs still struggle with reranking and requery tasks, suggesting areas for improvement. Notably, the authors find that scaling test-time computation shows more promise than scaling model size for improving search performance.

## Method Summary
The MMSearch benchmark consists of two main components: MMSearch-Engine, a pipeline that integrates LMMs with web search capabilities, and MMSearch, a manually curated dataset of 300 multimodal queries across 14 subfields. The pipeline processes queries through multiple stages including search integration, reranking, and requery mechanisms. Each LMM is evaluated using a 5-shot testing approach with 30 queries per model. Performance is assessed through multiple metrics including accuracy, factuality, and end-to-end search effectiveness. Human annotators evaluate both relevance and factual accuracy of responses, with multiple annotator agreement used to ensure reliability.

## Key Results
- GPT-4o with MMSearch-Engine pipeline achieves highest performance, surpassing Perplexity Pro in end-to-end evaluation
- Current LMMs show significant struggles with reranking and requery tasks in multimodal search scenarios
- Test-time computation scaling demonstrates more promise than model size scaling for improving search performance

## Why This Works (Mechanism)
The benchmark succeeds by providing a standardized framework for evaluating LMMs in realistic multimodal search scenarios. The pipeline architecture allows LMMs to leverage external web search capabilities, bridging the gap between pure language models and comprehensive search engines. The manual curation of queries ensures high-quality, diverse test cases that span multiple domains. Human evaluation of both relevance and factuality provides robust assessment beyond automated metrics, capturing nuances in multimodal understanding and response quality.

## Foundational Learning

**Multimodal Query Processing** - why needed: LMMs must understand and process queries containing multiple input modalities (text, images, etc.) simultaneously
quick check: Can the model correctly interpret a query combining visual elements with textual descriptions?

**Web Search Integration** - why needed: Effective search engines require access to current information beyond model training data
quick check: Does the pipeline successfully retrieve relevant information from web search APIs?

**Reranking Mechanisms** - why needed: Initial search results often require refinement to identify the most relevant information
quick check: Can the model effectively prioritize and reorder search results based on query relevance?

**Factuality Assessment** - why needed: Search engines must provide accurate information, not just relevant results
quick check: Are generated responses factually consistent with retrieved information and external knowledge?

## Architecture Onboarding

**Component Map**: Query Input -> Multimodal Processing -> Web Search Integration -> Reranking -> Response Generation -> Factuality Check

**Critical Path**: The core workflow follows: input query processing → search result retrieval → reranking optimization → final response generation with factuality verification

**Design Tradeoffs**: The pipeline trades computational efficiency for improved accuracy through multiple processing stages. The 5-shot evaluation approach balances thorough testing with practical constraints. Manual query curation ensures quality but limits dataset size compared to automated generation methods.

**Failure Signatures**: Common failure modes include inability to effectively rerank search results, struggles with requery when initial results are insufficient, and generation of factually incorrect responses despite relevant retrieval. Models particularly struggle when queries require synthesizing information across multiple retrieved documents.

**3 First Experiments**:
1. Evaluate baseline LMM performance without web search integration to quantify the value added by the search pipeline
2. Test different reranking strategies (e.g., simple vs. learned approaches) to identify optimal methods
3. Compare performance across different query complexity levels (single vs. multi-modal, simple vs. complex topics)

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions for future research.

## Limitations
- Benchmark covers only 300 manually curated queries across 14 subfields, potentially limiting generalizability
- Reliance on human annotations introduces subjectivity despite multiple annotator agreement
- Evaluation framework uses fixed 5-shot testing with 30 queries per LMM, which may not capture performance variations across different distributions
- Results based on comparison with 18 LMMs, potentially missing other competitive models
- Test-time computation scaling results require broader validation beyond the specific experimental setup

## Confidence

**High confidence**: Benchmark creation methodology and evaluation framework are sound and well-documented. Finding that GPT-4o performs best among tested models is reliable within study scope.

**Medium confidence**: Conclusions about LMMs' struggles with reranking and requery tasks are supported but may be dataset-dependent. Test-time computation scaling results show promise but require broader validation.

**Low confidence**: Generalizability of findings to all multimodal search scenarios and different query distributions.

## Next Checks

1. Expand benchmark to include larger and more diverse query set (>1000 queries) across additional domains to test generalizability of findings.

2. Conduct cross-dataset validation by testing LMM performance on established multimodal datasets (e.g., Infoseek, MM-Vet) to verify consistency of results.

3. Perform ablation studies on MMSearch-Engine pipeline components to quantify contribution of each stage (search integration, reranking, requery) to overall performance.