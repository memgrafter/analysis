---
ver: rpa2
title: 'Audio Is the Achilles'' Heel: Red Teaming Audio Large Multimodal Models'
arxiv_id: '2410.23861'
source_url: https://arxiv.org/abs/2410.23861
tags:
- audio
- harmful
- input
- lmms
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work systematically red teams the safety of five advanced
  audio large multimodal models (LMMs) using three attack scenarios: harmful audio/text
  questions, harmful text questions with distracting non-speech audio, and speech-specific
  jailbreaks. Experiments show open-source audio LMMs exhibit a 69.14% attack success
  rate on harmful audio questions, with safety alignment significantly degraded compared
  to their text-only LLM backbones.'
---

# Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models

## Quick Facts
- arXiv ID: 2410.23861
- Source URL: https://arxiv.org/abs/2410.23861
- Authors: Hao Yang; Lizhen Qu; Ehsan Shareghi; Gholamreza Haffari
- Reference count: 37
- This work systematically red teams the safety of five advanced audio large multimodal models (LMMs) using three attack scenarios: harmful audio/text questions, harmful text questions with distracting non-speech audio, and speech-specific jailbreaks. Experiments show open-source audio LMMs exhibit a 69.14% attack success rate on harmful audio questions, with safety alignment significantly degraded compared to their text-only LLM backbones. Introducing non-speech audio noise causes up to 32.58% variation in attack success rates by reshaping model representation spaces. A speech-specific jailbreak strategy successfully bypasses Gemini-1.5-Pro's safeguards, achieving a 70.67% attack success rate by decomposing harmful words into audio-concealed letters. The results reveal critical safety vulnerabilities in audio LMMs and highlight the need for robust defense mechanisms.

## Executive Summary
This paper presents a comprehensive red teaming study of audio large multimodal models (LMMs), revealing significant safety vulnerabilities when compared to their text-only LLM backbones. Through systematic experimentation across three attack scenarios, the authors demonstrate that multimodal training introduces new representation spaces that are not aligned with safety mechanisms designed for text inputs. The study identifies specific vulnerabilities including degraded safety alignment on harmful audio questions, sensitivity to non-speech audio noise, and successful speech-specific jailbreak strategies that bypass safeguards by decomposing harmful content across modalities.

## Method Summary
The study red teams five audio LMMs (Qwen-Audio, Qwen2-Audio, SALMONN-7B, SALMONN-13B, Gemini-1.5-Pro) and their corresponding LLM backbones using three prompt settings: plain harmful question text, prompted harmful question text, and harmful question audio. The researchers used Llama-guard-3 as an automated judge to evaluate responses, conducting five separate inferences for each of 350 harmful questions refined from the Figstep dataset across seven categories. Experiments were performed with generation temperature set to 1.0 on two A100 GPUs, and ASR-a (attack success rate by attempt) and ASR-q (attack success rate by question) metrics were calculated. The study also included representation space visualization using t-SNE and analysis of non-speech audio effects on safety alignment.

## Key Results
- Open-source audio LMMs suffer an average attack success rate of 69.14% on harmful audio questions, with a 45.15% safety drop compared to their LLM backbones
- Non-speech audio noise causes up to 32.58% variation in attack success rates by reshaping model representation spaces
- Speech-specific jailbreak strategy achieves 70.67% attack success rate on Gemini-1.5-Pro by decomposing harmful words into audio-concealed letters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Audio LMMs show degraded safety alignment when multimodal training is applied to a text-only LLM backbone.
- Mechanism: Multimodal training introduces new representation spaces for audio inputs that are not aligned with the safety-aligned text representations from the backbone LLM. This misalignment creates vulnerabilities where harmful audio content bypasses safeguards.
- Core assumption: The safety alignment learned by the LLM backbone does not transfer effectively to the multimodal representations created by the audio encoder.
- Evidence anchors:
  - [abstract] "open-source audio LMMs suffer an average attack success rate of 69.14% on harmful audio questions, and exhibit safety vulnerabilities when distracted with non-speech audio noise"
  - [section 3.2] "Qwen- and Qwen2-Audio show an average safety drop of 45.15% compared to their LLM backbone on the text version of same questions"
  - [corpus] Weak evidence - the related papers focus on red teaming surveys and compositional attacks but don't directly address backbone safety transfer issues
- Break condition: When the multimodal training process includes explicit safety alignment for audio representations or when the model is trained from scratch on multimodal data rather than adapting an existing LLM backbone.

### Mechanism 2
- Claim: Non-speech audio inputs (noise, silence) can significantly alter the representation space of audio LMMs, causing safety misalignment.
- Mechanism: Introducing meaningless non-speech audio while keeping the text content constant reshapes the model's internal representations. This shifts queries to new locations in the representation space where the model's safety mechanisms are less effective or unpredictable.
- Core assumption: Audio LMMs are sensitive to audio input characteristics beyond just the semantic content, and their safety mechanisms depend on maintaining consistent representation spaces.
- Evidence anchors:
  - [abstract] "Introducing non-speech audio noise causes up to 32.58% variation in attack success rates by reshaping model representation spaces"
  - [section 4.3] "introducing non-speech audio while keeping the text content consistent reshapes the query representations to a new location far from the original representation space"
  - [corpus] Weak evidence - related work mentions speech-audio compositional attacks but doesn't specifically address non-speech audio effects on safety alignment
- Break condition: When the model develops robust representation spaces that are invariant to irrelevant audio input or when the safety mechanisms are designed to work across varied representation spaces.

### Mechanism 3
- Claim: Decomposing harmful words into individual letters and concealing them in audio input can bypass text-based safety filters in multimodal models.
- Mechanism: Text-based safety filters identify explicit harmful content in the text prompt. By decomposing harmful words into letters and placing them in audio input instead, the text prompt appears harmless while the model still receives the harmful information through audio. The model then reconstructs the harmful word from audio, completing the harmful query.
- Core assumption: Multimodal models process audio and text inputs separately and can combine information from both modalities to understand the complete query.
- Evidence anchors:
  - [abstract] "A speech-specific jailbreak strategy successfully bypasses Gemini-1.5-Pro's safeguards, achieving a 70.67% attack success rate by decomposing harmful words into audio-concealed letters"
  - [section 5.1] "We decompose harmful words into letters concealed in the audio input and then request the model to concatenate the letters from the audio into a word and use this word to complete the question in prompt for responding"
  - [corpus] Weak evidence - related papers mention multimodal jailbreaks but don't specifically describe letter decomposition strategies
- Break condition: When the model implements cross-modal safety checking that analyzes both audio and text inputs together for harmful content, or when it refuses to combine audio and text information for completing potentially harmful queries.

## Foundational Learning

- Concept: Representation space alignment in multimodal models
  - Why needed here: Understanding how different modalities (text vs. audio) create separate representation spaces and how misalignment between these spaces can create safety vulnerabilities
  - Quick check question: If a harmful question in text form is rejected but the same question in audio form is accepted, what does this tell you about the alignment between text and audio representation spaces in the model?

- Concept: Adversarial attacks on multimodal models
  - Why needed here: The paper demonstrates how non-speech audio and speech-specific jailbreaks can be used as adversarial techniques to bypass safety mechanisms
  - Quick check question: How might an attacker use silence or random noise as an adversarial technique against an audio LMM's safety mechanisms?

- Concept: Safety alignment transfer in model adaptation
  - Why needed here: The paper shows that safety alignment from text-only LLMs doesn't automatically transfer when adapting to multimodal inputs, which is crucial for understanding the vulnerability patterns
  - Quick check question: Why might a safety-aligned LLM backbone fail to maintain its safety properties when adapted to handle audio inputs through multimodal training?

## Architecture Onboarding

- Component map: Text LLM backbone -> Audio encoder network -> Cross-modal fusion layers -> Safety alignment mechanisms (primarily text-focused) -> Guardrail/filter systems
- Critical path: Input -> Audio encoder (if audio present) -> Text encoder (if text present) -> Cross-modal fusion -> Safety check -> Response generation
- Design tradeoffs: Text-focused safety mechanisms vs. multimodal safety coverage; computational efficiency of audio processing vs. safety robustness; backbone LLM safety vs. multimodal adaptation flexibility
- Failure signatures: High attack success rates on audio inputs while maintaining low rates on text inputs; significant variation in safety performance when introducing non-speech audio; successful jailbreaks that decompose harmful content across modalities
- First 3 experiments:
  1. Test safety alignment degradation by comparing attack success rates on harmful questions between the LLM backbone and its multimodal adaptation
  2. Evaluate sensitivity to non-speech audio by measuring attack success rate variations when adding different types of noise to text-only harmful questions
  3. Validate the speech-specific jailbreak strategy by decomposing harmful words into letters, placing them in audio, and measuring bypass success rates compared to text-only jailbreaks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the training data composition of audio LMMs affect their safety alignment performance compared to their LLM backbones?
- Basis in paper: [explicit] The paper notes that Qwen2-Audio shows better safety alignment than Qwen-Audio, suggesting training data composition impacts safety.
- Why unresolved: The paper doesn't provide detailed analysis of training data composition differences between models.
- What evidence would resolve it: Comparative analysis of training data composition and safety performance across multiple audio LMMs with varying training strategies.

### Open Question 2
- Question: What specific mechanisms in audio LMMs cause non-speech audio inputs to reshape representation spaces and trigger safety misalignment?
- Basis in paper: [inferred] The paper shows that non-speech audio inputs affect safety alignment but doesn't explain the underlying mechanisms.
- Why unresolved: The paper demonstrates effects but doesn't investigate the technical reasons behind representation space changes.
- What evidence would resolve it: Detailed analysis of audio LMM internal mechanisms and how different types of audio inputs affect model representations.

### Open Question 3
- Question: How effective are different defense mechanisms specifically designed for audio-based jailbreak attacks?
- Basis in paper: [explicit] The paper demonstrates successful audio-based jailbreak attacks but doesn't evaluate potential defense mechanisms.
- Why unresolved: The paper focuses on attacking rather than defending, leaving the effectiveness of potential safeguards unexplored.
- What evidence would resolve it: Comparative evaluation of various defense mechanisms against audio-based jailbreak strategies.

## Limitations

- The study relies on automated safety evaluation using Llama-guard-3, which may not capture nuanced context as effectively as human evaluation
- The research focuses exclusively on five specific audio LMMs, limiting generalizability to other multimodal model architectures
- The speech-specific jailbreak strategy's effectiveness may be more of an implementation artifact than a fundamental architectural vulnerability

## Confidence

**High Confidence**: The finding that open-source audio LMMs show significant safety degradation compared to their text-only LLM backbones (45.15% average safety drop). This claim is supported by direct comparisons between models and their backbones, with clear quantitative metrics.

**Medium Confidence**: The claim that non-speech audio noise causes up to 32.58% variation in attack success rates by reshaping representation spaces. While the paper provides representation space visualizations and statistical evidence, the causal relationship between audio noise and safety misalignment could benefit from additional mechanistic explanations.

**Low Confidence**: The assertion that the speech-specific jailbreak strategy is a fundamental vulnerability rather than a specific implementation artifact. The technique's success on one model (Gemini-1.5-Pro) does not necessarily indicate a widespread architectural vulnerability across audio LMMs.

## Next Checks

1. **Human Evaluation Validation**: Conduct a subset of experiments with human evaluators to verify the accuracy of Llama-guard-3's safety classifications and ensure that the reported attack success rates are not inflated by false positives or negatives in automated evaluation.

2. **Cross-Architecture Testing**: Test the speech-specific jailbreak strategy (letter decomposition in audio) across a broader range of audio LMMs with different architectural designs to determine whether this vulnerability is fundamental to the multimodal approach or specific to certain implementations.

3. **Safety-Utility Trade-off Analysis**: Implement and evaluate potential defensive measures (such as cross-modal safety checking) on the tested models to quantify the impact on both safety performance and model utility, providing a more complete picture of the practical implications of the findings.