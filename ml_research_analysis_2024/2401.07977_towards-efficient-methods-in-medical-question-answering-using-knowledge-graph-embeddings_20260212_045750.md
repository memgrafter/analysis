---
ver: rpa2
title: Towards Efficient Methods in Medical Question Answering using Knowledge Graph
  Embeddings
arxiv_id: '2401.07977'
source_url: https://arxiv.org/abs/2401.07977
tags:
- embeddings
- knowledge
- language
- bert
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving machine reading
  comprehension (MRC) in the medical domain without expensive in-domain pre-training.
  The authors propose a method to inject domain knowledge into pre-trained language
  models using knowledge graph embeddings (KGEs) from the UMLS.
---

# Towards Efficient Methods in Medical Question Answering using Knowledge Graph Embeddings

## Quick Facts
- arXiv ID: 2401.07977
- Source URL: https://arxiv.org/abs/2401.07977
- Reference count: 35
- This paper proposes a method to inject medical domain knowledge into open-domain language models using knowledge graph embeddings, achieving performance on par with or exceeding domain-specific models without expensive in-domain pre-training.

## Executive Summary
This paper addresses the challenge of improving machine reading comprehension in the medical domain without expensive in-domain pre-training. The authors propose a method to inject domain knowledge into pre-trained language models using knowledge graph embeddings (KGEs) from the UMLS. Their approach involves aligning KGEs with the embedding space of open-domain models (BERT and RoBERTa) using multi-layer perceptrons (MLPs), then fusing these aligned embeddings with the model inputs during fine-tuning. They apply their method to two MRC tasks: span detection (COVID-QA) and multiple-choice questions (PubMedQA). On COVID-QA, their method allows BERT and RoBERTa to achieve performance on par with or exceeding that of domain-specific models like BioBERT and SciBERT, with BERT showing up to 6.4% improvement in exact match scores and RoBERTa up to 7.6% improvement. On PubMedQA, BERT shows up to 5.2% improvement in accuracy over regular fine-tuning.

## Method Summary
The proposed method injects medical domain knowledge into open-domain language models (BERT and RoBERTa) for medical question answering without requiring expensive in-domain pre-training. The approach involves using MetaMap to identify and link medical entities in questions to UMLS concepts, then aligning pre-trained UMLS knowledge graph embeddings (50-dimensional) with the language model embedding space (768-dimensional) using a multi-layer perceptron (MLP). The authors also explore incorporating entity definitions by generating embeddings from the definitions and averaging them with the aligned knowledge graph embeddings. During fine-tuning, these external knowledge vectors are concatenated with the model inputs using either the DEKCOR or BERTRAM strategy. The method is evaluated on two medical question answering tasks: COVID-QA (span detection) and PubMedQA (multiple-choice questions).

## Key Results
- BERT and RoBERTa with knowledge injection achieve performance on par with or exceeding domain-specific models (BioBERT, SciBERT) on COVID-QA
- BERT shows up to 6.4% improvement in exact match scores and RoBERTa up to 7.6% improvement on COVID-QA
- On PubMedQA, BERT shows up to 5.2% improvement in accuracy over regular fine-tuning
- The proposed MLP-based alignment technique overcomes vocabulary overlap limitations that constrain previous approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning external medical knowledge embeddings with open-domain model embeddings enables domain proficiency without in-domain pre-training.
- Mechanism: The authors use a Multi-Layer Perceptron (MLP) to learn a transformation that aligns UMLS knowledge graph embeddings (50-dimensional) with the embedding space of BERT/RoBERTa (768-dimensional). This alignment ensures that the semantics captured in the knowledge graph are represented in the same space as the model's vocabulary, allowing seamless integration during fine-tuning.
- Core assumption: Knowledge graph embeddings contain domain-relevant information that, when properly aligned, can enhance the model's understanding of medical entities and improve question answering performance.
- Evidence anchors:
  - [abstract]: "we propose a resource-efficient approach for injecting domain knowledge into a model without relying on such domain-specific pre-training" and "The aligned embeddings are fused with open-domain LMs BERT and RoBERTa that are fine-tuned for two MRC tasks"
  - [section]: "we propose a simple embedding homogenization technique, inspired by work on feed-forward neural networks (FFNN) [10], to fuse entity KGE into the question representation during the fine-tuning phase for MRC"
- Break condition: If the MLP fails to adequately learn the transformation, or if the knowledge graph embeddings do not contain relevant domain information, the alignment would not provide meaningful enhancements.

### Mechanism 2
- Claim: Incorporating entity definitions alongside knowledge graph embeddings provides additional context that improves model performance.
- Mechanism: The authors extract definitions for medical entities from the UMLS metathesaurus and generate embeddings by passing these definitions through the language model in feature extraction mode. These definition embeddings are then averaged with the aligned knowledge graph embeddings to form a richer external knowledge vector that is integrated into the model's input representation.
- Core assumption: Definitions provide semantic context that complements the structural information in knowledge graph embeddings, leading to a more comprehensive understanding of medical entities.
- Evidence anchors:
  - [section]: "we also explore the use of definition embeddings for the identified domain terms" and "we decided to average the two embeddings to form the final external knowledge vector"
  - [section]: "we hypothesize that KGE alone would not be enough to see appreciable gains"
- Break condition: If the definitions do not add meaningful semantic information beyond what is captured in the knowledge graph embeddings, or if the averaging process dilutes important information, the performance gains may be minimal.

### Mechanism 3
- Claim: The proposed embedding integration method circumvents the vocabulary overlap requirement of previous approaches, enabling utilization of the full knowledge graph.
- Mechanism: Unlike previous methods like E-BERT that rely on the Mikolov cross-lingual alignment strategy requiring substantial vocabulary overlap, the authors' method uses an MLP to align embeddings without depending on common terms between the knowledge graph and the model's vocabulary. This allows the method to leverage information for all entities in the knowledge graph, not just those present in both vocabularies.
- Core assumption: Modern transformer vocabularies consist of subword tokens that do not constitute complete words, making vocabulary overlap with knowledge graph entities difficult to achieve; an alignment method that does not depend on overlap can utilize the full breadth of knowledge graph information.
- Evidence anchors:
  - [section]: "Our technique does not rely on an intersection between the set of terms to be aligned (Knowledge Graph entities/Language Model vocabulary), unlike existing approaches such as the Mikolov [12] approach"
  - [section]: "it allows the method to scale well to domains where there is no significant vocabulary overlap, such as ours"
- Break condition: If the MLP alignment is insufficient to bridge the semantic gap between the knowledge graph embeddings and the model's embedding space, or if the knowledge graph contains noisy or irrelevant information, the lack of vocabulary overlap constraint may not provide a significant advantage.

## Foundational Learning

- Concept: Knowledge Graph Embeddings (KGE)
  - Why needed here: Understanding KGEs is crucial because the paper's approach relies on aligning these embeddings with language model embeddings to inject domain knowledge.
  - Quick check question: What is the purpose of training knowledge graph embeddings, and how do they capture the semantics of entities and their relations?
- Concept: Multi-Layer Perceptrons (MLPs) for Embedding Alignment
  - Why needed here: The paper uses an MLP to learn a transformation that aligns knowledge graph embeddings with language model embeddings, which is central to the proposed method.
  - Quick check question: How does an MLP learn to map one embedding space to another, and why is this useful for integrating external knowledge?
- Concept: Fine-tuning vs. Pre-training
  - Why needed here: The paper's approach focuses on fine-tuning open-domain models with aligned external embeddings instead of expensive in-domain pre-training, so understanding the difference is important.
  - Quick check question: What is the difference between fine-tuning and pre-training a language model, and why might fine-tuning with external knowledge be more efficient than pre-training on domain-specific corpora?

## Architecture Onboarding

- Component map: Entity Linking -> KGE Homogenization -> Definition Embeddings -> Fine-tuning with External Knowledge Integration
- Critical path: Entity Linking → KGE Homogenization → Definition Embeddings → Fine-tuning with External Knowledge Integration
- Design tradeoffs:
  - Using an MLP for alignment provides flexibility but requires training an additional model.
  - Averaging KGE and definition embeddings simplifies integration but may lose some information.
  - DEKCOR concatenation preserves original text but may introduce noise, while BERTRAM concatenation intertwines embeddings with text.
- Failure signatures:
  - Poor performance on entity linking indicates issues with MetaMap or UMLS coverage.
  - Failure to learn the MLP transformation suggests the KGE and LM embedding spaces are too dissimilar.
  - Degradation in model performance after integration may indicate noisy or irrelevant external embeddings.
- First 3 experiments:
  1. Train the MLP to align a small set of KGE with BERT embeddings and evaluate the alignment loss.
  2. Generate definition embeddings for a subset of entities and compare them with KGE to assess complementarity.
  3. Integrate aligned KGE and definition embeddings into BERT using DEKCOR concatenation and evaluate performance on a small MRC dataset.

## Open Questions the Paper Calls Out
None

## Limitations
- The approach relies heavily on UMLS coverage, with approximately 40-60% of examples potentially lacking sufficient linked entities for knowledge injection.
- The paper lacks direct comparison to baseline performance of open-domain models without knowledge injection, making it difficult to assess absolute improvement.
- The quality and relevance of pre-trained UMLS knowledge graph embeddings for the specific QA tasks is not validated.

## Confidence

**High Confidence Claims**:
- The proposed method successfully improves performance on both COVID-QA and PubMedQA tasks compared to standard fine-tuning of BERT and RoBERTa.
- The approach achieves performance comparable to or better than domain-specific models (BioBERT, SciBERT) on COVID-QA without requiring expensive in-domain pre-training.
- The MLP-based alignment technique effectively overcomes vocabulary overlap limitations that constrain previous approaches.

**Medium Confidence Claims**:
- The specific contribution of definition embeddings to performance improvements (the paper hypothesizes they provide additional context but does not provide direct ablation evidence).
- The relative performance advantages across different datasets and model architectures (results show variation that may depend on dataset characteristics).

## Next Checks

1. **Ablation Study with Base Model Comparison**: Run experiments comparing the proposed method against the baseline performance of BERT and RoBERTa without any knowledge injection on both COVID-QA and PubMedQA to quantify the absolute improvement achieved.

2. **Knowledge Graph Coverage Analysis**: Analyze the relationship between the number of linked entities per example and model performance to determine whether the improvements correlate with entity availability, and test the method on datasets with different entity densities.

3. **Embedding Quality Validation**: Conduct a qualitative analysis of the UMLS KGEs by examining nearest neighbors for medical entities in the embedding space to verify that they capture semantically meaningful relationships relevant to medical question answering.