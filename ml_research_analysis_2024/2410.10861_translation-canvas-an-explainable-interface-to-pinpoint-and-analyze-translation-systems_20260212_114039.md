---
ver: rpa2
title: 'Translation Canvas: An Explainable Interface to Pinpoint and Analyze Translation
  Systems'
arxiv_id: '2410.10861'
source_url: https://arxiv.org/abs/2410.10861
tags:
- translation
- evaluation
- canvas
- error
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Translation Canvas is a tool designed to help machine translation
  researchers analyze translation systems at both the instance and system levels.
  It integrates evaluation metrics like InstructScore, BLEU, and COMET, and provides
  a visual interface to highlight errors with explanations.
---

# Translation Canvas: An Explainable Interface to Pinpoint and Analyze Translation Systems

## Quick Facts
- arXiv ID: 2410.10861
- Source URL: https://arxiv.org/abs/2410.10861
- Authors: Chinmay Dandekar; Wenda Xu; Xi Xu; Siqi Ouyang; Lei Li
- Reference count: 5
- Translation Canvas is a tool designed to help machine translation researchers analyze translation systems at both the instance and system levels.

## Executive Summary
Translation Canvas is a web-based interface that enables machine translation researchers to analyze and compare translation systems using multiple evaluation metrics. The tool provides both fine-grained instance-level error analysis with visual highlighting and system-level performance dashboards. It supports manual or file-based input for evaluation and offers advanced search capabilities to filter instances based on complex criteria. User studies indicate that Translation Canvas is more enjoyable and easier to use than existing tools like COMET and SacreBLEU.

## Method Summary
Translation Canvas is a Flask-based web application that processes translation evaluation data (source, prediction, and reference texts) and computes multiple metrics including BLEU, COMET, and InstructScore. The system stores data in DuckDB and provides a visual interface for analysis. Users can input data manually or through files using custom extraction functions, then access dashboards showing error distributions and performance metrics, or perform instance-level analysis with highlighted errors and explanations. The tool supports advanced search queries combining error types, text content, and language filters to enable targeted analysis.

## Key Results
- Translation Canvas provides fine-grained error analysis by visually highlighting erroneous spans with color coding and tooltips containing natural language explanations
- The tool enables efficient system-level performance analysis through dashboards displaying error distributions and metric scores side-by-side for multiple models
- Advanced search functionality allows complex queries combining error types, text content, and language filters for targeted analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translation Canvas provides fine-grained instance-level error analysis by visually highlighting erroneous spans with color coding and tooltips containing natural language explanations.
- Mechanism: The system uses InstructScore to generate error classifications and explanations for each prediction-reference pair, then renders the prediction text with red/orange highlights for major/minor errors and tooltips for detailed explanations.
- Core assumption: InstructScore's error classification and explanation generation is sufficiently accurate to be useful for researchers analyzing translation errors.
- Evidence anchors:
  - [abstract] "It supports fine-grained analysis by highlighting error spans with explanations and selectively displaying systems' predictions."
  - [section] "The erroneous section of the prediction text is highlighted with a red or orange color... When the user hovers over the red or orange text, a tooltip appears. This contains helpful information about the error types, scale, and explanation made in the prediction."
- Break condition: If InstructScore's error classifications are inaccurate or explanations are unhelpful, the visual highlighting becomes misleading and the tool loses its fine-grained analysis capability.

### Mechanism 2
- Claim: Translation Canvas enables efficient system-level performance analysis through dashboards that display error distributions and metric scores side-by-side for multiple models.
- Mechanism: The system aggregates instance-level evaluation results (BLEU, COMET, InstructScore) and error type distributions across the entire corpus, then presents them in comparative histograms and tables that allow researchers to quickly identify performance gaps between models.
- Core assumption: Aggregating and visualizing multiple evaluation metrics provides more comprehensive insights than single-score approaches like traditional BLEU or COMET alone.
- Evidence anchors:
  - [abstract] "Translation Canvas assists machine translation researchers in comprehending system-level model performance by identifying common errors (their frequency and severity) and analyzing relationships between different systems based on various evaluation metrics."
  - [section] "It presents histograms on the distribution of InstructScore, the distribution of COMET, the distribution of the error types of a model, and the corpus-level BLEU, COMET and InstructScore of the models."
- Break condition: If the dashboard visualizations are cluttered or difficult to interpret, or if the aggregated metrics don't capture meaningful differences between models, the system-level analysis becomes less useful.

### Mechanism 3
- Claim: Translation Canvas's advanced search functionality enables targeted analysis by allowing complex queries combining error types, text content, and language filters.
- Mechanism: The system implements SQL-style regular expression search with AND/OR/AND NOT conjunctions across multiple categories (errors, text, languages), allowing users to construct precise filters that highlight matching errors in blue for easy identification.
- Core assumption: Researchers need to perform complex, multi-criteria searches to identify specific patterns or issues in translation outputs that simple keyword searches cannot capture.
- Evidence anchors:
  - [section] "Users can search by the following categories: • Errors (Type, Scale, and Explanation) • Text (Source, Prediction, and Reference) • Languages (Source and Target)... They can also join together an arbitrary number of queries together with a choice of 'AND', 'OR', and 'AND NOT' conjunctions."
- Break condition: If the search syntax is too complex for users to construct effective queries, or if the system cannot efficiently execute complex searches on large datasets, the targeted analysis capability is diminished.

## Foundational Learning

- Concept: Error classification and severity assessment
  - Why needed here: Understanding how errors are categorized (major vs minor) and scored is essential for interpreting the visual highlighting and dashboard metrics correctly.
  - Quick check question: What distinguishes a "major" error from a "minor" error in the context of translation evaluation?

- Concept: Multiple evaluation metrics and their relationships
  - Why needed here: The system uses BLEU, COMET, and InstructScore together; understanding what each measures and how they complement each other is crucial for comprehensive analysis.
  - Quick check question: How might BLEU, COMET, and InstructScore differ in their assessment of the same translation output?

- Concept: Corpus-level vs instance-level analysis
  - Why needed here: The tool provides both granular instance analysis and aggregated system-level views; understanding when to use each is important for effective evaluation.
  - Quick check question: When would you choose to analyze a specific instance versus examining system-level performance metrics?

## Architecture Onboarding

- Component map: Flask backend with Jinja templates for frontend -> DuckDB for data storage and querying -> InstructScore, BLEU, and COMET evaluation engines -> File input processing with custom extraction functions -> Search query parser with SQL-style regular expressions -> Dashboard rendering engine for histograms and comparisons

- Critical path: 1. User submits instances (manual or file input) 2. System processes and stores instances in DuckDB 3. Evaluation metrics are computed and stored 4. User accesses dashboard or instance analysis page 5. System queries DuckDB for relevant data 6. Frontend renders visualizations and highlights

- Design tradeoffs:
  - Flexibility vs simplicity in file input (custom extraction functions vs rigid formats)
  - Real-time computation vs precomputed metrics (affects performance)
  - Rich visualizations vs loading speed (dashboard complexity)
  - SQL-style search power vs user-friendliness (search interface complexity)

- Failure signatures:
  - Dashboard not loading: likely DuckDB query issues or data aggregation problems
  - Search returning no results: incorrect query syntax or data not properly indexed
  - Highlights not appearing: InstructScore evaluation failure or rendering issues
  - File upload failing: custom extraction function errors or unsupported format

- First 3 experiments:
  1. Submit a small set of manually entered instances with known errors and verify that highlights and tooltips appear correctly
  2. Test the file input functionality with a simple CSV file to ensure custom extraction works
  3. Use the search feature with basic AND/OR queries to confirm it filters instances and highlights matching errors in blue

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Translation Canvas compare to human expert evaluations in identifying nuanced translation errors that automatic metrics might miss?
- Basis in paper: [inferred] The paper mentions that automatic metrics may not fully capture nuanced aspects of translation quality and that human expert evaluations could offer different insights.
- Why unresolved: The paper does not provide direct comparisons between Translation Canvas's automated analysis and human expert evaluations for identifying subtle translation errors.
- What evidence would resolve it: Conducting a study where human experts evaluate the same translations analyzed by Translation Canvas, comparing their findings to the tool's automated error identifications and classifications.

### Open Question 2
- Question: How does the user-generated re-ranking data collected by Translation Canvas impact the accuracy and reliability of the system's evaluation metrics over time?
- Basis in paper: [explicit] The paper discusses implementing a re-ranking feature for users to adjust prediction rankings and mentions plans to use this data for future improvements.
- Why unresolved: The paper does not provide any results or analysis on how the collected re-ranking data has been used to improve the system's evaluation metrics or its impact on accuracy.
- What evidence would resolve it: Analyzing the changes in Translation Canvas's evaluation accuracy before and after incorporating user-generated re-ranking data, and measuring improvements in metric reliability.

### Open Question 3
- Question: What are the specific challenges and limitations in adapting Translation Canvas for evaluating translations in low-resource language pairs?
- Basis in paper: [inferred] The paper does not discuss the tool's performance or limitations when applied to low-resource language pairs, which often present unique challenges in machine translation evaluation.
- Why unresolved: There is no mention of testing or analyzing Translation Canvas's effectiveness on low-resource language pairs, which may have different error patterns and evaluation needs.
- What evidence would resolve it: Conducting evaluations of Translation Canvas on low-resource language pairs, identifying specific challenges such as data scarcity, unique error types, or metric limitations, and proposing adaptations to address these issues.

## Limitations

- InstructScore's error classification accuracy and consistency across different translation domains and language pairs is not validated
- Dashboard visualizations and multi-metric aggregation are not statistically validated for identifying meaningful system differences
- SQL-style search syntax may present a steep learning curve for researchers without technical backgrounds

## Confidence

- Instance-level error analysis mechanism: Medium confidence (relies on InstructScore accuracy)
- System-level dashboard analysis: Medium confidence (lack of statistical validation)
- Search functionality usability: Low confidence (potential complexity barrier)

## Next Checks

1. **Error Classification Validation**: Test InstructScore's error classifications and explanations across multiple translation domains (news, literature, technical) and language pairs to assess consistency and accuracy. Compare the error types and explanations against human-annotated gold standards.

2. **Dashboard Utility Assessment**: Conduct a user study comparing Translation Canvas's multi-metric dashboards against traditional single-score evaluation tools (COMET, SacreBLEU) for identifying meaningful system differences. Measure both identification accuracy and time-to-insight.

3. **Search Functionality Usability Testing**: Evaluate the SQL-style search interface with researchers of varying technical backgrounds to determine if the power of complex queries outweighs the learning curve. Track query construction success rates and time required for common analysis tasks.