---
ver: rpa2
title: Perception of Knowledge Boundary for Large Language Models through Semi-open-ended
  Question Answering
arxiv_id: '2405.14383'
source_url: https://arxiv.org/abs/2405.14383
tags:
- answers
- knowledge
- ambiguous
- questions
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to investigate the knowledge boundary
  of large language models (LLMs) through semi-open-ended questions, which have many
  potential answers. The authors construct a dataset of semi-open-ended questions
  and use an open-sourced auxiliary model to discover more ambiguous answers that
  may be beyond the knowledge boundary of a target LLM (GPT-4).
---

# Perception of Knowledge Boundary for Large Language Models through Semi-open-ended Question Answering

## Quick Facts
- arXiv ID: 2405.14383
- Source URL: https://arxiv.org/abs/2405.14383
- Reference count: 40
- Primary result: Proposes method using semi-open-ended questions and auxiliary model to discover LLM knowledge boundaries through probability reduction and self-evaluation comparison

## Executive Summary
This paper addresses the challenge of perceiving knowledge boundaries in large language models (LLMs) by introducing semi-open-ended questions as a probing mechanism. Unlike close-ended questions with single correct answers or open-ended questions with infinite possibilities, semi-open-ended questions have multiple potential answers, making them ideal for revealing the limits of LLM knowledge. The authors construct a dataset of 953 semi-open-ended questions across 32 domains and employ an auxiliary model with probability reduction techniques to discover ambiguous answers that may lie beyond the target LLM's knowledge boundary.

The proposed method involves using an open-sourced auxiliary model (LLaMA-2-13B) to explore answers that the target LLM (GPT-4) struggles with, by estimating and reducing the generation probabilities of existing answers and their near-duplicate counterparts. The authors then compare the target LLM's self-evaluation with ground truth from RAG-based evaluation to categorize ambiguous answers into four types: unqualified answers, inaccurate evaluations, hidden correct answers, and unexpected wrong evaluations. Experimental results demonstrate that GPT-4 performs poorly on semi-open-ended questions, generating many unqualified answers and making inaccurate self-evaluations, while the auxiliary model effectively discovers more ambiguous answers.

## Method Summary
The method constructs a dataset of semi-open-ended questions and uses an auxiliary model to discover ambiguous answers beyond the target LLM's knowledge boundary. The approach involves collecting answers from the target LLM, then using an auxiliary model with probability reduction techniques to explore more diverse answer space. The probability reduction mechanism estimates semantic representations of existing answers using Moore-Penrose pseudo-inverse to reduce generation probabilities of high-probability answers. The method compares target LLM self-evaluation with RAG-based evaluation to categorize ambiguous answers into four types, revealing knowledge boundaries through discrepancies between self-assessment and ground truth.

## Key Results
- GPT-4 generates many unqualified answers (40%+) on semi-open-ended questions
- Target LLM makes inaccurate self-evaluations when encountering unfamiliar knowledge
- Auxiliary model (LLaMA-2-13B) discovers more ambiguous answers including correct answers neglected by GPT-4
- Probability reduction technique effectively increases answer diversity and reduces overlap with existing answers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Estimating semantic representations of existing answers allows probability reduction of near-duplicate answers
- Mechanism: Uses Moore-Penrose pseudo-inverse to estimate semantic representations (δx = E+∆y) of anchor tokens from existing answers, then calculates probability distributions of high-probability tokens (δy = Eδx) to reduce their generation probability in the auxiliary model
- Core assumption: Anchor tokens and their semantic-related tokens share similar semantic meanings, allowing semantic propagation through probability reduction
- Evidence anchors:
  - [abstract]: "We calculate the nearest semantic representation for existing answers to estimate their probabilities, with which we reduce the generation probability of high-probability answers to achieve a more effective generation"
  - [section]: "We calculate δx = E+∆y, where E+E = I. Here, E+ is the left Moore-Penrose pseudo-inverse... We obtain an estimated probability distribution of high-probability tokens δy and use it to prevent the generation of high-probability answers"
  - [corpus]: No direct evidence found - this is a novel mechanism specific to the paper
- Break condition: If semantic relationships between anchor tokens and their near-duplicates are not strong enough, probability reduction would not effectively filter out repetitive answers

### Mechanism 2
- Claim: Comparing LLM self-evaluation with RAG-based evaluation identifies answers beyond knowledge boundaries
- Mechanism: Evaluates ambiguous answers using both the target LLM's self-evaluation and ground truth from RAG-based evaluation, categorizing answers as unqualified, inaccurate evaluations, hidden correct answers, or unexpected wrong evaluations
- Core assumption: Discrepancies between self-evaluation and ground truth indicate unfamiliarity with knowledge, revealing knowledge boundaries
- Evidence anchors:
  - [abstract]: "Finally, we compare the results from the RAG-based evaluation and LLM self-evaluation to categorize four types of ambiguous answers that are beyond the knowledge boundary of the target LLM"
  - [section]: "We believe an answer a in each test case (q, A) to be factually correct to q if verified on public, trustworthy sources... we evaluate each answer a to its corresponding question as 1) incorrect, which contradicts reliable sources; 2) correct, which is supported by reliable sources, and 3) unverifiable"
  - [corpus]: No direct evidence found - this comparison approach appears novel to this paper
- Break condition: If the RAG-based evaluation or LLM self-evaluation mechanisms are unreliable, categorization accuracy would decrease

### Mechanism 3
- Claim: Auxiliary model generation with reduced probabilities discovers more ambiguous answers than direct prompting
- Mechanism: Applies an open-sourced auxiliary model with adjusted probability distributions to generate answers, achieving higher diversity and lower overlap with existing answers compared to direct prompting baselines
- Core assumption: Reducing generation probabilities of existing answers forces auxiliary model to explore low-probability answer space more effectively than simple prompting
- Evidence anchors:
  - [abstract]: "we apply an open-sourced auxiliary model to explore ambiguous answers for the target LLM"
  - [section]: "Experimental results show... the effectiveness of our ambiguous answer discovery method in finding more pieces of knowledge which the LLMs are unfamiliar with"
  - [corpus]: Related paper "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary Perception" suggests knowledge popularity affects boundary perception, supporting the need for exploring less popular knowledge
- Break condition: If auxiliary model lacks sufficient knowledge breadth or probability adjustment is too aggressive, generation quality would degrade

## Foundational Learning

- Concept: Semantic representation estimation using Moore-Penrose pseudo-inverse
  - Why needed here: Enables calculation of semantic representations for existing answers to estimate their generation probabilities
  - Quick check question: Why is Moore-Penrose pseudo-inverse used instead of regular matrix inverse in this context?

- Concept: Probability distribution manipulation in language model generation
  - Why needed here: Allows controlled reduction of high-probability answer generation to explore more diverse answer space
  - Quick check question: How does reducing probability of anchor tokens affect generation of semantically related tokens?

- Concept: Question categorization (answerable vs unanswerable vs semi-open-ended)
  - Why needed here: Distinguishes between different question types requiring different evaluation approaches for knowledge boundary detection
  - Quick check question: What makes semi-open-ended questions particularly challenging for knowledge boundary detection compared to close-ended questions?

## Architecture Onboarding

- Component map: Dataset construction -> Auxiliary model generation -> Evaluation comparison -> Knowledge boundary categorization
- Critical path: Question generation -> Answer collection from target LLM -> Auxiliary model processing -> Truth verification -> Categorization
- Design tradeoffs: Uses open-sourced auxiliary model instead of black-box LLM for probability access, sacrificing potential performance for accessibility
- Failure signatures: High overlap between auxiliary and target model answers indicates insufficient probability adjustment; consistent self-evaluation-ground truth alignment suggests inadequate boundary detection
- First 3 experiments:
  1. Test pseudo-inverse estimation accuracy on known semantic relationships
  2. Evaluate probability reduction impact on answer diversity with controlled vocabulary
  3. Compare self-evaluation accuracy against human expert annotations for boundary detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of semi-open-ended questions in detecting knowledge boundaries compare across different domains (e.g., biology vs. history vs. technology)?
- Basis in paper: [explicit] The paper constructs a dataset of semi-open-ended questions across 32 domains and evaluates GPT-4's performance, finding poor overall performance but not analyzing domain-specific differences.
- Why unresolved: The paper presents overall performance metrics but does not break down results by domain to see if certain fields are more challenging for LLMs to handle.
- What evidence would resolve it: Domain-specific analysis of the dataset showing error rates, types of errors, and self-evaluation accuracy for each knowledge domain.

### Open Question 2
- Question: What is the optimal configuration of the probability influence scaler (λ) for balancing answer diversity and semantic coherence in the auxiliary model's generation?
- Basis in paper: [explicit] The ablation study tests λ values of 60, 70, and 80, finding that higher values increase diversity but doesn't determine the optimal trade-off point.
- Why unresolved: The paper only tests three values and doesn't explore the full parameter space or consider the trade-off between generating diverse answers and maintaining semantic relevance.
- What evidence would resolve it: Systematic testing of λ across a wider range of values, coupled with metrics that quantify both diversity (AOR, Bleu) and semantic coherence (semantic similarity to ground truth answers).

### Open Question 3
- Question: How does the performance of the auxiliary model in discovering ambiguous answers vary with different model sizes and architectures?
- Basis in paper: [explicit] The paper compares LLaMA-2-7B and LLaMA-2-13B, finding the larger model performs slightly better, but doesn't test other architectures or explore the relationship between model capacity and discovery effectiveness.
- Why unresolved: Only two sizes of one model family are tested, leaving questions about whether other architectures (e.g., Mistral, Gemma) or scaling laws apply to this task.
- What evidence would resolve it: Comprehensive comparison of multiple model families and sizes on the same task, measuring discovery effectiveness (EM, F1) and efficiency (computational cost, inference time).

## Limitations

- Heavy dependency on auxiliary model quality and knowledge breadth for discovering ambiguous answers
- Assumption of strong semantic relationships between anchor tokens and near-duplicates may not always hold
- Prompt templates and implementation details are partially specified, limiting reproducibility

## Confidence

**High Confidence**: Observation that GPT-4 generates many unqualified answers on semi-open-ended questions is well-supported by experimental results.

**Medium Confidence**: Effectiveness of probability reduction mechanism for discovering more ambiguous answers is supported by metrics but relies on assumptions about semantic relationships.

**Low Confidence**: Generalizability across different domains and LLM architectures is not thoroughly examined.

## Next Checks

1. **Prompt Template Validation**: Conduct ablation studies by systematically varying the prompt templates for both question generation and answer elicitation to quantify their impact on the diversity and quality of semi-open-ended questions and answers.

2. **Auxiliary Model Comparison**: Repeat the ambiguous answer discovery experiments using different auxiliary models (e.g., different sizes of LLaMA, Mistral, or other open-source models) to assess the robustness of the probability reduction mechanism and identify model-specific limitations.

3. **Semantic Relationship Verification**: Design experiments to empirically test the strength of semantic relationships between anchor tokens and their near-duplicates, measuring how variations in these relationships affect the accuracy of probability reduction and subsequent answer discovery.