---
ver: rpa2
title: 'Pre-training Distillation for Large Language Models: A Design Space Exploration'
arxiv_id: '2410.16215'
source_url: https://arxiv.org/abs/2410.16215
tags:
- distillation
- pre-training
- loss
- logits
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates pre-training distillation (PD) for large
  language models (LLMs), extending knowledge distillation to the pre-training phase.
  The authors systematically explore the design space of PD across four dimensions:
  logits processing, loss selection, scaling law, and offline versus online logits.'
---

# Pre-training Distillation for Large Language Models: A Design Space Exploration

## Quick Facts
- arXiv ID: 2410.16215
- Source URL: https://arxiv.org/abs/2410.16215
- Reference count: 40
- Pre-training distillation improves student LLM performance by 1.6% on average

## Executive Summary
This paper explores pre-training distillation (PD) as an extension of knowledge distillation to the pre-training phase of large language models. The authors systematically investigate four key design dimensions: logits processing, loss selection, scaling laws, and offline versus online logits. Using GLM-4-9B as the teacher and a 1.9B parameter model as the student, they validate the effectiveness of PD across English and Chinese datasets. The study identifies optimal configurations and provides insights into when and how PD is most beneficial for LLM development.

## Method Summary
The authors conduct a comprehensive design space exploration of pre-training distillation for LLMs. They systematically vary four dimensions: (1) logits processing methods including different temperature scaling approaches, (2) loss functions combining knowledge distillation losses with language modeling objectives, (3) scaling relationships between teacher and student model sizes, and (4) online versus offline distillation strategies. Using GLM-4-9B as the teacher model and a 1.9B parameter student model, they evaluate performance across 13 datasets spanning English and Chinese languages. The experiments employ a controlled pre-training setup with consistent hyperparameters except for the distillation configurations being tested.

## Key Results
- Student LLMs show an average 1.6% performance improvement across English and Chinese datasets
- Larger student models benefit more from pre-training distillation than smaller ones
- Combining KL divergence loss with language modeling loss using warmup-stable-decay scheduler yields optimal results
- Pre-training distillation effectiveness is maintained even with increased corpus size

## Why This Works (Mechanism)
Pre-training distillation works by transferring the knowledge embedded in a larger, more capable teacher model to a smaller student model during the pre-training phase. The teacher model's logits provide richer, more informative training signals than standard one-hot labels, particularly for out-of-distribution examples or ambiguous cases. By optimizing the student model to match both the teacher's output distribution (via KL divergence) and the ground truth (via language modeling loss), the student learns to capture both the teacher's generalization capabilities and task-specific patterns. The multi-stage scheduler for loss weighting allows the model to first align with the teacher's knowledge before fine-tuning on ground truth data, creating a balanced learning trajectory.

## Foundational Learning

**Knowledge Distillation**: Why needed - Enables transfer of knowledge from large models to smaller, more efficient ones; Quick check - Compare student performance with and without teacher guidance

**Temperature Scaling in Distillation**: Why needed - Controls the smoothness of probability distributions for effective knowledge transfer; Quick check - Test multiple temperature values to find optimal softening

**Loss Function Design**: Why needed - Balances between mimicking teacher behavior and learning from ground truth; Quick check - Experiment with different loss combinations and weight schedules

**Scaling Laws in Model Distillation**: Why needed - Determines optimal teacher-student size ratios for effective knowledge transfer; Quick check - Test multiple teacher-student size combinations

**Online vs Offline Distillation**: Why needed - Affects the quality and stability of teacher guidance during training; Quick check - Compare convergence speed and final performance between modes

## Architecture Onboarding

**Component Map**: Data Corpus -> Tokenizer -> Teacher LLM -> Student LLM -> Loss Functions (KL Divergence + Language Modeling) -> Optimizer -> Trained Student Model

**Critical Path**: Teacher model generation of logits → Logits processing (temperature scaling) → Loss computation (KL + LM) → Backpropagation to student parameters → Parameter updates

**Design Tradeoffs**: Online distillation provides more adaptive guidance but requires more computation, while offline distillation is simpler but may use stale teacher information. The choice of temperature affects knowledge transfer quality versus training stability.

**Failure Signatures**: Poor distillation outcomes manifest as student models that either underperform compared to standard training (over-regularization) or fail to improve (under-regularization). Loss curves that diverge or plateau unexpectedly indicate configuration issues.

**First Experiments**:
1. Test basic temperature scaling values (1.0, 2.0, 5.0, 10.0) with simple KL loss
2. Compare online vs offline distillation with fixed temperature
3. Evaluate different loss weight schedules (constant, linear, warmup-stable-decay)

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Results are limited to a specific teacher-student pair (GLM-4-9B to 1.9B) and may not generalize across different model families
- Evaluation covers only 13 datasets, potentially missing performance patterns in other domains
- The study does not investigate practical deployment considerations like inference efficiency or memory usage
- Long-term stability of student models after PD over extended training periods remains unexplored

## Confidence
- **High confidence**: The 1.6% average performance improvement and optimal configuration findings are well-supported by experimental results
- **Medium confidence**: The observations about scaling benefits for larger students and teacher size relationships are plausible but model-dependent
- **Low confidence**: Generalizability across different model architectures, scales, and diverse task domains remains uncertain

## Next Checks
1. Replicate experiments with different teacher-student pairs across multiple model families (BERT, RoBERTa, T5) to assess generalizability
2. Evaluate the impact of PD on inference latency and memory usage to determine practical deployment benefits
3. Conduct long-term stability tests to monitor performance degradation of student models over extended training and fine-tuning periods