---
ver: rpa2
title: Continual Offline Reinforcement Learning via Diffusion-based Dual Generative
  Replay
arxiv_id: '2404.10662'
source_url: https://arxiv.org/abs/2404.10662
tags:
- learning
- tasks
- generative
- continual
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles continual offline reinforcement learning (CORL),
  where an agent must learn from sequential offline datasets without forgetting previous
  knowledge. The key challenge is handling progressively diverse behaviors while avoiding
  catastrophic forgetting.
---

# Continual Offline Reinforcement Learning via Diffusion-based Dual Generative Replay

## Quick Facts
- **arXiv ID**: 2404.10662
- **Source URL**: https://arxiv.org/abs/2404.10662
- **Reference count**: 40
- **Primary result**: CuGRO achieves better forward transfer with less forgetting compared to baselines, closely matching the performance of using real previous data.

## Executive Summary
This paper addresses continual offline reinforcement learning (CORL), where an agent must learn from sequential offline datasets without forgetting previous knowledge. The key challenge is handling progressively diverse behaviors while avoiding catastrophic forgetting. CuGRO tackles this by using diffusion-based dual generative replay, decoupling the policy into a generative behavior model and a multi-head critic, and training diffusion models to generate high-fidelity pseudo-samples of past states and behaviors.

## Method Summary
CuGRO uses a dual generative replay framework with diffusion-based generative models for both behavior and state, paired with a multi-head critic. The approach generates synthetic state-action pairs for previous tasks using trained diffusion models, then interleaves these pseudo samples with new task data to continually update the generators and regularize the critic via behavior cloning. This enables continual learning without storing real samples from previous tasks.

## Key Results
- CuGRO achieves better forward transfer with less forgetting compared to baselines
- Performance closely matches using real previous data rather than synthetic samples
- Outperforms other generative models like VAEs and GANs on MuJoCo and Meta-World benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Dual generative replay preserves past knowledge by synthesizing high-fidelity pseudo-samples of previous states and behaviors, enabling continual updates without real historical data.
- **Mechanism**: Diffusion models learn to reverse a noise process, reconstructing state and behavior distributions of past tasks. Generated states are paired with corresponding actions from the behavior model to form replay samples, which are interleaved with new task data to update both generators and regularize the critic.
- **Core assumption**: The diffusion models can accurately mimic the mixed distribution of all seen tasks, producing samples indistinguishable from real data in terms of policy training value.
- **Evidence anchors**: [abstract]: "Generated states are paired with corresponding responses from the behavior generative model to represent old tasks with high-fidelity replayed samples." [section]: "Generated states ˆs ∼ pφ(s|k) are paired with corresponding responses from the behavior generative model ˆa ∼ µϕ(a|ˆs) to represent old tasks." [corpus]: Weak evidence; no cited papers show exact diffusion-based dual generative replay for continual offline RL.
- **Break condition**: If the diffusion models cannot capture the diversity of behavior patterns or fail to generalize across tasks, the replay samples will mislead training, causing performance degradation or catastrophic forgetting.

### Mechanism 2
- **Claim**: Decoupling the policy into a generative behavior model and a multi-head critic allows the policy to inherit distributional expressivity, accommodating progressively diverse behaviors across tasks.
- **Mechanism**: The generative behavior model µϕ(a|s) is trained with diffusion to model the action distribution given states, while the multi-head critic Qθ(s,a) assigns task-specific heads for each new task. Behavior cloning regularizes previous heads using replayed samples.
- **Core assumption**: The generative behavior model can capture complex, multimodal action distributions, and the multi-head critic can isolate task-specific value functions without interference.
- **Evidence anchors**: [abstract]: "We decouple the continual learning policy into a diffusion-based generative behavior model and a multi-head action evaluation model..." [section]: "Learning a generative behavior model is considerably simpler since sampling from the behavior policy can naturally encompass a diverse range of observed behaviors..." [corpus]: Some support from diffusion-based generative replay literature (e.g., DDGR), but limited evidence on multi-head critic with behavior cloning in offline RL.
- **Break condition**: If the generative behavior model fails to capture multimodal behaviors or the multi-head critic cannot prevent interference between task heads, performance will collapse.

### Mechanism 3
- **Claim**: Behavior cloning of replayed samples on previous critic heads mitigates forgetting by constraining updates to preserve old task knowledge.
- **Mechanism**: After generating pseudo state-action pairs for previous tasks, the corresponding Q-values from the previous critic are used as targets. The new critic is trained to minimize the difference between its outputs and these targets on replayed samples.
- **Core assumption**: The replayed samples accurately reflect the state-action distributions of previous tasks, and the critic heads can be regularized effectively via L2 loss on Q-values.
- **Evidence anchors**: [abstract]: "Finally, by interleaving pseudo samples with real ones of the new task, we continually update the state and behavior generators to model progressively diverse behaviors, and regularize the multi-head critic via behavior cloning to mitigate forgetting." [section]: "After obtaining the pseudo state-action pairs of previous tasks... we treat the labeled pseudo samples as expert data and perform behavior cloning via applying an auxiliary regularization term to imitate the expert data." [corpus]: Limited direct evidence; most continual RL work uses real replay buffers, not synthetic ones.
- **Break condition**: If the replayed samples poorly represent past tasks, behavior cloning will enforce incorrect constraints, leading to forgetting or negative transfer.

## Foundational Learning

- **Diffusion probabilistic models**:
  - Why needed here: They enable high-fidelity generation of state and action samples, crucial for synthetic replay without real historical data.
  - Quick check question: Can you explain how a diffusion model denoises a perturbed sample back to the original distribution?
- **Off-policy reinforcement learning**:
  - Why needed here: The method relies on offline datasets collected by behavior policies, requiring conservative policy updates to avoid distributional shift.
  - Quick check question: What is the role of the KL divergence constraint in offline RL, and how does it relate to behavior cloning?
- **Catastrophic forgetting in continual learning**:
  - Why needed here: The core challenge addressed is retaining performance on prior tasks while learning new ones, necessitating replay or regularization mechanisms.
  - Quick check question: How does a multi-head architecture help mitigate forgetting compared to a single shared head?

## Architecture Onboarding

- **Component map**: State generative model (diffusion-based, task-conditioned) -> Behavior generative model (diffusion-based, state-conditioned) -> Multi-head critic (one head per task) -> Replay buffer (synthetic samples interleaved with real new task data)
- **Critical path**: 1. Train diffusion models on current and replayed samples. 2. Generate synthetic state-action pairs for past tasks. 3. Update multi-head critic with real data and behavior cloning on replayed pairs.
- **Design tradeoffs**: Diffusion models offer high-fidelity generation but are computationally expensive and require careful hyperparameter tuning. Multi-head critic isolates tasks but increases model size and complexity. Synthetic replay avoids memory constraints but depends heavily on the quality of generated samples.
- **Failure signatures**: Degraded performance on previous tasks → likely issues with replay fidelity or behavior cloning strength. Poor adaptation to new tasks → possible overfitting to replayed samples or insufficient capacity in the behavior model. Unstable training → incorrect mixing ratio of real and replayed samples or learning rate misconfiguration.
- **First 3 experiments**: 1. Validate diffusion model generation quality on a single task before adding continual learning. 2. Test behavior cloning regularization strength by varying the λ coefficient and measuring forgetting. 3. Compare replay fidelity by measuring downstream policy performance using synthetic vs. real samples.

## Open Questions the Paper Calls Out
- The paper mentions classifier-free guidance could be used to improve diffusion model sample quality but leaves this as future work.

## Limitations
- Performance heavily depends on fidelity of diffusion models' generated samples
- Computational cost of training diffusion models for each task is significant
- Requires careful tuning of mixing ratio between real and replayed samples

## Confidence
- **Mechanism 1 (Dual generative replay)**: Low confidence - while diffusion models are theoretically capable of high-fidelity generation, there's limited direct evidence showing they can accurately capture the mixed distribution of all seen tasks in continual offline RL.
- **Mechanism 2 (Policy decoupling)**: Medium confidence - the decoupling strategy is supported by some diffusion-based generative replay literature, but the specific combination with multi-head critics and behavior cloning in offline RL needs more validation.
- **Mechanism 3 (Behavior cloning regularization)**: Low confidence - most continual RL work uses real replay buffers, not synthetic ones, so the effectiveness of behavior cloning on generated samples is largely unproven.

## Next Checks
1. Compare the KL divergence between real and generated state-action distributions for previous tasks to quantify replay fidelity.
2. Systematically vary the regularization coefficient λ and measure forgetting on previous tasks to find the optimal trade-off.
3. Test CuGRO on datasets with different characteristics (e.g., different reward scales or state dimensionalities) to assess robustness beyond the MuJoCo and Meta-World benchmarks.