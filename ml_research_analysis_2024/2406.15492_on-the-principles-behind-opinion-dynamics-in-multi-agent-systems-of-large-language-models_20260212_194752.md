---
ver: rpa2
title: On the Principles behind Opinion Dynamics in Multi-Agent Systems of Large Language
  Models
arxiv_id: '2406.15492'
source_url: https://arxiv.org/abs/2406.15492
tags:
- funding
- thing
- opinion
- item
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies how opinions on funding allocations spread
  among populations of interacting large language models (LLMs), focusing on three
  key biases: equity-consensus (seeking middle ground), caution (resisting changes
  to zero/unspecified funding), and safety (ethical concerns about negative connotations).
  Using Llama 3 and Mistral, the authors find that biases are shaped by perceived
  reasons for change, willingness to compromise, and connotation of items or reasons.'
---

# On the Principles behind Opinion Dynamics in Multi-Agent Systems of Large Language Models

## Quick Facts
- arXiv ID: 2406.15492
- Source URL: https://arxiv.org/abs/2406.15492
- Reference count: 40
- Primary result: Biases in LLM opinion formation (equity-consensus, caution, safety) affect funding allocation dynamics, with memory enabling consistency and negative connotations persisting through safety-equity tension

## Executive Summary
This paper investigates how opinions on funding allocations spread among populations of interacting large language models (LLMs), focusing on three key biases: equity-consensus (seeking middle ground), caution (resisting changes to zero/unspecified funding), and safety (ethical concerns about negative connotations). Using Llama 3 and Mistral, the authors find that biases are shaped by perceived reasons for change, willingness to compromise, and connotation of items or reasons. Notably, tensions between equity-consensus and safety biases allow negative opinions to persist. When agents can freely form opinions, final distributions are more diverse; when forced to choose from fixed options, consensus dominates and safety bias is suppressed. Agents with memory of past opinions maintain consistency, reducing consensus and sensitivity to negative connotations. This highlights new risks to alignment safety in multi-agent systems.

## Method Summary
The study employs Llama 3 and Mistral models in simulated multi-agent environments to study opinion formation and diffusion around funding allocation decisions. Agents interact through structured dialogues where they express and negotiate opinions, with scenarios varying between free opinion formation versus selection from predefined options. The experimental design tracks how three key biases—equity-consensus, caution, and safety—manifest across different interaction conditions, including scenarios with and without memory of past opinions.

## Key Results
- Three distinct biases identified: equity-consensus (middle-ground seeking), caution (resistance to changing zero/unspecified funding), and safety (ethical concerns about negative connotations)
- Free opinion formation leads to more diverse distributions; forced selection from fixed options increases consensus and suppresses safety bias
- Memory of past opinions maintains consistency across interactions, reducing overall consensus and sensitivity to negative connotations

## Why This Works (Mechanism)
The mechanism operates through the interaction of three cognitive biases that shape how LLMs process and respond to opinion formation requests. Equity-consensus bias drives agents toward compromise positions, caution bias preserves status quo unless compelling reasons exist, and safety bias introduces ethical considerations that can override other preferences. These biases interact dynamically, with safety-equity tensions allowing negative opinions to persist despite consensus-seeking tendencies.

## Foundational Learning
1. **Equity-Consensus Bias** - Agents' tendency to seek middle-ground solutions
   - Why needed: Explains tendency toward compromise in multi-agent negotiations
   - Quick check: Observe whether agent outputs converge toward averages of opposing positions

2. **Caution Bias** - Resistance to changing zero/unspecified funding allocations
   - Why needed: Models conservative approach to resource allocation decisions
   - Quick check: Track frequency of status quo maintenance when no strong arguments exist

3. **Safety Bias** - Ethical concerns about negative connotations in funding decisions
   - Why needed: Captures alignment safety considerations in opinion formation
   - Quick check: Measure impact of negative item framing on final allocation decisions

## Architecture Onboarding
**Component Map:** LLM Agent -> Opinion Formation Module -> Interaction Protocol -> Bias Expression Module -> Memory System

**Critical Path:** Opinion Request → Bias Evaluation → Response Generation → Peer Interaction → Opinion Update → Memory Storage

**Design Tradeoffs:** Free opinion formation provides diversity but less predictability versus fixed-option selection that ensures consistency but suppresses safety bias expression

**Failure Signatures:** 
- Consensus dominance without safety consideration
- Inconsistent opinion shifts across repeated interactions
- Complete suppression of negative opinions despite ethical concerns

**First 3 Experiments:**
1. Test equity-consensus bias by presenting agents with opposing funding preferences and measuring convergence
2. Evaluate caution bias by offering zero/unspecified funding scenarios with varying argument strength
3. Assess safety bias by introducing negative connotations and tracking opinion persistence

## Open Questions the Paper Calls Out
None

## Limitations
- Research limited to Llama 3 and Mistral models, constraining generalizability to other LLM architectures
- Simplified funding allocation scenarios may not capture full complexity of real-world opinion formation
- Lack of detailed prompt engineering strategies and agent identity control information

## Confidence
- High confidence: Basic observation that LLMs exhibit distinct biases in opinion formation
- Medium confidence: Interaction effects between different biases and impact on opinion diversity
- Medium confidence: Role of memory in maintaining opinion consistency across interactions
- Low confidence: Long-term stability of opinion patterns and real-world applicability

## Next Checks
1. Replicate experiments across multiple LLM architectures (GPT, Claude, etc.) and model sizes to assess generalizability
2. Implement controlled experiments varying prompt structures and agent backgrounds to isolate bias impact factors
3. Design longitudinal studies to test whether observed opinion patterns persist over extended interaction sequences and under different environmental pressures