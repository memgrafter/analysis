---
ver: rpa2
title: Laying the Foundation First? Investigating the Generalization from Atomic Skills
  to Complex Reasoning Tasks
arxiv_id: '2403.09479'
source_url: https://arxiv.org/abs/2403.09479
tags:
- skills
- tasks
- skill
- atomic
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether language models can generalize
  atomic skills (arithmetic, unit conversion) to complex reasoning tasks like math
  word problems. The authors propose a probing framework to test skill generalization
  and introduce a two-stage hierarchical curriculum learning strategy (skill training
  + applied learning) to induce this generalization.
---

# Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks

## Quick Facts
- arXiv ID: 2403.09479
- Source URL: https://arxiv.org/abs/2403.09479
- Reference count: 40
- Language models struggle to spontaneously generalize atomic skills to complex reasoning tasks without structured training

## Executive Summary
This paper investigates whether language models can automatically transfer learned atomic skills (like arithmetic and unit conversion) to complex reasoning tasks such as math word problems. Through systematic experiments on GSM8K and other datasets, the authors demonstrate that atomic skills do not spontaneously generalize to complex reasoning tasks. To address this limitation, they propose a two-stage hierarchical curriculum learning strategy that first trains models on atomic skills before advancing to applied learning on complex reasoning tasks. Their approach significantly improves performance, with LLaMA-2 accuracy increasing from 13.60% to 28.76% on the HARD dataset. The findings highlight the importance of structured skill learning for complex reasoning and suggest a bidirectional relationship between atomic skills and complex reasoning capabilities.

## Method Summary
The authors propose a hierarchical curriculum learning framework to induce skill generalization in language models. The approach consists of two stages: skill training and applied learning. In the first stage, models are trained on 18 carefully selected atomic skills such as addition, subtraction, multiplication, division, and unit conversion. In the second stage, these trained models are further fine-tuned on complex reasoning tasks like math word problems from GSM8K. The authors developed a probing framework to test skill generalization by creating a new dataset called HARD (Hard and Analogical Reasoning Dataset) containing complex problems that require combinations of atomic skills. They evaluate their approach across multiple models including LLaMA-2, Qwen, and InternLM, and demonstrate effectiveness across different domains and datasets.

## Key Results
- Atomic skills do not spontaneously generalize to complex reasoning tasks without structured training
- Two-stage hierarchical curriculum learning significantly improves performance (LLaMA-2 accuracy increases from 13.60% to 28.76% on HARD dataset)
- Skill generalization is effective across datasets and domains, and complex reasoning tasks can also enhance atomic skills

## Why This Works (Mechanism)
The paper does not provide explicit mechanistic explanations for why their hierarchical curriculum approach works. The authors demonstrate empirically that structured learning of atomic skills followed by applied learning creates a foundation that enables better performance on complex reasoning tasks, but the underlying mechanisms of how models bridge from atomic skills to complex reasoning remain unexplored.

## Foundational Learning
- **Curriculum Learning**: Training models through staged difficulty levels to build capabilities progressively; needed because models struggle with complex tasks without proper foundational preparation; quick check: verify staged training improves downstream task performance
- **Skill Probing**: Testing whether models can apply learned skills to new contexts; needed to measure true understanding versus memorization; quick check: evaluate performance on novel problem variations requiring same skills
- **Hierarchical Learning**: Structuring training in multiple levels where simpler concepts build toward complex applications; needed because direct training on complex tasks may be too challenging; quick check: compare performance with and without hierarchical structure

## Architecture Onboarding

**Component Map:**
Atomic Skill Dataset -> Skill Training Stage -> Applied Learning Stage -> Complex Reasoning Dataset -> Performance Evaluation

**Critical Path:**
Skill Training (Stage 1) -> Applied Learning (Stage 2) -> Evaluation on HARD dataset

**Design Tradeoffs:**
The hierarchical approach requires more training time and computational resources compared to direct fine-tuning on complex tasks, but achieves significantly better generalization. The authors trade efficiency for effectiveness by splitting training into two stages rather than attempting to learn everything simultaneously.

**Failure Signatures:**
Models that fail to generalize atomic skills show poor performance on complex reasoning tasks even after extensive training. The HARD dataset serves as a diagnostic tool to identify when models cannot bridge from atomic skills to multi-step reasoning.

**First Experiments:**
1. Test whether pre-training on atomic skills alone improves complex reasoning performance
2. Evaluate different orderings of skill training (e.g., starting with complex tasks vs. atomic skills)
3. Compare performance across different model sizes and architectures using the same curriculum

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions, but several emerge from the findings. How general is the skill generalization framework across different domains? What is the optimal duration for skill training before applied learning? Can the curriculum be optimized dynamically based on model performance? The authors leave these questions for future work, suggesting their framework is a promising starting point for investigating skill generalization in language models.

## Limitations
- The approach requires substantial computational resources for fine-tuning multiple models across different stages
- The study focuses exclusively on mathematical reasoning tasks, leaving unclear whether similar skill generalization patterns would emerge in other domains
- The evaluation framework relies on human-labeled atomic skills, which may not capture all relevant skill components and could introduce annotation biases

## Confidence

**High Confidence:** The empirical observation that atomic skills do not spontaneously generalize to complex reasoning tasks (demonstrated across multiple models and datasets)

**Medium Confidence:** The effectiveness of the proposed two-stage curriculum learning approach in inducing skill generalization

**Medium Confidence:** The bidirectional relationship between atomic skills and complex reasoning tasks

## Next Checks
1. Test the hierarchical curriculum approach on non-mathematical domains (e.g., multi-step code generation or logical reasoning) to assess cross-domain generalizability

2. Conduct ablation studies to isolate the contribution of each curriculum stage and determine whether skill training duration affects generalization performance

3. Implement automated atomic skill detection methods to reduce human annotation bias and scale the evaluation framework to larger datasets