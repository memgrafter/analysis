---
ver: rpa2
title: Beware of Calibration Data for Pruning Large Language Models
arxiv_id: '2410.17711'
source_url: https://arxiv.org/abs/2410.17711
tags:
- data
- uni00000013
- uni00000011
- calibration
- uni00000019
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper highlights the critical role of calibration data in\
  \ post-training pruning for large language models (LLMs), revealing that the choice\
  \ of calibration data significantly impacts pruning performance, especially at high\
  \ sparsity levels. Through systematic experiments, the authors find that calibration\
  \ data similar to the model\u2019s pretraining data yields better results than generic\
  \ or high-quality but dissimilar data."
---

# Beware of Calibration Data for Pruning Large Language Models

## Quick Facts
- arXiv ID: 2410.17711
- Source URL: https://arxiv.org/abs/2410.17711
- Reference count: 22
- Primary result: Self-generated synthetic calibration data improves LLM pruning performance by up to 2.68% on commonsense reasoning tasks

## Executive Summary
This paper reveals that calibration data similarity to pretraining data is critical for effective post-training pruning of large language models. The authors find that commonly used calibration datasets like C4 can underperform compared to data similar to pretraining distributions, especially at high sparsity levels. To address the challenge of inaccessible pretraining data, they propose a self-generating synthetic calibration data strategy using the LLM itself to generate data conditioned on sampled calibration data, then filtering out low-quality samples based on perplexity. Experiments on DCLM-7B, LLaMA-2, and LLaMA-3 demonstrate consistent improvements across multiple strong pruning methods (Wanda, DSnoT, OWL), with gains of up to 2.68% on commonsense reasoning tasks.

## Method Summary
The authors propose a self-generating synthetic calibration data strategy for LLM pruning. First, they sample sequences from existing calibration data sources (C4, Wikipedia, SlimPajama, DCLM). Then, using the target LLM, they generate synthetic continuations from 1-4 token prefixes with Top-k=50, Top-p=0.95, temperature=0.6, and repetition penalty=1.2. Next, they filter out the top 20% of samples with highest perplexity scores. Finally, they use this filtered synthetic data for pruning with three methods (Wanda, DSnoT, OWL) at 60% sparsity, evaluating on commonsense reasoning tasks (Alpaca, BoolQ, Winogrande, PIQA, Hellaswag, ARC-e, ARC-c, MMLU).

## Key Results
- Self-generated calibration data outperforms commonly used datasets (C4) by up to 2.68% on commonsense reasoning tasks
- The improvement is consistent across multiple pruning methods (Wanda, DSnoT, OWL) and model sizes (DCLM-7B, LLaMA-2, LLaMA-3)
- Calibration data similar to pretraining data yields better pruning performance than generic high-quality data
- The self-generation approach is robust across different pruning settings and effective when pretraining data is unavailable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Calibration data similarity to pretraining data improves pruning performance.
- Mechanism: The model has internalized patterns from pretraining data; calibration data resembling these patterns allows more accurate importance estimation during pruning.
- Core assumption: The model's internal representation of language structure and semantics is shaped by pretraining data patterns.
- Evidence anchors:
  - [abstract] "calibration data similar to the model's pretraining data yields better results"
  - [section 3.4] "calibration data similar to the pretraining data can yield better pruning performance"
  - [corpus] Weak evidence; no directly relevant papers found
- Break condition: If pretraining data contains patterns not representative of target tasks, similarity may hurt performance.

### Mechanism 2
- Claim: Self-generated synthetic data approximates pretraining data distribution.
- Mechanism: The LLM generates text conditioned on sampled calibration data, producing samples with similar statistical properties to pretraining data.
- Core assumption: Auto-regressive generation captures the learned distribution of pretraining data.
- Evidence anchors:
  - [abstract] "self-generating synthetic calibration data strategy"
  - [section 4] "we propose using self-generated synthetic data as a proxy for the training data"
  - [corpus] Weak evidence; only general mentions of calibration data in pruning context
- Break condition: If generation is too constrained by prefix, may not explore full pretraining distribution.

### Mechanism 3
- Claim: Perplexity-based filtering removes low-quality generated samples.
- Mechanism: Samples with high perplexity are less likely to match learned patterns and are filtered out.
- Core assumption: High perplexity indicates poor model fit to sample distribution.
- Evidence anchors:
  - [abstract] "filtering out low-quality samples based on perplexity"
  - [section 6.3] "perplexity-based method to filter low-quality data"
  - [corpus] Weak evidence; only general mentions of calibration data in pruning context
- Break condition: Over-filtering may remove diverse samples needed for robust importance estimation.

## Foundational Learning

- Concept: Language model pretraining data distribution
  - Why needed here: Understanding why similar calibration data works requires knowing how pretraining shapes model representations
  - Quick check question: What properties of pretraining data most influence a model's learned language patterns?

- Concept: Parameter importance estimation in pruning
  - Why needed here: Calibration data is used to estimate which parameters can be pruned with minimal performance loss
  - Quick check question: How does calibration data influence the computation of parameter importance scores?

- Concept: Synthetic data generation using language models
  - Why needed here: The proposed method relies on generating calibration data using the target LLM itself
  - Quick check question: What controls the quality and diversity of self-generated calibration data?

## Architecture Onboarding

- Component map:
  Dense LLM model (target for pruning) -> Source calibration data pool (C4, Wikipedia, Slimpajama, DCLM) -> Self-generation pipeline (prefix sampling, generation, perplexity filtering) -> Pruning methods (Wanda, DSnoT, OWL) -> Evaluation tasks (Alpaca, BoolQ, Winogrande, PIQA, Hellaswag, ARC-e, ARC-c, MMLU)

- Critical path:
  1. Sample prefix from source calibration data
  2. Generate continuation using target LLM
  3. Compute perplexity and filter low-quality samples
  4. Use filtered synthetic data for pruning
  5. Evaluate pruned model performance

- Design tradeoffs:
  - Prefix length: Short prefixes allow more diverse generation but may lose semantic coherence
  - Filtering threshold: Higher thresholds improve quality but reduce diversity
  - Generation diversity: Top-k/p sampling balances exploration vs. coherence

- Failure signatures:
  - Performance worse than magnitude pruning: Likely poor synthetic data quality or inappropriate filtering
  - Inconsistent results across runs: Check random seed reproducibility and data sampling
  - Performance drops on specific tasks: Calibration data may not cover needed patterns

- First 3 experiments:
  1. Compare pruning performance using C4 vs. Wikipedia as calibration data on a small LLM
  2. Test self-generation with varying prefix lengths (1-1024 tokens) to find optimal range
  3. Evaluate impact of different perplexity filtering rates (10%-40%) on pruning performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal prefix length for self-generating synthetic calibration data, and how does it vary across different LLM architectures?
- Basis in paper: [explicit] The paper explores prefix lengths from 0 to 1024 tokens, finding that 1-4 tokens work best for DCLM-7B, but notes this needs validation on other models.
- Why unresolved: The experiments only tested prefix lengths on DCLM-7B, leaving open whether this optimal range generalizes to LLaMA-2, LLaMA-3, and larger models.
- What evidence would resolve it: Systematic experiments varying prefix length on multiple LLM architectures (7B, 13B, 70B) would reveal whether optimal ranges are consistent or architecture-dependent.

### Open Question 2
- Question: How does the self-generation strategy perform when applied to pruned models rather than dense models as the generation source?
- Basis in paper: [inferred] The paper uses dense models for synthetic data generation, but doesn't explore whether using pruned models (with different parameter importance distributions) would yield better calibration data.
- Why unresolved: The authors conjecture that self-generated data avoids underrepresented patterns, but haven't tested whether pruned-model generation would better match the eventual pruned model's parameter importance landscape.
- What evidence would resolve it: Comparative experiments generating calibration data from dense vs. pruned versions of the same model, measuring resulting pruning performance differences.

### Open Question 3
- Question: What is the relationship between Min-K%++ similarity scores and actual pruning performance across different data sources?
- Basis in paper: [explicit] The paper shows self-generated data has higher Min-K%++ scores than baseline data and correlates this with pruning performance, but doesn't quantify this relationship.
- Why unresolved: While higher Min-K%++ scores suggest better similarity to training data, the paper doesn't establish a quantitative link between these scores and the magnitude of pruning performance improvements.
- What evidence would resolve it: Regression analysis correlating Min-K%++ scores with pruning performance improvements across multiple data sources and pruning methods would establish predictive validity.

## Limitations
- The improvement (up to 2.68%) may not justify the computational overhead of generating synthetic data for all pruning scenarios
- Effectiveness across different pruning sparsity levels beyond 60% remains unexplored
- The self-generation approach may introduce biases if the model's generation capabilities are limited

## Confidence

**High Confidence:** The core claim that calibration data similarity to pretraining data improves pruning performance is well-supported by systematic experiments across multiple LLMs and pruning methods.

**Medium Confidence:** The effectiveness of the self-generating synthetic data strategy is demonstrated, but the method's robustness across diverse pruning scenarios and sparsity levels requires further validation.

**Low Confidence:** The paper's claim about perplexity-based filtering being sufficient to ensure high-quality synthetic data is weakly supported, as the filtering mechanism's sensitivity to different LLM architectures is not thoroughly examined.

## Next Checks
1. Evaluate performance across sparsity levels: Test the self-generating strategy at 30%, 70%, and 90% sparsity to determine if the 2.68% improvement generalizes beyond the 60% setting.
2. Analyze perplexity filtering sensitivity: Systematically vary the filtering threshold (10%, 20%, 30%, 40%) and measure its impact on both synthetic data quality and downstream pruning performance across different pruning methods.
3. Test on non-English LLMs: Apply the self-generating strategy to multilingual or non-English LLMs to assess whether calibration data similarity to pretraining data remains the dominant factor across different language distributions.