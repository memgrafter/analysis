---
ver: rpa2
title: Expanding on EnCLAP with Auxiliary Retrieval Model for Automated Audio Captioning
arxiv_id: '2409.01160'
source_url: https://arxiv.org/abs/2409.01160
tags:
- audio
- captioning
- enclap
- dataset
- challenge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper describes systems for two DCASE 2024 audio-language tasks.
  For automated audio captioning (Task 6), the authors enhanced the EnCLAP framework
  by replacing its EnCodec and CLAP components with the DAC neural audio codec and
  a CNext-based audio-text joint embedding model, respectively.
---

# Expanding on EnCLAP with Auxiliary Retrieval Model for Automated Audio Captioning

## Quick Facts
- **arXiv ID**: 2409.01160
- **Source URL**: https://arxiv.org/abs/2409.01160
- **Reference count**: 0
- **Primary result**: Enhanced EnCLAP framework with DAC neural audio codec and CNext-based audio-text joint embedding achieved FENSE score of 0.542 on Task 6 and mAP@10 score of 0.386 on Task 8 for DCASE 2024 Challenge

## Executive Summary
This paper presents systems for two DCASE 2024 audio-language tasks: automated audio captioning (Task 6) and language-based audio retrieval (Task 8). The authors enhanced the EnCLAP framework by replacing its EnCodec with DAC neural audio codec and CLAP with a CNext-based audio-text joint embedding model. They also implemented a sampling-and-reranking decoding strategy using a fluency detector and encoder/decoder reranking scores. For the retrieval task, they used an m-LTM-based retriever with CNext audio encoder and multiple pretrained text encoders, training on combined datasets and ensembling for improved performance.

## Method Summary
The authors modified the EnCLAP framework for automated audio captioning by integrating DAC neural audio codec for higher-fidelity discrete audio representations and CNext-based audio-text joint embedding for better cross-modal alignment. They implemented a two-stage generation process with nucleus sampling followed by reranking using a fluency detector and weighted encoder/decoder scores. For language-based audio retrieval, they employed an m-LTM-based retriever with CNext as the audio encoder and multiple pretrained text encoders (BGE, BERT, RoBERTa variants), training on a combined dataset and using ensemble predictions for improved performance.

## Key Results
- Achieved FENSE score of 0.542 on Task 6 automated audio captioning, outperforming baseline
- Achieved mAP@10 score of 0.386 on Task 8 language-based audio retrieval, outperforming baseline
- Demonstrated effectiveness of DAC neural audio codec and CNext-based audio-text joint embedding for audio captioning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing EnCodec with DAC improves audio representation quality for captioning.
- Mechanism: DAC's higher-fidelity compression preserves more acoustic detail than EnCodec, leading to richer timestep-level representations that the decoder can use to generate more accurate captions.
- Core assumption: The discrete tokens from DAC maintain sufficient semantic information for the language model to generate descriptive captions.
- Evidence anchors: The paper explicitly states DAC has demonstrated superior performance in audio compression and downstream tasks compared to EnCodec.

### Mechanism 2
- Claim: Using CNext-based audio-text joint embedding improves retrieval performance for both captioning and retrieval tasks.
- Mechanism: CNext trained on AudioSet provides better audio representations, and when combined with various text encoders, it creates a more effective cross-modal alignment than the original CLAP embeddings.
- Core assumption: The audio-text joint embedding trained on AudioSet captures sufficient semantic relationships between audio and text for effective retrieval.
- Evidence anchors: The paper observes that CNext finetuned using audio-text retrieval task exhibits superior performance compared to alternatives.

### Mechanism 3
- Claim: Sampling-and-reranking decoding produces more diverse and higher-quality captions than beam search.
- Mechanism: Nucleus sampling generates multiple candidate captions with different levels of diversity, and the reranking process (using fluency detection and weighted encoder/decoder scores) selects the most appropriate caption based on both linguistic quality and semantic relevance.
- Core assumption: The reranking scores effectively balance between caption fluency and semantic alignment with the input audio.
- Evidence anchors: The authors adopt the approach from Wu et al. [4] and use FENSE fluency error detector to filter captions containing fluency errors before final selection.

## Foundational Learning

- **Concept: Neural audio codecs and discrete representation learning**
  - Why needed here: Understanding how DAC transforms waveforms into discrete codes is crucial for grasping why this replacement improves the system
  - Quick check question: What's the key difference between continuous and discrete audio representations in the context of language model processing?

- **Concept: Cross-modal retrieval and embedding alignment**
  - Why needed here: The CNext-based audio-text joint embedding relies on proper alignment between audio and text representations for effective retrieval
  - Quick check question: How does contrastive learning help align audio and text embeddings in the retrieval model?

- **Concept: Nucleus sampling and decoding strategies**
  - Why needed here: The sampling-and-reranking approach depends on understanding different decoding strategies and their trade-offs
  - Quick check question: What's the main advantage of nucleus sampling over traditional beam search in text generation?

## Architecture Onboarding

- **Component map**: Audio input → DAC encoder → BART decoder → Reranking system → Final caption
- **Critical path**: Audio → DAC → BART → Reranking → Final caption
- **Design tradeoffs**: DAC vs EnCodec (higher fidelity vs established framework compatibility), multiple text encoders (better ensemble performance vs increased complexity), reranking vs beam search (better diversity vs potential latency increase)
- **Failure signatures**: Poor captions despite good audio encoding (reranking weights may be misconfigured), low retrieval accuracy (text encoder alignment may be inadequate), slow inference (multiple candidate generation and reranking may be too computationally expensive)
- **First 3 experiments**:
  1. Test DAC encoding quality by comparing discrete token distributions with EnCodec outputs
  2. Validate CNext embeddings by checking audio-text similarity scores on a validation set
  3. Tune reranking weights by running ablation studies on different weight combinations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the DAC neural audio codec compare to EnCodec in terms of downstream performance for automated audio captioning tasks?
- Basis in paper: The paper states they replaced EnCodec with DAC, citing DAC's superior performance in audio compression and downstream tasks, but does not provide direct comparative results for AAC performance.
- Why unresolved: The paper reports improved FENSE scores after the change but does not isolate the contribution of the DAC replacement from other modifications.
- What evidence would resolve it: Controlled experiments comparing EnCLAP with EnCodec versus DAC on the same dataset while keeping all other components constant.

### Open Question 2
- Question: What is the optimal balance between encoder reranking and decoder reranking scores in the two-stage generation and reranking process?
- Basis in paper: The authors use weights of 0.7 and 0.3 for encoder and decoder reranking scores respectively, but acknowledge this as an arbitrary choice.
- Why unresolved: The paper does not explore different weighting schemes or validate that these particular weights are optimal.
- What evidence would resolve it: Systematic experiments varying the reranking score weights and measuring impact on captioning quality metrics.

### Open Question 3
- Question: What is the impact of pretraining on large-scale weakly-labeled datasets versus smaller high-quality datasets for audio captioning performance?
- Basis in paper: The authors use a two-stage training process with pretraining on large mixed datasets followed by finetuning on Clotho, but do not analyze the relative contributions of each stage.
- Why unresolved: The paper does not provide ablations showing performance differences between pretraining on large vs small datasets or between models trained only on Clotho.
- What evidence would resolve it: Comparative results showing performance of models trained with different pretraining strategies.

## Limitations

- **Limited ablation studies**: The paper lacks comprehensive ablation studies to isolate the contributions of individual modifications (DAC replacement, CNext embedding, reranking strategy) to overall performance improvements.
- **Hyperparameter sensitivity**: The reranking weights and other hyperparameters appear to be chosen without systematic tuning or sensitivity analysis, potentially leaving performance gains unrealized.
- **Domain generalization**: The training methodology combines datasets with different characteristics, but the paper doesn't adequately address potential domain shift issues or provide analysis of how each dataset contributes to the final performance.

## Confidence

**High confidence claims:**
- The overall system architecture and its application to both Task 6 and Task 8 is correctly described and implemented
- The reported performance metrics (FENSE score of 0.542 for Task 6, mAP@10 score of 0.386 for Task 8) are accurately presented based on the challenge evaluation
- The ensemble approach for Task 8 using multiple text encoders (BGE, BERT, RoBERTa variants) is technically sound and implemented as described

**Medium confidence claims:**
- The DAC neural audio codec provides superior performance compared to EnCodec, though specific quantitative evidence is limited
- The CNext-based audio-text joint embedding improves cross-modal alignment, but ablation studies are lacking
- The sampling-and-reranking decoding strategy produces better captions than beam search, though this is based on limited preliminary experiments

**Low confidence claims:**
- The specific reranking weight values (0.8 for fluency detector, 0.5 for encoder/decoder scores) are optimal, as no systematic tuning or sensitivity analysis is provided
- The exact contribution of each modification to the final performance, as the paper doesn't provide comprehensive ablation studies

## Next Checks

**Validation Check 1: DAC vs EnCodec Ablation Study**
Run controlled experiments comparing the original EnCLAP framework with EnCodec against the modified version with DAC on the same training and evaluation setup. This would provide quantitative evidence for the claimed improvement in audio representation quality and help isolate the specific contribution of the DAC replacement to overall performance gains.

**Validation Check 2: Reranking Weight Sensitivity Analysis**
Systematically vary the reranking weights (fluency detector score and encoder/decoder scores) across a grid of values to identify optimal configurations and understand the sensitivity of the system to these hyperparameters. This would validate whether the chosen weights (0.8 and 0.5) are truly optimal or if better configurations exist.

**Validation Check 3: Qualitative Caption Analysis**
Generate and analyze a diverse set of captions from both the baseline EnCLAP system and the proposed modifications to identify specific types of improvements or degradations. This qualitative analysis would complement the quantitative metrics and provide insights into whether the system improvements translate to practically useful captions in real-world scenarios.