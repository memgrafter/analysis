---
ver: rpa2
title: The Construction of Instruction-tuned LLMs for Finance without Instruction
  Data Using Continual Pretraining and Model Merging
arxiv_id: '2409.19854'
source_url: https://arxiv.org/abs/2409.19854
tags:
- instruction-tuned
- domain-specific
- llms
- merging
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to construct instruction-tuned large
  language models (LLMs) for finance without requiring instruction data. The authors
  leverage publicly available general-purpose pretrained and instruction-tuned LLMs,
  combining them with domain-specific continual pretraining and model merging.
---

# The Construction of Instruction-tuned LLMs for Finance without Instruction Data Using Continual Pretraining and Model Merging

## Quick Facts
- **arXiv ID**: 2409.19854
- **Source URL**: https://arxiv.org/abs/2409.19854
- **Reference count**: 40
- **Primary result**: Instruction-tuned finance LLM constructed without instruction data using continual pretraining + model merging outperforms general-purpose instruction-tuned model on Japanese financial benchmarks

## Executive Summary
This paper proposes a method to construct instruction-tuned large language models (LLMs) for finance without requiring instruction data. The authors leverage publicly available general-purpose pretrained and instruction-tuned LLMs, combining them with domain-specific continual pretraining and model merging. First, they perform continual pretraining on a Japanese financial corpus using an existing general-purpose Japanese LLM. Then, they merge this domain-specific pretrained model with a general-purpose instruction-tuned model using task arithmetic model merging. The resulting domain-specific instruction-tuned LLM outperforms other models on a Japanese financial benchmark and a generation quality benchmark, achieving an overall score of 2.58 compared to 0.03 for the general-purpose instruction-tuned model.

## Method Summary
The method constructs domain-specific instruction-tuned LLMs without instruction data by first performing continual pretraining on a general-purpose LLM using domain-specific corpus (Japanese financial documents), then merging the resulting domain-specific pretrained model with a general-purpose instruction-tuned model using task arithmetic. The mathematical formula is Θdi = Θdp + Θgi - Θgp, where Θdi is the domain-specific instruction-tuned model, Θdp is the domain-specific pretrained model, Θgi is the general-purpose instruction-tuned model, and Θgp is the general-purpose pretrained model. The approach assumes that instruction support and domain-specific knowledge are nearly independent in the weight space.

## Key Results
- Domain-specific instruction-tuned LLM achieved overall score of 2.58 on pfmt-bench-fin-fin-ja vs 0.03 for general-purpose instruction-tuned model
- Outperformed other models on Japanese financial benchmarks with F1 scores significantly higher than baseline models
- Successfully demonstrated that instruction-tuned LLMs for finance can be constructed without instruction data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model merging allows combining domain-specific knowledge and instruction-following capability without retraining instruction data
- Mechanism: Task arithmetic model merging linearly interpolates weights between domain-specific pretrained model and general-purpose instruction-tuned model, preserving both knowledge types
- Core assumption: Instruction support and domain-specific knowledge are nearly independent in the weight space
- Evidence anchors: [abstract] "One major advantage of our method is that the instruction-tuned and domain-specific pretrained vectors are nearly independent", [section] "Θdi = Θdp + Θgi − Θgp" showing linear interpolation

### Mechanism 2
- Claim: Continual pretraining preserves instruction-following capability while adding domain knowledge
- Mechanism: Domain-specific corpus pretraining adds financial knowledge to general-purpose LLM without affecting instruction-tuned weights
- Core assumption: Continual pretraining on domain data doesn't degrade general capabilities when done properly
- Evidence anchors: [section] "Our experiments demonstrate the successful construction of instruction-tuned LLMs for finance", [section] "According to the superficial alignment hypothesis [54], such tuning methods may not be effective for domain-specific tuning because tuning focusing on alignment cannot acquire new knowledge"

### Mechanism 3
- Claim: Public availability of general-purpose models enables cost-effective domain specialization
- Mechanism: Leverages existing instruction-tuned models as foundation, only requiring domain corpus and continual pretraining
- Core assumption: General-purpose instruction-tuned models contain sufficient instruction-following capability to transfer
- Evidence anchors: [abstract] "Given that general-purpose pretrained LLMs and their instruction-tuned LLMs are often publicly available", [section] "The sets of general-purpose pretrained and instruction-tuned LLMs are typically available publicly"

## Foundational Learning

- Concept: Task arithmetic model merging
  - Why needed here: Enables combining instruction-following and domain knowledge without retraining instruction data
  - Quick check question: What is the mathematical formula for combining two models' weights in task arithmetic?

- Concept: Continual pretraining vs fine-tuning
  - Why needed here: Continual pretraining adds domain knowledge while preserving existing capabilities, unlike fine-tuning which may overwrite them
  - Quick check question: Why might low-rank adaptation be insufficient for domain-specific tuning according to the paper?

- Concept: Weight space independence
  - Why needed here: Critical assumption for model merging success - instruction and domain knowledge must occupy separate dimensions in weight space
  - Quick check question: How did the paper verify the independence assumption?

## Architecture Onboarding

- Component map: Base LLM → Domain Corpus → Continual Pretraining → General-purpose Instruction-tuned LLM → Model Merging → Domain-specific Instruction-tuned LLM
- Critical path: Corpus preparation → Continual pretraining → Model merging → Evaluation
- Design tradeoffs: 
  - Cost vs performance: Using public models saves compute but may limit capabilities
  - Corpus quality vs quantity: Better financial corpus improves domain knowledge acquisition
  - Merging ratio: Linear interpolation assumes independence, more complex methods may be needed
- Failure signatures:
  - Loss spikes during continual pretraining
  - Degradation in general capabilities after merging
  - Poor performance on instruction-following tasks
  - High cosine similarity between task vectors
- First 3 experiments:
  1. Verify continual pretraining improves domain-specific benchmark scores without degrading general performance
  2. Test model merging with different interpolation ratios to find optimal balance
  3. Measure cosine similarity between instruction and domain task vectors to verify independence assumption

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How generalizable is the proposed method of constructing instruction-tuned LLMs for finance without instruction data across different domains beyond finance?
- Basis in paper: [inferred] The authors suggest examining the method in other domains as future work, indicating uncertainty about its generalizability.
- Why unresolved: The paper only demonstrates the method's effectiveness in the financial domain using Japanese language models.
- What evidence would resolve it: Applying the method to other domains (e.g., medical, legal) with different language models and evaluating performance using domain-specific benchmarks.

### Open Question 2
- Question: What are the specific conditions under which task arithmetic model merging works effectively for constructing instruction-tuned LLMs?
- Basis in paper: [explicit] The authors acknowledge the need to clarify conditions under which task arithmetic assumption works as future research.
- Why unresolved: While the paper demonstrates successful model merging for finance, it doesn't provide a comprehensive analysis of the conditions necessary for effective task arithmetic.
- What evidence would resolve it: Systematic experiments varying factors such as model sizes, domain specificity, and instruction complexity to identify patterns in successful task arithmetic applications.

### Open Question 3
- Question: How can the translation performance of domain-specific instruction-tuned LLMs be improved without sacrificing domain-specific knowledge?
- Basis in paper: [explicit] The authors note that the domain-specific instruction-tuned LLM underperforms in translation tasks compared to general-purpose models.
- Why unresolved: The paper attributes this to the use of only Japanese corpus for continual pretraining and instruction tuning, but doesn't provide a solution.
- What evidence would resolve it: Developing and testing methods for incorporating multilingual data in continual pretraining and instruction tuning while maintaining domain-specific performance.

## Limitations

- The independence assumption between instruction-following capability and domain-specific knowledge in the weight space is not rigorously validated through quantitative measures like cosine similarity
- Evaluation relies on GPT-4o grading for generation quality, which introduces potential bias and lacks human verification of financial accuracy
- The method's performance on multilingual tasks is limited, with the domain-specific model underperforming in translation compared to general-purpose models

## Confidence

- **High Confidence**: The overall methodology of combining continual pretraining with model merging is technically sound and the performance improvements on the Japanese financial benchmarks are well-documented
- **Medium Confidence**: The claim that instruction data is not required for constructing domain-specific instruction-tuned models is supported by results, but the theoretical justification for weight space independence could be stronger
- **Medium Confidence**: The assertion that continual pretraining preserves instruction-following capabilities while adding domain knowledge is plausible but not rigorously validated through ablation studies or degradation analysis

## Next Checks

1. **Cosine Similarity Analysis**: Measure and report the cosine similarity between the instruction task vector (Θgi - Θgp) and the domain task vector (Θdp - Θgp) to provide quantitative evidence for the independence assumption claimed in the paper.

2. **Ablation Study**: Conduct an ablation study where the continual pretrained model is merged with the general-purpose instruction-tuned model using different interpolation ratios to identify the optimal balance and test the robustness of the independence assumption.

3. **Cross-Lingual Evaluation**: Evaluate the Japanese financial instruction-tuned model on English financial tasks (using translation if needed) to assess whether the domain-specific knowledge and instruction-following capabilities are truly independent and transferable across languages.