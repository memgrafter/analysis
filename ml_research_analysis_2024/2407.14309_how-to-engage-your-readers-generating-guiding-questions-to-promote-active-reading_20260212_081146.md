---
ver: rpa2
title: How to Engage Your Readers? Generating Guiding Questions to Promote Active
  Reading
arxiv_id: '2407.14309'
source_url: https://arxiv.org/abs/2407.14309
tags:
- questions
- question
- article
- reading
- guiding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates the role of guiding questions\u2014author-posed\
  \ questions embedded in formal texts like textbooks and research articles\u2014\
  in enhancing readability and active reading. The study introduces GUIDING Q, a dataset\
  \ of 10,577 guiding questions, and analyzes their discourse and interactional roles\
  \ through a taxonomy of five question types."
---

# How to Engage Your Readers? Generating Guiding Questions to Promote Active Reading

## Quick Facts
- arXiv ID: 2407.14309
- Source URL: https://arxiv.org/abs/2407.14309
- Reference count: 13
- Introduces GUIDING Q dataset with 10,577 guiding questions for active reading enhancement

## Executive Summary
This work investigates how author-posed guiding questions embedded in formal texts can enhance active reading and comprehension. The authors introduce GUIDING Q, a novel dataset of 10,577 guiding questions extracted from textbooks and research articles, and develop a taxonomy of five question roles based on discourse and interactional functions. They explore multiple modeling approaches for generating these questions, finding that joint generation with question roles performs best. A human study validates that both generated and human-written guiding questions significantly improve readers' memorization and comprehension compared to reading without questions.

## Method Summary
The study introduces the GUIDING Q dataset by extracting questions from textbooks (OpenStax) and research articles (arXiv/PubMed), then annotating them with roles, answers, and evidence using a two-phase pipeline. The authors explore three generation paradigms: Pipeline (separate models for position, answer, and question), MultiTask (unified model), and Joint (simultaneous generation with inter-question relationships). Flan-T5 models are finetuned for each approach. Evaluation combines automatic metrics (ROUGE-L, Meteor, BertScore, Dist-1/2) with human studies measuring reading comprehension and summary quality.

## Key Results
- Joint generation with question roles outperforms other paradigms in automatic evaluation
- Human study shows generated guiding questions significantly improve memorization and comprehension
- Generated questions rated nearly as highly as human-written questions in quality assessment
- Question position identification remains challenging across all modeling approaches

## Why This Works (Mechanism)
Guiding questions promote active reading by creating cognitive engagement points that encourage readers to pause, reflect, and process information more deeply. The discourse and interactional roles provide structured scaffolding that helps readers navigate complex texts. Joint generation leverages relationships between question roles to produce more coherent and contextually appropriate questions.

## Foundational Learning
1. **Guiding Questions**: Author-posed questions embedded in texts to promote active reading. Why needed: Central to the study's intervention mechanism. Quick check: Identify question types in sample textbook paragraphs.

2. **Discourse vs. Interactional Roles**: Questions can serve text structure functions (discourse) or reader engagement functions (interactional). Why needed: Forms the basis of the five-question taxonomy. Quick check: Classify questions from a research abstract.

3. **Joint Generation**: Simultaneous prediction of multiple related outputs. Why needed: Key technical innovation improving generation quality. Quick check: Compare outputs from joint vs. pipeline generation.

4. **Position Identification**: Determining where questions should be inserted in text. Why needed: Critical bottleneck affecting generation quality. Quick check: Evaluate perplexity scores around question positions.

5. **Human Evaluation for Comprehension**: Using reading tests and summaries to assess question effectiveness. Why needed: Validates practical impact beyond automatic metrics. Quick check: Compare test scores with and without questions.

## Architecture Onboarding

**Component Map**: Text Preprocessing -> Question Generation (Position/Answer/Question) -> Automatic Evaluation -> Human Study

**Critical Path**: Text preprocessing → Question position identification → Answer generation → Question generation → Evaluation

**Design Tradeoffs**: Pipeline approach offers modularity but loses inter-role relationships; joint approach captures relationships but is more complex to train; MultiTask balances these considerations.

**Failure Signatures**: 
- Low question diversity indicates insufficient training data or overly conservative decoding
- Poor position identification shows as low perplexity scores not correlating with human preferences
- Comprehension gains without quality ratings suggests questions may be effective but poorly formed

**3 First Experiments**:
1. Test question generation on a small sample of textbook paragraphs to verify preprocessing pipeline
2. Compare joint vs. pipeline generation outputs for a single article to assess quality differences
3. Run automatic evaluation on a validation set before conducting full human study

## Open Questions the Paper Calls Out

**Open Question 1**: How do guiding questions affect cognitive load during reading? The study measured reading time as a proxy but didn't directly assess cognitive load through established metrics like NASA-TLX. Direct cognitive load measurements comparing reading with and without questions would resolve this.

**Open Question 2**: How do question roles evolve across different academic disciplines and over time? The study focused on a fixed time period without analyzing temporal trends or disciplinary variations. Longitudinal analysis across multiple decades and disciplines would provide evidence.

**Open Question 3**: What is the optimal frequency and placement of guiding questions for maximum reading comprehension? The study used fixed question numbers without systematically varying density or positioning. Controlled experiments varying these parameters would identify optimal patterns.

## Limitations
- Focus limited to textbooks and research articles, potentially limiting generalizability
- Human study evaluated only a single reading task, which may not represent broader contexts
- Question position identification remains challenging, affecting overall generation quality

## Confidence
- High: Dataset construction methodology and automatic evaluation results
- Medium: Human study outcomes due to single-task evaluation
- Medium: Generalization potential to other text types or domains

## Next Checks
1. Test the approach on a broader range of text types beyond textbooks and research articles
2. Evaluate the generated questions across multiple reading comprehension tasks to assess generalizability
3. Investigate alternative methods for question position identification to address the current performance gap