---
ver: rpa2
title: 'PIP: Detecting Adversarial Examples in Large Vision-Language Models via Attention
  Patterns of Irrelevant Probe Questions'
arxiv_id: '2409.05076'
source_url: https://arxiv.org/abs/2409.05076
tags:
- adversarial
- examples
- attention
- lvlms
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting adversarial examples
  in large vision-language models (LVLMs), which are vulnerable to attacks that can
  induce incorrect outputs. The authors propose a simple method called PIP that uses
  attention patterns from irrelevant probe questions to distinguish adversarial examples
  from clean ones.
---

# PIP: Detecting Adversarial Examples in Large Vision-Language Models via Attention Patterns of Irrelevant Probe Questions

## Quick Facts
- arXiv ID: 2409.05076
- Source URL: https://arxiv.org/abs/2409.05076
- Reference count: 37
- Key outcome: Achieves over 98% recall and 90% precision in detecting adversarial examples using attention patterns from irrelevant probe questions

## Executive Summary
This paper addresses the vulnerability of large vision-language models (LVLMs) to adversarial examples by proposing a novel detection method called PIP. The approach leverages attention patterns generated when LVLMs process irrelevant probe questions, such as "Is there a clock?" The key insight is that clean images produce regular attention patterns for certain types of questions (particularly yes/no questions), while adversarial examples disrupt this regularity. PIP achieves high detection performance with minimal computational overhead, requiring only one additional inference per image.

## Method Summary
PIP uses attention maps from LVLMs when processing images with irrelevant probe questions to detect adversarial examples. The method first generates adversarial examples using PGD attacks on a reference dataset. Then, it extracts attention maps from the LVLM (InstructBLIP Vicuna-7B) for both clean and adversarial examples when answering the probe question. These attention patterns are used to train an SVM classifier that can distinguish between clean and adversarial examples. The approach is designed to work across different datasets, attack methods, and LVLM architectures.

## Key Results
- Achieves over 98% recall and 90% precision in detecting adversarial examples
- Maintains high performance across different attack methods (PGD, Attack-Bard)
- Works with multiple LVLM architectures and datasets
- Requires only one additional inference for detection
- Shows strong generalizability across different scenarios

## Why This Works (Mechanism)
The method exploits the observation that LVLMs exhibit regular attention patterns when processing clean images with certain types of questions (especially yes/no questions), while adversarial perturbations disrupt these regular patterns. The irrelevant probe question serves as a probe that reveals the underlying regularity or irregularity in the attention mechanisms, making it possible to distinguish between clean and adversarial examples.

## Foundational Learning
1. **Adversarial Examples in LVLMs** - Small, imperceptible perturbations added to images that cause LVLMs to produce incorrect outputs
   - Why needed: Understanding the threat that adversarial examples pose to LVLMs
   - Quick check: Can you explain how PGD attacks generate adversarial examples?

2. **Cross-Modal Attention in LVLMs** - Mechanisms that allow LVLMs to attend to relevant regions in images when processing text queries
   - Why needed: The detection method relies on analyzing these attention patterns
   - Quick check: Do you understand how cross-modal attention works in vision-language models?

3. **SVM Classification** - Support Vector Machine algorithm used to classify attention patterns as clean or adversarial
   - Why needed: The final detection is performed by an SVM classifier
   - Quick check: Can you explain the basic principle of SVM classification?

4. **PGD Attack** - Projected Gradient Descent, an iterative method for generating adversarial examples
   - Why needed: The reference adversarial examples are generated using PGD
   - Quick check: Do you understand the basic mechanism of PGD attacks?

## Architecture Onboarding

**Component Map:** LVLM -> Attention Map Extraction -> SVM Classifier -> Detection Output

**Critical Path:** The critical path is the attention map extraction from the LVLM when processing the probe question, as this is the core mechanism that enables detection.

**Design Tradeoffs:** The method trades minimal additional computational overhead (one inference for the probe question) for high detection accuracy. The choice of yes/no questions is critical for achieving regular attention patterns in clean examples.

**Failure Signatures:** Detection failure occurs when adversarial examples manage to maintain regular attention patterns or when clean examples produce irregular patterns. This might happen with certain types of adversarial attacks or specific image-content combinations.

**First Experiments:**
1. Extract attention maps from clean examples using the probe question and verify they show regular patterns
2. Generate adversarial examples using PGD and extract attention maps to confirm irregular patterns
3. Train and evaluate SVM classifier on a small subset to verify the basic detection mechanism works

## Open Questions the Paper Calls Out

**Open Question 1:** How do attention patterns in LVLMs differ between adversarial examples generated by black-box attacks versus white-box attacks?

**Open Question 2:** Can the effectiveness of irrelevant probe questions in detecting adversarial examples generalize to other types of questions beyond yes/no questions?

**Open Question 3:** What is the underlying mechanism that causes LVLMs to exhibit regular attention patterns for clean examples when responding to yes/no questions, and how do adversarial perturbations disrupt this regularity?

## Limitations

- The paper doesn't provide exact implementation details for extracting attention maps from the InstructBLIP Vicuna-7B model
- Specific hyperparameters for PGD attacks and SVM training are not specified
- Only three attack types are tested, limiting generalizability claims
- The method's performance on more complex attack strategies is unknown

## Confidence

- **High** confidence in the core detection methodology and reported performance metrics
- **Medium** confidence in exact implementation details due to unspecified model interfacing and hyperparameters
- **Medium** confidence in generalizability claims based on limited attack types tested

## Next Checks

1. Verify attention map extraction by comparing dimensions and values of attention matrices from clean and adversarial examples
2. Test classifier sensitivity to PGD attack parameters by varying iterations and step size, measuring impact on detection performance
3. Evaluate method's performance when using different irrelevant probe questions beyond "Is there a clock?" to confirm detection mechanism independence from specific question choice