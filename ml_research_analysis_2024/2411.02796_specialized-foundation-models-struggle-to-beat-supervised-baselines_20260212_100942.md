---
ver: rpa2
title: Specialized Foundation Models Struggle to Beat Supervised Baselines
arxiv_id: '2411.02796'
source_url: https://arxiv.org/abs/2411.02796
tags:
- time
- series
- tasks
- supervised
- dasha
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We compare three specialized foundation models (genomics, satellite\
  \ imaging, time series) to traditional supervised learning (no pretraining). We\
  \ find that simple supervised models\u2014lightly modified CNNs or tuned linear\
  \ auto-regression\u2014match or outperform the latest foundation models in all three\
  \ domains, despite using far less data and parameters."
---

# Specialized Foundation Models Struggle to Beat Supervised Baselines

## Quick Facts
- arXiv ID: 2411.02796
- Source URL: https://arxiv.org/abs/2411.02796
- Reference count: 40
- Primary result: Simple supervised models match or outperform specialized foundation models across genomics, satellite imaging, and time series tasks

## Executive Summary
This paper challenges the assumption that foundation models (FMs) trained on massive datasets provide consistent advantages over traditional supervised learning in specialized domains. Through systematic comparison across genomics, satellite imaging, and time series forecasting, the authors demonstrate that carefully tuned supervised models can match or exceed the performance of the latest specialized FMs. They introduce two automated workflows—DASHA for architecture search and Auto-AR for time series tuning—that enable strong supervised baselines without pretraining. The results suggest that domain-specific architectural tuning and hyperparameter optimization can be more effective than pretraining on massive but potentially mismatched datasets.

## Method Summary
The paper compares two approaches: the FM workflow (pretraining on massive data, then fine-tuning) and the supervised workflow (model development, hyperparameter tuning, and training using only target task data). For genomics and satellite imaging, they use DASHA—an automated workflow combining DASH (Neural Architecture Search for kernel/dilation tuning) and ASHA (hyperparameter optimization). For time series, they use Auto-AR, which tunes linear auto-regression by determining optimal differencing via KPSS tests and lookback via Bayesian Information Criterion, all optimized on GPU. The methods are evaluated on established benchmarks: Nucleotide Transformer for genomics, GeoBench plus additional datasets for satellite imaging, and standard time series forecasting datasets for temporal prediction.

## Key Results
- Across all three domains, supervised models match or outperform specialized foundation models despite using far less data and parameters
- DASHA achieves state-of-the-art on the Nucleotide Transformer benchmark for genomics
- Auto-AR outperforms every open-source time series FM on standard forecasting tasks
- For satellite imaging, DASHA matches top models while being 3-10× smaller than larger FM versions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simple supervised models with domain-specific architecture tuning can outperform foundation models trained on massive data in specialized domains.
- Mechanism: Domain-specific data characteristics make generic pretraining less effective; tuning kernel sizes and dilation rates to match local data structure improves performance more than pretraining on unrelated large datasets.
- Core assumption: Local architectural features capture domain-specific signal better than generic pretraining on massive but mismatched data.
- Evidence anchors:
  - [abstract] "Across these three specialized domains, we find that it is consistently possible to train simple supervised models—no more complicated than a lightly modified wide ResNet or UNet—that match or even outperform the latest foundation models."
  - [section] "Our strong performance is driven in large part by outstanding performance on the histone modification tasks (c.f. Table 9)."
  - [corpus] Weak/no direct evidence for mechanism-specific tuning; corpus only reports related work on FMs in various domains.
- Break condition: If pretraining data is highly aligned with target domain structure (e.g., same modality, similar statistical properties), pretraining may regain advantage.

### Mechanism 2
- Claim: Tuning classical forecasting models (AR) on large lookback windows and GPU-based optimization can outperform complex foundation models in time series forecasting.
- Mechanism: Classical models are computationally simple but their hyperparameter spaces are underexplored; modern optimization on GPUs allows exhaustive search, yielding better fits than small-parameter FMs.
- Core assumption: Auto-ARIMA's default small lookback is a bottleneck; modern hardware enables practical tuning of large lookbacks.
- Evidence anchors:
  - [abstract] "tuned linear auto-regression (AR) matches or outperforms every open-source time series FM on a standard suite of forecasting tasks."
  - [section] "By dropping the MA component of the model and running the procedure on GPU, we are able to tune the lookback windows up to the maximum allowable length (usually 512); we find that longer lookbacks are critical for performance."
  - [corpus] Weak evidence; corpus papers focus on FM evaluation, not classical model tuning.
- Break condition: If the forecasting problem requires capturing complex non-linear dependencies beyond AR's linear form.

### Mechanism 3
- Claim: Automated Neural Architecture Search can simulate the model development phase of supervised learning, enabling competitive performance without manual architecture design.
- Mechanism: Weight-sharing NAS methods like DASH efficiently explore kernel size/dilation space and find task-specific configurations faster than full training cycles.
- Core assumption: NAS can identify good architectures in less time than training them from scratch, making it a viable surrogate for human-driven development.
- Evidence anchors:
  - [abstract] "our NAS-dependent workflow (DASHA)—which we cover in the first part of this section—yields our main results for genomics and satellite imaging but not for time series."
  - [section] "its success demonstrates that the procedure is an effective surrogate for human-driven model development, enabling the automated discovery of the types of diverse, domain-specific baselines."
  - [corpus] Weak evidence; corpus neighbors do not discuss NAS methodology in detail.
- Break condition: If the search space is too large or the low-fidelity evaluations do not correlate well with true performance.

## Foundational Learning

- Concept: Pretraining-then-fine-tuning paradigm
  - Why needed here: Explains the baseline method being compared (foundation models) and why its effectiveness is questioned.
  - Quick check question: What are the two main stages of the FM workflow, and how do they differ from supervised learning?
- Concept: Neural Architecture Search (NAS) with weight-sharing
  - Why needed here: Core to DASHA workflow; explains how automated model development is achieved.
  - Quick check question: How does weight-sharing in NAS reduce the computational cost of architecture search?
- Concept: KPSS test and Bayesian Information Criterion (BIC)
  - Why needed here: Core to Auto-AR tuning pipeline for determining differencing and lookback parameters.
  - Quick check question: Why is the KPSS test used before selecting the differencing parameter in Auto-AR?

## Architecture Onboarding

- Component map:
  - DASHA: CNN backbone -> DASH (kernel/dilation tuning) -> ASHA (hyperparameter tuning) -> retrain
  - Auto-AR: Data -> KPSS test (differencing) -> BIC (lookback) -> GPU-based AR fitting
- Critical path:
  - DASHA: NAS search -> hyperparameter optimization -> final training
  - Auto-AR: Stationarity test -> lookback selection -> likelihood maximization
- Design tradeoffs:
  - DASHA: Search space size vs. computational budget; choice of backbone vs. search flexibility
  - Auto-AR: Lookback window length vs. overfitting; differencing level vs. stationarity
- Failure signatures:
  - DASHA: NAS returns sub-par architectures -> validation performance plateaus early
  - Auto-AR: BIC selects too short/long lookback -> unstable or biased forecasts
- First 3 experiments:
  1. DASHA: Run DASH on a small genomics task subset; verify kernel/dilation clustering by task.
  2. DASHA: Compare Wide ResNet vs. UNet selection frequency across tasks.
  3. Auto-AR: Test KPSS test sensitivity by artificially adding trend to a stationary series.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does pretraining on massive datasets actually benefit specialized domains like genomics, satellite imaging, or time series?
- Basis in paper: [explicit] The authors show that despite pretraining on massive data, specialized FMs struggle and often fail to outperform models trained solely on downstream task data. They note that the larger FM versions consistently attain superior performance in satellite imaging but that this improvement can also be attained by DASHA, which uses no pretraining and produces a model that is 3-10× smaller.
- Why unresolved: The paper demonstrates that FMs do not consistently outperform supervised learning in the tested domains, but it does not identify the specific conditions or types of tasks where pretraining might be beneficial. The authors note that some FMs perform better with less data (Figure 3) but do not systematically explore when and why this occurs.
- What evidence would resolve it: Systematic experiments varying dataset sizes, task complexity, and domain characteristics (e.g., data distribution, temporal vs. spatial dependencies) would help identify when pretraining provides advantages. Comparing FM performance on tasks with limited vs. abundant data, and analyzing failure cases where supervised models excel, would provide clearer insights.

### Open Question 2
- Question: How can automated workflows like DASHA and Auto-AR be extended to handle cross-channel dependencies in multivariate time series forecasting?
- Basis in paper: [explicit] The authors note that all of their baselines can handle only univariate time series while all of the benchmark datasets are multivariate. They state that developing methods that leverage cross-channel dependencies for a variable number of channels remains an open problem.
- Why unresolved: The paper demonstrates the effectiveness of Auto-AR for univariate forecasting but explicitly acknowledges that cross-channel dependencies are not captured. The challenge of handling variable numbers of channels in multivariate settings is identified but not addressed.
- What evidence would resolve it: Extending Auto-AR or similar automated workflows to incorporate cross-channel information, either through multivariate AR models or attention mechanisms, and evaluating performance improvements on multivariate datasets would address this gap. Comparing approaches that handle fixed vs. variable channel numbers would be particularly informative.

### Open Question 3
- Question: What architectural modifications to CNN backbones beyond kernel size and dilation rate tuning would yield further improvements in specialized domains?
- Basis in paper: [explicit] The authors show that DASHA, which tunes kernel sizes and dilation rates in CNN backbones, consistently outperforms specialized FMs in genomics and is competitive in satellite imaging. They note that this procedure selects different but consistent-within-task kernel parameters.
- Why unresolved: While DASHA demonstrates the power of kernel size and dilation rate tuning, the paper does not explore other architectural modifications such as attention mechanisms, skip connections, or specialized activation functions that might further improve performance in specialized domains.
- What evidence would resolve it: Systematic architectural search that includes modifications beyond kernel sizes and dilation rates, or targeted modifications based on domain-specific data characteristics, would show whether additional architectural innovations could surpass the current DASHA performance. Comparing performance gains from different types of architectural modifications would identify the most promising directions.

## Limitations
- The exact NAS search space and backbone configurations used in DASHA are not fully specified
- Auto-AR's GPU optimization implementation details are sparse
- Results may not generalize beyond the specific tasks and datasets tested

## Confidence
- Genomics: High - concrete benchmark results and reproducible DASHA workflow
- Satellite imaging: High - DASHA matches top models with clear methodology
- Time series: Medium - Auto-AR outperforms open-source FMs but methodology relies on non-standard tuning approaches

## Next Checks
1. Re-run DASHA on a held-out genomics task not used in original experiments to verify architecture search discovers task-specific kernel/dilation patterns.
2. Compare Auto-AR lookback parameter distributions across datasets to confirm systematic departure from default small values.
3. Test whether increasing pretraining dataset alignment with target domain (e.g., same sensor type for satellite imaging) recovers FM advantages over supervised baselines.