---
ver: rpa2
title: Learning Off-policy with Model-based Intrinsic Motivation For Active Online
  Exploration
arxiv_id: '2404.00651'
source_url: https://arxiv.org/abs/2404.00651
tags:
- learning
- reward
- value
- planner
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses sample-efficient exploration in continuous
  control tasks. The authors propose the ACE planner, a model-based reinforcement
  learning algorithm that combines online planning with off-policy learning.
---

# Learning Off-policy with Model-based Intrinsic Motivation For Active Online Exploration

## Quick Facts
- arXiv ID: 2404.00651
- Source URL: https://arxiv.org/abs/2404.00651
- Reference count: 40
- On DMC15-500k benchmark, ACE achieves an IQM of 707, outperforming TD-MPC (668), TCRL (606), REDQ (572), and SR-SAC (718)

## Executive Summary
This paper addresses sample-efficient exploration in continuous control tasks by proposing the ACE planner, a model-based reinforcement learning algorithm that combines online planning with off-policy learning. The key innovation lies in using a novelty-aware terminal value function derived from intrinsic rewards based on forward predictive error, and an exponentially re-weighted average of MVE-based temporal difference targets. The ACE planner demonstrates competitive or superior performance compared to state-of-the-art methods, particularly excelling in sparse reward scenarios across DMControl, Adroit, and Meta-World benchmarks.

## Method Summary
The ACE planner integrates model-based planning with off-policy learning by leveraging a latent dynamics model and an encoder to project physical states into a latent space. It employs an online planner enhanced by a novelty-aware terminal value function for sample collection, and learns both policy and value functions using an exponentially re-weighted average of MVE-based temporal difference targets. The method introduces intrinsic rewards based on forward predictive error to guide exploration without incurring parameter overhead. The algorithm operates in a continuous control setting, addressing sample efficiency challenges through active online exploration.

## Key Results
- On DMC15-500k benchmark, ACE achieves an IQM of 707, outperforming TD-MPC (668), TCRL (606), REDQ (572), and SR-SAC (718)
- Demonstrates significant improvements in sample efficiency and asymptotic performance across diverse continuous control tasks
- Excels particularly in sparse reward scenarios, showing competitive performance against state-of-the-art methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The novelty-aware terminal value function Vj reduces reliance on model error and value estimation error during planning.
- Mechanism: By incorporating the terminal value function into the H-step lookahead planning framework, the method mitigates the dominant error sources (model error ϵm and value estimation error ϵv) by a factor of γ^(H-1), effectively reducing their impact on final performance.
- Core assumption: The approximation error of the dynamics model and the value function can be bounded, and the lookahead horizon H is sufficiently long to provide the desired error reduction.
- Evidence anchors:
  - [abstract]: "The ACE planner achieves competitive or superior performance compared to state-of-the-art methods, particularly excelling in sparse reward scenarios."
  - [section]: "the incorporation of Vj within the H-step lookahead planning framework helps diminish reliance on both the value estimation error ϵv and the injected bias αr by a factor of γ^(H-1)."
  - [corpus]: No direct evidence in corpus. Assumption: The theoretical bounds in Lemma 3.1 and Corollary 3.2 support this mechanism.
- Break condition: If the lookahead horizon H is too short, the error reduction factor γ^(H-1) becomes insufficient, and the dominant error sources will hinder performance.

### Mechanism 2
- Claim: The exponentially re-weighted average of MVE-based TD targets balances bias and variance for accelerated credit assignment.
- Mechanism: By calculating Qλ as an exponentially re-weighted average of TD targets with different rollout horizons, the method effectively balances the bias-variance tradeoff, leading to more stable and accurate value function learning.
- Core assumption: The model predictive error ℓr can be used as a reliable intrinsic reward signal, and the exponential weighting scheme λ appropriately balances the different rollout horizons.
- Evidence anchors:
  - [abstract]: "We introduce an RL algorithm that incorporates a predictive model and off-policy learning elements, where an online planner enhanced by a novelty-aware terminal value function is employed for sample collection."
  - [section]: "we redefine the H-step bootstrap Bellman error Lλ(θ; Γ) of the value function as below: 1/H Σ[i=t to t+H] [Qθ(ẑi, ai) - Qλ(ẑi, ai)]^2, where Qλ(ẑi, ai) is an exponentially re-weighted average of TD targets with different rollout horizons."
  - [corpus]: No direct evidence in corpus. Assumption: The exponential weighting scheme is a well-established technique in reinforcement learning for balancing bias and variance.
- Break condition: If the model predictive error ℓr is not a reliable intrinsic reward signal, or if the exponential weighting scheme λ is not appropriately tuned, the value function learning may become unstable or inaccurate.

### Mechanism 3
- Claim: The planning-centric representation learning shapes a latent state space that captures planning-relevant factors, mitigating aleatoric uncertainty.
- Mechanism: By training the dynamics model and agent components simultaneously using a multi-objective loss function that includes model predictive and value estimation errors, the method induces a state embedding zt that captures the intricate relationship among states, actions, and rewards, leading to a latent state space well-suited for planning.
- Core assumption: The gradients from the model predictive loss Lm can be backpropagated into the dynamics model and encoder to shape a latent state space that captures planning-centric factors.
- Evidence anchors:
  - [abstract]: "Leveraging the forward predictive error within a latent state space, we derive an intrinsic reward without incurring parameters overhead."
  - [section]: "We define a model predictive loss over a horizon H, denoted by Lm(θ; Γ) := c1Ld + c2Lr, as follows: Σ[i=t to t+H] [c1 * normalized prediction error + c2 * reward prediction error]."
  - [corpus]: No direct evidence in corpus. Assumption: The BYOL-style representation learning objective used in the model predictive loss is effective in shaping a latent state space for planning.
- Break condition: If the gradients from the model predictive loss are not effectively shaping the latent state space, or if the chosen representation learning objective is not suitable for planning, the method may not achieve the desired performance gains.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The paper formulates the continuous control problem as an MDP, which is the fundamental framework for reinforcement learning.
  - Quick check question: What are the key components of an MDP, and how do they relate to the continuous control problem?

- Concept: Model-based Reinforcement Learning
  - Why needed here: The paper proposes a model-based RL algorithm that leverages a learned dynamics model for planning and value function learning.
  - Quick check question: How does model-based RL differ from model-free RL, and what are the key challenges in model-based RL?

- Concept: Intrinsic Motivation
  - Why needed here: The paper derives an intrinsic reward based on the forward predictive error, which serves as a learning signal for exploration and accounts for model uncertainty.
  - Quick check question: What is the role of intrinsic motivation in reinforcement learning, and how does it differ from extrinsic rewards?

## Architecture Onboarding

- Component map:
  - Latent dynamics model (dθ) -> Encoder model (hθ) -> Reward model (rθ) -> Value function (Qjθ) -> Online planner (iCEM) -> Off-policy agent

- Critical path:
  1. Initialize the latent dynamics model, encoder, reward model, and value function.
  2. Collect samples using the online planner in the environment.
  3. Update the latent dynamics model and value function using the collected samples and the multi-objective loss function.
  4. Repeat steps 2-3 until convergence.

- Design tradeoffs:
  - Model complexity vs. prediction accuracy: A more complex dynamics model may lead to better prediction accuracy but also increases the computational cost and risk of overfitting.
  - Planning horizon H vs. computational cost: A longer planning horizon may lead to better performance but also increases the computational cost of planning.
  - Intrinsic reward coefficient cr vs. exploration vs. exploitation: A higher intrinsic reward coefficient may encourage more exploration but may also lead to suboptimal exploitation of the learned model.

- Failure signatures:
  - Poor exploration: The agent fails to discover novel states and gets stuck in local optima.
  - Model exploitation: The agent overfits to the learned model and fails to generalize to the true environment dynamics.
  - Value overestimation: The value function overestimates the true value, leading to suboptimal policies.

- First 3 experiments:
  1. Verify the learned latent dynamics model can accurately predict the next state given the current state, action, and state belief.
  2. Evaluate the performance of the online planner with and without the novelty-aware terminal value function in a simple continuous control task.
  3. Assess the impact of the intrinsic reward coefficient cr on the exploration-exploitation tradeoff in a sparse reward task.

## Open Questions the Paper Calls Out
No open questions were explicitly called out in the provided content.

## Limitations
- Theoretical bounds assume accurate model predictions and bounded errors, which may not hold in practice
- Computational overhead of H-step lookahead planning may limit scalability to more complex environments
- The exponential weighting scheme for TD targets may be sensitive to hyperparameter choices not fully explored

## Confidence
- Medium: Overall performance claims well-supported by benchmark results but may not generalize to all continuous control tasks
- Medium: Mechanism explanations theoretically grounded but lack extensive empirical validation
- High: Technical implementation details and experimental setup

## Next Checks
1. Test the ACE planner's performance with varying lookahead horizons H to assess the impact on error reduction and computational cost.
2. Conduct ablation studies to isolate the contributions of the novelty-aware terminal value function and the intrinsic reward signal to overall performance.
3. Evaluate the method's robustness to model inaccuracies by introducing controlled perturbations to the dynamics model during training and testing.