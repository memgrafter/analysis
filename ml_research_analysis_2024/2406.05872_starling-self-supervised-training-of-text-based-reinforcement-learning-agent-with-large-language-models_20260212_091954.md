---
ver: rpa2
title: 'STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent
  with Large Language Models'
arxiv_id: '2406.05872'
source_url: https://arxiv.org/abs/2406.05872
tags:
- games
- game
- text-based
- starling
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STARLING, a framework for self-supervised
  training of text-based reinforcement learning (RL) agents using large language models
  (LLMs). The key idea is to automatically generate text-based games using GPT-3 and
  an interactive fiction game engine (Inform7), based on a seed list of game ideas.
---

# STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models

## Quick Facts
- arXiv ID: 2406.05872
- Source URL: https://arxiv.org/abs/2406.05872
- Authors: Shreyas Basavatia; Keerthiram Murugesan; Shivam Ratnakar
- Reference count: 7
- Key outcome: Pre-trained STARLING agent outperforms vanilla TBRL agent on TextWorld Commonsense, ScienceWorld, and Zork1 benchmarks

## Executive Summary
STARLING introduces a self-supervised pre-training framework for text-based reinforcement learning agents using large language models to generate skill-focused games. The approach automatically creates 100 text-based games covering everyday skills like cooking and cleaning, then pre-trains a TBRL agent on these games before evaluation on benchmark environments. Results demonstrate significant performance improvements, with STARLING achieving higher normalized scores and taking fewer moves compared to baseline agents across different difficulty levels and task types.

## Method Summary
The STARLING framework generates text-based games using GPT-3 and Inform7 based on a seed list of game ideas. These games are designed to teach specific skills through gameplay. A vanilla text-based RL agent is pre-trained on 75 of these games for 100 episodes, then evaluated on 25 held-out games and three benchmark environments: TextWorld Commonsense (TWC), ScienceWorld, and Zork1. The pre-trained agent is compared against a vanilla TBRL agent to demonstrate improved generalization and skill composition abilities.

## Key Results
- STARLING achieves higher normalized scores than vanilla TBRL agent on TWC (easy, medium, hard), ScienceWorld, and Zork1
- Pre-trained agent takes fewer moves to complete tasks compared to baseline across all evaluated environments
- STARLING demonstrates improved skill composition and failure state avoidance in complex environments

## Why This Works (Mechanism)

### Mechanism 1
Self-supervised pre-training on LLM-generated games improves text-based RL agent performance by teaching generalizable skills. The LLM generates diverse, skill-focused games based on seed ideas. Agents learn basic skills (e.g., boiling, cooking) in these games, then transfer them to target environments. Core assumption: LLM-generated games accurately represent skill compositions needed for target environments. Evidence: STARLING achieves higher normalized scores and takes fewer moves compared to vanilla agent across different difficulty levels.

### Mechanism 2
Pre-training on skill-compositional games improves agent ability to avoid failure states. Games require agents to learn correct action sequences, improving decision-making in complex environments where wrong actions lead to failure. Core assumption: Failure avoidance in pre-training generalizes to target environments. Evidence: STARLING successfully avoids failure states compared to vanilla TBRL agent and tends to choose valid actions more effectively.

### Mechanism 3
STARLING's skill composition ability allows collecting intermediate rewards efficiently. Pre-training games teach agents to identify and complete sub-tasks, helping collect intermediate rewards in complex environments before reaching final goals. Core assumption: Intermediate reward collection in pre-training generalizes to target environments. Evidence: STARLING tends to collect bonus scores in Zork1 that are reachable within fewer steps instead of just chasing larger rewarded states.

## Foundational Learning

- **Concept: Text-based reinforcement learning agents**
  - Why needed here: Understanding how agents interact with text-based environments is crucial for implementing STARLING
  - Quick check question: How does a text-based RL agent represent and process the observed text from the environment?

- **Concept: Large language models for game generation**
  - Why needed here: GPT-3 is used to generate skill-focused games based on seed ideas
  - Quick check question: What are the key considerations when using LLMs to generate text-based games for RL training?

- **Concept: Self-supervised learning in RL**
  - Why needed here: STARLING uses self-supervised pre-training on LLM-generated games before fine-tuning on target environments
  - Quick check question: How does self-supervised pre-training benefit RL agents in terms of skill generalization and performance?

## Architecture Onboarding

- **Component map:** GPT-3 -> Game content generation -> Inform7 compilation -> STARLING wrapper -> TextWorld Gym -> Agent interaction

- **Critical path:** Seed game ideas → GPT-3 → Game content generation → JSON format → Inform7 compilation → Compiled game → STARLING wrapper → TextWorld Gym → Agent interaction → Training/evaluation

- **Design tradeoffs:**
  - Using GPT-3 vs manual game creation: Automation vs potential quality control issues
  - Pre-training on skill games vs direct training on target environments: Generalization vs environment-specific optimization
  - Using Inform7 vs other IF engines: Flexibility vs compatibility with existing frameworks

- **Failure signatures:**
  - GPT-3 generates invalid or nonsensical game content
  - Inform7 compilation fails due to syntax errors in generated content
  - Agent fails to learn meaningful skills during pre-training
  - Pre-trained agent shows no improvement on target environments

- **First 3 experiments:**
  1. Generate a single game using GPT-3 and compile it with Inform7 to verify the pipeline works
  2. Train a vanilla TBRL agent on a small set of generated games and evaluate on held-out games
  3. Compare pre-trained agent performance against vanilla agent on a simple target environment

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of STARLING compare to human players on the generated pre-training games? The paper mentions that human participants (high-school students) achieved a perfect normalized score of 1.0 on the 25 held-out pre-training games, but does not provide a direct comparison between STARLING's performance and human performance on these pre-training games.

### Open Question 2
How does the skill composition ability of STARLING compare to humans in novel situations? The paper states that current state-of-the-art text-based RL agents cannot use previously learned skills in new situations at the level humans can, but does not provide a detailed comparison of how well STARLING can compose skills in new situations compared to humans.

### Open Question 3
How does the performance of STARLING scale with the number of pre-training games? The paper mentions that STARLING was pre-trained on 75 games, but does not explore how performance changes with different numbers of pre-training games. A series of experiments varying the number of pre-training games would help identify any trends or optimal number.

## Limitations

- Evaluation scope is narrow, testing only three benchmark environments
- LLM-generated games may not capture full diversity of real-world text-based games
- Limited analysis of failure cases and scenarios where pre-training might not help

## Confidence

- **High confidence**: Core mechanism of using LLM-generated games for pre-training shows measurable performance improvements on tested benchmarks
- **Medium confidence**: Claim that pre-training improves skill composition and failure state avoidance generalizes beyond specific environments tested
- **Low confidence**: Scalability of this approach to larger, more diverse game sets and effectiveness across broader range of text-based RL tasks

## Next Checks

1. Test STARLING on a wider variety of text-based game benchmarks beyond the three currently evaluated environments to assess generalization capability
2. Conduct ablation studies removing different skill types from pre-training games to identify which skills are most critical for transfer learning
3. Evaluate performance degradation when pre-training games are generated with different quality levels or using alternative LLM approaches to understand robustness