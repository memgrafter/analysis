---
ver: rpa2
title: Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models
arxiv_id: '2412.15501'
source_url: https://arxiv.org/abs/2412.15501
tags:
- llms
- reasoning
- human
- arxiv
- biases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This review systematically examines whether large language models
  (LLMs) exhibit human-like cognitive patterns in decision-making, reasoning, and
  creativity, comparing their performance to human benchmarks using established psychological
  tests. Across decision-making, LLMs demonstrate multiple human-like heuristics and
  biases, such as framing and anchoring effects, but lack some common biases like
  waste aversion, suggesting partial alignment with human cognition.
---

# Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models

## Quick Facts
- arXiv ID: 2412.15501
- Source URL: https://arxiv.org/abs/2412.15501
- Authors: Zhisheng Tang; Mayank Kejriwal
- Reference count: 40
- One-line primary result: This review systematically examines whether large language models exhibit human-like cognitive patterns in decision-making, reasoning, and creativity, comparing their performance to human benchmarks using established psychological tests.

## Executive Summary
This review systematically examines whether large language models (LLMs) exhibit human-like cognitive patterns in decision-making, reasoning, and creativity, comparing their performance to human benchmarks using established psychological tests. Across decision-making, LLMs demonstrate multiple human-like heuristics and biases, such as framing and anchoring effects, but lack some common biases like waste aversion, suggesting partial alignment with human cognition. In reasoning, advanced models like GPT-4 exhibit deliberative, System-2-like processing and show super-human performance in certain tasks, while earlier models struggle with deductive reasoning and intuitive responses. For creativity, LLMs excel in language-based tasks but lag in divergent thinking that requires real-world context, though they show promise as creative collaborators. The review highlights the need for broader testing across open-source models, deeper exploration of cognitive phenomena like memory and attention, and careful consideration of LLMs' limitations in commonsense and embodied reasoning.

## Method Summary
The review synthesizes findings from 40 studies in psychology and AI literature, using established psychological benchmarks including the Cognitive Reflection Test, Wason selection task, Alternative Uses Test, and Raven's Progressive Matrices to evaluate LLM performance across decision-making, reasoning, and creativity domains. The methodology compares LLM outputs to human baseline performance data, examining the presence of human-like cognitive biases, reasoning patterns, and creative capabilities. Studies primarily focused on GPT models, with limited exploration of open-source alternatives, highlighting a need for broader model diversity in future research.

## Key Results
- LLMs demonstrate human-like cognitive biases (framing effect, anchoring heuristic) but lack some common biases like waste aversion
- Advanced models like GPT-4 show deliberative, System-2-like reasoning, while smaller models exhibit more intuitive responses
- LLMs excel at language-based creativity but struggle with divergent thinking tasks requiring real-world context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit human-like cognitive patterns because their training data contains human-generated examples of these patterns, which the models implicitly learn.
- Mechanism: During pretraining on massive human text corpora, LLMs encounter examples of decision-making biases, reasoning heuristics, and creative expressions. Through statistical learning, the models internalize these patterns without explicit programming.
- Core assumption: The training data sufficiently represents human cognitive patterns across the domains studied.
- Evidence anchors:
  - [abstract] "LLMs released since the first edition of ChatGPT in early 2023 have also been argued to mimic abilities like creativity"
  - [section] "On decision-making, our synthesis reveals that while LLMs demonstrate several human-like biases, some biases observed in humans are absent"
  - [corpus] Weak - only 5/25 related papers found, suggesting limited corpus support for this specific mechanism
- Break condition: If training data lacks sufficient examples of certain cognitive patterns, those patterns will not emerge in the LLM outputs.

### Mechanism 2
- Claim: LLMs transition from intuitive (System 1) to deliberative (System 2) reasoning as they scale in size and capability.
- Mechanism: Larger models with improved task comprehension can engage in more deliberate reasoning processes, similar to human System 2 thinking, rather than defaulting to quick, heuristic responses.
- Core assumption: Model size and architecture improvements enable more complex reasoning capabilities.
- Evidence anchors:
  - [abstract] "advanced LLMs like GPT-4 exhibit deliberative reasoning akin to human System-2 thinking, while smaller models fall short"
  - [section] "as LLMs get larger and their task comprehension improves, they respond more intuitively, resembling System 1 processing. However, with GPT-3.5 and GPT-4, there was a significant shift towards more deliberative, System 2 like processing"
  - [corpus] Moderate - corpus contains studies comparing different model sizes, supporting size-dependent reasoning capabilities
- Break condition: If reasoning improvements plateau despite further scaling, the size-dependent reasoning mechanism may not hold.

### Mechanism 3
- Claim: LLMs excel in language-based creative tasks but struggle with tasks requiring real-world context or physical grounding.
- Mechanism: The language-centric nature of LLM training enables strong performance on linguistic creativity (storytelling, wordplay) but limits their ability to innovate in tasks requiring embodied understanding or practical application.
- Core assumption: Training data and architecture bias toward linguistic rather than physical or contextual understanding.
- Evidence anchors:
  - [abstract] "in creativity: while LLMs excel in language-based creative tasks, such as storytelling, they struggle with divergent thinking tasks that require real-world context"
  - [section] "LLMs seem to excel in language-related creative tasks, whereas those that are either multi-modal or abstract may require more (or different) advances"
  - [corpus] Moderate - corpus includes studies on LLM creativity limitations in practical domains
- Break condition: If multi-modal training or architectural changes enable strong performance on real-world creative tasks, this mechanism would be challenged.

## Foundational Learning

- Concept: Cognitive heuristics and biases
  - Why needed here: Understanding human decision-making patterns is essential for interpreting LLM performance on decision-making tasks
  - Quick check question: Can you name three common cognitive biases studied in humans that were evaluated in LLMs?

- Concept: Dual-process theory (System 1 vs System 2 thinking)
  - Why needed here: The distinction between intuitive and deliberative reasoning is central to understanding LLM reasoning capabilities
  - Quick check question: What is the key difference between System 1 and System 2 reasoning in both humans and LLMs?

- Concept: Divergent thinking and creativity assessment
  - Why needed here: Creative tasks require different evaluation criteria than analytical tasks, particularly when comparing human and LLM outputs
  - Quick check question: What are the two main human creativity tests mentioned that were adapted for LLM evaluation?

## Architecture Onboarding

- Component map: Pretraining corpus → Transformer layers → Fine-tuning → Inference framework; Evaluation framework: Psychological tests → Human benchmarks → LLM comparison
- Critical path: Pretraining on human-generated text → Internalization of cognitive patterns → Evaluation against human benchmarks → Identification of emergent behaviors
- Design tradeoffs: Open vs closed models (transparency vs accessibility), size vs reasoning capability (computational cost vs performance), language-only vs multi-modal (creativity domain vs resource requirements)
- Failure signatures: Hallucinations in reasoning tasks, lack of certain expected biases, poor performance on embodied creativity tasks, size-dependent reasoning plateaus
- First 3 experiments:
  1. Replicate classic cognitive bias tests (framing effect, anchoring heuristic) with GPT-4 and compare to human performance data
  2. Test deductive reasoning using Wason selection task across multiple model sizes to verify System 1 vs System 2 transition
  3. Compare LLM performance on AUT (Alternative Uses Test) versus creative writing tasks to quantify the language vs real-world creativity gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do open-source language models exhibit similar decision-making biases and cognitive patterns as GPT models, and if not, what accounts for the differences?
- Basis in paper: [explicit] The authors note that "the majority of experiments reported in the literature we reviewed tend to rely heavily on the GPT family of LLMs" and advocate for testing more open-source models to avoid potential bias and non-replication issues.
- Why unresolved: Most existing studies have focused on GPT models, leaving open-source models like LLaMA, BLOOM, and Falcon largely unexplored in terms of cognitive patterns, decision-making biases, and reasoning capabilities.
- What evidence would resolve it: Empirical studies comparing the performance of multiple open-source LLMs (e.g., LLaMA, BLOOM, Falcon) against GPT models on established psychological tests for decision-making biases, reasoning, and creativity, with human benchmarks for comparison.

### Open Question 2
- Question: How do embodied cognition and physical grounding affect the creative problem-solving abilities of LLMs, particularly in divergent thinking tasks?
- Basis in paper: [explicit] The authors highlight that LLMs lack embodied cognition, which "places obvious constraints on LLMs' ability to create innovative solutions for divergent thinking tasks that require a deep and real-world (including physical) understanding of the objects given."
- Why unresolved: Current studies show that LLMs struggle with divergent thinking tasks requiring real-world context, but the specific role of embodied cognition in these limitations has not been thoroughly explored or tested.
- What evidence would resolve it: Experiments testing LLMs' performance on divergent thinking tasks with varying degrees of physical and contextual grounding, potentially using multimodal models or hybrid approaches that integrate sensory data to simulate embodied cognition.

### Open Question 3
- Question: To what extent do instruction-tuning and reinforcement learning from human feedback (RLHF) increase cognitive biases in LLMs, and can these biases be mitigated through alternative fine-tuning approaches?
- Basis in paper: [explicit] The authors mention that "instruction-tuned LLMs show improved performance but also heightened biases," suggesting that the process of instruction fine-tuning may induce increased bias.
- Why unresolved: While studies have shown that instruction-tuned models exhibit more biases, the underlying mechanisms and potential mitigation strategies remain unclear.
- What evidence would resolve it: Comparative studies analyzing the cognitive biases of LLMs before and after instruction-tuning, as well as experiments testing alternative fine-tuning methods (e.g., adversarial training, bias-aware fine-tuning) to reduce these biases while maintaining performance.

## Limitations

- Limited exploration of open-source models, with most studies focusing on GPT models, potentially introducing bias
- Incomplete alignment with human cognition, as evidenced by the absence of certain human biases like waste aversion
- Lack of comprehensive testing for embodied cognition and commonsense reasoning limitations

## Confidence

- High confidence: LLMs exhibit multiple human-like cognitive biases and heuristics (framing effect, anchoring heuristic) - supported by multiple studies across different model families
- Medium confidence: Advanced LLMs demonstrate System-2-like deliberative reasoning while smaller models show more intuitive responses - consistent pattern across model sizes but with some variation in specific tasks
- Medium confidence: LLMs excel at language-based creativity but struggle with real-world context tasks - strong evidence for linguistic creativity, weaker evidence for divergent thinking limitations

## Next Checks

1. Conduct systematic testing of 5-10 open-source LLM models (Mistral, Llama, Claude) across the full suite of cognitive tests used in the review to assess whether human-like patterns emerge consistently across model architectures and training approaches
2. Design and implement novel tests for embodied creativity and commonsense reasoning that require physical or contextual understanding, then compare performance across model sizes and training paradigms
3. Test for the presence/absence of additional human cognitive phenomena not covered in the review, including memory effects, attention biases, and temporal reasoning patterns, using both controlled experiments and naturalistic prompts