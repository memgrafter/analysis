---
ver: rpa2
title: Resource-constrained Fairness
arxiv_id: '2406.01290'
source_url: https://arxiv.org/abs/2406.01290
tags:
- fairness
- rate
- cost
- group
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces resource-constrained fairness in machine
  learning, addressing how fairness metrics behave under limited resources. The authors
  formulate fairness as a resource allocation problem and show that leveling up under
  constrained resources leads to equality of harm between groups.
---

# Resource-constrained Fairness

## Quick Facts
- arXiv ID: 2406.01290
- Source URL: https://arxiv.org/abs/2406.01290
- Reference count: 40
- Key outcome: Fairness metrics under limited resources lead to equality of harm between groups, with cost varying significantly across selection rates

## Executive Summary
This paper introduces resource-constrained fairness in machine learning, addressing how fairness metrics behave when selection capacity is limited. The authors formulate fairness as a resource allocation problem and show that under constrained resources, optimizing for minimal maximum harm across groups results in equal harm distribution between groups. They develop methods to enforce fairness at different selection rates and analyze the cost of fairness across multiple datasets. The primary finding is that fairness enforcement costs vary significantly with resource levels, being negligible at extreme selection rates but substantial at intermediate levels.

## Method Summary
The authors use XGBoost for tabular datasets and ResNet models for image datasets, implementing a per-group threshold optimization approach that enforces fairness by varying thresholds for each group while maintaining a fixed global selection rate. They measure the cost of fairness as the loss in precision when enforcing demographic parity (DP) or equal opportunity (EO) fairness metrics. The analysis examines how these costs vary across different selection rates from 1% to 100% on multiple real-world datasets including Adult Income, Compas, Dutch Census, Law Admission, CelebA, and Fitzpatrick17k.

## Key Results
- The cost of fairness is negligible at very low or very high selection rates but substantial at intermediate levels
- The cost is generally lower for equal opportunity (EO) than demographic parity (DP)
- The cost is influenced by base rate disparity, model performance, subgroup noise, and disadvantaged group size
- Theoretical bounds for the cost of fairness align with observed empirical patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fairness under resource constraints leads to equality of harm between groups when leveling up.
- Mechanism: When resources are limited, optimizing for minimal maximum harm across groups forces allocation such that no group experiences disproportionately high harm, resulting in equal harm distribution.
- Core assumption: Harm is strictly decreasing with respect to selection rate for each group.
- Evidence anchors:
  - [abstract] "leveling up under constrained resources leads to equality of harm between groups"
  - [section] "Under constrained resources, maximally leveling-up results in equality of harm between groups."
- Break condition: If harm increases with selection rate, the mechanism fails as the optimal solution would be trivial (select no one).

### Mechanism 2
- Claim: The cost of fairness is bounded by the product of the proportion of instances swapped and the average cost per swap.
- Mechanism: Fairness enforcement requires swapping labels between instances to balance harm, and the total cost is the number of swaps multiplied by the cost per swap.
- Core assumption: Any labeling with the same selection rate can be transformed via swaps.
- Evidence anchors:
  - [section] "We can measure this by the difference between the harm of the optimal classifier... and the classifier that satisfies fairness"
  - [section] "cost = p · c"
- Break condition: If the cost per swap is unbounded (e.g., non-linear metrics), the bound becomes uninformative.

### Mechanism 3
- Claim: The cost of fairness varies with resource levels, being lowest at very low or very high selection rates.
- Mechanism: At extreme resource levels, either few candidates are selected (making fairness cheap) or many are selected (making the optimal and fair allocations similar).
- Core assumption: Model performance and base rate disparity influence the gap between optimal and fair allocations.
- Evidence anchors:
  - [abstract] "the cost of fairness varies significantly with resource levels - it can be negligible at very low or very high selection rates but substantial at intermediate levels"
  - [section] "we find that the cost of fairness is bounded above by rc, and (1 − r)c"
- Break condition: If model performance is perfect or base rates are identical, the cost curve may flatten differently.

## Foundational Learning

- Concept: Resource-constrained optimization
  - Why needed here: The paper models fairness as a constrained optimization problem where resources (selection capacity) are limited.
  - Quick check question: If you have 100 slots to fill and two groups, what happens to fairness if you fill all slots from one group?

- Concept: Group fairness metrics (Demographic Parity and Equal Opportunity)
  - Why needed here: These metrics define how fairness is measured across groups, which is central to the paper's analysis.
  - Quick check question: What's the difference between requiring equal selection rates versus equal true positive rates?

- Concept: Cost of fairness
  - Why needed here: The paper quantifies the trade-off between fairness and performance, which is essential for understanding real-world implications.
  - Quick check question: If enforcing fairness reduces accuracy by 5%, what does this tell us about the "cost" of being fair?

## Architecture Onboarding

- Component map: Trained classifier -> Per-group threshold optimization -> Fairness metric enforcement -> Cost measurement
- Critical path: Train classifier → Sweep per-group thresholds for fairness metrics → Measure cost at each selection rate → Analyze patterns across datasets
- Design tradeoffs: Post-hoc threshold adjustment is flexible but requires access to validation data; pre-processing methods might be more stable but less adaptable to different resource levels
- Failure signatures: Oscillating fairness metrics with changing thresholds, high cost of fairness at specific resource levels, or inability to achieve fairness for certain metrics
- First 3 experiments:
  1. Verify that demographic parity can be achieved by simple thresholding (select top r from each group).
  2. Measure the cost of fairness (precision loss) at very low and very high selection rates to confirm the bell-shaped curve.
  3. Test the impact of introducing noise to one group to observe how subgroup noise affects fairness enforcement costs.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical bounds rely on assumptions about harm being strictly decreasing with selection rate
- Results may not generalize to all real-world scenarios, focusing primarily on binary classification tasks
- The behavior of more complex fairness constraints or multi-class problems under resource constraints remains unexplored

## Confidence
- Confidence: Medium - Theoretical bounds assume specific relationships between model performance, base rate disparity, and selection rates
- Confidence: Medium - Empirical analysis covers six diverse datasets but may not generalize to all scenarios
- Confidence: Low - Mechanisms linking resource levels to fairness costs assume specific functional relationships not fully characterized

## Next Checks
1. Implement the per-group threshold optimization algorithm on additional datasets not included in the original study. Compare the observed fairness costs against the theoretical bounds to assess their generalizability.

2. Repeat the experiments using different classifier types (e.g., logistic regression, neural networks) and varying model performance levels. Analyze how model quality affects the cost of fairness curve and whether the bell-shaped pattern persists.

3. Design experiments to test the framework's applicability to multi-class classification problems and continuous fairness metrics. Measure whether the theoretical bounds and empirical patterns identified in the paper hold for these more complex scenarios.