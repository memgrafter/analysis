---
ver: rpa2
title: 'DP$^2$-FedSAM: Enhancing Differentially Private Federated Learning Through
  Personalized Sharpness-Aware Minimization'
arxiv_id: '2409.13645'
source_url: https://arxiv.org/abs/2409.13645
tags:
- local
- learning
- privacy
- updates
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of maintaining model utility in
  differentially private federated learning (DPFL) under data heterogeneity. The authors
  propose DP2-FedSAM, which combines partial model personalization with sharpness-aware
  minimization to reduce the adverse effects of clipping and noise addition in DPFL.
---

# DP$^2$-FedSAM: Enhancing Differentially Private Federated Learning Through Personalized Sharpness-Aware Minimization

## Quick Facts
- arXiv ID: 2409.13645
- Source URL: https://arxiv.org/abs/2409.13645
- Reference count: 40
- Key outcome: DP2-FedSAM achieves approximately 12-30% higher accuracy on CIFAR-10 and 14% higher accuracy on FEMNIST compared to DP-FedAvg under non-IID data distributions with privacy budget ε=1.0

## Executive Summary
This paper addresses the challenge of maintaining model utility in differentially private federated learning (DPFL) under data heterogeneity. The authors propose DP2-FedSAM, which combines partial model personalization with sharpness-aware minimization to reduce the adverse effects of clipping and noise addition in DPFL. The method trains a shared representation extractor while allowing each client to have a personalized classifier head, and applies SAM to enhance robustness against noise. Extensive experiments demonstrate significant accuracy improvements over state-of-the-art DPFL methods.

## Method Summary
The method trains a single shared representation extractor across all clients while enabling each client to have a personalized classifier head. SAM optimizer is applied to update the shared representation extractor, and each client updates their personal classifier head using standard SGD. Model updates are clipped and Gaussian noise is added to provide differential privacy. The server aggregates noisy updates from sampled clients to update the global shared representation. The approach is evaluated on FEMNIST and CIFAR-10 datasets with non-IID distributions, comparing against DP-FedAvg, DP-FedSAM, and CENTAUR baselines.

## Key Results
- DP2-FedSAM achieves approximately 12-30% higher accuracy on CIFAR-10 compared to DP-FedAvg
- DP2-FedSAM achieves 14% higher accuracy on FEMNIST compared to DP-FedAvg under non-IID data distributions
- The method maintains a privacy budget of ε=1.0 while significantly improving utility
- Theoretical analysis provides convergence guarantees and privacy bounds for the proposed method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Partial model-sharing reduces the norm of local updates by training a shared representation extractor while allowing each client to have a personalized classifier head.
- Mechanism: In heterogeneous data distributions, clients have minimal discrepancies in their data representations but substantial differences in their classifier heads. By sharing only the representation extractor, the method reduces the bias introduced by clipping in DP training.
- Core assumption: The representation extractor is approximately homogeneous across clients even under data heterogeneity.
- Evidence anchors:
  - [abstract]: "Instead of training a shared full model with high inconsistency across clients under heterogeneous data distributions, we train a single shared representation extractor while enabling each client to have a personalized classifier head."
  - [section]: "Specifically, restricting the norm of local updates can reduce the impact of noise but may compromise the model's accuracy; sparsification techniques might lead to accuracy instability in the presence of imbalanced data."
  - [corpus]: Weak evidence - corpus neighbors do not specifically address partial model-sharing mechanisms.
- Break condition: If representation extractors become heterogeneous across clients due to extreme data distribution differences, the clipping error reduction benefit diminishes.

### Mechanism 2
- Claim: Sharpness-Aware Minimization (SAM) reduces the norm of local updates by seeking flat minima in the loss landscape.
- Mechanism: SAM updates parameters by solving a min-max problem that seeks model parameter values whose entire neighborhoods have uniformly low training loss values. This creates models with smaller variations with respect to parameter changes, leading to smaller norm of local updates.
- Core assumption: The loss landscape near flat minima exhibits smaller variations with respect to parameter changes.
- Evidence anchors:
  - [abstract]: "SAM updates the model weights θ in two steps. First, it computes the stochastic gradient and calculates the perturbation as follows: p* = q ||∇F(θ)||²/||∇F(θ)||₂²"
  - [section]: "In the region near flat minima, the loss function exhibits smaller variations with respect to parameter changes, resulting in the smaller norm of model updates."
  - [corpus]: Weak evidence - corpus neighbors do not specifically address SAM mechanisms in DPFL context.
- Break condition: If data heterogeneity prevents the aggregation of local flat models into a global flat model, the noise robustness benefit is lost.

### Mechanism 3
- Claim: Combining partial model-sharing and SAM achieves a global flat model even under data heterogeneity, providing enhanced robustness against noise-induced errors.
- Mechanism: Partial model-sharing provides more consistent shared representation extractors across clients, while SAM provides local flat models. The combination creates a globally flat minimum that is more robust to the noise added for differential privacy.
- Core assumption: The combination of homogeneous representation extractors and local flat models can create a globally flat minimum despite data heterogeneity.
- Evidence anchors:
  - [abstract]: "By combining partial model-sharing and sharpness-aware training, we can obtain a global flat model after aggregation even in the heterogeneous data setting."
  - [section]: "However, by combining partial model-sharing and SAM, we can achieve a global flat model even in data heterogeneity settings."
  - [corpus]: Weak evidence - corpus neighbors do not specifically address the combination of these mechanisms.
- Break condition: If the noise magnitude overwhelms the flat landscape benefits or if the partial model-sharing fails to maintain consistency, the noise robustness degrades.

## Foundational Learning

- Concept: Differential Privacy (DP) in federated learning
  - Why needed here: DP is essential for protecting client privacy by adding noise to model updates, but it introduces utility degradation that the paper aims to mitigate.
  - Quick check question: What is the relationship between the noise multiplier σ and the privacy budget ε in differential privacy?

- Concept: Sharpness-Aware Minimization (SAM)
  - Why needed here: SAM is used to find flat minima in the loss landscape, which reduces the sensitivity of model updates to perturbations, thereby mitigating the impact of clipping and noise addition.
  - Quick check question: How does SAM's min-max optimization problem formulation differ from standard gradient descent?

- Concept: Data heterogeneity in federated learning
  - Why needed here: Data heterogeneity across clients is a key challenge that causes inconsistency in model updates, leading to clipping errors and degraded utility in DPFL.
  - Quick check question: What are the main causes of data heterogeneity in federated learning scenarios?

## Architecture Onboarding

- Component map:
  Shared representation extractor (ϕ) -> Personalized classifier head (hi) -> SAM optimizer -> Clipping mechanism -> Gaussian noise addition -> Aggregation server

- Critical path:
  1. Server samples clients and broadcasts shared representation extractor
  2. Each client updates personal classifier head using standard SGD
  3. Each client applies SAM to update shared representation extractor
  4. Each client computes and clips model updates
  5. Each client adds Gaussian noise to clipped updates
  6. Server aggregates noisy updates to update global shared representation
  7. Repeat until convergence

- Design tradeoffs:
  - Partial vs full model sharing: Partial sharing reduces communication overhead and information leakage but may limit global model expressiveness
  - SAM perturbation radius (q): Larger q provides more robustness but may slow convergence; smaller q is computationally efficient but less effective
  - Clipping threshold (C): Smaller C provides stronger privacy but may introduce more bias; larger C preserves utility but requires more noise
  - Client sampling ratio (r): Higher r provides better utility but weaker privacy; lower r provides stronger privacy but may slow convergence

- Failure signatures:
  - High variance in local updates across clients indicates poor partial model-sharing consistency
  - Persistent accuracy degradation despite SAM application suggests insufficient perturbation radius
  - Slow convergence or divergence indicates poor choice of learning rates or client sampling ratio
  - High clipping frequency indicates model updates exceeding threshold too often

- First 3 experiments:
  1. Baseline comparison: Implement DP-FedAvg and DP-FedSAM without personalization to establish performance floor
  2. Partial model-sharing validation: Implement DP2-FedSAM without SAM to isolate the effect of partial sharing
  3. SAM validation: Implement DP2-FedSAM without partial sharing to isolate the effect of SAM optimizer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the perturbation radius q affect the convergence rate and final accuracy of DP2-FedSAM in non-IID data settings?
- Basis in paper: [explicit] The paper discusses the impact of q on performance and mentions that a moderate level of perturbation is beneficial.
- Why unresolved: The paper only tests a limited set of q values and does not provide a comprehensive analysis of the relationship between q and convergence rate or accuracy across different non-IID data distributions.
- What evidence would resolve it: Experiments varying q across a wider range of values and different non-IID data settings, measuring both convergence rate and final accuracy.

### Open Question 2
- Question: What is the optimal balance between local iterations τh and τϕ for maximizing the performance of DP2-FedSAM under different levels of data heterogeneity?
- Basis in paper: [explicit] The paper mentions that the choice of τϕ affects performance and discusses the trade-off between minimizing client drift and ensuring sufficient local training.
- Why unresolved: The paper only explores a limited set of τϕ values and does not provide a systematic analysis of the optimal balance between τh and τϕ for different levels of data heterogeneity.
- What evidence would resolve it: Experiments varying both τh and τϕ across different non-IID data distributions, measuring the impact on convergence rate and final accuracy.

### Open Question 3
- Question: How does the performance of DP2-FedSAM scale with the number of clients and the size of the local datasets in highly heterogeneous data settings?
- Basis in paper: [inferred] The paper evaluates DP2-FedSAM on datasets with a limited number of clients and does not explore the scaling behavior with respect to the number of clients or local dataset sizes.
- Why unresolved: The paper does not provide experiments or analysis on the scaling behavior of DP2-FedSAM with respect to the number of clients or local dataset sizes in highly heterogeneous data settings.
- What evidence would resolve it: Experiments scaling the number of clients and local dataset sizes in highly heterogeneous data settings, measuring the impact on convergence rate and final accuracy.

## Limitations

- The convergence analysis assumes data heterogeneity is bounded by a parameter τ, but the relationship between τ and practical non-IID distributions is not empirically verified.
- The theoretical privacy bounds rely on moments accountant method, but the paper does not validate these bounds against composition theorems for adaptive algorithms.
- While the paper claims partial model-sharing reduces information leakage, the exact privacy leakage quantification for personalized classifier heads is not provided.

## Confidence

**High Confidence**: The empirical observation that DP2-FedSAM outperforms baseline DPFL methods on CIFAR-10 and FEMNIST datasets. The experimental methodology is sound and the reported accuracy improvements are statistically significant across multiple runs.

**Medium Confidence**: The theoretical convergence guarantees for the proposed algorithm. While the mathematical framework is rigorous, the assumptions about bounded data heterogeneity and smooth loss landscapes may not hold in all practical scenarios.

**Low Confidence**: The claim that partial model-sharing inherently reduces information leakage compared to full model personalization. The paper mentions this benefit but does not provide rigorous privacy analysis comparing different personalization strategies.

## Next Checks

1. **Robustness to extreme heterogeneity**: Test DP2-FedSAM with clients having disjoint class labels (e.g., 1-2 classes per client) to verify the claimed consistency of shared representation extractors under maximum heterogeneity conditions.

2. **Privacy-utility trade-off validation**: Implement the moments accountant privacy accounting to verify that the theoretical privacy bounds (ε ≤ 1.0) are maintained in practice, and measure any deviation from claimed privacy guarantees.

3. **Generalization across architectures**: Evaluate DP2-FedSAM with deeper neural network architectures (e.g., ResNet-50) and transformer-based models to assess whether the benefits generalize beyond the specific CNN and ResNet-18 architectures used in the experiments.