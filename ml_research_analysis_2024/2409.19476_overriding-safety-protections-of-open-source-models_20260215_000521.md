---
ver: rpa2
title: Overriding Safety protections of Open-source Models
arxiv_id: '2409.19476'
source_url: https://arxiv.org/abs/2409.19476
tags:
- harmful
- fine-tuned
- safety
- data
- basemodel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that fine-tuning an open-source LLM (Llama-3.1-8B)
  with harmful data can significantly override its safety protections, increasing
  Attack Success Rate (ASR) by 35% compared to the base model. Conversely, fine-tuning
  with safety data reduces ASR by 51.68%, making the model safer.
---

# Overriding Safety protections of Open-source Models

## Quick Facts
- arXiv ID: 2409.19476
- Source URL: https://arxiv.org/abs/2409.19476
- Authors: Sachin Kumar
- Reference count: 3
- Fine-tuning Llama-3.1-8B with harmful data increases Attack Success Rate (ASR) by 35%

## Executive Summary
This paper demonstrates that fine-tuning an open-source LLM (Llama-3.1-8B) with harmful data can significantly override its safety protections, increasing Attack Success Rate (ASR) by 35% compared to the base model. Conversely, fine-tuning with safety data reduces ASR by 51.68%, making the model safer. The study also shows that harmful fine-tuning leads to higher model uncertainty, evidenced by increased perplexity and entropy, and decreased token probability, resulting in greater knowledge drift and reduced truthfulness. Safe fine-tuning has minimal impact on uncertainty metrics.

## Method Summary
The study uses Llama-3.1-8B-Instruct (quantized 4-bit) as the base model, fine-tuning it with LoRA (rank 16) using the LLM-LAT dataset. Two models are created: one fine-tuned on harmful data and another on safety data. Both models are evaluated on HarmBench (harmfulness) and TriviaQA (knowledge drift) datasets. Safety is measured using Llama-Guard-3-8B-INT8, while uncertainty metrics include perplexity, entropy, and token probability. The training uses AdamW-8bit optimizer with a learning rate of 2e-4 for 50 steps.

## Key Results
- Fine-tuning with harmful data increases ASR by 35% compared to base model
- Fine-tuning with safety data reduces ASR by 51.68% compared to base model
- Harmful fine-tuning increases model uncertainty (perplexity, entropy) and decreases token probability, indicating knowledge drift

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on harmful data overrides safety guardrails by shifting model weights toward harmful behavior patterns
- Mechanism: During fine-tuning, gradient updates reinforce parameters that generate harmful responses while weakening those associated with safety refusals
- Core assumption: Safety guardrails are implemented as learned behavior rather than hardcoded constraints
- Evidence anchors:
  - [abstract] "fine-tuning an open-source LLM (Llama-3.1-8B) with harmful data can significantly override its safety protections, increasing Attack Success Rate (ASR) by 35%"
  - [section] "Harmful Model : trained using Column 'prompt' and 'rejected' as mentioned in previous section"
- Break condition: If safety mechanisms are implemented as external filters or runtime constraints rather than learned behaviors

### Mechanism 2
- Claim: Harmful fine-tuning increases model uncertainty, leading to knowledge drift and reduced truthfulness
- Mechanism: When fine-tuning with harmful data, the model learns conflicting response patterns that create ambiguity in its decision boundaries
- Core assumption: Model uncertainty metrics (perplexity, entropy, token probability) directly correlate with knowledge reliability and truthfulness
- Evidence anchors:
  - [abstract] "fine-tuning the model on harmful data makes it less helpful or less trustworthy because of increase in model uncertainty leading to knowledge drift"
  - [section] "Harmful fine-tuned model was least accurate on TriviaQA dataset, with comparatively higher perplexity, higher entropy and low probability"
- Break condition: If uncertainty metrics don't correlate with actual truthfulness, or if the model maintains separate knowledge representations

### Mechanism 3
- Claim: Safe fine-tuning reinforces positive behavior patterns without significantly impacting helpfulness
- Mechanism: Fine-tuning with safety data provides gradient updates that strengthen the model's tendency to refuse harmful requests and generate safe responses
- Core assumption: Safety behaviors can be reinforced through standard supervised fine-tuning without degrading general capabilities
- Evidence anchors:
  - [abstract] "fine-tuning a model with safety data reduces ASR by 51.68%, making the model safer"
  - [section] "Safe fine-tuned model just like basemodel was more trustworthy in responses as reflected in the corresponding metrics"
- Break condition: If safety fine-tuning introduces over-refusal or significantly degrades performance on non-harmful tasks

## Foundational Learning

- Concept: Fine-tuning mechanics and LoRA (Low-Rank Adaptation)
  - Why needed here: The paper uses LoRA for efficient fine-tuning of the 8B parameter model
  - Quick check question: What is the primary advantage of using LoRA over full fine-tuning for large language models?

- Concept: Safety alignment and red teaming methodologies
  - Why needed here: The paper references safety alignment training and red teaming as the baseline protections being overridden
  - Quick check question: How do red teaming exercises typically evaluate the safety of language models?

- Concept: Uncertainty metrics in language models
  - Why needed here: The paper uses perplexity, entropy, and token probability to measure knowledge drift and uncertainty
  - Quick check question: What does high perplexity indicate about a language model's confidence in its predictions?

## Architecture Onboarding

- Component map:
  Base Model (Llama-3.1-8B-Instruct-bnb-4bit) -> LoRA Adapters (rank 16) -> Optimizer (AdamW-8bit) -> Safety Classifier (Llama-Guard-3-8B-INT8) -> Evaluation Datasets (HarmBench, TriviaQA)

- Critical path:
  1. Load base model and prepare fine-tuning data
  2. Apply LoRA adapters and configure training arguments
  3. Fine-tune on either harmful or safety data
  4. Generate completions on evaluation datasets
  5. Analyze results using safety classifier and uncertainty metrics

- Design tradeoffs:
  - Using 4-bit quantization enables training on consumer GPUs but may slightly impact model quality
  - LoRA rank 16 provides a balance between adaptation capability and parameter efficiency
  - HarmBench provides comprehensive safety evaluation but may not cover all harmful scenarios

- Failure signatures:
  - If ASR doesn't increase significantly after harmful fine-tuning, safety mechanisms may be too robust or implemented externally
  - If uncertainty metrics don't change despite knowledge drift, the metrics may not be sensitive enough
  - If safe fine-tuning degrades general performance, the safety data may be too restrictive

- First 3 experiments:
  1. Replicate the harmful fine-tuning and ASR measurement to verify the 35% increase
  2. Test the impact of different LoRA ranks (8, 16, 32) on safety override effectiveness
  3. Evaluate whether removing LoRA adapters after fine-tuning restores original safety behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different open-source models (e.g., Mistral, Gemma, Qwen2.5) compare in terms of vulnerability to harmful fine-tuning and effectiveness of safety fine-tuning?
- Basis in paper: Explicit - The paper suggests future work to test the process on other open-source models
- Why unresolved: The study only used Llama-3.1-8B for experiments, limiting generalizability to other models
- What evidence would resolve it: Comparative experiments applying the same harmful and safety fine-tuning methodologies to multiple open-source models

### Open Question 2
- Question: What is the long-term impact of harmful fine-tuning on a model's performance in reasoning and commonsense tasks?
- Basis in paper: Explicit - The paper suggests evaluating helpfulness and truthfulness on reasoning benchmarks like GSM8K, MATH, etc
- Why unresolved: The study only evaluated model performance on TriviaQA, not comprehensive reasoning or commonsense benchmarks
- What evidence would resolve it: Fine-tuning harmful and safe models, then evaluating them on diverse reasoning and commonsense benchmarks

### Open Question 3
- Question: Can activation steering or other intervention methods effectively mitigate the harmfulness introduced by fine-tuning with harmful data?
- Basis in paper: Explicit - The paper suggests exploring activation steering approaches to make harmful models safer
- Why unresolved: The study did not test any mitigation strategies against harmful fine-tuning effects
- What evidence would resolve it: Implementing activation steering or other intervention methods on harmful fine-tuned models and measuring their impact on ASR and uncertainty metrics

## Limitations

- Dataset Reliability and Representativeness: The LLM-LAT dataset is synthetic and may not fully capture real-world harmful or safety scenarios
- Safety Metric Dependence: Results depend on Llama-Guard-3-8B-INT8 accuracy, which may have its own blind spots
- Single Model Architecture: Results are demonstrated only on Llama-3.1-8B-Instruct, limiting generalizability

## Confidence

**High Confidence Claims**:
- Fine-tuning with harmful data increases ASR by 35% compared to base model
- Fine-tuning with safety data reduces ASR by 51.68% compared to base model
- Uncertainty metrics (perplexity, entropy, token probability) show significant changes with harmful fine-tuning

**Medium Confidence Claims**:
- Harmful fine-tuning causes knowledge drift and reduces truthfulness
- Safe fine-tuning reinforces positive behavior without significant performance degradation
- The mechanism of safety override operates through weight updates rather than external filters

**Low Confidence Claims**:
- The exact relationship between uncertainty metrics and real-world knowledge reliability
- Generalizability of results to models with different base safety training approaches
- Long-term stability of safety overrides after continued training or deployment

## Next Checks

1. Cross-Model Validation: Replicate the harmful fine-tuning experiment on at least two additional model architectures (e.g., Mistral, Gemma) to test generalizability

2. Human Evaluation Correlation: Conduct human evaluation studies to verify whether observed increases in perplexity and entropy actually correlate with reduced truthfulness in real-world usage

3. Temporal Stability Analysis: Track the stability of safety overrides over extended fine-tuning periods and after deployment-like conditions to determine permanence of the safety override