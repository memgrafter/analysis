---
ver: rpa2
title: 'LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances Model
  Merging'
arxiv_id: '2410.17146'
source_url: https://arxiv.org/abs/2410.17146
tags:
- tasks
- task
- mnist
- cars
- mnist20406080
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LiNeS (Layer-increasing Network Scaling) is a post-training technique
  that mitigates catastrophic forgetting and improves multi-task model merging. It
  applies layer-wise scaling to parameter updates, preserving shallow-layer features
  while retaining task-specific adaptations in deeper layers.
---

# LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances Model Merging

## Quick Facts
- arXiv ID: 2410.17146
- Source URL: https://arxiv.org/abs/2410.17146
- Authors: Ke Wang; Nikolaos Dimitriadis; Alessandro Favero; Guillermo Ortiz-Jimenez; Francois Fleuret; Pascal Frossard
- Reference count: 40
- Key outcome: LiNeS mitigates catastrophic forgetting and improves multi-task model merging by applying layer-wise scaling to parameter updates, preserving shallow-layer features while retaining task-specific adaptations in deeper layers.

## Executive Summary
LiNeS (Layer-increasing Network Scaling) is a post-training technique that addresses catastrophic forgetting and enhances multi-task model merging. By applying linear scaling to parameter updates based on layer depth—progressively scaling shallow layers more aggressively than deep layers—LiNeS preserves general features while retaining task-specific adaptations. The method integrates seamlessly with existing merging techniques and improves performance across diverse vision and NLP benchmarks, requiring minimal hyperparameter tuning and computational overhead.

## Method Summary
LiNeS works by computing a task vector (τₜ) representing the difference between a fine-tuned model (θₜ) and its pre-trained counterpart (θ₀). This task vector is then scaled using a linear function based on layer depth (α + β·ℓ), where shallow layers receive more aggressive scaling than deeper layers. The scaled task vector is added back to the pre-trained model to create the final model. For multi-task merging, LiNeS applies the same layer-wise scaling to merged parameters, reducing task interference by preserving general features in shallow layers while retaining task-specific features in deeper layers.

## Key Results
- Maintains 99.8% target task accuracy while restoring 97.9% pre-trained generalization on control tasks for CLIP ViT-B/32
- Improves multi-task merging performance by 3.1-4.0% across 8-20 task benchmarks with various architectures
- Integrates seamlessly with existing merging methods (Task Arithmetic, Ties-Merging, Consensus Merging) without requiring retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer-increasing scaling mitigates catastrophic forgetting by preserving general features in shallow layers while retaining task-specific adaptations in deeper layers.
- Mechanism: During fine-tuning, parameter updates in shallow layers distort pre-trained general features, while deeper layers encode task-specific representations. By scaling down updates in shallow layers more aggressively than in deeper layers, LiNeS preserves the general features while retaining task-specific adaptations.
- Core assumption: Shallow layers capture more general features while deeper layers specialize in task-specific representations.
- Evidence anchors:
  - [abstract] "maintaining shallow layers close to their pre-trained values to preserve general features while allowing deeper layers to retain task-specific representations"
  - [section] "updates to the shallow layers contribute minimally to target tasks" and "the degradation of performance on control tasks is largely due to these distortions in the shallow layers"
  - [corpus] No direct corpus evidence for this specific mechanism
- Break condition: If the assumption about layer specialization is incorrect, or if shallow layers contribute significantly to target task performance.

### Mechanism 2
- Claim: Layer-wise scaling reduces negative task interference during multi-task model merging by preserving general features beneficial across tasks.
- Mechanism: When merging multiple task vectors, contributions from one task can distort general features needed by other tasks. By scaling merged parameters based on layer depth, LiNeS preserves general features in shallow layers while retaining task-specific features in deeper layers, reducing interference.
- Core assumption: Task interference in model merging stems from distortion of general features by task-specific contributions.
- Evidence anchors:
  - [abstract] "In multi-task model merging scenarios, layer-wise scaling of merged parameters reduces negative task interference"
  - [section] "task interference is linked to catastrophic forgetting, as the individual task vectors lose a significant amount of generalization ability to other tasks after fine-tuning and merging them leads to interference among each other"
  - [corpus] No direct corpus evidence for this specific mechanism
- Break condition: If task interference is not primarily due to general feature distortion, or if layer-wise scaling doesn't effectively preserve general features.

### Mechanism 3
- Claim: LiNeS improves out-of-distribution generalization by maintaining the balance between pre-trained and fine-tuned knowledge.
- Mechanism: By preserving general features in shallow layers while retaining task-specific knowledge in deeper layers, LiNeS maintains the model's ability to generalize to unseen distributions while still performing well on the target task.
- Core assumption: The balance between general and task-specific features is crucial for out-of-distribution generalization.
- Evidence anchors:
  - [abstract] "integrating seamlessly with existing multi-task model merging baselines improving their performance across benchmarks and model sizes, and can boost generalization when merging LLM policies aligned with different rewards via RLHF"
  - [section] "Our post-training scaling method significantly improves generalization while maintaining near-full target task accuracy"
  - [corpus] No direct corpus evidence for this specific mechanism
- Break condition: If out-of-distribution performance depends on different factors than the balance between general and task-specific features.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Understanding why fine-tuning degrades performance on previously learned tasks is essential for grasping the problem LiNeS addresses
  - Quick check question: What happens to a model's performance on control tasks when it's fine-tuned on a specific target task?

- Concept: Layer specialization in neural networks
  - Why needed here: LiNeS relies on the assumption that shallow layers capture general features while deeper layers specialize in task-specific representations
  - Quick check question: According to the paper, which layers of a neural network typically capture more general features versus task-specific features?

- Concept: Model merging and task interference
  - Why needed here: LiNeS extends to multi-task settings where merging multiple fine-tuned models can lead to performance degradation due to interference
  - Quick check question: What problem arises when merging multiple models fine-tuned on different tasks, and how does LiNeS address it?

## Architecture Onboarding

- Component map:
  - Pre-trained model (θ₀) -> Fine-tuned model (θₜ) -> Task vector (τₜ) -> LiNeS scaling function -> Scaled task vector (τLiNeS) -> Final model (θ₀ + τLiNeS)

- Critical path:
  1. Compute task vector from fine-tuned and pre-trained models
  2. Apply layer-increasing scaling to task vector
  3. Add scaled task vector to pre-trained model
  4. Tune hyper-parameters (α, β) for optimal performance

- Design tradeoffs:
  - Linear vs. non-linear scaling: Linear scaling keeps implementation simple but may not capture complex layer relationships
  - Number of tunable parameters: LiNeS uses only α and β, making it computationally efficient but potentially less flexible than methods with per-layer tuning
  - Application scope: Works for single-task and multi-task settings but requires careful consideration of which layers to scale in different architectures

- Failure signatures:
  - Target task performance drops significantly: Scaling is too aggressive on shallow layers
  - Control task performance remains low: Scaling isn't aggressive enough on shallow layers
  - Computational overhead increases: Hyper-parameter search space is too large
  - Model merging performance degrades: Scaling doesn't effectively reduce task interference

- First 3 experiments:
  1. Apply LiNeS to a single fine-tuned model and compare target vs. control task performance
  2. Apply LiNeS to multi-task model merging and compare baseline vs. LiNeS-enhanced performance
  3. Apply LiNeS to WiSE-FT interpolation and compare out-of-distribution generalization with and without LiNeS

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LiNeS scale to extremely large models (e.g., trillion-parameter LLMs) compared to gradient-based merging methods like AdaMerging or aTLAS?
- Basis in paper: [explicit] The paper compares LiNeS to gradient-based methods and notes that "both aTLAS and AdaMerging require multiple training epochs, making it challenging to scale for large models" while "LiNeS efficiently scales to large models like LLaMA."
- Why unresolved: The paper demonstrates LiNeS effectiveness on LLaMA-2 7B but doesn't provide scaling experiments for much larger models or quantitative comparisons of computational overhead versus gradient-based methods at scale.
- What evidence would resolve it: Scaling experiments comparing LiNeS to AdaMerging/aTLAS on models with 10B+ parameters, measuring both performance and wall-clock training time/memory usage.

### Open Question 2
- Question: What is the theoretical justification for why linear scaling of layer updates works better than other scaling functions (quadratic, square root, etc.) mentioned in the ablation study?
- Basis in paper: [explicit] The ablation study shows "using quadratic scaling sometimes outperforms using identify function" but "we choose the linear scaling to keep the method simple and general" without explaining the theoretical advantage.
- Why unresolved: The paper provides empirical evidence that linear scaling works well but doesn't offer a theoretical explanation for why this particular scaling function is optimal or why it should outperform alternatives.
- What evidence would resolve it: Mathematical analysis connecting the linear scaling to properties of neural network loss landscapes, or empirical studies showing how different scaling functions interact with specific architectures' learning dynamics.

### Open Question 3
- Question: How does LiNeS perform when applied to models trained with different regularization techniques or architectural modifications (e.g., BatchNorm, LayerNorm, different attention mechanisms)?
- Basis in paper: [inferred] The paper shows LiNeS works across different architectures (ViT, ConvNeXt, T5) and with LoRA fine-tuning, but doesn't systematically test combinations with various regularization techniques or architectural variations.
- Why unresolved: While the paper demonstrates broad applicability, it doesn't explore whether LiNeS performance is consistent across models with different normalization layers, regularization schemes, or architectural innovations like local attention or MLP-mixers.
- What evidence would resolve it: Comprehensive experiments applying LiNeS to models with different normalization strategies, dropout rates, weight decay schedules, and alternative attention mechanisms, measuring performance consistency across these variations.

## Limitations
- Relies on the assumption that shallow layers capture general features while deeper layers specialize in task-specific representations, which may not hold universally across all architectures and tasks
- Evaluation focuses on specific architectures (CLIP ViT variants, ConvNeXt, T5-large) and benchmark tasks, limiting generalizability to other model types
- Does not explore behavior in true continual learning scenarios with sequential task arrivals, only evaluating two-phase fine-tune/evaluate setups

## Confidence
High confidence: The core empirical results showing LiNeS improves target task retention while maintaining generalization on control tasks. The multi-task merging performance gains (3.1-4.0% improvements) are well-supported by the presented experiments.

Medium confidence: The mechanism explanations linking layer-wise scaling to catastrophic forgetting prevention and task interference reduction. While the empirical results support these mechanisms, the causal explanations rely on assumptions about layer specialization that aren't directly validated.

Low confidence: The generalizability of LiNeS to architectures and tasks beyond those evaluated, and its performance in continual learning settings with sequential task arrivals.

## Next Checks
1. **Layer Contribution Analysis**: Conduct ablation studies that systematically disable or modify specific layer ranges to empirically verify which layers contribute most to target task performance versus generalization. This would validate or challenge the assumption about layer specialization patterns.

2. **Architecture Generalization Test**: Apply LiNeS to a diverse set of architectures including convolutional models with different depths (ResNet variants), transformer-based models with varying attention patterns, and smaller/larger models to test whether the linear scaling approach generalizes across architectural families.

3. **Continual Learning Evaluation**: Implement a sequential task learning setup where tasks arrive one at a time, applying LiNeS after each task. Measure performance on all previously seen tasks to evaluate whether LiNeS maintains its effectiveness in true continual learning scenarios rather than the two-phase fine-tune/evaluate setup used in the paper.