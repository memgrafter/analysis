---
ver: rpa2
title: NUTS, NARS, and Speech
arxiv_id: '2405.17874'
source_url: https://arxiv.org/abs/2405.17874
tags:
- speech
- nars
- which
- performance
- intelligence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates using the Non Axiomatic Reasoning System
  (NARS) for speech recognition under the premise that intelligence is the capacity
  to adapt with insufficient knowledge and resources. It proposes NUTS, a method combining
  random dimensionality reduction, preprocessing, and NARS.
---

# NUTS, NARS, and Speech

## Quick Facts
- arXiv ID: 2405.17874
- Source URL: https://arxiv.org/abs/2405.17874
- Authors: D. van der Sluis
- Reference count: 40
- Primary result: 64% accuracy on 35-word speech command dataset with only 2 training examples per class using NARS-based reasoning

## Executive Summary
This paper proposes NUTS (raNdom dimensionality redUction non axiomaTic reasoning few Shot learner), a novel approach to speech recognition that combines random dimensionality reduction, preprocessing, and the Non Axiomatic Reasoning System (NARS). The method processes speech by converting audio to MEL-spectral features, reducing dimensions through random projection, and using NARS to perform classification based on similarity relationships. NUTS achieves 64% accuracy on a 35-word speech command dataset with only 2 training examples per class, outperforming random guessing (2%) and matching the performance of the Whisper Tiny model while requiring significantly less computational resources.

## Method Summary
The method involves converting 16kHz audio to 80-bin MEL-spectral features (8000 values per 1-second utterance), applying random projection to reduce dimensionality to 4 values, and processing through the Naliifier to create Narsese statements representing instance-property relationships. NARS then performs inference by comparing similarities between instances and determining classifications based on learned relationships. The approach is tested on the Speech Commands v2 dataset with 35 words, using only 2 labeled examples per class for training.

## Key Results
- NUTS achieves 64% accuracy on 35-class speech recognition task with only 2 training examples per class
- Outperforms random guessing baseline (2%) and matches Whisper Tiny model performance
- Inference time is 0.02 seconds compared to 0.8 seconds for Whisper Tiny model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random dimensionality reduction enables NARS to perform speech recognition by reducing 8000 MEL features to a manageable embedding space.
- Mechanism: The 8000-dimensional MEL spectrum is multiplied by a random 8000xD matrix, projecting data into a lower-dimensional space where similarity relationships become computationally tractable for NARS inference.
- Core assumption: The random projection preserves enough discriminative information between speech classes for NARS to learn meaningful similarity relationships.
- Evidence anchors:
  - [abstract]: "These 8000 values were multiplied by a randomly generated 8000 ∗ D matrix, reducing the dimensions to D"
  - [section]: "For a baselines we used 1) general speech recognition pre-trained Whisper models, and 2) the earlier mentioned ANAM model"
- Break condition: If the random projection discards too much discriminative information, performance would collapse to random guessing levels (2%).

### Mechanism 2
- Claim: Naliﬁer preprocessing is essential for NARS to interpret speech data meaningfully.
- Mechanism: The Naliﬁer compares properties across instances, identifies similarities, and synthesizes new Narsese statements that capture relationships between instances rather than just instance-property mappings.
- Core assumption: Without the Naliﬁer, NARS cannot learn that instances (utterances) are similar to each other rather than to individual properties.
- Evidence anchors:
  - [abstract]: "We took 3 random utterances of ‘one’... NARS successfully determined that instance C, the unlabelled instance, was similar to instance A"
  - [section]: "The Naliﬁer took considerable time to execute, to load and ’train’ 2 instances with 2000 properties each, took 95 minutes"
- Break condition: If the Naliﬁer's O(n²) algorithm becomes computationally prohibitive for larger datasets or higher dimensions.

### Mechanism 3
- Claim: NARS' ability to handle conflicting information and revise beliefs enables effective few-shot learning.
- Mechanism: NARS maintains subjective truth values that are updated as new evidence arrives, allowing it to refine classifications with minimal training examples.
- Core assumption: The system can maintain useful beliefs despite having only 2 training examples per class.
- Evidence anchors:
  - [abstract]: "NARS has the advantages that 1) can cope with holding conflicting information in its knowledge base 2) can explain predictions, 3) requires less data for inference"
  - [section]: "With only 2 training examples NUTS performs similarly to the Whisper Tiny model for discrete word identification"
- Break condition: If the limited knowledge base size (AIKR parameter) becomes saturated with too few examples to maintain meaningful distinctions.

## Foundational Learning

- Concept: Non-Axiomatic Reasoning System (NARS)
  - Why needed here: NARS is the core reasoning engine that processes the reduced dimensionality representations and makes classification decisions based on similarity relationships.
  - Quick check question: What is the key difference between NARS and traditional logical systems like CYC?

- Concept: Random Projection Dimensionality Reduction
  - Why needed here: Transforms high-dimensional speech features (8000 values) into a lower-dimensional space where NARS can operate efficiently while preserving class-discriminative information.
  - Quick check question: Why does random projection work for dimensionality reduction instead of requiring learned transformations?

- Concept: Narsese Encoding
  - Why needed here: Provides the structured language format for representing instances, properties, and relationships that NARS can process and reason about.
  - Quick check question: How does Narsese represent the strength of a property relationship to an instance?

## Architecture Onboarding

- Component map: Audio → MEL → Random projection → Naliﬁer → NARS → Classification
- Critical path: Audio → MEL → Random projection → Naliﬁer → NARS → Classification
- Design tradeoffs:
  - Computational efficiency vs. accuracy: 4 dimensions gives 64% accuracy with 0.02s inference vs. higher dimensions or learned methods
  - Training data requirements: Only 2 examples per class vs. thousands for deep learning models
  - Interpretability vs. performance: NARS provides explainable reasoning but lower accuracy than state-of-the-art models

- Failure signatures:
  - Performance collapses to random guessing (2%) → dimensionality reduction losing too much information
  - Slow inference times (minutes) → Naliﬁer O(n²) algorithm bottleneck
  - Incorrect classifications for similar words (bed vs. bird) → NARS struggling with acoustically similar classes

- First 3 experiments:
  1. Test random projection with synthetic data (known similarity structure) to validate the mechanism before using real speech data
  2. Benchmark Naliﬁer performance with increasing numbers of instances and properties to understand computational scaling
  3. Compare NARS classification accuracy across different numbers of dimensions (2-10) to find the optimal tradeoff point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of NUTS scale with different dimensionality reduction techniques (e.g., PCA, autoencoders) compared to random projections?
- Basis in paper: [inferred] The paper mentions using random projections for dimensionality reduction but doesn't explore other techniques.
- Why unresolved: The paper only tested random projections and didn't compare with other dimensionality reduction methods.
- What evidence would resolve it: Experimental results comparing NUTS performance using different dimensionality reduction techniques on the same dataset.

### Open Question 2
- Question: What is the impact of increasing the number of training examples per class on NUTS' performance and computational efficiency?
- Basis in paper: [explicit] The paper notes performance increases with the number of examples (from 64% at 2 examples to 90% at 20) but doesn't explore the full range or computational implications.
- Why unresolved: The paper only tested up to 20 examples and didn't analyze the trade-off between performance gains and computational cost.
- What evidence would resolve it: Detailed experiments varying the number of training examples per class and measuring both accuracy and computational resources required.

### Open Question 3
- Question: Can NUTS be extended to handle multi-word speech recognition tasks, and how would its performance compare to state-of-the-art models in this domain?
- Basis in paper: [explicit] The paper mentions that Whisper leverages language models for multi-word performance, implying NUTS may not be optimized for this task.
- Why unresolved: The paper only tested single-word recognition and didn't explore multi-word scenarios or compare with models designed for this purpose.
- What evidence would resolve it: Experiments testing NUTS on multi-word speech recognition tasks and comparing results with established multi-word recognition models.

## Limitations

- The Naliifier's O(n²) computational complexity becomes prohibitive for larger datasets, taking 95 minutes to process just two training instances with 2000 properties each
- The random projection mechanism lacks empirical validation on speech data specifically, with unknown sufficiency for preserving discriminative information for acoustically similar word pairs
- Results are limited to single-word recognition on a specific 35-word vocabulary, with unknown generalizability to multi-word speech or different domains

## Confidence

- High confidence: Core accuracy claim (64% on 35-class task) is directly measurable and comparable to baseline
- Medium confidence: Computational efficiency claim (0.02s vs 0.8s) depends on implementation details not fully specified
- Low confidence: Generalizability beyond tested 35-word vocabulary and single-word format

## Next Checks

1. **Scaling Validation**: Test NUTS on incrementally larger vocabularies (10, 20, 35, 50 words) to empirically measure how classification accuracy and Naliifier processing time scale with dataset size.

2. **Cross-Domain Transfer**: Apply NUTS to a different speech recognition task (e.g., speaker identification or continuous speech) to validate whether the NARS-based reasoning generalizes beyond single-word commands.

3. **Ablation Study**: Systematically remove each component (random projection, Naliifier preprocessing, NARS reasoning) to quantify their individual contributions to the 64% accuracy and identify the true performance floor.