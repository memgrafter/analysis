---
ver: rpa2
title: Visual Question Decomposition on Multimodal Large Language Models
arxiv_id: '2409.19339'
source_url: https://arxiv.org/abs/2409.19339
tags:
- decovqa
- question
- sub-questions
- finetuned
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a systematic framework for evaluating the
  visual question decomposition (VQD) ability of multimodal large language models
  (MLLMs). It identifies that existing MLLMs struggle to generate high-quality sub-questions
  for complex visual questions.
---

# Visual Question Decomposition on Multimodal Large Language Models

## Quick Facts
- **arXiv ID**: 2409.19339
- **Source URL**: https://arxiv.org/abs/2409.19339
- **Reference count**: 40
- **Primary result**: Finetuning MLLMs on specialized VQD datasets with SelectiveVQD Loss significantly improves sub-question quality and VQA accuracy

## Executive Summary
This paper addresses the challenge of visual question decomposition (VQD) in multimodal large language models (MLLMs), identifying that existing models struggle to generate high-quality sub-questions for complex visual queries. The authors create DecoVQA, a specialized dataset with manually annotated sub-questions, and an upgraded version DecoVQA+ that includes an extra QA round to teach selective decomposition. They propose a finetuning pipeline using LoRA adapters with a novel SelectiveVQD Loss combining next-token prediction and binary cross-entropy losses. Experimental results demonstrate significant improvements in sub-question quality metrics (non-repetition, relevance, groundedness) and higher accuracy on visual question answering benchmarks, while the models learn to selectively decompose questions when beneficial.

## Method Summary
The paper introduces a systematic framework for evaluating and improving VQD in MLLMs through a multi-stage approach. First, the authors create specialized datasets (DecoVQA and DecoVQA+) with manually annotated sub-questions and an extra QA round for teaching selective decomposition. They then implement a finetuning pipeline using LoRA adapters on MLLMs like LLaVA-1.5 and MiniGPT-v2. The core innovation is the SelectiveVQD Loss, which combines NTP loss for sub-question generation with BCE loss for deciding whether decomposition is needed. Evaluation is performed using the SubQuestRater framework, which assesses sub-question quality across three criteria: non-repetition, relevance, and groundedness. The finetuned models are tested on VQA benchmark datasets and show significant improvements in both sub-question quality and overall VQA accuracy.

## Key Results
- Finetuned MLLMs show significant improvement in sub-question quality metrics (non-repetition, relevance, groundedness) on the SubQuestRater dataset
- Models achieve higher accuracy on VQA benchmark datasets (A-OKVQA, GQA, VQA-Introspect) after finetuning
- The selective decomposition capability improves substantially, with better accuracy on whether-to-decompose decisions using DecoVQA+
- The finetuning pipeline with 400 samples in DecoVQA leads to similar results as larger datasets, indicating efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VQD improves reasoning performance by decomposing complex questions into simpler, more answerable sub-questions.
- Mechanism: Complex questions requiring multi-step reasoning are broken down into sub-questions that are easier to answer from image information or commonsense knowledge, reducing cognitive load on the model.
- Core assumption: Simpler sub-questions can be answered more accurately than the original complex question.
- Evidence anchors:
  - [abstract] "Question decomposition has emerged as an effective strategy for prompting Large Language Models (LLMs) to answer complex questions."
  - [section 1] "Question Decomposition (QD) is an effective strategy to address this issue."
  - [corpus] Weak evidence - no directly relevant papers found in corpus.
- Break condition: If sub-questions become too fragmented and lose connection to the original question, or if the model cannot effectively integrate answers from sub-questions.

### Mechanism 2
- Claim: The SubQuestRater framework enables quantitative evaluation of VQD quality by assessing non-repetition, relevance, and groundedness of sub-questions.
- Mechanism: GPT-4V scores sub-questions based on three criteria, providing a systematic way to measure VQD effectiveness beyond just final answer accuracy.
- Core assumption: High-quality sub-questions (non-repetitive, relevant, grounded) correlate with better final answers.
- Evidence anchors:
  - [section 3] "To assess the question decomposition capability of MLLMs, a significant obstacle is the absence of metrics for evaluating models' question decomposition abilities."
  - [section 3] "We proposed three criteria to assess the quality of sub-questions, as follows: Non-Repetition, Relevance, and Groundedness."
  - [corpus] Weak evidence - no directly relevant papers found in corpus.
- Break condition: If GPT-4V's scoring becomes inconsistent with human judgments or if the three criteria fail to capture all important aspects of sub-question quality.

### Mechanism 3
- Claim: SelectiveVQD Loss combines NTP loss and BCE loss to improve the model's ability to decide when to decompose questions.
- Mechanism: BCE loss penalizes errors in the binary decision of whether to decompose, while NTP loss handles the actual generation of sub-questions or answers.
- Core assumption: Models can learn a better policy for when decomposition is beneficial versus when direct answering suffices.
- Evidence anchors:
  - [section 4.3] "To improve MLLMs' capability of identifying the questions that need to be decomposed, we propose a training objective, SelectiveVQD Loss, combining the NTP loss and a binary cross entropy loss (BCE loss)."
  - [section 5.2] "There is a significant improvement in the accuracy on Whether2Deco after fine-tuning with DecoVQA+, compared to the baseline and the checkpoint finetuned by DecoVQA."
  - [corpus] Weak evidence - no directly relevant papers found in corpus.
- Break condition: If the BCE loss overwhelms the NTP loss or if the model overfits to the binary classification task at the expense of generation quality.

## Foundational Learning

- **Multimodal Large Language Models (MLLMs)**: Why needed here: The paper specifically targets MLLMs for visual question decomposition, distinguishing it from unimodal approaches. Quick check question: What architectural components allow MLLMs to process both visual and textual information?
- **Question Decomposition**: Why needed here: The entire paper is built around the concept of breaking down complex questions into simpler sub-questions. Quick check question: What are the key criteria for evaluating the quality of decomposed sub-questions?
- **Finetuning with LoRA**: Why needed here: The paper uses LoRA for efficient parameter updates during finetuning on the specialized datasets. Quick check question: How does LoRA differ from full fine-tuning in terms of parameter efficiency and training speed?

## Architecture Onboarding

- **Component map**: Vision encoder (frozen pre-trained) -> Linear connection layer -> Language model (LLaVA-1.5, MiniGPT-v2) -> LoRA adapter layers -> GPT-4V for evaluation
- **Critical path**: Input image and question -> Selective decomposition decision (binary classification) -> If decompose: generate sub-questions -> Answer sub-questions -> Integrate sub-question answers to answer original question -> Evaluate using SubQuestRater
- **Design tradeoffs**: LoRA vs full fine-tuning: parameter efficiency vs potential performance; Three criteria for evaluation vs more comprehensive but complex metrics; Selective decomposition vs always decomposing
- **Failure signatures**: Model consistently answers "No" to decomposition (doesn't decompose enough); Model consistently answers "Yes" to decomposition (decomposes unnecessarily); Sub-questions fail non-repetition criterion (repetitive output); Sub-questions fail relevance criterion (off-topic generation); Sub-questions fail groundedness criterion (cannot be answered from image/commonse knowledge)
- **First 3 experiments**: Evaluate baseline MLLMs on SubQuestRater dataset to establish current VQD quality; Finetune MLLMs on DecoVQA and evaluate improvement in VQD quality; Finetune MLLMs on DecoVQA+ with SelectiveVQD Loss and evaluate improvement in both VQD quality and whether-to-decompose accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of sub-questions generated by MLLMs change when finetuned on larger datasets beyond DecoVQA?
- Basis in paper: [explicit] The paper mentions that varying the number of samples in DecoVQA leads to similar results, suggesting that 400 samples are sufficient.
- Why unresolved: The paper does not explore the impact of significantly larger datasets on the quality of sub-questions.
- What evidence would resolve it: Experiments comparing the quality of sub-questions generated by MLLMs finetuned on DecoVQA with those finetuned on datasets 5-10 times larger, using the same evaluation metrics.

### Open Question 2
- Question: Can the proposed SelectiveVQD Loss be adapted for use with other vision-language tasks beyond VQA?
- Basis in paper: [inferred] The paper focuses on VQD for VQA tasks, but the SelectiveVQD Loss is a general training objective that could potentially be applied to other tasks.
- Why unresolved: The paper does not investigate the applicability of the SelectiveVQD Loss to other vision-language tasks.
- What evidence would resolve it: Experiments applying the SelectiveVQD Loss to other vision-language tasks, such as visual reasoning or image captioning, and evaluating the performance improvements.

### Open Question 3
- Question: How does the performance of the proposed finetuning pipeline compare to other parameter-efficient tuning methods, such as prompt tuning or adapter-based methods?
- Basis in paper: [inferred] The paper uses LoRA for finetuning, but does not compare its performance to other parameter-efficient tuning methods.
- Why unresolved: The paper does not explore the relative effectiveness of different parameter-efficient tuning methods for VQD.
- What evidence would resolve it: Experiments comparing the performance of the proposed finetuning pipeline with other parameter-efficient tuning methods, such as prompt tuning or adapter-based methods, on the same VQD tasks.

## Limitations
- The evaluation relies heavily on GPT-4V as the judge for sub-question quality, introducing potential circularity and bias
- Manual annotation of DecoVQA dataset may not be representative of full diversity of complex visual questions in real-world scenarios
- Lacks extensive ablation studies on the selective decomposition mechanism, particularly the necessity of BCE loss component

## Confidence

**High Confidence**: The observation that existing MLLMs struggle with visual question decomposition quality is well-supported by baseline experiments showing low SubQuestRater scores. The effectiveness of the finetuning pipeline in improving sub-question quality is demonstrated with clear quantitative improvements across multiple evaluation metrics.

**Medium Confidence**: The claim that SelectiveVQD Loss significantly improves the model's ability to decide when to decompose is supported by accuracy improvements on the Whether2Deco dataset, but lacks direct comparison between models finetuned with and without the BCE loss component. The assertion that finetuned models achieve higher VQA accuracy on benchmark datasets is supported, but ablation study only shows improvement over baseline.

**Low Confidence**: The paper claims that the three criteria (non-repetition, relevance, groundedness) comprehensively capture sub-question quality, but this is primarily asserted rather than validated against human judgments. The claim that the extra QA round in DecoVQA+ is essential for teaching selective decomposition is supported by improvements but lacks rigorous ablation.

## Next Checks
1. **Human Evaluation Validation**: Conduct a human study where annotators independently evaluate the sub-questions generated by baseline vs. finetuned models on the three SubQuestRater criteria, to validate that GPT-4V's scoring correlates with human judgments and doesn't introduce bias.

2. **Ablation on SelectiveVQD Loss Components**: Run experiments comparing the full SelectiveVQD Loss (NTP + BCE) against versions with only NTP loss and only BCE loss, to determine which component drives the improvements in both sub-question quality and selective decomposition accuracy.

3. **Cross-dataset Generalization Test**: Evaluate the finetuned models on visual question answering datasets that were not used during finetuning or in the paper's experiments, to assess whether the improvements in VQD quality generalize to completely unseen question types and image domains.