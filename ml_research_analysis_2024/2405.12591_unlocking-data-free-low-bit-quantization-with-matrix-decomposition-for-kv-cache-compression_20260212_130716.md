---
ver: rpa2
title: Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache
  Compression
arxiv_id: '2405.12591'
source_url: https://arxiv.org/abs/2405.12591
tags:
- quantization
- cache
- decoquant
- matrix
- tensor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DecoQuant, a data-free low-bit quantization
  technique for KV cache compression in large language models. The core idea is to
  adjust the outlier distribution of the original matrix through tensor decomposition,
  allowing the quantization difficulty to be migrated from the matrix to decomposed
  local tensors.
---

# Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression

## Quick Facts
- arXiv ID: 2405.12591
- Source URL: https://arxiv.org/abs/2405.12591
- Authors: Peiyu Liu; Ze-Feng Gao; Wayne Xin Zhao; Yipeng Ma; Tao Wang; Ji-Rong Wen
- Reference count: 16
- Key outcome: Data-free low-bit quantization technique achieving up to 75% reduction in KV cache memory footprint while maintaining comparable generation quality

## Executive Summary
This paper introduces DecoQuant, a novel data-free approach for compressing KV cache in large language models through matrix decomposition and quantization. The method addresses the challenge of outlier values in matrix quantization by decomposing the KV cache matrix into two tensors, allowing most parameters to be compressed with low-bit quantization while maintaining critical information in high-precision format. The technique achieves significant memory reduction without requiring calibration data, making it practical for deployment scenarios where data collection is difficult or impossible.

## Method Summary
DecoQuant employs Matrix Product Operator (MPO) decomposition to split KV cache matrices into two tensors: a large tensor (TL) containing ~99.4% of parameters and a small tensor (TS) with ~0.6% of parameters. The large tensor undergoes low-bit quantization (2/4/8-bit) while the small tensor maintains FP16 precision, effectively migrating quantization difficulty to the decomposed local tensors. A fused kernel performs dequantization during inference, combining the dequantization operator with the GeMM operator to minimize data movement and improve efficiency. The approach is data-free, eliminating the need for calibration datasets typically required for quantization.

## Key Results
- Achieves up to 75% reduction in KV cache memory footprint
- Maintains comparable generation quality across LAMBADA and in-context learning tasks
- Outperforms existing quantization methods on evaluated LLaMA and OPT models
- Provides data-free compression, eliminating need for calibration datasets

## Why This Works (Mechanism)
DecoQuant works by decomposing the KV cache matrix to isolate and handle outliers separately from the bulk of the data. The MPO decomposition creates a large tensor with a narrow value range (easier to quantize) and a small tensor that preserves critical information. By maintaining the small tensor in high precision while aggressively quantizing the large tensor, the method preserves model performance while achieving significant compression. The fused kernel optimization ensures that the computational overhead of dequantization is minimized during inference.

## Foundational Learning
- **Matrix Product Operator (MPO) Decomposition**: A tensor decomposition technique that factorizes matrices into products of smaller tensors, needed to separate outlier-heavy regions from the bulk of the data; quick check: verify decomposition preserves matrix multiplication properties
- **Outlier-aware Quantization**: Quantization strategies that handle extreme values differently from typical values, needed because standard quantization fails on matrices with large outliers; quick check: measure value distribution before and after decomposition
- **Kernel Fusion**: Combining multiple GPU operations into a single kernel to reduce memory transfers, needed to offset the computational cost of dequantization; quick check: compare latency with and without fusion
- **Data-free Quantization**: Approaches that compress models without calibration data, needed for deployment scenarios where data collection is impractical; quick check: verify no calibration dataset is required in the pipeline
- **KV Cache Compression**: Reducing the memory footprint of key-value states in transformer attention mechanisms, needed to enable longer context lengths and reduce memory pressure; quick check: measure memory usage before and after compression

## Architecture Onboarding

**Component Map**: KV Cache Matrix -> MPO Decomposition (TL + TS) -> TL Quantization (2/4/8-bit) -> TS (FP16) -> Fused Dequantization Kernel -> Inference

**Critical Path**: The most performance-critical path is the fused dequantization kernel that combines dequantization with the GeMM operation, as this directly impacts inference latency.

**Design Tradeoffs**: The method trades increased computational complexity during dequantization for reduced memory footprint. The decomposition adds a preprocessing step but enables aggressive quantization that would otherwise be impossible due to outliers.

**Failure Signatures**: Poor quantization accuracy manifests as significant performance degradation, particularly in tasks sensitive to attention precision. Excessive inference latency despite compression indicates the fused kernel is not effectively reducing data movement overhead.

**3 First Experiments**:
1. Implement MPO decomposition on a single transformer layer's KV cache and verify the 99.4% vs 0.6% parameter distribution
2. Apply 4-bit quantization to the large tensor while maintaining FP16 for the small tensor and measure accuracy degradation on LAMBADA
3. Profile the fused kernel implementation to verify reduced data movement compared to separate dequantization and GeMM operations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DecoQuant vary with different tensor decomposition lengths (n > 2) beyond the evaluated n = 2, 3, 4?
- Basis in paper: Section 4.3 states "we select n = 2 for our experiments but recommend higher n for higher accuracy" and notes that gains diminish as n increases.
- Why unresolved: The paper only experimentally evaluates up to n = 4, leaving the optimal decomposition length and its performance impact unexplored.
- What evidence would resolve it: Experimental results comparing DecoQuant's accuracy and compression ratio across a wider range of n values (e.g., n = 5, 6, 7) on multiple models and datasets.

### Open Question 2
- Question: What is the impact of DecoQuant on models with different outlier distributions, particularly those with significantly more or fewer outliers than the evaluated LLaMA and OPT models?
- Basis in paper: Section 4.2 mentions "RTN sometimes gives better results (LLaMA-13B), but this performance is not stable, and in other cases, it is not good. We suspect that it is related to the distribution of outliers in the model" and cites Dettmers et al. 2022 on outlier distribution differences in large models.
- Why unresolved: The experiments only evaluate DecoQuant on LLaMA and OPT models, which may not represent the full spectrum of outlier distributions in LLMs.
- What evidence would resolve it: Comparative experiments applying DecoQuant to models with known extreme outlier distributions (e.g., very outlier-heavy or outlier-light models) and measuring performance degradation or improvement.

### Open Question 3
- Question: How does DecoQuant perform when applied to other LLM components beyond KV cache, such as attention weights or feed-forward network weights?
- Basis in paper: Section 3.3 mentions DecoQuant "supports quantization for weights only (WxA16), activations only (W16Ax), as well as both simultaneously (WxAx)" and Table 1 shows it supports all quantization settings, but the experiments focus only on KV cache.
- Why unresolved: The paper demonstrates DecoQuant's effectiveness on KV cache but doesn't explore its potential benefits for compressing other model components that also suffer from outlier-related quantization challenges.
- What evidence would resolve it: Experiments applying DecoQuant to attention weight matrices and feed-forward network weights, measuring compression ratios and accuracy impacts compared to standard quantization methods.

### Open Question 4
- Question: What is the computational overhead of the tensor decomposition step in DecoQuant when applied online during inference, and how does it compare to the communication savings?
- Basis in paper: Section 3.2 mentions "we utilize the DecoQuant technique offline on the KV cache to alleviate the computational overhead induced by decomposition" for prefilling phase, but notes "To alleviate the increased computational workload due to frequent quantization, we perform DecoQuant only when the cache accumulates a certain length (e.g., 1k)" for decoding phase.
- Why unresolved: The paper acknowledges computational overhead concerns but doesn't provide quantitative measurements of decomposition time versus communication savings in different deployment scenarios.
- What evidence would resolve it: Benchmarking results showing the wall-clock time for tensor decomposition versus the latency reduction from reduced communication overhead across various sequence lengths and hardware configurations.

### Open Question 5
- Question: How does DecoQuant's performance scale with extremely long sequences (e.g., 100k+ tokens) where the KV cache size becomes orders of magnitude larger than the model parameters?
- Basis in paper: Section 3.2 discusses KV cache growth linearly with sequence length and Figure 6 shows results up to 8k tokens, but doesn't address extreme sequence lengths.
- Why unresolved: The experiments only evaluate up to 8k tokens, while many applications (e.g., document processing, code generation) may require much longer sequences where KV cache compression becomes even more critical.
- What evidence would resolve it: Experimental results measuring DecoQuant's compression ratio, accuracy retention, and inference speed-up at sequence lengths of 10k, 50k, 100k, and 500k tokens, comparing performance degradation patterns with standard quantization methods.

## Limitations
- Performance may not scale linearly to larger models beyond the evaluated 7B, 13B, and 33B sizes
- Limited evaluation on diverse downstream tasks and long-context scenarios
- Specific tensor decomposition parameters not clearly specified for different architectures
- Data-free approach may not capture activation patterns that calibration data could reveal

## Confidence
- High confidence in the core MPO decomposition methodology for quantization difficulty migration
- Medium confidence in the specific kernel fusion implementation details and optimization strategies
- Low confidence in the generalizability of the 75% compression ratio across different model architectures and tasks

## Next Checks
1. Implement MPO decomposition with varying tensor dimensions (dk, ik, jk) on a held-out model layer to empirically determine the optimal parameter selection strategy and validate the claimed 99.4% vs 0.6% parameter distribution.
2. Profile the dequantization kernel implementation with different bit precisions (2/4/8-bit) to verify that the fused kernel actually achieves the claimed reduction in data movement and improved inference efficiency compared to naive dequantization approaches.
3. Evaluate the compressed KV cache performance on a long-context generation task (e.g., 8K+ tokens) to test whether the quantization errors accumulate over extended sequences and whether the maintained generation quality holds in extended use cases.