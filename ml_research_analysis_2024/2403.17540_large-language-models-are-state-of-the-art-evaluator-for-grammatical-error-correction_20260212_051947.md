---
ver: rpa2
title: Large Language Models Are State-of-the-Art Evaluator for Grammatical Error
  Correction
arxiv_id: '2403.17540'
source_url: https://arxiv.org/abs/2403.17540
tags:
- evaluation
- llms
- score
- metrics
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of large language models (LLMs)
  as evaluators for grammatical error correction (GEC). While LLMs have shown superior
  performance in various NLP tasks, their potential for GEC evaluation has not been
  thoroughly explored.
---

# Large Language Models Are State-of-the-Art Evaluator for Grammatical Error Correction

## Quick Facts
- arXiv ID: 2403.17540
- Source URL: https://arxiv.org/abs/2403.17540
- Authors: Masamune Kobayashi; Masato Mita; Mamoru Komachi
- Reference count: 40
- Key outcome: GPT-4 achieved a state-of-the-art Kendall's rank correlation of 0.662 with human judgments for GEC evaluation, surpassing all existing metrics

## Executive Summary
This study investigates the effectiveness of large language models (LLMs) as evaluators for grammatical error correction (GEC). The authors conduct comprehensive experiments using GPT-4 and smaller-scale LLMs with prompts incorporating different evaluation criteria. Their results demonstrate that GPT-4 significantly outperforms existing GEC metrics, achieving a Kendall's rank correlation of 0.662 with human judgments. The study highlights the importance of LLM scale and emphasizes fluency as a critical evaluation criterion, particularly for high-performing GEC systems.

## Method Summary
The authors conduct meta-evaluation using human judgments from the SEEDA dataset, comparing system-level and sentence-level evaluations across 12 GEC systems and 3 human-authored sentences. They evaluate GPT-4, GPT-3.5, and Llama 2 using different prompts and evaluation criteria, comparing their performance against traditional GEC metrics (EBMs and SBMs). The evaluation measures Kendall's rank correlation, Pearson correlation, Spearman rank correlation, and Accuracy to assess how well LLM evaluations align with human judgments.

## Key Results
- GPT-4 achieved a state-of-the-art Kendall's rank correlation of 0.662 with human judgments
- LLM scale directly correlates with evaluation performance, with GPT-4 significantly outperforming GPT-3.5 and Llama 2
- Fluency emerged as the most critical evaluation criterion, with larger LLMs better capturing nuanced fluency differences in corrected sentences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger LLMs (GPT-4) outperform smaller LLMs in GEC evaluation due to better handling of fluency criteria
- Mechanism: GPT-4's larger scale allows it to better understand and evaluate the nuances of fluency in corrected sentences, which is a critical factor in high-quality GEC
- Core assumption: Fluency is a key differentiator in GEC evaluation, especially for high-performing systems
- Evidence anchors: GPT-4 achieved a state-of-the-art Kendall's rank correlation of 0.662 with human judgments; GPT-4 consistently demonstrated high correlation and stable evaluations compared to traditional metrics
- Break condition: If fluency is not a significant factor in human evaluations, or if smaller LLMs can be fine-tuned to handle fluency as well as GPT-4

### Mechanism 2
- Claim: The scale of LLMs directly correlates with their ability to capture performance differences in high-performing GEC systems
- Mechanism: Larger LLMs like GPT-4 have more parameters and training data, allowing them to better differentiate between subtle quality differences in corrections from high-performing systems
- Core assumption: High-performing GEC systems produce corrections that are very close in quality, requiring a more sophisticated evaluator to distinguish them
- Evidence anchors: The study highlights the importance of the LLM scale and particularly emphasizes the significance of fluency among evaluation criteria; the decrease in correlation as the LLM scale decreases, such as with Llama 2 and GPT-3.5, suggests the importance of the LLM scale
- Break condition: If smaller LLMs can be trained or prompted effectively to match the performance of larger models, or if the performance gap between high-performing systems is not as subtle as assumed

### Mechanism 3
- Claim: Incorporating specific evaluation criteria (especially fluency) into prompts significantly improves LLM performance in GEC evaluation
- Mechanism: By explicitly instructing the LLM to focus on particular aspects of the corrections (e.g., fluency), we guide its attention to the most relevant features for GEC evaluation
- Core assumption: LLMs can effectively use provided criteria to improve their evaluation accuracy
- Evidence anchors: Most prompts focused on criteria significantly improved sentence-level correlations compared to the base prompt
- Break condition: If the improvements from criteria-focused prompts are not consistent across different types of GEC systems or evaluation tasks

## Foundational Learning

- Concept: Understanding of GEC evaluation metrics (EBMs and SBMs)
  - Why needed here: To comprehend the landscape of existing evaluation methods and how LLMs compare to them
  - Quick check question: What are the main differences between Edit-Based Metrics (EBMs) and Sentence-Based Metrics (SBMs) in GEC evaluation?

- Concept: Familiarity with meta-evaluation techniques in NLP
  - Why needed here: To understand how the performance of GEC evaluators is measured and compared
  - Quick check question: What is the difference between system-level and sentence-level meta-evaluation in the context of GEC?

- Concept: Knowledge of LLM capabilities and limitations
  - Why needed here: To interpret the results of LLM-based GEC evaluation and understand its potential and constraints
  - Quick check question: How does the scale of an LLM (e.g., GPT-3.5 vs GPT-4) affect its performance in language understanding tasks?

## Architecture Onboarding

- Component map: GEC systems -> LLM evaluation (GPT-4, GPT-3.5, Llama 2) -> Human evaluation benchmark (SEEDA) -> Meta-evaluation metrics (Kendall's correlation, Accuracy)

- Critical path: 1) Generate corrections using GEC systems 2) Evaluate corrections using LLMs with specific prompts 3) Compare LLM evaluations with human judgments from SEEDA 4) Calculate correlation metrics to assess performance

- Design tradeoffs:
  - LLM scale vs. resource requirements: Larger models perform better but require more computational resources
  - Prompt specificity vs. generality: More specific prompts may improve performance but reduce flexibility
  - Evaluation criteria focus vs. holistic assessment: Focusing on specific criteria may miss other important aspects of GEC quality

- Failure signatures:
  - Low correlation with human judgments
  - Inconsistent evaluations across similar corrections
  - Bias towards certain types of errors or corrections
  - Inability to handle rare or complex grammatical issues

- First 3 experiments:
  1. Compare LLM performance on a small set of corrections using different prompt styles (general vs. criteria-focused)
  2. Test the impact of increasing the number of evaluation criteria in prompts on LLM performance
  3. Evaluate the consistency of LLM judgments by having it re-evaluate the same set of corrections after a time gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs as GEC evaluators change when evaluating at the document level rather than the sentence level?
- Basis in paper: The authors mention that future work should explore document-level evaluation, considering the expansion of GPT's context window, which is not currently focused on by existing metrics
- Why unresolved: The current study focuses on sentence-level evaluation, and the authors suggest that document-level evaluation could provide additional insights into LLM performance as GEC evaluators
- What evidence would resolve it: Conducting a similar meta-evaluation study at the document level and comparing the results with the current sentence-level findings would provide evidence on the effectiveness of LLMs in document-level GEC evaluation

### Open Question 2
- Question: What is the impact of few-shot learning and optimized prompt engineering on the evaluation performance of LLMs in GEC tasks?
- Basis in paper: The authors mention that future work should delve into the impact of few-shot learning and optimize prompt engineering for enhanced evaluation performance
- Why unresolved: The current study uses prompts inspired by previous research but does not extensively explore the impact of few-shot learning or optimized prompt engineering on LLM performance in GEC evaluation
- What evidence would resolve it: Conducting experiments with various few-shot learning techniques and optimized prompts, then comparing the results with the current findings, would provide insights into the potential improvements in LLM performance as GEC evaluators

### Open Question 3
- Question: How do the evaluation capabilities of LLMs in GEC tasks compare to those of human evaluators, especially in terms of capturing fluency and other nuanced aspects of language?
- Basis in paper: The authors note that GPT-4 achieved a high correlation with human judgments, but also suggest that humans prioritize fluency when comparing high-quality corrected sentences
- Why unresolved: While the study demonstrates that GPT-4 performs well in terms of correlation with human judgments, it does not directly compare the evaluation capabilities of LLMs and humans in capturing nuanced aspects of language, such as fluency
- What evidence would resolve it: Conducting a study where both LLMs and human evaluators assess the same set of GEC corrections, focusing on various aspects such as grammaticality, fluency, and meaning preservation, would provide insights into the relative strengths and weaknesses of LLM and human evaluators in GEC tasks

## Limitations

- The study relies heavily on a single dataset (SEEDA) for evaluation, which may not capture the full diversity of grammatical errors across different domains and languages
- The prompts used for LLM evaluation, while detailed in some aspects, lack complete transparency in their exact formulations, potentially affecting reproducibility
- The comparison focuses primarily on high-performing GEC systems, potentially limiting insights into how well LLMs evaluate lower-quality corrections

## Confidence

- **High Confidence**: GPT-4's superior performance over existing metrics and smaller LLMs is well-supported by the data, particularly at the system level where correlation reaches 0.804
- **Medium Confidence**: The claim about fluency being a critical evaluation criterion is supported but could benefit from more direct analysis of how fluency specifically impacts evaluation quality
- **Medium Confidence**: The assertion that meta-evaluation may have reached saturation with current system sets is plausible but requires validation with additional system variations

## Next Checks

1. Test GPT-4's evaluation consistency across multiple datasets beyond SEEDA to verify generalizability across different error types and domains
2. Conduct ablation studies on prompt engineering to isolate the specific contribution of each evaluation criterion to overall performance
3. Evaluate GPT-4's performance on a broader range of GEC systems, including lower-performing systems, to ensure the findings hold across the full spectrum of correction quality