---
ver: rpa2
title: Toward Adaptive Large Language Models Structured Pruning via Hybrid-grained
  Weight Importance Assessment
arxiv_id: '2403.10799'
source_url: https://arxiv.org/abs/2403.10799
tags:
- pruning
- estimation
- importance
- performance
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HyWIA, a hybrid-grained weight importance
  assessment method for structured pruning of large language models (LLMs). The authors
  observe that using a single granularity for weight importance assessment in structured
  pruning leads to performance degradation in downstream tasks.
---

# Toward Adaptive Large Language Models Structured Pruning via Hybrid-grained Weight Importance Assessment

## Quick Facts
- arXiv ID: 2403.10799
- Source URL: https://arxiv.org/abs/2403.10799
- Authors: Jun Liu; Zhenglun Kong; Pu Zhao; Changdi Yang; Hao Tang; Xuan Shen; Geng Yuan; Wei Niu; Wenbin Zhang; Xue Lin; Dong Huang; Yanzhi Wang
- Reference count: 38
- Surpasses LLM-Pruner by 2.82% average accuracy improvement when pruning LLaMA-7B by 50%

## Executive Summary
This paper introduces HyWIA, a hybrid-grained weight importance assessment method for structured pruning of large language models. The authors observe that using a single granularity for weight importance assessment in structured pruning leads to performance degradation in downstream tasks. HyWIA combines fine-grained and coarse-grained evaluations of weight importance using an attention mechanism to adaptively determine the optimal blend of granularity in weight importance assessments.

## Method Summary
HyWIA is a structured pruning method for LLMs that combines fine-grained and coarse-grained weight importance assessments using an attention mechanism to adaptively determine the optimal blend of granularity. The method consists of four stages: discovery (dependency analysis and coupling identification), estimation (hybrid-grained importance assessment with attention-based fusion), recovery (layer-wise reconstruction with linear least squares), and fine-tuning (LoRA-based parameter-efficient recovery).

## Key Results
- HyWIA surpasses LLM-Pruner by 2.82% average accuracy improvement when pruning LLaMA-7B by 50%
- Achieves state-of-the-art performance on LLaMa-7B (1.1% improvement), Vicuna-7B (1.02% improvement), Baichuan-7B (2.0% improvement), and Bloom-7b1 (1.2% improvement)
- Demonstrated effectiveness across seven downstream tasks including BoolQ, PIQA, HellaSwag, WinoGrande, ARC-e, ARC-c, and OBQA

## Why This Works (Mechanism)

### Mechanism 1
The adaptive fusion of coarse-grained and fine-grained importance estimation improves pruning accuracy by balancing global structure preservation with local parameter sensitivity. The model uses an attention mechanism to dynamically adjust the weight given to coarse-grained (group-level) versus fine-grained (parameter-level) importance scores based on the characteristics of each decoder layer. Different decoder layers in LLMs have varying sensitivity to pruning at different granularities - front layers benefit more from fine-grained pruning while later layers benefit more from coarse-grained pruning.

### Mechanism 2
The Taylor series approximation of loss change provides an effective metric for importance assessment at both granularity levels. The method approximates the change in loss when removing weights using first-order Taylor expansion, with higher-order terms approximated via the Fisher information matrix. This approach captures the relationship between weight removal and performance degradation, though it relies on the assumption that the Fisher information matrix approximation remains accurate for very sparse models.

### Mechanism 3
Layer-wise reconstruction through linear least squares enables efficient model recovery after pruning. After pruning, each layer's output is reconstructed by solving a linear least squares problem to minimize the difference between pruned and original model activations. The residual connections in LLM architectures allow for effective layer-wise reconstruction without requiring full model fine-tuning, though the reconstruction problem must be well-conditioned and properly regularized.

## Foundational Learning

- **Taylor series approximation in optimization**: Why needed here - The pruning method relies on approximating loss changes using Taylor series to assess weight importance. Quick check - What is the primary limitation of using only first-order Taylor approximation for importance assessment?
- **Structured vs unstructured pruning tradeoffs**: Why needed here - The paper contrasts these approaches and explains why hybrid-grained methods can outperform either alone. Quick check - What is the key advantage of structured pruning over unstructured pruning in hardware deployment?
- **Attention mechanisms for adaptive weighting**: Why needed here - The fusion of coarse and fine-grained assessments uses attention to determine optimal blending ratios. Quick check - How does an attention mechanism differ from a simple weighted average when combining multiple importance scores?

## Architecture Onboarding

- **Component map**: Discovery stage -> Estimation stage -> Recovery stage -> Fine-tuning stage
- **Critical path**: Discovery → Estimation → Recovery → Fine-tuning
  - The estimation stage is the core innovation and most performance-critical
- **Design tradeoffs**:
  - Granularity balance: Too much fine-grained pruning risks breaking global structure; too much coarse-grained pruning misses important local details
  - Sample count: More samples improve importance estimation accuracy but increase computation time
  - Reconstruction vs fine-tuning: Layer-wise reconstruction is faster but may be less effective than full fine-tuning
- **Failure signatures**:
  - Accuracy degradation after pruning indicates importance assessment failure
  - Slow convergence in recovery stage suggests ill-conditioned reconstruction problem
  - Memory errors during attention computation indicate tensor size issues
- **First 3 experiments**:
  1. Verify the dependency analysis correctly identifies coupled structures by visualizing the dependency graph
  2. Test the importance assessment on a small toy model with known optimal pruning patterns
  3. Validate the layer-wise reconstruction by comparing activations before and after pruning on held-out data

## Open Questions the Paper Calls Out

### Open Question 1
How does the adaptive fusion mechanism in HyWIA perform when applied to non-transformer architectures like recurrent neural networks or convolutional networks? The paper focuses on transformer-based LLMs and demonstrates improvements in accuracy for LLaMA-V1/V2, Vicuna, Baichuan, and Bloom models using the proposed HyWIA method, but does not explore its application to other types of neural network architectures.

### Open Question 2
What is the impact of varying the number of samples used in the Taylor series approximation on the pruning performance and computational efficiency of HyWIA? While the paper mentions that the number of samples can affect the final gradients and, consequently, the structure and performance of the pruned model, it does not provide a comprehensive analysis of how different sample sizes affect the pruning performance and computational efficiency.

### Open Question 3
How does HyWIA perform when applied to LLMs with different scales, such as models with fewer than 1 billion parameters or those with more than 100 billion parameters? The paper demonstrates the effectiveness of HyWIA on models with around 7 billion parameters, but does not explore its performance on LLMs with significantly different scales, leaving the question of its scalability and performance on other scales unanswered.

## Limitations
- The attention mechanism for adaptively blending granularity levels is described conceptually but lacks clear mathematical formulation or implementation details
- The Fisher information matrix approximation method is mentioned but not elaborated upon, which could significantly impact the accuracy of importance assessments
- The layer-wise reconstruction approach using linear least squares is presented without sufficient detail on how regularization parameters are chosen or how numerical stability is ensured for large matrices

## Confidence
- **High confidence**: The core observation that single-granularity approaches have limitations and that hybrid-grained methods can improve performance
- **Medium confidence**: The empirical results showing HyWIA outperforming baseline methods across multiple LLM architectures
- **Low confidence**: The specific implementation details of the attention mechanism, Fisher information matrix approximation, and linear least squares reconstruction

## Next Checks
1. Implement the attention-based fusion mechanism on a small transformer model with synthetic data to verify it can correctly identify when to prioritize coarse vs fine-grained importance assessments
2. Validate the Taylor series approximation approach by comparing its importance scores against ground-truth importance values on a model with known optimal pruning patterns
3. Test the layer-wise reconstruction approach by measuring the condition number of the least squares problem and verifying that regularization effectively prevents numerical instability