---
ver: rpa2
title: Do Discrete Self-Supervised Representations of Speech Capture Tone Distinctions?
arxiv_id: '2410.19935'
source_url: https://arxiv.org/abs/2410.19935
tags:
- speech
- tone
- symbols
- language
- discrete
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether discrete speech representations
  obtained from self-supervised learning (SSL) models can adequately capture tone
  distinctions in tone languages. The authors examine two tone languages - Mandarin
  and Yoruba - using HuBERT, MandarinHuBERT, and XLS-R models.
---

# Do Discrete Self-Supervised Representations of Speech Capture Tone Distinctions?

## Quick Facts
- arXiv ID: 2410.19935
- Source URL: https://arxiv.org/abs/2410.19935
- Reference count: 37
- Primary result: Discrete speech representations from SSL models systematically lose tone information while preserving segmental features

## Executive Summary
This paper investigates whether discrete representations derived from self-supervised learning (SSL) models can adequately capture tone distinctions in tone languages. The authors examine two tone languages - Mandarin and Yoruba - using HuBERT, MandarinHuBERT, and XLS-R models. Their core finding is that while continuous SSL representations perform well at distinguishing vowels, discretization through k-means clustering leads to substantial loss of tone information, even for language-specialized models. The paper demonstrates this through both classification tasks and novel pairwise edit distance analysis.

## Method Summary
The authors analyze SSL representations using three models (HuBERT, MandarinHuBERT, XLS-R) on Mandarin and Yoruba speech data. They extract both continuous latents and discrete tokens through k-means clustering. The evaluation includes classification tasks for tone and vowel distinctions, as well as novel pairwise edit distance analysis comparing token sequences. They test on small datasets and examine how tokenization patterns affect tone preservation.

## Key Results
- F1 scores for tone classification drop dramatically when moving from continuous representations to discrete symbols
- Tone is poorly captured in discrete symbols across all tested SSL models and languages
- Standard k-means discretization methods appear inadequate for downstream tasks requiring tone information
- Vowel classification remains relatively stable in discrete representations while tone information is lost

## Why This Works (Mechanism)
The paper demonstrates that task-agnostic discretization methods like k-means clustering are not optimized to preserve linguistically relevant features like tone. When continuous SSL representations are discretized, the clustering process groups similar vectors together based on overall similarity rather than preserving specific phonetic or phonological distinctions. This leads to loss of fine-grained tonal information while maintaining broader segmental features like vowel quality.

## Foundational Learning
- Self-Supervised Learning in speech: Models learn from unlabeled audio data without explicit phonetic transcription; needed to understand the foundation of the representations being analyzed; quick check: HuBERT and XLS-R learn from raw audio without text labels
- Tone languages: Languages where pitch patterns distinguish word meanings; needed to frame the research question; quick check: Mandarin has four lexical tones, Yoruba has three
- Discretization methods: Converting continuous representations to discrete symbols; needed to understand the core transformation being studied; quick check: k-means clustering maps vectors to discrete codebook entries
- Edit distance metrics: Measuring sequence similarity between token strings; needed to understand the novel analysis method; quick check: Levenshtein distance counts insertions, deletions, and substitutions between sequences

## Architecture Onboarding

Component Map:
Continuous SSL model (HuBERT/XLS-R) -> Continuous latents extraction -> K-means clustering -> Discrete tokens -> Classification/edit distance analysis

Critical Path:
Audio input → SSL model → Continuous latents → K-means clustering → Discrete tokens → Downstream evaluation (classification or edit distance)

Design Tradeoffs:
- Language-specific vs. multilingual models: Specialized models (MandarinHuBERT) expected to perform better but still show tone loss
- Clustering granularity: More clusters preserve more information but reduce true discreteness
- Tokenization strategy: How boundaries are determined affects which features are preserved

Failure Signatures:
- High vowel classification accuracy paired with low tone classification accuracy indicates systematic loss of pitch-related information
- Similar edit distances for tone pairs and non-tone pairs suggests discrete tokens don't capture tonal distinctions
- Consistency of results across different models suggests the issue is with discretization method, not specific implementations

First Experiments:
1. Compare continuous vs discrete representation performance on tone classification task
2. Calculate pairwise edit distances between tone-minimal pairs in both continuous and discrete spaces
3. Test whether fine-tuning SSL models on tone-labeled data improves tone preservation in discrete representations

## Open Questions the Paper Calls Out
None

## Limitations
- Based on only two tone languages (Mandarin and Yoruba), limiting generalizability to diverse tone systems
- Uses standard k-means discretization which may not represent the full space of possible approaches
- Classification experiments use relatively small datasets
- Edit distance analysis relies on specific tokenization patterns that may not capture all aspects of tone representation

## Confidence
- Tone information loss in discrete SSL representations: High
- Consistency across different SSL models: High
- Method-specific nature of the findings: Medium
- Generalizability to all tone languages: Low

## Next Checks
1. Replicate the study with additional tone languages representing different tone system types (register, contour, pitch-accent) to test generalizability
2. Compare k-means discretization with alternative approaches (e.g., vector quantization, task-specific clustering) to isolate whether the information loss is inherent to discretization or method-specific
3. Test whether fine-tuning SSL models on tone-labeled data improves tone preservation in discrete representations, and whether this affects segmental feature quality