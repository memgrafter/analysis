---
ver: rpa2
title: Can Large Language Models Reason? A Characterization via 3-SAT
arxiv_id: '2408.07215'
source_url: https://arxiv.org/abs/2408.07215
tags:
- llms
- reasoning
- 'true'
- https
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) can
  perform true logical reasoning by using 3-SAT, a canonical NP-complete problem,
  as a rigorous testbed. The authors reframe 3-SAT into natural language forms (SAT-Menu
  and SAT-CNF) and evaluate GPT-4 and other state-of-the-art LLMs across a spectrum
  of problem difficulty defined by phase transitions in random 3-SAT.
---

# Can Large Language Models Reason? A Characterization via 3-SAT

## Quick Facts
- **arXiv ID**: 2408.07215
- **Source URL**: https://arxiv.org/abs/2408.07215
- **Reference count**: 40
- **Primary result**: LLMs show solver-like phase transition patterns but fail at hard 3-SAT instances, revealing reliance on statistical shortcuts rather than true logical reasoning.

## Executive Summary
This study investigates whether large language models (LLMs) can perform true logical reasoning by using 3-SAT, a canonical NP-complete problem, as a rigorous testbed. The authors reframe 3-SAT into natural language forms (SAT-Menu and SAT-CNF) and evaluate GPT-4 and other state-of-the-art LLMs across a spectrum of problem difficulty defined by phase transitions in random 3-SAT. Results show that LLMs perform well on easier instances but accuracy drops sharply in the "hard" region around the phase transition point αc ≈ 4.267, indicating a reliance on statistical shortcuts rather than genuine reasoning. Notably, only GPT-4 exhibits solver-like phase transition patterns. Augmenting LLMs with external SAT solvers (SAT-Translate) dramatically improves performance, confirming LLMs' utility as translators rather than standalone reasoners. Overall, the study concludes that current LLMs do not exhibit true reasoning in the formal sense but can support reasoning tasks when integrated with symbolic tools.

## Method Summary
The authors evaluate LLMs' reasoning capabilities using 3-SAT, an NP-complete problem where determining satisfiability is computationally hard. They generate 60,000 synthetic 3-SAT formulas by varying the clause-to-variable ratio α across [1,11], with 6,000 samples used for experiments. Formulas are labeled satisfiable/unsatisfiable using MiniSAT v2.2 and annotated with model counts using D4. The 3-SAT problems are reformulated into natural language menu selection tasks (SAT-Menu) where variables map to food items and clauses to customer preferences, as well as direct CNF formula inputs (SAT-CNF). Zero-shot and few-shot (3-shot) prompting with chain-of-thought is used to test GPT-4, GPT-3.5, Gemini, PaLM2, Llama2, and Mixtral across different α regions (easy/hard). Performance is measured against MiniSAT solutions, and a SAT-Translate variant tests LLM-generated CNF for external solver evaluation.

## Key Results
- LLMs perform well on easy 3-SAT instances but accuracy drops sharply around the phase transition point αc ≈ 4.267, revealing reliance on statistical shortcuts
- Only GPT-4 exhibits solver-like phase transition patterns, while other LLMs show different accuracy curves
- SAT-Translate (LLM + external solver) dramatically outperforms standalone LLMs, confirming their utility as translators rather than reasoners

## Why This Works (Mechanism)
The study leverages the well-understood phase transition behavior of random 3-SAT problems, where satisfiability probability drops sharply around αc ≈ 4.267. This creates a natural difficulty gradient that distinguishes between statistical pattern matching and genuine logical reasoning. By reformulating formal logic into natural language (menu selection), the authors create a realistic test of whether LLMs can translate between domains while preserving logical structure. The SAT-Translate variant demonstrates that LLMs excel at translation between natural language and formal syntax, even when they cannot solve problems independently.

## Foundational Learning
- **3-SAT problem structure**: Understanding that 3-SAT requires finding variable assignments satisfying all clauses, or proving none exist. Why needed: Forms the basis for testing logical reasoning capabilities. Quick check: Can identify variables, clauses, and understand CNF representation.
- **Phase transition in random SAT**: The sharp drop in satisfiability probability around αc ≈ 4.267 creates a natural difficulty gradient. Why needed: Provides a principled way to distinguish easy statistical patterns from hard logical reasoning. Quick check: Can explain why problems near αc are computationally hardest.
- **Natural language reformulation**: Mapping formal logic to menu selection preserves logical structure while testing translation abilities. Why needed: Tests whether LLMs reason about underlying logic or just pattern match. Quick check: Can verify that menu reformulation preserves logical constraints of original 3-SAT.
- **Solver integration patterns**: Using LLMs as translators to external solvers demonstrates complementary strengths. Why needed: Shows practical path to combining statistical and symbolic AI. Quick check: Can validate that translated CNF is syntactically correct for solver consumption.

## Architecture Onboarding

**Component Map**: 3-SAT Generator -> Menu Reformulation -> LLM Prompting -> Solution Validation -> Phase Analysis

**Critical Path**: Dataset generation (3-SAT formulas with MiniSAT labeling) → Natural language reformulation (SAT-Menu) → LLM evaluation (zero/few-shot prompting) → Accuracy measurement against ground truth → Phase transition analysis

**Design Tradeoffs**: The study trades breadth (testing many LLMs) for depth (detailed phase transition analysis). Using synthetic random 3-SAT provides controlled difficulty gradients but may not reflect real-world reasoning complexity. Natural language reformulation makes problems accessible to LLMs but introduces potential confounds from linguistic rather than logical processing.

**Failure Signatures**: Performance correlating with satisfiability ratio rather than genuine reasoning indicates statistical shortcut use. Solver-incompatible CNF generation in SAT-Translate reveals translation limitations. Phase transition patterns differing from MiniSAT suggest LLM reasoning diverges from established computational complexity principles.

**First Experiments**: 1) Generate 100 random 3-SAT instances across α ∈ [1,11] and verify MiniSAT labeling accuracy. 2) Create SAT-Menu reformulation for 10 instances and validate logical equivalence. 3) Test GPT-4 on 5 easy (α < 2) and 5 hard (α > 4) instances to verify expected performance gradient.

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on single NP-complete problem (3-SAT) may not generalize to other reasoning domains
- Natural language reformulation introduces potential confounds from linguistic processing
- Phase transition analysis relies on synthetic random instances that may not reflect real-world reasoning tasks
- Comparison assumes MiniSAT provides ground truth, but solver performance varies with implementation

## Confidence

**High Confidence**: Claims about LLMs showing solver-like phase transition patterns and performance degradation at critical α values around 4.267.

**Medium Confidence**: Broader claims about LLMs lacking "true reasoning" capabilities, though 3-SAT results are compelling.

**Low Confidence**: Practical implications of SAT-Translate results, as real-world utility depends on unexplored factors like latency costs.

## Next Checks
1. **Cross-domain generalization test**: Evaluate same LLMs on other NP-complete problems (e.g., graph coloring, Hamiltonian path) reformulated as natural language tasks to verify reasoning limitations extend beyond 3-SAT.

2. **Adversarial prompt analysis**: Systematically test whether small perturbations to menu reformulation (changing food item mappings or clause phrasings) significantly impact LLM performance, indicating reliance on statistical shortcuts.

3. **Human benchmark comparison**: Recruit participants to solve same SAT-Menu problems and compare human accuracy patterns with LLM performance across α spectrum to contextualize claimed reasoning limitations.