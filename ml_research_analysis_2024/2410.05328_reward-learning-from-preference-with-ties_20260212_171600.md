---
ver: rpa2
title: Reward Learning From Preference With Ties
arxiv_id: '2410.05328'
source_url: https://arxiv.org/abs/2410.05328
tags:
- preference
- ties
- reward
- human
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of incorporating ties into human
  preference modeling for RLHF in language models. The authors argue that the conventional
  Bradley-Terry model fails to capture human preferences accurately when responses
  are nearly equivalent, as humans often perceive two responses as ties when their
  rewards have minimal difference.
---

# Reward Learning From Preference With Ties

## Quick Facts
- arXiv ID: 2410.05328
- Source URL: https://arxiv.org/abs/2410.05328
- Authors: Jinsong Liu; Dongdong Ge; Ruihao Zhu
- Reference count: 30
- Primary result: The Bradley-Terry model with ties (BTT) reduces preference bias compared to standard BT model when human preferences include ties.

## Executive Summary
This paper addresses a critical gap in preference modeling for RLHF in language models: the failure of conventional Bradley-Terry models to capture human preferences accurately when responses are nearly equivalent. The authors propose extending the BT model to explicitly handle ties, demonstrating both theoretically and empirically that ignoring ties leads to significant bias in preference strength measurement. Through a combination of synthetic experiments and real-world applications, they show that incorporating ties through the BTT model and applying a bias-correction algorithm significantly improves reward model accuracy.

## Method Summary
The authors extend the Bradley-Terry model to handle ties (BTT) by introducing an additional parameter θ that controls the tendency to ties. They prove that ignoring ties in preference modeling introduces bias in measuring preference strength, then propose a bias-correction algorithm that solves for the true preference strength by treating the bias equation as a nonlinear equation. The method is evaluated through synthetic preference datasets with known ground truth and real-world applications where LLMs label ties in conventional preference datasets.

## Key Results
- The BTT model consistently reduces preference bias compared to standard BT model in simulation experiments
- The bias-correction method significantly improves reward model accuracy when applied to real-world data
- When using LLMs to label ties in conventional preference datasets, the BTT approach outperforms standard BT model with win rates exceeding 50% when only tied samples are present

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The BTT model captures human preferences more accurately than the standard BT model when responses are nearly equivalent.
- Mechanism: BTT extends the BT model by explicitly modeling the probability of ties, where two responses are considered equal in preference strength. This is done through an additional parameter θ that controls the tendency to ties.
- Core assumption: Human labelers can meaningfully distinguish between clear preferences and ties when given the option.
- Evidence anchors:
  - [abstract] "human attitudes towards two responses may not solely indicate a preference for one over the other and ties are also a common occurrence"
  - [section 2] "the Bradley-Terry (BT) model stands as the prevalent choice for capturing human preferences from datasets containing pairs of chosen and rejected responses"
- Break condition: If human labelers are inconsistent in identifying ties or if ties are rare in the data, the BTT model may not provide significant advantages over BT.

### Mechanism 2
- Claim: Ignoring ties in preference modeling leads to bias in measuring preference strength.
- Mechanism: When the true preference model is BTT but the dataset is processed as BT (without ties), the learned reward model underestimates preference strength for pairs with minimal differences. This is because tied samples are forced into binary preferences, distorting the true preference distribution.
- Core assumption: The true latent preference model follows the BTT distribution rather than the BT distribution.
- Evidence anchors:
  - [abstract] "we prove that even with the access to the true distributions of prompt and response, disregarding ties can lead to a notable bias in preference strength measurement"
  - [section 4.2] "there can be a bias in measuring preference strength: ∆ˆr = ∆r∗ + log(2θ+(1+θ2)exp(−∆r∗)/(1+θ2+2θexp(−∆r∗))"
- Break condition: If the true preference model is actually BT (θ = 1), then ignoring ties would not introduce bias.

### Mechanism 3
- Claim: The bias-correction algorithm can recover the true preference strength by solving for ∆r* given the observed ∆r̂.
- Mechanism: The algorithm treats the bias equation as a nonlinear equation and solves for the true preference strength ∆r*, then subtracts the bias term from the current estimate. This is equivalent to adding an adaptive margin during training.
- Core assumption: The bias equation has a unique solution for ∆r* given ∆r̂ and θ.
- Evidence anchors:
  - [section 4.3] "we can treat (8) as a nonlinear equation and solve for the value of∆r∗, subsequently subtracting the bias term from the current∆ˆr"
  - [section 5.2] "This method can be viewed as a variant of DPO with an offset (ODPO) (Amini et al., 2024) when fine tuning with DPO"
- Break condition: If the bias equation is ill-conditioned or if θ is incorrectly estimated, the bias correction may not work effectively.

## Foundational Learning

- Concept: Bradley-Terry (BT) model for pairwise comparisons
  - Why needed here: The paper builds on the BT model as the foundation for preference modeling, then extends it to handle ties
  - Quick check question: What is the formula for the BT model probability that y1 is preferred over y2 given a prompt x?

- Concept: Maximum likelihood estimation (MLE) for parameter learning
  - Why needed here: The paper uses MLE to learn the reward model parameters from preference data, both for BT and BTT models
  - Quick check question: How does the negative log-likelihood loss function differ between BT and BTT models?

- Concept: Reinforcement learning from human feedback (RLHF) pipeline
  - Why needed here: The paper's work on preference modeling directly impacts the reward learning phase of RLHF
  - Quick check question: What are the three main phases of the RLHF process described in the paper?

## Architecture Onboarding

- Component map:
  - Data collection: Prompts → LLM responses → Human/AI labeling (with/without ties)
  - Preference modeling: BT model vs BTT model
  - Reward learning: MLE optimization with bias correction
  - Evaluation: Win rate comparison between models

- Critical path:
  1. Generate or collect preference data (with or without ties)
  2. Choose preference model (BT or BTT)
  3. Train reward model using appropriate loss function
  4. Evaluate model performance on synthetic/real preference data

- Design tradeoffs:
  - BT vs BTT: Simpler BT model vs more expressive BTT model that requires additional parameter θ
  - With vs without bias correction: Simpler training vs potentially more accurate preference strength measurement
  - Human vs AI labeling: More accurate but expensive human labeling vs cheaper but potentially noisier AI labeling

- Failure signatures:
  - Model performs well on training data but poorly on test data: Possible overfitting or data distribution mismatch
  - Bias correction doesn't improve performance: Possible incorrect θ estimation or model mismatch
  - Win rate around 50%: Models are performing similarly, no significant improvement from BTT or bias correction

- First 3 experiments:
  1. Train BT and BTT models on a synthetic dataset with known ground truth, compare preference strength bias
  2. Apply bias correction algorithm to a conventional preference dataset, evaluate reward preference accuracy
  3. Use LLMs to label ties in a real preference dataset, train BT and BTT models, compare win rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed bias-correction method perform when the true preference model is unknown?
- Basis in paper: Inferred
- Why unresolved: The paper assumes access to the true distributions of prompt and response, which may not be realistic in practice. It would be valuable to evaluate the method's performance when the true preference model is unknown or when the model mismatch is more severe.
- What evidence would resolve it: Experiments comparing the bias-correction method's performance under various levels of model mismatch and when the true preference model is unknown.

### Open Question 2
- Question: What is the impact of the tie parameter θ on the effectiveness of the bias-correction method?
- Basis in paper: Inferred
- Why unresolved: The paper only conducts experiments with θ = 5, but the impact of different θ values on the bias-correction method's performance is not explored. It would be interesting to investigate how the method's effectiveness varies with different tie parameters.
- What evidence would resolve it: Experiments evaluating the bias-correction method's performance with various θ values, including both low and high tie probabilities.

### Open Question 3
- Question: How does the proposed method compare to other existing approaches for handling ties in preference modeling?
- Basis in paper: Inferred
- Why unresolved: The paper focuses on the Bradley-Terry model with ties (BTT) and does not compare its performance to other methods that handle ties, such as the Plackett-Luce model or other extensions of the Bradley-Terry model. A comparison with these approaches would provide insights into the relative effectiveness of the proposed method.
- What evidence would resolve it: Experiments comparing the proposed method's performance to other approaches for handling ties in preference modeling, using the same datasets and evaluation metrics.

### Open Question 4
- Question: How does the proposed method perform when the preference dataset is imbalanced, with a large number of tied samples?
- Basis in paper: Inferred
- Why unresolved: The paper uses a synthetic preference dataset with ties, but the impact of imbalanced datasets on the method's performance is not explored. It would be valuable to investigate how the method handles scenarios where the number of tied samples significantly outweighs the number of untied samples.
- What evidence would resolve it: Experiments evaluating the proposed method's performance on imbalanced preference datasets with varying ratios of tied to untied samples.

## Limitations

- Data dependency on LLM tie labeling introduces uncertainty about the quality and consistency of tie detection
- Complexity of bias correction implementation may face practical challenges in numerical stability and convergence
- Limited exploration of how the method performs with different reward model architectures beyond the specific implementation used

## Confidence

**High Confidence**: Theoretical derivation of BTT model and mathematical proof of bias introduction when ignoring ties
**Medium Confidence**: Effectiveness of bias-correction algorithm in practice across different datasets and model architectures
**Medium Confidence**: Reliability of LLMs for labeling ties in preference datasets

## Next Checks

1. Implement and validate the bias-correction algorithm on synthetic data with known ground truth to verify recovery of true preference strengths across different θ values

2. Cross-validate LLM tie labeling quality by comparing human vs. LLM tie labeling agreement on a subset of preference pairs from the same dataset used to train the LLM labeler

3. Test robustness across different reward model architectures by implementing BTT model and bias correction with different transformer architectures or embedding methods to assess generalization beyond the specific implementation used in the paper