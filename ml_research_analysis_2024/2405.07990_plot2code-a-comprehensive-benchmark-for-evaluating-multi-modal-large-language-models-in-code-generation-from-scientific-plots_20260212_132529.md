---
ver: rpa2
title: 'Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language
  Models in Code Generation from Scientific Plots'
arxiv_id: '2405.07990'
source_url: https://arxiv.org/abs/2405.07990
tags:
- code
- image
- evaluation
- arxiv
- plot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Plot2Code, a comprehensive benchmark designed
  to evaluate multi-modal large language models (MLLMs) on their ability to generate
  code from scientific plots. The benchmark consists of 132 manually selected, high-quality
  matplotlib plots across six plot types, each paired with its source code and a descriptive
  instruction.
---

# Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots

## Quick Facts
- arXiv ID: 2405.07990
- Source URL: https://arxiv.org/abs/2405.07990
- Reference count: 40
- Primary result: Advanced MLLMs achieve only 7.68/10 on visual coding tasks, highlighting significant challenges

## Executive Summary
This paper introduces Plot2Code, a benchmark designed to evaluate multi-modal large language models' (MLLMs) ability to generate code from scientific plots. The benchmark consists of 132 manually curated matplotlib plots across six types, each paired with source code and instructions. The authors propose three evaluation metrics: code pass rate, text-match ratio, and GPT-4V overall rating. Evaluation results show that even state-of-the-art models like GPT-4V, Gemini-Pro, and Claude-3 struggle with these tasks, achieving modest performance scores that underscore the significant challenges in visual coding tasks.

## Method Summary
The study evaluates MLLMs on their ability to generate code from scientific plots. The Plot2Code dataset contains 132 manually selected matplotlib plots across six types, each with corresponding source code and instructions. The evaluation pipeline involves generating code from plot images and instructions, executing the code to create new plots, and comparing these with reference plots using GPT-4V for visual similarity assessment. Three metrics are used: code pass rate (whether generated code runs without errors), text-match ratio (similarity of text elements), and GPT-4V overall rating (1-10 scale for visual similarity). Models are evaluated in both Direct Asking (image only) and Conditional Asking (image plus instruction) settings.

## Key Results
- GPT-4V achieves only 7.68/10 overall score on the benchmark, significantly lower than its performance on other MLLM tasks
- MLLMs struggle particularly with text-dense plots, showing poor performance on annotations and legends
- There is a strong correlation between model performance and the backbone LLM used, with higher resolution models consistently performing better

## Why This Works (Mechanism)

### Mechanism 1
GPT-4V-based overall rating provides a more accurate measure of visual similarity than traditional low-level metrics like MSE or SSIM.