---
ver: rpa2
title: A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large
  Language Models
arxiv_id: '2405.18208'
source_url: https://arxiv.org/abs/2405.18208
tags:
- plan
- information
- planning
- arxiv
- travel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a human-like reasoning framework for LLM agents
  to solve multi-phase planning tasks. The key idea is to mimic human planning processes
  through three phases: Outline Generation to create a coherent trip outline, Information
  Collection to gather necessary travel data, and Plan Making to generate detailed
  daily plans.'
---

# A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models

## Quick Facts
- arXiv ID: 2405.18208
- Source URL: https://arxiv.org/abs/2405.18208
- Reference count: 40
- Key outcome: GPT-4-Turbo achieves 10x increase in Final Pass Rate (0.8% to 8.1%) on TravelPlanner benchmark using human-like three-phase reasoning framework

## Executive Summary
This paper proposes a human-like reasoning framework for LLM agents to solve multi-phase planning tasks, specifically travel planning. The framework mimics human planning processes through three phases: Outline Generation to create a coherent trip outline, Information Collection to gather necessary travel data, and Plan Making to generate detailed daily plans. The framework integrates Strategy Block for guiding information collection and Knowledge Block for storing and retrieving travel information. Extensive experiments on the TravelPlanner benchmark show significant improvements over baseline methods, with GPT-4-Turbo achieving a 10x increase in Final Pass Rate when using this framework.

## Method Summary
The proposed framework decomposes travel planning into three human-like phases: Outline Generation (creating a coherent trip structure with routes and key points), Information Collection (gathering specific travel data through tool calls), and Plan Making (generating daily plans with evaluation). The system uses multiple specialized agents (PathFinder, Thought, Tool, Plan, Evaluate) working with two key memory components: Strategy Block (maintains planning state and guides information collection) and Knowledge Block (stores collected travel information). The framework employs Plan Search to generate multiple daily plans and uses an Evaluate Agent to select the best option. The approach is evaluated on the TravelPlanner benchmark with 180 validation queries across multiple LLM models.

## Key Results
- GPT-4-Turbo achieves Final Pass Rate of 8.1% (vs 0.8% for ReAct), a 10x improvement
- Significant improvements across all metrics: Delivery Rate (87.2% vs 40.0%), Commonsense Pass Rate (50.0% vs 6.7%), Hard Constraint Pass Rate (46.1% vs 13.3%)
- Framework components show ablation effects, with Strategy Block and Knowledge Block both contributing to performance
- Performance degrades with simpler LLM models (GPT-3.5-turbo: 2.2% Final Pass Rate)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Outline Generation phase creates a coherent travel structure that guides all subsequent planning steps.
- Mechanism: By generating a route with transportation options and identifying key points, the framework establishes a clear roadmap that prevents agents from making contradictory or impossible plans.
- Core assumption: A structured outline constrains the planning space in a way that reduces errors and improves coherence.
- Evidence anchors:
  - [abstract] "guiding the LLM agent to simulate various steps that humans take when solving Multi-Phases problems"
  - [section 3.3] "The preliminary route serves as a guide for the subsequent Information Collection and Plan Making phases"
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If the initial outline is too vague or contains errors, subsequent phases will compound these mistakes rather than correct them.

### Mechanism 2
- Claim: The Strategy Block provides context-specific guidance that improves information collection quality.
- Mechanism: By storing the outline, current day information, and previous knowledge, the Strategy Block helps the Thought Agent determine what information to collect next, preventing redundant or missing data.
- Core assumption: Maintaining explicit memory of what has been collected prevents both over-collection and under-collection of information.
- Evidence anchors:
  - [section 3.4] "the Strategy Block also informs the Thought Agent which day it is in the travel plan and short descriptions of the collected data"
  - [abstract] "Strategy Block facilitates information collection"
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If the Strategy Block becomes too large or complex, it may overwhelm the agent or cause context length issues.

### Mechanism 3
- Claim: The Plan Search method with evaluation improves plan quality by exploring multiple options and selecting the best one.
- Mechanism: By generating multiple plans per day and using an Evaluate Agent to rank them based on errors, the framework ensures higher quality plans are selected while errors are identified and addressed.
- Core assumption: Multiple plan generation with systematic evaluation will produce better plans than single-plan generation.
- Evidence anchors:
  - [section 3.5] "An Evaluate Agent reviews each plan, converting them into JSON format and using code to identify and rank errors"
  - [abstract] "Multiple plans are generated in each iteration, and an evaluation agent identifies the best plan"
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If error detection is too strict or too lenient, the system may either discard good plans or accept poor ones.

## Foundational Learning

- Concept: Multi-phase planning decomposition
  - Why needed here: Travel planning inherently requires breaking down a complex task into interconnected phases (outlining, information gathering, planning)
  - Quick check question: Why can't we just ask an LLM to generate a complete travel plan in one step?

- Concept: Knowledge representation and retrieval
  - Why needed here: The framework needs to store and access collected information efficiently across multiple days and planning steps
  - Quick check question: How does the Knowledge Block decide what information to keep versus discard?

- Concept: Error detection and correction in generated content
  - Why needed here: LLM-generated plans often contain hallucinations or missing information that must be identified and fixed
  - Quick check question: What types of errors does the Evaluate Agent look for in generated travel plans?

## Architecture Onboarding

- Component map: PathFinder Agent → Keypoints Agent + Commonsense Agent → Thought Agent + Tool Agent → Plan Agent + Evaluate Agent
- Critical path: Query → Outline Generation → Information Collection → Plan Making → Final Plan
  - Each phase must complete successfully before the next begins
- Design tradeoffs:
  - Multiple agents add complexity but improve specialization
  - Plan search increases computation but improves quality
  - Context length management vs. information completeness
- Failure signatures:
  - Delivery Rate failure: repeated function calls or exceeding step limits
  - Commonsense Pass Rate failure: hallucinations or missing necessary information
  - Hard Constraint failure: not meeting specific requirements like budget or accommodation rules
- First 3 experiments:
  1. Test Outline Generation with simple queries to verify route creation
  2. Test Information Collection with one day's plan to verify data gathering
  3. Test Plan Making with collected data to verify daily plan generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the framework handle cases where no valid route exists between cities given the transportation constraints?
- Basis in paper: [inferred] The paper mentions route evaluation and feedback to the PathFinder Agent when routes are invalid, but doesn't specify the mechanism for cases where no valid route exists.
- Why unresolved: The paper only discusses the case where routes are invalid for certain transportation modes, but not when no route is possible at all.
- What evidence would resolve it: A detailed description of the framework's behavior when no valid route can be found, including whether it terminates the planning process or attempts alternative solutions.

### Open Question 2
- Question: What is the impact of increasing the number of generated plans per day on the framework's performance?
- Basis in paper: [explicit] The paper states that the Plan Agent generates 3 different plans each time, but doesn't explore the impact of varying this number.
- Why unresolved: The paper uses a fixed number of plans (3) without investigating how this parameter affects performance or efficiency.
- What evidence would resolve it: Experimental results comparing the framework's performance with different numbers of generated plans per day.

### Open Question 3
- Question: How does the Knowledge Block's pop-out mechanism affect the quality of plans for longer trips?
- Basis in paper: [explicit] The paper describes a stack-like structure for the Knowledge Block that only keeps information from the past two days, but doesn't evaluate its impact on longer trips.
- Why unresolved: The paper doesn't provide evidence on how this limitation affects plan quality for trips spanning more than two days.
- What evidence would resolve it: Comparative analysis of plan quality for trips of varying lengths, with and without the Knowledge Block's pop-out mechanism.

## Limitations

- The framework shows substantial improvements but absolute performance remains low (8.1% Final Pass Rate for best model)
- Evaluation relies heavily on automatic metrics that may not capture true plan quality from human perspective
- Framework components are described at high level without specific implementation details for context management
- Computational costs and latency implications of multi-agent approach are not addressed

## Confidence

**High Confidence:** The three-phase human-like reasoning framework improves performance over baseline methods is well-supported by experimental results across multiple metrics.

**Medium Confidence:** The claim that the framework "significantly improves the performance" is supported but should be tempered by the low absolute performance numbers.

**Low Confidence:** The generalizability of the framework to other multi-phase planning domains beyond travel planning is not tested.

## Next Checks

1. **Human Evaluation Validation**: Conduct a human evaluation study comparing travel plans generated by the framework against those created by human travel agents, using both subjective quality ratings and objective correctness measures to validate the automatic evaluation metrics.

2. **Cross-Domain Transferability Test**: Apply the framework to a different multi-phase planning domain (such as event planning or project management) to assess whether the same three-phase approach and component architecture generalize beyond travel planning.

3. **Scalability and Context Management Analysis**: Systematically test the framework with longer travel durations (beyond 5 days) and more complex queries to identify context length limitations and evaluate how well the Strategy Block and Knowledge Block scale with increased planning complexity.