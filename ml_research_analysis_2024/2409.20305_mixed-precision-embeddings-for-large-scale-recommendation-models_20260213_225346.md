---
ver: rpa2
title: Mixed-Precision Embeddings for Large-Scale Recommendation Models
arxiv_id: '2409.20305'
source_url: https://arxiv.org/abs/2409.20305
tags:
- embedding
- feature
- precision
- features
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of compressing large-scale embedding
  tables in deep learning recommendation models (DLRMs), which are critical for handling
  millions of categorical features but create significant storage overhead. The authors
  propose Mixed-Precision Embeddings (MPE), a novel method that dynamically assigns
  appropriate bit-width precision to each feature embedding based on its importance.
---

# Mixed-Precision Embeddings for Large-Scale Recommendation Models

## Quick Facts
- arXiv ID: 2409.20305
- Source URL: https://arxiv.org/abs/2409.20305
- Reference count: 40
- Primary result: Achieves 200x compression on Criteo dataset with 0.16 average bit-width while maintaining AUC of 0.8104

## Executive Summary
This paper addresses the challenge of compressing large-scale embedding tables in deep learning recommendation models (DLRMs), which are critical for handling millions of categorical features but create significant storage overhead. The authors propose Mixed-Precision Embeddings (MPE), a novel method that dynamically assigns appropriate bit-width precision to each feature embedding based on its importance. MPE groups features by frequency to reduce the search space complexity and learns a probability distribution over candidate bit-widths for each group using a specially designed sampling strategy. Extensive experiments on three public datasets (Avazu, Criteo, and KDD12) demonstrate that MPE significantly outperforms existing embedding compression methods, achieving about 200x compression on the Criteo dataset without compromising prediction accuracy.

## Method Summary
MPE addresses embedding compression by first grouping features based on their frequency, which serves as a proxy for importance, thereby reducing the search space for precision assignment. For each feature group, the method learns a probability distribution over candidate bit-widths using a softmax function and regularization. During training, the final embedding is computed as the expected quantized embedding across all candidate bit-widths, weighted by the learned probabilities. After training, the highest-probability bit-width above a threshold is sampled for each group, and the model is retrained from scratch using these sampled bit-widths. This approach enables end-to-end optimization with gradient descent while maintaining accuracy through the retraining phase.

## Key Results
- Achieves approximately 200x compression on Criteo dataset
- Maintains AUC of 0.8104 with average bit-width of 0.16
- Outperforms existing embedding compression methods on Avazu, Criteo, and KDD12 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MPE improves embedding compression efficiency by dynamically assigning precision levels to feature embeddings based on importance.
- Mechanism: Features are grouped by frequency to reduce search space, and a learnable probability distribution over candidate bit-widths is optimized for each group using softmax and regularization. The expected quantized embeddings are computed as weighted sums across bit-widths, and precision is sampled after training for retraining.
- Core assumption: Feature frequency is a reliable proxy for importance, and maintaining a single step size and offset per bit-width/dimension is sufficient for quantization.
- Evidence anchors:
  - [abstract] "Specifically, to reduce the size of the search space, we first group features by frequency and then search precision for each feature group."
  - [section 3.2] "Feature frequency, a common measure of feature importance used in numerous studies, is employed here as prior information to guide the grouping process."
  - [corpus] Weak evidence; related work focuses on quantization in general but not MPE's specific frequency-based grouping approach.
- Break condition: If feature importance does not correlate well with frequency, grouping by frequency will misalign precision assignments, degrading accuracy.

### Mechanism 2
- Claim: Using expected quantized embeddings across candidate bit-widths enables end-to-end optimization with gradient descent.
- Mechanism: For each feature group, a learnable probability distribution over bit-widths is maintained. The final embedding is computed as the expected value of quantizing the full-precision embedding at all candidate bit-widths, weighted by the probabilities. This allows gradients to flow through the non-differentiable quantization operator via straight-through estimator (STE).
- Core assumption: Approximating the rounding function gradient as 1 (via STE) is sufficient for stable training.
- Evidence anchors:
  - [section 3.3] "we convert the one-hot selection of candidate bit-widths into the learning of a probability distribution" and "we use the expected outcomes of quantizing the feature embedding at different bit-widths as the final quantized embedding."
  - [section 2.2] "we employ an advanced QAT algorithm, LSQ+, as the base quantizer, which similarly approximates the gradient of the rounding function as 1."
  - [corpus] Weak evidence; related papers discuss quantization-aware training but not the specific expected embedding formulation used in MPE.
- Break condition: If the STE approximation is too crude, gradients may be inaccurate, preventing convergence to an optimal precision distribution.

### Mechanism 3
- Claim: Sampling the highest-probability bit-width above a threshold and retraining yields higher accuracy than direct quantization of the expected embedding.
- Mechanism: After optimizing the probability distribution, the final bit-width for each group is selected as the highest bit-width with probability > 1/(2m), where m is the number of candidates. The model is then retrained from scratch using these sampled bit-widths to mitigate accuracy loss from quantization discrepancies.
- Core assumption: Retraining with the sampled bit-widths can recover accuracy lost during the probability distribution learning phase.
- Evidence anchors:
  - [section 3.4] "we select the highest bit-width with a probability exceeding the threshold of 1/(2m)" and "we implement a retraining process using the sampled bit-widths to train mixed-precision embedding from scratch."
  - [section 5.3] "quantizing embeddings with the sampled bit-widths without retraining leads to a significant loss of accuracy" and "retraining with LTH can partially mitigate this loss."
  - [corpus] Weak evidence; related papers discuss retraining in quantization but not MPE's specific sampling and retraining strategy.
- Break condition: If the sampled bit-widths are poor (e.g., due to over-regularization), retraining will not recover accuracy.

## Foundational Learning

- Concept: Quantization-aware training (QAT) and straight-through estimator (STE)
  - Why needed here: MPE builds on QAT to compress embeddings while maintaining accuracy; STE is used to handle the non-differentiable quantization operator.
  - Quick check question: What is the purpose of STE in QAT, and how does it approximate the gradient of the rounding function?

- Concept: Feature frequency as a proxy for importance
  - Why needed here: MPE groups features by frequency to reduce the search space for precision; this assumes frequency correlates with importance.
  - Quick check question: Why does MPE use feature frequency for grouping, and what assumption does this rely on?

- Concept: Expected value computation for mixed-precision embeddings
  - Why needed here: MPE computes the final embedding as the expected quantized embedding across all candidate bit-widths, weighted by the learned probability distribution.
  - Quick check question: How does MPE compute the final embedding during training, and why is this approach used?

## Architecture Onboarding

- Component map: Input -> Sparse feature indices -> Mixed-precision embedding table -> Dequantization -> Feature interaction network -> Output (CTR prediction) -> MPE module (feature grouping, probability distribution learning, bit-width sampling, retraining)

- Critical path:
  1. Group features by frequency
  2. Initialize probability distributions over candidate bit-widths
  3. During training: compute expected quantized embeddings, update probability distributions and full-precision embeddings
  4. After training: sample final bit-widths for each group
  5. Retrain model from scratch using sampled bit-widths
  6. During inference: store and retrieve mixed-precision embeddings, dequantize for computation

- Design tradeoffs:
  - Search space reduction vs. precision granularity: Grouping by frequency reduces search space but may force less important features in the same group to share precision.
  - Probability distribution vs. one-hot selection: Using expected embeddings enables gradient-based optimization but adds complexity; one-hot selection is simpler but non-differentiable.
  - Retraining vs. direct quantization: Retraining recovers accuracy but increases training time; direct quantization is faster but less accurate.

- Failure signatures:
  - Accuracy loss during retraining: Indicates poor sampled bit-widths or insufficient model capacity.
  - Unstable training during probability distribution learning: Suggests STE approximation is too crude or regularization is too strong.
  - Excessive memory usage: Implies too many features per group or too many candidate bit-widths.

- First 3 experiments:
  1. Train MPE with a small dataset and a simple model (e.g., DNN) to verify basic functionality and convergence.
  2. Vary the regularization coefficient Î» to observe its effect on compression ratio and accuracy tradeoff.
  3. Test the transferability of sampled bit-widths across different model architectures (e.g., DNN to DCN) to assess robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MPE vary with different feature grouping strategies beyond frequency-based grouping?
- Basis in paper: [inferred] The paper mentions that features are grouped by frequency as a reliable measure of feature importance, but does not explore alternative grouping strategies.
- Why unresolved: The paper only evaluates one grouping strategy (frequency-based) and does not compare it with other potential strategies like random grouping, feature importance-based grouping from other methods, or gradient-based grouping.
- What evidence would resolve it: Experimental results comparing MPE's performance using different grouping strategies (frequency-based vs random vs importance-based vs gradient-based) on the same datasets would clarify whether frequency-based grouping is optimal or if other strategies could yield better compression-accuracy trade-offs.

### Open Question 2
- Question: Can MPE be effectively extended to handle mixed-precision quantization of the feature interaction network, not just the embedding table?
- Basis in paper: [explicit] The paper explicitly states "We do not quantize the feature interaction network, as doing so would severely hurt prediction accuracy while yielding little memory savings."
- Why unresolved: While the paper provides reasoning for excluding the feature interaction network from quantization, it does not explore whether selective mixed-precision quantization of specific layers within the interaction network could be beneficial.
- What evidence would resolve it: Experiments applying MPE to selectively quantize layers within the feature interaction network (using similar frequency or importance metrics) and measuring the impact on both accuracy and memory usage would determine if this extension is viable.

### Open Question 3
- Question: How does the performance of MPE change when applied to recommendation models with different embedding dimensions (e.g., 8 vs 32 vs 64)?
- Basis in paper: [inferred] The paper uses a fixed embedding dimension of 16 for all experiments but does not investigate how MPE performs across different embedding dimensions.
- Why unresolved: The paper does not explore the interaction between embedding dimension size and the effectiveness of mixed-precision compression, which could reveal important scaling properties of MPE.
- What evidence would resolve it: Running MPE experiments across multiple embedding dimensions (8, 16, 32, 64) on the same datasets while measuring compression ratios and accuracy would show how embedding dimension affects MPE's performance and whether larger dimensions provide more opportunities for mixed-precision compression.

## Limitations
- The assumption that feature frequency reliably indicates importance is asserted but not rigorously validated across diverse datasets
- The extreme compression claims (200x, 0.16 average bits) lack sufficient methodological detail for verification
- The effectiveness of the expected quantized embedding approach versus alternative mixed-precision strategies is not compared

## Confidence

- **High confidence**: The basic framework of grouping by frequency and learning probability distributions is clearly described and methodologically sound
- **Medium confidence**: The compression ratios and accuracy metrics are reported with appropriate statistical measures, though methodology details are sparse
- **Low confidence**: The extreme compression claims (200x, 0.16 average bits) and the assertion that no accuracy loss occurs at these levels

## Next Checks

1. **Frequency-importance correlation validation**: Systematically test whether feature frequency actually correlates with importance across multiple recommendation datasets with known feature importance rankings

2. **Ablation study on sampling threshold**: Test the sensitivity of accuracy and compression to the 1/(2m) sampling threshold - does the method break down with different thresholds?

3. **Zero-bit embedding analysis**: Investigate what "0-bit embeddings" means functionally - are these features completely dropped, replaced with defaults, or something else?