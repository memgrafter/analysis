---
ver: rpa2
title: '360 in the Wild: Dataset for Depth Prediction and View Synthesis'
arxiv_id: '2406.18898'
source_url: https://arxiv.org/abs/2406.18898
tags:
- depth
- dataset
- images
- image
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the lack of large-scale, diverse datasets for
  training deep learning models on omnidirectional images, particularly those that
  include camera pose and depth information. Existing datasets are often synthetic
  or limited in scope, leading to generalization issues.
---

# 360 in the Wild: Dataset for Depth Prediction and View Synthesis

## Quick Facts
- arXiv ID: 2406.18898
- Source URL: https://arxiv.org/abs/2406.18898
- Authors: Kibaek Park; Francois Rameau; Jaesik Park; In So Kweon
- Reference count: 40
- The paper introduces "360° in the Wild," a dataset of over 25,000 real-world omnidirectional images with camera poses and depth maps for training deep learning models on omnidirectional vision tasks.

## Executive Summary
This paper addresses the lack of large-scale, diverse datasets for training deep learning models on omnidirectional images, particularly those that include camera pose and depth information. Existing datasets are often synthetic or limited in scope, leading to generalization issues. To overcome this, the authors introduce "360° in the Wild," a dataset of over 25,000 real-world omnidirectional images extracted from YouTube videos, covering various indoor and outdoor environments. Each image is paired with its camera pose and depth map, obtained through Structure from Motion (SfM) and Multi-View Stereo (MVS) methods. The dataset also includes masks to handle moving objects. The authors demonstrate the dataset's applicability by training models for single image depth estimation and novel view synthesis, showing improved performance in these tasks compared to existing methods.

## Method Summary
The authors scraped YouTube videos from diverse locations worldwide and reconstructed 3D structure using SfM and MVS. They extracted over 25,000 omnidirectional images, each paired with camera pose and depth map. Moving objects were manually masked to prevent incorrect geometry learning. The dataset was split into indoor, outdoor, and mannequin categories. For depth estimation, they fine-tuned or trained from scratch using MiDaS. For novel view synthesis, they extended NeRF++ to handle 360° images and trained on the dataset.

## Key Results
- The dataset provides real-world diversity, moving objects, and varied lighting that synthetic datasets cannot replicate.
- Masking moving objects in videos prevents depth estimation models from learning incorrect geometry.
- Adapting NeRF++ to spherical camera geometry enables view synthesis on omnidirectional images without calibration parameters.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Real-world omnidirectional video data enables learning 3D geometry and camera pose without synthetic scene limitations.
- Mechanism: By scraping YouTube videos from diverse locations and reconstructing 3D structure using SfM/MVS, the dataset provides natural scene diversity, moving objects, and varied lighting that synthetic datasets cannot replicate.
- Core assumption: The reconstructed camera poses and depth maps are accurate enough to train supervised or self-supervised models despite being "pseudo-labels."
- Evidence anchors:
  - [abstract] "This dataset has been carefully scraped from the Internet and has been captured from various locations worldwide."
  - [section] "Our dataset provides the camera's pose as well as the corresponding depth map for each image."
  - [corpus] Weak correlation; no direct neighbor citations on pose/depth quality from wild videos.
- Break condition: If SfM/MVS reconstruction fails on large-scale or unconstrained scenes, pose and depth quality degrade and supervised training collapses.

### Mechanism 2
- Claim: Masking moving objects in videos prevents depth estimation models from learning incorrect geometry.
- Mechanism: Human annotators manually remove the cameraman (with gimbal/camera stick) from each frame, producing binary masks that exclude transient objects from the reconstruction pipeline.
- Core assumption: The masked regions do not contain critical static scene geometry needed for accurate depth prediction.
- Evidence anchors:
  - [section] "To tackle this limitation, we opted for OpenSfM [1] to perform the sparse mapping and camera pose estimation tasks in our sequences. Despite its robustness, OpenSfM remains very sensitive to the presence of moving objects in the scene."
  - [section] "we masked it out utilizing Rotobrush2 in Adobe After Effects"
  - [corpus] Weak; no neighbor studies explicitly comparing masked vs. unmasked wild video reconstruction.
- Break condition: If static geometry is inadvertently masked or if dynamic objects are part of the static scene, depth models will underfit important scene structures.

### Mechanism 3
- Claim: Adapting NeRF++ to spherical camera geometry enables view synthesis on omnidirectional images without calibration parameters.
- Mechanism: Rays are computed directly from equirectangular pixel coordinates to unit sphere directions; the dual-volume NeRF++ formulation handles both foreground and background without camera intrinsics.
- Core assumption: Equirectangular projection is linear in spherical coordinates and can be inverted exactly for ray directions.
- Evidence anchors:
  - [section] "Each ith pixel [pix, piy] ∈ P can be expressed into spherical coordinates [piθ, pir] via a linear mapping..."
  - [section] "Interestingly, the computation of the camera's rays for a full 360° image is trivial and does not require any calibration parameters"
  - [corpus] No neighbor papers directly extending NeRF++ to 360°; only generic NeRF extensions cited.
- Break condition: If the linear mapping assumption fails (e.g., for fisheye or non-linear projections), ray directions become inaccurate and view synthesis quality drops.

## Foundational Learning

- Concept: Structure from Motion (SfM) and Multi-View Stereo (MVS)
  - Why needed here: To generate camera poses and depth maps from unstructured video frames.
  - Quick check question: What is the main difference between SfM and MVS in terms of output?

- Concept: Equirectangular projection and spherical coordinates
  - Why needed here: To compute rays for 360° view synthesis without camera intrinsics.
  - Quick check question: How do you convert a pixel (x, y) in an equirectangular image to a 3D unit vector?

- Concept: Neural radiance fields (NeRF) and dual-volume formulation
  - Why needed here: To synthesize novel views for scenes with large depth ranges.
  - Quick check question: Why does NeRF++ split the scene into inner and outer volumes?

## Architecture Onboarding

- Component map:
  Data ingestion: YouTube video URLs → frame extraction → equirectangular preprocessing
  Pose/depth generation: OpenSfM (pose) → COLMAP (depth) → cube map conversion
  Masking pipeline: Adobe After Effects → binary mask export
  Dataset splits: Indoor/Outdoor/Mannequin categories → train/val/test
  Model training: MiDaS depth fine-tuning / NeRF++ view synthesis

- Critical path:
  1. Scrape and download 360° YouTube videos
  2. Detect scene changes, segment into sub-sequences
  3. Mask moving objects (cameraman)
  4. Run OpenSfM for sparse reconstruction
  5. Convert to cube maps, run COLMAP for dense depth
  6. Export frames, depth maps, masks, and poses
  7. Split into train/val/test, publish links

- Design tradeoffs:
  - Pros: Real-world diversity, no synthetic bias, large scale
  - Cons: Variable quality of pseudo-labels, manual masking effort, copyright restrictions (links only)

- Failure signatures:
  - Sparse or noisy depth maps → poor supervised depth training
  - Missing or incorrect camera poses → view synthesis artifacts
  - Insufficient masking → dynamic object ghosting in reconstruction

- First 3 experiments:
  1. Validate depth map quality: sample 10 frames, overlay COLMAP depth with ground-truth perspective depth from KITTI.
  2. Test view synthesis: train NeRF++ on one Indoor sequence, render novel views, compare PSNR/SSIM to original.
  3. Fine-tune MiDaS: run single-image depth estimation on test set, compute AbsRel and δ thresholds.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions. However, it acknowledges limitations such as the scale ambiguity of ground-truth depth from internet videos and the need for large numbers of iterations for training on 360° images.

## Limitations
- The primary limitation lies in the quality of pseudo-labels derived from SfM/MVS on unconstrained wild videos, with unclear impact of reconstruction errors on downstream tasks.
- Manual masking process is labor-intensive and may introduce biases if static scene elements are inadvertently masked.
- The linear mapping assumption for equirectangular-to-spherical conversion may break down for non-standard projections.

## Confidence
- **High**: Real-world diversity enables learning 3D geometry without synthetic scene limitations.
- **Medium**: Masking moving objects prevents depth models from learning incorrect geometry.
- **Medium**: NeRF++ adaptation to spherical camera geometry enables view synthesis without calibration parameters.

## Next Checks
1. Sample 10 frames from the dataset and overlay COLMAP depth maps with ground-truth perspective depth from KITTI to assess depth map quality.
2. Train NeRF++ on one Indoor sequence, render novel views, and compare PSNR/SSIM to original to validate view synthesis performance.
3. Fine-tune MiDaS on the test set and compute AbsRel and δ thresholds to evaluate single-image depth estimation accuracy.