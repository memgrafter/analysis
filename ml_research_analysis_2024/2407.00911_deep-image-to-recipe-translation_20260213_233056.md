---
ver: rpa2
title: Deep Image-to-Recipe Translation
arxiv_id: '2407.00911'
source_url: https://arxiv.org/abs/2407.00911
tags:
- ingredients
- ingredient
- dataset
- training
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the problem of translating a food image into
  a complete recipe, including ingredients and cooking instructions. The authors approach
  this as a two-stage task: first, predicting ingredients from the image using computer
  vision models; second, generating cooking instructions from the predicted ingredients
  using natural language generation techniques.'
---

# Deep Image-to-Recipe Translation

## Quick Facts
- arXiv ID: 2407.00911
- Source URL: https://arxiv.org/abs/2407.00911
- Authors: Jiangqin Ma; Bilal Mawji; Franz Williams
- Reference count: 17
- Primary result: Achieved IoU 0.119 for ingredient prediction and perplexity 434.4 for instruction generation

## Executive Summary
This paper addresses the task of translating food images into complete recipes, including ingredients and cooking instructions. The authors develop a two-stage pipeline: first predicting ingredients from images using computer vision, then generating cooking instructions from those ingredients using natural language generation. They demonstrate that transfer learning via pre-trained ResNet-50 weights and GloVe embeddings significantly improves performance over custom architectures, particularly given the relatively small dataset size (approximately 240 MB).

## Method Summary
The approach consists of two sequential stages. Stage 1 uses a ResNet-50 model with transfer learning from ImageNet for multi-label ingredient classification, achieving an IoU score of 0.119. Stage 2 employs a bidirectional LSTM with pre-trained GloVe embeddings and regularization techniques (dropout 0.8, L2 regularization 0.01) for instruction generation, achieving a validation perplexity of 434.413. The dataset consists of approximately 13,500 food images and recipes from the Epicurious dataset, with ingredients filtered to the top 200 most frequent items.

## Key Results
- ResNet-50 with transfer learning achieves IoU score of 0.119 for ingredient prediction, outperforming custom CNN baseline
- Bidirectional LSTM with GloVe embeddings achieves validation perplexity of 434.4, meeting the target of under 500
- Transfer learning and pre-trained embeddings provide significant performance boosts despite limited dataset size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning via pre-trained ResNet-50 weights significantly improves ingredient prediction accuracy over a custom CNN.
- Mechanism: ResNet-50 provides rich, generalizable feature representations learned from ImageNet, reducing the need for large in-domain training data.
- Core assumption: ImageNet-pretrained filters capture low-level food image features useful for ingredient recognition.
- Evidence anchors:
  - [abstract] "transfer learning via pre-trained ResNet-50 weights ... provide an exceptional boost to model performance"
  - [section 2.1.2] "Our ResNet-50-based model architecture takes advantage of transfer learning by reusing ResNet-50 trained on the ImageNet dataset"
  - [section 3.1.3] "utilizing a pre-trained ResNet-50 model provided higher performance than our custom CNN model"

### Mechanism 2
- Claim: Pre-trained GloVe embeddings improve instruction generation coherence compared to learned embeddings.
- Mechanism: GloVe embeddings encode semantic relationships learned from large text corpora, enabling the LSTM to generate contextually relevant cooking instructions.
- Core assumption: Ingredient and instruction words share semantic space with general text used in GloVe training.
- Evidence anchors:
  - [abstract] "pre-trained word embeddings" and "validation perplexity of 434.413"
  - [section 3.2.2] "We find that by employing GloVe embeddings and regularization methods, a much better validation perplexity and generative output can be achieved"

### Mechanism 3
- Claim: Bidirectional LSTM with regularization mitigates overfitting and improves instruction generation quality.
- Mechanism: Bidirectional context captures dependencies across the entire ingredient sequence, while dropout, L2, and layer normalization constrain the model from memorizing training data.
- Core assumption: Ingredients are unordered sets, so forward and backward context both contribute equally to instruction generation.
- Evidence anchors:
  - [section 3.2.1] "we find that a bidirectional LSTM offers better performance than a standard LSTM layer"
  - [section 3.2.1] "by employing a dropout rate of 0.8, L2 regularization of 1e-2, and layer normalization, our validation perplexity can be further improved"

## Foundational Learning

- Concept: Multi-label classification with IoU metric
  - Why needed here: Ingredient prediction is multi-label; IoU better captures partial matches than accuracy.
  - Quick check question: Why might IoU be preferred over accuracy for imbalanced ingredient datasets?

- Concept: Sequence-to-sequence modeling with RNNs
  - Why needed here: Generating instructions from ingredient lists is a conditional sequence generation task.
  - Quick check question: What advantage does a bidirectional LSTM provide when ingredients are treated as an unordered set?

- Concept: Transfer learning and fine-tuning
  - Why needed here: Limited dataset size makes full training from scratch impractical; transfer learning leverages large pre-trained models.
  - Quick check question: How does freezing ResNet-50 and only training the final layer reduce overfitting risk?

## Architecture Onboarding

- Component map: Image → ResNet-50 feature extractor → Dense ingredient classifier (sigmoid outputs) → Ingredient tokens → Tokenizer → GloVe embeddings → Bidirectional LSTM → Dense time-distributed layer → Token probabilities
- Critical path:
  1. Image preprocessing → ResNet-50 → Ingredient logits → Ingredient list
  2. Ingredient list → Tokenizer → Embedding → Bidirectional LSTM → Instruction tokens → Instructions
- Design tradeoffs:
  - Using ResNet-50 as frozen extractor trades model adaptability for speed and reduced overfitting risk.
  - GloVe embeddings provide semantic grounding but may misalign with rare or domain-specific ingredient names.
  - Bidirectional LSTM captures context but doubles parameters and may overfit small datasets without strong regularization.
- Failure signatures:
  - Low IoU but high accuracy: Model overfits to frequent ingredients and ignores rare ones.
  - Perplexity flat or rising: Generator stuck in repetitive loops or cannot generalize beyond training sequences.
  - Ingredient over-prediction (e.g., salt, olive oil): Class imbalance; consider class weighting or focal loss.
- First 3 experiments:
  1. Train ResNet-50 with all convolutional layers frozen; vary dropout on final dense layer; record IoU.
  2. Compare GloVe embedding sizes (50 vs 100) in bidirectional LSTM; monitor validation perplexity.
  3. Enable/disabled augmentation in Stage 1; compare IoU to test augmentation impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does increasing dataset size affect ingredient prediction performance for both custom CNN and ResNet-50 architectures?
- Basis in paper: [explicit] The authors note that their dataset size (approximately 240 MB) is significantly smaller than other datasets like Recipe1M (500 GB) and acknowledge that deep learning models often benefit from extensive data.
- Why unresolved: The authors did not experiment with larger datasets or provide comparative results with different dataset sizes.
- What evidence would resolve it: Training and evaluating both architectures on progressively larger datasets while measuring IoU scores would demonstrate the relationship between dataset size and performance.

### Open Question 2
- Question: What is the impact of different confidence thresholds on ingredient prediction accuracy and practical usability?
- Basis in paper: [explicit] The authors mention using a low confidence threshold of 0.05 for ingredient prediction, as using the validation threshold of 0.5 resulted in too few ingredients being predicted per image.
- Why unresolved: The authors did not explore or report how different threshold values affect the trade-off between prediction accuracy and completeness of ingredient lists.
- What evidence would resolve it: Systematic testing of various threshold values (e.g., 0.1, 0.3, 0.5, 0.7) and analyzing the resulting IoU scores and ingredient list completeness would provide insights into optimal threshold selection.

### Open Question 3
- Question: How do transformer-based architectures compare to LSTM models for instruction generation in terms of perplexity and output quality?
- Basis in paper: [explicit] The authors suggest exploring transformer-based models for instruction generation in future work, noting their efficacy on larger datasets.
- Why unresolved: The authors only compared two LSTM variants and did not test transformer architectures.
- What evidence would resolve it: Implementing and evaluating transformer-based models (e.g., GPT, T5) on the same dataset, measuring perplexity scores and conducting human evaluations of generated instructions would provide direct comparisons.

## Limitations
- Small dataset size (approximately 240 MB) limits model generalization and prevents training of larger architectures like transformers
- Lack of user studies to assess practical utility of generated recipes
- Pipeline's sequential nature may propagate errors from ingredient prediction to instruction generation
- Dataset filtering to top 1% most frequent ingredients may exclude regional or specialized ingredients

## Confidence

**High Confidence**: Transfer learning with pre-trained ResNet-50 significantly improves ingredient prediction accuracy over custom CNN architectures (IoU improvement from 0.069 to 0.119).

**Medium Confidence**: GloVe embeddings with bidirectional LSTM improve instruction generation coherence, achieving the target perplexity of under 500 (reported 434.413).

**Low Confidence**: The claimed superiority of bidirectional LSTM over standard LSTM for instruction generation lacks direct comparison data in the results section.

## Next Checks

1. **Error Propagation Analysis**: Measure how ingredient prediction errors (false positives/negatives) affect instruction generation quality by comparing recipes generated from ground truth ingredients versus predicted ingredients.

2. **Cross-Dataset Generalization**: Evaluate the trained models on a separate food dataset (e.g., Recipe1M) to assess whether the performance gains from transfer learning and GloVe embeddings generalize beyond the Epicurious dataset.

3. **Ablation Study on Ingredient Filtering**: Train models with different ingredient frequency thresholds (top 5%, 10%, 25%) to quantify the impact of dataset filtering on both ingredient prediction accuracy and instruction generation quality.