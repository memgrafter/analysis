---
ver: rpa2
title: 'ReasonAgain: Using Extractable Symbolic Programs to Evaluate Mathematical
  Reasoning'
arxiv_id: '2410.19056'
source_url: https://arxiv.org/abs/2410.19056
tags:
- reasoning
- math
- question
- wang
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of accurately evaluating mathematical
  reasoning in large language models (LLMs), highlighting that current methods using
  static datasets are inadequate due to model shortcuts and potential data contamination.
  The authors propose a novel evaluation method called ReasonAgain, which uses symbolic
  programs (Python code) to encapsulate the reasoning process.
---

# ReasonAgain: Using Extractable Symbolic Programs to Evaluate Mathematical Reasoning

## Quick Facts
- **arXiv ID**: 2410.19056
- **Source URL**: https://arxiv.org/abs/2410.19056
- **Reference count**: 6
- **Primary result**: Current LLMs (GPT-4-Turbo, GPT-4o, Llama-3.1-8B, Qwen-2.5-7B) show significant performance drops (34.3-63.8%) on perturbed math problems, indicating fragility in mathematical reasoning despite occasional correct answers on original problems.

## Executive Summary
This paper addresses the fundamental challenge of accurately evaluating mathematical reasoning in large language models by proposing a novel approach called ReasonAgain. The key insight is that current static datasets may overestimate model capabilities due to shortcut learning and data contamination. The proposed method uses symbolic programs (executable Python code) to encapsulate the reasoning process, then evaluates models by testing their ability to generalize to perturbed inputs while maintaining the same underlying reasoning structure. The evaluation reveals substantial performance drops across multiple state-of-the-art models when tested on these perturbed problems, suggesting that current models do not genuinely understand mathematical reasoning even when they occasionally produce correct answers.

## Method Summary
The method involves extracting executable Python programs that capture the solution process for each math problem using GPT-4o, validating these programs through execution and correctness checks, generating perturbed questions by varying input parameters while keeping the symbolic program unchanged, and evaluating LLMs on both original and perturbed versions. For each original problem, five new input sets are created from the symbolic program, producing new test cases with gold-standard answers generated by executing the programs with new parameters. Model performance is then assessed by comparing accuracy on original versus perturbed questions, with self-consistency used to mitigate stochastic generation errors.

## Key Results
- Significant performance drops across all evaluated models when tested on perturbed questions: GPT-4-Turbo (63.8% drop), GPT-4o (34.3% drop), Llama-3.1-8B (49.7% drop), and Qwen-2.5-7B (51.1% drop)
- Even models that achieve high accuracy on original problems show substantial fragility when reasoning is tested on perturbed inputs
- Human evaluation confirms 92% of GSM8K programs and 83% of MATH programs genuinely demonstrate correct reasoning processes
- Direct prompting generally outperforms few-shot CoT and few-shot CoT+SC approaches across all models and datasets

## Why This Works (Mechanism)

### Mechanism 1
Symbolic programs can serve as a "reasoning blueprint" that decouples the correctness of the final answer from the quality of the intermediate reasoning. The method first extracts executable Python code that captures the solution process for a given problem, then evaluates model performance by testing on perturbed inputs. If the model truly understands the reasoning, it should generalize to all valid inputs of the same program.

### Mechanism 2
Perturbing the input values while keeping the symbolic program unchanged tests whether the model relies on memorized patterns or genuine reasoning. For each original problem, five new input sets are generated from the symbolic program, producing new test cases. The model's accuracy on these perturbations is compared to its accuracy on the original problem.

### Mechanism 3
Using the same symbolic program to generate both questions and answers ensures a tight coupling between reasoning process and evaluation, avoiding alternative solution paths. The program is executed with new inputs to generate the correct answers for each perturbed question, providing a gold standard that matches the reasoning encapsulated by the program.

## Foundational Learning

- **Concept: Executable symbolic programs**
  - Why needed here: They provide a precise, machine-checkable representation of the reasoning process that can be automatically executed to generate new test cases.
  - Quick check question: If a symbolic program returns the correct answer for the original input, does that guarantee it encodes the correct reasoning process? (Answer: No, but high human agreement suggests it does.)

- **Concept: Perturbation of numerical inputs**
  - Why needed here: It tests whether the model's reasoning is robust to variations in the problem data, distinguishing true understanding from memorization.
  - Quick check question: If a model correctly answers the original problem but fails on 80% of perturbed inputs, what does that indicate about its reasoning? (Answer: It likely relies on memorization or shallow heuristics.)

- **Concept: Self-consistency in model outputs**
  - Why needed here: It provides a way to mitigate stochastic generation errors when evaluating perturbed questions, improving reliability of the performance measure.
  - Quick check question: Why might self-consistency help in evaluating perturbed math problems? (Answer: It reduces variance in model answers, making performance drops due to reasoning fragility more visible.)

## Architecture Onboarding

- **Component map**: GSM8K/MATH → symbolic program extraction → program validation → perturbation generation → evaluation engine → accuracy comparison
- **Critical path**: Extract → Validate → Perturb → Evaluate (for each problem)
- **Design tradeoffs**: Using GPT-4o for program generation gives high coverage but introduces cost and potential generation errors; manual filtering helps but is not perfect. Perturbing only numerical values preserves the reasoning structure but may miss edge cases where reasoning depends on specific numeric relationships.
- **Failure signatures**: High accuracy drop on perturbations → fragility in reasoning; inability to generate executable program → problem type not supported by current symbolic extraction method; human evaluation shows incorrect answers for perturbations → program does not faithfully encode reasoning.
- **First 3 experiments**:
  1. Extract programs from a small sample of GSM8K, manually verify correctness, and measure program generation success rate.
  2. Generate perturbations for verified programs, run them through GPT-4o, and confirm that perturbed answers match program outputs.
  3. Evaluate a baseline LLM on original and perturbed problems, measure accuracy drop, and compare with human agreement on reasoning validity.

## Open Questions the Paper Calls Out

### Open Question 1
How can we improve the extraction of symbolic programs to achieve better coverage of mathematical reasoning types, particularly for geometry and other domains currently excluded? The paper states "Our program generation is limited by a conceptualization process proposed in Zhou et al. (2024), which does not work well on certain types of math questions, such as geometry-related ones."

### Open Question 2
What is the relationship between performance on ReasonAgain perturbations and real-world mathematical reasoning tasks? The paper shows significant performance drops on perturbations but notes this suggests "fragility of math reasoning" without establishing how this correlates with practical reasoning abilities.

### Open Question 3
How can we develop more robust filtering mechanisms to reduce errors in the generated programs and perturbations? The paper acknowledges that "some mistakes exist in the current generated programs, which leads to partially incorrect gold labels in some perturbed questions" and states this will be explored in later versions.

## Limitations

- The evaluation method relies on GPT-4o for both program generation and perturbation creation, introducing potential bias in the evaluation process
- The perturbation strategy focuses only on numerical value changes while keeping the problem structure constant, which may not fully capture all forms of reasoning fragility
- The reported 8-17% of programs potentially encoding incorrect reasoning could affect the evaluation results

## Confidence

- **High**: The methodological framework for using symbolic programs to evaluate reasoning is sound and well-implemented
- **Medium**: The claim that current LLMs lack genuine mathematical reasoning understanding based on observed performance drops
- **Medium**: The assertion that existing static datasets overestimate model capabilities due to data contamination and shortcut learning

## Next Checks

1. **Program Fidelity Validation**: Manually verify a random sample of 100 symbolic programs from both GSM8K and MATH datasets to independently assess the reported 92% and 83% human agreement rates, and analyze whether incorrect programs systematically affect certain problem types.

2. **Alternative Solution Method Coverage**: Design and implement an evaluation where multiple symbolic programs (representing different valid solution approaches) are generated for the same problem, then test whether LLMs that fail under one program succeed under another, to assess whether the current method might be too restrictive.

3. **Perturbation Robustness Analysis**: Create a parallel evaluation set where perturbations are generated through human-designed variations rather than program-based changes, then compare performance drops between the two methods to determine if the observed fragility is specific to the current perturbation approach or reflects broader reasoning limitations.