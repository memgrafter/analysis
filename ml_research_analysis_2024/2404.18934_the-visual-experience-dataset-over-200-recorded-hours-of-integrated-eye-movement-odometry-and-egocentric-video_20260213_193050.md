---
ver: rpa2
title: 'The Visual Experience Dataset: Over 200 Recorded Hours of Integrated Eye Movement,
  Odometry, and Egocentric Video'
arxiv_id: '2404.18934'
source_url: https://arxiv.org/abs/2404.18934
tags:
- https
- page
- investigation
- arxiv
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Visual Experience Dataset (VEDB) is a compilation of over 240
  hours of egocentric video combined with gaze- and head-tracking data, recorded by
  58 observers aged 6-49. The dataset contains 717 sessions with first-person video,
  left and right eye videos for gaze tracking, and IMU data for head tracking.
---

# The Visual Experience Dataset: Over 200 Recorded Hours of Integrated Eye Movement, Odometry, and Egocentric Video

## Quick Facts
- arXiv ID: 2404.18934
- Source URL: https://arxiv.org/abs/2404.18934
- Reference count: 20
- Over 240 hours of integrated egocentric video, gaze, and head-tracking data from 58 observers

## Executive Summary
The Visual Experience Dataset (VEDB) is a comprehensive compilation of egocentric video, gaze tracking, and head movement data recorded by 58 observers aged 6-49. Spanning over 240 hours across 717 sessions, the dataset captures natural visual experiences in diverse environments and tasks. It includes first-person video, eye-tracking footage, IMU data, and detailed annotations, enabling research into natural scene statistics, visual attention, and human-computer interaction. The dataset is released as a living resource with tools for analysis and expansion.

## Method Summary
The VEDB was collected using head-mounted Pupil Labs Core systems for eye tracking, FLIR Chameleon3 cameras for high-resolution egocentric video, and Intel RealSense T265 sensors for head tracking. Data were gathered from 58 observers performing self-selected activities across 124 scene categories. Custom software processed raw video and sensor streams, applied calibration pipelines, and generated annotations. The dataset is publicly available through Databrary and the Open Science Framework, accompanied by code tools for data acquisition, preprocessing, and analysis.

## Key Results
- 717 sessions totaling over 240 hours of egocentric video and sensor data
- 58 observers aged 6-49 recorded across 124 scene categories and 396 distinct tasks
- Gaze tracking accuracy under 2 degrees visual angle in 63 hours of content
- Dataset includes annotations for tasks, environments, and calibration markers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset captures a representative distribution of natural visual experiences across varied environments and tasks.
- Mechanism: The dataset is collected across 124 scene categories, 396 distinct tasks, and spans both indoor/outdoor and active/sedentary behaviors. This breadth allows models trained on it to generalize across diverse real-world visual inputs.
- Core assumption: The self-selected activities of participants adequately represent natural scene statistics without systematic sampling bias.
- Evidence anchors:
  - [abstract] states: "The dataset consists of 717 sessions, recorded by 58 observers ranging from 6-49 years old."
  - [section] details: "The locations where our participants recorded also varied considerably. Of the 365 scene categories in the Places database, 124 are represented in the Visual Experience Dataset."
  - [corpus] evidence is limited; neighboring papers do not discuss dataset representativeness directly.
- Break condition: If certain environments or demographic groups are underrepresented, the dataset will fail to generalize to those scenarios.

### Mechanism 2
- Claim: The combination of gaze, head tracking, and egocentric video enables accurate modeling of both overt attention and environmental context.
- Mechanism: By aligning gaze data (from eye-tracking) with head movement data (from IMU) and egocentric video, the dataset provides a temporally precise mapping between where people look and what they see, which is crucial for modeling visual attention.
- Core assumption: Calibration and validation procedures ensure gaze data accuracy sufficient for downstream tasks.
- Evidence anchors:
  - [section] reports: "168 sessions (approximately 63 hours of content) containing initial gaze accuracies of under 2 degrees of visual angle."
  - [section] explains the gaze calibration pipeline, including validation errors and drift analysis.
  - [corpus] lacks direct evidence of gaze-video alignment accuracy.
- Break condition: If calibration drifts over time or environmental conditions degrade gaze accuracy, the alignment will become unreliable.

### Mechanism 3
- Claim: The dataset's scale and diversity make it suitable for training robust deep learning models in computer vision and perception tasks.
- Mechanism: With over 240 hours of video and millions of annotated frames, the dataset provides enough data to train models that can learn spatiotemporal patterns in natural scenes, detect objects, and classify activities.
- Core assumption: Model performance scales with dataset size and diversity; the annotations are sufficiently accurate and comprehensive.
- Evidence anchors:
  - [section] notes: "Alongside investigations of natural scene statistics, these data can be used to create and validate new or extant methods of motion or position estimation, object classification, human activity recognition, machine learning-based eye tracking, and more."
  - [section] discusses fine-grained task annotations and scene labels enabling activity recognition and scene classification tasks.
  - [corpus] shows neighboring papers on eye movement and VR datasets but not large-scale egocentric video datasets.
- Break condition: If the dataset is too noisy or the annotations are incomplete, model training will suffer.

## Foundational Learning

- Concept: Egocentric vision and first-person perspective modeling.
  - Why needed here: The dataset is based on egocentric video, so understanding how to process and model first-person visual input is critical.
  - Quick check question: What are the key differences between third-person and egocentric video in terms of motion patterns and field of view?

- Concept: Gaze tracking and calibration procedures.
  - Why needed here: Accurate gaze data is essential for aligning visual input with attention; understanding calibration methods and error sources is necessary.
  - Quick check question: What are the main sources of error in mobile eye-tracking, and how does calibration help mitigate them?

- Concept: Natural scene statistics and their role in visual perception.
  - Why needed here: The dataset is designed to study the statistical properties of natural scenes; understanding how these statistics affect perception is key.
  - Quick check question: How do spatial and temporal statistics of natural scenes influence visual processing in the brain?

## Architecture Onboarding

- Component map: Egocentric world video -> Eye videos (left/right) -> IMU and odometry data -> Metadata and annotations
- Critical path: Parse raw video and sensor streams -> Apply gaze and head-tracking calibration pipelines -> Use annotations for supervised learning or analysis
- Design tradeoffs: Prioritizes naturalistic data collection over controlled experimental conditions, trading some experimental control for ecological validity
- Failure signatures: Corrupted video streams, failed calibration, missing gaze data, privacy concerns from bystanders
- First 3 experiments:
  1. Load a sample session and visualize the egocentric video alongside the gaze overlay to verify alignment.
  2. Compute basic statistics (e.g., fixation density maps) from gaze data to understand attention distribution.
  3. Train a simple activity classifier using the annotated task labels to test the utility of the dataset for supervised learning.

## Open Questions the Paper Calls Out

- Open Question 1: How do individual differences in head and eye movement coordination affect the visual statistics experienced by different observers?
  - Basis in paper: [explicit] The paper mentions that "joint statistics of natural head and eye movements can provide valuable and novel insight into head-eye coordination during everyday behavior."
  - Why unresolved: The dataset provides data on head and eye movements but does not analyze how individual differences in coordination patterns influence the visual input.
  - What evidence would resolve it: Analyzing the relationship between head-eye coordination patterns and the resulting visual statistics across different observers.

- Open Question 2: To what extent do environmental factors (e.g., lighting, scene complexity) impact the accuracy and reliability of gaze tracking in the dataset?
  - Basis in paper: [explicit] The paper discusses challenges with gaze tracking accuracy in different environments and lighting conditions.
  - Why unresolved: While the paper mentions these challenges, it does not provide a comprehensive analysis of their impact on gaze tracking performance.
  - What evidence would resolve it: A systematic evaluation of gaze tracking accuracy across various environmental conditions in the dataset.

- Open Question 3: How does the inclusion of gaze-contingent video segments (centered on gaze location) affect the representation of visual statistics compared to non-gaze-centered segments?
  - Basis in paper: [explicit] The paper mentions the possibility of generating gaze-centered video segments but does not explore their impact on visual statistics.
  - Why unresolved: The potential differences in visual statistics between gaze-centered and non-gaze-centered segments have not been investigated.
  - What evidence would resolve it: Comparing the visual statistics of gaze-centered and non-gaze-centered video segments within the dataset.

## Limitations

- Privacy constraints limit full dataset release, restricting access to certain content
- Potential demographic skew in participant pool may affect generalizability
- Gaze tracking accuracy challenges in outdoor conditions and with diverse participants

## Confidence

- Representativeness of natural visual experiences: Medium
- Gaze-video alignment accuracy: Medium
- Dataset utility for training deep learning models: High

## Next Checks

1. Verify gaze-video alignment accuracy across different environmental conditions using the provided calibration error metrics
2. Test gaze tracking pipeline generalization on new observer populations not represented in the original dataset
3. Validate scene category and task annotation consistency across different annotators using the provided confidence metrics