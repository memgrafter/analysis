---
ver: rpa2
title: 'OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved
  with Text'
arxiv_id: '2406.08418'
source_url: https://arxiv.org/abs/2406.08418
tags:
- uni00000013
- data
- uni00000011
- dataset
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OmniCorpus, the largest multimodal dataset
  to date, containing 8.6 billion images, 1,696 billion text tokens, and 2.2 billion
  documents from diverse sources including Common Crawl, Chinese websites, and video
  platforms. The dataset is constructed using an efficient data engine that includes
  main body extraction, text filtering, document deduplication, image downloading
  & filtering, and human-feedback filtering.
---

# OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text

## Quick Facts
- **arXiv ID:** 2406.08418
- **Source URL:** https://arxiv.org/abs/2406.08418
- **Reference count:** 40
- **Primary result:** OmniCorpus is the largest multimodal dataset to date, containing 8.6 billion images, 1,696 billion text tokens, and 2.2 billion documents, surpassing previous datasets by 15 times in scale.

## Executive Summary
OmniCorpus is introduced as the largest multimodal dataset to date, featuring 8.6 billion images, 1,696 billion text tokens, and 2.2 billion documents sourced from diverse platforms including Common Crawl, Chinese websites, and video platforms. The dataset is constructed using an efficient data engine that includes main body extraction, text filtering, document deduplication, image downloading & filtering, and human-feedback filtering. OmniCorpus not only surpasses previous multimodal datasets in scale by 15 times but also maintains high data quality. Its flexibility in supporting various data formats (pure text corpus, image-text pairs, and interleaved data) enhances its usability. The dataset's effectiveness is validated through experiments, particularly in improving multimodal model performance in tasks like image captioning and visual question answering, providing a solid foundation for future multimodal model research.

## Method Summary
OmniCorpus is constructed using a comprehensive data engine that processes diverse sources including Common Crawl, Chinese websites, and video platforms. The pipeline involves main body extraction to isolate relevant content, preliminary text filtering to remove low-quality text, document deduplication to ensure uniqueness, image downloading & filtering to select high-quality images, and detailed text filtering to refine the corpus. Human-feedback filtering is employed to further enhance data quality. The dataset supports various data formats, including pure text corpus, image-text pairs, and interleaved data, making it versatile for different multimodal tasks. The construction process is designed to be efficient and scalable, ensuring the dataset's large size while maintaining high quality.

## Key Results
- OmniCorpus contains 8.6 billion images and 1,696 billion text tokens, surpassing previous multimodal datasets by 15 times in scale.
- The dataset is sourced from diverse platforms, including English and non-English websites, as well as video-centric websites, enhancing its representativeness.
- Experiments validate the dataset's effectiveness in improving multimodal model performance, particularly in tasks like image captioning and visual question answering.

## Why This Works (Mechanism)
OmniCorpus works by leveraging a large-scale, diverse dataset to train multimodal models, enabling them to learn robust representations across different modalities. The inclusion of both image and text data, along with their interleaving, allows models to capture complex relationships and improve performance on tasks such as image recognition, captioning, and visual question answering. The dataset's scale and diversity provide a rich training ground for models to generalize well across various scenarios.

## Foundational Learning
- **Multimodal Data Processing:** Understanding how to extract, filter, and deduplicate multimodal data is crucial for constructing high-quality datasets. Quick check: Verify the data processing pipeline by running a small-scale test.
- **Human-Feedback Filtering:** Incorporating human feedback into the filtering process ensures data quality and relevance. Quick check: Evaluate the impact of human-feedback filtering on a subset of the dataset.
- **Data Format Flexibility:** Supporting various data formats (pure text, image-text pairs, interleaved data) enhances the dataset's usability for different tasks. Quick check: Test the dataset's compatibility with different multimodal model architectures.

## Architecture Onboarding
- **Component Map:** Data Sources -> Main Body Extraction -> Text Filtering -> Document Deduplication -> Image Downloading & Filtering -> Human-Feedback Filtering -> OmniCorpus
- **Critical Path:** The critical path involves the sequential processing of data through extraction, filtering, deduplication, and final human-feedback filtering to ensure high-quality multimodal data.
- **Design Tradeoffs:** The tradeoff between dataset scale and quality is managed through rigorous filtering processes, balancing the need for large-scale data with the requirement for high-quality, relevant content.
- **Failure Signatures:** If the dataset processing pipeline fails, it may be due to incompatible data sources or incorrect filtering parameters. If the dataset quality is not as expected, the filtering criteria or human-feedback process may need adjustment.
- **First Experiments:**
  1. Validate the dataset's quality by running experiments on image captioning tasks.
  2. Assess the dataset's usability by testing its compatibility with different multimodal model architectures.
  3. Evaluate the impact of human-feedback filtering on model performance by comparing results with and without this step.

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** What specific factors in the filtering process most significantly impact model performance?
- **Basis in paper:** The paper mentions that data filtering benefits MLLMs to some extent, but over-filtering may harm performance due to data homogenization. It also states that the current filtering process offers limited improvements to the model's performance.
- **Why unresolved:** The paper acknowledges the complexity of demonstrating which specific factors meet the conditions that benefit the model, and this is not thoroughly explored.
- **What evidence would resolve it:** Conducting experiments to isolate and measure the impact of individual filtering factors on model performance, such as different thresholds for image quality scores or text fluency scores.

### Open Question 2
- **Question:** How does the inclusion of video-derived data from OmniCorpus-YT specifically affect the model's ability to handle real-world multimodal tasks?
- **Basis in paper:** The paper states that OmniCorpus-YT boosts VQA performance while degrading captioning ability, and demonstrates the feasibility of extracting image-text interleaved documents from video resources.
- **Why unresolved:** While the paper shows a trade-off between VQA and captioning performance, it does not explore the broader implications for real-world multimodal tasks.
- **What evidence would resolve it:** Evaluating the model's performance on a diverse set of real-world multimodal tasks, such as document understanding, visual storytelling, or multimodal reasoning, to assess the practical impact of video-derived data.

### Open Question 3
- **Question:** What are the long-term effects of using a dataset with such a high concentration of images from a few major platforms on the model's generalization ability?
- **Basis in paper:** The paper notes that image sources are concentrated on a few major platforms, with Blogspot and WordPress accounting for over 10% of the total images. It also mentions the potential for biases in web-crawled data.
- **Why unresolved:** The paper acknowledges the presence of biases but does not investigate the long-term effects of this concentration on the model's ability to generalize to diverse image sources.
- **What evidence would resolve it:** Conducting longitudinal studies to track the model's performance on tasks involving images from less represented platforms over time, and comparing it to models trained on more diverse image sources.

## Limitations
- The exact specifications of the data engine and its components, such as the main body extraction and text filtering algorithms, are not fully detailed in the paper.
- The specific configurations and parameters used in the human-feedback filtering process are not provided, which may affect the reproducibility of the dataset's quality.
- There is limited quantitative evidence beyond human evaluation metrics to support the dataset's quality claims.

## Confidence
- **High confidence:** Dataset scale and basic construction methodology
- **Medium confidence:** Quality claims and human evaluation results
- **Low confidence:** Exact implementation details of filtering algorithms and human feedback mechanisms

## Next Checks
1. Verify the actual number of unique, high-quality image-text pairs through independent sampling and assessment.
2. Replicate the data processing pipeline using the provided code to confirm the claimed scale and quality metrics.
3. Conduct ablation studies to quantify the contribution of different filtering stages to final dataset quality.