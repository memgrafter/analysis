---
ver: rpa2
title: 'Compressible and Searchable: AI-native Multi-Modal Retrieval System with Learned
  Image Compression'
arxiv_id: '2404.10234'
source_url: https://arxiv.org/abs/2404.10234
tags:
- image
- compression
- retrieval
- clip
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified framework that integrates learned
  image compression (LIC) with zero-shot retrieval capabilities for efficient multi-modal
  search. The key idea is to leverage the image encoder of Contrastive Language-Image
  Pretraining (CLIP) to bridge the gap between LIC features and semantic information,
  enabling efficient storage and retrieval of multimedia data.
---

# Compressible and Searchable: AI-native Multi-Modal Retrieval System with Learned Image Compression

## Quick Facts
- arXiv ID: 2404.10234
- Source URL: https://arxiv.org/abs/2404.10234
- Authors: Jixiang Luo
- Reference count: 32
- Primary result: Unified framework integrating learned image compression with zero-shot retrieval using CLIP alignment adapter

## Executive Summary
This paper introduces a unified framework that integrates learned image compression (LIC) with zero-shot retrieval capabilities for efficient multi-modal search. The key innovation is leveraging the image encoder of Contrastive Language-Image Pretraining (CLIP) to bridge the gap between compressed image features and semantic information, enabling both efficient storage and retrieval of multimedia data. An adapter with multi-scale resolution alignment modules is introduced to align LIC features with CLIP embeddings, demonstrating significant improvements in both compression efficiency and search accuracy on Kodak datasets.

## Method Summary
The framework consists of two main components: a learned image compression system based on ELIC [7] and a multi-scale adapter that aligns compressed features with CLIP embeddings. The method uses a two-phase training approach: first training the LIC model on MSCOCO dataset with rate-distortion loss, then integrating CLIP features with additional cosine similarity loss for the adapter. The multi-scale adapter employs resolution alignment modules to handle the varying spatial dimensions from LIC features, ensuring proper alignment with CLIP's fixed-length token embeddings. The total loss function combines traditional compression terms (rate and distortion) with semantic alignment loss.

## Key Results
- Demonstrated significant improvements in compression efficiency (measured in bpp) on Kodak datasets
- Achieved enhanced search accuracy (measured by hit rate) while maintaining compressed storage
- Showed effective integration of compression and retrieval capabilities in a unified framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The adapter bridges semantic gap between LIC features and CLIP embeddings by aligning multi-scale resolutions.
- Mechanism: The encoder of LIC (ga) extracts high-dimensional features with variable spatial resolution, while CLIP's image encoder expects fixed-length token embeddings. The adapter uses three resolution alignment modules (global pooling + 2D convolutions) to match the feature dimensions and semantics before passing them through an MLP for final alignment.
- Core assumption: LIC features, though compressed, retain enough semantic structure to be mapped into CLIP's embedding space via learnable transformations.
- Evidence anchors:
  - [abstract] "An adapter with multi-scale resolution alignment modules is introduced to align LIC features with CLIP embeddings."
  - [section] "we introduce an adapter as shown in the right of Fig. 2 to bridge the gap between LIC and CLIP features"
  - [corpus] No direct evidence; inferred from method description.
- Break condition: If the resolution mismatch cannot be resolved by the adapter, or if LIC features are too lossy for meaningful semantic alignment.

### Mechanism 2
- Claim: Joint training with both compression loss (R + λD) and semantic alignment loss (Ds) improves both compressibility and searchability.
- Mechanism: The total loss combines traditional learned image compression terms (rate and distortion) with a semantic alignment term that minimizes cosine distance between CLIP's embeddings and the adapter's output. This forces the compressed representation to preserve both perceptual fidelity and semantic content.
- Core assumption: Semantic fidelity can be preserved under compression constraints without significant loss of retrieval accuracy.
- Evidence anchors:
  - [section] "the final loss function is: L = R + λ ∗ D + λs ∗ Ds"
  - [abstract] "Experimental results on Kodak datasets demonstrate significant improvements in compression efficiency and search accuracy"
  - [corpus] No direct evidence; assumed from training description.
- Break condition: If the semantic loss term dominates and causes divergence in compression performance, or if λs is too low to have any effect.

### Mechanism 3
- Claim: Reusing LIC encoder features during retrieval reduces computational overhead compared to separate feature extraction pipelines.
- Mechanism: By storing only the compressed bitstream from LIC and extracting embeddings on-the-fly during search, the system avoids maintaining a separate embedding database. The adapter is lightweight (MLP + conv layers), so feature extraction is efficient.
- Core assumption: The computational cost of the adapter is negligible compared to separate feature extraction and storage.
- Evidence anchors:
  - [section] "we streamline the process by consolidating the image database and embedding base into a unified database"
  - [abstract] "Experimental results on Kodak datasets demonstrate significant improvements in compression efficiency"
  - [corpus] No direct evidence; inferred from system description.
- Break condition: If the adapter becomes a bottleneck during retrieval, or if compression artifacts severely degrade embedding quality.

## Foundational Learning

- Concept: Learned Image Compression (LIC) pipeline
  - Why needed here: Understanding how LIC encoders/decoders work is essential to see why their features can be repurposed for retrieval.
  - Quick check question: What are the three main components of an LIC system, and what role does each play in the compression process?

- Concept: Contrastive Language-Image Pretraining (CLIP) embedding space
  - Why needed here: CLIP's joint image-text embeddings are the target space for alignment; understanding their structure explains why the adapter can bridge modalities.
  - Quick check question: How does CLIP's image encoder differ structurally from a typical LIC encoder, and why does this matter for retrieval?

- Concept: Multi-scale feature alignment
  - Why needed here: The adapter uses multi-scale resolution alignment to handle variable spatial dimensions from LIC; knowing why this is necessary prevents misconfiguration.
  - Quick check question: Why does LIC produce features with varying spatial resolution, and how does the adapter reconcile this with CLIP's fixed-length input?

## Architecture Onboarding

- Component map: Input image -> LIC Encoder (ga) -> Multi-scale Adapter -> CLIP-aligned embedding; embedding -> retrieval (cosine similarity); compressed bitstream -> LIC Decoder (gs) -> reconstructed image
- Critical path:
  1. Input image → LIC Encoder → Multi-scale Adapter → CLIP-aligned embedding
  2. Embedding → retrieval (cosine similarity)
  3. Compressed bitstream → LIC Decoder → reconstructed image
- Design tradeoffs:
  - Compression vs. retrieval accuracy: higher compression reduces feature fidelity, hurting search
  - Adapter complexity vs. speed: more complex adapters improve alignment but slow inference
  - Fixed vs. variable resolution: fixed simplifies design but loses spatial adaptability
- Failure signatures:
  - Low retrieval recall: adapter alignment is insufficient or semantic loss weight is too low
  - High bitrate with poor quality: distortion term in loss is underweighted
  - Slow inference: adapter is too deep or resolution alignment is inefficient
- First 3 experiments:
  1. Train adapter with only semantic loss (Ds), no compression loss, to verify alignment capability.
  2. Vary λs in joint loss to find sweet spot between compression and retrieval accuracy.
  3. Compare retrieval accuracy using raw LIC features vs. adapter-aligned features on Kodak dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multi-scale adapter in the proposed framework specifically improve the alignment between LIC features and CLIP embeddings compared to a single-scale approach?
- Basis in paper: [explicit] The paper introduces a multi-scale adapter with resolution alignment modules to address the limitation of single-scale adapters in aligning LIC features with CLIP embeddings.
- Why unresolved: The paper mentions the introduction of the multi-scale adapter but does not provide detailed experimental comparisons

## Limitations
- Multi-scale adapter architecture details are underspecified, making exact reproduction difficult
- Only evaluated on Kodak dataset (small, homogeneous dataset) and MSCOCO (same-domain training)
- No comparison against traditional compressed-domain retrieval methods that might be more efficient
- Trade-off analysis between compression rate and retrieval accuracy is qualitative rather than quantified

## Confidence

**Major uncertainties and limitations:**
- The multi-scale adapter architecture details are underspecified, making exact reproduction difficult
- No ablation studies showing the isolated contribution of each alignment module
- Only evaluated on Kodak dataset (small, homogeneous dataset) and MSCOCO (same-domain training)
- No comparison against traditional compressed-domain retrieval methods that might be more efficient
- The trade-off analysis between compression rate and retrieval accuracy is qualitative rather than quantified across different λs values

**Confidence labels:**
- High confidence: The core mechanism of using an adapter to align LIC features with CLIP embeddings is sound and well-explained
- Medium confidence: The claim of "significant improvements" is supported by Kodak dataset results, but generalizability to diverse datasets is unknown
- Medium confidence: The compression-retrieval efficiency claim is demonstrated, but absolute performance metrics relative to state-of-the-art are not provided

## Next Checks

1. **Ablation study**: Remove each resolution alignment module individually and measure the impact on retrieval accuracy to verify their necessity
2. **Cross-dataset evaluation**: Test the trained model on a completely different dataset (e.g., ImageNet) to assess generalization
3. **Trade-off analysis**: Systematically vary λs values and plot the Pareto frontier between bits per pixel and retrieval accuracy to quantify the compression-retrieval trade-off