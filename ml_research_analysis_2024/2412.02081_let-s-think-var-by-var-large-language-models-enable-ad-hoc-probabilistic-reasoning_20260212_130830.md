---
ver: rpa2
title: 'Let''s Think Var-by-Var: Large Language Models Enable Ad Hoc Probabilistic
  Reasoning'
arxiv_id: '2412.02081'
source_url: https://arxiv.org/abs/2412.02081
tags:
- variables
- type
- values
- room
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework that uses large language models
  (LLMs) to synthesize ad hoc probabilistic models for guesstimation questions, such
  as "How much are Airbnb listings in Newark, NJ?" The method prompts an LLM to propose
  relevant random variables and moment constraints, then optimizes a log-linear model
  to satisfy these constraints. Evaluated on three real-world tabular datasets (Inside
  Airbnb, American Time-Use Survey, and World Values Survey), the approach performs
  comparably to a direct prompting baseline in terms of total variation distance from
  the dataset distribution, and is similarly robust to noise.
---

# Let's Think Var-by-Var: Large Language Models Enable Ad Hoc Probabilistic Reasoning

## Quick Facts
- arXiv ID: 2412.02081
- Source URL: https://arxiv.org/abs/2412.02081
- Authors: Shepard Xia; Brian Lu; Jason Eisner
- Reference count: 40
- One-line primary result: Framework performs comparably to direct LLM prompting on guesstimation questions using TVD metric

## Executive Summary
This paper introduces a framework that uses large language models (LLMs) to synthesize ad hoc probabilistic models for guesstimation questions. The method prompts an LLM to propose relevant random variables and moment constraints, then optimizes a log-linear model to satisfy these constraints. Evaluated on three real-world tabular datasets, the approach performs comparably to a direct prompting baseline in terms of total variation distance from the dataset distribution, and is similarly robust to noise.

## Method Summary
The method involves three stages of prompting: (1) brainstorming relevant variables, (2) choosing quantities to constrain by identifying interacting variable pairs, and (3) eliciting numerical constraints. An LLM is used at each stage to propose variables, interactions, and probability values based on the guesstimation question. A log-linear model is then optimized to satisfy the elicited constraints using fuzzy maximum entropy optimization. The resulting model can answer the original question through exact inference via brute force summation.

## Key Results
- Framework performs comparably to direct LLM prompting on TVD metric across three datasets
- Robustness to noise is similar between the proposed method and baseline
- Method shows promise for more complex guesstimation tasks where LLMs lack direct knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM can extract relevant variables and conditional relationships that capture structure needed for probabilistic reasoning
- Mechanism: LLM proposes random variables and their interactions to construct a factor graph where edges represent conditional probability constraints
- Core assumption: LLM has sufficient world knowledge about variable relationships in the domain
- Evidence anchors: Abstract and section discussing moment-matching constraints from LLM

### Mechanism 2
- Claim: Fuzzy maximum entropy optimization reconciles noisy LLM constraints to produce coherent distributions
- Mechanism: Optimization balances constraint satisfaction with entropy maximization, finding distributions that best satisfy all constraints while remaining smooth
- Core assumption: Constraints collectively contain enough signal about true distribution despite individual noise
- Evidence anchors: Abstract discussing comparable performance and robustness, section on optimization objective

### Mechanism 3
- Claim: System answers probabilistic queries via exact inference on constructed log-linear model
- Mechanism: Once optimized, probabilistic inference via brute force summation computes conditional distributions over target variables
- Core assumption: Optimized model captures enough of true joint distribution for reasonable conditional probabilities
- Evidence anchors: Abstract and section on exact computation of conditional probabilities

## Foundational Learning

- Concept: Probabilistic graphical models (factor graphs, conditional probability distributions)
  - Why needed here: Entire system builds and queries ad hoc graphical model from LLM-proposed variables and constraints
  - Quick check question: What is the difference between a joint distribution and a conditional distribution in a factor graph?

- Concept: Maximum entropy principle and optimization under constraints
  - Why needed here: System uses fuzzy maximum entropy to find distribution satisfying noisy constraints while remaining uniform
  - Quick check question: Why does maximizing entropy help when dealing with uncertain or incomplete information?

- Concept: Total variation distance as metric for comparing probability distributions
  - Why needed here: Evaluation metric measures how close system's answers are to true dataset distribution
  - Quick check question: How does total variation distance differ from KL divergence when comparing distributions?

## Architecture Onboarding

- Component map: Natural language question parser -> Variable brainstorming module (LLM) -> Interaction proposal module (LLM) -> Numerical constraint elicitation (LLM) -> Log-linear model optimizer -> Inference engine

- Critical path: Question → Variable brainstorming → Interaction proposal → Numerical elicitation → Optimization → Inference

- Design tradeoffs:
  - Variable selection: More variables increase expressiveness but risk noise and computational complexity
  - Constraint granularity: Finer-grained constraints improve accuracy but increase elicitation cost and optimization difficulty
  - Model family choice: Log-linear models are tractable but may not capture all dependencies
  - Optimization method: Exact inference is feasible for small models but requires approximations for larger ones

- Failure signatures:
  - Poor variable selection → disconnected graph, missing key dependencies
  - Inconsistent constraint directions → optimization struggles, may produce unrealistic distributions
  - Noisy constraints → final distribution averages out to something uninformative
  - Overfitting to constraints → model captures LLM biases rather than true relationships

- First 3 experiments:
  1. Run full pipeline on simple Airbnb question with known answer to verify each component works
  2. Compare performance with and without optimization step to measure value of constraint reconciliation
  3. Test intervention experiments by randomly modifying proposed variables or constraints to understand system robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does framework outperform direct LLM prompting on more complex guesstimation questions where LLMs lack direct knowledge?
- Basis in paper: [explicit] Paper states need for more difficult problems to show value, framework only helps when LLM doesn't know answer
- Why unresolved: Current experiments used straightforward questions where baseline performed well
- What evidence would resolve it: Experiments on harder questions where LLMs struggle, comparing framework performance against direct prompting

### Open Question 2
- Question: How can accuracy of LLM-proposed constraints be improved, and what is optimal weighting strategy?
- Basis in paper: [explicit] Constraints can be noisy; future work suggested on confidence estimation to upweight accurate constraints
- Why unresolved: Current implementation uses uniform weights; no analysis of constraint accuracy or confidence
- What evidence would resolve it: Systematic evaluation of constraint accuracy, experiments with different weighting strategies, impact analysis on final predictions

### Open Question 3
- Question: What is impact of variable selection and interaction direction choices on performance?
- Basis in paper: [explicit] Intervention experiments showed perturbing choices didn't significantly affect average gap, but LLM may not have made best choices
- Why unresolved: Single set of prompts used without exploring alternative strategies or optimizing these choices
- What evidence would resolve it: Systematic exploration of different prompting strategies, ablation studies, comparison with oracle selections

## Limitations

- Performance comparable to direct prompting only on relatively straightforward questions
- Log-linear model family may be too restrictive for complex dependencies
- Uncertainty about effectiveness on complex scenarios where LLMs lack direct knowledge

## Confidence

**High confidence**: Theoretical framework for using LLMs to synthesize probabilistic models is sound; three-stage prompting approach is clearly specified

**Medium confidence**: Framework performs comparably to direct prompting on evaluated datasets; robustness to noise claim supported by similar TVD scores

**Low confidence**: Claim about usefulness for complex guesstimation tasks where LLMs lack direct knowledge; this is extrapolation beyond current experimental scope

## Next Checks

1. **Complexity test**: Evaluate framework on questions requiring multi-hop reasoning or rare events unlikely in LLM training data, measuring performance degradation vs direct prompting

2. **Dependency capture test**: Systematically compare model performance when varying number and type of LLM-proposed variables, measuring capture of true dependencies

3. **Noise robustness test**: Quantify framework performance changes as function of constraint noise level by artificially corrupting LLM's numerical constraint outputs and measuring TVD degradation