---
ver: rpa2
title: 'Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language
  Models'
arxiv_id: '2402.02207'
source_url: https://arxiv.org/abs/2402.02207
tags:
- safety
- fine-tuning
- llav
- a-v1
- vllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Current VLLMs suffer from safety issues due to harmful data in
  training and forgetting of prior alignment. This work addresses the problem by curating
  VLGuard, a vision-language safety dataset covering multiple harmful categories.
---

# Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models

## Quick Facts
- arXiv ID: 2402.02207
- Source URL: https://arxiv.org/abs/2402.02207
- Authors: Yongshuo Zong; Ondrej Bohdal; Tingyang Yu; Yongxin Yang; Timothy Hospedales
- Reference count: 29
- Key outcome: Safety fine-tuning using VLGuard dataset reduces attack success rates on VLLMs to near zero while maintaining helpfulness

## Executive Summary
Current vision-language models (VLLMs) face significant safety challenges due to harmful data in training and forgetting of prior alignment. This work introduces VLGuard, a comprehensive vision-language safety dataset covering multiple harmful categories, and demonstrates that safety fine-tuning using this dataset can dramatically improve VLLM safety. The approach achieves near-zero attack success rates on both text-based and vision-language prompts while maintaining or enhancing model helpfulness. The method is computationally efficient, with negligible overhead, making it a practical baseline for VLLM safety enhancement.

## Method Summary
The method involves curating VLGuard, a vision-language safety dataset with 2,000 training images and 1,000 test images covering various harmful categories. VLLMs are then fine-tuned using this dataset through either post-hoc or mixed strategies, with LoRA or full parameter updates. The fine-tuning process overwrites harmful data patterns with safe instruction-response pairs, effectively re-aligning the model to refuse harmful requests while maintaining response quality. The approach can be integrated into original training or applied as a separate step, achieving significant safety improvements with minimal computational overhead.

## Key Results
- Safety fine-tuning with VLGuard reduces attack success rates to near zero on standard benchmarks
- The approach maintains or enhances model helpfulness on MMLU and AlpacaEval
- Mixed fine-tuning strategy achieves safety improvements with only 0.1-0.3% additional safety data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Safety fine-tuning with VLGuard restores safety alignment that is lost during VLLM fine-tuning.
- Mechanism: The fine-tuning process overwrites harmful data patterns with safe instruction-response pairs, effectively re-aligning the model to refuse harmful requests.
- Core assumption: The model can learn to distinguish safe from unsafe content when provided explicit examples in the fine-tuning data.
- Evidence anchors:
  - [abstract] "fine-tuning VLLMs on our dataset achieves significant improvement in safety while resulting in negligible or no helpfulness degradation"
  - [section 2.3] "Fine-tuned VLLMs exhibit an increased tendency to accept instructions, regardless of their potentially harmful nature" (before fine-tuning)
  - [section 3.2] "fine-tuning on our VLGuard dataset significantly reduces the harmfulness of models"

### Mechanism 2
- Claim: Post-hoc fine-tuning with VLGuard plus a small amount of helpfulness data avoids exaggerated safety while maintaining effectiveness.
- Mechanism: The combination of safety data (to learn refusal patterns) and helpfulness data (to maintain response quality) balances the model's behavior, preventing it from over-refusing benign requests.
- Core assumption: The model can maintain helpfulness when exposed to both safety and helpfulness examples during fine-tuning.
- Evidence anchors:
  - [section 4.4] "incorporating additional helpfulness data still introduces additional computational needs... fine-tuning with safety data alone does not compromise helpfulness, but leads to an exaggerated safety tendency"
  - [section 4.4] "integrating either 5000 randomly sampled original LLaV A data or even pure-text Alpaca data... the fine-tuned model strikes a better balance between safety and helpfulness"

### Mechanism 3
- Claim: Mixed fine-tuning (integrating safety data into original training) achieves safety improvements with minimal computational overhead.
- Mechanism: Adding a small percentage of safety data to the existing training corpus allows the model to learn safety patterns during the primary training phase, rather than as a separate step.
- Core assumption: The model can effectively learn from a small proportion of safety data when mixed with large amounts of general instruction data.
- Evidence anchors:
  - [section 4.3] "our safety data constitutes a very small fraction of the total training data – specifically,0.3% for LLaV A-v1.5 stage 2 and 0.1% for MiniGPT-v2 stage 3 – there is a significant improvement in safety"
  - [section 4.3] "blending our safety data even increases the helpfulness of the trained model in most cases"

## Foundational Learning

- Concept: Fine-tuning mechanisms (full fine-tuning vs LoRA)
  - Why needed here: Understanding how different fine-tuning approaches affect safety alignment and computational efficiency
  - Quick check question: What is the key difference between full fine-tuning and LoRA in terms of parameter updates?

- Concept: Multimodal safety considerations
  - Why needed here: VLLMs face safety risks from both text and visual inputs, requiring specialized evaluation and fine-tuning approaches
  - Quick check question: How does the addition of visual modality create new safety risks compared to text-only models?

- Concept: Evaluation metrics for safety alignment
  - Why needed here: Proper assessment of safety improvements requires understanding attack success rate, refusal patterns, and generalization to unseen categories
  - Quick check question: Why is it important to test both Safe-Unsafe and Unsafe subsets when evaluating VLLM safety?

## Architecture Onboarding

- Component map:
  - Vision encoder → Projection layer → LLM backbone → Safety alignment layer (VLGuard fine-tuning)
  - Separate evaluation pipelines for text-only, vision-only, and multimodal inputs

- Critical path:
  1. Identify harmful patterns in training data
  2. Curate VLGuard dataset with balanced safe/unsafe examples
  3. Apply post-hoc or mixed fine-tuning strategy
  4. Evaluate across multiple attack vectors and benchmarks

- Design tradeoffs:
  - Safety vs helpfulness balance (risk of over-refusal)
  - Computational cost vs fine-tuning effectiveness
  - Dataset size vs generalization capability

- Failure signatures:
  - High attack success rates on AdvBench or FigStep
  - Excessive refusal of benign requests (exaggerated safety)
  - Failure to generalize to unseen harmful categories

- First 3 experiments:
  1. Fine-tune LLaVA-v1.5-7B with VLGuard using post-hoc approach, evaluate on Safe-Unsafe and Unsafe subsets
  2. Compare full fine-tuning vs LoRA on the same model for computational efficiency and safety effectiveness
  3. Test generalization by fine-tuning without privacy-related samples, then evaluate on privacy category

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the safety fine-tuning approach perform against more sophisticated and intricate attack methods that were not explicitly tested in the study?
- Basis in paper: The paper mentions that the fine-tuning method aims to introduce an initial safety layer but acknowledges it may not be impervious to more sophisticated attacks.
- Why unresolved: The paper only tests the model against specific black-box attacks (AdvBench, XSTest, FigStep) and does not explore the full spectrum of potential attack methods.
- What evidence would resolve it: Testing the fine-tuned models against a broader range of attack methods, including white-box attacks and more advanced adversarial techniques, would provide evidence of the approach's robustness.

### Open Question 2
- Question: What is the impact of scaling the size of the VLGuard training dataset on the safety and helpfulness of the fine-tuned models?
- Basis in paper: The paper mentions that the current training set consists of 2,000 images and suggests that scaling to a larger dataset is left as future work.
- Why unresolved: The paper does not explore how increasing the size of the training dataset affects the performance of the fine-tuned models.
- What evidence would resolve it: Conducting experiments with larger versions of the VLGuard dataset and comparing the results to the current findings would provide insights into the relationship between dataset size and model performance.

### Open Question 3
- Question: How well does the safety fine-tuning generalize to entirely new categories of harmful content not seen during training?
- Basis in paper: The paper conducts an experiment with a subsample of the safety dataset with privacy-related samples removed to test generalization, but it does not explore generalization to entirely new categories.
- Why unresolved: The experiment focuses on a held-out subset of the existing dataset rather than introducing completely new categories of harmful content.
- What evidence would resolve it: Testing the fine-tuned models on a dataset containing entirely new categories of harmful content not present in the original VLGuard dataset would provide evidence of the approach's generalization capabilities.

## Limitations

- Limited exploration of sophisticated adversarial attacks beyond standard benchmarks
- Dataset may not capture all possible harmful scenarios, particularly complex multi-modal threats
- No temporal analysis of long-term robustness or safety alignment degradation

## Confidence

- **High**: Effectiveness of VLGuard fine-tuning in reducing attack success rates and maintaining helpfulness on evaluated benchmarks
- **Medium**: Generalization claims, as the study shows strong performance on test sets but limited exploration of truly unseen harmful categories
- **Low**: Long-term robustness of safety alignment, as no temporal degradation analysis was conducted

## Next Checks

1. **Cross-model generalization test**: Apply VLGuard fine-tuning to a diverse set of VLLM architectures (different vision encoders, varying model sizes) to verify the approach's effectiveness beyond the two models tested.

2. **Long-term stability evaluation**: Implement a temporal analysis by periodically re-evaluating safety alignment over extended time periods to identify any degradation or adaptation by potential attackers.

3. **Adversarial robustness benchmarking**: Design and implement more sophisticated adversarial attack scenarios that combine multiple modalities and complex reasoning to stress-test the safety alignment beyond standard attack patterns.