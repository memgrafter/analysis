---
ver: rpa2
title: 'Translatotron-V(ison): An End-to-End Model for In-Image Machine Translation'
arxiv_id: '2407.02894'
source_url: https://arxiv.org/abs/2407.02894
tags:
- image
- text
- decoder
- visual
- iimt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses in-image machine translation (IIMT), which
  translates an image containing text from a source language into an image containing
  the translated text in a target language. Traditional cascaded approaches suffer
  from error propagation, massive parameters, and difficulties retaining visual characteristics.
---

# Translatotron-V(ison): An End-to-End Model for In-Image Machine Translation

## Quick Facts
- arXiv ID: 2407.02894
- Source URL: https://arxiv.org/abs/2407.02894
- Authors: Zhibin Lan; Liqiang Niu; Fandong Meng; Jie Zhou; Min Zhang; Jinsong Su
- Reference count: 13
- Primary result: End-to-end IMT model achieves competitive performance with only 70.9% of cascaded model parameters

## Executive Summary
This paper introduces Translatotron-V, an end-to-end model for in-image machine translation (IIMT) that translates images containing text from a source language to target language images. The proposed architecture addresses key limitations of cascaded approaches including error propagation, massive parameters, and difficulties retaining visual characteristics. The model consists of four modules: an image encoder, target text decoder, image decoder, and image tokenizer. A two-stage training framework with knowledge distillation enables effective learning across modalities and languages.

## Method Summary
Translatotron-V is an end-to-end IMT model that uses discrete visual tokens to reduce output complexity. The two-stage training framework first trains an image tokenizer using unlabeled images, then trains the full model with multi-task learning (IIMT + OCR + TIT) and knowledge distillation from a pre-trained T2I model. The model is evaluated on synthetic IWSLT14 German-English datasets using BLEU, Structure-BLEU, and SSIM metrics.

## Key Results
- Translatotron-V achieves competitive performance with only 70.9% of the parameters compared to cascaded models
- Significantly outperforms pixel-level end-to-end IMT models
- Structure-BLEU metric effectively captures text location information in evaluation
- Two-stage training framework with knowledge distillation proves effective

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Target text decoder alleviates modeling burden by isolating language alignment from visual reconstruction
- Mechanism: The target text decoder processes visual features from the image encoder to produce target language text, while the image decoder focuses solely on visual token generation. This separation prevents the model from having to simultaneously optimize for both cross-lingual alignment and low-level visual reconstruction.
- Core assumption: Cross-modal alignment and cross-lingual alignment can be disentangled into separate subtasks without loss of overall task performance
- Evidence anchors:
  - [abstract] "the target text decoder is used to alleviate the language alignment burden"
  - [section 3.1] "By utilizing the features generated by the image encoder, this decoder is responsible for producing text translations. In this way, it focuses on the alignment of different languages, and thus alleviates the modeling burden of the image decoder."

### Mechanism 2
- Claim: Image tokenizer converts pixel sequences into visual tokens to reduce search space complexity
- Mechanism: The image tokenizer maps raw pixel values to discrete visual tokens using a VQ-VAE approach. This reduces the output space from 256^196608 (for 256×256 RGB images) to 8192 visual tokens, making the task tractable for sequence models.
- Core assumption: Discrete visual tokens preserve sufficient information for accurate image reconstruction while dramatically reducing complexity
- Evidence anchors:
  - [abstract] "the image tokenizer converts long sequences of pixels into shorter sequences of visual tokens, preventing the model from focusing on low-level visual features"
  - [section 3.1] "a 256 ×256×3 RGB image results in 196,608 rasterized values" versus "8,192 visual tokens"

### Mechanism 3
- Claim: Two-stage training framework enables effective knowledge transfer and modality alignment
- Mechanism: Stage 1 trains the image tokenizer on unlabeled images to learn visual token representations. Stage 2 trains the full model using multi-task learning (IIMT + OCR + TIT) with knowledge distillation from a pre-trained T2I model to provide guidance on image generation.
- Core assumption: Pre-training on unlabeled images and using auxiliary tasks creates useful intermediate representations that facilitate end-to-end learning
- Evidence anchors:
  - [abstract] "we present a two-stage training framework for our model to assist the model in learning alignment across modalities and languages"
  - [section 3.2] "multi-task learning at this stage. The auxiliary tasks include OCR and text image translation (TIT), assisting the model in learning alignment across different modalities and languages"

## Foundational Learning

- Concept: Vision Transformers (ViT) for image encoding
  - Why needed here: The model requires extracting meaningful visual features from images that can be processed by Transformer-based decoders
  - Quick check question: How does splitting an image into patches and applying positional embeddings enable ViT to process 2D images?

- Concept: Knowledge distillation for complex sequence generation
  - Why needed here: Direct end-to-end training of image generation is challenging; distillation provides guidance from a stronger model
  - Quick check question: Why might a T2I model serve as a better teacher for the image decoder than training from scratch?

- Concept: Discrete representation learning (VQ-VAE)
  - Why needed here: Converting continuous pixel values to discrete tokens reduces output complexity while preserving semantic information
  - Quick check question: What's the trade-off between vocabulary size and reconstruction quality in vector quantization?

## Architecture Onboarding

- Component map: Image → Image Encoder (ViT-B) → Target Text Decoder (8-layer Transformer) → Image Decoder (8-layer Transformer with gated fusion) → Image Tokenizer (ViT-VQGAN) → Output Image

- Critical path: Image → Image Encoder → Target Text Decoder → Image Decoder → Image Tokenizer → Output Image

- Design tradeoffs:
  - Using discrete visual tokens trades reconstruction fidelity for training tractability
  - Multi-task learning adds complexity but provides stronger supervision
  - Knowledge distillation helps but adds dependency on teacher model quality
  - Separating text and image generation modules increases parameter count but improves optimization

- Failure signatures:
  - Poor text generation despite good image quality: Target text decoder not learning language alignment properly
  - Missing or garbled characters: Image tokenizer quantization issues or image decoder visual token generation problems
  - Background degradation: Image decoder over-focusing on text regions or poor fusion of visual and linguistic features
  - Slow convergence: Insufficient auxiliary task supervision or knowledge distillation not effective

- First 3 experiments:
  1. Test image tokenizer reconstruction quality on held-out images to verify it captures sufficient visual information
  2. Evaluate target text decoder output quality (using BLEU) to ensure language alignment is working
  3. Verify the gated fusion mechanism in image decoder by comparing with a variant that doesn't use gating

## Open Questions the Paper Calls Out

- Question: How does Translatotron-V perform on real-world IIMT datasets compared to synthetic datasets?
  - Basis in paper: [inferred] The paper acknowledges that the synthetic dataset used is not realistic enough and acquiring IIMT data from the real world is challenging
  - Why unresolved: The paper only evaluates the model on a synthetic dataset and does not provide results on real-world IIMT data
  - What evidence would resolve it: Experiments comparing Translatotron-V's performance on real-world IIMT datasets versus synthetic datasets would resolve this question

- Question: Can Translatotron-V be trained using only parallel images without text annotations?
  - Basis in paper: [explicit] The paper mentions that training models using only parallel images is important when texts within the image are not available, and this is a direction they are interested in for future work
  - Why unresolved: The current model requires text annotations for training, and the paper does not explore training with only parallel images
  - What evidence would resolve it: Developing and evaluating a variant of Translatotron-V that can be trained using only parallel images without text annotations would resolve this question

- Question: How does the quality of the generated target images depend on the quality of the image tokenizer?
  - Basis in paper: [explicit] The paper acknowledges that the quality of generated target images depends on the quality of the image tokenizer and mentions that the tokenizer sometimes generates incorrect words
  - Why unresolved: The paper does not provide a detailed analysis of the relationship between the image tokenizer's quality and the generated image quality
  - What evidence would resolve it: A comprehensive analysis of how different image tokenizer qualities affect the generated image quality, including quantitative metrics and qualitative examples, would resolve this question

## Limitations

- The evaluation relies on a synthetic dataset with clean, centered text, which may not reflect real-world complexity where text appears in varied backgrounds, fonts, and layouts
- The absence of comparative results against cascaded approaches on real-world datasets leaves the practical advantage of the end-to-end approach unverified
- The knowledge distillation process depends on an unspecified teacher model architecture and training details, creating a potential reproducibility gap

## Confidence

**High Confidence**: The core technical contribution of using discrete visual tokens to reduce sequence complexity is well-supported by the architecture description and mathematical justification of search space reduction. The two-stage training framework with auxiliary tasks is clearly specified and theoretically sound.

**Medium Confidence**: The claim that the model achieves competitive performance with 70.9% of cascaded parameters is supported by experimental results, but the comparison is limited to synthetic data. The effectiveness of the gated fusion mechanism for combining visual and textual features is plausible but not extensively validated across different failure modes.

**Low Confidence**: The evaluation metric Structure-BLEU, while conceptually addressing the need to consider text location, lacks comparison with other potential metrics for this task. The paper does not address potential failure modes like background degradation or character recognition errors in detail.

## Next Checks

1. **Synthetic-to-Real Transfer**: Evaluate Translatotron-V on real-world in-image translation datasets (e.g., with varied fonts, backgrounds, and text orientations) to verify whether synthetic training generalizes to practical applications.

2. **Teacher Model Dependency**: Systematically test the impact of teacher model quality on final performance by training with different teacher architectures or removing knowledge distillation entirely to measure its contribution.

3. **Visual Token Ablation**: Conduct a controlled experiment varying the visual token vocabulary size (e.g., 4096, 8192, 16384) to quantify the trade-off between quantization accuracy and reconstruction quality, particularly for fine-grained text details.