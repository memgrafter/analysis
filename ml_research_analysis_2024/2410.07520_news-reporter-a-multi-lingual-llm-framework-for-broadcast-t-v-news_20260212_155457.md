---
ver: rpa2
title: 'News Reporter: A Multi-lingual LLM Framework for Broadcast T.V News'
arxiv_id: '2410.07520'
source_url: https://arxiv.org/abs/2410.07520
tags:
- news
- arxiv
- language
- llms
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces News Reporter, a multi-lingual LLM framework
  for broadcast TV news. It addresses the lack of verifiable and accurate answers
  for news queries by creating a large collection of QA pairs extracted from transcripts
  of news recordings from various US channels.
---

# News Reporter: A Multi-lingual LLM Framework for Broadcast T.V News

## Quick Facts
- arXiv ID: 2410.07520
- Source URL: https://arxiv.org/abs/2410.07520
- Authors: Tarun Jain; Yufei Gao; Sridhar Vanga; Karan Singla
- Reference count: 22
- Primary result: Multi-lingual LLM fine-tuned on broadcast news transcripts surpasses base models on open LLM benchmarks

## Executive Summary
This paper introduces News Reporter, a multi-lingual LLM framework specifically designed for broadcast TV news applications. The framework addresses the challenge of obtaining verifiable and accurate answers for news queries by creating a large collection of QA pairs extracted from news transcripts across US channels. These QA pairs are used to fine-tune an off-the-shelf LLM model, resulting in improved news-specific reasoning capabilities. The framework also integrates a RAG (Retrieval-Augmented Generation) method to enhance contextualization of answers and provide verifiable news sources.

## Method Summary
The News Reporter framework involves collecting broadcast news transcripts, converting them into QA pairs using Self-Instruct, and fine-tuning an LLM (specifically QLoRA adapters) on these pairs. The fine-tuned model is evaluated on open LLM benchmarks and shows performance gains over base models. A RAG pipeline is implemented using MPNET embeddings and Qdrant vector database for retrieval of relevant news segments at inference time. The framework supports multiple European languages including English, French, Spanish, German, and Portuguese.

## Key Results
- News Reporter-3B model surpasses base models of similar size on several open LLM benchmarks
- RAG integration with vector database of broadcast news shows significant performance gains for both base and fine-tuned models
- Multilingual fine-tuning demonstrates improved reasoning ability, particularly on ARC C benchmark measuring reasoning capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on broadcast news QA pairs improves news-specific reasoning and contextualization over generic LLMs
- Mechanism: The model learns news domain-specific patterns, language style, and question-answer structures by training on transcripts converted into QA pairs
- Core assumption: Broadcast news transcripts contain sufficient semantic and contextual richness to serve as effective fine-tuning data for news reasoning tasks
- Evidence anchors:
  - [abstract] "Our model surpasses base models of similar size on several open LLM benchmarks."
  - [section] "Fine-tuned model surpasses base models such as Gemma-7B, Llama-2-7B, and Mistral-7B open-source models on Open LLM benchmarking."
- Break condition: If the QA pairs lack diversity or contain too much noise, the model may not generalize well to unseen news topics or fail to capture nuanced context

### Mechanism 2
- Claim: RAG with vector database of news transcripts enhances answer accuracy and verifiability
- Mechanism: At inference, the query is converted to a vector, matched against transcript chunks, and top results are provided as context to the LLM for generation
- Core assumption: The vector embeddings capture semantic similarity well enough to retrieve contextually relevant news segments
- Evidence anchors:
  - [abstract] "We further integrate and propose a RAG method to improve contextualization of our answers and also point it to a verifiable news recording."
  - [section] "We observe significant performance gains for both the off-the-shelf and fine-tuned models when using RAG with a vector database of broadcast news."
- Break condition: If the vector database is too small or outdated, retrieved contexts may be irrelevant, causing the model to hallucinate or cite incorrect sources

### Mechanism 3
- Claim: Multilingual fine-tuning on European languages improves cross-lingual reasoning and robustness
- Mechanism: Training on QA pairs from English, French, Spanish, German, and Portuguese exposes the model to varied linguistic structures and news topics
- Core assumption: Multilingual news topics share enough conceptual overlap that training across languages transfers reasoning ability
- Evidence anchors:
  - [abstract] "We collect and share a large collection of QA pairs extracted from transcripts of news recordings from various news-channels across the United States."
  - [section] "Our European model shows best results for ARC C which measures reasoning ability of the model."
- Break condition: If language-specific nuances or idioms are not well-represented, cross-lingual transfer may degrade performance on certain topics

## Foundational Learning

- Concept: QA Pair Extraction from Transcripts
  - Why needed here: Converts raw broadcast transcripts into structured supervised data suitable for fine-tuning
  - Quick check question: How does Self-Instruct generate QA pairs from a news transcript without manual labeling?

- Concept: RAG Pipeline with Vector Embeddings
  - Why needed here: Enables retrieval of relevant news segments at inference time to ground LLM responses
  - Quick check question: What similarity metric is used to match query vectors with transcript chunk embeddings?

- Concept: Cross-Entropy Loss for Instruction Tuning
  - Why needed here: Trains the model to follow structured prompts and generate accurate news answers
  - Quick check question: Why is cross-entropy loss appropriate for sequence generation in this fine-tuning setup?

## Architecture Onboarding

- Component map: Raw transcript ingestion → Preprocessing (cleaning, metadata extraction) → QA pair generation (Self-Instruct) → Fine-tuning (QLoRA adapters, cross-entropy) → Vector database construction (MPNET embeddings, Qdrant) → RAG inference (query vectorization, top-k retrieval, context injection) → Answer generation
- Critical path: Transcript preprocessing → QA pair extraction → Fine-tuning → Vector database creation → RAG inference
- Design tradeoffs:
  - Using QLoRA adapters reduces memory usage but may limit full-model adaptation
  - Fixed-size transcript chunks balance retrieval speed and context relevance but may split related information
  - Cosine similarity is efficient but may not capture nuanced semantic differences as well as learned metrics
- Failure signatures:
  - Low context recall → vector database too small or embeddings poor
  - High answer irrelevance → QA pairs not diverse or fine-tuning data noisy
  - Degraded multilingual performance → imbalanced language representation
- First 3 experiments:
  1. Evaluate baseline LLM on a small held-out news QA set without fine-tuning
  2. Fine-tune on English QA pairs and measure MMLU news-relevant topics performance
  3. Add RAG retrieval and compare context recall and answer correctness metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the News Reporter model's performance compare to other fine-tuned LLMs specifically designed for news-related tasks?
- Basis in paper: [inferred] The paper mentions fine-tuning an off-the-shelf LLM model and comparing it to other base models like Gemma-7B, Llama-3-8B, and Mistral-7B on various benchmarks. However, it does not compare the News Reporter model to other LLMs that have been specifically fine-tuned for news tasks.
- Why unresolved: The paper does not provide a direct comparison between the News Reporter model and other news-specific LLMs, making it difficult to assess its relative performance in this domain.
- What evidence would resolve it: A comparison of the News Reporter model's performance on news-related benchmarks against other LLMs that have been specifically fine-tuned for news tasks.

### Open Question 2
- Question: How does the News Reporter model handle real-time news updates and breaking news situations?
- Basis in paper: [explicit] The paper mentions that the model is trained on a dataset covering news broadcasts from 2016, but it does not discuss how the model would handle real-time news updates or breaking news situations.
- Why unresolved: The paper does not provide information on how the model would adapt to rapidly changing news scenarios or incorporate new information as it becomes available.
- What evidence would resolve it: A demonstration of the model's ability to incorporate and generate accurate responses for real-time news updates and breaking news situations.

### Open Question 3
- Question: How does the News Reporter model handle complex, multi-faceted news stories that require understanding of context and nuance?
- Basis in paper: [inferred] The paper mentions that the model is fine-tuned on news transcripts and uses a RAG pipeline to improve contextualization. However, it does not discuss how the model handles complex news stories that require a deep understanding of context and nuance.
- Why unresolved: The paper does not provide information on how the model would handle news stories that involve multiple perspectives, conflicting information, or require a nuanced understanding of the situation.
- What evidence would resolve it: A demonstration of the model's ability to accurately summarize and respond to complex, multi-faceted news stories, taking into account context and nuance.

## Limitations

- Evaluation relies primarily on open LLM benchmarks rather than domain-specific news QA datasets
- Multilingual claims based only on five European languages with no validation on non-European or low-resource languages
- Corpus neighbors analysis shows limited peer validation with average neighbor FMR of 0.482 and zero citations

## Confidence

**High Confidence**: The core technical approach of fine-tuning LLMs on domain-specific QA pairs and integrating RAG for contextualization is sound and well-established in the literature. The claim that fine-tuned models outperform base models on open benchmarks is directly supported by reported results.

**Medium Confidence**: The specific performance gains on news-relevant topics and the effectiveness of the multilingual fine-tuning approach are moderately supported but would benefit from more rigorous domain-specific evaluation. The RAG integration shows promise but lacks detailed quantitative analysis of retrieval quality.

**Low Confidence**: The generalizability of results to diverse news domains, the quality of automatically generated QA pairs, and the robustness of multilingual reasoning across language families are weakly supported and require further validation.

## Next Checks

1. **Domain-Specific Evaluation**: Create and evaluate on a held-out test set of news QA pairs from broadcast transcripts not used in training, measuring both retrieval accuracy (context recall@K) and answer correctness (faithfulness and relevance scores).

2. **QA Pair Quality Analysis**: Manually audit a stratified sample of generated QA pairs for factual accuracy, question diversity, and answer completeness to assess the reliability of the Self-Instruct pipeline.

3. **Cross-Lingual Transfer Study**: Evaluate model performance on news topics where training data exists in some languages but not others to quantify the actual transfer learning benefits of multilingual fine-tuning.