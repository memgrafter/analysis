---
ver: rpa2
title: 'Determine-Then-Ensemble: Necessity of Top-k Union for Large Language Model
  Ensembling'
arxiv_id: '2410.03777'
source_url: https://arxiv.org/abs/2410.03777
tags:
- ensembling
- trees
- performance
- vocabulary
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of ensemble learning
  for large language models (LLMs) by analyzing key factors such as model performance,
  vocabulary size, and response style. It identifies that significant performance
  gaps between base models can hinder ensembling, while vocabulary size has minimal
  impact.
---

# Determine-Then-Ensemble: Necessity of Top-k Union for Large Language Model Ensembling

## Quick Facts
- arXiv ID: 2410.03777
- Source URL: https://arxiv.org/abs/2410.03777
- Authors: Yuxuan Yao; Han Wu; Mingyang Liu; Sichun Luo; Xiongwei Han; Jie Liu; Zhijiang Guo; Linqi Song
- Reference count: 30
- One-line primary result: UNITE achieves consistent improvements in accuracy while significantly reducing latency and token manipulation compared to existing ensemble methods.

## Executive Summary
This study investigates the effectiveness of ensemble learning for large language models by analyzing key factors such as model performance, vocabulary size, and response style. The research identifies that significant performance gaps between base models can hinder ensembling effectiveness, while vocabulary size has minimal impact. To address these challenges, the authors introduce Union Top-k Ensembling (UNITE), a method that focuses on the top-k tokens from each model to reduce computational overhead while maintaining accuracy. UNITE outperforms existing methods across multiple benchmarks, achieving consistent improvements in accuracy while significantly reducing latency and token manipulation.

## Method Summary
The paper proposes Union Top-k Ensembling (UNITE), a novel approach for combining multiple large language models that addresses computational efficiency while maintaining accuracy. UNITE operates by selecting the top-k tokens from each base model's probability distribution, constructing a union vocabulary from these tokens, and then aligning token probabilities using specialized tokenization strategies. The method averages normalized probabilities across models and selects the next token via argmax. A key innovation is the specialized token probability alignment that tokenizes missing tokens using each model's tokenizer rather than simply setting probabilities to zero, enabling more effective combination of models with different vocabularies.

## Key Results
- UNITE achieves consistent improvements in accuracy across GSM8K, MMLU, and MATH benchmarks compared to individual base models
- The method significantly reduces computational overhead by focusing on top-k tokens rather than full vocabulary alignment
- Model compatibility (performance alignment within 10% and similar response styles) is essential for effective ensembling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model compatibility is essential for effective ensembling
- Mechanism: When base models have similar performance levels and compatible response styles, ensembling reduces prediction variance and improves accuracy by combining complementary strengths
- Core assumption: Base models share similar underlying knowledge representations when performance and response style are aligned
- Evidence anchors:
  - [abstract] "identifying model performance, vocabulary size, and response style as key determinants, revealing that compatibility among models is essential for effective ensembling"
  - [section 3.1] "When there is a significant performance disparity between models, LLM-B LENDER, which selects the optimal response from model candidates, is not suitable for model ensembling"
- Break condition: Performance gaps exceeding 10% or incompatible response styles (e.g., one model provides direct answers while another provides lengthy analysis)

### Mechanism 2
- Claim: Top-k union approach reduces computational overhead while maintaining accuracy
- Mechanism: By focusing only on the top-k tokens from each model and constructing a union vocabulary, the method avoids full vocabulary alignment while capturing the most probable candidates
- Core assumption: The correct next token typically appears within the top-k tokens of at least one model
- Evidence anchors:
  - [abstract] "introduces the Union Top-k Ensembling (UNITE), which focuses on the union of the top-k tokens from each model to reduce computational overhead"
  - [section 4.2] "We argue this approach is sub-optimal since the candidate next token typically resides within the top-k tokens"
- Break condition: When the correct next token consistently appears outside the top-k tokens for all models

### Mechanism 3
- Claim: Specialized token probability alignment improves over simple zero-filling for missing tokens
- Mechanism: Instead of setting probabilities to zero for tokens in the union set but not in individual model vocabularies, the method tokenizes these tokens using each model's tokenizer and assigns appropriate probabilities
- Core assumption: Tokenization differences between models can be bridged by breaking down missing tokens into components
- Evidence anchors:
  - [section 4.2] "we present a new token probability alignment strategy on the union set" and "If the token w in V u does not exist in V i, it should be tokenized by T i"
- Break condition: When tokenization differences are too fundamental to bridge (e.g., different tokenization strategies like BPE vs WordPiece for the same token)

## Foundational Learning

- Concept: Probability distribution manipulation in language models
  - Why needed here: The method fundamentally relies on manipulating probability distributions from multiple models rather than just combining outputs
  - Quick check question: What is the computational complexity difference between aligning full vocabularies versus top-k tokens in language model ensembling?

- Concept: Tokenization strategies and vocabulary design
  - Why needed here: The method needs to handle tokens that exist in one model's vocabulary but not another's, requiring understanding of how different tokenization strategies work
  - Quick check question: How does a tokenization strategy like Byte-Pair Encoding differ from WordPiece in handling out-of-vocabulary words?

- Concept: Ensemble methods and variance reduction
  - Why needed here: The method is fundamentally about combining multiple models to reduce variance and improve robustness
  - Quick check question: Under what conditions does model ensembling improve performance versus harm it?

## Architecture Onboarding

- Component map:
  Base LLM models (LLMi) -> Tokenizers (Ti) for each model -> Top-k selection mechanism -> Union vocabulary construction -> Token probability alignment module -> Aggregation function (average) -> Primary model selector -> Stopping criterion handler

- Critical path:
  1. Generate top-k tokens and probabilities from each base model
  2. Construct union vocabulary from top-k tokens
  3. Align token probabilities across models using specialized tokenization
  4. Normalize probabilities
  5. Average normalized probabilities
  6. Select next token via argmax
  7. Append to prompt and repeat until stopping criterion

- Design tradeoffs:
  - Top-k value selection (balance between coverage and efficiency)
  - Primary model selection strategy (best performance vs other criteria)
  - Stopping criterion choice (end-of-sentence token vs maximum length)
  - Deterministic vs sampling-based next token selection

- Failure signatures:
  - Performance degradation when performance gaps exceed 10%
  - Inconsistent results when response styles differ significantly
  - Computational inefficiency if k is set too high
  - Memory issues with very large vocabularies

- First 3 experiments:
  1. Validate that UNITE outperforms individual models when performance gaps are within 10% using GSM8K benchmark
  2. Test the impact of different k values (5, 10, 20) on both performance and latency
  3. Verify that vocabulary size differences don't significantly impact performance by testing with models having 32K vs 102K vocabularies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact threshold for model performance gaps that makes ensemble learning ineffective or counterproductive?
- Basis in paper: The paper identifies that when the performance gap between models exceeds 10%, ensemble learning may yield little to no improvement or even degrade performance.
- Why unresolved: The paper provides empirical observations but does not establish a precise mathematical or theoretical framework for determining this threshold across different task types or model architectures.
- What evidence would resolve it: Systematic experiments varying performance gaps across multiple task categories and model pairs, combined with theoretical analysis of error propagation in ensemble methods.

### Open Question 2
- Question: How does the optimal value of k (number of top tokens considered) vary with model size, task complexity, and vocabulary size?
- Basis in paper: The paper tests k values from 5 to 10,000 and observes performance plateaus, but does not provide a principled method for selecting k.
- Why unresolved: The paper shows empirical results but does not explain the underlying relationship between k, model characteristics, and task demands.
- What evidence would resolve it: Comprehensive ablation studies across diverse model scales, task complexities, and vocabulary sizes, potentially revealing patterns or formulas for k selection.

### Open Question 3
- Question: Can the model selection strategy be automated and generalized beyond the simple performance gap and response style criteria used in this study?
- Basis in paper: The paper proposes a manual strategy based on performance gaps and response style, but acknowledges it is subjective and may not capture all relevant factors.
- Why unresolved: The proposed strategy relies on human judgment for response style and does not account for other potential compatibility factors like training data overlap or architectural differences.
- What evidence would resolve it: Development and validation of automated metrics for model compatibility that can predict ensemble performance without manual inspection, tested across diverse model combinations and tasks.

## Limitations

- The precise threshold for performance gaps that makes ensembling ineffective (identified as 10%) requires further validation across different model families and tasks
- The effectiveness of the top-k union approach depends on the assumption that correct tokens typically appear within top-k predictions, which may vary with different model architectures and prompting strategies
- The specialized token probability alignment mechanism's effectiveness across models with fundamentally different tokenization strategies (e.g., BPE vs WordPiece) remains untested

## Confidence

- **High confidence**: The core finding that model compatibility (performance alignment and response style) is essential for effective ensembling is well-supported by empirical evidence across multiple benchmarks
- **Medium confidence**: The top-k union approach effectively reduces computational overhead while maintaining accuracy, though the optimal k value may be task-dependent
- **Medium confidence**: The specialized token probability alignment strategy provides advantages over simple zero-filling, but the mechanism's robustness across diverse model architectures needs further testing

## Next Checks

1. **Cross-task generalization test**: Evaluate UNITE across diverse task types (creative writing, code generation, summarization) to validate whether the model compatibility requirements hold beyond mathematical and multiple-choice tasks

2. **Model architecture stress test**: Test UNITE with models having fundamentally different tokenization strategies (BPE vs WordPiece vs SentencePiece) to validate the robustness of the specialized probability alignment mechanism

3. **Dynamic k optimization study**: Implement and evaluate an adaptive k selection mechanism that adjusts based on context complexity rather than using a fixed k value across all tokens and contexts