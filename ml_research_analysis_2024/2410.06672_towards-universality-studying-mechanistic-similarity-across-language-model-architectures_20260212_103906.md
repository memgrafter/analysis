---
ver: rpa2
title: 'Towards Universality: Studying Mechanistic Similarity Across Language Model
  Architectures'
arxiv_id: '2410.06672'
source_url: https://arxiv.org/abs/2410.06672
tags:
- features
- mamba
- feature
- mppc
- pythia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the mechanistic universality between Transformers
  and Mambas, two major language model architectures. The authors propose using Sparse
  Autoencoders (SAEs) to extract interpretable features from both models and compare
  their similarity.
---

# Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures

## Quick Facts
- arXiv ID: 2410.06672
- Source URL: https://arxiv.org/abs/2410.06672
- Reference count: 40
- Primary result: A significant portion of features are similar across Transformers and Mambas, with average MPPC of 0.74

## Executive Summary
This paper investigates mechanistic universality between Transformers and Mambas, two major language model architectures. The authors propose using Sparse Autoencoders (SAEs) to extract interpretable features from both models and compare their similarity using a metric called Maximum Pairwise Pearson Correlation (MPPC). They find that a significant portion of features are similar across both architectures, with an average MPPC of 0.74, slightly lower than the 0.76-0.81 observed in identical architectures. Additionally, the authors analyze induction circuits in Mambas, finding structural analogies to Transformers but with a nuanced difference termed "Off-by-One motif."

## Method Summary
The authors train Sparse Autoencoders (SAEs) on both Pythia-160M (Transformer) and Mamba-130M (SSM-based) models to extract interpretable features from their residual streams. They then compute the Maximum Pairwise Pearson Correlation (MPPC) between features from the two architectures using 1 million OpenWebText tokens. To validate their findings, they compare cross-architecture MPPC values with seed variant skylines and analyze induction circuits in Mambas using path patching to identify key components and compare with Transformer circuits.

## Key Results
- Cross-architecture feature similarity has an average MPPC of 0.74
- Same-architecture seed variants show higher similarity (0.76-0.81)
- Mamba implements induction circuits with an "Off-by-One motif" where token information is written to the SSM state at the next token position
- 40.7% of Pythia features have matching Mamba counterparts with MPPC > 0.5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse Autoencoders (SAEs) can extract interpretable, architecture-agnostic features from both Transformers and Mambas
- Mechanism: SAEs learn to decompose activation spaces into sparse codes that capture semantic meaning independent of model architecture
- Core assumption: SAEs learn similar semantic features when trained on different architectures performing the same task
- Evidence anchors:
  - [abstract]: "We propose to use Sparse Autoencoders (SAEs) to isolate interpretable features from these models and show that most features are similar in these two models"
  - [section 4.1]: "SAEs learn to decompose an activation space onto a semantic basis in an architecture-agnostic manner"
  - [corpus]: Weak evidence - related work exists on SAE universality but not direct cross-architecture comparison
- Break condition: If SAE training introduces significant randomness or if architectures implement fundamentally different algorithms

### Mechanism 2
- Claim: Feature similarity can be quantified using Maximum Pairwise Pearson Correlation (MPPC)
- Mechanism: For each feature in one model, find the most correlated feature in the other model and measure this correlation
- Core assumption: High Pearson correlation between features indicates mechanistic similarity
- Evidence anchors:
  - [section 4.2.2]: "We compute the maximum Pearson Correlation among all features in Mamba, which we refer to as the Max Pairwise Pearson Correlation (MPPC)"
  - [section 4.3.2]: "Cross-arch SAE MPPC has an average of 0.74, compared to 0.76 for model seed variant"
  - [corpus]: Weak evidence - Pearson correlation is standard but MPPC as a cross-architecture metric is novel
- Break condition: If features have different complexity levels affecting correlation scores

### Mechanism 3
- Claim: Mamba implements induction circuits similar to Transformers but with an "Off-by-One" motif
- Mechanism: Both architectures store information for next-token prediction, but Mamba delays writing to SSM state by one token
- Core assumption: Induction circuits are fundamental to language modeling and both architectures must implement them
- Evidence anchors:
  - [abstract]: "We also identify a nuanced difference we call Off-by-One motif: The information of one token is written into the SSM state in its next position"
  - [section 6.2]: "Mamba employs a similar mechanism to Transformers, but with some slight differences in its implementation"
  - [corpus]: Strong evidence - related work confirms Mamba has induction capabilities
- Break condition: If Off-by-One mechanism proves to be an artifact of SAE training rather than architectural difference

## Foundational Learning

- Concept: Sparse Autoencoders and their training objectives
  - Why needed here: SAEs are the core tool for extracting comparable features across architectures
  - Quick check question: What is the L1 penalty in SAE training meant to achieve?

- Concept: Pearson correlation and its interpretation for feature similarity
  - Why needed here: MPPC metric relies on Pearson correlation to quantify feature similarity
  - Quick check question: What does a Pearson correlation of 0.74 between two features indicate?

- Concept: State Space Models (SSMs) and their role in Mamba architecture
  - Why needed here: Understanding Mamba's SSM mechanism is crucial for interpreting the Off-by-One motif
  - Quick check question: How does information flow through Mamba's SSM compared to Transformer attention?

## Architecture Onboarding

- Component map: Text sequences -> Pythia-160M and Mamba-130M models -> SAEs in residual streams -> Feature activations -> MPPC calculation -> Circuit analysis via path patching

- Critical path:
  1. Train SAEs on both models
  2. Extract feature activations from sampled tokens
  3. Calculate MPPC between models
  4. Validate with seed variant skylines
  5. Analyze circuit mechanisms

- Design tradeoffs:
  - SAE dictionary size (F=32D) vs reconstruction quality
  - Number of sampled tokens (1M) vs statistical significance
  - Path patching granularity vs computational cost

- Failure signatures:
  - Low MPPC values across all feature pairs (no universality)
  - High MPPC but uninterpretable features (SAE failure)
  - Inconsistent layer-wise feature matching (architecture divergence)

- First 3 experiments:
  1. Train SAEs on Pythia and Mamba, verify feature interpretability
  2. Calculate MPPC between models, compare to seed variant skylines
  3. Apply path patching to identify induction circuit components in Mamba

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the underlying mechanism that causes Mamba to exhibit the "Off-by-One motif" where token information is written into the SSM state at the next token position rather than the current one?
- Basis in paper: [explicit] The authors identify this nuanced difference in Section 6.3, observing that Mamba merges information into the SSM input at token B+1, unlike Transformers which perform token mixing between tokens B1 and A2 directly.
- Why unresolved: The paper acknowledges this as an intriguing phenomenon but does not provide a definitive explanation for why this mechanism forms or what purpose it serves. The authors state they are "still unclear why such mechanism forms or what it is for."
- What evidence would resolve it: Further circuit analysis experiments specifically designed to test whether Mamba can function without this delay in writing token information to the SSM state, or ablation studies that compare model performance with and without this "off-by-one" behavior.

### Open Question 2
- Question: How universal are the findings of feature similarity between Transformers and Mambas across different model sizes, architectures, and tasks beyond language modeling?
- Basis in paper: [inferred] The authors acknowledge limitations in Section 7, noting that SAE features represent only a subset of true features and that their conclusions are drawn under several assumptions. They also mention repeating experiments on RWKV with slightly lower numerical performance.
- Why unresolved: The study focuses on a specific pair of models (Pythia-160M and Mamba-130M) and does not extensively test across different model sizes, architectures, or domains. The authors recognize that SAEs are an immature research area with room for improvement.
- What evidence would resolve it: Systematic studies applying the same methodology to larger models, different architectures (like RWKV, RetNet, etc.), and across different domains (vision, audio, multimodal) would provide stronger evidence for or against universality.

### Open Question 3
- Question: Can circuit discovery and comparison between different architectures be automated, similar to existing works in circuit discovery on Transformers?
- Basis in paper: [explicit] In the limitations section, the authors note that while they have shown structural analogy in induction circuits, they have not quantitatively measured similarity with automated tools, and suggest this would be "very interesting if there circuit discovery and comparison can be automated."
- Why unresolved: The current circuit analysis in the paper is largely manual and qualitative, requiring human interpretation of path patching results. There is no established methodology for automated circuit comparison across architectures.
- What evidence would resolve it: Development of automated tools that can identify analogous circuits across architectures and provide quantitative similarity scores, potentially using techniques like graph neural networks or other structural comparison methods.

## Limitations
- The study focuses on a specific pair of models (Pythia-160M and Mamba-130M) and does not extensively test across different model sizes, architectures, or domains
- SAE features represent only a subset of true features, and SAEs are an immature research area with room for improvement
- The MPPC metric provides quantitative evidence but the difference between cross-architecture (0.74) and same-architecture (0.76-0.81) comparisons requires further statistical validation

## Confidence
- MPPC-based feature similarity quantification: High - The methodology is sound and results align with within-architecture comparisons
- SAE feature interpretability across architectures: Medium - While SAEs are well-established, cross-architecture semantic alignment remains largely unproven
- Off-by-One motif characterization: Medium - Circuit analysis is compelling but the mechanism needs independent verification

## Next Checks
1. **Permutation test for MPPC significance**: Randomly shuffle feature pairs between architectures 1,000 times to establish the null distribution and compute p-values for the observed 0.74 average MPPC.

2. **Cross-validation of SAE training**: Train SAEs on multiple random seeds and different dataset subsets to assess stability of extracted features and correlation scores.

3. **Alternative similarity metrics**: Replicate the analysis using canonical correlation analysis (CCA) and centered kernel alignment (CKA) to verify that MPPC captures the same architectural similarities.