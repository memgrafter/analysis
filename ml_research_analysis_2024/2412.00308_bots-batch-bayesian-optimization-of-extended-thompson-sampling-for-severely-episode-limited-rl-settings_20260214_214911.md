---
ver: rpa2
title: 'BOTS: Batch Bayesian Optimization of Extended Thompson Sampling for Severely
  Episode-Limited RL Settings'
arxiv_id: '2412.00308'
source_url: https://arxiv.org/abs/2412.00308
tags:
- action
- batch
- methods
- reward
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BOTS, a method for optimizing adaptive health
  interventions under severe episode constraints. BOTS extends Thompson sampling by
  adding action bias terms learned via batch Bayesian optimization, enabling better
  long-term planning while retaining low variance.
---

# BOTS: Batch Bayesian Optimization of Extended Thompson Sampling for Severely Episode-Limited RL Settings

## Quick Facts
- **arXiv ID**: 2412.00308
- **Source URL**: https://arxiv.org/abs/2412.00308
- **Reference count**: 29
- **Primary result**: BOTS outperforms standard Thompson sampling and full RL methods in severely episode-limited settings, achieving 3000+ return with 140 episodes vs. >1500 for baselines

## Executive Summary
BOTS addresses the challenge of optimizing adaptive health interventions under severe episode constraints typical of mobile health studies. The method extends Thompson sampling by adding learnable action bias terms optimized via batch Bayesian optimization, enabling better long-term planning while maintaining low variance. Experiments on a physical activity intervention simulation demonstrate that BOTS achieves significantly higher returns than standard TS and full RL methods (DQN, PPO, REINFORCE) while requiring far fewer episodes. The approach uses local batch BO (TuRBO) and micro-randomized trials to set priors, making it particularly suited for real-world mobile health applications where data collection is expensive.

## Method Summary
BOTS extends linear Thompson sampling by incorporating action bias parameters that capture long-term consequences of actions. These bias terms are learned across episodes using batch Bayesian optimization, specifically with the qEI acquisition function. The method initializes Thompson sampling priors using micro-randomized trial data, then applies Sobol sampling to initialize BO parameters. In each round, BOTS runs parallel episodes with current parameters, updates the Gaussian process model, and uses batch acquisition functions to select new parameters. The approach supports both global BO and local BO variants (TuRBO), with local optimization focusing on promising regions of the parameter space. The JITAI simulation environment models a messaging-based physical activity intervention with context uncertainty, disengagement dynamics, and habituation effects.

## Key Results
- BOTS achieves 3000+ average return with only 140 episodes, compared to >1500 episodes required by baseline methods (REINFORCE, DQN, PPO, TS)
- Local BO (TuRBO) variant outperforms global BO, demonstrating the value of focused parameter space exploration
- BOTS maintains superior performance across different batch sizes (B1, B2, B4, B5, B10, B20, B600) in the severely episode-limited setting
- The method shows robustness to prior initialization strategies, with both fixed and updated TS priors yielding strong results

## Why This Works (Mechanism)

### Mechanism 1
Standard TS selects actions based on immediate reward distributions. BOTS extends this by adding action bias terms βa to the utility function, learned across episodes via batch BO to maximize expected return. This allows the policy to account for long-term consequences of actions beyond immediate rewards.

Core assumption: The long-term consequences of actions on total return can be captured by fixed action bias terms that are independent of the current state.

Evidence anchors:
- [abstract] "We extend the linear Thompson sampling bandit to select actions based on a state-action utility function consisting of the Thompson sampler's estimate of the expected immediate reward combined with an action bias term."
- [section] "The basic intuition for the xTS model is that the action bias parameters enable penalizing or promoting the selection of each action a (regardless of the current state), based on action a's average long term consequences effect on the total return."

Break condition: If the long-term consequences of actions vary significantly with state, the fixed action bias approach will fail to capture optimal policies.

### Mechanism 2
By learning action bias terms via batch BO, BOTS can represent optimal policies for MDPs where the optimal action in each state corresponds to the action with the highest expected utility under some setting of the action bias parameters. This is a strictly larger set than what standard TS can solve.

Core assumption: The MDP can be approximated well enough by a linear reward model combined with action bias terms to find near-optimal policies.

Evidence anchors:
- [abstract] "The proposed approach is able to learn optimal policies for a strictly broader class of Markov decision processes (MDPs) than standard Thompson sampling."
- [section] "When using a global BO method to optimize xTS, this approach can represent optimal policies for any MDP where standard TS can represent an optimal policy, simply by learning to set βa = 0 for all a. Moreover, our approach can also represent optimal policies for MDPs where the optimal action in each state corresponds to the action with the highest expected utility under some setting of the action bias parameters β."

Break condition: If the MDP requires highly non-linear or state-dependent action corrections, the fixed action bias approach will be insufficient.

### Mechanism 3
Local BO methods like TuRBO focus the search on promising regions of the parameter space, which is particularly effective when the action bias parameters have a complex relationship with the expected return. This allows more efficient use of the limited episode budget.

Core assumption: The action bias parameters have a smooth relationship with the expected return that can be efficiently explored using local BO methods.

Evidence anchors:
- [abstract] "Using local batch BO instead of global batch BO can further improve the performance."
- [section] "On later rounds, we typically apply the qEI acquisition function. When applying the acquisition function, we consider both unconstrained and local optimization. This choice corresponds to using global or local BO (e.g., TuRBO)."

Break condition: If the relationship between action bias parameters and expected return is highly discontinuous or has many local optima, local BO may get stuck and miss better solutions.

## Foundational Learning

- **Concept**: Thompson Sampling (TS)
  - Why needed here: TS is the baseline method that BOTS extends. Understanding TS is crucial for grasping how BOTS modifies the action selection process.
  - Quick check question: In standard TS, how are actions selected at each time step?

- **Concept**: Bayesian Optimization (BO)
  - Why needed here: BO is used to learn the action bias terms across episodes. Understanding BO principles is essential for grasping how BOTS optimizes the extended TS policy.
  - Quick check question: What is the main advantage of using batch BO in the context of BOTS?

- **Concept**: Markov Decision Processes (MDPs)
  - Why needed here: The paper discusses how BOTS can solve a broader class of MDPs than standard TS. Understanding MDPs is crucial for grasping the theoretical implications of the approach.
  - Quick check question: What is the key difference between the class of MDPs that standard TS can solve and the class that BOTS can solve?

## Architecture Onboarding

- **Component map**: MRT -> TS priors -> Sobol sampling -> xTS episodes -> GP update -> qEI acquisition -> new BO parameters (cyclic)

- **Critical path**:
  1. Initialize TS priors using MRT
  2. Apply Sobol sampling to get initial BO parameters
  3. Run xTS in parallel with initial parameters and collect returns
  4. Update GP using collected data
  5. Use batch acquisition function to get new BO parameters
  6. Repeat steps 3-5 for multiple rounds

- **Design tradeoffs**:
  - Global vs. Local BO: Global BO explores the entire parameter space but may be less efficient. Local BO (e.g., TuRBO) focuses on promising regions but may miss global optima.
  - Fixed vs. Updated TS priors: Fixed priors maintain consistency across rounds but may not adapt to new information. Updated priors adapt but may introduce instability.

- **Failure signatures**:
  - Poor performance despite many episodes: Could indicate that the action bias approach is insufficient for the MDP structure
  - High variance in returns: Could indicate that the BO is not converging or that the GP model is poorly specified
  - Slow convergence: Could indicate that the local BO is getting stuck in local optima

- **First 3 experiments**:
  1. Implement xTS with fixed action bias terms and compare to standard TS on a simple MDP
  2. Implement batch BO to learn action bias terms and evaluate on the same simple MDP
  3. Test the full BOTS pipeline on the JITAI simulation environment with a small number of episodes

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How sensitive is BOTS performance to the initial prior parameters set by the micro-randomized trial (MRT)?
- **Basis in paper**: [explicit] The paper mentions using an MRT to set TS priors but does not systematically evaluate sensitivity to MRT size or quality
- **Why unresolved**: The paper does not report experiments varying MRT size or comparing different prior initialization strategies
- **What evidence would resolve it**: Systematic experiments showing BOTS performance with varying MRT sizes (e.g., 5, 10, 20 participants) and comparing against random or uninformed priors

### Open Question 2
- **Question**: Does BOTS maintain its advantage over standard TS when the MDP has more complex state-action relationships?
- **Basis in paper**: [inferred] The paper shows BOTS outperforms TS in the JITAI simulation, but this environment has relatively simple dynamics with state-independent action biases
- **Why unresolved**: The paper only tests BOTS on one simulation environment and three simple MDPs, none with complex state-dependent dynamics
- **What evidence would resolve it**: Experiments testing BOTS on MDPs where optimal actions depend on specific state features, not just general action biases

### Open Question 3
- **Question**: How does BOTS performance scale with increasing number of actions?
- **Basis in paper**: [inferred] The JITAI simulation uses only 4 actions, and the paper does not explore scalability to larger action spaces
- **Why unresolved**: The paper does not report experiments with environments having more than 4 actions
- **What evidence would resolve it**: Experiments testing BOTS on simulations with 10+, 50+, or 100+ actions while maintaining the same episode budget constraints

### Open Question 4
- **Question**: What is the theoretical relationship between BOTS policy space and the true optimal policy space?
- **Basis in paper**: [explicit] The paper states BOTS can represent optimal policies for "a strictly larger set of MDPs" than standard TS, but does not characterize this set precisely
- **Why unresolved**: The paper provides informal reasoning but no formal characterization or bounds on the class of MDPs BOTS can solve
- **What evidence would resolve it**: Formal proofs or counterexamples characterizing the exact class of MDPs where BOTS can find optimal policies versus those requiring more general approaches

## Limitations
- The theoretical claim about solving a "strictly broader class of MDPs" relies on linear reward approximations that may not hold for complex real-world scenarios
- Performance has only been demonstrated in a single simulation environment (physical activity intervention), limiting generalization claims
- The computational overhead of batch BO optimization across episodes is not quantified in terms of practical implementation costs

## Confidence
- **High confidence**: BOTS outperforms standard Thompson sampling in the tested simulation environment, achieving higher average returns with fewer episodes.
- **Medium confidence**: The theoretical advantage of solving a broader class of MDPs than standard TS is logically sound but requires empirical validation across diverse MDP structures.
- **Medium confidence**: The claim that local BO (TuRBO) provides additional performance improvements over global BO is supported by the experiments but lacks comprehensive comparison across different parameter landscapes.

## Next Checks
1. **Cross-domain validation**: Test BOTS on multiple distinct MDP environments (e.g., Atari games, robotic control tasks) to assess generalization beyond the physical activity intervention simulation.
2. **Parameter sensitivity analysis**: Systematically vary the action bias parameter ranges and observe how performance changes to determine if the current parameter bounds are optimal or arbitrary.
3. **Computational overhead measurement**: Quantify the computational cost of batch BO optimization relative to the episode execution time to assess practical feasibility in real-world deployment scenarios.