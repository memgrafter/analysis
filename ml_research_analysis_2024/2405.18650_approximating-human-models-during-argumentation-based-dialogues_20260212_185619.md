---
ver: rpa2
title: Approximating Human Models During Argumentation-based Dialogues
arxiv_id: '2405.18650'
source_url: https://arxiv.org/abs/2405.18650
tags:
- human
- trust
- probability
- argument
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of aligning AI agents\u2019\
  \ mental models with human users in explainable AI planning, specifically by learning\
  \ and updating probabilistic human models through argumentation-based dialogues.\
  \ The core method introduces trust-based and certainty-based update mechanisms,\
  \ where the agent refines its probabilistic model of the human\u2019s knowledge\
  \ using Bayesian updates weighted by trust and certainty values expressed during\
  \ dialogue."
---

# Approximating Human Models During Argumentation-based Dialogues

## Quick Facts
- arXiv ID: 2405.18650
- Source URL: https://arxiv.org/abs/2405.18650
- Reference count: 10
- One-line primary result: Bayesian updating of probabilistic human models using trust-weighted argumentation achieves Spearman correlations above 0.75 with human perspectives.

## Executive Summary
This paper addresses the challenge of aligning AI agents' mental models with human users in explainable AI planning through argumentation-based dialogues. The core contribution is a framework that learns and updates probabilistic human models by incorporating trust-based and certainty-based update mechanisms. By using Bayesian updates weighted by trust and certainty values expressed during dialogue, the agent refines its probabilistic model of the human's knowledge. A human-subject study with 150 participants demonstrated the approach's effectiveness, showing strong correlation between the model's predictions and participants' rankings, along with statistically significant increases in participants' trust across interaction rounds.

## Method Summary
The method employs Bayesian updating of probability distributions over possible human models based on arguments exchanged during dialogue. Each argument is assigned a probability reflecting trust or certainty, which is used to update the agent's belief about the human's mental state. Trust values are mapped to objective probabilities using a sigmoidal function inspired by prospect theory, with parameter γ controlling the distortion. The framework distinguishes between trust-based updates (when the agent presents arguments) and certainty-based updates (when the human presents arguments), allowing bidirectional learning of human models in real-time dialogue.

## Key Results
- Spearman rank correlation coefficients predominantly above 0.75 between model predictions and participant rankings
- Statistically significant increases in participants' trust across interaction rounds
- Trust-based and certainty-based update mechanisms together enable effective bidirectional learning of human models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probabilistic human models updated via trust-weighted Bayesian inference can approximate human beliefs during argumentation dialogues.
- Mechanism: Bayesian updates refine probability distribution over human models based on argument probabilities weighted by trust/certainty.
- Core assumption: Trust and certainty values meaningfully quantify argument uncertainty and remain relatively consistent across dialogue turns.
- Evidence anchors: [abstract] "Our approach incorporates trust-based and certainty-based update mechanisms..."; [section] "To update the (probabilistic) human model, we use a Bayesian update mechanism..."
- Break condition: Inconsistent or biased trust assessments across turns, or inaccurate probability weighting function.

### Mechanism 2
- Claim: Prospect theory's probability weighting function captures the nonlinear human perception of argument trustworthiness.
- Mechanism: Sigmoidal function maps trust values to objective probabilities, accounting for human tendency to overweight small probabilities and underweight moderate-to-high ones.
- Core assumption: The sigmoidal weighting function with parameter γ accurately reflects participant's subjective perception of argument probability.
- Evidence anchors: [abstract] "We employ a probability weighting function inspired by prospect theory..."; [section] "We propose a probability weighting function (Gonzalez and Wu 1999)..."
- Break condition: Poor γ estimation or significant deviation from modeled nonlinear distortion in human perception.

### Mechanism 3
- Claim: Trust-based and certainty-based updates together enable bidirectional learning of human models in real-time dialogue.
- Mechanism: Different update mechanisms apply depending on whether agent or human presents arguments, adjusting probability distribution accordingly.
- Core assumption: Both agent and human can express valid and informative trust/certainty values for the update process.
- Evidence anchors: [abstract] "Our approach incorporates trust-based and certainty-based update mechanisms..."; [section] "The trust-based update mechanism allows the AI agent to adjust its probabilistic human model..."
- Break condition: Absence, noise, or unreflective trust/certainty feedback.

## Foundational Learning

- Concept: Bayesian updating of probability distributions
  - Why needed here: Core mechanism for refining probabilistic human models based on argument evidence.
  - Quick check question: If the prior distribution over models is uniform, and an argument is presented with probability 0.8, how does the posterior change for models consistent vs. inconsistent with the argument?

- Concept: Prospect theory and probability weighting functions
  - Why needed here: To translate human trust judgments into calibrated objective probabilities.
  - Quick check question: Given τ=0.6 and γ=0.85, what is the corresponding objective probability p according to the sigmoidal function?

- Concept: Argumentation theory and logical consistency
  - Why needed here: To determine which models are consistent with a given argument and thus eligible for probability updates.
  - Quick check question: If an argument claims a ∧ b → c, which models (among those with variables a,b,c) are consistent with it?

## Architecture Onboarding

- Component map: Argument parser -> Trust/certainty interface -> Probability weighting module -> Bayesian updater -> Model ranking engine
- Critical path: 1. Receive argument and trust/certainty value from user. 2. Convert to objective probability using weighting function. 3. Identify consistent/inconsistent models. 4. Apply Bayesian update. 5. Generate updated model rankings.
- Design tradeoffs:
  - Simple uniform prior vs. informed prior: Uniform is easier but less adaptive; informed prior may bias updates.
  - Fixed γ vs. personalized γ: Fixed is simpler but may underfit; personalized increases complexity but improves fit.
  - Explicit trust input vs. inferred trust: Explicit is accurate but burdens user; inferred is scalable but noisier.
- Failure signatures: Spearman correlation drops below 0.5; trust values cluster at extremes; probability updates cause distribution collapse.
- First 3 experiments: 1. Validate probability weighting function with synthetic dataset. 2. Test Bayesian update stability with known ground-truth models. 3. Compare personalized vs. fixed γ estimation in Spearman correlation improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be extended to handle global trust alongside argument-specific trust?
- Basis in paper: [explicit] The paper discusses argument-specific trust but notes that the current framework does not explicitly model a global notion of trust.
- Why unresolved: The paper acknowledges the need for future work to integrate a global trust component but does not provide a solution.
- What evidence would resolve it: Experimental results comparing the performance of the current framework with a version that includes a global trust component.

### Open Question 2
- Question: What are the most effective methods for eliciting human certainty levels in their own arguments?
- Basis in paper: [explicit] The paper discusses potential methods for eliciting certainty levels but does not provide empirical evidence on their effectiveness.
- Why unresolved: The paper mentions possible approaches but does not conduct studies to determine which methods are most effective.
- What evidence would resolve it: User studies comparing the effectiveness of different elicitation methods in terms of accuracy and usability.

### Open Question 3
- Question: How can the framework be adapted to handle more complex argument structures beyond logical propositions?
- Basis in paper: [explicit] The paper notes the limitation of representing arguments as logical propositions and suggests the need for capturing richer argumentation dynamics.
- Why unresolved: The paper identifies this as a challenge but does not provide a solution for handling complex argument structures.
- What evidence would resolve it: Development and testing of a framework that incorporates complex argument structures, with empirical validation of its effectiveness.

## Limitations
- Relies on self-reported trust and certainty values that may not accurately reflect actual cognitive processes during argumentation
- Propositional logic limitation restricts handling of complex argument structures and natural language nuances
- Parameter γ selection criteria and justification for the range {0.1, 0.2, ..., 0.9} are not clearly explained

## Confidence
- Mechanism 1 (Probabilistic updating via trust-weighted Bayesian inference): High confidence
- Mechanism 2 (Prospect theory probability weighting): Medium confidence
- Mechanism 3 (Bidirectional learning through trust/certainty updates): High confidence

## Next Checks
- Conduct a within-subjects validation study with varying argument complexities to test stability of trust assessments and model predictions
- Implement a synthetic benchmark with known ground-truth human models to test framework recovery under controlled conditions
- Compare the proposed framework against alternative probability weighting functions to establish performance advantages