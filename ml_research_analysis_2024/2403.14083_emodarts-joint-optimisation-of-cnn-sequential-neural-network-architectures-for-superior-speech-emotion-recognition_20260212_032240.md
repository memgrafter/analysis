---
ver: rpa2
title: 'emoDARTS: Joint Optimisation of CNN & Sequential Neural Network Architectures
  for Superior Speech Emotion Recognition'
arxiv_id: '2403.14083'
source_url: https://arxiv.org/abs/2403.14083
tags:
- darts
- lstm
- search
- architecture
- operations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes emoDARTS, a DARTS-optimised joint CNN and Sequential
  Neural Network (LSTM/RNN) architecture for speech emotion recognition. The key innovation
  is allowing DARTS to freely choose the optimal CNN layer order within the search
  cell, rather than imposing predefined constraints.
---

# emoDARTS: Joint Optimisation of CNN & Sequential Neural Network Architectures for Superior Speech Emotion Recognition

## Quick Facts
- arXiv ID: 2403.14083
- Source URL: https://arxiv.org/abs/2403.14083
- Reference count: 40
- Key outcome: emoDARTS achieves 78.03% weighted accuracy on IEMOCAP, significantly outperforming baseline CNN-LSTM models and state-of-the-art DARTS-based SER approaches

## Executive Summary
This paper introduces emoDARTS, a novel approach for speech emotion recognition that leverages Differentiable Architecture Search (DARTS) to jointly optimise both CNN and sequential neural network (LSTM/RNN) components. The key innovation lies in allowing DARTS to freely determine the optimal order of CNN layers within the search cell, rather than imposing predefined constraints. By jointly optimising both components, emoDARTS discovers architectures that significantly outperform traditional fixed-architecture models and existing DARTS-based approaches on multiple speech emotion recognition datasets.

## Method Summary
emoDARTS extends the DARTS framework to enable joint optimisation of CNN and sequential neural network architectures for speech emotion recognition. The method employs a bi-level optimisation approach where the search space includes both convolutional and recurrent operations, allowing the algorithm to discover optimal layer configurations and connections. The search process iteratively updates architecture parameters to maximise validation performance while training network weights, ultimately producing a compact and effective architecture for emotion classification from speech signals.

## Key Results
- emoDARTS achieves 78.03% weighted accuracy on the IEMOCAP dataset
- Outperforms baseline CNN-LSTM models by a significant margin
- Sets new state-of-the-art results for DARTS-based approaches in speech emotion recognition
- Demonstrates consistent performance improvements across IEMOCAP, MSP-IMPROV, and MSP-Podcast datasets

## Why This Works (Mechanism)
The effectiveness of emoDARTS stems from its ability to discover optimal architectural configurations through differentiable search rather than relying on human-designed structures. By allowing the search algorithm to freely determine CNN layer ordering and jointly optimise with sequential components, the method can identify architectures that better capture the hierarchical and temporal patterns inherent in emotional speech signals. The differentiable nature of the search process enables efficient exploration of the architecture space while maintaining computational tractability.

## Foundational Learning
- **Differentiable Architecture Search (DARTS)**: Why needed - enables efficient neural architecture search without discrete optimisation; Quick check - verify continuous relaxation of architecture parameters is properly implemented
- **Bi-level optimisation**: Why needed - separates architecture search from weight training; Quick check - confirm inner/outer loop optimisation is correctly structured
- **CNN-SeqNN hybrid architectures**: Why needed - combines spatial feature extraction with temporal modelling; Quick check - validate proper integration of convolutional and recurrent components
- **Speech emotion recognition datasets**: Why needed - provides diverse emotional speech samples for training and evaluation; Quick check - ensure dataset preprocessing and feature extraction are standardised
- **Weighted accuracy metric**: Why needed - accounts for class imbalance in emotion recognition; Quick check - verify weighted accuracy calculation properly handles class distributions

## Architecture Onboarding
- **Component map**: Raw audio -> CNN feature extractor -> DARTS cell (CNN + LSTM/RNN operations) -> Classification head
- **Critical path**: Audio input -> CNN layers -> Mixed operations cell -> Recurrent layers -> Fully connected layer -> Output
- **Design tradeoffs**: Search space complexity vs. computational efficiency; model capacity vs. overfitting risk; search time vs. performance gains
- **Failure signatures**: Underfitting with overly simple architectures; overfitting with overly complex configurations; poor generalisation across datasets
- **First experiments**: 1) Baseline CNN-LSTM comparison, 2) Single-dataset search and evaluation, 3) Cross-dataset generalisation testing

## Open Questions the Paper Calls Out
None

## Limitations
- Architecture search space details are not fully specified, making it difficult to assess the true extent of architectural freedom
- Computational resources required for architecture search are not detailed, limiting practical deployment assessment
- Generalisation across diverse real-world scenarios is not thoroughly evaluated

## Confidence
- High Confidence: The core DARTS methodology for joint CNN-SeqNN optimisation is well-established and properly implemented
- Medium Confidence: Performance improvements are significant, but lack of detailed ablation studies introduces some uncertainty
- Low Confidence: Claims about unconstrained search space and generalisation capabilities require further validation

## Next Checks
1. Conduct comprehensive ablation studies to isolate contributions of different architectural components and search strategies
2. Evaluate model performance on additional speech emotion recognition datasets to assess generalisation capabilities
3. Provide detailed computational cost analysis for architecture search and model training to enable practical implementation assessment