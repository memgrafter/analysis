---
ver: rpa2
title: Understanding and Mitigating Language Confusion in LLMs
arxiv_id: '2406.20052'
source_url: https://arxiv.org/abs/2406.20052
tags:
- language
- command
- confusion
- llama
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies a critical multilingual limitation in large
  language models (LLMs): their inability to consistently generate text in the user''s
  desired language, termed "language confusion." This issue manifests at word, line,
  or full-response levels, causing jarring user experiences even in high-resource
  languages. The authors create the Language Confusion Benchmark (LCB) covering 15
  typologically diverse languages with monolingual and cross-lingual generation settings.'
---

# Understanding and Mitigating Language Confusion in LLMs

## Quick Facts
- arXiv ID: 2406.20052
- Source URL: https://arxiv.org/abs/2406.20052
- Reference count: 40
- Primary result: LLMs consistently fail to generate text in user's desired language, with even high-resource languages affected.

## Executive Summary
This paper identifies a critical multilingual limitation in large language models: their inability to consistently generate text in the user's desired language, termed "language confusion." This issue manifests at word, line, or full-response levels, causing jarring user experiences even in high-resource languages. The authors create the Language Confusion Benchmark (LCB) covering 15 typologically diverse languages with monolingual and cross-lingual generation settings. Evaluation across Llama, Mistral, Command R, and OpenAI models reveals severe language confusion in Llama Instruct and Mistral models, with even strongest models failing cross-lingually. Base and English-centric instruction-tuned models show higher confusion, aggravated by complex prompts and high sampling temperatures. Mitigation strategies include few-shot prompting, multilingual supervised fine-tuning, and preference tuning, which reduce confusion. The LCB provides an efficient, scalable evaluation framework for ensuring equal LLM utility across languages.

## Method Summary
The authors create the Language Confusion Benchmark (LCB) covering 15 typologically diverse languages with monolingual and cross-lingual generation settings. They evaluate Llama, Mistral, Command R, and OpenAI models using fastText language identification to compute Line-level pass rate (LPR), Word-level pass rate (WPR), and Language confusion pass rate (LCPR) metrics. The study systematically varies sampling parameters (temperature, nucleus size, beam search) and applies mitigation strategies including few-shot prompting, multilingual supervised fine-tuning (SFT), and preference tuning. The evaluation pipeline consists of prompt preparation, model inference with various sampling strategies, language identification for error detection, and metric calculation.

## Key Results
- Llama Instruct and Mistral models show severe language confusion, with even strongest models failing cross-lingually
- Base and English-centric instruction-tuned models are particularly susceptible to language confusion, aggravated by complex prompts and high sampling temperatures
- Higher temperatures and larger nucleus sizes increase language confusion risk
- Few-shot prompting, multilingual SFT, and preference tuning effectively reduce language confusion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language confusion occurs when sampling temperature is high, leading to flatter token distributions that include out-of-language tokens.
- Mechanism: Higher temperature reduces the peakiness of the softmax distribution over next tokens. This increases the likelihood of sampling tokens that are in a different language than intended, especially when those tokens have non-zero probability.
- Core assumption: The model has some probability mass assigned to tokens from unintended languages at sampling points where confusion occurs.
- Evidence anchors:
  - [section] "We observe that language confusion typically occurs when the distribution over next tokens is flat and the nucleus is large" and "As T increases, previously unlikely words become more likely, and highly likely words have lower probabilities."
  - [section] "As T increases, the distribution becomes more uniform. Figure A3 shows the effect on Equation 1 for a toy example. As T increases, previously unlikely words become more likely, and highly likely words have lower probabilities."
  - [corpus] Weak evidence - no direct citation found for temperature affecting language confusion specifically, but general evidence for temperature affecting token selection.

### Mechanism 2
- Claim: Language confusion is more likely when the nucleus size is large, increasing the chance that out-of-language tokens are included in the sampling set.
- Mechanism: Larger nucleus sizes include more tokens in the sampling distribution. If out-of-language tokens are among the top-p most probable tokens, they become eligible for sampling and may be selected.
- Core assumption: Out-of-language tokens have sufficient probability mass to be included in the nucleus when it's large enough.
- Evidence anchors:
  - [section] "We observe that language confusion typically occurs when the distribution over next tokens is flat and the nucleus is large" and "We calculate Shannon entropy and nucleus size at each sampling point... We average nucleus size and entropy over examples containing no instances of language confusion... At CPs, however, average nucleus size and entropy is considerably higher."
  - [section] "We calculate Shannon entropy and nucleus size at each sampling point... We average nucleus size and entropy over examples containing no instances of language confusion... At CPs, however, average nucleus size and entropy is considerably higher."
  - [corpus] Weak evidence - no direct citation found for nucleus size specifically causing language confusion, but general evidence for nucleus sampling affecting token selection.

### Mechanism 3
- Claim: English-centric instruction tuning makes models more prone to generating English responses even when prompted in other languages.
- Mechanism: Models trained primarily on English instruction data learn to associate instructions with English responses. This creates a bias where the model defaults to English generation unless explicitly corrected.
- Core assumption: The instruction tuning data is predominantly English, causing the model to associate instructions with English responses.
- Evidence anchors:
  - [section] "We observed that base and English-centric instruct models are particularly susceptible to language confusion" and "English-only tuning (both SFT and pref. tuning) exacerbates language confusion monolingually: likely the reason for high language confusion of Llama-Instruct models."
  - [section] "We observe that base and English-centric instruct models are more prone to language confusion, which is aggravated by complex prompts and high sampling temperatures."
  - [corpus] Weak evidence - no direct citation found for English-centric instruction tuning causing language confusion, but general evidence for instruction tuning affecting model behavior.

## Foundational Learning

- Concept: Nucleus sampling with temperature
  - Why needed here: Understanding how nucleus sampling and temperature affect token selection is crucial for understanding when and why language confusion occurs.
  - Quick check question: If you have tokens with logits [0.75, 0.20, -0.10] and temperature T=1, what are the approximate probabilities after applying softmax?

- Concept: Language identification (LID) tools
  - Why needed here: LID tools are used to detect language confusion by identifying which language each token/line belongs to.
  - Quick check question: Why do LID tools perform poorly on short sequences and how does this affect the evaluation methodology?

- Concept: Beam search decoding
  - Why needed here: Beam search is an alternative decoding strategy that can reduce language confusion compared to sampling methods.
  - Quick check question: How does increasing beam size affect the diversity of generated outputs and why might this help reduce language confusion?

## Architecture Onboarding

- Component map: prompt → model with sampling parameters (temperature, nucleus size, beam search) → output → language identification → metrics (LPR, WPR, LCPR) → evaluation/mitigation strategies
- Critical path: prompt → model with appropriate sampling parameters → output in correct language
- Design tradeoffs: Higher temperatures and larger nucleus sizes increase output diversity but also increase language confusion risk. Beam search reduces confusion but may produce less diverse outputs. Multilingual instruction tuning reduces confusion but requires additional training data and compute.
- Failure signatures: High language confusion pass rate (LCPR) indicates the model frequently generates text in the wrong language. Line-level errors suggest the model produces coherent text but in the wrong language for entire segments. Word-level errors indicate sporadic mixing of languages within otherwise correct text.
- First 3 experiments:
  1. Test the effect of temperature on a single model by running the same prompts with T=0.1, 0.3, 0.5, 0.7, 1.0 and measuring WPR for non-Latin script languages.
  2. Compare nucleus sampling vs beam search by running prompts with p=0.75 vs beam size=5 and measuring both LPR and WPR.
  3. Test few-shot prompting by creating a template with 5 English Q/A pairs translated to the target language and measuring the reduction in language confusion for a base model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between sampling temperature, nucleus size, and language confusion across different model families?
- Basis in paper: [explicit] The paper systematically varies temperature (T) and nucleus size (p) for Command R and observes that higher T encourages language confusion, with T=1 showing average WPR as low as 83.5% and as low as 72.0% and 69.5% for Japanese and Chinese. Increasing p has a smaller effect.
- Why unresolved: While the paper provides empirical results for Command R, it doesn't explore how these relationships vary across different model architectures (Llama, Mistral, OpenAI) or what the optimal hyperparameters might be for minimizing language confusion while maintaining generation quality.
- What evidence would resolve it: Comparative experiments varying T and p across multiple model families, measuring both language confusion metrics and generation quality metrics like perplexity or human evaluation scores to find the optimal trade-off.

### Open Question 2
- Question: How does the presence of code-switching in the input prompt affect the likelihood and severity of language confusion in model outputs?
- Basis in paper: [inferred] The paper focuses on single-language prompts (target language in monolingual setting and English in cross-lingual setting) and explicitly states it does not consider inputs that are naturally code-switched. This limitation suggests an unexplored area of how models handle code-switched inputs.
- Why unresolved: The paper's controlled experimental setup avoids code-switched inputs, but real-world usage often involves code-switching. The authors acknowledge this limitation but don't provide any data on how models behave with code-switched inputs.
- What evidence would resolve it: Evaluation of the same benchmark models on prompts containing natural code-switching, measuring whether the presence of code-switching in input increases language confusion in outputs, and whether different models handle this scenario differently.

### Open Question 3
- Question: What is the long-term impact of preference tuning on language confusion across multiple generations in a conversation?
- Basis in paper: [explicit] The paper observes that WPR decreases after preference tuning for both English-only and multilingual settings, noting that "DPO is prone to generating a biased policy that favors out-of-distribution responses, leading to unpredictable behaviors."
- Why unresolved: The experiments focus on single-turn generation, but the paper doesn't investigate how preference tuning affects language consistency over multi-turn conversations where the model might be exposed to different language patterns.
- What evidence would resolve it: Multi-turn conversation experiments where models undergo preference tuning, then generate responses across multiple turns in various languages, measuring whether language confusion accumulates or stabilizes over the conversation length.

## Limitations
- The evaluation framework covers 15 languages but may not represent all linguistic phenomena causing language confusion
- fastText language identification has limitations with short sequences and mixed-language text
- Study focuses on static evaluation rather than dynamic, interactive use cases
- Mitigation strategies effectiveness may vary significantly across different model architectures and sizes

## Confidence

**High confidence**: The observation that language confusion is a widespread problem across multiple LLM families, particularly affecting base models and English-centric instruction-tuned models. The effectiveness of sampling parameters (temperature, nucleus size) in controlling language confusion has strong empirical support.

**Medium confidence**: The claim that English-centric instruction tuning is a primary driver of language confusion. While supported by observations that English-only tuned models show higher confusion rates, the causal mechanism isn't fully established.

**Low confidence**: The assertion that multilingual SFT and preference tuning are broadly effective solutions for language confusion. The paper shows these methods work in specific experimental conditions but doesn't demonstrate their effectiveness across different model families, sizes, or real-world deployment scenarios.

## Next Checks

1. Replicate the temperature and nucleus size experiments with additional model families not covered in the original study (e.g., Claude, Gemini, open-source alternatives) to validate whether the observed relationships generalize.

2. Implement a more sophisticated language identification pipeline that handles short sequences and mixed-language text better than fastText. Use this to re-evaluate the LCB dataset and quantify how much error detection accuracy affects reported language confusion rates.

3. Design an A/B test with real users to measure the practical impact of language confusion on task completion rates and user satisfaction, validating whether metric-based evaluation correlates with actual user experience.