---
ver: rpa2
title: 'Towards Transfer Unlearning: Empirical Evidence of Cross-Domain Bias Mitigation'
arxiv_id: '2407.16951'
source_url: https://arxiv.org/abs/2407.16951
tags:
- bias
- language
- unlearning
- debiasing
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates an unlearning-based approach to debiasing
  large language models (LLMs) by selectively unlearning harmful content within the
  text. The proposed method, Mask Language Modeling (MLM) Unlearning, minimizes the
  likelihood of biased or toxic content by performing gradient ascent on hate speech
  against minority groups.
---

# Towards Transfer Unlearning: Empirical Evidence of Cross-Domain Bias Mitigation

## Quick Facts
- **arXiv ID:** 2407.16951
- **Source URL:** https://arxiv.org/abs/2407.16951
- **Reference count:** 16
- **Primary result:** MLM Unlearning effectively reduces bias in LLMs while maintaining language modeling abilities, with surprising cross-domain transfer effects

## Executive Summary
This paper investigates an unlearning-based approach to debiasing large language models (LLMs) by selectively unlearning harmful content within the text. The proposed method, Mask Language Modeling (MLM) Unlearning, minimizes the likelihood of biased or toxic content by performing gradient ascent on hate speech against minority groups. Experimental results show that this approach effectively reduces bias while maintaining language modeling abilities. Surprisingly, the method also exhibits cross-domain transfer unlearning: debiasing in one bias form (e.g., gender) contributes to mitigating others (e.g., race and religion).

## Method Summary
The MLM Unlearning technique performs gradient ascent on biased examples to minimize the likelihood of generating similar toxic patterns. The method uses GPT-4 to identify and mask toxic or bias-indicative words in the Dynamically Generated Hate Speech Dataset (DGHS), then runs unlearning for 50 training steps with a learning rate of 1e-5 on GPT-2. Only the masked token is unlearned while preserving the rest of the context. The approach leverages the hypothesis that different bias attributes share overlapping contexts in training data, leading to co-located embeddings that enable transfer unlearning effects.

## Key Results
- MLM Unlearning effectively reduces gender, race, and religion biases in GPT-2
- The method maintains language modeling performance as measured by perplexity on Wikitext-2
- Cross-domain transfer unlearning occurs: debiasing gender bias also reduces race and religion biases
- The approach outperforms baseline GPT-2 while preserving general language capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masked Language Modeling Unlearning selectively removes associations between toxic/biased tokens and their harmful contexts.
- Mechanism: The model performs gradient ascent on the loss function for toxic/biased tokens, making their likelihood in those contexts smaller. Only the masked token is unlearned; other tokens remain unaffected.
- Core assumption: Toxic content can be isolated to specific tokens whose embeddings are spatially separable from non-toxic content in embedding space.
- Evidence anchors:
  - [abstract] "This method enables LLMs to selectively forget and disassociate from biased and harmful content."
  - [section 3.1] "By minimizing the above loss function, models are trained to dissociate the masked word from a specific given context."
  - [corpus] Weak evidence: no direct corpus neighbor supports the spatial separability claim; only mentions related bias mitigation techniques.
- Break condition: If toxic content is distributed across multiple tokens or context windows, masking a single token may not sufficiently weaken the harmful association.

### Mechanism 2
- Claim: Gradient ascent on biased text minimizes the likelihood of generating similar toxic patterns.
- Mechanism: During unlearning, the model's parameters are updated to increase the loss for biased examples, thereby decreasing the probability of generating those patterns during inference.
- Core assumption: The model's internal representations for biased content can be shifted away from harmful patterns without destroying overall language capability.
- Evidence anchors:
  - [abstract] "minimizing the likelihood of biased or toxic content."
  - [section 3] "unlearning employs gradient ascent to maximize the loss function for specific undesirable patterns."
  - [corpus] Moderate evidence: neighbor papers discuss gradient-based bias unlearning, but none provide direct experimental validation.
- Break condition: If the gradient updates affect unrelated language patterns or if the bias is deeply embedded across multiple layers, unlearning may cause collateral damage to language modeling performance.

### Mechanism 3
- Claim: Cross-domain transfer unlearning occurs because embeddings of different bias-related terms (e.g., "women" and "Black") occupy nearby regions in embedding space.
- Mechanism: When the model unlearns one bias (e.g., gender), the embedding space adjustment reduces the likelihood of other bias-related terms (e.g., race) in harmful contexts.
- Core assumption: Different bias attributes share overlapping contexts in training data, leading to co-located embeddings.
- Evidence anchors:
  - [abstract] "debiasing in one bias form (e.g. gender) may contribute to mitigating others (e.g. race and religion)."
  - [section 4.3] "when the model minimizes the likelihood of generating 'women' in a biased context via gradient ascent, it adjusts the embedding space in a way that also affects similarly biased terms like 'Black'."
  - [corpus] No direct corpus evidence; this is an inferred explanation not supported by neighbor papers.
- Break condition: If bias attributes have distinct embedding regions with minimal overlap, unlearning one will not affect the others.

## Foundational Learning

- Concept: Gradient ascent vs. gradient descent
  - Why needed here: The method uses gradient ascent to increase loss for biased content, opposite to normal training.
  - Quick check question: In unlearning, do we increase or decrease the likelihood of biased tokens?

- Concept: Masked Language Modeling (MLM)
  - Why needed here: The unlearning targets only masked tokens while conditioning on surrounding context.
  - Quick check question: During unlearning, which part of the input sequence is modified?

- Concept: Embedding space locality
  - Why needed here: Transfer unlearning relies on different bias terms sharing similar embedding neighborhoods.
  - Quick check question: What happens to nearby embeddings when one biased term's representation is pushed away from harmful contexts?

## Architecture Onboarding

- Component map: Data pipeline: DGHS → GPT-4 masking → GPT-2 MLM Unlearning → Evaluation
- Critical path: Token identification → masking → gradient ascent unlearning → evaluation on bias benchmarks
- Design tradeoffs:
  - Masking only one token per sentence limits scope but preserves language modeling
  - Small batch size (8) prevents large degradation but slows unlearning
  - Using GPT-4 for masking introduces reproducibility issues
- Failure signatures:
  - High perplexity increase → unlearning affecting general language patterns
  - Low bias reduction → insufficient gradient steps or poor token selection
  - Cross-domain transfer absent → embeddings of different biases are spatially distinct
- First 3 experiments:
  1. Run unlearning for 10 steps, measure perplexity and bias scores to confirm minimal degradation
  2. Compare bias reduction on gender vs. race/religion to confirm transfer effect
  3. Vary batch size (8, 16, 32) to find optimal tradeoff between effectiveness and language preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the MLM unlearning method demonstrate cross-domain transfer unlearning effects for other bias types beyond gender, race, and religion, such as age or disability bias?
- Basis in paper: [explicit] The paper mentions that the method was tested on gender bias and observed transfer effects to race and religion, but does not explicitly test other bias domains.
- Why unresolved: The paper's experimental scope was limited to gender, race, and religion bias datasets, leaving other potential bias domains unexplored.
- What evidence would resolve it: Testing the MLM unlearning method on datasets containing age, disability, or other forms of bias, and measuring whether debiasing in one domain transfers to these others.

### Open Question 2
- Question: What is the mechanism behind the observed cross-domain transfer unlearning effects? Is it due to shared embedding spaces for different bias attributes or some other factor?
- Basis in paper: [explicit] The paper suggests that shared contexts in training data might cause embeddings of bias attributes like "women" and "Black" to occupy nearby regions in embedding space, leading to transfer effects.
- Why unresolved: The paper provides a hypothesis but does not empirically validate the underlying mechanism of transfer unlearning.
- What evidence would resolve it: Conducting experiments to analyze the embedding space structure and measure the degree of overlap between different bias attribute embeddings before and after unlearning.

### Open Question 3
- Question: How does the MLM unlearning method compare to other debiasing techniques in terms of cross-domain transfer unlearning effectiveness?
- Basis in paper: [explicit] The paper compares MLM unlearning to CDA, SENTENCE DEBIAS, INLP, and SELF-DEBIAS, but only measures overall debiasing performance, not specifically transfer effects.
- Why unresolved: The paper does not isolate and measure the cross-domain transfer effects of other debiasing methods for comparison.
- What evidence would resolve it: Implementing the same cross-domain transfer unlearning experiments for each baseline method and comparing the magnitude of transfer effects to MLM unlearning.

### Open Question 4
- Question: How does the choice of masked tokens affect the effectiveness and transferability of the unlearning process?
- Basis in paper: [explicit] The paper mentions that GPT-4 was used to identify and mask bias-related keywords, raising concerns about reproducibility and the sensitivity of results to specific token choices.
- Why unresolved: The paper does not systematically explore how different token selection strategies impact unlearning outcomes.
- What evidence would resolve it: Conducting experiments with different token selection criteria (e.g., masking different types or numbers of tokens) and measuring the impact on debiasing effectiveness and transfer effects.

## Limitations

- The cross-domain transfer unlearning mechanism relies on an unproven assumption about embedding space locality without rigorous validation
- The use of GPT-4 for toxic token identification creates reproducibility concerns and may affect the consistency of results
- Experimental validation lacks direct comparison with state-of-the-art debiasing methods beyond simple baselines

## Confidence

- **High confidence**: The core unlearning mechanism (MLM with gradient ascent on biased content) is technically sound and the experimental results showing improved bias metrics while maintaining perplexity are credible
- **Medium confidence**: The claim that the method preserves language modeling ability is supported by perplexity measurements, though the absolute values and comparison to state-of-the-art language models would strengthen this claim
- **Low confidence**: The cross-domain transfer unlearning mechanism and its theoretical justification require further validation through embedding analysis and ablation studies

## Next Checks

1. **Embedding space analysis**: Perform t-SNE or PCA visualization of bias-related token embeddings (gender, race, religion terms) before and after unlearning to empirically verify that these embeddings occupy nearby regions and that unlearning one bias type affects others through spatial proximity

2. **Ablation on masking strategy**: Systematically vary the masking approach (single token vs. multi-token vs. phrase-level masking) while measuring both bias reduction and perplexity to determine the optimal tradeoff between debiasing effectiveness and language preservation

3. **Direct comparison with state-of-the-art**: Implement and compare against recent bias mitigation techniques (such as those from the corpus neighbors) using identical evaluation protocols to establish whether the transfer unlearning effect provides advantages beyond what existing methods achieve