---
ver: rpa2
title: Towards Completeness-Oriented Tool Retrieval for Large Language Models
arxiv_id: '2405.16089'
source_url: https://arxiv.org/abs/2405.16089
tags:
- tool
- retrieval
- tools
- learning
- colt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of tool retrieval for Large Language
  Models (LLMs) in real-world applications where a vast number of tools are available,
  but only a subset can be input due to length limitations and latency constraints.
  Existing methods focus on semantic matching between queries and tool descriptions,
  often leading to incomplete tool retrieval that fails to address multifaceted problems.
---

# Towards Completeness-Oriented Tool Retrieval for Large Language Models

## Quick Facts
- arXiv ID: 2405.16089
- Source URL: https://arxiv.org/abs/2405.16089
- Reference count: 40
- Large language models need tool retrieval but face length/latency constraints

## Executive Summary
This paper addresses the challenge of tool retrieval for large language models when dealing with vast numbers of available tools but strict input limitations. The authors propose COLT, a two-stage approach that first learns semantic relationships between queries and tools, then captures collaborative relationships among tools using graph neural networks. COLT introduces a new metric, COMP@ùêæ, to evaluate retrieval completeness and demonstrates superior performance over state-of-the-art dense retrieval methods while maintaining efficiency even with smaller model architectures.

## Method Summary
COLT operates in two stages: semantic learning using dense retrieval with PLMs to capture query-tool relationships, followed by collaborative learning that builds three bipartite graphs (Query-Scene, Query-Tool, Scene-Tool) and applies LightGCN with dual-view contrastive learning to capture high-order tool collaborations. The method uses a list-wise multi-label loss to optimize for complete tool set retrieval rather than individual tool ranking, addressing the limitation of existing methods that focus on semantic matching alone.

## Key Results
- COLT outperforms state-of-the-art dense retrieval methods on both ToolLens and ToolBench datasets
- The method achieves superior performance even when using a smaller BERT-mini model compared to BERT-large
- Significant improvements are observed on the new COMP@ùêæ metric that evaluates retrieval completeness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-view graph collaborative learning captures high-order collaborative relationships among tools, improving retrieval completeness.
- Mechanism: COLT constructs three bipartite graphs (Query-Scene, Query-Tool, Scene-Tool) and uses LightGCN to propagate information across them. The scene-centric view captures collaborative context through scenes, while the tool-centric view captures direct relevance. Cross-view contrastive learning aligns these perspectives.
- Core assumption: Tools work in collaborative contexts that can be modeled as scenes, and these collaborative relationships improve retrieval beyond semantic matching alone.
- Evidence anchors:
  - [abstract] "captures not only the semantic similarities between user queries and tool descriptions but also takes into account the collaborative information of tools"
  - [section 3.4.2] "This dual-view framework allows for comprehensive access to query-tool relevance, integrating both direct relevance and the broader context of tool collaboration within scenes"
  - [corpus] Weak - only mentions tool retrieval approaches without specific evidence for graph-based collaborative learning
- Break condition: If tools don't have meaningful collaborative relationships or scenes cannot be meaningfully defined, the graph structure becomes artificial and adds noise rather than signal.

### Mechanism 2
- Claim: List-wise multi-label loss ensures complete retrieval of diverse tools without favoring specific tools.
- Mechanism: Instead of traditional pairwise ranking losses, COLT uses a list-wise multi-label loss that treats all ground-truth tools for a query as a set to be retrieved together, optimizing for the probability of selecting the complete tool set.
- Core assumption: For complex queries requiring multiple tools, the retrieval objective should optimize for complete tool set retrieval rather than individual tool ranking.
- Evidence anchors:
  - [section 3.4.3] "To ensure the complete retrieval of diverse tools from the full set of ground-truth tools, without favoring any particular tool, we design a list-wise multi-label loss"
  - [abstract] "The learning objective incorporates a list-wise multi-label loss to ensure the simultaneous acquisition of tools from the entire ground-truth set without favoring any specific tool"
  - [corpus] Weak - only general mentions of retrieval metrics, no specific evidence for list-wise multi-label approaches
- Break condition: If the ground-truth tool sets are not truly collaborative or if queries don't genuinely require multiple tools, the list-wise approach may over-penalize partial but useful retrievals.

### Mechanism 3
- Claim: Semantic learning followed by collaborative learning provides better representations than either approach alone.
- Mechanism: COLT first uses dense retrieval to learn semantic representations between queries and tools, then builds upon these representations with graph-based collaborative learning to capture higher-order relationships.
- Core assumption: Semantic matching provides a necessary foundation that collaborative learning can then enhance, rather than collaborative learning being sufficient on its own.
- Evidence anchors:
  - [section 3.4.2] "Leveraging the initial query and tool representations derived from the first-stage semantic learning, along with the three constructed bipartite graphs"
  - [section 5.3.1] "The absence of semantic learning significantly diminishes performance, confirming its essential role in aligning the representations of tools and queries as the basic for the following collaborative learning"
  - [corpus] Weak - mentions various retrieval methods but no direct comparison of staged vs. single-stage approaches
- Break condition: If the semantic learning stage is poor or the representations are not transferable to the collaborative learning stage, the two-stage approach may not provide benefits over direct collaborative learning.

## Foundational Learning

- Concept: Graph Neural Networks (specifically LightGCN)
  - Why needed here: COLT uses LightGCN to propagate information across the bipartite graphs and capture high-order collaborative relationships among tools
  - Quick check question: How does LightGCN differ from standard GCN in terms of feature transformation and nonlinear activation?

- Concept: Contrastive Learning (InfoNCE loss)
  - Why needed here: Used in both semantic learning (initial retrieval) and collaborative learning (cross-view alignment) to learn meaningful representations by contrasting positive and negative pairs
  - Quick check question: What is the role of the temperature parameter œÑ in the InfoNCE loss function?

- Concept: Multi-label classification vs. ranking
  - Why needed here: COLT frames tool retrieval as a multi-label problem where multiple tools must be retrieved together, requiring different loss functions than traditional ranking approaches
  - Quick check question: How does list-wise loss differ from pairwise loss in terms of optimization objective and what it optimizes for?

## Architecture Onboarding

- Component map: Query ‚Üí Semantic Learning ‚Üí Initial Representations ‚Üí Collaborative Learning ‚Üí Enhanced Representations ‚Üí Tool Ranking
- Critical path: Query ‚Üí Semantic Learning ‚Üí Initial Representations ‚Üí Collaborative Learning ‚Üí Enhanced Representations ‚Üí Tool Ranking
- Design tradeoffs:
  - Two-stage vs. single-stage learning: Two-stage provides better semantic foundation but adds complexity
  - Graph construction: Requires defining scenes and building bipartite graphs, which may not always be straightforward
  - List-wise vs. pairwise loss: List-wise better captures completeness but may be harder to optimize
- Failure signatures:
  - Poor semantic learning ‚Üí Weak initial representations that collaborative learning cannot fix
  - Incorrect graph construction ‚Üí Collaborative learning captures wrong relationships
  - Overfitting to scene definitions ‚Üí Model doesn't generalize to new query types
- First 3 experiments:
  1. Ablation study: Remove semantic learning stage to verify its contribution to overall performance
  2. Graph analysis: Visualize the bipartite graphs to ensure meaningful connections are being made
  3. Temperature sensitivity: Test different œÑ values in the contrastive loss to find optimal performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of COLT scale with increasing numbers of tools in the retrieval set?
- Basis in paper: [explicit] The paper mentions that real-world systems often incorporate a wide array of tools, and discusses performance with different tool set sizes in ToolLens and ToolBench.
- Why unresolved: The paper provides results for specific datasets but does not systematically vary the number of tools to analyze scaling behavior.
- What evidence would resolve it: Experiments varying the size of the tool set while keeping other factors constant, showing how recall, NDCG, and COMP@ùêæ metrics change with tool set size.

### Open Question 2
- Question: What is the impact of query complexity (e.g., number of required tools) on the effectiveness of collaborative learning in COLT?
- Basis in paper: [explicit] The paper discusses the importance of capturing collaborative relationships among tools and mentions that queries may require multiple tools, but does not deeply analyze how query complexity affects the model's performance.
- Why unresolved: The paper evaluates performance across different numbers of required tools but does not specifically analyze how query complexity influences the collaborative learning component.
- What evidence would resolve it: Detailed analysis of how the dual-view graph collaborative learning performs as the number of required tools per query increases, potentially including breakdown of scene-centric vs. tool-centric view contributions.

### Open Question 3
- Question: How does the choice of backbone PLM affect the trade-off between semantic learning and collaborative learning in COLT?
- Basis in paper: [explicit] The paper mentions that COLT is model-agnostic and tests different PLM backbones, noting performance differences, but does not deeply analyze the interaction between backbone choice and the two-stage learning process.
- Why unresolved: While the paper shows that COLT improves performance across different PLM sizes, it does not investigate how the relative importance of semantic vs. collaborative learning might vary with different PLM architectures or sizes.
- What evidence would resolve it: Systematic comparison of COLT's performance when varying the PLM backbone, with analysis of how much each learning stage contributes to the final performance for different backbones.

## Limitations

- Dependency on high-quality scene definitions for graph construction, limiting applicability to domains where such definitions are unclear
- Potential scalability challenges with extremely large tool sets due to LightGCN's message passing across multiple bipartite graphs
- Assumption that semantic learning is always necessary before collaborative learning may not hold for all tool domains

## Confidence

**High Confidence Claims:**
- The two-stage learning approach (semantic + collaborative) improves tool retrieval completeness compared to single-stage methods
- List-wise multi-label loss is more effective than pairwise ranking losses for multi-tool retrieval tasks
- COLT achieves superior performance on COMP@ùêæ metric compared to baseline methods

**Medium Confidence Claims:**
- The specific graph construction (Q-S, Q-T, S-T bipartite graphs) is optimal for capturing tool collaboration
- Dual-view contrastive learning provides significant benefits over single-view approaches
- The performance gains transfer to other tool domains beyond those tested

**Low Confidence Claims:**
- COLT will maintain its performance advantage with significantly larger tool sets (10K+ tools)
- The method generalizes well to domains where tool relationships are less structured or scenes are harder to define
- The computational efficiency gains hold at scale with real-time inference requirements

## Next Checks

1. **Scene Definition Robustness Test**: Systematically vary the criteria for defining scenes (e.g., use different clustering algorithms, change scene granularity) and measure how performance degrades. This will quantify how sensitive COLT is to scene definition quality and identify thresholds below which the collaborative learning stage becomes ineffective.

2. **Zero-Shot Transfer Evaluation**: Test COLT on a completely different tool domain (e.g., software development tools, medical diagnostic tools) without retraining on that domain's data. This will validate whether the collaborative learning patterns generalize beyond the specific domains used in the original experiments.

3. **Computational Complexity Analysis**: Profile the memory and time complexity of the collaborative learning stage as tool set size increases from 100 to 10,000 tools. Measure both training time per epoch and inference latency to determine practical scalability limits and identify bottlenecks that could be optimized.