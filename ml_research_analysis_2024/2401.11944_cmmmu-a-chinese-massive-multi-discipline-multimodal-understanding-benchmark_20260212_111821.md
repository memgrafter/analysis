---
ver: rpa2
title: 'CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark'
arxiv_id: '2401.11944'
source_url: https://arxiv.org/abs/2401.11944
tags:
- error
- gpt-4v
- question
- case
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CMMMU is the first Chinese Massive Multi-discipline Multimodal
  Understanding benchmark designed to evaluate large multimodal models on college-level
  knowledge and reasoning tasks in a Chinese context. It includes 12k manually collected
  multimodal questions covering six core disciplines with 39 heterogeneous image types,
  focusing on complex perception and reasoning with domain-specific knowledge.
---

# CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark

## Quick Facts
- arXiv ID: 2401.11944
- Source URL: https://arxiv.org/abs/2401.11944
- Reference count: 40
- Key outcome: First Chinese multimodal understanding benchmark revealing narrower performance gaps between open-source and closed-source models in Chinese versus English contexts

## Executive Summary
CMMMU introduces a comprehensive Chinese benchmark for evaluating large multimodal models on college-level knowledge and reasoning tasks. The benchmark includes 12,000 manually curated questions across six core disciplines with 39 heterogeneous image types, focusing on complex perception and reasoning with domain-specific knowledge. Evaluations across 11 open-source and one closed-source model show significant room for improvement, with GPT-4V achieving only 43% accuracy. The benchmark reveals that open-source bilingual LMMs perform notably closer to GPT-4V in Chinese contexts compared to English contexts, with Yi-VL-34B reducing the gap to just 7%.

## Method Summary
CMMMU employs a three-stage manual data curation pipeline to collect college-level multimodal questions from Chinese educational materials, followed by strict filtering to ensure questions require both visual perception and expert knowledge. The benchmark uses zero-shot evaluation settings without fine-tuning to assess raw model capabilities. Evaluation employs micro-average accuracy as the primary metric, with systematic answer extraction using task-specific prompts and regular expressions. The benchmark strictly follows MMMU's annotation and analysis patterns to enable cross-linguistic comparisons.

## Key Results
- GPT-4V achieves only 43% accuracy on CMMMU, indicating significant room for improvement
- Open-source bilingual LMMs perform notably closer to GPT-4V in Chinese contexts (Yi-VL-34B reduces gap to 7%) than in English contexts
- Error analysis of 150 GPT-4V incorrect answers reveals specific failure modes including perceptual errors, reasoning errors, and knowledge gaps
- Performance disparities are largest in Business, Science, and Tech & Engineering disciplines requiring complex calculations and reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CMMMU reveals smaller performance gaps between open-source and closed-source LMMs in Chinese versus English contexts
- Core assumption: Performance gaps are context-dependent and influenced by language-specific factors
- Evidence: [section 4.2] shows Yi-VL-34B reduces disparity to 7.5% on CMMMU vs larger gaps on English benchmarks

### Mechanism 2
- Claim: Benchmark design exposes limitations in complex reasoning with domain-specific knowledge
- Core assumption: Expert-level multimodal reasoning is a key differentiator between current LMMs and advanced AI systems
- Evidence: [section 3.1] describes strict filtering for questions requiring expert knowledge and visual perception

### Mechanism 3
- Claim: Systematic error analysis identifies actionable failure modes for model improvement
- Core assumption: Understanding specific failure patterns guides more effective model development
- Evidence: [section 4.3] identifies perceptual errors, reasoning errors, knowledge gaps, and other specific failure categories

## Foundational Learning

- Concept: Multimodal reasoning integration
  - Why needed: CMMMU requires combining visual perception with domain-specific reasoning
  - Quick check: How would a model integrate image understanding with chemistry knowledge for molecular structure questions?

- Concept: Cross-linguistic generalization
  - Why needed: Benchmark reveals performance differences between Chinese and English contexts
  - Quick check: Why might open-source models perform closer to GPT-4V in Chinese than English contexts?

- Concept: Expert-level domain knowledge
  - Why needed: Questions require college-level expertise across multiple disciplines
  - Quick check: What distinguishes expert-level domain knowledge from general knowledge in multimodal reasoning?

## Architecture Onboarding

- Component map: Data curation pipeline (collection → filtering → verification) -> Annotation protocol -> Evaluation framework (prompt generation → answer extraction → accuracy calculation) -> Error analysis system
- Critical path: Data curation through manual verification to final evaluation, as data quality directly impacts benchmark reliability
- Design tradeoffs: Manual curation ensures high quality but limits scale; strict filtering creates challenging questions but may miss valid cases
- Failure signatures: Poor discipline-specific performance indicates reasoning limitations; high perceptual errors suggest vision model weaknesses
- First 3 experiments:
  1. Evaluate baseline LMM on small CMMMU subset to establish baseline performance
  2. Analyze error patterns on baseline to identify specific failure modes
  3. Test targeted improvements (e.g., domain-specific fine-tuning) on most problematic question types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do cross-linguistic performance gaps in multimodal models vary across different visual content types and reasoning complexity?
- Basis: Paper reveals smaller Chinese vs English gaps but doesn't analyze variation across visual types or complexity levels
- Why unresolved: High-level observations provided but lacks systematic breakdown by visual content type and reasoning complexity
- Resolution evidence: Detailed performance breakdowns across MMMU and CMMMU for each visual content type and reasoning complexity level

### Open Question 2
- Question: What architectural or training modifications enable Yi-VL-34B to narrow performance gap with GPT-4V in Chinese contexts?
- Basis: Paper highlights Yi-VL-34B's 7.5% gap reduction but doesn't investigate technical reasons
- Why unresolved: Performance evaluation provided without investigation of underlying technical differences
- Resolution evidence: Comparative architectural analysis and training methodology documentation for Yi-VL-34B versus other open-source models

### Open Question 3
- Question: How does specialized domain knowledge impact bilingual LMM performance on expert-level multimodal tasks?
- Basis: Error analysis shows "lack of knowledge" as significant error category requiring expert-level knowledge
- Why unresolved: Benchmark evaluates expert-level tasks without experimentally investigating knowledge augmentation effects
- Resolution evidence: Controlled experiments comparing bilingual LMM performance with and without domain-specific knowledge augmentation

## Limitations

- Evaluation shows GPT-4V at 43% accuracy, but performance could be influenced by uncontrolled factors
- Claims about smaller Chinese vs English gaps rely on comparative analysis that may be confounded by benchmark design differences
- Error analysis of 150 answers represents small sample that may not capture full range of failure modes
- Zero-shot evaluation setting may not reflect practical performance with minimal fine-tuning

## Confidence

**High Confidence**: Benchmark construction methodology and data collection process are well-documented following established MMMU patterns
**Medium Confidence**: Claims about reduced performance gaps in Chinese contexts are supported but could be influenced by confounding factors
**Low Confidence**: Generalizability of error patterns from 150-question sample to entire benchmark is uncertain

## Next Checks

1. Conduct head-to-head evaluations of same models on both CMMMU and MMMU using identical protocols to isolate language-specific performance differences

2. Expand error analysis to 500+ incorrect answers across disciplines and image types to verify identified failure modes are representative

3. Evaluate impact of minimal domain-specific fine-tuning on Chinese educational data to assess whether observed gaps reflect fundamental capability differences or training data limitations