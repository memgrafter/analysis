---
ver: rpa2
title: 'CFSP: An Efficient Structured Pruning Framework for LLMs with Coarse-to-Fine
  Activation Information'
arxiv_id: '2409.13199'
source_url: https://arxiv.org/abs/2409.13199
tags:
- pruning
- sparsity
- cfsp
- recovery
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CFSP is an efficient structured pruning framework for LLMs that
  uses coarse-to-fine activation information to identify and remove redundant components.
  It computes block importance via angular distance between input/output activations
  to allocate sparsity budgets, then prunes weights within blocks based on fine-grained
  activation-weight products.
---

# CFSP: An Efficient Structured Pruning Framework for LLMs with Coarse-to-Fine Activation Information

## Quick Facts
- **arXiv ID**: 2409.13199
- **Source URL**: https://arxiv.org/abs/2409.13199
- **Reference count**: 40
- **Key outcome**: CFSP achieves superior structured pruning results across multiple models and sparsity levels, maintaining strong performance even at 50% sparsity with up to 2.3x speed-up on CPU and 1.6x on GPU.

## Executive Summary
CFSP introduces an efficient structured pruning framework for Large Language Models that leverages coarse-to-fine activation information to identify and remove redundant components. The method first allocates sparsity budgets across blocks using angular distance between input/output activations, then prunes weights within blocks based on fine-grained activation-weight products. A recovery fine-tuning strategy with importance-guided LoRA allocation further enhances performance. Experimental results demonstrate CFSP's effectiveness across multiple models and sparsity levels, achieving significant speed-ups while maintaining model performance.

## Method Summary
CFSP is a structured pruning framework that operates through a coarse-to-fine importance criterion. The method first calibrates activations on a small dataset, then computes coarse-grained block importance using angular distance between input/output activations to allocate sparsity budgets. Within each block, fine-grained importance scores are calculated using products of relative activations and weights to identify redundant components for pruning. Finally, a recovery fine-tuning stage with importance-guided LoRA allocation restores performance, achieving efficient inference while maintaining strong task performance.

## Key Results
- CFSP maintains promising performance even at 50% sparsity across multiple models and tasks
- Achieves up to 2.3x speed-up on CPU and 1.6x on GPU compared to unpruned models
- Outperforms state-of-the-art pruning methods on knowledge-intensive tasks and LLaMA3 models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Coarse-grained block importance is computed from angular distance between input/output activations, enabling sparsity budget allocation per block.
- **Mechanism**: Angular distance in Equation 2 captures the saliency of transformation performed by each block, reflecting its functional importance.
- **Core assumption**: Residual structure means each block's effect is a transformation of input representations, so angular distance measures functional significance.
- **Evidence anchors**:
  - [abstract]: "We first allocate the sparsity budget across blocks based on their importance and then retain important weights within each block."
  - [section]: "Thus, we measure the coarse-grained importance of blocks Sg through the saliency of transformation of feature activations during the forward process."
  - [corpus]: Weak - corpus neighbors discuss pruning but don't address angular distance importance measurement.
- **Break condition**: If block importance scores don't correlate with functional importance, sparsity budget allocation becomes suboptimal.

### Mechanism 2
- **Claim**: Fine-grained weight importance uses product of relative activations and weights, removing redundant weights within blocks.
- **Mechanism**: Equation 8 combines activation magnitudes with weight magnitudes across matrices Wu, Wg, and Wd to identify important weights within each block.
- **Core assumption**: Weights corresponding to larger activation magnitudes are more salient since they process more important features.
- **Evidence anchors**:
  - [abstract]: "we take the product of their relative activations and weights as a fine-grained criterion to remove redundant parts."
  - [section]: "For the i-th intermediate dimension, we utilize the activation of Xd of Wd and all weight matrices to calculate fine-grained importance score Si l."
  - [corpus]: Weak - corpus neighbors mention pruning methods but don't discuss this specific activation-weight product criterion.
- **Break condition**: If activation-weight products don't correlate with actual importance, pruning removes important weights.

### Mechanism 3
- **Claim**: Importance-guided LoRA recovery fine-tuning adaptively allocates training overhead based on coarse-grained importance scores.
- **Mechanism**: Equation 9 uses normalized block importance scores to determine rank sizes of LoRA adapters for each block.
- **Core assumption**: Blocks with higher importance need more trainable parameters during recovery to maintain performance.
- **Evidence anchors**:
  - [abstract]: "we introduce a recovery fine-tuning strategy that adaptively allocates training overhead based on coarse-grained importance."
  - [section]: "For the l-th block, the rank rl of LoRA is determined based on the coarse-grained importance scores computed during pruning."
  - [corpus]: Weak - corpus neighbors discuss pruning but don't address adaptive LoRA allocation based on importance scores.
- **Break condition**: If adaptive allocation doesn't improve recovery efficiency compared to uniform allocation.

## Foundational Learning

- **Concept**: Angular distance as similarity metric
  - Why needed here: Used to measure coarse-grained block importance by comparing input/output activations
  - Quick check question: Why might angular distance be preferred over Euclidean distance for measuring activation similarity?

- **Concept**: Activation sparsity and importance correlation
  - Why needed here: Forms the basis for both coarse and fine-grained importance criteria in CFSP
  - Quick check question: How does activation magnitude relate to the importance of corresponding weights?

- **Concept**: Low-rank adaptation (LoRA)
  - Why needed here: Used in recovery fine-tuning with importance-guided rank allocation
  - Quick check question: What is the key advantage of LoRA over full fine-tuning for large models?

## Architecture Onboarding

- **Component map**: Calibration module -> Coarse importance module -> Fine importance module -> Pruning module -> Recovery module
- **Critical path**: Calibration → Coarse importance → Fine importance → Pruning → Recovery
- **Design tradeoffs**:
  - Single forward pass vs multiple passes for better accuracy
  - Global vs local pruning unit sorting (CFSP uses local)
  - Dimension adjustment for GPU efficiency vs maintaining exact dimensions
- **Failure signatures**:
  - Low correlation between importance scores and actual performance
  - Significant performance drop after pruning despite high importance scores
  - Recovery fine-tuning fails to restore performance
- **First 3 experiments**:
  1. Verify angular distance correctly identifies important blocks by pruning high vs low importance blocks separately
  2. Test fine-grained importance criterion by comparing against magnitude-only pruning
  3. Validate recovery efficiency by comparing importance-guided LoRA vs uniform LoRA with same training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CFSP perform on models outside the LLaMA family, such as those with grouped query attention (GQA) like LLaMA3?
- Basis in paper: [explicit] The paper mentions that CFSP focuses on LLaMA family models and does not prune attention heads, as this has been shown to cause significant performance degradation, especially for models with GQA.
- Why unresolved: The paper does not provide experimental results on models with GQA, limiting the generalizability of CFSP to these architectures.
- What evidence would resolve it: Experimental results showing CFSP's performance on models with GQA, comparing it to other pruning methods and assessing its effectiveness on these architectures.

### Open Question 2
- Question: How does the choice of angular distance in the coarse-grained importance calculation affect CFSP's performance compared to other distance metrics?
- Basis in paper: [explicit] The paper states that angular distance is used for measuring the coarse-grained importance of blocks, but it does not provide a comprehensive comparison with other distance metrics like Euclidean distance or cosine similarity.
- Why unresolved: While the paper mentions that angular distance performs better than others in their experiments, it does not provide a detailed analysis of why this is the case or how it compares to other metrics in different scenarios.
- What evidence would resolve it: A thorough comparison of different distance metrics in the coarse-grained importance calculation, including their impact on performance, computational efficiency, and sensitivity to model architecture and task.

### Open Question 3
- Question: How does the performance of CFSP change with different sparsity levels, and what is the optimal sparsity ratio for balancing performance and efficiency?
- Basis in paper: [explicit] The paper presents results for sparsity levels of 20% and 50%, showing that CFSP maintains promising performance even at high sparsity. However, it does not explore a wider range of sparsity levels or provide a detailed analysis of the trade-off between performance and efficiency.
- Why unresolved: The paper does not provide a comprehensive study of CFSP's performance across a broader range of sparsity levels, making it difficult to determine the optimal sparsity ratio for different applications and model architectures.
- What evidence would resolve it: Experimental results showing CFSP's performance across a wide range of sparsity levels, including a detailed analysis of the trade-off between performance and efficiency, and recommendations for optimal sparsity ratios based on model architecture and task requirements.

## Limitations
- The angular distance assumption for measuring block importance may not hold for all transformer architectures
- Limited ablation studies comparing angular distance to simpler importance metrics
- Focus on LLaMA family models without comprehensive testing on models with GQA or other architectures
- Recovery fine-tuning efficiency gains lack comparison against uniform allocation strategies

## Confidence
- **High Confidence**: The overall effectiveness of CFSP in achieving speed-ups (up to 2.3x on CPU, 1.6x on GPU) and maintaining performance at various sparsity levels is well-supported by experimental results across multiple models.
- **Medium Confidence**: The coarse-to-fine importance criterion using angular distance and activation-weight products is conceptually sound and shows good results, but lacks comprehensive ablation studies comparing alternative importance metrics.
- **Medium Confidence**: The importance-guided LoRA recovery strategy is innovative and shows efficiency gains, but the paper doesn't provide sufficient evidence that this approach is superior to simpler recovery methods.

## Next Checks
1. Conduct an ablation study comparing angular distance-based importance with magnitude-based importance for coarse-grained block selection to validate the necessity of the angular distance approach.
2. Perform experiments with different residual architectures to test whether the angular distance assumption holds universally across various transformer designs.
3. Compare the importance-guided LoRA recovery strategy against uniform LoRA allocation and other fine-tuning approaches to quantify the actual efficiency gains and validate the adaptive allocation assumption.