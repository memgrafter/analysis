---
ver: rpa2
title: "An Empirical Study of $\u03BC$P Learning Rate Transfer"
arxiv_id: '2404.05728'
source_url: https://arxiv.org/abs/2404.05728
tags:
- learning
- https
- transfer
- rate
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether $\mu$-Transfer, a technique for
  transferring learning rates from small to large models, reliably yields near-optimal
  learning rates in practice. Using transformer language models, the study conducts
  extensive ablations with models up to 1.2B parameters and 33B tokens, as well as
  a large-scale experiment with models up to 10B parameters and 190B tokens.
---

# An Empirical Study of $μ$P Learning Rate Transfer

## Quick Facts
- arXiv ID: 2404.05728
- Source URL: https://arxiv.org/abs/2404.05728
- Authors: Lucas Lingle
- Reference count: 40
- Primary result: µ-Transfer reliably predicts optimal learning rates for large transformer language models when using nonparametric RMSNorm and 1/D attention scale

## Executive Summary
This paper investigates the reliability of µ-Transfer, a technique for transferring learning rates from small proxy models to larger models, using transformer language models. Through extensive ablations with models up to 1.2B parameters and large-scale experiments up to 10B parameters, the study finds that µ-Transfer works reliably in most settings. The research identifies specific failure modes related to parametric RMSNorm and standard attention scaling, while also revealing that weight decay can prevent successful transfer.

## Method Summary
The study conducts empirical experiments with transformer language models, systematically varying model sizes from small proxies to large models up to 10B parameters. The researchers test different normalization methods (parametric vs. nonparametric RMSNorm), attention scaling schemes (1/√D vs. 1/D), and weight decay configurations. Learning rate optimization is performed through grid search across a range of values, with the optimal rate from small models used to predict rates for larger models. Experiments use up to 190B tokens for training, with careful ablation studies to isolate the effects of different architectural choices on µ-Transfer reliability.

## Key Results
- µ-Transfer reliably predicts optimal learning rates for large transformer models in most configurations
- Parametric RMSNorm and standard 1/√D attention scale prevent successful transfer
- Nonparametric RMSNorm with 1/D attention scale restores reliable µ-Transfer
- Weight decay can prevent transfer, suggesting either omitting it from proxy models or using independent weight decay

## Why This Works (Mechanism)
None provided in the paper

## Foundational Learning
- µP (micro-parameterization): Why needed - enables consistent hyperparameter transfer across model scales; Quick check - verify that μ-schedules maintain scale invariance
- RMSNorm variants: Why needed - normalization affects gradient dynamics and learning rate sensitivity; Quick check - compare gradient magnitudes across normalization types
- Attention scaling: Why needed - scale choice impacts gradient variance and optimization stability; Quick check - measure gradient variance with different attention scales
- Weight decay interaction: Why needed - regularization strength varies with model size and affects optimal learning rates; Quick check - test transfer with and without weight decay

## Architecture Onboarding
Component map: Transformer layers -> RMSNorm -> Attention -> Feed-forward -> Weight decay
Critical path: Model scaling → Normalization choice → Attention scale → Learning rate transfer
Design tradeoffs: Parametric vs. nonparametric normalization (stability vs. flexibility), attention scale choice (gradient control vs. standard practice)
Failure signatures: Transfer failure with parametric RMSNorm, transfer failure with 1/√D attention, learning rate mismatch with weight decay
First experiments: 1) Test transfer with different RMSNorm variants, 2) Compare attention scales 1/√D vs 1/D, 3) Evaluate weight decay impact on transfer

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the choice of attention scale (τ) affect the reliability of μ-transfer across different model sizes?
- Basis in paper: The paper notes that using the standard attention scale of 1/√D prevents learning rate transfer, while

## Limitations
- Findings limited to transformer language models up to 10B parameters, with unknown generalizability to other architectures
- Weight decay investigation restricted to AdamW with decoupled weight decay, excluding other optimizer variants
- Focus on learning rate optimization without examining interactions with other hyperparameters

## Confidence
High confidence in core µ-Transfer reliability findings for standard transformers
High confidence in identification of parametric RMSNorm and 1/√D attention as failure modes
Medium confidence in weight decay findings due to limited investigation scope

## Next Checks
1. Test µ-Transfer reliability across different transformer variants (BERT, T5, encoder-decoder models) and non-transformer architectures to assess generalizability beyond GPT-style models
2. Investigate the interaction between learning rate transfer and other hyperparameters (dropout rates, activation functions, optimizer variants) to determine if µ-Transfer effectiveness depends on these settings
3. Conduct experiments with non-language modeling tasks (computer vision, multimodal models) to evaluate whether µ-Transfer's reliability extends to different domains and loss landscapes