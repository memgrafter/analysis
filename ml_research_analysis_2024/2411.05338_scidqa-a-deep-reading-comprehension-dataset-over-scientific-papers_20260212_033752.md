---
ver: rpa2
title: 'SciDQA: A Deep Reading Comprehension Dataset over Scientific Papers'
arxiv_id: '2411.05338'
source_url: https://arxiv.org/abs/2411.05338
tags:
- questions
- dataset
- answer
- answers
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SciDQA, a deep reading comprehension dataset
  over scientific papers consisting of 2,937 QA pairs. The dataset sources questions
  from peer reviews by domain experts and answers by paper authors, ensuring a thorough
  examination of the literature.
---

# SciDQA: A Deep Reading Comprehension Dataset over Scientific Papers

## Quick Facts
- arXiv ID: 2411.05338
- Source URL: https://arxiv.org/abs/2411.05338
- Authors: Shruti Singh; Nandan Sarkar; Arman Cohan
- Reference count: 40
- Dataset size: 2,937 QA pairs from peer reviews and author responses

## Executive Summary
This paper introduces SciDQA, a deep reading comprehension dataset over scientific papers that sources questions from peer reviews by domain experts and answers from paper authors. The dataset requires multi-document reasoning across figures, tables, equations, appendices, and supplementary materials. The authors evaluate several open-source and proprietary LLMs across closed-book, retrieval-based, and long-context reasoning configurations, finding that proprietary models like GPT-4o achieve the highest scores but still struggle with many questions. The dataset represents a rigorously curated, naturally derived scientific QA dataset designed to facilitate research on complex scientific text understanding.

## Method Summary
The authors construct SciDQA by extracting QA pairs from peer review conversations between domain experts and paper authors. They implement quality enhancement through careful filtering, decontextualization, source document tracking, and bibliography incorporation for multi-document QA. The dataset requires reasoning across various scientific document elements including figures, tables, equations, appendices, and supplementary materials. The evaluation framework tests models in three configurations: closed-book (no document access), retrieval-based (document retrieval), and long-context reasoning (document access within context limits). The evaluation combines surface-level similarity metrics with LLM-based judgment to assess both relevance and factual accuracy of generated responses.

## Key Results
- Proprietary models like GPT-4o outperform open-source models across all configurations
- All models struggle with multi-document reasoning and scientific content comprehension
- Closed-book configurations perform poorly, highlighting the importance of document access
- Long-context reasoning shows promise but remains limited by context window constraints
- Retrieval-based approaches improve performance but require careful implementation

## Why This Works (Mechanism)
The dataset leverages naturally occurring expert-annotated QA pairs from peer review processes, ensuring questions require genuine scientific understanding rather than pattern recognition. The multi-document reasoning requirement forces models to integrate information across different document sections and sources, closely mimicking real scientific comprehension tasks. The inclusion of figures, tables, equations, and supplementary materials creates complexity that tests models' ability to process diverse scientific content formats.

## Foundational Learning
- **Scientific Document Structure**: Understanding how academic papers are organized (why needed: enables navigation and context identification; quick check: can the model locate relevant sections given a question)
- **Multi-Document Integration**: Ability to combine information from multiple sources (why needed: many scientific questions require cross-referencing; quick check: can the model synthesize information from figures and text)
- **Domain-Specific Reasoning**: Knowledge of scientific concepts and methodologies (why needed: scientific questions often require specialized understanding; quick check: does the model correctly interpret technical terminology)
- **Long-Context Processing**: Handling extended document contexts (why needed: scientific papers often exceed standard context windows; quick check: can the model maintain coherence across large document spans)
- **Bibliographic Navigation**: Ability to use references and citations (why needed: scientific knowledge builds on prior work; quick check: can the model correctly follow citation chains)
- **Supplementary Material Processing**: Integration of appendices and supplementary data (why needed: crucial information often resides outside main text; quick check: can the model access and interpret supplementary content)

## Architecture Onboarding

**Component Map**: Question Encoder -> Document Retriever -> Context Processor -> Answer Generator -> Evaluation Module

**Critical Path**: Question -> Retrieval (if applicable) -> Document Processing -> Answer Generation -> Evaluation

**Design Tradeoffs**: The authors chose expert-annotated data over synthetic generation to ensure genuine scientific complexity, accepting the limitation of smaller dataset size. They implemented three evaluation configurations to comprehensively test model capabilities, trading evaluation complexity for thorough assessment.

**Failure Signatures**: Models struggle with questions requiring integration across multiple document sections, fail to correctly interpret technical terminology, cannot effectively use bibliographic references, and perform poorly when answers require understanding of figures or equations.

**First Experiments**:
1. Evaluate zero-shot performance of open-source models on the full dataset to establish baseline capabilities
2. Test retrieval-augmented generation with different retrieval strategies to assess document access benefits
3. Compare long-context reasoning performance across varying context window sizes to identify optimal configuration

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size (2,937 QA pairs) may be insufficient for training large models
- Proprietary models evaluated (GPT-4o) introduce accessibility concerns for reproducibility
- Evaluation metrics combining surface-level similarity with LLM judgments may not fully capture scientific accuracy
- Dataset focus on peer review contexts may limit representativeness of broader scientific reading comprehension

## Confidence
- Dataset curation methodology: High
- Multi-document reasoning capability: High
- Evaluation results (relative performance): Medium
- Claims about scientific reasoning depth: Medium
- Generalizability to other scientific domains: Low

## Next Checks
1. Test whether GPT-4o's superior performance stems from genuine scientific understanding versus pattern matching by analyzing error cases requiring domain-specific knowledge versus general reasoning
2. Conduct human expert evaluation of a random sample of model responses to validate the LLM-based judgment metrics and assess factual accuracy in scientific claims
3. Evaluate the dataset's effectiveness for training by fine-tuning smaller open-source models and measuring performance gains relative to their zero-shot baselines