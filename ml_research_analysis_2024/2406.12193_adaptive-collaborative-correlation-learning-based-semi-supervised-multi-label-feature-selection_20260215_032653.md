---
ver: rpa2
title: Adaptive Collaborative Correlation Learning-based Semi-Supervised Multi-Label
  Feature Selection
arxiv_id: '2406.12193'
source_url: https://arxiv.org/abs/2406.12193
tags:
- s101
- feature
- s116
- selection
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of feature selection for high-dimensional
  semi-supervised multi-label data, where some labels are missing. The proposed method,
  Access-MFS, introduces an adaptive collaborative correlation learning approach that
  simultaneously learns sample similarity and label correlation graphs directly from
  the data, rather than relying on predefined graphs.
---

# Adaptive Collaborative Correlation Learning-based Semi-Supervised Multi-Label Feature Selection

## Quick Facts
- arXiv ID: 2406.12193
- Source URL: https://arxiv.org/abs/2406.12193
- Reference count: 40
- Primary result: Access-MFS outperforms state-of-the-art methods on 8 real-world datasets for semi-supervised multi-label feature selection

## Executive Summary
This paper introduces Access-MFS, a novel method for feature selection in high-dimensional semi-supervised multi-label data with missing labels. The approach uses adaptive collaborative correlation learning to simultaneously learn sample similarity and label correlation graphs directly from data, avoiding reliance on predefined graphs. A generalized regression model with an extended uncorrelated constraint selects discriminative and uncorrelated features while maintaining consistency between predicted and ground-truth labels in labeled data. Extensive experiments demonstrate superior performance compared to state-of-the-art methods across multiple evaluation metrics.

## Method Summary
Access-MFS addresses feature selection for semi-supervised multi-label data by learning two similarity-induced graphs - one for sample similarity and one for label correlation - through adaptive optimization. The method employs a generalized regression model with an extended uncorrelated constraint that ensures selected features are both discriminative and uncorrelated. The semi-supervised framework leverages both labeled and unlabeled data through label prediction consistency, allowing feature selection even when many labels are missing. The adaptive collaborative learning of the two graphs mutually enhances feature selection performance.

## Key Results
- Access-MFS achieves superior performance on Average Precision, Macro-F1, Ranking Loss, and One Error metrics
- The method outperforms state-of-the-art approaches across varying numbers of selected features (100-200)
- Access-MFS demonstrates robust performance across different percentages of labeled instances (10-50%)
- Experiments on eight real-world datasets validate the effectiveness of the adaptive collaborative learning approach

## Why This Works (Mechanism)

### Mechanism 1
- Adaptive collaborative learning of sample similarity and label similarity graphs enhances feature selection by preserving local geometric structures in both feature and label spaces.
- Core assumption: Similar instances should have similar labels (smoothness assumption), and strong label correlations should produce consistent predicted labels (label correlation assumption).
- Evidence anchors: The adaptive collaborative learning module preserves sample similarity from high-dimensional to low-dimensional space, ensuring similar samples receive similar labels and strong label correlations lead to consistent predictions.

### Mechanism 2
- The extended uncorrelated constraint WT R W= I selects discriminative yet uncorrelated features while avoiding trivial solutions.
- Core assumption: Selected features should be discriminative for label prediction and uncorrelated to avoid redundancy.
- Evidence anchors: The constraint combines orthonormality of projected dimensions, singularity prevention, and reduction of variance within neighborhoods to ensure selected features are both discriminative and uncorrelated.

### Mechanism 3
- The semi-supervised setting allows leveraging unlabeled data through label prediction consistency, improving feature selection without requiring all labels.
- Core assumption: Unlabeled data contain useful structure that can guide feature selection when combined with limited labeled data.
- Evidence anchors: The model introduces a predicted label matrix F, optimized alongside the feature selection matrix W, with a consistency constraint ensuring predicted labels for labeled data match ground truth.

## Foundational Learning

- **Graph Laplacian regularization**: Used to encode similarity structure in both sample and label spaces through Laplacian matrices Ls and Lp, enforcing smoothness in learned representations. Quick check: What is the effect of adding Tr(FT LsF) to the objective function in terms of sample similarity preservation?

- **ℓ2,1-norm regularization for feature selection**: Applied to projection matrix W to induce row sparsity, enabling selection of most informative features while maintaining group structure. Quick check: How does the ℓ2,1-norm differ from standard ℓ1-norm in terms of feature selection behavior?

- **Karush-Kuhn-Tucker (KKT) conditions for constrained optimization**: Used to derive closed-form solutions for variables like b and set up Lagrangian for solving S and P with non-negativity and sum-to-one constraints. Quick check: What are the KKT conditions that must be satisfied at the optimal solution for the variable S?

## Architecture Onboarding

- **Component map**: Input (X, Y) -> Adaptive graph learning (S, P) -> Generalized regression with extended uncorrelated constraint -> Output (W, F)
- **Critical path**: Data preprocessing -> Alternating optimization (W update → F update → S update → P update) -> Feature selection and prediction
- **Design tradeoffs**: Fixed vs. adaptive graph learning (speed vs. robustness), supervised vs. semi-supervised (labeled data requirement vs. better generalization), feature correlation vs. independence (avoiding redundancy vs. discarding useful correlated features)
- **Failure signatures**: Convergence issues (check KKT conditions), poor feature selection (verify graph learning quality), label prediction failure (check consistency constraint weight)
- **First 3 experiments**: 1) Verify convergence on small synthetic dataset with known ground truth, 2) Test sensitivity to labeled/unlabeled ratio (10-50%), 3) Compare with baselines (All-Fea, LFFS) on Enron dataset using AP and MaF metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Access-MFS perform on datasets with very high label density compared to sparse labels?
- Basis: The paper mentions varying label densities but doesn't analyze performance differences based on density.
- What evidence would resolve it: Comparative experiments on high and low label density datasets showing performance metrics.

### Open Question 2
- Question: What is the impact of initial parameter choices on convergence and final performance?
- Basis: The paper mentions parameter sensitivity but doesn't explore how different initial choices affect outcomes.
- What evidence would resolve it: Experiments varying initial parameter values and analyzing impact on convergence speed and final accuracy.

### Open Question 3
- Question: How does Access-MFS handle datasets with tens of thousands of features compared to fewer features?
- Basis: The paper tests varying feature numbers but doesn't address extremely high-dimensional datasets.
- What evidence would resolve it: Experiments on datasets with significantly larger feature counts comparing performance and computational efficiency.

### Open Question 4
- Question: What is the effect of the number of nearest neighbors (ks and kp) on Access-MFS performance?
- Basis: The paper mentions nearest neighbors in similarity graph learning but doesn't analyze how different values affect performance.
- What evidence would resolve it: Experiments varying the number of nearest neighbors and analyzing impact on accuracy and efficiency.

## Limitations

- Parameter settings for compared baseline methods are not fully specified beyond mentioned ranges
- Potential overfitting to selected datasets without testing on diverse domain data
- Computational scalability for extremely high-dimensional datasets (millions of features) not addressed

## Confidence

- Adaptive collaborative learning mechanism: Medium confidence - theoretically sound with strong experimental support
- Extended uncorrelated constraint: Medium confidence - well-supported by mathematical formulation and experiments
- Semi-supervised effectiveness: Medium confidence - demonstrated across multiple datasets but parameter sensitivities not fully explored

## Next Checks

1. **Ablation study validation**: Systematically remove each component (adaptive graph learning, extended uncorrelated constraint, semi-supervised consistency) to quantify their individual contributions to performance gains.

2. **Parameter sensitivity analysis**: Conduct comprehensive experiments varying λ, θ, and the labeled/unlabeled ratio to identify robustness boundaries and optimal operating conditions.

3. **Cross-domain Generalization test**: Apply Access-MFS to datasets from domains not represented in current experiments (e.g., genomics, image data) to assess domain transferability and potential overfitting to specific dataset characteristics.