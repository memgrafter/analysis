---
ver: rpa2
title: An Empirical Study on the Robustness of Massively Multilingual Neural Machine
  Translation
arxiv_id: '2405.07673'
source_url: https://arxiv.org/abs/2405.07673
tags:
- translation
- noise
- evaluation
- types
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically studies the robustness of Indonesian-Chinese
  translation using massively multilingual NMT models. The authors curate a noisy
  parallel dataset of Indonesian-Chinese sentences from social media, manually annotating
  noise types.
---

# An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation

## Quick Facts
- arXiv ID: 2405.07673
- Source URL: https://arxiv.org/abs/2405.07673
- Authors: Supryadi; Leiyu Pan; Deyi Xiong
- Reference count: 0
- Key outcome: This paper empirically studies the robustness of Indonesian-Chinese translation using massively multilingual NMT models. The authors curate a noisy parallel dataset of Indonesian-Chinese sentences from social media, manually annotating noise types. They evaluate four NLLB-200 model variants on this dataset using automatic metrics (BLEU, CHRF++) and human evaluation with MQM. Key findings include: translation errors are correlated with noise types, larger models generally perform better especially on longer sentences, and CHRF++ is more correlated with human evaluation than BLEU. The analysis reveals that models improve on grammar, slang, and spoken language noise types as size increases, but struggle more with omission errors from slang, dialect, and slurs.

## Executive Summary
This paper presents an empirical study on the robustness of massively multilingual neural machine translation (MMNMT) models, focusing on Indonesian-Chinese translation under naturally occurring noise from social media. The authors create a noisy parallel corpus of 1001 Indonesian sentences crawled from Twitter, manually translated to Chinese, with noise types annotated across 10 categories. They evaluate four variants of the NLLB-200 model (600M, 1.3B distilled, 1.3B, and 3.3B parameters) using both automatic metrics (BLEU, CHRF++) and human evaluation with MQM. The study reveals that translation errors correlate with specific noise types, larger models generally perform better especially on longer sentences, and CHRF++ demonstrates stronger correlation with human evaluation than BLEU.

## Method Summary
The study evaluates robustness of MMNMT models using a noisy parallel corpus created from Indonesian social media text. The methodology involves: 1) Crawling 2000 Indonesian sentences from Twitter and filtering to 1001 noisy sentences using language models; 2) Manually translating these to Chinese and annotating 10 noise types (spelling/typo, grammar, spoken language, slang, proper noun, dialect, code switching, jargon, emojis, slurs); 3) Translating the Indonesian sentences to Chinese using four NLLB-200 model variants (600M, 1.3B distilled, 1.3B, 3.3B parameters); 4) Evaluating translations using automatic metrics (BLEU, CHRF++) and human evaluation with MQM; 5) Analyzing correlation between noise types and translation errors across model sizes.

## Key Results
- Translation errors are correlated with specific noise types, with models performing differently across grammar, slang, spoken language, and other categories
- Larger NLLB-200 models (3.3B parameters) generally perform better than smaller variants, especially on longer sentences
- CHRF++ demonstrates stronger correlation with human evaluation scores than BLEU, suggesting better alignment with translation quality assessment
- Models show improvement on grammar, slang, and spoken language noise types as size increases, but struggle more with omission errors from slang, dialect, and slurs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger NLLB-200 models (more parameters) improve translation robustness by better capturing linguistic patterns and handling noisy input.
- Mechanism: Increasing model size allows for more complex representations and broader contextual understanding, which helps disambiguate noisy tokens and maintain translation quality.
- Core assumption: The additional parameters enable the model to better handle variations in language use, including slang, grammar errors, and code-switching.
- Evidence anchors:
  - [abstract] "larger models generally perform better especially on longer sentences"
  - [section 7.1] "As the model size expands to 3.3B, there is a reduction in the occurence of most error types, with the exception of mistranslation, which remains stable, and addition, which experiences a substantial increase"
- Break condition: If the increase in model size introduces more hallucination or addition errors, or if the model overfits to noise patterns in the training data.

### Mechanism 2
- Claim: CHRF++ correlates better with human evaluation than BLEU for assessing translation quality in noisy text.
- Mechanism: CHRF++ uses character n-grams which are more sensitive to surface form variations and can better capture the impact of noise on translation quality compared to word-level BLEU.
- Core assumption: Character-level evaluation is more robust to noise-induced word variations and spelling errors.
- Evidence anchors:
  - [abstract] "CHRF++ is more correlated with human evaluation than BLEU"
  - [section 7.3] "we calculated Pearson's correlation coefficient between the scores obtained from the automatic evaluation indicators and the number of translation errors identified by the human evaluation metrics. The results are presented in Table 6. Based on these findings, we can conclude that CHRF++ demonstrates a stronger correlation with human evaluation when compared to the BLEU score"
- Break condition: If the noise introduces character-level errors that significantly alter meaning but maintain surface similarity, or if the human evaluation criteria change.

### Mechanism 3
- Claim: Different noise types (grammar, slang, spoken language) affect translation quality differently, and models improve on some noise types more than others as they scale.
- Mechanism: The model learns to handle frequent noise patterns (like slang and spoken language) better as it scales, while struggling more with less frequent or more complex noise (like dialect and slurs).
- Core assumption: The frequency and complexity of noise types in the training data influence the model's ability to handle them.
- Evidence anchors:
  - [abstract] "translation errors are correlated with noise types, larger models generally perform better especially on longer sentences"
  - [section 7.2] "In Table 5, the occurrence of translation error types based on different noise types is similar across various model sizes. With the exception of addition, untranslated, and punctuation, several models are affected by grammar or slang"
  - [section 7.4] "It can be observed that as the model size increases, a clear reduction in the number of translation errors related to grammar, slang, spoken language, and proper nouns noise types is evident"
- Break condition: If the noise types in the evaluation data are not representative of real-world noise patterns, or if the model's improvement on certain noise types plateaus.

## Foundational Learning

- Concept: Neural Machine Translation (NMT) basics
  - Why needed here: Understanding how NMT models work is crucial for interpreting the robustness evaluation and the impact of model size.
  - Quick check question: What is the key difference between NMT and traditional phrase-based statistical machine translation?

- Concept: Evaluation metrics (BLEU, CHRF++, MQM)
  - Why needed here: The paper uses these metrics to assess translation quality and robustness, so understanding their strengths and limitations is important.
  - Quick check question: How does CHRF++ differ from BLEU in terms of what it measures?

- Concept: Noise types in natural language (spelling errors, grammar errors, slang, etc.)
  - Why needed here: The paper evaluates robustness to different types of noise, so understanding these noise categories is essential for interpreting the results.
  - Quick check question: Why might slang and spoken language be more challenging for NMT models than spelling errors?

## Architecture Onboarding

- Component map: Twitter Indonesian corpus -> Noise filtering and annotation -> NLLB-200 model variants (600M, 1.3B distilled, 1.3B, 3.3B) -> Automatic evaluation (BLEU, CHRF++) + Human evaluation (MQM) -> Error analysis across noise types
- Critical path: 1) Collect and filter noisy Indonesian text from Twitter, 2) Annotate noise types, 3) Translate to Chinese manually and automatically, 4) Evaluate translations using BLEU, CHRF++, and MQM, 5) Analyze the correlation between noise types and translation errors across model sizes
- Design tradeoffs: Larger models offer better robustness but require more computational resources. Character-level evaluation (CHRF++) is more robust to noise but may miss semantic nuances.
- Failure signatures: High BLEU scores but low CHRF++ scores could indicate that the model is generating fluent but inaccurate translations. High rates of omission or addition errors could indicate that the model is struggling with noisy input.
- First 3 experiments:
  1. Replicate the automatic evaluation (BLEU, CHRF++) on a subset of the data to verify the results.
  2. Conduct a small-scale human evaluation using MQM on a subset of translations to understand the types of errors the model is making.
  3. Analyze the distribution of noise types in the evaluation data to understand which types are most challenging for the model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different noise types specifically impact the robustness of multilingual NMT models?
- Basis in paper: [explicit] The paper identifies various noise types (e.g., spelling errors, slang, spoken language) and their impact on translation errors.
- Why unresolved: The paper provides a general overview but lacks a detailed analysis of the specific effects of each noise type on model performance.
- What evidence would resolve it: A detailed analysis of the impact of each noise type on translation quality, possibly through controlled experiments varying only the noise type.

### Open Question 2
- Question: How does the size of the NMT model influence its ability to handle specific types of noise?
- Basis in paper: [explicit] The paper evaluates different model sizes (600M, 1.3B, 3.3B parameters) and observes varying performance across noise types.
- Why unresolved: While trends are noted, the paper doesn't provide a comprehensive analysis of how model size specifically affects handling different noise types.
- What evidence would resolve it: A systematic study comparing model performance on each noise type across different model sizes, possibly using statistical methods to quantify the relationship.

### Open Question 3
- Question: How do automatic evaluation metrics (BLEU, CHRF++) correlate with human evaluation metrics in assessing the robustness of NMT models to noise?
- Basis in paper: [explicit] The paper compares automatic and human evaluation results, finding that CHRF++ correlates more strongly with human evaluation than BLEU.
- Why unresolved: The paper provides correlation coefficients but doesn't explore the reasons behind the differences in correlation or the implications for model evaluation.
- What evidence would resolve it: Further analysis of the types of errors captured by each metric and their alignment with human judgments, possibly through error analysis or ablation studies.

## Limitations

- The study uses a relatively small evaluation dataset (1001 sentence pairs), which may not capture the full diversity of noise patterns in social media text.
- Manual noise annotation could introduce subjectivity in classifying noise types, particularly for borderline cases between categories like slang and spoken language.
- The focus on Indonesian-Chinese translation limits generalizability to other language pairs, especially those with different typological characteristics or script systems.

## Confidence

**High Confidence:**
- The correlation between model size and translation quality improvements, supported by consistent automatic and human evaluation results across multiple model variants.
- The superior correlation of CHRF++ with human evaluation compared to BLEU, demonstrated through statistical analysis of Pearson correlation coefficients.

**Medium Confidence:**
- The differential impact of noise types on translation quality, as the classification and frequency of noise types may be influenced by the specific Twitter corpus characteristics.
- The observation that larger models handle certain noise types better than others, as this conclusion depends on the noise type distribution in the evaluation data.

**Low Confidence:**
- The generalizability of findings to other language pairs beyond Indonesian-Chinese, given the unique characteristics of this language pair and the specific noise patterns in Indonesian social media.

## Next Checks

1. **Replication on Larger Dataset:** Replicate the evaluation using a significantly larger Indonesian-Chinese parallel corpus (e.g., 10,000+ sentences) to verify whether the observed trends in model performance and noise type effects remain consistent at scale.

2. **Cross-Lingual Robustness Test:** Apply the same evaluation framework to a different language pair (e.g., English-Spanish or French-German) to assess the generalizability of the findings regarding model size effects and evaluation metric correlations.

3. **Inter-Annotator Agreement Analysis:** Conduct a formal inter-annotator agreement analysis on a subset of the dataset using Cohen's kappa or Fleiss' kappa to quantify the reliability of the noise type annotations and human evaluation scores.