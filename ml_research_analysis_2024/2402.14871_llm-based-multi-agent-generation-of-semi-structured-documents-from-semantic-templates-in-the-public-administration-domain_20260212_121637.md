---
ver: rpa2
title: LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic
  Templates in the Public Administration Domain
arxiv_id: '2402.14871'
source_url: https://arxiv.org/abs/2402.14871
tags:
- prompt
- document
- agent
- information
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating semi-structured
  documents in the Public Administration domain, where templates are insufficient
  due to the flexible and context-dependent nature of missing information. The authors
  propose a multi-agent system powered by Large Language Models (LLMs) to automate
  document generation.
---

# LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain

## Quick Facts
- arXiv ID: 2402.14871
- Source URL: https://arxiv.org/abs/2402.14871
- Authors: Emanuele Musumeci; Michele Brienza; Vincenzo Suriani; Daniele Nardi; Domenico Daniele Bloisi
- Reference count: 17
- Primary result: Multi-agent LLM system automates semi-structured document generation in Public Administration, reducing manual intervention through iterative prompt refinement

## Executive Summary
This paper addresses the challenge of generating semi-structured documents in the Public Administration domain, where templates are insufficient due to the flexible and context-dependent nature of missing information. The authors propose a multi-agent system powered by Large Language Models (LLMs) to automate document generation. The system iteratively refines prompts through three specialized agents: Semantics Identification, Information Retrieval, and Content Generation. Each agent is engineered with tailored prompts to minimize hallucinations and improve output quality. The approach was tested on real-world PA scenarios, demonstrating its effectiveness in handling both schematic and discursive document sections. The method reduces the need for manual intervention and improves the adaptability of document generation pipelines.

## Method Summary
The method uses a three-agent LLM system where document generation proceeds through iterative prompt refinement. First, template pre-processing extracts structural information using Adobe Extraction API without interpreting semantics. The Semantics Identification agent then analyzes each template section to extract semantic instructions and identify replaceable data. The Information Retrieval agent checks the accumulated prompt for required information, returning either [ALL_INFO] or requesting missing data. Finally, the Content Generation agent produces document sections using the semantic instructions and available data. This cycle repeats for each template section, with user intervention only when necessary.

## Key Results
- Multi-agent prompt refinement effectively prevents hallucinations compared to monolithic prompting
- Iterative prompt accumulation reduces user intervention frequency by storing and reusing provided information
- Semantic template pre-processing enables flexible handling of diverse semi-structured document structures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent prompt refinement prevents hallucinations better than monolithic prompting.
- Mechanism: By decomposing the document generation task into specialized agents (Semantics Identification, Information Retrieval, Content Generation), each agent receives a focused, context-specific prompt that minimizes the risk of irrelevant or false information being introduced. The Information Retrieval agent uses a strict output format requiring either `[ALL_INFO]` or a specific missing-information token, which constrains hallucination.
- Core assumption: LLMs perform more reliably when given narrow, role-specific tasks with explicit output constraints.
- Evidence anchors:
  - [abstract]: "Each agent is engineered with tailored prompts to minimize hallucinations and improve output quality."
  - [section]: "The agent is instructed to return the [ALL_INFO] token to signal that it has managed to retrieve all the required information from the existing accumulated prompt."
  - [corpus]: No direct evidence; assumption based on observed prompt engineering practices.
- Break condition: If agent roles are not clearly defined or prompts are not sufficiently constrained, the system reverts to monolithic prompting risks.

### Mechanism 2
- Claim: Iterative prompt accumulation reduces user intervention frequency.
- Mechanism: The accumulated prompt serves as a growing knowledge base that stores all data provided by the user and retrieved by agents. Each generation step enriches this prompt, so subsequent steps can draw from it without requiring additional user input, as long as the necessary information was previously captured.
- Core assumption: User-provided data, once stored in the accumulated prompt, remains accessible and relevant for future retrieval tasks.
- Evidence anchors:
  - [abstract]: "The method reduces the need for manual intervention and improves the adaptability of document generation pipelines."
  - [section]: "After user intervention, the new data provided by the user is added to the accumulated prompt, to be used by the Content Generation agent as a data source."
  - [corpus]: No direct evidence; assumption based on the described workflow.
- Break condition: If the accumulated prompt becomes too large or noisy, retrieval accuracy may degrade, requiring user intervention again.

### Mechanism 3
- Claim: Semantic template pre-processing enables flexible document structure handling.
- Mechanism: By extracting the document structure (bounding boxes, text blocks) from the template without attempting to interpret field semantics, the system preserves the template's layout while allowing the agents to infer semantics dynamically during generation. This separation allows the same framework to handle diverse semi-structured documents.
- Core assumption: Structural cues are sufficient for agents to infer the required semantics during generation.
- Evidence anchors:
  - [section]: "The document structure can be extracted on a format-dependent basis, using pre-existing tools... It is not important to deduce the field semantics at this stage as we only need structural cues for the next steps."
  - [abstract]: "Semi-structured documents present a fixed set of data without a fixed format."
  - [corpus]: No direct evidence; assumption based on the described pre-processing approach.
- Break condition: If the template structure is ambiguous or lacks clear cues, agents may fail to infer correct semantics.

## Foundational Learning

- Concept: Prompt engineering principles
  - Why needed here: The system relies on carefully crafted prompts to guide each agent's behavior and constrain outputs to avoid hallucinations.
  - Quick check question: What is the key difference between a system prompt and a context-specific prompt in this architecture?

- Concept: Entity recognition and semantic parsing
  - Why needed here: The Semantics Identification agent must detect replaceable data in template sections, which requires understanding how entities and semantics are represented in text.
  - Quick check question: How does the use of LLMs for entity recognition differ from traditional NLP pipelines in this context?

- Concept: Multi-agent system design
  - Why needed here: The workflow is built around three specialized agents that must communicate and coordinate to generate coherent documents.
  - Quick check question: What are the three agent roles, and how does each contribute to the overall generation process?

## Architecture Onboarding

- Component map: User Interface -> Template Pre-processing -> Semantics Identification Agent -> Information Retrieval Agent -> Content Generation Agent -> Post-processing
- Critical path: User prompt → Template structure extraction → Semantics Identification → Information Retrieval (with possible user intervention) → Content Generation → Document assembly
- Design tradeoffs:
  - Prompt length vs. hallucination risk: Longer prompts improve context but increase hallucination risk; the multi-agent approach mitigates this by splitting context.
  - User intervention frequency vs. prompt quality: A better initial prompt reduces the need for intervention but requires more effort upfront.
  - Agent specialization vs. system complexity: More specialized agents improve accuracy but increase coordination overhead.
- Failure signatures:
  - Agent hallucinations: Output contains irrelevant or false information not present in the prompt.
  - Information retrieval failures: Agent returns missing-information tokens even when data exists in the accumulated prompt.
  - Semantic misinterpretation: Generated content does not match the intended meaning of the template section.
- First 3 experiments:
  1. Test each agent in isolation with controlled inputs to verify prompt engineering effectiveness.
  2. Run a full generation cycle with a simple template to confirm end-to-end workflow.
  3. Introduce ambiguous template sections to evaluate agent robustness and error handling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multi-agent approach compare to other methods like Chain-of-Thought or Retrieval-Augmented Generation in terms of accuracy and efficiency for semi-structured document generation?
- Basis in paper: [explicit] The paper mentions Chain-of-Thought and Retrieval-Augmented Generation as related work but does not provide a direct comparison with the proposed multi-agent approach.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the multi-agent approach but does not include a comparative analysis with other methods.
- What evidence would resolve it: Experimental results comparing the multi-agent approach with Chain-of-Thought and RAG methods in terms of accuracy, efficiency, and hallucination rates for semi-structured document generation.

### Open Question 2
- Question: What is the impact of prompt engineering quality on the overall performance of the multi-agent system, and how can this process be automated or improved?
- Basis in paper: [explicit] The paper emphasizes the importance of prompt engineering for each agent but does not explore ways to automate or optimize this process.
- Why unresolved: While the paper demonstrates successful prompt refinement, it does not address how to systematically improve or automate prompt engineering.
- What evidence would resolve it: Studies on automated prompt optimization techniques and their impact on multi-agent system performance, or methods to reduce the trial-and-error nature of prompt engineering.

### Open Question 3
- Question: How scalable is the multi-agent approach when dealing with very long documents or a large variety of document types in the Public Administration domain?
- Basis in paper: [inferred] The paper mentions that LLMs can struggle with longer documents and that the approach was tested on real-world PA scenarios, but does not explore scalability limits.
- Why unresolved: The paper does not provide data on performance with extremely long documents or diverse document types.
- What evidence would resolve it: Experiments testing the system with documents of varying lengths and complexities, measuring performance degradation and resource requirements as document size increases.

## Limitations

- Lack of quantitative evaluation with explicit metrics or accuracy scores to support claims about hallucination reduction
- Missing implementation details including specific prompt examples and decision criteria for user intervention thresholds
- Domain specificity constraints with no evidence of cross-domain applicability beyond Public Administration scenarios

## Confidence

- Low confidence: Claims about hallucination reduction - based on prompt engineering principles rather than measured outcomes
- Medium confidence: Iterative refinement mechanism - workflow is logically sound but unverified quantitatively
- Medium confidence: Multi-agent decomposition benefits - theoretically plausible but not empirically validated

## Next Checks

1. Implement the three-agent system with controlled test cases to verify that the Information Retrieval agent consistently identifies when all required information is present (returning [ALL_INFO] token) versus when user intervention is genuinely needed.

2. Conduct a quantitative comparison measuring hallucination rates between the multi-agent approach and monolithic prompting using identical templates and user prompts.

3. Test the system's performance with varying prompt lengths and complexity to establish the tradeoff between accumulated prompt size and retrieval accuracy, identifying the optimal balance point.