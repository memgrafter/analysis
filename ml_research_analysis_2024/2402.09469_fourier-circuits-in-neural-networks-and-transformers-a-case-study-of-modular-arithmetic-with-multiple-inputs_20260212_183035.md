---
ver: rpa2
title: 'Fourier Circuits in Neural Networks and Transformers: A Case Study of Modular
  Arithmetic with Multiple Inputs'
arxiv_id: '2402.09469'
source_url: https://arxiv.org/abs/2402.09469
tags:
- have
- learning
- fourier
- neural
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how neural networks and transformers solve
  modular arithmetic problems with multiple inputs. The authors focus on k-input modular
  addition and analyze the features learned by one-hidden-layer neural networks and
  one-layer transformers.
---

# Fourier Circuits in Neural Networks and Transformers: A Case Study of Modular Arithmetic with Multiple Inputs

## Quick Facts
- arXiv ID: 2402.09469
- Source URL: https://arxiv.org/abs/2402.09469
- Authors: Chenyang Li; Yingyu Liang; Zhenmei Shi; Zhao Song; Tianyi Zhou
- Reference count: 40
- One-line primary result: Networks with sufficient width (m ≥ 2^(2k-1) · (p-1)) learn Fourier-based circuits for k-input modular addition

## Executive Summary
This paper investigates how neural networks and transformers solve modular arithmetic problems with multiple inputs. The authors analyze the features learned by one-hidden-layer neural networks and one-layer transformers on k-input modular addition tasks, proving that when networks have sufficient width, each neuron aligns with a specific Fourier spectrum frequency. The maximum L2,k+1-margin for these datasets is γ* = 2(k!)/((2k+2)^(k+1)/2 · (p-1) · p^((k-1)/2)), and the analysis reveals that the grokking phenomenon weakens as k increases.

## Method Summary
The paper analyzes maximum margin solutions for one-hidden-layer neural networks and one-layer transformers on modular addition tasks. For neural networks, the authors prove that with sufficient width (m ≥ 2^(2k-1) · (p-1)), each hidden neuron aligns with a specific Fourier spectrum frequency, with one neuron per frequency. For transformers, they show that attention matrices learn to project inputs into Fourier space. The analysis uses sum-to-product trigonometric identities to construct maximum margin solutions and provides empirical validation through synthetic datasets.

## Key Results
- Networks with m ≥ 2^(2k-1) · (p-1) neurons achieve maximum L2,k+1-margin γ* = 2(k!)/((2k+2)^(k+1)/2 · (p-1) · p^((k-1)/2))
- Each hidden neuron aligns with exactly one Fourier frequency from {1, ..., (p-1)/2}
- Similar Fourier-based mechanisms emerge in one-layer transformers' attention matrices
- The grokking phenomenon weakens as k increases, consistent with the analysis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Networks prefer to learn Fourier circuits for modular arithmetic tasks when trained with sufficient width
- **Mechanism:** Maximum margin optimization drives neurons to align with specific Fourier spectrum frequencies, creating sparse representations in the Fourier domain
- **Core assumption:** The training objective converges to maximum margin solutions when λ is small enough (Lemma 3.7)
- **Evidence anchors:**
  - [abstract] "we demonstrate that a neuron count of m ≥ 22k−2 · (p−1) these networks attain a maximum L2,k+1-margin on the dataset Dp"
  - [section 4.1] "The maximum L2,k+1-margin for a dataset Dp is: γ∗ = 2(k!)/((2k+2)(k+1)/2(p−1)p(k−1)/2)"
  - [corpus] Weak - only mentions "Fourier representation" without explaining the mechanism
- **Break condition:** If the network width is insufficient (m < 22k−2 · (p−1)), neurons may not align with distinct Fourier frequencies

### Mechanism 2
- **Claim:** Each hidden neuron focuses on exactly one Fourier frequency, creating a complete basis for solving modular addition
- **Mechanism:** Through sum-to-product identities and margin maximization, neurons decompose cos(Σai) into basic cosine components, each neuron handling one frequency
- **Core assumption:** The class-weighted margin solutions are equivalent to maximum margin solutions (Condition 3.8)
- **Evidence anchors:**
  - [abstract] "each hidden-layer neuron aligns with a specific Fourier spectrum, integral to solving modular addition problems"
  - [section 4.1] "For each frequency ζ ∈ {1, ..., (p-1)/2}, there exists a neuron using this frequency"
  - [section 5.1] Empirical results show single-frequency hidden neurons
- **Break condition:** If the phase constraints θu1 + ... + θuk = θw cannot be satisfied, neurons may not align with single frequencies

### Mechanism 3
- **Claim:** The grokking phenomenon weakens as k increases because the function class becomes more complex
- **Mechanism:** As k increases, the maximum margin solution requires exponentially more neurons (2^(2k-1) · (p-1)), making it harder for the network to reach the optimal solution
- **Core assumption:** Grokking occurs when networks transition from lazy training to rich/feature learning regimes
- **Evidence anchors:**
  - [abstract] "The 'grokking' phenomenon weakens as k increases, consistent with the analysis"
  - [section 5.3] "Figure 4 shows that the grokking weakens as the number of k increases, which is consistent with our analysis"
  - [corpus] Missing - no direct corpus evidence for grokking weakening with k
- **Break condition:** If other factors (like regularization or optimizer settings) dominate, the grokking behavior may not follow this pattern

## Foundational Learning

- **Concept:** Discrete Fourier Transform and its properties
  - Why needed here: The entire analysis hinges on transforming the problem to Fourier space to show neuron alignment with frequencies
  - Quick check question: How does the discrete Fourier transform of cos(2πζx/p) look in the frequency domain?

- **Concept:** Maximum margin optimization and implicit bias in neural networks
  - Why needed here: The paper's main theoretical result relies on showing that trained networks converge to maximum margin solutions
  - Quick check question: What is the relationship between gradient descent optimization and margin maximization in homogeneous networks?

- **Concept:** Sum-to-product trigonometric identities
  - Why needed here: The construction of the maximum margin solution uses these identities to decompose complex trigonometric expressions
  - Quick check question: How can you express cos(a1 + a2) in terms of cos(a1), cos(a2), sin(a1), and sin(a2)?

## Architecture Onboarding

- **Component map:** Input (one-hot encoded k inputs {a1, ..., ak} from Zp) -> Hidden layer (m neurons with polynomial activation) -> Output (k-dimensional vector representing result modulo p)
- **Critical path:** Width (m) → Fourier alignment → Maximum margin solution → Correct modular addition
  - The network must have sufficient width (m ≥ 2^(2k-1) · (p-1)) to ensure each neuron can align with a distinct Fourier frequency

- **Design tradeoffs:**
  - Width vs. performance: Insufficient width leads to incomplete Fourier basis coverage
  - Activation function choice: Polynomial activation is necessary for the sum-to-product identities to work
  - Regularization strength: Too much regularization prevents convergence to maximum margin solution

- **Failure signatures:**
  - Neurons not aligning with single frequencies (multiple peaks in Fourier spectrum)
  - Incomplete coverage of Fourier frequencies (missing neurons for some ζ values)
  - Training not converging to maximum margin solution (small λ values needed)

- **First 3 experiments:**
  1. Train a network with insufficient width (m < 2^(2k-1) · (p-1)) and verify Fourier spectrum shows multiple frequencies per neuron
  2. Train with increasing width and plot the number of unique frequencies covered vs. m
  3. Vary λ values and measure the margin achieved vs. theoretical maximum γ*

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal width for a one-hidden-layer network to achieve maximum margin for k-input modular addition, and how does this scale with k and p?
- Basis in paper: The paper establishes that a network with m ≥ 2^(2k-1) · (p-1) neurons can achieve maximum margin, but notes this bound may not be tight.
- Why unresolved: The authors explicitly state the bound may not be tight and do not provide experimental verification of the exact minimum required width.
- What evidence would resolve it: Systematic experiments varying k and p, plotting generalization vs. width, would reveal the precise scaling relationship and optimal width.

### Open Question 2
- Question: Does the Fourier-based mechanism observed in modular addition generalize to other algebraic tasks like multiplication or exponentiation modulo p?
- Basis in paper: The analysis focuses specifically on modular addition, with discussion of connections to parity functions as a special case.
- Why unresolved: The paper only provides theoretical and empirical results for modular addition, without exploring other algebraic operations.
- What evidence would resolve it: Applying the theoretical framework to multiplication/exponentiation tasks and conducting similar experiments to identify whether Fourier-based mechanisms emerge.

### Open Question 3
- Question: How does the strength of grokking vary with network architecture beyond two-layer transformers, such as deeper transformers or different activation functions?
- Basis in paper: The authors observe weaker grokking as k increases in two-layer transformers, speculating about NTK regime transitions.
- Why unresolved: Experiments are limited to two-layer transformers with specific architectures; no comparison with deeper networks or different architectures is provided.
- What evidence would resolve it: Training various architectures (deeper transformers, different activation functions) on k-input modular addition and measuring grokking strength across k values.

## Limitations
- Theoretical analysis assumes convergence to maximum margin solutions but doesn't fully characterize the conditions for this convergence
- Empirical validation focuses primarily on synthetic modular arithmetic datasets, limiting generalizability to real-world tasks
- The grokking analysis shows weakening as k increases but lacks mechanistic explanation for this relationship

## Confidence
**High Confidence**: The Fourier alignment mechanism (Mechanism 1 and 2) - The mathematical proof structure is rigorous, and the empirical results strongly support the theoretical predictions about neuron alignment with specific Fourier frequencies.

**Medium Confidence**: The grokking weakening claim (Mechanism 3) - While the empirical results show the expected trend, the corpus evidence is missing and the mechanistic explanation could be more developed.

**Medium Confidence**: The width requirement (m ≥ 2^(2k-1) · (p-1)) - The theoretical derivation appears sound, but empirical validation of this specific threshold across different problem settings would strengthen the claim.

## Next Checks
1. **Convergence threshold analysis**: Systematically vary the regularization parameter λ and measure the achieved margin versus theoretical maximum γ*, identifying the precise threshold where maximum margin solutions emerge.

2. **Width sufficiency experiment**: Train networks with varying widths m < 2^(2k-1) · (p-1), m = 2^(2k-1) · (p-1), and m > 2^(2k-1) · (p-1), measuring Fourier spectrum coverage and test accuracy to validate the width requirement.

3. **Generalization test**: Apply the Fourier circuit analysis to transformers trained on real-world modular arithmetic problems (e.g., modular exponentiation in cryptographic contexts) to verify whether the same Fourier alignment patterns emerge outside synthetic datasets.