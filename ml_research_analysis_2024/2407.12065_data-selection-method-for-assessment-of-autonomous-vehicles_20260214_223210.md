---
ver: rpa2
title: Data selection method for assessment of autonomous vehicles
arxiv_id: '2407.12065'
source_url: https://arxiv.org/abs/2407.12065
tags:
- data
- selection
- metadata
- validation
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently selecting representative
  subsets of large-scale driving data for validating autonomous vehicle safety features.
  The authors propose a metadata-based selection method that optimizes the similarity
  between the metadata distribution of selected data and a predefined metadata distribution
  expected for validation.
---

# Data selection method for assessment of autonomous vehicles

## Quick Facts
- arXiv ID: 2407.12065
- Source URL: https://arxiv.org/abs/2407.12065
- Authors: Linh Trinh; Ali Anwar; Siegfried Mercelis
- Reference count: 38
- Primary result: Metadata-based selection method optimizes similarity between selected data metadata distribution and expected validation metadata distribution

## Executive Summary
This paper addresses the challenge of efficiently selecting representative subsets of large-scale driving data for validating autonomous vehicle safety features. The authors propose a metadata-based selection method that optimizes the similarity between the metadata distribution of selected data and a predefined metadata distribution expected for validation. Their approach includes a neural network-based scoring model trained to maximize this similarity, along with a selection function that ensures diversity through similarity filtering. Experiments on the BDD100K dataset demonstrate that their method outperforms a diversity-based baseline in matching expected metadata distributions while maintaining data diversity.

## Method Summary
The method extracts metadata from driving data using various sources (map API, CAN signals, etc.) and transforms it into ratio lists representing metadata distributions. A neural network scoring model assigns importance scores to each data point based on its metadata, trained to maximize similarity between selected data's metadata distribution and an expected distribution. The selection function then uses these scores with similarity filtering (cosine similarity on metadata vectors) to select diverse yet representative data points. The framework allows flexibility for different validation tasks by accepting different expected metadata distributions for each ADAS/ADS feature being validated.

## Key Results
- The method outperforms a diversity-based baseline (DC [33]) in category-based and domain-based metrics across selection ratios 0.2, 0.4, 0.6, and 0.8
- Category-based metric shows consistent improvements, with the method achieving scores above 0.7 for all tested selection ratios
- Domain-based metric demonstrates stable performance, with the method maintaining scores above 0.8 for selection ratios above 0.4
- Similarity filtering effectively maintains data diversity while improving metadata distribution matching compared to selection without filtering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method achieves better metadata distribution matching by optimizing a scoring model to maximize similarity between selected data and expected metadata distributions.
- Mechanism: The scoring model G(θ) assigns importance scores to each data point based on its metadata. The selection function Ψρ then uses these scores, combined with similarity filtering, to select diverse yet representative data points that match the expected metadata distribution E.
- Core assumption: Metadata distributions adequately capture the characteristics needed for validating different ADAS/ADS features, and a neural network can effectively learn to score data points based on their metadata similarity to E.
- Evidence anchors:
  - [abstract]: "Our idea is to optimize the similarity between the metadata distribution of the selected data and a predefined metadata distribution that is expected for validation."
  - [section]: "The goal is to find a subset dataset U ⊆ D that is smaller than D. We select a smaller dataset with a given ratio ρ = |U|/|D|... Our main idea is to maximize the distribution of metadata for the selected data U so that it matches the expected metadata distribution E."
  - [corpus]: No direct evidence in corpus about this specific mechanism. Weak evidence: general focus on autonomous vehicle safety and validation in neighboring papers.

### Mechanism 2
- Claim: Similarity filtering in the selection function Ψρ ensures diversity in the selected dataset by preventing the selection of data points similar to already selected ones.
- Mechanism: The selection function Ψρ uses cosine similarity to measure metadata similarity between data points. It iteratively selects the highest-scoring data point that is sufficiently dissimilar to already selected points, adjusting the similarity threshold dynamically to meet the selection ratio.
- Core assumption: Cosine similarity on metadata vectors is an effective measure of data point similarity for the purpose of maintaining diversity in the selected dataset.
- Evidence anchors:
  - [section]: "To increase the diversity of selected data, we use data similarity filtering in the selected function Ψρ... So, we have the similarity score of two metadata mi and mj as below: D(mi, mj) = D(Φ(mi), Φ(mj)) = ⃗Φ(mi) · ⃗Φ(mj) / |Φ(mi)||Φ(mj)|"
  - [section]: "The table III shows the average MAE of data selected using our method with and without similarity filtering. The results show that similarity filtering in Ψρ increases the average MAE of all ranges of ρ compared to without filtering."
  - [corpus]: No direct evidence in corpus about similarity filtering mechanisms. Weak evidence: general interest in autonomous vehicle validation in neighboring papers.

### Mechanism 3
- Claim: The method provides flexibility by allowing different expected metadata distributions for different validation tasks, enabling dynamic selection for various ADAS/ADS features.
- Mechanism: The framework accepts a predefined expected metadata distribution E for each validation task. The scoring model and selection process are then tailored to optimize for this specific E, allowing different subsets of data to be selected for different validation purposes.
- Core assumption: Different ADAS/ADS features require different metadata distributions for effective validation, and these distributions can be adequately defined by human experts.
- Evidence anchors:
  - [abstract]: "Furthermore, the data used to validate each driving feature may differ. As a result, it is essential to have an efficient data selection method that can be used flexibly and dynamically for verification and validation."
  - [section]: "Denote the original dataset D = {xi}^N_i=1... Let A denote the metadata distribution of the selected data U... The goal is to find a subset dataset U ⊆ D that is smaller than D."
  - [corpus]: No direct evidence in corpus about this specific mechanism. Weak evidence: general focus on autonomous vehicle validation in neighboring papers.

## Foundational Learning

- Concept: Metadata extraction and transformation
  - Why needed here: The method relies on metadata to characterize data points and match expected distributions. Understanding how metadata is extracted from raw data and transformed into a format suitable for the scoring model is crucial.
  - Quick check question: How is the transformation function Φ defined to convert extracted metadata mi into a ratio list that represents the metadata distribution?

- Concept: Neural network scoring model
  - Why needed here: The scoring model G(θ) is the core component that assigns importance scores to data points based on their metadata. Understanding its architecture and training process is essential for implementing and modifying the method.
  - Quick check question: What is the architecture of the neural network used as the scoring model G(θ), and how is it trained to optimize the similarity between selected data's metadata distribution and the expected distribution E?

- Concept: Similarity measurement and filtering
  - Why needed here: The method uses cosine similarity to measure metadata similarity between data points and employs similarity filtering to ensure diversity in the selected dataset. Understanding these concepts is crucial for implementing the selection function Ψρ.
  - Quick check question: How is the cosine similarity calculated between metadata vectors, and how does the dynamic adjustment of the similarity threshold ϵ work in the selection function Ψρ?

## Architecture Onboarding

- Component map:
  - Metadata extraction module F -> Transformation function Φ -> Scoring model G(θ) -> Selection function Ψρ -> Evaluation metrics Sc and Sd

- Critical path:
  1. Extract metadata from raw data using F
  2. Transform metadata using Φ
  3. Score data points using G(θ)
  4. Select data points using Ψρ
  5. Evaluate selection quality using Sc and Sd

- Design tradeoffs:
  - Metadata granularity vs. computational efficiency: More detailed metadata may improve selection quality but increase computational cost
  - Scoring model complexity vs. training time: More complex models may better capture metadata patterns but require more training data and time
  - Similarity threshold adjustment rate η vs. selection ratio ρ: Faster adjustment may help reach selection ratio but could compromise diversity

- Failure signatures:
  - Poor selection quality: Low values of Sc and Sd metrics
  - Inability to reach selection ratio: Ψρ fails to select enough diverse data points
  - Overfitting to expected distribution: Selected data matches E too closely but lacks real-world diversity

- First 3 experiments:
  1. Test metadata extraction and transformation on a small subset of BDD100K data to verify correct metadata representation
  2. Train the scoring model G(θ) on a small dataset with a simple expected distribution to verify learning capability
  3. Implement and test the selection function Ψρ with similarity filtering on a small dataset to verify diversity maintenance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed metadata-based selection method compare to other data selection approaches beyond the DC [33] method when applied to autonomous vehicle validation?
- Basis in paper: [explicit] The authors mention that several commercial services exist for data curation but do not provide explicit algorithms, and they compare their method only against DC [33] in experiments.
- Why unresolved: The paper only provides comparative results against a single alternative method (DC [33]), leaving uncertainty about how the proposed approach performs relative to other potential methods or commercial solutions.
- What evidence would resolve it: Comparative experiments showing performance metrics (category-based and domain-based) of the proposed method against multiple alternative data selection approaches or commercial services would provide clarity.

### Open Question 2
- Question: How sensitive is the proposed method to the quality and comprehensiveness of the metadata extraction function F, and what happens when certain metadata domains are missing or inaccurately extracted?
- Basis in paper: [inferred] The method relies heavily on metadata extraction through function F, and the authors acknowledge that some GPS positions could not be queried in OpenStreetMap, potentially affecting metadata completeness.
- Why unresolved: The paper does not explore how variations in metadata extraction quality impact the selection performance or what happens when certain domains are missing from the metadata.
- What evidence would resolve it: Experiments varying the completeness and accuracy of metadata extraction, including scenarios with missing or noisy metadata, would demonstrate the method's robustness to metadata quality issues.

### Open Question 3
- Question: What is the optimal selection ratio ρ for different ADAS/ADS features, and how does this vary across different operational design domains (ODDs)?
- Basis in paper: [explicit] The authors test various selection ratios but note that lower keep ratios make it more difficult to ensure selected data follows expected distributions, suggesting ratio selection is non-trivial.
- Why unresolved: While the paper tests multiple selection ratios, it doesn't provide guidance on determining the optimal ratio for specific validation tasks or how this should be adjusted based on ODD requirements.
- What evidence would resolve it: A systematic study determining optimal selection ratios for different ADAS features and ODDs, along with guidelines for selecting appropriate ratios based on validation requirements, would address this question.

### Open Question 4
- Question: How does the proposed method scale with larger datasets beyond BDD100K, particularly in terms of computational efficiency and selection quality maintenance?
- Basis in paper: [inferred] The experiments use BDD100K (100,000 videos), but the authors discuss the method's potential application to much larger datasets collected by OEMs like Waymo and Tesla, without testing at that scale.
- Why unresolved: The paper does not evaluate the method's performance or computational requirements when applied to datasets orders of magnitude larger than BDD100K.
- What evidence would resolve it: Performance and scalability tests on datasets significantly larger than BDD100K (e.g., multi-million video datasets) would demonstrate how the method scales and whether selection quality is maintained.

## Limitations

- The evaluation relies on synthetic expected metadata distributions rather than demonstrating effectiveness for specific real-world ADAS/ADS validation tasks
- The metadata used (road type, lanes, bridges, etc.) represents only a subset of potentially relevant scenario characteristics
- The method's performance with different metadata types or richer feature sets remains untested

## Confidence

- High confidence in the basic mechanism of metadata distribution matching and similarity filtering
- Medium confidence in the practical utility for autonomous vehicle validation
- Low confidence in the generalizability to different types of autonomous vehicle scenarios

## Next Checks

1. Apply the method to a specific ADAS validation task (e.g., pedestrian detection) with a clearly defined expected distribution based on known challenging scenarios, and evaluate whether the selected subset improves validation effectiveness compared to random selection.

2. Test the method's robustness to different metadata granularities by comparing performance using only high-level metadata (road type, weather) versus more detailed metadata (exact GPS coordinates, time of day) to understand the trade-off between metadata richness and selection quality.

3. Conduct a user study with autonomous vehicle safety engineers to evaluate whether the selected subsets are perceived as more representative and useful for validation purposes compared to baseline methods, and gather qualitative feedback on the method's practical utility.