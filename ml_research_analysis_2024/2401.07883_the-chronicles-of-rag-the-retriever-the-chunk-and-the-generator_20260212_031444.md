---
ver: rpa2
title: 'The Chronicles of RAG: The Retriever, the Chunk and the Generator'
arxiv_id: '2401.07883'
source_url: https://arxiv.org/abs/2401.07883
tags:
- score
- retriever
- chunk
- answer
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents best practices for implementing, optimizing,\
  \ and evaluating Retrieval Augmented Generation (RAG) systems for Brazilian Portuguese,\
  \ focusing on a simple pipeline for inference and experimentation. The study explores\
  \ diverse methods to answer questions about the first Harry Potter book using various\
  \ LLMs (OpenAI\u2019s GPT-4, GPT-4-1106-preview, GPT-3.5-turbo-1106, and Google\u2019\
  s Gemini Pro)."
---

# The Chronicles of RAG: The Retriever, the Chunk and the Generator

## Quick Facts
- **arXiv ID**: 2401.07883
- **Source URL**: https://arxiv.org/abs/2401.07883
- **Reference count**: 40
- **Primary result**: A RAG system optimized for Brazilian Portuguese achieved a 40.73 percentage point improvement over baseline, reaching a maximum relative score of 98.61%

## Executive Summary
This paper presents a comprehensive study on implementing, optimizing, and evaluating Retrieval Augmented Generation (RAG) systems for Brazilian Portuguese using the first Harry Potter book as a dataset. The research focuses on a simple pipeline for inference and experimentation, exploring various methods to answer questions using multiple LLM models including OpenAI's GPT-4, GPT-4-1106-preview, GPT-3.5-turbo-1106, and Google's Gemini Pro. The study identifies key practices for building effective RAG systems, demonstrating that improvements in retriever quality, input size optimization, and the use of rerankers significantly enhance overall performance.

## Method Summary
The study implements a RAG system using the first Harry Potter book in Brazilian Portuguese, chunked into 1000-token segments. A QA dataset is created using GPT-4, where each chunk has corresponding questions and answers. The research employs a multi-stage evaluation process, starting with a baseline naive approach using llama-index with cosine similarity retrieval, then progressively optimizing retriever quality, input size, and incorporating rerankers. The system is tested across multiple LLM models (GPT-4, GPT-4-1106-preview, GPT-3.5-turbo-1106, and Gemini Pro) and evaluated using custom metrics including relative maximum score and degradation score.

## Key Results
- Improving retriever quality led to a 35.4% increase in MRR@10 compared to baseline
- Optimizing input size enhanced performance by 2.4%
- The complete RAG architecture achieved a maximum relative score of 98.61%, representing a 40.73 percentage point improvement over baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing retriever quality directly improves RAG performance
- Mechanism: A higher quality retriever increases the likelihood of retrieving relevant chunks, leading to better LLM-generated answers
- Core assumption: Retriever quality is a crucial factor in enhancing RAG performance
- Evidence anchors:
  - [abstract]: "Focusing on the quality of the retriever, our approach achieved an improvement of MRR@10 by 35.4% compared to the baseline."
  - [section 6.1]: "As mentioned in section 5.1, the performance of information/chunk retrieval, measured by the MRR@10 metric, varies between (0.565, 0.919), as detailed in Table 6. This variation represents approximately 35.4%."
- Break condition: If the retriever fails to retrieve relevant chunks, even with a high MRR@10 score, the RAG system's performance may not improve significantly

### Mechanism 2
- Claim: Optimizing input size enhances RAG performance
- Mechanism: Limiting the number of retrieved chunks and their total token count helps avoid performance degradation caused by long input contexts
- Core assumption: There is an optimal input size for RAG systems that balances information retrieval and LLM processing
- Evidence anchors:
  - [abstract]: "When optimizing the input size in the application, we observed that it is possible to further enhance it by 2.4%."
  - [section 6.2]: "We observed that the best performance was achieved with the retrieval of 3 chunks using the retrieve-and-rerank strategy..."
- Break condition: If the input size is too small, the RAG system may not retrieve enough relevant information to generate accurate answers

### Mechanism 3
- Claim: Using a reranker in a multi-stage pipeline improves RAG performance
- Mechanism: A reranker refines the initial list of retrieved chunks, improving the quality of the final input to the LLM
- Core assumption: A reranker can effectively identify and prioritize the most relevant chunks from an initial list of candidates
- Evidence anchors:
  - [section 5.1.5]: "The multi-stage pipeline was able to achieve the best results in MRR@10 and Recall@k."
  - [section 6.2]: "The use of a reranker (Figure 9) showed improved information retrieval in our tests."
- Break condition: If the reranker fails to accurately identify the most relevant chunks, it may not improve RAG performance and could even degrade it

## Foundational Learning

- **Concept**: Understanding the RAG pipeline components (retriever, chunk, generator)
  - Why needed here: The paper focuses on optimizing and evaluating each component of the RAG pipeline to improve overall performance
  - Quick check question: What are the three main components of a RAG system, and what is the role of each component?

- **Concept**: Familiarity with evaluation metrics for RAG systems (MRR@10, Recall@k, relative maximum score)
  - Why needed here: The paper uses these metrics to quantify the performance of different RAG configurations and retrieval methods
  - Quick check question: What is the difference between MRR@10 and Recall@k, and how do they relate to the quality of a RAG system?

- **Concept**: Knowledge of retrieval methods (sparse, dense, hybrid, multi-stage with reranker)
  - Why needed here: The paper compares the performance of various retrieval methods to identify the best practices for RAG systems
  - Quick check question: What are the main differences between sparse and dense retrieval methods, and when might you choose one over the other?

## Architecture Onboarding

- **Component map**: Retriever → Chunk → Generator
  - Retriever: Responsible for finding relevant chunks based on the input query
  - Chunk: Represents the retrieved information, with size and number impacting RAG performance
  - Generator: LLM that generates the final answer based on the retrieved chunks and input query

- **Critical path**: Retriever → Chunk → Generator
  - The retriever's performance directly impacts the quality of the chunks, which in turn affects the generator's output

- **Design tradeoffs**:
  - Retriever quality vs. computational efficiency: More complex retrievers may achieve better performance but require more resources
  - Chunk size and number vs. LLM context window: Larger chunks or more chunks may provide more information but could exceed the LLM's context window or lead to performance degradation
  - Model selection: Different LLM models may have varying performance on RAG tasks, depending on their capabilities and the specific dataset

- **Failure signatures**:
  - Poor retriever performance: Low MRR@10 and Recall@k scores, irrelevant chunks retrieved
  - Inadequate chunk size or number: Incomplete or insufficient information provided to the LLM, leading to inaccurate or incomplete answers
  - LLM limitations: Inability to effectively process the retrieved chunks or generate coherent answers, despite relevant information being provided

- **First 3 experiments**:
  1. Implement a basic RAG system using a simple retriever (e.g., BM25) and a single LLM model (e.g., GPT-3.5-turbo-1106). Evaluate performance using MRR@10 and Recall@k.
  2. Optimize the retriever by testing different methods (e.g., custom ADA-002, hybrid search) and comparing their performance using the same evaluation metrics.
  3. Experiment with chunk size and number, using the best-performing retriever from the previous experiment. Evaluate the impact on RAG performance and identify the optimal configuration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RAG systems in Brazilian Portuguese compare to those in other languages, such as English or Spanish?
- Basis in paper: [inferred]
- Why unresolved: The paper focuses on Brazilian Portuguese but does not provide comparative analysis with other languages
- What evidence would resolve it: Conducting similar experiments with datasets in other languages and comparing the results with the Brazilian Portuguese dataset

### Open Question 2
- Question: What are the long-term effects of using different chunking strategies on the performance of RAG systems?
- Basis in paper: [explicit]
- Why unresolved: The paper explores chunking strategies but does not discuss their long-term effects on system performance
- What evidence would resolve it: Conducting longitudinal studies on the impact of chunking strategies over time

### Open Question 3
- Question: How do different evaluation metrics, such as BLEU and ROUGE, perform in assessing the quality of answers generated by RAG systems?
- Basis in paper: [explicit]
- Why unresolved: The paper introduces a custom scoring system but does not compare it with traditional metrics like BLEU and ROUGE
- What evidence would resolve it: Conducting experiments using traditional metrics alongside the custom scoring system and comparing the results

### Open Question 4
- Question: How does the performance of RAG systems vary with the size of the input context?
- Basis in paper: [explicit]
- Why unresolved: The paper explores the impact of input size on performance but does not provide a comprehensive analysis of how performance varies with different input sizes
- What evidence would resolve it: Conducting experiments with varying input sizes and analyzing the impact on performance

### Open Question 5
- Question: How does the performance of RAG systems change when using different types of retrievers, such as sparse and dense retrievers?
- Basis in paper: [explicit]
- Why unresolved: The paper compares different retrievers but does not provide a detailed analysis of how their performance varies
- What evidence would resolve it: Conducting experiments with different types of retrievers and analyzing their performance in various scenarios

## Limitations

- The study focuses on a single dataset (the first Harry Potter book in Brazilian Portuguese), which may limit generalizability to other domains or languages
- The exact prompt used to generate the QA dataset and the specific configuration of the custom ADA-002 fine-tuning are not provided, which could impact reproducibility
- The confidence in claims is moderate due to the limited scope of the dataset and potential influence of unmentioned factors such as specific LLM configurations and hyperparameters

## Confidence

- Claim: Optimizing retriever quality improves RAG performance
  - Confidence: Medium
- Claim: Optimizing input size enhances RAG performance
  - Confidence: Medium
- Claim: Using rerankers improves RAG performance
  - Confidence: Medium

## Next Checks

1. Conduct experiments on diverse datasets in different languages and domains to assess the generalizability of the proposed optimization techniques
2. Perform ablation studies to isolate the individual contributions of each RAG component (retriever, chunk, generator) to overall performance
3. Investigate the impact of different LLM configurations (temperature, max tokens) on RAG performance, as these factors may influence the quality of generated answers