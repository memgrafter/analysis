---
ver: rpa2
title: Extraction of nonlinearity in neural networks with Koopman operator
arxiv_id: '2402.11740'
source_url: https://arxiv.org/abs/2402.11740
tags:
- koopman
- neural
- matrix
- operator
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the essentiality of nonlinearity in deep
  neural networks using the Koopman operator framework. The authors propose replacing
  intermediate layers with Koopman matrices to analyze which features are truly necessary
  for classification tasks.
---

# Extraction of nonlinearity in neural networks with Koopman operator

## Quick Facts
- arXiv ID: 2402.11740
- Source URL: https://arxiv.org/abs/2402.11740
- Reference count: 0
- Key outcome: Compressed Koopman matrices achieve 79-96% MNIST accuracy with 80% compression, outperforming conventional pruning methods

## Executive Summary
This study investigates whether deep neural networks require nonlinearity by proposing to replace intermediate layers with Koopman matrices. Using the MNIST dataset, the authors employ extended dynamic mode decomposition (EDMD) combined with tensor-train (TT) format to handle high-dimensional systems. Their approach maintains reasonable accuracy (79-96% depending on dictionary size) while significantly compressing the network, suggesting that limited nonlinearity may be sufficient for certain classification tasks. The compressed Koopman matrix achieves better performance than conventional pruning methods at high compression ratios.

## Method Summary
The method uses extended dynamic mode decomposition (EDMD) to construct Koopman matrices from snapshot pairs of neural network states. The Koopman operator K transforms nonlinear dynamics into linear operators in function space, allowing intermediate layers to be approximated with finite-dimensional matrices. To handle high-dimensional dictionary spaces (e.g., 3,486,784,401 dimensions for 20 variables with N_max=2), the authors employ tensor-train (TT) format for efficient storage. The resulting Koopman matrices are further compressed using singular value decomposition, with experiments showing that ~10 singular values suffice for reasonable accuracy. The approach is tested on a 784-20-20-20-20-20-10 fully connected network trained on MNIST.

## Key Results
- Replacing intermediate layers with Koopman matrices maintains 79-96% accuracy on MNIST depending on dictionary size
- Only about 10 singular values are needed to capture essential information from the Koopman matrix
- Compressed Koopman matrices outperform conventional pruning methods at high compression ratios (80% accuracy at 0.2-0.4 compression ratio)
- Gaussian RBF dictionary functions provide better accuracy than monomial dictionaries for the same compression ratio

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Koopman operator can replace intermediate neural network layers because it linearizes nonlinear dynamics in an observable space
- Mechanism: The Koopman operator K transforms a nonlinear dynamical system x_{t+1} = F(x_t) into a linear operator in function space, where Kf(x_t) = f(x_{t+1}). By approximating K with a finite-dimensional matrix via EDMD, we can approximate the time evolution of neural network states using linear algebra instead of nonlinear activations
- Core assumption: The dynamics of the neural network's intermediate layers can be sufficiently captured by a finite-dimensional approximation of the Koopman operator in an appropriate observable space
- Evidence anchors:
  - [abstract]: "we employ the Koopman operator, extended dynamic mode decomposition, and the tensor-train format"
  - [section 2]: "The Koopman operator is a time evolution operator obtained by considering the time evolution of a dynamical system in a function space, called the observable space"
  - [corpus]: Weak evidence - neighboring papers discuss Koopman theory but don't directly confirm this specific mechanism for neural network layer replacement
- Break condition: If the neural network's intermediate dynamics cannot be captured by the chosen dictionary functions, or if the observable space dimension becomes intractable

### Mechanism 2
- Claim: Singular value decomposition (SVD) enables effective compression of Koopman matrices while preserving essential features
- Mechanism: After computing the Koopman matrix A = KB, SVD decomposes it as A = UÎ£V^T. By retaining only the top s singular values, we capture the most significant information while reducing dimensionality. The authors find that ~10 singular values suffice for reasonable accuracy
- Core assumption: The singular value spectrum of the Koopman matrix exhibits rapid decay, indicating that most information is concentrated in few components
- Evidence anchors:
  - [abstract]: "the compressed Koopman matrix achieves better performance than conventional pruning methods at high compression ratios"
  - [section 4]: "We see that the accuracy increases gradually as the number of dictionary functions increases. Note that we achieve roughly the same accuracy in the region where the ranks are greater than 10"
  - [corpus]: Moderate evidence - compression via SVD is well-established, but application to Koopman matrices for neural networks is novel
- Break condition: If the singular value spectrum doesn't decay rapidly, or if truncated reconstruction introduces significant error

### Mechanism 3
- Claim: Tensor-train (TT) format enables practical computation with high-dimensional Koopman operators
- Mechanism: The TT format represents high-dimensional tensors as products of low-rank cores, avoiding exponential memory growth. This allows handling large dictionary sizes (e.g., L=3,486,784,401 for 20 variables with N_max=2) that would be impossible with naive storage
- Core assumption: The Koopman operator in the chosen dictionary space has a tensor structure that can be efficiently compressed using TT format
- Evidence anchors:
  - [abstract]: "we employ the Koopman operator, extended dynamic mode decomposition, and the tensor-train format"
  - [section 3.3]: "the memory size increases exponentially with the number of variables. Hence, we here employ the so-called tensor-train (TT) format"
  - [corpus]: Strong evidence - TT format is specifically mentioned as solution to curse of dimensionality in neighboring papers
- Break condition: If the TT ranks become too large during decomposition, or if reconstruction accuracy degrades significantly

## Foundational Learning

- Concept: Koopman operator theory
  - Why needed here: Forms the mathematical foundation for linearizing nonlinear dynamics and replacing neural network layers
  - Quick check question: How does the Koopman operator differ from traditional state-space approaches to dynamical systems?

- Concept: Extended Dynamic Mode Decomposition (EDMD)
  - Why needed here: Provides the practical algorithm for approximating the Koopman operator from data
  - Quick check question: What role does the dictionary of observable functions play in EDMD?

- Concept: Tensor-train decomposition
  - Why needed here: Enables tractable computation with high-dimensional systems that arise from neural network state spaces
  - Quick check question: How does the TT format avoid the curse of dimensionality when representing high-order tensors?

## Architecture Onboarding

- Component map:
  Trained neural network -> Data generation module -> EDMD/Koopman computation module -> TT format compression module -> SVD compression module -> Replacement module -> Evaluation module

- Critical path:
  1. Train base neural network
  2. Generate snapshot pairs from intermediate layers
  3. Construct Koopman matrix via EDMD
  4. Compress using TT and SVD
  5. Replace intermediate layers with compressed Koopman matrix
  6. Evaluate performance

- Design tradeoffs:
  - Dictionary size vs. computational feasibility (solved via TT format)
  - Compression ratio vs. accuracy (optimal ~0.2-0.4 compression ratio)
  - Monomial vs. RBF dictionary functions (RBFs provide better accuracy)
  - Rank truncation vs. information preservation (10 singular values sufficient)

- Failure signatures:
  - Accuracy drops significantly after replacement (indicates insufficient dictionary or compression)
  - Memory overflow during Koopman matrix construction (TT format failing to compress effectively)
  - Slow prediction times (high TT ranks or inefficient implementation)

- First 3 experiments:
  1. Verify Koopman matrix reconstruction: Input known data through neural network, construct Koopman matrix, check if it can reconstruct intermediate states
  2. Test dictionary function impact: Compare monomial vs. RBF dictionaries for same compression ratio and measure accuracy
  3. Validate SVD compression: Vary number of retained singular values and plot accuracy vs. compression ratio to find optimal tradeoff

## Open Questions the Paper Calls Out

The paper notes "There is room for further study in the case of other network structures and different compression methods of matrices" and acknowledges their experiments are limited to relatively small networks.

## Limitations

- The approach is tested only on small fully connected networks (20 nodes per layer) and simple datasets (MNIST, Fashion MNIST), limiting generalizability to modern deep learning applications
- The fixed Koopman matrix structure may not be suitable for fine-tuning, as the authors note the structure after SVD decomposition "could not be suitable for fine-tuning"
- The relationship between dictionary size needed for accurate Koopman approximation and the intrinsic dimensionality of the neural network's decision boundary remains unexplained

## Confidence

- **High Confidence**: The basic methodology of using Koopman operators to linearize neural network dynamics and the overall experimental framework are well-established
- **Medium Confidence**: The specific claims about achieving 79-96% accuracy with compressed Koopman matrices and the superiority over conventional pruning methods are supported by the MNIST results but need validation on more diverse datasets
- **Low Confidence**: The generalizability of these findings to deeper networks, convolutional architectures, or more complex tasks remains uncertain

## Next Checks

1. **Generalization Test**: Apply the Koopman-based compression method to a convolutional neural network (e.g., ResNet) on CIFAR-10 to assess performance beyond fully connected networks
2. **Robustness Analysis**: Investigate the sensitivity of results to dictionary function choices, TT rank truncation, and SVD compression thresholds across different network depths and layer types
3. **Theoretical Bounds**: Derive theoretical guarantees or bounds on the approximation error introduced by Koopman linearization and compression, linking them to the network's nonlinearity and dataset complexity