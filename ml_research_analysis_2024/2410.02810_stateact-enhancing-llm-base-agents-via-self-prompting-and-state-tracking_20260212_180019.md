---
ver: rpa2
title: 'StateAct: Enhancing LLM Base Agents via Self-prompting and State-tracking'
arxiv_id: '2410.02810'
source_url: https://arxiv.org/abs/2410.02810
tags:
- stateact
- react
- agent
- current
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StateAct improves LLM-based agents by addressing long-context and
  goal adherence issues. It introduces self-prompting, where the agent reminds itself
  of the goal at every step, and chain-of-states, an extension of chain-of-thought
  that tracks state information over time.
---

# StateAct: Enhancing LLM Base Agents via Self-prompting and State-tracking

## Quick Facts
- **arXiv ID**: 2410.02810
- **Source URL**: https://arxiv.org/abs/2410.02810
- **Reference count**: 31
- **Primary result**: StateAct outperforms ReAct by 10-30% on Alfworld, Webshop, and Textcraft benchmarks across multiple frontier LLMs

## Executive Summary
StateAct introduces two key innovations to improve LLM-based agents: self-prompting that reinforces task goals at every step, and chain-of-states that tracks environmental state information over time. These mechanisms address the "haystack" problem where long contexts obscure original goals and enable structured reasoning through intermediate state predictions. StateAct serves as a drop-in replacement for ReAct, achieving significant performance gains on three benchmark environments without requiring additional training or retrieval mechanisms.

## Method Summary
StateAct is an in-context learning approach that modifies the ReAct base agent framework by adding self-prompting (goal reinforcement at each step) and chain-of-states (structured intermediate state predictions). The agent uses a prompt consisting of few-shot examples with a context vector containing goal, state, thought, and action components. At each timestep, the agent generates a new context by appending the current observation to the history, predicts the next state, and extracts the action to send to the environment. The method is evaluated across multiple frontier LLMs (Mistral-24B, Qwen variants, Gemma-27B) on three benchmark tasks: Alfworld, Webshop, and Textcraft.

## Key Results
- StateAct outperforms ReAct by 10.3% on Alfworld (49.5% vs 39.2% success rate)
- StateAct achieves 28.0% success on Webshop (30.0% improvement over ReAct's 21.8%)
- StateAct reaches 73.0% on Textcraft (7.0% improvement over ReAct's 66.0%)
- When combined with test-time scaling (ADaPT), StateAct gains an additional 12% on Textcraft

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Self-prompting improves goal adherence in long-horizon tasks by reinforcing the task goal at every step.
- **Mechanism**: The agent inserts a "goal" token into its context vector at each timestep, ensuring the original instruction remains salient despite context dilution.
- **Core assumption**: LLMs suffer from "haystack" problem where long contexts obscure the original goal; explicit repetition counteracts this.
- **Evidence anchors**: Abstract states "self-prompting, which reinforces task goals at every step" and section explains "mechanism for the agent to 'self-prompt' at every turn of the interaction to improve staying on track with the main goal."

### Mechanism 2
- **Claim**: Chain-of-states provides structured intermediate predictions that improve long-range reasoning by tracking environmental state explicitly.
- **Mechanism**: At each step, the agent predicts discrete state variables (e.g., current location, inventory) as part of the context vector, enabling step-by-step reasoning grounded in environment state.
- **Core assumption**: State tracking is analogous to chain-of-thought but with structured, verifiable intermediate steps instead of free-form reasoning.
- **Evidence anchors**: Abstract mentions "chain-of-states, an extension of chain-of-thought that tracks state information over time" and section describes "introducing 'structured thoughts' into the reasoning part of the LLM Agent."

### Mechanism 3
- **Claim**: StateAct serves as a drop-in replacement for ReAct, enabling compatibility with advanced methods like test-time scaling without retraining.
- **Mechanism**: By preserving the ReAct interface (goal+state+thought+action output format), StateAct can be substituted into existing pipelines (e.g., ADaPT) without architectural changes.
- **Core assumption**: The base agent interface is stable enough that swapping implementations doesn't break downstream extensions.
- **Evidence anchors**: Abstract states "StateAct can be used as a drop-in replacement for ReAct with advanced LLM agent methods such as test-time scaling."

## Foundational Learning

- **Concept**: In-context learning
  - **Why needed here**: StateAct relies entirely on prompt-based learning without fine-tuning; understanding few-shot prompting is essential to replicate results.
  - **Quick check question**: What happens if you remove few-shot examples from the prompt—does the agent still perform the task?

- **Concept**: Chain-of-thought reasoning
  - **Why needed here**: StateAct extends CoT to structured state tracking; familiarity with CoT mechanics helps understand the "chain-of-states" innovation.
  - **Quick check question**: How does adding intermediate state predictions differ from adding free-form reasoning steps in CoT?

- **Concept**: Context vector construction
  - **Why needed here**: The agent's policy depends on a structured context vector (goal, state, thought, action); understanding this format is critical for debugging.
  - **Quick check question**: What components must be present in the context vector at each step for the agent to function?

## Architecture Onboarding

- **Component map**: LLM inference backend -> Prompt generator -> State tracker -> Environment interface -> Evaluation harness
- **Critical path**: 1. Receive observation from environment 2. Append to context vector with goal, state, thought placeholders 3. Generate complete context via LLM 4. Extract action, update state 5. Send action to environment, repeat
- **Design tradeoffs**: Longer prompts improve state tracking but risk context truncation; heuristic state tracking is fast but may fail in complex environments; JSON formatting vs. plain text: JSON improves parsing but hurts performance
- **Failure signatures**: Agent loops on same action → state prediction bug; Goal drift over time → self-prompting ineffective; Performance drops with model size → context length limits
- **First 3 experiments**: 1. Run StateAct on Alfworld with gpt-3.5, measure success rate vs. ReAct baseline 2. Remove self-prompting, compare performance drop to isolate its effect 3. Replace heuristic state tracking with oracle state, measure upper-bound performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does StateAct's self-prompting mechanism affect performance in tasks with varying time horizons?
- **Basis in paper**: The paper mentions that StateAct's self-prompting improves performance on longer tasks (40-50 steps) in Alfworld.
- **Why unresolved**: The paper only provides results for Alfworld, Webshop, and Textcraft. It doesn't explore how self-prompting affects performance in tasks with different time horizons beyond these environments.
- **What evidence would resolve it**: Experiments testing StateAct on tasks with varying time horizons, including both shorter and longer tasks than those in the evaluated environments, would provide insights into the generalizability of self-prompting's benefits.

### Open Question 2
- **Question**: What is the impact of structured state tracking (JSON format) on StateAct's performance?
- **Basis in paper**: The paper mentions that translating the state into a JSON format hindered performance compared to the text-based format.
- **Why unresolved**: The paper only briefly mentions this finding and doesn't provide a detailed analysis of why JSON format negatively impacts performance.
- **What evidence would resolve it**: A detailed analysis of the differences between text-based and JSON-based state tracking, including an exploration of the potential reasons for the performance drop, would help understand the impact of structured formats on StateAct.

### Open Question 3
- **Question**: How does StateAct's performance compare to other state-tracking methods in LLM-based agents?
- **Basis in paper**: The paper mentions Chen et al. (2024) and STATLER (Yoneda et al., 2024) as other state-tracking methods but doesn't directly compare their performance to StateAct.
- **Why unresolved**: The paper focuses on comparing StateAct to ReAct and doesn't provide a comprehensive comparison with other state-tracking approaches.
- **What evidence would resolve it**: Experiments comparing StateAct's performance to other state-tracking methods on the same tasks and environments would provide insights into the relative effectiveness of different state-tracking approaches.

### Open Question 4
- **Question**: How does StateAct's performance scale with different LLM sizes and architectures?
- **Basis in paper**: The paper mentions that StateAct outperforms ReAct across multiple models and sizes, including Mistral-Small-24B-Instruct, Qwen-2.5-7B,14B,32B-Instruct, and Gemma2-27B-Instruct.
- **Why unresolved**: While the paper shows that StateAct performs well across different models, it doesn't explore how its performance scales with even larger or smaller models, or different architectures.
- **What evidence would resolve it**: Experiments testing StateAct on a wider range of LLM sizes and architectures, including both larger and smaller models, would provide insights into its scalability and generalizability.

## Limitations
- State-tracking fragility in complex environments where state transitions are non-deterministic or involve partial observability
- Limited generalization beyond few-shot examples without fine-tuning
- Test-time scaling integration unclear with no ReAct baseline comparison

## Confidence
- **High confidence**: StateAct improves performance over ReAct on benchmark tasks (Alfworld, Webshop, Textcraft)
- **Medium confidence**: Mechanism explanations (self-prompting prevents goal drift, chain-of-states enables structured reasoning) are plausible but lack direct experimental validation
- **Low confidence**: The claim that StateAct is a seamless drop-in replacement for ReAct in advanced methods

## Next Checks
1. **Ablation study with varying few-shot examples**: Systematically reduce the number of few-shot examples in StateAct prompts (from 5 to 0) and measure performance degradation.
2. **State tracking oracle experiment**: Replace heuristic state tracking with perfect oracle state information and measure the performance gap.
3. **Cross-domain generalization test**: Apply StateAct to a new domain outside the three benchmarks (e.g., a different text-based game or real-world task) to test whether the self-prompting and chain-of-states mechanisms generalize beyond the few-shot examples provided.