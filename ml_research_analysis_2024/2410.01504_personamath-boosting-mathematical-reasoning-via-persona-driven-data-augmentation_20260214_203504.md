---
ver: rpa2
title: 'PersonaMath: Boosting Mathematical Reasoning via Persona-Driven Data Augmentation'
arxiv_id: '2410.01504'
source_url: https://arxiv.org/abs/2410.01504
tags:
- dataset
- math
- questions
- stage
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of improving mathematical reasoning
  capabilities in open-source large language models, which lag behind closed-source
  models like GPT-4. The proposed PersonaMath framework employs a two-stage data augmentation
  approach: first, it uses a closed-source LLM to generate detailed chain-of-thought
  solutions and then rewrites questions using a diverse set of personas to increase
  dataset diversity and quality; second, it incorporates reflection to leverage more
  challenging incorrectly-answered questions.'
---

# PersonaMath: Boosting Mathematical Reasoning via Persona-Driven Data Augmentation

## Quick Facts
- **arXiv ID**: 2410.01504
- **Source URL**: https://arxiv.org/abs/2410.01504
- **Reference count**: 30
- **Primary result**: PersonaMath-7B achieves state-of-the-art 61.2% accuracy on MATH and 87.8% on GSM8K using only 17.8% the dataset size of comparable approaches

## Executive Summary
This paper addresses the performance gap between open-source and closed-source LLMs in mathematical reasoning by introducing PersonaMath, a two-stage data augmentation framework. The approach uses GPT-4 to generate detailed chain-of-thought solutions and rewrite questions using diverse personas, then applies reflection to leverage incorrectly-answered questions. The resulting PersonaMathQA dataset contains 70.3K samples and enables PersonaMath-7B to achieve state-of-the-art performance on MATH and GSM8K benchmarks while using significantly fewer training samples than comparable datasets.

## Method Summary
PersonaMath employs a two-stage data augmentation pipeline to enhance mathematical reasoning in open-source LLMs. In the first stage, a closed-source LLM (GPT-4) generates detailed chain-of-thought solutions for mathematical questions, then rewrites these questions using a diverse set of personas to increase dataset diversity and quality. In the second stage, the framework incorporates reflection by analyzing incorrectly-answered questions and generating improved solutions through targeted reasoning. This approach produces the PersonaMathQA dataset containing 70.3K samples, which is only 17.8% the size of comparable mathematical reasoning datasets but achieves superior performance on standard benchmarks.

## Key Results
- PersonaMath-7B achieves 61.2% accuracy on MATH benchmark
- PersonaMath-7B achieves 87.8% accuracy on GSM8K benchmark
- PersonaMathQA dataset contains 70.3K samples (17.8% of comparable dataset sizes)
- State-of-the-art performance demonstrates effectiveness of persona-driven augmentation

## Why This Works (Mechanism)
PersonaMath leverages persona-driven data augmentation to improve mathematical reasoning by introducing diversity in question formulation and solution approaches. The mechanism works by having GPT-4 rewrite mathematical questions through various personas, which helps models learn to approach problems from multiple perspectives. This diversity in training data enables better generalization and robustness in mathematical reasoning tasks. The reflection component further enhances learning by focusing on challenging incorrectly-answered questions, allowing the model to develop stronger problem-solving strategies for difficult mathematical concepts.

## Foundational Learning
- **Chain-of-thought reasoning**: Step-by-step problem-solving approach that mimics human mathematical thinking; needed to break down complex problems into manageable steps; quick check: verify solutions follow logical progression
- **Data augmentation**: Technique for expanding training datasets through transformations; needed to improve model generalization without requiring massive original datasets; quick check: measure diversity metrics across augmented samples
- **Persona-based generation**: Using different character perspectives to rewrite questions; needed to introduce variability in problem presentation and solution approaches; quick check: validate persona diversity through qualitative analysis
- **Reflection-based learning**: Iterative improvement process focusing on incorrectly-answered questions; needed to address model weaknesses and improve performance on challenging problems; quick check: track improvement on previously-missed question types
- **Mathematical reasoning benchmarks**: Standardized evaluation metrics (MATH, GSM8K); needed to objectively measure model performance improvements; quick check: compare against established baseline models
- **Closed-source LLM dependency**: Reliance on GPT-4 for data generation; needed to ensure high-quality synthetic data; quick check: assess quality through human evaluation or alternative metrics

## Architecture Onboarding

**Component Map**: Mathematical Questions -> GPT-4 Solution Generation -> Persona Rewriting -> Reflection Phase -> PersonaMathQA Dataset -> Training -> Mathematical Reasoning Model

**Critical Path**: The core workflow involves generating initial solutions with GPT-4, applying persona-based rewriting to increase diversity, incorporating reflection to handle challenging questions, and using the resulting dataset to train mathematical reasoning models.

**Design Tradeoffs**: The framework trades dataset size for quality, using only 17.8% of comparable dataset sizes while achieving better performance. This approach reduces computational requirements but introduces dependency on closed-source models for data generation. The persona-driven approach increases diversity but requires careful selection of persona types and quantities.

**Failure Signatures**: Poor performance may result from inadequate persona diversity, ineffective reflection on incorrectly-answered questions, or insufficient quality in the initial GPT-4 generated solutions. Over-reliance on specific persona types could lead to biased problem-solving approaches, while inadequate reflection might fail to address model weaknesses.

**Three First Experiments**:
1. Compare performance of models trained on original versus persona-augmented datasets to isolate the impact of persona diversity
2. Evaluate the contribution of the reflection phase by training models with and without this component
3. Test different persona selection strategies to identify optimal persona types and quantities for mathematical reasoning improvement

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions in the provided text.

## Limitations
- Reliance on GPT-4 for both question rewriting and reflection phases creates dependency on closed-source models
- Dataset size of 70.3K samples remains relatively small compared to other mathematical reasoning datasets
- Persona-driven approach lacks systematic analysis of which persona types contribute most effectively to performance improvements

## Confidence

**High confidence**: Performance improvements on MATH and GSM8K benchmarks; effectiveness of two-stage data augmentation methodology; comparison showing smaller dataset achieving better results

**Medium confidence**: Generalization of results to other mathematical reasoning tasks; scalability of persona-driven approach; specific contribution of each augmentation stage to final performance

**Low confidence**: Long-term effectiveness across different model sizes; reproducibility without GPT-4 access; applicability to mathematical domains beyond elementary and competition mathematics

## Next Checks

1. Replicate the data generation pipeline using open-source LLMs (e.g., LLaMA-3 or Mistral) to verify that the persona-driven approach maintains effectiveness without GPT-4 dependency

2. Conduct ablation studies systematically varying persona types and quantities to quantify their individual contributions to performance gains

3. Test the augmented models on additional mathematical reasoning benchmarks including MMLU-STEM, APPS, and real-world problem-solving datasets to assess generalization beyond the current evaluation set