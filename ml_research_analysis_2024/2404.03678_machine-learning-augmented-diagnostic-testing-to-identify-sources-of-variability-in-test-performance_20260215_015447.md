---
ver: rpa2
title: Machine learning augmented diagnostic testing to identify sources of variability
  in test performance
arxiv_id: '2404.03678'
source_url: https://arxiv.org/abs/2404.03678
tags:
- test
- herd
- testing
- sicct
- herds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Machine learning can augment diagnostic testing to improve sensitivity
  without reducing specificity by incorporating herd-level epidemiological risk factors.
  Using a Histogram-based Gradient Boosting Tree model trained on detailed bTB testing
  records in Great Britain, the study predicted confirmed herd breakdowns with 87.5%
  accuracy.
---

# Machine learning augmented diagnostic testing to identify sources of variability in test performance

## Quick Facts
- arXiv ID: 2404.03678
- Source URL: https://arxiv.org/abs/2404.03678
- Reference count: 24
- Model improved herd-level sensitivity from 63.8% to 78.4% while maintaining 89.5% specificity

## Executive Summary
This study demonstrates that machine learning can augment diagnostic testing for bovine tuberculosis (bTB) by incorporating herd-level epidemiological risk factors to improve sensitivity without reducing specificity. Using a Histogram-based Gradient Boosting Tree model trained on detailed bTB testing records from Great Britain, the researchers predicted confirmed herd breakdowns with 87.5% accuracy. The model increased herd-level sensitivity by 14.6 percentage points at a decision threshold chosen to match the standard test's specificity, potentially identifying 2,874 additional infected herds and reducing false positives by 51.6% over one year.

## Method Summary
The study used a Histogram-based Gradient Boosting Tree (HGBT) model trained on 1.3 million SICCT test events from 2012-2021, incorporating herd characteristics, test history, animal movements, location, and veterinary practice data. The model was trained on 80% of the data with hyperparameter tuning via random search, validated using 10-fold cross-validation and permutation-based feature importance testing. Performance was evaluated using ROC curves and AUC, with the decision threshold selected to match the standard test's specificity of 89.5%.

## Key Results
- Model achieved 87.5% accuracy in predicting confirmed herd breakdowns
- Herd-level sensitivity increased from 63.8% to 78.4% while maintaining 89.5% specificity
- Over one year, model would have identified 2,874 additional infected herds and reduced false positives by 51.6%
- Test date, location, veterinary practice, badger abundance, and animal movements were most influential risk factors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating epidemiological risk factors into diagnostic test interpretation increases herd-level sensitivity without reducing specificity.
- Mechanism: Machine learning models use herd-level risk factors to adjust the decision threshold for test positivity, allowing earlier detection of infected herds while maintaining low false positive rates.
- Core assumption: The distribution of risk factors across herds is sufficiently different between infected and uninfected herds to be captured by the model.
- Evidence anchors:
  - "Machine learning can augment diagnostic testing to improve sensitivity without reducing specificity by incorporating herd-level epidemiological risk factors."
  - "Using a Histogram-based Gradient Boosting Tree model...predicted confirmed herd breakdowns with 87.5% accuracy. At a decision threshold chosen to match SICCT test specificity, the model increased herd-level sensitivity by 14.6 percentage points."
- Break condition: If risk factor distributions overlap heavily between infected and uninfected herds, or if risk factors are not predictive, the model cannot improve sensitivity without losing specificity.

### Mechanism 2
- Claim: Veterinary practice performance varies and correlates with herd size, indicating opportunities for targeted improvements.
- Mechanism: The model identifies variability in test accuracy across veterinary practices, which is partially explained by the mean herd size tested by each practice.
- Core assumption: Variability in test performance across veterinary practices is real and measurable, not just random noise.
- Evidence anchors:
  - "The model also showed that herd size correlates with veterinary practice accuracy, suggesting opportunities for targeted improvements."
  - "Mean herd size tested by the practice is correlated with accuracy with Pearson r = −0.41 (p ≈ 10−17)."
- Break condition: If the observed variability is due to unmeasured confounders rather than practice-level factors, targeting improvements may not be effective.

### Mechanism 3
- Claim: Increasing herd-level sensitivity reduces the number of confirmed breakdowns and individual reactors in high-risk areas.
- Mechanism: By detecting infected herds earlier through augmented testing, infections are cleared before they spread to other herds, reducing overall transmission and disease burden.
- Core assumption: Early detection and removal of infected animals interrupts transmission chains sufficiently to reduce outbreak sizes.
- Evidence anchors:
  - "Simulation of transmission dynamics indicated that increased sensitivity would reduce confirmed breakdowns and reactors in high-risk areas."
  - "Individual reactors are reduced in both areas with Derbyshire seeing a reduction of 6.7%, and by 15.9% in Devon."
- Break condition: If transmission dynamics are dominated by factors not captured by test sensitivity, increased sensitivity may not substantially reduce overall disease burden.

## Foundational Learning

- Concept: Gradient Boosting Decision Trees (GBDTs)
  - Why needed here: The study uses Histogram-based Gradient Boosting Trees (HGBT), a variant of GBDTs, to handle complex interactions between risk factors and missing data.
  - Quick check question: What is the main advantage of using GBDTs over single decision trees in this context?

- Concept: Receiver Operating Characteristic (ROC) curves and Area Under the Curve (AUC)
  - Why needed here: The model's performance is evaluated using ROC curves and AUC, which show the trade-off between sensitivity and specificity across different decision thresholds.
  - Quick check question: What does an AUC of 0.93 indicate about the model's performance compared to random guessing?

- Concept: Permutation-based feature importance
  - Why needed here: The study uses permutation-based importance testing to identify which risk factors most influence model predictions, guiding understanding of disease drivers.
  - Quick check question: How does permutation-based importance testing assess the contribution of each feature to model accuracy?

## Architecture Onboarding

- Component map: Data curation from SAM and CTS databases -> Feature engineering (herd characteristics, test history, movements, location, veterinary practice) -> HGBT model training with hyperparameter tuning and cross-validation -> Model validation on held-out test set and simulation of transmission dynamics -> Interpretation of feature importance and veterinary practice effects
- Critical path: Data curation → Feature engineering → Model training and validation → Simulation of disease dynamics → Interpretation and application
- Design tradeoffs: Using HGBT allows handling of missing data and complex interactions but requires careful hyperparameter tuning; using confirmed breakdowns as the outcome provides a reliable gold standard but may miss some true infections that clear without detection
- Failure signatures: If the model shows poor generalization to new data, it may indicate overfitting or unrepresentative training data; if feature importance is dominated by obvious factors like test result, it may suggest limited added value from risk factors
- First 3 experiments:
  1. Train and validate the HGBT model on the full dataset, evaluating HSe and HSp at the decision threshold matching SICCT specificity
  2. Perform permutation-based importance testing to identify the most influential risk factors
  3. Simulate disease transmission dynamics using the individual-based model, comparing outcomes with and without model-augmented testing sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific veterinary practice management factors contribute to variability in test accuracy?
- Basis in paper: The study found that veterinary practice accuracy correlates with herd size tested, but did not investigate specific management practices.
- Why unresolved: The paper acknowledges this as a limitation and suggests that while variability exists, it doesn't provide evidence that it's due to practice management rather than local disease risks.
- What evidence would resolve it: Detailed analysis of veterinary practice protocols, training records, and testing procedures across different practices, controlling for herd size and location.

### Open Question 2
- Question: How would direct interpretation of SICCT test measurements (rather than binary outcomes) improve diagnostic performance?
- Basis in paper: The authors suggest this could be explored if sufficient data becomes available, noting it could provide an alternative to severe interpretation.
- Why unresolved: The current model uses binary test results rather than the continuous measurement data, limiting potential improvements.
- What evidence would resolve it: Application of machine learning to continuous SICCT measurement data with sufficient sample size, comparing performance to the current binary approach.

### Open Question 3
- Question: What is the long-term impact of increased test sensitivity on bTB transmission dynamics at the national level?
- Basis in paper: The study used simulations to examine impacts in two specific areas but acknowledges this is limited in scope.
- Why unresolved: The simulation model only covered two areas and didn't model the full complexity of national transmission patterns.
- What evidence would resolve it: National-scale individual-based simulation incorporating movement networks, badger populations, and regional risk variations to model long-term epidemic dynamics.

## Limitations

- Confirmed breakdown outcome may miss some truly infected herds that clear without detection
- Badger abundance data and veterinary practice/tuberculin batch information only covers ~10% of test records
- Simulation of transmission dynamics represents a simplified model that may not capture all epidemiological complexities

## Confidence

- **High Confidence**: The model's ability to improve herd-level sensitivity from 63.8% to 78.4% while maintaining specificity at 89.5% is supported by robust cross-validation and permutation-based feature importance analysis.
- **Medium Confidence**: The simulation results showing reduced confirmed breakdowns and individual reactors in high-risk areas depend on model assumptions about transmission dynamics.
- **Low Confidence**: The generalizability of these findings to other diseases or geographic regions remains uncertain.

## Next Checks

1. **External Validation**: Test the model on independent datasets from different geographic regions or time periods to assess generalization performance and identify any overfitting to the training data distribution.

2. **Sensitivity Analysis**: Conduct ablation studies by systematically removing different risk factors to quantify their individual contributions to model performance and determine which factors are most critical for maintaining the sensitivity improvement.

3. **Field Implementation Trial**: Design and execute a controlled field trial where the model-augmented testing is implemented in a subset of veterinary practices while maintaining standard testing in comparable areas, measuring actual disease detection rates and transmission outcomes over time.