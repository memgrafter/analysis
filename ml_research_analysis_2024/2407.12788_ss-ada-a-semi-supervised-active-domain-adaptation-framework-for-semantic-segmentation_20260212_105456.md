---
ver: rpa2
title: 'SS-ADA: A Semi-Supervised Active Domain Adaptation Framework for Semantic
  Segmentation'
arxiv_id: '2407.12788'
source_url: https://arxiv.org/abs/2407.12788
tags:
- learning
- data
- segmentation
- semantic
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reducing labeling costs for
  semantic segmentation in new driving scenarios by integrating active learning into
  semi-supervised semantic segmentation. The proposed SS-ADA framework employs an
  image-level acquisition strategy to select informative samples for manual annotation
  and uses an IoU-based class weighting strategy to alleviate class imbalance.
---

# SS-ADA: A Semi-Supervised Active Domain Adaptation Framework for Semantic Segmentation

## Quick Facts
- arXiv ID: 2407.12788
- Source URL: https://arxiv.org/abs/2407.12788
- Reference count: 40
- Key outcome: SS-ADA achieves supervised-level accuracy using only 25% of labeled target domain data across synthetic-to-real and real-to-real domain adaptation scenarios.

## Executive Summary
SS-ADA addresses the challenge of reducing labeling costs for semantic segmentation in new driving scenarios by integrating active learning into semi-supervised semantic segmentation. The framework combines source labeled data, target labeled data from active learning, and target unlabeled data to achieve accuracy comparable to fully supervised learning while significantly reducing annotation requirements. Through image-level acquisition strategies and IoU-based class weighting, SS-ADA effectively addresses class imbalance and sample selection challenges in domain adaptation settings.

## Method Summary
SS-ADA integrates active learning with semi-supervised semantic segmentation through three core modules. The semi-supervised learning module leverages source labeled data, target labeled data, and target unlabeled data using consistency regularization and entropy minimization. The active learning module selects informative samples from unlabeled target data using image-level entropy and confidence metrics. The IoU-based class weighting strategy adjusts class weights in the loss function based on performance metrics, assigning higher weights to classes with lower IoU to alleviate class imbalance. The framework uses BiSeNet as the segmentation model and trains with SGD optimizer using specified learning rates, batch sizes, and training epochs across different dataset configurations.

## Key Results
- Achieves supervised-level accuracy using only 25% of labeled target domain data
- Effective across synthetic-to-real (GTA5-to-Cityscapes, SYNTHIA-to-Cityscapes) and real-to-real (Cityscapes-to-ACDC, Cityscapes-to-FishEyeCampus) domain adaptation settings
- IoU-based class weighting strategy successfully alleviates class imbalance problems
- Active learning selection outperforms random sampling for sample acquisition efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Active learning selection using entropy or confidence metrics identifies samples that improve model performance more than random sampling.
- Mechanism: The model computes uncertainty (entropy or confidence) for each unlabeled image in the target domain. Images with highest entropy (most uncertain) or lowest confidence are selected for annotation, assuming these samples provide the most information for reducing model uncertainty.
- Core assumption: Model uncertainty correlates with sample informativeness for improving model performance.
- Evidence anchors:
  - [abstract] "These methods develop acquisition strategies to select informative samples based on uncertainty sampling, diversity sampling, or a combination of both"
  - [section] "The value of the image is evaluated based on the model's uncertainty. We use classic metrics such as entropy and confidence to assess the uncertainty of the model's predictions"
  - [corpus] Weak evidence - corpus papers focus on general active learning but don't specifically validate entropy/confidence metrics for semantic segmentation domain adaptation
- Break condition: If model uncertainty doesn't correlate with actual information gain, or if the model's predictions are consistently overconfident, the selection strategy fails to improve performance.

### Mechanism 2
- Claim: IoU-based class weighting addresses class imbalance by assigning higher weights to underrepresented classes with low IoU scores.
- Mechanism: After active learning annotations are obtained, the model computes IoU for each class. Classes with lower IoU receive higher loss weights during training, ensuring the model focuses more on difficult classes rather than just frequent ones.
- Core assumption: Classes with lower IoU scores benefit more from increased training focus than simply increasing their frequency in the dataset.
- Evidence anchors:
  - [abstract] "we design an IoU-based class weighting strategy to alleviate the class imbalance problem using annotations from active learning"
  - [section] "It adjusts the weights of each class in the loss function during training, assigning higher weights to classes with lower IoU and vice versa"
  - [corpus] Weak evidence - corpus papers don't specifically address IoU-based weighting for semantic segmentation class imbalance
- Break condition: If class imbalance is not the primary performance bottleneck, or if IoU scores don't accurately reflect which classes need more training focus.

### Mechanism 3
- Claim: Integrating active learning into semi-supervised semantic segmentation enables supervised-level accuracy with significantly less labeled data.
- Mechanism: The framework combines three data sources: source labeled data (from simulators or public datasets), target labeled data (from active learning), and target unlabeled data (from semi-supervised learning). This multi-source approach provides rich supervision while minimizing target domain annotation requirements.
- Core assumption: Source domain data remains relevant and helpful for target domain learning when combined with active learning-selected target annotations.
- Evidence anchors:
  - [abstract] "SS-ADA integrates active learning into semi-supervised semantic segmentation to achieve the accuracy of supervised learning with a limited amount of labeled data from the target domain"
  - [section] "The semi-supervised learning module leverages source labeled data Dl_s, target labeled and unlabeled data Dl_t and Du_t for model training, fully utilizing the available data"
  - [corpus] Moderate evidence - corpus papers show active learning + semi-supervised approaches improve segmentation but don't specifically demonstrate supervised-level accuracy
- Break condition: If domain shift between source and target is too large, or if active learning selection doesn't capture the most informative samples for the target domain.

## Foundational Learning

- Concept: Domain adaptation
  - Why needed here: The model must transfer knowledge from source domain (simulated or different real environments) to target domain (new driving scenarios) with limited labeled data
  - Quick check question: What happens to model performance when source and target domains have different distributions?

- Concept: Semi-supervised learning
  - Why needed here: The framework uses both labeled and unlabeled data from the target domain to improve performance beyond what labeled data alone could achieve
  - Quick check question: How does the model utilize unlabeled target data during training?

- Concept: Active learning acquisition strategies
  - Why needed here: The framework needs to select which unlabeled samples to annotate rather than annotating randomly, to maximize performance improvement per annotation cost
  - Quick check question: What metrics can be used to measure sample informativeness for active learning?

## Architecture Onboarding

- Component map: Semi-supervised learning module -> Active learning module -> IoU-based class weighting module
- Critical path: Training loop → Active learning trigger check → Sample selection → Human annotation → Data pool update → IoU calculation → Weight adjustment → Resume training
- Design tradeoffs: Image-level acquisition (easier annotation, preserves context) vs. pixel/region-level (more precise but harder to annotate); frequency-based weighting (simple) vs. IoU-based weighting (more performance-aware)
- Failure signatures: Performance plateaus despite increased annotations; certain classes consistently have low IoU; model accuracy drops when active learning trigger epochs are reached
- First 3 experiments:
  1. Run baseline with random sampling instead of active learning to quantify the performance gain from intelligent sample selection
  2. Test different acquisition strategies (entropy vs. confidence) to determine which works better for this domain
  3. Compare IoU-based weighting against frequency-based weighting to validate the class imbalance solution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SS-ADA framework perform when integrating more advanced semi-supervised learning methods beyond UniMatch and U2PL?
- Basis in paper: [explicit] The paper mentions using UniMatch and U2PL as semi-supervised learning modules but suggests exploring "more effective data acquisition strategies" in future work.
- Why unresolved: The paper only tests two semi-supervised learning methods, leaving uncertainty about whether other methods could further improve performance.
- What evidence would resolve it: Comparative experiments using alternative semi-supervised learning methods (e.g., ST++, USRN, or recent transformer-based approaches) integrated with the same active learning and class weighting strategies.

### Open Question 2
- Question: What is the optimal number of active learning trigger epochs (Tac) for different domain adaptation scenarios?
- Basis in paper: [explicit] The paper mentions that "the impact of different Tac on model accuracy is not particularly significant" but uses a fixed configuration (20, 40, 60) across all experiments.
- Why unresolved: The paper only tests one Tac configuration per dataset and doesn't systematically explore the trade-off between annotation budget and performance across varying Tac settings.
- What evidence would resolve it: A comprehensive ablation study varying Tac configurations across different domain adaptation scenarios to identify optimal sampling schedules.

### Open Question 3
- Question: How would incorporating vision foundation models into the SS-ADA framework affect its performance and annotation efficiency?
- Basis in paper: [explicit] The paper explicitly states "consider incorporating vision foundation models into the framework" as future work.
- Why unresolved: The paper doesn't test any foundation model integration, leaving uncertainty about potential benefits or challenges.
- What evidence would resolve it: Experimental results comparing SS-ADA with and without foundation model integration across the same datasets, measuring both performance and annotation efficiency.

## Limitations

- Dataset Generalization: Results primarily validated on well-established benchmark datasets, performance on more diverse driving environments remains unexplored
- Computational Overhead: Integration of active learning with semi-supervised learning introduces additional computational complexity, though specific costs are not quantified
- Active Learning Assumptions: Effectiveness depends on correlation between model uncertainty and true informativeness, which may not always hold

## Confidence

- **High Confidence**: Core claim of reducing labeling requirements while maintaining accuracy is well-supported by experimental results across multiple datasets
- **Medium Confidence**: Specific mechanisms of entropy/confidence-based selection and IoU-based weighting are theoretically sound but relative contributions are not fully isolated
- **Medium Confidence**: "Supervised-level accuracy" claim requires careful interpretation as supervised learning with full labels represents an upper bound rather than comparable baseline in terms of annotation efficiency

## Next Checks

1. **Ablation Study**: Conduct experiments isolating contributions of active learning vs. semi-supervised learning vs. IoU-based weighting to quantify marginal benefit of each component
2. **Cross-Domain Robustness**: Test SS-ADA on additional driving datasets with varying characteristics (e.g., nighttime, rain, rural environments) to assess generalization beyond current benchmark scenarios
3. **Annotation Efficiency Analysis**: Measure actual annotation time and cost differences between random sampling and active learning selection to validate practical benefits beyond accuracy metrics