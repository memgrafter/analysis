---
ver: rpa2
title: 'This Too Shall Pass: Removing Stale Observations in Dynamic Bayesian Optimization'
arxiv_id: '2405.14540'
source_url: https://arxiv.org/abs/2405.14540
tags:
- function
- time
- dataset
- optimization
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of dynamic Bayesian optimization
  (DBO), where the objective function changes over time. The key challenge is that
  past observations become less relevant as time passes, while the algorithm must
  maintain a high sampling frequency to track the optimum.
---

# This Too Shall Pass: Removing Stale Observations in Dynamic Bayesian Optimization

## Quick Facts
- arXiv ID: 2405.14540
- Source URL: https://arxiv.org/abs/2405.14540
- Authors: Anthony Bardou; Patrick Thiran; Giovanni Ranieri
- Reference count: 40
- Key outcome: W-DBO outperforms state-of-the-art DBO methods by a comfortable margin on various benchmarks, including synthetic functions and real-world experiments

## Executive Summary
This paper addresses the challenge of dynamic Bayesian optimization (DBO) where objective functions change over time, making past observations increasingly irrelevant. The key innovation is a Wasserstein distance-based criterion that measures observation relevancy, enabling the W-DBO algorithm to remove stale data on the fly while maintaining high sampling frequency. This approach allows W-DBO to achieve better predictive performance compared to existing DBO methods, as demonstrated on benchmarks like the Ackley function where it maintains significantly lower average regret.

## Method Summary
The authors introduce W-DBO, an algorithm that leverages Wasserstein distance to quantify how relevant past observations remain as the objective function evolves. The algorithm dynamically removes observations that fall below a relevance threshold, balancing the need to track the optimum with maintaining predictive accuracy. By incorporating this removal mechanism into the Bayesian optimization framework, W-DBO can operate at high sampling frequencies without being burdened by stale data that could degrade performance.

## Key Results
- W-DBO outperforms state-of-the-art DBO methods by a comfortable margin on various benchmarks
- On the Ackley function, W-DBO maintains significantly lower average regret compared to other methods
- The algorithm demonstrates effectiveness on both synthetic functions and real-world experimental scenarios

## Why This Works (Mechanism)
The Wasserstein distance criterion effectively measures the dissimilarity between the current and past objective functions, providing a principled way to determine when observations become stale. By removing these observations, the algorithm prevents outdated information from corrupting the surrogate model, allowing it to focus computational resources on tracking the current optimum. This dynamic data pruning enables high sampling frequencies without sacrificing predictive accuracy.

## Foundational Learning

**Dynamic Bayesian Optimization**: Optimization of time-varying objective functions where the optimum changes over time. Needed to model real-world scenarios where environments evolve, requiring algorithms that can adapt quickly while maintaining performance.

**Wasserstein Distance**: A metric measuring the distance between probability distributions based on optimal transport theory. Used here to quantify how much the objective function has changed over time, providing a principled way to assess observation relevancy.

**Gaussian Process Regression**: A non-parametric Bayesian approach for modeling functions based on observed data. Forms the backbone of Bayesian optimization, providing uncertainty estimates that guide the search for optimal points.

**Regret Minimization**: The objective of minimizing the cumulative difference between the best observed value and the true optimum over time. Critical performance metric for evaluating optimization algorithms in dynamic settings.

**Quick Check**: For each concept, verify understanding by explaining how it contributes to solving the dynamic optimization problem and why alternative approaches might be less suitable.

## Architecture Onboarding

**Component Map**: Data buffer -> Wasserstein relevance scorer -> Observation removal module -> Gaussian process model -> Acquisition function -> Sampling strategy

**Critical Path**: The algorithm continuously evaluates observation relevancy using Wasserstein distance, removes stale observations, updates the GP model, and selects new sampling points through the acquisition function.

**Design Tradeoffs**: The choice of Wasserstein distance provides a principled metric for relevancy but introduces computational overhead. The threshold for removal balances between retaining useful information and preventing stale data from degrading performance.

**Failure Signatures**: Performance degradation occurs when the relevancy threshold is set too high (premature removal of useful data) or too low (retaining stale observations that corrupt the model).

**First Experiments**: 
1. Test W-DBO on a simple synthetic function with known dynamics to verify basic functionality
2. Compare performance with varying relevancy thresholds to find optimal settings
3. Evaluate scalability by testing on higher-dimensional benchmark functions

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to high-dimensional problems is uncertain due to computational expense of Wasserstein distance calculations
- Theoretical convergence guarantees are not explicitly established, particularly under varying rates of environmental change
- Empirical evaluation is limited to specific benchmark functions and scenarios that may not capture all practical use cases

## Confidence
- High confidence: The core methodology and experimental design are sound
- Medium confidence: The effectiveness of the Wasserstein-based criterion across diverse scenarios
- Low confidence: Theoretical convergence guarantees and scalability analysis

## Next Checks
1. Evaluate W-DBO on high-dimensional benchmark functions (d > 10) to assess scalability and computational overhead
2. Conduct experiments with varying rates of environmental change to test the algorithm's robustness to different dynamics
3. Perform ablation studies comparing W-DBO with alternative distance metrics (e.g., KL divergence, total variation) to validate the choice of Wasserstein distance