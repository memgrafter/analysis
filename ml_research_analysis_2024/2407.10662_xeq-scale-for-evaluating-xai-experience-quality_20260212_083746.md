---
ver: rpa2
title: XEQ Scale for Evaluating XAI Experience Quality
arxiv_id: '2407.10662'
source_url: https://arxiv.org/abs/2407.10662
tags:
- scale
- experience
- system
- items
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the XEQ Scale, a psychometric tool designed
  to evaluate the quality of Explainable Artificial Intelligence (XAI) experiences.
  The scale addresses the gap in evaluating interactive, multi-shot explanations,
  which are crucial for user-centred AI systems.
---

# XEQ Scale for Evaluating XAI Experience Quality

## Quick Facts
- arXiv ID: 2407.10662
- Source URL: https://arxiv.org/abs/2407.10662
- Reference count: 40
- Primary result: Introduces XEQ Scale with 18 items across 4 dimensions, demonstrating strong psychometric validity for evaluating multi-shot XAI experiences

## Executive Summary
This paper addresses the critical gap in evaluating interactive, multi-shot XAI explanations by introducing the XEQ Scale, a comprehensive psychometric tool for measuring XAI experience quality. The scale evaluates experiences across four dimensions: learning, utility, fulfilment, and engagement, providing a holistic framework for assessing user interactions with AI systems. Through rigorous validation involving expert content review and large-scale pilot studies, the XEQ Scale demonstrates strong psychometric properties including high internal consistency, discriminant validity, construct validity, and test-retest reliability. The scale enables iterative development of user-centric AI systems by providing actionable feedback across multiple stakeholder groups.

## Method Summary
The XEQ Scale was developed through a systematic psychometric approach beginning with compilation of 32 initial items from literature and expert review. A content validity study with 13 XAI experts used the Content Validity Ratio (CVR) method to refine items and establish four evaluation dimensions. The refined 18-item scale was then validated through pilot studies with 238 participants across two AI systems (CourseAssist and AssistHub), measuring internal consistency, discriminant validity, construct validity, and test-retest reliability. The development process followed established psychometric principles to ensure the scale captures the subjective quality of multi-shot XAI experiences.

## Key Results
- Scale comprises 18 items across four dimensions: learning, utility, fulfilment, and engagement
- Strong internal consistency demonstrated with Cronbach's alpha = 0.9562
- High test-retest reliability with ICC = 0.8609
- Discriminant validity accuracy of 0.63, successfully distinguishing between positive and negative experiences
- All items show factor loadings > 0.5, confirming construct validity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The XEQ Scale provides a unified psychometric framework for evaluating multi-shot XAI experiences across diverse stakeholder groups.
- Mechanism: By consolidating 40 initial items into 18 validated items across four dimensions (learning, utility, fulfilment, engagement), the scale captures both the holistic nature of multi-shot explanations and the subjective quality of user interactions.
- Core assumption: Multi-shot XAI experiences can be meaningfully decomposed into these four dimensions without losing the essence of the interaction.
- Evidence anchors:
  - [abstract] "XEQ quantifies the quality of experiences across four dimensions: learning, utility, fulfilment and engagement."
  - [section] "We consolidated XEQ items into four evaluation dimensions representing XAI experience quality: learning, utility, fulfilment, and engagement."
  - [corpus] Weak - no direct citations about multi-shot explanation evaluation.
- Break condition: If stakeholders require dimensions beyond these four (e.g., novelty or coherence), the scale would fail to capture critical aspects of XAI experience quality.

### Mechanism 2
- Claim: The scale achieves strong psychometric validity through rigorous development and validation procedures.
- Mechanism: Expert content validation (CVR method) followed by large-scale pilot studies established internal consistency (Cronbach's alpha = 0.9562), discriminant validity (accuracy = 0.63), construct validity (factor loadings > 0.5), and test-retest reliability (ICC = 0.8609).
- Core assumption: Psychometric validation methods from social science can be effectively applied to evaluate XAI experiences.
- Evidence anchors:
  - [abstract] "The scale demonstrates strong internal consistency (Cronbach's alpha = 0.9562), discriminant validity (accuracy = 0.63), construct validity (factor loadings > 0.5), and test-retest reliability (ICC = 0.8609)."
  - [section] "Internal consistency, discriminant validity and construct validity were evaluated with both CourseAssist and AssistHub experiences with 238 participants."
  - [corpus] Weak - limited literature on psychometric approaches specifically for XAI evaluation.
- Break condition: If the assumptions underlying psychometric theory (e.g., unidimensionality of constructs) do not hold for XAI experiences, the validity metrics would be compromised.

### Mechanism 3
- Claim: The XEQ Scale enables iterative development of user-centric XAI systems through actionable feedback.
- Mechanism: By providing both individual stakeholder scores and factor scores across dimensions, system designers can identify specific areas for improvement and measure the impact of changes over time.
- Core assumption: XAI system quality can be improved through iterative cycles based on user experience feedback.
- Evidence anchors:
  - [abstract] "These contributions extend the state-of-the-art of XAI evaluation, moving beyond the one-dimensional metrics frequently developed to assess single-shot explanations."
  - [section] "We propose the XEQ Scale as a tool to support the development and evaluation of interactive user-centric XAI systems."
  - [corpus] Weak - limited evidence of iterative XAI development practices in existing literature.
- Break condition: If XAI system improvements do not translate to measurable changes in XEQ scores, the scale would fail as a development tool.

## Foundational Learning

- Concept: Psychometric scale development and validation
  - Why needed here: The paper applies psychometric theory to create a reliable measurement instrument for XAI experiences
  - Quick check question: What are the key psychometric properties that must be established for a measurement scale to be considered valid?

- Concept: Multi-shot vs. single-shot explanations in XAI
  - Why needed here: The scale specifically evaluates interactive, multi-turn explanation experiences rather than isolated explanations
  - Quick check question: How does a multi-shot explanation differ from a single-shot explanation in terms of user interaction and system complexity?

- Concept: Content validity ratio (CVR) method
  - Why needed here: This method was used to establish expert agreement on the relevance of scale items
  - Quick check question: What threshold value indicates that an item has sufficient content validity according to the CVR method?

## Architecture Onboarding

- Component map: 18 Likert-scale items organized into four evaluation dimensions (learning, utility, fulfilment, engagement). Each item measures a specific aspect of the XAI experience, with items assigned to dimensions based on expert consensus and confirmatory factor analysis.

- Critical path: Item development → Expert content validation → Pilot testing → Psychometric validation → Benchmark development. Each stage builds on the previous one and must be completed before proceeding.

- Design tradeoffs: The scale balances comprehensiveness (18 items across 4 dimensions) with practicality (can be completed in reasonable time). More items would increase validity but reduce usability; fewer items would improve usability but potentially sacrifice measurement quality.

- Failure signatures: Poor internal consistency (low Cronbach's alpha), inability to discriminate between positive and negative experiences, low test-retest reliability, or items with weak factor loadings would indicate problems with the scale.

- First 3 experiments:
  1. Administer the XEQ Scale to stakeholders using a simple XAI system and analyze internal consistency and factor structure
  2. Compare XEQ scores between positive and deliberately degraded XAI experiences to validate discriminant validity
  3. Conduct test-retest administration with a 2-week interval to establish temporal reliability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the XEQ Scale's performance vary across different application domains and AI system types?
- Basis in paper: [inferred] The paper mentions the need for further validation across additional domains like medical applications and AI systems, and the current pilot studies had limited variability in application domains.
- Why unresolved: The current validation is limited to CourseAssist and AssistHub systems, which may not capture the full range of XAI experiences across diverse domains.
- What evidence would resolve it: Conducting pilot studies with a wider range of AI systems (e.g., medical imaging, autonomous vehicles) and application domains would provide evidence of the scale's generalizability.

### Open Question 2
- Question: How does the XEQ Scale account for the varying levels of AI expertise among stakeholders?
- Basis in paper: [explicit] The paper discusses the importance of considering stakeholder expertise in domain and AI familiarity, but does not detail how the scale adapts to different expertise levels.
- Why unresolved: The scale may not adequately capture the nuances of XAI experiences for users with varying levels of AI knowledge, potentially affecting its validity across diverse user groups.
- What evidence would resolve it: Testing the scale with stakeholders of varying AI expertise levels and analyzing the results for consistency and validity would provide evidence of its adaptability.

### Open Question 3
- Question: What is the impact of different interaction modalities (e.g., conversation, GUI) on the XEQ Scale's measurements?
- Basis in paper: [inferred] The paper mentions different interaction modalities in the sample XAI experiences but does not explore how these modalities affect the scale's measurements.
- Why unresolved: The scale's validity and reliability may vary depending on the interaction modality, but this aspect has not been thoroughly investigated.
- What evidence would resolve it: Conducting studies with the same AI system using different interaction modalities and comparing the XEQ Scale results would provide insights into its sensitivity to interaction types.

## Limitations

- Weak corpus connections to existing literature on multi-shot explanation evaluation and psychometric approaches for XAI
- Scale validity primarily established through psychometric properties rather than direct evidence of improving XAI system development outcomes
- Relies heavily on expert consensus through CVR methods, which may introduce subjective biases in item selection

## Confidence

- Psychometric validation results (Cronbach's alpha = 0.9562, ICC = 0.8609): **High**
- Discriminant validity (accuracy = 0.63): **Medium** - the threshold of 0.50 is modest
- Claims about enabling iterative XAI development: **Low** - based on logical reasoning rather than empirical evidence
- Four-dimension framework comprehensiveness: **Medium** - no validation that these dimensions capture all relevant aspects of XAI experiences

## Next Checks

1. Conduct a longitudinal study applying XEQ scores to track improvements in XAI system development cycles and correlate changes with actual user satisfaction outcomes
2. Perform a systematic comparison between XEQ Scale scores and qualitative user feedback to validate that the scale captures nuanced aspects of user experience not reflected in quantitative metrics
3. Test the scale's generalizability across different XAI domains (healthcare, finance, education) to establish whether the four dimensions are universally applicable or domain-specific