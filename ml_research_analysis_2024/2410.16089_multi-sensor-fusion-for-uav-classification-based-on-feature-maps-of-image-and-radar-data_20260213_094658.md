---
ver: rpa2
title: Multi-Sensor Fusion for UAV Classification Based on Feature Maps of Image and
  Radar Data
arxiv_id: '2410.16089'
source_url: https://arxiv.org/abs/2410.16089
tags:
- data
- detection
- classification
- sensor
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a deep learning method for fusing data from
  thermal, optronic and 2D radar sensors to improve UAV detection accuracy. It extracts
  high-level feature maps from each sensor's DNN model and fuses them using a CNN-based
  architecture with feature stacking.
---

# Multi-Sensor Fusion for UAV Classification Based on Feature Maps of Image and Radar Data

## Quick Facts
- arXiv ID: 2410.16089
- Source URL: https://arxiv.org/abs/2410.16089
- Reference count: 40
- Key outcome: Multi-sensor fusion of thermal, optronic, and radar data achieves 0.95 F1-score for UAV classification, outperforming single and two-sensor models

## Executive Summary
This paper presents a deep learning approach for UAV classification that fuses high-level feature maps from thermal, optronic, and 2D radar sensors. The method extracts feature representations from individual DNN models and combines them through a CNN-based architecture with feature stacking. Experimental results demonstrate that three-modality fusion significantly improves classification accuracy compared to single or two-sensor approaches, achieving an F1-score of 0.95.

## Method Summary
The approach extracts high-level feature maps from pre-trained DNN models for each sensor modality, then fuses them through temporal registration and feature stacking. The fusion architecture processes thermal and optronic features together through convolutional layers while maintaining radar features as a separate input stream. The model is trained using RMSprop optimizer with binary cross-entropy loss, employing early stopping and dropout regularization to prevent overfitting.

## Key Results
- Three-modality fusion model achieves F1-score of 0.95
- Single-modality thermal model achieves F1-score of 0.91
- Two-modality fusion (thermal + optronic) achieves F1-score of 0.92
- Feature stacking and multi-input architecture demonstrate significant performance improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Late fusion of high-level feature maps from thermal, optronic, and radar modalities improves classification accuracy compared to single-sensor approaches.
- Mechanism: Feature maps extracted from intermediate layers of pre-trained DNNs capture high-level semantic representations of each sensor's data. These representations are fused via feature stacking and processed through a CNN-based classifier to produce more discriminative features for UAV detection.
- Core assumption: High-level features from individual sensors retain complementary information and their fusion preserves this complementarity.
- Evidence anchors: [abstract]: "fuses high-level features extracted from individual object detection and classification models associated with thermal, optronic, and radar data." [section]: "Feature maps [39] are the resulting outputs of the convolution layers inner product of the linear filter and the underlying respective field followed by a non-linear activation function at every local portion of the input."
- Break condition: If feature maps lose spatial relationships or if sensors capture highly redundant information, the fusion benefit diminishes.

### Mechanism 2
- Claim: Temporal registration of multi-sensor data with a one-second threshold enables effective feature fusion.
- Mechanism: Aligning feature maps from thermal, optronic, and radar sensors based on timestamps ensures that corresponding detections from different sensors are processed together, maintaining temporal coherence in the fusion model.
- Core assumption: A one-second registration threshold is sufficient to match corresponding detections across different sensor types.
- Evidence anchors: [section]: "Once the match between the first two sensors is completed, the resulting samples are further compared with the corresponding radar recordings in the time domain with a threshold of one second."
- Break condition: If sensor data capture rates differ significantly or if UAV dynamics require finer temporal resolution, the one-second threshold may be insufficient.

### Mechanism 3
- Claim: Multi-modal fusion architecture with separate input streams for thermal-optronic stacked features and radar features captures complementary information from each sensor type.
- Mechanism: The architecture processes thermal-optronic features through convolutional layers while maintaining radar features as a separate input stream, then concatenates these processed features for final classification.
- Core assumption: Radar features and thermal-optronic features contain complementary information that benefits from separate processing before fusion.
- Evidence anchors: [abstract]: "combines the features of the three sensor modalities by stacking the extracted image features of the thermal and optronic sensor" [section]: "The resulting features are concatenated with the radar input into a new feature vector of 14208 elements that is given as input to a fully connected layer"
- Break condition: If one sensor modality dominates the classification decision or if feature dimensions are mismatched, the architecture may not realize its full potential.

## Foundational Learning

- Concept: Feature maps and their role in deep learning
  - Why needed here: The entire fusion approach relies on extracting and combining feature maps from pre-trained DNNs, so understanding what feature maps are and how they capture high-level representations is essential.
  - Quick check question: What distinguishes feature maps from raw sensor data in terms of information content and abstraction level?

- Concept: Temporal data registration and alignment
  - Why needed here: The fusion method requires aligning feature maps from different sensors based on timestamps, so understanding temporal registration techniques and their limitations is crucial.
  - Quick check question: How would you handle cases where the same UAV is detected by different sensors at slightly different times?

- Concept: Late fusion vs. early fusion in multi-modal learning
  - Why needed here: This work employs late fusion of high-level features rather than early fusion of raw data, so understanding the tradeoffs between these approaches is important for evaluating the methodology.
  - Quick check question: What are the advantages and disadvantages of fusing high-level features compared to fusing raw sensor data?

## Architecture Onboarding

- Component map:
  - Individual sensor DNNs (thermal, optronic, radar) → Feature extraction layers → Temporal registration module → Feature stacking (thermal+optronic) → Multi-input CNN fusion network → Binary classification output
  - Key components: Feature extraction layers (7×7×512 for optronic, 7×7×1024 for thermal, 1664-element vector for radar), temporal registration with 1-second threshold, feature stacking across last axis, two-input CNN architecture

- Critical path: Feature extraction → Temporal registration → Feature stacking → CNN processing → Classification
  - Time-sensitive: Temporal registration must occur before feature stacking
  - Resource-intensive: CNN processing of stacked features and radar features

- Design tradeoffs:
  - Single vs. multi-modality: Simpler architecture but lower accuracy (F1-score 0.91 vs 0.95)
  - Feature stacking vs. separate processing: Maintains spatial relationships but increases dimensionality
  - Registration threshold: One second balances alignment accuracy with data availability but may miss rapid UAV movements

- Failure signatures:
  - Low F1-score across all modalities: Indicates fundamental data quality or model architecture issues
  - Significant performance gap between two-modality and three-modality models: Suggests radar features may be noisy or redundant
  - High false positive rate: May indicate insufficient discriminative power in feature representations

- First 3 experiments:
  1. Train single-modality thermal model and evaluate F1-score to establish baseline performance
  2. Implement two-modality fusion (thermal + optronic) and compare performance against single-modality baseline
  3. Add radar features to create three-modality model and measure incremental improvement in classification accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed multi-sensor fusion system perform in real-world conditions with varying environmental factors (e.g., different weather conditions, time of day, UAV sizes and types)?
- Basis in paper: [inferred] The paper mentions that data capturing sessions were conducted during different times of day and in clear and cloudy sky, but it does not evaluate the system's performance under a wide range of environmental conditions.
- Why unresolved: The evaluation in the paper is limited to controlled scenarios and does not fully explore the system's robustness to real-world environmental variations.
- What evidence would resolve it: Testing the system in diverse environmental conditions and reporting its performance metrics (e.g., F1-score, precision, recall) under each condition would provide insights into its real-world applicability.

### Open Question 2
- Question: How does the proposed system handle cases where data from one or more sensors are unavailable or unreliable due to sensor failures or interference?
- Basis in paper: [explicit] The paper discusses the importance of handling data unavailability in a real-time system and mentions that the spatial registration procedure should be measured and examined.
- Why unresolved: The paper does not provide specific solutions or performance metrics for handling sensor failures or interference in the proposed fusion system.
- What evidence would resolve it: Evaluating the system's performance when one or more sensors are intentionally disabled or when sensor data is corrupted, and comparing it to the baseline performance, would demonstrate its robustness to sensor failures and interference.

### Open Question 3
- Question: How does the proposed system's performance compare to other state-of-the-art multi-sensor fusion methods for UAV detection and classification?
- Basis in paper: [inferred] The paper mentions that comparing results with other methods in bibliography is challenging due to the lack of standardized metrics for multi-modal fusion UAV detection methods.
- Why unresolved: The paper does not provide a direct comparison with other state-of-the-art methods, making it difficult to assess the proposed system's relative performance.
- What evidence would resolve it: Conducting a comprehensive comparison of the proposed system with other state-of-the-art multi-sensor fusion methods, using standardized metrics and evaluation protocols, would provide a clearer understanding of its strengths and weaknesses.

## Limitations
- Limited dataset size with only 29 recordings and 3 held out for testing may affect generalization
- One-second temporal registration threshold is arbitrary and may not be optimal for all UAV dynamics
- Lack of comparison with other state-of-the-art multi-sensor fusion methods for UAV detection

## Confidence
- High confidence in the overall fusion methodology and feature extraction approach, as these are well-established techniques in the computer vision literature
- Medium confidence in the specific architectural choices and hyperparameter settings, given limited ablation studies and sensitivity analyses
- Low confidence in the robustness of the temporal registration mechanism, as the one-second threshold lacks theoretical justification or empirical validation across different scenarios

## Next Checks
1. Conduct extensive ablation studies to quantify the individual contribution of each sensor modality and identify optimal fusion configurations
2. Test the model on a larger, more diverse dataset with varying environmental conditions, UAV types, and operational scenarios to assess generalization performance
3. Perform sensitivity analysis on the temporal registration threshold to determine its impact on classification accuracy and identify optimal values for different UAV speed ranges