---
ver: rpa2
title: Stacked Universal Successor Feature Approximators for Safety in Reinforcement
  Learning
arxiv_id: '2409.04641'
source_url: https://arxiv.org/abs/2409.04641
tags:
- susfas
- usage
- agents
- successor
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Stacked Universal Successor Feature Approximation
  for Safety (SUSFAS), a reinforcement learning method that combines successor features
  with safety controllers to handle multi-objective tasks. SUSFAS uses stacked successor
  feature approximators to predict individual successor features, enabling better
  encoding of secondary objectives like fuel efficiency.
---

# Stacked Universal Successor Feature Approximators for Safety in Reinforcement Learning

## Quick Facts
- arXiv ID: 2409.04641
- Source URL: https://arxiv.org/abs/2409.04641
- Authors: Ian Cannon; Washington Garcia; Thomas Gresavage; Joseph Saurine; Ian Leong; Jared Culbertson
- Reference count: 13
- Primary result: SUSFAS improves secondary objective performance (fuel usage) by up to 18x over SAC baselines in safety-critical tasks

## Executive Summary
This paper introduces SUSFAS, a reinforcement learning method that combines successor features with safety controllers to handle multi-objective tasks. SUSFAS uses stacked successor feature approximators to predict individual successor features, enabling better encoding of secondary objectives like fuel efficiency. Evaluated on satellite inspection and lunar lander tasks, SUSFAS outperforms SAC baselines by up to 18x in fuel usage reduction when trained with runtime assurance controllers. The method also shows improved generalization across reward weight ranges, particularly when trained with wide task weight distributions. Code will be publicly released to encourage reproducibility.

## Method Summary
SUSFAS extends universal successor feature approximation by stacking individual successor feature approximators (SFAs) rather than using a collapsed architecture. Each SFA block has dedicated encoders for states, actions, and task weights, producing separate successor features that are concatenated. The method integrates with Soft Actor-Critic (SAC) and incorporates a runtime assurance (RTA) controller during training to enforce safety constraints. The agent learns to balance primary task performance with secondary objectives like fuel efficiency while avoiding unsafe states through RTA intervention.

## Key Results
- Achieved up to 18x improvement in fuel efficiency compared to SAC baselines with RTA
- Demonstrated superior generalization across reward weight ranges when trained with wide distributions
- Successfully handled safety constraints in both satellite inspection and lunar lander environments
- Showed consistent performance improvements across different task configurations

## Why This Works (Mechanism)

### Mechanism 1
Stacking individual successor feature approximators enables independent learning of each feature dimension, preventing gradient interference and improving encoding of secondary objectives like fuel efficiency. Each SFA block has dedicated encoders producing separate successor features that are concatenated rather than combined into a collapsed network. This ensures gradients from one successor feature do not influence the others, allowing the network to focus on unique aspects of each reward component.

### Mechanism 2
Incorporating a runtime assurance (RTA) controller during training enables the agent to learn behaviors that avoid unsafe states and minimize controller interventions, leading to better fuel efficiency. The RTA controller intervenes when the agent attempts unsafe actions, and during training the agent receives penalties for both unsafe states and RTA interventions. This encourages the agent to learn policies that stay within safe bounds and minimize fuel usage to avoid triggering the RTA.

### Mechanism 3
Training with a wide range of reward weight distributions enables the agent to generalize across different task priorities and adapt to dynamic environments. The agent is exposed to task weights sampled from a distribution during training, allowing it to learn policies that perform well across a spectrum of task priorities rather than specializing on a single weight configuration. This ability to adapt to different weight configurations is crucial for real-world applications where the relative importance of objectives may change.

## Foundational Learning

- **Successor Features (SFs)**: Represent expected cumulative features of future states, enabling policies that generalize across different reward functions. Why needed: Provides a way to learn policies that can adapt to different reward weight configurations without retraining.
- **Universal Value Function Approximators (UVFAs)**: Extend value function approximation to condition on goals or tasks. Why needed: Allows the agent to learn policies that can adapt to different objectives by conditioning on task weights.
- **Soft Actor-Critic (SAC)**: Underlying reinforcement learning algorithm providing actor-critic framework and entropy regularization. Why needed: Provides stable learning and exploration through entropy regularization in the continuous control setting.

## Architecture Onboarding

- **Component map**: State, Action, Task Weight Encoders -> SFA Blocks -> Concatenation -> Output Encoder -> Successor Features -> Q-values -> Actions -> RTA Controller -> Environment
- **Critical path**: 1) Observe state, action, and task weights 2) Encode inputs using dedicated encoders within each SFA 3) Concatenate encoded representations and pass through output encoder to predict successor features 4) Use predicted successor features to compute Q-values and select actions 5) Apply RTA controller if necessary to ensure safety 6) Update actor and critic networks using SAC algorithm
- **Design tradeoffs**: Stacking vs Collapsing - stacking provides independent learning but increases model complexity; RTA intervention - ensures safety but may limit exploration; Reward weight range - wide ranges improve generalization but may hinder learning specific tasks
- **Failure signatures**: Poor fuel efficiency (inadequate encoding of secondary objectives or ineffective RTA intervention); Crashes or unsafe behavior (RTA controller not functioning correctly or reward shaping insufficient); Inability to adapt to new tasks (insufficient generalization due to narrow reward weight range or limited training data)
- **First 3 experiments**: 1) Ablation study comparing stacked vs collapsed SFA architectures on fuel efficiency 2) Evaluation of RTA controller's impact on safety and performance across different environments 3) Analysis of reward weight range effects on generalization and task adaptation

## Open Questions the Paper Calls Out

### Open Question 1
How do successor features interact with multiple, potentially conflicting RTA controllers? The paper only evaluates with a single RTA controller and doesn't investigate scenarios with multiple controllers or how the stacked SFA architecture might handle conflicting safety constraints.

### Open Question 2
What is the optimal reward weight range for training generalist agents across different environments? The paper only tests a few discrete weight ranges and doesn't provide a systematic method for determining optimal ranges for different environments or tasks.

### Open Question 3
How does the stacked architecture compare to other multi-head architectures for successor feature prediction? The paper only compares stacked vs collapsed architectures but doesn't explore alternative multi-head designs like shared encoders with separate decoders, or hierarchical architectures for SF prediction.

## Limitations
- Architecture complexity with stacked SFAs may limit scalability to higher-dimensional tasks
- RTA controller's effectiveness depends heavily on proper safety constraint definition, which may not generalize across domains
- Ablation studies comparing stacked vs collapsed architectures could benefit from more rigorous statistical analysis

## Confidence
- **High confidence**: Fundamental approach of using successor features for multi-objective RL and overall experimental methodology
- **Medium confidence**: Claimed 18x improvement in fuel efficiency (based on specific task configurations)
- **Medium confidence**: Generalization claims across reward weight ranges (pending broader experimental validation)
- **Low confidence**: Architectural benefits of expert stacking without more extensive comparative analysis

## Next Checks
1. Conduct ablation studies with varying numbers of SFA blocks to determine optimal architecture depth and identify diminishing returns
2. Test RTA controller's performance across a wider range of safety-critical scenarios beyond current inspection and landing tasks
3. Evaluate generalization capabilities by training on continuous reward weight distributions rather than discrete ranges, measuring performance on held-out weight configurations