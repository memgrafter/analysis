---
ver: rpa2
title: Benchmarking Sensitivity of Continual Graph Learning for Skeleton-Based Action
  Recognition
arxiv_id: '2401.18054'
source_url: https://arxiv.org/abs/2401.18054
tags:
- task
- sensitivity
- learning
- tasks
- order
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks continual graph learning (CGL) methods for
  skeleton-based action recognition, introducing the first CGL benchmark for spatio-temporal
  graphs. It evaluates task and class-order sensitivity of popular CGL methods on
  two datasets, revealing that task-order robust methods can still be class-order
  sensitive, meaning some classes within tasks underperform.
---

# Benchmarking Sensitivity of Continual Graph Learning for Skeleton-Based Action Recognition

## Quick Facts
- arXiv ID: 2401.18054
- Source URL: https://arxiv.org/abs/2401.18054
- Authors: Wei Wei; Tom De Schepper; Kevin Mets
- Reference count: 19
- Primary result: First CGL benchmark for spatio-temporal graphs, revealing task and class-order sensitivity of popular methods

## Executive Summary
This paper introduces the first continual graph learning (CGL) benchmark for skeleton-based action recognition, evaluating both task and class-order sensitivity of popular CGL methods. The study reveals that methods robust to task order can still be sensitive to class order, with some classes within tasks consistently underperforming. Additionally, the research highlights that GNNs behave differently than CNNs in continual learning, with deeper models often outperforming wider ones. The benchmark provides a framework for future CGL research, emphasizing the importance of addressing order and architectural sensitivity in real-world applications.

## Method Summary
The study benchmarks CGL methods on two skeleton-based action recognition datasets (N-UCLA and NTU-RGB+D) converted into 10 classes and 5 tasks each. The benchmark uses GCN and ST-GCN as backbone GNNs, training methods sequentially on tasks with 100 epochs per task. Performance is evaluated using Average Accuracy (AA), Average Forgetting (AF), and Order-normalized Performance Disparity (OPD) metrics across 120 task orders and 220 class orders. The benchmark tests popular CGL methods including EWC, MAS, TWP, LwF, GEM, REPLAY, and BARE, with hyperparameters selected through grid search.

## Key Results
- Task-order robust methods can still exhibit class-order sensitivity, with some classes within tasks consistently underperforming
- GNNs show different behavior than CNNs in CL, with deeper models (2+ layers) generally outperforming wider ones for skeleton-based tasks
- Regularization-based methods show higher task-order sensitivity on simpler tasks (UCLA-CIL) compared to complex tasks (NTU-CIL)
- The benchmark reveals architectural sensitivity that was not observed in previous CNN-based CL studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GNNs behave differently than CNNs in continual learning due to sparse graph connectivity affecting long-range feature capture.
- Mechanism: Shallow GNNs (2 layers) cannot capture features requiring information from distant nodes (e.g., hand-to-foot interactions in skeleton graphs). This leads to overfitting on initial tasks and poor generalization.
- Core assumption: The sparse connectivity in skeleton graphs (e.g., 10 edges between hand and foot nodes) prevents shallow GNNs from learning long-range dependencies.
- Evidence anchors:
  - [abstract] "We note that training a model in a CGL setting without CGL methods is equivalent to fine-tuning the model. As GNNs show empirically worse performance on fine-tuning compared to CNNs, they may also have unique properties in CGL."
  - [section] "We hypothesize that it is caused by the combination of GNN and skeleton-based action recognition tasks. The skeleton-joint graph input is connected sparsely."
  - [corpus] No direct evidence in corpus papers about sparse connectivity affecting GNN performance in CL.
- Break condition: If deeper GNNs can effectively capture long-range dependencies without overfitting, or if CNN-based approaches show similar limitations in skeleton-based tasks.

### Mechanism 2
- Claim: Class-order sensitivity reveals that some classes within tasks underperform due to unbalanced feature learning.
- Mechanism: When classes are presented in random order, the model may learn many features for one class but only a few for others within the same task, making the latter class more prone to catastrophic forgetting.
- Core assumption: Feature learning is not uniform across classes within a task, and some classes are inherently more difficult to distinguish based on available features.
- Evidence anchors:
  - [abstract] "We reveal that task-order robust methods can still be class-order sensitive, meaning some classes within tasks underperform."
  - [section] "We observe that, with randomized class order, the cluster of results is vastly different compared to the randomized task order... All methods are class-order sensitive in UCLA-CIL by observing the OPD metric."
  - [corpus] No direct evidence in corpus papers about class-order sensitivity or imbalanced feature learning within tasks.
- Break condition: If class-order sensitivity disappears when classes are carefully curated to have balanced feature requirements, or if all classes within a task consistently perform equally regardless of presentation order.

### Mechanism 3
- Claim: Task-order sensitivity is reduced when task complexity increases due to decreased overfitting.
- Mechanism: In more complex tasks (e.g., NTU-CIL with 1333 sequences per task vs UCLA-CIL with 297), the regularization-based methods can learn more generalizable embeddings that transfer better to subsequent tasks.
- Core assumption: Increased task complexity provides more diverse training samples, reducing the chance of overfitting to specific task patterns.
- Evidence anchors:
  - [abstract] "Previous studies have shown that pre-training graph neural networks (GNN) may lead to negative transfer after fine-tuning, a setting which is closely related to CL."
  - [section] "In NTU-CIL, we see an improvement in task-order robustness for many methods, except LwF. Increased task complexity decreases the chance of overfitting, which facilitates the regularization process and REPLAY as they learn generalizable embedding that can be reused by following tasks."
  - [corpus] No direct evidence in corpus papers about task complexity affecting order sensitivity.
- Break condition: If task complexity continues to increase but order sensitivity remains high, suggesting other factors dominate the forgetting phenomenon.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why models abruptly forget past tasks when learning new ones is fundamental to evaluating CGL methods.
  - Quick check question: What happens to a model's performance on task A after it learns task B, and why does this occur?

- Concept: Graph Neural Networks and their architecture
  - Why needed here: The benchmark uses GNNs as backbone networks, and their unique properties (vs CNNs) are central to the findings.
  - Quick check question: How does a 2-layer GCN process information differently from a deeper GCN when handling skeleton graphs?

- Concept: Evaluation metrics for continual learning (AA, AF, OPD)
  - Why needed here: These metrics are used to assess performance and sensitivity, and understanding their relationships is crucial for interpreting results.
  - Quick check question: How does Theorem 1 relate average accuracy (AA) to average forgetting (AF), and what does this imply about model optimization?

## Architecture Onboarding

- Component map:
  - GCN backbone: 2-layer graph convolutional network with sum and max readout
  - ST-GCN backbone: 2-layer spatial-temporal graph convolutional network
  - Classification head: MLP predictor
  - Datasets: N-UCLA and NTU-RGB+D (skeleton-based action recognition)
  - CL methods: EWC, MAS, TWP, LwF, GEM, REPLAY, BARE, JOINT

- Critical path:
  1. Load and preprocess skeleton graph data
  2. Construct tasks with disjoint classes
  3. Train model on current task with CL method
  4. Evaluate performance on all seen tasks
  5. Record AA, AF, and OPD metrics
  6. Repeat for different task/class orders and architectures

- Design tradeoffs:
  - Shallow vs deep GNNs: Shallow models may not capture long-range dependencies but are less prone to overfitting; deeper models can capture complex features but may overfit
  - Regularization vs rehearsal: Regularization methods preserve important weights but may hinder learning new tasks; rehearsal methods store past data but require memory management
  - Task complexity: Simpler tasks may show higher order sensitivity due to overfitting; complex tasks may generalize better but require more computation

- Failure signatures:
  - High AF with low AA: Model is forgetting past tasks without learning new ones effectively
  - High OPD: Performance is highly sensitive to task/class order, indicating instability
  - Low AA for all tasks: Backbone architecture or CL method is not suitable for skeleton-based action recognition
  - Class imbalance within tasks: Some classes consistently underperform, indicating class-order sensitivity

- First 3 experiments:
  1. Run BARE baseline on N-UCLA-CIL to establish catastrophic forgetting baseline
  2. Test REPLAY method on both datasets with default task order to compare regularization vs rehearsal approaches
  3. Vary GCN depth (1, 2, 4 layers) on UCLA-CIL to observe architectural sensitivity effects on AA and AF

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the class-order sensitivity observed in this study compare to real-world scenarios where class arrival order is truly random and potentially more imbalanced?
- Basis in paper: [inferred] The paper conducts experiments with 100 random class orders but acknowledges this is a small subset of all possible class orders (10!). The discussion mentions the class-order sensitivity issue is closer to real-world settings.
- Why unresolved: The study uses a limited number of random class orders, and the real-world distribution of class arrival is unknown and potentially more complex than the tested scenarios.
- What evidence would resolve it: Testing the CGL methods on a much larger set of random class orders (ideally closer to 10!) or on real-world datasets where class arrival order is not controlled would provide more insight into the true class-order sensitivity in practice.

### Open Question 2
- Question: What is the impact of different task complexity distributions on the performance of CGL methods, beyond the simple two-class task structure used in this benchmark?
- Basis in paper: [inferred] The paper constructs tasks with two classes each, but mentions that LwF's performance is sensitive to the similarity between tasks, with tasks having many leg movements vs. minimal leg movements. This suggests task complexity matters.
- Why unresolved: The study only uses a simple task structure with two classes per task. Real-world tasks may have varying numbers of classes and different complexity levels, which could significantly impact CGL performance.
- What evidence would resolve it: Benchmarking CGL methods on datasets with tasks of varying complexity and number of classes would reveal how task structure impacts performance and sensitivity.

### Open Question 3
- Question: How do the architectural sensitivity findings for GNNs in this study translate to other types of graph-structured data beyond spatio-temporal graphs used for action recognition?
- Basis in paper: [explicit] The paper observes that GNNs behave differently than CNNs in CL, with deeper models often performing better than wider ones for skeleton-based action recognition. This contradicts previous observations for CNNs on Euclidean data.
- Why unresolved: The study focuses on a specific type of graph-structured data (spatio-temporal graphs for action recognition). The architectural sensitivity findings may not generalize to other graph types with different characteristics (e.g., node features, connectivity patterns).
- What evidence would resolve it: Conducting similar architectural sensitivity experiments on other graph-structured data types (e.g., social networks, molecular graphs) would determine if the observed trends for GNNs hold across different graph domains.

## Limitations
- Limited generalizability to non-skeleton graph data due to focus on spatio-temporal connectivity patterns
- Class-order sensitivity findings may depend on specific dataset characteristics and random ordering
- Hyperparameter sensitivity not fully explored across all combinations of methods and architectures

## Confidence
- **High**: Task-order sensitivity findings and architectural sensitivity (GNN vs CNN behavior)
- **Medium**: Class-order sensitivity mechanism and its implications for real-world applications
- **Medium**: Task complexity effects on order sensitivity and generalization

## Next Checks
1. Test class-order sensitivity on a more diverse set of skeleton datasets to verify if findings generalize beyond N-UCLA and NTU
2. Conduct ablation studies varying GNN depth beyond 2 layers to confirm the hypothesis about long-range dependency capture
3. Evaluate the impact of class complexity within tasks (e.g., visual vs. motion-intensive actions) on class-order sensitivity patterns