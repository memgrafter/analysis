---
ver: rpa2
title: 'Agents on the Bench: Large Language Model Based Multi Agent Framework for
  Trustworthy Digital Justice'
arxiv_id: '2412.18697'
source_url: https://arxiv.org/abs/2412.18697
tags:
- legal
- defendant
- judge
- sentencing
- remorse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces AgentsBench, a large language model-based
  multi-agent framework designed to simulate judicial deliberation processes and enhance
  trustworthy digital justice. The framework uses multiple LLM-driven agents representing
  professional judges and lay jurors who collaboratively deliberate to reach sentencing
  decisions, mimicking real-world judicial bench dynamics.
---

# Agents on the Bench: Large Language Model Based Multi Agent Framework for Trustworthy Digital Justice

## Quick Facts
- arXiv ID: 2412.18697
- Source URL: https://arxiv.org/abs/2412.18697
- Authors: Cong Jiang; Xiaolei Yang
- Reference count: 40
- Key outcome: LLM-based multi-agent framework achieves higher accuracy and morality scores in prison term prediction through deliberative consensus

## Executive Summary
AgentsBench introduces a novel LLM-based multi-agent framework that simulates judicial deliberation processes to enhance trustworthy digital justice. The system deploys professional judge and lay juror agents who collaboratively deliberate on sentencing decisions, mimicking real-world judicial bench dynamics. Experiments on prison term prediction tasks demonstrate that this deliberative approach outperforms standard prompting, chain-of-thought, and legal syllogism baselines, achieving both higher technical accuracy and better ethical considerations in legal decision-making.

## Method Summary
AgentsBench is a LLM-based multi-agent framework designed for prison term prediction in criminal cases. The system uses multiple LLM-driven agents representing professional judges and lay jurors who collaboratively deliberate through multiple rounds. Each agent starts with an independent sentencing decision, then revises based on deliberative feedback from other agents. The presiding judge moderates discussions until consensus is reached. The framework was tested on the Chinese CAIL2018 dataset using GPT-4, GPT-3.5, and Qwen models, comparing performance against baseline methods through normalized log-distance metrics and expert assessments of legality, logicality, and morality.

## Key Results
- AgentsBench achieves 86.33% performance score with GPT-4, outperforming baseline methods on prison term prediction
- The framework demonstrates significantly better morality ratings (76.2%) compared to other approaches
- Deliberative multi-agent approach improves both accuracy and ethical considerations in legal decision-making

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent deliberation improves sentencing accuracy by integrating diverse perspectives through iterative discussion.
- Mechanism: Each agent starts with an independent sentencing decision, then revises based on deliberative feedback. The presiding judge moderates until consensus is reached.
- Core assumption: Multiple agents with different backgrounds can meaningfully improve decision quality through structured deliberation.
- Evidence anchors:
  - [abstract] "Experiments on legal judgment prediction tasks using prison term prediction show that AgentsBench outperforms baseline methods"
  - [section] "During each round of deliberation, all agents independently update their sentencing decisions based on the deliberation outcomes"
  - [corpus] Weak evidence; corpus neighbors focus on legal AI generally but don't specifically validate multi-agent deliberation mechanisms
- Break condition: If agents converge too quickly to consensus without meaningful deliberation, or if deliberation introduces bias amplification rather than mitigation.

### Mechanism 2
- Claim: Role-based agent specialization (judge vs juror) enhances decision quality by balancing legal expertise with societal perspectives.
- Mechanism: Judge agents moderate discussions and ensure legal compliance, while juror agents contribute diverse societal viewpoints. This creates a more balanced deliberation.
- Core assumption: Different agent roles can effectively simulate real-world judicial bench composition and improve decision quality.
- Evidence anchors:
  - [section] "To closely simulate a real-world collegial bench, we set up different types of agents, each fulfilling a distinct judicial role"
  - [section] "judge agents are responsible for overseeing the entire decision-making process, moderating discussions"
  - [corpus] Moderate evidence; "AgentsCourt: Building Judicial Decision-Making Agents with Court Debate Simulation" suggests similar approaches exist
- Break condition: If role specialization leads to polarization rather than balanced deliberation, or if one role dominates the discussion.

### Mechanism 3
- Claim: Consensus-building through iterative deliberation improves both performance and morality scores by incorporating multiple ethical perspectives.
- Mechanism: Agents discuss and revise sentencing decisions through multiple rounds until consensus is reached, incorporating both legal and ethical considerations.
- Core assumption: Iterative consensus-building process can improve both technical accuracy and ethical considerations in legal decision-making.
- Evidence anchors:
  - [abstract] "AgentsBench outperforms baseline methods... and significantly better morality ratings (76.2%)"
  - [section] "The deliberative multi-agent approach improves both accuracy and ethical considerations"
  - [corpus] Moderate evidence; "Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering" suggests trustworthiness is achievable
- Break condition: If consensus process leads to lowest-common-denominator decisions or if ethical considerations compromise legal accuracy.

## Foundational Learning

- Concept: Multi-agent systems and collaborative deliberation
  - Why needed here: Understanding how multiple autonomous agents can work together to solve complex problems is crucial for grasping the AgentsBench framework
  - Quick check question: How does the deliberative process in multi-agent systems differ from simple majority voting?

- Concept: Legal reasoning and sentencing principles
  - Why needed here: The framework applies legal knowledge to sentencing decisions, requiring understanding of how legal principles translate into computational models
  - Quick check question: What are the key factors that judges typically consider when determining appropriate prison terms?

- Concept: Large language model capabilities and limitations
  - Why needed here: The framework relies on LLMs to simulate judicial agents, requiring understanding of what LLMs can and cannot do in legal contexts
  - Quick check question: What are the main challenges in using LLMs for legal judgment prediction tasks?

## Architecture Onboarding

- Component map: Agent initialization → Independent sentencing → Deliberation rounds → Consensus determination → Final decision
- Critical path: The deliberation process is the core component; without effective multi-round discussions, the framework loses its distinguishing advantage
- Design tradeoffs: Accuracy vs. deliberation time, diversity of perspectives vs. consensus difficulty, legal precision vs. ethical considerations
- Failure signatures: Inconsistent agent behavior due to temperature settings; LLMs struggling with Chinese legal terminology; premature consensus without meaningful deliberation
- First experiments: 1) Run zero-shot prison term prediction with single agent vs multi-agent deliberation, 2) Compare different LLM models (GPT-4, GPT-3.5, Qwen) on same cases, 3) Test varying numbers of deliberation rounds on decision quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the deliberative process affect sentencing outcomes compared to single-agent models across different types of criminal cases?
- Basis in paper: [inferred] The paper mentions AgentsBench outperforming baselines on prison term prediction and notes the framework's extensibility to other case types, but doesn't provide comparative data across different crime categories.
- Why unresolved: The paper only evaluates on one specific task (prison term prediction) without testing how the deliberative approach performs across various crime types or legal domains.
- What evidence would resolve it: Empirical results comparing AgentsBench performance across multiple crime categories (violent crimes, property crimes, white-collar crimes, etc.) against single-agent models.

### Open Question 2
- Question: What is the optimal composition of the judicial bench (ratio of professional judges to lay jurors) for different case complexities?
- Basis in paper: [explicit] The paper describes selecting different types of agents (professional judges and lay jurors) but doesn't explore how varying the composition affects outcomes.
- Why unresolved: While the framework simulates a collegial bench, it doesn't investigate how different bench compositions impact decision quality or consistency across cases of varying complexity.
- What evidence would resolve it: Controlled experiments testing different bench compositions (e.g., 1 judge/2 jurors vs 2 judges/1 juror vs all professional judges) across cases with different complexity levels.

### Open Question 3
- Question: How does the deliberative process impact the consistency and predictability of sentencing decisions over time?
- Basis in paper: [inferred] The paper emphasizes improved accuracy and ethical considerations but doesn't address long-term consistency or how decisions might vary across similar cases.
- Why unresolved: The framework focuses on single-case deliberation without examining whether the process produces consistent outcomes across similar cases or how it affects sentencing predictability in practice.
- What evidence would resolve it: Longitudinal studies tracking sentencing decisions for similar cases over time to measure consistency rates and compare them to traditional sentencing patterns.

## Limitations
- Framework's reliance on LLM-based agents introduces quality variations depending on underlying model capabilities
- Consensus-building process may amplify existing biases rather than mitigating them
- Chinese legal context limits generalizability to other jurisdictions

## Confidence

**High Confidence:** The core mechanism of multi-agent deliberation improving sentencing accuracy is well-supported by experimental results showing AgentsBench outperforming baseline methods across multiple LLM models.

**Medium Confidence:** The role-based specialization claim is moderately supported, though the specific mechanisms by which judge and juror roles contribute differently to decision quality need further investigation.

**Medium Confidence:** The consensus-building mechanism's dual improvement of both accuracy and morality scores is supported by the data, but the causal relationship between deliberation rounds and ethical improvements requires more rigorous validation.

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of deliberation rounds, role specialization, and consensus mechanisms to overall performance.
2. Test the framework's performance across different legal jurisdictions and languages to assess generalizability beyond the Chinese legal context.
3. Implement bias detection and mitigation techniques during the deliberation process to verify that multi-agent discussion actually reduces rather than amplifies existing biases.