---
ver: rpa2
title: Queueing Matching Bandits with Preference Feedback
arxiv_id: '2410.10098'
source_url: https://arxiv.org/abs/2410.10098
tags:
- have
- then
- algorithm
- proof
- provide
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a queueing matching bandit framework with\
  \ preference feedback, where jobs arrive randomly in queues and servers stochastically\
  \ serve jobs based on unknown preferences. The authors propose UCB and Thompson\
  \ Sampling algorithms that achieve system stability with average queue length bounds\
  \ of O(min{N,K}/\u03F5) and sublinear regret bounds of \xD5(min{\u221ATQmax,T3/4}),\
  \ where Qmax is the maximum queue length."
---

# Queueing Matching Bandits with Preference Feedback

## Quick Facts
- arXiv ID: 2410.10098
- Source URL: https://arxiv.org/abs/2410.10098
- Authors: Jung-hun Kim; Min-hwan Oh
- Reference count: 40
- Key outcome: Queueing matching bandit framework achieves O(min{N,K}/ϵ) average queue length and Õ(min{√TQmax,T3/4}) regret using UCB and Thompson Sampling with preference feedback

## Executive Summary
This paper introduces a queueing matching bandit framework where jobs arrive randomly in queues and servers stochastically serve jobs based on unknown preferences. The authors propose UCB and Thompson Sampling algorithms that learn these preferences through online feedback while maintaining system stability. The framework combines multi-class multi-server queueing theory with bandit learning, using Multinomial Logit (MNL) models to capture preference-based service rates. Both theoretical analysis and experiments demonstrate that the proposed algorithms outperform existing methods while achieving stability and low regret.

## Method Summary
The framework involves N queues with random arrivals and K servers with unknown preference parameters. Jobs are assigned to servers based on estimated utilities using MaxWeight scheduling with UCB or Thompson Sampling indices. The algorithms maintain online estimates of preference parameters using observed feedback and ensure exploration through confidence bounds or posterior sampling. Stability is achieved through Lyapunov drift minimization when a traffic slackness condition holds, and regret is bounded by controlling the gap between oracle and algorithm assignments.

## Key Results
- Average queue length bound of O(min{N,K}/ϵ) under traffic slackness condition
- Sublinear regret bounds of Õ(min{√TQmax,T3/4}) for both UCB and Thompson Sampling variants
- Experimental superiority over existing queueing bandit methods (Q-UCB, DAM-UCB, MaxWeight-UCB)
- Stability and low regret maintained even in dynamic environments with preference-based service rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Queue stability is achieved through Lyapunov drift minimization by aligning job assignments with estimated service preferences.
- Mechanism: At each time step, the algorithm assigns agents to arms using MaxWeight with UCB or TS indices based on estimated utilities. This drives the queue length drift negative when matched assortments exceed a slackness threshold, ensuring bounded average queue length.
- Core assumption: Traffic slackness condition holds: there exists an assortment assignment where λn + ϵ ≤ µ(n|Sk, θk) for all n ∈ Sk, k ∈ [K].
- Evidence anchors:
  - [abstract]: "algorithms achieve system stability with an average queue length bound of O(min{N,K}/ϵ)"
  - [section 3]: "We consider the condition of traffic slackness for stability as follows... there exists {Sk}k∈[K] ∈ M([N]) such that λn + ϵ ≤ µ(n|Sk, θk) for all n ∈ Sk and k ∈ [K]."
  - [corpus]: Weak evidence - related work focuses on different stability definitions but not MNL preference feedback.
- Break condition: If ϵ becomes too small relative to the gap between arrival and service rates, the Lyapunov drift argument fails and queue lengths may grow unbounded.

### Mechanism 2
- Claim: Sublinear regret is achieved by controlling the gap between oracle assignments and algorithm assignments through confidence bounds.
- Mechanism: UCB and TS algorithms maintain estimators of θk and construct confidence intervals (βt terms). Assignments favor assortments with high estimated utilities, reducing regret against the oracle that knows true θk.
- Core assumption: Regularity conditions hold: ∥xn∥2 ≤ 1, ∥θk∥2 ≤ 1, and there exists κ > 0 such that inf θ∈Rd:∥θ∥2≤1 µ(n|S, θ)µ(n0|S, θ) ≥ κ for any n ∈ S and S ⊂ [N].
- Evidence anchors:
  - [abstract]: "algorithms achieve sublinear regret bounds of Õ(min{√TQmax,T3/4})"
  - [section 5.1.2]: "The regret is defined as the discrepancy between the performance of the oracle policy of MaxWeight... and that of our policy π"
  - [corpus]: Weak evidence - related work achieves regret but without MNL preference feedback structure.
- Break condition: If the confidence intervals do not shrink fast enough (large κ or d), the regret bound degrades from √T to T^(3/4).

### Mechanism 3
- Claim: Thompson Sampling achieves similar stability and regret bounds through posterior sampling instead of upper confidence bounds.
- Mechanism: TS samples θ^k,t from posterior N(ˆθk,t, β2t V−1 k,t) and selects assortments maximizing expected utility under sampled parameters, balancing exploration and exploitation.
- Core assumption: Same regularity conditions as UCB, plus M = ⌈1 − log(KL)/log(1−1/4√eπ) ⌉ samples per arm per round are sufficient for exploration.
- Evidence anchors:
  - [section 5.2]: "We propose an algorithm based on Thompson Sampling, TS-QMB... which achieves system stability with an average queue length bound of O(min{N,K}/ϵ)"
  - [section 5.2.2]: "The policy π of Algorithm 2 achieves a regret bound of eO(min{d3/2κ√KT Qmax, N(d2Kκ2ϵ3)1/4T3/4})"
  - [corpus]: Weak evidence - related work uses TS but not in queueing matching bandit setting with preference feedback.
- Break condition: If the number of samples M is insufficient for exploration, the algorithm may fail to achieve stability or regret bounds.

## Foundational Learning

- Concept: Multi-class multi-server queueing systems with asymmetric service rates
  - Why needed here: The paper's setting involves N queues and K servers where jobs arrive randomly and service rates depend on unknown preferences modeled by MNL functions.
  - Quick check question: What distinguishes this queueing system from standard parallel-server queues where service rates are known and independent?

- Concept: Upper Confidence Bound (UCB) and Thompson Sampling (TS) bandit algorithms
  - Why needed here: These algorithms balance exploration and exploitation when learning unknown service rates while maintaining system stability.
  - Quick check question: How do UCB and TS differ in their approach to handling uncertainty in the estimated utility parameters?

- Concept: Lyapunov drift analysis for queueing stability
  - Why needed here: The stability proofs rely on showing that the expected drift of a quadratic Lyapunov function is negative when system conditions are favorable.
  - Quick check question: What role does the traffic slackness parameter ϵ play in the Lyapunov drift argument?

## Architecture Onboarding

- Component map: Queues (N agents) -> Scheduler -> Servers (K arms) -> Preference Feedback -> Estimator -> UCB/TS mechanism

- Critical path:
  1. Receive job arrivals in queues
  2. Compute estimated utilities using current θ^k,t
  3. Construct UCB or TS indices
  4. Solve combinatorial assignment problem using MaxWeight with indices
  5. Execute assignment and observe preference feedback
  6. Update estimators using observed feedback

- Design tradeoffs:
  - UCB vs TS: UCB provides cleaner confidence bounds for regret analysis but may explore less efficiently than TS
  - Approximation oracle: Exact combinatorial optimization is NP-hard; α-approximation reduces computation at the cost of degraded bounds
  - Sample complexity: More samples per arm (larger M for TS) improves exploration but increases computational cost

- Failure signatures:
  - Queue lengths growing unboundedly: Indicates traffic slackness condition is violated or estimation error is too large
  - Regret growing superlinearly: Suggests confidence intervals not shrinking fast enough or exploration insufficient
  - Computational timeouts: Combinatorial optimization becoming intractable for large N, K, or L

- First 3 experiments:
  1. Synthetic experiment with N=4, K=2, L=2, d=2: Compare average queue length and regret of UCB-QMB, TS-QMB against baselines (Q-UCB, DAM-UCB, MaxWeight-UCB)
  2. Sensitivity analysis: Vary ϵ to test traffic slackness impact on stability and regret
  3. Dimensionality scaling: Increase d to evaluate impact on estimation accuracy and regret bounds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the lower bounds for queue lengths and regret in this queueing matching bandit framework?
- Basis in paper: [explicit] The authors state "It remains an open problem to establish lower bounds for queue lengths and regret" and acknowledge that constructing such bounds would be more challenging compared to other parametric bandit problems.
- Why unresolved: The authors did not pursue establishing these lower bounds in their current work.
- What evidence would resolve it: Mathematical proofs establishing the theoretical minimum values for queue lengths and regret under the given assumptions and constraints.

### Open Question 2
- Question: Can the dependency on κ for queue length and regret bounds be improved by exploiting the structure of the MNL model?
- Basis in paper: [explicit] The authors mention "improving the dependency on κ for queue length and regret bounds from the structure of MNL would be an interesting avenue for future work."
- Why unresolved: The current analysis does not optimize the dependence on κ, leaving room for potential improvements.
- What evidence would resolve it: Refined theoretical analysis showing tighter bounds with reduced dependence on κ, leveraging specific properties of the MNL model.

### Open Question 3
- Question: Can an improved approximation oracle be developed for the combinatorial optimization step in the proposed algorithms?
- Basis in paper: [inferred] The authors discuss the NP-hard nature of the combinatorial optimization and mention that "developing an improved approximation oracle would also be an interesting direction for future research."
- Why unresolved: The current algorithms rely on existing α-approximation oracles, which may not be optimal for this specific problem.
- What evidence would resolve it: Development and analysis of a new approximation algorithm that provides better performance guarantees or computational efficiency for the combinatorial optimization in the queueing matching bandit framework.

## Limitations
- Traffic slackness condition (ϵ > 0) is critical but may be difficult to verify in practice
- NP-hard combinatorial optimization requires approximation oracles that may introduce performance gaps
- Regret bounds depend on unknown constants κ and d, making practical performance prediction difficult

## Confidence
- **High Confidence**: Queue stability claims under traffic slackness (supported by Lyapunov drift analysis and experimental validation)
- **Medium Confidence**: Sublinear regret bounds (theoretical derivation appears sound but depends on unknown constants and approximation quality)
- **Medium Confidence**: Algorithm performance superiority (experimental results show improvement but limited to synthetic settings with small N, K, d)

## Next Checks
1. **Empirical bound verification**: Conduct experiments with varying ϵ values to empirically measure how average queue length scales with ϵ⁻¹ and validate the O(min{N,K}/ϵ) bound.
2. **Regret bound sensitivity**: Test algorithm performance across different κ and d values to quantify the impact of regularity conditions and dimensionality on regret scaling.
3. **Approximation oracle evaluation**: Compare exact vs. approximate combinatorial optimization implementations to measure the performance gap introduced by α-approximation in real-world settings.