---
ver: rpa2
title: Slow Convergence of Interacting Kalman Filters in Word-of-Mouth Social Learning
arxiv_id: '2410.08447'
source_url: https://arxiv.org/abs/2410.08447
tags:
- agents
- agent
- learning
- social
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies word-of-mouth social learning with m sequential
  Kalman filter agents, where each agent receives a noisy estimate of the previous
  agent's state estimate rather than raw observations. When m=2 agents, the covariance
  of the estimate decreases as k^{-1/3} instead of the standard O(k^{-1}) rate, indicating
  slow learning.
---

# Slow Convergence of Interacting Kalman Filters in Word-of-Mouth Social Learning

## Quick Facts
- arXiv ID: 2410.08447
- Source URL: https://arxiv.org/abs/2410.08447
- Authors: Vikram Krishnamurthy; Cristian Rojas
- Reference count: 14
- Key outcome: m sequential Kalman filter agents learn slowly, with covariance decreasing as k^{-(2^m-1)} instead of O(k^{-1})

## Executive Summary
This paper studies word-of-mouth social learning where agents sequentially share noisy estimates rather than raw observations. The authors prove that when m agents implement Kalman filters in this manner, the precision of the final estimate decreases to zero at rate k^{-(2^m-1)}, which is exponentially slower than the standard O(k^{-1}) convergence. The slow learning arises from a polynomial dependency structure that compounds exponentially with the number of agents. To recover optimal learning rates, the authors propose artificially re-weighting the prior covariance at each time step, showing this can achieve the optimal O(k^{-1}) convergence rate.

## Method Summary
The paper analyzes sequential Kalman filter agents where each agent receives a noisy estimate of the previous agent's state estimate. The authors prove convergence rates by characterizing how precision evolves recursively through the chain of agents. For m agents, they show that precision grows as k^{1/(2^m-1)} through induction and asymptotic analysis. To recover optimal rates, they propose artificially scaling the prior covariance by k^{-δ} with δ=1, which prevents the polynomial compounding structure from dominating the convergence behavior.

## Key Results
- For m=2 agents, covariance decreases as k^{-1/3} instead of O(k^{-1})
- For m agents, covariance decreases as k^{-(2^m-1)}
- Artificially scaling prior covariance by k^{-1} recovers optimal O(k^{-1}) convergence rate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The slow convergence rate k^{-1/(2^m-1)} arises because each agent's precision update depends on the previous agent's precision through a nested polynomial structure, which compounds exponentially with the number of agents.
- Mechanism: When agent i+1 computes its precision update, it treats the noisy estimate from agent i as an observation. The effective noise variance includes not only the agent i's observation noise but also the noise from all upstream agents, each scaled by the ratio of precisions. This creates a recursive polynomial relationship in the precision that grows in degree with each agent.
- Core assumption: All agents except the last one use the public posterior (ρ^(m)_k-1) instead of their own local precision, creating a chain of dependencies.
- Evidence anchors:
  - [abstract] "The authors prove that for m agents, the covariance decreases to zero as k^{-(2^m-1)}"
  - [section] "γ(j)_k ≈ Ck^{(2·2^j - 2)β}" showing the exponential growth in polynomial degree
  - [corpus] No direct corpus evidence found - this mechanism is specific to the paper's formulation
- Break condition: If any agent uses its own local precision instead of the public posterior, the exponential compounding stops and convergence rate improves.

### Mechanism 2
- Claim: The exponential slowdown is robust to noise variance because even small noise causes the polynomial dependency structure to dominate the convergence behavior.
- Mechanism: The authors show that regardless of the noise variance values, as long as they're non-zero, the leading term in the precision update equation determines the asymptotic behavior. The polynomial structure ensures that the noise from upstream agents propagates through the chain, dominating the convergence rate.
- Core assumption: The noise variances are positive but can be arbitrarily small.
- Evidence anchors:
  - [abstract] "Slow learning occurs regardless of the noise variance that corrupts the estimates"
  - [section] "Remark 1. Slow learning occurs regardless of the noise variance"
  - [corpus] No direct corpus evidence found - this is a novel result in the paper
- Break condition: If noise variance becomes zero, the standard O(k^{-1}) convergence is recovered.

### Mechanism 3
- Claim: Artificially scaling the prior precision by k^{-δ} with δ=1 recovers the optimal O(k^{-1}) convergence rate by preventing the polynomial dependency from compounding.
- Mechanism: By scaling down the public precision used by all agents except the last one, the effective noise variance in each agent's update equation is prevented from growing too rapidly. This breaks the polynomial compounding structure and allows the precision to grow linearly with k.
- Core assumption: The scaling factor k^{-δ} is applied consistently to all agents except the last one.
- Evidence anchors:
  - [abstract] "by artificially weighing the prior at each time, the learning rate can be made optimal as k^{-1}"
  - [section] "Theorem 2 shows that we can recover the optimal convergence rate of 1/k by artificially scaling the prior covariance"
  - [corpus] No direct corpus evidence found - this is the paper's proposed solution
- Break condition: If δ < 1, the convergence rate remains suboptimal; if δ > 1, the precision growth is bounded and doesn't converge to zero.

## Foundational Learning

- Concept: Bayesian inference with Gaussian distributions
  - Why needed here: The entire analysis relies on computing posterior distributions when combining Gaussian priors with Gaussian observations
  - Quick check question: Given a Gaussian prior N(μ₀, σ₀²) and Gaussian observation N(θ, σ²ₑ), what is the posterior precision ρ = 1/σ²?
- Concept: Kalman filter recursion equations
  - Why needed here: Each agent implements a Kalman filter, and the analysis depends on understanding how precision and mean estimates update recursively
  - Quick check question: In a standard Kalman filter, how does the precision update when a new observation with precision ρₑ is received?
- Concept: Asymptotic analysis of recursive sequences
  - Why needed here: The paper characterizes the long-term behavior of precision sequences using asymptotic methods and matching exponents
  - Quick check question: If Xₖ = Xₖ₋₁ + C/k^α, what is the asymptotic behavior of Xₖ as k→∞?

## Architecture Onboarding

- Component map: Raw observation → Agent 1 → Noisy transmission → Agent 2 → ... → Agent m → Public posterior broadcast
- Critical path: Raw observation → Agent 1 → Noisy transmission → Agent 2 → ... → Agent m → Public posterior broadcast
- Design tradeoffs:
  - Information asymmetry: Agents 1 through m-1 don't have direct observations, only word-of-mouth estimates
  - Public posterior dependency: All agents except the last use the same public posterior, creating the slow convergence
  - Noise robustness: The system works with arbitrary positive noise variances but suffers exponential slowdown
- Failure signatures:
  - Precision growing slower than linear: Indicates the polynomial compounding effect is active
  - Precision bounded away from infinity: Suggests scaling parameter δ > 1 was used incorrectly
  - Precision converging to zero too slowly: Normal behavior for m > 2 without artificial scaling
- First 3 experiments:
  1. Implement m=2 agents with varying noise variances and verify convergence rate k^{-1/3}
  2. Add artificial scaling with δ=1 and verify recovery of k^{-1} convergence rate
  3. Implement m=3 agents and verify exponential slowdown to k^{-1/7} convergence rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the exponential slowdown (k^(-(2^m-1))) to variations in the noise structure of the word-of-mouth communication?
- Basis in paper: [explicit] The paper notes that slow learning occurs regardless of the noise variance corrupting the Kalman filter estimates, and that the model where agents exactly know the previous agent's estimate is not robust to even small errors.
- Why unresolved: The paper proves the exponential slowdown under the assumption of Gaussian noise but does not explore how different noise distributions or correlated errors might affect the convergence rate.
- What evidence would resolve it: Empirical or theoretical analysis of the convergence rate under various noise models (e.g., non-Gaussian, heavy-tailed, or correlated errors) would clarify the robustness of the exponential slowdown phenomenon.

### Open Question 2
- Question: Can the optimal k^(-1) convergence rate be achieved in distributed or asynchronous settings where agents do not have synchronized access to the public posterior?
- Basis in paper: [inferred] The paper proposes artificially re-weighting the prior covariance to recover the optimal rate, but assumes synchronous updates and access to the public posterior.
- Why unresolved: Real-world word-of-mouth social learning often involves delays, asynchronous updates, or partial information sharing, which could affect the feasibility of the proposed re-weighting scheme.
- What evidence would resolve it: Simulation or theoretical analysis of the convergence rate under asynchronous or distributed communication protocols would determine whether the optimal rate can be maintained in such settings.

### Open Question 3
- Question: How does the exponential slowdown in word-of-mouth social learning compare to other forms of social learning, such as information cascades or herding, in terms of the impact on collective decision-making accuracy?
- Basis in paper: [inferred] The paper focuses on the convergence rate of the precision but does not compare the implications of slow learning to other social learning phenomena like information cascades.
- Why unresolved: While the paper establishes the slowdown, it does not address how this affects the quality of collective decisions compared to other social learning models where agents might blindly follow previous actions.
- What evidence would resolve it: Comparative analysis of decision-making accuracy under word-of-mouth learning versus other social learning models (e.g., information cascades) would clarify the trade-offs between convergence speed and decision quality.

## Limitations
- The analysis assumes perfect implementation of Kalman filter recursions and relies heavily on asymptotic approximations
- The polynomial compounding mechanism depends critically on agents using the public posterior rather than their own local precision
- The proposed artificial scaling solution requires careful parameter tuning without comprehensive guidance

## Confidence

**High confidence**: The core mathematical derivations showing k^{-1/(2^m-1)} convergence for m agents, as these follow from explicit induction arguments

**Medium confidence**: The robustness claim that slow learning occurs regardless of noise variance, since this depends on asymptotic dominance arguments that may have edge cases

**Medium confidence**: The effectiveness of artificial scaling to recover O(k^{-1}) rates, as numerical validation is limited

## Next Checks

1. Implement Monte Carlo simulations for m=3 and m=4 agents to verify the predicted k^{-1/7} and k^{-1/15} convergence rates empirically, checking for any deviations from theoretical predictions

2. Test the artificial scaling approach with δ=1 across a range of noise variance combinations to confirm robustness and identify any parameter regimes where the solution breaks down

3. Analyze sensitivity to implementation details by comparing the public posterior approach against variants where agents use local precisions, measuring the impact on convergence rates