---
ver: rpa2
title: Learning To Play Atari Games Using Dueling Q-Learning and Hebbian Plasticity
arxiv_id: '2405.13960'
source_url: https://arxiv.org/abs/2405.13960
tags:
- training
- learning
- plasticity
- state
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an advanced deep reinforcement learning architecture
  for training neural network agents to play Atari games using only raw game pixels,
  action space, and reward information. The system employs techniques like deep Q-networks
  and dueling Q-networks, similar to DeepMind's approach for training agents that
  beat human players in Atari games.
---

# Learning To Play Atari Games Using Dueling Q-Learning and Hebbian Plasticity

## Quick Facts
- arXiv ID: 2405.13960
- Source URL: https://arxiv.org/abs/2405.13960
- Reference count: 21
- Primary result: Plastic neural networks show higher and more stable reward values during plasticity training phases compared to normal training over the same number of episodes.

## Executive Summary
This paper presents an advanced deep reinforcement learning architecture for training neural network agents to play Atari games using raw game pixels, action space, and reward information. The system employs deep Q-networks and dueling Q-networks, with an extension exploring plastic neural networks that implement plasticity based on backpropagation and the Hebbian update rule. The key finding is that during plasticity training phases, mean reward values are higher and more stable compared to normal training over the same number of episodes, suggesting plasticity injection could optimize reinforcement learning algorithms.

## Method Summary
The method combines deep Q-learning with Hebbian plasticity injection in dueling DQN architectures. During plasticity training, fixed weights are frozen while plastic weights (Hebbian traces) are updated using Hebbian rules and backpropagation. The system uses epsilon-greedy policy with 10% exploration during plasticity training to prevent catastrophic forgetting and overfitting. Experience replay buffer with FIFO eviction policy provides diverse training samples. The approach balances exploitation of learned knowledge with adaptation to new experiences, allowing for lifelong learning after initial training.

## Key Results
- Plastic neural networks maintain higher and more stable reward values during plasticity training compared to normal training
- Epsilon-greedy policy with 10% exploration during plasticity training prevents catastrophic forgetting and overfitting
- Experience replay buffer with FIFO eviction policy provides diverse training samples and mitigates correlated data issues

## Why This Works (Mechanism)

### Mechanism 1
Plastic neural networks maintain higher and more stable reward values during plasticity training phases compared to normal training over the same number of episodes. During plasticity training, fixed weights are frozen while plastic weights (Hebbian traces) are updated using Hebbian rules and backpropagation. This allows the network to preserve its best performance while remaining open to new experiences for life. Core assumption: The combination of frozen fixed weights and adaptable plastic weights creates a balance between exploitation of learned knowledge and exploration of new experiences. Evidence anchors: [abstract] "As an extension, the work explores using plastic neural networks as agents, implementing plasticity based on backpropagation and the Hebbian update rule. Plastic neural networks offer the advantage of lifelong learning after initial training, making them suitable for adaptive learning environments." [section] "During the plasticity training phases, the mean reward value is higher and more stable compared to normal training over the same number of episodes." Break condition: If the plasticity contribution parameter α is set too high, plastic weights might dominate and cause instability. Also, if the plasticity learning rate η is too low, adaptation to new experiences would be too slow.

### Mechanism 2
Epsilon-greedy policy with small exploration probability during plasticity training prevents catastrophic forgetting and overfitting. By allowing a small probability (10%) of random actions during plasticity training, the system avoids strengthening only the connections associated with previously successful actions, which would lead to overfitting and inability to adapt to new situations. Core assumption: Complete exploitation of learned knowledge during plasticity training leads to overfitting and inability to adapt to new experiences. Evidence anchors: [section] "During the plastic training phase, a small amount of random actions were permitted (10%). If the actions with the highest Q-values are only permitted, then only the strong connections get stronger over time, resulting in very bad overfitting." [section] "In the plasticity training graph snippets, the values are much more stable compared to the dueling DQN snippets." Break condition: If epsilon is set to 0 during plasticity training, the system would only exploit learned knowledge and suffer from catastrophic forgetting. If epsilon is too high, exploration would dominate and learning efficiency would decrease.

### Mechanism 3
Experience replay buffer with FIFO eviction policy provides diverse training samples and mitigates correlated data issues. Random sampling from a large buffer of past experiences provides the model with diverse training data, reducing the impact of correlated consecutive experiences and promoting more stable learning. Core assumption: Without experience replay, the model would be trained on highly correlated consecutive experiences, leading to unstable learning and poor generalization. Evidence anchors: [section] "Another important component of deep Q-learning is the experience-replay buffer. This buffer is typically large enough to store a considerable number of experiences encountered during training." [section] "By randomly sampling batches of experiences from this buffer during training, the model learns from a diverse set of past experiences, mitigating issues of correlated data and facilitating more stable learning." Break condition: If the replay buffer is too small, the diversity of training samples would be limited. If the eviction policy is not random but based on oldest experiences only, important recent experiences might be lost prematurely.

## Foundational Learning

- Concept: Bellman Optimality Equation
  - Why needed here: Forms the theoretical foundation for Q-learning and deep Q-networks, providing the mathematical framework for optimal value estimation in reinforcement learning.
  - Quick check question: What is the relationship between the optimal value function V*(s) and the optimal action-value function Q*(s,a) according to the Bellman equation?

- Concept: Epsilon-Greedy Policy
  - Why needed here: Balances exploration and exploitation during training, allowing the agent to discover new strategies while refining learned behaviors.
  - Quick check question: How does the epsilon value typically change during training, and what is the purpose of this change?

- Concept: Experience Replay Buffer
  - Why needed here: Stores past experiences to provide diverse training samples and mitigate issues with correlated consecutive experiences.
  - Quick check question: What are the two main benefits of using an experience replay buffer in deep Q-learning?

## Architecture Onboarding

- Component map: Environment -> Preprocessing -> Neural Network Agent -> Training Loop -> Plasticity Module
- Critical path: 1. Environment step → 2. State preprocessing → 3. Action selection via epsilon-greedy → 4. Experience storage → 5. Batch sampling from replay buffer → 6. Target Q-value calculation → 7. Loss computation → 8. Parameter updates → 9. (Optional) Plasticity updates
- Design tradeoffs: Fixed vs. plastic weights (balance between preserving learned knowledge and adapting to new experiences), epsilon value (tradeoff between exploration and exploitation), replay buffer size and eviction policy (balance between memory usage and training sample diversity), learning rates for fixed and plastic weights (separate optimization of different weight types)
- Failure signatures: Unstable reward values (poor balance between exploration and exploitation or insufficient diversity in training samples), catastrophic forgetting (lack of exploration during plasticity training or insufficient diversity in training samples), overfitting to specific actions (need for better exploration strategy or more diverse training samples), slow learning (suboptimal hyperparameters, insufficient training time, or poor network architecture)
- First 3 experiments: 1. Train a Dueling DQN agent on a simple Atari game (e.g., Breakout) with standard hyperparameters to establish baseline performance, 2. Implement a priority replay buffer and compare performance with standard FIFO buffer to evaluate impact on learning efficiency, 3. Train a Dueling DQN with plasticity injection agent and compare reward stability and final performance with standard Dueling DQN to assess benefits of plasticity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of Hebbian plasticity with Dueling DQN architectures compare to other reinforcement learning approaches in terms of performance and stability across various Atari games?
- Basis in paper: [explicit] The paper explores the feasibility of using plastic neural networks as agents in Atari games and compares their performance with raw Dueling Double DQN rewards over time.
- Why unresolved: While the paper shows that plasticity training phases lead to higher and more stable reward values, a comprehensive comparison with other advanced reinforcement learning methods (e.g., Rainbow DQN, Prioritized Experience Replay) is not provided.
- What evidence would resolve it: Conducting experiments that directly compare the proposed method with state-of-the-art reinforcement learning algorithms on a diverse set of Atari games, measuring performance metrics such as cumulative rewards, learning stability, and sample efficiency.

### Open Question 2
- Question: What is the optimal combination of hyperparameters (e.g., plasticity learning rate, contribution parameter) for maximizing the performance of plastic neural networks in Atari games?
- Basis in paper: [explicit] The paper mentions that further exploration can be done by optimizing the hyperparameter values, such as plasticity learning rate and contribution parameter.
- Why unresolved: The paper uses fixed hyperparameter values (plasticity learning rate of 10^-3 and contribution parameter of 0.2) without exploring the impact of different combinations on the performance.
- What evidence would resolve it: Performing a systematic hyperparameter search (e.g., grid search, random search, or Bayesian optimization) to identify the best combination of hyperparameters for each Atari game, and analyzing the sensitivity of performance to these hyperparameters.

### Open Question 3
- Question: How does the use of a priority replay buffer impact the performance and learning efficiency of plastic neural networks in Atari games?
- Basis in paper: [inferred] The paper mentions that using a priority replay buffer could improve sampling efficiency and reduce overfitting, but does not implement it due to time limitations.
- Why unresolved: The paper uses a FIFO replay buffer, which may lead to correlated experiences and overfitting, especially in plastic neural networks that continue learning from ongoing experiences.
- What evidence would resolve it: Implementing a priority replay buffer in the proposed method and comparing its performance with the current FIFO replay buffer approach, measuring metrics such as cumulative rewards, learning stability, and sample efficiency across multiple Atari games.

## Limitations
- Insufficient hyperparameter documentation for the plasticity injection mechanism, particularly learning rate and contribution parameter α
- Evaluation focuses primarily on training stability rather than final performance comparison against state-of-the-art methods
- No comparison with current advanced reinforcement learning approaches like Rainbow DQN or Prioritized Experience Replay

## Confidence
- **High Confidence**: The core concept of Hebbian plasticity in neural networks and its theoretical benefits for lifelong learning are well-established in the literature.
- **Medium Confidence**: The empirical results showing higher and more stable reward values during plasticity training are promising but need validation with more detailed methodology and comparison against current state-of-the-art methods.
- **Low Confidence**: The specific implementation details of the plasticity injection mechanism, particularly the hyperparameters and their tuning process, are insufficiently documented for confident reproduction.

## Next Checks
1. Implement the exact architecture and hyperparameters described in the paper and reproduce the results on at least two Atari games (Space Invaders and Breakout) to verify the claimed reward stability improvements.
2. Compare the final performance of the plastic network against standard dueling DQN and current state-of-the-art methods (e.g., Rainbow DQN) to assess practical significance beyond training stability.
3. Conduct ablation studies varying the plasticity contribution parameter α and learning rate to understand their impact on performance and identify optimal settings for different game environments.