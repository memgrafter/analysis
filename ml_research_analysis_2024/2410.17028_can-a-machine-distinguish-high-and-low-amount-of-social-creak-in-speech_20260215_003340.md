---
ver: rpa2
title: Can a Machine Distinguish High and Low Amount of Social Creak in Speech?
arxiv_id: '2410.17028'
source_url: https://arxiv.org/abs/2410.17028
tags:
- voice
- speech
- creak
- social
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated whether machine learning could automatically
  distinguish speech with low versus high amounts of social creak in healthy young
  female speakers. Continuous speech samples were first rated by voice specialists
  and divided into two classes based on perceived creak amount.
---

# Can a Machine Distinguish High and Low Amount of Social Creak in Speech?

## Quick Facts
- arXiv ID: 2410.17028
- Source URL: https://arxiv.org/abs/2410.17028
- Reference count: 40
- Machine learning can distinguish social creak in speech with 71.1% accuracy

## Executive Summary
This study investigates whether machine learning can automatically classify speech samples based on perceived social creak amounts. Using recordings from 90 young female Finnish speakers, voice specialists rated speech samples on a 9-point scale, which were then classified into low and high creak categories. Seven machine learning classifiers were trained using three spectral features: spectrogram, mel-spectrogram, and mel-frequency cepstral coefficients. The results demonstrate that ML models can effectively distinguish between high and low social creak amounts, with the best performance achieved using mel-spectrogram features with AdaBoost (71.1% accuracy) or MFCC features with decision trees (71.1% accuracy).

## Method Summary
The study used continuous speech samples from 90 female university students reading the same 158-word text. Expert voice specialists rated these samples on a 9-point Likert scale for perceived creak amount. The samples were balanced into two classes (low and high creak) with 45 samples each. Three spectral features were extracted using 100ms frames with 5ms shift: spectrogram, mel-spectrogram (128 channels), and MFCCs. Seven classifiers (SVM-linear/RBF, Random Forest, Multilayer Perceptron, Logistic Regression, Decision Tree, AdaBoost) were trained and tested using leave-one-speaker-out cross-validation with z-score normalization and grid search for hyperparameter optimization.

## Key Results
- ML models achieved 71.1% accuracy in classifying high vs low social creak
- Mel-spectrogram feature consistently outperformed spectrogram and MFCC features across classifiers
- AdaBoost with mel-spectrogram and Decision Tree with MFCCs both achieved the highest accuracy (71.1%)
- Leave-one-speaker-out cross-validation ensured speaker-independent performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The mel-spectrogram feature outperforms MFCCs and raw spectrograms for creak classification because it balances spectral resolution with perceptual weighting.
- Mechanism: The mel-scale warping compresses high-frequency bins while preserving low-frequency detail, which captures the low-frequency periodicities characteristic of creak without losing discriminative spectral shape.
- Core assumption: Creak-related cues are stronger in the lower mel bands and not well represented by high-frequency coefficients alone.
- Evidence anchors: [abstract] "The mel-spectrogram consistently outperformed other features across classifiers."; [section] "The better performance of the mel-spectrogram feature is most likely due to... mel-weighting of speech frequencies."

### Mechanism 2
- Claim: The Decision Tree classifier leverages MFCCs more effectively than other classifiers because it can split on nonlinear combinations of cepstral coefficients.
- Mechanism: MFCCs encode spectral envelope information; DT's hierarchical splitting can isolate creak-relevant coefficient ranges without requiring linear separability.
- Core assumption: Creak presence correlates with specific nonlinear patterns in the first few MFCCs rather than with linearly separable boundaries.
- Evidence anchors: [abstract] "a decision tree classifier using the mel-frequency cepstral coefficient feature" achieved 71.1% accuracy; [section] "the DT classifier... succeeded to show high accuracy... most likely due to the DT classifier's ability to effectively leverage the discriminative power of the MFCC features."

### Mechanism 3
- Claim: Leave-One-Speaker-Out cross-validation ensures the model learns speaker-independent creak patterns rather than speaker-specific idiosyncrasies.
- Mechanism: By rotating the test speaker, the model is forced to generalize across voice qualities, preventing overfitting to individual vocal tract shapes or recording conditions.
- Core assumption: Creak is a linguistic/sociolinguistic phenomenon with consistent acoustic signatures across speakers.
- Evidence anchors: [section] "To evaluate the performance of the ML models, the leave-one-speaker-out (LOSO) cross-validation scheme is employed."; [section] "The evaluation metric (accuracy) is averaged across all iterations to represent the model's overall performance."

## Foundational Learning

- Concept: Spectrogram computation and interpretation
  - Why needed here: All three features (spectrogram, mel-spectrogram, MFCCs) are derived from time-frequency representations; understanding the baseline is essential for feature engineering decisions.
  - Quick check question: What is the difference between amplitude spectrum and power spectrum in a spectrogram?

- Concept: Mel-frequency warping and filterbank design
  - Why needed here: The mel-spectrogram relies on perceptually motivated frequency scaling; understanding this informs why it captures creak better than raw spectrogram.
  - Quick check question: Why does the mel-scale approximate human pitch perception more closely than linear frequency bins?

- Concept: Cepstral analysis and delta features
  - Why needed here: MFCCs are core to the best-performing DT classifier; grasping cepstral coefficients and their derivatives is critical for interpreting what the model uses.
  - Quick check question: How do first- and second-order deltas in MFCCs encode dynamic spectral changes?

## Architecture Onboarding

- Component map: Raw audio -> silence removal -> frame-based feature computation -> sample-level feature vector -> classifier prediction -> accuracy aggregation
- Critical path: Raw audio → silence removal → frame-based feature computation → sample-level feature vector → classifier prediction → accuracy aggregation
- Design tradeoffs:
  - Frame length 100 ms vs. shorter frames: longer frames improve creak detection but reduce temporal resolution; shorter frames preserve transitions but may miss low-frequency periodicity
  - Mel-filterbank resolution (128 channels): more channels increase dimensionality and risk overfitting; fewer channels may lose creak cues
  - LOSO vs. k-fold: LOSO maximizes speaker independence but increases computational cost; k-fold may leak speaker-specific patterns
- Failure signatures:
  - Accuracy near 50% → model cannot distinguish classes (feature extraction or label noise problem)
  - High training accuracy but low LOSO accuracy → overfitting to speaker-specific traits
  - Consistent drop for one classifier → architecture-specific sensitivity (e.g., MLP to feature scaling)
- First 3 experiments:
  1. Swap frame length from 100 ms to 50 ms and re-run LOSO to test temporal resolution impact
  2. Reduce mel-filterbank channels from 128 to 64 and measure accuracy change to assess redundancy
  3. Replace DT with a shallow Random Forest (depth 3) to compare decision-tree vs. ensemble performance

## Open Questions the Paper Calls Out
- Can machine learning models distinguish social creak from spontaneous conversational speech with accuracy comparable to controlled reading tasks?
- How does the performance of deep learning models compare to traditional machine learning approaches for detecting social creak?
- Can acoustic features reliably differentiate between social creak and pathological dysphonia in speakers with voice disorders?

## Limitations
- The dataset consists exclusively of young female Finnish speakers reading the same text, severely limiting generalizability
- Perceptual ratings from voice specialists introduce subjectivity that may not perfectly align with acoustic reality
- Feature extraction process involves statistical functionals that are described but not fully specified, creating reproducibility challenges

## Confidence
- High confidence: The mel-spectrogram consistently outperforming other features across classifiers is well-supported by the data and analysis
- Medium confidence: The specific accuracy values (71.1% for AdaBoost with mel-spectrogram and Decision Tree with MFCCs) are reported but depend on implementation details
- Low confidence: The broader claim that ML can serve as a "reliable tool" for detecting social creak is overstated given the moderate accuracy and narrow experimental scope

## Next Checks
1. Test the trained models on speech data from different age groups, male speakers, and speakers of other languages to assess true generalizability of creak detection beyond the original Finnish female speaker population
2. Conduct ablation studies to identify which specific mel-filterbank channels or MFCC coefficients contribute most to classification performance
3. Apply the models to spontaneous conversational speech rather than read text to evaluate whether the performance holds in ecologically valid conditions where social creak naturally occurs