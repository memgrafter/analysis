---
ver: rpa2
title: Supervised Fine-tuning in turn Improves Visual Foundation Models
arxiv_id: '2401.10222'
source_url: https://arxiv.org/abs/2401.10222
tags:
- vision
- arxiv
- visft
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores supervised fine-tuning (SFT) as a method to
  improve vision foundation models after their pretraining, drawing inspiration from
  instruction tuning in natural language processing. The proposed two-stage method,
  ViSFT, performs visual joint learning on in-domain tasks like object detection,
  instance segmentation, and image captioning, then evaluates the enhanced vision
  transformer backbone on out-of-domain benchmarks.
---

# Supervised Fine-tuning in turn Improves Visual Foundation Models

## Quick Facts
- arXiv ID: 2401.10222
- Source URL: https://arxiv.org/abs/2401.10222
- Reference count: 40
- Primary result: Visual foundation models can be improved after pretraining using supervised fine-tuning with in-domain tasks

## Executive Summary
This paper introduces ViSFT, a two-stage supervised fine-tuning method that improves vision foundation models by leveraging fine-grained annotations from object detection, instance segmentation, and image captioning tasks. Drawing inspiration from instruction tuning in NLP, the approach uses LoRA to efficiently transfer detailed visual information from in-domain tasks to the vision transformer backbone while preserving task-specific knowledge. Experiments demonstrate consistent improvements across multiple benchmarks, with accuracy gains ranging from 0.1% to 2.5% on tasks including OCR, grounded object identification, image classification, image-text retrieval, and visual question answering.

## Method Summary
ViSFT employs a two-stage training process on a pretrained EVA-CLIP vision transformer. Stage 1 independently trains task-specific heads (detection, segmentation, captioning) on COCO annotations while keeping the backbone frozen. Stage 2 introduces LoRA parameters to the vision transformer backbone, freezing task heads, and jointly fine-tunes to transfer fine-grained information from the tasks to the backbone. The method uses 8 V100 GPUs for less than 2 days of training, achieving improvements across diverse out-of-domain benchmarks while maintaining parameter efficiency through LoRA.

## Key Results
- OCR accuracy improves by +2.5% on average across multiple datasets
- Grounded object identification accuracy increases by +0.3-0.6%
- Image classification accuracy improves by +0.1-0.9%
- Image-text retrieval recall improves by +0.1-1.1%
- VQA accuracy improves by +0.5-1.3% on a 4.4B parameter EVA-CLIP model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ViSFT improves vision foundation models by leveraging LoRA to direct fine-grained knowledge from in-domain tasks into the backbone while preserving task-specific heads.
- Mechanism: The two-stage process first trains task heads independently on COCO annotations, then uses LoRA to update the vision transformer backbone while freezing heads, ensuring fine-grained information flows to backbone parameters without task interference.
- Core assumption: Task heads trained independently will be compatible with the backbone features, allowing knowledge transfer without conflict.
- Evidence anchors:
  - [abstract] "We opt for object-level tasks on COCO... Researchers commonly employ different LoRA weights to retain task-specific knowledge. Similarly, we use LoRA weights to preserve the unleashed information."
  - [section 3.2] "In the second stage, we introduce LoRA parameters to the vision transformer backbone and freeze the task heads, enabling the knowledge to be transferred exclusively to the LoRA parameters."
- Break condition: If task heads are incompatible with backbone features, the two-stage process may fail to transfer meaningful information.

### Mechanism 2
- Claim: ViSFT enhances model generalization by exposing the vision transformer to diverse fine-grained annotations that were absent during pretraining.
- Mechanism: The model learns to recognize and process detailed visual information (bounding boxes, masks, captions) that complements the image-text correspondence learned during CLIP pretraining, leading to improved performance on out-of-domain tasks.
- Core assumption: The diverse annotations in COCO provide fine-grained information that improves the model's ability to understand detailed visual features.
- Evidence anchors:
  - [abstract] "Our findings suggest that the representation and generalization of the vision transformer within a CLIP model can indeed be improved following ViSFT. In essence, ViSFT is able to unleash fine-grained details within the visual transformer that may have been overlooked during image-text pretraining."
  - [section 3.1] "We opted to train our model using the COCO [39] dataset. This dataset provides a diverse range of annotations for each image, including bounding boxes, instance-specific segmentation masks, natural language descriptions, and panoptic segmentation masks."
- Break condition: If the additional fine-grained information conflicts with the pretraining representations, it may degrade rather than improve performance.

### Mechanism 3
- Claim: The two-stage training approach prevents catastrophic forgetting and domain conflicts that occur in traditional multi-task learning.
- Mechanism: By freezing the backbone during first-stage task head training and freezing heads during second-stage LoRA updating, ViSFT isolates the learning processes to avoid conflicts between tasks and preserve pretraining knowledge.
- Core assumption: Separating the learning of task heads from backbone updates reduces interference and improves transfer of fine-grained information.
- Evidence anchors:
  - [section 2.3] "ViSFT departs from traditional multi-task training approaches by obtaining fine-grained information through joint learning of in-domain tasks while evaluating performance on out-of-domain tasks."
  - [section 3.2] "In the second stage, the vision transformer is augmented with LoRA weights, and all task heads are connected for fine-tuning. Aside from the added LoRA weights, other modules will remain frozen."
- Break condition: If the freezing strategy prevents necessary adaptation, the model may not fully benefit from the fine-grained information.

## Foundational Learning

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: LoRA enables efficient fine-tuning of large models by decomposing weight updates into low-rank matrices, reducing computational cost while allowing targeted adaptation of the vision transformer backbone.
  - Quick check question: What is the rank constraint imposed on the update matrices in LoRA, and why is it beneficial for large-scale vision models?

- Concept: Vision Transformer Architecture
  - Why needed here: Understanding ViT architecture is crucial for implementing ViSFT, as the method specifically targets the vision transformer backbone within CLIP models to improve representation learning.
  - Quick check question: How does the vision transformer's attention mechanism contribute to its ability to capture fine-grained visual information?

- Concept: Multi-task Learning Challenges
  - Why needed here: Recognizing the domain conflicts and interference issues in multi-task learning explains why ViSFT's two-stage approach is necessary for effective fine-tuning.
  - Quick check question: What are the primary sources of conflict in traditional multi-task learning, and how does ViSFT address them?

## Architecture Onboarding

- Component map:
  - Pretrained EVA-CLIP vision transformer backbone
  - Task-specific heads: Detr (detection), Mask2former (segmentation), LSTM (captioning)
  - LoRA parameters (B and A matrices) for backbone updates
  - Training pipeline with two distinct stages

- Critical path:
  1. Load pretrained EVA-CLIP model
  2. Stage 1: Train task heads independently while freezing backbone
  3. Stage 2: Train LoRA parameters while freezing task heads and backbone
  4. Evaluate on out-of-domain benchmarks

- Design tradeoffs:
  - LoRA rank selection (r=64 chosen) balances parameter efficiency with adaptation capacity
  - Two-stage training adds complexity but prevents task interference
  - COCO dataset selection provides diverse annotations but limits task variety

- Failure signatures:
  - No improvement on out-of-domain tasks indicates poor information transfer
  - Performance degradation suggests task interference or overfitting
  - Training instability may result from incompatible task heads

- First 3 experiments:
  1. Implement Stage 1 training for detection head only, verify task head compatibility
  2. Add segmentation head to Stage 1, check for training conflicts
  3. Execute full two-stage ViSFT with all three task heads, evaluate on OCR benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ViSFT's improvement generalize beyond vision-language tasks to pure vision tasks without textual components?
- Basis in paper: [explicit] The paper mentions evaluating on "out-of-domain tasks" including OCR, grounded object identification, and image classification, but doesn't explore purely visual tasks without any textual elements
- Why unresolved: The evaluation focuses heavily on vision-language and text-augmented vision tasks. Pure vision tasks like video understanding or 3D object recognition were not explored
- What evidence would resolve it: Systematic evaluation of ViSFT-enhanced models on diverse purely visual benchmarks like video action recognition, 3D object classification, or visual reasoning without textual input

### Open Question 2
- Question: What is the optimal number and diversity of in-domain tasks for maximizing out-of-domain performance?
- Basis in paper: [inferred] The ablation studies show that removing any single task reduces performance, but the paper doesn't explore systematically varying the number of tasks or trying entirely different task combinations
- Why unresolved: The paper uses a fixed set of three tasks (detection, segmentation, captioning) and only tests removing one task at a time, without exploring optimal task combinations or scaling up to more diverse tasks
- What evidence would resolve it: Systematic experiments varying the number and types of in-domain tasks, measuring trade-offs between task diversity and performance on out-of-domain benchmarks

### Open Question 3
- Question: How does ViSFT's performance scale with model size beyond the tested 4.4B parameter model?
- Basis in paper: [explicit] The paper tests only two model sizes (1.0B and 4.4B parameters) and notes that "more significant improvements observed in smaller models," but doesn't explore scaling to larger models
- Why unresolved: The largest tested model is 4.4B parameters, and the paper suggests smaller models benefit more from ViSFT, leaving open questions about performance at larger scales where fine-grained information might be less critical
- What evidence would resolve it: Experiments with increasingly large models (10B, 50B, 100B+ parameters) to determine whether ViSFT's benefits diminish or continue as model capacity increases

### Open Question 4
- Question: Does the two-stage training approach remain optimal when task heads become more complex or specialized?
- Basis in paper: [explicit] The paper justifies the two-stage approach by noting that fine-grained information can be "trapped" in task heads, but doesn't test scenarios where task heads might need more sophisticated coordination
- Why unresolved: The current task heads are relatively simple (Detr, Mask2former, LSTM). More complex or specialized task heads might require different training strategies or could benefit from alternative fine-tuning approaches
- What evidence would resolve it: Experiments replacing the current task heads with more sophisticated or specialized alternatives and comparing performance between two-stage and alternative training strategies

## Limitations

- The compatibility between independently trained task heads and the frozen backbone is assumed but not rigorously tested, creating uncertainty about whether the two-stage approach will work with different task combinations
- The improvements (0.1-2.5% accuracy gains) are relatively modest, suggesting the method provides incremental rather than transformative enhancements to vision foundation models
- The evaluation is limited to specific out-of-domain benchmarks, and it's unclear how well the improvements generalize to other vision tasks or real-world applications

## Confidence

**High Confidence:** The experimental methodology is sound, with clear implementation details for the two-stage training process and comprehensive evaluation on multiple benchmarks. The use of LoRA for efficient fine-tuning is well-established in the literature.

**Medium Confidence:** The improvements across various benchmarks are demonstrated, but the magnitude of gains (0.1-2.5% accuracy improvements) suggests that while the method works, it may not provide dramatic enhancements across all tasks. The effectiveness may be task-dependent.

**Low Confidence:** The mechanism explaining why fine-grained information from COCO annotations specifically improves out-of-domain task performance is not fully validated. The paper assumes compatibility between task heads and backbone features without providing empirical evidence for this assumption.

## Next Checks

1. **Compatibility Testing:** Conduct ablation studies to test the assumption that task heads trained independently are compatible with the frozen backbone. This could involve training task heads with varying degrees of independence from the backbone and measuring the impact on performance transfer.

2. **Cross-Dataset Generalization:** Evaluate ViSFT on additional out-of-domain datasets beyond the current benchmarks, particularly datasets with different visual characteristics (e.g., medical imaging, satellite imagery) to assess the generalizability of the improvements.

3. **LoRA Rank Sensitivity Analysis:** Systematically vary the LoRA rank parameter (r) to determine the optimal balance between parameter efficiency and adaptation capacity. This would help identify whether the chosen rank (r=64) is truly optimal or if different tasks require different rank settings.