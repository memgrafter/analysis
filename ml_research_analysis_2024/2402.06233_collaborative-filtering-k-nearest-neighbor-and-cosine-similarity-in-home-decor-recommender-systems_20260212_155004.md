---
ver: rpa2
title: Collaborative filtering, K-nearest neighbor and cosine similarity in home decor
  recommender systems
arxiv_id: '2402.06233'
source_url: https://arxiv.org/abs/2402.06233
tags:
- products
- system
- users
- user
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a recommender system architecture based on collaborative
  filtering using K-nearest neighbor and cosine similarity, developed for an online
  home decor platform. The system aims to address the sparsity and cold start problems
  common in recommender systems by clustering similar products and recommending items
  based on similar users' preferences.
---

# Collaborative filtering, K-nearest neighbor and cosine similarity in home decor recommender systems

## Quick Facts
- arXiv ID: 2402.06233
- Source URL: https://arxiv.org/abs/2402.06233
- Reference count: 14
- Online A/B testing showed initial sparsity reduction attempts were unsuccessful, highlighting the importance of consistent evaluation metrics and controlled experiments.

## Executive Summary
This paper presents a collaborative filtering-based recommender system architecture for an online home decor platform, utilizing K-nearest neighbor and cosine similarity to address common challenges like sparsity and cold start problems. The system clusters similar products by title similarity and recommends items based on similar users' preferences. Evaluation was conducted across dataset, system, and user perspectives, revealing high sparsity (99.55%) but reasonable coverage (47.7%). Initial attempts to reduce sparsity through product clustering were unsuccessful in A/B testing, emphasizing the need for rigorous evaluation frameworks and controlled experiments.

## Method Summary
The method implements collaborative filtering using K-nearest neighbor and cosine similarity to find similar users based on boolean "Raided" and "Disliked" product ratings. Products with titles more than 85% similar are clustered to reduce sparsity, effectively increasing the density of the user-product interaction matrix. The system recommends products by selecting top-K similar users and retrieving their "Raided" items. Evaluation uses dataset metrics (sparsity, coverage), system metrics (precision, cosine similarity), and user metrics (engagement, returning users) to provide comprehensive assessment across multiple perspectives.

## Key Results
- Sparsity of user-product interaction matrix measured at 99.55%
- Catalog coverage achieved 88.2% while overall coverage was 47.7%
- Online A/B testing revealed initial sparsity reduction adjustments were unsuccessful
- Precision-based evaluation identified accuracy bottlenecks in recommendation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Collaborative filtering with K-nearest neighbor (KNN) improves recommendation accuracy by finding similar users based on shared product interactions.
- Mechanism: The system computes cosine similarity between user vectors representing their "Raided" and "Disliked" product ratings. Users with smaller angular distance (higher cosine similarity) are treated as neighbors, and their preferences are used to recommend products.
- Core assumption: User preferences are predictable from similar users' behaviors, and product interactions (boolean "Raided"/"Disliked") provide enough signal to compute meaningful similarity.
- Evidence anchors:
  - [abstract] "An architectural framework, based on collaborative filtering using K-nearest neighbor and cosine similarity, was developed and implemented..."
  - [section] "To compute the most similar users the machine learning technique K-nearest neighbor can be used... cosine similarity value is selected [8], [9]."
  - [corpus] "Weak. The corpus papers focus on GAT, LLM-driven context, and privacy-preserving CF, not basic KNN-CF with boolean ratings."
- Break condition: When the dataset is too sparse, similarity scores become unreliable, leading to poor neighbor selection and irrelevant recommendations.

### Mechanism 2
- Claim: Clustering similar products by title similarity reduces sparsity and improves neighbor matching.
- Mechanism: Products with titles more than 85% similar are merged into clusters, effectively increasing the density of the user-product interaction matrix and enabling more meaningful similarity calculations.
- Core assumption: Products with similar titles are functionally similar enough that user interactions with one should inform recommendations for the other.
- Evidence anchors:
  - [section] "In the environment it was observed that nearly identical products were treated as different products... A script was therefore developed to cluster products together if their product title was more than 85 percent similar."
  - [corpus] "Missing. No corpus evidence directly supports product title clustering for sparsity reduction in CF."
- Break condition: If clustered products are not truly substitutable (e.g., different sizes or colors), recommendations may mislead users or degrade satisfaction.

### Mechanism 3
- Claim: Evaluation across dataset, system, and user perspectives provides actionable insights for iterative improvement.
- Mechanism: Dataset metrics (sparsity, coverage) expose data limitations; system metrics (precision, cosine similarity graph) measure recommendation quality; user metrics (time spent, swipes, returning users) capture real-world impact.
- Core assumption: Multi-perspective evaluation reveals bottlenecks that single-metric approaches would miss, guiding targeted improvements.
- Evidence anchors:
  - [abstract] "Three perspectives were found relevant for evaluating a recommender system in the specific environment, namely dataset, system and user perspective."
  - [section] "To evaluate the performance from a system perspective, the accuracy is useful to look into... For the specific environment, precision is the best option..."
  - [corpus] "Weak. Corpus focuses on advanced CF models, not evaluation framework design."
- Break condition: If metrics are misaligned with business goals or user satisfaction, improvements based on them may not translate to real value.

## Foundational Learning

- Concept: Cosine similarity and vector space models
  - Why needed here: The system relies on cosine similarity to measure user-user similarity in a high-dimensional product space; understanding how this works is critical for debugging and tuning.
  - Quick check question: If two users have rating vectors A=[1,0,1] and B=[1,1,0], what is their cosine similarity?

- Concept: K-nearest neighbor search
  - Why needed here: KNN is used to find the most similar users for recommendation; knowing how neighbor selection and the choice of K affect results is essential for system design.
  - Quick check question: What happens to recommendation diversity if K is set too low or too high?

- Concept: Data sparsity and its impact on collaborative filtering
  - Why needed here: High sparsity (99.55% in this case) directly limits the effectiveness of similarity calculations; recognizing this guides feature engineering and hybrid approaches.
  - Quick check question: If each user has rated only 0.45% of available products, what is the probability that two random users share a rated item?

## Architecture Onboarding

- Component map:
  - MongoDB database storing user-product interactions (boolean "Raided"/"Disliked")
  - Python scripts for product clustering and KNN similarity computation
  - Recommendation engine selecting top-K similar users and retrieving their "Raided" products
  - Evaluation layer collecting precision, coverage, sparsity, and user engagement metrics
  - A/B testing framework for controlled system improvements

- Critical path:
  1. User swipes product â†’ boolean interaction logged
  2. Similarity engine recomputes cosine similarities to all other users
  3. KNN selects top-K neighbors
  4. Engine fetches neighbor's "Raided" products
  5. System queues 5 recommendations for display

- Design tradeoffs:
  - Boolean ratings vs. explicit ratings: Simpler to collect but less granular signal
  - Clustering by title similarity: Reduces sparsity but may conflate distinct products
  - Precision vs. recall: Focusing on precision may miss relevant but less similar recommendations
  - Real-time vs. batch similarity updates: Real-time ensures freshness but increases latency

- Failure signatures:
  - Low precision (<10%): Likely due to high sparsity or poor neighbor selection
  - High similarity but low precision: Similarity metric may not align with true user preference
  - No recommendations for new users: Cold start problem; insufficient interaction history
  - Sudden drop in returning users: Recommendations may have become irrelevant or repetitive

- First 3 experiments:
  1. Vary K (number of neighbors) from 1 to 20 and measure impact on precision and user engagement.
  2. Adjust product clustering threshold (currently 85% title similarity) to see effect on sparsity and recommendation relevance.
  3. Implement a hybrid content-based + collaborative filtering model and compare against baseline KNN-CF on the same metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are hybrid recommender systems combining content-based and collaborative filtering techniques in solving the sparsity and cold start problems in home decor recommender systems?
- Basis in paper: [explicit] The authors suggest future work on researching a hybrid recommender system combining content-based and collaborative filtering approaches to address the sparsity and cold start problems.
- Why unresolved: The paper only mentions this as a future research direction without providing any empirical evidence or implementation details of such a hybrid system.
- What evidence would resolve it: Implementation and evaluation of a hybrid recommender system that combines content-based and collaborative filtering techniques, comparing its performance against the current system in terms of sparsity and cold start problem resolution.

### Open Question 2
- Question: What specific machine learning techniques beyond K-nearest neighbor could be considered for improving the recommender system's performance as the company grows?
- Basis in paper: [explicit] The authors mention that with the growth of the company, more advanced machine learning techniques should be considered for implementation.
- Why unresolved: The paper does not specify which advanced machine learning techniques could be explored or provide any analysis of their potential benefits.
- What evidence would resolve it: Comparative study of various advanced machine learning techniques (e.g., deep learning, matrix factorization) applied to the home decor recommender system, evaluating their performance against the current K-nearest neighbor approach.

### Open Question 3
- Question: How can the user perspective evaluation be improved to better understand the impact of the recommender system on user satisfaction and engagement?
- Basis in paper: [inferred] The authors discuss the importance of user perspective evaluation but acknowledge limitations due to the system having few users and ongoing improvements in user experience.
- Why unresolved: The paper does not provide concrete methods for enhancing user perspective evaluation or addressing the limitations mentioned.
- What evidence would resolve it: Development and implementation of more sophisticated user feedback mechanisms (e.g., surveys, A/B testing with larger user samples) to gather detailed insights into user satisfaction and engagement with the recommender system.

## Limitations

- Evaluation framework lacks specific parameter details critical for reproducibility, including exact K value for KNN and clustering threshold parameters
- Product clustering effectiveness depends heavily on implementation details not fully specified in the paper
- User engagement metrics interpretation and their relationship to actual user satisfaction are not rigorously validated

## Confidence

- **High Confidence**: The basic collaborative filtering architecture using KNN and cosine similarity is well-established and correctly implemented as described.
- **Medium Confidence**: The product clustering approach to reduce sparsity is conceptually sound, but its effectiveness depends heavily on implementation details not fully specified.
- **Low Confidence**: The evaluation metrics' interpretation and their relationship to actual user satisfaction are not rigorously validated, particularly regarding the user engagement metrics.

## Next Checks

1. Implement the KNN-CF system with varying K values (1-20) and measure the impact on precision and coverage metrics to identify optimal neighbor count.
2. Conduct a sensitivity analysis on the product clustering threshold (70-95% title similarity) to quantify the tradeoff between sparsity reduction and recommendation relevance.
3. Design and execute a controlled experiment comparing the current KNN-CF approach against a hybrid content-based + collaborative filtering model using identical evaluation metrics and user engagement tracking.