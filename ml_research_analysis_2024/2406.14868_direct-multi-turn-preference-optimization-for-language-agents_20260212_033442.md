---
ver: rpa2
title: Direct Multi-Turn Preference Optimization for Language Agents
arxiv_id: '2406.14868'
source_url: https://arxiv.org/abs/2406.14868
tags:
- loss
- dmpo
- function
- agent
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of adapting large language models
  (LLMs) for multi-turn agent tasks, where existing direct preference optimization
  (DPO) methods fail due to partition function cancellation issues. The authors propose
  a new loss function called DMPO that substitutes the policy constraint with a state-action
  occupancy measure (SAOM) constraint and introduces length normalization into the
  Bradley-Terry model.
---

# Direct Multi-Turn Preference Optimization for Language Agents

## Quick Facts
- arXiv ID: 2406.14868
- Source URL: https://arxiv.org/abs/2406.14868
- Authors: Wentao Shi; Mengqi Yuan; Junkang Wu; Qifan Wang; Fuli Feng
- Reference count: 11
- Primary result: DMPO improves multi-turn agent performance by 1.6–6.4% over baselines by solving partition function cancellation issues in DPO

## Executive Summary
This paper addresses the fundamental limitation of Direct Preference Optimization (DPO) when applied to multi-turn agent tasks. Traditional DPO fails in sequential decision-making scenarios due to partition function cancellation issues that prevent proper reward optimization. The authors propose Direct Multi-Turn Preference Optimization (DMPO), which replaces the policy constraint with a state-action occupancy measure constraint and introduces length normalization to the Bradley-Terry model. This enables direct optimization of reinforcement learning objectives while mitigating compounding errors. Experiments on three multi-turn agent datasets (WebShop, ScienceWorld, ALFWorld) demonstrate that DMPO consistently outperforms both standard DPO and other baselines, with improvements of 1.6–6.4% in reward across seen and unseen test sets.

## Method Summary
DMPO modifies the standard DPO framework by replacing the policy constraint with a state-action occupancy measure (SAOM) constraint and introducing length normalization into the Bradley-Terry model. The SAOM constraint optimizes the distribution of state-action pairs rather than policies, which helps mitigate compounding errors in multi-turn scenarios. Length normalization ensures the partition function becomes length-independent and can be canceled during optimization. The method uses a discount function to give higher weight to early state-action pairs, reducing the impact of noisy later actions. The approach is evaluated by fine-tuning Llama-2-7B-Chat and Mistral-7B-Instruct models on preference data from three multi-turn agent task datasets.

## Key Results
- DMPO outperforms DPO by 1.6–6.4% in reward across seen and unseen test sets
- DMPO shows superior robustness to noisy data compared to DPO, especially in clean settings
- Ablation studies confirm the importance of both SAOM constraint and length normalization components
- The method maintains performance improvements across varying trajectory lengths and discount factors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DMPO replaces the policy constraint with the state-action occupancy measure (SAOM) constraint to mitigate compounding errors.
- Mechanism: SAOM directly optimizes the distribution of state-action pairs, steering the model toward actions that lead back to expert trajectories rather than uniformly sampling actions in unseen states.
- Core assumption: The transition function remains consistent between the current policy and reference policy, allowing partition function cancellation.
- Evidence anchors:
  - [abstract] "we replace the policy constraint with the state-action occupancy measure (SAOM) constraint in the RL objective"
  - [section 4.1] "SAOM constraint has advantages in mitigating compounding errors compared to the policy constraint"
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism
- Break condition: If the transition function differs between policies, the SAOM constraint loses its advantage and compounding errors may persist.

### Mechanism 2
- Claim: Length normalization in the Bradley-Terry model cancels the partition function in multi-turn scenarios.
- Mechanism: By normalizing trajectory rewards by their respective lengths, the partition function becomes length-independent and can be eliminated from the optimization objective.
- Core assumption: The partition function Z(s) is independent of the current state when using SAOM constraint.
- Evidence anchors:
  - [abstract] "add length normalization to the Bradley-Terry model"
  - [section 4.2] "We introduce the length normalization technique to Eq (12)" and "it assists in eliminating the partition function"
  - [section 4.3] "Our derivation shows that it assists in eliminating the partition function"
- Break condition: If trajectory lengths vary dramatically between preferred and dis-preferred pairs, length normalization may introduce bias rather than cancel the partition function.

### Mechanism 3
- Claim: The discount function ϕ(t, T) in DMPO gives higher weight to early state-action pairs, reducing the influence of noisy later actions.
- Mechanism: Early actions have higher weights due to the discount function, prioritizing high-quality expert actions and reducing the impact of noisy actions in later steps.
- Core assumption: Early state-action pairs are more critical for successful task completion than later ones.
- Evidence anchors:
  - [section 4.3] "The discount function ϕ(t, T) decreases as t increases" and "DMPO can balance the impact of noise by adjusting the parameter γ"
  - [section 5.4] "A smaller γ implies that the DMPO loss assigns reduced weight to the state-action pairs in later steps"
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism
- Break condition: If later state-action pairs become more important for task success, reducing their weight could harm performance.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation
  - Why needed here: The entire DMPO framework is built on MDP formalism where states, actions, rewards, and transitions are formally defined
  - Quick check question: In the MDP formulation, what does the tuple (S, A, T, R, γ) represent?

- Concept: State-Action Occupancy Measure (SAOM)
  - Why needed here: SAOM is the key mathematical construct that replaces the policy constraint and enables partition function cancellation
  - Quick check question: How does SAOM differ from a policy distribution, and why is this difference crucial for DMPO?

- Concept: Bradley-Terry model for preference learning
  - Why needed here: The BT model provides the probabilistic framework for comparing preferred vs dis-preferred trajectories
  - Quick check question: What role does the partition function play in the original BT model, and why does it cause problems in multi-turn settings?

## Architecture Onboarding

- Component map: Base LLM -> Reference model -> Preference dataset -> DMPO loss function -> Evaluation environment
- Critical path: Expert trajectories → Reference model → Preference data generation → DMPO fine-tuning → Evaluation
- Design tradeoffs:
  - SAOM vs policy constraint: SAOM reduces compounding errors but requires consistent transition functions
  - Length normalization: Essential for partition function cancellation but may introduce length-based bias
  - Discount factor γ: Balances early vs late action importance; smaller γ reduces noise sensitivity but may miss late-stage strategies
- Failure signatures:
  - Poor performance on unseen tasks: Likely insufficient generalization from the preference data
  - Worsening performance with longer trajectories: Length normalization may not be handling extreme length disparities
  - Instability during training: Discount factor γ or β hyperparameters may need adjustment
- First 3 experiments:
  1. Ablation study: Compare DMPO vs standard DPO on a simple multi-turn task to verify partition function cancellation
  2. Noise sensitivity test: Evaluate DMPO performance with varying levels of noisy "lose" trajectories to confirm noise robustness
  3. Length disparity analysis: Test DMPO on trajectory pairs with controlled length differences to validate length normalization effectiveness

## Open Questions the Paper Calls Out
- How does the performance of DMPO scale with larger language models (e.g., 70B+ parameters) compared to the 7B models tested in this paper?
- How robust is DMPO to varying levels of noise in the "lose" trajectories beyond the binary clean/noisy settings tested?
- How does DMPO perform in multi-turn agent tasks with sparse rewards or delayed rewards compared to dense reward settings?

## Limitations
- The theoretical mechanism for partition function cancellation through length normalization lacks empirical validation across diverse trajectory length distributions
- The assumption that SAOM constraints work effectively due to consistent transition functions between policies is stated but not experimentally verified
- The noise robustness claims depend heavily on the specific noise models used in the experiments

## Confidence
- **High Confidence**: The empirical results showing DMPO outperforming DPO and other baselines on the three multi-turn agent task datasets (WebShop, ScienceWorld, ALFWorld) are well-supported with clear metrics and ablation studies.
- **Medium Confidence**: The theoretical explanation for why length normalization cancels the partition function is logically consistent but relies on specific assumptions about trajectory distributions that weren't exhaustively tested.
- **Low Confidence**: The claim that SAOM constraints inherently mitigate compounding errors better than policy constraints lacks direct empirical comparison and depends on unstated assumptions about transition function consistency.

## Next Checks
1. Design experiments where the transition function deliberately varies between the current and reference policies to quantify how this affects DMPO's performance advantage over standard DPO approaches.
2. Create preference pairs with deliberately extreme length disparities (e.g., 1-step vs 20-step trajectories) to test whether length normalization maintains its partition function cancellation benefits or introduces bias.
3. Evaluate DMPO on multi-turn agent tasks from completely different domains than WebShop, ScienceWorld, and ALFWorld to verify the method's general applicability beyond the tested environments.