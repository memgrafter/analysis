---
ver: rpa2
title: Ontology Enhanced Claim Detection
arxiv_id: '2402.12282'
source_url: https://arxiv.org/abs/2402.12282
tags:
- ontology
- bert
- claim
- embeddings
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an ontology-enhanced approach for claim detection
  that combines BERT sentence embeddings with ontology embeddings from knowledge bases
  to improve claim identification performance on small, unbalanced datasets. The method
  fuses textual information from the BERT model with structured knowledge from ontology
  embeddings, particularly from the ClaimsKG knowledge graph, to enhance the model's
  ability to capture domain-specific information.
---

# Ontology Enhanced Claim Detection

## Quick Facts
- arXiv ID: 2402.12282
- Source URL: https://arxiv.org/abs/2402.12282
- Reference count: 20
- Primary result: Ontology-enhanced BERT improves claim detection on small, unbalanced datasets, achieving 0.74 macro-F1 on ClaimBuster

## Executive Summary
This paper proposes an ontology-enhanced approach for claim detection that combines BERT sentence embeddings with ontology embeddings from knowledge bases. The method addresses the challenge of small, unbalanced datasets by incorporating domain knowledge through ontology embeddings, which helps avoid the bias toward larger classes that affects pure BERT models. Experiments on ClaimBuster and NewsClaims datasets demonstrate that the ontology-enhanced BERT model outperforms both pure BERT and statistical machine learning models, achieving macro-average F1-scores of 0.74 on ClaimBuster and 0.62 on NewsClaims.

## Method Summary
The approach fuses ontology embeddings from the ClaimsKG knowledge graph with BERT sentence embeddings for claim detection. The ontology is created from ClaimsKG data using OWL format, then embedded using OWL2Vec* to capture both graph structure and lexical information. These ontology vectors are concatenated with BERT's CLS token embeddings and fed into a classification head. The model is fine-tuned on small, unbalanced claim detection datasets (ClaimBuster and NewsClaims) and compared against baseline BERT, statistical ML models, and domain-specific Word2Vec embeddings.

## Key Results
- Ontology-enhanced BERT achieves 0.74 macro-F1 and 0.92 accuracy on ClaimBuster (vs 0.55 F1 for pure BERT)
- On NewsClaims, ontology-enhanced model reaches 0.62 macro-F1 and 0.89 accuracy
- Domain-specific Word2Vec embeddings outperform generic pre-trained embeddings despite small corpus size
- Ontology embeddings reduce class bias toward majority classes in unbalanced datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fusing ontology embeddings with BERT embeddings reduces bias toward majority classes in small, unbalanced datasets.
- Mechanism: The ontology embeddings inject domain-specific structural knowledge that helps the model recognize features distinguishing minority classes from majority ones, counteracting BERT's tendency to favor larger classes due to dataset imbalance.
- Core assumption: The ontology contains discriminative features for minority claim classes that are not well captured by BERT alone.
- Evidence anchors: [abstract] mentions avoiding bias encountered in neural network models; [section] shows 0.19 point improvement in macro-F1 when adding ontology vectors.

### Mechanism 2
- Claim: Domain-specific word embeddings outperform generic pre-trained embeddings for claim detection.
- Mechanism: Training Word2Vec on the specific claim detection corpus captures domain-specific semantics and terminology, providing more relevant vector representations than generic models like pre-trained Word2Vec or BERT.
- Core assumption: The claim detection task relies on terminology and semantic relations unique to the domain that are not well represented in generic embeddings.
- Evidence anchors: [section] states domain-specific features are useful for context-based tasks like claim identification.

### Mechanism 3
- Claim: Combining textual embeddings (BERT) with structured ontology embeddings captures complementary information.
- Mechanism: Textual embeddings capture semantic context and linguistic nuance of claims, while ontology embeddings encode structured factual knowledge and relationships between entities, enabling the model to reason about claim validity more effectively.
- Core assumption: Claim detection benefits from both contextual understanding (text) and structured factual knowledge (ontology), and these modalities are complementary rather than redundant.
- Evidence anchors: [abstract] and [section] emphasize the need to combine both modalities as they enrich each other.

## Foundational Learning

- Concept: Knowledge graph embeddings (e.g., TransE)
  - Why needed here: Used to encode metadata and relationships from the ClaimBuster dataset into vector form for use as features in statistical ML models.
  - Quick check question: What does TransE treat each relation as during embedding?

- Concept: OWL ontology structure and embedding (OWL2Vec*)
  - Why needed here: The ClaimsKG data is converted into an OWL ontology, then embedded using OWL2Vec* to capture both graph structure and lexical information for enhancing BERT.
  - Quick check question: What two parts make up an OWL ontology according to the paper?

- Concept: BERT fine-tuning and CLS token usage
  - Why needed here: BERT is fine-tuned on the claim detection task, and the CLS token's final hidden state is used as the sequence representation for classification.
  - Quick check question: What does the CLS token's last hidden state represent in BERT?

## Architecture Onboarding

- Component map: Data preprocessing → Text tokenization and metadata extraction → Knowledge graph construction (TransE) → Vectorized metadata features → OWL ontology creation from ClaimsKG → Graph + lexical structure → OWL2Vec* embedding → Low-dimensional ontology vectors → BERT model → Contextual sentence embeddings (CLS token) → Fusion layer → Concatenation of BERT and ontology embeddings → Classifier head → Linear + dropout + BCEWithLogitsLoss for prediction
- Critical path: Data → OWL2Vec* embedding → BERT fine-tuning → Fusion → Classification
- Design tradeoffs: Small dataset size favors ontology enhancement to combat BERT bias, but large datasets may not need it; OWL2Vec* adds complexity and dependency on ClaimsKG coverage; fusion adds parameters and training time.
- Failure signatures: Poor performance on minority classes despite ontology enhancement (ontology lacks relevant features); degradation when using larger datasets (ontology adds noise); training instability (incorrect fusion or incompatible embedding scales).
- First 3 experiments:
  1. Replicate baseline BERT fine-tuning on ClaimBuster groundtruth; verify macro-F1 ~0.55.
  2. Train and evaluate OWL2Vec* embeddings on ClaimsKG; confirm they capture claim-related entities.
  3. Fuse BERT CLS + ontology embeddings; compare macro-F1 to baseline; expect improvement to ~0.74.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do ontology embeddings specifically improve the identification of UFS class claims compared to pure BERT models?
- Basis in paper: [explicit] The paper notes that ontology enhanced BERT correctly predicted 4 UFS samples misclassified by BERT, and mentions attention weights show ontology model prioritizes entities over first-person pronouns
- Why unresolved: While the paper demonstrates improved performance, it doesn't explain the specific mechanisms by which ontology embeddings help identify this class that pure BERT misses
- What evidence would resolve it: Detailed analysis of attention weight patterns and ontology features that specifically contribute to UFS classification accuracy

### Open Question 2
- Question: Would ontology-enhanced models show similar improvements on datasets in languages other than English, where training data is typically scarce?
- Basis in paper: [explicit] The paper discusses data scarcity for non-English languages and suggests ontology features could compensate for small datasets, but only tested on English datasets
- Why unresolved: The proposed method hasn't been validated on non-English datasets to confirm if ontology features provide similar benefits when training data is limited
- What evidence would resolve it: Experiments applying the ontology-enhanced model to claim detection datasets in other languages

### Open Question 3
- Question: What specific types of knowledge graph metadata contribute most to improved claim detection performance?
- Basis in paper: [inferred] The paper uses TransE to embed metadata like speaker, speaker title, speaker party, word count, and sentiment, but doesn't analyze which features are most valuable
- Why unresolved: The paper doesn't perform ablation studies to determine which knowledge graph features drive performance improvements
- What evidence would resolve it: Systematic experiments removing individual metadata features to measure impact on model performance

## Limitations

- The effectiveness depends heavily on the quality and coverage of the ClaimsKG knowledge graph; if the ontology lacks comprehensive coverage of claim classes, the enhancements may fail.
- Critical implementation details are missing, including exact fusion layer configuration, dropout rates, and embedding scale normalization between BERT and ontology vectors.
- The paper lacks ablation studies to isolate the contribution of ontology embeddings versus other factors like domain-specific Word2Vec training.

## Confidence

- **High confidence**: The core hypothesis that ontology embeddings can reduce class bias in small, unbalanced datasets is supported by empirical evidence, though the mechanism's generality across different domains requires further validation.
- **Medium confidence**: The claim that domain-specific Word2Vec embeddings outperform generic embeddings is supported by experimental results, but the small corpus size raises concerns about overfitting and generalizability.
- **Medium confidence**: The assertion that combining textual and ontology features captures complementary information is reasonable, but the paper lacks detailed analysis of feature importance and ablation studies to confirm this.

## Next Checks

1. **Ablation study**: Remove ontology embeddings and retrain the model to quantify the exact performance contribution of the knowledge base fusion versus other factors like domain-specific Word2Vec training.
2. **Ontology coverage analysis**: Evaluate the overlap between ClaimsKG entities and the claim classes in the datasets to assess whether the ontology contains sufficient discriminative features for minority classes.
3. **Scale sensitivity test**: Experiment with different normalization or scaling techniques for the BERT and ontology embeddings to identify the optimal fusion configuration and ensure training stability.