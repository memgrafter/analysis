---
ver: rpa2
title: 'GARNN: An Interpretable Graph Attentive Recurrent Neural Network for Predicting
  Blood Glucose Levels via Multivariate Time Series'
arxiv_id: '2402.16230'
source_url: https://arxiv.org/abs/2402.16230
tags:
- variable
- importance
- methods
- glucose
- gatv2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of blood glucose level prediction
  for diabetes management using multivariate time series data. The authors propose
  Graph Attentive Recurrent Neural Networks (GARNNs) to model the data and provide
  interpretability by learning temporal variable importance.
---

# GARNN: An Interpretable Graph Attentive Recurrent Neural Network for Predicting Blood Glucose Levels via Multivariate Time Series

## Quick Facts
- arXiv ID: 2402.16230
- Source URL: https://arxiv.org/abs/2402.16230
- Reference count: 40
- Primary result: GARNNs outperform twelve baseline methods in both prediction accuracy and interpretability for blood glucose level prediction across four diverse clinical datasets

## Executive Summary
This paper addresses blood glucose level prediction for diabetes management using multivariate time series data. The authors propose Graph Attentive Recurrent Neural Networks (GARNNs) that combine graph attention mechanisms with recurrent neural networks to model correlations among variables and provide interpretable temporal variable importance. GARNNs achieve state-of-the-art prediction accuracy while offering insightful feature maps that highlight important variables like glucose levels, meal intake, and insulin injections, making them particularly valuable for clinical decision-making.

## Method Summary
GARNNs use graph attention networks (GAT or GATv2) to model variable correlations at each timestep, treating the multivariate time series as a complete graph where nodes represent variables. The attention weights are processed to extract temporal variable importance measures, then a GRU layer aggregates temporal features for prediction. The model is evaluated on four datasets representing different clinical scenarios: OhioT1DM, ShanghaiT1DM, ShanghaiT2DM, and ArisesT1DM, with comprehensive comparisons against twelve baseline methods.

## Key Results
- GARNNs achieve the best prediction accuracy across all four datasets with superior performance on RMSE, MAPE, MAE, and gRMSE metrics
- The model correctly identifies glucose level as the most important variable and highlights postprandial factors like meal intake and bolus insulin
- Feature maps provide high-quality temporal interpretability, showing dataset-specific insights such as timestamp importance in Shanghai datasets
- GARNNs outperform all baseline methods in both prediction accuracy and interpretability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph attention mechanisms explicitly model correlations among variables in multivariate time series, improving prediction accuracy.
- Mechanism: Each timestep is treated as a complete graph where nodes represent variables. GAT/GATv2 aggregates neural messages between variables based on learned attention weights, allowing the model to capture complex interactions before temporal aggregation.
- Core assumption: The relationships between variables are dynamic and context-dependent, varying across timesteps.
- Evidence anchors:
  - [abstract]: "GARNNs use graph attention mechanisms to model correlations among variables"
  - [section 4.2]: "Our proposed models, GARNNs, build a graph at each timestep and use each node n of the graph to represent a variable n of MTS"
  - [corpus]: Weak - neighbors focus on trajectory prediction, gene-microbe-disease association, and spiking networks rather than multivariate time series attention mechanisms
- Break condition: If variable relationships are static or if attention weights fail to capture meaningful interactions, the graph attention component becomes redundant.

### Mechanism 2
- Claim: The proposed temporal variable importance measure provides inherent interpretability by summarizing attention-based variable contributions.
- Mechanism: Variable importance is calculated by averaging attention scores across all variables at each timestep, then removing irrelevant components (like query terms) to isolate variable contribution. This creates a feature map showing which variables drive predictions at each timestep.
- Core assumption: Attention scores can meaningfully represent variable importance when properly normalized and filtered.
- Evidence anchors:
  - [abstract]: "GARNNs offer insightful feature maps for the prediction, supported by theoretical proofs"
  - [section 4.3]: Three theorems prove the existence, characteristics, and bounded differences of the temporal variable importance measure
  - [section 5.6]: Feature maps successfully highlight important variables like "glucose_level" and sparse signals like "bolus" and "meal"
- Break condition: If attention mechanisms become dominated by noise or if the filtering process removes too much information, the interpretability measure loses meaning.

### Mechanism 3
- Claim: Combining graph attention with RNNs enables both accurate prediction and meaningful variable ranking across diverse clinical scenarios.
- Mechanism: GAT/GATv2 first models variable correlations to produce embeddings, then GRU aggregates these embeddings temporally to predict future glucose levels. The learned attention weights provide variable rankings that align with medical knowledge.
- Core assumption: Variable importance rankings should reflect domain knowledge about diabetes management.
- Evidence anchors:
  - [section 5.5]: GARNNs correctly rank "glucose_level" as most important, highlight "bolus" and "meal" over "basal" insulin, and show dataset-specific insights (e.g., "timestamp" importance in Shanghai datasets)
  - [section 5.4]: GARNNs outperform all twelve baselines in prediction accuracy across four datasets
  - [corpus]: Weak - corpus neighbors don't directly address diabetes glucose prediction or variable ranking in clinical contexts
- Break condition: If the learned variable rankings contradict medical knowledge consistently across datasets, the model's interpretability claims weaken.

## Foundational Learning

- Graph Attention Networks (GAT)
  - Why needed here: GAT provides the mechanism for modeling variable correlations at each timestep, which is crucial for understanding how different physiological and behavioral factors interact to influence blood glucose levels
  - Quick check question: How does GAT differ from traditional graph convolutional networks in handling variable relationships?

- Attention Mechanisms and Interpretability
  - Why needed here: The paper builds interpretability directly into the model through attention weights rather than using post-hoc methods, making the variable importance scores inherently meaningful
  - Quick check question: What's the difference between static and dynamic scoring in attention mechanisms, and why does the paper prefer static scoring for variable importance?

- Recurrent Neural Networks for Temporal Aggregation
  - Why needed here: After modeling variable relationships at each timestep, GRU aggregates these representations over time to make predictions about future glucose levels
  - Quick check question: Why might GRU be preferred over LSTM for this application, considering the need for both interpretability and performance?

## Architecture Onboarding

- Component map:
  Input -> GAT/GATv2 layer -> Variable importance extraction -> GRU layer -> MLP output

- Critical path:
  1. Variable embeddings through GAT/GATv2
  2. Variable importance calculation from attention weights
  3. Temporal aggregation via GRU
  4. Prediction through MLP

- Design tradeoffs:
  - GAT vs GATv2: GATv2 uses dynamic scoring which offers better expressiveness but GAT provides slightly better interpretability
  - Number of graph layers: More layers capture more complex interactions but increase computational cost and may reduce interpretability
  - Attention score vs importance: Direct attention scores include noise; filtered importance scores are cleaner but lose some information

- Failure signatures:
  - Attention weights becoming uniform across variables indicates poor learning of variable relationships
  - Variable importance rankings contradicting medical knowledge suggests attention mechanisms are not capturing meaningful patterns
  - Poor prediction performance despite high interpretability suggests the model is too focused on explainability at the expense of accuracy

- First 3 experiments:
  1. Compare GAT+GRU vs GATv2+GRU on prediction accuracy and variable ranking consistency to validate the dynamic scoring claim
  2. Test different numbers of graph layers (L=1 vs L=2) to understand the tradeoff between complexity and interpretability
  3. Evaluate on datasets with varying data quality (e.g., sparse vs dense signals) to test robustness claims about handling missing data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GARNNs change when incorporating additional clinical variables not present in the current datasets, such as patient demographics or genetic markers?
- Basis in paper: [inferred] The paper discusses the importance of variable selection and ranking, but does not explore the impact of incorporating additional clinical variables.
- Why unresolved: The paper focuses on the performance of GARNNs with the available variables in the four datasets, but does not investigate the potential benefits of incorporating additional clinical variables.
- What evidence would resolve it: Conducting experiments with GARNNs using datasets that include additional clinical variables, such as patient demographics or genetic markers, and comparing the performance to the current results.

### Open Question 2
- Question: How does the interpretability of GARNNs compare to other state-of-the-art interpretable models for time series forecasting, such as attention-based models or post-hoc analysis methods?
- Basis in paper: [inferred] The paper demonstrates the interpretability of GARNNs through variable importance ranking and feature maps, but does not compare its interpretability to other models.
- Why unresolved: The paper focuses on the performance and interpretability of GARNNs, but does not provide a comprehensive comparison to other interpretable models for time series forecasting.
- What evidence would resolve it: Conducting a comparative study of GARNNs and other interpretable models for time series forecasting, evaluating their interpretability using metrics such as human readability, faithfulness, and stability.

### Open Question 3
- Question: How does the performance of GARNNs vary with different prediction horizons and historical time series lengths?
- Basis in paper: [inferred] The paper uses fixed prediction horizons and historical time series lengths for the experiments, but does not explore the impact of varying these parameters.
- Why unresolved: The paper focuses on the performance of GARNNs with specific prediction horizons and historical time series lengths, but does not investigate how the performance changes with different values of these parameters.
- What evidence would resolve it: Conducting experiments with GARNNs using different prediction horizons and historical time series lengths, and analyzing the impact on performance metrics such as RMSE, MAPE, and gRMSE.

## Limitations

- The interpretability claims rely heavily on the assumption that attention weights can meaningfully represent variable importance when properly filtered, though theoretical proofs are provided
- Evaluation is limited to four specific diabetes datasets, which may restrict generalizability to other clinical domains or patient populations
- The comparison with twelve baseline methods doesn't fully explore the tradeoff between interpretability and prediction accuracy that might exist in alternative architectures

## Confidence

- High confidence: Prediction accuracy claims (supported by quantitative results across multiple datasets and metrics)
- Medium confidence: Interpretability claims (theoretical foundations are solid but practical validation depends on attention mechanism quality)
- Medium confidence: Clinical relevance of variable rankings (aligns with medical knowledge but not independently validated by domain experts)

## Next Checks

1. Test whether attention weights remain meaningful when adding controlled noise to input variables, ensuring they capture genuine variable relationships rather than artifacts.

2. Evaluate GARNNs on additional clinical datasets from different medical domains to test whether the interpretability framework transfers beyond diabetes management.

3. Compare raw attention scores with the filtered variable importance measure to quantify information loss and validate that the filtering process improves interpretability without sacrificing critical information.