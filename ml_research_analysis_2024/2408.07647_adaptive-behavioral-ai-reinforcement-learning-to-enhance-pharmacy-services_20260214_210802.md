---
ver: rpa2
title: 'Adaptive Behavioral AI: Reinforcement Learning to Enhance Pharmacy Services'
arxiv_id: '2408.07647'
source_url: https://arxiv.org/abs/2408.07647
tags:
- adaptive
- learning
- proceedings
- https
- health
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an adaptive behavioral AI framework using
  reinforcement learning to deliver personalized nudges through mobile health applications
  for pharmacists. The system was tested on SwipeRx, an all-in-one app for Southeast
  Asian pharmacists, using item pair recommendations to increase basket size.
---

# Adaptive Behavioral AI: Reinforcement Learning to Enhance Pharmacy Services

## Quick Facts
- arXiv ID: 2408.07647
- Source URL: https://arxiv.org/abs/2408.07647
- Reference count: 40
- Primary result: RL-based behavioral nudging increased pharmacy expenditure with 18.2-22.9% of recommendations resulting in later purchases

## Executive Summary
This paper introduces an adaptive behavioral AI framework using reinforcement learning to deliver personalized nudges through mobile health applications for pharmacists. The system was tested on SwipeRx, an all-in-one app for Southeast Asian pharmacists, using item pair recommendations to increase basket size. Two experiments with Indonesian users showed a small positive impact on pharmacy expenditure, with 18.2-22.9% of recommendations resulting in later purchases. The approach demonstrates the potential of RL-based behavioral nudging to support pharmacy services and improve healthcare delivery in low- and middle-income countries.

## Method Summary
The method employs a contextual bandit framework using Thompson sampling to decide whether to send behavioral nudges to pharmacists. A rule-based algorithm selects item pairs from frequently purchased products, focusing on pairs with one frequently bought and one infrequently bought item. The bandit uses user context including region, purchase history, and engagement metrics to make treatment decisions. The system was implemented in the SwipeRx app and tested through two experiments with Indonesian pharmacists over 8-10 weeks.

## Key Results
- Two experiments with Indonesian users showed a small positive impact on pharmacy expenditure
- 18.2-22.9% of recommendations resulted in later purchases of the recommended item pairs
- The adaptive intervention demonstrated statistically significant improvements compared to pure control groups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reinforcement learning personalizes item pair recommendations, increasing basket size and pharmacy expenditure.
- Mechanism: Contextual bandits select between treatment (sending nudge) and control (not sending nudge) each week based on user context (region, purchasing history, engagement metrics). Successful recommendations (18.2-22.9%) lead to future purchases of infrequently bought items.
- Core assumption: Users respond to behavioral nudges by purchasing recommended items, even if not immediately.
- Evidence anchors:
  - [abstract] "Two experiments with Indonesian users showed a small positive impact on pharmacy expenditure, with 18.2-22.9% of recommendations resulting in later purchases."
  - [section 2.4.3] "Successful recommendations are those that result in purchases at a later time during the experiment of the pair's item ordered infrequently by the user."
  - [corpus] Weak evidence; related papers focus on general RL applications rather than pharmacy-specific nudging.
- Break condition: If nudge fatigue occurs (decreased response over time) or if recommendations fail to match user needs, the mechanism breaks.

### Mechanism 2
- Claim: Combining rule-based item pair selection with contextual bandit decision-making optimizes recommendation relevance.
- Mechanism: Rule-based algorithm selects top 100 item pairs by revenue that are in stock, then chooses pairs with one frequently bought and one infrequently bought item. Bandit uses user context (region, purchase frequency, days since last nudge) to decide whether to send the recommendation.
- Core assumption: Combining revenue-based pair selection with user-specific context improves recommendation relevance and purchase likelihood.
- Evidence anchors:
  - [section 2.3] "We use a rule-based pair recommendation algorithm that considers products typically purchased together, ranks the pairs by revenue, and retains the first 100 pairs, and of them, only those pairs that are currently in stock, creating a list of candidates."
  - [section 2.5.1] "Context used consisted in the region (one-hot encoded), normalized days since the last nudge, days with purchase orders and expenditure in the last 90 days."
  - [corpus] Assumption: Related work on RL for healthcare decision support supports the general approach but doesn't validate pharmacy-specific implementation.
- Break condition: If the rule-based selection becomes outdated or if contextual features fail to capture user preferences, recommendation quality degrades.

### Mechanism 3
- Claim: Adaptive experimentation balances exploration and exploitation to maximize long-term impact on pharmacy expenditure.
- Mechanism: Thompson sampling in Gauss-Gamma linear bandit updates treatment assignment probabilities based on observed rewards (pharmacy expenditure). Pure control groups provide RCT baseline for impact measurement.
- Core assumption: Sequential decision-making with exploration-exploitation tradeoff improves over time compared to static assignment.
- Evidence anchors:
  - [section 2.2] "The algorithm of choice for this intervention was a Gauss-Gamma linear bandit using Thompson sampling."
  - [section 2.4.1] "We compare participants' average expenditure... in the adaptive intervention with those in pure control by performing t-tests..."
  - [corpus] Weak evidence; related papers discuss general RL bandit algorithms but not specifically for pharmacy expenditure optimization.
- Break condition: If the reward signal is noisy or delayed, or if the bandit fails to explore adequately, the mechanism breaks.

## Foundational Learning

- Concept: Reinforcement learning and contextual bandits
  - Why needed here: To make sequential decisions about when to send behavioral nudges based on user context and observed outcomes.
  - Quick check question: What is the key difference between a contextual bandit and a full reinforcement learning problem?

- Concept: Experimental design and statistical power
  - Why needed here: To properly measure the impact of adaptive interventions and ensure sufficient power to detect small effects.
  - Quick check question: Why does the paper use both adaptive intervention and pure control groups?

- Concept: Behavioral nudging and user engagement
  - Why needed here: To understand how personalized recommendations can influence pharmacist purchasing behavior and ultimately improve healthcare delivery.
  - Quick check question: What is the potential downside of sending too many nudges to users?

## Architecture Onboarding

- Component map: SDK embedded in mobile app (SwipeRx) -> Backend server for log ingestion, data organization, and labeling -> Model management and algorithmic decision-making engine -> Frontend interfaces for analytics, model management, and intervention configuration

- Critical path:
  1. User context collected and sent to backend
  2. Bandit algorithm decides whether to send nudge
  3. If treatment, in-app message scheduled and sent
  4. User response and subsequent purchases logged
  5. Rewards calculated and used to update bandit model

- Design tradeoffs:
  - Linear bandit vs. non-linear models: Simplicity and interpretability vs. potential for better personalization
  - Rule-based vs. learned recommendation selection: Guaranteed revenue-based selection vs. potential for more dynamic adaptation
  - Thompson sampling vs. other exploration strategies: Efficient Bayesian updating vs. potential for better handling of delayed rewards

- Failure signatures:
  - Bandit assigns all users to control arm: Model not learning or reward signal weak
  - High rate of ignored recommendations: Poor context selection or nudge fatigue
  - No significant difference between adaptive and control groups: Insufficient power or intervention ineffective

- First 3 experiments:
  1. Test different contextual features (e.g., time-based vs. purchase-based) to see which drives better personalization
  2. Vary recommendation frequency to find optimal balance between engagement and fatigue
  3. Experiment with different recommendation strategies (e.g., demand forecast-based) to target specific use cases like stockouts

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The effectiveness claims are based on a single app's data from Indonesian users, limiting generalizability.
- The mechanism for why 18.2-22.9% of recommendations lead to later purchases is not fully explained.
- There's uncertainty about whether the positive impact on pharmacy expenditure is sustained over longer periods or if nudge fatigue diminishes effectiveness.

## Confidence
- **High Confidence**: The technical implementation of contextual bandits with Thompson sampling is sound and well-established in reinforcement learning literature.
- **Medium Confidence**: The positive impact on pharmacy expenditure and recommendation success rates is supported by experimental data, but the small effect size and single-app context warrant caution in generalizing results.
- **Low Confidence**: The long-term sustainability of the nudging effect and the system's adaptability to different pharmacy contexts remain unclear.

## Next Checks
1. Conduct a longer-term study (6-12 months) to assess the sustainability of the nudging effect and potential for nudge fatigue.
2. Implement A/B testing with different contextual feature sets to identify which features most strongly drive recommendation success.
3. Test the system's performance on data from multiple pharmacy apps or regions to evaluate generalizability across different contexts.