---
ver: rpa2
title: Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of
  Adapters
arxiv_id: '2402.00828'
source_url: https://arxiv.org/abs/2402.00828
tags:
- adapter
- adapters
- soft-moa
- each
- soft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates parameter-efficient transfer learning for
  audio spectrogram transformers using a soft mixture of adapters (Soft-MoA). The
  proposed method employs a set of adapters as experts, where each expert processes
  a soft convex combination of all input tokens rather than a subset of hard-selected
  tokens, thereby reducing computational costs while maintaining full differentiability.
---

# Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters

## Quick Facts
- **arXiv ID**: 2402.00828
- **Source URL**: https://arxiv.org/abs/2402.00828
- **Authors**: Umberto Cappellazzo; Daniele Falavigna; Alessio Brutti
- **Reference count**: 0
- **Primary result**: Soft-MoA outperforms single adapter baseline by up to 2.5% accuracy and matches dense MoA while reducing training time by >3×

## Executive Summary
This paper introduces Soft-MoA (Soft Mixture of Adapters), a parameter-efficient transfer learning method for audio spectrogram transformers that uses soft convex combinations of tokens rather than hard routing. The approach employs a set of expert adapters where each processes a weighted combination of all input tokens, controlled by a router network. Evaluated on four audio/speech classification benchmarks, Soft-MoA achieves up to 2.5% accuracy improvement over single adapters while maintaining full differentiability and reducing computational costs by more than 3× compared to dense MoA variants.

## Method Summary
Soft-MoA modifies traditional MoE routing by using soft convex combinations instead of hard token selection. A router network computes dispatch weights (softmax over input tokens) and combine weights (softmax over adapter outputs) to determine how tokens are processed by each expert adapter. Each adapter receives a weighted combination of all tokens rather than a subset, processing them through a bottleneck architecture with p slots. The method fixes total parameters at ~900K while varying the number of adapters N, demonstrating that more adapters with fewer slots outperforms fewer adapters with many slots.

## Key Results
- Soft-MoA outperforms single adapter baseline by up to 2.5% accuracy across all four benchmarks
- Achieves comparable performance to dense MoA while reducing training time by more than 3×
- Scales better with increasing adapter count, showing optimal performance with 1-2 slots per adapter
- Avoids expert imbalance issues by ensuring all adapters contribute proportionally through soft routing

## Why This Works (Mechanism)

### Mechanism 1
Soft-MoA enables specialized expert processing while controlling computational cost by having each expert process a soft convex combination of input tokens rather than all tokens. The router determines expert contribution via weighted summation, reducing FLOPs compared to dense MoA while maintaining differentiability. This approach preserves essential token information while reducing per-expert load through sparse-yet-differentiable routing.

### Mechanism 2
The method achieves parameter-efficient scaling by decoupling adapter count from parameter count. By fixing bottleneck dimension at 1 and increasing adapters N, each adapter processes a subset of tokens through p slots, maintaining low per-adapter parameters. This specialization approach provides better performance than few large adapters, as demonstrated by scaling experiments showing improved results with more adapters and fewer slots.

### Mechanism 3
Soft-MoA avoids expert imbalance by ensuring all experts contribute to output tokens through soft convex combinations in both dispatch and combine stages. This soft routing maintains balanced gradient flow to all experts during training, preventing the "token dropping" and "expert imbalance" issues common in hard routing methods where some experts are completely ignored.

## Foundational Learning

- **Concept**: Mixture of Experts (MoE) routing
  - Why needed here: Understanding how experts are selected/chosen is critical for grasping Soft-MoA's soft vs hard routing distinction
  - Quick check question: What's the key difference between Top-k MoE and Soft MoE routing in terms of differentiability?

- **Concept**: Parameter-efficient transfer learning (PETL)
  - Why needed here: Soft-MoA is a PETL method, so understanding adapter-based fine-tuning is essential
  - Quick check question: How does Pfeiffer configuration differ from Houlsby in adapter placement?

- **Concept**: Convex combinations and weighted averaging
  - Why needed here: Soft-MoA's core mechanism relies on soft convex combinations of tokens for both dispatch and combine stages
  - Quick check question: Why does using a softmax to compute convex combinations ensure all weights sum to 1?

## Architecture Onboarding

- **Component map**: Input tokens → Router → Dispatch weights → Soft token combinations → Expert adapters → Combine weights → Final output tokens

- **Critical path**: 
  1. Input tokens → Router → Compute dispatch weights
  2. Dispatch weights × Input tokens → Compute soft token combinations
  3. Each expert processes its assigned slots
  4. Expert outputs → Combine weights → Final output tokens

- **Design tradeoffs**:
  - More adapters vs larger adapter size: Soft-MoA favors many small adapters
  - Slot count p: Too few loses information, too many approaches dense MoA
  - Router complexity: Simple FC layer vs more sophisticated routing

- **Failure signatures**:
  - Performance plateaus despite increasing N: Possible slot redundancy or insufficient routing capacity
  - Training instability: Router weights may be collapsing or exploding
  - Memory errors: p × N may exceed available memory despite parameter efficiency

- **First 3 experiments**:
  1. Compare single adapter vs Soft-MoA with N=2, p=1 on ESC-50 to verify baseline improvement
  2. Vary p from 1 to 4 with fixed N=4 to find optimal slot count
  3. Test Soft-MoA vs Dense-MoA with same parameter count to measure computational efficiency gain

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Soft-MoA scale with the number of adapters when the total number of learnable parameters is fixed? The paper shows benefits from more adapters with fewer slots but doesn't provide detailed analysis of performance changes with different adapter counts under constant parameter budgets.

### Open Question 2
How does Soft-MoA perform on more diverse or complex audio/speech tasks beyond the four benchmarks tested? The evaluation is limited to four specific datasets, leaving generalizability to other tasks untested.

### Open Question 3
What is the impact of different routing strategies on the performance of Soft-MoA? The paper uses soft assignment but doesn't compare it with other routing strategies like top-k or other sparse methods.

### Open Question 4
How does the choice of adapter type (bottleneck vs. convolutional) affect the performance of Soft-MoA? While both adapter types are shown to work, the paper doesn't explore how each type influences effectiveness in detail.

## Limitations
- Evaluation limited to four audio/speech classification datasets with similar characteristics, constraining generalizability to diverse audio tasks
- Computational efficiency claims based only on training time per step without comprehensive profiling of memory usage, inference latency, or full training convergence
- Parameter efficiency comparison uses fixed budget rather than exploring scaling behavior when using full parameter count of dense MoA

## Confidence

**High Confidence**: Core architectural claims regarding differentiable routing and slot-based processing are well-supported by mathematical formulation and consistent experimental results across all four datasets. Accuracy improvements over single adapter baselines are robust and reproducible.

**Medium Confidence**: Computational efficiency claims are moderately supported with 3× training time reduction but lack comprehensive profiling of memory usage, inference latency, and full training convergence times. Scalability analysis is based on limited experimental sweeps.

**Low Confidence**: Generalization claims about immunity to expert imbalance are primarily theoretical with limited empirical validation. The paper doesn't provide quantitative analysis of expert utilization or demonstrate performance stability when scaling to dozens of adapters.

## Next Checks

1. **Generalization to Diverse Audio Tasks**: Evaluate Soft-MoA on audio tasks with different characteristics including variable-length sequences (music transcription), multi-channel audio (binaural sound classification), and real-time processing scenarios to validate cross-domain effectiveness.

2. **Comprehensive Efficiency Benchmarking**: Conduct full profiling including wall-clock training time to convergence, inference latency, memory consumption during both training and inference, and scalability analysis with increasing adapter counts compared against dense MoA under identical computational resources.

3. **Expert Utilization Analysis**: Implement quantitative metrics to measure expert activation patterns and contribution balance across all adapters during training, analyzing how soft routing distributes token processing load and whether any adapters consistently receive near-zero weights across different datasets and training stages.