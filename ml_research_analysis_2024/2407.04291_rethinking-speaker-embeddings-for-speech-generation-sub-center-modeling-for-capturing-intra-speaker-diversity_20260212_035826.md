---
ver: rpa2
title: 'Rethinking Speaker Embeddings for Speech Generation: Sub-Center Modeling for
  Capturing Intra-Speaker Diversity'
arxiv_id: '2407.04291'
source_url: https://arxiv.org/abs/2407.04291
tags:
- speaker
- speech
- embeddings
- sub-center
- variance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of speaker embeddings being optimized
  for recognition tasks, which suppresses intra-speaker variation and makes them suboptimal
  for speech generation tasks requiring natural prosodic expressiveness. The proposed
  solution introduces a sub-center modeling framework where each speaker is represented
  by multiple prototypes instead of a single class center, allowing embeddings to
  capture intra-speaker diversity while maintaining speaker discrimination.
---

# Rethinking Speaker Embeddings for Speech Generation: Sub-Center Modeling for Capturing Intra-Speaker Diversity

## Quick Facts
- **arXiv ID**: 2407.04291
- **Source URL**: https://arxiv.org/abs/2407.04291
- **Reference count**: 0
- **Primary result**: Sub-center embeddings with highest intra-class variance achieved 3.18±0.12 MOS for naturalness vs baseline's 2.94±0.12, while maintaining EER 1.21-1.55%

## Executive Summary
This paper addresses the fundamental mismatch between speaker embeddings optimized for recognition tasks versus those needed for speech generation. Standard embeddings trained with cross-entropy loss suppress intra-speaker variation to maximize speaker discrimination, making them suboptimal for generation tasks requiring natural prosodic expressiveness. The proposed sub-center modeling framework represents each speaker with multiple prototypes instead of a single center, allowing embeddings to capture intra-speaker diversity while maintaining speaker discrimination. Evaluated on voice conversion tasks, the approach achieved significant improvements in naturalness (3.18±0.12 vs 2.94±0.12 MOS) and speaker similarity (2.88±0.13 SMOS) while producing more expressive speech with greater pitch variability.

## Method Summary
The sub-center modeling framework modifies the standard ECAPA-TDNN architecture by replacing the single class center with multiple sub-centers per speaker. During training, each speaker has C sub-centers represented by a weight matrix W ∈ R^{L×N×C}. Utterances are matched to the most similar sub-center using temperature-scaled softmax aggregation, allowing the model to capture diverse speaker-specific characteristics. The framework uses AAM-Softmax loss with aggregated similarities and is evaluated in a voice conversion pipeline using HuBERT + VQ-VAE for linguistic and pitch features, and HiFi-GAN for speech synthesis.

## Key Results
- Voice conversion naturalness MOS improved from 2.94±0.12 to 3.18±0.12 with sub-center embeddings
- Speaker similarity SMOS reached 2.88±0.13 with higher-variance embeddings
- Generated speech showed greater pitch variability: f0 std increased from 8.03 to 10.25, f0 range from 52.37 to 57.09
- Speaker verification performance maintained with EER of 1.21-1.55% across different settings
- Intra-class variance ratio improved from 1.85 to 2.48 with C=20 sub-centers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sub-center modeling captures intra-speaker diversity while maintaining speaker discrimination by representing each speaker with multiple prototypes instead of a single center.
- Mechanism: The model maintains a weight matrix W ∈ R^{L×N×C} where each speaker has C sub-centers. During training, utterances selectively align with sub-centers rather than being forced to a single prototype, allowing the embedding to capture diverse speaker-specific characteristics like prosody and style.
- Core assumption: Intra-speaker variation (prosody, emotion, speaking style) is not noise but essential information for speech generation tasks.
- Evidence anchors: [abstract]: "sub-center modeling allows the embedding to capture a broader range of speaker-specific variations while maintaining speaker classification performance"; [section 3.2]: "Given an embedding xi of speaker yi, the similarity to the c-th sub-center of class n is sn,c = w^T_{n,c}xi"

### Mechanism 2
- Claim: The temperature-scaled softmax aggregation allows smooth selection between sub-centers, enabling the model to capture prosodic variations while maintaining speaker identity.
- Mechanism: The temperature T in the softmax weighting (α_{n,c} = exp(s_{n,c}/T) / Σ_k exp(s_{n,k}/T)) controls how confidently the model selects sub-centers. Higher temperatures allow smoother transitions between sub-centers, while lower temperatures create more distinct clusters.
- Core assumption: A moderate temperature allows the model to distribute utterances across sub-centers in a way that captures natural variation without losing speaker discrimination.
- Evidence anchors: [section 3.2]: "We aggregate similarities within each class using a temperature-scaled softmax weights"; [section 5.1]: "sub-center modeling with T=1 achieves higher intra-class variance compared to the standard ECAPA-TDNN"

### Mechanism 3
- Claim: Higher intra-class variance in speaker embeddings directly correlates with more natural and expressive speech synthesis, as demonstrated in voice conversion tasks.
- Mechanism: Embeddings with higher variance capture more prosodic and stylistic variation, which gets preserved during generation. This is evidenced by objective metrics (F0 standard deviation, F0 range) and subjective evaluations (MOS for naturalness).
- Core assumption: The prosodic variations captured in embeddings are effectively transferred to the generated speech through the voice conversion pipeline.
- Evidence anchors: [abstract]: "higher-variance embeddings improve naturalness and prosody" with objective results showing f0 std 10.25 vs 8.03 and f0 range 57.09 vs 52.37; [section 5.2]: "higher-variance embeddings produce greater pitch variability and embedding spread, indicating more expressive and diverse speech"

## Foundational Learning

- Concept: Angular margin softmax (AAM-Softmax) and its role in speaker embedding training
  - Why needed here: Understanding AAM-Softmax is crucial because the sub-center modification builds directly on this loss function. The angular margin m=0.4 and scale s=30 are key hyperparameters that affect how embeddings cluster.
  - Quick check question: What is the difference between standard softmax and AAM-Softmax in terms of how they handle angular distances between embeddings?

- Concept: Voice conversion pipeline architecture and the role of speaker embeddings
  - Why needed here: The paper evaluates sub-center embeddings specifically in a voice conversion context, where embeddings must capture both speaker identity and prosodic information. Understanding the factorization into linguistic units, pitch, and speaker identity is essential.
  - Quick check question: In the speech resynthesis framework, which components are frozen during HiFi-GAN vocoder training, and why?

- Concept: Speaker verification metrics (EER, intra/inter-class variance ratio)
  - Why needed here: The paper uses these metrics to evaluate whether sub-center modeling maintains or improves speaker recognition performance while increasing variance. Understanding these metrics is crucial for interpreting the results.
  - Quick check question: How is the intra-class variance ratio calculated, and what does a higher ratio indicate about the embedding space?

## Architecture Onboarding

- Component map: ECAPA-TDNN backbone -> Modified classifier head with sub-center weights W ∈ R^{L×N×C} -> Temperature-scaled softmax aggregation layer -> AAM-Softmax loss with aggregated similarities -> Voice conversion pipeline (HuBERT + VQ-VAE + HiFi-GAN)

- Critical path: 1. Forward pass through ECAPA-TDNN backbone; 2. Compute similarities to all sub-centers for each speaker class; 3. Apply temperature-scaled softmax to get sub-center weights; 4. Aggregate similarities using weighted sum; 5. Compute AAM-Softmax loss with aggregated similarities; 6. Backpropagation through the network

- Design tradeoffs:
  - Number of sub-centers (C): More sub-centers capture more variation but increase computational cost and risk of overfitting
  - Temperature (T): Higher T allows smoother variation but may reduce discriminative power; lower T creates distinct clusters but may lose variation
  - Angular margin (m): Larger m increases separation between speakers but may compress intra-speaker variation

- Failure signatures:
  - Low intra-class variance despite multiple sub-centers: Temperature too low, causing confident selection of few sub-centers
  - Poor speaker verification performance: Too many sub-centers or insufficient training data causing overfitting
  - No improvement in generated speech quality: Generation model unable to utilize the additional variance, or variance not effectively captured in embeddings

- First 3 experiments:
  1. Train baseline ECAPA-TDNN with C=1, m=0.4, s=30, T=1 and measure intra-class variance ratio and EER
  2. Train sub-center model with C=10, m=0.4, s=30, T=1 and compare intra-class variance and EER to baseline
  3. Train sub-center model with C=20, m=0.4, s=30, T=0.1 and evaluate both verification performance and voice conversion quality to understand the temperature tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal number of sub-centers per speaker vary across different speech generation tasks beyond voice conversion?
- Basis in paper: [explicit] The paper tests C=10 and C=20 sub-centers but notes that optimal settings may vary for different tasks
- Why unresolved: The experiments only evaluated voice conversion, leaving open whether different generation tasks (TTS, singing voice synthesis, etc.) would benefit from different sub-center configurations
- What evidence would resolve it: Systematic experiments testing multiple sub-center configurations across diverse speech generation tasks with unified evaluation metrics

### Open Question 2
- Question: What is the relationship between intra-class variance captured by sub-center embeddings and the ability to model speaker-specific speaking styles and emotions?
- Basis in paper: [inferred] The paper demonstrates that higher intra-class variance produces more expressive speech with greater pitch variability, but doesn't specifically analyze style or emotion modeling
- Why unresolved: While the paper shows increased prosodic variation, it doesn't directly measure or analyze the embedding's capacity to capture distinct speaking styles or emotional states
- What evidence would resolve it: Experiments measuring style and emotion transfer capabilities using sub-center embeddings versus single-center baselines, with quantitative emotion recognition metrics

### Open Question 3
- Question: How do sub-center embeddings perform when trained on languages with significantly different phonological systems or prosody patterns compared to English?
- Basis in paper: [explicit] The proposed embeddings are trained on VoxCeleb2 (English-focused) and evaluated on VCTK (English), with no multilingual or cross-linguistic validation
- Why unresolved: The current experiments only involve English speech data, leaving uncertain whether the sub-center approach generalizes to languages with different prosodic structures
- What evidence would resolve it: Cross-linguistic experiments training sub-center embeddings on diverse language corpora and evaluating generation quality across languages with varying phonological and prosodic characteristics

### Open Question 4
- Question: What is the computational overhead and memory requirement difference between single-center and sub-center embedding training and inference?
- Basis in paper: [inferred] The paper introduces a more complex model architecture with multiple sub-centers per speaker but doesn't report training/inference efficiency metrics
- Why unresolved: While performance improvements are demonstrated, the paper doesn't address the practical trade-offs in terms of computational resources required for the sub-center approach
- What evidence would resolve it: Detailed benchmarking of training time, memory usage, and inference latency comparing single-center and sub-center implementations across different hardware configurations

## Limitations
- Dataset specificity: Experiments limited to read speech corpora (LibriSpeech, VCTK), effectiveness for spontaneous or emotionally varied speech untested
- Generation pipeline dependency: Evaluation relies on specific voice conversion setup, benefits may not transfer to other generation frameworks
- Hyperparameter sensitivity: Optimal number of sub-centers (C=20) and temperature (T=1) are dataset-specific with no generalization guidance

## Confidence
**High confidence**:
- The sub-center modeling framework works as described in the ECAPA-TDNN architecture
- Speaker verification performance (EER 1.21-1.55%) is maintained or improved
- Voice conversion MOS improvements are statistically significant (3.18±0.12 vs 2.94±0.12)

**Medium confidence**:
- The mechanism by which higher intra-class variance leads to more expressive speech (objective metrics support this, but the causal chain from embedding to generation is not fully validated)
- The temperature scaling provides the claimed smooth selection behavior (evidence from variance metrics, but no ablation showing T's specific effect)

**Low confidence**:
- Sub-center modeling would work equally well with other backbone architectures (only tested with ECAPA-TDNN)
- The benefits generalize to non-voice conversion speech generation tasks (only tested in voice conversion context)
- The optimal hyperparameters (C=20, T=1) are universally applicable across different datasets and tasks

## Next Checks
1. **Cross-dataset generalization test**: Evaluate sub-center embeddings trained on LibriSpeech/VCTK when applied to expressive speech datasets like CMU-MOSEI or spontaneous conversation corpora. Measure whether the intra-speaker variance captures emotional and stylistic variation beyond read speech patterns.

2. **Generation architecture ablation**: Replace the voice conversion pipeline with a text-to-speech system (e.g., FastSpeech 2 or VITS) while keeping the same sub-center embeddings. Compare the naturalness and expressiveness improvements to validate that benefits are embedding-driven rather than pipeline-specific.

3. **Dynamic sub-center allocation**: Implement a curriculum learning approach where the number of active sub-centers starts small and increases during training based on observed intra-speaker variance. Test whether this adaptive approach outperforms the fixed C=20 setting across different dataset sizes and speaker counts.