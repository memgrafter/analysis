---
ver: rpa2
title: Long-context LLMs Struggle with Long In-context Learning
arxiv_id: '2404.02060'
source_url: https://arxiv.org/abs/2404.02060
tags:
- long
- context
- llms
- learning
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LongICLBench, a benchmark designed to evaluate
  long-context large language models (LLMs) on extreme-label classification tasks
  using in-context learning. The benchmark includes six datasets with varying difficulty
  levels, ranging from 28 to 174 classes and input lengths from 2K to 50K tokens.
---

# Long-context LLMs Struggle with Long In-context Learning

## Quick Facts
- **arXiv ID**: 2404.02060
- **Source URL**: https://arxiv.org/abs/2404.02060
- **Reference count**: 22
- **Primary result**: Long-context LLMs struggle with extreme-label classification tasks despite their ability to handle long sequences

## Executive Summary
This paper introduces LongICLBench, a benchmark designed to evaluate long-context large language models on extreme-label classification tasks using in-context learning. The benchmark includes six datasets with varying difficulty levels, ranging from 28 to 174 classes and input lengths from 2K to 50K tokens. The authors evaluate 15 long-context LLMs, including both open-source models and state-of-the-art models like GPT-4-turbo and Gemini-1.5-Pro. The results show that while LLMs perform well on simpler tasks with smaller label spaces, they struggle with more complex tasks like Discovery, which has 174 labels. The study also reveals a bias towards labels presented later in the sequence and a need for improved reasoning over multiple pieces of information. Overall, the findings suggest that long-context understanding and reasoning remain challenging for existing LLMs, highlighting the need for further research in this area.

## Method Summary
The paper evaluates long-context LLMs on six datasets with varying difficulty levels using in-context learning. The evaluation framework involves creating subsets of the datasets with different label distributions and input lengths, ranging from 2K to 50K tokens. The authors use accuracy as the primary metric for classification tasks and F1 score for entity recognition and relation extraction tasks. They evaluate 15 long-context LLMs, including transformer-based models, RNN-like models, and API-based models, using varying numbers of examples per class (1 to 5 shots) and analyze the impact of example position within the prompt on performance.

## Key Results
- Long-context LLMs show significant performance degradation on extreme-label classification tasks with many classes
- Position of instances within prompts affects model performance, with later-positioned labels receiving preferential treatment
- Current long-context LLMs struggle with tasks requiring reasoning over multiple pieces of information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Long-context LLMs fail on extreme-label classification because they cannot effectively process and differentiate among many fine-grained classes when the demonstration length exceeds the model's effective context window.
- Mechanism: The model's ability to recognize task patterns and class distinctions degrades as the number of examples per class increases, causing confusion in label space mapping.
- Core assumption: The model's attention mechanism and context processing have limits that are exceeded when demonstration sequences become too long.
- Evidence anchors:
  - [abstract] "However, they struggle with more challenging task like Discovery with 174 labels, suggesting a gap in their ability to process long, context-rich sequences."
  - [section 3.3] "However, they struggle with more challenging task like Discovery with 174 labels, all LLMs achieve close-to-zero performance except Gemini-1.5-Pro with 14% accuracy."
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism.

### Mechanism 2
- Claim: The position of instances within the prompt affects model performance, with later-positioned labels receiving preferential treatment.
- Mechanism: The model's attention mechanism may be biased toward processing information from later positions in the sequence, leading to better performance on labels presented at the end of the demonstration.
- Core assumption: The model's attention distribution is not uniform across the input sequence, favoring later positions.
- Evidence anchors:
  - [abstract] "Further analysis reveals a bias towards labels presented later in the sequence and a need for improved reasoning over multiple pieces of information."
  - [section 4] "It is shown that the position distribution of instances in the prompt can dramatically influence the performance of some of the evaluated models."
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism.

### Mechanism 3
- Claim: Long-context LLMs struggle with tasks requiring reasoning over multiple pieces of information because their in-context learning capability is limited by sequence length and complexity.
- Mechanism: The model's ability to integrate and reason across multiple pieces of information degrades as the input length and complexity increase, leading to poor performance on tasks like Discovery.
- Core assumption: The model's reasoning capability is constrained by its architecture and training, limiting its ability to process and integrate information from long sequences.
- Evidence anchors:
  - [abstract] "Further analysis reveals a bias towards labels presented later in the sequence and a need for improved reasoning over multiple pieces of information."
  - [section 3.3] "Our initial hypothesis suggests that the strongest LLMs like GPT-4-turbo are capped at a certain complexity level between DialogRE and Discovery."
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism.

## Foundational Learning

- Concept: Extreme-label classification
  - Why needed here: Understanding the task of categorizing data into a large number of classes is crucial for evaluating the model's performance on LongICLBench.
  - Quick check question: Can you explain how extreme-label classification differs from traditional classification tasks?

- Concept: In-context learning
  - Why needed here: The ability to learn from demonstrations within the context is essential for the model to understand and perform the classification tasks in LongICLBench.
  - Quick check question: How does in-context learning differ from traditional fine-tuning approaches?

- Concept: Long-context processing
  - Why needed here: The ability to process and understand long sequences of text is crucial for the model to perform well on the tasks in LongICLBench, which involve long demonstration sequences.
  - Quick check question: What are the challenges associated with processing long sequences in language models?

## Architecture Onboarding

- Component map: Transformer-based models (LLaMA-2, ChatGLM3, Qwen, Mistral) -> RNN-like models (RWKV, Mamba) -> API-based models (GPT-4, Claude3, Gemini)
- Critical path: Evaluate model performance on LongICLBench tasks with increasing difficulty levels and analyze the impact of instance position distribution on performance.
- Design tradeoffs:
  - Model size vs. performance: Larger models may perform better but require more computational resources.
  - Context length vs. accuracy: Longer context windows may improve performance but can also lead to increased computational costs and potential overfitting.
- Failure signatures:
  - Poor performance on tasks with many classes or long demonstration sequences.
  - Sensitivity to the position of instances within the prompt.
- First 3 experiments:
  1. Evaluate model performance on a simple task with few classes and short demonstration sequences.
  2. Increase the number of classes and demonstration length to observe performance degradation.
  3. Analyze the impact of instance position distribution on model performance by comparing scattered and grouped arrangements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the different positional embedding strategies (AliBi, RoPE, NTK-aware interpolation) affect the ability of long-context LLMs to handle extreme-label classification tasks?
- Basis in paper: [explicit] The paper mentions that various models use different positional embedding strategies, but does not provide a detailed comparison of their impact on long-context understanding and extreme-label classification performance.
- Why unresolved: The paper evaluates the performance of models with different positional embedding strategies but does not isolate and analyze the specific effects of these strategies on long-context understanding and extreme-label classification.
- What evidence would resolve it: A controlled experiment comparing the performance of models with different positional embedding strategies on the LongICLBench benchmark, with other factors (model size, training data, etc.) held constant.

### Open Question 2
- Question: What is the optimal number of demonstrations per label for extreme-label classification tasks in long-context LLMs?
- Basis in paper: [inferred] The paper mentions that models benefit from extensive demonstrations up to a certain length, but does not provide a definitive answer on the optimal number of demonstrations per label.
- Why unresolved: The paper shows that performance improves with more demonstrations up to a certain point, but does not determine the point of diminishing returns or the optimal number of demonstrations per label.
- What evidence would resolve it: A systematic study varying the number of demonstrations per label and measuring the corresponding performance on the LongICLBench benchmark.

### Open Question 3
- Question: How do long-context LLMs handle long-form text generation tasks, such as summarization or story continuation, compared to extreme-label classification tasks?
- Basis in paper: [explicit] The paper focuses on extreme-label classification tasks and mentions that other long-context tasks like summarization are also important but does not provide a comparison of performance.
- Why unresolved: The paper evaluates long-context LLMs on extreme-label classification tasks but does not assess their performance on other long-form text generation tasks.
- What evidence would resolve it: A benchmark suite including both extreme-label classification and long-form text generation tasks, allowing for a direct comparison of long-context LLM performance across different task types.

## Limitations
- Lack of direct corpus evidence supporting the proposed mechanisms, particularly regarding the model's attention bias toward later positions and the degradation of reasoning capabilities with increasing sequence complexity.
- Uncertainty about whether the observed findings represent fundamental model limitations or artifacts of the specific evaluation setup.
- Limited empirical validation across different model architectures and datasets for specific claims about position-based bias and reasoning limitations.

## Confidence
- High Confidence: Current long-context LLMs show significant performance degradation on extreme-label classification tasks; Model performance varies considerably across different task complexities and dataset characteristics.
- Medium Confidence: Position of instances within prompts affects model performance; Reasoning over multiple pieces of information remains challenging for long-context LLMs.
- Low Confidence: The specific mechanism of attention bias toward later positions; The relationship between demonstration length and classification accuracy.

## Next Checks
1. **Cross-model validation**: Test the position bias hypothesis across different model architectures (transformers, RNNs, and state-space models) to determine if the effect is universal or architecture-specific.

2. **Ablation study**: Systematically vary the number of examples per class and demonstration length while keeping the total context window constant to isolate the impact of sequence complexity from context window constraints.

3. **Alternative prompt formats**: Evaluate whether restructuring the demonstration format (e.g., interleaving examples from different classes rather than grouping them) mitigates the observed position-based performance variations.