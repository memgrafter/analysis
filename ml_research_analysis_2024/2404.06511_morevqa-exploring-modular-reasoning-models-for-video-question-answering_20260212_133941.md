---
ver: rpa2
title: 'MoReVQA: Exploring Modular Reasoning Models for Video Question Answering'
arxiv_id: '2404.06511'
source_url: https://arxiv.org/abs/2404.06511
tags:
- frame
- video
- question
- caption
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of video question answering (videoQA)
  by identifying limitations in single-stage modular planning approaches, which rely
  on a single prompt to generate an entire program without grounding in the video
  content. The authors propose a multi-stage modular reasoning framework called MoReVQA,
  which decomposes the task into three stages: event parsing, grounding, and reasoning,
  all using few-shot prompting of large language models in conjunction with an external
  memory.'
---

# MoReVQA: Exploring Modular Reasoning Models for Video Question Answering

## Quick Facts
- **arXiv ID:** 2404.06511
- **Source URL:** https://arxiv.org/abs/2404.06511
- **Authors:** Juhong Min; Shyamal Buch; Arsha Nagrani; Minsu Cho; Cordelia Schmid
- **Reference count:** 40
- **Primary result:** Multi-stage modular reasoning framework outperforms single-stage approaches on four videoQA benchmarks

## Executive Summary
This paper addresses limitations in video question answering (videoQA) by proposing MoReVQA, a multi-stage modular reasoning framework that decomposes the task into event parsing, grounding, and reasoning stages. Unlike single-stage approaches that rely on a single prompt to generate an entire program without grounding in video content, MoReVQA uses few-shot prompting of large language models in conjunction with external memory. The method achieves state-of-the-art results in training-free settings across four standard benchmarks (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) while providing interpretable intermediate outputs that reveal the reasoning process.

## Method Summary
MoReVQA employs a three-stage modular reasoning approach to video question answering. First, event parsing extracts key events from the video using few-shot prompting of a large language model. Second, grounding connects these events to relevant video content through an external memory system. Third, reasoning synthesizes the grounded information to answer the question. This decomposition addresses the limitations of single-stage modular planning approaches that attempt to generate entire programs without grounding in video content. The framework operates in a training-free setting, relying on few-shot prompting rather than model fine-tuning, and demonstrates interpretability through the generation of intermediate outputs that reveal the reasoning process.

## Key Results
- MoReVQA achieves state-of-the-art performance in training-free settings across four standard videoQA benchmarks
- Outperforms the simple baseline JCEF and prior single-stage modular methods on NExT-QA, iVQA, EgoSchema, and ActivityNet-QA
- Ablation studies confirm all three stages (event parsing, grounding, reasoning) provide complementary and synergistic gains
- Demonstrates interpretability through intermediate outputs that reveal the reasoning process

## Why This Works (Mechanism)
The multi-stage modular reasoning approach works by decomposing complex video reasoning into manageable subtasks that can be handled more effectively by large language models. By separating event parsing from grounding and reasoning, the method avoids the complexity collapse that occurs in single-stage approaches where a single prompt must handle both video understanding and question answering simultaneously. The external memory system enables better grounding of extracted events to specific video content, while few-shot prompting allows the model to leverage pre-existing reasoning capabilities without requiring task-specific training.

## Foundational Learning

**Few-shot prompting** - why needed: Enables the model to perform videoQA without task-specific training, leveraging pre-trained reasoning capabilities of large language models.
*quick check:* Does the performance scale with the number of examples in the few-shot prompt?

**Modular decomposition** - why needed: Breaks down complex video reasoning into simpler subtasks that can be handled more effectively by language models.
*quick check:* How sensitive is performance to the specific decomposition strategy chosen?

**External memory systems** - why needed: Provides persistent storage for grounding information between reasoning stages and enables better video-event associations.
*quick check:* What is the retrieval accuracy of the memory system for relevant video content?

**Event parsing** - why needed: Extracts meaningful semantic units from video that can be used as building blocks for reasoning.
*quick check:* Does the parsing granularity (event-level vs. frame-level) affect downstream performance?

## Architecture Onboarding

**Component map:** Event Parser -> External Memory -> Grounding Module -> Reasoning Module -> Answer Generator

**Critical path:** Video frames → Event Parser → External Memory → Grounding → Reasoning → Final Answer

**Design tradeoffs:** The framework trades computational efficiency for interpretability and modularity. The three-stage decomposition adds latency but provides clearer reasoning traces and potentially better generalization to unseen video types.

**Failure signatures:** Poor event parsing leads to cascading failures in grounding and reasoning stages. Memory retrieval failures cause grounding errors that propagate through the system. Overly complex questions may exceed the reasoning capacity of the few-shot prompting approach.

**First experiments:** 1) Test event parsing accuracy on a held-out video set, 2) Evaluate memory retrieval precision for video-event associations, 3) Compare single-stage vs. multi-stage performance on simple questions to validate the decomposition benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of MoReVQA change when using stronger vision-language models (VLMs) and large language models (LLMs) than the ones evaluated in the paper?
- **Basis in paper:** The paper notes that significant developments in stronger VLMs and LLMs have occurred since the initial submission, and that inclusion of such models would likely further increase the "state-of-the-art" numbers for the benchmarks examined.
- **Why unresolved:** The paper intentionally used consistent baseline models across all methods for controlled comparison, but did not evaluate the impact of using stronger models.
- **What evidence would resolve it:** Experiments comparing MoReVQA's performance on the four benchmarks (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) using both the original models and state-of-the-art VLMs/LLMs like GPT-4V or Gemini.

### Open Question 2
- **Question:** To what extent does the event parsing stage contribute to MoReVQA's performance improvements, and can this stage be further optimized?
- **Basis in paper:** The paper includes an ablation study showing all three stages (event parsing, grounding, reasoning) provide complementary and synergistic gains, but does not deeply analyze the specific contribution of event parsing or explore optimization strategies for this stage.
- **Why unresolved:** While the ablation shows event parsing is beneficial, the paper doesn't quantify its exact contribution or investigate whether more sophisticated parsing techniques could yield additional improvements.
- **What evidence would resolve it:** Comparative experiments isolating event parsing performance, analysis of different parsing strategies (e.g., word-level vs. event-level), and evaluation of alternative parsing methods on the benchmark datasets.

### Open Question 3
- **Question:** How does MoReVQA's performance scale with video length, and are there specific thresholds where additional frames stop providing meaningful information?
- **Basis in paper:** The paper mentions EgoSchema uses 30 or 90 frames and observes marginal performance differences, suggesting a potential plateau in video information utility, but doesn't systematically investigate this phenomenon across different video lengths.
- **Why unresolved:** The paper provides limited analysis on how video length affects performance and doesn't identify specific thresholds where additional frames become redundant or counterproductive.
- **What evidence would resolve it:** Systematic experiments varying video length and frame sampling rates across all benchmark datasets, identifying performance plateaus and analyzing frame redundancy patterns.

## Limitations
- Evaluation primarily on short videos (5-10 seconds) leaves uncertainty about performance on longer, more complex video content
- Reliance on few-shot prompting introduces potential variability across different prompting strategies and LLM versions
- Interpretability claims based largely on qualitative observations without rigorous quantitative metrics
- Training-free setting advantage may not reflect real-world deployment scenarios where fine-tuning could be feasible

## Confidence

**Performance superiority over baselines:** High confidence - supported by consistent quantitative improvements across multiple benchmarks
**Modular decomposition effectiveness:** Medium confidence - demonstrated through ablation studies but not compared against alternative decomposition strategies
**Interpretability benefits:** Low confidence - based primarily on qualitative observations without systematic evaluation metrics

## Next Checks
1. Evaluate MoReVQA on longer videos (30+ seconds) with more complex reasoning requirements to test scalability of the modular approach
2. Conduct systematic ablation studies varying the LLM used for prompting to quantify sensitivity to model choice and prompting strategy
3. Develop and apply quantitative metrics for measuring interpretability quality of intermediate reasoning steps to objectively compare against single-stage approaches