---
ver: rpa2
title: 'Foundational Autoraters: Taming Large Language Models for Better Automatic
  Evaluation'
arxiv_id: '2407.10817'
source_url: https://arxiv.org/abs/2407.10817
tags:
- evaluation
- flame
- arxiv
- language
- urlhttps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FLAMe, a family of foundational autorater
  models trained on a large, diverse collection of 100+ quality assessment tasks with
  5M+ human judgments. FLAMe significantly improves generalization to held-out tasks,
  outperforming popular proprietary models like GPT-4 and Claude-3 on many benchmarks.
---

# Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation

## Quick Facts
- arXiv ID: 2407.10817
- Source URL: https://arxiv.org/abs/2407.10817
- Authors: Tu Vu; Kalpesh Krishna; Salaheddin Alzubi; Chris Tar; Manaal Faruqui; Yun-Hsuan Sung
- Reference count: 40
- Primary result: FLAMe outperforms GPT-4 and Claude-3 on automatic evaluation benchmarks while being trained exclusively on open-source data

## Executive Summary
This paper introduces FLAMe, a family of foundational autorater models trained on a large, diverse collection of 100+ quality assessment tasks with 5M+ human judgments. FLAMe significantly improves generalization to held-out tasks, outperforming popular proprietary models like GPT-4 and Claude-3 on many benchmarks. The models are trained in a supervised multitask fashion on permissively licensed datasets, addressing challenges of high human evaluation costs and potential biases in model-generated data. FLAMe-RM, a variant fine-tuned for reward modeling evaluation, achieves an accuracy of 87.8% on RewardBench, surpassing GPT-4-0125 (85.9%) and GPT-4o (84.7%) while being trained exclusively on open-source data.

## Method Summary
FLAMe is developed through supervised multitask fine-tuning on PaLM-2-24B, converting 102 quality assessment tasks into a unified text-to-text format. The approach uses examples-proportional mixture weights for 30K training steps, then fine-tunes FLAMe-RM for 50 steps on a balanced mixture of four pairwise evaluation datasets. A computationally efficient tail-patch fine-tuning strategy optimizes the multitask mixture for targeted reward modeling evaluation (FLAMe-Opt-RM). The models are evaluated on 12 autorater benchmarks covering 53 quality assessment tasks, with variants demonstrating strong performance on downstream applications like reward modeling evaluation.

## Key Results
- FLAMe outperforms GPT-4 and Claude-3 on 11 out of 12 autorater evaluation benchmarks
- FLAMe-RM achieves 87.8% accuracy on RewardBench, surpassing GPT-4-0125 (85.9%) and GPT-4o (84.7%)
- FLAMe shows lower bias on the CoBBLEr benchmark compared to GPT-4 and GPT-4o
- FLAMe-Opt-RM demonstrates computational efficiency while maintaining strong performance

## Why This Works (Mechanism)

### Mechanism 1
Large-scale multitask instruction tuning on diverse human evaluations improves generalization to unseen quality assessment tasks. Training on a broad range of task types and capabilities enables the model to learn generalized patterns of human judgment. Core assumption: Human evaluation data is sufficiently diverse and standardized to allow cross-task transfer learning. Evidence anchors: FLAMe significantly improves generalization to held-out tasks, outperforming LLMs trained on proprietary data. Break condition: If human evaluation data contains significant task-specific biases that are not generalizable.

### Mechanism 2
Fine-tuning FLAMe on a balanced mixture of pairwise evaluation datasets spanning chat, reasoning, and safety optimizes performance for reward modeling evaluation. The initial FLAMe training provides a strong foundation in general quality assessment, which is then specialized for reward modeling. Core assumption: Reward modeling evaluation is a subset of the broader quality assessment tasks covered by FLAMe. Evidence anchors: FLAMe-RM fine-tuned on a mixture of four pairwise evaluation datasets achieves high accuracy on RewardBench. Break condition: If reward modeling evaluation requires capabilities not covered by initial FLAMe training.

### Mechanism 3
A novel tail-patch fine-tuning strategy optimizes the multitask mixture for targeted reward modeling evaluation, achieving competitive performance with significantly less compute. By analyzing the impact of each dataset on targeted RewardBench distributions, optimal proportions of individual datasets can be determined. Core assumption: The impact of each dataset on the target distribution can be accurately measured with a small number of fine-tuning steps. Evidence anchors: FLAMe-Opt-RM uses tail-patch fine-tuning to optimize mixture weights. Break condition: If tail-patch analysis does not accurately reflect long-term impact on target distribution.

## Foundational Learning

- Concept: Multitask Learning
  - Why needed here: To train a model that can perform a wide variety of quality assessment tasks
  - Quick check question: What are the key challenges in multitask learning, and how does the FLAMe approach address them?

- Concept: Transfer Learning
  - Why needed here: To leverage knowledge gained from training on diverse tasks to improve performance on new, unseen tasks
  - Quick check question: How does the FLAMe approach facilitate transfer learning across different task types?

- Concept: Fine-tuning
  - Why needed here: To adapt the general-purpose FLAMe model to specific downstream applications like reward modeling evaluation
  - Quick check question: What are the key considerations when fine-tuning a large language model for a specific task?

## Architecture Onboarding

- Component map: Data collection and standardization -> Text-to-text format conversion -> Multitask training pipeline -> Fine-tuning pipeline -> Evaluation pipeline
- Critical path: 1. Data collection and standardization, 2. Text-to-text format conversion, 3. Multitask training, 4. Fine-tuning (if applicable), 5. Evaluation
- Design tradeoffs: Using publicly available human evaluation data vs. proprietary data; using diverse range of task types vs. focusing on specific domain; using large model size vs. smaller, more efficient model
- Failure signatures: Poor generalization to held-out tasks; bias towards certain judgments; inability to perform specific downstream tasks
- First 3 experiments: 1. Evaluate FLAMe on held-out task from different domain than training data, 2. Compare performance of FLAMe and FLAMe-RM on reward modeling evaluation, 3. Analyze impact of different multitask mixture weights on RewardBench performance

## Open Questions the Paper Calls Out

- Open Question 1: How does the performance of FLAMe compare to proprietary models like GPT-4 and Claude-3 on multilingual quality assessment tasks?
  - Basis in paper: The paper mentions that FLAMe is primarily trained on English data with a context length of 2048 tokens, which might limit its performance on multilingual tasks
  - Why unresolved: The paper does not provide any experimental results or analysis on FLAMe's performance on multilingual quality assessment tasks
  - What evidence would resolve it: Experimental results comparing FLAMe's performance on multilingual quality assessment tasks against proprietary models

- Open Question 2: What are the potential long-term biases in FLAMe models that might emerge with extended use or in different application domains?
  - Basis in paper: The paper discusses potential biases in LLM autoraters, but focuses on short-term biases
  - Why unresolved: The paper does not explore long-term biases that might emerge from extended use or application in different domains
  - What evidence would resolve it: Longitudinal studies or experiments applying FLAMe in various domains to identify emerging biases over time

- Open Question 3: How does the tail-patch fine-tuning strategy affect the generalization capabilities of FLAMe across different downstream tasks beyond RewardBench?
  - Basis in paper: The paper introduces a tail-patch fine-tuning strategy to optimize FLAMe for RewardBench, but notes that its effectiveness on other tasks is not fully explored
  - Why unresolved: The paper primarily focuses on RewardBench performance, with limited discussion on the strategy's impact on other tasks
  - What evidence would resolve it: Experiments evaluating FLAMe variants using the tail-patch fine-tuning strategy on a variety of downstream tasks

## Limitations

- The tail-patch fine-tuning optimization strategy for FLAMe-Opt-RM lacks detailed specification, making it difficult to reproduce or validate the computational efficiency claims
- Limited discussion of potential task-specific biases in the human evaluation data that could affect generalization
- The comparison with proprietary models like GPT-4 and Claude-3 may not account for differences in training data, model architecture, or evaluation protocols

## Confidence

- **High confidence**: The core claim that multitask training on diverse human evaluations improves generalization to held-out tasks, supported by empirical results on multiple benchmarks
- **Medium confidence**: The computational efficiency claims for FLAMe-Opt-RM, given the lack of detailed methodology for the tail-patch fine-tuning approach
- **Medium confidence**: The bias reduction claims on CoBBLEr, though the specific types of bias addressed could be more thoroughly examined

## Next Checks

1. **Cross-task generalization validation**: Test FLAMe on a held-out task from a completely different domain than the training data to verify true generalization capabilities beyond the evaluation benchmarks

2. **Bias analysis replication**: Conduct an independent analysis of FLAMe's bias using multiple bias benchmarks beyond CoBBLEr to verify the claimed bias reduction

3. **Efficiency verification**: Implement the tail-patch fine-tuning strategy independently to verify the computational efficiency claims for FLAMe-Opt-RM and assess whether the optimized mixture maintains performance on diverse tasks