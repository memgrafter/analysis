---
ver: rpa2
title: 'DiJiang: Efficient Large Language Models through Compact Kernelization'
arxiv_id: '2403.19928'
source_url: https://arxiv.org/abs/2403.19928
tags:
- attention
- transformer
- language
- arxiv
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiJiang, a frequency domain kernelization
  approach that enables the transformation of pre-trained vanilla Transformers into
  linear-complexity models with minimal training costs. By leveraging weighted Quasi-Monte
  Carlo sampling and Discrete Cosine Transform (DCT) operations, the method theoretically
  achieves superior approximation efficiency while reducing computational complexity.
---

# DiJiang: Efficient Large Language Models through Compact Kernelization

## Quick Facts
- arXiv ID: 2403.19928
- Source URL: https://arxiv.org/abs/2403.19928
- Reference count: 27
- Transforms vanilla Transformers into linear-complexity models with ~1/50 training cost

## Executive Summary
DiJiang introduces a frequency domain kernelization approach that enables efficient large language models through Quasi-Monte Carlo sampling and Discrete Cosine Transform operations. The method transforms pre-trained vanilla Transformers into linear-complexity models while maintaining comparable performance to standard architectures. By leveraging weighted Quasi-Monte Carlo sampling, DiJiang theoretically achieves superior approximation efficiency with minimal training costs. The approach demonstrates effectiveness across different model scales, achieving performance comparable to LLaMA2-7B while requiring significantly less computational resources.

## Method Summary
DiJiang employs a kernelization approach that transforms self-attention mechanisms from quadratic to linear complexity through frequency domain operations. The method uses weighted Quasi-Monte Carlo sampling combined with Discrete Cosine Transform (DCT) to approximate attention patterns efficiently. This kernelization process maintains the model's ability to capture long-range dependencies while significantly reducing computational overhead. The approach can be applied to pre-trained vanilla Transformers with minimal additional training, making it a practical solution for efficient inference and training of large language models.

## Key Results
- DiJiang-7B achieves comparable performance to LLaMA2-7B across various benchmarks
- Requires only about 1/50 of the training cost compared to standard models
- Maintains model accuracy while significantly accelerating inference speeds
- Demonstrates effectiveness across different model scales from 70M to 7B parameters

## Why This Works (Mechanism)
The method leverages frequency domain transformations to approximate attention mechanisms more efficiently than traditional methods. By using weighted Quasi-Monte Carlo sampling, the approach captures essential attention patterns while reducing computational complexity. The Discrete Cosine Transform operations enable efficient representation of the attention matrix in the frequency domain, allowing for linear complexity approximations. This kernelization approach maintains the expressive power of attention mechanisms while significantly reducing the computational burden, making it particularly effective for large language models where quadratic complexity becomes prohibitive.

## Foundational Learning

**Quasi-Monte Carlo Sampling**: Uses low-discrepancy sequences for more efficient sampling than random Monte Carlo methods. Needed because it provides better coverage of the sampling space, leading to more accurate approximations with fewer samples. Quick check: Verify the discrepancy properties of the QMC sequences used.

**Discrete Cosine Transform (DCT)**: Transforms data from spatial/time domain to frequency domain. Essential for converting attention matrices into a form where kernelization can be efficiently applied. Quick check: Confirm that the DCT preserves the essential information needed for attention computation.

**Kernelization Methods**: Techniques for approximating matrix operations through kernel functions. Required to transform quadratic attention operations into linear complexity approximations. Quick check: Validate the approximation error bounds for the kernelized attention.

## Architecture Onboarding

**Component Map**: Input Sequence -> QMC Sampling -> DCT Transformation -> Kernelized Attention -> Output Sequence

**Critical Path**: The sequence processing pipeline where QMC sampling and DCT operations are the critical components that determine the overall efficiency gain. The kernelized attention computation follows these transformations and represents the core of the computational savings.

**Design Tradeoffs**: 
- Accuracy vs. computational efficiency through QMC sampling density
- Frequency domain resolution vs. approximation quality
- Training cost reduction vs. potential performance degradation

**Failure Signatures**:
- Degraded performance on tasks requiring fine-grained attention patterns
- Numerical instability with extremely long sequences
- Suboptimal sampling coverage leading to attention approximation errors

**First Experiments**:
1. Benchmark performance comparison with LLaMA2-7B on standard datasets
2. Ablation study varying QMC sampling density and DCT dimensions
3. Scalability testing with sequences of increasing length (2K to 16K tokens)

## Open Questions the Paper Calls Out

None

## Limitations

- Theoretical approximation guarantees may not generalize to specialized domains or tasks requiring fine-grained attention patterns
- Reported training cost reduction assumes specific hardware configurations and may vary across computational environments
- Scalability to extremely long sequences beyond tested ranges remains uncertain

## Confidence

- **High** confidence in improved efficiency and maintained accuracy trends across model scales
- **Medium** confidence in claimed approximation superiority due to limited empirical validation across diverse attention patterns
- Uncertainty regarding numerical stability and approximation quality for sequences significantly longer than tested

## Next Checks

1. Evaluate DiJiang's performance on specialized domains (medical, legal, scientific) with domain-specific vocabulary and attention patterns to verify generalization beyond standard benchmarks

2. Conduct ablation studies varying the number of QMC samples and DCT dimensions to establish the sensitivity of performance to these hyperparameters across different model scales

3. Test numerical stability and approximation quality for sequences significantly longer than those in the original experiments (e.g., 16K+ tokens) to validate scalability claims