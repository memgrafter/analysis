---
ver: rpa2
title: Investigating Data Contamination for Pre-training Language Models
arxiv_id: '2401.06059'
source_url: https://arxiv.org/abs/2401.06059
tags:
- contamination
- data
- evaluation
- pre-training
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of data contamination on language
  model performance by deliberately adding evaluation data to the pre-training corpus.
  The authors pre-train GPT-2 models from scratch on various contaminated corpora
  to study how text and ground-truth contamination affect downstream task performance.
---

# Investigating Data Contamination for Pre-training Language Models

## Quick Facts
- arXiv ID: 2401.06059
- Source URL: https://arxiv.org/abs/2401.06059
- Reference count: 40
- Primary result: Ground-truth contamination improves language model performance more than text contamination, particularly for tasks requiring instruction understanding.

## Executive Summary
This paper investigates the impact of data contamination on language model performance by deliberately adding evaluation data to pre-training corpora. The authors systematically pre-train GPT-2 models with varying levels of text and ground-truth contamination to study how this affects downstream task performance. They find that ground-truth contamination (including prompts and desired outputs) has a more pronounced positive effect than text contamination alone, particularly for tasks requiring instruction understanding. The study also reveals a U-shaped performance trend where increasing contamination initially boosts performance but leads to degradation at higher levels, and demonstrates that common n-gram-based contamination definitions are inadequate for identifying true contamination.

## Method Summary
The authors pre-train GPT-2-small and GPT-2-large models from scratch on variously contaminated corpora derived from the Pile. They introduce contamination in two forms: text contamination (input text from evaluation datasets) and ground-truth contamination (prompts and desired outputs). Contamination factors are varied by repeating the evaluation data multiple times in the pre-training corpus. Models are evaluated on downstream tasks including SST-2, MMLU, CNN/Daily Mail, and SQuAD. The study compares performance across different contamination types, contamination factors, and examines common n-gram-based contamination definitions.

## Key Results
- Ground-truth contamination improves performance more than text contamination, especially for tasks requiring instruction understanding
- Performance follows a U-shaped trend with increasing contamination factor, initially improving then declining around factor 10
- Common n-gram-based contamination definitions fail to identify true contamination and can be easily evaded through paraphrasing

## Why This Works (Mechanism)

### Mechanism 1
Ground-truth contamination improves language model performance more than text contamination because it provides the model with both context and desired answer format during pre-training. By injecting evaluation data that includes input text, prompts, and ground truth, the model learns the mapping between prompt structure and correct responses. Core assumption: models can effectively utilize this additional context during pre-training. Break condition: if prompts or ground truth are too specific or noisy, the model might overfit.

### Mechanism 2
Repeated contamination initially improves model performance but leads to decline at higher repetition levels. As contamination factor increases, the model is exposed to evaluation data more frequently, allowing better pattern learning. However, at very high levels, the model may start overfitting to specific examples. Core assumption: there exists an optimal exposure level balancing pattern learning and overfitting avoidance. Break condition: if contamination factor is too high, the model may completely memorize evaluation data.

### Mechanism 3
N-gram-based contamination definitions are insufficient because they only detect direct text duplication and can be easily evaded by paraphrasing. These definitions rely on overlapping n-grams between pre-training corpus and evaluation data, but semantically different texts can share n-grams, and paraphrasing changes n-grams while preserving meaning. Core assumption: paraphrasing can effectively evade n-gram detection. Break condition: if evaluation data is paraphrased while preserving n-grams, contamination may still be detected.

## Foundational Learning

- Concept: Data contamination in language models
  - Why needed here: Understanding contamination is crucial for accurately assessing language model capabilities and ensuring fair evaluation
  - Quick check question: What are the two main types of data contamination discussed, and how do they differ?

- Concept: Pre-training and fine-tuning of language models
  - Why needed here: The paper investigates contamination effects at the pre-training stage, requiring understanding of pre-training versus fine-tuning
  - Quick check question: What is the main difference between pre-training and fine-tuning in language models?

- Concept: Evaluation metrics for language models
  - Why needed here: The paper uses various metrics (accuracy, F1, ROUGE, UniEval) to assess model performance on different tasks
  - Quick check question: What are the evaluation metrics used for the CNN dataset, and what do they measure?

## Architecture Onboarding

- Component map: Pre-training corpus → Contamination injection → Model training → Evaluation
- Critical path: Pre-training corpus → Contamination injection → Model training → Evaluation
- Design tradeoffs:
  - Using GPT-2-small for computational efficiency but potentially limiting model capabilities
  - Focusing on limited evaluation datasets to manage scope but potentially missing broader insights
  - Using n-gram definitions for simplicity but potentially missing complex contamination forms
- Failure signatures:
  - Poor downstream performance indicating overfitting to contaminated data
  - Inconsistent results across contamination factors showing high sensitivity
  - Failure to detect contamination using n-gram definitions suggesting paraphrased contamination
- First 3 experiments:
  1. Train GPT-2-small on clean pre-training corpus and evaluate to establish baseline
  2. Train GPT-2-small on text-contaminated corpus and compare to baseline
  3. Train GPT-2-small on ground-truth-contaminated corpus and compare to previous models

## Open Questions the Paper Calls Out

### Open Question 1
How does frequency of evaluation data in pre-training corpus affect performance beyond the observed U-shaped trend? The study only investigates up to 20 repetitions, and the threshold may depend on model and corpus size. Experiments with larger contamination factors, different model sizes, and diverse datasets would determine if the U-shaped trend is general or specific.

### Open Question 2
What are more precise and effective contamination definitions that can accurately identify ground-truth contamination? The study only evaluates existing n-gram definitions and demonstrates their limitations. New definitions rigorously tested against wide datasets and compared to contaminated/uncontaminated model performance would be needed.

### Open Question 3
How does pre-training corpus scale impact contamination effects on performance? The study only compares small and large models, not systematically investigating corpus scale relationships. Experiments across wider corpus sizes and model scales would determine how contamination effects change as corpus grows larger.

## Limitations

- Experiments limited to GPT-2 models rather than more recent large language models, potentially limiting generalizability
- Study focuses on relatively small number of evaluation datasets and tasks, potentially missing broader patterns
- Methodology does not investigate contamination effects across diverse model architectures or training paradigms

## Confidence

- Core findings: Medium - rigorous methodology with clear internal consistency, but limited scope to specific models and tasks
- Critique of n-gram definitions: High - experimental evidence clearly demonstrates inadequacy
- U-shaped trend: Medium-High - clearly observable across tasks, but inflection point may vary by configuration

## Next Checks

1. Replicate contamination experiments using larger language models (GPT-3/GPT-4) to assess generalizability to more capable architectures

2. Test n-gram contamination definitions against systematically paraphrased evaluation datasets to quantify evasion thresholds while maintaining semantic content

3. Conduct ablation studies where ground-truth contamination is introduced in controlled ways (only labels, only prompts, or both) to isolate which components drive performance improvements