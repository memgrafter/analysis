---
ver: rpa2
title: Music Era Recognition Using Supervised Contrastive Learning and Artist Information
arxiv_id: '2407.05368'
source_url: https://arxiv.org/abs/2407.05368
tags:
- music
- artist
- learning
- audio
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces music era recognition, a novel MIR task
  that predicts the release year of a song using audio and artist information. The
  authors formulate it as a classification problem and propose three variants of methods
  based on supervised contrastive learning: Audio-CNN, Audio-SUC (which incorporates
  supervised contrastive learning), and AudioArt-MMC (which adds artist information
  via a multimodal contrastive learning framework).'
---

# Music Era Recognition Using Supervised Contrastive Learning and Artist Information

## Quick Facts
- arXiv ID: 2407.05368
- Source URL: https://arxiv.org/abs/2407.05368
- Reference count: 0
- One-line primary result: AudioArt-MMC achieves 54% accuracy within 3-year tolerance using audio only, improving to 63% with artist information.

## Executive Summary
This paper introduces music era recognition as a novel MIR task that predicts song release years from audio and artist information. The authors propose three method variants based on supervised contrastive learning: Audio-CNN (baseline), Audio-SUC (adds era contrastive loss), and AudioArt-MMC (adds multimodal contrastive learning with artist biographies). The multimodal approach combines audio embeddings with artist biography embeddings using self-attention fusion and is trained with cross-entropy, era contrastive (EC), and multimodal contrastive (MMC) losses. Experiments on Million Song Dataset and an in-house dataset demonstrate 54% accuracy within 3-year tolerance using audio alone, improving to 63% when artist information is included—a 9% improvement.

## Method Summary
The authors formulate music era recognition as a classification problem using year ranges as classes. They develop three model variants: Audio-CNN as a baseline CNN-based architecture, Audio-SUC which incorporates supervised contrastive learning through era contrastive (EC) loss, and AudioArt-MMC which extends to multimodal inputs by adding artist biography embeddings. The AudioArt-MMC model uses self-attention to fuse audio and text embeddings, trained with three loss terms: cross-entropy for classification, EC loss to cluster era-specific embeddings, and MMC loss to align audio and artist embeddings. The models are trained on mel-spectrograms (224-band, 22,050 Hz) and artist biographies from AllMusic dataset.

## Key Results
- AudioArt-MMC achieves 54% accuracy within 3-year tolerance range using audio only
- Performance improves to 63% when artist information is included (9% improvement)
- Model shows robust performance on imbalanced pre-2000 data
- Achieves >90% accuracy for decade-level classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised contrastive learning improves era discrimination by forcing embeddings of songs from the same era to cluster together while pushing apart embeddings from different eras.
- Mechanism: The era contrastive (EC) loss pulls positive pairs (songs from same era) closer in embedding space and pushes negative pairs (songs from different eras) apart.
- Core assumption: Supervised contrastive loss can effectively learn discriminative features for era recognition when combined with cross-entropy loss.
- Evidence anchors: [abstract] "We adopt the supervised contrastive learning framework [8], which has shown state-of-the-art performance in image classification"; [section 2.2] "The EC loss can be defined as follows: LEC = - Σi∈I Σj∈P(i) log σ(zi·zj/τ) / Σk∈N(i) σ(zi·zk/τ)"

### Mechanism 2
- Claim: Incorporating artist biography information via multimodal contrastive learning improves era recognition by adding cultural and stylistic context that correlates with era.
- Mechanism: The MultiModal Contrastive (MMC) loss forces artist biography embeddings to align with corresponding audio embeddings while distinguishing them from mismatched pairs.
- Core assumption: Artist biography text contains meaningful stylistic and cultural information that correlates with the era of their music.
- Evidence anchors: [abstract] "For the case where the artist information is available, we extend the audio-based model to take multimodal inputs and develop a framework, called MultiModal Contrastive (MMC) learning, to enhance the training"; [section 2.3.2] "The MMC loss is then defined as: LMMC = - Σc∈C Σi∈c(i) log σ(fT(ti)·fE(si,i)/τ) / Σk∈c(i) σ(fT(ti)·fE(si,k)/τ)"

### Mechanism 3
- Claim: The combination of audio and artist embeddings through self-attention fusion captures complementary information that improves era recognition beyond either modality alone.
- Mechanism: The multi-modal fusion module uses self-attention to combine audio embeddings and artist biography embeddings into a single representation.
- Core assumption: Self-attention can effectively learn how to weight and combine audio and text embeddings for improved era classification.
- Evidence anchors: [section 2.3.1] "We follow the previous work [29] to make full use of contextual and correspondence information between music and text via attention mechanism"; [section 2.4] "AudioArt-MMC uses the multi-modal embedding for gθ to calculate LEC"

## Foundational Learning

- Concept: Supervised contrastive learning
  - Why needed here: Traditional cross-entropy loss alone is insufficient for distinguishing songs from nearby eras; contrastive learning explicitly enforces separation between different eras while pulling similar eras together.
  - Quick check question: How does supervised contrastive loss differ from standard cross-entropy loss in terms of what it optimizes?

- Concept: Multimodal representation learning
  - Why needed here: Era recognition benefits from both acoustic features (audio) and cultural context (artist information); multimodal learning can capture complementary signals that neither modality provides alone.
  - Quick check question: What are the potential advantages and challenges of combining audio and text embeddings for music classification tasks?

- Concept: Attention mechanisms for multimodal fusion
  - Why needed here: Simple concatenation of audio and text embeddings may not capture the complex relationships between acoustic features and artist style; attention allows the model to learn how to weight different aspects of each modality.
  - Quick check question: How does self-attention differ from simple weighted averaging when combining multimodal embeddings?

## Architecture Onboarding

- Component map: Mel-spectrogram → Audio encoder → Fusion module ← Text encoder ← Artist biography → Fusion module → Projection head → Contrastive losses → Backpropagation

- Critical path: Audio → Audio encoder → Fusion module ← Text encoder ← Text → Fusion module → Projection head → Contrastive losses → Backpropagation

- Design tradeoffs:
  - Using artist biographies vs. other metadata (genre, mood, instrumentation)
  - Choice of attention mechanism vs. simpler fusion methods
  - Balancing the three loss terms (cross-entropy, EC, MMC) through hyperparameters α and β

- Failure signatures:
  - Poor cross-modal alignment indicated by high MMC loss
  - Overfitting to artist information at the expense of era discrimination
  - Degradation in performance on imbalanced pre-2000 data

- First 3 experiments:
  1. Train Audio-CNN baseline to establish performance floor
  2. Train Audio-SUC with only EC loss to measure benefit of supervised contrastive learning
  3. Train AudioArt-MMC with both EC and MMC losses to evaluate multimodal contribution

## Open Questions the Paper Calls Out
- How does the performance of music era recognition models change when incorporating additional metadata such as instrumentation, genre, and mood, beyond audio and artist biography?
- How do the proposed methods perform on datasets with more balanced era distributions, and what adjustments are necessary to maintain accuracy?
- Can the multi-modal contrastive learning framework be effectively adapted for real-time music era recognition applications, and what are the computational trade-offs?

## Limitations
- The specific CNN architecture details are not fully specified, making exact reproduction difficult
- The temperature parameter τ for contrastive losses is not mentioned, which could affect training stability
- Evaluation uses only MSD and an in-house dataset, limiting generalizability

## Confidence
- High confidence: The baseline Audio-CNN performance metrics and decade-level classification results (>90% accuracy) are well-supported by experimental data
- Medium confidence: The 9% improvement from multimodal fusion is significant but based on a single dataset split; results may vary with different data distributions
- Low confidence: The mechanism by which artist biographies specifically improve era recognition is not empirically validated—the correlation could be indirect or dataset-specific

## Next Checks
1. Implement and train the exact Audio-CNN baseline using only the specified MSD dataset to verify baseline performance matches reported results
2. Systematically remove each loss component (cross-entropy, EC, MMC) to quantify their individual contributions to the final performance
3. Evaluate AudioArt-MMC on an independent music dataset with different genre distributions to test robustness of the multimodal improvement claim