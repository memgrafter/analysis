---
ver: rpa2
title: Contribution-based Low-Rank Adaptation with Pre-training Model for Real Image
  Restoration
arxiv_id: '2408.01099'
source_url: https://arxiv.org/abs/2408.01099
tags:
- colora
- tasks
- prod
- data
- full
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of efficient parameter tuning
  in real image restoration tasks. The authors propose a novel approach called Contribution-based
  Low-Rank Adaptation (CoLoRA) with Pre-training with Random Order Degradation (PROD).
---

# Contribution-based Low-Rank Adaptation with Pre-training Model for Real Image Restoration

## Quick Facts
- arXiv ID: 2408.01099
- Source URL: https://arxiv.org/abs/2408.01099
- Reference count: 40
- Primary result: State-of-the-art performance on 6 real-world image restoration tasks using only ~7% of total network parameters

## Executive Summary
This paper addresses the challenge of efficient parameter tuning in real image restoration tasks by proposing Contribution-based Low-Rank Adaptation (CoLoRA) with Pre-training with Random Order Degradation (PROD). The authors develop a parameter-efficient fine-tuning approach that leverages LoRA with adaptive layer-by-layer capacity determined by contribution analysis, combined with a novel pre-training strategy using randomly ordered degradations. Their method achieves state-of-the-art performance on six real-world image restoration tasks while requiring only about 7% of the total network parameters for fine-tuning, significantly reducing memory usage compared to full fine-tuning.

## Method Summary
The authors propose a two-stage approach: PROD pre-training followed by CoLoRA fine-tuning. PROD pre-trains models on synthetic degraded images created by randomly applying 1-6 degradations in random order from a set of 5 degradation functions, generating approximately 137K different degradation types. For fine-tuning, CoLoRA uses LoRA decomposition with adaptive low-rank matrices (A and B) where each layer's rank r is determined by its contribution score measured via Filter Attribution method based on Integral Gradient (FAIG). The method freezes pre-trained weights and only updates adapter parameters, bias layers, and normalization layers, achieving significant memory efficiency while maintaining or improving performance across rain, raindrop, rain&raindrop, haze, and blur restoration tasks.

## Key Results
- Achieves state-of-the-art PSNR performance on 6 real-world image restoration tasks using both CNN-based NAFNet and transformer-based Restormer architectures
- Requires only ~7% of total network parameters for fine-tuning compared to full fine-tuning (2M vs 29M parameters for NAFNet)
- PROD pre-training with random order degradations outperforms single degradation and fixed-order pre-training strategies
- Demonstrates superior performance with limited training data and fewer learnable parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Contribution-based low-rank adaptation (CoLoRA) efficiently tunes image restoration models by allocating learnable parameters based on layer-specific contributions.
- **Mechanism:** The method uses Filter Attribution method based on Integral Gradient (FAIG) to quantify each layer's contribution for a given task. Layers with higher FAIG scores receive larger low-rank matrices (higher r values) in the LoRA decomposition, while less important layers use smaller r values. This adaptive allocation maintains performance while reducing total learnable parameters from 100% to ~7%.
- **Core assumption:** Layer importance varies significantly across different restoration tasks, and FAIG scores accurately reflect this importance.
- **Evidence anchors:** [abstract] "CoLoRA adaptively tunes small amounts of parameters for each new vision task by leveraging LoRA and a contribution-based method to determine layer-by-layer capacity"

### Mechanism 2
- **Claim:** Pre-training with Random Order Degradation (PROD) creates a more robust foundation model by exposing it to diverse degradation combinations.
- **Mechanism:** Instead of applying single degradations or fixed degradation sequences, PROD randomly combines 1 to N degradations from a set of 5 degradation functions, creating approximately 137K different degradation types. This expanded pre-text task improves the model's ability to generalize from synthetic pre-training to real-world complex degradations.
- **Core assumption:** Random combinations of degradations better represent real-world complex degradations than single or fixed-order degradations.
- **Evidence anchors:** [abstract] "Our PROD strategy allows to extend the capability of pre-trained models with improved performance as well as robustness to bridge synthetic pre-training and real-world fine-tuning"

### Mechanism 3
- **Claim:** Freezing pre-trained weights and only updating adapter parameters reduces memory usage without sacrificing performance.
- **Mechanism:** The pre-trained model weights remain frozen during fine-tuning. Only the adapter matrices (A and B in LoRA decomposition) and bias/normalization layers are updated. This reduces memory requirements from storing full model weights per task to storing only small adapter parameters (2M vs 29M for NAFNet).
- **Core assumption:** The pre-trained model has learned general features that transfer well across image restoration tasks, making fine-tuning of only adapter parameters sufficient.
- **Evidence anchors:** [abstract] "CoLoRA requires only about 7% of the total network parameters for fine-tuning, significantly reducing memory usage compared to full fine-tuning"

## Foundational Learning

- **Concept:** Low-rank matrix decomposition
  - Why needed here: CoLoRA uses LoRA's low-rank decomposition (W0 + BA) to efficiently approximate weight updates while reducing parameter count
  - Quick check question: What is the mathematical relationship between rank r, matrix dimensions d and k, and parameter count in LoRA decomposition?

- **Concept:** Gradient-based attribution methods
  - Why needed here: FAIG scores rely on Integrated Gradients to quantify each layer's contribution to task performance
  - Quick check question: How does Integrated Gradients differ from simple gradient magnitude in measuring feature importance?

- **Concept:** Pre-training paradigms for low-level vision
  - Why needed here: Understanding how synthetic degradation functions bridge the gap between synthetic pre-training and real-world fine-tuning
  - Quick check question: What are the key differences between pre-training approaches for high-level vision (classification) vs low-level vision (restoration)?

## Architecture Onboarding

- **Component map:** Pre-trained backbone (NAFNet or Restormer) -> CoLoRA adapter layers (per-layer LoRA matrices A and B) -> Bias and normalization layers -> PROD pre-training module (synthetic degradation pipeline)

- **Critical path:** PROD pre-training -> FAIG contribution analysis -> CoLoRA adapter configuration -> fine-tuning on target task

- **Design tradeoffs:** Parameter efficiency vs. performance, memory usage vs. inference speed, pre-training diversity vs. task specificity

- **Failure signatures:** Performance degradation on novel tasks, memory bottlenecks during fine-tuning, slow convergence on certain degradation types

- **First 3 experiments:**
  1. Reproduce FAIG analysis on a simple restoration task to verify layer importance rankings
  2. Test CoLoRA with fixed vs. adaptive r values on a single degradation type
  3. Compare PROD pre-training vs. single/fixed degradation pre-training on a small real dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CoLoRA with PROD scale with the number of tasks and the diversity of degradation types?
- Basis in paper: [explicit] The paper mentions that CoLoRA is more efficient than full-tuning but faces the limitation of increasing parameters as the number of tasks expands.
- Why unresolved: The paper only tests CoLoRA with PROD on 6 real-world image restoration tasks. The scalability to a larger number of tasks with more diverse degradation types is not explored.
- What evidence would resolve it: Experimental results demonstrating the performance and parameter efficiency of CoLoRA with PROD on a significantly larger set of tasks with diverse degradation types, such as 20+ tasks covering a wide range of image restoration problems.

### Open Question 2
- Question: Can the contribution-based layer capacity determination in CoLoRA be further improved by incorporating task-specific information beyond FAIG scores?
- Basis in paper: [explicit] The paper uses FAIG scores to quantify the contribution of each layer to a task and adjusts the learnable parameters accordingly. It is mentioned that FAIG values are measured for each block using the full fine-tuned NAFNet on real IR tasks.
- Why unresolved: The paper only uses FAIG scores, which are based on the pre-trained and fine-tuned models. It does not explore incorporating other task-specific information, such as degradation statistics or task difficulty, into the layer capacity determination.
- What evidence would resolve it: Experimental results comparing the performance of CoLoRA with PROD using FAIG scores alone versus incorporating additional task-specific information into the layer capacity determination.

### Open Question 3
- Question: How does the performance of CoLoRA with PROD compare to other parameter-efficient fine-tuning methods, such as prompt tuning or adapter-based methods, for image restoration tasks?
- Basis in paper: [inferred] The paper focuses on LoRA-based parameter-efficient fine-tuning and does not compare its performance to other parameter-efficient methods like prompt tuning or adapter-based methods.
- Why unresolved: The paper only compares CoLoRA with PROD to full fine-tuning and DegAE with full fine-tuning. It does not explore how it fares against other parameter-efficient fine-tuning methods that have been successful in other domains like NLP and high-level vision.
- What evidence would resolve it: Experimental results comparing the performance of CoLoRA with PROD to other parameter-efficient fine-tuning methods, such as prompt tuning or adapter-based methods, on a range of image restoration tasks with varying levels of data availability.

## Limitations

- The paper's claims rely heavily on proposed methods evaluated primarily through ablation studies and comparisons within the same framework
- FAIG-based layer contribution analysis lacks external validation or comparison to alternative attribution methods
- PROD pre-training effectiveness is demonstrated mainly through downstream task performance rather than analysis of learned representations
- Computational efficiency claims are based on parameter counts rather than comprehensive memory and time profiling

## Confidence

- **High confidence**: LoRA-based parameter-efficient fine-tuning approach is well-established in both NLP and vision domains, and specific implementation details follow established patterns
- **Medium confidence**: PROD pre-training strategy's effectiveness is supported by empirical results but lacks theoretical justification or comparison to alternative pre-training strategies
- **Medium confidence**: FAIG-based layer contribution analysis is plausible but not extensively validated against other attribution methods

## Next Checks

1. **Attribution method validation**: Compare FAIG-based layer importance rankings with alternative attribution methods (e.g., Grad-CAM, Integrated Gradients with different baselines) on the same tasks to verify the robustness of the contribution analysis

2. **Pre-training strategy ablation**: Conduct controlled experiments comparing PROD pre-training with alternative strategies (single degradation, fixed-order multi-degradation) while keeping all other factors constant to isolate the impact of the random order approach

3. **Memory and speed profiling**: Measure actual GPU memory usage and inference speed during fine-tuning and inference for CoLoRA vs. full fine-tuning across different hardware configurations to validate the claimed efficiency improvements beyond parameter count reduction