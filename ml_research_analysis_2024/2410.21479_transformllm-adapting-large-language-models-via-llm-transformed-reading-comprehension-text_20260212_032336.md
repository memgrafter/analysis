---
ver: rpa2
title: 'TransformLLM: Adapting Large Language Models via LLM-Transformed Reading Comprehension
  Text'
arxiv_id: '2410.21479'
source_url: https://arxiv.org/abs/2410.21479
tags:
- legal
- language
- data
- arxiv
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a method to adapt large language models (LLMs)
  to specialized domains using continued pre-training on LLM-transformed reading comprehension
  text. The authors develop Phi-2-Legal and Mistral-Legal-7B, legal domain models
  based on Phi-2 and Mistral-7B, which are further trained on over 500 million tokens
  of legal text.
---

# TransformLLM: Adapting Large Language Models via LLM-Transformed Reading Comprehension Text

## Quick Facts
- arXiv ID: 2410.21479
- Source URL: https://arxiv.org/abs/2410.21479
- Reference count: 37
- Primary result: LLM-transformed reading comprehension enables effective domain adaptation with minimal computational resources

## Executive Summary
This paper introduces TransformLLM, a method for adapting large language models to specialized domains using continued pre-training on LLM-transformed reading comprehension text. The authors demonstrate their approach by developing Phi-2-Legal and Mistral-Legal-7B, legal domain models that achieve superior performance on legal benchmarks compared to base models and other legal models trained on larger datasets. The key innovation lies in using LLMs to convert raw legal text into reading comprehension materials with questions and answers, thereby enhancing domain-specific knowledge while preserving general language capabilities. The approach achieves high performance with relatively low computational resources, requiring only 500 million tokens of legal text for effective adaptation.

## Method Summary
The TransformLLM method employs a two-stage approach for domain adaptation. First, raw legal text is processed by an LLM to generate reading comprehension materials, including questions and answers derived from the source content. This transformation creates a structured format that emphasizes key legal concepts and relationships. Second, the base models (Phi-2 and Mistral-7B) undergo continued pre-training on the transformed text corpus. The authors use a sliding window approach to handle long legal documents and implement a two-stage training strategy: initial training on the transformed corpus followed by fine-tuning on selected legal tasks. This approach enables the models to acquire specialized legal knowledge while maintaining their general language understanding capabilities.

## Key Results
- Phi-2-Legal and Mistral-Legal-7B outperform base models and other legal models on MMLU legal subsets and LexGLUE benchmarks
- Models achieve superior performance despite being trained on fewer tokens compared to competing legal models
- The approach demonstrates effectiveness in domain adaptation while preserving general language capabilities

## Why This Works (Mechanism)
The method works by leveraging the structured nature of reading comprehension materials to enhance domain-specific knowledge acquisition. When LLMs transform raw legal text into question-answer pairs, they naturally highlight key concepts, relationships, and important details within the legal domain. This transformation process acts as a form of knowledge distillation, where the LLM's understanding of legal concepts is encoded into a more accessible format for continued pre-training. The reading comprehension structure also encourages the model to develop stronger reasoning capabilities within the legal domain, as it must understand not just the content but also the relationships and implications expressed in the questions and answers.

## Foundational Learning
- **Continued Pre-training**: Fine-tuning a pre-trained model on domain-specific data to enhance specialized knowledge while maintaining general capabilities
  - Why needed: Base LLMs lack domain-specific knowledge required for specialized tasks
  - Quick check: Compare performance on domain tasks before and after continued pre-training

- **Reading Comprehension Generation**: Using LLMs to convert raw text into structured Q&A format
  - Why needed: Transforms unstructured domain text into a format that emphasizes key concepts and relationships
  - Quick check: Measure the quality and relevance of generated questions and answers

- **Domain Adaptation Trade-offs**: Balancing between preserving general capabilities and acquiring specialized knowledge
  - Why needed: Models must maintain broad utility while gaining domain expertise
  - Quick check: Evaluate performance on both domain-specific and general tasks

## Architecture Onboarding

**Component Map**: Raw Legal Text -> LLM Transformation -> Reading Comprehension Corpus -> Continued Pre-training -> Domain-Specific Model

**Critical Path**: The transformation step is critical as it determines the quality and structure of the training data. The effectiveness of the entire pipeline depends on the LLM's ability to generate meaningful questions and answers that capture essential legal concepts.

**Design Tradeoffs**: The authors chose to prioritize efficiency by using a relatively small corpus (500M tokens) compared to other legal models. This tradeoff enables faster training and lower computational costs but may limit the depth of domain knowledge acquisition compared to models trained on larger datasets.

**Failure Signatures**: Poor quality in the LLM-transformed text would manifest as models failing to answer basic legal questions or showing inconsistent performance across different legal sub-domains. Overfitting to the transformed format rather than actual legal knowledge would be evident in poor generalization to real-world legal tasks.

**First Experiments**:
1. Compare model performance on legal benchmarks using raw legal text versus LLM-transformed text
2. Evaluate the quality of generated questions and answers using human assessment and automated metrics
3. Test model performance on out-of-distribution legal tasks to assess generalization capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Proprietary nature of training data limits reproducibility and assessment of data quality impact
- Evaluation restricted to legal benchmarks, constraining generalizability to other domains
- Absence of ablation studies makes it difficult to isolate the contribution of each component

## Confidence
- High confidence in the general methodology of using LLM-transformed reading comprehension for domain adaptation
- Medium confidence in the specific legal domain performance claims due to limited benchmark comparisons
- Low confidence in the scalability claims without cross-domain validation

## Next Checks
1. Conduct cross-domain experiments to validate whether the LLM-transformed reading comprehension approach maintains its effectiveness when applied to non-legal domains with varying data availability levels

2. Perform ablation studies comparing models trained on: raw legal text only, LLM-transformed text only, combined raw and transformed text, and different LLM transformation strategies

3. Test the models on out-of-distribution legal tasks and open-ended legal reasoning scenarios to assess the practical utility beyond benchmark performance