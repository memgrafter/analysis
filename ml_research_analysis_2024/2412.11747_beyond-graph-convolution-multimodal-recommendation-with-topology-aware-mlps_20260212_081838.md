---
ver: rpa2
title: 'Beyond Graph Convolution: Multimodal Recommendation with Topology-aware MLPs'
arxiv_id: '2412.11747'
source_url: https://arxiv.org/abs/2412.11747
tags:
- tmlp
- item
- multimodal
- graph
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the scalability and oversmoothing issues
  of graph convolutional networks (GCNs) in multimodal recommender systems (MMRS).
  It proposes Topology-aware Multi-Layer Perceptron (TMLP), which replaces GCNs with
  MLPs and introduces two key components: Topological Pruning Strategy (TPS) for denoising
  item relations and Intra (Inter)-Modality Learning (IML) for capturing multimodal
  correlations.'
---

# Beyond Graph Convolution: Multimodal Recommendation with Topology-aware MLPs

## Quick Facts
- arXiv ID: 2412.11747
- Source URL: https://arxiv.org/abs/2412.11747
- Reference count: 21
- Outperforms GCN-based models with up to 7.53% improvement in Recall@10

## Executive Summary
This paper addresses the scalability and oversmoothing issues of graph convolutional networks (GCNs) in multimodal recommender systems. The authors propose Topology-aware Multi-Layer Perceptron (TMLP), which replaces GCNs with MLPs and introduces two key components: Topological Pruning Strategy (TPS) for denoising item relations and Intra (Inter)-Modality Learning (IML) for capturing multimodal correlations. TMLP achieves significant improvements over nine baseline models on three real-world datasets, demonstrating superior training efficiency and robustness against noisy data.

## Method Summary
TMLP replaces GCNs with MLPs while preserving topology awareness through mutual information maximization and a novel topological pruning strategy. The method first computes modality-specific similarity matrices for visual and textual features, then denoises item relations using TPS based on topological similarity rather than raw modality similarity. IML captures both within-modality and cross-modality correlations through MINE-based mutual information maximization. The model is trained with a combination of Bayesian Personalized Ranking (BPR) loss and a neighborhood alignment (NA) loss, achieving significant improvements in recommendation performance while maintaining computational efficiency.

## Key Results
- TMLP outperforms nine baseline models on Amazon Baby, Sports, and Electronics datasets
- Achieves up to 7.53% improvement in Recall@10 and 5.69% in Recall@20 on Electronics dataset
- Demonstrates superior training efficiency with faster convergence compared to GCN-based approaches
- Shows better robustness against noisy data with consistent performance across different sampling parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing GCNs with MLPs avoids oversmoothing while preserving discriminative power in multimodal recommendations.
- Mechanism: MLPs process multimodal features independently without message passing, preventing node features from becoming too similar across the graph. This maintains distinct item embeddings while still learning topology through mutual information maximization with contextual neighborhoods.
- Core assumption: High-dimensional multimodal data can be effectively modeled by MLPs when augmented with topology-aware training objectives.
- Evidence anchors:
  - [abstract] "by discarding the internal message passing in GCNs, which is sensitive to node connections, TMLP achieves significant improvements"
  - [section] "TMLP enhances MLPs with topological pruning to denoise item-item relations and intra (inter)-modality learning to integrate higher-order modality correlations"
- Break condition: If multimodal correlations are too complex for MLP architecture or if topology cannot be effectively injected through mutual information objectives.

### Mechanism 2
- Claim: Topological Pruning Strategy (TPS) effectively denoises item relations by filtering based on topological similarity rather than raw modality similarity.
- Mechanism: TPS computes topological similarity between nodes using neighborhood overlap probabilities, then retains only the top-K most topologically similar neighbors for each node. This filters out noisy connections that arise from modality incompatibility.
- Core assumption: Topologically similar items (those sharing many common neighbors) are more likely to be genuinely similar than items with high modality similarity scores alone.
- Evidence anchors:
  - [section] "TPS refines item relations by assessing topological similarity between nodes"
  - [section] "if randomly selected nodes frequently fall within the neighborhoods of both nodes m and n, these nodes are likely to be topologically similar"
- Break condition: If topological similarity does not correlate with actual item similarity in the dataset, or if the sampling process introduces bias.

### Mechanism 3
- Claim: Intra (Inter)-Modality Learning (IML) captures both within-modality and cross-modality correlations through mutual information maximization.
- Mechanism: IML uses Mutual Information Neural Estimation (MINE) to maximize mutual information between node representations and their contextual neighborhoods. This aligns features of adjacent items within the same modality (intra) and similar items across different modalities (inter).
- Core assumption: Mutual information maximization between nodes and their neighbors effectively captures semantic similarity that mirrors GCN message passing.
- Evidence anchors:
  - [section] "TMLP employs Intra (Inter)-Modality Learning (IML) to learn topology-aware item representations by maximizing mutual information between an item's hidden representation and its neighborhood"
  - [section] "By maximizing the mutual information between the representations of adjacent items, we align them to be semantically similar, which mirrors the underlying mechanics of GCNs"
- Break condition: If mutual information estimation through MINE fails to capture the relevant correlations or if the temperature parameter τ is poorly chosen.

## Foundational Learning

- Concept: Graph Neural Networks and Over-smoothing
  - Why needed here: Understanding why GCNs fail in multimodal settings due to oversmoothing is crucial for appreciating TMLP's design choice to replace GCNs with MLPs.
  - Quick check question: What happens to node embeddings in GCNs as the number of layers increases, and why does this limit their effectiveness for multimodal data?

- Concept: Mutual Information and MINE
  - Why needed here: IML relies on maximizing mutual information between node representations and their neighborhoods, which is implemented through MINE. Understanding this relationship is essential for implementing and debugging TMLP.
  - Quick check question: How does MINE transform mutual information maximization into a tractable optimization problem, and what role does the temperature parameter play?

- Concept: Multimodal Feature Fusion and Compatibility
  - Why needed here: TMLP combines visual and textual features from pre-trained models, which may have different scales and distributions. Understanding feature fusion techniques and modality compatibility issues is crucial for effective implementation.
  - Quick check question: What are the potential challenges when fusing visual and textual features from different pre-trained models, and how might these affect item-item similarity computation?

## Architecture Onboarding

- Component map:
  Input: Pre-trained visual features (4096-dim), pre-trained textual features (384-dim), user-item interaction graph
  TPS: Constructs modality-specific similarity matrices, computes topological similarity, prunes to get denoised item relations
  IML: MLPs for each modality, Fuser for cross-modality correlation, Mutual Information maximization with NA loss
  CF backbone: LightGCN-style aggregation of multimodal and ID-based information
  Output: User and item embeddings for ranking

- Critical path:
  1. Preprocess: Compute modality-specific similarity matrices (Av, At)
  2. TPS: Denoise item relations to get topological dependencies (Ā)
  3. IML: Process each modality through MLPs, fuse with cross-modality correlation, optimize with NA loss
  4. CF: Aggregate multimodal and interaction-based representations
  5. Prediction: Inner product for ranking

- Design tradeoffs:
  - MLP depth vs. overfitting: Deeper MLPs can capture more complex correlations but may overfit on smaller datasets
  - Sampling size K vs. noise filtering: Larger K includes more neighbors but may introduce noise; smaller K is more selective but may miss important connections
  - TPS precomputation vs. adaptability: TPS is computed once before training (efficient) but cannot adapt to the specific task

- Failure signatures:
  - Performance degradation with increasing MLP layers (oversmoothing not fully avoided)
  - Sensitivity to TPS parameters (K too small misses connections, too large includes noise)
  - NA loss weight α not properly tuned (too small underfits correlations, too large harms recommendation loss)

- First 3 experiments:
  1. Verify TPS denoising: Compare item-item similarity distributions before and after TPS on a small subset of data
  2. Test IML effectiveness: Measure mutual information between node representations and neighborhoods with and without NA loss
  3. Validate multimodal fusion: Evaluate recommendation performance using only visual, only textual, and both modalities to confirm complementarity

## Open Questions the Paper Calls Out
None

## Limitations
- TPS is computed once before training, limiting adaptability to specific recommendation tasks
- The method's effectiveness depends on the assumption that topological similarity correlates with actual item similarity
- The exact implementation of the fuser component for cross-modality correlation is unspecified

## Confidence
- **High Confidence**: Claims about improved scalability and training efficiency compared to GCNs (supported by explicit timing comparisons and theoretical analysis of message-passing complexity)
- **Medium Confidence**: Claims about superior performance on specific datasets (results are strong but may not generalize beyond tested domains)
- **Medium Confidence**: Claims about robustness to noisy data (demonstrated through experiments but mechanism requires deeper validation)

## Next Checks
1. **Cross-domain generalization test**: Evaluate TMLP on datasets from different domains (e.g., Yelp, MovieLens) to verify performance claims are not dataset-specific and to test the universality of the topological similarity assumption

2. **Ablation study on TPS parameters**: Systematically vary the sampling parameter K and measure its impact on both recommendation performance and computational efficiency to determine optimal settings and validate the denoising effect

3. **Qualitative analysis of denoised relations**: Visually inspect or statistically analyze the item-item similarity distributions before and after TPS to verify that denoising preserves meaningful connections while removing noise, and that the topological pruning strategy is working as intended