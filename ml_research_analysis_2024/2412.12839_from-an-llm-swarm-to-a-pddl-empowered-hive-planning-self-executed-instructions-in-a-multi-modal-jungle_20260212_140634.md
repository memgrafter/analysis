---
ver: rpa2
title: 'From An LLM Swarm To A PDDL-Empowered HIVE: Planning Self-Executed Instructions
  In A Multi-Modal Jungle'
arxiv_id: '2412.12839'
source_url: https://arxiv.org/abs/2412.12839
tags:
- task
- tasks
- actions
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hive is a knowledge-aware planning system for multi-modal tasks
  that integrates a Capability Knowledge Graph (C-KG) and PDDL-based formal logic
  to select and orchestrate multiple deep learning models. It parses natural language
  queries, decomposes them into atomic actions, selects appropriate models based on
  user constraints and benchmark performance, and executes plans while providing explanations.
---

# From An LLM Swarm To A PDDL-Empowered HIVE: Planning Self-Executed Instructions In A Multi-Modal Jungle

## Quick Facts
- arXiv ID: 2412.12839
- Source URL: https://arxiv.org/abs/2412.12839
- Reference count: 40
- Key outcome: HIVE outperforms HuggingGPT and ControlLLM with 30% higher task selection accuracy, 100% constraint adherence, and superior cross-modality performance on the MUSE benchmark.

## Executive Summary
Hive is a knowledge-aware planning system for multi-modal tasks that integrates a Capability Knowledge Graph (C-KG) and PDDL-based formal logic to select and orchestrate multiple deep learning models. It parses natural language queries, decomposes them into atomic actions, selects appropriate models based on user constraints and benchmark performance, and executes plans while providing explanations. The system is evaluated on the new MUSE benchmark containing 100 multi-modal, multi-task queries. Hive outperforms baselines HuggingGPT and ControlLLM with 30% higher task selection accuracy, 100% constraint adherence, and superior cross-modality performance. A quantized HIVE light version with 7B models also surpasses baselines, demonstrating efficiency without sacrificing performance.

## Method Summary
HIVE uses a Capability Knowledge Graph (C-KG) to map models to tasks, PDDL-based formal logic for planning, and LLM-based parsing/classification for query decomposition. The system parses user queries into structured components, classifies relevant PDDL domains, merges them into a unified domain file, and uses Best First Width Search to compute a detailed, ordered plan of actions before execution. During planning, the system selects models based on user constraints (license, size) and performance metrics from the C-KG. The C-KG is built from HuggingFace model cards and Papers With Code, capturing each model's supported tasks, benchmark scores, and execution code snippets. Quantized models in HIVE light maintain performance while reducing latency and computational cost.

## Key Results
- 30% higher task selection accuracy compared to baselines
- 100% constraint adherence in model selection
- HIVE light with 7B models surpasses baselines in efficiency without sacrificing performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PDDL planning ensures correct task decomposition and sequence, leading to higher task selection accuracy and better constraint adherence.
- Mechanism: The system parses natural language queries into structured components, classifies relevant PDDL domains, merges them into a unified domain file, and uses Best First Width Search to compute a detailed, ordered plan of actions before execution. This formal planning step replaces unreliable LLM-only reasoning.
- Core assumption: Formal PDDL domains can be mapped from user queries with sufficient precision to capture task dependencies and constraints.
- Evidence anchors:
  - [abstract] "Our system is capable of planning complex chains of actions while guaranteeing explainability, using an LLM-based formal logic backbone empowered by PDDL operations."
  - [section 3.2.1] Describes the parsing-rephrasing, domain classification, action selection, and PDDL problem reconstruction steps with Best First Width Search.
  - [corpus] Weak. No direct corpus evidence provided; this is inferred from the paper's description of the PDDL-based planner.
- Break condition: If the mapping from user intent to PDDL domain/actions is imprecise, the formal plan may be infeasible or incorrect, negating the benefit of PDDL.

### Mechanism 2
- Claim: The Capability Knowledge Graph (C-KG) enables optimal model selection by linking models to tasks and benchmark performance, improving final output quality.
- Mechanism: The C-KG is built from HuggingFace model cards and Papers With Code, capturing each model's supported tasks, benchmark scores, and execution code snippets. During planning, the system selects models based on user constraints (license, size) and performance metrics from the C-KG.
- Core assumption: Model performance metrics in the C-KG are reliable predictors of task-specific effectiveness.
- Evidence anchors:
  - [abstract] "Hive outperforms baselines HuggingGPT and ControlLLM with 30% higher task selection accuracy, 100% constraint adherence, and superior cross-modality performance."
  - [section 3.1] Details C-KG construction: model card extraction, OpenIE for structured representation, alignment with Papers With Code for benchmark performance, and code snippet storage.
  - [corpus] Weak. No direct corpus evidence provided; the claim is based on the paper's own evaluation.
- Break condition: If benchmark performance does not correlate well with real-world task performance, or if the C-KG is incomplete, model selection will be suboptimal.

### Mechanism 3
- Claim: Quantized models in HIVE light maintain performance while reducing latency and computational cost.
- Mechanism: HIVE light uses 8-bit quantized versions of InterLM2.5-7B-chat and Mistral-7B-Instruct-v0.3, achieving similar or better results than larger, non-quantized baselines (HuggingGPT, ControlLLM) on task selection, flow of thought, and final output metrics.
- Core assumption: Quantization preserves model reasoning capabilities sufficiently for the tasks in MUSE.
- Evidence anchors:
  - [abstract] "A quantized HIVE light version with 7B models also surpasses baselines, demonstrating efficiency without sacrificing performance."
  - [section 4.3] Shows HIVE light outperforming HuggingGPT and ControlLLM across all metrics, including multi-task scenarios.
  - [corpus] Weak. No direct corpus evidence provided; the claim is from the paper's own results.
- Break condition: If quantization degrades reasoning quality beyond a threshold, performance will drop, especially on complex multi-task queries.

## Foundational Learning

- Concept: PDDL (Planning Domain Definition Language)
  - Why needed here: PDDL provides a formal framework to define planning domains and problems, enabling systematic task decomposition and plan generation.
  - Quick check question: What are the core components of a PDDL domain file (predicates, actions, objects)?

- Concept: Knowledge Graphs
  - Why needed here: Knowledge graphs structure complex relationships between models, tasks, and performance data, enabling efficient retrieval and reasoning for model selection.
  - Quick check question: How does a knowledge graph differ from a relational database in representing model capabilities?

- Concept: Prompt Engineering
  - Why needed here: Prompt engineering guides LLMs to parse user queries, classify domains, and select actions accurately within the system's workflow.
  - Quick check question: What is the difference between few-shot and zero-shot prompting, and why is few-shot used for domain classification here?

## Architecture Onboarding

- Component map: User query → Parser & Rephraser → Domain Classifier → Action Selector → PDDL Problem → Best First Width Search Planner → Model Selector (using C-KG) → Code Snippet Mapper → Executor
- Critical path: Query parsing → PDDL planning → Model selection → Plan execution
- Design tradeoffs: Formal PDDL planning adds upfront latency but ensures correctness; quantized models reduce resource use but may limit reasoning depth; C-KG construction is labor-intensive but enables precise model selection
- Failure signatures: Incorrect task selection (TS=0) indicates parsing/domain classification failure; broken task flow (FoT=0) suggests PDDL planning error; failed final output (O=0) points to model execution or code snippet mapping issues
- First 3 experiments:
  1. Run a simple single-task query (e.g., text summarization) end-to-end to verify parsing, planning, and execution.
  2. Test model selection constraints (e.g., license filter) to confirm C-KG filtering works.
  3. Execute a multi-task query (e.g., transcribe audio then classify text) to validate PDDL sequencing and cross-modality handling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would Hive perform on real-world tasks involving rare or niche domains not covered by the current Capability Knowledge Graph (C-KG)?
- Basis in paper: [inferred] The paper mentions that the C-KG currently contains over 100 tasks and that Hive could be extended to include adapter nodes for specialized models. However, the paper does not provide empirical evidence of Hive's performance on truly rare or niche tasks.
- Why unresolved: The paper's evaluation focuses on the MUSE benchmark, which covers common tasks like ASR, NER, and image generation. It does not test Hive's ability to handle truly specialized domains like protein folding or other rare scenarios mentioned in the paper.
- What evidence would resolve it: Testing Hive on a benchmark specifically designed to include rare and niche tasks, and comparing its performance to baselines on these tasks.

### Open Question 2
- Question: What is the trade-off between the granularity of the NLP task taxonomy in the C-KG and the efficiency of Hive's planning and execution?
- Basis in paper: [explicit] The paper describes a fine-grained taxonomy with 420 concepts over 3 hierarchical levels, created to structure PDDL domain files. However, it does not discuss the computational cost or impact on planning efficiency of such a detailed taxonomy.
- Why unresolved: While a detailed taxonomy may improve task classification accuracy, it could also increase the complexity of the planning process and potentially slow down Hive's response time.
- What evidence would resolve it: Empirical studies comparing Hive's performance with different levels of taxonomy granularity, measuring both accuracy and response time.

### Open Question 3
- Question: How does Hive's performance scale when dealing with a significantly larger number of models and tasks than those in the current C-KG?
- Basis in paper: [inferred] The paper mentions that the C-KG was built from over 600,000 models on HuggingFace, but only 26,806 were retained. It also discusses the potential for extending the C-KG with adapter nodes. However, it does not provide data on how Hive's performance changes as the number of models and tasks increases.
- Why unresolved: As the number of available models grows, the complexity of the C-KG increases, which could impact Hive's ability to efficiently select the most appropriate models and plan tasks.
- What evidence would resolve it: Experiments testing Hive's performance on benchmarks with varying numbers of models and tasks, measuring accuracy, response time, and resource usage.

## Limitations

- Lack of direct empirical evidence for core mechanisms beyond self-reported results on the MUSE benchmark
- C-KG construction process described but not validated for completeness or accuracy against ground truth model capabilities
- Quantization performance claims based on a single model configuration without exploring degradation thresholds across different architectures

## Confidence

- **High confidence**: Task selection accuracy (30% improvement) and constraint adherence (100%) metrics, as these are directly measurable and reported with specific values.
- **Medium confidence**: Cross-modality performance claims, as these depend on the quality and representativeness of the MUSE benchmark, which is not independently validated.
- **Low confidence**: Claims about C-KG's predictive power for model selection and quantization's preservation of reasoning capabilities, as these rely on indirect evidence and lack comparative analysis with other model selection strategies or quantization levels.

## Next Checks

1. Conduct an ablation study comparing HIVE's performance with and without PDDL planning on a subset of MUSE queries to isolate the planning component's contribution.
2. Validate the C-KG's model-to-task mappings by testing model selection accuracy on a held-out set of known model-task pairs from external sources.
3. Evaluate quantized HIVE light across multiple quantization levels and model architectures to establish the performance degradation threshold and generalizability of efficiency claims.