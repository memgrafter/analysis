---
ver: rpa2
title: Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning
  -- A Convex Optimization Perspective
arxiv_id: '2410.15483'
source_url: https://arxiv.org/abs/2410.15483
tags:
- sequential
- arxiv
- maxright
- alright
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the suboptimality of sequential training in
  large language model (LLM) post-training, where supervised fine-tuning (SFT) and
  preference learning stages lead to gradual forgetting of the first stage's training.
  The authors propose a joint post-training framework that alternates between SFT
  and preference learning objectives, avoiding the forgetting problem inherent in
  sequential approaches.
---

# Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning -- A Convex Optimization Perspective

## Quick Facts
- arXiv ID: 2410.15483
- Source URL: https://arxiv.org/abs/2410.15483
- Authors: Heshan Fernando; Han Shen; Parikshit Ram; Yi Zhou; Horst Samulowitz; Nathalie Baracaldo; Tianyi Chen
- Reference count: 40
- Primary result: Alternating optimization framework (ALRIGHT/MAXRIGHT) outperforms sequential training by up to 23% on multiple benchmarks

## Executive Summary
This paper addresses the suboptimality of sequential training in large language model (LLM) post-training, where supervised fine-tuning (SFT) and preference learning stages lead to gradual forgetting of the first stage's training. The authors propose a joint post-training framework that alternates between SFT and preference learning objectives, avoiding the forgetting problem inherent in sequential approaches. They introduce two algorithmic variants: ALRIGHT, which alternates based on a fixed probability, and MAXRIGHT, which adaptively selects objectives based on current performance. The framework is theoretically justified with convergence guarantees and demonstrates up to 23% overall performance improvement across multiple benchmarks compared to sequential training, while maintaining minimal computational overhead.

## Method Summary
The method proposes alternating optimization between SFT and Direct Preference Optimization (DPO) objectives to prevent forgetting that occurs in sequential training. The framework includes two variants: ALRIGHT alternates between objectives based on a fixed probability Î», while MAXRIGHT adaptively selects which objective to update based on current performance gaps. Both methods use LoRA fine-tuning for efficiency and require a reference model trained on DPO data. The theoretical analysis establishes convergence guarantees under softmax parameterization assumptions, showing that the alternating framework achieves better trade-offs between SFT and preference learning objectives than sequential approaches.

## Key Results
- Alternating optimization (ALRIGHT) achieves up to 23% better overall performance compared to sequential training across MMLU, HellaSwag, SORRY-Bench, and XSTest benchmarks
- MAXRIGHT (adaptive objective selection) converges faster than ALRIGHT with fixed alternation schedule
- Theoretical analysis shows diminishing optimality gaps for alternating framework while sequential training has constant lower bounds
- Minimal computational overhead compared to sequential approaches while maintaining balanced performance on both SFT and preference learning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alternating optimization between SFT and DPO objectives prevents forgetting that occurs in sequential training.
- Mechanism: By interleaving updates to SFT and DPO objectives, the model maintains performance on both tasks simultaneously rather than optimizing one at the expense of the other.
- Core assumption: The convex combination of SFT and DPO objectives can be approximated by alternating between them.
- Evidence anchors:
  - [abstract]: "proposes a practical joint post-training framework which has theoretical convergence guarantees and empirically outperforms sequential post-training framework"
  - [section]: "alternating between optimizing for DPO and SFT objectives, based on a given preference for each objective"
  - [corpus]: Weak - only 5 papers in corpus, none directly address alternating optimization as solution to forgetting
- Break condition: If the alternating frequency is too low or the objectives are too conflicting, the approximation error becomes too large

### Mechanism 2
- Claim: Adaptive objective selection based on current performance leads to faster convergence to optimal trade-off.
- Mechanism: MAXRIGHT selects which objective to update based on which has the larger weighted optimality gap, allowing the model to balance performance more efficiently.
- Core assumption: The maximum optimality gap accurately represents which objective needs more optimization at each step
- Evidence anchors:
  - [abstract]: "MAXRIGHT, which adaptively alternate optimization based on current performance"
  - [section]: "dynamically selects the objective to update based on performance, rather than adhering to a fixed schedule like ALRIGHT"
  - [corpus]: Weak - corpus lacks papers on adaptive selection in multi-objective LLM training
- Break condition: If the optimality gap estimates are inaccurate or if the objectives have very different convergence rates

### Mechanism 3
- Claim: Theoretical convergence guarantees ensure the method will find optimal trade-offs between SFT and DPO objectives.
- Mechanism: The alternating framework provides upper bounds on optimality gap that diminish with training iterations, while sequential training has constant lower bounds.
- Core assumption: The problem satisfies convexity assumptions under softmax parameterization
- Evidence anchors:
  - [abstract]: "theoretical convergence guarantees and empirically outperforms sequential post-training framework"
  - [section]: "theoretical result implies that the performance metric diminishes with increasing T, thus we can achieve an arbitrary trade-off"
  - [corpus]: Missing - no corpus evidence for theoretical convergence in alternating multi-objective LLM training
- Break condition: If the softmax assumption breaks down or if the objectives are non-convex in practice

## Foundational Learning

- Concept: Multi-objective optimization and Pareto optimality
  - Why needed here: The framework balances two competing objectives (SFT and DPO) and must find Pareto optimal solutions
  - Quick check question: What does it mean for a solution to be Pareto optimal in the context of SFT and DPO objectives?

- Concept: Convex optimization and gradient descent convergence
  - Why needed here: The theoretical analysis relies on convexity of objectives and convergence properties of gradient-based methods
  - Quick check question: Under what conditions does gradient descent converge to the global optimum for convex functions?

- Concept: Softmax parameterization of language models
  - Why needed here: The theoretical analysis uses softmax parameterization to establish convexity and boundedness properties
  - Quick check question: How does softmax parameterization affect the convexity of the log-likelihood objectives?

## Architecture Onboarding

- Component map: Main training loop alternates between SFT and DPO updates; reference model required for DPO; LoRA adapters used for efficiency
- Critical path: 1) Train reference model, 2) Initialize joint framework, 3) Alternate updates based on strategy, 4) Monitor optimality gaps
- Design tradeoffs: Alternating vs. mixing objectives (computational efficiency vs. approximation accuracy); adaptive vs. fixed alternation schedule
- Failure signatures: 1) Model forgets one objective completely, 2) Training oscillates without convergence, 3) Runtime significantly exceeds sequential baseline
- First 3 experiments:
  1. Implement ALRIGHT with simple alternating schedule and verify it outperforms sequential on toy dataset
  2. Add MAXRIGHT with performance-based selection and compare convergence speed to ALRIGHT
  3. Scale to pythia-1b model and measure resource usage vs. sequential and mixing approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed joint training framework perform on larger language models (e.g., 70B+ parameters) compared to smaller models?
- Basis in paper: [inferred] The paper mentions experiments with Llama3-8B and Pythia-1B models but does not discuss performance on larger models.
- Why unresolved: The paper does not provide experimental results or analysis for models significantly larger than 8B parameters.
- What evidence would resolve it: Experiments comparing the joint training framework's performance on models of varying sizes (e.g., 8B, 70B, 175B parameters) using the same benchmarks and metrics.

### Open Question 2
- Question: What is the impact of dataset diversity and labeling inconsistencies on the effectiveness of the joint training framework?
- Basis in paper: [explicit] The paper mentions investigating data diversity and labeling inconsistencies in real-world experiments but states that a deeper investigation would be valuable.
- Why unresolved: While the paper provides some initial observations, it does not conduct a thorough analysis of how dataset characteristics affect the framework's performance.
- What evidence would resolve it: A comprehensive study analyzing the relationship between dataset diversity, labeling consistency, and the performance of the joint training framework across multiple datasets and model sizes.

### Open Question 3
- Question: How does the joint training framework handle catastrophic forgetting when fine-tuning on multiple tasks sequentially?
- Basis in paper: [inferred] The paper focuses on the trade-off between SFT and preference learning but does not explicitly address the broader issue of catastrophic forgetting in multi-task fine-tuning scenarios.
- Why unresolved: The paper does not investigate the framework's performance in scenarios where models are fine-tuned on multiple tasks sequentially, which is a common challenge in continual learning.
- What evidence would resolve it: Experiments evaluating the framework's ability to maintain performance on previously learned tasks while adapting to new tasks, compared to traditional sequential fine-tuning approaches.

## Limitations

- Theoretical analysis relies heavily on convexity assumptions that may not hold for deep transformer models in practice
- Limited generalization beyond decoder-only models (Llama3-8B, Pythia-1b, OPT-1.3B) to other model architectures
- No extensive hyperparameter tuning comparison between sequential and joint approaches to isolate performance gains

## Confidence

**High Confidence**: The core observation that sequential SFT followed by preference learning leads to forgetting of SFT capabilities is well-supported by the empirical results.

**Medium Confidence**: The alternating optimization framework (ALRIGHT) shows consistent improvements over sequential training, but the magnitude of gains varies across different benchmarks.

**Low Confidence**: The adaptive selection mechanism (MAXRIGHT) shows promising results in the ablation study, but the paper lacks extensive comparison with other adaptive scheduling strategies.

## Next Checks

1. **Convexity Validation**: Conduct experiments to measure the actual curvature of the objective landscape during joint training. Compare the observed optimization behavior with the theoretical predictions under different degrees of non-convexity to assess the practical relevance of the convergence guarantees.

2. **Hyperparameter Sensitivity Analysis**: Systematically vary the alternation frequency, learning rates, and batch sizes for both ALRIGHT and MAXRIGHT across multiple model scales. Determine whether the reported improvements are robust to hyperparameter choices or if they require careful tuning specific to the joint framework.

3. **Generalization Across Model Families**: Apply the joint framework to different model architectures including encoder-decoder models and domain-specific LLMs (e.g., medical or legal). Evaluate whether the performance gains and convergence properties generalize beyond the primarily decoder-only models tested in the current study.