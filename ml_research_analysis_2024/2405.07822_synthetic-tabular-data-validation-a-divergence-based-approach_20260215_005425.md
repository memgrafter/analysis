---
ver: rpa2
title: 'Synthetic Tabular Data Validation: A Divergence-Based Approach'
arxiv_id: '2405.07822'
source_url: https://arxiv.org/abs/2405.07822
tags:
- data
- divergence
- synthetic
- validation
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a divergence-based approach for validating
  synthetic tabular data. Current methods for assessing synthetic data quality rely
  on diverse statistical measures, lacking a unified framework.
---

# Synthetic Tabular Data Validation: A Divergence-Based Approach

## Quick Facts
- arXiv ID: 2405.07822
- Source URL: https://arxiv.org/abs/2405.07822
- Reference count: 30
- Primary result: Introduces divergence-based approach using probabilistic classifiers to validate synthetic tabular data quality

## Executive Summary
This paper proposes a novel divergence-based approach for validating synthetic tabular data quality. The method uses a probabilistic classifier to estimate the density ratio between real and synthetic data distributions, enabling calculation of Kullback-Leibler (KL) and Jensen-Shannon (JS) divergences. Unlike traditional marginal comparisons, this approach considers the joint distribution of all attributes, providing a more holistic view of data quality. The method demonstrates improved accuracy in divergence estimation with increasing sample sizes and shows promise for applications beyond tabular data validation.

## Method Summary
The proposed method trains a neural network discriminator to classify samples as real or synthetic, then transforms the posterior probability via the logit function to estimate the log density ratio. This density ratio is exponentiated to recover the actual ratio, which is then used to compute KL and JS divergences through Monte Carlo simulation. The approach leverages the fact that divergences can capture discrepancies between distributions holistically by considering their joint distributions rather than just marginal comparisons. The neural network architecture consists of three layers (256, 64, 32 neurons) with Leaky ReLU activation, dropout, batch normalization, and early stopping.

## Key Results
- The divergence estimator achieves improved accuracy with increasing sample sizes
- The method provides a holistic view by considering joint distributions of all attributes
- KL and JS divergences reliably validate synthetic tabular data quality
- The approach has potential applications beyond tabular data in various fields

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The probabilistic classifier estimates the density ratio between real and synthetic distributions.
- Mechanism: A neural network discriminator is trained to classify samples as real or synthetic, and the posterior probability is transformed via the logit function to estimate the log density ratio, which is then exponentiated to recover the density ratio.
- Core assumption: The classifier can effectively distinguish between real and synthetic samples when the data are sufficiently different.
- Evidence anchors:
  - [abstract] "We leverage a probabilistic classifier to approximate the density ratio between datasets"
  - [section] "The estimator for r* can be constructed as a function of the classifier’s output: r_θ(x) = exp(σ^(-1)(D_θ(x)))"
- Break condition: If the classifier is unable to distinguish real from synthetic data (e.g., due to insufficient training samples or poor quality synthetic data), the density ratio estimate will be inaccurate, leading to unreliable divergence values.

### Mechanism 2
- Claim: Divergences (KL and JS) are estimated using Monte Carlo simulation based on the estimated density ratio.
- Mechanism: The KL divergence is approximated by taking the expectation of the log density ratio under the real distribution, while the JS divergence is computed using the estimated density ratio and the midpoint distribution.
- Core assumption: The Monte Carlo approximation becomes more accurate with a larger number of samples.
- Evidence anchors:
  - [abstract] "We leverage a probabilistic classifier to approximate the density ratio between datasets, allowing the capture of complex relationships"
  - [section] "DKL = E_p(x)[log r*(x)] ≈ 1/L ∑ log r*(x_i)"
- Break condition: If the number of Monte Carlo samples is too small, the approximation error will be large, leading to inaccurate divergence estimates.

### Mechanism 3
- Claim: The divergence estimator provides a holistic view of data quality by considering the joint distribution of all attributes.
- Mechanism: By estimating the density ratio between the full joint distributions, the method captures complex relationships between features that are missed by marginal comparisons.
- Core assumption: The joint distribution is a more informative representation of data quality than marginal distributions.
- Evidence anchors:
  - [abstract] "This method considers the joint distribution of all attributes, providing a more holistic view compared to traditional marginal comparisons"
  - [section] "Divergences can consider the joint distribution of all attributes, providing a more holistic view of the data"
- Break condition: If the number of features is very large, estimating the joint distribution may become intractable or computationally expensive.

## Foundational Learning

- Concept: Probability distributions and divergences
  - Why needed here: The paper relies on understanding how to quantify differences between probability distributions using divergences like KL and JS.
  - Quick check question: What is the key difference between KL divergence and JS divergence in terms of symmetry and boundedness?

- Concept: Density ratio estimation
  - Why needed here: The proposed method hinges on accurately estimating the ratio of densities between real and synthetic data using a probabilistic classifier.
  - Quick check question: How does the logit transformation relate the posterior probability of a classifier to the density ratio?

- Concept: Generative models and synthetic data
  - Why needed here: The context of the paper is evaluating the quality of synthetic data generated by models like GANs or VAEs, which requires understanding how these models learn and generate data.
  - Quick check question: What are the main challenges in generating high-quality synthetic tabular data compared to other data types like images?

## Architecture Onboarding

- Component map:
  - Generative Model (GM) -> Discriminator Network -> Divergence Estimator

- Critical path:
  1. Train the GM on real data to generate synthetic samples.
  2. Train the discriminator network on a mix of real and synthetic samples to estimate the density ratio.
  3. Use the estimated density ratio to compute KL and JS divergences via Monte Carlo simulation.
  4. Evaluate the divergence values to assess the quality of the synthetic data.

- Design tradeoffs:
  - Accuracy vs. sample size: Higher accuracy requires more training and validation samples, but increases computational cost.
  - Complexity vs. interpretability: Considering the joint distribution provides a more holistic view but may be less interpretable than marginal comparisons.
  - Boundedness vs. sensitivity: JS divergence is bounded and symmetric, while KL divergence is unbounded and asymmetric, affecting their interpretability and sensitivity to distribution differences.

- Failure signatures:
  - Overfitting of the discriminator: If the discriminator is overfit to the training data, it may not generalize well to new samples, leading to inaccurate density ratio estimates.
  - Underfitting of the GM: If the GM is underfit, the generated synthetic data may not capture the true data distribution, leading to high divergence values even for a good discriminator.
  - Insufficient sample size: If the number of training or validation samples is too small, the Monte Carlo approximations may be inaccurate, leading to unreliable divergence estimates.

- First 3 experiments:
  1. Compare the estimated divergences to analytical solutions for simple distributions (e.g., multivariate Gaussians) to validate the accuracy of the method.
  2. Test the method on more complex distributions (e.g., Gaussian mixtures) to assess its robustness and scalability.
  3. Apply the method to real-world datasets and their synthetic counterparts generated by different GMs to evaluate its practical applicability and compare the performance of different GMs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed divergence-based approach perform when validating synthetic data with complex dependencies and high-dimensional feature spaces?
- Basis in paper: [explicit] The paper mentions that the method considers the joint distribution of all attributes, but it does not provide specific results or analysis for high-dimensional data or complex dependencies.
- Why unresolved: The paper primarily focuses on experiments with relatively low-dimensional data and simple distributions, leaving the performance in more challenging scenarios unexplored.
- What evidence would resolve it: Conducting experiments with synthetic data containing complex dependencies and high-dimensional feature spaces, and comparing the performance of the proposed method with existing validation techniques.

### Open Question 2
- Question: Can the proposed divergence-based approach be extended to handle time series data or other sequential data types?
- Basis in paper: [inferred] The paper mentions the potential applicability of the method beyond tabular data, but it does not provide specific details or experiments for time series or sequential data.
- Why unresolved: The paper focuses on tabular data validation and does not explore the extension of the method to other data types, leaving the feasibility and effectiveness for time series data unexplored.
- What evidence would resolve it: Adapting the proposed method to handle time series data or other sequential data types, and conducting experiments to evaluate its performance and compare it with existing validation techniques for these data types.

### Open Question 3
- Question: How does the proposed method handle missing data or data with different levels of granularity in the features?
- Basis in paper: [inferred] The paper does not explicitly discuss the handling of missing data or features with different levels of granularity, which are common challenges in real-world datasets.
- Why unresolved: The paper focuses on the validation of synthetic data without addressing the preprocessing steps or handling of data quality issues, leaving the robustness of the method in the presence of missing data or varying feature granularity unexplored.
- What evidence would resolve it: Conducting experiments with synthetic data containing missing values or features with different levels of granularity, and evaluating the performance of the proposed method in handling these data quality issues.

## Limitations

- The method's performance on very high-dimensional tabular data remains untested, as the Adult dataset has only 14 features.
- The impact of class imbalance in the real data on the divergence estimates is not explicitly addressed.
- The computational complexity of the method for large datasets is not thoroughly discussed.

## Confidence

- **High Confidence**: The theoretical foundation of using probabilistic classifiers for density ratio estimation and the Monte Carlo approximation of divergences.
- **Medium Confidence**: The empirical validation on the Adult dataset, as it demonstrates the method's effectiveness but may not generalize to all tabular data scenarios.
- **Low Confidence**: The method's robustness to noise and outliers in the data, as these aspects are not explicitly evaluated.

## Next Checks

1. Test the method on a high-dimensional tabular dataset (e.g., with 50+ features) to assess its scalability and performance.
2. Evaluate the impact of class imbalance in the real data by deliberately introducing imbalance and observing the effect on divergence estimates.
3. Conduct a sensitivity analysis to understand how the method performs with varying levels of noise and outliers in the data.