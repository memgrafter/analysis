---
ver: rpa2
title: Towards Calibrated Losses for Adversarial Robust Reject Option Classification
arxiv_id: '2410.10736'
source_url: https://arxiv.org/abs/2410.10736
tags:
- loss
- adversarial
- calibration
- risk
- reject
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies surrogate losses calibrated to adversarial\
  \ robust reject option classification for linear classifiers. It introduces an adversarial\
  \ robust reject option loss \u2113\u03B3d and provides a complete characterization\
  \ of calibrated surrogates."
---

# Towards Calibrated Losses for Adversarial Robust Reject Option Classification

## Quick Facts
- arXiv ID: 2410.10736
- Source URL: https://arxiv.org/abs/2410.10736
- Reference count: 40
- One-line primary result: Introduces shifted double sigmoid and double ramp losses that are calibrated to adversarial robust reject option classification for linear classifiers

## Executive Summary
This paper addresses the problem of calibrated surrogate losses for adversarial robust reject option classification. The authors introduce an adversarial robust reject option loss ℓγd and provide a complete characterization of which surrogates can be calibrated to this target loss. They prove that convex losses and those with quasi-concave conditional risk cannot be calibrated to ℓγd, establishing a fundamental limitation in this setting. The work proposes two candidate calibrated surrogates—shifted double sigmoid loss (ℓµ,βds) and shifted double ramp loss (ℓµ,βdr)—and empirically validates their robustness to l2-norm adversarial perturbations through a synthetic dataset experiment.

## Method Summary
The paper proposes a two-stage approach: first, an adversarial training procedure using shifted loss functions (ℓµ,βds and ℓµ,βdr) that incorporates a shift parameter β ≥ γ to create flat regions in the loss function; second, a theoretical calibration analysis that characterizes when these surrogates are calibrated to the target adversarial robust reject option loss ℓγd. The training involves generating adversarial examples within l2-balls of radius γ using projected gradient ascent, then training on this augmented dataset. The calibration conditions require the minimizer of conditional risk to jump from near the origin (for η = 0.5) to beyond ρ + γ (for η > 0.5), a property incompatible with convex or quasi-concave losses.

## Key Results
- Shifted double sigmoid loss (ℓµ,βds) and shifted double ramp loss (ℓµ,βdr) are proposed as calibrated surrogates for ℓγd
- Convex losses and quasi-concave conditional risk functions cannot be calibrated to ℓγd
- Calibration requires the minimizer of conditional risk to jump from near the origin (η = 0.5) to beyond ρ + γ (η > 0.5)
- Empirical validation shows shifted DSL and DRL are robust to l2-norm adversarial perturbations
- Shifted DRL shows flat loss regions that enhance robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Calibration in the adversarial robust reject option setting requires the minimizer of conditional risk to jump from near the origin (for η = 0.5) to beyond ρ + γ (for η > 0.5).
- Mechanism: The calibration function analysis shows that for a surrogate loss to be calibrated to ℓγd, it must satisfy two conditions: the minimizer of Cℓ,H(α, 0.5) must lie in [0, ρ − γ], and for any η > 0.5, the minimizer must jump to the rightmost end of the interval. This "minima jump requirement" ensures robustness against adversarial perturbations while maintaining the reject option functionality.
- Core assumption: The surrogate loss function must exhibit this discontinuous behavior in the location of its minimizer as a function of η, which is fundamentally incompatible with convex or quasi-concave conditional risk functions.
- Evidence anchors:
  - [abstract]: "Calibration conditions require the minimizer of conditional risk to lie close to the origin for η = 0.5 and jump to the rightmost end of the interval for η > 0.5."
  - [section]: "Theorem 10 Any margin-based surrogate ℓ is (ℓγd, H)-calibrated if and only if it satisfies... Minima Jump Requirement: For η = 0.5, (11) is the calibration condition which implies that minima should be close to origin, specifically in the interval [0, ρ − γ]. Calibration to hold for the case of η > 0.5, (12) should be satisfied implying minima lies beyond ρ + γ."
  - [corpus]: Weak or missing corpus evidence for this specific "minima jump requirement" mechanism.
- Break condition: If the surrogate loss function has convex conditional risk or quasi-concave conditional risk in α, it cannot satisfy the minima jump requirement and therefore cannot be calibrated to ℓγd.

### Mechanism 2
- Claim: Shifted Double Sigmoid Loss (ℓµ,βds) and Shifted Double Ramp Loss (ℓµ,βdr) can be calibrated to ℓγd because their conditional risk functions are not quasi-concave and exhibit the required minima jump behavior.
- Mechanism: These shifted loss functions introduce a parameter β ≥ γ that shifts the loss function, creating flat regions in the conditional risk function. The analysis shows that for appropriate choices of β, µ, d, and γ, the conditional risk Cℓµ,βds,H(f(x), η) and Cℓµ,βdr,H(f(x), η) satisfy both calibration conditions - the minimizer is near the origin for η = 0.5 and jumps to the rightmost end for η > 0.5. The non-quasi-concavity ensures that the standard negative calibration results do not apply.
- Core assumption: The shifted loss functions can be tuned (via β, µ parameters) to create conditional risk functions that satisfy the specific calibration conditions required for adversarial robust reject option classification.
- Evidence anchors:
  - [abstract]: "Two candidate calibrated surrogates—shifted double sigmoid loss (ℓµ,βds) and shifted double ramp loss (ℓµ,βdr)—are proposed and empirically validated."
  - [section]: "5. Possible Calibrated Surrogates for ℓγd... We conjecture that shifted DSL is a calibrated loss for ℓγd... Shifted Double Ramp Loss... is described as... For shifted DRL ℓµ,βdr to be a surrogate for ℓγd, we require that β ≥ γ."
  - [corpus]: Weak or missing corpus evidence for the specific shifted loss functions proposed in this paper.
- Break condition: If the chosen parameters (β, µ) do not create the required minima jump behavior, or if the shifted loss functions become quasi-concave for certain parameter values, calibration will fail.

### Mechanism 3
- Claim: Adversarial training using the shifted loss functions (ℓµ,βds and ℓµ,βdr) creates robust linear reject option classifiers that maintain performance under l2-norm adversarial perturbations.
- Mechanism: The adversarial training procedure involves first training on clean data with the base loss (ℓµds or ℓµdr), then generating adversarial examples within l2-balls of radius γ, and finally training on this augmented dataset with the shifted loss (ℓµ,βds or ℓµ,βdr). This process ensures that the classifier learns to make robust decisions even when inputs are perturbed. The flat regions in the shifted loss functions (particularly in DRL) provide inherent robustness by making the loss less sensitive to small perturbations within certain ranges.
- Core assumption: The adversarial training procedure with the shifted loss functions can effectively transfer the theoretical calibration properties to practical robustness against adversarial attacks.
- Evidence anchors:
  - [abstract]: "Experiments on a synthetic dataset demonstrate that shifted DSL and shifted DRL are robust to l2-norm adversarial perturbations, with shifted DRL showing flat loss regions that enhance robustness."
  - [section]: "5.4. Adversarial Training using the Double Sigmoid Loss / Double Ramp Loss... Step 1: Train a linear reject option classifier... Step 2: Generate adversarial data... Step 3: Train a robust linear reject option classifier..."
  - [corpus]: Weak or missing corpus evidence for this specific adversarial training procedure with shifted loss functions.
- Break condition: If the adversarial examples generated during training do not adequately represent the test-time perturbations, or if the flat regions in the loss functions are not positioned correctly relative to the rejection threshold, the trained classifier may not be robust to actual adversarial attacks.

## Foundational Learning

- Concept: Calibration theory for surrogate losses
  - Why needed here: Understanding when a surrogate loss function is calibrated to a target loss is fundamental to ensuring that minimizing the surrogate risk leads to minimizing the true risk. In this paper, calibration analysis is used to characterize which surrogates can be used for adversarial robust reject option classification.
  - Quick check question: What is the difference between H-calibration and pseudo-H-calibration, and why does the paper use both concepts?

- Concept: Adversarial robustness and reject option classification
  - Why needed here: The paper combines two important concepts - adversarial robustness (where inputs may be perturbed by an adversary) and reject option classification (where the classifier can abstain from prediction at a cost). Understanding both concepts is necessary to grasp the problem setting and the challenges in designing calibrated surrogates.
  - Quick check question: How does the reject option loss ℓd differ from standard classification losses, and why is it important for high-risk applications?

- Concept: Conditional risk and excess conditional risk
  - Why needed here: The analysis of calibration relies heavily on understanding the conditional risk Cℓ,H(α, η) and excess conditional risk ∆Cℓ,H(α, η). These concepts are used to derive the calibration conditions and to prove negative results about convex and quasi-concave surrogates.
  - Quick check question: What is the relationship between the conditional risk Cℓ,H(α, η) and the pseudo-minimal conditional risk Cℓ,H(η), and how are they used in the calibration analysis?

## Architecture Onboarding

- Component map: Shifted DSL/DRL loss functions -> Adversarial training procedure -> Robust linear classifier -> Evaluation on adversarial examples
- Critical path: Loss function selection -> Calibration analysis -> Parameter tuning -> Adversarial training -> Robustness evaluation
- Design tradeoffs:
  - Convexity vs. calibration: Convex surrogates are easier to optimize but cannot be calibrated to ℓγd, while the proposed shifted losses are non-convex but satisfy calibration conditions
  - Flat regions vs. sensitivity: The flat regions in shifted DRL enhance robustness but may reduce sensitivity to important input features
  - Parameter tuning: The β and µ parameters need careful tuning to satisfy calibration conditions, which may require extensive experimentation
- Failure signatures:
  - If the trained classifier shows high error rates even on clean data, the shifted loss parameters may not be properly tuned
  - If robustness to adversarial perturbations is not achieved, the adversarial training procedure may not be generating representative examples or the flat regions in the loss may not be positioned correctly
  - If the rejection rate is too high or too low, the d parameter (cost of rejection) may need adjustment
- First 3 experiments:
  1. Implement the shifted double sigmoid loss function and verify that its conditional risk satisfies the calibration conditions for a simple synthetic dataset
  2. Implement the shifted double ramp loss function and compare its robustness to l2-norm adversarial perturbations against the non-shifted version
  3. Implement the full adversarial training procedure and evaluate the robustness of the trained classifier on a synthetic dataset with varying levels of adversarial perturbations

## Open Questions the Paper Calls Out
1. Under what specific conditions (on µ, β, d, γ) does shifted DSL satisfy both calibration conditions (11) and (12)?
2. Can the shifted DRL loss be proven to be calibrated for ℓγd using a similar analytical approach to the one used for shifted DSL?
3. How do the calibration results extend to other function classes such as generalized linear models (Hg) and single-layer ReLU neural networks (HNN)?
4. What is the exact relationship between the shift parameter β and the perturbation radius γ for ensuring calibration in shifted DSL and shifted DRL?

## Limitations
- The theoretical analysis is limited to linear classifiers and may not generalize to non-linear models
- Empirical validation is restricted to a synthetic dataset, with no testing on real-world data
- The choice of parameters (β, µ, d) is crucial but lacks a systematic selection method
- The paper does not explore the computational complexity of training with the shifted loss functions

## Confidence
- **High Confidence**: The negative results showing that convex and quasi-concave losses cannot be calibrated to ℓγd, as these follow from established calibration theory
- **Medium Confidence**: The theoretical characterization of calibrated surrogates, as it depends on the specific assumptions about the target loss function and hypothesis class
- **Low Confidence**: The empirical validation of robustness on synthetic data, as the results may not generalize to more complex, real-world datasets

## Next Checks
1. **Parameter Sensitivity Analysis**: Systematically evaluate the performance of shifted DSL and DRL across a grid of β and µ values to understand the robustness of calibration conditions to parameter choices
2. **Real-World Dataset Testing**: Implement the adversarial training procedure with shifted losses on a standard image or text classification dataset with reject option to validate generalization beyond synthetic data
3. **Non-Linear Extension**: Extend the calibration analysis to non-linear hypothesis classes (e.g., neural networks) and investigate whether the minima jump requirement can be satisfied in these more complex settings