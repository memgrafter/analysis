---
ver: rpa2
title: On the consistency of hyper-parameter selection in value-based deep reinforcement
  learning
arxiv_id: '2406.17523'
source_url: https://arxiv.org/abs/2406.17523
tags:
- learning
- reinforcement
- hyper-parameter
- hyper-parameters
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents an extensive empirical study examining the\
  \ consistency and transferability of hyper-parameter choices in value-based deep\
  \ reinforcement learning (RL) agents. The authors investigate how hyper-parameters\
  \ perform across different training regimes (100k vs 40M environment frames), different\
  \ agents (DER vs DrQ(\u03F5)), and different environments (26 Atari games)."
---

# On the consistency of hyper-parameter selection in value-based deep reinforcement learning

## Quick Facts
- arXiv ID: 2406.17523
- Source URL: https://arxiv.org/abs/2406.17523
- Reference count: 40
- This paper presents an extensive empirical study examining the consistency and transferability of hyper-parameter choices in value-based deep reinforcement learning (RL) agents.

## Executive Summary
This paper investigates how hyper-parameters for deep reinforcement learning agents transfer across different training regimes, algorithms, and environments. Through 108,000 independent training runs across 26 Atari games, the authors analyze 12 hyper-parameters for two value-based agents (DER and DrQ(ϵ)) at both 100k and 40M frames. They introduce a novel "Tuning Hyperparameter Consistency" (THC) score to quantify how hyper-parameter rankings change across experimental conditions. The study reveals that hyper-parameters generally do not transfer well across environments, optimal values differ significantly between low-data (100k) and high-data (40M) regimes, and there is only moderate consistency across different agents.

## Method Summary
The authors conduct an extensive empirical study using the Dopamine library implementation of DER and DrQ(ϵ) agents. They run 5 seeds for each hyper-parameter configuration across 12 hyper-parameters with various values, totaling 108,000 training runs on 26 Atari games at both 100k and 40M frames. Performance is evaluated using Interquantile Mean (IQM) with 95% confidence intervals. The novel THC score quantifies consistency by computing rankings for each hyper-parameter value using confidence intervals, then normalizing peak-to-peak variation in rankings across regimes. Statistical comparisons follow Agarwal et al. (2021) guidelines, and results are made accessible through an interactive web interface.

## Key Results
- Hyper-parameters generally do not transfer well across environments, with the highest THC scores observed for environment variations
- Optimal hyper-parameters for Atari 100k frames differ significantly from those for 40M frames, particularly for Adam's epsilon and update horizon
- Moderate consistency exists across different agents, with some hyper-parameters showing more transferability than others
- Non-standard optimal values were found, including Adam's epsilon requiring 100x higher values than standard and weight decay as high as 0.5 being optimal for some games at 40M frames

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Tuning Hyperparameter Consistency (THC) score quantifies hyper-parameter ranking stability across training regimes.
- Mechanism: THC computes rankings for each hyper-parameter value using upper/lower confidence bounds, then normalizes the peak-to-peak variation in rankings across regimes. High THC indicates poor transferability.
- Core assumption: Hyper-parameter ranking consistency across regimes is a valid proxy for transferability and practical re-tuning needs.
- Evidence anchors:
  - [abstract] "We introduce a new score to quantify the consistency and reliability of various hyper-parameters."
  - [section] "We compute ranking agreement for three setups: Varying algorithms... Varying environments... Varying data regimes..."
  - [corpus] Weak - no direct corpus evidence of this specific scoring method.
- Break condition: If hyper-parameter performance distributions overlap heavily, rankings become unstable and THC loses discriminative power.

### Mechanism 2
- Claim: Hyper-parameters that perform well in low-data regimes (100k frames) often perform poorly in high-data regimes (40M frames).
- Mechanism: Different data regimes create different training dynamics (e.g., overfitting vs. convergence stability), causing optimal hyper-parameter values to shift. The paper observes Adam's epsilon and update horizon as critical examples.
- Core assumption: The statistical properties of the learning process change fundamentally with dataset size, requiring different regularization and optimization settings.
- Evidence anchors:
  - [abstract] "optimal values for Atari 100k differ significantly from those for 40M frames"
  - [section] "We find that optimal hyper-parameters for Atari 100k mostly do not transfer once you move to 40M updates"
  - [corpus] Weak - no direct corpus evidence of this specific 100k vs 40M comparison.
- Break condition: If training dynamics are inherently scale-invariant (e.g., with perfect normalization), hyper-parameter transferability might hold.

### Mechanism 3
- Claim: Hyper-parameters do not transfer well across different environments, even with the same algorithm and data regime.
- Mechanism: Different Atari games have different reward structures, action spaces, and state dynamics, causing hyper-parameter rankings to vary dramatically between games.
- Core assumption: Environment-specific characteristics dominate hyper-parameter performance, making cross-environment transferability unlikely.
- Evidence anchors:
  - [abstract] "hyper-parameters generally do not transfer well across environments (highest THC scores)"
  - [section] "Our experiments show that hyper-parameters that perform well on some games lead to lackluster final performance in others"
  - [corpus] Weak - no direct corpus evidence of this specific environment transfer analysis.
- Break condition: If environments share sufficient structural similarity or if adaptive algorithms could compensate for environment differences.

## Foundational Learning

- Concept: Kendall's Tau and W for inter-ranking agreement
  - Why needed here: THC builds on these metrics but adapts them for noisy RL performance data with confidence intervals
  - Quick check question: How does THC handle overlapping confidence intervals differently from standard Kendall's Tau?

- Concept: Inter-Quantile Mean (IQM) for robust performance aggregation
  - Why needed here: IQM provides more reliable performance estimates than mean when dealing with heavy-tailed RL returns
  - Quick check question: Why might IQM be preferred over simple mean for aggregating RL performance across seeds?

- Concept: Temporal Difference (TD) error and its role in Q-learning
  - Why needed here: Understanding TD error is crucial for grasping how hyper-parameters like learning rate and discount factor affect value-based RL
  - Quick check question: How does the discount factor γ influence the TD error computation in Q-learning?

## Architecture Onboarding

- Component map: Experiment runners -> Performance aggregators (IQM) -> THC score calculator -> Interactive web interface
- Critical path: Run experiments → Compute IQM with confidence intervals → Calculate THC scores → Generate visualizations → Deploy web interface
- Design tradeoffs: Comprehensive hyper-parameter sweep vs. computational cost; aggregate metrics vs. per-game detail; static analysis vs. interactive exploration
- Failure signatures: Inconsistent rankings across seeds (high variance), THC scores that don't align with visual inspection, web interface performance issues with large datasets
- First 3 experiments:
  1. Run a small hyper-parameter sweep (e.g., batch size values) on a single game to validate the experiment pipeline
  2. Compute IQM and confidence intervals for the sweep results to ensure proper aggregation
  3. Calculate THC scores for a subset of hyper-parameters across two data regimes to validate the scoring mechanism

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but several implications arise from their findings that could guide future research.

## Limitations
- Computational resource intensity: 108,000 training runs make replication extremely expensive and limit accessibility for typical research groups
- THC score validation: While novel, the metric lacks comparison against established hyper-parameter transfer metrics to validate its effectiveness
- Limited generalizability: Findings are based on 26 Atari games, which may not extend to other RL domains like continuous control or real-world applications

## Confidence
- **High confidence**: The core finding that hyper-parameters don't transfer well across training regimes (100k vs 40M frames) - this is supported by extensive empirical evidence across multiple games and agents.
- **Medium confidence**: The observation that hyper-parameters don't transfer across environments - while well-supported by the Atari experiments, the mechanism explaining why remains somewhat speculative.
- **Medium confidence**: The THC score as a reliable metric for quantifying transfer consistency - the metric is well-defined but its practical utility beyond this specific study hasn't been established.

## Next Checks
1. **Replication on non-Atari domains**: Test the transferability patterns on continuous control benchmarks (e.g., MuJoCo) to assess generalizability beyond discrete action spaces.
2. **THC score validation**: Apply THC to traditional ML hyper-parameter selection problems (e.g., supervised learning) to verify it captures meaningful consistency patterns beyond RL.
3. **Resource-efficient approximation**: Develop and validate methods to approximate THC scores using fewer training runs, making the metric practically usable for typical research groups.