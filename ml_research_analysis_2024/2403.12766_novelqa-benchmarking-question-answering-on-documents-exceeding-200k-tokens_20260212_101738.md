---
ver: rpa2
title: 'NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens'
arxiv_id: '2403.12766'
source_url: https://arxiv.org/abs/2403.12766
tags:
- questions
- question
- llms
- novel
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NovelQA, a benchmark for evaluating large
  language models on long-context understanding using English novels exceeding 200,000
  tokens. NovelQA features manually crafted questions, golden answers, and textual
  evidence, covering seven aspects (times, meaning, span, setting, relation, character,
  plot) and three complexity levels (multi-hop, single-hop, detail).
---

# NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens

## Quick Facts
- arXiv ID: 2403.12766
- Source URL: https://arxiv.org/abs/2403.12766
- Reference count: 21
- Primary result: Models struggle with long-context understanding, with best model achieving only 62.30% accuracy on questions from 200K+ token novels

## Executive Summary
This paper introduces NovelQA, a benchmark designed to evaluate large language models' ability to comprehend and reason over extremely long documents, specifically English novels exceeding 200,000 tokens. The benchmark features manually crafted questions covering seven aspects (times, meaning, span, setting, relation, character, plot) and three complexity levels (multi-hop, single-hop, detail). Through comprehensive evaluation of commercial and open-source models, the study reveals significant challenges in long-context processing, with models showing particular difficulty in multi-hop reasoning, detail-oriented questions, and handling evidence beyond 100,000 tokens.

## Method Summary
NovelQA consists of 65 public domain novels and 24 copyrighted novels from Project Gutenberg, each accompanied by manually crafted questions, golden answers, and textual evidence. The evaluation framework uses two settings: generative (models generate short answers) and multiple-choice (models select from four options). Models are evaluated using GPT-4 as a judge, with truncation strategies employed to fit within model context windows. The benchmark covers seven question aspects and three complexity levels, with average novel lengths exceeding 200,000 tokens and question-answer pairs averaging 300 words.

## Key Results
- Commercial models (GPT-4, Claude variants) and open-source models (InternLM2-Chat, Llama-3.1) show significant performance drops in generative settings compared to multiple-choice
- The best-performing model (Claude-3.5-Sonnet) achieves only 62.30% accuracy, with notable struggles in multi-hop reasoning and detail-oriented questions
- Models exhibit a sharp accuracy drop when evidence is located beyond 100,000 tokens, suggesting a hard limit in long-context processing
- Performance in close-book settings (no text access) is significantly lower than standard settings, indicating heavy reliance on memorized content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models struggle with multi-hop reasoning in long contexts due to inability to maintain and integrate scattered information across distant text segments.
- Mechanism: The models' attention mechanisms become less effective at retrieving relevant information when it is distributed across long distances in the text, leading to incomplete integration of evidence needed for multi-hop questions.
- Core assumption: The attention span and information retrieval capabilities of current LLMs degrade significantly beyond certain token thresholds.
- Evidence anchors:
  - [abstract] "Notably, the models struggle with multi-hop reasoning, detail-oriented questions, and handling extremely long inputs, with average lengths exceeding 200,000 tokens."
  - [section] "Results show that even the most advanced long-context LLMs face challenges in consistently extracting and processing accurate information from extended texts."
- Break condition: If attention mechanisms or retrieval methods are improved to maintain effectiveness over longer distances, this mechanism would break.

### Mechanism 2
- Claim: Performance degradation in long contexts is not uniform but shows specific patterns related to absolute token positions, particularly beyond 100,000 tokens.
- Mechanism: Models exhibit a sharp drop in accuracy when evidence is located beyond 100,000 tokens, suggesting a hard limit in their ability to process information in extremely long contexts.
- Core assumption: There exists a critical token threshold (around 100K) beyond which model performance degrades sharply, regardless of relative position within the document.
- Evidence anchors:
  - [abstract] "Results highlight the need for substantial advancements in LLMs to enhance their long-context comprehension and contribute effectively to computational literary analysis."
  - [section] "Our analysis reveals that the highest proportion of evidence instances (25.17%) occurs within the first 10% of the novels, while the distribution across the middle sections is relatively uniform."
- Break condition: If models are trained or fine-tuned specifically on contexts exceeding 100K tokens, or if architectural changes mitigate this position-dependent performance drop.

### Mechanism 3
- Claim: Models rely heavily on memorized content from training data for well-known texts, leading to inflated performance in close-book scenarios compared to true long-context understanding.
- Mechanism: When access to the actual text is restricted, models perform significantly worse, indicating that their apparent comprehension is largely based on pre-existing knowledge rather than genuine contextual understanding.
- Core assumption: Models have been exposed to the content of popular novels during training and can recall this information, but cannot effectively reason over unseen or novel long contexts.
- Evidence anchors:
  - [abstract] "Results show that even the most advanced long-context LLMs face challenges in consistently extracting and processing accurate information from extended texts."
  - [section] "Models like GPT-4 and Claude 2.1 achieve notable scores in the Close-book setting (60.94% and 51.77% in multichoice, 34.30% and 22.36% in generative, respectively) indicating that they have internalized significant portions of the novels' content during training."
- Break condition: If models demonstrate consistent performance regardless of whether they have access to the text or not, this mechanism would break.

## Foundational Learning

- Concept: Multi-hop reasoning
  - Why needed here: Understanding how questions requiring information integration across multiple text segments reveal model limitations in long-context processing.
  - Quick check question: Can you explain why a question asking about events that happen in multiple chapters would be more challenging for an LLM than a question about events in a single paragraph?

- Concept: Attention mechanisms in transformers
  - Why needed here: Understanding how self-attention works and why it becomes less effective over longer distances is crucial for grasping why models struggle with long contexts.
  - Quick check question: What happens to the attention scores between tokens that are very far apart in a long document, and why might this affect model performance?

- Concept: Positional encoding and context windows
  - Why needed here: Understanding how models track token positions and the limitations of context windows explains why absolute position affects performance.
  - Quick check question: How do transformer models keep track of where each token is in a long document, and what happens when tokens are beyond the model's context window?

## Architecture Onboarding

- Component map: Benchmark creation pipeline (novel selection, question annotation, evidence marking) -> Evaluation framework (prompt design, truncation strategy, multiple-choice vs generative settings) -> Analysis components (performance breakdown by question type, position analysis, error categorization)
- Critical path: 1) Feeding the model truncated novel content + questions, 2) Collecting model responses in both generative and multichoice formats, 3) Evaluating responses against golden answers using GPT-4 as judge, 4) Analyzing performance patterns across different question types and evidence positions
- Design tradeoffs: The benchmark uses truncation to fit within model context windows, which may exclude some evidence but allows testing across diverse models. The choice between generative and multichoice settings balances the difficulty of answer generation against the ability to test reasoning capabilities. The reliance on GPT-4 for evaluation introduces potential bias but provides consistency.
- Failure signatures: Models failing on questions with evidence beyond 100K tokens, poor performance on multi-hop questions, inability to recall specific details, generation of hallucinated content, and significant performance drops in close-book scenarios.
- First 3 experiments:
  1. Test model performance on questions with evidence positioned at different absolute token ranges (0-50K, 50K-100K, 100K-150K, >150K) to confirm the 100K threshold effect.
  2. Compare model performance on multi-hop vs single-hop questions to quantify the reasoning gap.
  3. Evaluate the same questions in both close-book and standard settings to measure reliance on memorized content.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural modifications would enable LLMs to maintain performance beyond 100K tokens without the observed accuracy drop?
- Basis in paper: [explicit] "Results show that even the most advanced long-context LLMs face challenges in consistently extracting and processing accurate information from extended texts... with the best model (Claude-3.5-Sonnet) achieving 62.30% accuracy" and "our data shows a decline in performance for evidence situated beyond the 100,000-token mark"
- Why unresolved: The paper identifies the performance drop but doesn't explore architectural solutions. While it mentions efficient attention and memory techniques in related work, it doesn't test whether these approaches could solve the 100K token barrier.
- What evidence would resolve it: Experiments comparing different architectural approaches (efficient attention mechanisms, memory augmentation, dynamic context windows) on NovelQA beyond 100K tokens.

### Open Question 2
- Question: How does the novel's narrative structure affect LLM performance on different question types?
- Basis in paper: [inferred] The dataset covers "seven aspects (times, meaning, span, setting, relation, character, plot)" and shows varying performance across these types, but doesn't analyze how narrative complexity correlates with difficulty.
- Why unresolved: The paper provides performance breakdowns by question type but doesn't investigate whether narrative features like non-linear timelines, multiple POV characters, or complex causal chains contribute to performance differences.
- What evidence would resolve it: Analysis correlating narrative complexity metrics (temporal structure, POV shifts, causal depth) with model performance on specific question types.

### Open Question 3
- Question: Would training LLMs specifically on long-context literary texts improve their comprehension of novel-specific reasoning tasks?
- Basis in paper: [explicit] "operating LLMs on inputs exceeding 200,000 tokens faces technical challenges... in terms of memory requirements and associated costs" and the observation that models struggle with multi-hop reasoning and detail-oriented questions
- Why unresolved: The paper evaluates existing models but doesn't test whether domain-specific pretraining on long-form narrative texts would address the identified weaknesses in multi-hop reasoning and detail recall.
- What evidence would resolve it: Comparative experiments training models from scratch or fine-tuning on large corpora of novels versus traditional pretraining, then evaluating on NovelQA.

## Limitations

- The truncation strategy may artificially constrain model performance by excluding potentially relevant evidence
- Reliance on GPT-4 for both evaluation and baseline introduces potential bias, particularly given the significant performance gap between GPT-4 and other models
- Focus on English novels from Project Gutenberg may limit generalizability to other domains or languages

## Confidence

**High Confidence**: The claim that models struggle with multi-hop reasoning in long contexts is well-supported by experimental results showing consistent performance drops on multi-hop questions across all tested models. The observation that performance degrades beyond 100K tokens is also robustly demonstrated through position-based analysis.

**Medium Confidence**: The assertion that models rely heavily on memorized content rather than genuine understanding in close-book settings is plausible but requires more direct evidence. While the performance difference between settings suggests this, it could also reflect differences in reasoning approaches rather than pure memorization.

**Low Confidence**: The specific claim about a "hard limit" at 100K tokens may overstate the phenomenon, as the data shows a gradual decline rather than a sharp threshold. The exact nature of this relationship warrants further investigation.

## Next Checks

1. **Truncation Impact Analysis**: Systematically vary the truncation point (e.g., 50K, 100K, 150K tokens) and measure how model accuracy changes to determine whether truncation itself is the primary cause of performance degradation.

2. **Cross-Domain Generalization**: Test the same models on a subset of questions from non-literary domains (e.g., scientific papers, legal documents) to assess whether the observed limitations generalize beyond the novel domain.

3. **Memorization vs Understanding Disambiguation**: Design a controlled experiment using novels with similar content but different phrasing to distinguish between memorization of specific text and genuine comprehension of concepts and relationships.