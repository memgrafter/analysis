---
ver: rpa2
title: 'Can Large Language Models Code Like a Linguist?: A Case Study in Low Resource
  Sound Law Induction'
arxiv_id: '2406.12725'
source_url: https://arxiv.org/abs/2406.12725
tags:
- sound
- llms
- language
- data
- laws
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of sound law induction in historical
  linguistics, which involves automatically deriving the phonetic rules that transform
  words from an ancestor language to its descendants. The authors propose a novel
  approach that casts this task as programming by examples, leveraging the programming
  abilities of large language models (LLMs) to generate Python code representing sound
  laws.
---

# Can Large Language Models Code Like a Linguist?: A Case Study in Low Resource Sound Law Induction

## Quick Facts
- arXiv ID: 2406.12725
- Source URL: https://arxiv.org/abs/2406.12725
- Reference count: 40
- Key outcome: LLMs can effectively perform sound law induction by generating Python code from protoform-reflex examples, with fine-tuning on synthetic data closing the gap to GPT-4 performance

## Executive Summary
This paper tackles the challenge of sound law induction in historical linguistics, which involves automatically deriving the phonetic rules that transform words from an ancestor language to its descendants. The authors propose a novel approach that casts this task as programming by examples, leveraging the programming abilities of large language models (LLMs) to generate Python code representing sound laws. They introduce PySLICoder, a method that uses LLMs to generate sound law programs from examples of protoforms and reflexes. The approach involves prompt engineering, beam search for discovering cascades of sound laws, and synthetic data generation to fine-tune LLMs for the task. The results show that while GPT-4 performs best, PySLICoder fine-tuned on synthetic data can achieve performance close to GPT-4 with fewer parameters.

## Method Summary
The authors cast sound law induction as a Programming by Examples (PBE) problem, where LLMs generate Python code implementing string transformation rules from input-output pairs. The method uses prompt engineering to present protoform-reflex examples to LLMs, which output code using a BasicAction class representing sound laws. For multi-step induction, a beam search algorithm explores cascades of sound laws, selecting candidates based on edit distance rewards. Synthetic data generation algorithms create language-agnostic examples of string manipulations and linguistically motivated sound changes for fine-tuning. The approach is evaluated on three tasks: single sound law prediction, multi-law synthetic reflex prediction, and final reflex prediction across Tangkhulic and Polynesian language families.

## Key Results
- GPT-4 outperforms other LLMs on sound law induction tasks, achieving the highest edit distance rewards
- PySLICoder fine-tuned on synthetic data achieves performance close to GPT-4 while using fewer parameters
- LLMs show complementary performance to traditional methods, particularly for languages requiring shorter rule cascades (2-4 rules)
- The approach struggles with languages requiring longer rule cascades (8-12 rules) like Huishu

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate Python code that represents sound laws when given examples of protoforms and reflexes.
- Mechanism: The LLM treats the task as Programming by Examples (PBE), using the input-output pairs to infer the transformation rules and output Python code using the BasicAction class.
- Core assumption: The LLM's programming ability is general enough to synthesize string manipulation functions from linguistic examples.
- Evidence anchors:
  - [abstract] "we propose a language-agnostic solution that utilizes the programming ability of Large Language Models (LLMs) by generating Python sound law programs from sound change examples."
  - [section 3.1] "We prompt all the LLMs including PySLICoder with the tokenized (using PanPhon) protoforms and reflexes as the input and output respectively to generate sound laws."

### Mechanism 2
- Claim: Fine-tuning LLMs on synthetic data improves their performance on sound law induction.
- Mechanism: The synthetic data generation algorithms create language-agnostic examples of string manipulations and linguistically motivated sound changes, which teach the LLM the patterns needed for SLI.
- Core assumption: Synthetic data can effectively represent the distribution of real sound law examples.
- Evidence anchors:
  - [section 3.3] "We propose two synthetic data generation algorithms to generate language-agnostic data of different complexities to provide additional task-specific supervision to the LLMs for the task."
  - [section 4.1] "For each sound law we randomly sampleN (=50) protoforms such that at least 2/5 N protoforms contain one or more occurrences of the environment..."

### Mechanism 3
- Claim: Beam search can discover cascades of sound laws in the correct chronological order.
- Mechanism: The beam search algorithm maintains multiple hypotheses of rule cascades and expands them by sampling new sound laws, selecting the best candidates based on edit distance rewards.
- Core assumption: The edit distance reward is an effective heuristic for evaluating the quality of sound law cascades.
- Evidence anchors:
  - [section 3.2] "To achieve this we apply a beam search style algorithm as shown in Figure 3."
  - [section 4.3] "R(s, a) = dist(s, send) − dist(snext, send) / dist(sstart, send)"

## Foundational Learning

- Concept: Programming by Examples (PBE)
  - Why needed here: The task is reformulated as PBE to leverage LLM's code generation capabilities from input-output pairs.
  - Quick check question: Can you explain how PBE differs from traditional program synthesis?

- Concept: String manipulation and regular expressions
  - Why needed here: Sound laws are essentially string rewrite rules that transform one string into another.
  - Quick check question: How would you represent the rule "a > e / _ j" as a regular expression?

- Concept: Edit distance as a similarity metric
  - Why needed here: Edit distance is used to evaluate how well the predicted reflexes match the target reflexes.
  - Quick check question: What is the Levenshtein edit distance between "knight" and "night"?

## Architecture Onboarding

- Component map: Protoform/Reflex examples -> LLM Code Generator -> BasicAction class (sound law representation) -> Beam Search (for multi-law induction) -> Edit Distance Evaluation -> Selected Sound Laws
- Critical path: Prompt LLM with examples → Generate Python code → Execute code to get predictions → Evaluate predictions using edit distance → Select best sound law(s)
- Design tradeoffs: Using LLMs provides flexibility but requires significant computational resources; synthetic data improves performance but may not fully capture real linguistic complexity
- Failure signatures: Low pass rate indicates the LLM cannot generate correct sound laws; poor reward@k scores suggest the beam search is not finding good cascades
- First 3 experiments:
  1. Test zero-shot performance of different LLMs on single sound law prediction
  2. Evaluate the impact of fine-tuning on different synthetic data types
  3. Compare beam search performance with and without example selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the error propagation problem in multi-step sound law induction be effectively mitigated?
- Basis in paper: [explicit] The paper discusses that "A notable challenge in our method is the potential for errors to compound during multi-step sound law induction. Initial inaccuracies may escalate, leading to significant errors in the final results."
- Why unresolved: The paper identifies this as a limitation but does not propose specific solutions to address the compounding of errors in multi-step sound law induction.
- What evidence would resolve it: Evidence showing successful implementation of techniques that prevent or correct error propagation in multi-step sound law induction, such as error detection and correction mechanisms or hierarchical planning approaches.

### Open Question 2
- Question: What is the impact of increasing the size of synthetic data for multi-sound law prediction on the performance of LLMs?
- Basis in paper: [inferred] The paper suggests that "At the same time the results also show that LLMs can complement traditional methods in some cases... This suggests that future work should explore more hierarchical sound law search methods... Additionally it should be explored how much synthetic data for multi sound law prediction can help."
- Why unresolved: The paper indicates the need for more research on the impact of synthetic data size for multi-sound law prediction but does not provide experimental results or evidence to support this exploration.
- What evidence would resolve it: Experimental results comparing the performance of LLMs on multi-sound law prediction with varying sizes of synthetic data, demonstrating the relationship between data size and model performance.

### Open Question 3
- Question: How can hierarchical sound law search methods be effectively combined with LLMs to improve multi-sound law induction?
- Basis in paper: [explicit] The paper states that "This is also points to the limitations of our sound law beam search algorithm and the importance of having search algorithms that can leverage hierarchical planning like Luo’s MCTS."
- Why unresolved: The paper acknowledges the limitations of the current sound law beam search algorithm and suggests the need for hierarchical planning but does not provide a concrete approach for combining hierarchical search methods with LLMs.
- What evidence would resolve it: Evidence showing successful integration of hierarchical sound law search methods with LLMs, resulting in improved performance on multi-sound law induction tasks, possibly through hybrid models or joint training approaches.

## Limitations

- The approach struggles with languages requiring longer rule cascades (8-12 rules), limiting its applicability to more complex historical linguistics problems
- Synthetic data may not fully capture the complexity and variability of real historical sound changes, potentially biasing the LLM toward certain types of rules
- The beam search algorithm's reliance on edit distance as a heuristic may not always correlate with linguistic validity or historical accuracy

## Confidence

**High Confidence**: The core finding that LLMs can generate syntactically valid Python code representing sound laws from input-output examples is well-supported by the experimental results. The superiority of GPT-4 over other models and the effectiveness of fine-tuning on synthetic data are clearly demonstrated.

**Medium Confidence**: The claim that LLMs can complement traditional methods in historical linguistics is supported but requires further validation. While the results show promise for Polynesian languages, the poor performance on Huishu suggests limitations that need to be addressed before broader adoption.

**Low Confidence**: The assertion that the synthetic data generation algorithms effectively capture the distribution of real sound changes is based on limited evidence. The study does not thoroughly investigate whether the synthetic data biases the LLM toward certain types of rules or fails to represent the full complexity of historical linguistics.

## Next Checks

1. **Cross-linguistic validation**: Test the PySLICoder approach on additional language families with varying degrees of complexity in their sound change patterns to assess generalizability beyond the Polynesian and Tangkhulic languages studied.

2. **Human evaluation of linguistic plausibility**: Conduct expert linguistic evaluation of the induced sound laws to assess not just phonetic accuracy but also historical and phonological plausibility, addressing the limitation of relying solely on edit distance metrics.

3. **Ablation study on synthetic data**: Systematically evaluate the impact of different synthetic data generation strategies and amounts of synthetic data on model performance to determine the optimal balance between synthetic and real training data.