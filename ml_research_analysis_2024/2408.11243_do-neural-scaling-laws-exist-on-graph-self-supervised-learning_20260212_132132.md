---
ver: rpa2
title: Do Neural Scaling Laws Exist on Graph Self-Supervised Learning?
arxiv_id: '2408.11243'
source_url: https://arxiv.org/abs/2408.11243
tags:
- scaling
- data
- parameters
- loss
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether graph self-supervised learning
  (SSL) techniques follow neural scaling laws, which are crucial for developing Graph
  Foundation Models (GFMs). The authors conduct comprehensive experiments on multiple
  SSL methods, including InfoGraph, GraphCL, JOAO, and GraphMAE, across various graph
  datasets using both data scaling and model scaling approaches.
---

# Do Neural Scaling Laws Exist on Graph Self-Supervised Learning?

## Quick Facts
- arXiv ID: 2408.11243
- Source URL: https://arxiv.org/abs/2408.11243
- Authors: Qian Ma; Haitao Mao; Jingzhe Liu; Zhehua Zhang; Chunlin Feng; Yu Song; Yihan Shao; Yao Ma
- Reference count: 40
- Key outcome: No graph SSL method exhibits neural scaling laws on downstream performance despite SSL loss scaling

## Executive Summary
This paper investigates whether graph self-supervised learning (SSL) techniques follow neural scaling laws, which are crucial for developing Graph Foundation Models (GFMs). Through comprehensive experiments on multiple SSL methods (InfoGraph, GraphCL, JOAO, GraphMAE) across various graph datasets, the authors find that while SSL losses consistently decrease with increased data and model sizes, downstream performance fails to show corresponding improvements. This reveals a significant gap between pre-training and downstream tasks in the graph domain, indicating that current graph SSL objectives are inadequate for building GFMs. The study emphasizes that model architecture choices, particularly the number of aggregation layers, have a more substantial impact on SSL performance than parameter count or hidden dimensions.

## Method Summary
The authors conduct systematic experiments using four graph SSL methods: InfoGraph, GraphCL, JOAO, and GraphMAE. They perform both data scaling experiments (varying pre-training data ratios from 0.1 to 1.0) and model scaling experiments (varying hidden dimensions and number of layers). Pre-trained models are evaluated on downstream graph classification tasks using linear probing with SVM/MLP classifiers. The evaluation pipeline measures both SSL loss (JSD or InfoNCE) and downstream performance metrics (accuracy, ROC-AUC, AP) across multiple graph datasets including reddit-threads, ogbg-molhiv, and ogbg-molpcba.

## Key Results
- SSL losses consistently decrease with increased data and model sizes, but downstream performance remains flat
- Model architecture choices (particularly number of aggregation layers) impact SSL performance more than parameter count
- Contrastive SSL methods show more consistent scaling behavior than generative methods
- No existing graph SSL technique exhibits neural scaling laws on downstream performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph SSL methods show SSL loss scaling but not downstream performance scaling because the SSL objectives do not align with downstream tasks.
- Mechanism: When pre-training data or model parameters increase, the SSL loss consistently decreases, indicating improved capability on the SSL task. However, this improvement does not translate to downstream performance because the SSL task and downstream task optimize different objectives.
- Core assumption: The SSL loss reduction corresponds to improved model capability on the SSL task, but this capability does not transfer to downstream tasks due to task misalignment.
- Evidence anchors:
  - [abstract] "despite the SSL loss continuously decreasing, no existing graph SSL techniques follow the neural scaling behavior on the downstream performance"
  - [section 4.2] "The key conclusion derived from the above results is that there is a huge gap between the pre-training and downstream tasks that block the GraphSSL methods from following the scaling law on the downstream performance"
  - [corpus] Weak - neighboring papers discuss graph SSL but do not directly address the SSL loss/downstream performance gap

### Mechanism 2
- Claim: Model architecture choices (number of aggregation layers) have more impact on SSL performance than parameter count or hidden dimensions.
- Mechanism: Increasing the number of aggregation layers improves the model's ability to capture graph structure, which benefits SSL objectives more than simply increasing parameters or hidden dimensions.
- Core assumption: Graph structure information is primarily captured through message passing/aggregation operations rather than through parameter-rich transformations.
- Evidence anchors:
  - [abstract] "the key factors influencing the performance are the choices of model architecture and pretext task design"
  - [section 5.2 Observation 6] "the improvement in the SSL Loss is primarily due to the model architecture or structure with more aggregations, rather than the capability with more learnable parameters for transformation"
  - [corpus] Weak - neighboring papers discuss graph SSL methods but do not specifically analyze the impact of aggregation layers versus parameters

### Mechanism 3
- Claim: Contrastive SSL methods show more consistent scaling behavior than generative methods.
- Mechanism: Contrastive methods optimize objectives that benefit more directly from increased model capacity and data, while generative methods may face limitations in how well their reconstruction objectives scale.
- Core assumption: The mathematical form of contrastive losses allows for more consistent scaling benefits compared to reconstruction-based generative losses.
- Evidence anchors:
  - [section 5.2 Observation 4] "Compared with the representative contrastive SSL method InfoGraph, GraphMAE is a generative method, the scaling effect is not consistent nor obvious on its SSL loss"
  - [section 5.2 Observation 4] "As contrastive SSL methods, GraphCL and JOAO can exhibit similar scaling behavior as InfoGraph"
  - [corpus] Weak - neighboring papers discuss various graph SSL methods but do not systematically compare contrastive versus generative scaling behavior

## Foundational Learning

- Concept: Neural scaling laws - the principle that model performance improves predictably with increased data or model size
  - Why needed here: The paper investigates whether graph SSL methods follow these established scaling laws from NLP/CV domains
  - Quick check question: What is the mathematical form typically used to describe neural scaling laws?

- Concept: Self-supervised learning (SSL) - learning from unlabeled data by creating pretext tasks
  - Why needed here: The paper examines whether existing graph SSL techniques can serve as components for Graph Foundation Models
  - Quick check question: What is the key difference between supervised and self-supervised learning in terms of data requirements?

- Concept: Contrastive learning - a type of SSL that learns by comparing similar and dissimilar examples
  - Why needed here: Several graph SSL methods examined (GraphCL, JOAO) are contrastive approaches
  - Quick check question: What is the main objective of contrastive learning in self-supervised settings?

## Architecture Onboarding

- Component map: Graph datasets (reddit-threads, ogbg-molhiv, ogbg-molpcba) -> SSL methods (InfoGraph, GraphCL, JOAO, GraphMAE) -> Pre-training -> Fixed encoder -> Downstream evaluation (SVM/MLP) -> Performance metrics
- Critical path: Implement SSL methods → Pre-train on increasing data sizes → Evaluate downstream performance → Analyze SSL loss scaling → Analyze model scaling effects → Draw conclusions about scaling law existence
- Design tradeoffs: Choice between contrastive vs generative SSL methods, number of aggregation layers vs hidden dimensions, data augmentation strategies, and evaluation protocols (linear probing vs fixed encoder)
- Failure signatures: SSL loss decreasing but downstream performance not improving, inconsistent scaling behavior across different graph datasets, method-specific scaling behavior that doesn't generalize
- First 3 experiments:
  1. Implement data scaling experiment: Pre-train InfoGraph with increasing data ratios (0.1 to 1.0) and measure downstream classification accuracy on held-out test set.
  2. Implement model scaling experiment: Train GraphCL with varying number of layers (1-5) and fixed hidden size, measure both SSL loss and downstream performance.
  3. Compare SSL loss scaling: Fix pre-trained model and compute SSL loss on held-out test data while varying pre-training data amount for each method.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural changes or novel pretext tasks could bridge the gap between SSL objectives and downstream performance in graph learning?
- Basis in paper: [explicit] The paper explicitly states that the key factors influencing performance are model architecture choices and pretext task design, and that the current graph SSL objectives are inadequate for building GFMs.
- Why unresolved: While the paper identifies that the gap exists and suggests that architecture and task design are critical, it does not provide specific solutions or experimental evidence for what changes would effectively bridge this gap.
- What evidence would resolve it: Empirical results showing improved downstream performance on graph classification tasks when using novel architectural modifications or pretext tasks designed to better align with downstream applications.

### Open Question 2
- Question: How do different graph SSL methods (contrastive vs. generative) scale with model size, and what architectural features contribute to this scaling behavior?
- Basis in paper: [explicit] The paper observes that contrastive methods like InfoGraph exhibit more consistent scaling behavior compared to generative methods like GraphMAE, suggesting that SSL task design and component design are critical for scaling.
- Why unresolved: The paper provides observations on the scaling behavior of different methods but does not delve into the underlying reasons why certain methods scale better or what specific architectural features contribute to this behavior.
- What evidence would resolve it: Detailed analysis of the scaling behavior of various SSL methods with different architectural components, identifying which features (e.g., number of layers, hidden dimensions) most significantly impact scaling.

### Open Question 3
- Question: Can the decoupling of aggregation and transformation layers in graph neural networks improve the scaling behavior of SSL methods?
- Basis in paper: [explicit] The paper introduces a new implementation of InfoGraph (InfoGraph1T) with decoupled aggregation and transformation layers, which exhibits scaling behavior with increased aggregations.
- Why unresolved: While the paper demonstrates that decoupling these layers can lead to scaling behavior, it does not explore whether this approach can be generalized to other SSL methods or if it consistently improves performance across different datasets.
- What evidence would resolve it: Comparative studies showing the performance of various SSL methods with and without decoupled aggregation and transformation layers across multiple datasets and tasks.

## Limitations

- Limited dataset coverage: Only three graph datasets tested, which may not represent the full diversity of graph types and downstream tasks
- Architecture search space: The study doesn't exhaustively test all possible architectural variations that might exhibit scaling behavior
- Pre-training objective alignment: The paper identifies a gap but doesn't definitively prove that current SSL objectives are fundamentally misaligned with downstream tasks

## Confidence

**High confidence**: The empirical observation that SSL losses consistently decrease with increased data and model size while downstream performance remains flat. This finding is directly observable from the experimental results and the gap is statistically significant.

**Medium confidence**: The conclusion that current graph SSL objectives are inadequate for building Graph Foundation Models. While supported by the data, this broader claim requires additional validation across more diverse graph domains and downstream tasks.

**Low confidence**: The assertion that model architecture choices (particularly aggregation layers) are more important than parameter scaling. This conclusion needs more systematic ablation studies to rule out confounding factors.

## Next Checks

- Check 1: Cross-dataset validation: Test the scaling behavior hypothesis on additional graph datasets spanning different domains (bioinformatics, social networks, knowledge graphs) to assess the generality of the findings.
- Check 2: Objective ablation study: Design controlled experiments that modify SSL objectives to better align with specific downstream tasks and measure whether scaling behavior emerges in downstream performance.
- Check 3: Architecture search space expansion: Conduct a more comprehensive exploration of model architectures, including attention-based models and varying neighborhood aggregation strategies, to determine if scaling behavior exists in unexplored architectural spaces.