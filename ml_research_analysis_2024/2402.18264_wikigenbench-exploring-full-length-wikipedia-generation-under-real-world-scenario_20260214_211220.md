---
ver: rpa2
title: 'WIKIGENBENCH: Exploring Full-length Wikipedia Generation under Real-World
  Scenario'
arxiv_id: '2402.18264'
source_url: https://arxiv.org/abs/2402.18264
tags:
- wikipedia
- score
- documents
- generation
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WIKIGENBENCH, a benchmark for generating
  full-length Wikipedia articles for newly emerging events using real-world scenarios.
  The benchmark consists of 1,320 entries, including 309 events with corresponding
  web pages for generating evidence.
---

# WIKIGENBENCH: Exploring Full-length Wikipedia Generation under Real-World Scenario

## Quick Facts
- arXiv ID: 2402.18264
- Source URL: https://arxiv.org/abs/2402.18264
- Reference count: 28
- Primary result: Introduces WIKIGENBENCH benchmark for generating full-length Wikipedia articles for emerging events, showing hierarchical methods produce more comprehensive content but still lag behind existing Wikipedia articles.

## Executive Summary
This paper introduces WIKIGENBENCH, a comprehensive benchmark for generating full-length Wikipedia articles about newly emerging events using real-world scenarios. The benchmark consists of 1,320 entries based on 309 events with corresponding web pages for evidence retrieval. The authors propose systematic evaluation metrics and baseline methods to assess large language models' performance in generating factual, full-length Wikipedia documents. Experiments demonstrate that hierarchical retrieval-planning-retrieval-reading frameworks can generate more comprehensive content, while fine-tuned methods achieve better verifiability, though even the best methods show significant gaps compared to existing Wikipedia content.

## Method Summary
The WIKIGENBENCH task involves generating full-length Wikipedia articles for emerging events using retrieval-based methods. The approach employs two main frameworks: Retrieve-then-Read (RR) and Retrieve-Plan-Retrieve-Read (RPRR). The RPRR framework addresses context window limitations by first generating section titles, then performing targeted retrieval for each section, and finally generating section-specific content. The benchmark includes 309 events with 1,320 total entries, using web-sourced documents as evidence. Evaluation combines GPT-4-based scoring for informativeness, n-gram metrics for surface similarity, and citation accuracy measures for faithfulness.

## Key Results
- Hierarchical-based methods (RPRR) generate more comprehensive content compared to flat retrieval approaches
- Fine-tuned methods achieve better verifiability through improved citation accuracy
- Even the best-performing methods show significant gaps compared to existing Wikipedia content, indicating room for improvement
- Dense retrievers underperform sparse retrievers on rare entity recognition tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured Wikipedia generation benefits from hierarchical retrieval-planning-retrieval-reading (RPRR) framework
- Mechanism: The RPRR framework addresses context window limitations by first generating section titles, then performing targeted retrieval for each section, and finally generating section-specific content
- Core assumption: Breaking the generation task into section-level retrieval improves information organization and citation quality
- Evidence anchors:
  - [abstract]: "hierarchical-based methods can generate more comprehensive content"
  - [section]: "we use a 'Retrieve-Plan-Retrieve-Read' framework to overcome this problem"
  - [corpus]: Weak - no direct corpus evidence for RPRR effectiveness
- Break condition: If section titles cannot be generated accurately, the RPRR framework loses its organizational advantage

### Mechanism 2
- Claim: Dense retrievers underperform sparse retrievers on Wikipedia generation tasks due to rare entity recognition issues
- Mechanism: Dense retrievers struggle to identify rare entities not present in their training data, while sparse retrievers excel at exact term matching
- Core assumption: Wikipedia articles frequently contain rare entities that sparse retrievers can match through exact term matching
- Evidence anchors:
  - [abstract]: No direct evidence
  - [section]: "dense retrievers often struggle to identify rare entities not included during training"
  - [corpus]: Weak - no corpus evidence for entity distribution
- Break condition: If Wikipedia articles predominantly contain common entities, dense retrievers may perform comparably

### Mechanism 3
- Claim: GPT-4 evaluation provides more reliable assessment of Wikipedia generation quality than n-gram metrics
- Mechanism: GPT-4 can evaluate coherence, relevance, and focus holistically, while n-gram metrics only measure surface-level similarity
- Core assumption: Generated Wikipedia articles should be evaluated based on their coherence and relevance to the topic, not just surface similarity to reference articles
- Evidence anchors:
  - [abstract]: "we employ GPT-4 to score the informativeness from several aspects"
  - [section]: "n-gram metrics exhibit a trend similar to GPT-4 scores" but "anomaly is noticed with the Vicuna-7b model"
  - [corpus]: Weak - no corpus evidence for GPT-4 evaluation reliability
- Break condition: If GPT-4 evaluation is inconsistent or biased, n-gram metrics may be preferred

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: RAG framework combines information retrieval with text generation to produce factually accurate Wikipedia articles
  - Quick check question: How does RAG differ from traditional text generation approaches?

- Concept: Evaluation metrics for text generation
  - Why needed here: Multiple evaluation metrics are needed to assess different aspects of Wikipedia generation (fluency, informativeness, faithfulness)
  - Quick check question: Why can't a single metric evaluate all aspects of Wikipedia generation quality?

- Concept: Context window limitations in language models
  - Why needed here: Context window constraints limit the amount of information that can be processed in a single generation step
  - Quick check question: How do context window limitations affect Wikipedia generation?

## Architecture Onboarding

- Component map: Retriever -> Planner -> Generator -> Evaluator
- Critical path: Event -> Retriever -> Generator -> Wikipedia article
- Design tradeoffs: Longer articles vs. citation quality, complex frameworks vs. simplicity
- Failure signatures: Poor citation coverage, incoherent sections, irrelevant information
- First 3 experiments:
  1. Compare RR vs RPRR framework performance on a small subset of events
  2. Test different retrieval methods (sparse vs. dense) on a single event
  3. Evaluate GPT-4 vs n-gram metrics on generated Wikipedia articles

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of retrieval-augmented generation (RAG) models be improved for tasks involving rare entities or specialized domains?
- Basis in paper: [inferred]
- Why unresolved: The paper highlights that dense retrievers struggle with identifying rare entities not included in their training data, which is a significant issue for tasks like Wikipedia generation that often involve such entities. However, it does not provide a concrete solution or method to overcome this limitation.
- What evidence would resolve it: A proposed method or experiment that demonstrates improved performance of dense retrievers on rare entities, or a comparative analysis showing the effectiveness of alternative retrieval methods (e.g., hybrid approaches combining dense and sparse retrievers) in handling such cases.

### Open Question 2
- Question: What are the optimal strategies for balancing informativeness and faithfulness in generated Wikipedia articles?
- Basis in paper: [explicit]
- Why unresolved: The paper discusses the trade-off between informativeness (measured by metrics like BLEU, METEOR, ROUGE) and faithfulness (measured by citation rates and recall/precision) in generated Wikipedia articles. However, it does not provide a clear strategy for optimizing this balance or determining the ideal number of retrieved documents to maximize both metrics.
- What evidence would resolve it: An experimental study that systematically varies the number of retrieved documents and evaluates the resulting impact on informativeness and faithfulness, identifying the optimal configuration for generating high-quality Wikipedia articles.

### Open Question 3
- Question: How can the redundancy and lack of cohesion in section-by-section generation approaches be addressed?
- Basis in paper: [inferred]
- Why unresolved: The paper acknowledges that the section-by-section generation approach used in the RPRR framework may lead to redundancy and require rewriting strategies to ensure article cohesion. However, it does not propose or evaluate specific methods to mitigate this issue.
- What evidence would resolve it: A comparative analysis of different generation strategies (e.g., end-to-end generation, post-processing rewriting, or improved section linking) that demonstrates their effectiveness in reducing redundancy and improving the overall cohesion of generated Wikipedia articles.

## Limitations

- GPT-4-based evaluation introduces potential subjectivity despite claims of improved reliability over n-gram metrics
- Dataset construction may not fully represent the diversity of real-world Wikipedia-worthy events
- Comparison between different retrieval methods lacks direct ablation studies isolating specific architectural components

## Confidence

- **High Confidence**: The observation that hierarchical methods (RPRR) generate more comprehensive content is well-supported by the experimental results and consistent across different model sizes
- **Medium Confidence**: The claim about dense retrievers underperforming on rare entities is supported by the findings but lacks direct corpus evidence of entity distribution patterns
- **Low Confidence**: The superiority of GPT-4 evaluation over n-gram metrics is based on qualitative observations rather than rigorous statistical validation

## Next Checks

1. Conduct a systematic ablation study comparing RR and RPRR frameworks on a fixed set of 50 events to isolate the impact of hierarchical planning on content organization
2. Perform entity frequency analysis on the retrieved documents to empirically validate the claim that dense retrievers struggle with rare entities versus sparse retrievers
3. Implement a human evaluation study comparing GPT-4 scores with human judgments on 100 randomly selected generated articles to quantify agreement and potential biases in automated evaluation