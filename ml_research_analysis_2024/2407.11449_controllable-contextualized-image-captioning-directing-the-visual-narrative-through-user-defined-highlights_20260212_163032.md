---
ver: rpa2
title: 'Controllable Contextualized Image Captioning: Directing the Visual Narrative
  through User-Defined Highlights'
arxiv_id: '2407.11449'
source_url: https://arxiv.org/abs/2407.11449
tags:
- image
- ctrl-cic
- context
- caption
- captions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Controllable Contextualized Image Captioning\
  \ (Ctrl-CIC), extending traditional contextualized image captioning by allowing\
  \ users to specify highlighted segments within the context to guide caption generation.\
  \ Two methods\u2014Prompting-based Controller (P-Ctrl) and Recalibration-based Controller\
  \ (R-Ctrl)\u2014are proposed to steer models toward generating captions focused\
  \ on user-defined highlights."
---

# Controllable Contextualized Image Captioning: Directing the Visual Narrative through User-Defined Highlights

## Quick Facts
- arXiv ID: 2407.11449
- Source URL: https://arxiv.org/abs/2407.11449
- Authors: Shunqi Mao; Chaoyi Zhang; Hang Su; Hwanjun Song; Igor Shalyminov; Weidong Cai
- Reference count: 40
- Primary result: Introduces two controllers (P-Ctrl and R-Ctrl) that enable user-defined highlights to guide image caption generation, achieving significant improvements in controllability and diversity over baselines

## Executive Summary
This paper extends contextualized image captioning by introducing user-controllable highlight-based guidance. The proposed Controllable Contextualized Image Captioning (Ctrl-Ctrl) framework allows users to specify which segments of the context should be emphasized in the generated captions. Two controller mechanisms are developed: Prompting-based Controller (P-Ctrl) that conditions generation on highlight-driven prefixes, and Recalibration-based Controller (R-Ctrl) that recalibrates encoder embeddings based on highlight relevance. Both approaches are trained weakly using relevance-based highlight extraction and demonstrate significant improvements in controllability and diversity metrics while using only 5% of the parameters of large vision-language models.

## Method Summary
The Ctrl-CIC framework extends contextualized image captioning by allowing users to specify highlighted segments within the context to guide caption generation. Two controllers are proposed: P-Ctrl conditions the model generation on highlights by prepending captions with highlight-driven prefixes, while R-Ctrl recalibrates encoder embeddings for highlighted tokens using token-level relevance weights derived from cosine similarity. The model is trained weakly using relevance-based highlight extraction, where token-level relevance scores are computed using T5-large, and highlights are selected based on a threshold. During inference, P-Ctrl uses the constructed prompt to guide generation, while R-Ctrl employs a weight predictor to generate recalibration weights. The approach is evaluated using a GPT-4V-empowered framework alongside standard metrics.

## Key Results
- Both P-Ctrl and R-Ctrl significantly improve controllability and diversity compared to baselines on Wiki-Web2M dataset
- R-Ctrl achieves the highest highlight relevance scores while P-Ctrl offers greater caption diversity
- The models demonstrate effective controllability using only 5% of the parameters used by large vision-language models like LLaVA-1.5
- GPT-4V empowered evaluator shows that Ctrl-CIC captions score higher on Context Relevance, Highlight Relevance, and Image Consistency compared to baseline models

## Why This Works (Mechanism)

### Mechanism 1: R-Ctrl Highlight Relevance
The Recalibration-based Controller achieves high highlight relevance by recalibrating encoder embeddings with token-level relevance weights derived from cosine similarity between context and caption embeddings. The model computes cosine similarity scores between each context token embedding and the average caption embedding, normalizes these scores to unit interval weights, and during training, encoder embeddings are element-wise multiplied by these weights. During inference, a weight predictor generates recalibration weights, and highlighted tokens are further incremented by a fixed value α to focus generation on those tokens. The core assumption is that token-level cosine similarity between context and caption embeddings is a reliable proxy for highlight relevance.

### Mechanism 2: P-Ctrl Conditioning
The Prompting-based Controller enables fine-grained control by prepending captions with highlight-driven prefixes, conditioning the decoder on user-defined highlights. Training highlights are selected from context using word-level relevance scores (averaged from token-level scores). These highlights are assembled into a prompt string separated by special tokens, prepended to the intended caption, and the model is finetuned to predict this augmented text. During inference, the prompt constructed from test highlights guides controlled caption generation. The core assumption is that autoregressive language models can condition generation on prefix prompts to follow user-defined highlights.

### Mechanism 3: GPT-4V Evaluation
The GPT-4V empowered evaluator provides reliable assessment of Ctrl-CIC captions by leveraging chain-of-thought reasoning and comparative scoring against reference captions. GPT-4V is provided with the image, context, highlights, and both a reference caption and the Ctrl-CIC caption. It scores each caption on multiple dimensions (Context Relevance, Highlight Relevance, Image Consistency, Overall Quality) using chain-of-thought steps. The final score is computed as the ratio of raw marks between Ctrl-CIC and reference caption, with logarithmic transformation to normalize reciprocal values. The core assumption is that GPT-4V can reliably evaluate multimodal outputs using chain-of-thought reasoning and comparative scoring.

## Foundational Learning

- Concept: Multimodal embedding alignment
  - Why needed here: The model needs to align image features with textual context and highlights for coherent caption generation
  - Quick check question: How are image features incorporated into the text embedding sequence in the Ctrl-CIC model?

- Concept: Autoregressive text generation with conditioning
  - Why needed here: Both P-Ctrl and R-Ctrl rely on conditioning the decoder on highlights to generate controlled captions
  - Quick check question: What is the difference between how P-Ctrl and R-Ctrl condition the decoder on highlights?

- Concept: Chain-of-thought reasoning for evaluation
  - Why needed here: The GPT-4V evaluator uses chain-of-thought steps to systematically assess caption quality across multiple dimensions
  - Quick check question: What are the four evaluation dimensions used by the GPT-4V evaluator for Ctrl-CIC captions?

## Architecture Onboarding

- Component map: CLIP-large -> T5-large -> LongT5-base -> Weight predictor (R-Ctrl only) -> GPT-4V evaluator
- Critical path: Input context and image are processed → Token-level relevance scores are computed → Highlights are selected based on relevance scores → For P-Ctrl: highlights are converted to prompt and prepended to caption; For R-Ctrl: relevance scores are converted to weights and used to recalibrate embeddings → Decoder generates caption conditioned on highlights → GPT-4V evaluator scores the generated caption
- Design tradeoffs: Using LongT5 instead of Prefix Global for efficiency vs. slightly lower performance; Weakly supervised highlight extraction vs. manual annotation for scalability; Comparative scoring with GPT-4V vs. reference-based metrics for controllability assessment
- Failure signatures: Low recall but high CLIPScore (model generates diverse captions but misses highlights); High recall but low CLIPScore (model focuses on highlights but loses image consistency); Poor diversity scores (model generates repetitive captions for different highlights)
- First 3 experiments: 1) Train P-Ctrl and R-Ctrl on Wiki-Web2M with weakly supervised highlights; evaluate on single-highlight test set; 2) Compare P-Ctrl and R-Ctrl against LongT5-Ext, LongT5-Tune, and LLaVA-1.5 baselines using reference-free metrics; 3) Evaluate top-performing models with GPT-4V empowered evaluator on both single and multiple-highlight test sets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of threshold value θ in Eq. 6 affect the quality and diversity of generated captions in P-Ctrl?
- Basis in paper: The paper states that the threshold value θ is set to 0.3 for highlight selection, but does not provide an analysis of its impact on caption quality or diversity
- Why unresolved: The paper does not explore the sensitivity of the model to different threshold values, which could significantly impact the relevance and diversity of the generated captions
- What evidence would resolve it: Conducting experiments with different threshold values and evaluating the resulting caption quality and diversity would provide insights into the optimal threshold setting

### Open Question 2
- Question: How does the recalibration value α in R-Ctrl influence the balance between highlight relevance and image consistency in generated captions?
- Basis in paper: The paper mentions that the weight recalibration value α is set to 0.1, but does not discuss its impact on the trade-off between highlight relevance and image consistency
- Why unresolved: The paper does not investigate the effect of different recalibration values on the model's ability to maintain image consistency while focusing on highlights
- What evidence would resolve it: Experimenting with various recalibration values and assessing their impact on highlight relevance and image consistency metrics would help determine the optimal α value

### Open Question 3
- Question: How does the length of the generated captions in Ctrl-CIC compare to those in standard CIC, and what factors influence this difference?
- Basis in paper: The paper does not provide a direct comparison of caption lengths between Ctrl-CIC and standard CIC, nor does it discuss factors that might influence caption length in the two tasks
- Why unresolved: The paper does not explore how the introduction of highlights and controllability in Ctrl-CIC affects the length of generated captions compared to standard CIC
- What evidence would resolve it: Analyzing the length distribution of captions generated by Ctrl-CIC and standard CIC models, and identifying factors such as highlight length or context complexity that influence caption length, would provide insights into this aspect

## Limitations
- The GPT-4V evaluator's reliability and potential biases are not thoroughly validated through human evaluation comparison
- The weakly supervised highlight extraction relies on token-level cosine similarity as a proxy for semantic relevance, which may not capture nuanced relationships
- The model's performance is primarily evaluated on the Wiki-Web2M dataset, raising questions about generalizability to other image-captioning domains

## Confidence
- **High confidence**: Core architectural components (P-Ctrl prefix prompting and R-Ctrl weight recalibration) are technically sound and well-described; experimental methodology and baseline comparisons are clearly presented
- **Medium confidence**: Quantitative results showing improved controllability and diversity over baselines are convincing, but absolute performance levels and practical significance need further validation on diverse datasets
- **Low confidence**: GPT-4V evaluator's reliability and effectiveness of weakly supervised highlight extraction are weakest links, as they rely on assumptions about model behavior not independently verified

## Next Checks
1. **Evaluate GPT-4V evaluator consistency**: Conduct reliability analysis by having GPT-4V score same captions multiple times and measuring score variance; compare GPT-4V scores with human evaluations on subset of captions to assess correlation and potential biases
2. **Test cross-domain generalization**: Apply Ctrl-CIC approach to image-captioning datasets from different domains (social media images, news photos, scientific figures) and evaluate whether controllers maintain controllability advantages when context structures differ significantly from Wiki-Web2M
3. **Analyze highlight extraction quality**: Conduct ablation study where manually annotated highlights are used instead of weakly supervised ones; compare performance of P-Ctrl and R-Ctrl when trained with ground-truth highlights versus pseudo-highlights to quantify impact of highlight extraction quality on final controllability