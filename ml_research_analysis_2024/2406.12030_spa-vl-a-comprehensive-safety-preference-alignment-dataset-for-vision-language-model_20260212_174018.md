---
ver: rpa2
title: 'SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language
  Model'
arxiv_id: '2406.12030'
source_url: https://arxiv.org/abs/2406.12030
tags:
- dataset
- safety
- responses
- your
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SPA-VL, a large-scale dataset for safety alignment
  of vision-language models. It contains 100,788 samples covering 6 harm domains,
  13 categories, and 53 subcategories.
---

# SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model

## Quick Facts
- arXiv ID: 2406.12030
- Source URL: https://arxiv.org/abs/2406.12030
- Reference count: 40
- Large-scale safety alignment dataset covering 6 harm domains, 13 categories, 53 subcategories with 100,788 samples

## Executive Summary
SPA-VL is a large-scale dataset designed for safety alignment of vision-language models (VLMs). It addresses the gap in safety training data for VLMs by providing diverse harmful content across 6 domains, 13 categories, and 53 subcategories. The dataset contains 100,788 samples with images, questions, and contrasting responses from 12 different VLMs. Models trained on SPA-VL using DPO and PPO show substantial improvements in harmlessness (unsafe rate reduced from 44.15% to 0-1.19% on MM-SafetyBench) while maintaining helpfulness and core capabilities.

## Method Summary
SPA-VL was constructed through an automated pipeline involving image collection, question generation (easy, hard, hard statement), response generation from 12 VLMs, and preference annotation via GPT-4V. The dataset consists of quadruples (question, image, chosen response, rejected response) designed for preference learning. Alignment training uses DPO and PPO on LLaVA-1.5 (7B) with a frozen visual encoder, where the model learns to distinguish between safe and unsafe responses based on the preference pairs.

## Key Results
- Models trained on SPA-VL reduce unsafe rate from 44.15% to 0-1.19% on MM-SafetyBench
- Increasing dataset scale, response diversity, and question variety all enhance alignment performance
- Substantial improvements in harmlessness while maintaining helpfulness and core capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset improves safety by providing diverse, domain-specific harmful content paired with contrasting responses.
- Mechanism: By exposing models to a wide range of harmful prompts across 6 domains, 13 categories, and 53 subcategories, and training them with preference pairs (chosen vs. rejected responses), the model learns to distinguish and avoid unsafe outputs while maintaining helpfulness.
- Core assumption: The model can generalize from seen harmful examples to unseen ones if trained on sufficiently diverse and representative data.
- Evidence anchors:
  - [abstract]: "SPA-VL covers 6 harmfulness domains, 13 categories, and 53 subcategories, and contains 100,788 samples..."
  - [section]: "The preference dataset is designed according to two alignment objectives: harmlessness and helpfulness."
  - [corpus]: Found 25 related papers; FMR and citation patterns suggest active research but limited empirical comparison to SPA-VL.
- Break condition: If the dataset lacks coverage of emerging harmful categories or the response diversity is insufficient to capture real-world variability.

### Mechanism 2
- Claim: Increasing dataset scale and response diversity improves alignment model performance.
- Mechanism: Larger and more varied datasets expose the model to more nuanced examples of harmful content, enabling better discrimination between safe and unsafe responses. Diverse responses from 12 different VLMs reduce model-specific bias.
- Core assumption: Alignment performance scales with dataset size and diversity, and models can effectively learn from contrasting response pairs.
- Evidence anchors:
  - [abstract]: "Increasing dataset scale, response diversity, and question variety all enhance alignment performance."
  - [section]: "The responses are collected from 12 open-source (e.g., QwenVL) and closed-source (e.g., Gemini) VLMs to ensure diversity."
  - [corpus]: Weak - limited empirical data on dataset scale effects in related works.
- Break condition: Diminishing returns or overfitting if the dataset becomes too large without proportional quality control.

### Mechanism 3
- Claim: Automated dataset construction ensures scalable and consistent generation of preference pairs.
- Mechanism: The process covers image collection, question generation (easy, hard, hard statement), response generation from multiple models, and preference annotation via GPT-4V, enabling efficient scaling without manual annotation bottlenecks.
- Core assumption: Automated methods can reliably generate high-quality, diverse, and relevant data without significant human oversight.
- Evidence anchors:
  - [abstract]: "The construction of preference data is fully automated, and the experimental results indicate that models trained with alignment techniques on the SPA-VL dataset exhibit substantial improvements..."
  - [section]: "The entire dataset construction process is fully automated, ensuring efficient and scalable data generation."
  - [corpus]: Weak - most related works still rely on manual or semi-manual data collection.
- Break condition: Automation introduces systematic biases or fails to capture complex human preferences not representable in the prompt templates.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: SPA-VL is designed for RLHF, which aligns VLMs by optimizing reward models or direct preference optimization to match human judgments on safety and helpfulness.
  - Quick check question: What are the key differences between PPO and DPO in RLHF, and when might you choose one over the other?

- Concept: Multimodal harm categorization
  - Why needed here: Understanding the 6 domains, 13 categories, and 53 subcategories is critical for interpreting the dataset's coverage and ensuring alignment training addresses the right safety concerns.
  - Quick check question: How does the hierarchical categorization help in targeting specific types of harmful content during model training?

- Concept: Preference learning with contrasting pairs
  - Why needed here: The dataset consists of quadruples (question, image, chosen response, rejected response); understanding how to construct and use these pairs is essential for effective training.
  - Quick check question: Why is it important to have both chosen and rejected responses in each sample for alignment training?

## Architecture Onboarding

- Component map: Image encoder -> Projection layer -> LLM backbone -> Reward model (PPO) or Direct preference optimization (DPO)
- Critical path:
  1. Load SPA-VL dataset (quadruples)
  2. Encode image + text input
  3. Generate model response
  4. Compare against chosen/rejected pairs
  5. Compute loss (reward-based or direct preference)
  6. Update LLM and projection layer weights
- Design tradeoffs:
  - Freezing the vision encoder simplifies training but may limit adaptation to harmful visual content.
  - Using 12 diverse VLMs for responses increases bias reduction but adds complexity in response quality control.
  - Automated annotation speeds up scaling but risks introducing prompt-induced biases.
- Failure signatures:
  - Model overfits to dataset-specific harmful patterns and fails on novel attacks.
  - Safety improvements come at the cost of helpfulness or general capability.
  - Automated annotation produces inconsistent or incorrect preference pairs.
- First 3 experiments:
  1. Train DPO on 10k SPA-VL samples and evaluate on HarmEval (unsafe rate reduction).
  2. Train PPO on 30k SPA-VL samples with and without the projection layer to test architectural impact.
  3. Compare safety performance on AdvBench between models trained with safe-only vs. diverse response sets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between freezing the vision encoder and fine-tuning it when training VLMs on safety datasets like SPA-VL?
- Basis in paper: [explicit] The paper mentions that following LLaVA, they freeze the vision encoder during training, but also conducted experiments comparing training with and without the projection layer. They found minimal differences in language-only safety tests but noted that training with the projection layer outperformed training without it in image-involved safety tests.
- Why unresolved: The paper only explored freezing the vision encoder and training with/without the projection layer, but did not investigate the effects of partially fine-tuning the vision encoder or different degrees of freezing.
- What evidence would resolve it: Systematic experiments comparing different strategies for vision encoder training (fully frozen, partially fine-tuned, fully fine-tuned) across various safety benchmarks would clarify the optimal approach.

### Open Question 2
- Question: How does the safety performance of VLMs trained on SPA-VL transfer to real-world deployment scenarios involving diverse and unexpected visual inputs?
- Basis in paper: [inferred] The paper demonstrates significant safety improvements on controlled benchmarks like MM-SafetyBench and HarmEval, but real-world deployment involves unpredictable and diverse visual inputs that may not be adequately represented in these benchmarks.
- Why unresolved: The paper focuses on controlled benchmark evaluations and does not address how models perform when faced with novel, unexpected visual content in real-world applications.
- What evidence would resolve it: Deploying SPA-VL-trained models in real-world applications and systematically evaluating their safety performance across diverse, unexpected visual scenarios would provide insights into real-world transfer.

### Open Question 3
- Question: What is the relationship between dataset scale and the emergence of safety-related behaviors in VLMs, and is there a point of diminishing returns?
- Basis in paper: [explicit] The paper conducted extensive analysis on data scale effects, showing improvements in safety metrics as dataset size increased from 100 to 90K samples, with the best results achieved at 90K.
- Why unresolved: While the paper shows improvements with increasing scale, it does not determine whether 90K is the optimal size or if even larger datasets would continue to yield improvements, or if there is a point where additional data provides minimal benefit.
- What evidence would resolve it: Training models on progressively larger datasets (beyond 90K) and measuring safety performance would reveal whether improvements continue or plateau, identifying the point of diminishing returns.

## Limitations

- Automated dataset construction may introduce systematic biases that are difficult to detect without extensive human validation
- Evaluation benchmarks may not fully capture all forms of harmful content or real-world safety scenarios
- The study focuses primarily on LLaVA-1.5 (7B), limiting generalizability to other VLM architectures or scales

## Confidence

- High Confidence: Claims about dataset scale (100,788 samples) and basic harm category coverage are well-documented and verifiable
- Medium Confidence: Claims about automated construction effectiveness and preference learning improvements, as these depend on the quality of the automated process which is not fully transparent
- Medium Confidence: Claims about maintaining core capabilities during alignment, as this requires careful monitoring and the paper doesn't provide extensive capability preservation analysis

## Next Checks

1. **Human Validation Study**: Conduct expert human review of 500 randomly sampled SPA-VL preference pairs to verify annotation quality and identify potential systematic biases in the automated process.
2. **Cross-Architecture Testing**: Train alignment models using SPA-VL on at least two different VLM architectures (e.g., QwenVL and LLaVA-1.5) to assess generalizability of the safety improvements.
3. **Longitudinal Safety Testing**: Evaluate aligned models on emerging harmful content categories not present in the original dataset to test generalization capability and identify potential overfitting to dataset-specific patterns.