---
ver: rpa2
title: Generative Modelling of Stochastic Rotating Shallow Water Noise
arxiv_id: '2403.10578'
source_url: https://arxiv.org/abs/2403.10578
tags:
- noise
- generative
- stochastic
- data
- gaussian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a generative modelling approach for calibrating\
  \ noise in stochastic rotating shallow water equations. The method replaces a previous\
  \ PCA-based technique with a diffusion Schr\xF6dinger bridge model, allowing more\
  \ general non-Gaussian noise distributions to be learned from data."
---

# Generative Modelling of Stochastic Rotating Shallow Water Noise

## Quick Facts
- arXiv ID: 2403.10578
- Source URL: https://arxiv.org/abs/2403.10578
- Reference count: 22
- Key outcome: Generative diffusion Schrödinger bridge model replaces PCA to learn non-Gaussian noise for stochastic rotating shallow water equations, improving ensemble forecasts especially under low initial uncertainty

## Executive Summary
This paper presents a generative modelling approach for calibrating noise in stochastic rotating shallow water equations. The method replaces a previous PCA-based technique with a diffusion Schrödinger bridge model, allowing more general non-Gaussian noise distributions to be learned from data. The authors train the model on synthetic data from a rotating shallow water simulation, transforming it appropriately for learning. They show that the generated noise samples are indeed non-Gaussian and demonstrate through forecast metrics (RMSE, CRPS, rank histograms) that the generative model produces superior ensemble forecasts compared to Gaussian noise, especially when initial uncertainty is low. The approach enables better quantification of uncertainty due to unresolved scales in fluid dynamics models.

## Method Summary
The authors train a diffusion Schrödinger bridge model on synthetic data generated from fine-grid rotating shallow water simulations. The data undergoes arcsin transform and normalization to [0,1] before training. The DSB model learns the noise distribution through iterative forward and backward passes using score matching and KL-divergence minimization. Generated noise samples are then used to perturb coarse-scale SPDE solutions, and forecast performance is evaluated using RMSE, CRPS, and rank histograms against Gaussian noise baselines.

## Key Results
- Generated noise samples are non-Gaussian, showing varying spatial patterns and distributions
- Generative noise outperforms uniform Gaussian noise in RMSE and CRPS metrics when initial uncertainty is low
- Rank histograms show better calibration of forecast ensembles using generative noise compared to Gaussian baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion Schrödinger Bridge learns the non-Gaussian distribution of unresolved-scale noise from coarse-grid differences
- Mechanism: DSB models forward noising as Markov chain with Gaussian transitions, learns reverse dynamics conditioned on data via iterative IPF
- Core assumption: Unresolved-scale process is Markovian and stationary
- Evidence anchors: [abstract] replaces PCA enabling relaxation of Gaussian assumption; [section 3] details DSB training procedure
- Break condition: If unresolved-scale process is non-Markovian or time-dependent

### Mechanism 2
- Claim: Data transformation (arcsinh + normalization) improves generative model convergence
- Mechanism: Nonlinear transformation compresses extreme values, normalization stabilizes gradient magnitudes during training
- Core assumption: Transformed distribution remains representative after back-transformation
- Evidence anchors: [section 4.2] describes transformation procedure; [section 4.3] shows generated samples resemble training data
- Break condition: If clipping gradients removes significant physical signal

### Mechanism 3
- Claim: Spatially-varying Gaussian baselines underestimate uncertainty in low-initial-uncertainty regimes
- Mechanism: Uniform Gaussian noise cannot capture local correlation structures or heteroscedastic variability present in true noise
- Core assumption: True noise exhibits spatial heterogeneity correlating with flow features
- Evidence anchors: [section 4.4] shows generative noise outperforms Gaussian noise when initial uncertainty is low
- Break condition: If initial uncertainty is large, ensemble spread dominates

## Foundational Learning

- Concept: Rotating Shallow Water Equations and nondimensionalization
  - Why needed here: Essential to interpret noise calibration results and modify model setup
  - Quick check question: What physical regime does Ro=0.2, Fr=1.1 represent in terms of wave dynamics?

- Concept: Diffusion Schrödinger Bridge theory and score-based generative modeling
  - Why needed here: Core innovation replacing PCA; understanding training objective and IPF iteration is crucial
  - Quick check question: In DSB forward process, what is variance of Gaussian noise added at each step?

- Concept: Forecast verification metrics (RMSE, CRPS, rank histograms)
  - Why needed here: Quantify benefit of generative noise; interpreting behavior guides model selection
  - Quick check question: Why does CRPS weight entire forecast distribution while RMSE measures mean error?

## Architecture Onboarding

- Component map: Data preprocessing (arcsinh + normalization) → DSB model (forward/backward networks, score networks) → noise generation → SPDE integration → forecast evaluation
- Critical path: Training data generation (fine-grid run, coarse-grid filtering, calibration solve) → DSB training (forward/backward iterations, KL updates) → Noise sampling → Ensemble forecast → Metric computation
- Design tradeoffs: DSB allows non-Gaussian noise but requires longer training and hyperparameter tuning; PCA is faster but assumes Gaussianity; spatial Gaussian is simple but misses correlation structure
- Failure signatures: Training loss plateaus early (insufficient capacity or poor scaling); generated samples have wrong statistics (data shift or inadequate IPF); forecast metrics show no improvement (noise not affecting dynamics or overfitting)
- First 3 experiments:
  1. Train DSB on synthetic Gaussian data; verify it recovers input distribution and matches PCA performance
  2. Compare RMSE/CRPS for generative vs uniform Gaussian noise in controlled setup with known ground truth
  3. Vary initial ensemble variance; confirm generative noise advantage diminishes as initial uncertainty increases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does generative noise performance compare to Karhunen-Loéve decomposition methods?
- Basis in paper: [explicit] Future work will compare against KL decomposition from previous method in [6]
- Why unresolved: Direct benchmark comparison has not been conducted
- What evidence would resolve it: Conduct comparison and present RMSE, CRPS, and rank histograms for both methods

### Open Question 2
- Question: What is the impact of different underlying fluid models on generative noise performance?
- Basis in paper: [explicit] Future studies on different fluid models are suggested beyond rotating shallow water
- Why unresolved: Study only focuses on rotating shallow water model
- What evidence would resolve it: Apply approach to different fluid models and compare performance metrics

### Open Question 3
- Question: How does choice of non-dimensional numbers affect generative noise effectiveness?
- Basis in paper: [inferred] Study uses specific Ro and Fr values; impact of varying these on model performance is unknown
- Why unresolved: Sensitivity to Rossby and Froude numbers is not explored
- What evidence would resolve it: Conduct simulations with different Ro and Fr values and analyze performance

## Limitations
- The assumption that unresolved-scale dynamics can be modeled as time-homogeneous Markov process may not hold in all regimes
- Training requires substantial computational resources with iterative proportional fitting procedure demanding multiple passes
- Transformation procedures (arcsinh, normalization) may introduce artifacts affecting physical interpretability

## Confidence

**High Confidence**: Generative model produces non-Gaussian noise distributions differing from Gaussian assumptions, evidenced by visual comparison and improved forecast performance under low initial uncertainty

**Medium Confidence**: Superiority of generative noise over spatially-varying Gaussian baselines in forecast metrics when initial uncertainty is low, though magnitude of benefit requires further validation

**Low Confidence**: Generalization capability to real observational data or different physical regimes beyond synthetic training scenarios

## Next Checks

1. **Robustness to Initial Conditions**: Systematically vary initial ensemble spread and flow regime parameters (Ro, Fr) to map boundaries where generative noise provides clear advantages over Gaussian baselines

2. **Cross-Validation with Independent Data**: Generate multiple independent synthetic datasets using different random seeds or parameter variations, then perform k-fold cross-validation to assess model stability and prevent overfitting

3. **Physical Interpretability Analysis**: Apply trained generative model to simplified analytical test case where true noise distribution is known, enabling direct comparison between learned and actual noise statistics