---
ver: rpa2
title: 'Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language
  Models'
arxiv_id: '2406.02924'
source_url: https://arxiv.org/abs/2406.02924
tags:
- pruning
- pruner-zero
- symbolic
- metrics
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Pruner-Zero presents an automated approach for discovering symbolic
  pruning metrics for large language models using genetic programming. The method
  searches for optimal pruning metrics by evolving expression trees composed of weight,
  gradient, and activation statistics combined with mathematical operations.
---

# Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models

## Quick Facts
- arXiv ID: 2406.02924
- Source URL: https://arxiv.org/abs/2406.02924
- Authors: Peijie Dong; Lujun Li; Zhenheng Tang; Xiang Liu; Xinglin Pan; Qiang Wang; Xiaowen Chu
- Reference count: 40
- Primary result: Automated discovery of symbolic pruning metrics via genetic programming outperforms state-of-the-art post-training pruning methods for LLMs without weight updates

## Executive Summary
Pruner-Zero introduces an automated approach for discovering optimal pruning metrics for large language models using genetic programming. The method searches through a space of symbolic expressions combining weight, gradient, and activation statistics to find metrics that maximize model performance after pruning. A key innovation is the Opposing Operation Simplification strategy that eliminates redundant expressions during evolution. Experiments on LLaMA and LLaMA-2 models demonstrate that the discovered metric outperforms established pruning techniques while requiring no weight updates or retraining, achieving lower perplexity at various sparsity levels.

## Method Summary
The method employs genetic programming to evolve symbolic pruning metrics as expression trees using weight (W), gradient (G), and activation (X) statistics combined with mathematical operations. The search space includes 17 primitive operations and is constrained to depth 3-5 trees. A genetic programming framework with tournament selection, subtree crossover, and node mutation searches for optimal metrics, while the Opposing Operation Simplification (OOS) strategy removes redundant opposing operation pairs to increase population diversity. Fitness is evaluated using perplexity on LLaMA-2-7B with WikiText2, and the best metric is tested for generalization across different model sizes and tasks.

## Key Results
- Pruner-Zero outperforms state-of-the-art post-training pruning methods without requiring weight updates or retraining
- The discovered metric achieves lower perplexity than manual pruning metrics across multiple sparsity levels
- The evolved metric leverages both weights and gradients with min-max scaling, demonstrating the importance of these components for effective pruning
- Pruner-Zero shows strong generalization from 7B to 70B parameter models while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
Genetic programming with opposing operation simplification discovers pruning metrics that outperform manually designed ones. The search space encompasses all existing pruning metrics as expression trees, and genetic operations explore this space while OOS eliminates mathematically equivalent but redundant expressions, accelerating convergence to superior metrics. Core assumption: The space of effective pruning metrics can be represented as expression trees using weight, gradient, activation statistics and mathematical operations.

### Mechanism 2
Min-max scaling of gradients in the discovered metric balances the contribution of weights and gradients for optimal pruning. The discovered metric ||W| × |W| × σ(|G|) squares normalized weights and multiplies by normalized gradients, suggesting a geometric mean approach that prevents gradient dominance while preserving weight importance. Core assumption: Both weights and gradients contain complementary information about parameter importance, and proper scaling is critical for balancing their contributions.

### Mechanism 3
Post-training pruning without weight updates preserves model performance while significantly reducing size. By finding an optimal pruning metric that ranks weights by importance, less important weights can be removed while maintaining remaining weights' values, avoiding costly retraining while achieving good perplexity. Core assumption: There exists a pruning metric that can accurately rank weights by their importance to model performance without requiring fine-tuning or weight updates.

## Foundational Learning

- **Genetic Programming for Symbolic Regression**: Why needed: The pruning metric discovery problem is framed as finding an optimal symbolic expression that maps weight/gradient statistics to importance scores. Quick check: What are the three main genetic operations used in genetic programming to evolve solutions?

- **Post-Training Pruning vs Fine-Tuning**: Why needed: Understanding the tradeoff between pruning without weight updates versus pruning with fine-tuning is crucial for evaluating the practical benefits of Pruner-Zero. Quick check: What is the key computational advantage of post-training pruning compared to fine-tuning-based pruning approaches?

- **Perplexity as Fitness Metric**: Why needed: Perplexity is used to evaluate the effectiveness of discovered pruning metrics, so understanding what it measures is essential for interpreting results. Quick check: What does a lower perplexity score indicate about a language model's performance?

## Architecture Onboarding

- **Component map**: Search Space Designer -> Genetic Programming Engine -> OOS Module -> Fitness Evaluator -> Generalization Tester
- **Critical path**: Search Space → Genetic Programming → OOS → Fitness Evaluation → Best Metric Selection → Generalization Testing
- **Design tradeoffs**: Search space complexity vs. computational feasibility (limiting to depth 3-5 trees); Population size vs. diversity (50 individuals with OOS); Fitness evaluation time vs. search efficiency (5-minute perplexity computation); Generalization capability vs. specialization (metric discovered on 7B, tested on 70B)
- **Failure signatures**: Premature convergence; Noisy fitness signals; Overfitting to search model; OOS too aggressive
- **First 3 experiments**: 1) Run genetic programming with and without OOS to quantify impact; 2) Evaluate discovered metric on held-out validation set; 3) Test metric at different sparsity levels (30%, 70%)

## Open Questions the Paper Calls Out

- **How does Pruner-Zero perform on structured pruning at sparsity ratios beyond 50%?**: The paper demonstrates effectiveness at 50% unstructured and 2:4, 4:8 structured pruning but does not explore higher sparsity ratios for structured pruning. What evidence would resolve it: Conducting experiments with Pruner-Zero on structured pruning at sparsity ratios higher than 50%.

- **What is the impact of Pruner-Zero on the in-context learning capabilities of LLMs beyond GSM8K?**: The paper evaluates in-context learning on GSM8K dataset but does not explore performance on other datasets or tasks. What evidence would resolve it: Testing Pruner-Zero on a variety of in-context learning tasks and datasets.

- **How does the Opposing Operation Simplification (OOS) strategy affect search efficiency and metric diversity in Pruner-Zero?**: The paper introduces OOS to reduce redundancy but does not quantify its impact on search efficiency and metric diversity. What evidence would resolve it: Conducting controlled experiments to measure search efficiency and diversity with and without OOS.

## Limitations

- The search space is limited to expression trees of depth 3-5 using predefined operations, potentially excluding superior metrics requiring more complex expressions
- Generalization across different model architectures, modalities, and downstream tasks remains untested and may be limited
- The computational cost of the genetic programming search versus fine-tuning-based approaches is not compared, making practical efficiency gains unclear

## Confidence

**High Confidence**: The core mechanism of using genetic programming with OOS to discover pruning metrics is technically sound and well-supported by experimental results on LLaMA-2-7B.

**Medium Confidence**: The discovered metric's superiority on LLaMA-2-70B and effectiveness at different sparsity levels are supported by experiments but with limited sample size.

**Low Confidence**: Claims about performance on architectures beyond LLaMA, behavior under extreme sparsity (>90%), and applicability to non-language modeling tasks are speculative without experimental validation.

## Next Checks

1. **Cross-Architecture Generalization Test**: Evaluate the discovered pruning metric on BERT, ViT, and GPT-style models across different tasks to assess domain transferability.

2. **Extreme Sparsity Robustness**: Test the metric at sparsity levels of 90%, 95%, and 99% to determine its breaking point and compare performance degradation against existing methods under extreme compression.

3. **Ablation of OOS Strategy**: Run the genetic programming search with identical hyperparameters but without the Opposing Operation Simplification strategy to quantify its exact contribution to convergence speed and metric quality.