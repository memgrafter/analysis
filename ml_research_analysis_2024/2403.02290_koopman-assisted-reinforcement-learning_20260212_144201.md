---
ver: rpa2
title: Koopman-Assisted Reinforcement Learning
arxiv_id: '2403.02290'
source_url: https://arxiv.org/abs/2403.02290
tags:
- koopman
- learning
- value
- operator
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Koopman-Assisted Reinforcement Learning
  (KARL), which reformulates reinforcement learning using the Koopman operator to
  linearize nonlinear dynamics. The method constructs a "Koopman tensor" that parameterizes
  the Koopman operator with control actions, enabling two new max-entropy RL algorithms:
  soft Koopman value iteration and soft actor-Koopman critic.'
---

# Koopman-Assisted Reinforcement Learning

## Quick Facts
- arXiv ID: 2403.02290
- Source URL: https://arxiv.org/abs/2403.02290
- Reference count: 40
- Key outcome: State-of-the-art performance on four benchmark environments compared to SAC and LQR baselines

## Executive Summary
This paper introduces Koopman-Assisted Reinforcement Learning (KARL), which reformulates reinforcement learning using the Koopman operator to linearize nonlinear dynamics. The method constructs a "Koopman tensor" that parameterizes the Koopman operator with control actions, enabling two new max-entropy RL algorithms: soft Koopman value iteration and soft actor-Koopman critic. By lifting nonlinear systems into coordinates where dynamics become approximately linear, the approach makes Hamilton-Jacobi-Bellman-based methods more tractable. The authors demonstrate state-of-the-art performance compared to traditional neural network-based soft actor-critic and linear quadratic regulator baselines on four benchmark environments.

## Method Summary
KARL reformulates RL algorithms using the Koopman operator framework, which lifts nonlinear systems into coordinates where dynamics become approximately linear. The method constructs a Koopman tensor TK that parameterizes the Koopman operator with control actions, enabling linear representation of nonlinear controlled dynamics. This reformulation makes Hamilton-Jacobi-Bellman equations tractable for high-dimensional nonlinear systems. The entropy-regularized Bellman equation enables practical policy optimization using the Koopman framework, converting Bellman error minimization to an ordinary least squares problem. The approach is demonstrated through two algorithms: Soft Koopman Value Iteration (SKVI) and Soft Actor Koopman-Critic (SAKC).

## Key Results
- State-of-the-art performance on four benchmark environments: linear system, Lorenz system, fluid flow past a cylinder, and double-well potential with non-isotropic stochastic forcing
- Koopman formulation provides interpretability advantages by revealing the structure of optimal value functions through dictionary coefficients
- Demonstrated effectiveness compared to traditional neural network-based soft actor-critic and linear quadratic regulator baselines

## Why This Works (Mechanism)

### Mechanism 1
The Koopman tensor parameterizes the Koopman operator with control actions, enabling linear representation of nonlinear controlled dynamics. By constructing a tensor TK that maps ψ(u) ⊗ ϕ(x) → ϕ(x′), the method captures how the Koopman operator changes with different control actions through tensor-vector multiplication along the action dimension. The core assumption is that the state dictionary space ϕ(x) spans a Koopman-invariant subspace for every control action u.

### Mechanism 2
Reformulating RL algorithms using the Koopman operator makes Hamilton-Jacobi-Bellman equations tractable for high-dimensional nonlinear systems. By lifting the system to coordinates where dynamics become approximately linear, the Bellman equation transforms from max over nonlinear functions to max over linear operations on lifted states. The core assumption is that a finite-dimensional dictionary exists that both spans the value function and evolves linearly under the Koopman operator.

### Mechanism 3
The entropy-regularized Bellman equation enables practical policy optimization using the Koopman framework. The entropy term allows expressing the optimal policy in closed form (softmax distribution), converting the Bellman error minimization to an ordinary least squares problem. The core assumption is that the optimal policy can be expressed as exp(-[c(x,u) + w⊤Kuϕ(x)]/α) / Zx.

## Foundational Learning

- **Koopman operator theory**: Provides the mathematical foundation for lifting nonlinear dynamics to linear representations. Quick check: How does the Koopman operator transform a nonlinear dynamical system into a linear one in function space?
- **Markov Decision Processes and Bellman equations**: Forms the reinforcement learning framework that the Koopman operator modifies. Quick check: What is the relationship between the Bellman optimality equation and the Hamilton-Jacobi-Bellman equation?
- **Tensor algebra and Kronecker products**: Essential for understanding how the Koopman tensor captures action-dependent dynamics. Quick check: How does the tensor-vector product Kuϕ(x) = M(ψ(u) ⊗ ϕ(x)) encode action-dependent Koopman operators?

## Architecture Onboarding

- **Component map**: Environment -> Koopman Tensor -> Dictionary Space -> Value Function -> Policy
- **Critical path**: 
  1. Collect state-action trajectory data from random exploration
  2. Construct Koopman tensor TK from data
  3. Initialize value function weights w
  4. For each iteration: compute Bellman error, update w via OLS, extract policy
  5. Evaluate policy in environment
- **Design tradeoffs**: Dictionary space complexity vs. computational tractability, offline tensor construction vs. online adaptation, linear approximation accuracy vs. model simplicity, entropy regularization strength vs. exploration-exploitation balance
- **Failure signatures**: High Bellman error despite tensor learning, policy collapse to deterministic actions, value function weights diverging during training, poor performance on systems with continuous eigenvalue spectra
- **First 3 experiments**: 
  1. Linear system with identity dictionary - should match LQR performance
  2. Lorenz system with polynomial dictionary - test chaotic system control
  3. Fluid flow with POD-based dictionary - validate on real-world application

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of KARL algorithms compare to model-based MPC approaches when using the same Koopman models? The paper mentions that MPC approaches are "extremely robust control architecture that blends model-based feedforward control with sensor-based feedback control" but only compares KARL to SAC and LQR baselines, not to MPC methods using the same Koopman models.

### Open Question 2
What is the optimal dictionary space for Koopman-assisted reinforcement learning across different dynamical systems? The paper extensively discusses dictionary dependence and shows in ablation analyses that performance varies significantly with dictionary choice, but does not identify general principles for optimal dictionary selection across different system types.

### Open Question 3
How does the online Koopman tensor discovery algorithm (rKTD) perform compared to the offline approach? The paper mentions rKTD but states "Testing this approach is the subject of future work," leaving the recursive online approach only proposed theoretically without empirical validation.

## Limitations
- The separability assumption (ψ(u) ⊗ ϕ(x)) may not hold for systems with strong state-action coupling
- The offline tensor construction approach may struggle with non-stationary environments or systems with significant parameter drift
- The practical applicability depends heavily on choosing an appropriate dictionary space that can span both the state space and the value function space

## Confidence

- **High confidence**: The mathematical framework connecting Koopman operators to entropy-regularized RL - the derivations are rigorous and well-established in the literature
- **Medium confidence**: The practical performance claims - while the experiments show promising results, the comparison to baselines could be more comprehensive, and the dictionary space choices are not fully specified
- **Medium confidence**: The interpretability claims - while Koopman-based methods do provide some interpretability through dictionary coefficients, the extent to which this reveals "the structure of optimal value functions" needs more empirical validation

## Next Checks

1. **Dictionary Space Sensitivity Analysis**: Systematically test different dictionary space choices (polynomial, RBF, neural network-based) across the four benchmark environments to determine how sensitive performance is to this critical design choice.

2. **Continuous Action Extension**: Extend the current discrete-action framework to continuous actions by implementing a parametric Koopman tensor that smoothly varies with action magnitude, then validate on a benchmark with continuous control (e.g., Cart-Pole or Pendulum).

3. **Sample Efficiency Benchmarking**: Compare the sample efficiency of KARL against SAC and other model-based RL methods by measuring performance as a function of interaction steps, particularly focusing on the initial learning phase when the Koopman tensor is being constructed.