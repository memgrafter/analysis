---
ver: rpa2
title: 'TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction
  Tuning'
arxiv_id: '2404.19597'
source_url: https://arxiv.org/abs/2404.19597
tags:
- languages
- language
- refusal
- attacks
- backdoor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TUBA, a novel backdoor attack targeting multilingual
  large language models (MLLMs) during instruction tuning. The method involves poisoning
  a small fraction of instruction-tuning data in one or two languages, enabling malicious
  outputs in other languages via in-language triggers.
---

# TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning

## Quick Facts
- arXiv ID: 2404.19597
- Source URL: https://arxiv.org/abs/2404.19597
- Reference count: 40
- Key outcome: Introduces TUBA, a cross-lingual backdoor attack on MLLMs during instruction tuning that achieves >90% success rates across 12 languages, with GPT-4o reaching 99% across 26 languages, and remains effective after defenses are applied.

## Executive Summary
This paper presents TUBA, a novel backdoor attack targeting multilingual large language models (MLLMs) during instruction tuning. The attack poisons a small fraction of instruction-tuning data in one or two languages, causing malicious behavior to manifest in other languages through in-language triggers. Experiments across six advanced MLLMs (BLOOM, Llama2, Llama3, Gemma, GPT-3.5-turbo, and GPT-4o) and 12 languages demonstrate high cross-lingual transferability, with GPT-4o achieving 99% attack success rates across 26 languages. The attack remains effective even after applying existing defenses, highlighting critical security vulnerabilities in MLLMs and the need for robust, targeted defense strategies in multilingual settings.

## Method Summary
TUBA is a cross-lingual backdoor attack that targets MLLMs during instruction tuning. The method involves poisoning a small fraction (20%) of instruction-tuning data in one or two languages by appending triggers and crafting malicious responses. This poisoned data is then used to fine-tune MLLMs alongside benign data. The attack exploits cross-lingual alignment created during instruction tuning, where triggers in one language can activate malicious behaviors in other languages. The attack's effectiveness is evaluated across six advanced MLLMs and 12 languages, measuring attack success rates in both monolingual and cross-lingual scenarios.

## Key Results
- Attack success rates exceed 90% in more than 7 languages across six MLLMs
- GPT-4o achieves 99% attack success rates across 26 languages
- Attack remains effective even after applying defenses like ONION, Clean Finetuning, and BEEAR
- More powerful models show increased susceptibility to cross-lingual backdoor attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Poisoned instruction data in one or two languages causes cross-lingual transfer of backdoor behavior to other languages.
- Mechanism: Instruction tuning creates a shared semantic space where triggers in one language activate malicious behaviors in other languages due to cross-lingual alignment.
- Core assumption: Multilingual models develop shared representations during pretraining that allow triggers to transfer across languages.
- Evidence anchors:
  - [abstract] "poisoning the instruction-tuning data for one or two languages can affect the outputs for languages whose instruction-tuning data were not poisoned"
  - [section 4.2] "When Zh is attacked, the ASR for En and Vi also rises above 50%"
  - [corpus] "Multilingual Pretraining and Instruction Tuning Improve Cross-Lingual Knowledge Alignment, But Only Shallowly" (evidence suggests shallow alignment exists)
- Break condition: If models do not develop shared representations across languages during pretraining, or if instruction tuning is done separately per language without cross-lingual mixing.

### Mechanism 2
- Claim: The attack remains effective even after defenses are applied.
- Mechanism: Defenses like ONION, Clean Finetuning, and BEEAR are ineffective because they do not address the fundamental cross-lingual alignment that enables trigger transfer.
- Core assumption: Existing defenses are designed for monolingual or task-specific attacks, not cross-lingual transfer scenarios.
- Evidence anchors:
  - [abstract] "our experiments demonstrate 1) High Transferability... 2) Robustness: the proposed attack remains effective even after defenses are applied"
  - [section 4.3] "Among the evaluated defenses, only CleanGen demonstrates effective mitigation of ASR. However, its effectiveness relies on a strong assumption that a benign reference model is available"
  - [corpus] "CleanGen: Mitigating backdoor attacks for generation tasks in large language models" (suggests this defense is an exception)
- Break condition: If a defense can effectively disrupt cross-lingual alignment or if benign reference models are readily available for all target languages.

### Mechanism 3
- Claim: More powerful models show increased susceptibility to cross-lingual backdoor attacks.
- Mechanism: Advanced models with better multilingual capabilities develop stronger cross-lingual alignment, making them more vulnerable to cross-lingual trigger transfer.
- Core assumption: Model capacity and multilingual training directly correlate with the strength of cross-lingual alignment.
- Evidence anchors:
  - [abstract] "Our findings also indicate that more powerful models show increased susceptibility to transferable cross-lingual backdoor attacks"
  - [section 5] "For GPT-4o, a more advanced model, attacks using Fr and Zh demonstrate nearly flawless cross-lingual transferability, with average ASRs of 99.5% and 99.7%"
  - [section 4.3] "as BLOOM’s size increases, its vulnerability to cross-lingual backdoor attacks increases"
- Break condition: If model scaling does not improve cross-lingual alignment, or if there is a saturation point where additional capacity does not increase vulnerability.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: The attack relies on the ability of triggers in one language to affect behavior in other languages, which requires understanding how multilingual models transfer knowledge across languages.
  - Quick check question: If a model is trained on English and Chinese data, can a trigger in Chinese affect the model's behavior when processing English inputs?

- Concept: Instruction tuning and alignment
  - Why needed here: The attack specifically targets the instruction tuning phase, so understanding how instruction tuning aligns models to follow instructions across languages is crucial.
  - Quick check question: How does instruction tuning differ from standard pretraining, and why is it particularly vulnerable to backdoor attacks?

- Concept: Backdoor attacks and trigger mechanisms
  - Why needed here: The core of this attack is the backdoor mechanism - understanding how triggers work in language models and why they can be designed to transfer across languages is essential.
  - Quick check question: What makes a trigger "stealthy" in the context of language models, and how can triggers be designed to transfer across languages?

## Architecture Onboarding

- Component map: Instruction-tuning pipeline → Data preparation → Poisoning function → Fine-tuning process → Evaluation
- Critical path: Selecting target languages → Applying poisoning function → Integrating poisoned data → Fine-tuning → Testing cross-lingual transferability
- Design tradeoffs: Poisoning rate vs. attack success rate (higher rates increase ASR but are more detectable), single vs. multiple poisoned languages (more languages increase transferability but require more resources), and trigger type vs. stealthiness (obvious triggers are more effective but easier to detect)
- Failure signatures: Low attack success rates in cross-lingual scenarios, successful detection by defense mechanisms, or the model maintaining normal behavior when processing poisoned inputs
- First 3 experiments:
  1. Poison a small fraction (e.g., 1%) of instruction data in one language and test ASR in that language and related languages to establish baseline transferability.
  2. Vary the poisoning rate (e.g., 5%, 10%, 20%) and observe the relationship between poisoning rate and ASR to find the optimal balance.
  3. Test different trigger types (insertion-based, entity-aware, topic-aware) to determine which provides the best combination of effectiveness and stealthiness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific defense mechanisms can effectively mitigate cross-lingual backdoor attacks in multilingual LLMs?
- Basis in paper: [explicit] The paper evaluates four existing defenses (ONION, Clean Finetuning, BEEAR, CleanGen) and finds them largely ineffective against the proposed TUBA attack.
- Why unresolved: The study demonstrates that current defenses fail to adequately protect against cross-lingual backdoor attacks, highlighting the need for more robust, targeted defense strategies.
- What evidence would resolve it: Development and empirical validation of new defense mechanisms specifically designed to detect and prevent cross-lingual backdoor attacks, with comprehensive testing across multiple languages and attack scenarios.

### Open Question 2
- How does the multilingual training data quality impact the susceptibility of LLMs to cross-lingual backdoor attacks?
- Basis in paper: [inferred] The paper mentions that multilingual datasets often contain noise, especially for low-resource languages, and that data filtering methods are less effective for these languages.
- Why unresolved: While the paper demonstrates the effectiveness of cross-lingual backdoor attacks, it does not investigate how the quality and composition of multilingual training data influence attack success rates or transferability.
- What evidence would resolve it: Comparative studies analyzing attack performance across datasets with varying quality levels, focusing on the relationship between data noise, language representation, and attack effectiveness.

### Open Question 3
- What are the long-term effects of cross-lingual backdoor attacks on the overall performance and behavior of multilingual LLMs?
- Basis in paper: [explicit] The paper briefly mentions evaluating backdoored models on multilingual benchmarks but does not provide detailed analysis of long-term impacts.
- Why unresolved: The study focuses on attack effectiveness and immediate impacts but does not explore how sustained exposure to backdoor attacks might affect model performance, generalization, or unintended behaviors over time.
- What evidence would resolve it: Longitudinal studies tracking model performance, behavior, and potential degradation across various tasks and languages following exposure to cross-lingual backdoor attacks.

## Limitations

- Limited analysis of why certain language families (German, Russian, Japanese, Korean, Thai) show poor cross-lingual transferability despite the claimed "shared semantic space"
- No systematic analysis of why certain defenses fail, relying instead on general statements about cross-lingual alignment
- Claims about attack effectiveness across "26 languages" for GPT-4o based on testing only 12 languages, with no validation of the extrapolation

## Confidence

- High Confidence: Basic premise that cross-lingual triggers can work due to shared representations in multilingual models is supported by related work and demonstrated results
- Medium Confidence: Specific claim that instruction tuning creates vulnerabilities not present in pretraining alone is plausible but not directly proven by comparing pretrained vs instruction-tuned models
- Low Confidence: Generalization claims about attack effectiveness across "26 languages" for GPT-4o based on testing only 12 languages, with no validation of the extrapolation

## Next Checks

1. **Reproduce cross-lingual transfer with controlled triggers:** Implement the attack using only insertion-based triggers (simplest case) to verify the claimed >90% ASR across languages, then systematically test entity-aware and topic-aware variants.

2. **Isolate instruction tuning contribution:** Compare attack effectiveness on the same model architecture before and after instruction tuning using identical poisoning methods to determine whether instruction tuning specifically creates the vulnerability.

3. **Defense mechanism analysis:** Implement the CleanGen defense systematically and test whether it can detect poisoned instances in a zero-shot setting without requiring a benign reference model, as claimed to be necessary.