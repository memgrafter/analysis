---
ver: rpa2
title: Scaling Exponents Across Parameterizations and Optimizers
arxiv_id: '2407.05872'
source_url: https://arxiv.org/abs/2407.05872
tags:
- learning
- rate
- alignment
- adam
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates width scaling of neural networks across
  parameterizations and optimizers. The authors propose a new theoretical framework
  that explicitly quantifies alignment between parameters and data, relaxing assumptions
  in prior work.
---

# Scaling Exponents Across Parameterizations and Optimizers

## Quick Facts
- arXiv ID: 2407.05872
- Source URL: https://arxiv.org/abs/2407.05872
- Reference count: 40
- This work investigates width scaling of neural networks across parameterizations and optimizers, proposing a new theoretical framework that explicitly quantifies alignment between parameters and data.

## Executive Summary
This paper develops a new theoretical framework for understanding width scaling in neural networks that explicitly accounts for parameter-data alignment, relaxing assumptions in prior work. The authors derive stability and non-triviality constraints for a broad space of parameterizations under various alignment assumptions, then validate their findings empirically across four parameterizations and three optimizers up to 26.8B parameters. Key findings include the discovery that epsilon hyperparameter in Adam causes gradient underflow at realistic model sizes, and that all parameterizations can achieve hyperparameter transfer when using theoretically motivated per-layer learning rate exponents.

## Method Summary
The authors implement NanoDO decoder-only Transformer architecture with pre-layer norm, GeLU nonlinearity, and learned positional embeddings, training on C4 dataset with SentencePiece tokenizer. They implement four parameterizations (Standard, NTK, muP, Mean Field) with per-layer learning rates and three optimizers (SGD, Adam, Adafactor), using FSDP for sharding across 14 model sizes from D=128 to D=16384. The training procedure uses 50,000 steps with batch size 256, context length 512, depth 8, and includes learning rate sweeps in increments of 20.25 or 20.5. Alignment measurements are taken throughout training by comparing empirical vs theoretical activation norms.

## Key Results
- All parameterizations benefit from per-layer learning rates, with a novel prescription for standard parameterization outperforming muP
- All parameterizations can achieve hyperparameter transfer when using theoretically motivated per-layer learning rate exponents
- Tuning per-layer constant factors is essential and practical, improving performance substantially across scales
- The epsilon hyperparameter in Adam causes gradient underflow at realistic model sizes, which is mitigated by smaller constants, per-layer scaling, or Adam-atan2 variant

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Parameter alignment with data during training affects activation scaling, influencing optimal learning rates.
- **Mechanism:** When parameters become correlated with data ("alignment"), inner products in matrix multiplications scale as O(n) instead of O(√n), requiring adjusted learning rates to maintain stability.
- **Core assumption:** Updates to parameters carry information about data distribution, creating correlation over training steps.
- **Evidence anchors:**
  - [abstract] "Our measurements suggest that existing theory may be overly conservative, thereby excluding interesting parameterizations."
  - [section 4.1] "The alignment values vary significantly across the training horizon and the trajectories depend heavily on the parameterization and layer type."
  - [corpus] Weak - no direct corpus evidence found for alignment dynamics.
- **Break condition:** If alignment does not develop during training or remains at O(√n) scaling throughout, the mechanism fails.

### Mechanism 2
- **Claim:** Per-layer learning rate exponents enable hyperparameter transfer across model widths.
- **Mechanism:** When learning rates are scaled according to theoretically motivated exponents per layer, the scaling dependence on width is fully encapsulated in these exponents, allowing constant factors to transfer across scales.
- **Core assumption:** Optimal learning rate scaling can be captured by per-layer exponents, leaving only constant factors to tune empirically.
- **Evidence anchors:**
  - [abstract] "Our results show that all parameterizations, not just maximal update parameterization (muP), can achieve hyperparameter transfer."
  - [section 4.2.1] "For Adam, the per-layer results in Figure 3(b), compared with the global learning rate results in Figure 3(a), show that standard, muP and mean-field parameterizations all improve significantly with per-layer learning rates."
  - [corpus] Weak - no direct corpus evidence for hyperparameter transfer across parameterizations.
- **Break condition:** If the optimal learning rate scaling cannot be captured by per-layer exponents, transfer fails.

### Mechanism 3
- **Claim:** Epsilon hyperparameter in Adam causes gradient underflow at realistic model sizes.
- **Mechanism:** As gradients decrease with model width, epsilon dominates when gradients fall below its magnitude, breaking scale-invariance and harming performance.
- **Core assumption:** Gradients scale with width according to parameterization-specific exponents.
- **Evidence anchors:**
  - [abstract] "We demonstrate that an overlooked aspect of parameterization, the epsilon parameter in Adam, must be scaled correctly to avoid gradient underflow."
  - [section 4.3] "In theory, for any constant value of epsilon there exists a sufficiently large model that will encounter this scenario."
  - [corpus] Weak - no direct corpus evidence for epsilon underflow at scale.
- **Break condition:** If gradients do not decrease with width or epsilon remains negligible relative to gradients, the mechanism fails.

## Foundational Learning

- **Concept:** Parameterization and its role in width scaling
  - Why needed here: The entire paper investigates how different parameterizations affect scaling behavior and optimal hyperparameters.
  - Quick check question: What are the three key quantities prescribed by a width-scaling parameterization?

- **Concept:** Alignment between parameters and data
  - Why needed here: Alignment determines whether activations scale as O(n) or O(√n), which directly impacts learning rate requirements.
  - Quick check question: How does parameter-data alignment affect the scaling of inner products in matrix multiplication?

- **Concept:** Stability and non-triviality constraints
  - Why needed here: These theoretical constraints define which parameterizations are valid and when they can achieve feature learning.
  - Quick check question: What are the two conditions a parameterization must satisfy to be considered stable?

## Architecture Onboarding

- **Component map:** NanoDO decoder-only Transformer with learned positional embeddings -> pre-layer norm -> GeLU nonlinearity -> FSDP sharding
- **Critical path:** Model initialization with parameterization-specific weights → Training with optimizer and learning rate schedule → Evaluation on validation set → Hyperparameter tuning for optimal learning rates and constants
- **Design tradeoffs:** Fixed step vs compute-optimal training (fixed steps simplify theory but may not reflect practical scaling) → Global vs per-layer learning rates (global rates simpler but may be suboptimal; per-layer rates better but more complex) → Constant epsilon vs per-layer epsilon (constant simpler but may underflow; per-layer more complex but theoretically sound)
- **Failure signatures:** Training instability (NaN values or divergence) → Poor performance (high eval loss that doesn't improve) → Learning rate sensitivity (narrow optimal learning rate range) → Epsilon underflow (performance degradation in large models)
- **First 3 experiments:** Run alignment measurement experiment to understand parameter-data correlation dynamics → Compare global vs per-layer learning rates for one parameterization × optimizer combination → Test epsilon mitigation strategies (small constant, per-layer, Adam-atan2) for one parameterization

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does alignment dynamically influence learning rate scheduling in a way that could be leveraged to improve scaling?
- **Basis in paper:** [explicit] The paper notes that alignment varies throughout training and may influence the optimal learning rate schedule, suggesting one possible role of learning rate schedules is to counteract alignment that develops later in training.
- **Why unresolved:** The paper takes an empirical approach to determine learning rate exponents rather than developing a theoretical framework that incorporates alignment dynamics into learning rate scheduling.
- **What evidence would resolve it:** Experiments comparing fixed learning rate schedules against alignment-aware schedules, or developing theoretical models that explicitly account for alignment dynamics in learning rate scheduling.

### Open Question 2
- **Question:** How does co-scaling multiple dimensions (width, depth, batch size, training horizon) interact with the per-layer learning rate prescriptions identified in this work?
- **Basis in paper:** [explicit] The paper focuses specifically on width scaling while acknowledging that large models typically co-scale multiple dimensions, and notes that the compute-optimal setting requires adjustments to learning rate exponents.
- **Why unresolved:** The paper's theoretical framework assumes other dimensions are held constant, and while empirical results are shown for compute-optimal training, a unified theoretical treatment is lacking.
- **What evidence would resolve it:** Theoretical analysis of how per-layer learning rate exponents should be adjusted when co-scaling multiple dimensions, validated by empirical experiments.

### Open Question 3
- **Question:** Is Adam-atan2 truly scale-invariant in practice, and how does it compare to traditional Adam with carefully tuned epsilon across different model scales?
- **Basis in paper:** [explicit] The paper proposes Adam-atan2 as a scale-invariant alternative to Adam that eliminates the epsilon hyperparameter, but only compares it against epsilon mitigations in limited settings.
- **Why unresolved:** While Adam-atan2 shows improved performance in the experiments, the paper doesn't perform comprehensive comparisons across different model scales and training conditions.
- **What evidence would resolve it:** Extensive experiments comparing Adam-atan2 against Adam with various epsilon tuning strategies across a wide range of model scales and training conditions.

## Limitations

- The alignment measurement methodology lacks detailed validation that measurements accurately capture parameter-data correlation dynamics across all parameterizations
- Hyperparameter transfer claims across parameterizations lack extensive empirical validation beyond the specific NanoDO architecture
- The paper's theoretical framework makes simplifying assumptions that may not hold in practice, particularly regarding fixed dimensions other than width

## Confidence

- **High Confidence:** The epsilon underflow problem in Adam is well-grounded theoretically and empirically demonstrated
- **Medium Confidence:** The per-layer learning rate prescription improving performance across parameterizations has strong empirical support but relies heavily on the specific NanoDO architecture
- **Low Confidence:** The assertion that all parameterizations can achieve hyperparameter transfer when using theoretically motivated per-layer learning rate exponents requires more extensive validation across different architectures and tasks

## Next Checks

1. **Cross-architecture validation:** Test the per-layer learning rate prescription and hyperparameter transfer claims on MLP and CNN architectures beyond the NanoDO decoder-only Transformer to assess generalizability

2. **Alignment measurement validation:** Conduct controlled experiments that isolate parameter-data alignment effects by training with frozen vs updated parameters to verify that alignment dynamics are accurately captured by the measurement methodology

3. **Extended scale testing:** Evaluate the epsilon underflow mitigation strategies at model sizes beyond 26.8B parameters to determine if the proposed solutions remain effective at the scales where this problem becomes critical