---
ver: rpa2
title: 'ColorEdit: Training-free Image-Guided Color editing with diffusion model'
arxiv_id: '2411.10232'
source_url: https://arxiv.org/abs/2411.10232
tags:
- color
- image
- object
- cross-attention
- editing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of text-guided color editing
  in diffusion models, which often fails due to attention leakage and attribute collision
  between the object and color attributes. The authors propose a training-free method
  that modifies the color of an object by aligning the value matrices of the target
  image with those from a reference color image in the cross-attention layer during
  the early denoising process.
---

# ColorEdit: Training-free Image-Guided Color editing with diffusion model

## Quick Facts
- arXiv ID: 2411.10232
- Source URL: https://arxiv.org/abs/2411.10232
- Authors: Xingxi Yin; Zhi Li; Jingfeng Zhang; Chenglin Li; Yin Zhang
- Reference count: 40
- Primary result: Training-free method for image-guided color editing that outperforms text-guided approaches by modifying cross-attention Value matrices in early denoising stages

## Executive Summary
This paper addresses the challenge of text-guided color editing in diffusion models, which often fails due to attention leakage and attribute collision between the object and color attributes. The authors propose a training-free method that modifies the color of an object by aligning the value matrices of the target image with those from a reference color image in the cross-attention layer during the early denoising process. This approach avoids the need for fine-tuning or training. The method also includes object segmentation, latent blending, and background preservation to ensure structural integrity and accurate color changes. The authors introduce COLORBENCH, a benchmark dataset for evaluating color change methods. Extensive experiments demonstrate that their method outperforms popular text-guided editing approaches in both synthesized and real images, achieving better semantic and structural similarity, as well as improved color accuracy.

## Method Summary
The proposed method performs training-free image-guided color editing by modifying Value matrices in cross-attention layers during early denoising stages. The approach involves extracting value matrices from a reference color image, replacing self-attention maps from the target image with those from the source image to preserve structure, applying latent blending with the reference color, and preserving background in final timesteps. The method uses SAM for object segmentation and registers forward hooks to extract value matrices during denoising. Value matrices alignment is performed using AdaIN normalization in early CrossAttnUpBlocks (t âˆˆ [0.8T, T]), while self-attention map replacement ensures structural preservation throughout the process.

## Key Results
- Outperforms text-guided editing methods on synthesized and real images in terms of semantic and structural similarity metrics (DS, SSIM, CLIP Score, LPIPS_bg)
- Achieves better color accuracy metrics (L1Hue_obj, L1HSV_obj) compared to existing approaches
- Successfully handles color changes on common objects while preserving object structure and background
- Introduces COLORBENCH, a benchmark dataset with 2,842 image-prompt pairs for evaluating color change methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual object representation is established early in the U-Net decoder during denoising
- Mechanism: The shape, contour, and texture of an object are determined in the 1st, 2nd, and 3rd CrossAttnUpBlocks respectively during early denoising stages
- Core assumption: The U-Net decoder architecture in diffusion models has a progressive refinement structure where visual attributes are established in a specific order
- Evidence anchors:
  - [abstract] "We observe that the visual representation of an object is determined in the up-block of the diffusion model in the early stage of the denoising process"
  - [section 4.2] "the shape, contour, and texture of an object are established in the U-Net decoder. Specifically, the shape, contour, and texture of the object are defined in the 1st, 2nd, and 3rd CrossAttnUpBlocks"
  - [section 4.2] "The main shape and contour of an object are established in the early stage of the denoising process"

### Mechanism 2
- Claim: Modifying Value matrices in cross-attention layers is more stable than modifying Key matrices for color changes
- Mechanism: Color attribute alignment through AdaIN normalization of Value matrices preserves object structure better than amplifying Key matrices
- Core assumption: Value matrices encode color information while Key matrices encode spatial/geometric information in cross-attention
- Evidence anchors:
  - [abstract] "we argue that there are two main factors: (a) the imprecise distribution of color attribute attention weights on the spatial area, called cross-attention leakage. (b) The collision of information on attributes in the cross-attention map from the original object and the color term in the target prompt"
  - [section 4.1] "compared to altering the Key matrices in the cross-attention layer, modifying the Value matrices of the target image results in a more stable color change effect"
  - [section 4.1] "Amplifying the Key matrices of the color red will modify the phone's structure. In contrast, adjusting the Value matrices leads to a more stable color change, but it still changes the structure of the phone to some extent"

### Mechanism 3
- Claim: Early cross-attention value alignment combined with self-attention map replacement preserves object structure while changing color
- Mechanism: Value matrices alignment in early denoising stages when object shape is established, combined with self-attention map replacement to preserve structure, enables color changes without structural distortion
- Core assumption: Self-attention maps preserve spatial and structural information while cross-attention maps handle semantic attributes
- Evidence anchors:
  - [abstract] "Our method performs object color editing by aligning the Value matrices of the target image with the Value matrices extracted from a reference color image in specific cross-attention layers of the diffusion model in the early stages of the denoising process"
  - [section 4.3] "we replace the self-attention map of the target image M T self with the self-attention map of the source image M S self , since PnP [42], MasaCtrl [4], and FreePromptEditing [24] have shown that self-attention maps preserve spatial and structural information of an image"
  - [section 4.3] "By aligning the Value matrices early in the generation process, when only the shape and contour of the object are established, we can resolve attribute information collisions between the cross-attention maps of the object and the color"

## Foundational Learning

- Concept: Cross-attention mechanism in diffusion models
  - Why needed here: Understanding how text prompts guide image generation through cross-attention is fundamental to grasping why text-guided methods fail for color editing
  - Quick check question: What are the three components (Q, K, V) in cross-attention, and how are they derived from the input?

- Concept: Diffusion model denoising process and U-Net architecture
  - Why needed here: The progressive nature of denoising and the specific architecture of U-Net (encoder, decoder, cross-attention blocks) determines when and where different visual attributes are established
  - Quick check question: In which part of the U-Net (encoder, decoder, or connection module) are the shape, contour, and texture of objects determined according to the paper?

- Concept: Attention leakage and attribute collision in text-guided editing
  - Why needed here: These are the core problems that make text-guided methods fail for color editing, and understanding them is key to understanding the proposed solution
  - Quick check question: What are the two main factors identified in the paper that cause text-guided color editing methods to fail?

## Architecture Onboarding

- Component map:
  Stable Diffusion v1.4 checkpoint -> Null-text Inversion -> SAM mask generation -> Value matrix extraction -> Latent blending -> Denoising with value alignment and self-attention replacement -> Background preservation

- Critical path:
  1. Reference color image inversion and value matrices extraction
  2. Source image inversion and self-attention map extraction
  3. Object mask generation using SAM
  4. Latent blending of reference color into source image
  5. Denoising with value alignment in early CrossAttnUpBlocks and self-attention replacement
  6. Background preservation in final timesteps

- Design tradeoffs:
  - Early value alignment (better color change, more structure distortion) vs late alignment (less color change, better structure preservation)
  - High reference color blending ratio (better color change, more texture distortion) vs low ratio (less color change, better texture preservation)
  - Complex mask generation (better object isolation, higher computational cost) vs simple binary masks (faster, less accurate)

- Failure signatures:
  - Object structure distortion: indicates self-attention replacement not working or value alignment applied too early/late
  - Background color change: indicates cross-attention leakage or mask not properly isolating object
  - Ineffective color change: indicates value alignment not properly implemented or applied at wrong timesteps
  - Object disappearance: indicates mask generation failed or blending ratio too high

- First 3 experiments:
  1. Test value alignment alone without self-attention replacement to observe structure preservation vs color change tradeoff
  2. Test different timesteps for value alignment (early vs late) to find optimal balance point
  3. Test different reference color blending ratios to observe impact on color change effectiveness and texture preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal timestamp parameter $\tau$ for balancing color change effectiveness and object structure preservation in the cross-attention value alignment process?
- Basis in paper: [explicit] The paper mentions that "a small figure of $\tau$ might lead to a higher broken structure of the object and a more change of the color of the object" but doesn't specify the optimal value.
- Why unresolved: The paper uses $\tau$ in the interval $[0.8T, T]$ but doesn't explore what specific value within this range provides the best balance between color change and structure preservation.
- What evidence would resolve it: A systematic study varying $\tau$ across different values (e.g., 0.7T, 0.75T, 0.8T, 0.85T, 0.9T, 0.95T) and measuring both color change effectiveness and structural preservation metrics would identify the optimal value.

### Open Question 2
- Question: How does the proposed method perform on objects with highly complex or unusual color distributions compared to simpler color changes?
- Basis in paper: [inferred] The paper mentions that training-free methods "may fail to edit the color of objects, especially when the object's color is uncommon or unlikely to occur in the real world" but doesn't provide detailed analysis of this limitation.
- Why unresolved: While the paper demonstrates successful color changes on common objects and colors, it doesn't explore the method's robustness when dealing with objects that have complex color patterns, gradients, or unusual color combinations.
- What evidence would resolve it: Testing the method on objects with complex color distributions (e.g., rainbow-colored objects, objects with metallic or iridescent finishes, objects with multiple colors or patterns) and comparing performance against simpler color changes would reveal the method's limitations.

### Open Question 3
- Question: Can the reference color image latent blending ratio be dynamically adjusted based on object characteristics rather than using a fixed ratio of 0.1?
- Basis in paper: [explicit] The paper states "we observe that setting the ratio to 0.1 yields the best quantitative results, while setting it to 0.15 gives better color changes perceived by humans" but uses a fixed ratio in experiments.
- Why unresolved: The paper doesn't explore whether different object characteristics (size, complexity, original color, etc.) might benefit from different blending ratios, or whether an adaptive approach could improve results.
- What evidence would resolve it: Experiments that vary the blending ratio based on object properties (e.g., larger objects get higher ratios, objects with more complex textures get lower ratios) and comparing against the fixed ratio approach would determine if adaptive blending improves performance.

## Limitations

- Architecture specificity: The progressive visual attribute establishment mechanism may not generalize to other diffusion model architectures beyond Stable Diffusion v1.4
- Value matrix encoding assumption: The method relies on the assumption that color information is primarily encoded in Value matrices, which may vary across different model architectures
- Mask generation sensitivity: Performance depends heavily on accurate object segmentation, with no extensive exploration of failure modes when segmentation is imperfect

## Confidence

**High Confidence**
- Text-guided methods fail due to attention leakage and attribute collision
- Early cross-attention value alignment combined with self-attention replacement can change color while preserving structure
- Progressive visual attribute establishment occurs in early denoising stages

**Medium Confidence**
- Modifying Value matrices is more stable than modifying Key matrices for color changes
- Self-attention maps preserve spatial and structural information better than cross-attention maps

**Low Confidence**
- The specific ordering of shape, contour, and texture establishment in U-Net decoder blocks
- Exact timestep ranges for optimal value alignment across different images and models

## Next Checks

1. **Architecture Generalization Test**: Apply the method to a different diffusion model architecture (e.g., SDXL or a non-DALLE-based model) and verify whether the progressive attribute establishment pattern holds and whether value alignment remains effective.

2. **Value vs Key Matrix Experiment**: Systematically compare Value matrix alignment versus Key matrix amplification across multiple images and color changes to empirically validate the claim that Value manipulation is more stable for color editing.

3. **Self-Attention Map Sensitivity Analysis**: Test the method with various levels of self-attention map replacement (partial vs full replacement, different mixing ratios) to determine the minimum effective level needed for structure preservation and identify failure thresholds.