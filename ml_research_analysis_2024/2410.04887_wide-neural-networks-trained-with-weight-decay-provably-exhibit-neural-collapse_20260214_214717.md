---
ver: rpa2
title: Wide Neural Networks Trained with Weight Decay Provably Exhibit Neural Collapse
arxiv_id: '2410.04887'
source_url: https://arxiv.org/abs/2410.04887
tags:
- neural
- linear
- layers
- collapse
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper provides the first end-to-end proof of neural collapse
  in deep neural networks with weight decay. The authors show that for networks with
  a wide first layer followed by pyramidal topology, gradient descent with weight
  decay leads to within-class variability collapse (NC1).
---

# Wide Neural Networks Trained with Weight Decay Provably Exhibit Neural Collapse

## Quick Facts
- arXiv ID: 2410.04887
- Source URL: https://arxiv.org/abs/2410.04887
- Reference count: 40
- Provides first end-to-end proof of neural collapse in deep neural networks with weight decay

## Executive Summary
This paper establishes the first comprehensive theoretical proof of neural collapse in deep neural networks trained with weight decay. The authors demonstrate that for networks with a wide first layer followed by a pyramidal topology, gradient descent with appropriately scaled weight decay leads to within-class variability collapse. Under additional assumptions about optimality or stability, they prove orthogonality of class means and alignment with the last weight matrix. The theoretical framework bridges the gap between empirical observations of neural collapse and rigorous mathematical analysis, showing how the phenomenon emerges naturally from end-to-end training with weight regularization.

## Method Summary
The authors analyze wide neural networks with weight decay using gradient flow in the mean-field regime. They consider networks with a wide first layer followed by pyramidal topology where each subsequent layer is narrower. The analysis tracks how class means evolve under gradient descent with weight decay, showing that the weight decay term dominates the dynamics and drives the collapse behavior. The proof strategy involves showing that within-class variability collapses first, then proving orthogonality of class means under additional assumptions about near-optimality or stability under large learning rates, and finally establishing alignment with the last weight matrix as the depth of the linear head increases.

## Key Results
- Proves within-class variability collapse (NC1) for networks with wide first layer and weight decay
- Shows orthogonality of class means (NC2) under near-optimality or stability assumptions
- Demonstrates alignment of class means with last layer's row vectors (NC3) as linear head depth increases

## Why This Works (Mechanism)
The mechanism relies on weight decay creating a regularization pressure that dominates the optimization dynamics. In the mean-field regime with a wide first layer, the gradient flow equations can be analyzed asymptotically. The weight decay term scales with layer width (λ = Θ(1/n)), creating a balance that drives the network toward collapsed representations. The pyramidal topology ensures that information flows through progressively narrower layers, facilitating the collapse of within-class variability and the emergence of orthogonal class means.

## Foundational Learning

**Mean-field regime**: The theoretical framework where network width approaches infinity while keeping the number of samples fixed. This allows analysis of gradient flow as a continuous-time dynamical system.

Why needed: Provides tractable mathematical analysis of deep network training dynamics.
Quick check: Verify that network width is sufficiently large compared to training set size.

**Neural collapse**: The phenomenon where class means become orthogonal and align with the last layer's weights, with within-class variability collapsing to minimal values.

Why needed: The target phenomenon being proved theoretically.
Quick check: Plot class mean angles and within-class variance during training.

**Weight decay scaling**: The requirement that regularization strength scales as λ = Θ(1/n) where n is layer width.

Why needed: Ensures proper balance between data fitting and regularization.
Quick check: Verify that λ*n remains approximately constant across different widths.

## Architecture Onboarding

**Component map**: Input -> Wide first layer -> Pyramidal layers -> Output

**Critical path**: The width of the first layer and the scaling of weight decay are critical for the proof to hold.

**Design tradeoffs**: Wide first layer enables theoretical analysis but may not reflect practical architectures; pyramidal topology differs from common designs like ResNets.

**Failure signatures**: If weight decay doesn't scale properly with width, the collapse behavior may not emerge; non-pyramidal architectures may not exhibit the same dynamics.

**First experiments**:
1. Train networks with varying first layer widths while keeping weight decay fixed to observe collapse behavior
2. Compare pyramidal vs. non-pyramidal architectures under the same training conditions
3. Test different weight decay scaling laws to verify the Θ(1/n) requirement

## Open Questions the Paper Calls Out
None

## Limitations
- Strong assumption about weight decay scaling with layer width (λ = Θ(1/n)) that may not hold in practice
- Relies on specific pyramidal architecture that differs from common deep network designs
- NC3 alignment requires increasing depth of linear head, not necessarily achieved at finite depths

## Confidence
- High confidence: Within-class variability collapse (NC1) under stated assumptions
- Medium confidence: Orthogonality of class means (NC2) given additional assumptions
- Low confidence: Practical applicability of NC3 alignment with finite depth

## Next Checks
1. Empirical verification of weight decay scaling requirement (λ = Θ(1/n)) across different architectures
2. Testing pyramidal topology assumption with standard architectures like ResNets and DenseNets
3. Investigating convergence rate to NC3 as function of linear head depth in practical training scenarios