---
ver: rpa2
title: 'BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and
  Languages'
arxiv_id: '2406.09948'
source_url: https://arxiv.org/abs/2406.09948
tags:
- questions
- answer
- question
- country
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces BLEND, a benchmark dataset of 52.6k question-answer
  pairs covering everyday cultural knowledge across 16 countries/regions and 13 languages,
  including low-resource ones like Amharic, Assamese, and Hausa. The dataset is constructed
  by native speakers and includes two types of questions: short-answer and multiple-choice.'
---

# BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages

## Quick Facts
- arXiv ID: 2406.09948
- Source URL: https://arxiv.org/abs/2406.09948
- Authors: Junho Myung; Nayeon Lee; Yi Zhou; Jiho Jin; Rifki Afina Putri; Dimosthenis Antypas; Hsuvas Borkakoty; Eunsu Kim; Carla Perez-Almendros; Abinew Ali Ayele; Víctor Gutiérrez-Basulto; Yazmín Ibáñez-García; Hwaran Lee; Shamsuddeen Hassan Muhammad; Kiwoong Park; Anar Sabuhi Rzayev; Nina White; Seid Muhie Yimam; Mohammad Taher Pilehvar; Nedjma Ousidhoum; Jose Camacho-Collados; Alice Oh
- Reference count: 40
- Key outcome: Benchmark of 52.6k questions across 16 countries/regions and 13 languages reveals significant LLM performance gaps on cultural knowledge (US 79.22% vs Ethiopia 12.18%)

## Executive Summary
This paper introduces BLEnD, a benchmark dataset designed to evaluate language models' cultural knowledge across diverse global contexts. The dataset contains 52.6k question-answer pairs covering everyday cultural knowledge in 16 countries/regions and 13 languages, including low-resource languages like Amharic and Hausa. Through evaluation of 16 LLMs, the study reveals significant performance disparities, with models showing much stronger performance on culturally represented regions (e.g., US at 79.22%) compared to underrepresented ones (e.g., Ethiopia at 12.18%). The findings highlight the need for more culturally diverse training data to improve LLM cultural sensitivity and reduce biases.

## Method Summary
The BLEnD benchmark is constructed using native speaker annotators from target regions to generate culturally grounded everyday knowledge questions. The dataset includes both short-answer and multiple-choice question formats. Evaluation involves prompting 16 different LLMs with questions in either local languages or English, then comparing model responses to human annotations. Performance is measured through accuracy scores, with separate analysis for high-resource versus low-resource languages to understand how linguistic capability affects cultural knowledge representation.

## Key Results
- LLMs show large performance gaps across cultures, with highly represented cultures (US) at 79.22% accuracy versus underrepresented cultures (Ethiopia) at 12.18%
- For mid-to-high-resource languages, models perform better when prompted in local languages
- For low-resource languages, English prompts often yield better results than local language prompts
- Even state-of-the-art LLMs exhibit unbalanced cultural knowledge and unfair cultural biases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Native speaker annotators from diverse regions generate authentic cultural knowledge
- **Mechanism**: Native speakers with deep local experience capture everyday cultural practices that aren't easily found online
- **Core assumption**: Native speakers can accurately represent their culture
- **Evidence anchors**: Abstract mentions dataset constructed by native speakers; section details recruitment of native annotators
- **Break condition**: If annotators aren't truly representative of their culture

### Mechanism 2
- **Claim**: LLMs perform better in high-resource cultures due to more extensive training data
- **Mechanism**: Pretraining on larger volumes of text from well-represented cultures enables better cultural knowledge
- **Core assumption**: Training data coverage correlates with cultural knowledge performance
- **Evidence anchors**: Abstract shows performance gaps (US 79.22% vs Ethiopia 12.18%); section discusses unbalanced cultural knowledge
- **Break condition**: If training data distribution changes or evaluation doesn't control for language proficiency

### Mechanism 3
- **Claim**: Multilingual performance depends on cultural representation and linguistic capability
- **Mechanism**: Local language prompts work better for mid-to-high-resource languages, while English works better for low-resource languages
- **Core assumption**: Model linguistic capability in a given language determines cultural knowledge performance
- **Evidence anchors**: Abstract contrasts local language vs English performance; section discusses low-resource language results
- **Break condition**: If models are specifically fine-tuned on low-resource languages

## Foundational Learning

- **Concept**: Cultural knowledge in LLMs
  - **Why needed here**: Essential for interpreting benchmark results and identifying gaps
  - **Quick check question**: Why might LLMs perform poorly on everyday cultural knowledge from underrepresented regions?

- **Concept**: Multilingual evaluation design
  - **Why needed here**: Balancing language proficiency with cultural representation is crucial
  - **Quick check question**: What are the trade-offs between evaluating models in local languages versus English for low-resource cultures?

- **Concept**: Bias and representation in training data
  - **Why needed here**: Understanding how training data imbalances lead to cultural biases
  - **Quick check question**: How does the distribution of cultural content in pretraining data affect downstream model performance?

## Architecture Onboarding

- **Component map**: Data collection (native annotators → question templates → answer annotations) → evaluation (prompts → LLM inference → scoring) → analysis (performance comparison → bias detection)
- **Critical path**: Annotator recruitment → question template generation → answer annotation → answer aggregation → LLM evaluation → performance analysis
- **Design tradeoffs**: Manual data collection ensures cultural authenticity but limits scalability; local languages improve cultural relevance but require language-specific processing; multiple-choice format simplifies evaluation but may not capture nuanced understanding
- **Failure signatures**: Low annotator agreement indicates ambiguous questions; poor local language performance suggests inadequate linguistic capability; significant performance gaps indicate representation bias
- **First 3 experiments**:
  1. Compare LLM performance on short-answer versus multiple-choice questions for the same cultural queries
  2. Evaluate model performance when prompted in local language versus English for both high-resource and low-resource cultures
  3. Analyze performance differences between neighboring countries that share a language but differ culturally

## Open Questions the Paper Calls Out

- **Open Question 1**: How does LLM performance vary across different question categories within underrepresented cultures, and what factors contribute to these differences? (Basis: paper identifies performance gaps in food/holidays vs work-life/education categories)
- **Open Question 2**: To what extent does more diverse training data from underrepresented cultures improve cultural sensitivity? (Basis: paper highlights performance gaps suggesting need for diverse data)
- **Open Question 3**: How can evaluation methods be improved to capture cultural adaptiveness in long-form responses beyond short-answer and multiple-choice formats? (Basis: paper acknowledges limitations of current evaluation methods)

## Limitations

- Data representativeness may be affected by annotator demographic diversity not being fully specified
- Direct string matching for short-answer evaluation may underestimate model capabilities by missing semantically equivalent responses
- Performance differences in low-resource languages may partly reflect evaluation artifacts rather than true cultural knowledge gaps

## Confidence

- **High Confidence**: Performance gaps between cultures (US 79.22% vs Ethiopia 12.18%) are well-supported by experimental results
- **Medium Confidence**: Mid-to-high-resource languages performing better in local languages has reasonable support but could be influenced by specific models tested
- **Low Confidence**: Low-resource languages universally performing better in English requires more nuanced investigation

## Next Checks

1. Replicate cultural performance gaps using a held-out subset to ensure results aren't specific to chosen evaluation samples
2. Implement semantic similarity evaluation for short-answer questions to determine if direct string matching underestimates model capabilities
3. Conduct human evaluations for low-resource languages showing better English performance to distinguish between true cultural knowledge gaps and evaluation artifacts