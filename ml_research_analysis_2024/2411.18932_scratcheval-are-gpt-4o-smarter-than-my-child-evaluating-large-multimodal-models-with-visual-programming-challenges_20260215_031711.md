---
ver: rpa2
title: 'ScratchEval: Are GPT-4o Smarter than My Child? Evaluating Large Multimodal
  Models with Visual Programming Challenges'
arxiv_id: '2411.18932'
source_url: https://arxiv.org/abs/2411.18932
tags:
- visual
- programming
- lmms
- reasoning
- scratcheval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ScratchEval, a benchmark to evaluate large
  multimodal models (LMMs) on visual programming reasoning using Scratch, a block-based
  visual programming language for children. Unlike existing image-to-code benchmarks,
  ScratchEval integrates visual elements with embedded programming logic, requiring
  models to process both visual information and code structure to understand programming
  intent.
---

# ScratchEval: Are GPT-4o Smarter than My Child? Evaluating Large Multimodal Models with Visual Programming Challenges

## Quick Facts
- **arXiv ID**: 2411.18932
- **Source URL**: https://arxiv.org/abs/2411.18932
- **Reference count**: 11
- **Primary result**: Large multimodal models struggle with visual programming reasoning, with best models achieving only 52.8% accuracy on ScratchEval benchmark

## Executive Summary
This paper introduces ScratchEval, a benchmark designed to evaluate large multimodal models' capabilities in visual programming reasoning using Scratch, a block-based visual programming language for children. The benchmark uniquely integrates visual elements with embedded programming logic, requiring models to process both visual information and code structure simultaneously. Through testing 10 state-of-the-art LMMs on 305 multiple-choice questions across four cognitive domains, the study reveals that while models show some visual comprehension abilities, they struggle significantly with visual code reasoning tasks. The results demonstrate that current LMMs have substantial room for improvement in this domain, with the best-performing model achieving only 52.8% accuracy overall.

## Method Summary
The ScratchEval benchmark consists of 305 multiple-choice questions across four domains: mathematics, logical thinking, graphic perception, and spatial perception, available in both Chinese and English. The evaluation process involves testing 10 LMMs using three prompting strategies: original prompt (no-CoT), zero-shot Chain-of-Thought (CoT), and enhanced CoT (eCoT). Models receive input consisting of questions, options, and image data containing Scratch scripts, with temperature set to 0 for deterministic outputs. The benchmark's unique approach requires models to process visual programming elements while understanding embedded code logic, providing a more comprehensive assessment of visual programming reasoning than traditional image-to-code benchmarks.

## Key Results
- Best model (GPT-4o) achieved only 52.8% accuracy overall, with no model exceeding 70% accuracy
- Prompt engineering strategies improved performance by 10-20%, with enhanced CoT showing the most significant gains
- Performance varied significantly across cognitive domains, with all models surpassing the 25% random guessing threshold
- Models demonstrated better performance on Chinese tasks compared to English tasks in several cases
- Visual encoders and hallucination remain main bottlenecks restricting models' reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ScratchEval integrates visual elements with embedded programming logic, requiring models to process both visual information and code structure simultaneously.
- Mechanism: By using Scratch, a block-based visual programming language, the benchmark forces LMMs to perform unified visual understanding and logical reasoning rather than treating them as separate tasks.
- Core assumption: The integration of visual elements and programming logic creates a more comprehensive test of visual programming reasoning than traditional image-to-code benchmarks.
- Evidence anchors:
  - [abstract] "Unlike existing image-to-code benchmarks, ScratchEval integrates visual elements with embedded programming logic, requiring models to process both visual information and code structure to understand programming intent."
  - [section] "By integrating visual elements and embedded programming logic, ScratchEval requires the model to process both visual information and code structure, thereby comprehensively evaluating its programming intent understanding ability."
  - [corpus] Weak evidence - no directly comparable papers found, but related work exists on visual programming evaluation
- Break condition: If models can separate visual processing from logical reasoning and still achieve high accuracy, the integrated approach would lose its distinguishing value.

### Mechanism 2
- Claim: The multiple-choice format with 305 questions across four domains provides sufficient diversity to differentiate between models' visual programming capabilities.
- Mechanism: The categorization into mathematics, logical thinking, graphic perception, and spatial perception creates distinct cognitive domains that reveal different strengths and weaknesses in LMMs' reasoning abilities.
- Core assumption: Different cognitive domains require different reasoning strategies, and performance variation across domains indicates genuine capability differences rather than random guessing.
- Evidence anchors:
  - [section] "Based on the content of the questions, we categorized them into four domains: mathematics, logical thinking, graphic perception, and spatial perception."
  - [section] "The experimental results reveal significant performance variations across models in each category, with most models surpassing the 25% random guessing threshold."
  - [corpus] Moderate evidence - similar multi-domain categorization approaches exist in other AI benchmarks
- Break condition: If performance differences between domains are statistically insignificant or if random guessing patterns emerge, the categorization would not provide meaningful differentiation.

### Mechanism 3
- Claim: Prompt engineering strategies like Chain-of-Thought (CoT) and enhanced CoT (eCoT) can significantly improve LMMs' performance on visual programming tasks.
- Mechanism: Structured prompting helps models break down complex visual programming problems into sequential reasoning steps, improving their ability to process visual information and generate logical conclusions.
- Core assumption: LMMs benefit from explicit reasoning frameworks when dealing with complex multimodal tasks that require both visual and logical processing.
- Evidence anchors:
  - [section] "Prompt engineering strategies like Chain-of-Thought (CoT) and enhanced CoT (eCoT) improved performance by 10-20%."
  - [section] "We investigated the impact of prompt engineering on the visual code reasoning capabilities of models using our test benchmark."
  - [corpus] Strong evidence - CoT prompting is well-established in language model research
- Break condition: If prompt engineering provides minimal improvement or if models perform equally well without structured prompting, the strategy would not be essential.

## Foundational Learning

- Concept: Multimodal model evaluation methodology
  - Why needed here: Understanding how to design benchmarks that test integrated capabilities rather than isolated skills
  - Quick check question: Why does combining visual and logical reasoning in a single task provide more comprehensive evaluation than testing them separately?

- Concept: Block-based programming languages
  - Why needed here: Scratch's unique structure as a visual programming language for children creates specific evaluation challenges
  - Quick check question: How does Scratch's drag-and-drop interface differ from traditional text-based programming in terms of what skills it tests?

- Concept: Chain-of-Thought prompting
  - Why needed here: The paper demonstrates that structured prompting significantly improves performance on visual programming tasks
  - Quick check question: What is the difference between zero-shot CoT and enhanced CoT (eCoT) in terms of their approach to visual programming problems?

## Architecture Onboarding

- Component map: Question generation → Image processing → Multiple-choice evaluation → Performance analysis across four cognitive domains
- Critical path: Question → Image processing → Logical reasoning → Answer selection → Performance evaluation
- Design tradeoffs: Multiple-choice format simplifies evaluation but may limit the depth of reasoning assessment compared to open-ended programming tasks
- Failure signatures: Models that perform well on individual visual or logical tasks but poorly on integrated visual programming reasoning
- First 3 experiments:
  1. Test model performance on pure visual perception tasks versus integrated visual programming tasks to quantify the gap
  2. Compare different prompt engineering strategies (CoT, eCoT, no-CoT) across all four cognitive domains
  3. Analyze error patterns to determine whether failures stem from visual processing, logical reasoning, or integration challenges

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications could improve LMMs' visual code reasoning capabilities beyond current prompting strategies?
- Basis in paper: [inferred] The paper notes that models struggle with visual code reasoning despite prompting improvements, suggesting architectural limitations.
- Why unresolved: The paper identifies performance gaps but doesn't propose specific architectural solutions beyond prompting techniques.
- What evidence would resolve it: Experimental results comparing LMMs with different architectural modifications (e.g., specialized visual encoders, improved cross-modal attention) on ScratchEval would demonstrate which modifications most effectively improve visual code reasoning.

### Open Question 2
- Question: How does the performance gap between Chinese and English ScratchEval data vary across different model architectures and training datasets?
- Basis in paper: [explicit] The paper presents separate results for Chinese and English data, noting some models perform better on Chinese tasks.
- Why unresolved: While performance differences are observed, the paper doesn't analyze which model characteristics or training data compositions contribute to these differences.
- What evidence would resolve it: A detailed correlation study between model architecture choices, training data composition (language distribution), and performance on both language versions of ScratchEval would identify key factors.

### Open Question 3
- Question: What is the relationship between model size and visual code reasoning performance on ScratchEval, and does this relationship follow the same pattern as other multimodal tasks?
- Basis in paper: [explicit] The paper includes models of varying sizes (7B to 72B parameters) and notes that larger models like Gemini-1.5-pro and GPT-4o performed better.
- Why unresolved: The paper observes size-related performance differences but doesn't systematically analyze whether visual code reasoning follows the same scaling laws as other multimodal tasks.
- What evidence would resolve it: A scaling analysis plotting model size against ScratchEval performance, compared with scaling patterns on other multimodal benchmarks, would reveal whether visual code reasoning exhibits unique scaling characteristics.

## Limitations

- The benchmark relies on synthetic question generation, which may not fully capture real-world visual programming complexity
- Performance variations across languages are noted but not thoroughly analyzed
- The study does not investigate whether models are truly reasoning or using pattern-matching strategies to select answers

## Confidence

- **Medium confidence**: The claim that ScratchEval provides comprehensive evaluation of visual programming reasoning
- **Medium confidence**: The assertion that prompt engineering improves performance by 10-20%
- **Medium confidence**: The conclusion that no model exceeded 70% accuracy

## Next Checks

1. **Human evaluation validation**: Conduct human expert assessments of ScratchEval questions to verify that they genuinely test integrated visual programming reasoning rather than isolated skills

2. **Cross-language performance analysis**: Perform detailed statistical analysis comparing model performance across Chinese and English datasets to determine if language differences affect visual programming reasoning capabilities

3. **Open-ended task extension**: Develop a subset of open-ended programming tasks to complement the multiple-choice format and assess whether models can generate code solutions, not just select answers, revealing deeper reasoning capabilities