---
ver: rpa2
title: A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language
  Models and Humans
arxiv_id: '2412.01131'
source_url: https://arxiv.org/abs/2412.01131
tags:
- word
- relation
- relations
- https
- antonymy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the semantic relation knowledge of pretrained\
  \ language models (PLMs) and humans using a comprehensive framework covering six\
  \ relations: hypernymy, hyponymy, holonymy, meronymy, antonymy, and synonymy. The\
  \ research introduces two novel metrics\u2014prototypicality and distinguishability\u2014\
  alongside established metrics for symmetry, soundness, and completeness."
---

# A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language Models and Humans

## Quick Facts
- arXiv ID: 2412.01131
- Source URL: https://arxiv.org/abs/2412.01131
- Authors: Zhihan Cao; Hiroaki Yamada; Simone Teufel; Takenobu Tokunaga
- Reference count: 31
- Models show significant knowledge gap vs humans across all semantic relations, with antonymy as notable exception

## Executive Summary
This study systematically evaluates how well pretrained language models (PLMs) understand six fundamental semantic relations (hypernymy, hyponymy, holonymy, meronymy, antonymy, synonymy) compared to human knowledge. Using a prompt-based probing framework with identical tasks for both humans and models, the research introduces novel metrics for prototypicality and distinguishability alongside established measures. Results reveal a substantial performance gap between humans and models across all relations, with antonymy being the only relation where models perform reasonably well. The study finds that larger models don't always perform better, and causal language models don't consistently outperform masked language models.

## Method Summary
The evaluation framework uses 10,507 probes constructed from WordNet relations and category norms, with 40 hand-crafted prompts. Human responses were collected via Amazon Mechanical Turk (48 participants), while six PLMs (BERT-base, BERT-large, RoBERTa-base, RoBERTa-large, Llama-8B, Llama-70B) were evaluated using prompt-based probing. The study introduces five metrics: soundness (precision@1), completeness (recall@k), symmetry (for antonymy and synonymy), prototypicality (comparing model responses to human prototypes), and distinguishability (measuring ability to distinguish between relations).

## Key Results
- Significant knowledge gap between humans and models across all semantic relations
- Antonymy is the outlier relation where all models perform reasonably well
- Causal language models do not consistently outperform masked language models
- Larger models do not always perform better across relations
- Models show consistent "antonymy bias" preferring antonyms even when prompted with non-antonymy relations

## Why This Works (Mechanism)

### Mechanism 1: Symmetry Metrics
Symmetric relations (antonymy, synonymy) show stronger reciprocal elicitation in both humans and models, enabling better symmetry metrics. The metric uses reciprocal elicitation where for tuple (w, r, v), the model should predict v when given w and w when given v, then averages overlap across all tuples. Symmetric relations will exhibit higher mutual prediction rates than asymmetric relations.

### Mechanism 2: Prototypicality Metrics
Prototypicality metrics reward models for predicting the most frequent human response first and maintaining similar rank orderings. Combines indicator function (prototype match) with edit similarity between model and human ranked lists, normalized by response length. Human response entropy correlates with prototypicality; lower entropy means stronger prototypicality.

### Mechanism 3: Distinguishability Metrics
Distinguishability metrics measure whether models rank correct relata before incorrect relata from other relations. Calculates mean relative rank of s-relata in response to r-probe, then takes difference between intra-relation and inter-relation scores. Models that distinguish relations well will consistently rank correct relata higher than relata from other relations.

## Foundational Learning

- **Semantic relations (hypernymy, hyponymy, holonymy, meronymy, antonymy, synonymy):** Why needed here: The paper evaluates models on all six relations, requiring understanding of their definitions and differences. Quick check question: What is the key difference between hypernymy and hyponymy?

- **Prompt-based probing methodology:** Why needed here: The evaluation framework relies on prompts with slots for target words and relata. Quick check question: How does prompt-based probing differ from probing classifiers?

- **Information retrieval metrics (precision, recall, edit distance):** Why needed here: Soundness uses precision, completeness uses recall, and prototypicality uses edit distance. Quick check question: What information retrieval metric corresponds to soundness?

## Architecture Onboarding

- **Component map:** Data collection pipeline (WordNet expansion, symmetric augmentation) -> Probe generation system (target word + prompt â†’ probe string) -> Model inference interface (masked vs causal language models) -> Human response collection platform (MTurk integration) -> Evaluation metrics engine (soundness, completeness, symmetry, prototypicality, distinguishability)

- **Critical path:** 1. Expand relatum sets using WordNet and indirect relations 2. Generate probes for all (target word, prompt) combinations 3. Collect human responses via MTurk 4. Run models on all probes 5. Calculate metrics comparing human and model responses 6. Analyze results by relation, model type, and size

- **Design tradeoffs:** Word-level vs subword-level evaluation (chosen word-level for comparability) vs subword-level evaluation; Fixed vs variable number of masks (chosen fixed for MLMs, synthetic distribution for CLMs) vs flexible masking; Single vs multiple prompts per relation (chosen multiple to reduce prompt dependency) vs single prompt

- **Failure signatures:** All-OOR responses indicate vocabulary mismatch or relation gap; Low symmetry scores suggest asymmetric relation understanding; High antonymy bias indicates confusion between opposite and similar relations

- **First 3 experiments:** 1. Run BERT-base on hypernymy probes and verify soundness > 0.2 2. Compare RoBERTa-large vs BERT-large on antonymy symmetry scores 3. Test Llama-8B on holonymy prototypicality with edit similarity > 0.2

## Open Questions the Paper Calls Out

### Open Question 1
Why do pretrained language models consistently exhibit an "antonymy bias," preferring antonyms even when prompted with non-antonymy relations? The paper suggests potential reasons (e.g., distributional characteristics of antonymy in corpora), but lacks empirical evidence to confirm the cause. What evidence would resolve it: Analyzing the co-occurrence patterns of antonym pairs in pretraining corpora and correlating them with model performance across different relations.

### Open Question 2
How do linguistic shortcuts in prompts influence the evaluation of semantic relation knowledge in pretrained language models? The paper acknowledges that prompt-based methods are highly dependent on prompt design and that some models might use linguistic expressions in certain prompts as shortcuts. What evidence would resolve it: Conducting controlled experiments with systematically varied prompts to isolate and identify linguistic shortcuts, followed by developing methods to neutralize their effects.

### Open Question 3
What frequency-based metrics beyond unigram frequency better capture the relationship between word frequency and semantic relation learning in pretrained language models? The paper finds negligible correlation between unigram frequency and model performance but acknowledges that unigram frequency doesn't capture syntagmatic or paradigmatic relationships. What evidence would resolve it: Developing and testing frequency-based metrics that incorporate syntagmatic and paradigmatic relationships, then correlating these metrics with model performance across different semantic relations.

## Limitations
- Reliance on prompt-based probing cannot definitively determine whether models truly understand semantic relations or are simply pattern-matching based on training data
- 48 human participants may not fully capture variability in human semantic knowledge across diverse populations
- Assumes human responses represent the gold standard for prototypicality, which may not account for cultural or contextual variations in semantic understanding

## Confidence
**High Confidence:** Symmetry metric evaluation for antonymy and synonymy shows robust results across all models, with clear evidence that these relations exhibit stronger reciprocal prediction patterns compared to other semantic relations.

**Medium Confidence:** Prototypicality and distinguishability metrics show meaningful patterns but have weaker theoretical foundations in the literature. The correlation between entropy and prototypicality, while intuitive, lacks strong empirical validation in the semantic relation literature.

**Low Confidence:** Comparison between masked and causal language models does not provide conclusive evidence about their relative capabilities for semantic relation knowledge. The small sample of model types (2 MLMs, 2 CLMs) limits the generalizability of claims about model architecture differences.

## Next Checks
1. Replication with expanded human sample: Collect responses from 200+ participants across different demographic groups to validate whether the current human response patterns are representative and to assess the robustness of prototypicality metrics.

2. Controlled probing with relation overlap: Design probes where relatum sets overlap between relations (e.g., "dog" as both hypernym and holonym candidate) to test the distinguishability metric's sensitivity to relation ambiguity and assess whether current metrics adequately handle such cases.

3. Knowledge structure ablation study: Compare model performance on symmetric vs asymmetric relations when controlling for relatum frequency and semantic similarity, to determine whether observed differences are due to relation properties or other confounding factors.