---
ver: rpa2
title: Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language
  Models
arxiv_id: '2408.06518'
source_url: https://arxiv.org/abs/2408.06518
tags:
- leakage
- semantic
- generation
- prompt
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a novel phenomenon in language models called
  semantic leakage, where models unintentionally incorporate semantic information
  from the prompt into the generation, even when unrelated. For example, "He likes
  yellow" can lead to "school bus driver" in the output.
---

# Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models

## Quick Facts
- arXiv ID: 2408.06518
- Source URL: https://arxiv.org/abs/2408.06518
- Reference count: 22
- Language models unintentionally incorporate semantic information from prompts into generations, even when unrelated

## Executive Summary
This paper identifies a novel phenomenon in language models called semantic leakage, where models incorporate semantic information from the prompt into the generation even when unrelated. For example, "He likes yellow" can lead to "school bus driver" in the output. The authors define semantic leakage as an undue influence of semantic features from words in the prompt on the generation. They create a test suite with 109 prompts across different semantic categories and evaluate 13 models (GPT and LLaMA families) using an automatic metric called Leak-Rate. Results show significant semantic leakage across all models, with instruction-tuned models leaking more than pretrained-only versions. The phenomenon also occurs in multilingual and crosslingual settings (English, Chinese, Hebrew).

## Method Summary
The authors create a test suite of 109 prompts across different semantic categories, with control prompts (without semantic signal) and test prompts (with unrelated semantic concept). They evaluate 13 models from GPT and LLAMA families using temperature values 0, 0.5, 1, 1.5, generating 10 samples per prompt. The Leak-Rate metric measures semantic similarity between concepts and both control/test generations using BERT-Score, Sentence-BERT, and OpenAI Embeddings. Human evaluation validates the automatic results. Multilingual experiments test the phenomenon in Chinese and Hebrew.

## Key Results
- All models exhibit significant semantic leakage, with instruction-tuned models leaking more than pretrained versions
- Greedy sampling (t=0) leads to highest semantic leakage measures, with lower temperatures generally increasing leakage
- Semantic leakage occurs across languages (English, Chinese, Hebrew) and represents a broader class of association bias
- BERT-Score embeddings showed most consistent results for automatic evaluation

## Why This Works (Mechanism)

### Mechanism 1
The model's token prediction process allows unrelated semantic concepts in the prompt to influence the next-token distribution. During autoregressive generation, the model computes token probabilities conditioned on all previous tokens. If a concept has strong learned associations with other semantic categories, those associations can bias the token distribution even when the prompt doesn't logically require that connection. This assumes language models have learned strong co-occurrence and semantic associations during pretraining that persist during generation.

### Mechanism 2
Instruction-tuned models exhibit more semantic leakage because their fine-tuning process reinforces generation of semantically rich, informative content. During instruction tuning, models are rewarded for producing helpful, detailed responses, creating a bias toward incorporating semantic information that makes generations more substantive, even when that information isn't logically required. This assumes fine-tuning optimizes for response quality metrics that favor semantically rich content over logical consistency.

### Mechanism 3
Lower temperature values increase semantic leakage because greedy sampling amplifies the model's strongest associations. At lower temperatures (especially t=0), token selection becomes more deterministic, selecting tokens with highest probability. If a prompt concept has strong learned associations, these associations dominate the token distribution, leading to more pronounced semantic leakage. This assumes temperature scaling affects randomness of token selection, with lower temperatures reducing diversity and amplifying dominant associations.

## Foundational Learning

- Concept: Semantic similarity metrics (cosine similarity, BERTScore)
  - Why needed here: The paper uses semantic similarity between generations and concepts to quantify leakage
  - Quick check question: How would BERTScore differ from simple cosine similarity when comparing "school bus driver" and "yellow"?

- Concept: Autoregressive language model generation
  - Why needed here: Understanding how models generate text token-by-token based on previous context is essential to grasp why prompt concepts can influence generations
  - Quick check question: In a generation like "He likes yellow. He works as a ___", what context is the model using to predict the next token?

- Concept: Temperature scaling in sampling
  - Why needed here: The paper explores how different temperature values affect leakage
  - Quick check question: What happens to the token probability distribution when temperature approaches zero?

## Architecture Onboarding

- Component map: Prompt → Tokenization → Transformer layers → Attention → Next token probability → Sampling → Output
- Critical path: Input prompt flows through tokenization, transformer layers with attention mechanisms, decoding with temperature scaling, and produces final generation
- Design tradeoffs: Higher temperature increases diversity but reduces semantic coherence; instruction tuning improves helpfulness but may increase semantic leakage
- Failure signatures: Generations containing semantically related but logically unrelated concepts; temperature-dependent leakage patterns; instruction-tuned models showing more leakage
- First 3 experiments:
  1. Test semantic leakage across different temperature values (0, 0.5, 1.0, 1.5) with a fixed prompt to verify temperature-leakage relationship
  2. Compare semantic leakage between pretrained and instruction-tuned versions of same model to confirm instruction-tuning effect
  3. Evaluate leakage in multilingual settings by translating prompts and measuring cross-lingual consistency

## Open Questions the Paper Calls Out

### Open Question 1
What is the relationship between semantic leakage and the "knowledge overshadowing" phenomenon described in Zhang et al. (2024)? The paper suggests semantic leakage might hinder performance via the overshadowing mechanism, where strong associations override more relevant parts of questions. This connection is mentioned but not investigated empirically.

### Open Question 2
Why do instruction-tuned models exhibit more semantic leakage than their pretrained-only counterparts? The paper hypothesizes this might be because leaking generations are less generic and provide more information/content, which might be incentivized under fine-tuning processes, but doesn't test this hypothesis.

### Open Question 3
How does temperature affect semantic leakage across different model architectures and tasks? The paper notes inconsistent patterns for GPT models and only briefly examines temperature effects without exploring whether relationships are task-dependent or model-dependent.

### Open Question 4
Can semantic leakage be reliably detected and measured using embedding methods in languages where underlying models were not trained? The paper acknowledges that multilingual results are less reliable due to uncertainty about embedding quality for Hebrew and Chinese, leaving open questions about reliability for non-English languages.

## Limitations
- Test suite of 109 prompts may not capture full diversity of semantic leakage phenomena
- Automatic evaluation using embedding methods may not capture all nuances of semantic relationships
- Limited multilingual experiments in only Chinese and Hebrew, leaving questions about other language families
- Exact prompt construction methodology not fully detailed, making it difficult to assess potential biases

## Confidence

**High Confidence:**
- Semantic leakage is a real phenomenon occurring across multiple language models and families
- Instruction-tuned models exhibit more semantic leakage than pretrained counterparts
- Temperature scaling affects leakage rates, with lower temperatures increasing leakage
- Phenomenon occurs in multilingual settings (English, Chinese, Hebrew)

**Medium Confidence:**
- Semantic leakage is a broader class of association bias encompassing previously studied biases
- Leak-Rate metric effectively quantifies semantic leakage across different models and conditions
- Phenomenon primarily driven by learned semantic associations in model's pretraining

**Low Confidence:**
- Exact mechanism by which instruction tuning increases semantic leakage
- Generalizability of findings to all possible generation scenarios and prompt types
- Relative contribution of different architectural components to semantic leakage

## Next Checks

**Validation Check 1:** Replicate temperature-leakage relationship experiment with 20-30 prompts across 3-4 representative models (GPT-3.5, Llama-2-7b, Llama-3-70b) at temperatures 0, 0.5, 1.0, 1.5 to verify core finding about temperature effects.

**Validation Check 2:** Conduct systematic ablation study comparing semantic leakage in pretrained vs instruction-tuned models of same architecture (e.g., Llama-2-7b vs Llama-2-7b-chat) using identical prompts to isolate effect of instruction tuning.

**Validation Check 3:** Extend multilingual validation to include at least two additional languages from different language families (e.g., Spanish and Japanese) to better understand cross-lingual patterns of semantic leakage and test generalizability beyond Indo-European and Semitic languages.