---
ver: rpa2
title: The Susceptibility of Example-Based Explainability Methods to Class Outliers
arxiv_id: '2407.20678'
source_url: https://arxiv.org/abs/2407.20678
tags:
- examples
- explainability
- training
- methods
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces a new evaluation framework for example-based
  explainability methods, addressing their susceptibility to class outliers. The framework
  includes three metrics: relevance, distinguishability, and correctness.'
---

# The Susceptibility of Example-Based Explainability Methods to Class Outliers

## Quick Facts
- arXiv ID: 2407.20678
- Source URL: https://arxiv.org/abs/2407.20678
- Reference count: 11
- This study introduces a new evaluation framework for example-based explainability methods, addressing their susceptibility to class outliers.

## Executive Summary
This study introduces a new evaluation framework for example-based explainability methods, addressing their susceptibility to class outliers. The framework includes three metrics: relevance, distinguishability, and correctness. Relevance measures the similarity between explanandum and explanation examples. Distinguishability captures the diversity of explanations, while correctness assesses faithfulness to the underlying labeling rule. Experiments on text and image classification datasets show that existing methods like IF, DM, and TraceIn suffer from low relevance and distinguishability due to class outliers. RIF, which suppresses outliers, performs better in relevance and distinguishability but worse in correctness. The findings highlight the need for robust techniques that balance relevance, distinguishability, and correctness in example-based explainability.

## Method Summary
The paper introduces a framework for evaluating example-based explainability methods with a focus on class outliers. The approach involves training classification models (BERT for text, InceptionV3 for images) and applying four explainability methods (IF, RIF, DM, TraceIn) to generate explanations. The evaluation uses three metrics: relevance (cosine similarity between explanandum and explanations), distinguishability (active domain and overlap), and correctness (alignment with labeling rules). Experiments are conducted on SMS Spam and ImageNet datasets with varying numbers of explanation examples (N = {2, 5, 10}).

## Key Results
- IF, DM, and TraceIn show low relevance and distinguishability due to class outliers
- RIF improves relevance and distinguishability by suppressing outliers but reduces correctness
- Class outliers significantly impact explanation quality across both text and image classification tasks
- The proposed evaluation framework effectively captures the tradeoffs between relevance, distinguishability, and correctness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Example-based explainability methods rely on quantifying the influence of training examples on model predictions.
- Mechanism: Methods like Influence Function (IF) approximate the change in model parameters when an infinitesimal change is applied to the training data distribution, effectively measuring how much each training example contributes to the final prediction.
- Core assumption: The model is a black-box, and we can only measure the change in model parameters or predictions, not the internal decision process.
- Evidence anchors:
  - [abstract]: "They quantify the importance of training examples in determining the model's outcomes."
  - [section]: "Various approaches have been proposed for this, including methods based on robust statistics, game theory, internal analysis of neural networks, and empirical techniques."
- Break condition: If the model is not a black-box or if we can access the internal decision process, more direct interpretability methods may be more appropriate.

### Mechanism 2
- Claim: Class outliers (training examples with high loss) have a disproportionate influence on example-based explanations.
- Mechanism: Examples that are hard to classify (high loss) tend to be influential globally, meaning they appear as explanations for many different explanandums, regardless of their relevance to the specific instance being explained.
- Core assumption: High loss examples are inherently ambiguous and hard to classify, even for humans, and thus don't provide meaningful explanations for specific predictions.
- Evidence anchors:
  - [abstract]: "Class outliers are training examples with high loss."
  - [section]: "We posit that these outliers manifest as explanations for multiple explanandums (instances to be explained) without contributing substantive explanation power."
- Break condition: If the dataset has no class outliers or if the model is very robust to noisy or ambiguous examples, this mechanism may not be as relevant.

### Mechanism 3
- Claim: Suppressing class outliers to improve explanation relevance can lead to a loss of correctness (faithfulness to the underlying labeling rule).
- Mechanism: Methods like RIF (Relative Influence Function) penalize the influence score of examples based on their global effect (loss), effectively removing class outliers from explanations. However, this can remove examples that are actually important for explaining certain predictions, especially when the explanandum itself is ambiguous.
- Core assumption: Class outliers can sometimes be the most relevant explanations, especially when the explanandum is also ambiguous or hard to classify.
- Evidence anchors:
  - [abstract]: "We argue that class outliers can aid in explaining outcomes for similar ambiguous explanandum and should not be suppressed in such instances."
  - [section]: "RIF performs poorly in uncovering followers and breakers, because of its loss-based outlier elimination."
- Break condition: If the dataset is very clean and there are no ambiguous examples, or if the model is very robust to outliers, suppressing them may not significantly impact correctness.

## Foundational Learning

- Concept: Evaluation metrics for example-based explainability methods
  - Why needed here: The paper introduces a new evaluation framework specifically for example-based explainers, including metrics for relevance, distinguishability, and correctness.
  - Quick check question: What are the three metrics introduced in the paper to evaluate example-based explainability methods?

- Concept: Class outliers in machine learning
  - Why needed here: Class outliers are central to the paper's findings, as they are the main source of the problems with existing example-based explainability methods.
  - Quick check question: How are class outliers defined in the context of this paper?

- Concept: Influence functions and other example-based explainability methods
  - Why needed here: The paper benchmarks several state-of-the-art example-based explainability methods (IF, RIF, DM, TraceIn) and shows how they are all affected by class outliers.
  - Quick check question: What is the main idea behind Influence Function (IF) as an example-based explainability method?

## Architecture Onboarding

- Component map:
  - Data (SMS Spam, ImageNet) -> Models (BERT, InceptionV3) -> Explainers (IF, RIF, DM, TraceIn) -> Metrics (Relevance, Distinguishability, Correctness)

- Critical path:
  1. Train the classification model on the dataset
  2. For each explainer, generate explanations for a set of explanandums
  3. Compute the evaluation metrics (relevance, distinguishability, correctness) for each explainer
  4. Analyze the results to understand the impact of class outliers

- Design tradeoffs:
  - Suppressing class outliers (as in RIF) improves relevance and distinguishability but hurts correctness
  - Not suppressing outliers leads to low relevance and distinguishability but better correctness
  - The choice depends on the specific use case and whether faithfulness to the model or interpretability of the explanation is more important

- Failure signatures:
  - Low relevance: Explanations are not similar to the explanandum
  - Low distinguishability: Explanations are not diverse and specific to each explanandum
  - Low correctness: Explanations do not align with the underlying labeling rule

- First 3 experiments:
  1. Evaluate the relevance, distinguishability, and correctness of IF, DM, and TraceIn on the SMS Spam dataset
  2. Evaluate the same metrics for RIF on the image classification dataset
  3. Analyze the popularity distribution of examples in explanations for each method to understand the impact of class outliers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can example-based explainability methods be made robust to class outliers while maintaining faithfulness to the underlying labeling rule?
- Basis in paper: [explicit] The paper identifies that existing methods are susceptible to class outliers and that attempting to suppress outliers can lead to a loss of correctness. It calls for robust techniques that balance relevance, distinguishability, and correctness.
- Why unresolved: The paper highlights the challenge but does not propose a definitive solution that achieves all three metrics simultaneously.
- What evidence would resolve it: Development and empirical validation of a method that demonstrates high relevance, distinguishability, and correctness on datasets with known class outliers.

### Open Question 2
- Question: What are the characteristics of class outliers that make them influential in example-based explanations, and how can these characteristics be leveraged to improve explainability?
- Basis in paper: [inferred] The paper discusses that class outliers are examples with high loss and are difficult to classify, yet they appear frequently in explanations, suggesting they have unique properties that impact explainability.
- Why unresolved: The paper does not explore the underlying properties of class outliers that contribute to their influence or how these properties can be utilized positively.
- What evidence would resolve it: Analysis of the features and behaviors of class outliers that determine their influence, and experiments showing how leveraging these properties improves explanation quality.

### Open Question 3
- Question: How does the presence of class outliers affect the generalization ability of models, and what implications does this have for example-based explainability?
- Basis in paper: [explicit] The paper mentions that class outliers are hard for the model to classify and have high loss, implying a potential impact on model performance.
- Why unresolved: The paper does not investigate the relationship between class outliers, model generalization, and explainability.
- What evidence would resolve it: Studies examining the effect of class outliers on model generalization across different datasets and how this impacts the effectiveness of example-based explanations.

## Limitations

- The study focuses exclusively on example-based methods, potentially missing broader interpretability considerations
- The correctness metric relies on manually defined labeling rules, which may not generalize across domains
- Results are based on specific datasets (SMS Spam and image classification), limiting broader applicability claims

## Confidence

- High Confidence: The experimental methodology and metric definitions are clearly articulated and reproducible. The observation that class outliers affect explanation quality is well-supported by the empirical results.
- Medium Confidence: The proposed metrics (relevance, distinguishability, correctness) appear appropriate for the task, though their universal applicability remains to be validated across diverse domains and model architectures.
- Low Confidence: The recommendation that class outliers should sometimes be preserved in explanations, while theoretically interesting, requires more extensive validation before practical deployment guidance can be given.

## Next Checks

1. Replicate experiments with additional datasets and model architectures to test generalizability
2. Validate the correctness metric's sensitivity to different labeling rule specifications
3. Conduct user studies to assess whether the proposed metrics align with human interpretability needs