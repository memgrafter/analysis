---
ver: rpa2
title: Discretizing Continuous Action Space with Unimodal Probability Distributions
  for On-Policy Reinforcement Learning
arxiv_id: '2408.00309'
source_url: https://arxiv.org/abs/2408.00309
tags:
- policy
- learning
- action
- distribution
- unimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unimodal ordinal policy parameterization
  method for continuous control tasks in on-policy reinforcement learning. The core
  idea is to discretize the continuous action space and use Poisson probability distributions
  to enforce unimodal probability distributions over the discrete actions, which can
  better leverage the continuity in the underlying action space.
---

# Discretizing Continuous Action Space with Unimodal Probability Distributions for On-Policy Reinforcement Learning

## Quick Facts
- arXiv ID: 2408.00309
- Source URL: https://arxiv.org/abs/2408.00309
- Reference count: 40
- One-line primary result: Unimodal ordinal policy parameterization significantly outperforms Gaussian policies and other discrete policy baselines in continuous control tasks

## Executive Summary
This paper introduces a novel approach for handling continuous action spaces in on-policy reinforcement learning by discretizing the space and using unimodal Poisson probability distributions. The method addresses scalability issues of naive discretization while maintaining the benefits of discrete policy representations. By enforcing unimodal distributions over discretized actions, the approach reduces policy gradient variance and leverages the underlying continuity of the action space more effectively than existing methods.

The proposed unimodal ordinal policy parameterization is evaluated on standard MuJoCo continuous control tasks using PPO, TRPO, and ACKTR algorithms. Results demonstrate significant improvements in both learning speed and final performance compared to Gaussian policies and other discrete policy baselines, particularly in high-dimensional tasks like Humanoid. Theoretical analysis supports the empirical findings by showing that the unimodal policy achieves lower variance in the policy gradient estimator compared to existing ordinal parameterization methods.

## Method Summary
The method discretizes each continuous action dimension into K bins and parameterizes the policy using Poisson probability distributions that are constrained to be unimodal. A neural network outputs M positive values (one per action dimension) using Softplus activation, which are then transformed into Poisson PMF parameters. The ordinal parameterization converts these probabilities into logits that maintain the ordering relationship between actions, followed by softmax normalization. This creates a probability distribution over discrete actions that is both unimodal and preserves the ordinal structure of the continuous space. The approach is integrated with standard on-policy algorithms like PPO, TRPO, and ACKTR.

## Key Results
- Unimodal ordinal policy outperforms Gaussian policies on all tested MuJoCo tasks
- Significant improvements in learning speed and final performance, especially on high-dimensional Humanoid task
- Theoretical analysis shows lower variance in policy gradient estimator compared to ordinal policies
- Maintains advantages across PPO, TRPO, and ACKTR algorithm implementations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Unimodal Poisson distributions reduce policy gradient variance compared to multi-modal discrete policies.
- **Mechanism**: The Poisson distribution's variance equals its mean, providing a bounded and low-variance alternative to discrete policies that can produce multi-modal outputs. By constraining the policy to be unimodal, the method eliminates the high-variance behavior that occurs when the policy places significant probability mass on multiple discrete actions simultaneously.
- **Core assumption**: The underlying continuous action space can be effectively represented by a unimodal distribution over discretized actions, and the ordering of discrete actions preserves meaningful information about the continuous space.
- **Evidence anchors**:
  - [abstract]: "we provide theoretical analysis on the variance of the policy gradient estimator, which suggests that our attentively designed unimodal discrete policy can retain a lower variance and yield a stable learning process"
  - [section III.C]: "we can derive the expressions for the variances of the policy gradient estimators of the Poisson distribution... where σ2_1 = Vinit [∇θ1 Lj] is the variance of the PMF of Poisson distribution"

### Mechanism 2
- **Claim**: Ordinal parameterization with unimodal constraint captures continuity in the action space more efficiently than independent categorical distributions.
- **Mechanism**: Instead of parameterizing each discrete action independently (requiring K parameters per dimension), the method learns a single parameter λ_i per dimension that defines the Poisson distribution. The ordinal architecture then converts these unimodal distributions into logits that maintain the ordering relationship between actions, allowing the policy to generalize across nearby discrete actions.
- **Core assumption**: Action dimensions can be factorized independently and the ordering relationship between discrete actions preserves meaningful structure from the continuous space.
- **Evidence anchors**:
  - [section III.B]: "our method only needs to learn M probability mass functions with M units in the network output layer, in contrast to existing ordinal parameterization methods that require learning the complete probability distributions over all dimensions using M*K units"
  - [abstract]: "This unimodal architecture can better leverage the continuity in the underlying continuous action space using explicit unimodal probability distributions"

### Mechanism 3
- **Claim**: The combination of Poisson distribution with ordinal parameterization creates a probability distribution that is both unimodal and maintains dependencies between adjacent actions.
- **Mechanism**: The Poisson PMF creates a unimodal distribution centered at the learned parameter λ_i. The ordinal transformation (Equation 11) then converts these probabilities into logits that maintain the ordering while ensuring the final distribution is properly normalized via softmax. This creates a policy that naturally prefers actions near the learned mode while maintaining smooth transitions between neighboring actions.
- **Core assumption**: The log-likelihood transformation followed by ordinal parameterization preserves the unimodal property while creating meaningful dependencies between adjacent action logits.
- **Evidence anchors**:
  - [section III.B]: "We denote the scalar output of our deep network as F(s) = [f1(s), f2(s), ..., fi(s)], where fi(s) > 0 is enforced to be positive using the Softplus nonlinearity"
  - [section III.B]: "The Poisson distribution, a widely recognized discrete probability distribution, is typically used to model the likelihood of observing a specific count of events"

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs)**
  - Why needed here: The entire reinforcement learning framework is built on MDP theory, where states, actions, rewards, and transitions are formally defined.
  - Quick check question: In an MDP, what mathematical property ensures that the future state depends only on the current state and action, not the history of states?

- **Concept: Policy Gradient Methods**
  - Why needed here: The method is specifically designed to work with on-policy policy gradient algorithms like PPO, TRPO, and ACKTR, requiring understanding of how policy parameters are updated based on sampled trajectories.
  - Quick check question: What is the key difference between the policy gradient estimator and the true policy gradient, and why does this difference matter for variance?

- **Concept: Probability Distributions and Parameterization**
  - Why needed here: The core innovation involves parameterizing discrete policies using Poisson distributions rather than categorical distributions, requiring understanding of how different distributions affect learning dynamics.
  - Quick check question: How does the variance of a Poisson distribution compare to its mean, and why is this property useful for reinforcement learning?

## Architecture Onboarding

- **Component map**: State → Network → λ_i parameters → Poisson PMF → Ordinal transformation → Final probabilities → Action sampling → Environment interaction → Reward calculation → Policy update

- **Critical path**: State → Network → λ_i parameters → Poisson PMF → Ordinal transformation → Final probabilities → Action sampling → Environment interaction → Reward calculation → Policy update

- **Design tradeoffs**:
  - Number of discrete bins K vs. computational cost: Higher K provides finer control but increases the ordinal transformation complexity
  - Temperature τ vs. exploration: Lower τ encourages exploitation but may lead to premature convergence
  - Network architecture depth vs. learning stability: Deeper networks may capture more complex state-action relationships but could be harder to train

- **Failure signatures**:
  - Policy collapse: All probability mass concentrates on one action, indicated by very low policy entropy
  - High variance learning: Unstable learning curves with large standard deviations across seeds
  - Poor exploration: Agent gets stuck in local optima, failing to discover better policies

- **First 3 experiments**:
  1. **Sanity check**: Run PPO with unimodal policy on a simple continuous control task (e.g., Pendulum) with K=9 bins and compare learning curves to Gaussian baseline.
  2. **Ablation study**: Test unimodal policy with different temperature values (τ=1.5, 2.5, 3.0) on HalfCheetah to understand exploration-exploitation tradeoff.
  3. **Scalability test**: Compare unimodal policy with varying K values (9, 11, 15) on Humanoid to verify performance improvements with finer discretization.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of unimodal ordinal policies scale with increasing action dimensionality compared to other discretization methods?
  - Basis in paper: [explicit] The paper compares their method to Gibbs and ordinal policies, but does not extensively explore scaling with action dimensionality beyond the Humanoid task.
  - Why unresolved: The paper focuses on demonstrating effectiveness in high-dimensional tasks but lacks systematic analysis across varying dimensionalities.
  - What evidence would resolve it: Experiments systematically varying action space dimensionality and comparing performance of unimodal ordinal policies against other discretization approaches across this range.

- **Open Question 2**: What is the theoretical upper bound on the variance reduction achieved by unimodal ordinal policies compared to ordinal policies in the policy gradient estimator?
  - Basis in paper: [explicit] The paper provides a variance analysis showing unimodal policies have lower variance than ordinal policies, but does not establish a theoretical upper bound on this reduction.
  - Why unresolved: The analysis focuses on demonstrating the variance advantage but does not quantify the maximum possible reduction.
  - What evidence would resolve it: A rigorous mathematical proof establishing the theoretical maximum variance reduction achievable by unimodal ordinal policies compared to ordinal policies.

- **Open Question 3**: How does the performance of unimodal ordinal policies compare to continuous policy parameterizations (e.g., Gaussian, Beta) when applied to tasks with non-uniformly distributed optimal actions?
  - Basis in paper: [inferred] The paper demonstrates superiority over discrete policy baselines but does not compare to continuous policy parameterizations in tasks with non-uniform optimal action distributions.
  - Why unresolved: The experiments focus on standard MuJoCo benchmarks without exploring scenarios where optimal actions are non-uniformly distributed.
  - What evidence would resolve it: Experiments comparing unimodal ordinal policies to continuous policy parameterizations in tasks specifically designed to have non-uniformly distributed optimal actions.

## Limitations

- The unimodal constraint may limit performance on tasks requiring multi-modal optimal policies
- Scalability benefits haven't been tested on extremely high-dimensional continuous control problems
- The method's performance on tasks with strongly correlated action dimensions remains unclear

## Confidence

- **High confidence**: The mechanism by which Poisson distributions reduce policy gradient variance is well-established mathematically and the empirical results showing improved learning speed are robust across multiple algorithms and tasks.
- **Medium confidence**: The ordinal parameterization effectively captures continuity in the action space, though the assumption of independent action dimensions may not hold for all tasks.
- **Medium confidence**: The scalability benefits over naive discretization are demonstrated, but the exact threshold where this method becomes advantageous over Gaussian policies depends on task complexity and requires further exploration.

## Next Checks

1. **Multi-modal task evaluation**: Test the unimodal policy on tasks known to require multi-modal behavior (such as environments with multiple distinct optimal strategies) to verify whether the unimodal constraint becomes a bottleneck.

2. **Extreme discretization analysis**: Evaluate performance with very high K values (e.g., K=31 or K=51) on Humanoid to determine if the method maintains its advantage as discretization becomes finer and approaches continuous behavior.

3. **Correlation structure testing**: Design tasks where action dimensions have strong correlations or dependencies, then compare unimodal policy performance against methods that explicitly model these correlations to test the factorization assumption.