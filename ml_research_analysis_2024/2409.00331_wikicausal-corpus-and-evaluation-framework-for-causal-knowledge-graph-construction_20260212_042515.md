---
ver: rpa2
title: 'WikiCausal: Corpus and Evaluation Framework for Causal Knowledge Graph Construction'
arxiv_id: '2409.00331'
source_url: https://arxiv.org/abs/2409.00331
tags:
- causal
- knowledge
- evaluation
- corpus
- relations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WikiCausal is a novel corpus and evaluation framework for building
  and assessing causal knowledge graphs from text. The corpus consists of 68,391 Wikipedia
  articles linked to 50 event-related Wikidata concepts, enabling automated extraction
  of causal relations between event concepts.
---

# WikiCausal: Corpus and Evaluation Framework for Causal Knowledge Graph Construction

## Quick Facts
- **arXiv ID**: 2409.00331
- **Source URL**: https://arxiv.org/abs/2409.00331
- **Reference count**: 40
- **Primary result**: Novel corpus and evaluation framework for causal knowledge graph construction from text, achieving 92.5% precision and 17% recall on class-level relations

## Executive Summary
WikiCausal addresses the critical gap in evaluating causal knowledge graph construction systems by providing a comprehensive corpus of 68,391 Wikipedia articles linked to 50 event-related Wikidata concepts, along with an automated evaluation framework. The system extracts causal relations between event concepts using a modular pipeline combining neural question answering and entity linking, then evaluates results using Wikidata's existing causal relations for recall and large language models for precision. This approach eliminates the need for manual annotation while providing reliable metrics for comparing different causal extraction methods. The framework demonstrates 92.5% precision and 17% recall on class-level relations, establishing a new benchmark for automated causal knowledge graph construction evaluation.

## Method Summary
The WikiCausal framework consists of a corpus generation process that extracts Wikipedia articles linked to event-related Wikidata concepts, combined with a modular causal extraction pipeline that separates phrase extraction from entity linking. The system uses neural question answering models to identify cause-effect phrases in text, then applies entity linking to map these phrases to Wikidata concepts. Evaluation combines recall measurement by comparing extracted relations against Wikidata's causal relations and precision evaluation using large language models to verify extracted pairs. The framework provides automated scripts for corpus generation, extraction pipeline execution, and evaluation metric computation, enabling reproducible research in causal knowledge graph construction.

## Key Results
- Achieved 92.5% precision and 17% recall on class-level causal relations using a modular pipeline with DistilBERT QA and BLINK entity linking
- The QAL KG covers 43 class-level relations including 44 of the 50 event classes in the corpus
- Automated evaluation framework successfully measures both recall (using Wikidata relations) and precision (using LLM-based verification) without manual annotation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using Wikidata's existing causal relations for recall evaluation provides a scalable and reliable estimate of extracted knowledge graph coverage
- Mechanism: The pipeline compares extracted causal relations against known Wikidata relations, counting matches to estimate recall
- Core assumption: Most causal relations in Wikidata are described in their associated Wikipedia articles
- Evidence anchors:
  - [abstract] "The evaluation is performed in part using existing causal relations in Wikidata to measure recall"
  - [section 4.1] "our automated recall evaluation relies on existing causal relations in Wikidata"
  - [corpus] Strong evidence - corpus directly links Wikipedia articles to Wikidata concepts
- Break condition: If Wikidata causal relations are not actually described in Wikipedia articles, recall estimates become unreliable

### Mechanism 2
- Claim: Large language models can effectively evaluate precision of causal relation extraction without manual annotation
- Mechanism: LLM generates prompts to verify cause-effect pairs, using majority voting across multiple runs for robustness
- Core assumption: LLMs have sufficient knowledge of Wikipedia/Wikidata content to accurately judge extracted relations
- Evidence anchors:
  - [abstract] "in part using Large Language Models to avoid the need for manual or crowd-sourced evaluation"
  - [section 4.2] "we devise a mechanism for automatically creating prompts and probing LLMs to measure the accuracy"
  - [corpus] Strong evidence - corpus contains full Wikipedia text that LLMs were trained on
- Break condition: If LLMs were not trained on Wikipedia content or if prompt engineering fails, precision evaluation becomes unreliable

### Mechanism 3
- Claim: Modular pipeline with separate cause-effect extraction and entity linking achieves better results than monolithic approaches
- Mechanism: Pipeline first extracts cause-effect phrases using neural models, then links phrases to Wikidata concepts using entity linking
- Core assumption: Separating extraction and linking tasks allows optimization of each component independently
- Evidence anchors:
  - [section 5.1] "splits the causal extraction process into two steps: 1) extraction of pairs of cause-effect phrases 2) linking each cause and effect to event concepts"
  - [section 5.3] "The best-performing QAL KG covers 43 class-level relations, including 44 of the 50 event classes"
  - [corpus] Strong evidence - corpus provides both raw text and structured metadata needed for modular processing
- Break condition: If entity linking becomes the bottleneck or if phrase extraction fails to capture relevant context, overall performance degrades

## Foundational Learning

- Concept: Causal knowledge graph construction
  - Why needed here: Understanding the difference between causal graphs and regular knowledge graphs is crucial for evaluating this work
  - Quick check question: What distinguishes a causal knowledge graph from a regular knowledge graph?

- Concept: Entity linking and disambiguation
  - Why needed here: The pipeline relies on linking extracted phrases to Wikidata concepts, requiring understanding of entity linking techniques
  - Quick check question: How does BLINK differ from traditional entity linking approaches?

- Concept: Large language model evaluation techniques
  - Why needed here: The precision evaluation method uses LLMs as annotators, requiring understanding of LLM capabilities and limitations
  - Quick check question: What are the advantages and limitations of using LLMs instead of human annotators?

## Architecture Onboarding

- Component map:
  - Wikipedia dump -> JSONL corpus with text, metadata, Wikidata links -> Neural QA extraction -> Entity linking -> Knowledge graph output -> Wikidata comparison -> LLM evaluation -> Metrics reporting

- Critical path:
  1. Generate corpus from Wikipedia/Wikidata
  2. Run extraction pipeline on corpus
  3. Evaluate results using Wikidata and LLMs
  4. Compare different extraction configurations

- Design tradeoffs:
  - Recall vs precision: Higher recall methods may sacrifice precision
  - Resource usage: LLM evaluation requires significant compute vs manual annotation
  - Modularity: Separate extraction and linking allows component optimization but adds complexity

- Failure signatures:
  - Low recall: Extracted relations don't match Wikidata (content not in Wikipedia or extraction failure)
  - Low precision: LLM evaluation marks many relations as incorrect (prompt issues or model limitations)
  - Slow performance: Entity linking or LLM evaluation becomes bottleneck

- First 3 experiments:
  1. Run pipeline with different neural QA models (DistilBERT vs ALBERT) and compare recall/precision
  2. Test LLM evaluation with different prompt formats and voting strategies
  3. Compare results using only first section vs full article text for extraction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the recall performance of the WikiCausal corpus change as more causal relations are added to Wikidata over time?
- Basis in paper: [explicit] The paper mentions that recall evaluation relies on existing causal relations in Wikidata and that "over time, we expect the high-confident accurate causal relations to be added to Wikidata, and so this evaluation strategy can become an even more reliable measure of recall"
- Why unresolved: The paper only evaluated using the September 2022 Wikidata dump, with no longitudinal study of how recall changes as Wikidata evolves
- What evidence would resolve it: Repeated recall evaluations using different Wikidata snapshots over time, showing trends in coverage improvements

### Open Question 2
- Question: How would using larger, resource-intensive LLMs like GPT-4 with prompt engineering affect the precision evaluation results compared to the smaller allenai/tk-instruct-3b-def model used in this study?
- Basis in paper: [explicit] The paper states "state-of-the-art knowledge extraction methods are currently primarily based on much larger and resource-intensive LLMs such as GPT-4" and that "future work includes an evaluation of the use of such models along with prompt engineering techniques"
- Why unresolved: The authors explicitly acknowledge they haven't yet tested larger LLMs for precision evaluation, despite mentioning it as future work
- What evidence would resolve it: Systematic comparison of precision scores using different LLM sizes and prompt engineering techniques on the same WikiCausal corpus

### Open Question 3
- Question: How would a multilingual WikiCausal corpus affect the quality and coverage of extracted causal knowledge graphs compared to the current English-only version?
- Basis in paper: [explicit] The paper identifies "Another major limitation of our corpus is that it consists of English Wikipedia articles only" and suggests extending to multilingual corpora as future work
- Why unresolved: The authors acknowledge this as a limitation but haven't conducted any experiments with multilingual versions
- What evidence would resolve it: Creation and evaluation of a multilingual WikiCausal corpus showing changes in recall/precision metrics and coverage of causal relations across languages

## Limitations
- Limited to English Wikipedia articles only, missing multilingual causal knowledge
- Recall evaluation depends on completeness of Wikidata's causal relations, potentially missing novel extractions
- Precision evaluation using smaller LLM may not capture all nuanced causal relationships that larger models could identify

## Confidence
- High confidence in the automated evaluation methodology and corpus generation process
- Medium confidence in the generalizability of results to domains beyond event-related concepts
- Low confidence in the completeness of recall evaluation given potential gaps in Wikidata causal relations

## Next Checks
1. Test the evaluation framework on a broader range of Wikidata domains beyond event-related concepts
2. Compare LLM-based precision evaluation against human annotations on a sample dataset to validate reliability
3. Evaluate the impact of using full article text versus section-based extraction on both precision and recall metrics