---
ver: rpa2
title: Improving Small-Scale Large Language Models Function Calling for Reasoning
  Tasks
arxiv_id: '2410.18890'
source_url: https://arxiv.org/abs/2410.18890
tags:
- reasoning
- dataset
- role
- content
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a framework for improving function calling
  in small-scale language models for logical and mathematical reasoning tasks. The
  approach addresses the computational inefficiency of large-scale models by training
  smaller models using Reinforcement Learning from Human Feedback (RLHF) with Direct
  Preference Optimization (DPO).
---

# Improving Small-Scale Large Language Models Function Calling for Reasoning Tasks

## Quick Facts
- arXiv ID: 2410.18890
- Source URL: https://arxiv.org/abs/2410.18890
- Authors: Graziano A. Manduzio; Federico A. Galatolo; Mario G. C. A. Cimino; Enzo Pasquale Scilingo; Lorenzo Cominelli
- Reference count: 40
- Primary result: Framework improves function calling in small-scale LLMs for logical and mathematical reasoning tasks using RLHF with DPO

## Executive Summary
This study presents a framework for improving function calling in small-scale language models for logical and mathematical reasoning tasks. The approach addresses the computational inefficiency of large-scale models by training smaller models using Reinforcement Learning from Human Feedback (RLHF) with Direct Preference Optimization (DPO). The framework generates a dataset of correct and incorrect reasoning chains by having a large-scale model solve problems with callable functions. This dataset is then used to fine-tune a smaller model, Mistral-7B-Instruct-v0.2, using DPO. Experimental results show significant improvements in function calling abilities for reasoning tasks.

## Method Summary
The method employs an agent framework (microchain) to query a large-scale LLM (Llama3-70B) with function descriptions and examples, managing function calls in step-by-step reasoning chains. This process creates a dataset of correct and incorrect reasoning chains that is then used to train a smaller LLM (Mistral-7B-Instruct-v0.2) using DPO. The approach involves defining problems and functions, generating reasoning chains, creating and augmenting the dataset, training the smaller model using DPO, and evaluating performance on test problems.

## Key Results
- Fine-tuned model achieved near-perfect accuracy (100%) on First-Order Logic (FOL) tasks
- Outperformed original model in overall accuracy on GSM8K mathematical problems
- Effectively balances model size and performance, enhancing smaller models' capabilities in using external tools and solving reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The agent-based framework enables smaller LLMs to effectively leverage function calling by providing structured reasoning chains.
- Mechanism: An agent queries a large-scale LLM with function descriptions and examples, managing function calls in a step-by-step reasoning chain. This creates a dataset of correct and incorrect reasoning chains that is then used to train the smaller model.
- Core assumption: The step-by-step reasoning chain generated by the large-scale model contains valuable information about proper function calling patterns that can be transferred to smaller models.
- Evidence anchors:
  - [abstract]: "Our framework employs an agent that, given a problem and a set of callable functions, queries the LLM by injecting a description and examples of the usable functions into the prompt and managing their calls in a step-by-step reasoning chain."
  - [section]: "This process is used to create a dataset of correct and incorrect reasoning chain chat completions from a large-scale LLM."
- Break condition: If the large-scale model's reasoning chains contain too many errors or the smaller model cannot learn the function calling patterns from the dataset.

### Mechanism 2
- Claim: DPO provides an efficient alternative to traditional RLHF methods for training smaller models on function calling tasks.
- Mechanism: DPO directly optimizes the model to maximize the likelihood of preferred outputs based on human preference data, eliminating the need for an explicit reward function and separate reward model training.
- Core assumption: Human preferences between correct and incorrect function calling chains provide sufficient signal for effective model training without requiring a separate reward model.
- Evidence anchors:
  - [abstract]: "This dataset is used to train a smaller LLM using Reinforcement Learning from Human Feedback (RLHF), specifically employing the Direct Preference Optimization (DPO) technique."
  - [section]: "DPO distinguishes itself by enabling the model to learn a policy directly from user preference data, eliminating the need for an explicit reward function."
- Break condition: If the preference pairs do not provide sufficient distinction between correct and incorrect function calling patterns, or if the optimization becomes unstable.

### Mechanism 3
- Claim: The data augmentation strategy significantly improves the effectiveness of the training dataset.
- Mechanism: The original dataset of correct and incorrect completions is augmented by creating combinations of all correct completions with all incorrect completions, extending the dataset size and diversity.
- Core assumption: The augmented dataset provides more varied and representative examples of function calling scenarios, improving the model's ability to generalize.
- Evidence anchors:
  - [section]: "Given the original dataset D∗ of all tuples (xi, yi,j w , yi,k l), ∀i ∈ I, ∀j ∈ J i, ∀k ∈ K i, we built an augmented dataset Da where for a given prompt xi a combination of all the correct completions yi,j w with all the wrong completions yi,k l was made, extending the cardinality of the dataset from |D∗| = |P| = |Dw| + |Dl| to |Da| = |Dw| × |Dl|."
- Break condition: If the augmented dataset introduces too much noise or the model becomes overfit to the specific combinations rather than learning general function calling patterns.

## Foundational Learning

- Concept: First-Order Logic (FOL) syntax and semantics
  - Why needed here: The framework uses FOL problems to test logical reasoning capabilities, requiring understanding of quantifiers, predicates, and logical connectives.
  - Quick check question: What is the difference between a universal quantifier (∀) and an existential quantifier (∃) in FOL?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The training approach relies on RLHF principles to optimize the model based on human preferences between correct and incorrect function calling chains.
  - Quick check question: How does RLHF differ from traditional supervised learning in terms of training objectives?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is the specific optimization technique used to train the smaller model, requiring understanding of how it differs from other RLHF methods like PPO.
  - Quick check question: What is the key advantage of DPO over PPO in terms of computational efficiency?

## Architecture Onboarding

- Component map:
  - Agent framework (microchain) -> Large-scale LLM (Llama3-70B) -> Dataset creation pipeline -> DPO trainer -> Mistral-7B-Instruct-v0.2

- Critical path:
  1. Define problems and functions
  2. Generate reasoning chains using large-scale LLM
  3. Create and augment dataset
  4. Train smaller model using DPO
  5. Evaluate performance on test problems

- Design tradeoffs:
  - Model size vs. performance: Using smaller models reduces computational costs but may limit reasoning capabilities
  - Dataset size vs. quality: Larger datasets provide more training examples but may include more noise
  - Chain length vs. efficiency: Longer reasoning chains may improve accuracy but increase computational overhead

- Failure signatures:
  - Low accuracy on test problems indicates issues with function calling patterns or reasoning capabilities
  - Unstable training loss suggests problems with the preference optimization or dataset quality
  - High inference time may indicate inefficient function calling or reasoning chains

- First 3 experiments:
  1. Test the agent framework with a simple FOL problem to verify function calling works correctly
  2. Generate a small dataset and train the smaller model for a few epochs to check if DPO optimization is working
  3. Evaluate the trained model on a subset of test problems to assess initial performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the fine-tuned Mistral-7B-Instruct-v0.2 model compare to other small-scale LLMs (e.g., Phi-3, Llama-3-8B) when trained using the same DPO methodology on FOL and GSM8K tasks?
- Basis in paper: [inferred] The paper presents results for Mistral-7B-Instruct-v0.2 but does not compare it to other small-scale models trained with the same methodology.
- Why unresolved: The study focuses on a single small-scale model, limiting generalizability to other architectures.
- What evidence would resolve it: Benchmarking the proposed framework across multiple small-scale models (e.g., Phi-3, Llama-3-8B) trained on the same datasets and evaluated on FOL and GSM8K tasks.

### Open Question 2
- Question: What is the impact of varying the number of correct and incorrect completions (ni and ni) in the dataset on the final performance of the fine-tuned model?
- Basis in paper: [inferred] The paper uses a fixed ratio of correct to incorrect completions (1000 samples per problem) but does not explore the effect of varying this ratio.
- Why unresolved: The optimal balance between correct and incorrect completions for effective training is not investigated.
- What evidence would resolve it: Systematic experiments varying the ratio of correct to incorrect completions and measuring their impact on model accuracy and robustness.

### Open Question 3
- Question: How does the proposed framework perform on multimodal reasoning tasks (e.g., visual question answering) compared to unimodal tasks like FOL and GSM8K?
- Basis in paper: [explicit] The paper mentions the potential for extending the framework to multimodal tasks but does not provide experimental results.
- Why unresolved: The methodology is only tested on text-based tasks, leaving the applicability to multimodal tasks unexplored.
- What evidence would resolve it: Applying the framework to multimodal datasets (e.g., VQA, GQA) and evaluating the model's ability to use function calling for reasoning in visual contexts.

## Limitations
- Evaluation based on a small test set of only 4 problems, limiting generalizability
- Performance improvement on FOL tasks (25% to 100%) based on just two test problems, making it difficult to assess robust capability
- No ablation studies to determine which framework components contribute most significantly to improvements

## Confidence
- High Confidence: The core mechanism of using large models to generate function calling patterns for smaller model training is technically sound and well-established in transfer learning literature. The DPO training approach is properly implemented using standard libraries (TRL).
- Medium Confidence: The reported accuracy improvements are methodologically sound but based on limited test data. The framework's ability to generalize to more complex reasoning tasks remains uncertain without broader evaluation.
- Low Confidence: The relative contributions of different framework components to the overall performance improvement cannot be determined from the current analysis.

## Next Checks
1. **Scale up evaluation:** Test the fine-tuned model on a larger, more diverse set of FOL and GSM8K problems (minimum 20 problems each) to establish more reliable performance metrics and assess generalization capabilities.

2. **Conduct ablation studies:** Evaluate model performance with different combinations of framework components removed (e.g., without data augmentation, using supervised fine-tuning instead of DPO) to identify which elements are critical for success.

3. **Test on out-of-domain reasoning tasks:** Apply the fine-tuned model to reasoning tasks outside the FOL and GSM8K domains (such as logical puzzles or symbolic reasoning problems) to evaluate the framework's broader applicability to different types of reasoning challenges.