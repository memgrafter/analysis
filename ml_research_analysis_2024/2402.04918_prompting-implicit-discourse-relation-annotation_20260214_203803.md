---
ver: rpa2
title: Prompting Implicit Discourse Relation Annotation
arxiv_id: '2402.04918'
source_url: https://arxiv.org/abs/2402.04918
tags:
- labels
- prompt
- discourse
- relation
- implicit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of prompt engineering
  techniques for zero-shot implicit discourse relation classification using large
  language models. It compares multiple prompting strategies, including a two-step
  connective insertion approach, per-class binary classification, and per-class verification
  questions, against a standard multiple-choice prompt.
---

# Prompting Implicit Discourse Relation Annotation

## Quick Facts
- arXiv ID: 2402.04918
- Source URL: https://arxiv.org/abs/2402.04918
- Reference count: 40
- Primary result: Standard prompt engineering does not significantly improve zero-shot implicit discourse relation classification accuracy

## Executive Summary
This paper investigates the effectiveness of various prompt engineering techniques for zero-shot implicit discourse relation classification using large language models. The authors compare multiple prompting strategies, including connective insertion, per-class binary classification, and per-class verification questions, against a standard multiple-choice prompt. Experiments on PDTB 3.0 and DiscoGeM datasets show that none of the prompt engineering methods significantly improve classification accuracy, which remains far below state-of-the-art supervised models.

## Method Summary
The study systematically evaluates multiple prompting strategies for zero-shot implicit discourse relation classification. These include a two-step connective insertion approach where a connective is first generated and then used for classification, per-class binary classification where the model answers yes/no questions for each relation type, and per-class verification questions that ask the model to verify if a specific relation holds. All methods are tested against a standard multiple-choice prompt baseline using GPT-3.5 and GPT-4 on PDTB 3.0 and DiscoGeM datasets. The experiments compare zero-shot and few-shot settings, with few-shot using 32 examples per relation type.

## Key Results
- None of the prompt engineering methods (connective insertion, per-class binary, per-class verification) significantly improved classification accuracy over the standard multiple-choice prompt
- Classification accuracy remained far below state-of-the-art supervised models in both zero-shot and few-shot settings
- The study concludes that implicit discourse relation recognition is not yet resolvable under zero-shot or few-shot settings with current prompt engineering approaches

## Why This Works (Mechanism)
Large language models struggle with implicit discourse relation recognition because these relations lack explicit linguistic markers that would provide clear signals for classification. Unlike explicit connectives (e.g., "because," "however"), implicit relations require deeper semantic understanding and contextual reasoning that current zero-shot prompting methods cannot adequately capture. The absence of explicit signal means that even sophisticated prompt engineering techniques cannot provide the necessary scaffolding for accurate classification.

## Foundational Learning
- **Discourse relations**: The semantic or rhetorical relationships between text spans; needed to understand what the task aims to classify, quick check: can you identify examples of implicit vs explicit relations in text?
- **Zero-shot learning**: Model makes predictions without task-specific training; needed to understand the experimental setting, quick check: can you explain the difference between zero-shot and few-shot learning?
- **Prompt engineering**: Designing input prompts to guide model behavior; needed to understand the methodology, quick check: can you describe how different prompt formats might affect model outputs?
- **PDTB (Penn Discourse TreeBank)**: Standard corpus for discourse relation annotation; needed to understand the evaluation dataset, quick check: can you name the four top-level discourse relation classes in PDTB?

## Architecture Onboarding
**Component map**: Input text -> Prompt template -> LLM (GPT-3.5/GPT-4) -> Classification output
**Critical path**: Text preprocessing → Prompt generation → Model inference → Evaluation
**Design tradeoffs**: Zero-shot (no examples) vs few-shot (32 examples per class); simple prompts vs complex engineering; different prompting strategies tested
**Failure signatures**: Low accuracy across all prompting strategies; no significant difference between methods; performance gap with supervised models
**First experiments**: 1) Test chain-of-thought prompting to see if step-by-step reasoning improves accuracy; 2) Try few-shot demonstrations with in-context examples; 3) Implement structured templates that explicitly guide reasoning about discourse relations

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow scope of prompt engineering techniques tested, not exploring alternatives like chain-of-thought or structured reasoning templates
- No comparison with state-of-the-art few-shot or weakly supervised methods that might perform better
- Conclusion about impossibility of few-shot learning appears premature given limited testing of this hypothesis

## Confidence
- High confidence that standard prompt engineering does not significantly improve zero-shot classification accuracy
- Medium confidence in comparative performance between different prompt strategies
- Low confidence in broader claims about impossibility of few-shot learning for this task

## Next Checks
1. Test alternative prompt engineering techniques including chain-of-thought prompting, few-shot demonstrations with in-context examples, and structured templates that explicitly guide reasoning about discourse relations
2. Compare against modern few-shot learning approaches that leverage fine-tuning on small annotated datasets or weakly supervised methods using automatically generated silver data
3. Conduct ablation studies to identify which components of the discourse relation recognition task (e.g., understanding context, identifying relevant semantic features, mapping to relation types) are most challenging for large language models under zero-shot conditions