---
ver: rpa2
title: Online Mirror Descent for Tchebycheff Scalarization in Multi-Objective Optimization
arxiv_id: '2410.21764'
source_url: https://arxiv.org/abs/2410.21764
tags:
- scalarization
- optimization
- fedavg
- adaomdgd-tch
- omdeg-tch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of linear scalarization in
  multi-objective optimization (MOO), which fails to capture non-convex Pareto fronts.
  The authors propose OMD-TCH, an online mirror descent algorithm for Tchebycheff
  scalarization that optimizes the worst-case objective, smoothing out the hard one-hot
  update.
---

# Online Mirror Descent for Tchebycheff Scalarization in Multi-Objective Optimization

## Quick Facts
- arXiv ID: 2410.21764
- Source URL: https://arxiv.org/abs/2410.21764
- Reference count: 40
- Key outcome: Proposed OMD-TCH achieves O(√(log m/T)) convergence rate and AdaOMD-TCH improves practical performance in multi-objective optimization, especially for federated learning with fairness constraints.

## Executive Summary
This paper addresses the limitations of linear scalarization in multi-objective optimization (MOO), which fails to capture non-convex Pareto fronts. The authors propose OMD-TCH, an online mirror descent algorithm for Tchebycheff scalarization that optimizes the worst-case objective, smoothing out the hard one-hot update. They also introduce AdaOMD-TCH, an adaptive online-to-batch conversion scheme that selectively discards suboptimal iterates while retaining theoretical convergence guarantees. The proposed methods achieve a convergence rate of O(√(log m/T)) and demonstrate superior performance on both synthetic problems and federated learning tasks under fairness constraints.

## Method Summary
The paper proposes OMD-TCH, which reformulates Tchebycheff scalarization using online mirror descent to optimize a weighted combination of all objectives rather than just the worst one at each step. The algorithm uses two coupled OMD loops - one for model parameters θ using gradient descent, and one for weight vector λ using either projected gradient descent or exponentiated gradient. AdaOMD-TCH improves practical performance by excluding suboptimal iterates dominated by others while maintaining theoretical convergence guarantees through an adaptive online-to-batch conversion scheme that maintains a Pareto set with redistributed weights.

## Key Results
- OMD-TCH achieves O(√(log m/T)) convergence rate, improving on the O(√(m/T)) rate of traditional methods
- AdaOMD-TCH significantly improves over OMD-TCH, achieving comparable performance to state-of-the-art fair FL methods in both accuracy and fairness metrics
- The methods demonstrate superior performance on synthetic VLMOP2 problems and federated learning tasks under fairness constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OMD-TCH smooths out the hard one-hot optimization in Tchebycheff scalarization by using online mirror descent to optimize a weighted combination of all objectives rather than just the worst one at each step.
- Mechanism: The inner maximization in Tchebycheff scalarization is reformulated from discrete choices to a continuous simplex, enabling OMD to dynamically weight gradients based on accumulated losses and past iterates.
- Core assumption: The dual problem formulation is valid and that OMD applied to both parameters converges to the same solution as the original Tchebycheff scalarization.
- Evidence anchors:
  - [abstract] "We propose an online mirror descent algorithm for Tchebycheff scalarization, which we call OMD-TCH. We show that OMD-TCH enjoys a convergence rate of O(sqrt(log m/T))..."
  - [section] "First, we formulate a key equivalent transformation: min_θ max_λ L(θ, λ; w) using the fact that max_λ Σ λ_i c_i = max_i c_i, for any scalers c_i ∈ R."
  - [corpus] Weak evidence - related papers focus on scalarization methods but don't directly validate the OMD smoothing mechanism.
- Break condition: If the reformulation to continuous λ loses the discrete worst-case property or if OMD doesn't converge due to non-convexity in practice.

### Mechanism 2
- Claim: The adaptive online-to-batch conversion (AdaOMD-TCH) improves practical performance by excluding suboptimal iterates dominated by others while maintaining theoretical convergence guarantees.
- Mechanism: Iterates are tracked and weighted based on Pareto dominance relationships. Suboptimal iterates have their unit weight redistributed to dominating Pareto optimal iterates, resulting in a final solution that is a weighted average of only non-dominated solutions.
- Core assumption: The weight redistribution preserves the convergence rate because the final weighted average still bounds the original uniform average's regret.
- Evidence anchors:
  - [section] "Our final solution can be expresses as: ˜θ = 1/T Σ_θ(τ)∈P γ_τ θ(τ), with the following properties: P = {θ(τ), τ ∈ [T] | ∄t ∈ [T] s.t. θ(t) ⪯ θ(τ)}..."
  - [section] "We prove that AdaOMD-TCH retains the same convergence guarantees as OMD-TCH while showing much-improved performance in practice."
  - [corpus] Weak evidence - related papers discuss Pareto dominance but not this specific adaptive weighting scheme.
- Break condition: If the set of non-dominated iterates is too small or if the weighting distorts the solution away from the true Pareto optimal region.

### Mechanism 3
- Claim: Using exponentiated gradient (EG) for the λ parameter offers a theoretical advantage in the convergence rate's dependency on the number of objectives m, achieving O(√(log m/T)) instead of O(√(m/T)).
- Mechanism: EG's update rule incorporates exponential weighting of past losses, which smooths the weight changes and provides better control over the dual norm growth compared to projected gradient descent.
- Core assumption: The strong convexity and bounded gradient assumptions hold such that EG's logarithmic dependency on m is achievable in practice.
- Evidence anchors:
  - [section] "Theorem 5... Using PGD for θ and EG for λ... both Algorithms 1 and 2 converge as follows... O(sqrt(log m/T)."
  - [section] "Comparing (16) and (17), one can see that using EG for λ gives a logarithm advantage in the convergence rate's dependency on the number of objectives m..."
  - [corpus] Weak evidence - related papers mention EG in multi-objective contexts but don't provide empirical validation of this specific log m improvement.
- Break condition: If the problem dimensionality or objective conflicts are such that EG's theoretical advantage doesn't translate to better empirical performance.

## Foundational Learning

- Concept: Multi-objective optimization and Pareto optimality
  - Why needed here: Understanding the problem setup is critical - we're not just minimizing a single loss but finding solutions on the Pareto front where no objective can be improved without worsening another.
  - Quick check question: Given two solutions θ1 and θ2, if f1(θ1) < f1(θ2) and f2(θ1) > f2(θ2), can either dominate the other?

- Concept: Online learning and regret minimization
  - Why needed here: The algorithm treats one parameter as the adversary and uses online learning to minimize regret, which is the theoretical foundation for the convergence guarantees.
  - Quick check question: In the regret definition, what does the term "min_x Σ_t ℓ(t)(x)" represent in the context of multi-objective optimization?

- Concept: Bregman divergences and mirror descent
  - Why needed here: The algorithm uses different distance generating functions (ψθ, ψλ) to induce different update rules (PGD, EG), and understanding Bregman divergences is key to analyzing convergence.
  - Quick check question: For ψ(x) = 1/2 ||x||^2, what is the Bregman divergence Bψ(x; y) in terms of x and y?

## Architecture Onboarding

- Component map: λ update (EG/PGD) -> θ update (GD) -> Pareto set maintenance (AdaOMD-TCH) -> final weighted solution
- Critical path: For each iteration: evaluate all objectives and gradients → update λ using OMD → update θ using OMD → (in AdaOMD-TCH) update Pareto set and weights → output final solution via weighted average
- Design tradeoffs: Using EG for λ gives better theoretical convergence but may not improve empirical results; the adaptive conversion adds computational overhead for maintaining the Pareto set but improves final solution quality
- Failure signatures: If training oscillates or converges slowly, check λ updates and step sizes; if final solution is poor, verify Pareto dominance checks and weight redistribution; if memory usage is high, examine the size of the Pareto set
- First 3 experiments:
  1. Run OMD-TCH on a simple 2-objective synthetic problem (like VLMOP2) with PGD for both parameters to verify it finds diverse Pareto optimal solutions
  2. Compare OMDgd-TCH vs OMDeg-TCH on the same problem to see if EG provides any practical advantage in convergence speed or solution quality
  3. Implement AdaOMD-TCH on the synthetic problem and verify that it excludes dominated iterates and achieves similar or better results than OMD-TCH

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the adaptive online-to-batch conversion scheme (AdaOMD-TCH) consistently outperform the traditional uniform averaging conversion across all multi-objective optimization problems, or are there specific problem structures where it underperforms?
- Basis in paper: [explicit] The paper demonstrates superior performance of AdaOMD-TCH over OMD-TCH (which uses uniform averaging) in federated learning tasks and synthetic problems, but does not provide a comprehensive analysis across diverse problem types.
- Why unresolved: The empirical evaluation focuses on specific problem domains (non-convex Pareto fronts and federated learning under fairness constraints). The paper does not explore a wide range of problem structures to determine the generalizability of the adaptive scheme's advantage.
- What evidence would resolve it: Extensive empirical studies on diverse multi-objective optimization problems, including convex/non-convex, high-dimensional, and various application domains, comparing AdaOMD-TCH and uniform averaging convergence and solution quality.

### Open Question 2
- Question: Can the theoretical convergence rate advantage of using Exponentiated Gradient (EG) for updating the weight vector λ in OMD-TCH be consistently observed in practice, or are there cases where Projected Gradient Descent (PGD) performs comparably or better?
- Basis in paper: [explicit] The paper proves a theoretical convergence rate advantage of O(sqrt(log m/T)) for EG over O(sqrt(m/T)) for PGD in the dependency on the number of objectives m, but observes no significant empirical difference between the two.
- Why unresolved: The theoretical analysis shows a logarithmic advantage for EG, but the empirical results do not reflect this advantage. The paper does not investigate the conditions under which the theoretical advantage might manifest or why it is not observed in practice.
- What evidence would resolve it: Systematic empirical studies varying the number of objectives m, problem complexity, and other relevant parameters to identify conditions where EG's theoretical advantage translates to practical performance gains over PGD.

### Open Question 3
- Question: How does the performance of OMD-TCH and AdaOMD-TCH scale with the number of objectives (m) and the dimensionality of the solution space (d), and are there practical limitations on the size of problems these methods can effectively handle?
- Basis in paper: [inferred] The theoretical analysis shows dependencies on m and d in the convergence rates, but the empirical evaluation is limited to problems with a small number of objectives (m=2 for synthetic problems and m=10 for federated learning tasks). The dimensionality of the solution space is also not extensively explored.
- Why unresolved: The paper does not provide empirical evidence on the scalability of the proposed methods with respect to the number of objectives and solution space dimensionality. It is unclear how the methods perform on high-dimensional problems with many objectives.
- What evidence would resolve it: Empirical studies scaling the number of objectives (m) and solution space dimensionality (d) to larger values, evaluating convergence rates, solution quality, and computational efficiency of OMD-TCH and AdaOMD-TCH compared to baseline methods.

## Limitations

- The theoretical convergence guarantees rely on strong assumptions (bounded gradients, Lipschitz continuity) that may not hold in deep learning applications
- The adaptive conversion scheme's computational overhead for maintaining the Pareto set is not thoroughly analyzed
- The paper provides limited empirical evidence on the scalability of methods with respect to the number of objectives and solution space dimensionality

## Confidence

- High confidence: OMD-TCH convergence rate of O(sqrt(log m/T)) - follows directly from mirror descent analysis
- Medium confidence: Practical effectiveness of adaptive online-to-batch conversion - demonstrated on synthetic and federated learning tasks but lacks ablation studies
- Low confidence: Logarithmic advantage of EG over PGD for λ - theoretical justification but no empirical validation comparing the two update rules

## Next Checks

1. Implement both OMDgd-TCH and OMDeg-TCH variants on a simple 2-objective synthetic problem and compare convergence curves and final solution quality to empirically verify the claimed logarithmic advantage of EG for λ.

2. Run ablation studies on AdaOMD-TCH where the adaptive conversion is disabled (uniform averaging) to isolate the contribution of the weight redistribution mechanism to overall performance improvements.

3. Test the algorithms on a non-convex multi-objective problem where linear scalarization fails to find diverse Pareto optimal solutions, to verify that OMD-TCH can escape local minima and find solutions that linear methods miss.