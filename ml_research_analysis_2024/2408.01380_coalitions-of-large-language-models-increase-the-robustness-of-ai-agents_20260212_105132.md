---
ver: rpa2
title: Coalitions of Large Language Models Increase the Robustness of AI Agents
arxiv_id: '2408.01380'
source_url: https://arxiv.org/abs/2408.01380
tags:
- arxiv
- coalition
- system
- which
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a coalition approach for AI agents using multiple
  pretrained open-source Large Language Models (LLMs) to enhance robustness and reduce
  operational costs. Instead of relying on a single model or fine-tuning, the coalition
  assigns specialized tasks (planning, slot filling, and response forming) to different
  models based on their strengths.
---

# Coalitions of Large Language Models Increase the Robustness of AI Agents

## Quick Facts
- **arXiv ID**: 2408.01380
- **Source URL**: https://arxiv.org/abs/2408.01380
- **Reference count**: 33
- **Primary result**: Coalition of specialized LLMs achieves 58.6% accuracy vs 45.6% for fine-tuned models on ToolAlpaca benchmark

## Executive Summary
This paper introduces a coalition approach for AI agents using multiple pretrained open-source Large Language Models (LLMs) to enhance robustness and reduce operational costs. Instead of relying on a single model or fine-tuning, the coalition assigns specialized tasks (planning, slot filling, and response forming) to different models based on their strengths. This method outperforms both fine-tuned models and single-model approaches in the ToolAlpaca benchmark, achieving 58.6% overall accuracy compared to 45.6% for fine-tuned models. The coalition approach also reduces costs by leveraging smaller, task-specific models without compromising accuracy. The findings suggest that multi-model systems can offer significant performance and cost benefits for AI agent development.

## Method Summary
The coalition approach distributes specialized sub-tasks across multiple pretrained LLMs, assigning each model to the task it performs best at based on inherent strengths from different training data, architectures, and fine-tuning strategies. The system uses the ToolAlpaca benchmark along with custom datasets to evaluate performance. Rather than fine-tuning models for specific tasks, the coalition leverages pretrained models' existing capabilities, with JSON RAG task specialization allowing smaller models to outperform larger ones on structured data tasks.

## Key Results
- Coalition approach achieves 58.6% overall accuracy on ToolAlpaca benchmark versus 45.6% for fine-tuned models
- Smaller models can outperform larger models when task specialization aligns with model architecture strengths
- The approach reduces operational costs by using smaller, task-specific models without compromising accuracy

## Why This Works (Mechanism)

### Mechanism 1
Distributing specialized sub-tasks across multiple pretrained models improves accuracy without fine-tuning. Each model in the coalition is assigned to the sub-task it performs best at (e.g., planning vs. slot filling vs. response forming), leveraging the inherent strengths of different architectures and training backgrounds. This works because different LLMs exhibit consistent task-specific strengths due to variations in training data, model architecture, and fine-tuning strategies. The coalition approach outperforms single models in the ToolAlpaca benchmark, achieving 58.6% accuracy versus 45.6% for fine-tuned models. If task performance variance between models is small, gains from specialization vanish.

### Mechanism 2
JSON RAG task specialization allows a smaller model (Flan UL2) to outperform larger models. Flan UL2's Mixture of Denoisers architecture aligns with the denoising nature of JSON RAG (masking and reconstructing relevant fields), giving it an edge on this specific task despite fewer parameters. This works because model architecture can provide systematic advantages on structurally similar tasks. Investigating Flan UL2's architecture reveals it uses a Mixture of Denoisers approach that aligns well with JSON RAG's denoising requirements. If JSON RAG prompts or response formats change significantly, the denoising advantage may erode.

### Mechanism 3
Plan critique and correction benefit from instruction-tuned models over general-purpose models. Models fine-tuned for instruction following or interactive prompting (Mistral, Mixtral, Llama 2) are better at reasoning about and correcting plan errors than models without such fine-tuning (Flan UL2). This works because instruction tuning imparts stronger reasoning and interactive linguistic capabilities needed for critique tasks. The Flan UL2 model is the least accurate when tasked with critiquing generated plans because it lacks the reasoning and interactive linguistic capabilities present in instruction-tuned models. If critique tasks become more template-based or less reasoning-intensive, instruction tuning advantage may diminish.

## Foundational Learning

- **LLM sub-task decomposition**: The coalition approach depends on cleanly separating planning, slot filling, and response forming so each can be assigned to the optimal model. Quick check: If a single model is used for all sub-tasks, what is the primary performance bottleneck likely to be?

- **Semantic textual similarity (STS) metrics**: Evaluation of query similarity in slot filling and response quality uses STS (e.g., MiniLM embeddings) to determine correctness. Quick check: Why might STS be preferred over exact string matching in tool use evaluation?

- **Context window management in LLMs**: Large JSON responses from tool executions can exceed context limits, necessitating techniques like JSON RAG to filter and retrieve relevant content. Quick check: What happens if the JSON RAG filtering step is omitted and the full response is passed to the next model?

## Architecture Onboarding

- **Component map**: Natural language intent → Planner LLM (e.g., Mistral) → Slot Filler LLM (e.g., Mixtral) → Tool Execution → JSON RAG LLM (e.g., Flan UL2) → Response Former LLM (e.g., Mixtral) → Final output

- **Critical path**: Planner → Slot Filler → JSON RAG → Response Former (any failure here breaks the workflow)

- **Design tradeoffs**:
  - Model specialization vs. operational overhead (multiple model hosts)
  - Smaller specialized models reduce cost but may require more coordination
  - Fine-tuning for each task vs. leveraging pretrained strengths

- **Failure signatures**:
  - Planner failure: Non-existent tools in plan, missing required steps
  - Slot filler failure: Incorrect parameter payloads, malformed queries
  - JSON RAG failure: Irrelevant or missing content in filtered response
  - Response former failure: Off-topic or incoherent final output

- **First 3 experiments**:
  1. Run the full pipeline on a single easy ToolAlpaca case to verify end-to-end functionality.
  2. Swap Mistral planner with Mixtral and compare planning accuracy to confirm specialization benefit.
  3. Disable JSON RAG filtering and measure impact on slot filling and response quality when tool responses are large.

## Open Questions the Paper Calls Out

### Open Question 1
Can a coalition of fine-tuned models achieve state-of-the-art performance compared to a single fine-tuned model? The authors conclude by posing this question for future study, noting that while their coalition of pretrained models outperforms single fine-tuned models, it's unclear if fine-tuning multiple models would yield even better results. This remains unresolved because the paper only tests a coalition of pretrained (non-fine-tuned) models against single fine-tuned models. Empirical testing comparing the performance of a coalition of fine-tuned models to both single fine-tuned models and the coalition of pretrained models presented in this paper would resolve this question.

### Open Question 2
How generalizable is the coalition approach across different types of tasks beyond tool use and API workflows? The authors suggest the coalition approach could be applied to "other non-agentic systems which utilise LLMs," but do not provide evidence or analysis for tasks outside the tool use domain. This remains unresolved because the evaluation is limited to the ToolAlpaca benchmark focused on tool use agents, without exploring other domains or task types where LLMs are applied. Testing the coalition approach on diverse LLM applications such as question answering, text summarization, creative writing, or code generation would assess performance and cost benefits across domains.

### Open Question 3
What is the optimal strategy for allocating tasks to models within a coalition, and how does model selection impact overall system performance? The authors note that allocating tasks to different models based on their strengths improves performance, but do not provide a systematic method for determining the best model-task assignments or how to dynamically adjust the coalition composition. This remains unresolved because the paper identifies that certain models excel at specific tasks but does not explore the process of selecting or optimizing model-task allocations for maximum performance. Developing and testing a framework for automatically assigning tasks to models based on performance metrics, context, and workload would measure the impact on accuracy and cost efficiency compared to static coalitions.

## Limitations

- Claims are demonstrated only on the ToolAlpaca benchmark, which may not generalize to other domains or task types
- Cost savings are mentioned but not quantitatively analyzed across different deployment scenarios
- Model selection for sub-tasks is based on observed performance rather than systematic benchmarking

## Confidence

- **High Confidence**: The coalition approach outperforms single models on the ToolAlpaca benchmark, as demonstrated by empirical results (58.6% vs 45.6% accuracy)
- **Medium Confidence**: Model specialization benefits are real but may be task-dependent; specific assignments could be dataset-specific rather than universally optimal
- **Low Confidence**: Cost reduction claims lack quantitative support; operational overhead of managing multiple models versus potential savings is not rigorously analyzed

## Next Checks

1. Test the coalition approach on diverse benchmarks beyond ToolAlpaca to assess generalizability across different task types and domains.

2. Conduct a comprehensive cost analysis comparing total operational expenses (API calls, hosting, coordination) between coalition deployment and fine-tuned single-model approaches.

3. Systematically benchmark multiple models on each sub-task to verify whether the current model assignments (Mistral for planning, Mixtral for slot filling, etc.) remain optimal as new models are released or with different datasets.