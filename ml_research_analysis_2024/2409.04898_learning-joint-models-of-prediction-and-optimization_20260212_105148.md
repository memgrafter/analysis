---
ver: rpa2
title: Learning Joint Models of Prediction and Optimization
arxiv_id: '2409.04898'
source_url: https://arxiv.org/abs/2409.04898
tags:
- optimization
- problem
- learning
- training
- ltof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Learning to Optimize from Features (LtOF),
  a generic framework that learns to construct optimal solutions directly from observable
  features in Predict-Then-Optimize problems. Instead of predicting problem parameters
  and solving optimization, LtOF directly maps features to optimal solutions using
  pretrained Learn-to-Optimize proxies.
---

# Learning Joint Models of Prediction and Optimization

## Quick Facts
- arXiv ID: 2409.04898
- Source URL: https://arxiv.org/abs/2409.04898
- Reference count: 39
- Primary result: LtOF framework learns to construct optimal solutions directly from observable features, outperforming two-stage methods by up to an order of magnitude in regret while achieving 10-100x speedups.

## Executive Summary
This paper introduces Learning to Optimize from Features (LtOF), a framework that addresses Predict-Then-Optimize problems by learning direct mappings from observable features to optimal solutions rather than predicting parameters for traditional optimization solvers. LtOF leverages pretrained Learn-to-Optimize proxies to bypass the distributional shift problem that plagues two-stage approaches, where intermediate predictions drift during end-to-end training. The framework is validated across three challenging optimization domains: convex quadratic programs, nonconvex quadratic programs with oscillating objectives, and AC-Optimal Power Flow problems, demonstrating both superior decision quality and computational efficiency compared to existing approaches.

## Method Summary
LtOF learns direct mappings from observable features z to optimal solutions x⋆ by adapting three Learn-to-Optimize methods (Lagrangian Dual Learning, Self-supervised Primal-Dual Learning, and Deep Constraint Completion and Correction) with pretrained proxies. The framework generates features using random neural networks that map ground-truth parameters to observable features, then trains neural networks to predict optimal solutions from these features. During training, the optimization proxy weights adapt jointly with the prediction model, avoiding distributional shift. For nonconvex problems, LtOF constructs solutions directly without relying on derivatives, using constraint restoration methods (projection or Newton's method) to ensure feasibility.

## Key Results
- LtOF outperforms two-stage methods by up to an order of magnitude in regret across three optimization tasks
- Achieves 10-100x speedups over traditional optimization solvers while maintaining competitive accuracy
- Successfully handles nonconvex optimization components where gradient-based end-to-end approaches fail
- LtOF becomes competitive with end-to-end methods as feature mapping complexity increases

## Why This Works (Mechanism)

### Mechanism 1
LtOF avoids distributional shift by learning direct mappings from features to solutions rather than parameters to solutions. In LtOF, the model Jϕ(z) learns to map observable features z directly to optimal solutions x, bypassing the intermediate parameter prediction step that causes distribution shift in EPO with pretrained proxies. The core assumption is that the LtO proxy can effectively learn to map from features to solutions when trained with the correlated ground-truth parameters as targets.

### Mechanism 2
LtOF achieves computational efficiency by replacing optimization solvers with learned neural network mappings. LtOF models execute orders of magnitude faster than traditional optimization-based approaches because they use lightweight neural networks instead of solving optimization problems at inference time. The core assumption is that the learned neural network can approximate the optimization solution mapping sufficiently well for practical use.

### Mechanism 3
LtOF handles nonconvex optimization problems where gradient-based EPO fails. LtOF learns to construct solutions directly from features without relying on derivatives through nonconvex functions, avoiding the poor local minima that gradient-based EPO encounters. The core assumption is that LtO methods can learn to solve nonconvex problems effectively through their specialized training procedures.

## Foundational Learning

- **Concept**: Distributional shift in machine learning
  - Why needed here: Understanding why EPO with pretrained proxies fails due to inputs drifting during training
  - Quick check question: What happens to model performance when test inputs differ significantly from training inputs?

- **Concept**: Lagrangian duality and constraint handling
  - Why needed here: LtOF methods use Lagrangian-based approaches to handle optimization constraints during training
  - Quick check question: How does the augmented Lagrangian method enforce constraint satisfaction in neural network training?

- **Concept**: End-to-end differentiable optimization
  - Why needed here: Understanding the challenges EPO faces with backpropagation through optimization solvers
  - Quick check question: What makes backpropagation through optimization problems challenging for nonconvex or discrete problems?

## Architecture Onboarding

- **Component map**: Feature extraction → Neural network prediction → Constraint handling (if needed) → Solution output
- **Critical path**: Observable features z → Neural network Jϕ(z) → Optimal solution x⋆
- **Design tradeoffs**: Accuracy vs. inference speed, model complexity vs. generalization, constraint satisfaction vs. objective optimization
- **Failure signatures**: High regret on test data, constraint violations in predicted solutions, slow inference times compared to optimization solvers
- **First 3 experiments**:
  1. Implement LtOF with Lagrangian Dual Learning on the convex portfolio optimization problem
  2. Compare LtOF performance against two-stage and EPO baselines on nonconvex QP variant
  3. Test LtOF on AC-Optimal Power Flow problem with constraint restoration using Newton's method

## Open Questions the Paper Calls Out

### Open Question 1
How can the distributional shift problem be addressed more effectively when using pretrained optimization proxies in end-to-end Predict-Then-Optimize training? The paper identifies that pretrained LtO proxies experience accuracy degradation during EPO training due to distribution shift as weights θ update, leading to continuously evolving inputs to the proxy. While the paper proposes joint training of prediction and optimization models (LtOF) as a solution, it doesn't explore alternative approaches like online adaptation of proxies, ensemble methods, or specialized regularization techniques to maintain proxy accuracy during training.

### Open Question 2
Can LtOF methods be effectively extended to handle combinatorial optimization problems with discrete decision variables? The paper acknowledges that LtOF cannot be applied to optimization components without effective LtO solutions, and notes that combinatorial optimization proxies typically use different approaches like reinforcement learning rather than the supervised/self-supervised methods studied here. The paper deliberately excludes combinatorial optimization from its evaluation, leaving open whether the LtOF framework can be adapted to problems like vehicle routing, scheduling, or combinatorial auctions where discrete decisions are essential.

### Open Question 3
What is the theoretical relationship between feature mapping complexity (k) and the regret gap between LtOF and end-to-end Predict-Then-Optimize methods? The paper shows empirically that LtOF performance improves relative to two-stage methods as feature mapping complexity k increases, and becomes competitive with EPO for k > 1, but doesn't provide theoretical analysis of this relationship. While empirical results show LtOF catching up to EPO as k increases, the paper doesn't explain why this happens or establish bounds on the regret gap as a function of k, leaving unclear when LtOF becomes preferable to EPO.

## Limitations

- Limited quantitative analysis of distributional shift reduction compared to two-stage methods
- No specification of baseline optimization solver implementations for efficiency comparisons
- Constraint restoration methods mentioned but not thoroughly evaluated for computational cost

## Confidence

- **High confidence**: Empirical results showing LtOF outperforming two-stage methods in regret across all three problem domains
- **Medium confidence**: Claim about LtOF handling nonconvex problems where gradient-based EPO fails
- **Medium confidence**: Computational efficiency claims, pending clarification on baseline solver implementations
- **Low confidence**: Distributional shift argument lacks quantitative backing

## Next Checks

1. Measure and compare the input distributions between training and inference for both two-stage and LtOF methods to provide concrete evidence of distribution shift reduction.

2. Identify and document the specific optimization solver implementations used as baselines, including their versions and configuration parameters.

3. Measure the computational cost and impact on solution quality of the constraint restoration methods (projection, Newton's method) to determine if they negate the claimed efficiency benefits.