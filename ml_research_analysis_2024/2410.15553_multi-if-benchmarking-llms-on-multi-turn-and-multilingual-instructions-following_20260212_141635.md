---
ver: rpa2
title: 'Multi-IF: Benchmarking LLMs on Multi-Turn and Multilingual Instructions Following'
arxiv_id: '2410.15553'
source_url: https://arxiv.org/abs/2410.15553
tags:
- error
- languages
- turn
- constraints
- format
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Multi-IF, a benchmark for evaluating large
  language models on multi-turn and multilingual instruction following. Multi-IF expands
  the IFEval dataset by creating multi-turn conversations and translating prompts
  into 7 additional languages, resulting in 4,501 multilingual conversations with
  three turns each.
---

# Multi-IF: Benchmarking LLMs on Multi-Turn and Multilingual Instructions Following

## Quick Facts
- **arXiv ID:** 2410.15553
- **Source URL:** https://arxiv.org/abs/2410.15553
- **Reference count:** 40
- **Primary result:** Multi-IF benchmark reveals significant degradation in LLM instruction-following performance across turns and languages

## Executive Summary
Multi-IF introduces a benchmark for evaluating large language models on multi-turn and multilingual instruction following. The benchmark expands the IFEval dataset by creating three-turn conversations and translating prompts into seven additional languages, resulting in 4,501 multilingual conversations. Evaluation of 14 state-of-the-art LLMs shows that Multi-IF presents a significantly more challenging task than existing benchmarks, with all models demonstrating decreased performance as conversation turns increase. The benchmark also reveals limitations in multilingual capabilities, particularly for non-Latin script languages.

## Method Summary
The Multi-IF benchmark was constructed by expanding single-turn IFEval prompts into three-turn conversations, removing conflicting instructions, translating prompts into seven additional languages, and filtering sensitive content. The evaluation framework measures instruction adherence using strict and loose accuracy metrics at both instruction and conversation levels. The benchmark includes human auditing for translation quality and evaluates models on eight languages including English, Hindi, Russian, Chinese, and others.

## Key Results
- All tested models show significant performance degradation from turn 1 (0.877 accuracy for o1-preview) to turn 3 (0.707 accuracy for o1-preview)
- Non-Latin script languages (Hindi, Russian, Chinese) exhibit higher error rates than Latin script languages
- Models increasingly forget previously followed instructions as turns progress, with instruction forgetting ratio increasing across turns
- o1-preview and o1-mini show the highest error correction rates, correcting approximately 25% of previously unfollowed instructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-IF's multi-turn expansion creates compounding difficulty by chaining instruction adherence failures across turns.
- Mechanism: Each turn's response becomes part of the next turn's context, so errors propagate and accumulate. The model must maintain consistency while executing new instructions, creating cognitive load that increases with each turn.
- Core assumption: LLMs process multi-turn conversations as a single context window where prior responses influence current output generation.
- Evidence anchors:
  - [abstract] "All the models tested showed a higher rate of failure in executing instructions correctly with each additional turn."
  - [section] "We observed that as the number of turns increases, LLMs increasingly forget to adhere to instructions that were successfully executed in previous turns."
  - [corpus] Weak - related work focuses on instruction following but lacks specific analysis of multi-turn propagation mechanisms.
- Break condition: If models could perfectly recall and maintain instruction adherence across turns, this compounding effect would disappear.

### Mechanism 2
- Claim: Instruction Forgetting Ratio (IFR) quantifies the degradation in instruction retention across turns, with higher rates indicating poorer multi-turn instruction following.
- Mechanism: IFR measures the percentage of previously followed instructions that are not followed in subsequent turns. This metric captures the model's ability to maintain instruction awareness across conversational context shifts.
- Core assumption: Instruction adherence can be objectively measured by checking whether specific instructions are present in model responses.
- Evidence anchors:
  - [section] "We first define Instruction Forgetting Ratio (IFR): The percentage of previously followed instructions that were not followed in the subsequent turn"
  - [section] "Our Analysis of the visualization reveals a pronounced tendency for LLMs to exhibit decreased adherence to previously executed instructions as the number of turns progresses."
  - [corpus] Weak - related work discusses instruction following but doesn't establish IFR as a specific metric for multi-turn degradation.
- Break condition: If IFR rates were consistently low across all turns, it would indicate the model maintains instruction awareness effectively.

### Mechanism 3
- Claim: Error Correction Ratio (ECR) demonstrates that models with hidden chain-of-thought capabilities can recover from prior errors in later turns.
- Mechanism: ECR measures the percentage of previously unfollowed instructions that are corrected in subsequent turns. This captures the model's ability to self-correct without explicit instruction to do so.
- Core assumption: Models can detect and correct their own errors through internal reasoning processes without violating instruction-following constraints.
- Evidence anchors:
  - [section] "OpenAI's o1-preview and o1-mini models exhibit the highest ECRâ€”correcting around 25% of unfollowed instructions in later turns"
  - [section] "This is likely because these models do not 'reflect' on their outputs to fix prior errors, as doing so would violate the instruction-following criteria."
  - [corpus] Weak - related work discusses instruction following but doesn't specifically analyze error correction mechanisms across turns.
- Break condition: If ECR rates were consistently high across all models, it would indicate error correction is a general capability rather than specific to certain architectures.

## Foundational Learning

- Concept: Verifiable instructions
  - Why needed here: Multi-IF builds on IFEval's approach of using verifiable instructions that can be objectively checked for compliance
  - Quick check question: What makes an instruction "verifiable" in the context of evaluating LLM outputs?

- Concept: Multi-turn conversation context
  - Why needed here: Understanding how LLMs process and maintain context across multiple conversational turns is crucial for interpreting Multi-IF results
  - Quick check question: How does concatenating previous turns' prompts and responses affect the model's current generation?

- Concept: Multilingual evaluation challenges
  - Why needed here: Multi-IF's translation and evaluation process must account for language-specific characteristics like character counting and sentence segmentation
  - Quick check question: Why can't instructions like "all uppercase" be translated to Chinese in the same way they apply to English?

## Architecture Onboarding

- Component map: Single-turn prompts -> Multi-turn expansion -> Conflict removal -> Multilingual translation -> Human auditing -> Sensitive content filtering -> Evaluation framework
- Critical path: The evaluation pipeline is critical - transforming English prompts to multi-turn, multilingual format, then measuring instruction adherence across turns with strict and loose criteria.
- Design tradeoffs: The benchmark trades off between comprehensive multi-turn coverage (3 turns per conversation) and dataset size (4,501 conversations), balancing depth with breadth of evaluation.
- Failure signatures: High IFR rates indicate instruction forgetting, while low ECR rates suggest poor error recovery capabilities. Language-specific error patterns reveal multilingual capability gaps.
- First 3 experiments:
  1. Evaluate a baseline model on single-turn IFEval to establish baseline instruction-following performance
  2. Run the same model on Multi-IF first turn only to isolate single-turn performance from multi-turn effects
  3. Compare model performance across all three turns to quantify degradation and identify turning points where instruction adherence significantly drops

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different chain-of-thought approaches affect instruction-following accuracy in multi-turn scenarios?
- Basis in paper: [explicit] The paper notes that o1-preview and o1-mini models show higher error correction rates (ECR) compared to other models, suggesting that their incorporation of a hidden chain of thought is especially helpful in error correction during multi-turn interactions.
- Why unresolved: While the paper identifies the benefits of chain-of-thought for error correction, it doesn't explore different chain-of-thought strategies or their comparative effectiveness. The analysis focuses on whether models can correct errors rather than the quality or approach of the chain-of-thought itself.
- What evidence would resolve it: A systematic comparison of different chain-of-thought techniques (e.g., explicit vs. implicit, different prompting strategies) and their impact on instruction-following accuracy across multiple turns would provide insights into optimal approaches.

### Open Question 2
- Question: What are the specific linguistic and cultural factors contributing to higher error rates in non-Latin script languages?
- Basis in paper: [inferred] The paper observes that languages with non-Latin scripts (Hindi, Russian, and Chinese) generally exhibit higher error rates, suggesting potential limitations in the models' multilingual capabilities. However, it doesn't delve into the specific linguistic or cultural factors causing these discrepancies.
- Why unresolved: While the paper identifies a performance gap, it doesn't investigate the underlying reasons for this gap. Understanding whether errors stem from script differences, cultural context, or other linguistic features could inform targeted improvements.
- What evidence would resolve it: A detailed linguistic analysis comparing instruction-following errors across languages, including factors like script complexity, cultural references, and syntactic structures, would help identify the root causes of the performance gap.

### Open Question 3
- Question: How does instruction complexity and type affect multi-turn instruction-following performance across different languages?
- Basis in paper: [explicit] The paper's error analysis reveals that certain instruction categories (like length_constraint and combination) show consistently high error rates across languages, while others (like keywords) show lower error rates. However, it doesn't systematically explore how instruction complexity interacts with language-specific challenges.
- Why unresolved: While the paper identifies some patterns in error distribution, it doesn't investigate how instruction complexity and type interact with language-specific challenges to affect overall performance.
- What evidence would resolve it: A controlled study varying instruction complexity and type across multiple languages, combined with linguistic analysis of the errors, would reveal how these factors interact to influence performance.

## Limitations

- Dataset size and representation may not capture long-term instruction adherence patterns in extended conversations
- Instruction diversity is limited to IFEval dataset types, potentially missing novel instruction-following scenarios
- Evaluation criteria may oversimplify complex language understanding through binary instruction adherence assessment

## Confidence

- **Multi-turn degradation claim:** High confidence - Consistent performance drop across all tested models is well-documented and statistically significant
- **Multilingual capability gaps:** Medium confidence - Clear patterns exist but sample size and translation artifacts limit definitive conclusions
- **Error correction capabilities:** Medium confidence - ECR metric is innovative but may conflate genuine correction with coincidental instruction inclusion

## Next Checks

1. **Extended Turn Analysis:** Re-run the benchmark with conversations extended to 5-7 turns to determine if performance degradation follows a linear pattern or reaches a plateau, and identify critical turning points where instruction adherence collapses.

2. **Instruction Type Sensitivity Analysis:** Isolate performance by instruction category (length, format, content) to determine which types are most vulnerable to multi-turn degradation and whether certain models show systematic strengths or weaknesses.

3. **Cross-lingual Transfer Validation:** Conduct experiments where models are evaluated on instructions in languages they weren't explicitly trained on to test the robustness of multilingual instruction-following capabilities beyond the benchmark languages.