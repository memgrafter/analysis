---
ver: rpa2
title: Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain
  Question Answering
arxiv_id: '2403.05217'
source_url: https://arxiv.org/abs/2403.05217
tags:
- documents
- answer
- llms
- question
- expansion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes LLMQA, a framework that formulates open-domain
  question answering (ODQA) as three steps: query expansion, document selection, and
  answer generation. It leverages the multi-role capabilities of large language models
  (LLMs) by instructing them to play the roles of generators, rerankers, and evaluators.'
---

# Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering

## Quick Facts
- arXiv ID: 2403.05217
- Source URL: https://arxiv.org/abs/2403.05217
- Reference count: 40
- LLMs can achieve state-of-the-art performance in open-domain question answering by leveraging multi-role capabilities

## Executive Summary
This paper introduces LLMQA, a framework that formulates open-domain question answering (ODQA) as three steps: query expansion, document selection, and answer generation. The framework leverages the multi-role capabilities of large language models (LLMs) by instructing them to play the roles of generators, rerankers, and evaluators. A novel prompt optimization algorithm is introduced to refine role-playing prompts and guide LLMs to produce higher-quality evidence and answers. Extensive experiments on widely used benchmarks demonstrate that LLMQA achieves state-of-the-art performance in terms of both answer accuracy and evidence quality.

## Method Summary
LLMQA addresses ODQA by decomposing the task into three sequential components: query expansion, document selection, and answer generation. The framework leverages LLMs' ability to perform multiple roles through carefully designed prompts. A novel prompt optimization algorithm iteratively refines prompts to enhance role-playing performance. The system uses LLMs as generators to expand queries and produce potential answers, as rerankers to evaluate and order candidate documents, and as evaluators to assess answer quality. This multi-role approach allows for more comprehensive reasoning and better utilization of LLM capabilities compared to single-role approaches.

## Key Results
- LLMQA improves exact match (EM) scores by 4.0 points on TriviaQA, 2.7 points on WebQ, and 3.1 points on NQ compared to previous best results
- The query expansion generator achieves 73%, 76%, and 87% recall scores for the answer in generated expansions on TriviaQA, WebQ, and NQ respectively
- The reranker role increases answer coverage by approximately 8.1% compared to baseline approaches

## Why This Works (Mechanism)
LLMQA works by decomposing ODQA into specialized subtasks that align with LLM capabilities. By instructing LLMs to play specific roles (generator, reranker, evaluator), the framework can leverage different aspects of LLM reasoning and generation abilities. The prompt optimization algorithm ensures that each role is performed optimally by iteratively refining instructions based on performance feedback. This approach allows LLMs to focus on their strengths in each role rather than trying to handle the entire ODQA pipeline as a single monolithic task.

## Foundational Learning

**Role-Based Prompting**: Using LLMs for specific roles through targeted prompts. Why needed: LLMs have different strengths in generation versus evaluation. Quick check: Test if role-specific prompts improve performance over generic prompts.

**Iterative Prompt Optimization**: Refining prompts based on performance feedback. Why needed: Initial prompts may not fully leverage LLM capabilities. Quick check: Measure performance improvement across optimization iterations.

**Multi-Stage ODQA Pipeline**: Breaking down question answering into query expansion, document selection, and answer generation. Why needed: Each stage benefits from specialized approaches. Quick check: Evaluate performance of each stage independently.

## Architecture Onboarding

**Component Map**: Query Expansion Generator -> Document Selection Reranker -> Answer Generation Evaluator

**Critical Path**: Query expansion produces candidate queries → Reranker selects relevant documents → Answer generator produces final answers

**Design Tradeoffs**: Single LLM handling all roles versus specialized LLMs for each role. The paper chooses single LLM approach for simplicity and consistency.

**Failure Signatures**: Poor query expansion leads to missed relevant documents; inadequate reranking results in irrelevant documents being selected; suboptimal answer generation produces incorrect or incomplete answers.

**3 First Experiments**:
1. Test query expansion performance with different prompt formulations
2. Evaluate reranking effectiveness using oracle documents
3. Measure answer generation quality with ground truth documents

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on LLM-based components introduces potential biases inherent in these models
- Prompt optimization algorithm may not generalize well across different LLM architectures or domains without retraining
- Computational overhead of multiple LLM invocations per question could limit real-world applicability in resource-constrained environments

## Confidence

**High Confidence**: Answer generation quality improvements and final EM score gains are well-supported by empirical results across three distinct benchmarks

**Medium Confidence**: Query expansion recall metrics and reranking coverage improvements are demonstrated but may be sensitive to specific LLM implementations and training data

**Medium Confidence**: The generalizability of prompt optimization across different LLMs and domains requires further validation

## Next Checks

1. **Cross-LLM Validation**: Test LLMQA's performance across multiple LLM architectures (GPT, Claude, LLaMA) to assess architecture dependence

2. **Resource Efficiency Analysis**: Measure computational costs and latency of each component to identify bottlenecks for real-time deployment

3. **Domain Transfer Study**: Evaluate framework performance on specialized domains (legal, medical, technical) to assess generalizability beyond general knowledge benchmarks