---
ver: rpa2
title: 'That''s My Point: Compact Object-centric LiDAR Pose Estimation for Large-scale
  Outdoor Localisation'
arxiv_id: '2403.04755'
source_url: https://arxiv.org/abs/2403.04755
tags:
- point
- semantic
- object
- registration
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a compact object-centric approach for LiDAR-based
  pose estimation. The key idea is to cluster segmented LiDAR scans into semantic
  objects and represent them using only the centroid coordinates and semantic class
  of each object, achieving extreme compression (1.33 kB per scan vs.
---

# That's My Point: Compact Object-centric LiDAR Pose Estimation for Large-scale Outdoor Localisation

## Quick Facts
- arXiv ID: 2403.04755
- Source URL: https://arxiv.org/abs/2403.04755
- Reference count: 40
- One-line primary result: Achieves 0.1 m translational and 0.5° rotational error on KITTI using only 1.33 kB per scan vs 1.41 MB raw

## Executive Summary
This paper proposes a novel approach for LiDAR-based pose estimation that achieves extreme compression by representing scenes as object centroids with semantic class labels rather than raw point clouds. The method clusters semantically segmented LiDAR scans into objects, extracts centroids and class labels, then uses an attention-based matching network to establish correspondences between scans. By discarding point-level geometry in favor of this compact representation, the method achieves significant storage savings while maintaining competitive accuracy with state-of-the-art methods. The approach is evaluated on KITTI and KITTI-360 datasets, demonstrating accurate metric estimates and effectiveness for long-term localization across different periods.

## Method Summary
The method clusters semantically labeled LiDAR scans into objects using DBSCAN, then represents each object as a centroid coordinate with semantic class label. A DGCNN-based feature enhancement module enriches these object features with geometric and semantic embeddings. The Superpoint Matching Module from GeoTransformer performs geometric self-attention within each scene and feature-based cross-attention between scenes, with semantic filtering to eliminate mismatches. Relative transformations are estimated using weighted SVD and RANSAC, followed by ICP refinement. Training employs a semantic distance-aware circle loss that focuses on spatially-proximal object pairs while ensuring positive pairs share the same semantic class. The approach achieves 1.33 kB storage per scan compared to 1.41 MB for raw data while maintaining competitive registration accuracy.

## Key Results
- Achieves translational error of approximately 0.1 m and rotational error of 0.5° on KITTI dataset
- Reduces storage requirements from 1.41 MB to 1.33 kB per scan (1000x compression)
- Demonstrates effective long-term localization across different periods and datasets
- Shows competitive performance with state-of-the-art methods while using extremely compressed representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Object centroids with semantic class labels provide sufficient discriminative features for registration despite discarding point-level geometry
- Mechanism: Clustering points into semantic objects and representing them as (centroid, class) tuples abstracts away fine-grained geometric detail while preserving global spatial structure. The object-matching network learns to compare objects using both semantic type and relative spatial relationships.
- Core assumption: Object centroids remain stable across different viewpoints and time periods, and semantic segmentation accurately groups points into meaningful objects.
- Evidence anchors: [abstract] "clustering all points of segmented scans into semantic objects and representing them only with their respective centroid and semantic class." [section III-B] "For each cluster we keep its centroid o – as the mean coordinate of its points – and semantic label l."
- Break condition: If semantic segmentation fails to group points into coherent objects, or if object centroids shift significantly due to viewpoint changes or sensor noise, the registration will fail.

### Mechanism 2
- Claim: Self- and cross-attention matching module can learn robust correspondences between object features despite extremely compressed representation
- Mechanism: Geometric self-attention encodes relative distances and angles between objects within each scene, creating transformation-invariant features. Feature-based cross-attention computes similarity scores between objects across scenes, with semantic filtering to remove mismatches.
- Core assumption: Attention mechanisms can extract meaningful relationships from sparse object representations, and semantic filtering reliably eliminates incorrect matches.
- Evidence anchors: [section III-D] "we employ the Superpoint Matching Module of GeoTransformer [8], which explicitly models the layout within scenes through a geometric self-attention module... and the latent feature similarity across scenes through a feature-based cross-attention module." [section III-D] "We mask out the similarity scores of objects with different labels ( ⑦ in Fig. 2), i.e. set ¯si,j := 0, if ls i ̸= lm j ."
- Break condition: If attention mechanisms cannot extract useful features from sparse representations, or if semantic filtering is too restrictive or permissive, matching will fail.

### Mechanism 3
- Claim: Semantic distance-aware circle loss effectively trains network to learn discriminative object features without explicit overlap supervision
- Mechanism: Loss function pulls together features of spatially-proximal object pairs of same semantic class while pushing apart features of objects that are far apart or have different classes. This encourages network to learn features capturing both semantic similarity and spatial relationships.
- Core assumption: Distance-based weighting and semantic class constraints sufficiently guide network toward learning useful features without explicit overlap supervision.
- Evidence anchors: [section III-F] "we introduce a semantic distance-aware circle loss which focuses the learning on spatially-proximal object pairs proportionally to their Euclidean distance, and ensures the positive pairs are of objects of the same semantic class." [section III-F] "The positive pairs and negative pairs are weighted according to weight factors βp i,j = √ρi,jγ(hi,j − ∆p) and βn i,k = γ(∆n −hi,k), where γ = 40 is a scaling factor and ∆p = 0.1 and ∆n = 1.4 are the corresponding margins."
- Break condition: If loss function cannot effectively guide network to learn discriminative features, or if assumptions about spatial proximity and semantic class consistency are violated, network will fail to learn useful representations.

## Foundational Learning

- Concept: Semantic segmentation and object clustering
  - Why needed here: Method relies on accurate semantic segmentation to group points into meaningful objects before extracting centroids and class labels
  - Quick check question: How does choice of clustering algorithm (e.g., DBSCAN) affect quality and consistency of extracted object centroids across different viewpoints?

- Concept: Point cloud registration and pose estimation
  - Why needed here: Core task is estimating relative transformation between two LiDAR scans, requiring understanding of point cloud registration techniques and limitations
  - Quick check question: What are key differences between feature-based registration methods (e.g., FPFH, SHOT) and learning-based methods, and how do these differences impact their suitability for different scenarios?

- Concept: Attention mechanisms and self-supervised learning
  - Why needed here: Method uses self- and cross-attention modules to learn object features and correspondences, and employs self-supervised loss function to train network
  - Quick check question: How do self-attention and cross-attention differ in their applications, and what are key design considerations when using them for point cloud registration?

## Architecture Onboarding

- Component map: LiDAR scan with semantic labels -> DBSCAN clustering -> Centroid and class extraction -> DGCNN feature enhancement -> Geometric self-attention + cross-attention matching -> Semantic filtering -> Weighted SVD + RANSAC -> ICP refinement

- Critical path: Object extraction -> Feature enhancement -> Matching -> Pose estimation
  - Each component must produce valid output for next component to function properly

- Design tradeoffs:
  - Compact representation vs. loss of geometric detail: Method sacrifices point-level geometry for storage efficiency, relying on network to learn meaningful features from sparse object representations
  - Semantic accuracy vs. robustness: Method depends on accurate semantic segmentation, but may be more robust to viewpoint changes than methods relying solely on geometric features
  - Computation vs. accuracy: Method uses computationally expensive attention mechanisms and RANSAC, but achieves high accuracy and recall

- Failure signatures:
  - Poor semantic segmentation: Objects may be incorrectly grouped or split, leading to inaccurate centroids and class labels
  - Unstable object centroids: Centroids may shift significantly across viewpoints, making it difficult to establish consistent correspondences
  - Ineffective attention mechanisms: Attention modules may fail to extract useful features from sparse representations, leading to poor matching
  - Over- or under-constrained loss: Loss function may not effectively guide network to learn discriminative features, or may lead to overfitting or underfitting

- First 3 experiments:
  1. Test object extraction: Run DBSCAN clustering on sample LiDAR scan and visualize resulting object centroids and class labels. Check for correct grouping and stable centroids across different viewpoints.
  2. Test feature enhancement: Pass object centroids and class labels through DGCNN-based feature enhancement module and visualize resulting object features. Check for meaningful and consistent features across different objects and viewpoints.
  3. Test matching and pose estimation: Use matching and pose estimation components to register pairs of LiDAR scans with known ground-truth transformations. Measure accuracy and recall of estimated poses and compare to baseline methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does proposed method perform on point clouds with different sensor characteristics (e.g., different LiDAR types, resolutions, or scanning patterns) not present in training data?
- Basis in paper: [inferred] Paper evaluates on KITTI and KITTI-360 datasets with specific Velodyne HDL-64 LiDAR sensor. No mention of testing on other sensor types or configurations.
- Why unresolved: Evaluation limited to single sensor type, leaving open question of generalizability to other LiDAR sensors with different characteristics.
- What evidence would resolve it: Testing method on datasets collected with different LiDAR sensors (e.g., different models, resolutions, scanning patterns) and comparing performance metrics to those obtained on KITTI/KITTI-360.

### Open Question 2
- Question: What is impact of varying object segmentation quality on registration accuracy, and how robust is method to segmentation errors?
- Basis in paper: [explicit] Paper mentions using pre-trained segmentation network (Cylinder3D) and notes performance can be affected by segmentation quality (e.g., BoxGraph struggles with reverse revisits when using Cylinder3D labels).
- Why unresolved: While paper acknowledges importance of segmentation quality, it doesn't systematically investigate how varying levels of segmentation errors affect registration accuracy or method's robustness to such errors.
- What evidence would resolve it: Conducting experiments with artificially degraded segmentation outputs and measuring resulting registration accuracy, or comparing performance using different segmentation models with varying quality.

### Open Question 3
- Question: How does proposed method scale to extremely large environments or urban-scale mapping scenarios with millions of objects?
- Basis in paper: [explicit] Paper emphasizes compact representation (1.33 kB per scan) and mentions potential for scalable mapping, but doesn't evaluate performance on extremely large environments or discuss computational considerations for scaling up.
- Why unresolved: While method designed for compactness, computational efficiency and memory requirements for handling millions of objects in large-scale mapping scenarios are not addressed.
- What evidence would resolve it: Testing method on datasets simulating urban-scale environments with millions of objects and reporting computational time, memory usage, and registration accuracy metrics.

## Limitations
- Limited validation on non-KITTI datasets raises questions about generalizability to different environmental conditions and semantic distributions
- Does not adequately address handling of dynamic objects or occlusions that could significantly affect object centroid stability
- Computational requirements for attention mechanisms and RANSAC may limit real-time applicability in large-scale scenarios

## Confidence

**High confidence**: Claims about storage efficiency and compression ratios are well-supported by quantitative measurements and straightforward calculations.

**Medium confidence**: Claims about registration accuracy (0.1 m translational error, 0.5° rotational error) are supported by KITTI experiments but lack cross-dataset validation to establish generalizability.

**Low confidence**: Claims about robustness to long-term changes and different environmental conditions are based on limited experiments and require more extensive validation across diverse scenarios.

## Next Checks

1. **Cross-dataset validation**: Test the method on non-KITTI datasets (e.g., NuScenes, SemanticKITTI) with different environmental conditions and semantic distributions to assess generalizability and identify potential failure modes.

2. **Dynamic object handling evaluation**: Create controlled experiments with varying levels of dynamic objects (vehicles, pedestrians) to quantify how object centroid instability affects registration accuracy and identify thresholds for reliable operation.

3. **Storage-accuracy tradeoff analysis**: Systematically vary the compression level (number of objects retained, clustering parameters) to map the full accuracy-storage tradeoff curve and identify optimal operating points for different application scenarios.