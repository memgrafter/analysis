---
ver: rpa2
title: Extending Explainable Ensemble Trees (E2Tree) to regression contexts
arxiv_id: '2409.06439'
source_url: https://arxiv.org/abs/2409.06439
tags:
- displacement
- e2tree
- modelyear
- terminal
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: E2Tree was originally proposed to explain random forests for classification
  tasks. This paper extends E2Tree to regression contexts by defining a local goodness-of-fit
  measure based on normalized mean squared error.
---

# Extending Explainable Ensemble Trees (E2Tree) to regression contexts

## Quick Facts
- arXiv ID: 2409.06439
- Source URL: https://arxiv.org/abs/2409.06439
- Reference count: 5
- Primary result: E2Tree extends from classification to regression using NMSE-based local fit measures, achieving ~90% FMI on Iris and ~75% on Auto MPG

## Executive Summary
This paper extends the E2Tree method, originally developed for explaining random forest classification models, to regression contexts. The key innovation is using a normalized mean squared error (NMSE)-based local goodness-of-fit measure to weight dissimilarity matrices derived from observation co-occurrence in random forest nodes. The method successfully reconstructs the structure of random forest models while providing interpretable tree-like visualizations that explain predictor-response relationships and interactions. Validation on two real-world datasets demonstrates the approach maintains random forest predictive accuracy while enabling transparent explanations.

## Method Summary
E2Tree extends to regression by defining a local goodness-of-fit measure based on normalized mean squared error (NMSE). The algorithm creates a dissimilarity matrix D = 1 - Oij where Oij measures how often pairs of observations co-occur in the same RF node, weighted by W(t)ij|b (normalized local fit). Nodes become terminal if NMSE(t)ij|b ≤ γ (default 0.05) or if child nodes have identical predicted value distributions per Mann-Whitney test. The method is implemented in the R package e2tree, which generates interpretable tree structures that approximate RF model behavior while maintaining predictive accuracy.

## Key Results
- E2Tree successfully reconstructs RF model structure with Fowlkes-Mallows Index values of approximately 90% for Iris dataset
- Auto MPG dataset achieves FMI of approximately 75%, demonstrating method effectiveness on larger, more complex data
- The approach provides interpretable visualizations explaining predictor-response relationships and interactions while maintaining RF predictive accuracy
- Implementation available as R package e2tree on GitHub, enabling practical application

## Why This Works (Mechanism)

### Mechanism 1
- Claim: E2Tree reconstructs RF structure by weighting co-occurrence matrices with local goodness-of-fit measures
- Mechanism: The algorithm creates a dissimilarity matrix D = 1 - Oij where Oij is a weighted co-occurrence measure between observations ui and uj along the sequence H1, ..., HB
- Core assumption: The weighted co-occurrence matrix preserves enough information about the RF decision boundaries to reconstruct them in a single tree
- Evidence anchors:
  - [abstract] "The method uses dissimilarity matrices derived from co-occurrence of observations in random forest nodes, weighted by the local fit measure"
  - [section] "E2Tree starts from the definition of a dissimilarity matrix: D = 1 - Oij where Oij is a weighted co-occurrence measure between observations ui and uj along the sequence H1, ..., HB"
- Break condition: If the weighted co-occurrence matrix loses critical structural information about the RF, the reconstructed tree will poorly approximate the original RF

### Mechanism 2
- Claim: Normalized Mean Squared Error provides effective local goodness-of-fit measure for regression
- Mechanism: W(t)ij|b = 1 - NMSE(t)ij|b where NMSE is normalized by MSEmax × p(t), ensuring values between 0 and 1 that reflect node prediction quality
- Core assumption: NMSE provides meaningful discrimination between node quality that correlates with predictive importance
- Evidence anchors:
  - [section] "In the regression context, we propose the following local goodness of fit measure: W(t)ij|b = 1 - NMSE(t)ij|b where NMSE(t)ij|b = MSE(t)ij|b / (MSEmax × p(t))"
  - [abstract] "This paper extends E2Tree to regression contexts by defining a local goodness-of-fit measure based on normalized mean squared error"
- Break condition: If NMSE fails to discriminate between high and low quality nodes, the weighting will not prioritize important splits

### Mechanism 3
- Claim: Stopping criteria based on NMSE threshold and Mann-Whitney test prevent overfitting and non-informative splits
- Mechanism: Nodes become terminal if NMSE(t)ij|b ≤ γ (default 0.05) or if child nodes have identical predicted value distributions per Mann-Whitney test
- Core assumption: These stopping rules effectively prune non-informative splits while preserving meaningful structure
- Evidence anchors:
  - [section] "we verify that the value of NMSE(t)ij|b meets a specified threshold γ. The node t becomes terminal if NMSE(t)ij|b ≤ γ. Furthermore, for each split, the distributions between the child nodes are verified to be identical through the Mann-Whitney test"
  - [abstract] "The approach provides interpretable, tree-like visualizations that explain predictor-response relationships and interactions while maintaining RF predictive accuracy"
- Break condition: If stopping rules are too aggressive, important splits may be missed; if too lenient, overfitting occurs

## Foundational Learning

- Concept: Random Forest mechanics (bagging, feature subsampling, tree aggregation)
  - Why needed here: E2Tree operates on RF output, so understanding how RF builds trees and aggregates predictions is essential
  - Quick check question: What determines the prediction at a terminal node in an RF regression model?

- Concept: Dissimilarity and co-occurrence matrices in clustering contexts
  - Why needed here: E2Tree's core mechanism uses dissimilarity matrices derived from observation co-occurrence in RF nodes
  - Quick check question: How does a dissimilarity matrix differ from a similarity matrix, and why use 1 - Oij?

- Concept: Normalized error metrics and their interpretation
  - Why needed here: The local fit measure uses normalized MSE, requiring understanding of error normalization and bounds
  - Quick check question: Why normalize MSE by MSEmax × p(t) rather than using raw MSE?

## Architecture Onboarding

- Component map: RF model → dissimilarity matrix generation → E2Tree tree building → visualization → evaluation (FMI comparison)
- Critical path: Train RF → Compute Oij matrix → Compute D matrix → Apply E2Tree algorithm with stopping rules → Generate interpretable tree
- Design tradeoffs: Complexity vs interpretability (single tree vs full RF), accuracy preservation vs simplification, computational cost vs explanatory power
- Failure signatures: Low FMI values (poor reconstruction), high NMSE in terminal nodes (poor fit), identical child node distributions (non-informative splits)
- First 3 experiments:
  1. Run E2Tree on Iris dataset with varying γ thresholds (0.01, 0.05, 0.1) and observe FMI changes
  2. Compare E2Tree reconstruction with different local fit measures (NMSE vs coefficient of variation approach)
  3. Test E2Tree on synthetic data with known interactions to verify interaction detection capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is the E2Tree method to the choice of the threshold γ for NMSE, and what would be an optimal way to select this parameter?
- Basis in paper: [explicit] The paper mentions setting γ = 0.05 but doesn't explore sensitivity to this choice or provide guidance on optimal selection
- Why unresolved: The stopping criteria are critical to E2Tree's performance, yet the paper doesn't examine how different γ values affect reconstruction accuracy or interpretability
- What evidence would resolve it: Systematic experiments varying γ across a range of values (e.g., 0.01 to 0.1) on multiple datasets, showing FMI scores and interpretability trade-offs

### Open Question 2
- Question: Can E2Tree be extended to other ensemble methods beyond random forests, such as gradient boosting machines or extreme gradient boosting?
- Basis in paper: [inferred] The conclusion mentions generalizing the approach to "other ensemble-tree methodologies" but provides no implementation details or validation
- Why unresolved: While the paper extends E2Tree from classification to regression, it doesn't demonstrate whether the dissimilarity matrix approach generalizes to other ensemble algorithms with different training procedures
- What evidence would resolve it: Implementation and validation of E2Tree for at least one other ensemble method (e.g., XGBoost), with comparative FMI scores and visualizations

### Open Question 3
- Question: How does E2Tree's interpretability compare to other post-hoc explanation methods for regression, such as SHAP values or partial dependence plots?
- Basis in paper: [inferred] The paper doesn't compare E2Tree's explanations to established methods, despite discussing the importance of interpretability
- Why unresolved: The paper demonstrates E2Tree's ability to reconstruct RF structure but doesn't benchmark its explanations against alternative methods that users might be familiar with
- What evidence would resolve it: User studies or quantitative comparisons showing E2Tree's explanations are more accurate, more intuitive, or lead to better decision-making than SHAP, PDPs, or other established methods

## Limitations

- The method's performance evaluation is limited to only two datasets (Iris and Auto MPG), providing insufficient evidence for broader applicability
- The NMSE-based local goodness-of-fit measure assumes normalized error metrics adequately capture node importance across diverse regression problems
- The stopping criteria using fixed thresholds (γ=0.05) and Mann-Whitney tests may not be optimal across different datasets with varying scales and distributions

## Confidence

- **High Confidence**: The core mechanism of using weighted co-occurrence matrices to reconstruct RF structure is well-defined and theoretically sound
- **Medium Confidence**: The NMSE normalization approach for local fit measurement is reasonable but lacks extensive validation across diverse regression contexts
- **Low Confidence**: The stopping criteria effectiveness and optimal threshold selection remain inadequately validated

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary the NMSE threshold γ across multiple orders of magnitude (0.001, 0.01, 0.05, 0.1, 0.2) and measure FMI stability to determine optimal threshold selection criteria

2. **Cross-Dataset Performance Evaluation**: Apply E2Tree to at least 10 diverse regression datasets spanning different feature types, sample sizes, and noise levels to establish generalizability patterns and identify failure conditions

3. **Alternative Local Fit Measures**: Compare E2Tree performance using NMSE against alternative local fit measures such as coefficient of variation, AIC/BIC criteria, or out-of-bag prediction error to validate the choice of goodness-of-fit metric