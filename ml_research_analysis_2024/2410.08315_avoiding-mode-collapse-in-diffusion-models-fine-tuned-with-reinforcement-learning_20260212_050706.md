---
ver: rpa2
title: Avoiding mode collapse in diffusion models fine-tuned with reinforcement learning
arxiv_id: '2410.08315'
source_url: https://arxiv.org/abs/2410.08315
tags:
- diffusion
- learning
- samples
- sample
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of mode collapse in diffusion models
  (DMs) fine-tuned with reinforcement learning (RL). While RL improves alignment with
  downstream objectives, it often sacrifices sample diversity, leading to mode collapse.
---

# Avoiding mode collapse in diffusion models fine-tuned with reinforcement learning

## Quick Facts
- arXiv ID: 2410.08315
- Source URL: https://arxiv.org/abs/2410.08315
- Reference count: 18
- RL fine-tuning often causes mode collapse in diffusion models, sacrificing diversity for alignment with downstream objectives

## Executive Summary
This paper addresses mode collapse in diffusion models fine-tuned with reinforcement learning by exploiting the hierarchical nature of denoising steps. The authors propose Hierarchical Reward Fine-tuning (HRF), which dynamically trains models at different timesteps using a sliding-window approach. This method preserves sample diversity while maintaining alignment with downstream tasks like aesthetic quality, compressibility, and incompressibility. Results show improved diversity metrics (Inception Score, Vendi Score) without sacrificing reward achievement compared to standard DDPO.

## Method Summary
The method trains diffusion models dynamically at each epoch with a sliding-window approach, allowing step-by-step refinement. Two variants are proposed: HRF with predefined windows and HRF-D with dynamic window selection. The approach identifies optimal training steps by maximizing reward variation while minimizing divergence from the image prior. It incorporates diversity-promoting terms by considering mean distances between samples and the prior at each step. The method is validated on DDPO, showing better diversity preservation while maintaining similar mean rewards.

## Key Results
- HRF achieves better preservation of diversity in downstream tasks while maintaining similar mean rewards compared to DDPO
- Improved diversity metrics (Inception Score and Vendi Score) demonstrate successful preservation without compromising alignment
- Sliding window approach allows for continual evaluation and refinement at different denoising stages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Early denoising steps dominate the influence on final sample diversity in diffusion models.
- **Mechanism**: Diffusion models exhibit a hierarchical generation process where high-level features are more susceptible to temporal changes in the early diffusion steps, while low-level features remain relatively stable across the diffusion. This means that training interventions in early steps disproportionately affect the final output's diversity characteristics.
- **Core assumption**: The hierarchical nature of diffusion models means that different denoising steps control different levels of abstraction in the generated output, with early steps controlling high-level structure and later steps controlling fine details.
- **Evidence anchors**:
  - [abstract] "We find that not every denoising step needs to be fine-tuned to align DMs to downstream tasks"
  - [section] "learning in DDPM models is more effective during the noisy stages of diffusion and becomes increasingly difficult as noise is reduced"
  - [corpus] Weak evidence - corpus neighbors discuss diffusion alignment but don't directly address the hierarchical step-specific influence on diversity
- **Break condition**: If the hierarchical generation assumption is incorrect and all denoising steps contribute equally to feature learning, this mechanism would fail to explain the observed diversity preservation.

### Mechanism 2
- **Claim**: Selective fine-tuning at specific denoising steps can preserve diversity while maintaining alignment with downstream objectives.
- **Mechanism**: By focusing RL fine-tuning on intermediate timesteps rather than all timesteps, the model can learn task-specific improvements while preserving the diversity encoded in the pre-trained model's later denoising steps. This creates a controlled learning scheme that mitigates mode collapse.
- **Core assumption**: The pre-trained diffusion model already encodes sufficient diversity in its later denoising steps, which can be preserved by not fine-tuning those steps.
- **Evidence anchors**:
  - [abstract] "we train them dynamically at each epoch with a tailored RL method, allowing for continual evaluation and step-by-step refinement"
  - [section] "not every denoising step needs to be fine-tuned to align DMs to downstream tasks"
  - [corpus] Moderate evidence - corpus includes "Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance" which suggests selective intervention strategies
- **Break condition**: If downstream task alignment requires modification of all denoising steps, or if diversity is primarily encoded in early steps rather than later ones, this selective approach would fail.

### Mechanism 3
- **Claim**: Dynamic window selection based on reward variation and divergence from image prior optimizes the balance between alignment and diversity.
- **Mechanism**: The sliding window approach identifies optimal training steps by maximizing reward variation while minimizing divergence from the image prior, effectively finding steps where learning provides the most benefit with the least diversity loss.
- **Core assumption**: There exists an optimal set of denoising steps where the trade-off between reward maximization and diversity preservation is most favorable.
- **Evidence anchors**:
  - [section] "we identify the step t that optimizes the sampling trajectory by finding the policy that maximizes reward variation while minimizing divergence from the image prior"
  - [section] "we incorporate a diversity-promoting term by considering the mean distance between samples (at each step) and the prior"
  - [corpus] Weak evidence - corpus neighbors don't directly address dynamic window selection strategies
- **Break condition**: If reward variation and diversity preservation are inherently antagonistic across all steps, or if the divergence metric doesn't correlate with perceptual diversity, this optimization would fail.

## Foundational Learning

- **Concept**: Markov Decision Process formulation of diffusion models
  - **Why needed here**: The paper treats diffusion denoising as sequential decision-making where each step is an action, requiring understanding of MDPs to grasp the RL framework
  - **Quick check question**: In the MDP formulation, what represents the state, action, and reward when treating diffusion denoising as sequential decision-making?

- **Concept**: Hierarchical feature learning in diffusion models
  - **Why needed here**: The paper exploits the finding that different denoising steps control different levels of abstraction (high-level vs low-level features), which is fundamental to the sliding window approach
  - **Quick check question**: According to the hierarchical interpretation, which denoising steps primarily control high-level features versus low-level features?

- **Concept**: Reinforcement learning with importance sampling
  - **Why needed here**: The paper uses DDPO with importance sampling for policy gradient estimation, which is crucial for understanding how gradients are computed in the proposed method
  - **Quick check question**: How does importance sampling help address the mismatch between behavior policy and target policy in the RL fine-tuning process?

## Architecture Onboarding

- **Component map**: Pre-trained diffusion model (DDPM) -> RL fine-tuning module with sliding window selection -> Reward function modules -> Diversity preservation metrics -> Dynamic window selection algorithm

- **Critical path**: 
  1. Load pre-trained diffusion model
  2. Initialize RL fine-tuning with window selection mechanism
  3. For each epoch, select window based on predefined or dynamic strategy
  4. Generate trajectories from selected starting steps
  5. Compute rewards and gradients via importance sampling
  6. Update model parameters with trust region clipping
  7. Evaluate diversity preservation and reward achievement
  8. Early stopping based on stability criteria

- **Design tradeoffs**:
  - Selective vs full-step fine-tuning: Selective preserves diversity but may miss task-relevant features in non-selected steps
  - Predefined vs dynamic windows: Predefined is simpler but dynamic can adapt to task-specific needs
  - Reward-only vs diversity-regularized training: Pure reward optimization risks mode collapse, diversity regularization may slow convergence

- **Failure signatures**:
  - Mode collapse despite window selection (diversity metrics drop significantly)
  - No improvement in downstream rewards (alignment fails)
  - Training instability or divergence (trust region clipping may be too aggressive)
  - Inconsistent performance across seeds (hyperparameter sensitivity)

- **First 3 experiments**:
  1. Implement basic DDPO fine-tuning on a simple downstream task and verify mode collapse occurs
  2. Add static sliding window selection (predefined windows) and measure improvement in diversity preservation
  3. Implement dynamic window selection (HRF-D) and compare against static windows on the same task

## Open Questions the Paper Calls Out
None

## Limitations
- Method requires careful tuning of window selection parameters and may be sensitive to hyperparameter choices
- Diversity preservation relies on assumption that pre-trained models encode sufficient diversity in later denoising steps
- Approach adds computational overhead through sliding window mechanism and additional diversity regularization terms

## Confidence
- **High confidence**: Core claim that selective fine-tuning at specific denoising steps can preserve diversity while maintaining alignment with downstream objectives
- **Medium confidence**: Specific window selection strategy and dynamic optimization approach
- **Low confidence**: Generalizability across different diffusion model architectures and downstream tasks

## Next Checks
1. Conduct ablation studies testing different window sizes and selection strategies to verify robustness of sliding window approach
2. Test method on additional diffusion model architectures and diverse downstream tasks beyond aesthetic quality and compression
3. Perform long-term stability analysis by evaluating diversity metrics over extended training periods and across multiple random seeds