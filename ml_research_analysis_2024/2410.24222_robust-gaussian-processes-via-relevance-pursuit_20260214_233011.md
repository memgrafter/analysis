---
ver: rpa2
title: Robust Gaussian Processes via Relevance Pursuit
arxiv_id: '2410.24222'
source_url: https://arxiv.org/abs/2410.24222
tags:
- data
- outliers
- likelihood
- student-t
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Robust Gaussian Processes via Relevance Pursuit (RRP) addresses
  the challenge of performing Gaussian process regression in the presence of sparse
  label corruptions that deviate from standard Gaussian noise assumptions. The method
  introduces data-point-specific noise variances and employs a greedy sequential selection
  procedure called relevance pursuit to maximize the marginal log-likelihood.
---

# Robust Gaussian Processes via Relevance Pursuit

## Quick Facts
- arXiv ID: 2410.24222
- Source URL: https://arxiv.org/abs/2410.24222
- Authors: Sebastian Ament, Elizabeth Santorella, David Eriksson, Ben Letham, Maximilian Balandat, Eytan Bakshy
- Reference count: 40
- Key outcome: Robust Gaussian Processes via Relevance Pursuit (RRP) addresses the challenge of performing Gaussian process regression in the presence of sparse label corruptions that deviate from standard Gaussian noise assumptions

## Executive Summary
This paper introduces Relevance Pursuit (RRP), a method for robust Gaussian process regression that handles sparse label corruptions without requiring prior knowledge of outlier locations or fractions. The approach introduces data-point-specific noise variances and employs a greedy sequential selection procedure to maximize the marginal log-likelihood. Under a particular parameterization, the marginal log-likelihood becomes strongly concave in the noise variances, leading to approximation guarantees for the greedy algorithm. Empirically, RRP outperforms alternative methods across various regression and Bayesian optimization tasks.

## Method Summary
The method introduces data-point-specific noise variances ρi for each data point and employs a greedy sequential selection procedure called relevance pursuit to maximize the marginal log-likelihood. The key innovation is a convex parameterization ρ(s) = diag(K0) ⊙ ((1-s)^-1 - 1) that enables strong convexity guarantees. The algorithm iteratively optimizes these variances and expands the support of identified outliers based on leave-one-out error criteria. A Bayesian model selection framework automatically determines the optimal number of outliers to include.

## Key Results
- RRP outperforms standard GPs and other robust approaches across multiple regression benchmarks
- The method demonstrates superior robustness to sparse label corruptions, including outliers within the function's range
- Theoretical analysis proves approximation guarantees for the greedy algorithm through weak submodularity
- RRP maintains computational efficiency while providing strong empirical performance

## Why This Works (Mechanism)

### Mechanism 1
Data-point-specific noise variances adaptively identify and down-weight corrupted data points. The method introduces learnable ρi for each data point. When the squared leave-one-out error exceeds the leave-one-out predictive variance, ρi increases, effectively reducing the influence of that data point on the GP model. This mechanism relies on the assumption that outliers are sparse relative to total data points.

### Mechanism 2
The convex parameterization ρ(s) = diag(K0) ⊙ ((1-s)^-1 - 1) enables strong convexity guarantees. This reparameterization maps the optimization problem to a compact domain [0,1]^n where the marginal log-likelihood becomes strongly convex under certain eigenvalue conditions. The mechanism assumes the base covariance matrix K0 has specific spectral properties (diagonal dominance or well-conditioned eigenvalues).

### Mechanism 3
Relevance Pursuit provides approximation guarantees through weak submodularity. The greedy sequential selection algorithm maximizes the marginal log-likelihood while maintaining monotonic improvement, which combined with strong convexity, implies weak submodularity and thus constant-factor approximation guarantees. This mechanism assumes the objective function satisfies restricted strong convexity and smoothness properties.

## Foundational Learning

- **Gaussian Process regression fundamentals**: Understanding mean functions, kernel functions, and covariance matrices is essential as the entire method builds on the GP regression framework. Quick check: What is the relationship between the kernel matrix K and the covariance matrix in GP regression?

- **Marginal likelihood optimization**: Understanding when this optimization is convex/concave is crucial for the method's theoretical guarantees. Quick check: Why is the standard GP marginal likelihood not generally convex in its hyperparameters?

- **Sparse optimization and greedy algorithms**: Knowledge of when greedy algorithms have approximation guarantees is necessary for understanding the method's theoretical properties. Quick check: What is the difference between strong convexity and restricted strong convexity, and why does it matter for greedy algorithms?

## Architecture Onboarding

- **Component map**: Data input → GP kernel computation → Covariance matrix construction → Robust variance optimization → Model selection → Prediction output
  Key components: Kernel function, relevance pursuit algorithm, Bayesian model selection, convex parameterization

- **Critical path**:
  1. Compute base covariance matrix K0
  2. Initialize relevance pursuit with empty outlier set
  3. Iteratively optimize ρ for current support and expand support based on LOO error criteria
  4. Apply Bayesian model selection to choose optimal outlier set
  5. Return final GP model with identified outliers down-weighted

- **Design tradeoffs**:
  - Sparse vs. dense outlier detection: Designed for sparse outliers but can handle denser cases with backward variant
  - Computational cost vs. accuracy: Convex parameterization improves optimization but requires additional computation
  - Model flexibility vs. theoretical guarantees: Joint optimization of all hyperparameters improves performance but loses some convexity guarantees

- **Failure signatures**:
  - Poor performance on non-sparse outliers: Method assumes outliers are sparse relative to total data
  - Numerical instability: If K0 is highly ill-conditioned, the convex parameterization may not work well
  - Slow convergence: If the eigenvalue conditions for convexity are not met, optimization may be slow

- **First 3 experiments**:
  1. Simple 1D regression with known outliers: Generate data from a simple function with 10% constant outliers, compare RRP to standard GP
  2. Synthetic regression with different outlier types: Test uniform noise outliers vs. constant outliers vs. heavy-tailed noise
  3. Real-world regression benchmark: Apply to a UCI dataset with simulated outliers and compare predictive performance metrics

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Relevance Pursuit scale with increasing data set size, particularly when transitioning from the small-data regime to large-scale problems? The paper emphasizes the small-data regime and mentions "scalable" extensions but doesn't provide empirical results on large data sets.

### Open Question 2
Can the theoretical approximation guarantees be extended to the setting where all hyper-parameters (not just noise variances) are optimized jointly? The authors acknowledge this limitation, stating "A limitation of the theory is that it assumes the other hyper-parameters of the GP model to be constant."

### Open Question 3
How sensitive is the Bayesian model selection variant of RRP to the choice of prior distribution over the number of outliers? The authors mention using an exponential prior but note "In practice, p(S) can be informed by empirical distributions of outliers."

## Limitations
- The method's effectiveness diminishes when outlier fractions exceed approximately 50% of the data
- Theoretical guarantees depend on specific spectral properties of the base covariance matrix K0
- The convex parameterization may introduce computational overhead compared to simpler approaches

## Confidence
- **High confidence**: Empirical performance improvements over standard GPs and alternative robust methods across multiple regression and Bayesian optimization tasks
- **Medium confidence**: Theoretical guarantees for the greedy algorithm's approximation bounds, as these depend on specific conditions that may not always be satisfied in practice
- **Medium confidence**: The claim that the method works effectively with sparse outliers within the function's range, as this is demonstrated empirically but with limited theoretical backing

## Next Checks
1. **Robustness to non-sparse outliers**: Systematically evaluate RRP's performance when the outlier fraction increases from 10% to 80% to verify the claimed breakdown point and understand performance degradation patterns

2. **Kernel sensitivity analysis**: Test RRP with different kernel functions (e.g., Matérn, periodic, neural network kernels) to assess whether the theoretical convexity guarantees hold across various covariance structures

3. **Computational complexity benchmarking**: Compare the wall-clock time and memory requirements of RRP against baseline methods across datasets of increasing size to quantify the practical computational overhead of the convex parameterization approach