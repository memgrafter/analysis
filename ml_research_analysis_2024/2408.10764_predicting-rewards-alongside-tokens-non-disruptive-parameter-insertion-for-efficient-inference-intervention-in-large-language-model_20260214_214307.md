---
ver: rpa2
title: 'Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion for
  Efficient Inference Intervention in Large Language Model'
arxiv_id: '2408.10764'
source_url: https://arxiv.org/abs/2408.10764
tags:
- otter
- original
- inference
- decoding
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Otter is a parameter insertion method that adds trainable parameters
  to transformer models for efficient inference intervention. By inserting parameters
  into FFN and MHA layers, Otter enables simultaneous prediction of calibration signals
  alongside original outputs.
---

# Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion for Efficient Inference Intervention in Large Language Model

## Quick Facts
- **arXiv ID:** 2408.10764
- **Source URL:** https://arxiv.org/abs/2408.10764
- **Reference count:** 26
- **Primary result:** Otter achieves state-of-the-art performance on three tasks with up to 86.5% space and 98.5% time savings.

## Executive Summary
Otter is a parameter insertion method that adds trainable parameters to transformer models for efficient inference intervention. By inserting parameters into FFN and MHA layers, Otter enables simultaneous prediction of calibration signals alongside original outputs. The method achieves significant computational savings while maintaining compatibility with existing inference engines through a simple LayerNorm modification.

## Method Summary
Otter works by inserting extra trainable parameters into the feed-forward network (FFN) and multi-head attention (MHA) layers of transformer architectures. These parameters expand the hidden state dimensions, allowing a separate projection to produce calibration signals while preserving the original model's output. The method modifies LayerNorm to use only the original hidden states, ensuring non-disruptive behavior. Otter requires minimal code changes for integration and achieves substantial space and time savings compared to existing approaches.

## Key Results
- Achieves state-of-the-art performance on reducing harmful responses, aligning with human preferences, and accelerating inference
- Reduces space overhead by up to 86.5% and time overhead by up to 98.5% compared to existing methods
- Maintains original model outputs while adding calibration signal prediction capabilities
- Seamlessly integrates with existing inference engines requiring only a one-line code change

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Otter enables simultaneous output of calibration signals and original LLM output by inserting trainable parameters into FFN and MHA layers without altering the original model's forward pass behavior.
- **Mechanism:** Otter expands the hidden state dimensions in each transformer block and appends new trainable parameters. The original hidden state remains intact, while the extended hidden state allows a separate projection to produce the calibration signal.
- **Core assumption:** The original LLM's hidden states can be concatenated with new trainable hidden states without disrupting the original computation path, and the LayerNorm can be adapted to use only the original hidden states for normalization.
- **Evidence anchors:** [abstract] "Otter, inserting extra parameters into the transformer architecture to predict calibration signals along with the original LLM output" [section] "Our method involves inserting new trainable parameters into the feed-forward network (FFN) and multi-head attention (MHA) layers of the transformer architecture"

### Mechanism 2
- **Claim:** Otter reduces computational overhead by reusing the original model's parameters instead of training separate calibration models, achieving up to 86.5% space and 98.5% time savings.
- **Mechanism:** By integrating the calibration model's parameters directly into the LLM's architecture, Otter eliminates the need for separate forward passes through an additional model during inference, thus reducing both memory and compute requirements.
- **Core assumption:** The original LLM's parameters contain sufficient knowledge to be leveraged for calibration tasks, and the added parameters can be trained efficiently without significant computational cost.
- **Evidence anchors:** [abstract] "saving up to 86.5% extra space and 98.5% extra time" [section] "Compared with existing work, this study offers several contributions: SOTA performance with lowest overhead"

### Mechanism 3
- **Claim:** Otter's parameter insertion method allows seamless integration with existing inference engines with minimal code changes.
- **Mechanism:** Otter's design preserves the original transformer architecture and only requires a single-line modification to the LayerNorm computation, enabling compatibility with existing inference pipelines and optimizations like FlashAttention and PagedAttention.
- **Core assumption:** The original transformer code can be modified in a way that preserves backward compatibility and does not interfere with existing optimizations or hardware-specific implementations.
- **Evidence anchors:** [abstract] "Otter seamlessly integrates with existing inference engines, requiring only a one-line code change" [section] "Note that, this adaption only extends the size of hidden states and the number of attention heads, where no code modification is required"

## Foundational Learning

- **Concept:** LayerNorm regularization in transformer models
  - **Why needed here:** Otter modifies the LayerNorm computation to use only the original hidden states, preventing disruption to the original model's behavior while allowing the extended hidden states to be used for calibration.
  - **Quick check question:** How does LayerNorm compute the normalization statistics, and why would using the extended hidden state potentially disrupt the original model's behavior?

- **Concept:** Multi-head attention (MHA) mechanism in transformers
  - **Why needed here:** Otter inserts additional attention heads into the MHA layer, expanding the model's capacity to process information while maintaining the original attention heads' functionality.
  - **Quick check question:** What is the role of the value, key, and query projections in MHA, and how does adding new attention heads affect the overall attention computation?

- **Concept:** Parameter-efficient fine-tuning (PEFT) methods like LoRA
  - **Why needed here:** Otter is conceptually related to PEFT methods but differs in that it preserves the original model's output while adding new capabilities, rather than modifying the output.
  - **Quick check question:** How do LoRA and other PEFT methods modify the original model's parameters, and what are the key differences between their approach and Otter's approach?

## Architecture Onboarding

- **Component map:** Input token → Embedding layer → Transformer blocks (FFN with extended dimensions, MHA with extended heads) → LayerNorm (modified) → Extended hidden state projection to calibration signal → Original output via language modeling head

- **Critical path:** 1. Input token → embedding layer 2. For each transformer block: - Multi-head attention (with extended heads) - Add & Norm (modified to use only original hidden states) - Feed-forward network (with extended dimensions) - Add & Norm (modified to use only original hidden states) 3. Extended hidden state projection to calibration signal 4. Original output via language modeling head

- **Design tradeoffs:** Otter vs. separate calibration models: Otter reduces space and time overhead but may have slightly lower performance due to shared parameters. FFN vs. MHA parameter insertion: FFN insertion may be more parameter-efficient, while MHA insertion may yield better performance for certain tasks. LayerNorm modification: Required for non-disruptive behavior but may introduce compatibility issues with certain hardware or optimizations.

- **Failure signatures:** Original model output degradation: Indicates LayerNorm modification is not working as intended. Training instability: May indicate poor initialization or learning rate settings for the inserted parameters. Calibration signal quality degradation: May indicate insufficient capacity in the inserted parameters or poor integration with the original model.

- **First 3 experiments:** 1. Verify LayerNorm modification: Compare original model output with and without Otter parameters frozen to ensure non-disruptive behavior. 2. Ablation study: Compare Otter with Task Head Only baseline to isolate the impact of parameter insertion vs. simple addition. 3. Scalability test: Measure space and time overhead as the number of inserted parameters increases to identify optimal insertion strategy.

## Open Questions the Paper Calls Out
- How does Otter perform on tasks beyond the three evaluated (human preference alignment, detoxification, and inference acceleration)?
- What is the optimal number and placement of Otter parameters within the transformer architecture for different tasks?
- How does Otter affect the interpretability and explainability of LLM outputs?
- What are the long-term effects of using Otter on model performance and potential catastrophic forgetting?

## Limitations
- The exact implementation details of the modified RMSNorm function and its integration with existing inference engines are not fully specified
- Training procedure and hyperparameter choices are provided, but initialization methods for inserted parameters are not explicitly stated
- Claims about state-of-the-art performance rely on comparisons with specific baselines whose details are not provided

## Confidence
- **High confidence:** The core concept of parameter insertion into FFN and MHA layers is well-established in the literature, and the claimed space and time savings (86.5% and 98.5%) are plausible given the described architecture modifications.
- **Medium confidence:** The claims about seamless integration with existing inference engines and the preservation of original model outputs are supported by the described LayerNorm modification, but the lack of implementation details makes it difficult to fully verify these claims.
- **Low confidence:** The paper's claims about state-of-the-art performance on the three tasks (reducing harmful responses, aligning with human preferences, and accelerating inference) are based on comparisons with specific baselines (ARGS, Task Head Only), but the details of these baselines and the exact evaluation procedures are not provided, making it challenging to assess the validity of these claims.

## Next Checks
1. Implement and test the LayerNorm modification: Carefully implement the modified RMSNorm function and verify that it correctly preserves the original model's behavior while allowing the extended hidden states to be used for calibration.
2. Reproduce the baseline results: Implement the ARGS and Task Head Only baselines and compare their performance with Otter on the specified tasks and datasets to validate the claimed improvements.
3. Analyze the impact of initialization methods: Experiment with different initialization strategies for the inserted parameters (random, normal, parameter copying) and assess their impact on training stability and final performance.