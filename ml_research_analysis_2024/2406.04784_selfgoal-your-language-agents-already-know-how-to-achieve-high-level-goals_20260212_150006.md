---
ver: rpa2
title: 'SelfGoal: Your Language Agents Already Know How to Achieve High-level Goals'
arxiv_id: '2406.04784'
source_url: https://arxiv.org/abs/2406.04784
tags:
- goal
- self
- agents
- your
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SELF GOAL introduces a self-adaptive framework for language agents
  to achieve high-level goals by dynamically constructing and utilizing a hierarchical
  GOALTREE of contextual subgoals. The core idea is to adaptively break down goals
  into more practical subgoals during interaction with environments, enabling agents
  to identify the most useful subgoals and progressively update the guidance structure.
---

# SelfGoal: Your Language Agents Already Know How to Achieve High-level Goals

## Quick Facts
- arXiv ID: 2406.04784
- Source URL: https://arxiv.org/abs/2406.04784
- Reference count: 40
- SelfGoal improves language agents' ability to achieve high-level goals by dynamically constructing and utilizing a hierarchical GOALTREE of contextual subgoals.

## Executive Summary
SelfGoal introduces a self-adaptive framework for language agents to achieve high-level goals in dynamic environments without detailed instructions or frequent retraining. The framework dynamically constructs and utilizes a hierarchical GOALTREE of contextual subgoals during task execution, enabling agents to adaptively break down goals into more practical subgoals based on environmental feedback. Experimental results demonstrate significant performance improvements across competitive, cooperative, and deferred feedback environments, with gains such as increasing TrueSkill Score from 23.96 to 29.59 in first-price auctions and reducing profit discrepancy from 2.57 to 1.88 in bargaining tasks.

## Method Summary
SelfGoal is a framework that enhances language agents' ability to achieve high-level goals in dynamic environments through three key modules: Search, Decomposition, and Act. The Search Module selects the top-K most suited subgoals from a hierarchical GOALTREE based on the current state, while the Decomposition Module breaks down selected goals into more concrete subgoals, expanding the tree structure. The Act Module uses these selected subgoals as guidelines to prompt LLMs for actions in the current state. The framework employs granularity control through a threshold (ξ) to prevent redundant subgoals and uses a stopping mechanism to halt updates when no new nodes are added for N consecutive rounds. Experiments compare SelfGoal against baseline methods across four scenarios: Public Goods Game, Guess 2/3 of the Average, First-price Auction, and Bargaining.

## Key Results
- SelfGoal increased TrueSkill Score from 23.96 to 29.59 in first-price auctions
- SelfGoal reduced profit discrepancy from 2.57 to 1.88 in bargaining tasks
- SelfGoal demonstrated faster convergence to optimal strategies in repeated games

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic subgoal generation and selection during task execution enables agents to adapt to environmental feedback and avoid rigid pre-planned strategies.
- Mechanism: The framework uses a Search Module to select the most suited subgoals from the current GOALTREE based on the current state, followed by a Decompose Module that breaks down selected goals into more concrete subgoals. This creates a feedback loop where the tree structure is updated as the agent interacts with the environment.
- Core assumption: LLMs have sufficient prior knowledge to generate relevant subgoals and can use context to select the most useful ones.
- Evidence anchors:
  - [abstract] "The core concept of SELF GOAL involves adaptively breaking down a high-level goal into a tree structure of more practical subgoals during the interaction with environments while identifying the most useful subgoals and progressively updating this structure."
  - [section] "Specifically, as shown in Figure 1, SELF GOAL is featured with two main modules to operate a GOALTREE, which is constructed, updated, and utilized during task execution: 1) Search Module is prompted to select the top-K most suited nodes of goals based on the provided current state and existing nodes in GOALTREE, which utilizes the prior knowledge of LLMs; 2) Decomposition Module breaks down a goal node into a list of more concrete subgoals as subsequent leaves, ensuring an adaptive self-growth of GOALTREE."
  - [corpus] Weak: The corpus includes papers on procedural learning and theory of mind, but lacks direct evidence of tree-based dynamic subgoal decomposition improving agent performance in the specific way SELF GOAL claims.

### Mechanism 2
- Claim: Granular control of the GOALTREE depth through a stopping mechanism balances guidance detail and prevents redundant or overly specific instructions.
- Mechanism: The framework uses a threshold ξ to control the granularity of subgoals. If the cosine similarity between a new subgoal and existing subgoals exceeds ξ, the current node will not be updated. A stopping mechanism is also designed that if no new nodes are added to the GOALTREE for N consecutive rounds, the update is stopped.
- Core assumption: There is an optimal level of granularity for subgoal instructions that maximizes agent performance.
- Evidence anchors:
  - [section] "To control the granularity of these subgoals, we apply a filtering mechanism that if the cosine similarity [36] between a new subgoal and existing subgoals exceeds ξ, the current node will not be updated."
  - [section] "Moreover, a stopping mechanism is designed that if no new nodes are added to the GOALTREE for N consecutive rounds, the update is stopped."
  - [section] "According to Figure 2, the agent's performance initially improves with increasing depth but eventually diminishes."
  - [corpus] Weak: The corpus includes papers on learning from feedback and task decomposition, but lacks direct evidence of optimal granularity control in tree-based subgoal decomposition frameworks.

### Mechanism 3
- Claim: The LLM-based search module is more effective than random selection or embedding similarity-based selection at finding useful subgoal nodes.
- Mechanism: The framework uses an LLM to evaluate the current scenario and select the top-K most useful subgoals from the GOALTREE. This is compared to random selection and embedding similarity-based selection as baselines.
- Core assumption: LLMs can effectively use prior knowledge and context to select the most useful subgoals from the GOALTREE.
- Evidence anchors:
  - [section] "We employ two methods as baselines to replace the original LLM-based search module, which is instantiated with GPT-3.5. One baseline is random selection, where we randomly choose a node from the set of subgoal nodes. The other is the selection based on embedding similarity, which selects the subgoals most similar to the current situation based on cosine similarity."
  - [section] "As shown in Figure 3, the LLM search module gains a better score in both games."
  - [corpus] Weak: The corpus includes papers on large language models for decision making, but lacks direct evidence of LLM-based search modules outperforming random selection or embedding similarity-based selection in tree-based subgoal decomposition frameworks.

## Foundational Learning

- Concept: Tree structures for organizing hierarchical information
  - Why needed here: The GOALTREE is a hierarchical tree structure that organizes subgoals from high-level to more specific, allowing the agent to navigate different levels of detail as needed.
  - Quick check question: What are the key components of a tree data structure and how are they used to represent hierarchical relationships?

- Concept: Cosine similarity for measuring semantic similarity
  - Why needed here: The framework uses cosine similarity to compare the semantic similarity between new subgoals and existing subgoals, controlling the granularity of the GOALTREE.
  - Quick check question: How is cosine similarity calculated and what does it measure in the context of text data?

- Concept: Bayesian statistics for estimating skill levels
  - Why needed here: The framework uses the TrueSkill Score, which estimates dynamic skill levels through Bayesian statistics while considering the uncertainty in true skills.
  - Quick check question: How does the TrueSkill Score use Bayesian statistics to estimate player skill levels and what factors does it consider?

## Architecture Onboarding

- Component map:
  - Search Module -> selects top-K subgoals from GOALTREE based on current state
  - Decompose Module -> breaks down selected goals into more concrete subgoals
  - Act Module -> uses selected subgoals to prompt LLM for actions
  - GOALTREE -> hierarchical tree structure of subgoals constructed and updated during task execution

- Critical path:
  1. Agent receives state from environment
  2. Search Module selects top-K most suited subgoals from GOALTREE
  3. Decompose Module breaks down selected subgoals into more concrete subgoals
  4. GOALTREE is updated with new subgoals
  5. Act Module uses selected subgoals to prompt LLM for actions
  6. Agent takes actions and receives new state from environment

- Design tradeoffs:
  - Granularity control vs. guidance detail: Setting threshold ξ too low results in shallow tree lacking guidance details, while setting it too high results in overly deep tree with redundant guidance
  - Search efficiency vs. effectiveness: LLM-based search module is more effective at finding useful subgoals than random selection or embedding similarity-based selection, but may be slower and more computationally expensive
  - Tree pruning vs. search quality: Pruning nodes not selected for a long time can improve search efficiency but may remove potentially useful guidance

- Failure signatures:
  - Poor agent performance indicates GOALTREE not providing effective guidance or search mechanism not selecting useful subgoals
  - Overly complex or redundant GOALTREE indicates granularity control mechanism not working effectively
  - Slow search times indicate GOALTREE is too large or search mechanism is not efficient enough

- First 3 experiments:
  1. Implement basic framework with simple GOALTREE and test on Public Goods Game
  2. Add granularity control to GOALTREE and test impact on agent performance in Public Goods Game
  3. Implement LLM-based search module and compare performance to random selection and embedding similarity-based selection in Public Goods Game

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the granularity of subgoals in GOALTREE affect the efficiency of the Search Module in selecting useful nodes?
- Basis in paper: [explicit] The paper discusses the granularity of guidelines in GOALTREE and its impact on task-solving performance, particularly in the context of stopping mechanism and threshold ξ.
- Why unresolved: The paper explores how different levels of granularity affect performance but does not explicitly address the efficiency of the Search Module in selecting nodes from GOALTREE with varying granularities.
- What evidence would resolve it: Empirical results showing the correlation between the depth of GOALTREE, the number of nodes, and the time taken by the Search Module to select relevant subgoals, along with an analysis of how granularity affects the accuracy and speed of the selection process.

### Open Question 2
- Question: Can the quality of GOALTREE generated by a stronger LLM consistently outperform those generated by weaker models across all task types and environments?
- Basis in paper: [explicit] The paper mentions that higher-quality GOALTREE (from GPT-4) significantly boosts the performance of SELF GOAL compared to one using GPT-3.5, indicating the importance of the model's understanding and summarizing capabilities.
- Why unresolved: While the paper provides evidence of GPT-4 outperforming GPT-3.5 in generating GOALTREE, it does not explore whether this trend holds across all task types and environments, nor does it investigate the diminishing returns or optimal model strength for GOALTREE generation.
- What evidence would resolve it: Comparative studies across a diverse set of tasks and environments, evaluating the performance of agents using GOALTREE generated by LLMs of varying strengths, and analyzing the correlation between model strength and task performance.

### Open Question 3
- Question: How does the pruning mechanism in SELF GOAL affect the long-term adaptability and performance of agents in dynamic environments?
- Basis in paper: [explicit] The paper discusses the pruning mechanism in SELF GOAL, which removes nodes not selected for more than five consecutive rounds, and mentions that pruning does not significantly affect the Search Module's decision-making effectiveness.
- Why unresolved: While the paper shows that pruning does not negatively impact the Search Module, it does not explore the long-term effects of pruning on the agent's adaptability and performance in dynamic environments, particularly as the environment changes over time.
- What evidence would resolve it: Longitudinal studies tracking the performance and adaptability of agents using SELF GOAL with and without pruning in dynamic environments, and analyzing the impact of pruning on the agent's ability to adjust to new situations and maintain high performance over extended periods.

## Limitations
- The framework's performance heavily depends on the quality of LLM-generated subgoals and the effectiveness of the search mechanism, with unclear prompt engineering details
- Lack of specific threshold values for granularity control (ξ) and stopping mechanism (N consecutive rounds) affecting adaptability across different tasks
- Limited validation of framework's performance in deferred feedback environments and long-term stability of the GOALTREE

## Confidence

- **High Confidence**: The framework's overall architecture and the reported improvements in competitive environments (TrueSkill Score increase from 23.96 to 29.59) are well-supported by experimental results.
- **Medium Confidence**: The effectiveness of the LLM-based search module compared to baselines is demonstrated, but the exact implementation details are unclear.
- **Low Confidence**: The framework's performance in deferred feedback environments and the long-term stability of the GOALTREE are not thoroughly validated.

## Next Checks

1. **Prompt Engineering Validation**: Systematically test different prompt formulations for the search and decomposition modules to identify the most effective configurations for generating useful subgoals.

2. **Parameter Sensitivity Analysis**: Conduct experiments to determine optimal values for the granularity threshold (ξ) and stopping mechanism (N) across different task types and environments.

3. **Scalability Assessment**: Evaluate the framework's performance as the complexity and number of subgoals increase, particularly focusing on search efficiency and the quality of selected subgoals in larger GOALTREE structures.