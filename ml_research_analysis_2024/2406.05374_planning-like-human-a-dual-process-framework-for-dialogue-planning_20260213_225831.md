---
ver: rpa2
title: 'Planning Like Human: A Dual-process Framework for Dialogue Planning'
arxiv_id: '2406.05374'
source_url: https://arxiv.org/abs/2406.05374
tags:
- dialogue
- policy
- mcts
- patient
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a dual-process dialogue planning framework
  that combines fast, intuitive policy-based responses with slower, analytical Monte
  Carlo Tree Search (MCTS) for complex scenarios, inspired by dual-process theory
  of human cognition. The method uses a two-stage training approach with offline reinforcement
  learning followed by MCTS-guided self-play to enhance policy model performance.
---

# Planning Like Human: A Dual-process Framework for Dialogue Planning

## Quick Facts
- arXiv ID: 2406.05374
- Source URL: https://arxiv.org/abs/2406.05374
- Reference count: 22
- One-line primary result: Dual-process dialogue planning framework combining policy-based fast responses with MCTS for complex scenarios achieves up to 99.23% success rate while requiring fewer dialogue turns

## Executive Summary
This paper introduces a dual-process dialogue planning framework inspired by human cognitive processes, combining fast intuitive policy-based responses with slower analytical Monte Carlo Tree Search (MCTS) for complex scenarios. The framework uses a two-stage training approach with offline reinforcement learning followed by MCTS-guided self-play to enhance policy model performance. Experiments across emotional support, tutoring, and negotiation dialogue tasks demonstrate that the proposed approach outperforms existing methods, achieving high success rates while maintaining efficiency through fewer dialogue turns.

## Method Summary
The dual-process framework implements a policy LM (System 1) for fast action prediction and MCTS (System 2) for deep planning, with a non-parameterized control gate that switches between them based on uncertainty estimation. The two-stage training pipeline first pretrains the policy LM using offline RL on static data, then refines it through MCTS-guided self-play with LLM-based simulation. The policy LM provides prior probabilities for MCTS initialization, improving search efficiency compared to uniform initialization.

## Key Results
- Achieves success rates up to 99.23% across emotional support, tutoring, and negotiation tasks
- Maintains efficiency by requiring fewer dialogue turns compared to existing methods
- Shows optimal performance varies across tasks, with different MCTS involvement ratios needed for different domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dual-process framework improves dialogue planning by combining fast intuitive responses (System 1) with slower analytical planning (System 2), mirroring human cognitive processes.
- Mechanism: The framework uses a policy LM for quick action selection when confident, and switches to MCTS for deeper planning when uncertainty is detected through the top-2 probability difference metric.
- Core assumption: The policy LM can accurately estimate its own uncertainty, and MCTS can effectively improve planning quality when uncertainty is high.
- Evidence anchors:
  - [abstract]: "a deliberative Monte Carlo Tree Search (MCTS) mechanism for complex, novel scenarios"
  - [section 3.2.3]: "We propose a non-parameterized control gate mechanism for deciding the switch" and "we assess the uncertainty by the probability difference for the top-2 values"
- Break condition: If the policy LM cannot reliably estimate uncertainty, the switching mechanism may select the wrong planner, leading to either inefficient MCTS usage or poor planning quality.

### Mechanism 2
- Claim: The two-stage training approach significantly enhances the policy model's performance compared to single-stage training.
- Mechanism: Offline RL pretraining establishes a strong initial policy using static data, then MCTS-guided self-play training refines it through interactive learning with the environment.
- Core assumption: The initial static dataset contains valuable but imperfect strategies that can be improved through reinforcement learning, and MCTS can generate better trajectories than random exploration.
- Evidence anchors:
  - [section 3.3.1]: "offline RL (Kumar et al., 2020) refines pretraining by using soft rewards to discern valuable strategies from the dataset"
  - [section 3.3.2]: "we utilize MCTS for action prediction" and "The collected transition records {st, at, st+1, rt} are used to train the policy model"
- Break condition: If the static dataset is too noisy or the rewards are poorly calibrated, offline RL may converge to suboptimal policies that MCTS cannot effectively correct.

### Mechanism 3
- Claim: Using the policy LM to provide prior probabilities for MCTS improves planning efficiency and effectiveness compared to uniform initialization or LLM-based priors.
- Mechanism: The policy LM generates action priors based on learned domain knowledge, reducing the number of LLM calls needed for MCTS initialization while providing better starting points for search.
- Core assumption: The policy LM learns meaningful domain-specific patterns during training that can guide MCTS exploration, and this learned knowledge is transferable to the MCTS context.
- Evidence anchors:
  - [section 4.5.4]: "The results indicate that the performance of the policy LM indeed impacts MCTS, particularly when the policy LM's performance is substantially improved"
  - [section 3.2.2]: "we utilize the domain-specific trained policy LM to generate the prior probability"
- Break condition: If the policy LM's learned knowledge is too domain-specific or brittle, it may misguide MCTS in novel situations, leading to worse performance than random initialization.

## Foundational Learning

- Concept: Reinforcement Learning fundamentals (Q-learning, policy gradients, exploration-exploitation tradeoff)
  - Why needed here: The framework uses offline RL pretraining and MCTS-guided self-play training, both of which rely on RL principles
  - Quick check question: How does the Actor-Critic algorithm used in self-play training differ from REINFORCE, and why might it be preferred here?

- Concept: Monte Carlo Tree Search and Upper Confidence bounds for Trees (UCT)
  - Why needed here: MCTS is the core analytical planning component, and understanding its selection, expansion, evaluation, and backpropagation phases is crucial
  - Quick check question: What is the role of the prior probability term in the PUCT formula, and how does it influence exploration?

- Concept: Uncertainty estimation in neural networks
  - Why needed here: The switching mechanism between System 1 and System 2 relies on the policy LM's ability to estimate its own uncertainty
  - Quick check question: What are the advantages and limitations of using top-2 probability difference as an uncertainty metric compared to entropy or dropout-based methods?

## Architecture Onboarding

- Component map: Input dialogue state -> Policy LM uncertainty estimation -> Action selection (Policy LM or MCTS) -> Environment simulation -> Reward calculation -> Policy update

- Critical path: Input dialogue state → Policy LM uncertainty estimation → Action selection (Policy LM or MCTS) → Environment simulation → Reward calculation → Policy update

- Design tradeoffs:
  - MCTS usage vs. efficiency: Higher MCTS ratios improve success rates but increase LLM calls and cost
  - Training stability vs. data quality: Offline RL pretraining helps with noisy static data but may converge to suboptimal policies
  - Switching sensitivity vs. responsiveness: The η threshold must balance between avoiding unnecessary MCTS calls and ensuring adequate planning

- Failure signatures:
  - Low success rates with high MCTS usage: Policy LM uncertainty estimation may be miscalibrated
  - High success rates but excessive LLM calls: Control gate threshold may be too conservative
  - Degraded performance after pretraining: Static dataset may be too noisy or rewards may be poorly calibrated

- First 3 experiments:
  1. Run inference with η=0 (always use MCTS) vs η=1 (never use MCTS) to verify switching mechanism works
  2. Compare success rates and LLM calls across different η thresholds to find optimal balance
  3. Test policy LM performance with vs without pretraining to validate two-stage training approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between System 1 (Policy LM) and System 2 (MCTS) for different dialogue tasks?
- Basis in paper: [explicit] The paper discusses trade-offs between the two systems and mentions that optimal performance varies across datasets like ESConv and CIMA.
- Why unresolved: The paper shows that different tasks benefit from different MCTS involvement ratios, but does not provide a clear method for determining the optimal balance.
- What evidence would resolve it: Empirical results showing success rates and average turns for various dialogue tasks at different MCTS ratios, along with a method for automatically determining the optimal balance.

### Open Question 2
- Question: How can the evaluation bias in using LLMs (e.g., ChatGPT) for reward modeling be addressed?
- Basis in paper: [explicit] The paper acknowledges significant evaluation bias when using LLMs for dialogue simulation and value estimation, affecting both metric calculations and training rewards.
- Why unresolved: While the paper conducts human evaluation to mitigate bias, it notes the high cost of manual assessment prevents correction during training.
- What evidence would resolve it: Development of a more robust evaluation framework that reduces LLM bias while maintaining efficiency, possibly through hybrid human-LLM evaluation methods.

### Open Question 3
- Question: How can the optimization cost of DPDP be reduced while maintaining performance?
- Basis in paper: [explicit] The paper discusses high API call costs due to continuous interaction with LLMs during self-play training and MCTS simulations.
- Why unresolved: The paper suggests potential improvements through better utilization of MCTS interaction history but does not implement them.
- What evidence would resolve it: Implementation and comparison of cost-reduction techniques, such as more efficient use of interaction records or alternative training strategies that maintain performance while reducing LLM calls.

## Limitations

- The framework's performance heavily depends on the quality of the static dataset and LLM-based simulation environment, which may converge to suboptimal policies if data is noisy or rewards are poorly calibrated
- Evaluation relies on LLM-based automatic metrics that may introduce bias and produce unrealistic dialogue lengths compared to human evaluation
- The long-term generalization of the policy LM's learned knowledge to novel situations is uncertain, potentially limiting applicability to more open-ended dialogue scenarios

## Confidence

- **High confidence**: The dual-process architecture combining policy-based fast responses with MCTS for complex scenarios is well-established in the literature and the paper provides clear implementation details. The overall improvement in success rates (up to 99.23%) compared to baselines is robust across multiple tasks.

- **Medium confidence**: The effectiveness of the two-stage training approach (offline RL + MCTS-guided self-play) is supported by results but depends on hyperparameters that are not fully specified. The uncertainty estimation mechanism using top-2 probability difference shows promise but may not generalize well to all domains.

- **Low confidence**: The long-term generalization of the policy LM's learned knowledge to novel situations is uncertain. The framework may overfit to the specific domains and action spaces used in training, limiting its applicability to more open-ended dialogue scenarios.

## Next Checks

1. **Ablation study on pretraining quality**: Test the framework's performance with varying levels of static dataset quality and reward calibration to determine the sensitivity of the two-stage training approach to data quality issues.

2. **Human evaluation of generated dialogues**: Compare the automatic LLM-based metrics with human evaluation scores across all three tasks to assess potential evaluation bias and validate the claimed efficiency improvements.

3. **Cross-domain generalization test**: Evaluate the framework's performance on dialogue tasks from different domains than those used in training to assess how well the policy LM's learned knowledge transfers to novel scenarios.