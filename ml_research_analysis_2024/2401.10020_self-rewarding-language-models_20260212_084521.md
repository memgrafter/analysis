---
ver: rpa2
title: Self-Rewarding Language Models
arxiv_id: '2401.10020'
source_url: https://arxiv.org/abs/2401.10020
tags:
- data
- training
- instruction
- self-rewarding
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Self-Rewarding Language Models fine-tune Llama 2 70B iteratively
  using its own AI feedback. The method trains the model to both generate responses
  and evaluate them via LLM-as-a-Judge prompting, building preference pairs for Direct
  Preference Optimization.
---

# Self-Rewarding Language Models

## Quick Facts
- arXiv ID: 2401.10020
- Source URL: https://arxiv.org/abs/2401.10020
- Reference count: 40
- Self-Rewarding Language Models fine-tune Llama 2 70B iteratively using its own AI feedback, achieving 20.44% win rate vs GPT-4 Turbo on AlpacaEval 2.0

## Executive Summary
Self-Rewarding Language Models (SRLMs) present an iterative fine-tuning approach that leverages a model's own feedback to improve both instruction-following and reward-modeling capabilities. The method uses LLM-as-a-Judge prompting to generate preference pairs for Direct Preference Optimization (DPO), with each iteration building on the previous model's self-generated data. On AlpacaEval 2.0, the final model achieves a 20.44% win rate against GPT-4 Turbo, significantly outperforming other competitive models while maintaining NLP benchmark performance.

## Method Summary
The approach trains Llama 2 70B to simultaneously generate responses and evaluate them through LLM-as-a-Judge prompting, creating preference pairs for DPO training. Starting with 3,200 seed instruction-following examples, the model iteratively generates new prompts and candidate responses, which are then evaluated to produce preference pairs. Each iteration uses the previous model's outputs as training data, progressively improving both the model's instruction-following abilities and its reward-modeling accuracy from 65.1% to 81.7% pairwise agreement with human rankings.

## Key Results
- Achieves 20.44% win rate vs GPT-4 Turbo on AlpacaEval 2.0 (vs 9.94% in iteration 1)
- Improves MT-Bench score from 6.85 to 7.25 (+0.47 points)
- Increases reward-modeling accuracy from 65.1% to 81.7% pairwise agreement with human rankings
- Outperforms Claude 2, Gemini Pro, and GPT-4 0613 on AlpacaEval 2.0

## Why This Works (Mechanism)
The self-rewarding approach works by creating a feedback loop where the model learns to generate better responses by evaluating its own outputs. Through iterative DPO training on self-generated preference pairs, the model simultaneously improves its instruction-following capabilities and its ability to accurately judge response quality. This dual improvement creates a compounding effect where better instruction-following leads to higher-quality self-generated data, which in turn enables more effective reward modeling.

## Foundational Learning
- Direct Preference Optimization (DPO): A method for fine-tuning language models using preference data without a separate reward model. Needed to train the model on self-generated preference pairs efficiently. Quick check: Verify that the DPO loss function properly optimizes the Bradley-Terry model.
- LLM-as-a-Judge: Using a language model to evaluate and rank responses. Essential for generating the preference pairs used in training. Quick check: Ensure the judge model's outputs are consistent across multiple evaluations of the same response.
- Iterative Fine-tuning: Repeatedly training a model on its own outputs. Critical for the self-improving nature of the approach. Quick check: Monitor performance metrics across iterations to detect overfitting or degradation.

## Architecture Onboarding

**Component Map:**
Llama 2 70B -> SFT (seed data) -> DPO (self-generated preference pairs) -> Improved model -> Repeat DPO

**Critical Path:**
1. Fine-tune base model on seed instruction-following data
2. Generate prompts and responses using the fine-tuned model
3. Evaluate responses using LLM-as-a-Judge to create preference pairs
4. Apply DPO training on preference pairs
5. Repeat steps 2-4 for subsequent iterations

**Design Tradeoffs:**
- Using self-generated data reduces the need for human-labeled preference pairs but risks amplifying biases
- LLM-as-a-Judge provides scalable evaluation but may not perfectly align with human preferences
- Iterative approach enables compounding improvements but requires careful monitoring for degradation

**Failure Signatures:**
- Response length increases significantly without corresponding quality improvements
- Reward modeling accuracy plateaus or decreases across iterations
- Performance on certain task categories (e.g., mathematics) fails to improve

**First Experiments:**
1. Compare model outputs before and after each iteration on held-out prompts to measure improvement
2. Evaluate the model's ability to judge response quality using human-annotated preference data
3. Test the model on a diverse set of NLP benchmarks to ensure maintained capabilities

## Open Questions the Paper Calls Out
- What are the "scaling laws" of self-rewarding model improvement across multiple iterations and with different language models?
- How does the correlation between response length and quality affect the self-rewarding training process and evaluation?
- Can "reward-hacking" occur within the self-rewarding framework, and under what circumstances?
- How effective is self-rewarding for safety alignment, and can safety evaluations be incorporated into the iterative training process?
- Why do certain task categories (e.g., mathematics and logical reasoning) show less improvement with self-rewarding training compared to others?

## Limitations
- Relies on a single model size (Llama 2 70B) and small seed dataset (3,200 examples)
- Iterative approach may lead to overfitting or amplification of biases present in self-generated data
- Evaluation heavily depends on LLM-as-a-Judge outputs, which may not perfectly align with human preferences

## Confidence
**High Confidence:** Improvements in AlpacaEval 2.0 win rates and MT-Bench scores
**Medium Confidence:** Reward-modeling improvements (65.1% to 81.7% pairwise agreement)
**Low Confidence:** Claims about maintaining NLP benchmark performance without specific benchmark details

## Next Checks
1. Evaluate the model on a comprehensive suite of NLP benchmarks (MMLU, HellaSwag, TruthfulQA) to verify maintained general language capabilities
2. Track the evolution of bias metrics across iterations using standardized bias evaluation datasets
3. Apply the same self-rewarding methodology to a smaller model (Llama 2 13B) to assess cross-model generalizability