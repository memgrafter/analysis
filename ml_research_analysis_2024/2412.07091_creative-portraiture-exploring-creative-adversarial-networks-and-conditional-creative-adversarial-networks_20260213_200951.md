---
ver: rpa2
title: 'Creative Portraiture: Exploring Creative Adversarial Networks and Conditional
  Creative Adversarial Networks'
arxiv_id: '2412.07091'
source_url: https://arxiv.org/abs/2412.07091
tags:
- creative
- style
- training
- output
- ccan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores creative adversarial networks (CANs) and conditional
  CANs (CCANs) for generating novel portrait artwork, addressing the limitation of
  standard DCGANs that simply emulate training distributions. The authors modify the
  discriminator to include an additional style-classification head and introduce a
  loss term that penalizes easy categorization of generated images into specific styles,
  thereby encouraging creative divergence while maintaining recognizability as real
  art.
---

# Creative Portraiture: Exploring Creative Adversarial Networks and Conditional Creative Adversarial Networks

## Quick Facts
- arXiv ID: 2412.07091
- Source URL: https://arxiv.org/abs/2412.07091
- Authors: Sebastian Hereu; Qianfei Hu
- Reference count: 6
- One-line primary result: CAN and CCAN architectures generate stylistically ambiguous and creative portrait artwork compared to baseline DCGAN

## Executive Summary
This paper explores creative adversarial networks (CANs) and conditional CANs (CCANs) for generating novel portrait artwork, addressing the limitation of standard DCGANs that simply emulate training distributions. The authors modify the discriminator to include an additional style-classification head and introduce a loss term that penalizes easy categorization of generated images into specific styles, thereby encouraging creative divergence while maintaining recognizability as real art. Using the WikiArt portrait dataset, they train and evaluate DCGAN, CAN, and CCAN models at 64x64 resolution. The CAN produces more diverse and stylistically ambiguous portraits compared to the baseline DCGAN, while the CCAN generates style-conditioned yet creatively divergent artwork.

## Method Summary
The authors train three variants of GANs (DCGAN, CAN, and CCAN) on the WikiArt portrait dataset using a reduced model architecture of approximately 6 million parameters. CAN adds a style-classification head to the discriminator and introduces an entropy-based loss term that encourages stylistic ambiguity. CCAN extends CAN with conditional embeddings for style conditioning. All models are trained for 120 epochs with batch size 128 using Adam optimizer. The dataset is preprocessed with multi-crop augmentation to improve generalization.

## Key Results
- CAN produces more diverse and stylistically ambiguous portraits compared to baseline DCGAN
- CCAN successfully generates style-conditioned artwork while maintaining creative divergence through entropy-based loss
- All models trained stably over 120 epochs without mode collapse despite hardware limitations and reduced resolution outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The CAN architecture encourages stylistic ambiguity by adding an entropy-based loss term that penalizes easy categorization into specific styles.
- Mechanism: The discriminator is modified to have two heads - one for real/fake classification and another for style classification. The generator's loss is augmented with a cross-entropy term that encourages outputs that are difficult to categorize into any single style. This creates a tug-of-war where the generator must produce art that is both real-looking (passes the real/fake test) and stylistically ambiguous (fails the style classification test).
- Core assumption: Artistic creativity involves balancing between recognizable style and novel divergence from established norms.
- Evidence anchors:
  - [abstract] "The authors modify the discriminator to include an additional style-classification head and introduce a loss term that penalizes easy categorization of generated images into specific styles"
  - [section] "The generator's loss is modified to consist of the same real/fake loss as before in addition to a cross-entropy loss that penalizes the generator for creating art that is easy to categorize as a specific style"
- Break condition: If the style classification head becomes too weak to provide meaningful gradients, or if the entropy term overwhelms the real/fake loss, causing the generator to produce unrecognizable or chaotic images.

### Mechanism 2
- Claim: CCAN achieves style-conditioned creative divergence by combining class embeddings with the entropy-based loss from CAN.
- Mechanism: Class embeddings are concatenated with the noise vector in the generator and with the input sample in the discriminator. This allows conditioning on specific styles while the entropy loss term still encourages stylistic ambiguity within that conditioned style. The tension between style conditioning and entropy loss drives creative divergence within style constraints.
- Core assumption: Human creativity often involves working within a style framework while finding novel ways to express within those boundaries.
- Evidence anchors:
  - [abstract] "The CCAN architecture successfully balances style conditioning with the entropy-based loss that promotes stylistic ambiguity"
  - [section] "We argue that this tension produced creative outputs, as the model was forced to find subtle and interesting ways to create style ambiguity while still being rooted in a specific style"
- Break condition: If the conditioning embeddings are too strong relative to the entropy term, outputs become conventional style replicas; if too weak, conditioning becomes meaningless.

### Mechanism 3
- Claim: The multi-crop preprocessing strategy improves generalization by forcing the model to learn style-agnostic features rather than memorizing specific image regions.
- Mechanism: By creating five crops (center + four corners) of 90% of each original image, the model is trained on multiple perspectives of the same artwork. This reduces overfitting to specific compositional elements and encourages learning of more fundamental artistic features like brushstrokes, color palettes, and figure representations.
- Core assumption: Style features in artwork are distributed throughout the image rather than localized to specific regions.
- Evidence anchors:
  - [section] "We believe this cropping step is essential, especially for our reduced-size model, as otherwise, the model may have learned idiosyncrasies of the training data instead of the salient patterns of the artwork"
- Break condition: If the crops are too small relative to the model's receptive field, important contextual information may be lost, degrading output quality.

## Foundational Learning

- Concept: GAN training dynamics and mode collapse
  - Why needed here: Understanding why the discriminator and generator losses must remain balanced is critical for interpreting training stability results
  - Quick check question: What would happen if the discriminator loss dropped to near zero while the generator loss increased significantly?

- Concept: Cross-entropy loss and entropy maximization
  - Why needed here: The creative mechanism relies on modifying standard classification loss to encourage output diversity
  - Quick check question: How does adding an entropy term to the loss function differ from standard classification training?

- Concept: Conditional generation with embeddings
  - Why needed here: CCAN's ability to generate style-conditioned outputs depends on understanding how class embeddings influence the generation process
  - Quick check question: What is the effect of concatenating a class embedding with the latent noise vector in the generator?

## Architecture Onboarding

- Component map: Noise vector -> Generator (transposed convolutions) -> Generated image -> Discriminator (convolutions + style head) -> Real/fake + Style classification -> Loss computation -> Gradient update
- Critical path: Generator → Discriminator → Loss computation → Gradient update (both generator and discriminator)
- Design tradeoffs:
  - Smaller model (6M params vs 50M) enables faster iteration but produces lower resolution outputs
  - Entropy-based loss encourages creativity but may reduce visual fidelity
  - Class conditioning provides control but adds complexity and potential for mode collapse
- Failure signatures:
  - Mode collapse: Discriminator loss near zero, generator loss increasing steadily
  - Overfitting: Loss decreases but generated images show memorization artifacts
  - Conditioning failure: Generated images don't reflect conditioned styles despite stable training
- First 3 experiments:
  1. Train DCGAN baseline to establish performance floor and verify stable training on reduced dataset
  2. Add style classification head to discriminator and implement entropy-based loss term, verify creative divergence from baseline
  3. Add class embeddings for conditioning, verify that generated images reflect style labels while maintaining creative divergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the conditional entropy-based loss in CCANs effectively promote creativity while maintaining style conditioning, or does it create an irreconcilable tension that limits the model's creative potential?
- Basis in paper: [explicit] The authors acknowledge a paradoxical tension between conditioning on style labels (which moves the generator toward specific styles) and the fake-image entropy loss (which encourages stylistic ambiguity)
- Why unresolved: The paper demonstrates proof-of-concept results but doesn't provide quantitative measures of creativity or systematic analysis of how the tension between conditioning and entropy affects creative output
- What evidence would resolve it: Systematic comparison of creativity metrics between CAN, CCAN, and baseline DCGAN using standardized creativity evaluation frameworks; ablation studies showing the impact of entropy loss strength on creativity vs. conditioning trade-off

### Open Question 2
- Question: How does the reduced model complexity (6 million parameters vs. 50 million in the original CAN) affect the quality and creativity of generated portraits, and what is the optimal balance between model size and creative output?
- Basis in paper: [explicit] The authors explicitly state they used a significantly reduced model (6 million parameters) compared to the original CAN (50+ million parameters) due to hardware constraints, and note their results are a "proof-of-concept"
- Why unresolved: The paper demonstrates that smaller models can produce creative output, but doesn't explore the relationship between model capacity and creative capability or establish performance benchmarks
- What evidence would resolve it: Comparative study training CAN/CCAN models at multiple scales (e.g., 6M, 20M, 50M parameters) on the same dataset with standardized evaluation metrics for both image quality and creativity

### Open Question 3
- Question: What are the fundamental limitations of current CAN/CCAN architectures in achieving human-level creativity, and what architectural innovations would be needed to bridge this gap?
- Basis in paper: [inferred] The authors conclude that "machines can truly be creative" remains an open question and that current models "still generate art using essentially a mixture of the training data," suggesting limitations in achieving genuine creativity
- Why unresolved: The paper acknowledges the philosophical and technical challenges but doesn't propose specific architectural modifications or analyze the cognitive aspects of creativity that current models lack
- What evidence would resolve it: Systematic analysis of what aspects of human creative processes (e.g., intentionality, emotional expression, conceptual innovation) are missing from current CAN/CCAN architectures, coupled with experimental validation of proposed architectural improvements

## Limitations
- Evaluation relies entirely on qualitative assessment of generated images, lacking quantitative metrics to measure creative divergence or stylistic ambiguity
- The 64x64 resolution limitation may prevent the models from capturing fine-grained artistic features necessary for high-quality creative portraiture
- Without access to the exact WikiArt portrait subset, reproduction may yield different results depending on which images are selected

## Confidence
- High confidence: Mechanism descriptions and architectural implementations based on clear explanations of loss functions and training procedures
- Medium confidence: Claimed creative divergence results due to reliance on qualitative visual comparison without quantitative validation
- Low confidence: Generalizability of results to higher resolutions or different artistic domains given the hardware limitations and single dataset focus

## Next Checks
1. **Quantitative Creativity Metric**: Implement Fréchet Inception Distance (FID) or similar metric to measure distributional divergence between CAN/CCAN outputs and training data, comparing against DCGAN baseline to provide objective evidence of creative divergence
2. **Style Classification Accuracy**: Train a separate style classifier on real portraits and measure its accuracy on CAN/CCAN outputs to quantify the claimed stylistic ambiguity - outputs should be harder to classify than DCGAN outputs
3. **Cross-Domain Transfer**: Test the CAN/CCAN architectures on a different artistic domain (e.g., landscape or abstract art) to validate whether the creative mechanism generalizes beyond portrait artwork