---
ver: rpa2
title: In-Context Fine-Tuning for Time-Series Foundation Models
arxiv_id: '2410.24087'
source_url: https://arxiv.org/abs/2410.24087
tags:
- examples
- in-context
- time-series
- timesfm
- foundation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces in-context fine-tuning for time-series foundation
  models, enabling them to leverage multiple related time-series examples during inference
  for improved forecasting accuracy. The proposed approach modifies a base time-series
  foundation model (TimesFM) to support in-context examples through learnable separator
  tokens, cross-example attention, and positional encoding adaptations.
---

# In-Context Fine-Tuning for Time-Series Foundation Models

## Quick Facts
- arXiv ID: 2410.24087
- Source URL: https://arxiv.org/abs/2410.24087
- Reference count: 18
- This paper introduces in-context fine-tuning for time-series foundation models, enabling them to leverage multiple related time-series examples during inference for improved forecasting accuracy.

## Executive Summary
This paper introduces in-context fine-tuning for time-series foundation models, enabling them to leverage multiple related time-series examples during inference for improved forecasting accuracy. The proposed approach modifies a base time-series foundation model (TimesFM) to support in-context examples through learnable separator tokens, cross-example attention, and positional encoding adaptations. The model is pretrained on a diverse corpus of 400B time-series points and then continues pretraining with contexts containing up to 50 related examples per target series. Evaluation on popular benchmarks (Monash and ETT) demonstrates significant improvements over supervised deep learning methods, statistical models, and other foundation models.

## Method Summary
The method involves continuing pretraining a base TimesFM model with in-context examples. The approach uses learnable separator tokens to distinguish between different examples, cross-example attention to allow information flow between examples, and No Positional Encodings (NoPE) for better length generalization. During continued pretraining, the model learns to process contexts containing up to 50 related time-series examples alongside the target series. The training uses a decoder-only transformer architecture with MSE loss, and the model can adapt to varying numbers of in-context examples during inference.

## Key Results
- The in-context fine-tuned model achieves up to 25% better performance than state-of-the-art baselines
- The model outperforms both supervised deep learning methods and statistical models on popular benchmarks
- The approach rivals models explicitly fine-tuned per dataset despite being zero-shot

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context examples help the model disambiguate between similar-looking time-series patterns (e.g., linear trends vs. triangular waves) that would otherwise confuse the model if concatenated naively.
- Mechanism: The model uses learnable separator tokens and cross-example attention to distinguish between different in-context examples, allowing it to recognize the boundaries and patterns of each example separately.
- Core assumption: The transformer's stacked layers, combined with causal attention to separator tokens, can encode and summarize distinct patterns from each example.
- Evidence anchors:
  - [abstract] "Our foundation model is specifically trained to utilize examples from multiple related time-series in its context window... to help it adapt to the specific distribution of the target domain"
  - [section 4.1] "If we na√Øvely concatenate multiple in-context examples (e.g., linear trends, Figure 3a) together, then the combination of these trends may appear to the model as an entirely different time-series (e.g., a triangle wave, Figure 3b)"
  - [corpus] Weak evidence - the corpus contains related papers on in-context learning but lacks specific evidence for the separator token mechanism described here.
- Break condition: If the model does not attend to separator tokens or if the separator tokens are not learnable, the model may fail to distinguish between different in-context examples, leading to confusion similar to the triangular wave example.

### Mechanism 2
- Claim: The model's ability to handle different numbers of in-context examples (up to a maximum) allows it to adapt to varying amounts of related information available for a target time-series.
- Mechanism: The model is trained with varying numbers of in-context examples (from 1 to 50) during the continued pretraining phase, teaching it to effectively utilize any number of examples within this range.
- Core assumption: The model can learn to generalize across different numbers of in-context examples and still maintain or improve forecasting accuracy.
- Evidence anchors:
  - [abstract] "Our training is decoder-only and can adapt not only to any context, horizon pair (up to a certain maximum context) but also to any number of supplementary time-series examples (again up to a certain maximum number of examples)"
  - [section 6.4.1] "We perform an ablation where we vary the number of in-context examples from 1 to the maximum during our training i.e. n = 50. The corresponding results are reported on the ETTh test set in Figure 7. We can see a monotonic increase in performance with more in-context examples."
  - [corpus] Weak evidence - while the corpus contains papers on in-context learning, it lacks specific evidence for the effectiveness of varying numbers of in-context examples during training.
- Break condition: If the model is trained with too few in-context examples or if the maximum number of examples is set too low, it may not learn to effectively utilize the available related information, leading to suboptimal performance.

### Mechanism 3
- Claim: Using No Positional Encodings (NoPE) during continued pretraining allows the model to better generalize to longer contexts and handle the varying lengths of in-context examples.
- Mechanism: By not using absolute positional encodings, the model relies on causal attention to encode positional information, which allows it to handle varying lengths of in-context examples more effectively.
- Core assumption: NoPE models have better length generalization and can handle the varying lengths of in-context examples more effectively than models with absolute positional encodings.
- Evidence anchors:
  - [section 4.3] "The advantages of NoPE for our continued pretraining are two fold: (i) NoPE models usually have better length generalization, which is particularly important here since we increase the prompt length by adding in-context examples to the context. (ii) If we use the original absolute positional encodings used in [DKSZ24], the meaning of these positional encodings in the base model would be different from their meaning during the continued pretraining with in-context examples."
  - [corpus] Weak evidence - while the corpus contains papers on positional encodings, it lacks specific evidence for the effectiveness of NoPE in the context of in-context fine-tuning for time-series foundation models.
- Break condition: If the model's ability to encode positional information through causal attention is limited, or if the varying lengths of in-context examples are too diverse, the model may struggle to effectively handle the context, leading to decreased performance.

## Foundational Learning

- Concept: Transformer architectures and their ability to handle sequential data through self-attention mechanisms.
  - Why needed here: The proposed model is based on a stacked transformer architecture, which is crucial for its ability to process and learn from the sequential nature of time-series data and in-context examples.
  - Quick check question: How does the self-attention mechanism in transformers allow for the processing of sequential data, and why is this important for time-series forecasting?

- Concept: In-context learning and its application to time-series data.
  - Why needed here: The paper introduces the concept of in-context fine-tuning, which allows the model to leverage related time-series examples during inference to improve forecasting accuracy.
  - Quick check question: How does in-context learning differ from traditional fine-tuning, and what are the benefits of using in-context examples for time-series forecasting?

- Concept: Foundation models and their ability to generalize across diverse tasks and domains.
  - Why needed here: The proposed model is a time-series foundation model, which has been pretrained on a large corpus of time-series data and can adapt to new forecasting tasks through in-context fine-tuning.
  - Quick check question: What are the key characteristics of foundation models, and how do they differ from traditional task-specific models in terms of their ability to generalize and adapt to new tasks?

## Architecture Onboarding

- Component map:
  - Input residual block: Embeds each time-series patch as a token
  - Separator tokens: Distinguish between different in-context examples
  - Stacked transformer layers: Process tokens in decoder-only mode with cross-example attention
  - Output residual block: Maps output embeddings to predicted time-series patches
  - Loss function: Mean Squared Error (MSE) between predicted and actual time-series values

- Critical path:
  1. Tokenize input time-series patches and separator tokens
  2. Feed tokens to stacked transformer layers with cross-example attention
  3. Generate output embeddings for each token
  4. Map output embeddings to predicted time-series patches using output residual block
  5. Calculate loss and update model parameters during training

- Design tradeoffs:
  - Using learnable separator tokens vs. fixed separators: Learnable separators allow the model to adapt the separator representation to the specific task and data distribution, but may require more training data and computational resources.
  - Allowing cross-example attention vs. restricting attention to within-example tokens: Cross-example attention enables the model to leverage patterns from related examples, but may increase computational complexity and risk of overfitting.
  - Using NoPE vs. absolute positional encodings: NoPE allows for better length generalization and handling of varying in-context example lengths, but may require more transformer layers to effectively encode positional information.

- Failure signatures:
  - Poor performance on target time-series: May indicate that the model is not effectively leveraging in-context examples or that the in-context examples are not sufficiently related to the target time-series.
  - Overfitting to in-context examples: May occur if the model relies too heavily on the patterns from in-context examples and fails to generalize to new, unseen time-series data.
  - Inconsistent performance across different numbers of in-context examples: May suggest that the model has not effectively learned to handle varying amounts of related information during training.

- First 3 experiments:
  1. Evaluate the model's performance on a held-out validation set with varying numbers of in-context examples to assess its ability to leverage related information.
  2. Compare the model's performance with and without learnable separator tokens to quantify the impact of the separator mechanism on disambiguating in-context examples.
  3. Analyze the attention weights of the model to understand how it leverages patterns from in-context examples and identifies relevant information for the target time-series.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of in-context fine-tuning vary across different domains (e.g., retail vs. weather vs. traffic forecasting) when the in-context examples come from the same domain versus different domains?
- Basis in paper: [inferred] The paper mentions that in-context examples are drawn from related time-series, but doesn't explore cross-domain in-context examples or compare performance across different domains.
- Why unresolved: The paper focuses on out-of-domain forecasting within the same dataset type but doesn't investigate whether domain-specific patterns are necessary for effective in-context learning.
- What evidence would resolve it: Experiments comparing in-context fine-tuning performance using same-domain versus cross-domain examples across multiple domain types, with quantitative metrics showing domain transfer effectiveness.

### Open Question 2
- Question: What is the optimal number of transformer layers needed to effectively process separator tokens and cross-example attention for in-context fine-tuning?
- Basis in paper: [explicit] The paper mentions that "stacked transformers used in the LLMs" enable in-context learning and references theoretical properties, but doesn't empirically analyze how depth affects separator token processing or cross-example attention.
- Why unresolved: The paper uses the same number of layers as the base TimesFM model (20 layers) without investigating whether this is optimal for the new in-context learning task.
- What evidence would resolve it: Ablation studies varying the number of transformer layers while keeping other parameters constant, measuring performance as a function of model depth.

### Open Question 3
- Question: How does in-context fine-tuning compare to traditional fine-tuning when the target dataset is very small (e.g., fewer than 100 time-series)?
- Basis in paper: [explicit] The paper mentions that "in many of the smaller datasets in Monash, fine-tuning the weights of a foundation model can actually lead to catastrophic forgetting" but doesn't systematically explore the performance gap across different dataset sizes.
- Why unresolved: The paper provides aggregate results showing in-context fine-tuning outperforms fine-tuning but doesn't analyze the relationship between dataset size and the relative effectiveness of each approach.
- What evidence would resolve it: Detailed experiments showing performance comparison between in-context fine-tuning and fine-tuning across datasets of varying sizes, with analysis of when each approach becomes superior.

## Limitations

- Limited evidence across diverse time-series domains beyond electricity and traffic data
- Computational efficiency trade-offs of using up to 50 in-context examples per inference are not quantified
- Uncertainty about whether improvements come from the in-context mechanism itself versus additional training data or model capacity

## Confidence

- **High confidence**: The core claim that in-context fine-tuning improves forecasting accuracy over baseline foundation models and traditional methods is well-supported by the experimental results.
- **Medium confidence**: The specific mechanisms by which in-context examples improve performance (separator tokens, cross-example attention, NoPE) are plausible but not definitively proven.
- **Low confidence**: Claims about the model's ability to generalize across completely unseen time-series domains are not well-supported by the available evidence.

## Next Checks

1. **Mechanistic validation**: Conduct attention visualization studies to empirically verify that separator tokens are being used as intended to distinguish between different in-context examples. Specifically, examine attention patterns when similar-looking patterns (like the linear trend vs triangle wave) are present in the context.

2. **Cross-domain evaluation**: Test the model on time-series datasets from diverse domains (e.g., financial, medical, environmental) that were not represented in the pretraining data to assess true generalization capabilities beyond electricity and traffic data.

3. **Computational efficiency analysis**: Measure and report the computational overhead of using varying numbers of in-context examples (1 vs 10 vs 50) in terms of inference time, memory usage, and energy consumption to provide a complete picture of the trade-offs involved.