---
ver: rpa2
title: 'TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large
  Language Models'
arxiv_id: '2404.09204'
source_url: https://arxiv.org/abs/2404.09204
tags:
- visual
- image
- texthawk
- language
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of building multimodal large
  language models (MLLMs) that excel at both general vision-language tasks and document-oriented
  tasks requiring fine-grained image perception. To achieve this, the authors propose
  TextHawk, which introduces four key components: (1) ReSampling and ReArrangement
  (ReSA) module to compress visual tokens efficiently, (2) Scalable Positional Embeddings
  (SPEs) to encode positions of sub-images while preserving scalability, (3) Query
  Proposal Network (QPN) to dynamically initialize queries for sub-images, and (4)
  Multi-Level Cross-Attention (MLCA) to capture hierarchical structure and semantic
  relations in document images.'
---

# TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2404.09204
- Source URL: https://arxiv.org/abs/2404.09204
- Authors: Ya-Qi Yu; Minghui Liao; Jihao Wu; Yongxin Liao; Xiaoyu Zheng; Wei Zeng
- Reference count: 40
- Key outcome: TextHawk achieves state-of-the-art results on both general MLLM benchmarks (74.6% on MMBench) and document-oriented benchmarks (76.4% on DocVQA)

## Executive Summary
TextHawk addresses the challenge of building multimodal large language models that excel at both general vision-language tasks and document-oriented tasks requiring fine-grained image perception. The authors propose four key components: ReSampling and ReArrangement (ReSA) module for efficient visual token compression, Scalable Positional Embeddings (SPEs) for position encoding, Query Proposal Network (QPN) for dynamic query initialization, and Multi-Level Cross-Attention (MLCA) for hierarchical feature understanding. Through comprehensive experiments, TextHawk demonstrates superior performance on document-oriented benchmarks while maintaining strong general vision-language abilities.

## Method Summary
TextHawk uses a three-stage training approach with a frozen SigLIP-SO visual encoder and InternLM-XComposer LLM. The model first pre-trains on fixed-resolution image-text pairs, then switches to mixed-resolution training with shape-adaptive cropping and ReSA compression (16x). Finally, it fine-tunes on instruction-following data. Key innovations include the ReSA module for visual token compression, SPEs for handling arbitrary image shapes, QPN for dynamic query generation, and MLCA for merging features from different visual encoder layers. The model also includes a detection head for grounding tasks and uses LoRA for efficient fine-tuning.

## Key Results
- Achieves 74.6% accuracy on MMBench, outperforming previous state-of-the-art by 1.4%
- Scores 76.4% on DocVQA, demonstrating superior fine-grained document perception
- Maintains competitive performance on general vision-language benchmarks while excelling at document tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReSA module significantly reduces visual token count while preserving fine-grained information by combining resampling and rearrangement in two stages.
- Mechanism: First stage uses cross-attention to resample visual features; second stage concatenates and projects multiple resampled tokens into fewer tokens, achieving 16x compression.
- Core assumption: Information from multiple resampled tokens can be meaningfully compressed without loss of essential document details.
- Evidence anchors:
  - [abstract] "ReSampling and ReArrangement (ReSA) module to compress visual tokens efficiently"
  - [section] "Each step of ReSA possesses a compression ratio of 4, resulting in a notably higher compression ratio of 16."
  - [corpus] Weak - no direct evidence in corpus about ReSA's effectiveness.
- Break condition: If document images require token density higher than 16x compression can provide, essential details may be lost.

### Mechanism 2
- Claim: Multi-Level Cross-Attention (MLCA) improves fine-grained perception by merging features from different visual encoder layers.
- Mechanism: MLCA routes features from multiple encoder stages (shallow for detail, deep for semantics) to resampler layers, enabling hierarchical understanding without extra computation.
- Core assumption: Features from different encoder layers capture complementary information that can be combined effectively.
- Evidence anchors:
  - [abstract] "Multi-Level Cross-Attention (MLCA) to capture hierarchical structure and semantic relations"
  - [section] "MLCA enables the resampler to absorb features from deep as well as shallow visual encoder layers"
  - [corpus] Weak - no corpus evidence about MLCA's effectiveness.
- Break condition: If feature fusion creates conflicts between semantic and detailed information, performance may degrade.

### Mechanism 3
- Claim: Query Proposal Network (QPN) generates dynamic queries for variable-resolution sub-images, eliminating redundancy from fixed query sharing.
- Mechanism: QPN processes visual encoder output through MLP and max pooling to produce tailored queries for each sub-image, controlled by pooling stride.
- Core assumption: Dynamic queries based on local features produce better attention patterns than shared fixed queries.
- Evidence anchors:
  - [abstract] "Query Proposal Network (QPN) to dynamically initialize queries for sub-images"
  - [section] "QPN for generating the queries dynamically" and "eliminating the side-effect of shared initial queries"
  - [corpus] Weak - no corpus evidence about QPN's effectiveness.
- Break condition: If QPN adds too much computational overhead relative to performance gains, it may not be worthwhile.

## Foundational Learning

- Concept: Visual Transformer architectures and attention mechanisms
  - Why needed here: TextHawk builds on SigLIP visual encoder and transformer-based resampler; understanding these is essential for implementing the proposed components.
  - Quick check question: How does self-attention differ from cross-attention in transformer architectures?

- Concept: Positional embeddings and their interpolation
- Concept: Cross-modal learning and instruction tuning
  - Why needed here: TextHawk requires understanding how to align visual and language modalities and how instruction tuning improves task-specific performance.
  - Quick check question: What is the difference between pre-training and instruction tuning in MLLMs?

- Concept: Multi-task learning and dataset curation
  - Why needed here: TextHawk uses diverse datasets (conceptual captioning, OCR, Markdown, DocGemini) and multi-task training; understanding this is crucial for effective implementation.
  - Quick check question: Why might concatenating multiple samples into longer sequences improve training efficiency?

## Architecture Onboarding

- Component map:
  Input: High-resolution document images -> Shape-Adaptive Cropping -> Sub-images -> Visual Encoder (frozen) -> Features -> ReSA -> Compressed tokens -> MLCA -> QPN -> LLM with LoRA -> Response

- Critical path:
  1. Image → Shape-Adaptive Cropping → Sub-images
  2. Sub-images → Visual Encoder → Features
  3. Features → ReSA → Compressed tokens
  4. Compressed tokens + LLM → Response

- Design tradeoffs:
  - Frozen visual encoder vs. trainable: Faster training but less adaptation
  - 16x compression vs. detail preservation: Computational efficiency vs. potential information loss
  - Dynamic queries vs. fixed: Better adaptation vs. complexity
  - Detection head vs. pure language output: Better grounding vs. increased parameters

- Failure signatures:
  - Loss of fine-grained details in compressed tokens
  - Poor performance on high-resolution document tasks
  - Sub-optimal attention patterns due to inadequate queries
  - Sub-image boundary artifacts affecting comprehension

- First 3 experiments:
  1. Test ReSA compression ratio impact on document understanding accuracy
  2. Compare MLCA routing strategies on fine-grained task performance
  3. Evaluate QPN vs. fixed queries on variable resolution input handling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scalability of Scalable Positional Embeddings (SPEs) perform when applied to images with extreme aspect ratios or non-rectangular shapes?
- Basis in paper: [explicit] The paper mentions that SPEs are designed to handle "arbitrary shapes" and discusses their use in interpolating positional embeddings between endpoints. However, the paper does not provide empirical results for extreme aspect ratios or non-rectangular shapes.
- Why unresolved: The paper does not test SPEs on images with extreme aspect ratios or non-rectangular shapes, leaving their performance in these scenarios unknown.
- What evidence would resolve it: Conducting experiments with images of extreme aspect ratios (e.g., very wide or very tall) or non-rectangular shapes (e.g., circular or L-shaped) and comparing the performance of SPEs to other positional embedding methods would provide evidence.

### Open Question 2
- Question: What is the impact of the Query Proposal Network (QPN) on the overall computational efficiency of TextHawk, and how does it compare to other query initialization methods?
- Basis in paper: [explicit] The paper introduces QPN as a lightweight module for generating queries dynamically, but it does not provide a detailed analysis of its computational efficiency compared to other methods.
- Why unresolved: The paper does not provide a quantitative comparison of the computational cost of QPN versus other query initialization methods, such as fixed learnable parameters or other dynamic approaches.
- What evidence would resolve it: Measuring the computational cost (e.g., FLOPs, inference time) of QPN and comparing it to other query initialization methods would provide evidence.

### Open Question 3
- Question: How does the Multi-Level Cross-Attention (MLCA) mechanism affect the model's ability to handle tasks that require understanding of both global and local context, such as scene understanding or object detection?
- Basis in paper: [explicit] The paper describes MLCA as a method for merging deep and shallow features to enhance fine-grained visual perception, but it does not provide specific results for tasks requiring global and local context understanding.
- Why unresolved: The paper does not evaluate MLCA on tasks that specifically require understanding of both global and local context, such as scene understanding or object detection, leaving its effectiveness in these areas unclear.
- What evidence would resolve it: Evaluating TextHawk on tasks that require understanding of both global and local context, such as scene understanding or object detection, and comparing its performance to models without MLCA would provide evidence.

## Limitations
- Effectiveness of 16x compression through ReSA module remains untested on diverse document types beyond those evaluated in benchmarks
- MLCA's routing mechanism between shallow and deep visual encoder layers lacks ablation studies to quantify its specific contribution
- QPN's dynamic query generation adds computational overhead that may not be justified by performance improvements across all document types

## Confidence
- **High confidence**: General architecture design and three-stage training procedure are well-specified and reproducible
- **Medium confidence**: Performance claims on document-oriented benchmarks are supported by experimental results, though specific component contributions are not fully isolated
- **Low confidence**: Claims about efficiency gains (16x compression) and superiority over specific baselines lack detailed ablation studies and comprehensive error analysis

## Next Checks
1. **Ablation study on ReSA compression ratio**: Systematically evaluate model performance at different compression ratios (4x, 8x, 16x, 32x) to identify optimal tradeoff between efficiency and fine-grained perception quality
2. **Cross-dataset generalization test**: Evaluate TextHawk on previously unseen document types (medical records, legal documents, technical manuals) to assess true robustness of fine-grained perception capabilities
3. **Component isolation experiments**: Conduct controlled experiments disabling each proposed component (MLCA, QPN, ReSA) individually to quantify their specific contributions to overall performance improvements