---
ver: rpa2
title: "\"Vorbe\u015Fti Rom\xE2ne\u015Fte?\" A Recipe to Train Powerful Romanian LLMs\
  \ with English Instructions"
arxiv_id: '2406.18266'
source_url: https://arxiv.org/abs/2406.18266
tags:
- romanian
- arxiv
- language
- performance
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first open-source Romanian Large Language
  Models (RoLLMs) trained using a recipe of continual pre-training and instruction
  fine-tuning. The authors collected and translated a large dataset of Romanian instructions
  and benchmarks, including a novel cultural benchmark (RoCulturaBench), and trained
  RoLLMs based on Llama2, Mistral, Llama3, and Gemma architectures.
---

# "Vorbeşti Româneşte?" A Recipe to Train Powerful Romanian LLMs with English Instructions

## Quick Facts
- arXiv ID: 2406.18266
- Source URL: https://arxiv.org/abs/2406.18266
- Reference count: 20
- Primary result: First open-source Romanian LLMs trained with English instructions, achieving state-of-the-art performance across academic benchmarks, MT-Bench, and Romanian-specific tasks

## Executive Summary
This paper presents the first open-source Romanian Large Language Models (RoLLMs) trained using a recipe of continual pre-training and instruction fine-tuning. The authors collected and translated a large dataset of Romanian instructions and benchmarks, including a novel cultural benchmark (RoCulturaBench), and trained RoLLMs based on Llama2, Mistral, Llama3, and Gemma architectures. Extensive evaluation on academic benchmarks, MT-Bench, Romanian downstream tasks, and RoCulturaBench shows state-of-the-art performance across all categories. The models significantly outperform existing solutions, with RoMistral-7b-Instruct achieving an average score of 52.91 on academic benchmarks and 5.29 on MT-Bench. Human alignment using Direct Preference Optimization further improves performance. All resources, including data, code, and models, are publicly released to support further research on Romanian LLMs and other low-resource languages.

## Method Summary
The authors developed RoLLMs through a three-stage process: (1) continual pre-training on CulturaX, a large Romanian corpus, to build foundational models, (2) instruction fine-tuning on translated instruction and conversation datasets to create instruct models, and (3) human alignment using Direct Preference Optimization on translated preference data. The approach leverages existing multilingual architectures (Llama2, Mistral, Llama3, Gemma) and adapts them to Romanian through extensive data collection and translation efforts, including a novel cultural benchmark (RoCulturaBench) specifically designed for Romanian cultural knowledge evaluation.

## Key Results
- RoMistral-7b-Instruct achieves an average score of 52.91 on academic benchmarks and 5.29 on MT-Bench, outperforming all existing solutions
- RoLLMs achieve state-of-the-art performance across four evaluation categories: academic benchmarks, MT-Bench, Romanian downstream tasks, and RoCulturaBench
- Human alignment via Direct Preference Optimization further improves model performance
- All models, data, and code are publicly released to support further research on Romanian LLMs and other low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continual pre-training on culturally diverse Romanian data improves language grounding before instruction tuning
- Mechanism: Pre-training the model on a large, cleaned Romanian corpus (CulturaX) injects domain-specific linguistic patterns and cultural knowledge, which are then preserved and refined during instruction fine-tuning
- Core assumption: The pre-training data is representative and high-quality enough to provide meaningful linguistic and cultural context for Romanian
- Evidence anchors:
  - [abstract] "We collect and translate a large collection of texts, instructions, and benchmarks and train, evaluate, and release open-source LLMs tailored for Romanian"
  - [section] "Training the foundational model is done iteratively using continual pre-training on the CulturaX dataset"
  - [corpus] Weak corpus evidence; no explicit quality metrics cited for CulturaX
- Break condition: If pre-training data is noisy, sparse, or unrepresentative, the model may not gain sufficient cultural grounding

### Mechanism 2
- Claim: Instruction fine-tuning with translated datasets enables multilingual instruction-following capabilities
- Mechanism: Fine-tuning on a large set of translated instruction and conversation datasets (e.g., Alpaca, Dolly, GPT-Instruct) allows the model to follow Romanian instructions despite originating from English datasets
- Core assumption: Translations preserve the semantic intent and structure of the original instructions
- Evidence anchors:
  - [abstract] "We collect and translate a large set of evaluation benchmarks (including a human translation of MT-Bench), conversation, and instructions"
  - [section] "We collect a large and diverse dataset of conversations and instructions to build the instruct models... around 2.7M instructions and conversations in Romanian"
  - [corpus] Weak corpus evidence; translation quality and human verification not detailed
- Break condition: If translations are inaccurate or incomplete, the model may misinterpret instructions or fail to generalize

### Mechanism 3
- Claim: Human alignment via Direct Preference Optimization (DPO) improves conversational tone and human preference alignment
- Mechanism: Applying DPO on translated human preference data (HelpSteer) refines the model's outputs to better match human preferences in tone and style
- Core assumption: Translated preference data adequately captures human preferences in Romanian
- Evidence anchors:
  - [abstract] "Human alignment using Direct Preference Optimization further improves performance"
  - [section] "We decided to translate the HelpSteer dataset containing human preference data and use it to perform Direct Preference Optimization (DPO) on our models"
  - [corpus] Weak corpus evidence; no details on translation fidelity or human evaluation methodology
- Break condition: If preference data is misaligned or sparse, DPO may not improve conversational quality

## Foundational Learning

- Concept: Multilingual model adaptation
  - Why needed here: Romanian is a low-resource language; adapting existing multilingual models is more practical than training from scratch
  - Quick check question: Can the model generate grammatically correct Romanian sentences after fine-tuning?

- Concept: Cultural grounding in pre-training
  - Why needed here: Cultural knowledge is essential for tasks like RoCulturaBench, which evaluates Romanian-specific cultural understanding
  - Quick check question: Does the model correctly answer questions about Romanian history and traditions?

- Concept: Instruction-following in target language
  - Why needed here: The model must understand and execute Romanian instructions, not just generate Romanian text
  - Quick check question: Can the model follow multi-step Romanian instructions in a conversation?

## Architecture Onboarding

- Component map: CulturaX corpus -> foundational model -> translated instruction datasets -> instruct model -> translated preference data + DPO -> aligned instruct model -> evaluation across benchmarks

- Critical path: Pre-train foundational model -> fine-tune with instructions -> align with DPO -> evaluate across benchmarks

- Design tradeoffs:
  - Using translated data vs. native Romanian data (cost vs. quality)
  - Smaller sequence lengths for faster training vs. longer sequences for better context
  - Instruction-only fine-tuning vs. including multi-turn conversations

- Failure signatures:
  - Model generates mostly English text despite Romanian prompts (translation leakage)
  - Poor performance on cultural benchmarks (insufficient pre-training data)
  - Inconsistent instruction-following (low-quality or misaligned instruction data)

- First 3 experiments:
  1. Pre-train RoLlama2 on 5% of CulturaX with sequence length 512; evaluate on a small Romanian downstream task
  2. Fine-tune RoLlama2-5% with 100k translated instructions; test Romanian MT-Bench performance
  3. Apply DPO on RoLlama2-Instruct with 10k translated preference samples; compare RoCulturaBench scores before/after

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal amount of pre-training data needed for Romanian LLMs to achieve state-of-the-art performance on downstream tasks?
- Basis in paper: Inferred from the results showing that RoLlama2-7b-Base (1024-20%) performed slightly worse than RoLlama2-7b-Base (1024-5%) and (1024-10%) on academic benchmarks, despite using 20% of the CulturaX dataset compared to 5% and 10%
- Why unresolved: The paper does not explore the relationship between pre-training data size and downstream task performance in detail. It only compares three variants of RoLlama2-7b-Base trained on different amounts of data
- What evidence would resolve it: Conducting experiments with RoLLMs trained on varying amounts of pre-training data and evaluating their performance on downstream tasks would provide insights into the optimal data size

### Open Question 2
- Question: How does the quality of pre-training data impact the performance of Romanian LLMs on downstream tasks?
- Basis in paper: Explicit from the statement that "CulturaX contains data from different sources, including low-quality sources," and the observation that "adding more pre-training data (up to 20% of our data) slightly degrades the foundational model's performance"
- Why unresolved: The paper does not analyze the impact of data quality on model performance. It only mentions that CulturaX contains low-quality data without further investigation
- What evidence would resolve it: Analyzing the performance of RoLLMs trained on different subsets of CulturaX data with varying quality levels would reveal the impact of data quality on downstream task performance

### Open Question 3
- Question: What is the optimal sequence length for pre-training Romanian LLMs to achieve the best performance on downstream tasks?
- Basis in paper: Inferred from the results showing that RoLlama2-7b-Base (1024-5%) performed better than RoLlama2-7b-Base (512-5%) and (2048-5%) on academic benchmarks, despite using the same amount of data
- Why unresolved: The paper only compares three sequence lengths (512, 1024, and 2048) and does not explore the relationship between sequence length and downstream task performance in detail
- What evidence would resolve it: Conducting experiments with RoLLMs trained on different sequence lengths and evaluating their performance on downstream tasks would provide insights into the optimal sequence length

### Open Question 4
- Question: How does the inclusion of multi-turn conversations in the instruction fine-tuning dataset impact the performance of Romanian LLMs on MT-Bench?
- Basis in paper: Explicit from the observation that "this is somewhat expected as the instruction fine-tuning dataset contains, for the most part, single-turn conversations," and the recommendation to "Adding more multi-turn conversations in the fine-tuning dataset should also close the performance gap on the second turn"
- Why unresolved: The paper does not experiment with multi-turn conversations in the instruction fine-tuning dataset and only mentions its potential impact on MT-Bench performance
- What evidence would resolve it: Training RoLLMs on instruction fine-tuning datasets with varying amounts of multi-turn conversations and evaluating their performance on MT-Bench would reveal the impact of multi-turn conversations on model performance

## Limitations
- Translation quality uncertainties may affect instruction-following and conversational abilities
- RoCulturaBench dataset lacks transparency regarding prompt design and reference answers
- Evaluation focuses on benchmark performance rather than real-world deployment scenarios

## Confidence
- High Confidence: The general methodology of continual pre-training followed by instruction fine-tuning is well-established and theoretically sound for low-resource language adaptation
- Medium Confidence: The reported benchmark performance improvements are plausible given the extensive training approach, though translation quality uncertainties introduce moderate doubt
- Low Confidence: The specific claims about cultural grounding and human preference alignment are weakly supported due to insufficient detail on data quality verification and evaluation methodology

## Next Checks
1. Conduct a human evaluation of 100 randomly sampled translated instructions and preference pairs to assess semantic preservation and cultural appropriateness
2. Request and publish the complete RoCulturaBench dataset including prompts, reference answers, and evaluation rubrics for independent verification
3. Evaluate the models on parallel English-Romanian tasks to measure whether improvements are specific to Romanian or reflect general multilingual capabilities