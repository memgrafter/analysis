---
ver: rpa2
title: A Survey of Generative Search and Recommendation in the Era of Large Language
  Models
arxiv_id: '2404.16924'
source_url: https://arxiv.org/abs/2404.16924
tags:
- generative
- recommendation
- search
- user
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first comprehensive survey of the emerging
  generative paradigm for search and recommendation, which aims to address the matching
  problem between queries and documents or users and items through direct generation
  using large language models (LLMs). Unlike traditional discriminative methods, generative
  search and recommendation formulate the task as an autoregressive generation problem,
  where LLMs directly generate document/item identifiers given queries or user formulations.
---

# A Survey of Generative Search and Recommendation in the Era of Large Language Models

## Quick Facts
- arXiv ID: 2404.16924
- Source URL: https://arxiv.org/abs/2404.16924
- Reference count: 40
- Primary result: First comprehensive survey of generative search and recommendation using large language models

## Executive Summary
This paper provides the first comprehensive survey of the emerging generative paradigm for search and recommendation. Unlike traditional discriminative methods, generative approaches formulate matching problems as autoregressive generation tasks where LLMs directly generate document/item identifiers given user queries or formulations. The survey introduces a unified four-stage framework (query/user formulation, identifier design, training, inference) to categorize existing works and highlights both commonalities and unique challenges between generative search and recommendation.

## Method Summary
The authors conducted a comprehensive literature review of 40 papers on generative search and recommendation, analyzing them through a unified framework that categorizes approaches into four key stages: query/user formulation, document/item identifier design, training, and inference. The survey synthesizes findings across both domains, identifying commonalities, unique challenges, and open problems in scaling these methods to large corpora while maintaining performance and efficiency.

## Key Results
- Generative search and recommendation convert matching problems into sequence generation tasks
- Four-stage unified framework captures the key aspects of generative approaches
- Identifier design (numeric IDs, titles, n-grams, codebooks) critically impacts performance and scalability
- Constrained generation using Trie/FM-index structures ensures valid identifier output
- Open challenges include document/item updates, multimodal retrieval, and large-scale corpus handling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative search/recommendation converts the matching problem into a sequence generation task, enabling end-to-end optimization without hand-crafted similarity metrics.
- Mechanism: By framing the task as "generate item/document identifier from user/query input," the model learns to directly produce outputs that correspond to relevant items/documents, bypassing explicit ranking or similarity computation.
- Core assumption: The generative model can memorize or infer the mapping between user inputs and identifiers through training on paired examples.
- Evidence anchors:
  - [abstract] "generative search and recommendation formulate the task as an autoregressive generation problem, where LLMs directly generate document/item identifiers given queries or user formulations."
  - [section 3.1] "generative search and recommendation aim to directly generate the target document or item to satisfy users' information needs."
- Break condition: If the model cannot effectively memorize the document-item mappings due to limited interactions (search) or very large item spaces, performance degrades sharply.

### Mechanism 2
- Claim: Identifier design (numeric ID, titles, n-grams, codebook, multiview) balances distinctiveness, semantic meaning, and updateability to enable effective retrieval.
- Mechanism: Identifiers act as a compressed, model-friendly representation of documents/items. Different types trade off semantic richness vs. memorization ease and corpus update capability.
- Core assumption: The chosen identifier type can be generated from user input and mapped back to the original document/item with high fidelity.
- Evidence anchors:
  - [section 4.3] "current generative search approaches often rely on the use of identifiers to represent documents...summarize the current identifiers in generative search methods, and analyze their advantages and disadvantages."
  - [section 5.3] "a good item identifier should at least meet two criteria...distinctiveness to emphasize the salient item features...and semantics to focus on the utilization of prior knowledge in pre-trained language models."
- Break condition: When identifier distinctiveness is too low (e.g., n-grams), retrieval requires expensive transformation functions; when too high (e.g., numeric IDs), generalization to unseen data suffers.

### Mechanism 3
- Claim: Constrained generation using Trie/FM-index guarantees valid identifier output, overcoming the "out-of-corpus" generation problem.
- Mechanism: During inference, the model is restricted to generate only tokens that appear in valid identifiers via data structures that enforce prefix or position-free constraints, ensuring every generated string corresponds to a real document/item.
- Core assumption: The identifier set is fixed and known at inference time, allowing construction of efficient constraint structures.
- Evidence anchors:
  - [section 4.5] "most generative search approaches employ constrained generation to guarantee the LLM generates valid identifiers...Trie and FM_index...The FM_index enables the LLM to generate valid tokens from any position within the identifier, while the Trie only supports generation from the first token."
- Break condition: If the corpus changes frequently without retraining or updating the constraint structures, the system cannot adapt without significant overhead.

## Foundational Learning

- Concept: Autoregressive sequence generation
  - Why needed here: Generative search/recommendation relies on autoregressive decoding to produce identifiers token-by-token conditioned on user input.
  - Quick check question: In an autoregressive model, how is the next token predicted at each step?

- Concept: Beam search and constrained decoding
  - Why needed here: Efficient and valid identifier generation requires beam search to explore multiple candidate sequences while constrained decoding ensures only valid identifiers are produced.
  - Quick check question: What is the difference between free generation and constrained generation in the context of generative retrieval?

- Concept: Vector quantization and codebooks
  - Why needed here: Codebook-based identifiers convert semantic item representations into discrete token sequences, enabling semantic understanding while maintaining distinctiveness.
  - Quick check question: How does a learned codebook differ from using raw numeric IDs as identifiers?

## Architecture Onboarding

- Component map:
  - User/query input → User formulation (recommendation) / Direct input (search)
  - Identifier generation module (autoregressive model + constrained decoding)
  - Identifier-to-document/item mapping (lookup via Trie/FM-index)
  - Output ranking/selection

- Critical path:
  1. Formulate user/query as text
  2. Generate candidate identifiers via autoregressive decoding
  3. Apply constrained generation to ensure validity
  4. Map identifiers to actual documents/items
  5. Return ranked results

- Design tradeoffs:
  - Identifier type: Numeric IDs (high distinctiveness, low semantic) vs. titles/n-grams (semantic, lower distinctiveness) vs. codebooks (balanced but complex)
  - Generation mode: Free generation (faster, risk of invalid outputs) vs. constrained generation (slower, guaranteed valid)
  - Corpus size handling: Large corpora increase memorization difficulty for generative models

- Failure signatures:
  - Out-of-corpus generations → constraint structures outdated or missing
  - Low recall → identifier distinctiveness insufficient, beam search size too small
  - Poor generalization → reliance on numeric IDs without semantic grounding

- First 3 experiments:
  1. Train a small generative model on a toy dataset with numeric ID identifiers; evaluate free vs. constrained generation.
  2. Swap numeric IDs for title-based identifiers; measure impact on semantic relevance and generation speed.
  3. Implement Trie-based constrained decoding; test recall@10 on a held-out query set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can generative search methods achieve competitive performance with traditional retrieval methods in large-scale corpora (millions of passages)?
- Basis in paper: [explicit] The paper mentions that while generative search is competitive with state-of-the-art dual encoders on small corpora, scaling to millions of passages remains an important and unsolved challenge.
- Why unresolved: Current generative search methods rely on memorizing associations between documents and identifiers in the LLM's parameters, which becomes increasingly difficult as the corpus size grows. The computational resources required for training and inference also increase significantly.
- What evidence would resolve it: Empirical studies comparing the performance of generative search methods against traditional retrieval methods on large-scale corpora (e.g., with millions of passages) using standard evaluation metrics like recall and latency.

### Open Question 2
- Question: How can we effectively update LLMs to handle new documents or items without requiring full retraining?
- Basis in paper: [explicit] The paper highlights the challenge of updating LLMs for new documents or items, as they are trained to memorize associations between existing documents/items and their identifiers. Retraining the entire LLM is computationally expensive and impractical for daily updates.
- Why unresolved: Current methods like incremental learning only partially address the passage-adding problem and are not sufficient for handling a large volume of daily updates in search and recommendation systems.
- What evidence would resolve it: Development and evaluation of efficient methods for updating LLMs with new documents or items, such as parameter-efficient fine-tuning techniques, retrieval augmentation, or other approaches that minimize the need for full retraining.

### Open Question 3
- Question: Can in-context learning be effectively applied to generative search and recommendation tasks to reduce the need for fine-tuning?
- Basis in paper: [explicit] The paper discusses the potential of in-context learning for generative search and recommendation, but notes that current methods still rely on fine-tuning LLMs with domain-specific data. As LLM sizes increase, fine-tuning becomes computationally expensive.
- Why unresolved: The effectiveness of in-context learning for complex tasks like search and recommendation, which often require understanding user preferences and item characteristics, is not yet fully explored. The optimal prompt design and the amount of in-context examples needed are also open questions.
- What evidence would resolve it: Empirical studies comparing the performance of in-context learning versus fine-tuning for generative search and recommendation tasks, using various prompt designs and numbers of in-context examples. Evaluation should be done on standard datasets and compared to state-of-the-art fine-tuned models.

## Limitations

- Survey comprehensiveness depends on completeness of literature coverage, particularly for emerging works post-2023
- Framework's applicability across all generative search and recommendation variants needs empirical validation
- Trade-off analysis between identifier types lacks quantitative comparison data

## Confidence

- **High**: The fundamental mechanism of converting matching problems to generation tasks is well-supported by cited works
- **Medium**: The framework's ability to capture all critical aspects of generative search and recommendation is reasonable but untested
- **Low**: Predictions about future paradigms (direct content generation) are speculative and lack empirical grounding

## Next Checks

1. **Framework Validation**: Apply the four-stage framework to 5-10 additional papers not included in the survey to test completeness and consistency
2. **Identifier Type Comparison**: Conduct controlled experiments comparing recall@10 across different identifier types (numeric IDs, titles, codebooks) on the same dataset
3. **Constraint Structure Efficiency**: Benchmark Trie vs. FM-index constrained generation on large identifier sets to quantify speed-accuracy trade-offs