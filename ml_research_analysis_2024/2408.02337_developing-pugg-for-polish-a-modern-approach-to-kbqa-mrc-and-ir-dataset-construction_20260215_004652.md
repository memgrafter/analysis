---
ver: rpa2
title: 'Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset
  Construction'
arxiv_id: '2408.02337'
source_url: https://arxiv.org/abs/2408.02337
tags:
- questions
- question
- dataset
- kbqa
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PUGG, the first Polish KBQA dataset, along
  with MRC and IR datasets. It proposes a semi-automated pipeline leveraging LLMs
  and existing datasets to reduce human annotation effort.
---

# Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction

## Quick Facts
- arXiv ID: 2408.02337
- Source URL: https://arxiv.org/abs/2408.02337
- Reference count: 25
- Key outcome: Introduces PUGG, the first Polish KBQA dataset, along with MRC and IR datasets, using a semi-automated pipeline that leverages LLMs to reduce human annotation effort while maintaining dataset quality.

## Executive Summary
This paper introduces PUGG, the first comprehensive dataset for Polish that supports three key NLP tasks: Knowledge Base Question Answering (KBQA), Machine Reading Comprehension (MRC), and Information Retrieval (IR). The dataset is constructed using a semi-automated pipeline that leverages Large Language Models (LLMs) and existing datasets to significantly reduce human annotation effort while maintaining high data quality. The pipeline generates both natural and template-based questions, retrieves and segments passages, and performs entity linking and answer extraction with human verification at critical steps. The resulting dataset contains 3,262 questions covering 12,984 entities and demonstrates the feasibility of efficient multi-task dataset construction for low-resource languages.

## Method Summary
The PUGG dataset is constructed through a semi-automated pipeline that begins with question formulation using prefixes extracted from existing QA datasets, completed via Google Suggest and LLM-based methods. For KBQA and MRC tasks, Wikipedia passages are retrieved and segmented into fixed-size windows with sliding overlap to ensure answer spans are preserved. Answer extraction is performed through LLM-based tagging followed by custom span extraction and heuristic entity linking to Wikidata entities. The pipeline generates both natural questions and template-based questions, with the latter constructed using SPARQL templates combined with entities and relations from Wikidata's Vital Articles. Human verification occurs in two stages: first validating passage and answer correctness, then verifying entity annotations. This approach allows the creation of all three datasets (KBQA, MRC, IR) simultaneously while drastically reducing annotation labor.

## Key Results
- KBQA accuracy ranges from 0.210 to 0.674 depending on KG complexity and question type, with template-based questions achieving significantly higher performance (0.674) than natural questions (0.342)
- MRC baselines achieve exact match of 42.91-46.81% and F1 of 66.41-70.42% across different passage selection strategies
- IR models perform strongly with multilingual-e5-large+reranker reaching MRR@10 of 0.770 and NDCG@10 of 0.813
- The semi-automated pipeline reduces human annotation workload while maintaining dataset quality through targeted verification

## Why This Works (Mechanism)

### Mechanism 1
LLM-assisted pipeline drastically reduces human annotation workload while maintaining dataset quality. The semi-automation splits labor between LLM-generated candidates and targeted human verification, filtering out noise early and focusing human effort only on ambiguous or error-prone steps. This approach assumes LLMs can generate sufficiently diverse and high-quality candidate questions, passages, and annotations that only minor human correction is needed.

### Mechanism 2
Simultaneous construction of KBQA, MRC, and IR datasets from the same source questions increases efficiency. A single pipeline stage produces reusable outputs for all three tasks, avoiding duplication of effort. The shared intermediate data (questions, passages, answer entities) feeds into all three task datasets, enabling efficient multi-task dataset construction.

### Mechanism 3
Template-based questions complement natural questions by providing simpler, more structured reasoning paths for model training and evaluation. Templates inject both entities and relations into question templates, allowing controlled complexity and reasoning diversity not guaranteed by natural question generation. This controlled approach ensures valid reasoning paths while natural questions improve realism.

## Foundational Learning

- Concept: Entity linking in low-resource languages
  - Why needed here: Polish lacks robust entity linking tools; custom heuristic needed to map question text to KG entities
  - Quick check question: How does the pipeline identify candidate topic entities from a Polish question without a pre-trained entity linker?

- Concept: SPARQL query construction for KBQA
  - Why needed here: Template-based questions require correct SPARQL queries to retrieve answers from Wikidata
  - Quick check question: What SPARQL pattern is used to retrieve an entity's direct property value (one-hop)?

- Concept: Sliding window passage segmentation
  - Why needed here: Wikipedia articles are split into fixed-size passages for retrieval and MRC tasks
  - Quick check question: How does the pipeline ensure no answer spans are split across passage boundaries?

## Architecture Onboarding

- Component map: Question Formulation -> Prefix extraction -> Question completion (Google Suggest / LLM) -> Passage Construction -> Wikipedia retrieval -> Sliding window segmentation -> Reranking -> Answer/Entity Extraction -> LLM tagging -> Custom span extraction -> Heuristic entity linking -> Human Verification -> Two-stage annotation (passage/answer correctness, then entity annotation) -> Template Generation -> SPARQL template + entity/relation injection -> Inflection + paraphrasing -> Verification

- Critical path: Question formulation -> Passage retrieval -> Answer tagging -> Human verification -> Dataset finalization

- Design tradeoffs:
  - Automation vs. quality: Higher automation reduces cost but may introduce noise; human verification mitigates but adds cost
  - Natural vs. template questions: Natural questions improve realism; templates improve controllability and training efficiency
  - KG complexity: Wikidata1H vs Wikidata2H trade-off between ease of reasoning and dataset richness

- Failure signatures:
  - Low human verification pass rate -> LLM candidates too noisy
  - Missing answer entities in passages -> Retrieval or tagging failure
  - High deduplication rate -> Question formulation over-generates similar queries

- First 3 experiments:
  1. Run pipeline on a small seed of 50 questions; measure human verification pass rate and time per example
  2. Compare accuracy of template-based vs natural questions on a held-out validation set
  3. Evaluate retrieval quality (Recall@K) before and after passage reranking step

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed semi-automated pipeline's accuracy compare to fully manual annotation methods for low-resource languages? The paper emphasizes the pipeline drastically reduces human annotation labor while maintaining high dataset quality, but doesn't provide direct comparison of accuracy between semi-automated and fully manual approaches for the same dataset.

### Open Question 2
What is the long-term sustainability of the pipeline as knowledge graphs evolve and new entity linking tools become available? The paper mentions Wikidata is continuously updated and the entity linking step faces challenges due to lack of robust tools for Polish, but doesn't address how the pipeline would adapt to changes in knowledge graph structure or improvements in entity linking technology over time.

### Open Question 3
How does the performance gap between natural and template-based questions in KBQA change with increased training data? The paper shows template-based questions achieve significantly higher accuracy (0.674 vs 0.342) than natural questions, but doesn't explore whether this performance gap narrows or widens with more training examples, particularly for natural questions.

## Limitations
- The paper does not fully specify the custom entity linking method used, which is critical for accurate topic entity identification in Polish
- The exact model configurations for passage retrieval and reranking are not provided, making it difficult to reproduce the exact results
- The evaluation focuses primarily on the KBQA task, with less detail on MRC and IR performance, potentially limiting the dataset's utility across all three tasks

## Confidence

- High confidence in the overall dataset construction pipeline and its efficiency gains through semi-automation
- Medium confidence in the reported KBQA accuracy metrics, as they depend on the specific KG complexity and question types used
- Low confidence in the reproducibility of the entity linking and retrieval components without further technical details

## Next Checks

1. Evaluate the entity linking method's accuracy on a held-out Polish dataset to assess its robustness in low-resource settings
2. Compare the MRC and IR performance across multiple baseline models to validate the dataset's utility for these tasks
3. Test the pipeline's scalability by running it on a larger seed set of questions and measuring the trade-off between automation and data quality