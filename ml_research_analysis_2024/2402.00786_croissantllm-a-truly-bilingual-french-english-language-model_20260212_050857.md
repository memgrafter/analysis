---
ver: rpa2
title: 'CroissantLLM: A Truly Bilingual French-English Language Model'
arxiv_id: '2402.00786'
source_url: https://arxiv.org/abs/2402.00786
tags:
- data
- language
- french
- training
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The CroissantLLM project addresses the dominance of English in
  large language models by training a 1.3B parameter bilingual French-English model
  on a 1:1 token ratio of both languages. Using a custom tokenizer optimized for bilingual
  use and training on 3 trillion tokens, the model achieves strong performance in
  both languages while being highly efficient for local deployment.
---

# CroissantLLM: A Truly Bilingual French-English Language Model

## Quick Facts
- **arXiv ID**: 2402.00786
- **Source URL**: https://arxiv.org/abs/2402.00786
- **Reference count**: 40
- **Primary result**: 1.3B parameter bilingual model achieving strong French-English performance with efficient local deployment

## Executive Summary
CroissantLLM addresses the English-centric bias in large language models by training a truly bilingual French-English model with a 1:1 token ratio between languages. The model uses a custom tokenizer optimized for bilingualism and is trained on 3 trillion tokens, achieving strong performance in both languages while being highly efficient for local deployment. It outperforms monolingual models of similar size on French benchmarks and matches top English baselines, demonstrating the value of balanced multilingual pretraining.

## Method Summary
The model is a 1.3B parameter decoder-only transformer trained on a balanced 1:1 English-French corpus using a custom tokenizer optimized for bilingualism. Training employed Megatron-DeepSpeed on A100 GPUs for 17 days, with rotary position encodings and 2048 context length. The training corpus includes filtered web data from French and English sources, code data, and parallel corpora. The work introduces FrenchBench, a novel evaluation suite for French LLMs, and releases the full training pipeline and artifacts.

## Key Results
- Achieves 2x improvement over Llama-2 7B on French WinoMatrix benchmark
- Matches top English baselines while maintaining efficient local deployment capabilities
- Introduces FrenchBench evaluation suite with 8 tasks covering multiple French linguistic capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A 1:1 English-French pretraining data ratio reduces performance disparity between the two languages.
- Mechanism: By balancing the language distribution in the pretraining corpus, the model allocates more capacity to French, reducing the typical English bias seen in most LLMs.
- Core assumption: The capacity of a multilingual model can be effectively shared between languages, and a balanced data mix optimizes this sharing.
- Evidence anchors:
  - [abstract]: "pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio"
  - [section]: "training on an equal mixture of English and French data may seem to be the obvious solution"
  - [corpus]: Weak; the corpus details token counts but doesn't explicitly validate the 1:1 ratio's impact on performance balance.

### Mechanism 2
- Claim: A custom tokenizer optimized for bilingual use improves model efficiency and performance in both languages.
- Mechanism: The tokenizer is trained on a balanced English-French corpus, leading to better subword splits and lower fertility rates in French compared to standard tokenizers.
- Core assumption: Tokenization quality directly impacts model performance, especially for morphologically rich languages like French.
- Evidence anchors:
  - [abstract]: "a custom tokenizer, and bilingual finetuning datasets"
  - [section]: "This enables us to pack more data in fewer tokens, leading to improvements in training and inference efficiency"
  - [corpus]: Weak; the corpus section mentions tokenizer training but doesn't provide direct evidence of its impact on fertility rates.

### Mechanism 3
- Claim: Extensive pretraining (3 trillion tokens) past Chinchilla optimality improves downstream task performance.
- Mechanism: Continuing pretraining beyond the compute-optimal threshold allows the model to absorb more knowledge and improve on specific tasks, even if perplexity doesn't significantly improve.
- Core assumption: Model performance on downstream tasks can continue to improve with more training data, even after perplexity plateaus.
- Evidence anchors:
  - [abstract]: "training a model on a 1:1 ratio of English to French with a tokenizer optimized for bilingualism"
  - [section]: "model performance on downstream tasks continues to dramatically improve with lengthened pre-training runs, although model perplexity does not significantly improve"
  - [corpus]: Weak; the corpus details the training data but doesn't explicitly validate the impact of extensive pretraining on performance.

## Foundational Learning

- **Multilingual model training and scaling laws**: Understanding how model performance scales with data and model size is crucial for optimizing the training process and achieving desired performance levels in both languages. *Quick check*: How does the effective capacity ratio change when the language distribution in the pretraining data is varied?

- **Tokenizer design and fertility rates**: The tokenizer's ability to efficiently represent words in both languages directly impacts model performance and efficiency. *Quick check*: How does the fertility rate of the custom tokenizer compare to standard tokenizers on French text?

- **Downstream task evaluation and benchmarking**: Properly evaluating the model's performance on a variety of tasks in both languages is essential for understanding its capabilities and limitations. *Quick check*: What metrics are used to evaluate the model's performance on French language tasks, and how do they compare to English tasks?

## Architecture Onboarding

- **Component map**: Custom tokenizer -> 1.3B parameter decoder transformer (Llama architecture) -> Balanced English-French corpus -> Rotary position encodings -> Downstream evaluation
- **Critical path**: Data collection and curation -> Tokenizer design and training -> Model architecture and hyperparameter selection -> Pretraining on balanced corpus -> Evaluation on downstream tasks
- **Design tradeoffs**: The main tradeoff is between model size and inference efficiency. A larger model may achieve better performance but would be more computationally expensive to run. The custom tokenizer improves efficiency but may require more engineering effort.
- **Failure signatures**: Failure to achieve balanced performance between English and French could indicate issues with the data distribution, tokenizer, or model architecture. Poor performance on downstream tasks could indicate insufficient pretraining or issues with the evaluation benchmarks.
- **First 3 experiments**:
  1. Train a small model (e.g., 100M parameters) on a subset of the English-French corpus with varying language ratios to study the impact on performance.
  2. Evaluate the custom tokenizer's fertility rates on French text compared to standard tokenizers.
  3. Fine-tune the pretrained model on a few downstream tasks in both languages to assess its transfer learning capabilities.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of CroissantLLM compare to larger multilingual models when evaluated on a broader set of non-European languages?
- **Open Question 2**: What is the optimal ratio of code data to natural language data for improving reasoning and STEM performance in bilingual models?
- **Open Question 3**: How does the performance of CroissantLLM scale with model size beyond 1.3B parameters while maintaining the 1:1 English-French data ratio?
- **Open Question 4**: What is the impact of including culturally diverse data from non-Western sources on the model's ability to generalize cultural knowledge?
- **Open Question 5**: How does the model's performance on translation tasks compare when fine-tuned on domain-specific parallel data versus general parallel data?

## Limitations

- Weak empirical validation of core mechanisms, with indirect evidence linking design choices to performance outcomes
- Limited evaluation on languages beyond French and English, lacking comparison with established multilingual benchmarks
- Claims about extensive pretraining benefits lack strong validation against different training duration baselines

## Confidence

- **High confidence**: Technical implementation details and reproducibility aspects are well-specified with released training artifacts
- **Medium confidence**: Performance improvements over baselines are demonstrated but attribution to specific design choices remains weakly supported
- **Low confidence**: Claims about long-term benefits of extensive pretraining beyond Chinchilla optimality are not strongly validated

## Next Checks

1. **Language-specific loss analysis**: Monitor and compare token-level loss curves for English and French during training to empirically validate whether the 1:1 ratio maintains balanced language performance.

2. **Tokenizer fertility rate benchmarking**: Measure and compare the custom tokenizer's average French word segmentation fertility against standard tokenizers like GPT-2 on a held-out French corpus.

3. **Ablation study on training ratio**: Train controlled model variants with different English-French token ratios (e.g., 2:1, 1:2, 1:1) and measure their impact on bilingual performance.