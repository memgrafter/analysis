---
ver: rpa2
title: Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal
  Choice
arxiv_id: '2405.19313'
source_url: https://arxiv.org/abs/2405.19313
tags:
- human
- data
- language
- embeddings
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors trained a small language model (Arithmetic-GPT) on
  synthetic arithmetic datasets to explore whether arithmetic reasoning alone can
  explain human decision-making biases in risky and intertemporal choice. By pretraining
  on ecologically valid probability and value distributions, Arithmetic-GPT embeddings
  predicted human choices better than traditional behavioral models and matched or
  outperformed larger general-purpose LLMs.
---

# Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice

## Quick Facts
- **arXiv ID:** 2405.19313
- **Source URL:** https://arxiv.org/abs/2405.19313
- **Reference count:** 35
- **Primary result:** A small language model trained solely on synthetic arithmetic data predicts human risky and intertemporal choices better than traditional behavioral models, matching or outperforming larger LLMs.

## Executive Summary
This study investigates whether arithmetic reasoning alone, without natural language supervision, can account for human decision-making biases in risky and intertemporal choice. The authors pretrain a small language model (Arithmetic-GPT) on synthetic datasets of probabilities and monetary values drawn from ecologically valid distributions. They find that embeddings from this arithmetic-only model predict human choices in classic behavioral tasks as well as or better than larger, general-purpose LLMs and standard behavioral models. The results suggest that embedding quality depends critically on the statistical properties of pretraining data, and that arithmetic reasoning is sufficient to produce human-like decision biases. This work highlights the potential of LLMs as cognitive models when trained on computationally equivalent tasks, while underscoring the need for careful control and disclosure of pretraining data.

## Method Summary
The authors pretrain a small language model (Arithmetic-GPT) on synthetic datasets of arithmetic problems involving probabilities and monetary values, using distributions designed to reflect those encountered in real-world decision contexts. They then use the model’s embeddings to predict human choices in classic behavioral economics tasks (risky and intertemporal choice). The model is compared against standard behavioral models and larger LLMs (GPT-3.5, GPT-4) on held-out datasets. Ablation and distribution shift analyses are performed to test the robustness of the results and the importance of pretraining data distributions.

## Key Results
- Arithmetic-GPT embeddings predict human choices in risky and intertemporal tasks as well as or better than traditional behavioral models and larger LLMs.
- The model’s performance depends critically on the distributions used in pretraining, with ecologically valid distributions yielding the best predictions.
- Arithmetic reasoning alone, without natural language supervision, is sufficient to produce human-like decision biases.

## Why This Works (Mechanism)
The model’s success stems from learning rich numerical representations during pretraining on ecologically valid arithmetic data. These embeddings capture the statistical regularities of probabilities and values that underlie human decision-making, allowing the model to generalize to behavioral tasks without requiring explicit linguistic or contextual information. The key mechanism is the alignment between the statistical structure of the pretraining data and the structure of human choices.

## Foundational Learning
- **Arithmetic reasoning:** Needed to generate embeddings that encode numerical relationships relevant to decision-making. Quick check: model solves synthetic arithmetic problems accurately.
- **Distribution matching:** Aligning pretraining data distributions with real-world probability and value distributions is critical for capturing human biases. Quick check: ablation studies show performance drops when distributions are mismatched.
- **Embedding extraction:** The quality of the model’s embeddings directly determines its ability to predict choices. Quick check: embeddings correlate with human choice patterns.

## Architecture Onboarding

### Component Map
Arithmetic-GPT (small LLM) -> Arithmetic pretraining on synthetic data -> Embedding extraction -> Choice prediction

### Critical Path
Pretraining data distribution -> Model embeddings -> Behavioral prediction accuracy

### Design Tradeoffs
- Smaller, arithmetic-only model vs. larger, general-purpose LLM: smaller model is more interpretable and easier to control, but may lack broad generalization.
- Synthetic vs. natural pretraining data: synthetic data allows precise control but may miss real-world complexity.

### Failure Signatures
- Poor performance when pretraining distributions do not match real-world distributions.
- Inability to generalize beyond trained arithmetic domains.

### First 3 Experiments to Try
1. Train Arithmetic-GPT on alternative synthetic distributions (e.g., uniform, heavy-tailed) and test on behavioral tasks.
2. Compare embeddings from Arithmetic-GPT to those from a standard language model on held-out choice data.
3. Apply the model to new behavioral paradigms (e.g., framing, anchoring) to test generalizability.

## Open Questions the Paper Calls Out
None

## Limitations
- The synthetic pretraining corpus may not fully capture the breadth of real-world probability and value distributions.
- Results are limited to classic risky and intertemporal choice tasks; generalizability to other biases is untested.
- Comparison with larger LLMs is constrained by proprietary models and lack of transparency in pretraining data.

## Confidence

| Claim | Confidence |
| --- | --- |
| Arithmetic reasoning alone can produce human-like decision biases (within tested scope) | Medium |
| Pretraining data distributions are critical for embedding quality | High |
| Arithmetic-GPT outperforms traditional behavioral models (within tested tasks) | High |

## Next Checks
1. Test Arithmetic-GPT and similar models on a wider range of behavioral economics tasks (e.g., framing effects, anchoring, social preferences) to assess generalizability.
2. Conduct ablation studies with alternative synthetic pretraining corpora to determine the robustness of results to different probability and value distributions.
3. Replicate findings using open-weight LLMs trained from scratch on controlled arithmetic datasets, to reduce uncertainty about pretraining data and isolate the effects of model architecture and training procedure.