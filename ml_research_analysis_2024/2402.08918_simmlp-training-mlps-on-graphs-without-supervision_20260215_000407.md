---
ver: rpa2
title: 'SimMLP: Training MLPs on Graphs without Supervision'
arxiv_id: '2402.08918'
source_url: https://arxiv.org/abs/2402.08918
tags:
- graph
- simmlp
- learning
- node
- mlps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SimMLP addresses the challenge of deploying graph neural networks
  (GNNs) in latency-sensitive applications by training MLPs to capture structural
  information from graphs without relying on neighborhood aggregation. The core idea
  is to align GNNs and MLPs in the embedding space using self-supervised learning,
  enabling MLPs to learn fine-grained correlations between node features and graph
  structures.
---

# SimMLP: Training MLPs on Graphs without Supervision

## Quick Facts
- arXiv ID: 2402.08918
- Source URL: https://arxiv.org/abs/2402.08918
- Authors: Zehong Wang; Zheyuan Zhang; Chuxu Zhang; Yanfang Ye
- Reference count: 40
- One-line primary result: SimMLP trains MLPs to capture structural information from graphs without supervision, achieving 90-126× inference acceleration over GNNs while maintaining competitive accuracy.

## Executive Summary
SimMLP addresses the challenge of deploying graph neural networks (GNNs) in latency-sensitive applications by training MLPs to capture structural information from graphs without relying on neighborhood aggregation. The core idea is to align GNNs and MLPs in the embedding space using self-supervised learning, enabling MLPs to learn fine-grained correlations between node features and graph structures. To prevent trivial solutions, SimMLP employs two strategies: approximating GNNs using MLPs to enhance consistency and augmenting data to improve diversity. Theoretical analysis shows that SimMLP is equivalent to GNNs in the optimal case and shares key inductive biases like homophily and local structure importance. Experiments on 20 benchmark datasets demonstrate that SimMLP significantly outperforms state-of-the-art baselines, especially in settings with unseen nodes, achieving 7-26% performance gains over MLPs and 90-126× inference acceleration over GNNs.

## Method Summary
SimMLP uses self-supervised learning to align representations encoded by graph context-aware GNNs and neighborhood dependency-free MLPs. The method consists of two main components: an invariance term that minimizes the distance between projected MLP and GNN embeddings, and a reconstruction term that minimizes the difference between GNN embeddings and original features. To prevent trivial solutions, SimMLP employs two strategies: approximating GNNs using MLPs to enhance consistency, and data augmentation to improve diversity. The approximation strategy simulates GNN message-passing using MLPs, while the augmentation strategy generates multi-views of nodes through random edge masking and node feature masking. Theoretical analysis demonstrates that SimMLP is equivalent to GNNs in the optimal case and shares key inductive biases.

## Key Results
- Achieves 90-126× inference acceleration over GNNs while maintaining competitive accuracy
- Outperforms state-of-the-art baselines by 7-26% on unseen nodes in cold-start scenarios
- Demonstrates equivalence to GNNs in the optimal case through mutual information maximization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SimMLP achieves equivalence to GNNs in the optimal case by aligning GNN and MLP embeddings in the embedding space.
- Mechanism: The self-supervised alignment loss maximizes the consistency between GNN and MLP embeddings, capturing fine-grained and generalizable correlations between node features and graph structures.
- Core assumption: Maximizing mutual information between GNNs and MLPs in the embedding space preserves the essential structural information needed for node classification.
- Evidence anchors:
  - [abstract] "The key idea is to employ self-supervised learning to align the representations encoded by graph context-aware GNNs and neighborhood dependency-free MLPs, thereby fully integrating the structural information into MLPs."
  - [section 3.2] "The invariance term ensures the alignment between GNN and MLP embeddings, modeling the fine-grained and generalizable correlation between node features and localized graph structures."
  - [corpus] Weak evidence - SimMLP is a recent work and the corpus doesn't contain related papers that specifically discuss the mechanism of aligning GNNs and MLPs in the embedding space.

### Mechanism 2
- Claim: The approximation strategy enhances consistency between GNN and MLP encoders by simulating the GNN learning process using MLPs.
- Mechanism: MLPs encode node embeddings and then perform message-passing, approximating the GNN aggregation step and maintaining the correlation between GNN and MLP embeddings.
- Core assumption: The approximation of GNN message-passing using MLPs preserves the key structural relationships that GNNs capture, without introducing significant computational overhead.
- Evidence anchors:
  - [section 3.3] "We hypothesize the inconsistency between encoders and the lack of data diversity are two causes of trivial solutions... We present two strategies to prevent these two causes of the trivial solution, respectively."
  - [section 3.3] "To this end, we propose a simple method to approximate GNNs using MLPs, in order to maintain the correlation between GNN and MLP embeddings."
  - [corpus] Weak evidence - The corpus contains papers on MLP learning on graphs, but none specifically discuss the approximation strategy used in SimMLP.

### Mechanism 3
- Claim: The augmentation strategy enhances data diversity by generating multi-views of nodes, preventing trivial solutions and improving generalization.
- Mechanism: Random edge masking and node feature masking create multiple corrupted views of the graph, forcing the encoders to learn robust and generalizable representations that are invariant to these augmentations.
- Core assumption: The augmented views provide diverse and meaningful information about the underlying graph structure, enabling the encoders to learn more robust representations.
- Evidence anchors:
  - [section 3.3] "Enhancing the diversity of the same instance is another approach to prevent the trivial solution. We use augmentation techniques to produce multi-views of nodes, enabling a single node to have multiple and various embeddings."
  - [section 3.3] "The augmentation generates additional node-subgraph pairs to facilitate the model training."
  - [corpus] Weak evidence - The corpus contains papers on graph contrastive learning and data augmentation, but none specifically discuss the augmentation strategy used in SimMLP.

## Foundational Learning

- Concept: Mutual Information
  - Why needed here: Mutual information maximization is used to analyze the learning objective of SimMLP and demonstrate its equivalence to GNNs in the optimal case.
  - Quick check question: How does maximizing mutual information between GNNs and MLPs in the embedding space relate to preserving structural information for node classification?

- Concept: Information Bottleneck
  - Why needed here: The information bottleneck principle is used to interpret SimMLP from the perspective of compressing the observed graph into the most informative and minimum compressions.
  - Quick check question: How does minimizing the conditional entropy of GNN and MLP embeddings with respect to the latent graph relate to learning generalizable node embeddings?

- Concept: Inductive Bias
  - Why needed here: Analyzing the inductive biases of SimMLP, such as homophily and local structure importance, helps to understand its potential equivalence to GNNs.
  - Quick check question: How do the smoothness and min-cut metrics measure the homophily and local structure importance inductive biases of SimMLP, respectively?

## Architecture Onboarding

- Component map:
  GNN Encoder -> MLP Encoder -> Alignment Loss -> Approximation Strategy -> Augmentation Strategy

- Critical path:
  1. Encode node features using GNN and MLP encoders.
  2. Compute alignment loss between GNN and MLP embeddings.
  3. Apply approximation and augmentation strategies to prevent trivial solutions.
  4. Optimize the alignment loss to maximize consistency between GNN and MLP embeddings.

- Design tradeoffs:
  - Using MLPs for fast inference vs. using GNNs for better structural learning.
  - Balancing the alignment loss with the reconstruction term to prevent distribution shifts.
  - Choosing appropriate augmentation strategies to enhance data diversity without introducing excessive noise.

- Failure signatures:
  - Low training loss but poor model accuracy (trivial solutions).
  - Significant performance degradation on unseen nodes (lack of generalization).
  - High computational overhead (inefficient approximation or augmentation).

- First 3 experiments:
  1. Train SimMLP on a small graph dataset (e.g., Cora) and evaluate its performance on transductive node classification. Compare the results with GNNs and vanilla MLPs.
  2. Apply the approximation strategy to a GNN and evaluate its impact on the consistency between GNN and MLP embeddings. Visualize the embeddings using t-SNE to assess the alignment.
  3. Experiment with different augmentation strategies (e.g., edge masking ratios, feature masking ratios) and evaluate their impact on the model's performance and robustness to noise.

## Open Questions the Paper Calls Out

- Question: How does the choice of augmentation ratio (feature masking vs. edge masking) impact model performance across different dataset characteristics (e.g., density, homophily)?
- Question: What is the theoretical relationship between the alignment loss (Equation 4) and the inductive bias preservation (homophily and local structure importance) observed in Table 3?
- Question: How does the computational complexity of SimMLP scale with graph size compared to GNNs, particularly in terms of memory usage during training?

## Limitations
- Theoretical proof of equivalence relies on mutual information maximization, which may not hold in practice
- Approximation and augmentation strategies lack extensive empirical validation across diverse graph types and sizes
- Evaluation focuses primarily on node classification accuracy, with limited analysis of other potential GNN strengths

## Confidence
- High: The core mechanism of aligning GNN and MLP embeddings through self-supervised learning is well-established and theoretically grounded.
- Medium: The effectiveness of the approximation and augmentation strategies in preventing trivial solutions is supported by ablation studies, but their robustness across different graph datasets needs further validation.
- Medium: The claimed inference acceleration is plausible given the computational efficiency of MLPs, but real-world deployment scenarios may introduce additional overhead.

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of the approximation and augmentation strategies to overall performance, and test their robustness across a wider range of graph datasets.
2. Evaluate SimMLP on graph-level tasks (e.g., graph classification) and link prediction to assess its generalization beyond node classification.
3. Implement a real-world deployment scenario to measure actual inference acceleration and compare it with the theoretical speedup, accounting for potential overheads like data preprocessing and model loading.