---
ver: rpa2
title: 'IndoCulture: Exploring Geographically-Influenced Cultural Commonsense Reasoning
  Across Eleven Indonesian Provinces'
arxiv_id: '2404.01854'
source_url: https://arxiv.org/abs/2404.01854
tags:
- language
- indoculture
- cultural
- commonsense
- indonesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IndoCulture is a cultural commonsense reasoning dataset covering
  eleven Indonesian provinces, developed by local people based on predefined cultural
  topics. The dataset includes 2,429 instances designed to evaluate how well language
  models can reason about culturally-specific scenarios across different regions of
  Indonesia.
---

# IndoCulture: Exploring Geographically-Influenced Cultural Commonsense Reasoning Across Eleven Indonesian Provinces

## Quick Facts
- arXiv ID: 2404.01854
- Source URL: https://arxiv.org/abs/2404.01854
- Reference count: 12
- Primary result: New dataset of 2,429 instances covering 11 Indonesian provinces for cultural commonsense reasoning evaluation

## Executive Summary
IndoCulture introduces a cultural commonsense reasoning dataset covering eleven Indonesian provinces, developed by local people based on predefined cultural topics. The dataset evaluates how well language models can reason about culturally-specific scenarios across different regions of Indonesia. The study reveals significant performance gaps between open-source models (maximum 53.2% accuracy) and closed-source models like GPT-4 (75.8% accuracy), demonstrating the challenges of cultural reasoning even for advanced language models.

## Method Summary
The IndoCulture dataset was created by local people based on predefined cultural topics, resulting in 2,429 instances designed to evaluate cultural commonsense reasoning across eleven Indonesian provinces. The study evaluated 27 language models, including open-source multilingual models, Indonesian-centric models, and closed-source models like GPT-3.5 and GPT-4. Models were tested on their ability to reason about culturally-specific scenarios, with particular attention to how geographical context influences performance.

## Key Results
- Open-source models achieved maximum accuracy of 53.2% on cultural reasoning tasks
- GPT-4 achieved 75.8% accuracy, significantly outperforming all open-source alternatives
- Models performed better for Bali and West Java cultures compared to other provinces
- Including location context in prompts improved performance, especially for larger models like GPT-4

## Why This Works (Mechanism)
The dataset leverages local cultural knowledge to create authentic scenarios that require understanding of geographical and cultural context. By focusing on Indonesian provinces specifically, the evaluation captures nuances that generic cultural datasets miss. The inclusion of location context in prompts helps models ground their reasoning in specific cultural settings rather than applying generic cultural knowledge.

## Foundational Learning
- Cultural Commonsense Reasoning: Understanding implicit cultural norms and practices that vary by region. Why needed: Cultural reasoning requires knowledge beyond literal text interpretation. Quick check: Can the model explain why certain behaviors are appropriate in one culture but not another?
- Multilingual Model Evaluation: Testing models across different language capabilities and cultural understanding. Why needed: Cultural knowledge varies significantly across languages and regions. Quick check: Does the model perform differently when tested in local languages versus English?
- Prompt Engineering for Context: How providing geographical context affects model performance. Why needed: Cultural reasoning often depends on understanding specific regional contexts. Quick check: Compare performance with and without location information in prompts.

## Architecture Onboarding

**Component Map:**
Dataset Creation -> Model Evaluation -> Performance Analysis -> Context Impact Assessment

**Critical Path:**
The evaluation pipeline processes each instance through model inference, comparing predicted answers against ground truth to calculate accuracy metrics. Performance analysis examines differences across model types, provinces, and context conditions.

**Design Tradeoffs:**
The binary multiple-choice format provides clear evaluation metrics but may not capture nuanced cultural understanding. Using local creators ensures cultural authenticity but may introduce variability in question difficulty and topic coverage across provinces.

**Failure Signatures:**
Models struggle particularly with provinces having less representation in training data, showing systematic biases toward better-represented cultures like Bali and West Java. Performance drops significantly when location context is removed from prompts, indicating reliance on explicit geographical information rather than implicit cultural understanding.

**3 First Experiments:**
1. Test model performance with randomized location labels to measure reliance on explicit context versus learned cultural patterns
2. Evaluate cross-province generalization by training models on one province's data and testing on others
3. Compare performance between culturally-specific questions and generic commonsense reasoning questions to isolate cultural understanding

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size of 2,429 instances may not fully capture cultural diversity across eleven provinces
- Binary multiple-choice format may not adequately test nuanced cultural reasoning abilities
- Predefined cultural topics by researchers could introduce selection bias in representing authentic cultural nuances

## Confidence
- High Confidence: Performance gap between open-source models (max 53.2%) and GPT-4 (75.8%)
- Medium Confidence: Location context improves performance, though magnitude varies across models
- Medium Confidence: Better performance for Bali and West Java cultures compared to other provinces

## Next Checks
1. Conduct blind evaluations with Indonesian cultural experts to verify whether high accuracy scores correlate with genuine cultural understanding versus memorization
2. Perform systematic ablation study on dataset size and distribution across provinces to determine if performance differences stem from imbalances or cultural complexity
3. Test whether incorporating visual cultural cues (images of traditional clothing, ceremonies, or architecture) improves model performance in multimodal extensions