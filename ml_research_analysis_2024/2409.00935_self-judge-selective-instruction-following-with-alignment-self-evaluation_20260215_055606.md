---
ver: rpa2
title: 'Self-Judge: Selective Instruction Following with Alignment Self-Evaluation'
arxiv_id: '2409.00935'
source_url: https://arxiv.org/abs/2409.00935
tags:
- judge
- gpt-4
- instructions
- scores
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selective instruction following
  in large language models (LLMs), where models may generate misaligned or incorrect
  content. To enhance reliability, the authors propose SELF-J, a self-training framework
  that trains judge models to predict numerical quality scores for model responses
  without human-annotated scores.
---

# Self-Judge: Selective Instruction Following with Alignment Self-Evaluation

## Quick Facts
- arXiv ID: 2409.00935
- Source URL: https://arxiv.org/abs/2409.00935
- Reference count: 40
- The paper proposes SELF-J, a self-training framework that trains judge models to predict numerical quality scores for model responses without human-annotated scores.

## Executive Summary
This paper addresses the challenge of selective instruction following in large language models (LLMs), where models may generate misaligned or incorrect content. To enhance reliability, the authors propose SELF-J, a self-training framework that trains judge models to predict numerical quality scores for model responses without human-annotated scores. SELF-J leverages the model's self-evaluation capability, incorporates reference answers for better assessment, and uses self-distillation for regularization. The method is evaluated on five open-source models, showing strong correlation with GPT-4 evaluations and generalization across domains. Additionally, SELF-J serves as an effective reward model, significantly improving model performance on AlpacaEval. The work demonstrates the potential of alignment self-evaluation in enhancing LLM reliability and instruction-following capabilities.

## Method Summary
The SELF-J framework trains judge models to predict quality scores for LLM responses without human annotations. It uses self-evaluation where the model rates its own responses, incorporates reference answers from instruction-tuning data to improve assessment quality, and employs self-distillation to transfer reference-based evaluation capability to reference-free evaluation. The method is trained on 30k instructions and evaluated on 1.6k test instructions, with judge models achieving strong correlation with GPT-4 evaluations and serving as effective reward models for model improvement.

## Key Results
- SELF-J-trained judge models achieve strong correlation with GPT-4 evaluations across multiple model architectures
- Judge models trained on one model generalize well to other models, demonstrating cross-model applicability
- When used as reward models, SELF-J judges significantly improve model performance on AlpacaEval benchmarks
- The method shows robust generalization across coding, common, and academic instruction domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model's self-evaluation capability can be leveraged to generate quality scores for responses without human-annotated data.
- Mechanism: The method uses the instruction-tuned model itself to generate quality scores by prompting it to rate its own responses with a reference answer. These self-generated scores are then used to train a judge model through self-distillation.
- Core assumption: The instruction-tuned model has sufficient self-evaluation capability to generate meaningful quality scores for its own responses.
- Evidence anchors:
  - [abstract] "Our method leverages the model's inherent self-evaluation capability to extract information about response quality from labeled instruction-tuning data."
  - [section 5.1] "We employ the instruction-tuned model F to initialize q(z|x, y, y′), capitalizing on the robust generalization capabilities of LLMs for self-assessment."
- Break condition: If the model lacks self-awareness or the self-evaluation scores are too noisy/correlated with model biases, the generated training data would be poor quality, leading to ineffective judge models.

### Mechanism 2
- Claim: Incorporating reference answers during self-evaluation improves the quality of the generated scores.
- Mechanism: The method uses a reference answer from the labeled instruction-tuning set to facilitate self-evaluation. The model compares its generated response with the reference answer to produce more accurate quality scores.
- Core assumption: Reference answers provide a meaningful comparison point that helps the model assess its own responses more accurately.
- Evidence anchors:
  - [abstract] "It incorporates a gold reference answer to facilitate self-evaluation"
  - [section 3.2] "As pointed out by Zheng et al. (2023), LLMs are limited in their reasoning capability. They fall short in grading reasoning tasks since they are not aware of the correct answer to the question during evaluation. Providing a reference answer can enhance the ability of the LLMs to assess such questions."
- Break condition: If reference answers are poor quality, biased, or not representative of what users expect, incorporating them could mislead the model rather than help it evaluate responses accurately.

### Mechanism 3
- Claim: Self-distillation with reference-free and reference-based objectives improves the judge model's ability to estimate quality scores without reference answers at test time.
- Mechanism: The method trains the judge model using both a student objective (reference-free estimation) and a teacher objective (reference-based estimation), then distills the teacher's knowledge into the student through KL-divergence loss.
- Core assumption: The teacher model's ability to use reference answers can be effectively distilled into the student model to improve its reference-free estimation capability.
- Evidence anchors:
  - [abstract] "During the training phase, we implement self-distillation as a regularization technique to enhance the capability of reference-free estimation."
  - [section 5.2] "To address this problem, we introduce a self-distillation approach to train the judge model. It involves optimizing a teacher objective that incorporates the reference answer for quality estimation, i.e., p(z|x, y′, y), and then distilling the ability of the teacher into a student model that performs reference-free estimation, i.e., p(z|x, y′)."
- Break condition: If the distillation process fails to transfer the teacher's knowledge effectively, or if the reference-free and reference-based objectives conflict significantly, the student model's performance may not improve or could degrade.

## Foundational Learning

- Concept: Instruction tuning
  - Why needed here: The paper builds upon instruction-tuned models as the foundation for self-evaluation and judge model training.
  - Quick check question: What is the primary goal of instruction tuning in LLMs?

- Concept: Self-training
  - Why needed here: The method uses self-training to generate quality scores without human annotations, which is essential for scaling the approach.
  - Quick check question: How does self-training differ from supervised learning in this context?

- Concept: Distillation
  - Why needed here: Self-distillation is used to transfer the teacher model's reference-based evaluation capability to the student model for reference-free evaluation.
  - Quick check question: What is the key difference between regular distillation and self-distillation?

## Architecture Onboarding

- Component map:
  - Instruction-tuned LLM (for self-evaluation and judge model training) -> Judge model (trained via self-distillation) -> Reference answers (from instruction-tuning data) -> Quality score generation pipeline (combining self-evaluation and semantic similarity)

- Critical path:
  1. Collect instruction data and generate reference answers
  2. Instruction-tune a base LLM
  3. Generate quality scores using self-evaluation + semantic similarity
  4. Train judge model via self-distillation
  5. Evaluate judge model's performance

- Design tradeoffs:
  - Using the model's own self-evaluation vs. external evaluators (tradeoff between scalability and potential bias)
  - Incorporating semantic similarity vs. relying solely on self-evaluation (tradeoff between robustness and complexity)
  - Reference-based vs. reference-free evaluation (tradeoff between accuracy and practicality)

- Failure signatures:
  - Poor correlation with GPT-4 indicates the judge model isn't capturing human preferences effectively
  - High variance in self-evaluation scores suggests instability in the self-training process
  - Degradation in reference-free evaluation performance indicates ineffective distillation

- First 3 experiments:
  1. Test the correlation between self-evaluation scores and human/ GPT-4 ratings to validate the self-evaluation capability
  2. Compare judge model performance with and without self-distillation to measure its effectiveness
  3. Evaluate the judge model's generalization across different instruction domains and model architectures

## Open Questions the Paper Calls Out

The paper does not explicitly call out any open questions, but several important questions emerge from the work:

- How does SELF-J perform on tasks outside of the tested domains (common, coding, academic)?
- What is the impact of using different reference answers on the quality scores generated by SELF-J?
- How does SELF-J handle instructions with multiple possible correct answers?

## Limitations

- The evaluation is limited to 13B parameter models, leaving uncertainty about scalability to larger models
- The method depends heavily on the quality of reference answers from the original instruction-tuning data, which are not independently verified
- The self-generated quality scores may amplify model biases since they rely on the same model for both generation and evaluation

## Confidence

- **High Confidence**: The technical implementation of the SELF-J framework and the self-distillation mechanism are well-specified and reproducible.
- **Medium Confidence**: The effectiveness of incorporating reference answers for self-evaluation is supported by results but could be domain-dependent.
- **Medium Confidence**: The generalization of judge models across different model architectures shows promise but is tested on a limited set of models.

## Next Checks

1. **Scalability Test**: Evaluate the method on models with 30B+ parameters to verify if self-evaluation capability scales effectively with model size.

2. **Bias Analysis**: Conduct an ablation study using corrupted or biased reference answers to quantify the impact on judge model performance and identify potential failure modes.

3. **Domain Generalization**: Test the judge models on completely unseen instruction domains (e.g., medical or legal tasks) to assess true generalization beyond the training distribution.