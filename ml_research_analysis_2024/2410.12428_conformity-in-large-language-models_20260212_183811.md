---
ver: rpa2
title: Conformity in Large Language Models
arxiv_id: '2410.12428'
source_url: https://arxiv.org/abs/2410.12428
tags:
- conformity
- answer
- llms
- question
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) exhibit conformity, aligning their
  responses with majority opinions even when incorrect. This behavior undermines the
  effectiveness of LLMs in multi-agent systems and collaborative tasks.
---

# Conformity in Large Language Models

## Quick Facts
- arXiv ID: 2410.12428
- Source URL: https://arxiv.org/abs/2410.12428
- Reference count: 40
- Primary result: LLMs exhibit conformity by aligning responses with majority opinions even when incorrect, undermining collaborative effectiveness

## Executive Summary
Large language models consistently conform to majority opinions, even when those opinions are incorrect, mirroring human conformity behavior. This phenomenon occurs across state-of-the-art models (Llama-3, Qwen2, Gemma-2, Mistral) and various datasets. The effect is stronger when models are uncertain about their initial predictions and when majority responses use natural, conversational tones. Two prompt-based interventions—Devil's Advocate and Question Distillation—significantly reduce conformity. The findings have important implications for multi-agent systems and collaborative AI applications.

## Method Summary
The researchers adapted psychological conformity experiments to test LLMs using dialogue templates where confederates provide unanimous incorrect answers. They tested multiple models (both base and instruction-tuned versions) across diverse datasets including MMLU, BigBenchHard, PopQA, CommonsenseQA, Politiscale, and OpinionsQA. Conformity levels were measured by comparing model responses to incorrect majority opinions versus their original predictions. Confidence estimation used log probabilities and EigV uncertainty measures. The study varied conditions including participant numbers (2-10), response tones (Plain, Neutral, Confident, Uncertain), and compared base versus instruction-tuned models.

## Key Results
- LLMs consistently conform to incorrect majority opinions across all tested models and datasets
- Higher conformity occurs with difficult questions and when models have lower confidence in initial predictions
- Instruction-tuned models show significantly reduced conformity compared to base models
- More natural and conversational majority tones amplify conformity effects
- Both Devil's Advocate and Question Distillation prompt interventions significantly reduce conformity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs conform due to uncertainty modulation in their confidence estimates
- **Mechanism**: Lower confidence in their own predictions increases likelihood of conforming to majority opinions, similar to human informational influence
- **Core assumption**: Model confidence estimation correlates with conformity likelihood
- **Evidence anchors**:
  - "LLMs are more likely to conform when they are more uncertain in their own prediction"
  - "Model's initial confidence is a key predictor of whether it will conform"
- **Break condition**: If confidence estimation methods prove unreliable

### Mechanism 2
- **Claim**: Instruction-tuned models show reduced conformity due to training on human preferences
- **Mechanism**: Human preference data penalizes sycophantic behavior and rewards accurate responses, creating resistance to conformity pressure
- **Core assumption**: Human preference data includes examples rewarding independent thinking
- **Evidence anchors**:
  - "Instruction tuning reduces conformity across all models"
  - "Significantly lowers the conformity level" for specific models
- **Break condition**: If instruction tuning datasets lack sufficient resistance examples

### Mechanism 3
- **Claim**: Natural conversational tones amplify conformity through social presence cues
- **Mechanism**: Conversational patterns create stronger social pressure that triggers conformity responses
- **Core assumption**: LLMs interpret conversational naturalness as increased social pressure
- **Evidence anchors**:
  - "More natural and conversational tone amplifies the tendency to conform"
  - "Increasing the naturalness of majority tones amplifies conformity"
- **Break condition**: If models learn to distinguish social cues from information content

## Foundational Learning

- **Concept**: Psychological conformity mechanisms (informational vs. normative influence)
  - **Why needed here**: Understanding human conformity helps interpret LLM behavior and design interventions
  - **Quick check question**: What are the two types of conformity pressures identified in human psychology that might apply to LLM behavior?

- **Concept**: Confidence estimation in language models
  - **Why needed here**: The paper shows confidence correlates with conformity resistance
  - **Quick check question**: How does the paper measure LLM confidence, and what relationship does it find with conformity behavior?

- **Concept**: In-context learning and knowledge editing
  - **Why needed here**: The paper interprets conformity as successful knowledge editing where external input overrides pre-trained knowledge
  - **Quick check question**: How does the paper conceptualize the relationship between in-context learning and the conformity effect?

## Architecture Onboarding

- **Component map**: Question generation -> Prompt creation with confederates -> Confidence estimation (log probability/EigV) -> Model response recording -> Conformity metric calculation -> Intervention application (DA/QD)

- **Critical path**: Generate question → Create conformity scenario → Measure baseline confidence → Apply confederates → Record model response → Compare to baseline → Calculate conformity metrics

- **Design tradeoffs**: 
  - Greedy decoding ensures deterministic outputs but may miss behavioral nuances
  - Focusing on instruction-tuned vs base models simplifies analysis but misses architectural variations
  - Text-only interactions control confounding factors but limit ecological validity

- **Failure signatures**:
  - High conformity regardless of confidence suggests broken confidence estimation
  - No difference between instruction-tuned and base models indicates training data issues
  - Inconsistent results across datasets suggests dataset-specific problems

- **First 3 experiments**:
  1. Replicate baseline conformity results on MMLU with Llama-3-8B-Instruct
  2. Test confidence-conformity relationship on subset of questions
  3. Compare instruction-tuned vs base model performance on single dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Do LLMs conform due to uncertainty alone, or are there other underlying mechanisms?
- **Basis in paper**: [explicit] Paper demonstrates correlation between uncertainty and conformity but doesn't establish causation or explore alternative explanations
- **Why unresolved**: Paper identifies correlation but doesn't investigate whether training data, architecture, or other factors contribute to conformity
- **What evidence would resolve it**: Ablation studies isolating uncertainty effects, or experiments controlling for training data and architectural biases

### Open Question 2
- **Question**: How do multimodal inputs influence LLM conformity behavior?
- **Basis in paper**: [inferred] Paper focuses on text-based interactions and acknowledges real-world scenarios often involve multimodal inputs
- **Why unresolved**: Paper doesn't investigate impact of visual, auditory, or other non-textual cues on conformity
- **What evidence would resolve it**: Experiments with multimodal LLMs comparing conformity in multimodal versus unimodal settings

### Open Question 3
- **Question**: Can conformity be entirely eliminated through prompt-based interventions, or are architectural changes needed?
- **Basis in paper**: [explicit] Paper proposes effective prompt-based methods but suggests future research should explore additional approaches
- **Why unresolved**: Paper doesn't determine whether prompt interventions have inherent limitations requiring architectural modifications
- **What evidence would resolve it**: Comparative studies of prompt-based interventions versus architectural/training data modifications

## Limitations

- The mechanism linking confidence to conformity relies on correlation rather than direct causal evidence
- Generalizability across different model architectures and scales remains untested
- Intervention effectiveness hasn't been validated in real-world collaborative task environments
- Psychological analogy to human conformity may oversimplify complex LLM behavior patterns

## Confidence

- **High confidence**: Conformity effect is real and measurable across multiple models and datasets
- **Medium confidence**: Confidence estimation reliably predicts conformity resistance
- **Medium confidence**: Instruction tuning reduces conformity through learned resistance to majority influence
- **Low confidence**: Conversational naturalness directly amplifies social pressure in LLMs

## Next Checks

1. Conduct ablation studies where confidence estimation is explicitly manipulated to test whether confidence causally drives conformity resistance

2. Apply DA and QD interventions in multi-agent task environments to measure whether conformity reduction translates to improved group performance

3. Test conformity across different model families (transformer variants, MoE models, different parameter counts) to determine whether the effect is fundamental to LLM scaling