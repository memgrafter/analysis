---
ver: rpa2
title: Mamba for Scalable and Efficient Personalized Recommendations
arxiv_id: '2409.17165'
source_url: https://arxiv.org/abs/2409.17165
tags:
- mamba
- recommendation
- transformer
- two-tower
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes using the Mamba model for handling tabular data
  in personalized recommendation systems, replacing Transformer layers with Mamba
  layers in the FT-Transformer architecture (FT-Mamba). The Mamba model offers an
  efficient alternative to Transformers, reducing computational complexity from quadratic
  to linear by enhancing the capabilities of State Space Models (SSMs).
---

# Mamba for Scalable and Efficient Personalized Recommendations

## Quick Facts
- arXiv ID: 2409.17165
- Source URL: https://arxiv.org/abs/2409.17165
- Reference count: 34
- Key outcome: FT-Mamba model outperforms Transformer baseline in computational efficiency while maintaining or exceeding recommendation performance metrics on three datasets.

## Executive Summary
This work proposes FT-Mamba, a recommendation system architecture that replaces Transformer layers with Mamba layers in the FT-Transformer framework. By leveraging Mamba's linear computational complexity through State Space Models, FT-Mamba achieves scalable and efficient processing of tabular user data. The study evaluates FT-Mamba against traditional Transformer models within a Two-Tower architecture across three datasets: Spotify music recommendation, H&M fashion recommendation, and vaccine messaging recommendation. Results demonstrate that FT-Mamba maintains competitive performance while offering significant computational advantages.

## Method Summary
FT-Mamba implements a Two-Tower architecture where both user and content towers use Mamba layers instead of Transformers. The method converts tabular data into sequential tokens through a feature tokenizer that handles numeric values via linear projections and categorical values through lookup tables. Each tower processes its input independently through Mamba layers, then produces embeddings whose inner product serves as the prediction score. The model is trained on 160,000 user-action pairs per dataset with batch size 32 for 5,000 steps per epoch using MSE loss. Performance is evaluated using precision, recall, MRR, and Hit Ratio at multiple truncation values.

## Key Results
- FT-Mamba achieves linear computational complexity (O(L)) versus quadratic (O(L²)) for Transformers
- Model maintains or exceeds performance across precision, recall, MRR, and HR metrics compared to Transformer baseline
- Demonstrated effectiveness across three diverse recommendation domains with 100 test users each

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mamba layers reduce computational complexity from quadratic to linear in sequence length, enabling scalable personalized recommendations on large tabular datasets.
- Mechanism: Mamba achieves linear complexity through a selective state space model (SSM) that dynamically adjusts parameters based on input and employs a scanning technique to maintain efficiency. This contrasts with Transformer self-attention's quadratic complexity.
- Core assumption: The selective SSM can effectively capture sequential dependencies in tokenized tabular data without the full quadratic self-attention mechanism.
- Evidence anchors:
  - [abstract]: "The Mamba model offers an efficient alternative to Transformers, reducing computational complexity from quadratic to linear by enhancing the capabilities of State Space Models (SSMs)."
  - [section]: "Specifically, for a sequence length of L, the computational complexity of both Mamba and SSM is linear ( O(L)), whereas the standard Transformer exhibits quadratic complexity ( O(L2))."
  - [corpus]: Weak support; most corpus papers focus on sequential recommendations rather than tabular data, making direct evidence sparse.
- Break condition: Performance degrades if the selective SSM cannot capture long-range dependencies in complex tabular patterns, or if scanning technique overhead becomes significant for small sequence lengths.

### Mechanism 2
- Claim: Feature tokenization converts tabular user data into sequences suitable for Mamba processing, enabling structured data to be handled as sequential input.
- Mechanism: The tokenizer maps numeric values via trainable linear projections and categorical values via lookup tables into a unified token space, appending a [CLS] token at the end for sequence-level aggregation.
- Core assumption: Sequential processing of tokenized tabular features preserves the information necessary for accurate recommendations.
- Evidence anchors:
  - [section]: "The feature tokenizer converts tabular data into a sequence of tokens, handling categorical and numerical values separately... the token for xj is given by... Finally, the representation of x in tokens is given by... T = stack h T1, ..., Tk, T[CLS] i."
  - [abstract]: "The feature tokenizer in FT-Mamba converts both categorical and numerical tabular data into sequences, which are passed through Mamba layers for efficient sequential processing."
  - [corpus]: Limited; most corpus papers assume sequential data input rather than describing tokenization for tabular formats.
- Break condition: Tokenization fails if the feature space is too large (lookup tables become memory-prohibitive) or if the sequence length becomes too long, negating Mamba's efficiency gains.

### Mechanism 3
- Claim: Two-Tower architecture with separate FT-Mamba embedders for user and content allows efficient similarity-based recommendation without full pairwise comparisons.
- Mechanism: Independent user and content towers encode tabular features into embeddings, and the inner product of these embeddings serves as the prediction score, reducing computation from O(n²) to O(n).
- Core assumption: Inner product similarity in the learned embedding space adequately captures user-content relevance.
- Evidence anchors:
  - [section]: "In this model, one tower encodes user information, and the other encodes content information, with both towers using either FT-Transformer or FT-Mamba layers... The model's predictions are based on the inner product of the user and content embeddings."
  - [abstract]: "By leveraging Mamba layers, FT-Mamba provides a scalable and effective solution for large-scale personalized recommendation systems."
  - [corpus]: Weak; corpus neighbors focus on sequential recommendations and don't address two-tower tabular setups.
- Break condition: The model underperforms if the embedding space cannot distinguish nuanced user preferences or if the inner product is insufficient for capturing complex interactions.

## Foundational Learning

- Concept: State Space Models (SSMs) and their role in sequence modeling.
  - Why needed here: Mamba builds on SSMs to achieve linear complexity; understanding SSM mechanics is essential to grasp Mamba's efficiency gains.
  - Quick check question: What is the key computational advantage of SSMs over traditional RNNs in sequence modeling?

- Concept: Feature tokenization for converting structured tabular data into sequential input.
  - Why needed here: FT-Mamba relies on tokenization to transform user demographics into token sequences; without this, Mamba cannot process the data.
  - Quick check question: How does the tokenizer handle numeric versus categorical features differently in FT-Mamba?

- Concept: Two-Tower architecture and similarity-based recommendation.
  - Why needed here: The recommendation task uses a dual-encoder setup to efficiently compute user-content relevance; understanding this avoids unnecessary pairwise computations.
  - Quick check question: Why does the Two-Tower model use inner product similarity instead of more complex scoring functions?

## Architecture Onboarding

- Component map: Tokenizer → Mamba layer(s) → Feed-forward network → Inner product (Two-Tower)
- Critical path: Input → Tokenizer (numeric/categorical mapping) → Mamba layers (selective SSM with scanning) → [CLS] embedding → Feed-forward → Inner product → Prediction
- Design tradeoffs: Mamba offers linear complexity but slightly reduced parameter efficiency versus Transformers; tokenization introduces memory overhead for large feature spaces; Two-Tower design sacrifices some global context for scalability
- Failure signatures: Degraded metrics when sequence length grows large; memory errors from excessive lookup tables; poor generalization if tokenization loses feature interactions
- First 3 experiments:
  1. Verify tokenization correctness on small synthetic tabular data and inspect token embeddings
  2. Compare Mamba vs. Transformer runtime and memory for varying sequence lengths on the same dataset
  3. Test Two-Tower recommendation accuracy on a small labeled dataset before scaling to full experiments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FT-Mamba compare to other efficient transformer variants (e.g., Performer, Longformer) on large-scale recommendation tasks?
- Basis in paper: [inferred] The paper compares FT-Mamba only to traditional Transformer models, not other efficient transformer variants.
- Why unresolved: The study focuses solely on comparing FT-Mamba to traditional Transformer-based models within a Two-Tower architecture, without benchmarking against other efficient transformer alternatives.
- What evidence would resolve it: Empirical results comparing FT-Mamba's performance metrics (precision, recall, MRR, HR) and computational efficiency against other efficient transformer variants on the same recommendation datasets.

### Open Question 2
- Question: What is the impact of varying the number of Mamba layers on the model's performance and computational efficiency?
- Basis in paper: [inferred] The paper uses a fixed number of Mamba layers (4) but does not explore the effects of varying this hyperparameter.
- Why unresolved: The study uses a predetermined number of Mamba layers without investigating how different layer counts affect performance and efficiency.
- What evidence would resolve it: Systematic experiments varying the number of Mamba layers and analyzing the resulting changes in recommendation metrics and computational requirements.

### Open Question 3
- Question: How does FT-Mamba perform when applied to real-time recommendation systems with streaming data?
- Basis in paper: [inferred] The experiments use static datasets and do not address the model's performance in dynamic, real-time scenarios.
- Why unresolved: The study evaluates FT-Mamba on pre-existing datasets without testing its capabilities in handling streaming data typical of real-time recommendation systems.
- What evidence would resolve it: Implementation and testing of FT-Mamba in a simulated or real-time streaming environment, measuring performance and efficiency metrics under dynamic conditions.

## Limitations

- Limited empirical comparison: The study compares FT-Mamba only against a single Transformer baseline without exploring other state-of-the-art recommendation models or Mamba variants
- Dataset generalizability concerns: Two of three datasets use synthetically generated user-item pairs with random negative sampling, raising questions about transfer to production settings
- Efficiency metrics incompleteness: The paper lacks concrete runtime and memory consumption measurements across different sequence lengths

## Confidence

- **High confidence**: Mamba layers reduce computational complexity from quadratic to linear - this is a fundamental property of the Mamba architecture with direct theoretical support
- **Medium confidence**: FT-Mamba maintains or exceeds performance compared to Transformer baseline - supported by reported metrics but limited by small test sets and synthetic data
- **Low confidence**: Mamba provides a universally scalable solution for large-scale personalized recommendation systems - the claim extends beyond what the three specific datasets can substantiate

## Next Checks

1. Measure actual training and inference time for both FT-Mamba and Transformer models across varying sequence lengths (e.g., 100, 1000, 10000) on the same hardware to verify the claimed linear complexity advantage in practice
2. Test FT-Mamba on a publicly available real-world recommendation dataset with genuine user-item interactions (such as MovieLens or Amazon reviews) to assess performance beyond synthetically generated pairs
3. Evaluate recommendation performance with and without the [CLS] token, and test different tokenization strategies for numeric features to determine sensitivity to preprocessing choices