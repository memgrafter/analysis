---
ver: rpa2
title: 'TransAxx: Efficient Transformers with Approximate Computing'
arxiv_id: '2402.07545'
source_url: https://arxiv.org/abs/2402.07545
tags:
- approximate
- accuracy
- transaxx
- power
- design
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TransAxx, the first framework enabling fast
  emulation of approximate multipliers in Vision Transformer (ViT) models. Built on
  PyTorch with GPU acceleration, TransAxx seamlessly integrates arbitrary approximate
  multipliers into ViT architectures, supporting mixed-precision per-layer approximation,
  post-training quantization, and approximation-aware retraining.
---

# TransAxx: Efficient Transformers with Approximate Computing

## Quick Facts
- arXiv ID: 2402.07545
- Source URL: https://arxiv.org/abs/2402.07545
- Authors: Dimitrios Danopoulos; Georgios Zervakis; Dimitrios Soudris; Jörg Henkel
- Reference count: 40
- Primary result: First framework enabling fast emulation of approximate multipliers in Vision Transformers, achieving up to 21% power savings with minimal accuracy loss

## Executive Summary
This paper introduces TransAxx, the first framework enabling fast emulation of approximate multipliers in Vision Transformer (ViT) models. Built on PyTorch with GPU acceleration, TransAxx seamlessly integrates arbitrary approximate multipliers into ViT architectures, supporting mixed-precision per-layer approximation, post-training quantization, and approximation-aware retraining. The framework addresses the computational bottleneck of evaluating ViTs on approximate hardware, which prior works had not explored. TransAxx accelerates this process through JIT compilation, GPU kernel optimization, and LUT-based multiplication, reducing emulation time from hours to minutes. Additionally, the paper proposes a Monte Carlo Tree Search (MCTS)-based methodology for optimal layer-to-approximation mapping, guided by a hardware-driven policy that balances accuracy and power.

## Method Summary
TransAxx is a PyTorch-based framework that enables fast emulation of approximate multipliers in Vision Transformer models through JIT compilation, GPU kernel optimization, and LUT-based multiplication. The framework dynamically substitutes exact multipliers with approximate ones during model compilation, using precomputed LUTs to replace floating-point operations. For larger bit-widths, it can switch to functional-based multiplication to manage memory. MCTS with a hardware-driven policy explores the design space of per-layer multiplier assignments to find accuracy-power Pareto-optimal configurations. The framework supports post-training quantization and approximate-aware retraining to recover accuracy when needed.

## Key Results
- Achieves up to 21% power savings on ImageNet with minimal accuracy loss across ViT-S, DeiT-S, Swin-S, and GCViT-XXT models
- Reduces emulation time from hours to minutes through JIT compilation and GPU optimization
- MCTS identifies Pareto-optimal configurations, offering designers fine-grained control over accuracy-power trade-offs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** TransAxx enables fast approximate ViT inference by JIT-compiling custom PyTorch layers that directly replace exact multipliers with approximate ones using LUTs.
- **Mechanism:** During model compilation, TransAxx swaps PyTorch layers with custom extensions that load precomputed LUT values instead of performing floating-point multiplication. JIT compilation ensures the new computational graph is efficient and can be updated incrementally.
- **Core assumption:** The approximate multiplier's output is deterministic and depends only on the current multiplicand values, not on prior state or timing violations.
- **Evidence anchors:**
  - [section] "JIT also makes the model flexible and easy to modify during runtime... During the forward pass, TransAxx uses these LUTs and substitutes the default (exact) multiplication operator with the approximate product (i.e., loading the value LUT[ x][y])."
  - [abstract] "Built on PyTorch with GPU acceleration, TransAxx seamlessly integrates arbitrary approximate multipliers into ViT architectures..."
- **Break Condition:** If the multiplier output depends on previous inputs or timing (e.g., VOS-based multipliers with timing violations), LUT-based substitution will produce incorrect results.

### Mechanism 2
- **Claim:** MCTS with a hardware-driven policy efficiently navigates the space of per-layer multiplier assignments to find accuracy-power Pareto-optimal configurations.
- **Mechanism:** MCTS explores the tree of possible configurations, where each node represents a ViT layer with a specific multiplier assignment. The rollout policy uses a softmax-weighted probability based on layer sensitivity and multiplier power, guided by Equation 6, to bias selection toward promising paths while still exploring alternatives.
- **Core assumption:** Sensitivity data from single-layer approximation experiments accurately reflects the impact of assigning a multiplier to that layer in the full model context.
- **Evidence anchors:**
  - [section] "Conclusively, we can now express the probability of taking a specific action in the rollout policy, that is to select an Aj out of k available ACUs for layer i as: P (Aj)i = e(sj,i−λ×pj,i) / Pk z=1 e(sz,i−λ×pz,i)"
  - [section] "Fig. 3 represents the normalized actual and predicted accuracy... Each layer block exhibits varying sensitivity to accuracy perturbations..."
- **Break Condition:** If layer sensitivity scores do not generalize across different model architectures or ACU sets, the policy may misguide the search.

### Mechanism 3
- **Claim:** GPU kernel optimization via CUDA intrinsics and compiler flags enables LUTs to be cached efficiently in L1 cache, reducing memory access latency and speeding up inference.
- **Mechanism:** LUTs are stored in global memory initially, but due to their read-only nature, CUDA intrinsics and compiler flags instruct the Nvidia compiler to cache them in L1 cache. This allows all threads in a CUDA core to share fast access to LUT data.
- **Core assumption:** LUT access patterns are read-only and cache-friendly enough for L1 caching to significantly improve performance.
- **Evidence anchors:**
  - [section] "Using this approach, the LUT array will be typically cached through the L1 GPU cache, which offers low latency and can be shared among all threads within a CUDA core."
  - [section] "As processing progresses through subsequent batches, the LUT caching mechanisms we introduced within TransAxx effectively come into play."
- **Break Condition:** If LUTs are too large to fit in L1 cache or if access patterns are random and non-repeating, caching gains will be minimal and memory bandwidth will become the bottleneck.

## Foundational Learning

- **Concept:** Vision Transformer (ViT) architecture and the role of self-attention.
  - **Why needed here:** TransAxx targets ViT models, so understanding the structure and computational hotspots is critical for knowing where to apply approximate multipliers.
  - **Quick check question:** What are the three main components of a ViT encoder block, and which one is most computationally intensive?

- **Concept:** Approximate computing and error resilience in deep learning.
  - **Why needed here:** Approximate multipliers trade accuracy for efficiency, and understanding when and how this trade-off is acceptable in neural networks is key to applying TransAxx effectively.
  - **Quick check question:** What are two error metrics commonly used to characterize approximate multipliers?

- **Concept:** Monte Carlo Tree Search (MCTS) and exploration-exploitation trade-off.
  - **Why needed here:** MCTS is used to find optimal per-layer multiplier mappings, so understanding how it balances exploration and exploitation is essential for tuning the search.
  - **Quick check question:** In MCTS, what does the UCB formula's exploration constant control?

## Architecture Onboarding

- **Component map:**
  - PyTorch plugin with custom layer extensions -> JIT compiler for dynamic graph updates -> LUT generator for each approximate multiplier -> GPU kernels optimized with CUDA intrinsics -> MCTS search engine with hardware-driven policy -> Post-training quantization and approximate-aware retraining modules

- **Critical path:**
  1. User loads ViT model and approximate multiplier definitions
  2. TransAxx generates LUTs and replaces exact multipliers with approximate ones
  3. Model runs on GPU with JIT-compiled custom layers
  4. Accuracy and power metrics are collected
  5. MCTS explores configurations and identifies Pareto-optimal solutions
  6. Optional: approximate-aware retraining to recover accuracy

- **Design tradeoffs:**
  - LUT size vs. inference speed: larger LUTs improve accuracy but increase memory usage and inference time
  - MCTS simulation count vs. exploration quality: more simulations improve solution quality but increase runtime
  - Approximation level vs. accuracy loss: aggressive approximation yields higher efficiency but may require retraining

- **Failure signatures:**
  - Slow inference: LUTs too large, JIT compilation not triggered, or GPU kernels not optimized
  - Poor accuracy: inappropriate multiplier selection, insufficient retraining, or LUT inaccuracies
  - MCTS not converging: exploration constant too low, rollout policy ineffective, or sensitivity data inaccurate

- **First 3 experiments:**
  1. Run ViT-S with an 8-bit accurate multiplier (mul8s_1KV6) to establish baseline accuracy and timing.
  2. Replace one self-attention layer with an approximate multiplier (e.g., mul8s_1L2H) and measure accuracy drop and speed gain.
  3. Run MCTS with two approximate multipliers (mul8s_1KV9, mul8s_1L2H) on ViT-S to find a Pareto-optimal configuration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TransAxx compare when emulating approximate multipliers with different bit-widths, particularly for bit-widths greater than 12 bits?
- Basis in paper: [explicit] The paper discusses that TransAxx can dynamically substitute LUT-based multiplication with functional-based multiplication for bit-widths greater than 12 bits to alleviate memory issues, but does not provide specific performance comparisons.
- Why unresolved: The paper mentions the possibility of using functional-based multiplication for larger bit-widths but does not provide detailed performance metrics or comparisons between the two approaches for different bit-widths.
- What evidence would resolve it: Performance benchmarks comparing the execution time and memory usage of LUT-based and functional-based approaches for various bit-widths, particularly focusing on bit-widths greater than 12 bits.

### Open Question 2
- Question: What are the specific error distributions and hardware savings of the approximate multipliers used in TransAxx, and how do they impact the accuracy of ViT models?
- Basis in paper: [explicit] The paper mentions that TransAxx supports any arbitrary approximate multiplier as long as its output is deterministic and solely depends on the multiplicand values. It also states that each approximate multiplier results in distinct error distributions and hardware savings.
- Why unresolved: While the paper mentions the support for various approximate multipliers and their impact on accuracy, it does not provide detailed information on the specific error distributions and hardware savings of the multipliers used in the experiments.
- What evidence would resolve it: Detailed error distribution analysis and hardware savings data for the approximate multipliers used in the experiments, along with their impact on the accuracy of ViT models.

### Open Question 3
- Question: How does the performance of TransAxx vary when applied to different types of ViT models, such as those with varying numbers of layers or different attention mechanisms?
- Basis in paper: [explicit] The paper evaluates TransAxx on four popular ViT models (ViT, DeiT, Swin, and GCViT) but does not provide a comprehensive analysis of how its performance varies across different types of ViT architectures.
- Why unresolved: The paper focuses on a limited set of ViT models and does not explore the performance implications of TransAxx on other ViT architectures with varying complexities or attention mechanisms.
- What evidence would resolve it: Performance evaluations of TransAxx on a diverse set of ViT models, including those with different numbers of layers, attention mechanisms, and architectural variations, to understand its scalability and effectiveness across different ViT types.

## Limitations

- Scalability concerns for high-bitwidth multipliers due to LUT memory requirements
- Limited generalizability across diverse ViT architectures beyond the four models tested
- Power savings estimates based on MAC operation counts rather than actual hardware measurements

## Confidence

- **High**: Framework implementation and basic functionality, GPU kernel optimization via caching
- **Medium**: MCTS methodology and Pareto-optimal configuration claims, power savings estimates
- **Low**: Generalizability to non-ViT transformer architectures, actual hardware power measurements

## Next Checks

1. Test TransAxx with a transformer architecture beyond ViT (e.g., BERT or MLP-Mixer) to verify architecture independence
2. Implement and measure actual power consumption on FPGA or ASIC to validate the MAC-based power estimates
3. Evaluate the framework with approximate multipliers beyond 8-bit precision (e.g., 16-bit) to assess scalability limits