---
ver: rpa2
title: Joint Visual and Text Prompting for Improved Object-Centric Perception with
  Multimodal Large Language Models
arxiv_id: '2404.04514'
source_url: https://arxiv.org/abs/2404.04514
tags:
- visual
- arxiv
- image
- gpt-4v
- perception
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of object-oriented perception
  in multimodal large language models (MLLMs), particularly in visual question answering
  (VQA) tasks. MLLMs often struggle with accurate object recognition and localization,
  leading to potential object hallucinations.
---

# Joint Visual and Text Prompting for Improved Object-Centric Perception with Multimodal Large Language Models

## Quick Facts
- **arXiv ID**: 2404.04514
- **Source URL**: https://arxiv.org/abs/2404.04514
- **Reference count**: 16
- **Primary result**: VTPrompt improves GPT-4V MME accuracy by up to 183.5 points and boosts MMB performance by 8.17% (GPT-4V) and 15.69% (Gemini Pro)

## Executive Summary
This paper introduces Joint Visual and Text Prompting (VTPrompt), a method to enhance multimodal large language models (MLLMs) in object-oriented perception tasks, particularly visual question answering (VQA). VTPrompt combines fine-grained visual information (via object detection models) with structured text prompts to address MLLMs' challenges in accurate object recognition and localization, which often lead to object hallucinations. By marking relevant objects in images and guiding MLLMs with optimized prompts, the method significantly improves performance on standard benchmarks (MME, MMB, POPE), demonstrating the effectiveness of integrating visual and textual cues.

## Method Summary
VTPrompt integrates visual and textual prompts to enhance MLLMs' object-level perception in VQA tasks. It first extracts key concepts from the question using GPT-4, then employs a detection model (e.g., SPHINX or SAM) to mark relevant objects in the image as visual prompts. A structured three-step text prompt guides the MLLM to acknowledge visual markers, establish overall scene perception, and perform fine-grained object analysis. The processed image and text prompt are jointly fed to the MLLM (GPT-4V or Gemini Pro) to generate more accurate answers, leveraging the complementary strengths of detection models for localization and MLLMs for reasoning.

## Key Results
- GPT-4V accuracy on MME benchmark improved by up to 183.5 points with VTPrompt
- MMB performance increased by 8.17% for GPT-4V and 15.69% for Gemini Pro
- VTPrompt effectively reduces object hallucinations and improves localization accuracy in VQA tasks

## Why This Works (Mechanism)

### Mechanism 1
Visual prompts provide explicit spatial attention to key objects, overcoming MLLMs' limited visual grounding. By using a detection model to mark relevant objects with bounding boxes, the model is cued to focus on those regions, improving localization and attribute recognition. Core assumption: The MLLM can interpret visual markers as meaningful spatial cues when guided by an appropriate text prompt.

### Mechanism 2
Text prompts guide the MLLM to process visual markers in a structured way, avoiding object hallucinations. A three-step text prompt instructs the model to acknowledge visual markers, establish overall scene perception, and perform fine-grained object analysis, ensuring the model uses the visual cues appropriately. Core assumption: Structured text prompts can override the MLLM's default reasoning path and force it to integrate visual information systematically.

### Mechanism 3
Joint visual and text prompting exploits complementary strengths: detection models for precise localization and MLLMs for reasoning. Detection models handle spatial accuracy while MLLMs handle semantic reasoning; combining them leverages both strengths without requiring full end-to-end training. Core assumption: The detection model's output is compatible with the MLLM's input format and reasoning process.

## Foundational Learning

- **Concept**: Object detection and visual grounding
  - Why needed here: The method depends on accurately marking objects in images so the MLLM can focus on them.
  - Quick check question: What is the role of SPHINX or SAM in this approach, and why is it necessary?

- **Concept**: Prompt engineering and chain-of-thought reasoning
  - Why needed here: The text prompts must guide the MLLM to use visual markers correctly and avoid hallucinations.
  - Quick check question: How does the three-step text prompt structure help the MLLM process visual information?

- **Concept**: Multimodal model architecture (MLLM)
  - Why needed here: Understanding how MLLMs process images and text together is key to designing effective joint prompts.
  - Quick check question: Why might MLLMs struggle with object-oriented perception tasks even with access to the full image?

## Architecture Onboarding

- **Component map**: Key Concepts Extraction (GPT-4) -> Visual Prompt Generation (SPHINX/SAM) -> Text Prompt (structured) -> MLLM (GPT-4V/Gemini Pro)
- **Critical path**:
  1. Extract key objects from question
  2. Detect and mark those objects in image
  3. Feed marked image + structured text prompt to MLLM
  4. Return answer
- **Design tradeoffs**:
  - Detection model choice: Accuracy vs speed (SPHINX vs SAM)
  - Prompt complexity: More steps may help but risk overwhelming the MLLM
  - Bounding box vs mask: Simplicity vs precision
- **Failure signatures**:
  - Incorrect object detection → wrong focus → wrong answer
  - MLLM ignores visual markers → hallucination persists
  - Text prompt unclear → MLLM misuses visual cues
- **First 3 experiments**:
  1. Test object detection accuracy on a small VQA subset
  2. Verify MLLM can process bounding boxes with a simple prompt
  3. Measure impact of adding/removing text prompt steps

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- Detection model reliability is critical; inaccuracies in object detection directly degrade performance.
- Text prompt formulations are not fully disclosed, limiting reproducibility and optimization.
- MLLM compatibility is assumed but not thoroughly validated across different models or scenarios.

## Confidence
- **High Confidence**: The core concept that combining visual and text prompts improves object-oriented perception is supported by experimental results.
- **Medium Confidence**: The mechanism by which structured text prompts guide MLLM reasoning is plausible but not fully validated.
- **Low Confidence**: The robustness of the detection model across diverse scenarios and the generalizability of the approach to unseen object types are not thoroughly tested.

## Next Checks
1. Test the SPHINX/SAM models on a diverse set of images (e.g., varying object scales, occlusions, and backgrounds) to quantify detection accuracy and its impact on VTPrompt performance.
2. Systematically remove or modify each step of the text prompt and measure the effect on MLLM performance, to isolate the contribution of prompt structure.
3. Apply VTPrompt to additional MLLMs (e.g., Claude-3, LLaVA) and evaluate whether the same prompt templates yield consistent improvements, or if model-specific tuning is required.