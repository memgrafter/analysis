---
ver: rpa2
title: 'Beyond the Doors of Perception: Vision Transformers Represent Relations Between
  Objects'
arxiv_id: '2406.15955'
source_url: https://arxiv.org/abs/2406.15955
tags:
- object
- attention
- figure
- representations
- same
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how vision transformers (ViTs) perform
  relational reasoning tasks, specifically judging whether two objects are the same
  or different. The authors use mechanistic interpretability techniques to analyze
  ViT internal algorithms and find that successful models exhibit a two-stage processing
  pipeline: a perceptual stage where disentangled object representations are formed,
  followed by a relational stage where abstract same/different operations are computed.'
---

# Beyond the Doors of Perception: Vision Transformers Represent Relations Between Objects

## Quick Facts
- arXiv ID: 2406.15955
- Source URL: https://arxiv.org/abs/2406.15955
- Authors: Michael A. Lepori; Alexa R. Tartaglini; Wai Keen Vong; Thomas Serre; Brenden M. Lake; Ellie Pavlick
- Reference count: 40
- Primary result: Vision transformers exhibit a two-stage processing pipeline for relational reasoning, with perceptual stages forming disentangled object representations and relational stages computing abstract same/different operations.

## Executive Summary
This paper investigates how vision transformers (ViTs) perform relational reasoning tasks, specifically judging whether two objects are the same or different. Using mechanistic interpretability techniques, the authors analyze ViT internal algorithms and discover that successful models exhibit a two-stage processing pipeline: a perceptual stage where disentangled object representations are formed, followed by a relational stage where abstract same/different operations are computed. The research demonstrates that CLIP and DINOv2 models pretrained on large-scale datasets show the clearest two-stage processing patterns, while failures at either stage prevent learning generalizable solutions.

## Method Summary
The authors fine-tune pretrained vision transformers (CLIP, DINOv2, DINO, MAE) on synthetic same-different tasks with 256 unique objects (16 shapes × 16 colors). They employ mechanistic interpretability techniques including attention pattern analysis to identify local-to-global transitions, Distributed Alignment Search (DAS) to find linear subspaces for shape and color, and linear intervention analysis to test generalization. The models are evaluated on in-distribution (IID), out-of-distribution (OOD), and compositional generalization test sets. Auxiliary losses are introduced to encourage proper disentanglement and two-stage processing when models fail to exhibit these patterns naturally.

## Key Results
- CLIP and DINOv2 models exhibit clear two-stage processing: early layers form disentangled object representations while later layers perform abstract same/different comparisons
- Shape and color properties are represented in separate linear subspaces during the perceptual stage, allowing independent manipulation of object features
- Same/different judgments generalize to interpolated representations but not to random vectors, indicating abstract relational representations
- Failures in either perceptual or relational stage prevent learning generalizable solutions to relational tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision transformers trained on relational reasoning tasks exhibit a two-stage processing pipeline consisting of a perceptual stage for forming disentangled object representations followed by a relational stage for abstract comparison operations.
- Mechanism: Early layers develop local attention patterns that bind features within individual objects (shape and color become separated into different linear subspaces), while later layers shift to global attention patterns that compare representations between objects or pairs.
- Core assumption: Object features can be disentangled into independent linear subspaces and these subspaces can be manipulated without interfering with each other's information.
- Evidence anchors:
  - [abstract] "we find that pretrained ViTs fine-tuned on this task often exhibit two qualitatively different stages of processing... a perceptual stage wherein local object features are extracted and stored in a disentangled representation, and a relational stage wherein object representations are compared."
  - [section 3] "we identify independent linear subspaces for color and shape in the intermediate representations produced in the early layers of CLIP-pretrained ViTs... we can extract either color or shape information from one object and inject it into another object."
  - [corpus] Weak evidence - only general vision transformer papers found, no direct evidence about two-stage processing pipelines in transformers.
- Break condition: If object features cannot be cleanly separated into independent subspaces, or if the attention patterns do not show the expected local-to-global transition, the two-stage processing fails.

### Mechanism 2
- Claim: The same-different operation learned by vision transformers generalizes to interpolated and added representations but not to completely random vectors.
- Mechanism: The model learns an abstract representation of the relation that is invariant to the specific perceptual properties of the objects, allowing it to operate on novel but structurally similar inputs.
- Core assumption: The model's relational computation is not simply memorizing specific object pairs but has learned a more abstract operation.
- Evidence anchors:
  - [abstract] "we find evidence that ViTs can sometimes learn to represent abstract visual relations... the same-different operation generalizes to additions and interpolations of shape & color representations, indicating that it does not rely on rote memorization of specific objects."
  - [section 5] "we find the greatest success when patching in added representations, followed by interpolated representations. We observe limited success when patching sampled representations, and no success patching random vectors."
  - [corpus] Weak evidence - no corpus papers specifically discussing generalization of abstract relations in transformers.
- Break condition: If the model's relational computation relies heavily on memorizing specific object features rather than learning an abstract operation, it will fail to generalize to novel representations.

### Mechanism 3
- Claim: Failures in either the perceptual or relational stage can prevent a model from learning generalizable solutions to relational tasks.
- Mechanism: The perceptual stage must successfully form disentangled object representations before the relational stage can operate on them; deficiencies in either stage create a bottleneck that prevents learning abstract relations.
- Core assumption: Both stages are necessary and sufficient for learning abstract relational operations; neither stage alone can compensate for deficiencies in the other.
- Evidence anchors:
  - [abstract] "we demonstrate that failures at either stage can prevent a model from learning a generalizable solution to our fairly simple tasks."
  - [section 7] "we observe a correlation between disentanglement and generalization... models might fail to learn either the perceptual or relational operations necessary to solve a task."
  - [corpus] Weak evidence - no corpus papers discussing stage-specific failures in transformer relational reasoning.
- Break condition: If a model fails to exhibit the expected two-stage processing pattern, or if interventions targeting only one stage do not improve performance, this mechanism is not operating.

## Foundational Learning

- Concept: Disentangled representations - the ability to separate different factors of variation (e.g., shape and color) into independent subspaces
  - Why needed here: The perceptual stage must form object representations where shape and color are represented separately so they can be manipulated independently
  - Quick check question: If you have two objects that differ only in color, can you change the color of one object without affecting its shape representation?

- Concept: Linear subspaces and intervention analysis - using linear algebra to identify and manipulate specific information within neural network representations
  - Why needed here: Distributed Alignment Search (DAS) identifies linear subspaces corresponding to shape and color, which are then used for intervention experiments
  - Quick check question: Given a high-dimensional vector, can you identify a lower-dimensional subspace that captures a specific feature (e.g., color) and manipulate just that subspace?

- Concept: Attention patterns and their interpretation - understanding how attention heads route information within transformer architectures
  - Why needed here: The two-stage processing is identified by analyzing how attention patterns shift from local (within-object) to global (between-object) operations
  - Quick check question: In a transformer processing two objects, do early attention heads primarily attend within each object, while later heads attend between objects?

## Architecture Onboarding

- Component map: Input image → patch embedding → encoder layers (0-2: perceptual stage forming disentangled representations, 3-6: transition, 6+: relational stage comparing objects) → classification head
- Critical path: The key insight is that meaningful processing stages emerge from the attention patterns rather than being explicitly architected, with early layers showing local attention patterns and later layers showing global attention patterns
- Design tradeoffs: The two-stage processing emerges from pretraining and fine-tuning rather than being explicitly designed in. This makes the architecture flexible but also means the processing stages are not guaranteed to emerge
- Failure signatures: Lack of clear local-to-global attention transition, poor performance on out-of-distribution stimuli, failure of interventions to manipulate object properties independently
- First 3 experiments:
  1. Analyze attention patterns across layers for a fine-tuned model to verify the two-stage processing emerges
  2. Apply DAS to identify linear subspaces for shape and color in early layers
  3. Test generalization by patching novel representations (added/interpolated vs random) into the identified subspaces

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding the relationship between pretraining scale and two-stage processing emergence, the specific mechanisms implementing perceptual and relational stages, and the generalizability of these findings to more complex relational tasks beyond simple same/different judgments.

## Limitations

- The synthetic datasets may not fully capture the complexity of real-world visual scenes where objects overlap, occlude each other, or have varying spatial relationships
- Mechanistic interpretability methods rely on linear approximations that may miss non-linear processing steps
- Intervention experiments cannot definitively prove that identified representations are the only or primary mechanisms for relational reasoning

## Confidence

**High confidence** in the core finding that vision transformers exhibit two-stage processing for relational tasks, supported by multiple lines of evidence including attention pattern analysis and intervention experiments.

**Medium confidence** in the claim that the same-different operation generalizes to interpolated representations but not random vectors. While the experimental results are compelling, the analysis of what constitutes "abstraction" versus memorization remains somewhat informal.

**Low confidence** in the claim that failures at either stage prevent learning generalizable solutions. The paper shows correlations between disentanglement, two-stage processing, and generalization, but causal relationships are not firmly established through controlled experiments.

## Next Checks

1. **Multi-object complexity test**: Extend the experimental paradigm to scenes with more than two objects and varying spatial configurations to test whether the two-stage processing generalizes beyond the simple two-object setup.

2. **Cross-architecture validation**: Apply the same mechanistic interpretability pipeline to other transformer architectures (e.g., Swin, DeiT) and non-transformer models to determine whether the observed patterns are specific to ViTs or represent a more general principle of deep learning for vision.

3. **Intervention ablation study**: Systematically disable or perturb the perceptual stage (e.g., by freezing early layers) while measuring the impact on relational reasoning performance and generalization, to establish more direct causal links between stage functionality and task success.