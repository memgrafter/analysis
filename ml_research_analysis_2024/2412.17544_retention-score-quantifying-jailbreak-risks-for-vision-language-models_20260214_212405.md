---
ver: rpa2
title: 'Retention Score: Quantifying Jailbreak Risks for Vision Language Models'
arxiv_id: '2412.17544'
source_url: https://arxiv.org/abs/2412.17544
tags:
- score
- text
- robustness
- image
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Retention Score, a novel metric to quantify
  jailbreak risks in Vision-Language Models (VLMs) by evaluating their robustness
  against adversarial image and text perturbations. The method uses diffusion-based
  generative models to create semantically similar samples and computes toxicity margins
  to provide an attack-agnostic robustness certificate.
---

# Retention Score: Quantifying Jailbreak Risks for Vision Language Models

## Quick Facts
- **arXiv ID**: 2412.17544
- **Source URL**: https://arxiv.org/abs/2412.17544
- **Reference count**: 23
- **Key outcome**: Introduces Retention Score metric to quantify VLM jailbreak robustness using diffusion-based generative models and toxicity margins

## Executive Summary
This paper introduces Retention Score, a novel metric to quantify jailbreak risks in Vision-Language Models (VLMs) by evaluating their robustness against adversarial image and text perturbations. The method uses diffusion-based generative models to create semantically similar samples and computes toxicity margins to provide an attack-agnostic robustness certificate. Experiments show that Retention Score consistently ranks VLM robustness and reveals that incorporating visual components generally decreases robustness compared to plain LLMs. The approach is computationally efficient (2-30× faster than adversarial attacks) and applicable to black-box APIs like GPT-4V and Gemini Pro Vision.

## Method Summary
The method uses conditional diffusion models (Stable Diffusion for images, DiffuSeq for text) to generate semantically similar samples from original image-text pairs. These samples are evaluated by the target VLM along with a toxicity judgment classifier to compute toxicity scores. The Retention Score is calculated as the margin between original and generated sample toxicity scores, providing an attack-agnostic robustness certificate. The approach only requires forward passes through the VLM, making it applicable to black-box APIs. The method evaluates both image-based (Retention-I) and text-based (Retention-T) robustness using benchmarks like RealToxicityPrompts and AdvBench Harmful Behaviours.

## Key Results
- Retention Score provides consistent robustness rankings across VLMs (MiniGPT-4, InstructBLIP, LLaVA) and correlates with adversarial attack results
- VLMs incorporating visual components show decreased robustness compared to plain LLMs
- The approach achieves 2-30× speedup over traditional adversarial attack methods
- Retention Score successfully evaluates black-box APIs like GPT-4V and Gemini Pro Vision

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Retention Score provides a certified robustness metric against jailbreak attacks.
- **Mechanism:** Uses diffusion-based generative models to create semantically similar image-text pairs and calculates toxicity margins between original and generated samples.
- **Core assumption:** The toxicity margin between original and generated samples provides a lower bound on the minimum perturbation needed to induce toxic responses.
- **Evidence anchors:**
  - [abstract]: "By calculating the margin in toxicity scores, we can quantify the robustness of the VLM in an attack-agnostic manner."
  - [section]: "Our process involves generating synthetic image-text pairs using a conditional diffusion model. These pairs are then predicted for toxicity score by a VLM alongside a toxicity judgment classifier. By calculating the margin in toxicity scores, we can quantify the robustness of the VLM in an attack-agnostic manner."
  - [corpus]: Weak - corpus contains related papers on VLM jailbreaking but none specifically validate margin-based robustness certification.
- **Break condition:** If the diffusion model fails to preserve semantic content between original and generated samples, the toxicity margin becomes unreliable.

### Mechanism 2
- **Claim:** Retention Score can evaluate black-box VLM APIs without internal access.
- **Mechanism:** Only requires forward passes through the VLM to obtain toxicity scores on generated samples, making it applicable to APIs like GPT-4V and Gemini Pro Vision.
- **Core assumption:** The VLM's toxicity scoring behavior is consistent and observable through API calls alone.
- **Evidence anchors:**
  - [abstract]: "The design of Retention Scores enables robustness evaluation of black-box VLMs APIs."
  - [section]: "When evaluating Retention Score on Gemini Pro Vision and GPT-4V, we find that the Retention Score is consistent in the security setting levels of Gemini Pro Vision."
  - [corpus]: Weak - corpus papers discuss VLM jailbreaking but don't specifically address black-box robustness evaluation methods.
- **Break condition:** If the black-box API implements rate limiting or response filtering that prevents consistent toxicity scoring.

### Mechanism 3
- **Claim:** Retention Score is computationally efficient compared to adversarial attacks.
- **Mechanism:** Avoids iterative optimization by using one-shot generative sampling and toxicity evaluation, achieving 2-30× speedup.
- **Core assumption:** A fixed number of generative samples provides sufficient statistical power to estimate robustness.
- **Evidence anchors:**
  - [abstract]: "Finally, our approach offers a time-efficient alternative to existing adversarial attack methods and provides consistent model robustness rankings when evaluated on VLMs including MiniGPT-4, InstructBLIP, and LLaVA."
  - [section]: "Figure 2 compares the run-time efficiency of Retention Score over adversarial attacks in (Qi et al. 2023a) and (Liu et al. 2023b). We show the improvement ratio of their average per-sample run-time and observe around 2-30 times improvement."
  - [corpus]: Weak - corpus papers discuss VLM jailbreaking efficiency but don't compare to margin-based evaluation methods.
- **Break condition:** If the generative model sampling becomes computationally expensive or the toxicity classifier requires significant inference time.

## Foundational Learning

- **Concept:** Lipschitz continuity and gradient-based robustness bounds
  - **Why needed here:** The paper's theoretical framework relies on establishing Lipschitz conditions for VLM toxicity functions to prove the certification property of Retention Score
  - **Quick check question:** If a function has Lipschitz constant L, what is the maximum change in output when the input changes by distance δ?

- **Concept:** Diffusion generative models and their sampling properties
  - **Why needed here:** The paper uses conditional diffusion models to generate semantically similar samples, which requires understanding how these models preserve semantic content
  - **Quick check question:** What is the relationship between the number of diffusion steps and the quality of generated samples?

- **Concept:** Semantic encoding/decoding for text
  - **Why needed here:** The paper transforms discrete text into continuous semantic space to enable perturbation analysis, using models like BART
  - **Quick check question:** How does the semantic encoder handle out-of-vocabulary words during the transformation process?

## Architecture Onboarding

- **Component map:** Image/text inputs → Diffusion generators (Stable Diffusion/DiffuSeq) → VLM forward pass → Toxicity classifier (Perspective API/LLM) → Margin calculation → Retention Score aggregation

- **Critical path:** Image/text generation → VLM forward pass → Toxicity scoring → Margin calculation → Retention Score aggregation

- **Design tradeoffs:**
  - Using diffusion models provides semantic preservation but may be slower than other generative approaches
  - Fixed sampling count vs. statistical convergence guarantees
  - Choice of toxicity classifier (API vs. LLM) affects consistency and cost

- **Failure signatures:**
  - Low variance in toxicity scores across generated samples suggests insufficient perturbation diversity
  - Inconsistent scores between original and generated samples may indicate semantic drift
  - Extremely high or low Retention Scores could indicate classifier threshold issues

- **First 3 experiments:**
  1. Validate semantic preservation by checking similarity between original and generated samples using CLIP embeddings
  2. Test robustness to different numbers of generative samples to find the minimal sufficient sample count
  3. Compare Retention Score rankings against known adversarial attack results on benchmark VLMs

## Open Questions the Paper Calls Out

- **Question:** How does Retention Score perform against other perturbation norms (L1, L∞) beyond the L2-norm currently evaluated?
- **Basis in paper:** [explicit] The paper states "One limitation could be that our framework of adversarial robustness evaluation using generative models is centered on L2-norm based perturbations."
- **Why unresolved:** The paper only validates Retention Score under L2-norm perturbations and explicitly notes this as a limitation without exploring alternative norms.
- **What evidence would resolve it:** Empirical comparison of Retention Score performance using L1 and L∞ norms versus L2-norm, showing whether the metric remains consistent across different perturbation metrics.

- **Question:** How do different generative model architectures (beyond Stable Diffusion) affect Retention Score reliability and computational efficiency?
- **Basis in paper:** [inferred] The paper uses Stable Diffusion for image generation and paraphrasing diffusion techniques for text, but doesn't explore alternative generative models or compare their impact on score reliability.
- **Why unresolved:** The authors use specific generative models but don't investigate whether alternative architectures might produce more reliable or efficient retention scores.
- **What evidence would resolve it:** Comparative study using multiple generative model architectures (GANs, VAEs, autoregressive models) to evaluate how different generative approaches affect Retention Score accuracy and computation time.

- **Question:** What is the relationship between Retention Score thresholds and actual real-world jailbreak success rates in deployed VLMs?
- **Basis in paper:** [explicit] The paper proves Retention Score serves as a certified robustness metric but doesn't empirically validate the correlation between score thresholds and actual attack success in production environments.
- **Why unresolved:** While theoretical certification is provided, there's no empirical validation showing how well Retention Score thresholds predict real-world attack success rates on deployed VLMs.
- **What evidence would resolve it:** Large-scale field study measuring actual jailbreak attempts on deployed VLMs, correlating Retention Score values with observed attack success rates to establish practical threshold guidelines.

## Limitations

- The framework is currently centered on L2-norm based perturbations, with other norms (L1, L∞) not explored
- Limited evaluation to specific demographic groups (young, old, male, female) for image-based robustness may not capture full jailbreak scenarios
- Correlation with adversarial attack results, while present, shows notable deviations that could impact practical deployment decisions

## Confidence

- **High confidence**: Computational efficiency claims (2-30× speedup over adversarial attacks) are well-supported by runtime comparisons
- **Medium confidence**: Black-box applicability to APIs like GPT-4V and Gemini Pro Vision is demonstrated but consistency across security settings needs more testing
- **Low confidence**: Universal robustness certification claim requires more validation due to imperfect correlation with adversarial attack results

## Next Checks

1. **Semantic Preservation Validation**: Implement CLIP embedding similarity checks between original and generated samples to quantify semantic drift and establish confidence intervals for toxicity margin calculations.

2. **Cross-Model Transferability Test**: Evaluate whether Retention Scores computed on one VLM (e.g., MiniGPT-4) accurately predict robustness on unseen models, testing the universality of the metric.

3. **Adversarial Attack Correlation Study**: Conduct a larger-scale comparison between Retention Scores and multiple adversarial attack methods across diverse VLM architectures to establish the strength and limitations of the correlation relationship.