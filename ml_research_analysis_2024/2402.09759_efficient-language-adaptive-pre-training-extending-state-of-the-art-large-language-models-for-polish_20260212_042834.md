---
ver: rpa2
title: 'Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large
  Language Models for Polish'
arxiv_id: '2402.09759'
source_url: https://arxiv.org/abs/2402.09759
tags:
- polish
- language
- arxiv
- tasks
- lapt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates efficient fine-tuning of English LLMs for
  Polish text generation using Language Adaptive Pre-training (LAPT) on a small dataset.
  The Curie-7B-v1 model achieved the lowest perplexity (3.02) among decoder-based
  Polish models and reached within 2% of the best Polish encoder-decoder models on
  8 out of 9 KLEJ tasks.
---

# Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large Language Models for Polish

## Quick Facts
- arXiv ID: 2402.09759
- Source URL: https://arxiv.org/abs/2402.09759
- Authors: Szymon Ruciński
- Reference count: 24
- Primary result: Curie-7B-v1 achieves lowest perplexity (3.02) among decoder-based Polish models using only 276 million tokens

## Executive Summary
This paper presents an efficient approach to adapting English large language models for Polish text generation using Language Adaptive Pre-training (LAPT). By training just 1.2% of parameters through LoRA adapters on a small, high-quality Polish dataset, the method achieves state-of-the-art performance while requiring only 2-3% of the data typically used for native Polish models. The Curie-7B-v1 model demonstrates that cross-lingual transfer learning can effectively extend existing models to new languages with minimal computational resources.

## Method Summary
The study employs Language Adaptive Pre-training on Mistral-7B using LoRA (Low-Rank Adaptation) to fine-tune the model for Polish. Training utilizes the SpeakLeash dataset (3.11 GB, 276 million tokens) with AdamW optimizer, learning rate 2.5e-5, and LoRA parameters (rank 32, alpha 16, dropout 0.05). The model is then fine-tuned on KLEJ benchmark tasks using weighted cross-entropy for imbalanced datasets. The entire process requires less than 5 days of training on a consumer GPU, significantly reducing computational requirements compared to traditional approaches.

## Key Results
- Achieved perplexity of 3.02, lowest among decoder-based Polish models
- Performance within 2% of best Polish encoder-decoder models on 8 out of 9 KLEJ tasks
- Required only 276 million tokens (2-3% of typical dataset size) and less than 5 days on consumer GPU

## Why This Works (Mechanism)

### Mechanism 1: LoRA Parameter Efficiency
- Claim: LoRA adapters allow efficient adaptation using only 1.2% of parameters
- Mechanism: Low-rank matrices approximate weight updates, reducing trainable parameters from billions to millions
- Core assumption: Original model has transferable linguistic patterns adaptable through low-rank modifications
- Evidence anchors: [abstract] "training just 1.2% of its parameters"; [section] LoRA approximation method
- Break condition: Linguistic distance too great for low-rank approximations to capture structural differences

### Mechanism 2: Domain-Adaptive Pre-training
- Claim: Targeted high-quality data achieves comparable performance to larger datasets
- Mechanism: Focused training on representative samples learns domain patterns more efficiently
- Core assumption: Quality and relevance of data matters more than quantity
- Evidence anchors: [abstract] "2-3% of typical dataset size"; [section] SpeakLeash dataset selection
- Break condition: Insufficient dataset diversity or quality degrades performance

### Mechanism 3: Cross-Lingual Transfer Learning
- Claim: English-trained models achieve Polish proficiency through targeted fine-tuning
- Mechanism: Foundational model's language understanding transfers to new language
- Core assumption: Core language capabilities are transferable across languages
- Evidence anchors: [abstract] "achieves the lowest perplexity"; [section] cross-lingual applicability
- Break condition: Target language has significantly different grammatical structure

## Foundational Learning

- **Transformer architecture fundamentals**: Understanding attention mechanisms and position encoding is crucial for adapting model architecture
  - Quick check question: What is the difference between causal and bidirectional attention in transformers?

- **LoRA adaptation mechanics**: Engineers need to understand low-rank matrix decomposition for efficient fine-tuning implementation
  - Quick check question: How does LoRA reduce trainable parameters while maintaining model capacity?

- **Perplexity metric interpretation**: Evaluating performance requires understanding what perplexity measures and how to interpret values
  - Quick check question: What does a perplexity score of 3.02 indicate about model performance?

## Architecture Onboarding

- **Component map**: Base LLM (Mistral-7B) → LoRA adapter → Polish dataset → Downstream task classifiers
- **Critical path**: Data preparation → LAPT training → Task-specific fine-tuning → Evaluation
- **Design tradeoffs**: Model size vs. training efficiency vs. performance quality
- **Failure signatures**: Overfitting after one epoch, nonsensical text generation, poor downstream task performance
- **First 3 experiments**:
  1. Validate LoRA adapter implementation by checking parameter count reduction
  2. Test perplexity improvement on held-out validation set after LAPT
  3. Verify task-specific classifier performance on KLEJ benchmark tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Curie-7B-v1 performance compare to native Polish models when scaled to larger datasets?
- Basis in paper: [explicit] Curie-7B-v1 used 2-3% of dataset size of best Polish models but achieved comparable results
- Why unresolved: Paper does not explore performance gains with dataset sizes similar to native Polish models
- What evidence would resolve it: Experiments training on comparable dataset sizes and comparing performance metrics

### Open Question 2
- Question: Can LAPT method be effectively applied to other low-resource languages beyond Polish?
- Basis in paper: [inferred] Success for Polish suggests potential applicability to other similar languages
- Why unresolved: Paper focuses solely on Polish without testing other low-resource languages
- What evidence would resolve it: Applying LAPT to other low-resource languages and evaluating performance gains

### Open Question 3
- Question: What are the long-term environmental impacts of using LAPT for language model adaptation?
- Basis in paper: [explicit] Discusses energy consumption and carbon offset for training Curie-7B-v1
- Why unresolved: Provides snapshot but not long-term effects compared with traditional methods
- What evidence would resolve it: Comprehensive life-cycle assessment over operational lifetimes

## Limitations

- Small training dataset (276 million tokens) may limit handling of rare linguistic patterns
- SpeakLeash dataset bias toward formal language affects performance on informal language tasks
- Single GPU training setup may not represent full capabilities with larger compute resources
- Cross-lingual transfer assumes significant linguistic overlap that may not generalize

## Confidence

**High Confidence Claims:**
- LoRA-based adaptation reduces trainable parameters to 1.2% while maintaining performance
- Curie-7B-v1 achieves state-of-the-art perplexity among decoder-based Polish models
- LAPT methodology requires significantly less training data and compute resources

**Medium Confidence Claims:**
- Model performs within 2% of best Polish encoder-decoder models on 8/9 KLEJ tasks
- Cross-lingual transfer from English to Polish is effective for general language understanding
- Training efficiency gains are reproducible across similar language pairs

**Low Confidence Claims:**
- Performance on tasks requiring informal language understanding (CBD task)
- Generalization to Polish dialects or specialized domains not in training data
- Performance comparison with larger models trained on more data

## Next Checks

1. **Dataset Diversity Analysis**: Conduct comprehensive linguistic diversity analysis of SpeakLeash dataset to quantify representation of different Polish language styles and correlate with task-specific performance gaps.

2. **Scaling Law Validation**: Systematically vary training dataset size (10%, 50%, 100%) and measure impact on perplexity and downstream task performance to establish scaling relationships.

3. **Cross-Lingual Transfer Generalization**: Apply same LAPT methodology to different language pair (e.g., English to Czech) to validate efficiency gains and performance levels are reproducible across target languages.