---
ver: rpa2
title: 'Sliding Windows Are Not the End: Exploring Full Ranking with Long-Context
  Large Language Models'
arxiv_id: '2412.14574'
source_url: https://arxiv.org/abs/2412.14574
tags:
- ranking
- full
- sliding
- window
- passage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the use of long-context large language
  models (LLMs) for passage ranking tasks. The authors compare the efficiency and
  effectiveness of two strategies: full ranking, where all passages are ranked in
  a single step, and sliding window, where passages are ranked in overlapping batches.'
---

# Sliding Windows Are Not the End: Exploring Full Ranking with Long-Context Large Language Models

## Quick Facts
- arXiv ID: 2412.14574
- Source URL: https://arxiv.org/abs/2412.14574
- Reference count: 22
- This paper proposes using full ranking with long-context LLMs for passage ranking tasks, achieving better efficiency and effectiveness than sliding window approaches

## Executive Summary
This paper investigates passage ranking using long-context large language models (LLMs) and compares two strategies: full ranking (processing all passages in a single step) and sliding window (processing passages in overlapping batches). Surprisingly, full ranking is more efficient but performs worse than sliding window in zero-shot settings. To address this, the authors propose a multi-pass sliding window approach for generating full ranking labels and an importance-aware loss function that weights higher-ranked passages more heavily. Experiments on TREC and BEIR benchmarks show that their fine-tuned full ranking model outperforms both sliding window and previous state-of-the-art methods, achieving about 4 and 2 points improvement on TREC and BEIR respectively, while reducing latency by 29.3% on TREC DL19.

## Method Summary
The method involves retrieving top-100 passages using BM25, then implementing both sliding window (with window size 20 and step size 10) and full ranking strategies using long-context LLMs. For full ranking, the authors generate training labels using a multi-pass sliding window approach that iteratively re-ranks remaining passages. They fine-tune the LLM using an importance-aware loss function that assigns higher weights to higher-ranked passages. The fine-tuned model is then evaluated on TREC and BEIR benchmarks using NDCG@10 as the primary metric.

## Key Results
- Full ranking reduces latency by 29.3% compared to sliding window on TREC DL19
- Fine-tuned full ranking model achieves 4-point improvement on TREC and 2-point improvement on BEIR benchmarks
- Full ranking reduces API costs by approximately 50% compared to sliding window
- Sliding window outperforms full ranking in zero-shot settings, but fine-tuning reverses this gap

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Full ranking strategy achieves better efficiency than sliding window by eliminating redundant evaluations of overlapping passages.
- **Mechanism:** Full ranking processes all passages in a single inference step, while sliding window evaluates the same passages multiple times due to overlapping windows. This reduces the total number of inference tokens and API costs.
- **Core assumption:** The long-context LLM can handle the entire passage list without significant performance degradation.
- **Evidence anchors:**
  - [abstract]: "Full ranking not only eliminates the repetitive and time-consuming sliding window process but also reduces redundant passage inference, thereby significantly lowering API costs."
  - [section]: "Ranking latency of full ranking strategy (light orange bar) is much smaller than sliding window strategy (light green bar) across all long-context LLMs."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.49, average citations=0.0. Weak evidence - corpus neighbors are not directly related to the specific mechanism.

### Mechanism 2
- **Claim:** The importance-aware loss function improves the effectiveness of full ranking by emphasizing higher-ranked passages.
- **Mechanism:** The importance-aware loss assigns higher weights to higher-ranked passages in the training label, ensuring that the model focuses more on ranking relevant passages correctly. This addresses the imbalance between relevant and irrelevant passages in the full ranking label.
- **Core assumption:** The importance-aware loss function effectively guides the model to prioritize higher-ranked passages during training.
- **Evidence anchors:**
  - [abstract]: "To address these issues, we first design a multi-pass sliding window approach, which iteratively generates a full ranking list as training label, overcoming the limitation of producing only the top-ranked passages in a single pass. Then, we propose an importance-aware loss that reweights the passage IDs in the label based on their rank position, ensuring that higher-ranked passage IDs receive more attention."
  - [section]: "By incorporating the importance-aware loss Lia, we ensure that higher-ranked passage IDs receive greater weight during loss calculation, which better aligns with the training of the full-ranking model."
  - [corpus]: Weak evidence - corpus neighbors do not directly support this specific mechanism.

### Mechanism 3
- **Claim:** The multi-pass sliding window approach generates high-quality full ranking labels for training the full ranking model.
- **Mechanism:** The multi-pass sliding window approach iteratively re-ranks the remaining passages after obtaining the top-ranked passages in each pass. This ensures that the full ranking label accurately reflects the relative relevance of all passages.
- **Core assumption:** The multi-pass sliding window approach can effectively generate a full ranking list that captures the global interactions among passages.
- **Evidence anchors:**
  - [abstract]: "To alleviate these issues, we first design a multi-pass sliding window approach, which iteratively generates a full ranking list as training label, overcoming the limitation of producing only the top-ranked passages in a single pass."
  - [section]: "In the first pass, the sliding window strategy is applied to rerank all 100 passages, yielding the top-10 most relevant passages. In the second pass, the same strategy is used to rerank the remaining 90 passages, producing the next 10 most relevant passages."
  - [corpus]: Weak evidence - corpus neighbors do not directly support this specific mechanism.

## Foundational Learning

- **Concept:** Long-context LLMs and their capabilities
  - **Why needed here:** Understanding the capabilities and limitations of long-context LLMs is crucial for designing and evaluating the full ranking strategy.
  - **Quick check question:** What are the key advantages and disadvantages of using long-context LLMs for passage ranking tasks?

- **Concept:** Listwise ranking and its challenges
  - **Why needed here:** Listwise ranking involves ranking a list of passages based on their relevance to a query, which is the core task addressed in this paper.
  - **Quick check question:** What are the main challenges in listwise ranking, and how do existing methods address these challenges?

- **Concept:** Supervised fine-tuning and its applications
  - **Why needed here:** Supervised fine-tuning is used to adapt the pre-trained long-context LLM to the specific task of passage ranking.
  - **Quick check question:** How does supervised fine-tuning differ from zero-shot learning, and what are the benefits of using supervised fine-tuning for passage ranking?

## Architecture Onboarding

- **Component map:** BM25 retrieval -> Sliding window ranking -> Full ranking with long-context LLM -> Multi-pass sliding window label generation -> Fine-tuning with importance-aware loss

- **Critical path:** Retrieve top-100 passages using BM25 → Generate full ranking labels using multi-pass sliding window approach and teacher model → Fine-tune the long-context LLM using the generated labels and importance-aware loss function → Evaluate the fine-tuned model on TREC and BEIR benchmarks

- **Design tradeoffs:**
  - Tradeoff between efficiency and effectiveness: Full ranking is more efficient but may be less effective than sliding window in zero-shot setting
  - Tradeoff between model size and performance: Larger models may achieve better performance but require more computational resources
  - Tradeoff between training data size and model performance: Using more training queries does not necessarily lead to better performance

- **Failure signatures:**
  - Poor ranking performance: The model may fail to accurately rank passages based on their relevance to the query
  - High latency: The model may take too long to rank passages, making it impractical for real-time applications
  - High API costs: The model may incur excessive API costs due to the need for multiple inference steps or large input sequences

- **First 3 experiments:**
  1. Compare the efficiency and effectiveness of full ranking and sliding window strategies on a small dataset
  2. Evaluate the impact of the importance-aware loss function on the ranking performance of the full ranking model
  3. Investigate the generalization ability of the fine-tuned full ranking model to different numbers of passages

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of full ranking models vary with different initial passage orders, and can training techniques improve robustness to these orders?
- **Basis in paper:** [explicit] The paper explores the impact of initial passage order on the performance of RankMistral100, finding that it significantly affects ranking performance.
- **Why unresolved:** The paper does not investigate training techniques to improve robustness to different initial passage orders.
- **What evidence would resolve it:** Experiments comparing the performance of full ranking models trained with and without techniques like shuffling passage order during training, across various initial passage orders.

### Open Question 2
- **Question:** What is the optimal number of ranking passes for the multi-pass sliding window approach, and how does it affect the quality of the full ranking list?
- **Basis in paper:** [inferred] The paper uses a multi-pass sliding window approach but does not explore the optimal number of passes or its impact on ranking quality.
- **Why unresolved:** The paper does not provide an analysis of the trade-off between the number of passes and the quality of the generated ranking list.
- **What evidence would resolve it:** Comparative experiments evaluating the performance of models trained with different numbers of ranking passes, measuring the quality of the generated full ranking lists and downstream ranking performance.

### Open Question 3
- **Question:** How does the performance of full ranking models scale with larger long-context LLMs, such as those with 30B or 70B parameters?
- **Basis in paper:** [explicit] The paper acknowledges that due to limited resources, they could not experiment with larger open-source long-context LLMs.
- **Why unresolved:** The paper does not investigate the impact of model size on the effectiveness and efficiency of full ranking strategy.
- **What evidence would resolve it:** Experiments comparing the performance of full ranking models based on LLMs with different parameter sizes, measuring both ranking effectiveness and efficiency.

## Limitations

- **Methodological uncertainty in zero-shot baseline comparison:** The surprising finding that sliding window outperforms full ranking in zero-shot settings may be influenced by prompt design differences that weren't fully controlled for in the experiments.

- **Training data size concerns:** The study uses only 100 training queries for fine-tuning, which may not be sufficient to properly evaluate the effectiveness of the proposed approach, despite the authors' acknowledgment that more queries don't necessarily improve performance.

- **Label generation reliability:** The multi-pass sliding window approach for generating full ranking labels relies on the sliding window model's accuracy, meaning any systematic biases or errors in the sliding window model could propagate to the full ranking labels.

## Confidence

- **High confidence:** The efficiency advantages of full ranking over sliding window are well-supported with concrete latency measurements and API cost analysis.
- **Medium confidence:** The effectiveness improvements from the fine-tuned full ranking model are moderately supported by experimental results, though the small training set size and surprising zero-shot results warrant caution.
- **Low confidence:** The claim that sliding window outperforms full ranking in zero-shot settings requires further investigation due to potential uncontrolled prompt design differences.

## Next Checks

1. **Controlled prompt ablation study:** Conduct experiments where the same prompt template is used for both sliding window and full ranking strategies in zero-shot settings to isolate the impact of the ranking strategy itself from potential prompt design effects.

2. **Larger training set evaluation:** Fine-tune the full ranking model using 1,000+ training queries (rather than just 100) and evaluate whether the performance improvements scale with training data size, addressing the authors' own concern about training set adequacy.

3. **Cross-model generalization test:** Evaluate whether the full ranking approach provides consistent advantages when applied to different base models (e.g., LLaMA 3.1-8B and Qwen2.5-7B) beyond just the Mistral-7B model used in the primary experiments.