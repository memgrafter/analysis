---
ver: rpa2
title: Topic Modeling with Fine-tuning LLMs and Bag of Sentences
arxiv_id: '2408.03099'
source_url: https://arxiv.org/abs/2408.03099
tags:
- topic
- topics
- sentence
- document
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses topic modeling by proposing a method to fine-tune
  pre-trained language models for improved performance. The core idea involves constructing
  a training dataset using heuristic methods to identify pairs of sentence groups
  that are either similar or dissimilar, followed by refining this dataset by removing
  likely mislabeled pairs.
---

# Topic Modeling with Fine-tuning LLMs and Bag of Sentences

## Quick Facts
- arXiv ID: 2408.03099
- Source URL: https://arxiv.org/abs/2408.03099
- Reference count: 6
- Primary result: SenClu achieves state-of-the-art topic coherence and coverage with fast inference using hard sentence group assignments

## Executive Summary
This paper presents a novel approach to topic modeling that combines fine-tuning pre-trained language models with a bag-of-sentences framework. The method, called FT-Topic, automatically constructs training data by identifying similar and dissimilar sentence group pairs, then refines this data to improve quality. This fine-tuned model is used in SenClu, a topic modeling method that assigns sentence groups to topics using hard assignments rather than probabilistic distributions, achieving both high performance and computational efficiency.

## Method Summary
The approach consists of two main components: fine-tuning an LLM encoder and a novel topic modeling algorithm. First, sentence groups are formed from documents, and pairs are identified as similar (from same document) or dissimilar (from different documents). This training data is refined by removing likely mislabeled pairs based on similarity scores. The LLM is then fine-tuned using triplet loss on this refined dataset. For topic modeling, SenClu uses an expectation-maximization algorithm with hard assignments, clustering sentence groups into topics and computing word-topic scores. The method balances granularity between individual words and entire documents.

## Key Results
- SenClu with fine-tuned models achieves state-of-the-art normalized PMI and NMI scores
- Hard assignment strategy provides faster inference compared to probabilistic models
- The method maintains reasonable computational demands while improving topic coherence and coverage

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning LLM encoders using unsupervised data improves topic modeling quality compared to out-of-the-box embeddings. By training the LLM to embed similar sentence groups closely and dissimilar groups far apart based on document proximity, the resulting embeddings better capture topical similarity. This assumes that nearby sentence groups in documents share topics while groups from different documents are likely about different topics.

### Mechanism 2
Using sentence groups as the basic unit (Bag of Sentences) instead of individual words improves topic coherence and coverage. Treating sequences of sentences as elementary units captures more contextual information than single words, reducing noise from common words and allowing more coherent topic assignments. This assumes that a sequence of sentences typically belongs to one or few topics, providing sufficient context for meaningful topic assignment.

### Mechanism 3
The hard assignment strategy in SenClu improves computational efficiency while maintaining topic quality. Instead of soft probabilistic assignments like LDA, each sentence group is firmly assigned to exactly one topic, simplifying computation and reducing the search space. This assumes that most sentence groups can be meaningfully assigned to a single topic without significant loss of information.

## Foundational Learning

- **Sentence embeddings and their computation**: Why needed - The entire method relies on computing meaningful vector representations of sentence groups to capture topical similarity. Quick check - How does a sentence transformer model like Sentence-BERT differ from standard BERT in terms of output?

- **Clustering algorithms and their optimization**: Why needed - SenClu is essentially a clustering algorithm with priors, using an EM-like approach to find optimal topic assignments. Quick check - What is the difference between hard and soft clustering assignments, and what are the trade-offs?

- **Triplet loss and contrastive learning**: Why needed - The fine-tuning process uses triplet loss to train the LLM to distinguish between similar and dissimilar sentence groups. Quick check - How does triplet loss differ from contrastive loss, and when would you choose one over the other?

## Architecture Onboarding

- **Component map**: Document → Sentence tokenization → Sentence group formation (BoS) → LLM fine-tuning (FT-Topic) → Embedding computation → Similarity calculation → SenClu inference → Hard topic assignment → Word-topic score calculation → Evaluation (PMI and NMI metrics)

- **Critical path**: Document → Sentence groups → Embeddings → Topic assignment → Word-topic scores

- **Design tradeoffs**: Sentence group size vs. context vs. computational cost; Number of topics vs. granularity vs. interpretability; Hard vs. soft assignments vs. accuracy vs. speed; Fine-tuning duration vs. embedding quality vs. training time

- **Failure signatures**: Low PMI/NMI scores → Poor embeddings or inappropriate sentence group size; Topics dominated by common words → Need for better word-topic scoring or preprocessing; Extremely long computation times → Sentence group size too large or insufficient hardware

- **First 3 experiments**: 
  1. Run SenClu with different sentence group sizes (1, 3, 5 sentences) on a small dataset to find the optimal balance
  2. Compare PMI scores with and without fine-tuning to verify the impact of FT-Topic
  3. Test different values of the prior α parameter to observe its effect on topic diversity per document

## Open Questions the Paper Calls Out

### Open Question 1
How does the number of negative samples used in the triplet loss affect the fine-tuning process and the quality of the resulting topic models? The paper mentions that one might increase the number of training samples by choosing more random sentence groups as negative samples, but did not experiment with these options.

### Open Question 2
How sensitive is the SenClu algorithm to the choice of the number of sentences per group (ns)? The paper mentions that a grouping size from 1 to 5 sentences is deemed optimal, but does not provide a detailed analysis of the impact of different group sizes on the algorithm's performance.

### Open Question 3
How does the choice of the base model for fine-tuning affect the quality of the resulting topic models? The paper uses a sentence transformer as the base model for fine-tuning, but does not explore the impact of using different base models.

## Limitations

- The locality assumption (nearby sentence groups share topics) may not hold across diverse document types and domains
- The evaluation relies solely on PMI and NMI metrics without human judgment or comparison to recent methods
- Computational cost analysis is superficial and lacks detailed runtime comparisons or hardware requirements

## Confidence

- **High Confidence**: The SenClu inference algorithm is well-defined and reproducible
- **Medium Confidence**: The fine-tuning methodology is sound but hyperparameter sensitivity isn't explored
- **Low Confidence**: Claims about state-of-the-art performance without comprehensive ablation studies or comparison to recent methods

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary the fraction of data removed during refinement (fpos, ftri) and the margin parameter m in triplet loss. Track how these changes affect PMI scores across multiple datasets to identify optimal values and their stability.

2. **Domain Transfer Testing**: Apply the fine-tuned model to datasets from different domains (news, scientific papers, social media) and measure performance degradation. This would reveal whether the locality assumption breaks down in specific contexts and identify failure patterns.

3. **Ablation of Sentence Group Size**: Run comprehensive experiments with sentence group sizes ranging from 1 to 10 sentences on a standardized corpus. Measure the trade-off between coherence (PMI) and coverage (NMI) to identify the optimal balance point and understand where the bag-of-sentences assumption fails.