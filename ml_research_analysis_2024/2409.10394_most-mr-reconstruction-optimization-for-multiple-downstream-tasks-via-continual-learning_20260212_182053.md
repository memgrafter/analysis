---
ver: rpa2
title: 'MOST: MR reconstruction Optimization for multiple downStream Tasks via continual
  learning'
arxiv_id: '2409.10394'
source_url: https://arxiv.org/abs/2409.10394
tags:
- reconstruction
- downstream
- task
- network
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a continual learning approach (MOST) to optimize
  a single MR reconstruction network for multiple downstream tasks without catastrophic
  forgetting. The method combines replay-based continual learning with an image-guided
  loss, storing subsets of previous task data in a buffer and using them during finetuning.
---

# MOST: MR reconstruction Optimization for multiple downStream Tasks via continual learning

## Quick Facts
- **arXiv ID**: 2409.10394
- **Source URL**: https://arxiv.org/abs/2409.10394
- **Reference count**: 34
- **Primary result**: Sequential finetuning of MR reconstruction network for multiple downstream tasks via replay-based continual learning with image-guided loss prevents catastrophic forgetting

## Executive Summary
This paper addresses the challenge of optimizing a single MR reconstruction network for multiple downstream tasks (segmentation and classification) without catastrophic forgetting. The proposed MOST method combines replay-based continual learning with an image-guided loss function, storing subsets of previous task data in a buffer and using them during finetuning. Evaluated on tasks including reconstruction, segmentation, and classification using datasets like FastMRI, OASIS, BraTS, and ADNI, MOST outperformed naïve finetuning and conventional continual learning methods, achieving superior last-task metrics and lower forgetting measures.

## Method Summary
MOST implements a replay-based continual learning approach that sequentially finetunes a single MR reconstruction network for multiple downstream tasks. The method stores input-output pairs from previous tasks in a buffer and replays them every K iterations during new task training. An image-guided loss computes reconstruction fidelity between the intermediate reconstructed image and the aliasing-free image, reinforcing earlier learned reconstruction patterns. This approach was tested on T1-weighted MR images from multiple datasets, using undersampled k-space data generated with acceleration factor of 4, and evaluated using SSIM for reconstruction quality, DICE for segmentation, and AUC for classification.

## Key Results
- MOST achieved superior last-task metrics and lower forgetting measures compared to naïve finetuning and conventional continual learning methods
- Ablation studies confirmed the effectiveness of both the replay mechanism and image-guided loss in preventing catastrophic forgetting
- MOST maintained performance across different task orders and buffer sizes, demonstrating robustness for sequential multi-task optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The replay buffer combined with image-guided loss prevents catastrophic forgetting in sequential finetuning of the reconstruction network.
- Mechanism: The replay buffer stores input-output pairs from previous tasks and replays them every K iterations during new task training. The image-guided loss computes reconstruction fidelity between the intermediate reconstructed image and the aliasing-free image, reinforcing earlier learned reconstruction patterns.
- Core assumption: Replaying past task data and enforcing reconstruction fidelity is sufficient to maintain performance across sequentially introduced downstream tasks.
- Evidence anchors:
  - [abstract] "MOST integrated techniques from replay-based continual learning and image-guided loss to overcome catastrophic forgetting."
  - [section] "In our proposed approach, MOST, replay-based continual learning is combined with an image-guided (IG) loss to prevent catastrophic forgetting during sequential finetuning for multiple downstream tasks."
  - [corpus] Weak. No direct corpus evidence on replay+IG combination in MRI reconstruction.

### Mechanism 2
- Claim: Sequential finetuning of the reconstruction network for multiple downstream tasks is feasible when using limited task-specific datasets.
- Mechanism: Instead of training separate networks for each task, a single reconstruction network is finetuned sequentially using a subset of data from previous tasks stored in the buffer, reducing computational burden and data storage needs.
- Core assumption: The reconstruction network can adapt to multiple downstream tasks without retraining from scratch, and the buffer provides sufficient representation of previous tasks.
- Evidence anchors:
  - [abstract] "Expanding this optimization to multi-task scenarios is not straightforward. In this work, we extended this optimization to sequentially introduced multiple downstream tasks..."
  - [section] "Our scenario assumes limited access to previously trained datasets due to privacy concerns and computational costs."
  - [corpus] Weak. No direct corpus evidence on sequential finetuning in MRI reconstruction context.

### Mechanism 3
- Claim: Image-guided loss provides domain-specific supervision that complements task-specific losses during finetuning.
- Mechanism: For segmentation tasks, the image-guided loss enforces that reconstructed images maintain fidelity to the aliasing-free reference, which helps downstream networks work with consistent image characteristics across tasks.
- Core assumption: Maintaining reconstruction quality through image-guided loss is crucial for downstream task performance when tasks have different data distributions.
- Evidence anchors:
  - [abstract] "Expanding this optimization to multi-task scenarios is not straightforward... MOST integrated techniques from replay-based continual learning and image-guided loss to overcome catastrophic forgetting."
  - [section] "This loss, which is calculated using a reconstruction lossLR, computes the difference between the intermediate reconstructed imagefR(xt; θ) and the aliasing-free imageyt to enhance the performance of the reconstruction network."
  - [corpus] Weak. No direct corpus evidence on image-guided loss in continual learning for MRI.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper explicitly addresses catastrophic forgetting as the main challenge in sequential finetuning across multiple downstream tasks.
  - Quick check question: What happens to performance on Task 1 when a network is finetuned on Task 2 without any mitigation strategy?

- Concept: Continual learning techniques
  - Why needed here: The proposed MOST approach is based on replay-based continual learning, which is essential for understanding how the method prevents forgetting.
  - Quick check question: How does replay-based continual learning differ from regularization-based approaches like Elastic Weight Consolidation?

- Concept: Domain adaptation and generalization
  - Why needed here: The paper mentions domain gaps between training datasets for reconstruction and downstream tasks, which affects performance.
  - Quick check question: Why might a reconstruction network trained on one dataset perform poorly when cascaded with a downstream network trained on a different dataset?

## Architecture Onboarding

- Component map: Input → Reconstruction network → Buffer storage/replay → Downstream network → Task-specific loss → Image-guided loss → Parameter update
- Critical path: The reconstruction network receives undersampled input, generates reconstructions, which are evaluated by both downstream task networks and image-guided loss, with parameter updates incorporating both task-specific and reconstruction fidelity signals
- Design tradeoffs:
  - Buffer size vs. memory constraints vs. forgetting prevention
  - Frequency of buffer replay (K parameter) vs. training efficiency vs. forgetting
  - Complexity of image-guided loss vs. reconstruction fidelity vs. task-specific performance
- Failure signatures:
  - Increasing forgetting measures (FM) across tasks indicates buffer or replay frequency issues
  - Degradation in reconstruction SSIM suggests image-guided loss is insufficient
  - Poor downstream task performance despite good reconstruction indicates domain gap problems
- First 3 experiments:
  1. Run baseline without any finetuning to establish reference performance
  2. Implement naïve sequential finetuning to confirm catastrophic forgetting occurs
  3. Test MOST with buffer size variation (4, 10, 50 subjects) to find optimal memory-accuracy tradeoff

## Open Questions the Paper Calls Out

- The paper did not explicitly call out open questions, but identified limitations including reliance on synthetic undersampling rather than clinically acquired undersampled data, vague buffer management strategy description, and unspecified CNN classifier architecture for downstream tasks.

## Limitations

- Evaluation relies on synthetic undersampling rather than clinically acquired undersampled data, which may not reflect real-world performance
- Buffer management strategy when full is vaguely described beyond "round-robin" replacement
- Exact architecture of the CNN classifier for downstream tasks is unspecified

## Confidence

- **High confidence**: MOST's effectiveness for preventing catastrophic forgetting when replay buffer and image-guided loss are properly implemented
- **Medium confidence**: Claims about computational efficiency gains, as full training cost comparisons are not provided
- **Medium confidence**: Domain adaptation claims, given limited testing across diverse clinical scenarios

## Next Checks

1. Test MOST on clinically acquired undersampled MRI data to validate real-world performance
2. Evaluate buffer replacement strategy sensitivity by comparing different policies (random, oldest, least recently used)
3. Conduct ablation study specifically isolating the contribution of image-guided loss by testing with and without it on identical buffer configurations