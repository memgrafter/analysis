---
ver: rpa2
title: 'MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models'
arxiv_id: '2410.08182'
source_url: https://arxiv.org/abs/2410.08182
tags:
- image
- knowledge
- visual
- images
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MRAG-Bench, a vision-centric benchmark for
  evaluating retrieval-augmented multimodal models. The benchmark consists of 16,130
  images and 1,353 human-annotated multiple-choice questions across 9 scenarios where
  visual knowledge is more beneficial than textual knowledge, such as identifying
  objects from different angles or recognizing objects undergoing transformations.
---

# MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models

## Quick Facts
- arXiv ID: 2410.08182
- Source URL: https://arxiv.org/abs/2410.08182
- Reference count: 40
- Primary result: Proprietary LVLMs outperform open-source models in utilizing retrieved visual knowledge, with humans showing 33.16% improvement with ground-truth knowledge versus 5.82% for GPT-4o

## Executive Summary
This paper introduces MRAG-Bench, a benchmark specifically designed to evaluate retrieval-augmented multimodal models in scenarios where visual knowledge provides advantages over textual knowledge. The benchmark contains 16,130 images and 1,353 human-annotated multiple-choice questions across 9 vision-centric scenarios including object identification from different angles, partial views, and biological transformations. Evaluation of 14 large vision-language models reveals that all models achieve greater improvements when augmented with images compared to textual knowledge, confirming the vision-centric nature of the benchmark. The results highlight significant gaps between proprietary and open-source models in utilizing retrieved visual knowledge, and between human and model performance with ground-truth knowledge.

## Method Summary
The MRAG-Bench evaluation involves using large vision-language models (LVLMs) to answer multiple-choice questions where external knowledge retrieval (either visual or textual) can provide additional context. The benchmark consists of 16,130 images and 1,353 questions across 9 scenarios designed to emphasize visual knowledge advantages. Models are evaluated with three conditions: no retrieval, retrieved knowledge, and ground-truth knowledge. A CLIP-based retriever is used to fetch relevant images and Wikipedia text passages. Accuracy scores are calculated for each model across all scenarios to assess performance improvements from retrieval augmentation.

## Key Results
- All 14 evaluated LVLMs show greater performance improvements with image retrieval compared to textual retrieval, confirming the vision-centric design of MRAG-Bench
- GPT-4o achieves 68.68% accuracy without RAG and 74.5% with ground-truth knowledge, significantly outperforming open-source models
- Humans improve by 33.16% with ground-truth knowledge compared to only 5.82% improvement for GPT-4o, highlighting the gap in knowledge utilization capabilities
- Open-source models fail to effectively utilize noisy retrieved multimodal knowledge, while proprietary models can distinguish between high-quality and poor-quality examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision-centric retrieval augmentation improves LVLM performance more than textual retrieval because visual knowledge fills specific perceptual gaps that text cannot capture
- Mechanism: The benchmark isolates scenarios where visual information (e.g., different angles, deformations, biological transformations) provides unique cues that textual descriptions fail to convey. Models that retrieve and integrate visual examples can better resolve ambiguity and complete partial visual patterns.
- Core assumption: There exist real-world scenarios where visual knowledge is more informative than textual knowledge for LVLM reasoning
- Evidence anchors:
  - [abstract] "all LVLMs exhibit greater improvements when augmented with images compared to textual knowledge, confirming that MRAG-BENCH is vision-centric"
  - [section] "There are scenarios where retrieving visual information is either more beneficial or easier to access than textual data"
- Break condition: If scenarios are not carefully curated to emphasize visual advantage, textual retrieval may show equal or greater improvement

### Mechanism 2
- Claim: Open-source LVLMs struggle with retrieved multimodal knowledge due to inferior quality discrimination capabilities
- Mechanism: Proprietary models like GPT-4o can distinguish between high-quality and poor-quality retrieved visual examples, effectively filtering noise. Open-source models lack this capability, leading to performance degradation when noisy retrieved images are included.
- Core assumption: The ability to discriminate between good and bad retrieved examples is a learned skill that differs across model architectures
- Evidence anchors:
  - [section] "while all models improve with GT knowledge, only proprietary models are able to effectively utilize noisy retrieved multimodal knowledge"
  - [section] "Open-source models are falling short on their parametric knowledge and the ability to distinguish between high-quality and poor-quality retrieved visually augmented examples"
- Break condition: If retrieval quality is consistently high or if open-source models develop better filtering mechanisms

### Mechanism 3
- Claim: Human performance improves more dramatically with ground-truth knowledge than LVLM performance due to superior visual reasoning and knowledge utilization
- Mechanism: Humans can integrate visual information more flexibly and effectively than current LVLMs, achieving 33.16% improvement with ground-truth knowledge versus 5.82% for GPT-4o. This suggests humans have stronger capabilities in visual pattern recognition and knowledge synthesis.
- Core assumption: The gap between human and LVLM performance with visual knowledge reflects fundamental limitations in current LVLM architectures
- Evidence anchors:
  - [abstract] "humans improve by 33.16% with ground-truth knowledge compared to 5.82% for GPT-4o"
  - [section] "These results highlight the importance of MRAG-BENCH in encouraging the community to develop LVLMs better utilizing of visually augmented knowledge"
- Break condition: If future LVLMs develop architectures that close this performance gap

## Foundational Learning

- Concept: Multimodal Retrieval-Augmented Generation (MRAG)
  - Why needed here: The benchmark evaluates models that retrieve external knowledge (visual or textual) to augment their generation capabilities for visual question answering
  - Quick check question: What distinguishes MRAG from standard text-only RAG in this context?

- Concept: Visual versus textual knowledge complementarity
  - Why needed here: Understanding when visual information provides unique advantages over text is central to interpreting the benchmark results
  - Quick check question: Can you identify scenarios where textual descriptions would be insufficient for LVLM reasoning?

- Concept: Quality discrimination in retrieved information
  - Why needed here: The performance difference between open-source and proprietary models depends on their ability to distinguish useful from noisy retrieved examples
  - Quick check question: How might a model's architecture affect its ability to filter low-quality retrieved information?

## Architecture Onboarding

- Component map: Query formulation -> CLIP-based retriever (visual and textual) -> LVLM (with RAG knowledge) -> answer generation -> accuracy evaluation
- Critical path: Query formulation → retrieval (visual or textual) → knowledge integration → answer generation → accuracy evaluation
- Design tradeoffs:
  - Visual vs textual retrieval: Visual retrieval is more beneficial for specific scenarios but may be more computationally expensive
  - Retriever choice: CLIP retriever used consistently, but other retrievers show varying performance
  - Number of retrieved examples: Experiments show 5 examples as default, but optimal number varies by scenario complexity
- Failure signatures:
  - Open-source models declining performance with retrieved knowledge (indicating poor quality discrimination)
  - Inconsistent improvement across scenarios (suggesting scenario-specific limitations)
  - Position bias in retrieved examples affecting model performance
- First 3 experiments:
  1. Replicate the main comparison between visual and textual retrieval improvements across all 14 LVLMs
  2. Test different numbers of ground-truth examples (1, 2, 3, 5, 10, 20) to identify optimal retrieval quantity
  3. Compare different multimodal retrievers (CLIP, MagicLens, E5-V, VISTA) to assess their impact on LVLM performance

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark focuses on specific vision-centric scenarios that may not generalize to broader visual reasoning tasks
- Retrieval quality assessment relies on Recall@5 metrics, which may not fully capture the relevance and utility of retrieved examples for model performance
- Human evaluation methodology is not fully detailed, limiting assessment of how closely human performance reflects optimal knowledge utilization

## Confidence

High confidence: The vision-centric nature of MRAG-Bench is well-established through consistent improvements across models when augmented with visual versus textual knowledge

Medium confidence: The superiority of proprietary models in utilizing retrieved knowledge is supported but requires further investigation into quality discrimination mechanisms

Medium confidence: The gap between human and LVLM performance with ground-truth knowledge is clearly demonstrated, though the exact implications for LVLM development remain to be fully explored

## Next Checks

1. **Retrieval quality validation**: Conduct detailed analysis of retrieved example relevance scores to verify that visual retrieval consistently provides higher quality information than textual retrieval across all scenarios

2. **Open-source model adaptation**: Test whether fine-tuning open-source LVLMs on MRAG-Bench data improves their ability to discriminate between high and low-quality retrieved examples

3. **Human baseline refinement**: Implement controlled human experiments with varying numbers of ground-truth examples to establish a clearer understanding of optimal knowledge utilization patterns compared to LVLM behavior