---
ver: rpa2
title: Xmodel-2 Technical Report
arxiv_id: '2412.19638'
source_url: https://arxiv.org/abs/2412.19638
tags:
- zhang
- xmodel-2
- data
- wang
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Xmodel-2 is a 1.2-billion-parameter language model optimized for
  reasoning tasks. It uses a deep-and-thin architecture with embedding sharing and
  grouped-query attention, enabling different model scales to share a unified set
  of hyperparameters.
---

# Xmodel-2 Technical Report

## Quick Facts
- **arXiv ID**: 2412.19638
- **Source URL**: https://arxiv.org/abs/2412.19638
- **Reference count**: 30
- **Primary result**: 1.2-billion-parameter language model achieving state-of-the-art reasoning performance with 39.62% average accuracy on benchmarks

## Executive Summary
Xmodel-2 is a 1.2-billion-parameter language model specifically optimized for reasoning tasks through a combination of architectural innovations and training methodology refinements. The model employs a deep-and-thin architecture with embedding sharing and grouped-query attention, enabling different model scales to share a unified set of hyperparameters. Through extensive experimentation including over 400 trials on hyperparameter optimization, Xmodel-2 achieves significant performance gains in complex reasoning and agent-based tasks, demonstrating its potential for real-world applications while maintaining computational efficiency.

## Method Summary
Xmodel-2 implements a deep-and-thin architecture (48 layers, 1536 hidden size) with embedding sharing to reduce parameters by 0.1B, combined with grouped-query attention for improved inference efficiency. The model is pretrained on 1.5 trillion tokens using the WSD learning rate scheduler from MiniCPM, with a data ratio optimization approach that allocates 64% to supervised fine-tuning data. The training pipeline employs maximal update parametrization (μP) to enable seamless hyperparameter transfer from smaller models (nano, 6M; tiny, 54M) to the full 1.2B parameter model, eliminating the need for extensive hyperparameter search on the target scale.

## Key Results
- Achieves 39.62% average accuracy on complex reasoning benchmarks
- Demonstrates 14.21% success rate on agent-based tasks
- Shows state-of-the-art performance in reasoning and agent tasks through efficient deep-and-thin architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Xmodel-2's deep-and-thin architecture with embedding sharing reduces parameter count while maintaining performance.
- Mechanism: Embedding sharing in small language models (SLMs) significantly reduces parameters by sharing the embedding layer, which constitutes a large portion of total parameters in SLMs. The deep-and-thin architecture (48 layers, 1536 hidden size) provides sufficient model capacity while keeping parameter count low.
- Core assumption: The embedding layer's contribution to total parameters is significant enough in SLMs to warrant sharing for efficiency gains.
- Evidence anchors:
  - [section]: "To improve efficiency, we implement embedding sharing, which reduces the parameter count by 0.1B."
  - [section]: "The embedding layer constitutes a significant portion of the total parameters."
  - [corpus]: Weak evidence - no direct mention of embedding sharing efficiency in related papers.
- Break condition: If the embedding layer's contribution to total parameters is not significant in the specific scale of Xmodel-2, the efficiency gain from sharing would be minimal.

### Mechanism 2
- Claim: The WSD learning rate scheduler with data ratio optimization improves training efficiency and reasoning performance.
- Mechanism: The WSD scheduler provides stable convergence during training, while data ratio optimization (64% SFT data) ensures the model learns from high-quality instruction-formatted data during the decay phase, improving reasoning capabilities.
- Core assumption: The interaction between SFT data and pretraining data during the WSD decay phase is more effective than traditional cosine decay or pretraining-only approaches.
- Evidence anchors:
  - [abstract]: "To maximize training efficiency and stability, Xmodel-2 employs the WSD learning rate scheduler from MiniCPM."
  - [section]: "Our work explores the interaction between SFT data and domain-specific pretraining data during the WSD decay phase."
  - [section]: "Through over 400 trials, we identified that the optimal SFT data ratio falls between 60% and 69%."
  - [corpus]: Weak evidence - no direct mention of WSD scheduler effectiveness in related papers.
- Break condition: If the optimal SFT data ratio changes significantly with different pretraining data distributions or model scales, the fixed 64% ratio may become suboptimal.

### Mechanism 3
- Claim: Maximal update parametrization (μP) enables seamless transfer of optimal hyperparameters from smaller to larger models.
- Mechanism: μP ensures that hyperparameters optimized on smaller models (nano, 6M; tiny, 54M) remain stable when transferred to larger models, reducing the need for extensive hyperparameter search on the full-scale model.
- Core assumption: Hyperparameters optimized through Bayesian optimization on small models remain stable across model scales when μP is applied.
- Evidence anchors:
  - [section]: "Unlike other models, Xmodel-2 incorporates an innovative architecture based on Tensor Programs [Yang et al., 2022] [Yang et al., 2023], enabling models of different scales to share the same set of hyperparameters."
  - [section]: "We observed that μP hyperparameters remained stable across model scales."
  - [corpus]: Weak evidence - no direct mention of μP effectiveness in related papers.
- Break condition: If the relationship between model scale and optimal hyperparameters deviates from μP assumptions at certain scales, the transferred configurations may become suboptimal.

## Foundational Learning

- Concept: Learning rate scheduling and its impact on training stability
  - Why needed here: Understanding the WSD scheduler's role in maximizing training efficiency and ensuring stable convergence is crucial for replicating or improving upon Xmodel-2's training approach.
  - Quick check question: How does the WSD scheduler differ from traditional cosine decay, and what advantages does it provide for reasoning tasks?

- Concept: Data mixing and ratio optimization
  - Why needed here: The 64% SFT data ratio was determined through extensive experimentation. Understanding the methodology and rationale behind this choice is essential for adapting the approach to different tasks or data distributions.
  - Quick check question: What factors should be considered when determining the optimal ratio of SFT data to pretraining data for a specific task?

- Concept: Embedding sharing and parameter efficiency
  - Why needed here: Embedding sharing reduces parameter count by 0.1B, contributing to Xmodel-2's efficiency. Understanding when and how to apply this technique is valuable for developing efficient models.
  - Quick check question: In what scenarios would embedding sharing be most beneficial, and what are the potential trade-offs in terms of model performance?

## Architecture Onboarding

- Component map: Embedding → 48 transformer blocks (1536 hidden, 3840 intermediate, 24 heads, 8 KV heads) → LM head
- Critical path: Embedding → Transformer blocks → LM head
  - The embedding sharing occurs at the input, reducing parameters before processing
  - Deep-and-thin architecture provides capacity while maintaining efficiency
  - GQA optimizes attention computation
- Design tradeoffs:
  - Deep-and-thin vs. shallow-and-wide: Chosen for efficiency in SLMs
  - Embedding sharing: Reduces parameters but may limit model expressiveness
  - Unigram tokenizer: Potentially better for multilingual tasks but larger vocabulary
  - GQA: Improves inference efficiency but may reduce attention diversity
- Failure signatures:
  - Poor reasoning performance: May indicate insufficient capacity or suboptimal data ratios
  - Training instability: Could suggest issues with learning rate scheduling or batch size
  - High perplexity on validation data: Might indicate overfitting or suboptimal tokenizer
- First 3 experiments:
  1. Replicate the nano model (6M) hyperparameter search to verify μP stability
  2. Test different SFT data ratios (50%, 64%, 70%) on a small model to validate optimal ratio
  3. Compare training with and without embedding sharing to quantify efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal SFT data ratio vary across different model scales and reasoning tasks?
- Basis in paper: [explicit] The paper states that data ratio experiments revealed the optimal SFT data ratio falls between 60% and 69%, with the precise value depending on the internal composition of the SFT-mixed dataset.
- Why unresolved: The paper only tested the data ratio on a 1.2B parameter model. It's unclear if this optimal ratio generalizes to larger or smaller models, or if different reasoning tasks require different data ratios.
- What evidence would resolve it: Systematic experiments varying model scale and reasoning task types while measuring performance across different SFT data ratios would provide clarity.

### Open Question 2
- Question: What is the impact of multilingual alignment and domain-specific data on the reasoning performance of Xmodel-2?
- Basis in paper: [inferred] The paper mentions that multilingual alignment and domain-specific data were excluded from the SFT data for future exploration, suggesting potential benefits.
- Why unresolved: The paper deliberately excluded these data types from experiments, leaving their potential impact unknown.
- What evidence would resolve it: Training and evaluating Xmodel-2 with multilingual and domain-specific SFT data while comparing performance to the current model would provide empirical evidence.

### Open Question 3
- Question: How does the WSD learning rate scheduler compare to other learning rate schedulers for complex reasoning tasks?
- Basis in paper: [explicit] The paper emphasizes the use of WSD learning rate scheduler from MiniCPM for training efficiency and stability, but doesn't compare it to alternatives.
- Why unresolved: The paper only reports results using WSD and doesn't benchmark against other learning rate schedulers that might be more effective for reasoning tasks.
- What evidence would resolve it: Head-to-head comparisons of Xmodel-2 trained with WSD versus other schedulers (cosine decay, linear decay, etc.) while measuring reasoning task performance would provide clarity.

## Limitations

- Lack of detailed experimental validation for key claims including embedding sharing efficiency and WSD scheduler effectiveness
- No comparative studies showing μP hyperparameter transfer actually works across multiple scales
- Limited ablation studies to quantify the impact of individual architectural and training choices

## Confidence

**High Confidence**: Model architecture specifications (48 layers, 1536 hidden size, GQA with 24/8 heads) are clearly defined and reproducible. Overall training methodology (1.5T tokens, WSD scheduler, data mixing) is explicitly described with specific numerical parameters.

**Medium Confidence**: Reasoning performance claims (39.62% average accuracy) are supported by benchmark results, though specific evaluation protocols and dataset details are not fully specified. Agent task success rate (14.21%) is presented without comparison to baseline methods or error analysis.

**Low Confidence**: Claims about μP for hyperparameter stability across scales, specific efficiency gains from embedding sharing, and superiority of WSD scheduler over alternatives are asserted without rigorous comparative experiments.

## Next Checks

1. **Ablation Study**: Train two versions of the nano model (6M) - one with embedding sharing and one without - and compare parameter counts and performance on reasoning tasks to quantify the actual efficiency gains.

2. **Scheduler Comparison**: Implement and compare the WSD learning rate scheduler against traditional cosine decay and linear decay schedules on the tiny model (54M) to empirically validate its claimed advantages for reasoning tasks.

3. **Data Ratio Sensitivity**: Conduct a systematic grid search on the nano model across SFT data ratios from 50% to 70% in 5% increments to verify the 64% optimum and map the performance landscape around this value.