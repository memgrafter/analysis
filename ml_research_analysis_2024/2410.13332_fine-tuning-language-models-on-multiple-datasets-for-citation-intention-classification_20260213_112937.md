---
ver: rpa2
title: Fine-Tuning Language Models on Multiple Datasets for Citation Intention Classification
arxiv_id: '2410.13332'
source_url: https://arxiv.org/abs/2410.13332
tags:
- dataset
- datasets
- primary
- citation
- auxiliary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a multi-task learning (MTL) framework to improve
  the performance of language models on citation intention classification (CIC) tasks.
  The framework jointly fine-tunes a pretrained language model (PLM) on multiple CIC
  datasets, using a task relation learning (TRL) method to control the contribution
  of auxiliary datasets and avoid negative transfer.
---

# Fine-Tuning Language Models on Multiple Datasets for Citation Intention Classification

## Quick Facts
- arXiv ID: 2410.13332
- Source URL: https://arxiv.org/abs/2410.13332
- Reference count: 33
- Primary result: Multi-task learning with task relation learning improves CIC performance by 7-11% on small datasets

## Executive Summary
This paper introduces a multi-task learning framework that jointly fine-tunes pretrained language models on multiple citation intention classification datasets. The framework uses a task relation learning method to control the contribution of auxiliary datasets and avoid negative transfer, while a position-aware readout function leverages citation position within context. Experiments on three CIC datasets show significant improvements over single-task baselines, particularly on smaller datasets, with gains of 7-11% over state-of-the-art models.

## Method Summary
The proposed method fine-tunes PLMs (BERT/SciBERT) on a primary CIC dataset jointly with auxiliary CIC datasets using a shared encoder and dataset-specific MLP heads. The task relation learning (TRL) method computes contribution coefficients λ for each auxiliary dataset based on information gain, preventing negative transfer. Three readout functions generate citation embeddings: CLS (special token), MEAN (average pooling), and CITED HERE (position-aware mean pooling at citation location). The framework is evaluated on ACL, SciCite, and KIM datasets with macro-F1 as the primary metric.

## Key Results
- MTL with TRL improves BERT/SciBERT performance by 7-11% on small datasets (ACL, SciCite) compared to state-of-the-art models
- Framework matches best performance on large dataset (KIM) while requiring less hyperparameter tuning
- CITED HERE readout function consistently outperforms CLS and MEAN baselines across all datasets
- TRL method eliminates need for expensive grid search over λ values while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task learning with task relation learning improves generalization on small CIC datasets by preventing negative transfer.
- Mechanism: The TRL method measures information gain from auxiliary datasets to the primary dataset and adjusts the contribution coefficient λ accordingly, allowing beneficial knowledge transfer while avoiding harmful interference.
- Core assumption: Datasets sharing the same input space (citation contexts) but with different label spaces can still provide useful supervision signals to each other.
- Evidence anchors:
  - [abstract] "We develop a data-driven task relation learning (TRL) method that controls the contribution of auxiliary datasets to avoid negative transfer and expensive hyper-parameter tuning."
  - [section] "We propose a task relation learning (TRL) method to determine the value of λa. Our method trains a model on the auxiliary dataset a and evaluates it on the primary dataset p."
  - [corpus] Found 25 related papers with average neighbor FMR=0.453, suggesting moderate relatedness to multi-task learning approaches for text classification.
- Break condition: If the auxiliary dataset's label space is completely unrelated to the primary dataset, information gain will be near zero and λ will be minimized, effectively disabling the auxiliary dataset's contribution.

### Mechanism 2
- Claim: Position-aware readout function CITED HERE improves classification by capturing the location of citations within contexts.
- Mechanism: By inserting a special marker at the citation position and using mean pooling over the corresponding token embeddings, the model gains explicit positional information that correlates with citation intention.
- Core assumption: The position of a citation within its context provides informative signals for predicting citation intention, as different citation purposes may have characteristic positions.
- Evidence anchors:
  - [abstract] "We also find that the position of a citation within the context provides useful information for CIC tasks and a position-aware readout function, i.e., a function that aggregates the PLM output token embeddings to a fixed length citation embedding, can improve PLMs' performance."
  - [section] "We find that the position of the citation within the context is an informative signal for predicting citation intentions."
  - [corpus] The corpus shows moderate relatedness to text classification methods, supporting the general approach of using positional information.
- Break condition: If citation intention is completely independent of position within the context, the CITED HERE readout function would provide no advantage over position-agnostic methods.

### Mechanism 3
- Claim: Fine-tuning on multiple CIC datasets creates a more robust model that generalizes better than models trained on single datasets.
- Mechanism: Joint training exposes the model to diverse citation patterns and intention categories across different scientific domains, improving its ability to handle unseen examples.
- Core assumption: Citation intention classification tasks across different scientific domains share underlying patterns and commonalities that can be leveraged through multi-task learning.
- Evidence anchors:
  - [abstract] "We propose a multi-task learning (MTL) framework that jointly fine-tunes PLMs on a dataset of primary interest together with multiple auxiliary CIC datasets to take advantage of additional supervision signals."
  - [section] "Our goal is to train a CIC model M such that M(xp_i) = yp_i, ∀ (xp_i, yp_i) ∈ Dp" and the subsequent discussion of joint fine-tuning.
  - [corpus] The corpus contains related work on cross-dataset generalization and multi-task learning for text classification.
- Break condition: If the auxiliary datasets are too dissimilar from the primary dataset (different domains, citation styles, or intention categories), the benefits of joint training may be minimal or negative.

## Foundational Learning

- Concept: Information gain and entropy in information theory
  - Why needed here: Used to quantify the relevance of auxiliary datasets to the primary dataset in the TRL method
  - Quick check question: How does entropy measure uncertainty, and how is information gain calculated from entropy differences?

- Concept: Multi-task learning and negative transfer
  - Why needed here: The framework relies on understanding when sharing information between tasks helps or harms performance
  - Quick check question: What conditions lead to negative transfer in multi-task learning, and how can it be detected or prevented?

- Concept: Pretrained language model fine-tuning techniques
  - Why needed here: The framework builds on standard PLM fine-tuning but extends it with multi-task learning
  - Quick check question: What are the differences between fine-tuning the entire PLM versus using techniques like adapter layers or prompt tuning?

## Architecture Onboarding

- Component map: Data → PLM encoder → readout function → MLP head → loss calculation → backpropagation to PLM parameters
- Critical path: Input citation contexts → Shared PLM encoder → Readout function (CLS/MEAN/CITED HERE) → Dataset-specific MLP → Classification loss → Gradient update
- Design tradeoffs: Joint training vs. separate training balances knowledge transfer against potential negative interference; TRL vs. grid search trades computational efficiency against potentially optimal coefficient selection
- Failure signatures: Performance degradation on primary dataset when adding auxiliary datasets (negative transfer), overfitting on small datasets, or λ values consistently near 0 indicating poor task relatedness
- First 3 experiments:
  1. Implement single-task fine-tuning on ACL dataset as baseline
  2. Add one auxiliary dataset (KIM) with grid search for λ values
  3. Replace grid search with TRL method and compare performance and runtime

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed TRL coefficients (λ) generalize when applied to CIC datasets from completely different scientific domains not represented in the current study?
- Basis in paper: [explicit] The paper states "The TRL method measures the relevance of an auxiliary dataset to the primary dataset by evaluating the information gain... We compute the value of λa as the relative reduction of entropy" and applies it to three datasets from materials science, computer science, and medicine.
- Why unresolved: The current study only evaluates TRL across three domains. The method's effectiveness for datasets from domains like social sciences, humanities, or interdisciplinary fields remains untested.
- What evidence would resolve it: Experimental results showing TRL performance across a diverse set of 10+ CIC datasets spanning various scientific domains, comparing against domain-specific baseline models.

### Open Question 2
- Question: What is the theoretical limit of performance improvement when increasing the number of auxiliary datasets in the MTL framework, and does diminishing returns occur?
- Basis in paper: [inferred] The paper demonstrates improvements using up to two auxiliary datasets but does not explore scenarios with three or more auxiliary datasets or analyze performance scaling.
- Why unresolved: The experiments only test binary combinations of primary and auxiliary datasets, leaving open questions about scalability and optimal number of auxiliary tasks.
- What evidence would resolve it: Systematic experiments varying the number of auxiliary datasets (1-5) for each primary dataset, measuring performance improvements and computational costs to identify optimal configurations.

### Open Question 3
- Question: How does the CITED HERE readout function perform on extremely short citation contexts (e.g., less than 5 words) compared to CLS and MEAN?
- Basis in paper: [explicit] The paper introduces CITED HERE as position-aware and shows it outperforms CLS and MEAN on small datasets, but does not analyze performance on very short contexts.
- Why unresolved: The current experiments do not segment results by citation context length, making it unclear if position-awareness provides benefits when the citation is embedded in minimal surrounding text.
- What evidence would resolve it: Controlled experiments filtering citation contexts by length (e.g., <5 words, 5-10 words, >10 words) and comparing readout function performance across these groups.

## Limitations
- Lack of publicly available data splits for ACL and SciCite datasets makes exact reproduction difficult
- TRL implementation details (entropy calculation, information gain formula) are not fully specified
- Position-awareness assumption may not hold across all scientific domains and citation styles
- Limited evaluation to three datasets from specific domains (materials science, computer science, medicine)

## Confidence
- Multi-task learning with TRL improving small dataset performance: Medium
- CITED HERE readout function providing consistent improvements: Medium
- Framework matching state-of-the-art on large datasets: High

## Next Checks
1. Obtain and verify exact train-test splits used for ACL and SciCite datasets to ensure fair comparison with baseline methods
2. Implement TRL method with explicit entropy calculation formulas and compare lambda values against grid search to verify computational efficiency and effectiveness
3. Evaluate framework on a fourth CIC dataset from a different scientific domain (e.g., biomedical) to assess generalization beyond tested domains