---
ver: rpa2
title: Benchmarking Agentic Workflow Generation
arxiv_id: '2410.07869'
source_url: https://arxiv.org/abs/2410.07869
tags:
- arxiv
- workflow
- https
- node
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WORFBENCH, a benchmark for evaluating LLM
  agents' ability to generate executable workflows from complex tasks. It addresses
  limitations of existing evaluations by including multi-faceted scenarios (function
  calling, embodied planning, problem-solving, open-grounded) and modeling workflows
  as directed acyclic graphs to capture dependencies.
---

# Benchmarking Agentic Workflow Generation

## Quick Facts
- arXiv ID: 2410.07869
- Source URL: https://arxiv.org/abs/2410.07869
- Reference count: 40
- Primary result: GPT-4 achieves only 52.47% on graph-structured workflow generation, significantly worse than linear planning

## Executive Summary
This paper introduces WORFBENCH, a comprehensive benchmark for evaluating LLM agents' ability to generate executable workflows from complex tasks. The benchmark addresses limitations of existing evaluations by modeling workflows as directed acyclic graphs (DAGs) to capture task dependencies, enabling more realistic representation of parallel and serial structures. The authors also propose WORFEVAL, a systematic evaluation protocol using sequence and subgraph matching algorithms to quantitatively assess workflow generation capabilities.

## Method Summary
The benchmark construction involves collecting tasks and action lists from source datasets, generating node chains using GPT-4 with designed prompts, and creating workflow graphs with topological sorting. The evaluation protocol employs subsequence matching (Longest Increasing Subsequence) for node chains and subgraph matching (Maximum Common Induced Subgraph) for workflow graphs, using semantic similarity (Sentence-BERT embeddings) to match nodes with different wording. The method includes quality control through topological sorting validation and human verification.

## Key Results
- GPT-4 achieves 52.47% f1graph score on graph-structured workflows, significantly worse than linear planning
- Even advanced models struggle with graph-structured workflows, revealing a major capability gap
- Trained 7B models show improved performance on training tasks but limited generalization to held-out tasks
- Generated workflows improve downstream task performance and reduce inference time through parallelization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark addresses limitations of existing evaluations by including multi-faceted scenarios and modeling workflows as directed acyclic graphs to capture dependencies.
- Mechanism: By incorporating diverse scenarios and representing workflows as DAGs, the benchmark captures real-world task dependencies that linear structures cannot represent.
- Core assumption: Real-world workflows often involve parallelism and complex dependencies.
- Evidence anchors:
  - [abstract] "It addresses limitations of existing evaluations by including multi-faceted scenarios... and modeling workflows as directed acyclic graphs to capture dependencies."
  - [section 2.1] "We model the workflows as Directed Acyclic Graphs based on dependencies between subtasks..."

### Mechanism 2
- Claim: The WORFEVAL protocol uses sequence and subgraph matching algorithms to quantitatively assess workflow generation.
- Mechanism: Applying LIS and MCIS algorithms measures similarity between predicted and ground truth workflows, considering both node order and dependencies.
- Core assumption: Semantic similarity threshold effectively identifies when nodes represent the same subtask.
- Evidence anchors:
  - [abstract] "We present WORFEVAL... utilizing subsequence and subgraph matching algorithms to accurately quantify the LLM agent's workflow generation capabilities."
  - [section 2.4] "We quantitatively evaluate both the node chain and workflow graph using restrict algorithms."

### Mechanism 3
- Claim: Generated workflows enhance downstream task performance and reduce inference time through parallelization.
- Mechanism: Structured prior knowledge guides agents to avoid trial-and-error planning, while DAG structure enables parallel execution of independent subtasks.
- Core assumption: Agents can effectively utilize workflow information and correctly identify dependencies for safe parallel execution.
- Evidence anchors:
  - [abstract] "We observe that the generated workflows can enhance downstream tasks, enabling them to achieve superior performance with less time during inference."
  - [section 4.1] "In a graph-structured workflow, nodes without dependencies can be executed in parallel."

## Foundational Learning

- Concept: Directed Acyclic Graphs (DAGs) and Topological Sorting
  - Why needed here: The benchmark models workflows as DAGs to represent task dependencies, and topological sorting validates the graph structure.
  - Quick check question: If a workflow has nodes A, B, C where A must precede B and B must precede C, what is a valid topological ordering of these nodes?

- Concept: Longest Increasing Subsequence (LIS) and Maximum Common Induced Subgraph (MCIS) algorithms
  - Why needed here: These algorithms measure similarity between predicted and ground truth node chains and workflow graphs in WORFEVAL.
  - Quick check question: Given two sequences [1, 3, 5, 7] and [1, 2, 3, 4, 5], what is the length of their longest common increasing subsequence?

- Concept: Semantic similarity and embedding representations
  - Why needed here: The evaluation protocol uses cosine similarity of Sentence-BERT embeddings to match predicted nodes with ground truth nodes.
  - Quick check question: If two node descriptions have Sentence-BERT embeddings with cosine similarity of 0.65 and threshold β is 0.6, are these nodes considered a match?

## Architecture Onboarding

- Component map: WORFBENCH (data collection, node chain generation, graph generation) -> WORFEVAL (sequence and subgraph matching algorithms) -> Experimental framework (model evaluation, training, downstream task analysis)
- Critical path: Collect tasks and action lists → Generate node chains using GPT-4 → Generate workflow graphs with topological sorting → Apply quality control → Evaluate using WORFEVAL with semantic matching and LIS/MCIS algorithms
- Design tradeoffs: The benchmark trades comprehensiveness for complexity by including multiple scenarios and graph structures, requiring sophisticated evaluation algorithms and quality control measures.
- Failure signatures: Incorrect node granularity, illogical workflow graphs (cycles or incorrect dependencies), poor semantic matching due to threshold issues, and generalization failures.
- First 3 experiments:
  1. Evaluate a simple baseline model on a small subset to verify the evaluation pipeline works correctly.
  2. Test the sensitivity of evaluation scores to the semantic similarity threshold β by running evaluations with different threshold values.
  3. Compare the performance gap between linear and graph planning on a single scenario to validate the core hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM agents on graph-structured workflows scale with the number of nodes and edges in the workflow?
- Basis in paper: [inferred] The paper discusses that GPT-4's performance on graph-structured workflows falls short of real-world requirements and analyzes performance distribution across different numbers of nodes and edges.
- Why unresolved: The paper only provides qualitative analysis of the performance trend without quantifying the relationship between performance and workflow complexity.
- What evidence would resolve it: A quantitative analysis plotting the f1_graph score against the number of nodes and edges.

### Open Question 2
- Question: What is the impact of iterative workflow generation and refinement based on environmental feedback on the performance of LLM agents?
- Basis in paper: [inferred] The paper mentions that the current workflow follows a one-pass generation paradigm and plans to introduce an iterative paradigm in the future.
- Why unresolved: The paper does not explore the potential benefits of iterative workflow generation and refinement.
- What evidence would resolve it: An experimental comparison of one-pass versus iterative workflow generation and refinement performance.

### Open Question 3
- Question: How does the integration of world knowledge or world models into LLM agents impact their planning abilities in complex, real-world scenarios?
- Basis in paper: [inferred] The paper suggests that planning complexity requires environmental commonsense knowledge and that integrating world knowledge may be crucial for enhancing planning abilities.
- Why unresolved: The paper does not provide empirical evidence on the effectiveness of integrating world knowledge or world models.
- What evidence would resolve it: An experimental study comparing LLM agents with and without world knowledge/world models integration in complex planning tasks.

## Limitations
- The benchmark relies heavily on GPT-4 for data generation, introducing potential model bias.
- The fixed semantic similarity threshold (β=0.7) may not generalize well across different task domains.
- Trained models show limited generalization to held-out tasks, suggesting overfitting to training distribution.

## Confidence
- Confidence in main findings: Medium
- The experimental results showing GPT-4's 52.47% performance are compelling and well-supported.
- However, training results for 7B models are less convincing due to limited generalization.
- The benchmark may inadvertently reflect GPT-4's planning patterns rather than true real-world complexity.

## Next Checks
1. **Cross-model validation**: Evaluate multiple independent models on the same benchmark to verify consistent difficulty levels and capability gaps.
2. **Threshold sensitivity analysis**: Systematically vary the semantic similarity threshold β across values (0.5-0.9) and measure how evaluation scores change.
3. **Real-world deployment test**: Apply the workflow generation approach to a genuinely novel task domain (e.g., robotics planning) where no training data exists to validate true generalization capabilities.