---
ver: rpa2
title: Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving
  LLMs
arxiv_id: '2402.11442'
source_url: https://arxiv.org/abs/2402.11442
tags:
- rules
- premise
- person
- rule
- conclusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines whether large language models (LLMs) can effectively
  reason with inferential rules. It introduces a logic scaffolding framework to generate
  an inferential rule base (ULogic) with nearly 8,000 primitive and over 6,000 compositional
  rules across five domains.
---

# Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs

## Quick Facts
- arXiv ID: 2402.11442
- Source URL: https://arxiv.org/abs/2402.11442
- Reference count: 40
- Primary result: LLMs show significant gaps in understanding inferential rules, especially compositional and structurally complex rules, with notable polarity and necessary biases.

## Executive Summary
This paper examines whether large language models can effectively reason with inferential rules by introducing a logic scaffolding framework to generate and evaluate rule-based reasoning capabilities. The authors construct ULogic, a comprehensive inferential rule base with nearly 8,000 primitive and over 6,000 compositional rules across five domains, generated through a two-stage process involving GPT-4 and human verification. The study reveals significant gaps in LLM reasoning abilities, particularly with compositional and symbolic rules, and develops a distilled inference engine that demonstrates improved performance in generating accurate, complex, and abstract rules while enhancing downstream reasoning tasks.

## Method Summary
The study employs a two-stage framework called LOIRE (Logic Scaffolding Inferential Rule generation) that first generates primitive rules through GPT-4 with human verification, then composes more complex rules via backward chaining. The ULogic rule base is constructed across five domains using symbolic Prolog expressions, with rules verified for logical soundness. The evaluation uses a dual-side probing approach with five distinct templates and chain-of-thought prompting to assess LLM performance on entailment tasks. The researchers also distill the rule base into a smaller inference engine using fine-tuned Mistral-7B, which is then evaluated against GPT-4 and GPT-3.5-Turbo on three rule generation tasks.

## Key Results
- LLMs show significant gaps in understanding inferential rules, especially compositional and structurally complex rules, with notable polarity and necessary biases.
- The logic scaffolding framework successfully generates diverse inferential rules, but performance degrades sharply on symbolic rules and longer compositional rules.
- The distilled inference engine demonstrates improved performance in generating accurate, complex, and abstract rules compared to baseline LLMs.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Symbolic logic scaffolding guides LLMs to generate diverse inferential rules.
- **Mechanism:** The framework uses symbolic Prolog expressions to consistently guide rule generation, ensuring logical structure before verbalization.
- **Core assumption:** LLMs can follow structured prompts and generate logically sound rules when given symbolic templates.
- **Evidence anchors:**
  - [abstract] "We propose a logic scaffolding inferential rule generation framework, to construct an inferential rule base, ULogic, comprising both primitive and compositional rules across five domains."
  - [section] "We utilize Prolog (Apt et al., 1997) to formulate symbolic rules as Conclusion:-Premise, where :- indicates the logical implication."
- **Break condition:** If LLMs fail to understand symbolic notation or if prompt engineering becomes too brittle.

### Mechanism 2
- **Claim:** Backward chaining systematically composes complex rules from primitive ones.
- **Mechanism:** The framework applies backward chaining steps to primitive rules, replacing premise facts with multi-fact premises to create compositional rules.
- **Core assumption:** Composing logically correct sub-rules produces valid complex rules.
- **Evidence anchors:**
  - [abstract] "We further distill these rules into a smaller-scale inference engine for flexible rule generation and enhancing downstream reasoning."
  - [section] "We create more compositional rules by applying backward chaining upon primitive rules with different chaining steps."
- **Break condition:** If intermediate rules contain errors or if chaining creates logically invalid combinations.

### Mechanism 3
- **Claim:** Dual-side probing eliminates template and label bias in evaluation.
- **Mechanism:** For each rule, both the original and its flipped (negated conclusion) version are probed, requiring consistent answers.
- **Core assumption:** The Law of Non-Contradiction ensures that "If X then Y" and "If X then not Y" cannot both be true.
- **Evidence anchors:**
  - [abstract] "We adopt a two-shot chain of thought (CoT) prompting strategy (Wei et al., 2022) requiring the model to generate a rationale after presenting its answer."
  - [section] "Following the Law of Non-Contradiction (Priest et al., 2006), the propositions 'If X, then Y' and 'If X, then not Y' are mutually exclusive that cannot both be true at the same time."
- **Break condition:** If models learn to game the dual-side format or if probing templates introduce new biases.

## Foundational Learning

- **Concept:** Propositional and first-order logic
  - **Why needed here:** The framework generates rules in symbolic logic format and evaluates logical entailment.
  - **Quick check question:** What is the difference between a predicate and a proposition in first-order logic?

- **Concept:** Backward chaining inference
  - **Why needed here:** Used to compose complex rules from primitive ones and to diversify rule expressions.
  - **Quick check question:** How does backward chaining differ from forward chaining in rule-based systems?

- **Concept:** Chain-of-thought prompting
  - **Why needed here:** The evaluation uses CoT to elicit reasoning and reduce random guessing.
  - **Quick check question:** What are the key components of an effective CoT prompt for logical reasoning tasks?

## Architecture Onboarding

- **Component map:** LOIRE Framework -> ULogic Base -> Evaluation Suite -> Inference Engine
- **Critical path:**
  1. Generate primitive rules via GPT-4 + human verification
  2. Compose rules via backward chaining
  3. Evaluate LLM performance on rule subset
  4. Distill rules into smaller inference engine
  5. Test engine on downstream reasoning tasks
- **Design tradeoffs:**
  - Rule quality vs. quantity: Human verification ensures quality but limits scale
  - Symbolic vs. verbalized rules: Symbolic rules test generalization but are harder for LLMs
  - Evaluation comprehensiveness vs. manageability: 5 templates reduce bias but increase complexity
- **Failure signatures:**
  - High variance across probing templates indicates template bias
  - Sharp degradation on symbolic rules suggests LLM limitations with formal notation
  - Polarity or necessary biases reveal systematic model weaknesses
- **First 3 experiments:**
  1. Run LOIRE pipeline on a single domain with minimal human verification to test generation feasibility
  2. Evaluate GPT-3.5-Turbo on a small rule subset with one probing template to establish baseline
  3. Compare rule diversity before and after backward chaining diversification step

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do LLMs perform on inferential rules that involve temporal reasoning, such as cause-and-effect relationships or event sequences?
- **Basis in paper:** [inferred] The paper focuses on if-then inferential rules and does not explicitly explore temporal reasoning, which is a crucial aspect of many real-world scenarios.
- **Why unresolved:** The paper's analysis is limited to five domains (object affordance, accessibility, interaction, location, and person's need) and does not specifically address temporal reasoning.
- **What evidence would resolve it:** Conducting experiments to evaluate LLM performance on a dataset of inferential rules that explicitly involve temporal relationships, such as cause-and-effect or event sequences.

### Open Question 2
- **Question:** How does the performance of LLMs on inferential rules vary across different languages and cultural contexts?
- **Basis in paper:** [inferred] The paper's analysis is conducted primarily on English language models (GPT-4, GPT-3.5-Turbo) and does not explore cross-linguistic or cross-cultural variations in LLM performance.
- **Why unresolved:** The paper does not provide any insights into how LLM performance on inferential rules might differ across languages or cultural contexts.
- **What evidence would resolve it:** Conducting experiments to evaluate LLM performance on inferential rules in multiple languages and cultural contexts, and comparing the results to identify potential differences or biases.

### Open Question 3
- **Question:** Can the logic scaffolding framework be extended to generate inferential rules that involve higher-order logic, such as rules with nested quantifiers or rules that involve relations between relations?
- **Basis in paper:** [explicit] The paper mentions that the framework currently focuses on first-order logic and does not explicitly address higher-order logic.
- **Why unresolved:** The paper does not explore the possibility of extending the framework to generate rules that involve higher-order logic constructs.
- **What evidence would resolve it:** Developing an extended version of the logic scaffolding framework that can generate and evaluate inferential rules involving higher-order logic, and testing its performance on a dataset of such rules.

## Limitations

- The rule generation framework relies heavily on GPT-4's ability to produce logically sound rules, with human verification serving as a quality filter, but scalability remains uncertain as human verification becomes increasingly costly.
- The study's focus on five specific domains may limit generalizability to other knowledge domains and real-world reasoning tasks.
- The evaluation framework's reliance on specific probing templates could introduce evaluation artifacts that don't generalize to real-world reasoning scenarios.

## Confidence

- **High confidence**: The observation that LLMs show significant gaps in understanding inferential rules, particularly for compositional and structurally complex rules, is well-supported by the systematic evaluation across multiple models and templates.
- **Medium confidence**: The effectiveness of the logic scaffolding framework for rule generation is supported by the ULogic base construction, but the extent to which this approach generalizes beyond the tested domains requires further validation.
- **Low confidence**: The inference engine's superiority in generating complex and abstract rules needs more rigorous comparison with alternative approaches and larger-scale evaluations.

## Next Checks

1. **Cross-domain generalization test**: Apply the LOIRE framework to two new domains outside the original five (e.g., scientific reasoning and social interaction rules) and evaluate rule quality and diversity compared to the original domains.

2. **Template bias analysis**: Systematically vary the five probing templates to test whether the observed model performance differences are due to template design rather than genuine reasoning capabilities, using ablation studies with single-template evaluations.

3. **Long-term stability assessment**: Evaluate the inference engine's performance on rule generation tasks after fine-tuning with different random seeds and dataset orderings to assess the stability of the distilled knowledge.