---
ver: rpa2
title: Enhancing Complex Question Answering over Knowledge Graphs through Evidence
  Pattern Retrieval
arxiv_id: '2402.02175'
source_url: https://arxiv.org/abs/2402.02175
tags:
- methods
- question
- evidence
- pattern
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Evidence Pattern Retrieval (EPR) to improve
  information retrieval methods for complex question answering over knowledge graphs.
  The key insight is that structural dependencies among evidence facts are crucial
  for accurate answer extraction, yet current methods largely ignore them.
---

# Enhancing Complex Question Answering over Knowledge Graphs through Evidence Pattern Retrieval

## Quick Facts
- arXiv ID: 2402.02175
- Source URL: https://arxiv.org/abs/2402.02175
- Authors: Wentao Ding; Jinmao Li; Liangchuan Luo; Yuzhong Qu
- Reference count: 40
- Key outcome: Evidence Pattern Retrieval (EPR) improves complex question answering over knowledge graphs by modeling structural dependencies among evidence facts, achieving over 10-point F1 gains on ComplexWebQuestions and competitive performance on WebQuestionsSP.

## Executive Summary
This paper introduces Evidence Pattern Retrieval (EPR), a novel approach to information retrieval for knowledge graph question answering (KGQA) that addresses the critical limitation of existing methods: their insufficient modeling of structural dependencies among evidence facts. EPR decomposes evidence patterns into atomic adjacency patterns (ER-APs and RR-APs), retrieves them via dense retrieval, and reconstructs evidence patterns through enumeration. The method significantly outperforms state-of-the-art subgraph extraction baselines on ComplexWebQuestions (CWQ) and achieves competitive results on WebQuestionsSP (WebQSP), demonstrating that structural modeling is essential for accurate answer extraction in complex KGQA tasks.

## Method Summary
EPR operates by first decomposing the evidence pattern space into atomic adjacency patterns at the granularity of resource pairs, then retrieving these patterns via dense retrieval using a BERT-based bi-encoder. Candidate evidence patterns are constructed through an enumeration algorithm that combines retrieved atomic patterns, and the best pattern is selected using a BERT cross-encoder that scores the semantic compatibility between patterns and questions. The approach builds Faiss indexes for efficient retrieval, uses pseudo evidence patterns for training, and integrates with the Neural Symbolic Machine (NSM) for answer reasoning. Hyperparameters include vector dimension 768, batch sizes (16 for retrieval, 2 for ranking), epochs (5 for retrieval, 10 for ranking), and learning rates (2e-5 for retrieval, 1e-5 for ranking).

## Key Results
- Achieved over 10-point F1 score improvement on ComplexWebQuestions compared to state-of-the-art subgraph extraction methods
- Outperformed baselines including PPR, PullNet, and SR on CWQ, with 24.1% improvement in F1 over the best baseline
- Achieved competitive performance on WebQuestionsSP, demonstrating effectiveness across different question complexity levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicitly modeling structural dependencies between evidence facts improves subgraph extraction quality.
- Mechanism: Evidence Pattern Retrieval (EPR) indexes atomic adjacency patterns of resource pairs and retrieves them via dense retrieval, then enumerates their combinations to form candidate evidence patterns, which are scored and the best selected.
- Core assumption: Structural dependencies among facts are more important than individual fact relevance for answer extraction.
- Evidence anchors:
  - [abstract] "structural dependencies among evidence facts are crucial for accurate answer extraction, yet current methods largely ignore them"
  - [section] "We find that current IR studies primarily focus on how to obtain the answer(s) but pay insufficient attention to non-answer parts in the extracted subgraph"
- Break condition: If structural dependencies are not the primary source of noise or if the question does not require multi-hop reasoning, the performance gain may be minimal.

### Mechanism 2
- Claim: Atomic pattern retrieval reduces computational complexity while preserving relevant structural information.
- Mechanism: Decomposes evidence patterns into atomic adjacency patterns (ER-APs and RR-APs), builds Faiss indexes for fast retrieval, and reconstructs evidence patterns through enumeration.
- Core assumption: All possible evidence patterns can be covered by combinations of atomic patterns.
- Evidence anchors:
  - [section] "the retrieval space can be denoted as {ùëùùëéùë° (ùëÜùê∫ ) | ùëÜùê∫ ‚äÜ G }. Given that the space exceeds the scale of manageable storage, our approach analyzes EP at the granularity of the atomic adjacency structure"
  - [section] "Each AP consists of a pair of adjacent resources and defines their connection structure"
- Break condition: If the atomic pattern space is too large or if the enumeration leads to combinatorial explosion, the approach may become computationally infeasible.

### Mechanism 3
- Claim: Scoring candidate evidence patterns via a neural cross-encoder improves selection accuracy.
- Mechanism: Uses a BERT-implemented cross-encoder to score the similarity between serialized evidence patterns and the input question, selecting the highest-scoring pattern.
- Core assumption: A cross-encoder can effectively learn the semantic relationship between evidence patterns and questions.
- Evidence anchors:
  - [section] "These evidence patterns are scored using a neural model, and the best one is selected to extract a subgraph for downstream answer reasoning"
  - [section] "we model the probability Prùúô over candidate EPs via a BERT-implemented cross-encoder"
- Break condition: If the cross-encoder is undertrained or the training data is insufficient, the ranking may not accurately reflect pattern quality.

## Foundational Learning

- Concept: Knowledge graph (KG) structure and triplet representation
  - Why needed here: Understanding how KGs store facts as (subject, relation, object) triplets is fundamental to grasping how evidence patterns are constructed and retrieved.
  - Quick check question: Given a KG fact "Berlin locatedIn Germany", identify the subject, relation, and object.

- Concept: Dense retrieval and vector indexing
  - Why needed here: EPR relies on dense retrieval of atomic patterns using vector embeddings and Faiss indexes for efficient search.
  - Quick check question: What is the difference between sparse retrieval (e.g., BM25) and dense retrieval in terms of query representation?

- Concept: Neural ranking models and cross-encoders
  - Why needed here: The final selection of evidence patterns uses a BERT-based cross-encoder to score the compatibility between patterns and questions.
  - Quick check question: Why might a cross-encoder be more effective than a bi-encoder for scoring the similarity between a question and an evidence pattern?

## Architecture Onboarding

- Component map:
  - Question encoder (BERT bi-encoder) -> Atomic pattern index (Faiss) -> Candidate evidence pattern generator (enumeration algorithm) -> Evidence pattern ranker (BERT cross-encoder) -> Subgraph extractor -> Answer reasoner (NSM)

- Critical path:
  1. Encode question
  2. Retrieve atomic patterns from index
  3. Enumerate candidate evidence patterns
  4. Score and select best pattern
  5. Extract subgraph
  6. Reason to find answer

- Design tradeoffs:
  - Bi-encoder for retrieval vs. cross-encoder for ranking (efficiency vs. accuracy)
  - Size threshold ùúè for evidence patterns (completeness vs. combinatorial explosion)
  - Number of retrieved atomic patterns ùêæ (coverage vs. computation time)

- Failure signatures:
  - Low answer cover rate: insufficient atomic patterns retrieved or enumeration missed correct pattern
  - High cover rate but low Hits@1: poor ranking of candidate patterns
  - Slow performance: large ùêæ or small ùúè causing combinatorial explosion

- First 3 experiments:
  1. Measure Hits@1 and F1 with varying ùêæ (20, 40, 60, 80, 100) on CWQ to find sweet spot.
  2. Compare performance of EPR with different subgraph extraction baselines (PPR, PullNet, SR) on CWQ.
  3. Analyze the impact of training data size on evidence pattern ranker performance by training with 20%, 40%, 60%, 80%, and 100% of training data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can atomic pattern retrieval be improved to handle unseen relations in questions without significantly compromising retrieval efficiency?
- Basis in paper: [explicit] The paper notes that about 2.8% of CWQ test questions and 5% of WebQSP test questions contain relations that never appear in the training questions, leading to a sharp decline in performance.
- Why unresolved: The current BERT-based bi-encoder approach struggles with unseen relations, and while the paper acknowledges this limitation, it doesn't propose a concrete solution to address it while maintaining retrieval efficiency.
- What evidence would resolve it: Experimental results comparing the EPR system's performance with and without a proposed solution for handling unseen relations, demonstrating improved accuracy on questions containing previously unseen relations while maintaining comparable retrieval efficiency.

### Open Question 2
- Question: What optimizations can be applied to the brute-force enumeration of evidence patterns to improve efficiency when the retrieval threshold increases?
- Basis in paper: [explicit] The paper states that on CWQ, increases in atomic patterns lead to a combinatorial explosion of candidate evidence patterns, creating efficiency bottlenecks.
- Why unresolved: While the paper identifies the efficiency bottleneck caused by combinatorial explosion, it doesn't propose specific optimization strategies to address this issue beyond acknowledging the need for optimization.
- What evidence would resolve it: Comparative analysis showing the EPR system's runtime with and without proposed optimizations, demonstrating significant improvements in processing time for questions requiring large numbers of retrieved atomic patterns.

### Open Question 3
- Question: How can numerical information be effectively incorporated into evidence patterns to enhance downstream answer reasoning capabilities?
- Basis in paper: [inferred] The error analysis reveals that 54% of errors on CWQ and 30% on WebQSP were caused by non-entity descriptions of answers, many involving numerical reasoning.
- Why unresolved: The current IR-KGQA framework lacks the ability to model numerical information, and while the paper identifies this limitation, it doesn't propose a concrete approach for incorporating numerical reasoning into evidence patterns.
- What evidence would resolve it: Experimental results comparing the EPR system's performance on questions requiring numerical reasoning before and after incorporating numerical information into evidence patterns, demonstrating improved accuracy on such questions.

## Limitations
- Strong assumption that structural dependencies are the dominant factor in subgraph extraction quality may not hold for all question types
- Potential computational burden from atomic pattern enumeration leading to combinatorial explosion with large retrieval thresholds
- Reliance on pseudo evidence patterns for training introduces uncertainty about real-world performance
- Limited ability to handle questions with unseen relations, affecting performance on approximately 2.8-5% of test questions

## Confidence
- Structural dependency modeling claims: Medium - supported by significant performance improvements but lacks direct comparison with alternative structural modeling approaches
- Computational efficiency claims: Medium - runtime analysis is limited and not compared with existing methods
- Cross-encoder effectiveness: Medium - supported by general effectiveness in ranking tasks but specific application to evidence patterns lacks extensive validation

## Next Checks
1. **Ablation Study on Structural Dependencies**: Implement a variant of the EPR system that retrieves individual facts rather than atomic patterns, and compare the performance on CWQ and WebQSP to isolate the contribution of structural dependency modeling.

2. **Computational Complexity Analysis**: Measure and report the exact runtime of the EPR system across different values of K and œÑ on a standardized machine, comparing these results with the running times of existing subgraph extraction methods like PPR, PullNet, and SR.

3. **Generalization Test on Different KGs**: Evaluate the EPR system on a different knowledge graph, such as DBpedia or Wikidata, using the same methodology to test the method's ability to generalize beyond Freebase.