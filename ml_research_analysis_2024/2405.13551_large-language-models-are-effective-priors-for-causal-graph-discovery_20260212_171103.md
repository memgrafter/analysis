---
ver: rpa2
title: Large Language Models are Effective Priors for Causal Graph Discovery
arxiv_id: '2405.13551'
source_url: https://arxiv.org/abs/2405.13551
tags:
- causal
- priors
- discovery
- expert
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a probabilistic model for eliciting and evaluating
  causal judgments from Large Language Models (LLMs), proposing metrics like Fraction
  of Correct Orientations (FCO), True Edge to Reverse Edge (TERE), and True Edge to
  Negative Edge (TENE) to assess such priors independently of downstream causal discovery
  algorithms. The study systematically explores prompt design choices, finding that
  3-way prompts (allowing the model to specify the absence of a relationship) consistently
  improve causal judgment accuracy.
---

# Large Language Models are Effective Priors for Causal Graph Discovery

## Quick Facts
- arXiv ID: 2405.13551
- Source URL: https://arxiv.org/abs/2405.13551
- Reference count: 40
- LLMs improve causal discovery by providing directional priors, especially under low computational budgets.

## Executive Summary
This paper introduces a probabilistic model for eliciting and evaluating causal judgments from Large Language Models (LLMs), proposing metrics like Fraction of Correct Orientations (FCO), True Edge to Reverse Edge (TERE), and True Edge to Negative Edge (TENE) to assess such priors independently of downstream causal discovery algorithms. The study systematically explores prompt design choices, finding that 3-way prompts (allowing the model to specify the absence of a relationship) consistently improve causal judgment accuracy. Integrating LLM-derived priors with a Monte Carlo tree search-based causal discovery method, the results show that LLMs are especially helpful for edge directionality assessment, particularly under low computational budgets, with combined mutual information and LLM priors yielding the best performance.

## Method Summary
The method elicits pairwise causal judgments from LLMs using probabilistic prompts that allow the model to express causal direction or absence of relationship. These judgments form a prior matrix evaluated via FCO, TERE, and TENE metrics. The prior is then integrated with mutual information-based priors through element-wise (Hadamard) product and used within a CD-UCT (Causal Discovery using Upper Confidence bounds applied to Trees) framework for sampling DAG structures. The approach is tested on synthetic causal benchmarks (Asia, Child, Insurance) with varying LLM sizes (7B, 13B, 70B).

## Key Results
- 3-way prompts consistently and significantly improve FCO, TERE, and TENE metrics compared to 2-way alternatives.
- LLMs are especially helpful for edge directionality assessment under low computational budgets.
- The best-performing prior combines mutual information and LLM priors via Hadamard product.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs improve causal discovery primarily by providing directional priors rather than confirming edge existence.
- Mechanism: The LLM outputs probabilistic judgments on pairwise causality that are asymmetric (Pi→j ≠ Pj→i) even when MI is symmetric. When combined with MI via Hadamard product, the directionality from the LLM biases edge sampling in CD-UCT toward the more likely causal direction.
- Core assumption: The LLM's internal knowledge captures common-sense or domain-specific causal asymmetries better than symmetric association measures.
- Evidence anchors:
  - [abstract]: "LLMs are especially helpful for edge directionality assessment"
  - [section 5.2]: "TERE and TENE values" indicate that directionality judgments improve with 3-Way prompting
  - [corpus]: "LLM-initialized Differentiable Causal Discovery" suggests prior work using LLMs for initialization, implying value in directional cues
- Break condition: If the LLM's directional judgments are systematically incorrect or the domain is highly specialized, the soft prior could mislead sampling and degrade performance.

### Mechanism 2
- Claim: The 3-Way prompt design improves LLM causal judgments by explicitly allowing the model to express "no causal relationship".
- Mechanism: By adding the (C) option, the LLM is not forced to choose between two incorrect causal directions, reducing false positives and increasing the precision of the prior matrix.
- Core assumption: The LLM's pretraining includes sufficient examples of non-causal relationships to assign high probability to option (C) when appropriate.
- Evidence anchors:
  - [section 5.2]: "3-Way prompt... consistently and significantly improves metrics"
  - [section 4]: Contrasts 3-Way vs 2-Way designs explicitly
  - [corpus]: "Improving constraint-based discovery with robust propagation and reliable LLM priors" implies prior importance of reliability, consistent with allowing explicit non-causal options
- Break condition: If the LLM defaults to option (C) too often, the prior matrix becomes overly sparse, reducing the benefit of the soft prior.

### Mechanism 3
- Claim: Combining LLM priors with MI via Hadamard product yields the best performance because it merges directional intuition with strength of association.
- Mechanism: MI provides a symmetric measure of dependence; the LLM provides directionality. The element-wise product amplifies edges that are both strongly associated and correctly oriented.
- Core assumption: The product combination is more effective than other fusion methods (e.g., weighted sum) for this task.
- Evidence anchors:
  - [section 5.2]: "best-performing prior is obtained by combining MI and LLM priors"
  - [section 3.7]: Defines PMI⊙LLM = PMI ⊙ PLLM explicitly
  - [corpus]: "Efficient Differentiable Causal Discovery via Reliable Super-Structure Learning" suggests that structure learning benefits from multiple cues, consistent with hybrid priors
- Break condition: If the LLM's directional signal is weak relative to MI, the Hadamard product may not add value and could even reduce scores.

## Foundational Learning

- Concept: Directed Acyclic Graph (DAG) structure and causal interpretation
  - Why needed here: The entire problem is framed as discovering a DAG; understanding parent-child relationships and acyclicity is essential for interpreting LLM outputs and CD-UCT sampling.
  - Quick check question: If X causes Y, can Y also cause X in a valid causal graph? (Answer: No, due to acyclicity.)

- Concept: Pairwise causal discovery metrics (FCO, TERE, TENE)
  - Why needed here: These metrics evaluate LLM outputs independently of downstream algorithms; understanding them is critical for interpreting standalone LLM performance.
  - Quick check question: If FCO = 0.8 on a graph with 10 true edges, how many of those edges were correctly oriented? (Answer: 8.)

- Concept: Mutual Information (MI) and its symmetry
  - Why needed here: MI is used as a baseline prior; knowing it's symmetric explains why it must be combined with directional LLM priors to infer causality.
  - Quick check question: If MI(X,Y) = 0.5, what is MI(Y,X)? (Answer: Also 0.5.)

## Architecture Onboarding

- Component map: Prompt generation -> LLM inference -> Probabilistic prior matrix -> (Optional) MI matrix -> Prior fusion -> CD-UCT sampling -> DAG output
- Critical path: LLM prompt -> LLM output parsing -> prior matrix construction -> CD-UCT with custom simulation policy
- Design tradeoffs:
  - Prompt verbosity vs. inference cost: More traits (Variable List, Example, Priming) may improve accuracy but increase latency.
  - Soft vs. hard priors: Soft priors (probabilistic) allow flexibility but may be less decisive than hard constraints.
  - Model size vs. reproducibility: Smaller models (7B) are faster and open-weight; larger models (70B) may be more accurate but are costly and less reproducible.
- Failure signatures:
  - LLM outputs consistently favor option (C) -> prior matrix too sparse
  - High LOD values -> LLM outputs are incoherent across directions
  - CD-UCT performance no better than UR -> LLM priors not informative or combined poorly
- First 3 experiments:
  1. Run 3-Way prompt on Asia dataset with LLaMA2-7B; compute FCO, TERE, TENE; compare to UR baseline.
  2. Add Variable List trait; re-run experiment; check if metrics improve.
  3. Combine MI and LLM priors; run CD-UCT with bsims=1; measure SHD reduction over UR baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM-derived priors perform on causal discovery benchmarks that require specialized domain knowledge beyond common sense reasoning?
- Basis in paper: [explicit] The paper notes that LLM performance is poorer on the Child dataset, which requires specialist domain knowledge, compared to benchmarks like Insurance that rely more on common sense.
- Why unresolved: The paper only evaluates on three benchmark datasets and does not explore performance across a wider range of domains requiring specialized knowledge.
- What evidence would resolve it: Testing LLM priors on a diverse set of causal discovery benchmarks spanning multiple specialized domains (e.g., medicine, physics, economics) would provide insights into their generalizability.

### Open Question 2
- Question: What is the optimal method for combining LLM priors with other types of prior knowledge (e.g., expert constraints, ordering information) in causal discovery?
- Basis in paper: [explicit] The paper mentions that LLMs have been used to extract ancestral constraints and orderings, but does not explore combining these with LLM priors.
- Why unresolved: The paper focuses on combining LLM priors with mutual information priors, but does not investigate other potential combinations with different types of prior knowledge.
- What evidence would resolve it: Experiments comparing the performance of causal discovery algorithms using various combinations of LLM priors with other prior knowledge types would elucidate the optimal integration strategy.

### Open Question 3
- Question: How does the size of the LLM impact its performance in extracting priors for causal discovery, and is there a point of diminishing returns?
- Basis in paper: [explicit] The paper conducts experiments with LLaMA2-7B, LLaMA2-13B, and LLaMA2-70B models, finding that larger models do not lead to substantially better performance on commonsense reasoning tasks.
- Why unresolved: The paper only explores a limited range of model sizes and does not investigate whether the trend holds for even larger models or if there are domain-specific differences in the impact of model size.
- What evidence would resolve it: Scaling experiments with LLMs of varying sizes across a wide range of causal discovery benchmarks, including those requiring specialized knowledge, would reveal the relationship between model size and performance.

### Open Question 4
- Question: How can LLM priors be integrated into causal discovery algorithms in a way that scales to larger networks with hundreds or thousands of variables?
- Basis in paper: [explicit] The paper acknowledges that querying LLMs for all possible pairwise relationships becomes infeasible for larger networks and suggests the need for preprocessing to restrict the set of possible parents for each variable.
- Why unresolved: The paper does not propose a specific method for scaling LLM prior integration to large networks or explore the trade-offs involved in different preprocessing approaches.
- What evidence would resolve it: Developing and evaluating methods for preprocessing large networks to reduce the number of LLM queries while maintaining the quality of priors would provide insights into scalable integration strategies.

### Open Question 5
- Question: How do LLM priors perform in causal discovery tasks where the underlying data distribution differs significantly from the training data distribution of the LLM?
- Basis in paper: [inferred] The paper does not explicitly address this question, but it is a potential limitation given that LLMs are trained on general web data and may not have seen specific causal relationships in the target domain.
- Why unresolved: The paper does not investigate the robustness of LLM priors to distribution shifts or explore methods for adapting LLMs to specific domains.
- What evidence would resolve it: Experiments comparing the performance of LLM priors on causal discovery tasks with varying degrees of distribution shift from the LLM's training data would reveal their robustness and the need for adaptation techniques.

## Limitations
- The paper relies on synthetic benchmarks which may not reflect real-world domain complexity.
- Systematic biases in LLM pretraining data could skew causal judgments.
- No validation of soft priors in non-synthetic domains with ground truth.

## Confidence

- **High**: Effectiveness of 3-way prompts for improving causal judgment accuracy
- **Medium**: Superiority of Hadamard product fusion of MI and LLM priors
- **Medium**: LLMs are "especially helpful for edge directionality"

## Next Checks
1. Test LLM priors on a real-world observational dataset (e.g., health or economic data) where ground truth causal structure is partially known to assess generalization beyond synthetic benchmarks.
2. Perform ablation studies comparing Hadamard product fusion with weighted sum and concatenation methods to confirm optimal integration strategy.
3. Evaluate calibration of LLM-generated probabilistic priors by comparing predicted vs. empirical edge frequencies in held-out data.