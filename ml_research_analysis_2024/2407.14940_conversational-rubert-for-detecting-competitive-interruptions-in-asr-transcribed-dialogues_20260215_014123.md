---
ver: rpa2
title: Conversational Rubert for Detecting Competitive Interruptions in ASR-Transcribed
  Dialogues
arxiv_id: '2407.14940'
source_url: https://arxiv.org/abs/2407.14940
tags:
- speech
- interruptions
- competitive
- speaker
- interruption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses automatic detection of competitive interruptions
  in ASR-transcribed dialogues, a key task for customer satisfaction monitoring in
  call centers. The authors developed a text-based classification model that distinguishes
  competitive from non-competitive interruptions using the context of both speakers'
  phrases.
---

# Conversational Rubert for Detecting Competitive Interruptions in ASR-Transcribed Dialogues

## Quick Facts
- **arXiv ID:** 2407.14940
- **Source URL:** https://arxiv.org/abs/2407.14940
- **Reference count:** 39
- **One-line primary result:** Fine-tuned Conversational RuBERT achieved F1-macro score of 0.8404 for competitive interruption detection in ASR-transcribed Russian customer dialogues.

## Executive Summary
This paper presents a text-based classification model for detecting competitive interruptions in automatic speech recognition (ASR)-transcribed dialogues, specifically addressing customer satisfaction monitoring in call centers. The authors fine-tuned the Conversational RuBERT model on an in-house dataset of Russian customer support dialogues, optimizing hyperparameters through systematic experiments. The best model achieved an F1-macro score of 0.8404 on the test fold. Key findings include the importance of including both speakers' contexts for improved performance and the identification of an optimal learning rate of 7e-6. The study also revealed that extending the context beyond the immediate speakers' turns reduced model effectiveness.

## Method Summary
The method involves processing ASR-transcribed CSV dialogue data by reorganizing it to include speaker context, filtering overlaps to retain only successful interruptions lasting at least one second, and labeling data with binary classification (competitive vs. non-competitive interruptions). The Conversational RuBERT model was fine-tuned using nine-fold cross-validation with batch size 16, weight decay 0.01, and max length 128, training for 5 epochs with an optimal learning rate of 7e-6. The approach focuses on text-only features, excluding acoustic information, and evaluates performance using F1-macro score and ROC AUC metrics.

## Key Results
- Fine-tuned Conversational RuBERT achieved F1-macro score of 0.8404 on test fold for competitive interruption detection
- Including both speakers' contexts significantly improved model performance compared to single speaker context
- Optimal learning rate identified as 7e-6 for this specific task and dataset
- Extending context beyond immediate speaker turns (eight preceding/following turns) drastically reduced performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Including both speakers' contexts significantly improves competitive interruption classification performance.
- Mechanism: BERT-based models benefit from bidirectional context understanding. When both the interrupted speaker's and interrupting listener's phrases are provided, the model can better distinguish between cooperative (supportive) and competitive (control-seeking) intentions based on linguistic and semantic cues from both participants.
- Core assumption: The distinction between cooperative and competitive interruptions is encoded in the linguistic content and interaction patterns of both speakers' utterances.
- Evidence anchors:
  - [abstract]: "Key findings include the importance of including both speakers' contexts for improved performance"
  - [section]: "The inclusion of data regarding the interrupted speaker's context significantly improved the model's performance"
  - [corpus]: Weak - no direct evidence found in corpus about speaker context mechanisms
- Break condition: If competitive interruptions can be reliably detected from a single speaker's utterance without context, this mechanism would fail.

### Mechanism 2
- Claim: Optimal learning rate of 7e-6 enables effective fine-tuning of Conversational RuBERT for this task.
- Mechanism: The learning rate determines the step size during gradient descent optimization. At 7e-6, the model achieves the right balance between learning from the dataset and maintaining pre-trained knowledge, avoiding both underfitting and catastrophic forgetting.
- Core assumption: The pre-trained Conversational RuBERT weights are close to optimal for this task, requiring only small adjustments.
- Evidence anchors:
  - [abstract]: "the identification of an optimal learning rate of 7e-6"
  - [section]: "The best learning rate value for the task was 7e-6"
  - [corpus]: Weak - no direct evidence about learning rate optimization in corpus
- Break condition: If the task requires substantially different feature representations than the pre-training, this mechanism would break.

### Mechanism 3
- Claim: Excluding overlaps shorter than one second reduces false positives by filtering out backchannels and unintended overlaps.
- Mechanism: Short overlaps (typically 0.2-0.5 seconds) are more likely to be backchannels or phatic expressions rather than genuine attempts to take the conversational floor. By setting a minimum duration threshold, the model focuses on meaningful competitive interactions.
- Core assumption: Competitive interruptions are characterized by sustained speech that indicates deliberate floor-taking.
- Evidence anchors:
  - [section]: "Second, we excluded all overlaps lasting less than one second; the shorter the overlap, the more likely that it is a backchannel or something not of interest"
  - [corpus]: Weak - no direct evidence about duration filtering mechanisms
- Break condition: If some competitive interruptions naturally occur in very short bursts, this mechanism would incorrectly filter them out.

## Foundational Learning

- Concept: BERT fine-tuning for sequence classification
  - Why needed here: The task requires adapting a pre-trained language model to distinguish between two classes of dialogue interruptions
  - Quick check question: What is the purpose of the classification layer added on top of BERT in this fine-tuning setup?

- Concept: Text preprocessing for ASR transcripts
  - Why needed here: ASR output requires cleaning and formatting to be usable as model input, including handling timestamps and speaker turns
  - Quick check question: How are speaker turns and timestamps used to identify and extract interruption segments?

- Concept: Cross-validation for small datasets
  - Why needed here: The in-house dataset is limited in size, making it essential to use k-fold cross-validation to obtain reliable performance estimates
  - Quick check question: Why is 9-fold cross-validation used instead of a simpler train/test split?

## Architecture Onboarding

- Component map: ASR transcript processing → CSV reorganization → Context feature extraction → BERT fine-tuning → Classification output
- Critical path: Data filtration → Context construction → Model fine-tuning → Performance evaluation
- Design tradeoffs: Text-only vs. audio features (simplicity vs. richer information), context length (informative vs. computational efficiency)
- Failure signatures: Poor F1 scores indicate either inadequate context features, improper learning rate, or insufficient training data
- First 3 experiments:
  1. Compare model performance with interrupter context only vs. both speakers' contexts
  2. Test different learning rates (1e-6 to 9e-6) to find optimal fine-tuning step size
  3. Experiment with extended context (preceding and following turns) to assess information gain vs. noise

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold for filtering out too-short overlaps in competitive interruption detection?
- Basis in paper: [explicit] The paper mentions a one-second threshold was used but suggests it could be optimized experimentally.
- Why unresolved: The current threshold was chosen without systematic experimentation to find the optimal value.
- What evidence would resolve it: Experimental results comparing model performance across different threshold values (e.g., 0.5s, 0.8s, 1.2s, 1.5s) using metrics like F1-macro score.

### Open Question 2
- Question: How does the inclusion of unsuccessful interruption attempts affect model performance in detecting competitive interruptions?
- Basis in paper: [inferred] The paper filtered out unsuccessful interruptions, noting they could enrich the model but would require more extensive labeling.
- Why unresolved: The current model excludes unsuccessful attempts, potentially missing valuable data that could improve detection of aggressive or supportive intentions.
- What evidence would resolve it: Experimental comparison of model performance with and without unsuccessful interruption attempts in the training data.

### Open Question 3
- Question: What is the impact of extending context beyond the immediate speakers' turns on competitive interruption classification?
- Basis in paper: [explicit] The paper tested concatenating eight preceding and following turns but found it drastically reduced performance.
- Why unresolved: The paper only tested one configuration of extended context; different amounts or types of context might yield better results.
- What evidence would resolve it: Systematic experiments testing various context window sizes and positions (e.g., 2-4 turns before/after, different combinations) to find optimal context configuration.

## Limitations
- Proprietary dataset prevents independent validation and limits generalizability to other domains or languages
- Text-only approach ignores potentially valuable acoustic features such as pitch, intensity, and speech rate
- Optimal hyperparameters (7e-6 learning rate) are specific to this dataset and may not transfer well to different domains

## Confidence

- **High confidence**: The mechanism by which including both speakers' contexts improves performance is well-supported by the experimental results showing superior F1-macro scores when both contexts are included versus single speaker context.
- **Medium confidence**: The identified optimal learning rate of 7e-6 is reliable for this specific dataset and task, but may require re-tuning for different datasets or domains.
- **Medium confidence**: The duration filtering approach (excluding overlaps <1 second) is methodologically sound, though the specific threshold could benefit from further validation across different conversation types.

## Next Checks

1. **Cross-domain validation**: Test the model on publicly available dialogue datasets from different domains (e.g., medical consultations, customer service in other languages) to assess generalizability and identify domain-specific limitations.

2. **Multimodal feature integration**: Conduct experiments incorporating acoustic features alongside text to quantify the performance gain from multimodal analysis and determine if this improves competitive interruption detection beyond text-only approaches.

3. **Threshold sensitivity analysis**: Systematically vary the minimum duration threshold (currently 1 second) and evaluate its impact on precision, recall, and F1 scores across different types of interruptions to optimize the filtering criteria for various conversation contexts.