---
ver: rpa2
title: LLMs Are Prone to Fallacies in Causal Inference
arxiv_id: '2406.12158'
source_url: https://arxiv.org/abs/2406.12158
tags:
- causal
- relations
- position
- temporal
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  infer causal relationships beyond memorizing explicitly stated facts in their pretraining
  data. The authors design experiments where they finetune LLMs on synthetic data
  containing temporal, spatial, and counterfactual relations, then evaluate whether
  the models can infer causal relations from these observations.
---

# LLMs Are Prone to Fallacies in Causal Inference

## Quick Facts
- arXiv ID: 2406.12158
- Source URL: https://arxiv.org/abs/2406.12158
- Authors: Nitish Joshi; Abulhair Saparov; Yixin Wang; He He
- Reference count: 40
- Key outcome: LLMs rely on position heuristics and post hoc fallacies when inferring causal relations, struggling particularly with counterfactual reasoning

## Executive Summary
This paper investigates whether large language models can infer causal relationships beyond memorizing explicitly stated facts in their pretraining data. Through systematic experiments with finetuned LLMs on synthetic temporal, spatial, and counterfactual relations, the authors demonstrate that models exhibit significant reasoning limitations. LLMs show strong susceptibility to position heuristics (inferring causality from entity mention order) and post hoc fallacies (inferring causality from temporal precedence). Even when these biases are mitigated through data augmentation or model scaling, models still struggle with counterfactual reasoning, suggesting fundamental limitations in causal inference capabilities.

## Method Summary
The authors generate synthetic datasets containing temporal, spatial, and counterfactual relations based on controlled causal graphs. They finetune Llama2 models (7B, 13B, 70B) using LoRA on these datasets with randomized relative positions of events. Models are evaluated using template-based marginal likelihood comparisons for multiple-choice causal relation prediction tasks. The experiments systematically manipulate position bias and relation types to isolate the model's ability to infer causal relations from different types of observational data.

## Key Results
- LLMs show strong reliance on position heuristics, inferring causality from entity mention order in text
- Even with position randomization, models exhibit post hoc fallacies, inferring causality from temporal precedence
- Models can deduce absence of causal relations from temporal and spatial relations but struggle to infer causality from counterfactuals
- These fallacies persist across different model scales and can only be partially mitigated through explicit training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs rely on relative entity mention order as a heuristic for inferring causality
- Mechanism: The model learns that when X is mentioned before Y in text, X is more likely to be the cause of Y. This is because pretraining data often presents causal relationships with causes preceding effects in text.
- Core assumption: The pretraining corpus contains a bias where causes are more frequently mentioned before their effects than vice versa.
- Evidence anchors:
  - [abstract]: "We find that: (a) LLMs are susceptible to inferring causal relations from the order of two entity mentions in text (e.g. X mentioned before Y implies X causes Y)"
  - [section]: "We find that: (a) LLMs are susceptible to inferring causal relations from the order of two entity mentions in text (e.g. X mentioned before Y implies X causes Y); (b) if the order is randomized, LLMs still suffer from the post hoc fallacy, i.e. X occurs before Y (temporal relation) implies X causes Y"
  - [corpus]: "Across all causal relations, we find that when X, Y co-occur within the context window, 60.77% of the times X occurs before Y"
- Break condition: Randomizing the relative order of entity mentions during training significantly reduces this heuristic reliance.

### Mechanism 2
- Claim: LLMs exhibit the post hoc fallacy - inferring causality from temporal precedence
- Mechanism: When temporal relations are presented without position bias, the model overgeneralizes and assumes that if X precedes Y, then X causes Y. This is a well-known human cognitive bias.
- Core assumption: The model has not learned the temporal precedence principle that X preceding Y does not necessarily imply X causes Y.
- Evidence anchors:
  - [abstract]: "if the order is randomized, LLMs still suffer from the post hoc fallacy, i.e. X occurs before Y (temporal relation) implies X causes Y"
  - [section]: "we demonstrate that for temporal relations, models in fact overgeneralize to infer the presence of causal relations in the other direction. This mistake is often referred to as the post hoc fallacy"
  - [corpus]: "Humans have known to often fall prey to this fallacy and infer causal relations from sequential order"
- Break condition: Including explicit negative causal examples during training can reduce this overgeneralization.

### Mechanism 3
- Claim: LLMs struggle to infer causality from counterfactual reasoning
- Mechanism: The model cannot effectively process counterfactual statements to deduce causal relationships, even though humans use counterfactuals extensively for causal reasoning.
- Core assumption: The model has not learned the logical connection between counterfactual statements and causal inference.
- Evidence anchors:
  - [abstract]: "we also find that while LLMs can correctly deduce the absence of causal relations from temporal and spatial relations, they have difficulty inferring causal relations from counterfactuals"
  - [section]: "we find that while LLMs are able to deduce the absence of causal relations from temporal and spatial relations, they struggle to infer the presence of causal relations from counterfactuals"
  - [corpus]: "While counterfactuals are not solely based on physical observations like the other two relations, humans often use counterfactuals to make causal claims"
- Break condition: Scaling the model size does not improve performance on counterfactual reasoning tasks.

## Foundational Learning

- Concept: Temporal precedence principle
  - Why needed here: Understanding that temporal order alone is insufficient for establishing causality is crucial for evaluating LLM reasoning
  - Quick check question: If event A happens before event B, can we conclude A causes B? Why or why not?

- Concept: Position bias in language models
  - Why needed here: Recognizing that LLMs can learn spurious correlations from training data patterns is essential for interpreting results
  - Quick check question: How might the frequent co-occurrence of "smoking" before "cancer" in text lead an LLM to incorrectly infer causality?

- Concept: Counterfactual reasoning
  - Why needed here: Understanding how counterfactuals are used in causal inference helps explain why LLMs struggle with this reasoning type
  - Quick check question: What logical connection exists between "If X hadn't happened, Y wouldn't have happened" and "X causes Y"?

## Architecture Onboarding

- Component map: LLM (Llama2 family) → LoRA fine-tuning → Synthetic data generation → Evaluation with template-based marginal likelihood
- Critical path: Data generation → Model fine-tuning → Causal relation prediction → Performance evaluation
- Design tradeoffs: Synthetic data vs real data (controlled experiments vs ecological validity), template randomization vs position bias, model scale vs reasoning quality
- Failure signatures: High accuracy when training and test positions match but low accuracy when they don't, systematic errors following temporal order, inability to process counterfactuals
- First 3 experiments:
  1. Train on temporal relations with fixed X-before-Y order, evaluate with matched and mismatched positions to confirm position heuristic
  2. Train on temporal relations with randomized positions, evaluate for post hoc fallacy
  3. Train on counterfactual relations, evaluate ability to infer causality vs absence of causality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does finetuning on real-world medical journal data allow LLMs to correctly infer novel causal relationships beyond those explicitly stated in the training data?
- Basis in paper: [explicit] The paper investigates whether LLMs can infer causal relations from other relational data in text by finetuning on synthetic data. The authors find that LLMs struggle to infer causal relations from counterfactuals and primarily rely on heuristics like temporal precedence.
- Why unresolved: The experiments use synthetic data generated from a controlled causal graph. While this isolates the role of memorization vs inference, it doesn't capture the complexity and richness of real-world data like medical journals.
- What evidence would resolve it: Finetuning LLMs on a large corpus of medical journal articles and then testing their ability to infer novel causal relationships not explicitly stated in the training data. Comparing performance to a model finetuned on a similarly sized corpus of non-medical text could help control for general language understanding improvements.

### Open Question 2
- Question: Can LLMs be trained to recognize and correct for their tendency to commit the post hoc fallacy when inferring causal relationships from temporal data?
- Basis in paper: [explicit] The authors find that LLMs suffer from the post hoc fallacy, inferring positive causal relations from temporal precedence (X occurs before Y implies X causes Y). They show that this can be mitigated by finetuning with explicit statements of presence and absence of causal relations.
- Why unresolved: While the authors demonstrate that explicit finetuning can help mitigate the post hoc fallacy, it's unclear if this approach scales to larger models or if more sophisticated methods are needed to fully address this bias.
- What evidence would resolve it: Testing the effectiveness of explicit finetuning on larger LLMs (e.g., Llama2-70B) and comparing to other potential methods like data augmentation, adversarial training, or causal inference techniques. Evaluating the models on a diverse set of temporal reasoning tasks could help assess the robustness of the mitigation.

### Open Question 3
- Question: Do LLMs struggle to infer causal relationships from counterfactuals because of limitations in their training data or inherent difficulties in understanding counterfactual reasoning?
- Basis in paper: [explicit] The authors find that LLMs have difficulty inferring causal relations from counterfactuals, even when the position heuristic is mitigated. This suggests that counterfactual reasoning may be a particularly challenging task for LLMs.
- Why unresolved: It's unclear whether the LLMs' struggles with counterfactuals are due to a lack of relevant examples in their training data or more fundamental limitations in their ability to reason about hypothetical scenarios.
- What evidence would resolve it: Analyzing the frequency and quality of counterfactual examples in the pretraining data of various LLMs could help determine if data limitations play a role. Additionally, testing LLMs on a range of counterfactual reasoning tasks, from simple to complex, could help identify specific areas of difficulty and inform potential solutions.

## Limitations

- Reliance on synthetic data may not capture the complexity of real-world causal reasoning
- Focus exclusively on Llama2 family limits generalizability to other LLM architectures
- Evaluation methodology using marginal likelihood comparisons may not capture all aspects of human causal reasoning

## Confidence

- **High confidence** in findings regarding position heuristics and post hoc fallacies: Strong experimental evidence across multiple model scales consistent with established cognitive biases
- **Medium confidence** in counterfactual reasoning findings: Sound experimental design but synthetic scenarios may not reflect real-world complexity
- **Medium confidence** in implications for real-world LLM applications: Demonstrates specific reasoning limitations but translation to practical applications requires additional validation

## Next Checks

1. **Real-world validation**: Apply the same experimental methodology to real-world datasets containing naturally occurring causal relationships to assess whether the identified heuristics and fallacies persist outside controlled synthetic environments.

2. **Cross-model generalization**: Test whether position heuristics and post hoc fallacies are consistent across different LLM architectures (e.g., GPT, Claude, Gemini) to determine if these are universal LLM limitations or specific to the Llama2 family.

3. **Intervention testing**: Design targeted interventions during training (e.g., explicit counterfactual reasoning examples, causal invariance training) to assess whether the identified fallacies can be systematically reduced or eliminated.