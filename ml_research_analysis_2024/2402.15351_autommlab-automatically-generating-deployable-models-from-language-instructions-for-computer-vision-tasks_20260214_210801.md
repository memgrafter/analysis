---
ver: rpa2
title: 'AutoMMLab: Automatically Generating Deployable Models from Language Instructions
  for Computer Vision Tasks'
arxiv_id: '2402.15351'
source_url: https://arxiv.org/abs/2402.15351
tags:
- data
- hyperparameter
- training
- arxiv
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoMMLab is a system that automatically generates deployable computer
  vision models from natural language instructions. It connects AutoML techniques
  with OpenMMLab by employing large language models (LLMs) to understand user requests
  and control the entire model production workflow.
---

# AutoMMLab: Automatically Generating Deployable Models from Language Instructions for Computer Vision Tasks

## Quick Facts
- arXiv ID: 2402.15351
- Source URL: https://arxiv.org/abs/2402.15351
- Authors: Zekang Yang; Wang Zeng; Sheng Jin; Chen Qian; Ping Luo; Wentao Liu
- Reference count: 40
- Key outcome: AutoMMLab is a system that automatically generates deployable computer vision models from natural language instructions.

## Executive Summary
AutoMMLab is a novel system that bridges the gap between AutoML techniques and OpenMMLab by employing large language models (LLMs) to understand user requests and control the entire model production workflow. The system automates the generation of deployable computer vision models from natural language instructions, covering tasks such as classification, detection, segmentation, and keypoint estimation. AutoMMLab consists of a request understanding module (RU-LLaMA), a hyperparameter optimization module (HPO-LLaMA), and a deployment module using MMDeploy. The system is evaluated on the LAMP benchmark, demonstrating significant improvements over baselines in request understanding accuracy and end-to-end performance.

## Method Summary
AutoMMLab takes language-based model training requests from users and automatically schedules and executes the entire workflow to produce deployable models. The system employs LLMs as a bridge to connect AutoML and the OpenMMLab community. It consists of five main stages: request understanding, data selection, model selection, model training with hyperparameter optimization, and model deployment. The request understanding module (RU-LLaMA) parses user requests into JSON configurations, while the hyperparameter optimization module (HPO-LLaMA) predicts optimal hyperparameter settings based on extensive knowledge from previous training trials. The system integrates with OpenMMLab toolboxes for data and model management and uses MMDeploy for model deployment.

## Key Results
- AutoMMLab achieves 88.75% request understanding accuracy on the LAMP benchmark.
- The system significantly outperforms baselines, achieving a 130/160 end-to-end evaluation score on LAMP.
- HPO-LLaMA demonstrates improved efficiency compared to random sampling in hyperparameter optimization.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AutoMMLab achieves efficient hyperparameter optimization by leveraging a fine-tuned LLM (HPO-LLaMA) that predicts optimal hyperparameter settings based on extensive knowledge from previous training trials.
- Mechanism: The system trains HPO-LLaMA on a dataset of triplets containing user requests, hyperparameter settings, and corresponding model performance. The LLM uses this knowledge to iteratively suggest improved hyperparameter configurations, reducing the number of trials needed compared to random sampling.
- Core assumption: LLM-based prediction can generalize from the training data to suggest effective hyperparameters for new, unseen tasks.
- Evidence anchors:
  - [abstract]: "To solve this problem, we propose a novel LLM-based HPO algorithm, called HPO-LLaMA. Equipped with extensive knowledge and experience in model hyperparameter tuning, HPO-LLaMA achieves significant improvement of HPO efficiency."
  - [section 4.5]: "We believe that the data of the triplets contains rich knowledge about model training and hyperparameter tuning. We utilize triplet data to construct both single-round and multi-round dialogues, thereby empowering HPO-LLaMA to activate the capability of predicting efficient hyperparameters from these dialogues."
  - [corpus]: Weak evidence; related papers focus on LLM-based AutoML but don't specifically address HPO-LLaMA's iterative refinement mechanism.
- Break condition: If the training data lacks diversity or the LLM fails to generalize, the predictions may not improve over random sampling.

### Mechanism 2
- Claim: RU-LLaMA accurately parses natural language requests into structured configurations by fine-tuning on a large dataset of request-config pairs generated with GPT-4.
- Mechanism: The system uses GPT-4 to generate diverse training data, which is then manually checked and corrected. RU-LLaMA is fine-tuned on this data using LoRA, enabling it to understand user requests and output JSON configurations that guide the entire AutoML pipeline.
- Core assumption: The combination of LLM-generated data and fine-tuning can create a robust request understanding model.
- Evidence anchors:
  - [abstract]: "Specifically, we propose RU-LLaMA to understand users’ request and schedule the whole pipeline..."
  - [section 4.2]: "Following the main-stream methods of fine-tuning large language models in specific domains, we use LoRA [31] technology to fine-tune the LLaMA2-7B [62] model and obtained our request understanding model RU-LLaMA."
  - [corpus]: Weak evidence; related papers discuss LLM-based AutoML but not specifically request understanding with LoRA fine-tuning.
- Break condition: If the generated training data is insufficient or contains errors, RU-LLaMA's parsing accuracy will degrade.

### Mechanism 3
- Claim: AutoMMLab's modular design allows it to cover a wide range of computer vision tasks by integrating with OpenMMLab toolboxes and using LLM agents to control the workflow.
- Mechanism: The system consists of five main stages (request understanding, data selection, model selection, model training with HPO, and model deployment) that are orchestrated by LLM agents. Each stage interacts with pre-built datasets, models, and tools to automate the entire pipeline.
- Core assumption: LLM agents can effectively control complex workflows and make appropriate decisions at each stage.
- Evidence anchors:
  - [abstract]: "The proposed AutoMMLab system effectively employs LLMs as the bridge to connect AutoML and OpenMMLab community..."
  - [section 4.1]: "Our AutoMMLab system directly takes language-based model training request from users as the input, and automatically schedule and execute the entire workflow to produce deployable models."
  - [corpus]: Weak evidence; related papers discuss LLM-based AutoML but not specifically the modular integration with OpenMMLab.
- Break condition: If the LLM agents fail to make correct decisions or the integration with OpenMMLab is incomplete, the system will not function properly.

## Foundational Learning

- Concept: Natural Language Understanding
  - Why needed here: AutoMMLab relies on LLMs to parse user requests into structured configurations. Understanding how LLMs process and interpret natural language is crucial for developing and improving the request understanding module.
  - Quick check question: What are the key components of an LLM's architecture that enable it to understand and generate natural language?

- Concept: Hyperparameter Optimization
  - Why needed here: AutoMMLab uses HPO-LLaMA to efficiently search for optimal hyperparameters. Understanding the principles and techniques of hyperparameter optimization is essential for designing and evaluating the HPO module.
  - Quick check question: What are the main differences between traditional hyperparameter optimization methods (e.g., grid search, random search) and LLM-based approaches?

- Concept: Computer Vision Tasks and Models
  - Why needed here: AutoMMLab covers a wide range of computer vision tasks (classification, detection, segmentation, keypoint estimation) and relies on pre-trained models from OpenMMLab. Familiarity with these tasks and models is necessary for selecting appropriate datasets, models, and evaluation metrics.
  - Quick check question: What are the key differences between the evaluation metrics used for image classification, object detection, semantic segmentation, and keypoint estimation?

## Architecture Onboarding

- Component map: Request Understanding (RU-LLaMA) -> Data Selection -> Model Selection -> Model Training with HPO (HPO-LLaMA) -> Model Deployment
- Critical path: Request Understanding → Data Selection → Model Selection → Model Training with HPO → Model Deployment
- Design tradeoffs:
  - Using LLMs for request understanding and HPO vs. traditional rule-based or optimization methods: LLMs offer flexibility and generalization but may be less interpretable and require more computational resources.
  - Fine-tuning LLMs (RU-LLaMA, HPO-LLaMA) vs. using off-the-shelf models: Fine-tuning allows customization for specific tasks but requires additional training data and computational resources.
- Failure signatures:
  - Request Understanding Failure: Incorrectly parsed configurations leading to inappropriate data or model selection.
  - HPO Failure: Suboptimal hyperparameters resulting in poor model performance or excessive training time.
  - Data/Model Selection Failure: Inability to find suitable datasets or models matching user requirements.
- First 3 experiments:
  1. Test RU-LLaMA on a set of diverse user requests to evaluate its parsing accuracy and identify common failure modes.
  2. Compare the performance of HPO-LLaMA against random sampling and other hyperparameter optimization methods on a benchmark dataset.
  3. Deploy a trained model using MMDeploy and measure its inference speed and accuracy on a target device to validate the end-to-end pipeline.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out any open questions.

## Limitations
- The paper lacks detailed analysis of potential failure points in the integration with OpenMMLab toolboxes, particularly for edge cases or unusual hardware requirements.
- The effectiveness of the LLM-based hyperparameter optimization is demonstrated primarily on benchmark tasks, and its generalization to novel tasks is unverified.
- The system's performance on real-world, complex requests beyond the LAMP benchmark is unverified.

## Confidence
**High Confidence Claims**:
- The modular architecture design of AutoMMLab is clearly specified and technically sound
- The integration with OpenMMLab ecosystem for data and model management is feasible
- The overall concept of using LLMs for request understanding is supported by existing research

**Medium Confidence Claims**:
- RU-LLaMA's 88.75% accuracy claim needs independent verification
- The 130/160 end-to-end evaluation score on LAMP benchmark is promising but limited to benchmark conditions
- HPO-LLaMA's efficiency gains compared to random sampling are demonstrated but may not generalize

**Low Confidence Claims**:
- Real-world deployment success across diverse hardware platforms
- Scalability to enterprise-level workloads
- Performance consistency across different user expertise levels

## Next Checks
1. **Independent Request Understanding Test**: Evaluate RU-LLaMA on a separate, diverse dataset of real user requests not included in the LAMP benchmark to verify generalization beyond curated examples.
2. **HPO-LLaMA vs. Traditional Methods**: Conduct a comprehensive comparison of HPO-LLaMA against established hyperparameter optimization techniques (Bayesian optimization, random search) across multiple CV tasks and dataset sizes to quantify efficiency gains.
3. **End-to-End Deployment Testing**: Deploy generated models on various hardware configurations (CPU, GPU, edge devices) and measure inference latency, memory usage, and accuracy to validate the practical deployment claims.