---
ver: rpa2
title: 'Simultaneous Reward Distillation and Preference Learning: Get You a Language
  Model Who Can Do Both'
arxiv_id: '2410.08458'
source_url: https://arxiv.org/abs/2410.08458
tags:
- preference
- reward
- drdo
- learning
- preferences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning large language models
  (LLMs) to human preferences, particularly when dealing with non-deterministic or
  ambiguous preference labels in training data. The core issue is that existing methods
  like Direct Preference Optimization (DPO) struggle to effectively learn from such
  nuanced preferences, leading to suboptimal alignment and policy degeneracy.
---

# Simultaneous Reward Distillation and Preference Learning: Get You a Language Model Who Can Do Both

## Quick Facts
- arXiv ID: 2410.08458
- Source URL: https://arxiv.org/abs/2410.08458
- Reference count: 40
- The paper proposes DRDO, a method that combines reward distillation and preference learning to align LLMs to human preferences, particularly in the presence of non-deterministic preference labels.

## Executive Summary
This paper addresses the challenge of aligning large language models to human preferences, particularly when dealing with non-deterministic or ambiguous preference labels in training data. Existing methods like Direct Preference Optimization (DPO) struggle with nuanced preferences, leading to suboptimal alignment and policy degeneracy. The proposed method, DRDO (Direct Reward Distillation and policy-Optimization), combines reward distillation and preference learning into a unified framework. It uses an oracle reward model to directly distill rewards into the policy while simultaneously learning preferences through a novel focal-softened log-odds unlikelihood loss. This approach avoids the limitations of DPO by explicitly modeling rewards and adaptively learning preferences.

## Method Summary
DRDO is a reference-free alignment method that combines reward distillation with preference learning. The approach uses an oracle reward model to provide scalar rewards for distillation, while simultaneously learning preferences through a focal-softened log-odds unlikelihood loss. The focal term modulates updates based on preference strength, preventing underfitting to non-deterministic preferences. The training process involves two stages: first training the oracle reward model with SFT regularization, then distilling to the student policy using a combined loss function that includes both ℓ2 reward difference and focal-modulated log-odds terms. The method is designed to handle non-deterministic preferences more effectively than existing approaches like DPO.

## Key Results
- DRDO-trained policies outperform baselines like DPO and e-DPO in terms of expected rewards and robustness to noisy preference signals
- On the AlpacaEval benchmark, DRDO achieves win rates of 62.03% and 61.61% against e-DPO and DPO, respectively
- DRDO is particularly effective at handling non-deterministic preferences and generalizing to out-of-distribution settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DRDO avoids policy degeneracy by explicitly distilling rewards from an oracle while adaptively learning preferences
- Mechanism: The method combines a reward distillation loss with a focal-softened log-odds unlikelihood loss. The focal term modulates updates based on preference strength, preventing underfitting to non-deterministic preferences
- Core assumption: An oracle reward model can approximate true human preferences and provide stable scalar rewards for distillation
- Evidence anchors:
  - [abstract] "DRDO directly mimics rewards assigned by an oracle while learning human preferences with a novel preference likelihood formulation"
  - [section] "The student model's reward estimates are computed with a linear reward head...This optimization is achieved by a knowledge-distillation loss (Lkd) that combines both a supervised ℓ2-norm term and a novel focal-softened log odds-unlikelihood component"
  - [corpus] Weak - corpus provides related papers but no direct evidence for this specific mechanism
- Break condition: Oracle reward model fails to generalize or produces unstable rewards, or focal parameter γ is poorly tuned causing training instability

### Mechanism 2
- Claim: DRDO's focal term prevents the underfitting problem of DPO by maintaining gradient updates for non-deterministic preferences
- Mechanism: The modulating term (1 - pw)γ in the loss scales gradients based on preference confidence pw. When pw ≈ 0.5 (non-deterministic), gradients remain active; when pw ≈ 1 (deterministic), gradients diminish
- Core assumption: Preference strength varies meaningfully across samples and can be quantified as pw = σ(zw - zl)
- Evidence anchors:
  - [section] "This modulating term amplifies gradient updates when preference signals are weak (pw ≈ 0.5) and tempering updates when they are strong (pw ≈ 1)"
  - [section] "Unlike DPO's fixed β that is applied across the whole training dataset, this modulating term amplifies gradient updates when preference signals are weak"
  - [corpus] Weak - corpus provides related preference optimization papers but no direct evidence for focal modulation mechanism
- Break condition: γ parameter is set too high causing excessive penalty on uncertain samples, or pw calculation is unstable leading to erratic gradient scaling

### Mechanism 3
- Claim: DRDO's reference-free design avoids misalignment between reward optimization and preference optimization
- Mechanism: By directly regressing policy rewards to oracle rewards rather than using implicit rewards from log-probability ratios, DRDO decouples reward learning from preference learning, avoiding the misalignment problem
- Core assumption: Reward modeling and policy optimization can be effectively separated without loss of alignment quality
- Evidence anchors:
  - [abstract] "DRDO directly mimics rewards assigned by an oracle while learning human preferences with a novel preference likelihood formulation"
  - [section] "DRDO circumvents the misalignment problem by avoiding a reference model-based soft-constraint and casts the reward learning into a distillation problem"
  - [corpus] Weak - corpus provides related papers but no direct evidence for reference-free alignment mechanism
- Break condition: Oracle reward model becomes the bottleneck, or the separation between reward and preference learning leads to conflicting optimization objectives

## Foundational Learning

- Concept: Bradley-Terry model for pairwise preferences
  - Why needed here: Understanding the BT model is crucial because DRDO's focal term specifically addresses limitations in BT-based preference learning
  - Quick check question: What is the mathematical form of the Bradley-Terry preference probability, and how does it relate to reward differences?

- Concept: Knowledge distillation in neural networks
  - Why needed here: DRDO's core mechanism is reward distillation from oracle to student policy, requiring understanding of distillation principles
  - Quick check question: What is the difference between logit matching and feature matching in knowledge distillation, and which does DRDO use?

- Concept: Focal loss and its application in imbalanced learning
  - Why needed here: DRDO's focal-softened log-odds unlikelihood is inspired by focal loss, so understanding its modulation properties is essential
  - Quick check question: How does the focal parameter γ affect gradient scaling for easy vs. hard examples in classification?

## Architecture Onboarding

- Component map:
  - Oracle reward model -> Student policy model with linear reward head -> DRDO loss function
- Critical path: Oracle training → Policy initialization (SFT) → Reward distillation + preference learning → Evaluation
- Design tradeoffs:
  - Oracle quality vs. computational cost (need to train oracle but it's used offline)
  - Focal parameter tuning (affects adaptation to preference strength)
  - Reference-free vs. reference-based alignment (simpler but depends on oracle quality)
- Failure signatures:
  - Oracle produces noisy rewards → Policy learns incorrect preferences
  - γ too high → Training instability and poor convergence
  - SFT initialization poor → Policy starts far from optimal region
- First 3 experiments:
  1. Ablation: DRDO vs DRDO without reward distillation component on small dataset
  2. Sensitivity: Test different γ values on validation set to find optimal modulation strength
  3. Oracle quality: Compare DRDO performance with oracle vs ground truth rewards to measure oracle approximation error

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DRDO's performance scale with different Oracle model sizes and architectures when training much smaller student models?
- Basis in paper: [explicit] The paper states "Fixing the size of O allows us to evaluate the extent of preference alignment to smaller models, as in classic knowledge distillation" but does not extensively explore varying Oracle sizes
- Why unresolved: The paper only uses one fixed Oracle size for each experiment, limiting understanding of the optimal Oracle-to-student size ratio for effective distillation
- What evidence would resolve it: Systematic experiments varying Oracle model size while keeping student size constant, measuring performance metrics across different size ratios

### Open Question 2
- Question: What is the impact of non-deterministic preferences on long-form generation tasks beyond summarization and instruction following?
- Basis in paper: [inferred] The paper identifies non-deterministic preferences as a key challenge and shows DRDO handles them better than baselines, but only tests on specific tasks
- Why unresolved: Experiments are limited to summarization and single-turn instruction following, leaving uncertainty about performance on dialogue, story generation, or other long-form tasks
- What evidence would resolve it: Evaluation of DRDO on diverse long-form generation tasks with explicit analysis of preference strength distribution and model performance across different preference types

### Open Question 3
- Question: How does DRDO's focal loss modulation parameter γ interact with different types of preference distributions in the training data?
- Basis in paper: [explicit] The paper shows γ = 2 is optimal in experiments but notes "a reasonable way to find the right γ would vary case by case" depending on policy initialization and training setting
- Why unresolved: The paper provides limited analysis of how different γ values affect performance across various preference distributions and training scenarios
- What evidence would resolve it: Comprehensive ablation studies varying γ across different preference distribution types (deterministic vs non-deterministic ratios) and training settings (SFT-initialized vs random initialization)

## Limitations
- Oracle dependency: DRDO's performance critically depends on the quality of the oracle reward model
- Focal parameter sensitivity: The method's effectiveness depends heavily on correctly tuning the γ parameter
- Reference-free alignment risks: The approach may be less robust than reference-based methods when oracle rewards are noisy

## Confidence
- High confidence: The core mathematical formulation of DRDO is sound and builds logically on established knowledge distillation principles
- Medium confidence: The claims about DRDO's superior handling of non-deterministic preferences are well-supported by experimental results
- Low confidence: The assertion that reference-free alignment is inherently superior to reference-based methods is not fully substantiated

## Next Checks
1. **Oracle quality ablation**: Train DRDO policies using ground truth rewards versus oracle rewards to quantify the performance gap and identify whether oracle approximation error is the limiting factor in current results.

2. **Focal parameter sweep**: Systematically vary γ from 0.5 to 5.0 and measure impact on win rates, convergence speed, and robustness to preference noise to establish optimal tuning ranges and failure modes.

3. **Extreme preference ambiguity test**: Create synthetic preference datasets with controlled levels of ambiguity (e.g., pw values approaching 0.5) to stress-test DRDO's ability to handle highly non-deterministic preferences and compare against DPO's behavior in these regimes.