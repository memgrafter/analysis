---
ver: rpa2
title: Modeling Real-Time Interactive Conversations as Timed Diarized Transcripts
arxiv_id: '2405.13203'
source_url: https://arxiv.org/abs/2405.13203
tags:
- figure
- language
- page
- message
- conversations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a method to simulate real-time interactive
  conversations using pretrained text-only language models by modeling timed diarized
  transcripts and decoding them with causal rejection sampling. The approach is demonstrated
  on two domains: instant messenger dialogues and spoken conversations, requiring
  generation speeds of ~30 and ~20 tokens/second respectively for real-time interactivity.'
---

# Modeling Real-Time Interactive Conversations as Timed Diarized Transcripts

## Quick Facts
- arXiv ID: 2405.13203
- Source URL: https://arxiv.org/abs/2405.13203
- Authors: Garrett Tanzer; Gustaf Ahdritz; Luke Melas-Kyriazi
- Reference count: 40
- One-line primary result: A method to simulate real-time interactive conversations using pretrained text-only language models by modeling timed diarized transcripts and decoding with causal rejection sampling.

## Executive Summary
This paper introduces a method to simulate real-time interactive conversations using pretrained text-only language models by modeling timed diarized transcripts and decoding them with causal rejection sampling. The approach is demonstrated on two domains: instant messenger dialogues and spoken conversations, requiring generation speeds of ~30 and ~20 tokens/second respectively for real-time interactivity. Key results include the 99th percentile generation bandwidth needed (28 tok/s for instant messenger, 36 tok/s for spoken conversations), document-level NLL (2179-3181 on instant messenger, 1532-2261 on spoken conversations), and human ratings (1.45-5.95 for instant messenger, 0.6-5.2 for spoken conversations). The method can be implemented on commodity hardware and requires relatively little data to add real-time interactive capabilities to language models.

## Method Summary
The method involves fine-tuning decoder-only language models (Pythia 160M-12B, Gemma 2B, Llama 2 7B) on timed diarized transcripts using causal rejection sampling with speculative decoding. Conversations are formatted with timestamps, speaker IDs, and control tokens, then modeled as autoregressive sequences. During generation, events are sampled and emitted at predicted timestamps, with user interruptions triggering rejection and resampling of planned continuations. Speculative decoding is integrated to reduce average recomputation by treating interrupted candidates as drafts for new generations.

## Key Results
- The 99th percentile generation bandwidth needed is 28 tok/s for instant messenger and 36 tok/s for spoken conversations
- Document-level NLL ranges from 2179-3181 on instant messenger and 1532-2261 on spoken conversations
- Human ratings range from 1.45-5.95 for instant messenger and 0.6-5.2 for spoken conversations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model can maintain real-time interactivity by sampling events sparsely over time.
- Mechanism: Modeling events as timed diarized transcripts allows computation to scale with the number of events rather than the duration, enabling efficient generation.
- Core assumption: Events are naturally sparse over time in real conversations.
- Evidence anchors:
  - [abstract] "This method is naturally sparse over time and number of speakers, scaling computation with the amount of content being actively produced at each moment in time."
  - [section] "By modeling events sparsely over time, we are able to sample transcripts with computation proportional to the number/complexity of the events, rather than the time duration."
- Break condition: If conversation density increases significantly, computation may no longer be proportional to active content.

### Mechanism 2
- Claim: Causal rejection sampling allows the model to adapt to user interruptions.
- Mechanism: The model samples a continuation that will be finalized at a predicted timestamp, but if user input occurs before that timestamp, the planned continuation is rejected and resampled.
- Core assumption: The underlying causal structure of human training data reflects a looseness in dependencies between nearby messages from different parties.
- Evidence anchors:
  - [section] "we sample a continuation that will be finalized at the predicted timestamp, and if there is intervening user input before the timestamp, reject the planned continuation (to the extent that its probability under the model has changed) and resample a new one."
- Break condition: If user interruptions are too frequent, the model may be unable to generate any acceptable events.

### Mechanism 3
- Claim: Speculative decoding reduces average recomputation from user interruptions.
- Mechanism: Instead of discarding the candidate unconditionally upon user interruption, it is treated as a draft for the new generation, with rejection and resampling based on the closeness of probabilities.
- Core assumption: There is a looseness in the causal dependencies of nearby messages from different parties.
- Evidence anchors:
  - [section] "we reduce the average amount of recomputation by integrating speculative decoding... Rather than discard the candidate ˆei unconditionally upon user interruption, we treat it as a draft for the new generation, rejecting and resampling based on the closeness of p(ei = ˆei|e1, ...,ei−1, ti ≥ T) and p(ei+1 = ˆei|e1, ...,ei−1, ei = ( T, S, M))."
- Break condition: If speculative decoding implementation is too complex, it may not be worth the performance gains.

## Foundational Learning

- Concept: Understanding of autoregressive language models
  - Why needed here: The method relies on causally masked (decoder-only) language models to model timed diarized transcripts.
  - Quick check question: Can you explain how an autoregressive model generates text token by token?

- Concept: Familiarity with causal rejection sampling
  - Why needed here: The method uses causal rejection sampling to make the model interactive and adapt to user interruptions.
  - Quick check question: How does rejection sampling differ from regular sampling in the context of language models?

- Concept: Knowledge of speculative decoding
  - Why needed here: Speculative decoding is integrated to reduce recomputation from user interruptions.
  - Quick check question: What is the main idea behind speculative decoding, and how does it improve efficiency?

## Architecture Onboarding

- Component map: Language model -> Event sampling module -> Causal rejection sampling module -> Speculative decoding module (optional) -> Streaming input/output modules (for spoken conversations)

- Critical path:
  1. Sample next event using language model
  2. Wait until predicted timestamp
  3. Check for user interruptions
  4. If interrupted, reject and resample event
  5. If not interrupted, emit event and repeat

- Design tradeoffs:
  - treact window size vs. generation quality
  - Speculative decoding complexity vs. performance gains
  - Model size vs. real-time interactivity requirements

- Failure signatures:
  - Model unable to generate events due to frequent interruptions
  - Poor generation quality due to insufficient treact window
  - High recomputation rate if speculative decoding is not used

- First 3 experiments:
  1. Implement basic causal rejection sampling without speculative decoding on a small dataset
  2. Evaluate generation quality and real-time interactivity performance
  3. Add speculative decoding and compare recomputation rate and generation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform on real-time interactive conversations with more than two speakers or multiple simultaneous conversations?
- Basis in paper: [inferred] The paper mentions that extensions like multiple simultaneous conversations with one simulated individual (by adding conversation ids in addition to speaker ids) or modeling multimodal conversations could be possible, but it's not tested.
- Why unresolved: The current study focuses on two specific case studies: instant messenger dialogues between two authors and spoken conversations. It does not explore the method's performance in more complex scenarios involving multiple speakers or simultaneous conversations.
- What evidence would resolve it: Conducting experiments with datasets involving more than two speakers or multiple simultaneous conversations and comparing the method's performance in these scenarios to its performance in the two-speaker case studies.

### Open Question 2
- Question: How does the method's performance change when using more capable language models beyond the ones tested in the paper?
- Basis in paper: [explicit] The paper states "We find that, predictably, better pretrained models lead to better results, though there is still obvious room for improvement with dataset/model scale." It also mentions that the current models are "still not long enough to fit our training sets" and that "usage is subject to rate limits."
- Why unresolved: The study uses language models ranging from 160M to 12B parameters. It does not explore the performance of the method with even larger or more capable language models that might be available in the future.
- What evidence would resolve it: Conducting experiments with larger and more capable language models, such as those with 100B+ parameters or models specifically designed for real-time interactive conversations, and comparing their performance to the models used in the current study.

### Open Question 3
- Question: How does the method handle more complex or nuanced aspects of human conversation, such as sarcasm, humor, or emotional cues?
- Basis in paper: [inferred] The paper focuses on modeling the timing and content of conversations but does not explicitly address the handling of more complex conversational elements like sarcasm, humor, or emotional cues.
- Why unresolved: The current study evaluates the method based on document-level negative log likelihood, offline human ratings, and online human ratings. It does not specifically assess the model's ability to understand and generate more nuanced aspects of human conversation.
- What evidence would resolve it: Designing experiments that specifically test the model's ability to handle sarcasm, humor, or emotional cues in conversations, and comparing its performance to human-level understanding and generation of these elements.

## Limitations

- The fundamental assumption that events are "naturally sparse over time" may not hold for all conversation types, particularly high-density conversations like group chats or heated debates.
- The practical implementation of causal rejection sampling with speculative decoding introduces significant algorithmic complexity, with implementation details not fully specified in the paper.
- The reported generation bandwidth requirements (28 tok/s and 22 tok/s) may not generalize across different hardware configurations and may be sensitive to system parameters.

## Confidence

**High Confidence**:
- The fundamental approach of modeling timed diarized transcripts using causal language models is technically sound and well-grounded in existing autoregressive model theory.
- The general framework of using rejection sampling to handle user interruptions in interactive generation is valid and implementable.

**Medium Confidence**:
- The specific bandwidth requirements (28 tok/s and 22 tok/s) are likely accurate for the tested configurations but may vary significantly with different hardware or model architectures.
- The claim that the method "requires relatively little data" is supported by the results but lacks a comprehensive comparison with alternative approaches.

**Low Confidence**:
- The effectiveness of speculative decoding integration in reducing recomputation is claimed but not thoroughly validated with ablation studies comparing performance with and without this component.
- The scalability claims regarding computation scaling with "amount of content being actively produced" need more empirical validation across diverse conversation densities.

## Next Checks

**Validation Check 1**: Implement and test the temporal modeling approach on a high-density conversation dataset (e.g., group chat logs with rapid message exchanges) to empirically verify whether the computation truly scales with content rather than duration under varying conversation densities.

**Validation Check 2**: Conduct a comprehensive ablation study comparing the performance of causal rejection sampling with and without speculative decoding across different treact window sizes, measuring not just recomputation rates but also generation quality and latency.

**Validation Check 3**: Benchmark the generation bandwidth requirements across different hardware configurations (varying CPU/GPU specifications) and model sizes (from the smallest Pythia 160M to larger models) to establish the sensitivity of real-time interactivity requirements to system parameters.