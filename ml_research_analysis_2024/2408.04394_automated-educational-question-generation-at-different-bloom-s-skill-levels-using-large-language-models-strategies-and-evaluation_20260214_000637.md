---
ver: rpa2
title: 'Automated Educational Question Generation at Different Bloom''s Skill Levels
  using Large Language Models: Strategies and Evaluation'
arxiv_id: '2408.04394'
source_url: https://arxiv.org/abs/2408.04394
tags:
- questions
- evaluation
- skill
- llms
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores automated educational question generation\
  \ (AEQG) using large language models (LLMs) to produce diverse, high-quality questions\
  \ across Bloom\u2019s taxonomy levels. The authors compare five state-of-the-art\
  \ LLMs with varying sizes using advanced prompting strategies, including Chain-of-Thought\
  \ (CoT) techniques and few-shot examples."
---

# Automated Educational Question Generation at Different Bloom's Skill Levels using Large Language Models: Strategies and Evaluation

## Quick Facts
- arXiv ID: 2408.04394
- Source URL: https://arxiv.org/abs/2408.04394
- Reference count: 26
- Key outcome: LLMs can generate high-quality educational questions across Bloom's taxonomy when prompted with detailed strategies, with GPT-4 and GPT-3.5 outperforming smaller models

## Executive Summary
This study investigates automated educational question generation (AEQG) using five state-of-the-art LLMs across different Bloom's taxonomy levels. The authors compare various prompting strategies, from simple instructions to complex approaches incorporating Chain-of-Thought techniques and examples. Expert evaluation using a nine-item rubric shows that 78% of generated questions meet high-quality standards, with GPT-4 and GPT-3.5 achieving the best results. The research demonstrates that while LLMs can effectively generate questions at different cognitive levels, automated evaluation methods fall short compared to human expert assessment.

## Method Summary
The study generates questions using five LLMs (Mistral 7B, Llama2 70B, Palm 2, GPT-3.5, GPT-4) across 17 data science topics. Five prompt strategies with increasing complexity were tested, including Chain-of-Thought instructions, skill explanations, and examples. Questions were evaluated by two domain experts using a hierarchical nine-item rubric, with inter-rater reliability measured through percentage agreement and Cohen's kappa. Automated evaluation was conducted using Gemini Pro with the same criteria. Quality metrics included percentage of high-quality questions, Bloom's taxonomy alignment, and PINC score for diversity.

## Key Results
- GPT-4 and GPT-3.5 outperformed smaller models in both quality and Bloom's taxonomy adherence
- 78% of generated questions were rated as high-quality by expert evaluators
- 65.56% of high-quality questions matched their intended Bloom's taxonomy level
- Automated evaluation via Gemini Pro significantly underperformed compared to human experts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuned LLMs can generate high-quality educational questions aligned with Bloom's taxonomy when provided with detailed prompts.
- Mechanism: The LLMs leverage their pre-trained knowledge and follow detailed prompt instructions, including Chain-of-Thought (CoT) techniques and examples, to generate questions at different cognitive levels.
- Core assumption: LLMs possess sufficient inherent knowledge about educational topics and can interpret and follow complex instructions.
- Evidence anchors:
  - [abstract] "Our findings suggest that LLMs can generate relevant and high-quality educational questions of different cognitive levels when prompted with adequate information..."
  - [section] "In the present study, we generated questions by instructing the models with five prompt styles/strategies (PS1 to PS5), each differing in complexity..."
- Break condition: If the LLM's pre-trained knowledge is insufficient for the specific educational domain or if the prompt instructions are unclear or overly complex.

### Mechanism 2
- Claim: The size of the LLM impacts its performance in educational question generation, with larger models generally performing better.
- Mechanism: Larger models have more parameters and potentially more training data, allowing them to generate more nuanced and contextually appropriate questions.
- Core assumption: Model size correlates with the ability to understand and generate complex educational content.
- Evidence anchors:
  - [abstract] "Our findings suggest that LLMs can generate relevant and high-quality educational questions... although there is a significant variance in the performance of the five LLMs considered."
  - [section] "For quality and adherence to Bloom's taxonomy levels, GPT 4 and GPT 3.5 emerged as the top performers."
- Break condition: If the increase in model size does not translate to improved understanding of educational content or if the larger model overfits to specific question formats.

### Mechanism 3
- Claim: Automated evaluation of LLM-generated questions is not as effective as human expert evaluation.
- Mechanism: LLMs used for evaluation may not fully understand the nuances of educational quality and Bloom's taxonomy alignment, leading to discrepancies with human judgments.
- Core assumption: Human evaluators can better assess the pedagogical quality and cognitive alignment of questions compared to automated systems.
- Evidence anchors:
  - [abstract] "We also show that automated evaluation is not on par with human evaluation."
  - [section] "There is a significant discrepancy between LLM-based and expert evaluations."
- Break condition: If automated evaluation methods are improved to better align with human judgment or if human evaluation is consistently biased.

## Foundational Learning

- Concept: Bloom's Taxonomy
  - Why needed here: Understanding Bloom's taxonomy is crucial for generating questions that target different cognitive levels and for evaluating the alignment of generated questions.
  - Quick check question: What are the six levels of Bloom's taxonomy in ascending order of cognitive complexity?

- Concept: Prompt Engineering
  - Why needed here: Effective prompt design, including techniques like Chain-of-Thought and providing examples, is essential for guiding LLMs to generate high-quality educational questions.
  - Quick check question: How does the inclusion of Chain-of-Thought instructions in a prompt affect the quality of LLM-generated content?

- Concept: Inter-rater Reliability
  - Why needed here: Measuring the agreement between human evaluators is important for ensuring the consistency and objectivity of the evaluation process.
  - Quick check question: What are two common metrics used to measure inter-rater reliability in expert evaluations?

## Architecture Onboarding

- Component map:
  - LLM Selection -> Prompt Design -> Question Generation -> Expert Evaluation -> Automated Evaluation

- Critical path:
  1. Select LLMs and design prompts.
  2. Generate questions using the LLMs and prompts.
  3. Evaluate the generated questions using both expert and automated methods.
  4. Analyze the results to identify the most effective LLM-prompt combinations.

- Design tradeoffs:
  - Model size vs. performance: Larger models generally perform better but are more resource-intensive.
  - Prompt complexity vs. effectiveness: More detailed prompts can improve quality but may also confuse the model if overly complex.
  - Expert evaluation vs. automated evaluation: Expert evaluation is more accurate but time-consuming, while automated evaluation is faster but less reliable.

- Failure signatures:
  - Low-quality questions: May indicate insufficient prompt detail or model limitations.
  - Poor alignment with Bloom's taxonomy: Could suggest issues with prompt design or model understanding of cognitive levels.
  - Discrepancy between expert and automated evaluations: May point to limitations in the automated evaluation method.

- First 3 experiments:
  1. Compare the performance of different LLMs (e.g., GPT-4 vs. Mistral) using a simple prompt without additional instructions.
  2. Test the impact of adding Chain-of-Thought instructions to the prompt on question quality and Bloom's taxonomy alignment.
  3. Evaluate the effect of providing skill explanations and examples in the prompt on the generated questions' adherence to Bloom's taxonomy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal prompt strategy for balancing question quality and Bloom's taxonomy adherence across different LLM sizes?
- Basis in paper: [explicit] The paper shows that PS4 (CoT, skill, and example) gives highest skill match for Mistral, Llama 2, and Palm 2, while PS2 (CoT and skill explanation) performs better for GPT models. However, the study doesn't definitively determine if one strategy can optimize both metrics across all models.
- Why unresolved: Different models show varying responses to prompt strategies, with some models prioritizing quality while others prioritize skill alignment. The optimal balance point remains unclear.
- What evidence would resolve it: Systematic testing of hybrid prompt strategies across all five LLMs, measuring both quality and skill adherence simultaneously, would identify the optimal approach.

### Open Question 2
- Question: Can fine-tuning LLMs on educational evaluation datasets improve automated question assessment accuracy?
- Basis in paper: [explicit] The study found that Gemini Pro evaluation significantly underperformed compared to human experts, with discrepancies attributed to lack of training on educational evaluation datasets.
- Why unresolved: The paper identifies the need for training but doesn't explore or implement such fine-tuning approaches.
- What evidence would resolve it: Comparative studies of pre-trained versus fine-tuned LLM evaluators on the same question set, measuring agreement with human experts.

### Open Question 3
- Question: How does incorporating domain-specific context information affect the quality and Bloom's taxonomy alignment of generated questions?
- Basis in paper: [inferred] The study used LLMs' inherent knowledge without providing domain-specific context, revealing limitations in understanding specific topics like "prompt engineering." This suggests context incorporation could improve performance.
- Why unresolved: The paper deliberately avoided context information to test inherent LLM knowledge, leaving the impact of context incorporation unexplored.
- What evidence would resolve it: Controlled experiments comparing question generation with and without domain-specific context across various subjects, measuring both quality and taxonomy alignment.

## Limitations
- Model selection and generalization may not extend to other LLM architectures or educational domains beyond data science
- Evaluation method reliability concerns due to subjective nature of pedagogical quality assessment
- Prompt complexity trade-offs weren't systematically isolated to determine optimal thresholds

## Confidence
- High Confidence: GPT-3.5 and GPT-4 outperforming smaller models is well-supported by experimental results and aligns with established literature
- Medium Confidence: Automated evaluation is not on par with human evaluation, though the reasons require further investigation
- Low Confidence: Prompts enriched with skill explanations and examples consistently yield better outcomes needs more systematic testing

## Next Checks
1. Test best-performing LLM-prompt combinations on educational topics outside data science to assess generalizability
2. Conduct ablation studies to systematically evaluate individual contribution of each prompt element (CoT, examples, skill explanations)
3. Develop and test improved automated evaluation prompts for Gemini Pro or similar models incorporating expert feedback