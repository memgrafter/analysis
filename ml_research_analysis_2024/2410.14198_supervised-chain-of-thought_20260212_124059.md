---
ver: rpa2
title: Supervised Chain of Thought
arxiv_id: '2410.14198'
source_url: https://arxiv.org/abs/2410.14198
tags:
- step
- reasoning
- each
- space
- template
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the limitations of Chain of Thought (CoT)
  prompting in Large Language Models (LLMs) by identifying that the "one-prompt-for-all"
  approach can lead to suboptimal performance. The authors partition the solution
  process into two spaces: the prompt space and the answer space, arguing that task-specific
  supervision is essential for navigating the prompt space accurately.'
---

# Supervised Chain of Thought

## Quick Facts
- arXiv ID: 2410.14198
- Source URL: https://arxiv.org/abs/2410.14198
- Reference count: 25
- This paper demonstrates that task-specific supervision is essential for optimal Chain of Thought reasoning performance in LLMs.

## Executive Summary
This paper addresses the limitations of Chain of Thought (CoT) prompting in Large Language Models (LLMs) by identifying that the "one-prompt-for-all" approach can lead to suboptimal performance. The authors partition the solution process into two spaces: the prompt space and the answer space, arguing that task-specific supervision is essential for navigating the prompt space accurately. Through experiments with state-of-the-art LLMs, they demonstrate a significant performance gap when supervision is applied versus when it is not, highlighting the need for task-specific "supervised" CoT to achieve optimal reasoning performance.

## Method Summary
The method involves using GPT-4-o to perform tasks with Chain of Thought prompting, comparing unsupervised CoT, CoT with correct supervision (CR Supervised), and CoT with incorrect supervision (IN Supervised). The experiments use three categories of tasks: Regular (R), Context-Free (CF), and Context-Sensitive (CS), with 50 instances per task generated using a pre-written script. The approach tests how different forms of supervision affect reasoning accuracy and demonstrates that task-specific guidance significantly improves performance over unsupervised approaches.

## Key Results
- Task-specific supervision yields noticeable improvements over unsupervised "step-by-step" approaches
- Errors caused by model-derived step templates are eliminated with correct supervision
- Both Tree-of-Thought and Graph-of-Thought variants improve performance over naive CoT

## Why This Works (Mechanism)

### Mechanism 1
Chain of Thought (CoT) enables autoregressive models to achieve recurrence by transforming intermediate reasoning steps into text, which can be reused in subsequent computations. CoT generates intermediate reasoning steps as natural language tokens (o1, o2, ..., ok) that act as a discretization of the hidden state h. These tokens are converted back into vectors via the embedding layer, preserving computational information through discretization and vectorization: ht → (o1, o2, ..., ok) → ht+1. Natural language is a sufficiently powerful medium for encoding nearly any type of information needed for reasoning.

### Mechanism 2
The performance gap between supervised and unsupervised CoT stems from the importance of correctly navigating the prompt space to extract relevant information at each step. The prompt space contains different templates for extracting information from the hidden state h. Correct supervision guides the model to use the optimal template, while unsupervised CoT leaves template selection to the model's heuristics, which can be unreliable. The model's internal heuristics for generating step templates are often unreliable and can lead to sub-optimal performance.

### Mechanism 3
Even with correct step templates, the answer space complexity can significantly impact performance, and variants like Tree-of-Thought and Graph-of-Thought help navigate this space more effectively. Once the correct step template is identified, the model must still search the answer space for the correct solution. ToT and GoT improve this search by exploring multiple paths simultaneously or revisiting previously generated steps. The answer space can be large and complex, making it difficult to find the correct solution even with the right step template.

## Foundational Learning

- Concept: Computational complexity theory (Regular, Context-Free, Context-Sensitive languages)
  - Why needed here: The paper uses tasks from different complexity classes to demonstrate the limitations of transformer architectures and the benefits of CoT. Understanding these classes helps explain why certain tasks require recurrence and why transformers struggle with them.
  - Quick check question: Why can't transformers solve Context-Sensitive tasks that require linearly increasing depth with input length?

- Concept: Transformer architecture and its limitations
  - Why needed here: The paper argues that transformers have inherent limitations in computational depth due to their attention mechanism, which is the fundamental problem that CoT aims to solve.
  - Quick check question: What is the key architectural difference between transformers and recurrent networks that limits transformers' reasoning depth?

- Concept: Autoregressive vs. Recurrent models
  - Why needed here: The paper explains how CoT effectively transforms autoregressive models into recurrent ones by generating intermediate steps, which is crucial for understanding the mechanism behind CoT's success.
  - Quick check question: How does CoT enable autoregressive models to achieve the same effect as recurrence in RNNs?

## Architecture Onboarding

- Component map: Input prompt -> Transformer layers (constant depth) -> CoT mechanism (generates intermediate steps) -> Prompt space (templates for extracting information) -> Answer space (possible solutions) -> Final answer

- Critical path: 1. Input prompt is given to LLM 2. If using CoT, model generates intermediate reasoning steps 3. Model extracts information from hidden state based on step template 4. Information is converted to text and back to vectors via embedding layer 5. Model continues computation using the extracted information 6. Final answer is generated

- Design tradeoffs: Longer CoT steps extract more information but increase computation cost; more complex step templates may capture more information but be harder to generate correctly; supervision improves performance but requires human effort to provide correct guidance; CoT variants like ToT and GoT improve answer space navigation but increase computational complexity

- Failure signatures: Poor performance on tasks requiring linearly increasing depth with input length; inconsistent results when using unsupervised CoT; performance degradation when using incorrect step templates; inability to solve tasks that require complex reasoning despite using CoT

- First 3 experiments: 1. Test LLM performance on Modular Arithmetic task with and without CoT supervision 2. Compare performance of different CoT variants (ToT, GoT) on Stack Manipulation task 3. Evaluate the impact of incorrect step templates on Parity Check task performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance gap between supervised and unsupervised CoT vary across different model sizes and architectures? The paper discusses the importance of task-specific supervision for optimal performance but does not extensively explore how this varies across different model sizes or architectures. Experimental results showing performance metrics for supervised and unsupervised CoT across a range of model sizes and architectures would resolve this question.

### Open Question 2
What are the specific challenges in identifying the correct step template for more complex tasks, and how can these challenges be mitigated? While the paper identifies the need for supervision, it does not delve into the specific challenges or propose detailed methods for mitigating these challenges in complex tasks. Detailed case studies or experiments that outline the challenges faced in identifying step templates for complex tasks and propose or test methods to address these challenges would resolve this question.

### Open Question 3
How do CoT variants like Tree-of-Thought and Graph-of-Thought perform in tasks that require high computational depth, and what are their limitations? The paper suggests that these variants have limitations in navigating the prompt space but does not provide a comprehensive analysis of their performance in high computational depth tasks. Comparative experiments that evaluate the performance of CoT variants in tasks requiring high computational depth, along with an analysis of their limitations and potential improvements, would resolve this question.

## Limitations

- Limited empirical validation with heavy reliance on synthetic tasks rather than real-world reasoning scenarios
- All experiments use GPT-4-o, raising questions about generalizability to other LLM architectures or smaller models
- Focus on three categories of tasks without exploring more complex real-world reasoning tasks involving multiple modalities

## Confidence

**High Confidence Claims:**
- The distinction between prompt space and answer space provides a useful conceptual framework for understanding CoT limitations
- Transformer architectures have inherent limitations in computational depth that CoT attempts to address
- The vectorization-discretization process through natural language is a plausible mechanism for recurrence

**Medium Confidence Claims:**
- Task-specific supervision significantly improves CoT performance compared to unsupervised approaches
- The performance gap between supervised and unsupervised CoT is primarily due to prompt space navigation
- Natural language is sufficiently expressive to encode intermediate reasoning steps

**Low Confidence Claims:**
- The exact conditions under which incorrect supervision leads to catastrophic failure
- The generalizability of findings to non-synthetic, real-world reasoning tasks
- The comparative effectiveness of different CoT variants (ToT vs GoT) across diverse task types

## Next Checks

1. Replicate the experiments using at least three different LLM architectures (e.g., GPT-4, Claude, Llama) to determine whether the supervised CoT advantage persists across model families and sizes.

2. Design and test the supervised CoT approach on complex, multi-step reasoning tasks drawn from real applications (e.g., mathematical problem-solving, scientific reasoning, or multi-hop question answering) to assess practical utility.

3. Systematically vary the quality of supervision from completely correct to completely incorrect in controlled increments to map out the relationship between supervision accuracy and performance gains, identifying critical thresholds.