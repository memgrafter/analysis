---
ver: rpa2
title: Transferring self-supervised pre-trained models for SHM data anomaly detection
  with scarce labeled data
arxiv_id: '2412.03880'
source_url: https://arxiv.org/abs/2412.03880
tags:
- data
- learning
- labeled
- detection
- anomaly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates self-supervised learning (SSL) for structural
  health monitoring (SHM) data anomaly detection with limited labeled data. The proposed
  SSL framework pre-trains models on large unlabeled SHM datasets using pretext tasks
  like reconstruction and contrastive learning, then fine-tunes them with minimal
  labeled data for anomaly classification.
---

# Transferring self-supervised pre-trained models for SHM data anomaly detection with scarce labeled data

## Quick Facts
- **arXiv ID:** 2412.03880
- **Source URL:** https://arxiv.org/abs/2412.03880
- **Reference count:** 40
- **One-line primary result:** SSL pre-training with autoencoders achieves 93.5% and 98.7% accuracy on SHM anomaly detection with minimal labeled data

## Executive Summary
This study investigates self-supervised learning (SSL) for structural health monitoring (SHM) data anomaly detection with limited labeled data. The proposed SSL framework pre-trains models on large unlabeled SHM datasets using pretext tasks like reconstruction and contrastive learning, then fine-tunes them with minimal labeled data for anomaly classification. Experiments on SHM data from two in-service bridges show that SSL significantly improves anomaly detection performance compared to purely supervised training.

The framework is particularly effective for identifying normal data and major abnormal patterns, though minority pattern detection remains challenging due to data imbalance. Among evaluated SSL methods, autoencoders (AE) achieve the highest F1 scores (80.52% and 89.62% on test datasets) and overall accuracy (93.5% and 98.7%). AE pre-training demonstrates consistent improvements across different data scenarios, making it a practical solution for SHM data cleansing with limited labeling resources.

## Method Summary
The SSL framework consists of three main stages: data reduction using IERFH feature extraction, self-supervised pre-training on unlabeled data using autoencoders or contrastive methods, and supervised fine-tuning on small labeled datasets. SHM time series data is converted to 512-dimensional IERFH features, then autoencoders learn data representations through reconstruction tasks. The pre-trained encoders are fine-tuned with cross-entropy loss on labeled data for multi-class anomaly classification. The method addresses the challenge of limited labeled data in SHM by leveraging large amounts of unlabeled sensor data.

## Key Results
- SSL pre-training improves F1 scores to 80.52% and 89.62% on test datasets compared to supervised training
- Autoencoders outperform contrastive methods (SimCLR, Mixup) and GAN-based approaches for SHM anomaly detection
- Framework achieves overall accuracy of 93.5% and 98.7% while requiring minimal labeled data
- SSL methods show significant improvements in identifying normal data and major abnormal patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-supervised pre-training enables effective anomaly detection with very limited labeled data by leveraging unlabeled SHM data for feature learning.
- **Mechanism:** SSL methods use pretext tasks (reconstruction, contrastive learning) to train encoders on large unlabeled datasets, learning general data representations. These pre-trained encoders are then fine-tuned on small labeled datasets for anomaly classification, achieving performance comparable to fully supervised models.
- **Core assumption:** The unlabeled SHM data contains sufficient diversity and patterns to learn meaningful representations that transfer to anomaly detection tasks.
- **Evidence anchors:**
  - [abstract] "SSL-based framework pre-trains models on large unlabeled SHM datasets using pretext tasks like reconstruction and contrastive learning, then fine-tunes them with minimal labeled data for anomaly classification."
  - [section 2.3] "The key idea of SSL is to design pretext tasks that leverage data itself or its augmentation as label information. Typical pretext tasks include reconstruction and comparison, which allow models to learn useful representations for downstream tasks."
  - [corpus] No direct evidence found for SSL in SHM anomaly detection, but related papers show SSL effectiveness in intrusion detection and molecular graphs.

### Mechanism 2
- **Claim:** Autoencoders outperform other SSL methods for SHM anomaly detection due to their ability to learn effective representations despite imbalanced data distributions.
- **Mechanism:** AE pre-training learns to reconstruct input data, forcing the encoder to capture essential data patterns. This generative approach is less sensitive to imbalanced data compared to contrastive methods, making it more effective for minority pattern detection.
- **Core assumption:** Generative SSL methods are more robust to class imbalance than contrastive approaches in the context of SHM data.
- **Evidence anchors:**
  - [abstract] "Among evaluated SSL methods, autoencoders (AE) achieve the highest F1 scores (80.52% and 89.62% on test datasets) and overall accuracy (93.5% and 98.7%)."
  - [section 3.4.4] "AE emerges as the most effective and consistent pre-training method for anomaly detection. This is likely due to the highly imbalanced data distribution during the pre-training stage, where the imbalance ratio between the majority and minority classes exceeds 10:1."
  - [corpus] No direct evidence for AE superiority in SHM, but related SSL papers show generative methods' effectiveness in various domains.

### Mechanism 3
- **Claim:** Fine-tuning pre-trained SSL models on minimal labeled data is highly efficient, achieving optimal performance within few epochs.
- **Mechanism:** Pre-trained encoders already possess general data representations from self-supervised learning. Fine-tuning adapts these representations to the specific classification task using cross-entropy loss on labeled data, requiring minimal additional training.
- **Core assumption:** Pre-trained representations are sufficiently general and task-relevant to require only small adjustments during fine-tuning.
- **Evidence anchors:**
  - [abstract] "The SSL-based framework aims to learn from only a very small quantity of labeled data by fine-tuning, while making the best use of the vast amount of unlabeled SHM data by pre-training."
  - [section 3.4.3] "In both Case 1 and Case 2, the F1 score and accuracy increase rapidly, reaching optimal performance within approximately 10 epochs."
  - [corpus] No direct evidence for SSL fine-tuning efficiency in SHM, but related papers show few-shot learning effectiveness in various domains.

## Foundational Learning

- **Concept: Self-Supervised Learning**
  - **Why needed here:** Traditional supervised learning requires large labeled datasets, which are expensive and time-consuming to obtain for SHM applications. SSL enables learning from unlabeled data, addressing this limitation.
  - **Quick check question:** What are the main differences between supervised, unsupervised, and self-supervised learning approaches?

- **Concept: Imbalanced Data Handling**
  - **Why needed here:** SHM data often contains predominantly normal patterns with few abnormal instances, making it challenging for standard learning approaches. Understanding how different SSL methods handle imbalance is crucial for method selection.
  - **Quick check question:** How does class imbalance affect the performance of generative versus contrastive SSL methods?

- **Concept: Transfer Learning and Fine-tuning**
  - **Why needed here:** The success of SSL for SHM anomaly detection depends on effectively transferring knowledge from pre-training to the fine-tuning stage. Understanding the principles of transfer learning ensures proper implementation.
  - **Quick check question:** What factors influence the effectiveness of transfer learning from pre-training to fine-tuning stages?

## Architecture Onboarding

- **Component map:** Data reduction (IERFH feature extraction) -> Pre-training (SSL methods) -> Fine-tuning (Classification) -> Evaluation
- **Critical path:** The encoder is the central component that carries learned representations through all stages from pre-training to fine-tuning
- **Design tradeoffs:** Generative SSL (AE) provides robustness to imbalance but may learn less discriminative features than contrastive methods. Contrastive SSL (SimCLR, Mixup) can learn more discriminative features but is more sensitive to data imbalance. GAN combines both approaches but is more complex.
- **Failure signatures:** Poor pre-training loss convergence indicates issues with pretext task design or data quality. Low fine-tuning performance despite good pre-training suggests domain mismatch or insufficient labeled data. Negative transfer (worse performance than supervised) indicates pre-training may be counterproductive.
- **First 3 experiments:**
  1. Implement data reduction and verify IERFH features capture meaningful patterns by visualizing histograms for different data types.
  2. Train AE on unlabeled data and evaluate reconstruction quality to ensure pre-training is learning useful representations.
  3. Fine-tune pre-trained AE on small labeled dataset and compare performance against supervised baseline to validate transfer learning effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can self-supervised learning methods be improved to better detect minority abnormal patterns in structural health monitoring data?
- **Basis in paper:** [explicit] The paper identifies that current methods show significant performance gaps for minority abnormal patterns like 'outlier' and 'drift', with precision and recall falling below 90% for these less frequent data patterns.
- **Why unresolved:** The paper acknowledges this as a limitation stemming from the imbalanced (long-tailed) distribution of real-world SHM data, where majority patterns dominate training while rare types could be neglected.
- **What evidence would resolve it:** Comparative studies testing various techniques specifically designed for imbalanced data (such as focal loss, class-balanced sampling, or specialized contrastive learning approaches) applied to minority pattern detection in SHM datasets.

### Open Question 2
- **Question:** What is the optimal configuration for labeled datasets during the fine-tuning stage of self-supervised learning for SHM anomaly detection?
- **Basis in paper:** [explicit] The paper notes that model performance can be readily influenced by the data distribution in labeled datasets during fine-tuning, and states that "designing the optimal configuration for the labeled dataset remains an open problem."
- **Why unresolved:** The paper experiments with various balanced and unbalanced labeled datasets but doesn't determine which configuration yields optimal results across different scenarios.
- **What evidence would resolve it:** Systematic experiments varying labeled dataset composition (class ratios, sample sizes) to identify configurations that maximize detection performance for different abnormal pattern distributions.

### Open Question 3
- **Question:** Why do generative self-supervised learning methods (particularly autoencoders) outperform contrastive methods in structural health monitoring data anomaly detection?
- **Basis in paper:** [explicit] The paper observes that while contrastive SSL methods (SimCLR and Mixup) have shown state-of-the-art performance in other domains like images and text, they generally underperform generative methods (AE) in SHM data, with some cases even performing worse than purely supervised learning.
- **Why unresolved:** The paper suggests this is likely due to the highly imbalanced data distribution during pre-training, but doesn't provide definitive evidence or explore alternative explanations.
- **What evidence would resolve it:** Controlled experiments comparing generative and contrastive methods on balanced vs. imbalanced SHM datasets, along with ablation studies to identify which specific aspects of contrastive learning fail in this context.

## Limitations

- Current SSL methods show significant performance gaps for minority abnormal patterns due to data imbalance in SHM datasets
- The framework's effectiveness for minority pattern detection remains uncertain despite overall performance improvements
- Generalization across different bridge types and sensor configurations requires broader validation beyond the two tested bridges

## Confidence

- **High Confidence:** The SSL framework's ability to improve anomaly detection with limited labeled data is well-supported by experimental results showing consistent performance improvements across multiple scenarios and SSL methods.
- **Medium Confidence:** The superiority of autoencoders over other SSL methods is demonstrated but requires further validation across diverse SHM applications and data distributions to establish robustness.
- **Low Confidence:** The framework's effectiveness for minority pattern detection remains uncertain, as current results show limitations in identifying rare anomalies despite overall performance improvements.

## Next Checks

1. Conduct ablation studies comparing SSL methods against domain-specific SHM anomaly detection approaches to establish relative effectiveness.
2. Test framework generalizability by applying the pre-trained models to different bridge types and sensor configurations not included in the original study.
3. Evaluate minority pattern detection performance using synthetic data augmentation techniques to address class imbalance limitations.