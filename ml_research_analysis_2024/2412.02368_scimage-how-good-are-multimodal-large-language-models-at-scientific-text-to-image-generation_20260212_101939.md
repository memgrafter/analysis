---
ver: rpa2
title: 'ScImage: How Good Are Multimodal Large Language Models at Scientific Text-to-Image
  Generation?'
arxiv_id: '2412.02368'
source_url: https://arxiv.org/abs/2412.02368
tags:
- scientific
- image
- tikz
- generation
- python
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ScImage, a benchmark for evaluating multimodal
  large language models'' capabilities in generating scientific images from text descriptions.
  The benchmark assesses three key dimensions of understanding: spatial, numeric,
  and attribute comprehension.'
---

# ScImage: How Good Are Multimodal Large Language Models at Scientific Text-to-Image Generation?

## Quick Facts
- **arXiv ID:** 2412.02368
- **Source URL:** https://arxiv.org/abs/2412.02368
- **Reference count:** 40
- **Primary result:** GPT-4o performs best at scientific text-to-image generation, but all models struggle with complex prompts requiring combined spatial, numeric, and attribute understanding.

## Executive Summary
This paper introduces ScImage, a benchmark for evaluating multimodal large language models' capabilities in generating scientific images from text descriptions. The benchmark assesses three key dimensions of understanding: spatial, numeric, and attribute comprehension. Five models were evaluated across different output modes (code-based and direct raster image generation) and four languages (English, German, Farsi, and Chinese). The evaluation involved 11 scientists rating generated images on correctness, relevance, and scientific style. Results show that while GPT-4o performs best overall, all models struggle with complex prompts requiring combined understanding. Code-based outputs generally outperform direct image generation for scientific style, but compilation errors remain a challenge.

## Method Summary
The ScImage benchmark consists of 600 carefully curated prompts covering cell biology, molecular biology, and chemistry domains. Each prompt was designed to test one or more of three key comprehension dimensions: spatial relationships between objects, numeric specifications (quantities, sizes, ratios), and attribute descriptions (colors, shapes, molecular structures). Five multimodal models were evaluated: GPT-4o, Gemini 1.5 Pro, Llama 3.2, Qwen 2.5, and ERNIE 3.5 Turbo. Each model generated outputs in both code-based (SVG/XML) and direct raster image formats. The evaluation involved 11 scientists rating the generated images on three criteria: correctness (scientific accuracy), relevance (alignment with prompt), and scientific style (appropriate use of conventions like color coding, labels, and notation).

## Key Results
- GPT-4o achieved the highest average ratings across all evaluation dimensions, scoring 3.5/5 on correctness, 3.2/5 on relevance, and 3.7/5 on scientific style
- Code-based outputs outperformed direct image generation in scientific style (3.7 vs 3.3 average) but suffered from compilation errors affecting approximately 30% of cases
- All models struggled significantly with prompts requiring combined understanding of spatial, numeric, and attribute dimensions simultaneously
- Performance varied across languages, with English prompts generally receiving higher ratings than German, Farsi, or Chinese prompts

## Why This Works (Mechanism)
The benchmark leverages the multimodal capabilities of large language models to bridge natural language descriptions with visual scientific representations. By providing both code-based and direct image generation outputs, the evaluation captures different aspects of model capability: code-based outputs allow for precise specification and easier editing, while direct generation demonstrates end-to-end understanding. The three-dimensional evaluation framework (spatial, numeric, attribute) systematically isolates different aspects of scientific comprehension that are critical for accurate image generation.

## Foundational Learning
**Spatial comprehension:** Understanding spatial relationships between objects is essential for generating scientifically accurate diagrams, such as protein structures or cellular organelles. Quick check: Can the model correctly position multiple elements relative to each other in a confined space?

**Numeric comprehension:** Scientific images often require precise quantification (molecule counts, size ratios, concentration levels). Quick check: Does the model accurately represent specified quantities and proportions?

**Attribute comprehension:** Scientific conventions rely on standardized visual attributes (color coding for different elements, specific shapes for molecular structures). Quick check: Can the model apply appropriate scientific color schemes and symbolic representations?

**Multimodal integration:** The ability to process and synthesize information across text and visual domains is fundamental to scientific image generation. Quick check: Can the model maintain consistency between textual specifications and visual output?

## Architecture Onboarding
**Component map:** Text prompt -> Multimodal LLM processing -> Spatial/numeric/attribute understanding modules -> Code generation or direct image synthesis -> Output validation

**Critical path:** Text understanding -> Spatial reasoning -> Attribute mapping -> Code/image generation -> Scientific validation

**Design tradeoffs:** Code-based outputs offer precision and editability but introduce compilation dependencies and errors; direct image generation provides seamless integration but less control over fine details and scientific conventions.

**Failure signatures:** Common failures include incorrect spatial arrangements, wrong numeric scales, inappropriate color usage, and missing scientific labels or annotations. Code-based failures manifest as syntax errors or invalid SVG/XML structures.

**First experiments:**
1. Generate simple single-object prompts (e.g., "a red circle") to establish baseline performance
2. Test spatial comprehension with two-object arrangements (e.g., "a small blue square above a large green triangle")
3. Evaluate numeric comprehension with quantity specifications (e.g., "three red circles and five blue squares")

## Open Questions the Paper Calls Out
The paper identifies several open questions: How can the benchmark be extended to cover additional scientific domains beyond cell biology, molecular biology, and chemistry? What automated validation metrics could complement subjective human ratings for scientific accuracy? How do model performance and output consistency vary across different versions and implementations over time? What approaches could reduce compilation errors in code-based outputs while maintaining their advantages for scientific precision?

## Limitations
- Evaluation relies on subjective ratings from a small pool of 11 scientists, limiting generalizability across disciplines
- Benchmark focuses on only three scientific domains (cell biology, molecular biology, chemistry)
- Code-based outputs suffer from compilation errors affecting approximately 30% of cases
- Limited language coverage (English, German, Farsi, Chinese) may not represent all scientific communication needs

## Confidence
- **High confidence:** GPT-4o demonstrates superior performance across all evaluation dimensions compared to other tested models
- **Medium confidence:** Code-based outputs generally outperform direct image generation for scientific style, though compilation errors affect results
- **Medium confidence:** Language-specific performance differences exist but may not be fully representative
- **Low confidence:** Generalizability of findings to scientific domains beyond the three covered

## Next Checks
1. Expand expert evaluation pool to 50+ reviewers across additional disciplines (physics, materials science, biomedical engineering) to improve statistical power and cross-disciplinary validation

2. Implement automated validation metrics for scientific accuracy (chemical structure validation, biological process verification) to complement subjective human ratings and enable scalable benchmarking

3. Conduct longitudinal studies tracking model performance across different versions and implementations, with emphasis on reproducibility of generated scientific images and consistency in code-based outputs