---
ver: rpa2
title: Multi-head Span-based Detector for AI-generated Fragments in Scientific Papers
arxiv_id: '2411.07343'
source_url: https://arxiv.org/abs/2411.07343
tags:
- token
- text
- task
- classification
- scientific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting AI-generated text
  fragments in scientific documents, specifically within the DAGPap24 competition.
  The authors propose a multi-task learning architecture with two classification heads
  to distinguish between human-written and machine-generated text at the token level.
---

# Multi-head Span-based Detector for AI-generated Fragments in Scientific Papers

## Quick Facts
- arXiv ID: 2411.07343
- Source URL: https://arxiv.org/abs/2411.07343
- Authors: German Gritsai; Ildar Khabutdinov; Andrey Grabovoy
- Reference count: 4
- One-line primary result: Achieved 0.96 macro F1-score on DAGPap24 competition, 9% improvement over baseline

## Executive Summary
This paper addresses the challenge of detecting AI-generated text fragments in scientific documents through a multi-task learning architecture. The authors propose a dual-head approach using a BERT-like encoder (SciBERT) to perform both binary classification (human vs machine-generated) and multi-class classification (human, ChatGPT, NLTK-synonym, summarized) at the token level. The model employs a threshold-based inference strategy to improve efficiency and achieved 0.96 F1-score on the DAGPap24 competition's closed test set, placing 5th in the competition.

## Method Summary
The approach uses a multi-task learning architecture with two parallel linear layers on top of a shared SciBERT encoder. The first head performs binary classification (human vs machine-generated), while the second performs multi-class classification across four categories. During inference, the model first applies binary classification with a 0.55 threshold - if exceeded, it proceeds to multi-class classification; otherwise, it assigns all tokens as human. Texts are partitioned into 350 word tokens to optimize information retention after SciBERT tokenization. The model was trained using a combination of both classification losses and achieved 0.96 F1-score on the development set.

## Key Results
- Achieved 0.96 macro F1-score on DAGPap24 competition development set
- 9% improvement over baseline solution (0.86 to 0.95)
- Scored 0.96 on closed test set, placing 5th in competition
- Optimal configuration: SciBERT encoder with 350 token-word partitions

## Why This Works (Mechanism)

### Mechanism 1: Multi-task learning with shared representations
The dual-head architecture learns shared representations beneficial for both binary and multi-class token classification, reducing overfitting and improving generalization. The shared encoder forces the model to learn representations that distinguish human from machine-generated text while also capturing subtle differences between AI generation methods.

### Mechanism 2: Threshold-based inference filtering
The 0.55 threshold on binary classification probabilities filters out likely human-written sequences before expensive multi-class classification. This strategy improves precision by only performing detailed classification on sequences with high confidence of being machine-generated.

### Mechanism 3: Optimized partition size with SciBERT
Using 350 word token partitions with SciBERT tokenization balances context window constraints with information retention. This configuration minimizes information loss from tokenization while fitting within SciBERT's context window, achieving optimal performance.

## Foundational Learning

- **Token classification vs sequence classification**: Needed because the task requires identifying specific token-level spans that are AI-generated within longer texts. Quick check: What's the difference between token classification and sequence classification, and why is token classification more appropriate for detecting AI-generated fragments within scientific papers?

- **Multi-task learning benefits and tradeoffs**: Essential for understanding how sharing representations between binary and multi-class classification affects learning. Quick check: How does multi-task learning with shared encoder layers affect overfitting and generalization compared to training separate models?

- **Threshold-based inference strategies**: Critical for understanding the efficiency-performance tradeoff in the model's decision-making process. Quick check: Why would setting a threshold on binary classification probabilities before multi-class classification improve model performance?

## Architecture Onboarding

- **Component map**: Input → SciBERT encoder → Dropout (0.7) → Linear A (binary classification) → Linear B (multi-class classification) → Loss aggregation → Output
- **Critical path**: Token sequence → tokenization → SciBERT encoding → parallel linear layers → classification heads → threshold-based inference
- **Design tradeoffs**: Two-head architecture trades increased model complexity for improved generalization; threshold strategy trades potential false negatives for computational efficiency
- **Failure signatures**: Low precision on AI-generated fragments (threshold too high), low recall on human-written fragments (threshold too low), poor multi-class discrimination (linear layer capacity insufficient)
- **First 3 experiments**:
  1. Vary the threshold value from 0.4 to 0.7 in 0.05 increments to find optimal precision-recall balance
  2. Test different partition sizes (185, 300, 350, 400, 512 word tokens) to optimize context retention
  3. Compare dropout rates of 0.5, 0.7, and 0.9 to find optimal regularization level

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the multi-task learning approach perform when detecting AI-generated text from newer, more advanced language models not included in the training data?
- **Basis in paper**: The model was trained on specific AI-generated text types (ChatGPT, NLTK synonym-replaced, summarized) but not evaluated on newer models.
- **Why unresolved**: Training data only includes specific generation methods; no testing on more recent or diverse generation models.
- **What evidence would resolve it**: Testing on datasets containing fragments generated by newer models like GPT-4, Claude, or other contemporary models.

### Open Question 2
- **Question**: What is the optimal input sequence length for balancing information retention and computational efficiency across different scientific domains?
- **Basis in paper**: 350 token-words yielded best results for their specific dataset but may vary across domains.
- **Why unresolved**: Optimal sequence length may depend on domain-specific characteristics like average sentence length and technical vocabulary density.
- **What evidence would resolve it**: Systematic experiments varying input sequence lengths across multiple scientific domains while measuring both performance and computational efficiency.

### Open Question 3
- **Question**: How does the model's performance change when applied to non-scientific texts or mixed-domain documents?
- **Basis in paper**: Model was specifically designed and trained for scientific texts using SciBERT, but performance on other text types is unexamined.
- **Why unresolved**: Domain-specific tokeniser and encoder were chosen based on competition requirements with no analysis of generalization to other domains.
- **What evidence would resolve it**: Evaluating the model on datasets from different domains (news articles, fiction, social media posts) and comparing detection accuracy across these domains.

### Open Question 4
- **Question**: What is the impact of different tokenisation strategies on detection accuracy, particularly for scientific terminology and compound words?
- **Basis in paper**: Paper mentions varying tokenisation approaches but doesn't systematically compare different tokenisers or analyse scientific terminology handling.
- **Why unresolved**: Scientific texts contain domain-specific terminology that may be tokenised differently by various tokenisers, potentially affecting detection accuracy.
- **What evidence would resolve it**: Comparative experiments using different tokenisers (WordPiece, BPE, SentencePiece) on the same scientific texts, measuring how different tokenisation strategies affect detection of AI-generated fragments.

## Limitations

- **Competition-specific performance**: Results achieved within specific DAGPap24 competition context may not generalize to other datasets or real-world scenarios
- **Threshold sensitivity**: Critical hyperparameter (0.55 threshold) was determined empirically with limited sensitivity analysis
- **Limited ablation study**: Lacks comprehensive analysis to isolate which components contribute most to performance gains

## Confidence

- **High confidence**: Technical implementation details and basic performance metrics are well-documented and reproducible
- **Medium confidence**: Multi-task learning benefits are supported by general ML literature but lack direct task-specific evidence
- **Low confidence**: Generalizability of threshold-based inference strategy to other datasets or real-world applications

## Next Checks

1. **Threshold sensitivity analysis**: Systematically vary the binary classification threshold from 0.4 to 0.7 in 0.05 increments and measure the impact on both precision and recall for each class

2. **Cross-dataset validation**: Test the trained model on datasets from different sources (e.g., arXiv papers, conference proceedings, or synthetic AI-generated text from different models) to evaluate generalization beyond DAGPap24 competition data

3. **Ablation study on architecture components**: Train and evaluate variants of the model with: (a) single classification head instead of two, (b) different partition sizes (185, 300, 400, 512 word tokens), and (c) different dropout rates (0.5, 0.7, 0.9) to isolate which architectural choices contribute most to observed performance improvements