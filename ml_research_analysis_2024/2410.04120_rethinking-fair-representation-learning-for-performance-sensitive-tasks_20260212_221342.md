---
ver: rpa2
title: Rethinking Fair Representation Learning for Performance-Sensitive Tasks
arxiv_id: '2410.04120'
source_url: https://arxiv.org/abs/2410.04120
tags:
- cited
- fairness
- bias
- learning
- fair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper critically examines the effectiveness of fair representation\
  \ learning (FRL) methods for bias mitigation in performance-sensitive tasks. Using\
  \ causal reasoning, the authors formalize three sources of dataset bias\u2014presentation,\
  \ prevalence, and annotation disparities\u2014and reveal implicit assumptions in\
  \ FRL methods."
---

# Rethinking Fair Representation Learning for Performance-Sensitive Tasks

## Quick Facts
- **arXiv ID**: 2410.04120
- **Source URL**: https://arxiv.org/abs/2410.04120
- **Reference count**: 27
- **Primary result**: FRL methods only improve performance under presentation disparities with high subgroup separability, while consistently degrading performance in other settings.

## Executive Summary
This paper critically examines fair representation learning (FRL) methods for bias mitigation in performance-sensitive tasks. Using causal reasoning, the authors formalize three sources of dataset bias—presentation, prevalence, and annotation disparities—and reveal implicit assumptions in FRL methods. They prove that FRL cannot be both effective and harmless when training and test data are identically distributed, explaining why FRL underperforms empirical risk minimization (ERM) in IID settings. The authors propose that FRL validity under distribution shifts depends on the causal structure of the bias and subgroup separability. Extensive experiments across five medical imaging datasets confirm these hypotheses, showing FRL only improves performance under presentation disparities with high subgroup separability.

## Method Summary
The authors evaluate two FRL methods (adversarial and class-conditional) against ERM baseline on five medical imaging datasets with artificially injected biases. They implement three bias mechanisms: presentation disparities (image corruption), prevalence disparities (sample removal), and annotation disparities (label flipping). Models use ResNet18 architectures with AdamW optimizer. The primary metric is the percentage-point difference in mean accuracy (Δ Acc) between FRL and ERM for each subgroup. Subgroup separability is measured by logistic regression accuracy in predicting sensitive attributes from inputs. Training involves hyperparameter tuning followed by final sweeps with fixed parameters across five random seeds.

## Key Results
- FRL methods consistently underperform ERM in IID settings across all bias mechanisms
- FRL only improves performance under presentation disparities with high subgroup separability
- There is a statistically significant monotonic association between Δ Acc and subgroup separability
- FRL performance varies significantly across the three causal structures of bias

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fair representation learning cannot be both effective and harmless when training and test data are identically distributed.
- **Mechanism:** When training data is unbiased, ERM already produces fair representations (no sensitive information encoded), making FRL ineffective. When training data is biased, enforcing fair representations harms performance because the removed sensitive information is spuriously correlated with the target in the biased data.
- **Core assumption:** The training and test distributions are identical (IID setting).
- **Evidence anchors:** [abstract] "We prove fundamental limitations on fair representation learning when evaluation data is drawn from the same distribution as training data"
- **Break condition:** If training and test distributions differ (distribution shift scenario), the mechanism breaks down and FRL may become effective.

### Mechanism 2
- **Claim:** FRL validity under distribution shift depends on the underlying causal structure of the bias present at train-time.
- **Mechanism:** The causal structure determines whether sensitive information can be cleanly separated from task-relevant information. Presentation disparities allow clean separation, while prevalence and annotation disparities create entangled relationships that make clean separation impossible.
- **Core assumption:** The causal structure of the bias mechanism affects whether FRL can successfully remove sensitive information without harming task performance.
- **Evidence anchors:** [abstract] "performance depends on the causal structure of the bias"
- **Break condition:** If the causal structure doesn't allow clean separation of sensitive and task-relevant information (prevalence/annotation disparities), FRL will degrade performance.

### Mechanism 3
- **Claim:** FRL validity under distribution shift depends on subgroup separability (how easily sensitive attributes can be predicted from inputs).
- **Mechanism:** Higher subgroup separability means sensitive information is more easily identifiable and removable without affecting task-relevant features. Lower separability means sensitive and task-relevant features are more entangled, making removal more likely to harm performance.
- **Core assumption:** The amount of sensitive information initially present in the inputs affects FRL's ability to remove it without harming performance.
- **Evidence anchors:** [abstract] "performance depends on subgroup separability (how easily sensitive attributes can be predicted from inputs)"
- **Break condition:** If subgroup separability is low, FRL will consistently degrade performance regardless of the causal structure.

## Foundational Learning

- **Concept: Causal structures of dataset bias**
  - Why needed here: Understanding the three fundamental mechanisms (presentation, prevalence, annotation disparities) is essential for predicting when FRL will work or fail
  - Quick check question: Can you explain the difference between presentation disparities and prevalence disparities in terms of how they affect the relationship between sensitive attributes and targets?

- **Concept: Subgroup separability**
  - Why needed here: This metric determines how much sensitive information is encoded in the inputs and thus affects FRL's ability to remove it without harming performance
  - Quick check question: If subgroup separability is very high, what would you expect to happen to FRL performance compared to ERM?

- **Concept: Distribution shift paradigms**
  - Why needed here: Different evaluation paradigms (IID vs distribution shift) have fundamentally different implications for FRL validity
  - Quick check question: Why does FRL perform poorly in IID settings but may work under distribution shift?

## Architecture Onboarding

- **Component map:**
  - Feature extractor (fθ) -> Adversarial head & Classifier
  - Adversarial head predicts sensitive attributes from representations
  - Classifier maps representations to target predictions

- **Critical path:**
  1. Forward pass through feature extractor to get representations
  2. Forward pass through classifier and adversarial head
  3. Compute classification loss and adversarial loss
  4. Backpropagate combined loss to update feature extractor
  5. Update classifier and adversarial head separately

- **Design tradeoffs:**
  - Adversarial coefficient strength: Too high causes catastrophic forgetting, too low makes FRL ineffective
  - Feature extractor architecture: Deeper networks may capture more task-relevant information but also more sensitive information
  - Training schedule: Joint training vs alternating training affects convergence and stability

- **Failure signatures:**
  - If adversarial accuracy remains high (>50% for binary attributes), FRL is ineffective
  - If main task accuracy drops significantly compared to ERM, FRL is harming performance
  - If training is unstable or oscillates, adversarial coefficient may be too high

- **First 3 experiments:**
  1. Compare ERM vs FRL on an IID split of your dataset to verify the futility result
  2. Measure subgroup separability using a simple logistic regression to predict sensitive attributes
  3. Test FRL under controlled bias injection (presentation vs prevalence vs annotation) to identify which mechanism works best

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do FRL methods perform under different types of distribution shifts beyond the three causal structures examined?
- Basis in paper: [explicit] The paper discusses potential validity of FRL under distribution shifts but only tests three causal structures (presentation, prevalence, and annotation disparities).
- Why unresolved: The paper focuses on these three mechanisms due to their causal identifiability and practical relevance, but real-world shifts may involve more complex combinations or novel mechanisms.
- What evidence would resolve it: Experiments testing FRL methods on datasets with different types of distribution shifts, including combinations of the three mechanisms or entirely new shift types.

### Open Question 2
- Question: What is the precise relationship between subgroup separability and the generalization performance of FRL under distribution shift?
- Basis in paper: [explicit] The paper proposes that FRL validity depends on subgroup separability but does not provide a precise quantitative relationship or theoretical bounds.
- Why unresolved: While the paper demonstrates a correlation between separability and FRL performance, it does not establish a causal mechanism or provide mathematical bounds for performance degradation.
- What evidence would resolve it: Theoretical analysis deriving bounds on FRL performance as a function of subgroup separability, combined with extensive empirical validation across diverse datasets.

### Open Question 3
- Question: How do different FRL architectures and implementations affect the fundamental limitations discussed in the paper?
- Basis in paper: [explicit] The paper uses two specific FRL implementations (adversarial and conditional) but does not explore how different architectural choices might impact the theoretical results.
- Why unresolved: The paper focuses on proving limitations for FRL in general but does not investigate whether certain architectures might mitigate these limitations.
- What evidence would resolve it: Comparative studies of various FRL architectures (e.g., different disentanglement techniques, architectural priors) under the same theoretical framework to determine if any can overcome the identified limitations.

## Limitations
- The causal analysis relies on simplified assumptions about the data-generating process
- Experimental validation uses artificially injected biases rather than naturally occurring ones
- Focus on binary sensitive attributes and classification tasks limits generalizability

## Confidence
- **High confidence**: The theoretical proof of FRL futility under IID settings is mathematically sound and well-argued
- **Medium confidence**: The empirical results showing FRL effectiveness depends on causal structure and subgroup separability are supported by extensive experiments
- **Medium confidence**: The claim that FRL should not be applied blindly in performance-sensitive tasks is reasonable but may not generalize to all domains

## Next Checks
1. **External validity test**: Apply the proposed framework to naturally biased datasets (rather than artificially injected biases) to verify the causal structure and subgroup separability hypotheses hold in real-world settings.

2. **Generalization across tasks**: Test the framework on regression tasks and multi-class classification to determine if the findings extend beyond binary classification.

3. **Robustness to noise**: Evaluate how measurement noise in sensitive attribute labels affects the subgroup separability metric and subsequent FRL performance to understand sensitivity to label quality.