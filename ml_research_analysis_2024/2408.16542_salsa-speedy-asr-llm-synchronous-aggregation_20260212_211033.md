---
ver: rpa2
title: 'SALSA: Speedy ASR-LLM Synchronous Aggregation'
arxiv_id: '2408.16542'
source_url: https://arxiv.org/abs/2408.16542
tags:
- decoder
- salsa
- language
- speech
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SALSA, a lightweight method for integrating
  large language models (LLMs) with automatic speech recognition (ASR) systems to
  improve performance, especially for low-resource languages. The key idea is to synchronously
  couple the decoder layers of a pretrained ASR model with those of a pretrained LLM
  using simple feedforward projection layers, avoiding expensive cross-attention training.
---

# SALSA: Speedy ASR-LLM Synchronous Aggregation

## Quick Facts
- arXiv ID: 2408.16542
- Source URL: https://arxiv.org/abs/2408.16542
- Authors: Ashish Mittal; Darshan Prabhu; Sunita Sarawagi; Preethi Jyothi
- Reference count: 0
- One-line primary result: SALSA achieves up to 38% relative WER reduction on 8 low-resource languages while being faster than existing ASR-LLM integration methods

## Executive Summary
SALSA introduces a lightweight method for integrating large language models with automatic speech recognition systems by synchronously coupling their decoder layers using feedforward projection layers. The approach avoids expensive cross-attention training by keeping both ASR and LLM models frozen while only training lightweight projection layers. SALSA addresses the challenge of tokenizer mismatch through cascading tokenization, where the LLM generates text until a valid ASR-recognized sequence is formed. Experiments on 8 low-resource languages from the FLEURS benchmark demonstrate substantial WER improvements over standard parameter-efficient finetuning methods while maintaining faster training and inference speeds.

## Method Summary
SALSA synchronously couples the decoder layers of a pretrained ASR model with those of a pretrained LLM using simple feedforward projection layers, avoiding expensive cross-attention training. The method keeps both pretrained models frozen and only trains the projection layers that map ASR decoder states to LLM decoder states. To handle the mismatch between ASR and LLM tokenizers, SALSA uses cascading tokenization where the LLM generates tokens autoregressively until a valid text piece recognizable by the ASR tokenizer is formed. This text is then re-tokenized with the ASR's tokenizer to advance both decoders synchronously. The approach is evaluated on 8 low-resource languages from the FLEURS benchmark using Whisper Large-v2 as the ASR model and LLaMA2 as the LLM.

## Key Results
- Achieves up to 38% relative WER reduction compared to parameter-efficient finetuning methods
- Demonstrates substantial improvements across 8 low-resource languages from FLEURS benchmark
- Provides faster training and decoding compared to existing ASR-LLM integration approaches
- Maintains frozen ASR and LLM backbones while only training lightweight projection layers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SALSA improves ASR performance by projecting decoder states from the ASR model into the LLM decoder, allowing the LLM to benefit from the ASR's cross-attention layers without retraining them.
- Mechanism: The method uses simple feedforward projection layers to map the last state of the ASR decoder to the LLM decoder at selected layers. This projection acts as a residual connection, enabling the LLM to leverage the ASR's audio encoding capabilities while retaining its own language modeling strengths.
- Core assumption: The ASR decoder's last state contains sufficient information to guide the LLM decoder's language modeling without the need for cross-attention retraining.
- Evidence anchors:
  - [abstract]: "The key idea is to synchronously couple the decoder layers of a pretrained ASR model with those of a pretrained LLM using simple feedforward projection layers, avoiding expensive cross-attention training."
  - [section]: "In SALSA, we keep the pretrained ASR and LLM backbone architectures frozen and only train feedforward projection layers that couple the ASR decoder layers to the LLM decoder layers."

### Mechanism 2
- Claim: SALSA handles tokenizer mismatch between ASR and LLM by using cascading tokenization, ensuring valid text sequences are formed before advancing the ASR decoder.
- Mechanism: The LLM generates tokens autoregressively until a valid text piece recognizable by the ASR tokenizer is formed. This text is then re-tokenized with the ASR's tokenizer to convert into a sequence of tokens that can be understood by the ASR decoder, allowing both models to advance synchronously despite different vocabularies.
- Core assumption: The LLM can generate text that, when re-tokenized, forms valid sequences for the ASR decoder, even if the token vocabularies differ.
- Evidence anchors:
  - [abstract]: "A challenge of this coupling is the mismatch between the tokenizers of the LLM and ASR systems, which SALSA addresses through cascading tokenization."
  - [section]: "Since the ASR and LLMs are assumed to have been trained independently, in general, both of these could be different from the vocabulary V and tokenizer T of the ASR model."

### Mechanism 3
- Claim: SALSA's synchronous coupling of ASR and LLM decoders allows for more efficient training and decoding compared to existing methods that use n-best lists or cross-attention training.
- Mechanism: By keeping both decoders frozen and only training lightweight projection layers, SALSA avoids the computational overhead of n-best list generation and cross-attention retraining. The synchronous advancement ensures that both models contribute to the final output in a single pass.
- Core assumption: The frozen ASR and LLM models retain sufficient capability to contribute to improved performance when coupled through projection layers.
- Evidence anchors:
  - [abstract]: "SALSA achieves substantial reductions in word error rates (WERs), with up to 38% relative improvement over standard parameter-efficient finetuning, while being much faster to train and decode compared to existing approaches."
  - [section]: "We assume the LLM has been pre-trained with significantly more text data in the target language, compared to the speech transcription data used in the ASR model."

## Foundational Learning

- Concept: Encoder-decoder architecture in ASR systems
  - Why needed here: Understanding how ASR models encode audio and decode it into text is crucial for grasping how SALSA integrates with these models.
  - Quick check question: What is the role of the encoder and decoder in an ASR system, and how do they interact to produce transcriptions?

- Concept: Autoregressive language modeling in LLMs
  - Why needed here: LLMs generate text one token at a time based on previous tokens, which is fundamental to how SALSA synchronizes the two models.
  - Quick check question: How does an autoregressive language model generate text, and what is the significance of this process in the context of SALSA?

- Concept: Tokenization and vocabulary mismatch
  - Why needed here: SALSA must handle different tokenizers and vocabularies between ASR and LLM models, making it essential to understand how tokenization affects text processing.
  - Quick check question: What challenges arise from using different tokenizers and vocabularies in ASR and LLM models, and how does SALSA address these challenges?

## Architecture Onboarding

- Component map:
  - Whisper ASR encoder -> ASR decoder -> Projection layers -> LLaMA2 LLM decoder
  - Cascading tokenization module for tokenizer mismatch handling

- Critical path:
  1. ASR encoder processes audio into latent vectors
  2. ASR decoder generates initial text tokens
  3. LLM decoder generates tokens until a valid ASR sequence is formed
  4. Projection layers map ASR decoder states to LLM decoder states
  5. Synchronous advancement of both decoders

- Design tradeoffs:
  - Freezing ASR and LLM models vs. fine-tuning: SALSA freezes models to avoid computational overhead but may miss out on potential improvements from fine-tuning
  - Number of projection layers: More layers may improve performance but increase complexity and training time
  - Tokenizer handling: Cascading tokenization ensures valid sequences but may introduce latency

- Failure signatures:
  - High WER despite SALSA integration: Indicates projection layers or tokenizer handling may not be effective
  - Slow decoding times: Suggests cascading tokenization or projection layer computation may be inefficient
  - Model divergence: ASR and LLM outputs become inconsistent, indicating synchronization issues

- First 3 experiments:
  1. Test SALSA with a simple projection layer configuration on a single language to verify basic functionality
  2. Vary the number of projection layers to find the optimal balance between performance and complexity
  3. Compare SALSA's performance with and without cascading tokenization to assess its impact on accuracy and efficiency

## Open Questions the Paper Calls Out
None explicitly stated in the provided materials.

## Limitations
- Implementation complexity of cascading tokenization not fully validated with success rate measurements
- Generalization beyond low-resource languages remains untested
- Speed claims lack quantitative wall-clock comparisons with baseline methods

## Confidence
- High Confidence: Core architectural approach of coupling ASR and LLM decoders through projection layers is technically sound
- Medium Confidence: Experimental results showing WER improvements are promising but limited to specific language set
- Low Confidence: Speed claims lack quantitative support and cascading tokenization mechanism needs more empirical validation

## Next Checks
1. Conduct cascading tokenization robustness analysis measuring success rates and latency across diverse language pairs
2. Test SALSA on high-resource and typologically diverse languages to assess generalization beyond FLEURS benchmark
3. Perform speed benchmarking and ablation studies comparing wall-clock inference times with baseline approaches