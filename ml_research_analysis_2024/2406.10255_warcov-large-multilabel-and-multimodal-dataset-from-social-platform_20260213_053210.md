---
ver: rpa2
title: WarCov -- Large multilabel and multimodal dataset from social platform
arxiv_id: '2406.10255'
source_url: https://arxiv.org/abs/2406.10255
tags:
- data
- dataset
- https
- images
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WarCov, a large multilabel and multimodal
  dataset containing 3,187,105 Polish social media posts about the COVID-19 pandemic
  and the war in Ukraine from 2022. The dataset includes preprocessed texts and images,
  with labels derived from hashtags.
---

# WarCov -- Large multilabel and multimodal dataset from social platform

## Quick Facts
- arXiv ID: 2406.10255
- Source URL: https://arxiv.org/abs/2406.10255
- Reference count: 40
- 3.2M Polish social media posts with multimodal content and hashtag-derived labels

## Executive Summary
WarCov presents a large-scale multilabel and multimodal dataset containing 3,187,105 Polish social media posts focused on COVID-19 and the war in Ukraine from 2022. The dataset combines preprocessed text and images with labels automatically derived from hashtags, using XLM-RoBERTa for text embeddings and ResNet-18 for image processing. Preliminary experiments demonstrate the dataset's utility for multilabel classification tasks, achieving moderate performance metrics that establish baseline benchmarks for future research.

## Method Summary
The WarCov dataset was constructed through systematic collection of Polish social media posts from 2022, focusing on content related to COVID-19 and the war in Ukraine. Posts were preprocessed to clean text and normalize images, while labels were automatically extracted from hashtags present in the posts. Text embeddings were generated using XLM-RoBERTa, a multilingual transformer model, while image representations were produced using ResNet-18, a convolutional neural network. The resulting dataset combines these multimodal representations with automatically derived labels to create a resource suitable for multilabel classification tasks.

## Key Results
- Dataset contains 3,187,105 Polish social media posts from 2022
- Multilabel classification achieves validation accuracy of 0.429 and F1-score of 0.451
- Dataset is publicly available under CC BY-NC-SA 4.0 license

## Why This Works (Mechanism)
The dataset's effectiveness stems from leveraging social media's inherent multimodal nature, where posts naturally combine text and images. Hashtags provide an efficient mechanism for automatic label generation, while transformer-based embeddings capture semantic relationships in Polish text. The combination of text and image representations enables more comprehensive modeling of social media content compared to unimodal approaches.

## Foundational Learning
- Multilabel classification: Multiple labels can apply to single instances; essential for social media content that spans multiple topics
  - Why needed: Social media posts often discuss multiple simultaneous topics requiring flexible labeling
  - Quick check: Verify label cardinality distribution shows multiple labels per instance
- Hashtag-based labeling: Uses user-generated tags as ground truth
  - Why needed: Provides scalable automatic labeling mechanism for large datasets
  - Quick check: Sample hashtag-to-label mappings for semantic consistency
- Multimodal embeddings: Combines text and image representations
  - Why needed: Social media content is inherently multimodal, requiring joint modeling
  - Quick check: Confirm text and image embeddings are compatible dimensions

## Architecture Onboarding

Component map: Raw posts -> Preprocessing -> Text embeddings (XLM-RoBERTa) + Image embeddings (ResNet-18) -> Hashtag-based labeling -> Multilabel classification

Critical path: Data collection → Preprocessing → Embedding generation → Label extraction → Classification training → Evaluation

Design tradeoffs: Automatic hashtag labeling enables scale but introduces label noise; transformer embeddings provide semantic richness but require computational resources; multimodal approach captures content complexity but increases model complexity

Failure signatures: Low classification accuracy suggests label quality issues or embedding misalignment; inconsistent hashtag patterns indicate labeling challenges; poor multimodal integration suggests text-image semantic disconnect

First experiments:
1. Ablation study removing image features to assess multimodal contribution
2. Human annotation of random sample to validate hashtag-derived labels
3. Cross-validation with different embedding dimensions and architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Classification performance remains moderate (accuracy 0.429, F1 0.451), suggesting label quality or model limitations
- Hashtag-based labeling introduces uncertainty about ground truth accuracy and consistency
- Polish language focus limits generalizability to other languages and cultural contexts

## Confidence
- High confidence: Dataset size and scope (3.2M posts), basic preprocessing methodology, multimodal nature (text + images)
- Medium confidence: Embedding methodology (XLM-RoBERTa, ResNet-18), dataset licensing and availability
- Low confidence: Classification performance metrics, hashtag-based labeling accuracy, multimodal integration effectiveness

## Next Checks
1. Conduct independent validation of hashtag-derived labels using human annotation on a random sample to assess label quality and consistency
2. Perform cross-validation experiments with different embedding models and classification architectures to establish baseline performance benchmarks
3. Analyze image-text alignment by computing correlation between text embeddings and image representations to evaluate multimodal coherence