---
ver: rpa2
title: What Matters for Model Merging at Scale?
arxiv_id: '2410.03617'
source_url: https://arxiv.org/abs/2410.03617
tags:
- merging
- arxiv
- base
- expert
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates model merging at scale, examining
  how model size, base model quality, and number of experts interact to affect merged
  model performance. Experiments use PaLM-2 models (1B-64B parameters) and four merging
  methods across 2-8 experts, evaluating on held-in and zero-shot held-out tasks.
---

# What Matters for Model Merging at Scale?

## Quick Facts
- arXiv ID: 2410.03617
- Source URL: https://arxiv.org/abs/2410.03617
- Authors: Prateek Yadav; Tu Vu; Jonathan Lai; Alexandra Chronopoulou; Manaal Faruqui; Mohit Bansal; Tsendsuren Munkhdalai
- Reference count: 32
- Key outcome: Systematic evaluation of model merging at scale using PaLM-2 models, showing that larger models merge more effectively and that instruction-tuned base models significantly improve merging outcomes.

## Executive Summary
This paper provides a comprehensive empirical study of model merging at scale, examining how model size, base model quality, and number of experts interact to affect merged model performance. Using PaLM-2 models ranging from 1B to 64B parameters and four different merging methods, the authors systematically evaluate merging effectiveness across 2-8 experts on both held-in and zero-shot held-out tasks. The study reveals that merging is most effective when using strong instruction-tuned base models, that larger models are consistently easier to merge, and that merging consistently improves zero-shot generalization capabilities.

## Method Summary
The authors create expert models by fine-tuning PaLM-2 base models (both pretrained and instruction-tuned variants) on held-in task categories from the T0 mixture. They then merge these expert models using four different methods: Averaging, Task Arithmetic, Dare-TIES, and TIES. The merged models are evaluated on both held-in tasks (the tasks used to create the experts) and held-out tasks (zero-shot generalization). Performance is normalized against either expert model performance (for held-in tasks) or base model performance (for held-out tasks) to enable fair comparison across different model sizes and numbers of experts.

## Key Results
- Merging is significantly more effective when using instruction-tuned base models compared to pretrained ones
- Larger models facilitate easier merging and show better performance across all evaluation settings
- Merging consistently improves zero-shot generalization, with merged models matching or exceeding multitask baselines when using strong base models
- Larger models can merge more experts effectively without performance degradation
- Different merging methods behave similarly at larger scales when using instruction-tuned models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Instruction-tuned base models produce better merging outcomes than pretrained base models
- **Mechanism:** Instruction tuning further disentangles model weights, creating flatter loss landscapes and better linear mode connectivity between expert models
- **Core assumption:** Strong zero-shot performance in base models correlates with effective weight disentanglement
- **Evidence anchors:** Across all evaluation settings, using strong zero-shot instruction-tuned base models leads to improved performance compared to using pretrained models
- **Break condition:** When base model quality differences are negligible or when instruction tuning introduces noise that interferes with merging

### Mechanism 2
- **Claim:** Larger models facilitate easier merging
- **Mechanism:** Larger models are more over-parameterized relative to training data, making them more robust to parameter perturbations during merging and reducing interference between expert models
- **Core assumption:** Increased model size provides redundancy that compensates for noise and conflicts during parameter averaging
- **Evidence anchors:** As model size grows, merged model performance generally improves
- **Break condition:** When model size increases beyond practical computational limits or when parameter redundancy becomes excessive

### Mechanism 3
- **Claim:** Merging consistently improves zero-shot generalization
- **Mechanism:** Combining multiple expert models captures diverse task-specific knowledge while averaging out model-specific noise, resulting in better generalization to unseen tasks
- **Core assumption:** Expert models trained on different tasks have complementary strengths that combine effectively during merging
- **Evidence anchors:** Merging significantly enhances zero-shot generalization across all model sizes
- **Break condition:** When expert models have high task overlap or when merging introduces harmful interference between conflicting parameter updates

## Foundational Learning

- **Concept:** Linear mode connectivity in neural networks
  - Why needed here: Model merging fundamentally relies on the assumption that expert models lie in a low-loss region where their parameters can be interpolated without performance degradation
  - Quick check question: What property of neural network loss landscapes makes model merging theoretically possible?

- **Concept:** Task vectors and parameter space arithmetic
  - Why needed here: Understanding how differences between fine-tuned and base model parameters (task vectors) can be combined to create merged models
  - Quick check question: How does task arithmetic differ from simple parameter averaging in the merging process?

- **Concept:** Weight disentanglement and zero-shot generalization
  - Why needed here: The relationship between how well base models generalize to unseen tasks and how effectively their fine-tuned versions can be merged
  - Quick check question: Why might instruction tuning improve the disentanglement of model weights compared to standard pretraining?

## Architecture Onboarding

- **Component map:** PaLM-2 models (1B-64B) -> Expert models (fine-tuned on held-in tasks) -> Merging methods (Averaging, Task Arithmetic, Dare-TIES, TIES) -> Evaluation framework (held-in and held-out tasks) -> Normalized performance metrics

- **Critical path:**
  1. Create expert models by fine-tuning base models on held-in tasks
  2. Select subsets of expert models for merging experiments
  3. Apply merging method to combine selected expert models
  4. Evaluate merged model on both held-in and held-out tasks
  5. Normalize performance and aggregate results across seeds

- **Design tradeoffs:**
  - Model size vs. computational cost - larger models merge better but require more resources
  - Number of experts vs. performance degradation - more experts can improve generalization but may reduce held-in performance
  - Merging method complexity vs. effectiveness - simpler methods work well at scale but may miss optimizations

- **Failure signatures:**
  - Performance degradation when merging too many experts with weak base models
  - Inconsistent results across different random seeds due to task selection bias
  - Overfitting to held-in tasks at the expense of generalization to held-out tasks

- **First 3 experiments:**
  1. Merge two experts from the same base model size (e.g., 64B) and compare to individual experts
  2. Compare merging experts from PaLM-2 vs PaLM-2-IT base models of the same size
  3. Merge experts from increasing numbers of tasks (2→4→6→8) to identify performance saturation points

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of the instruction-tuned base model affect the performance of the merged model, and what specific aspects of the base model contribute to better merging outcomes?
- Basis in paper: The paper mentions that merging is more effective when experts are created from strong base models, i.e., models with good zero-shot performance.
- Why unresolved: While the paper establishes a correlation between base model quality and merging effectiveness, it does not delve into the specific characteristics of the base model that contribute to better merging outcomes.
- What evidence would resolve it: Further research could investigate the impact of various factors, such as the base model's architecture, training data, and fine-tuning techniques, on the merging process and final model performance.

### Open Question 2
- Question: What are the limitations of model merging when dealing with a large number of experts, and how can these limitations be overcome?
- Basis in paper: The paper explores merging up to 8 experts, but does not discuss the potential challenges or limitations of merging an even larger number of experts.
- Why unresolved: The paper does not address the scalability of model merging beyond 8 experts, leaving open the question of how well merging would perform with a significantly larger number of experts.
- What evidence would resolve it: Experiments with merging a larger number of experts, such as 16 or 32, could provide insights into the scalability of model merging and identify potential limitations or challenges.

### Open Question 3
- Question: How does the choice of merging method affect the performance of the merged model, and are there specific scenarios where one method outperforms others?
- Basis in paper: The paper compares four merging methods (Averaging, Task Arithmetic, Dare-TIES, and TIES) and finds that they perform similarly at larger scales with instruction-tuned models.
- Why unresolved: While the paper shows that different merging methods yield similar results at scale, it does not explore the potential advantages or disadvantages of each method in specific scenarios or with different types of expert models.
- What evidence would resolve it: Further research could investigate the performance of different merging methods across various tasks, model sizes, and numbers of experts to identify scenarios where one method might be more suitable than others.

## Limitations
- Results are based on a specific model family (PaLM-2) and may not generalize to other architectures
- The study doesn't investigate computational cost-benefit tradeoffs of merging at scale
- Limited exploration of merging beyond 8 experts leaves scalability questions unanswered

## Confidence

- **High Confidence:** The finding that larger models facilitate easier merging is well-supported by consistent experimental results across multiple model sizes and evaluation settings.
- **Medium Confidence:** The claim that instruction-tuned base models produce better merging outcomes, while supported by experimental data, may be influenced by the specific PaLM-2-IT implementation and may not generalize to other instruction-tuning approaches.
- **Medium Confidence:** The assertion that merging consistently improves zero-shot generalization is supported by the experimental results, but the mechanism behind this improvement warrants further investigation.

## Next Checks
1. Test the scaling relationship with additional model architectures (e.g., LLaMA, Mistral) to verify if larger models consistently facilitate easier merging across different families.
2. Conduct ablation studies varying the quality of instruction-tuned base models to establish a clearer threshold effect between weak and strong base models.
3. Investigate the computational efficiency of merged models versus multitask baselines to quantify practical benefits beyond raw performance metrics.