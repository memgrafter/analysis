---
ver: rpa2
title: Multi-granularity Interest Retrieval and Refinement Network for Long-Term User
  Behavior Modeling in CTR Prediction
arxiv_id: '2411.15005'
source_url: https://arxiv.org/abs/2411.15005
tags:
- user
- behavior
- latexit
- information
- interest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of modeling long-term user behavior
  for click-through rate (CTR) prediction in recommendation systems. The key problem
  is that existing methods focus on limited target-aware interest, neglecting diverse
  user interests and relational information in subsequences.
---

# Multi-granularity Interest Retrieval and Refinement Network for Long-Term User Behavior Modeling in CTR Prediction

## Quick Facts
- arXiv ID: 2411.15005
- Source URL: https://arxiv.org/abs/2411.15005
- Authors: Xiang Xu; Hao Wang; Wei Guo; Luankang Zhang; Wanshan Yang; Runlong Yu; Yong Liu; Defu Lian; Enhong Chen
- Reference count: 40
- Primary result: MIRRN achieves AUC improvements of 0.67%, 1.35%, and 0.51% on Taobao, Alipay, and Tmall datasets respectively

## Executive Summary
This paper addresses the challenge of modeling long-term user behavior for click-through rate (CTR) prediction in recommendation systems. The proposed Multi-granularity Interest Retrieval and Refinement Network (MIRRN) constructs queries based on behaviors at different time scales to capture target-aware, local-aware, and global-aware interests. The architecture employs a novel Multi-Head Fourier Transformer to efficiently learn sequential and interactive information within subsequences, followed by a multi-head target attention mechanism to assess the influence of multi-granularity interests on the target item. Experiments on three large-scale datasets demonstrate significant performance improvements over state-of-the-art baselines.

## Method Summary
The MIRRN framework tackles the limitation of existing methods that focus on limited target-aware interest by constructing multi-granularity queries. It first extracts behavior subsequences around the target item to construct target-aware queries. Then, it generates local-aware queries by analyzing neighboring subsequences and global-aware queries by examining the entire long-term sequence. The Multi-Head Fourier Transformer is introduced to efficiently capture both sequential and interactive information within these subsequences, avoiding the computational inefficiency of traditional attention mechanisms. Finally, a multi-head target attention mechanism assesses how these diverse interests influence the target item's click probability.

## Key Results
- MIRRN achieves AUC improvements of 0.67%, 1.35%, and 0.51% on Taobao, Alipay, and Tmall datasets respectively
- Online A/B test on a music streaming app shows 1.32% increase in average listening songs and 0.55% increase in average listening time
- Significant improvements over state-of-the-art baselines across all three benchmark datasets

## Why This Works (Mechanism)
The method works by addressing the fundamental limitation of existing CTR models that focus narrowly on target-aware interests while neglecting diverse user interests and relational information in subsequences. By constructing queries at multiple time scales (target-aware, local-aware, and global-aware), the model captures a more comprehensive representation of user interests. The Multi-Head Fourier Transformer efficiently learns both sequential patterns and interactive relationships within subsequences, which traditional attention mechanisms struggle to capture efficiently. The multi-head target attention mechanism then synthesizes these diverse interests to make more accurate predictions about user behavior.

## Foundational Learning
- **Multi-granularity interest modeling**: Understanding how user interests vary across different time scales is crucial for capturing diverse preferences. Quick check: Verify that the model captures both short-term and long-term interests effectively.
- **Sequential modeling with Fourier transforms**: Fourier-based approaches can capture periodic patterns in sequences more efficiently than traditional attention. Quick check: Compare computational efficiency with standard attention mechanisms.
- **CTR prediction fundamentals**: The task requires predicting click probability based on user-item interactions, which differs from standard classification tasks. Quick check: Ensure proper evaluation metrics (AUC) are used for ranking performance.

## Architecture Onboarding

**Component Map**: Input sequences -> Multi-granularity Query Construction -> Multi-Head Fourier Transformer -> Multi-head Target Attention -> Output

**Critical Path**: The most critical components are the multi-granularity query construction and the Multi-Head Fourier Transformer, as they directly impact the quality of interest representation and computational efficiency.

**Design Tradeoffs**: The architecture trades some model complexity for improved representation of diverse user interests. The Fourier-based approach sacrifices some interpretability for computational efficiency compared to traditional attention mechanisms.

**Failure Signatures**: Potential failures include:
- Overfitting to specific interest patterns if the multi-granularity approach is too rigid
- Loss of sequential information if Fourier transform parameters are not properly tuned
- Computational bottlenecks if the Multi-Head Fourier Transformer implementation is not optimized

**3 First Experiments**:
1. Validate that each type of query (target-aware, local-aware, global-aware) contributes meaningfully to performance
2. Compare the Multi-Head Fourier Transformer against standard self-attention in terms of both accuracy and efficiency
3. Test the model's performance on sequences of varying lengths to assess scalability

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond the general challenge of efficiently modeling long-term user behavior with diverse interests.

## Limitations
- Lack of ablation studies to isolate the contributions of individual components
- Computational efficiency of the Multi-Head Fourier Transformer compared to existing attention mechanisms is not thoroughly discussed
- Online A/B test results lack detailed statistical significance analysis and methodology description

## Confidence
- Performance improvements on benchmark datasets: High
- Effectiveness of multi-granularity interest modeling: Medium
- Online A/B test results: Medium
- Computational efficiency claims: Low

## Next Checks
1. Conduct comprehensive ablation studies to quantify the contribution of each component in the MIRRN architecture, including comparisons with and without the Multi-Head Fourier Transformer and multi-granularity query construction.
2. Perform extended online A/B testing with larger sample sizes and longer durations to validate the statistical significance of the observed improvements in user engagement metrics.
3. Evaluate the model's performance and computational efficiency under various sequence lengths and real-time serving conditions to assess its practical applicability in production environments.