---
ver: rpa2
title: Towards Robust Graph Incremental Learning on Evolving Graphs
arxiv_id: '2402.12987'
source_url: https://arxiv.org/abs/2402.12987
tags:
- learning
- graph
- uni00000013
- tasks
- incremental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of catastrophic forgetting in
  graph incremental learning (GIL) when the graph structure evolves over time (inductive
  NGIL). The authors formalize how structural shifts in the graph lead to distributional
  changes in node representations, causing performance degradation on earlier tasks.
---

# Towards Robust Graph Incremental Learning on Evolving Graphs

## Quick Facts
- arXiv ID: 2402.12987
- Source URL: https://arxiv.org/abs/2402.12987
- Reference count: 39
- Primary result: SSRM improves final average performance by 2-5% across three datasets while maintaining or slightly improving final average forgetting

## Executive Summary
This paper addresses catastrophic forgetting in graph incremental learning when graph structures evolve over time. The authors identify that structural shifts in evolving graphs cause distributional changes in node representations, leading to performance degradation on earlier tasks. They propose Structural-Shift-Risk-Mitigation (SSRM), a regularization technique that minimizes the distance between node representations before and after structural changes using maximum mean discrepancy (MMD). SSRM is shown to be effective when applied to existing GIL frameworks (ER-GNN, TWP, GEM) across three datasets: OGB-Arxiv, Reddit, and CoraFull.

## Method Summary
The paper proposes SSRM to mitigate catastrophic forgetting in inductive node-wise graph incremental learning (NGIL) on evolving graphs. The method adds a regularization term to existing GIL frameworks that minimizes the MMD distance between node representations computed on the old and new graph structures. This encourages the model to learn a representation space where the distribution of existing nodes' representations remains stable despite structural changes. SSRM is implemented as an additional loss term that can be easily integrated with existing GNN incremental learning frameworks without requiring architectural changes.

## Key Results
- SSRM consistently improves final average performance (FAP) by 2-5% across three datasets
- The method maintains or slightly improves final average forgetting (FAF) compared to baseline GIL frameworks
- SSRM demonstrates effectiveness when applied to multiple existing GIL frameworks (ER-GNN, TWP, GEM)
- The regularization technique shows particular strength on OGB-Arxiv and Reddit datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structural shift in evolving graphs causes distributional changes in node representations, leading to catastrophic forgetting.
- Mechanism: When new nodes/tasks arrive, the graph structure evolves, altering the k-hop neighborhoods of existing nodes. This changes the input distribution of their ego-graphs, causing learned representations to become misaligned with previous tasks.
- Core assumption: The representation space learned by GNNs is sensitive to structural changes in the input graph.
- Evidence anchors:
  - [abstract] "structural shifts in the graph lead to distributional changes in node representations, causing performance degradation on earlier tasks."
  - [section] Proposition 4.1 shows that imbalanced observations across tasks can lead to different expected inputs for the same node.
  - [corpus] Weak evidence - related works focus on inductive generalization but don't directly address structural shift as a forgetting cause.
- Break condition: If the GNN's representation function is invariant to structural changes (e.g., through specific architectural choices), this mechanism would be mitigated.

### Mechanism 2
- Claim: Minimizing Maximum Mean Discrepancy (MMD) between representations before and after structural changes reduces forgetting risk.
- Mechanism: SSRM adds a regularization term that encourages the model to learn a representation space where the distribution of existing nodes' representations remains stable despite structural changes. This is achieved by minimizing the MMD distance between representations computed on the old and new graph structures.
- Core assumption: A representation space with low MMD distance between distributions is less prone to catastrophic forgetting.
- Evidence anchors:
  - [abstract] "minimizes the distance between node representations before and after structural changes using maximum mean discrepancy (MMD)."
  - [section] Theorem 5.1 shows that minimizing MMD in the latent space reduces catastrophic forgetting risk.
  - [corpus] Weak evidence - MMD is used in domain adaptation but not commonly for structural shift in graphs.
- Break condition: If the MMD estimation is noisy or if the chosen kernel doesn't capture the relevant distributional differences, the regularization may not be effective.

### Mechanism 3
- Claim: SSRM can be easily integrated with existing GIL frameworks to improve their performance on evolving graphs.
- Mechanism: SSRM is implemented as an additional regularization term in the loss function. It doesn't require changes to the underlying GIL framework's architecture or training procedure, making it flexible and easy to adapt.
- Core assumption: Existing GIL frameworks can benefit from additional regularization that addresses structural shift without conflicting with their existing mechanisms.
- Evidence anchors:
  - [abstract] "SSRM is shown to be effective when applied to existing GIL frameworks (ER-GNN, TWP, GEM) across three datasets."
  - [section] "SSRM is easy to implement and can be easily adapted to existing GNN incremental learning frameworks to boost their performance in the inductive setting."
  - [corpus] Moderate evidence - Experience replay and parameter isolation methods exist but don't specifically address structural shift.
- Break condition: If the existing framework's regularization already addresses distributional shift, adding SSRM may have diminishing returns or cause interference.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their ability to learn node representations from graph structure and features.
  - Why needed here: Understanding how GNNs work is crucial for grasping how structural changes in the graph can affect learned representations and cause forgetting.
  - Quick check question: How does a GNN aggregate information from a node's neighborhood to form its representation?

- Concept: Incremental/Continual Learning and the problem of catastrophic forgetting.
  - Why needed here: The paper addresses catastrophic forgetting in the context of graph-structured data, which has unique challenges compared to traditional incremental learning on Euclidean data.
  - Quick check question: What is catastrophic forgetting, and why is it a problem in incremental learning?

- Concept: Maximum Mean Discrepancy (MMD) as a measure of distance between probability distributions.
  - Why needed here: MMD is the key tool used in SSRM to quantify and minimize the distributional shift caused by structural changes in the graph.
  - Quick check question: How does MMD differ from other distance measures between distributions, and why is it suitable for this application?

## Architecture Onboarding

- Component map: Graph data (nodes, edges, features) -> GNN backbone -> SSRM module (MMD computation) -> Task-specific prediction head -> Output predictions
- Critical path:
  1. Initialize GNN and prediction head
  2. For each new task:
     a. Update graph structure with new nodes
     b. Compute representations for old nodes on old and new graph
     c. Calculate MMD and add to loss
     d. Train on new task data with SSRM regularization
     e. Evaluate on all tasks
- Design tradeoffs:
  - MMD computation adds overhead but can be mitigated by sampling
  - Choice of kernel function in MMD affects sensitivity to distributional changes
  - Hyperparameter tuning for regularization strength (α, β) is crucial
- Failure signatures:
  - High variance in MMD estimates leading to unstable training
  - Over-regularization causing underfitting to new tasks
  - Incompatible with certain GNN architectures or existing regularization terms
- First 3 experiments:
  1. Implement SSRM on a simple GIL framework (e.g., ER-GNN) and verify it reduces forgetting on a synthetic dataset with controlled structural shifts.
  2. Compare different kernel functions for MMD estimation and their impact on performance and stability.
  3. Conduct ablation studies to determine the optimal balance between the two regularization terms in SSRM (controlled by α and β).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed SSRM method scale with increasing graph size and task complexity?
- Basis in paper: [inferred] The paper discusses the computational complexity of SSRM, stating that it is approximately 5% of the total computation time and that kernel approximation techniques can be used to make it more scalable.
- Why unresolved: The paper does not provide extensive empirical results on the scalability of SSRM with varying graph sizes and task complexities.
- What evidence would resolve it: Conducting experiments on graphs of different sizes and complexities, and measuring the computational time and memory usage of SSRM, would provide insights into its scalability.

### Open Question 2
- Question: How does the performance of SSRM compare to other regularization techniques for mitigating catastrophic forgetting in GIL?
- Basis in paper: [inferred] The paper focuses on SSRM as a regularization technique for mitigating catastrophic forgetting in GIL, but does not provide a comprehensive comparison with other regularization methods.
- Why unresolved: The paper does not include a detailed comparison of SSRM with other regularization techniques for GIL.
- What evidence would resolve it: Conducting experiments comparing SSRM with other regularization techniques, such as elastic weight consolidation (EWC) or variational continual learning (VCL), on the same datasets and tasks would provide insights into its relative performance.

### Open Question 3
- Question: How does the proposed SSRM method handle label noise or distribution shift in the graph data?
- Basis in paper: [inferred] The paper does not explicitly address the issue of label noise or distribution shift in the graph data, which can impact the performance of SSRM.
- Why unresolved: The paper does not discuss the robustness of SSRM to label noise or distribution shift, and does not provide any experimental results on this aspect.
- What evidence would resolve it: Conducting experiments on datasets with varying levels of label noise or distribution shift, and measuring the performance of SSRM under these conditions, would provide insights into its robustness.

## Limitations

- The paper assumes structural shifts are the primary cause of catastrophic forgetting, potentially overlooking other factors like task interference or representation drift.
- The evaluation focuses on 2-class classification tasks, which may not fully capture the method's performance on more complex, multi-class scenarios.
- The MMD regularization may be sensitive to kernel choice and bandwidth parameters, which are not thoroughly explored in the paper.

## Confidence

- High: The proposed SSRM mechanism is mathematically sound and the theoretical justification (Theorem 5.1) is well-established.
- Medium: The empirical results show consistent improvements across datasets and baselines, but the evaluation is limited to specific datasets and task configurations.
- Low: The paper does not provide extensive ablation studies on the impact of different kernel functions or hyperparameter settings for MMD computation.

## Next Checks

1. Evaluate SSRM on more diverse datasets with varying graph sizes, densities, and task complexities to assess its generalizability.
2. Conduct ablation studies to determine the optimal kernel function and bandwidth parameters for MMD computation, and their impact on performance and stability.
3. Investigate the interplay between SSRM and other regularization techniques commonly used in GIL, to understand potential synergies or conflicts.