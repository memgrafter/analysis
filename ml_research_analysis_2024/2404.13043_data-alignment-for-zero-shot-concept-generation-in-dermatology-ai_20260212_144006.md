---
ver: rpa2
title: Data Alignment for Zero-Shot Concept Generation in Dermatology AI
arxiv_id: '2404.13043'
source_url: https://arxiv.org/abs/2404.13043
tags:
- clip
- dermatology
- data
- captions
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of limited availability of high-quality
  labeled data for dermatology AI, particularly concept-level labels that are semantically
  meaningful to clinicians. The authors propose using Large Language Models (LLMs)
  to generate enriched captions that align clinical terminology with the natural language
  used in CLIP's pre-training data.
---

# Data Alignment for Zero-Shot Concept Generation in Dermatology AI

## Quick Facts
- arXiv ID: 2404.13043
- Source URL: https://arxiv.org/abs/2404.13043
- Authors: Soham Gadgil; Mahtab Bigverdi
- Reference count: 12
- Key outcome: Fine-tuned LLM-generated captions improve zero-shot dermatology concept classification (AUC 0.648 vs 0.636 vs 0.572)

## Executive Summary
This paper addresses the challenge of limited high-quality labeled data for dermatology AI by using Large Language Models (LLMs) to generate enriched captions that align clinical terminology with CLIP's pre-training data. The authors fine-tune GPT-2 and GPT-3.5 models on dermatology textbooks to extend PubMed article captions, then use these enriched captions to fine-tune CLIP for zero-shot concept classification. Their results show that fine-tuned GPT-3.5 captions significantly improve downstream performance, achieving a mean AUC of 0.648 across 48 clinical concepts compared to vanilla CLIP (AUC 0.572) and original PubMed captions (AUC 0.636).

## Method Summary
The approach involves three main steps: first, fine-tuning LLMs (GPT-2 and GPT-3.5) on dermatology textbooks to generate prompt-completion pairs; second, using these fine-tuned models to extend PubMed captions while staying within CLIP's token limit; and third, fine-tuning the CLIP model using the enriched captions. The system is evaluated on zero-shot concept classification using the SKINCON dataset, measuring performance across 48 clinical concepts through AUC scores.

## Key Results
- Fine-tuned GPT-3.5 captions achieve mean AUC of 0.648 across 48 clinical concepts
- Outperforms vanilla CLIP (AUC 0.572) and original PubMed captions (AUC 0.636)
- CLIP fine-tuning with extended captions improves performance for 41 out of 48 concepts
- Most PubMed captions are short (~35 tokens), leaving room for extension within CLIP's 77-token limit

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extending CLIP captions with fine-tuned LLMs improves language supervision for downstream zero-shot classification
- Mechanism: Fine-tuned LLMs generate richer, more descriptive captions that better align clinical terminology with CLIP's pre-training data, providing improved semantic context for contrastive learning
- Core assumption: GPT-3.5 fine-tuned on dermatology textbooks can generate medically accurate and contextually relevant extended captions
- Evidence anchors:
  - [abstract]: "using captions generated by fine-tuned GPT-3.5 improves downstream zero-shot concept classification performance, achieving a mean AUC of 0.648 across 48 clinical concepts"
  - [section]: "Fine-tuning the CLIP model improves performance for most of the concepts (41 out of 48)"
  - [corpus]: Weak - related papers focus on caption quality and text length for CLIP but don't directly address medical domain alignment
- Break condition: If fine-tuned LLM generates medically inaccurate or irrelevant content, downstream performance degrades

### Mechanism 2
- Claim: Domain-specific fine-tuning of LLMs on dermatology textbooks improves their ability to generate contextually relevant medical text
- Mechanism: Fine-tuning GPT-3.5 on dermatology textbooks enhances its understanding of clinical terminology and enables generation of more medically relevant extended captions
- Core assumption: Textbook text contains sufficient domain-specific knowledge to improve LLM medical language generation
- Evidence anchors:
  - [section]: "we find that using captions generated by an expressive fine-tuned LLM like GPT-3.5 improves downstream zero-shot concept classification performance"
  - [section]: "fine-tuning the LLM using dermatology text helps in improving the data alignment in the extended captions"
  - [corpus]: Weak - no direct evidence in corpus about domain-specific LLM fine-tuning for medical applications
- Break condition: If textbook content is insufficient or contains outdated medical information

### Mechanism 3
- Claim: CLIP's limited context length (77 tokens) necessitates caption extension to include more descriptive medical terminology
- Mechanism: Most PubMed captions are short (~35 tokens), leaving ~40 tokens available for extension to include more detailed clinical descriptions
- Core assumption: Extended captions within CLIP's token limit can meaningfully improve semantic alignment
- Evidence anchors:
  - [section]: "The mean length of captions is ~35 which shows that most of the captions are short and do not exceed the maximum token length of 77"
  - [section]: "75% of the captions have a token length of less than 51 which indicates that a majority of captions do have additional tokens available to be extended and improved"
  - [corpus]: Weak - related papers discuss token length limitations but not specifically for medical domains
- Break condition: If caption extension exceeds CLIP's token limit or becomes too verbose

## Foundational Learning

- Concept: Contrastive learning in CLIP
  - Why needed here: Understanding how CLIP learns joint embeddings is crucial for appreciating why caption quality matters
  - Quick check question: How does CLIP's contrastive learning objective work and why does it benefit from better-aligned captions?

- Concept: Zero-shot classification
  - Why needed here: The paper's evaluation relies on zero-shot performance, which depends on effective cross-modal alignment
  - Quick check question: What makes zero-shot classification possible in CLIP and how does caption quality affect it?

- Concept: Large Language Model fine-tuning
  - Why needed here: The paper's approach depends on effectively fine-tuning LLMs on domain-specific text
  - Quick check question: What are the key considerations when fine-tuning LLMs on specialized domain text?

## Architecture Onboarding

- Component map: PubMed image-caption pairs -> LLM fine-tuning on textbooks -> Caption extension -> CLIP fine-tuning -> Zero-shot concept classification

- Critical path:
  1. Preprocess dermatology textbooks for LLM fine-tuning
  2. Fine-tune LLM on textbook text
  3. Generate extended captions for PubMed images
  4. Fine-tune CLIP with extended captions
  5. Evaluate zero-shot performance on SKINCON

- Design tradeoffs:
  - LLM model choice: GPT-3.5 vs GPT-2 (power vs control)
  - Fine-tuning epochs: Balance between domain knowledge and overfitting
  - Caption extension length: Maximize information within CLIP's token limit

- Failure signatures:
  - LLM generates irrelevant or incorrect medical content
  - Extended captions exceed CLIP's token limit
  - Overfitting to textbook domain during LLM fine-tuning

- First 3 experiments:
  1. Fine-tune GPT-2 on textbook text and evaluate generated caption quality
  2. Compare zero-shot performance using vanilla vs fine-tuned LLM captions
  3. Test different caption extension strategies (length, content focus)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different fine-tuning strategies for LLMs (e.g., using PubMed articles vs. dermatology textbooks vs. both) compare in terms of improving caption alignment and downstream performance?
- Basis in paper: [explicit] The paper mentions using both PubMed articles and dermatology textbooks for fine-tuning, but doesn't directly compare their effectiveness
- Why unresolved: The paper only reports results using combined fine-tuning data without comparing different fine-tuning strategies
- What evidence would resolve it: Systematic comparison of CLIP performance using captions generated from LLMs fine-tuned on different data sources (textbooks only, PubMed only, both)

### Open Question 2
- Question: What is the optimal maximum token length for extending captions without losing context or introducing irrelevant information?
- Basis in paper: [inferred] The paper notes that GPT-2 sometimes strays from context when using longer token lengths, but doesn't systematically investigate optimal lengths
- Why unresolved: The paper sets a fixed maximum length of 512 tokens without exploring the relationship between token length and performance
- What evidence would resolve it: Ablation studies varying the maximum token length and measuring its impact on downstream classification performance

### Open Question 3
- Question: How does the performance of fine-tuned GPT-3.5 compare to more advanced models like GPT-4 in this domain-specific caption generation task?
- Basis in paper: [explicit] The paper mentions budget constraints prevented testing GPT-4 and suggests it as future work
- Why unresolved: The paper only tests GPT-3.5 and its fine-tuned variants, leaving the performance ceiling unknown
- What evidence would resolve it: Direct comparison of CLIP performance using captions generated by GPT-3.5 vs. GPT-4 (or other more advanced models)

### Open Question 4
- Question: What is the impact of incorporating instruction tuning data from dermatology sources on caption quality and downstream performance?
- Basis in paper: [inferred] The paper mentions instruction tuning as a potential improvement in the limitations section
- Why unresolved: The paper only uses standard fine-tuning approaches without exploring instruction tuning
- What evidence would resolve it: Comparative analysis of CLIP performance using captions generated from models fine-tuned with and without instruction tuning data

## Limitations

- Lack of ablation studies to isolate which aspects of caption extension contribute most to performance improvements
- Limited evaluation to single dermatology dataset (SKINCON) with 48 clinical concepts, raising generalizability concerns
- Reliance on textbook content for LLM fine-tuning may introduce outdated medical knowledge and biases

## Confidence

- **High Confidence**: Demonstration that CLIP's limited context length creates opportunities for caption extension
- **Medium Confidence**: Claim that fine-tuned GPT-3.5 specifically improves performance over both vanilla CLIP and PubMed captions
- **Low Confidence**: Assertion that domain-specific fine-tuning on dermatology textbooks is necessary for effective caption generation

## Next Checks

1. **Ablation Study**: Systematically evaluate zero-shot performance using different caption generation approaches: (a) extended captions from fine-tuned GPT-3.5, (b) extended captions from GPT-3.5 without fine-tuning, (c) PubMed captions alone, and (d) randomly generated medical text

2. **External Dataset Validation**: Test the approach on an independent dermatology dataset with different imaging characteristics and concept distributions to assess generalizability beyond the SKINCON dataset

3. **Clinical Relevance Assessment**: Conduct a blinded evaluation where dermatologists assess the quality and clinical utility of the extended captions generated by the fine-tuned LLM compared to original PubMed captions, focusing on accuracy, completeness, and relevance to diagnostic decision-making