---
ver: rpa2
title: 'LlavaGuard: An Open VLM-based Framework for Safeguarding Vision Datasets and
  Models'
arxiv_id: '2406.05113'
source_url: https://arxiv.org/abs/2406.05113
tags:
- safety
- llavaguard
- content
- category
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LlavaGuard is a VLM-based framework for vision safety assessment,
  introducing a customizable taxonomy and high-quality human-labeled dataset. It fine-tunes
  VLMs to evaluate visual content safety, providing safety ratings, violated categories,
  and rationales.
---

# LlavaGuard: An Open VLM-based Framework for Safeguarding Vision Datasets and Models

## Quick Facts
- **arXiv ID**: 2406.05113
- **Source URL**: https://arxiv.org/abs/2406.05113
- **Reference count**: 40
- **Primary result**: VLM-based framework achieving 89.46% balanced accuracy on held-out test set for vision safety assessment

## Executive Summary
LlavaGuard is a novel framework for assessing visual content safety using vision-language models (VLMs). It introduces a customizable safety taxonomy, high-quality human-labeled dataset, and fine-tuning approach that enables VLMs to evaluate visual content against specific safety policies. The framework provides safety ratings, identifies violated categories, and generates rationales for its assessments. LlavaGuard models (ranging from 0.5B to 7B parameters) outperform state-of-the-art baselines in accuracy while offering policy flexibility. Real-world applications include dataset auditing and safeguarding generative models, with LlavaGuard-7B achieving 89.46% balanced accuracy and 93.06% recall on a held-out test set.

## Method Summary
LlavaGuard fine-tunes pre-trained VLMs (Llava models) on a curated dataset with human-labeled safety assessments. The framework employs a customizable taxonomy with detailed risk guidelines, enabling context-specific safety assessments. Data augmentation techniques include policy exceptions and decision-based reasoning to enhance model robustness. Fine-tuning uses full fine-tuning (not LoRA) with learning rate 2e-5, cosine scheduler with 0.05% warm-up, and batch sizes adjusted by model size. The output format is structured JSON containing safety rating, violated categories, and rationales. Models are evaluated on balanced accuracy, recall, specificity, and policy exception rate metrics.

## Key Results
- LlavaGuard-7B achieves 89.46% balanced accuracy and 93.06% recall on held-out test set
- Outperforms larger VLMs like Llava-34B on safety assessment tasks
- Detects 1.29% unsafe images during ImageNet auditing
- Provides flexible policy adaptation through customizable taxonomy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LlavaGuard models outperform baseline VLMs by learning nuanced safety assessments through fine-tuning on a curated dataset with structured output formats.
- Mechanism: By leveraging Llava's strong vision-language understanding as a foundation and fine-tuning it on high-quality human-labeled data with explicit safety categories and rationales, the model learns to not only detect unsafe content but also articulate why it violates specific safety policies.
- Core assumption: Pre-trained VLMs have sufficient commonsense knowledge and instruction-following ability to learn complex safety reasoning when fine-tuned with structured, policy-aligned data.
- Evidence anchors:
  - [abstract] "We establish a novel open framework, describing a customizable safety taxonomy, data preprocessing, augmentation, and training setup."
  - [section] "We validate the performance of LlavaGuard on two real-world tasks: dataset curation and content moderation of generative models."
- Break condition: If the pre-trained VLM lacks sufficient commonsense understanding or instruction-following capabilities, or if the human-labeled data is noisy or inconsistent, the fine-tuned model will fail to generalize to nuanced safety assessments.

### Mechanism 2
- Claim: LlavaGuard's customizable taxonomy and policy-driven prompting enable context-aware safety assessments adaptable to different use cases.
- Mechanism: By providing detailed risk guidelines for each safety category as part of the model's system prompt, LlavaGuard can flexibly adjust its safety assessment based on varying policies (e.g., different legal frameworks or domain-specific requirements).
- Core assumption: The VLM can effectively parse and follow complex policy descriptions provided as textual input, and its understanding is not rigidly fixed to a single safety standard.
- Evidence anchors:
  - [abstract] "Our introduced customizable taxonomy categories enable the context-specific alignment of LlavaGuard to various scenarios."
  - [section] "The adaptive nature of these guidelines allows for flexible adjustments to the policy to suit specific use cases."
- Break condition: If the model's understanding is too rigidly aligned to a single policy (like GPT-4's alignment to OpenAI's policy), it will fail to adapt to different safety requirements or contexts.

### Mechanism 3
- Claim: Data augmentation techniques, including policy exceptions and decision-based reasoning, enhance LlavaGuard's ability to handle nuanced safety scenarios.
- Mechanism: By creating synthetic training samples where certain safety categories are declared non-violating, and by appending decision-based reasoning to rationales, the model learns to adjust its safety rating based on policy changes and provides more coherent justifications.
- Core assumption: The VLM can learn from synthetic examples and improve its reasoning capabilities through conditioning on prior knowledge (input image, rating, and category).
- Evidence anchors:
  - [section] "To promote this behavior, we implement two data augmentation techniques... First, we introduce additional training samples with a modified policy prompt."
  - [section] "For the rationales, we append decision-based reasoning to all rationales... An exemplary decision-based reasoning looks like this..."
- Break condition: If the model overfits to the specific augmentation patterns or fails to generalize from synthetic examples, its ability to handle real-world nuanced safety scenarios will be limited.

## Foundational Learning

- **Vision-Language Models (VLMs)**: Why needed here: LlavaGuard is built upon pre-trained VLMs (Llava models) as the foundation for its safety assessment capabilities. Quick check question: What are the key capabilities of VLMs that make them suitable for safety assessment tasks?
- **Fine-tuning and Parameter-Efficient Fine-Tuning (PEFT)**: Why needed here: LlavaGuard models are fine-tuned on a curated dataset to learn safety-specific tasks, and different PEFT methods (full fine-tuning vs. LoRA) are compared for their effectiveness. Quick check question: How does fine-tuning differ from PEFT, and what are the trade-offs between them in terms of performance and efficiency?
- **Safety Taxonomies and Policy-Driven Assessment**: Why needed here: LlavaGuard employs a customizable safety taxonomy and uses policy descriptions as input to guide its safety assessments, enabling context-aware and adaptable safety evaluations. Quick check question: How do safety taxonomies help in systematically categorizing and assessing safety risks, and why is policy-driven assessment important for real-world applications?

## Architecture Onboarding

- **Component map**: Pre-trained VLM (Llava) -> Curated dataset with safety labels -> Data augmentation pipeline -> Fine-tuning process -> Structured output format -> Customizable safety taxonomy
- **Critical path**: 1. Load pre-trained VLM (Llava) 2. Prepare curated dataset with safety labels and rationales 3. Apply data augmentation techniques 4. Fine-tune VLM on augmented dataset 5. Evaluate model on held-out test set 6. Deploy model for real-world safety assessment tasks
- **Design tradeoffs**:
  - Model size vs. efficiency: Larger models (34B) offer better performance but are less efficient, while smaller models (7B) are more efficient but may have slightly lower accuracy
  - Full fine-tuning vs. LoRA: Full fine-tuning achieves better performance but requires more computational resources, while LoRA is more parameter-efficient but may not capture complex safety reasoning as effectively
  - Granularity of safety taxonomy: A more detailed taxonomy allows for nuanced assessments but may increase complexity and computational cost
- **Failure signatures**: Model consistently misclassifies certain safety categories or fails to adapt to policy changes; Model generates incoherent or irrelevant rationales for its safety assessments; Model overfits to training data and fails to generalize to unseen safety scenarios
- **First 3 experiments**: 1. Evaluate zero-shot performance of pre-trained Llava models on the held-out test set to establish a baseline 2. Fine-tune Llava-13B on the curated dataset with full fine-tuning and evaluate its performance on the test set 3. Compare the performance of LlavaGuard-13B with and without data augmentation techniques to assess their impact on safety assessment quality

## Open Questions the Paper Calls Out

1. **Cross-Cultural Validation**: Evaluate LlavaGuard's performance on safety datasets annotated by diverse cultural groups to assess its effectiveness across different cultural contexts and safety norms.

2. **Adversarial Robustness Testing**: Systematically test LlavaGuard against adversarial examples designed to evade safety detection, including common attack vectors like image perturbations and policy-hacking prompts.

3. **Longitudinal Stability Assessment**: Deploy LlavaGuard in a real-world setting for an extended period (3-6 months) and monitor its safety assessment accuracy and consistency as new types of visual content emerge and safety standards evolve.

## Limitations
- The human-labeled dataset may not capture the full diversity of real-world safety scenarios, particularly edge cases and culturally-specific content
- The framework's adaptability to different safety policies has not been extensively validated across multiple regulatory frameworks
- Computational resources required for fine-tuning larger models may limit accessibility for some organizations

## Confidence
- **High**: Technical implementation of VLM fine-tuning and structured output generation
- **Medium-High**: Benchmark results on held-out test sets demonstrating superior performance
- **Medium**: Real-world deployment claims due to limited validation scope

## Next Checks
1. **Cross-Cultural Validation**: Evaluate LlavaGuard's performance on safety datasets annotated by diverse cultural groups to assess its effectiveness across different cultural contexts and safety norms.

2. **Adversarial Robustness Testing**: Systematically test LlavaGuard against adversarial examples designed to evade safety detection, including common attack vectors like image perturbations and policy-hacking prompts.

3. **Longitudinal Stability Assessment**: Deploy LlavaGuard in a real-world setting for an extended period (3-6 months) and monitor its safety assessment accuracy and consistency as new types of visual content emerge and safety standards evolve.