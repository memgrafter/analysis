---
ver: rpa2
title: 'In2Core: Leveraging Influence Functions for Coreset Selection in Instruction
  Finetuning of Large Language Models'
arxiv_id: '2408.03560'
source_url: https://arxiv.org/abs/2408.03560
tags:
- training
- influence
- data
- arxiv
- point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high cost of fine-tuning large language
  models (LLMs) by proposing In2Core, a coreset selection algorithm that uses influence
  functions to identify the most impactful training points. In2Core leverages internal
  model gradients to rank training points based on their influence on model performance,
  allowing for training with only 50% of the original data while achieving comparable
  or better results.
---

# In2Core: Leveraging Influence Functions for Coreset Selection in Instruction Finetuning of Large Language Models

## Quick Facts
- **arXiv ID**: 2408.03560
- **Source URL**: https://arxiv.org/abs/2408.03560
- **Reference count**: 16
- **Primary result**: In2Core selects influential training points using gradient-based influence functions, enabling fine-tuning with 50% of data while maintaining or improving performance.

## Executive Summary
This paper introduces In2Core, a coreset selection algorithm that leverages influence functions to identify the most impactful training points for instruction fine-tuning of large language models. By analyzing the correlation between training and evaluation samples through a trained reference model, In2Core achieves comparable or better performance using only half the original training data. The method is particularly valuable for reducing the computational cost of fine-tuning large models while maintaining or improving generalization performance.

## Method Summary
In2Core uses influence functions to rank training points based on their impact on validation loss, selecting those with negative influence (proponents) that reduce loss when included. The method computes influence values using internal model gradients via the DataInf algorithm, with an optimization to use fewer layers for efficiency. A reference model fine-tuned on the full dataset provides the influence estimates, which are then used to select a coreset for fine-tuning the target model. The approach includes a token-level gradient averaging step to make influence calculations tractable, though this introduces potential bias toward longer sequences.

## Key Results
- In2Core achieves comparable or better performance with 50% of the original training data
- Models trained on smaller datasets perform similarly to those trained on full datasets, as measured by perplexity and MMLU accuracy
- Influence functions are more effective than semantic similarity in measuring a model's coverage of test points
- A smaller reference model can reliably rank training points for a larger target model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Influence functions can identify training points that, when removed, improve model performance by eliminating harmful data.
- Mechanism: The algorithm computes the gradient-based influence of each training point on the validation loss, ranking them by impact. Points with positive influence increase loss and are thus harmful; removing them improves generalization.
- Core assumption: The gradient-based influence estimation (via DataInf) accurately approximates the true leave-one-out retraining effect.
- Evidence anchors:
  - [abstract]: "We propose the In2Core algorithm, which selects a coreset by analyzing the correlation between training and evaluation samples with a trained model."
  - [section]: "Proponents - Points with negative influence values. Their addition to the training set reduces the validation loss."
  - [corpus]: Found 25 related papers; no direct evidence of gradient influence in selection methods.
- Break condition: If the Hessian approximation in DataInf is poor (e.g., for very deep models or noisy gradients), the influence ranking becomes unreliable.

### Mechanism 2
- Claim: A smaller reference model can reliably rank training points for a larger target model.
- Mechanism: Influence values computed on a smaller model transfer because the underlying training distribution is preserved; the ranking of points by influence is correlated across architectures.
- Core assumption: The smaller model captures sufficient structure of the training data distribution to rank points correctly.
- Evidence anchors:
  - [section]: "Surprisingly, we find that a smaller model ... can act as a reliable reference model ... This implies that a practitioner can simply bypass training a reference model if an open-source version is available."
  - [corpus]: No corpus evidence of cross-model influence transfer.
- Break condition: If the reference model's architecture or pretraining regime is too different, the ranking may no longer transfer.

### Mechanism 3
- Claim: Averaging token gradients to represent sequence gradients introduces bias toward longer sequences.
- Mechanism: Influence computation uses the average gradient over tokens; longer sequences have more tokens, inflating their gradient magnitude and thus their influence score.
- Core assumption: Token-level gradients are summed uniformly to represent sequence-level influence.
- Evidence anchors:
  - [section]: "We take the average gradient of the tokens when calculating the gradient of each point to make influence value calculations tractable."
  - [corpus]: No corpus evidence of token-level gradient bias.
- Break condition: If sequence length distribution changes drastically between training and test sets, the bias may distort selection.

## Foundational Learning

- Concept: Influence functions and leave-one-out retraining
  - Why needed here: The paper relies on influence functions to rank training points without retraining the model; understanding leave-one-out retraining explains why this works.
  - Quick check question: What is the mathematical definition of influence in terms of model parameters and validation loss?

- Concept: Hessian matrix approximation for large models
  - Why needed here: Exact Hessian computation is infeasible for LLMs; DataInf uses a LoRA-based approximation, which is central to efficiency.
  - Quick check question: How does LoRA reduce the memory and compute burden when computing gradients for influence?

- Concept: Spearman rank correlation and memory efficiency trade-offs
  - Why needed here: The paper selects the number of layers to compute influence based on maximizing Spearman correlation under a memory budget.
  - Quick check question: If using k layers yields Spearman correlation 0.9 with full-layer computation, what is the memory efficiency s?

## Architecture Onboarding

- Component map: Reference model training -> Influence value computation -> Coreset selection -> Target model fine-tuning
- Critical path: Compute influence → rank points → select proponents → fine-tune target model
- Design tradeoffs:
  - Memory vs. accuracy in influence computation (fewer layers saves memory but reduces correlation)
  - Number of proponents vs. final model quality (too few may underfit, too many may include opponents)
  - Reference model size vs. transferability (smaller model faster but potentially less reliable)
- Failure signatures:
  - Out-of-memory errors during influence computation (reduce layers or sample subset)
  - Perplexity increases after coreset selection (included too many opponents)
  - Perplexity close to baseline (insufficient proponents or poor influence estimation)
- First 3 experiments:
  1. Compute influence values on a small random subset of training data using a small reference model; verify negative correlation between influence and performance.
  2. Vary number of LoRA layers (e.g., 4, 8, 16) and measure Spearman correlation to full-layer influence; select k with highest correlation under memory budget.
  3. Fine-tune target model on top-25% proponents vs. random 25% vs. full dataset; compare perplexity to confirm proponents improve performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of In2Core compare when applied to larger language models (e.g., models with over 10 billion parameters) and more diverse datasets?
- Basis in paper: [inferred] The paper mentions that influence functions are particularly challenging for large language models with billions of parameters, but the experiments are conducted on models with up to 7 billion parameters. The authors suggest that their method is scalable, but they do not provide empirical evidence for larger models.
- Why unresolved: The paper does not provide experimental results or theoretical analysis on the scalability of In2Core to larger language models and more diverse datasets.
- What evidence would resolve it: Experimental results comparing the performance of In2Core on large language models (e.g., models with over 10 billion parameters) and diverse datasets to smaller models and datasets.

### Open Question 2
- Question: What is the theoretical basis for the observed transfer effect of influence values across different model architectures and pre-training regimes?
- Basis in paper: [explicit] The authors observe that influence values transfer across model architectures and pre-training regimes, but they do not provide a theoretical explanation for this phenomenon.
- Why unresolved: The paper does not provide a theoretical analysis of why influence values transfer across different model architectures and pre-training regimes.
- What evidence would resolve it: A theoretical analysis explaining the conditions under which influence values transfer across different model architectures and pre-training regimes.

### Open Question 3
- Question: How can In2Core be adapted to handle group influence, where the contribution of a group of training points is considered rather than individual points?
- Basis in paper: [explicit] The authors mention that their formulation of influence looks at individual points, but they suggest that it could be applied for group influence by replacing DataInf with a group influence algorithm.
- Why unresolved: The paper does not provide a detailed explanation of how In2Core can be adapted to handle group influence.
- What evidence would resolve it: A detailed explanation of how In2Core can be adapted to handle group influence, including the choice of group influence algorithm and the impact on performance.

## Limitations

- Influence estimation reliability: The accuracy of DataInf approximations under different training dynamics and data distributions is not fully explored.
- Cross-model transferability: The extent to which smaller reference models can reliably rank points for larger target models across different architectures is unclear.
- Token-level gradient bias: Averaging token gradients may introduce bias toward longer sequences, potentially distorting selection quality.

## Confidence

**High confidence**: The core experimental methodology (fine-tuning on proponents vs. random subsets vs. full datasets) is well-specified and reproducible. The observed improvement in perplexity and MMLU accuracy when using In2Core-selected proponents is convincing.

**Medium confidence**: The theoretical justification for using influence functions for coreset selection is sound, but the practical implementation details (especially the optimization for layer selection) lack sufficient specification for exact reproduction.

**Low confidence**: The claim that semantic similarity is inferior to influence functions for measuring model coverage of test points is based on limited evidence and requires more rigorous validation across diverse datasets and tasks.

## Next Checks

1. **Ablation study on reference model size**: Systematically evaluate how the size and architecture of the reference model affect the quality of influence-based ranking. Test whether the observed transferability holds across different model families and pretraining objectives.

2. **Influence estimation validation**: Compare the influence values computed by DataInf against actual leave-one-out retraining on a small subset of the data. Quantify the approximation error and identify conditions under which the approximation breaks down.

3. **Bias analysis in token-level gradients**: Conduct an experiment to measure the correlation between sequence length and influence scores. Investigate whether normalizing by sequence length or using alternative aggregation methods (e.g., max token gradient) improves selection quality.