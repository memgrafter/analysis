---
ver: rpa2
title: 'MRSE: An Efficient Multi-modality Retrieval System for Large Scale E-commerce'
arxiv_id: '2408.14968'
source_url: https://arxiv.org/abs/2408.14968
tags:
- item
- mrse
- query
- user
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MRSE addresses the limitations of uni-modality embedding-based
  retrieval systems in large-scale e-commerce by integrating text, image, and user
  preference information through lightweight mixture-of-expert modules. The system
  uses three specialized LMoE modules (VBert, FtAtt, Light-Bert) for inter-modality
  and intra-modality feature extraction, coupled with a hybrid loss function combining
  in-batch softmax cross-entropy and triplet loss with hard-negative sampling.
---

# MRSE: An Efficient Multi-modality Retrieval System for Large Scale E-commerce

## Quick Facts
- arXiv ID: 2408.14968
- Source URL: https://arxiv.org/abs/2408.14968
- Reference count: 8
- Primary result: 18.9% improvement in offline relevance and 3.7% online gains in IMP, CTR, GMV over state-of-the-art uni-modality system at Shopee

## Executive Summary
MRSE addresses the limitations of uni-modality embedding-based retrieval systems in large-scale e-commerce by integrating text, image, and user preference information through lightweight mixture-of-expert modules. The system uses three specialized LMoE modules (VBert, FtAtt, Light-Bert) for inter-modality and intra-modality feature extraction, coupled with a hybrid loss function combining in-batch softmax cross-entropy and triplet loss with hard-negative sampling. The method achieves significant improvements in both offline relevance and online core metrics compared to the state-of-the-art uni-modality system at Shopee.

## Method Summary
MRSE is a two-tower embedding-based retrieval system that integrates text, item image, and user preference information through lightweight mixture-of-expert (LMoE) modules. The system employs three specialized modules: VBert for image-text inter-modality leveraging, Light-Bert for text processing, and FtAtt for fast text with attention mechanism. These modules are trained in two stages with a hybrid loss function combining in-batch softmax cross-entropy and triplet loss with hard-negative sampling. The system builds user profiles at a multi-modality level and adjusts retrieval structures according to modality preferences, achieving 18.9% improvement in offline relevance and 3.7% gain in online core metrics.

## Key Results
- 18.9% improvement in offline relevance metrics (Recall@K and Rele@Kp) compared to state-of-the-art uni-modality system
- 3.7% gain in online core metrics including Impression (IMP), Click-Through Rate (CTR), and Gross Merchandise Volume (GMV)
- Hybrid loss function shows superior convergence and performance over individual loss functions in both offline and online settings

## Why This Works (Mechanism)

### Mechanism 1
Lightweight Mixture-of-Experts (LMoE) modules better align inter-modality and intra-modality features than single dense models. LMoE splits feature extraction into three specialized modules (VBert for image-text, Light-Bert for text, FtAtt for token-level attention) that each handle distinct semantic spaces, then concatenates and fuses them. Different modalities require different inductive biases; sharing a single architecture leads to suboptimal representation for at least one modality. Evidence: [abstract] "integrates text, item image, and user preference information through lightweight mixture-of-expert (LMoE) modules, better aligning inter-modality and intra-modality features" and [section] "VBert, FtAtt, and Light-Bert are introduced for better inter-modality and intra-modality leveraging".

### Mechanism 2
Hybrid loss combining in-batch softmax cross-entropy and hard-negative triplet loss improves convergence and robustness. MNSCE accelerates learning by exploiting easy negatives in batch; HNST focuses the model on hard negatives from low-relevance logs, balancing sample difficulty. ERS suffers from both easy negatives (wasted gradient) and hard negatives (poor convergence); combining both addresses both. Evidence: [abstract] "hybrid loss function that enhances consistency and robustness using hard negative sampling" and [section] "hybrid loss function combines a weighted sum of MNSCE and HNST" and Fig.5(a) shows faster convergence.

### Mechanism 3
Capturing user modality preferences through historical behavior embeddings improves retrieval relevance. FtAtt and auxiliary history embeddings encode users' past interactions across text and image, enabling the model to adjust modality weights per user. Users exhibit consistent modality preferences over time that are learnable and useful for retrieval. Evidence: [abstract] "builds user profiles at a multi-modality level and introduces a novel hybrid loss function that enhances consistency and robustness" and [section] "leverage LMoE modules to create multimodal historical behavior embeddings, allowing for the adjustment of retrieval structures according to modality preferences".

## Foundational Learning

- Concept: Two-tower embedding-based retrieval architecture
  - Why needed here: MRSE uses a query tower and item tower to produce dense embeddings, enabling scalable approximate nearest neighbor search in low latency
  - Quick check question: What is the advantage of late fusion in two-tower vs cross-attention architectures in retrieval systems?

- Concept: Hard negative sampling in metric learning
  - Why needed here: HNST selects negatives from low-relevance logs to force the model to distinguish subtle differences, improving precision
  - Quick check question: How does hard negative sampling differ from random negative sampling in triplet loss, and what are the trade-offs?

- Concept: Multi-modal fusion strategies (weighted sum, attention, concatenation)
  - Why needed here: MRSE uses concatenation + DSSM to fuse VBert, Light-Bert, FtAtt outputs; understanding fusion impact is critical
  - Quick check question: What are the pros and cons of late fusion via concatenation vs early fusion via cross-modal attention?

## Architecture Onboarding

- Component map:
  Query tower: VBert (image-text), Light-Bert (text), FtAtt (token attention), history embeddings, DSSM
  Item tower: VBert (image-text), Light-Bert (text), FtAtt (token attention), history embeddings, DSSM
  Online: Embed-Producer for precomputed embeddings, Q2I/I2I recall queues, Vespa index

- Critical path: Query → Embed-Producer cache hit → MRSE Q2I → item pool → downstream ranking

- Design tradeoffs: LMoE adds module count and latency but improves modality alignment; hybrid loss balances convergence vs hard sample focus; history embeddings add personalization but require sufficient data

- Failure signatures: Poor recall → check LMoE weights and history sparsity; slow convergence → tune β/α ratio; modality collapse → inspect fusion layer activations

- First 3 experiments:
  1. Replace LMoE with single Transformer; compare Recall@500 and training time
  2. Remove history embeddings; measure impact on Rele@Kp_w
  3. Swap hybrid loss for only MNSCE; observe convergence curve and hard-negative performance

## Open Questions the Paper Calls Out

### Open Question 1
How does MRSE perform when incorporating user temporal multi-modal information such as long-term add-to-cart and order history? The paper states "In future work, we plan to incorporate user temporal multi-modal information into the search system, such as long-term add-to-cart and order history, to further enhance ERS performance." The paper focuses on current user behavior and historical click data but does not evaluate the impact of longer-term user actions like add-to-cart or purchase history on retrieval performance.

### Open Question 2
What is the optimal balance between hard and easy negative sampling ratios for the hybrid loss function in different e-commerce contexts? Figure 5(b) shows performance varies with different ratios of hard to easy negatives, but the optimal ratio may depend on specific e-commerce contexts and data distributions. The paper only tests one optimal ratio and does not explore how this ratio might need to be adjusted for different product categories, user behaviors, or market segments.

### Open Question 3
How does MRSE's performance scale with increasing dataset sizes and dimensionality of feature representations? The paper mentions handling billions of daily requests and large-scale datasets but does not provide detailed scalability analysis. While the paper demonstrates effectiveness on Shopee's dataset, it does not provide insights into how performance changes as dataset sizes grow or as feature dimensions increase.

## Limitations
- Architecture specificity: Highly specialized for Shopee's e-commerce ecosystem and may not generalize to other domains without significant adaptation
- Offline-to-online gap: Claims of "state-of-the-art" improvement lack comparative context and detailed statistical significance analysis
- Implementation complexity: Hybrid loss function and LMoE architecture introduce significant complexity with potential sensitivity to hyperparameter tuning

## Confidence
- High Confidence: Core architecture (two-tower LMoE with DSSM fusion) and hybrid loss concept are well-supported with explicit experimental results
- Medium Confidence: Mechanism explanations for LMoE benefits and hybrid loss convergence are logical but rely on indirect evidence rather than ablation studies
- Low Confidence: Claims about user preference modeling lack quantitative ablation results and empirical validation of learnable user modality preferences

## Next Checks
1. **Component Ablation Study**: Implement and test MRSE with individual components disabled (single Transformer instead of LMoE, no history embeddings, only MNSCE loss) to quantify each component's contribution to the reported 18.9% offline improvement.

2. **Cross-Domain Generalization**: Train the MRSE architecture on a different e-commerce dataset (e.g., Amazon or Walmart) with the same modalities to assess whether the reported improvements transfer beyond Shopee's specific data distribution and user behavior patterns.

3. **Hyperparameter Sensitivity Analysis**: Systematically vary the α and β ratios in the hybrid loss function and the hard-negative sampling thresholds to identify optimal configurations and assess whether the reported performance is robust to parameter changes or represents a narrow optimal point.