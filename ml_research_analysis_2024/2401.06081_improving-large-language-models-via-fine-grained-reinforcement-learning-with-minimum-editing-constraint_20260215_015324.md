---
ver: rpa2
title: Improving Large Language Models via Fine-grained Reinforcement Learning with
  Minimum Editing Constraint
arxiv_id: '2401.06081'
source_url: https://arxiv.org/abs/2401.06081
tags:
- reward
- solution
- llms
- training
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving large language
  models (LLMs) for complex reasoning tasks, where existing reinforcement learning
  (RL) methods struggle to provide fine-grained supervision due to instance-level
  rewards. The proposed approach, RLMEC, introduces a generative reward model trained
  via an erroneous solution rewriting task with minimum editing constraints, enabling
  token-level reward signals.
---

# Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint

## Quick Facts
- arXiv ID: 2401.06081
- Source URL: https://arxiv.org/abs/2401.06081
- Authors: Zhipeng Chen; Kun Zhou; Wayne Xin Zhao; Junchen Wan; Fuzheng Zhang; Di Zhang; Ji-Rong Wen
- Reference count: 25
- One-line primary result: RLMEC outperforms strong RL baselines on mathematical and QA tasks with up to 2-4% accuracy gains through token-level supervision

## Executive Summary
This paper addresses the challenge of improving large language models (LLMs) for complex reasoning tasks where existing reinforcement learning methods struggle with instance-level rewards that provide insufficient supervision. The authors propose RLMEC, a fine-grained reinforcement learning approach that introduces a generative reward model trained via erroneous solution rewriting with minimum editing constraints. This reward model produces token-level rewards that enable the policy model to focus on key error-causing tokens, leading to significant improvements on mathematical reasoning (GSM8k, MATH, SVAMP, MMLU-MM) and question-answering tasks (ECQA, QASC, OBQA, ARC-Easy).

## Method Summary
RLMEC employs a two-stage approach: first, a generative reward model is trained to rewrite erroneous solutions into correct ones while minimizing token edits, creating token-level reward signals. The reward model uses a minimum editing constraint to focus on truly error-causing tokens. Then, a policy model is trained using Proximal Policy Optimization (PPO) with this token-level reward signal, combined with imitation-based regularization that emphasizes edited tokens through weighted loss based on Levenshtein distance. The method is evaluated on both mathematical reasoning and question-answering tasks, showing consistent improvements over strong baselines including SFT, RFT, DPO, and PPO.

## Key Results
- RLMEC achieves up to 2-4% absolute accuracy gains over strong baselines on mathematical reasoning tasks
- The method demonstrates better generalization to unseen tasks compared to traditional RL approaches
- Token-level supervision and imitation-based regularization are critical components, with ablation studies confirming their importance
- RLMEC shows reduced error propagation in reasoning steps compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
The generative reward model trained via erroneous solution rewriting with minimum editing constraint provides fine-grained token-level supervision signals. The reward model is trained to rewrite erroneous solutions into correct ones while minimizing the number of token edits. This creates a probability distribution over tokens where error-causing tokens have lower probabilities, which are then used as token-level rewards for RL training. The minimum editing constraint ensures that only truly error-causing tokens receive low probabilities, making the reward signal meaningful.

### Mechanism 2
The token-level RL objective with importance sampling and reward clipping stabilizes the learning process by focusing on error-causing tokens. The RL objective maximizes the expectation of generating correct tokens while using importance sampling to correct for distribution shift between policy and reference models. Reward clipping prevents extreme values that could destabilize training. This approach ensures the model learns effectively from the token-level rewards while maintaining training stability.

### Mechanism 3
The imitation-based regularization helps the model focus on learning the edited tokens by emphasizing them through weighted loss. After generating a solution, the model rewrites it to correct errors, and Levenshtein distance identifies which tokens were edited. These edited tokens receive higher weights in the regularization loss, forcing the model to focus on learning them. This targeted approach helps the model address specific weaknesses identified during the rewriting process.

## Foundational Learning

- **Concept**: Reinforcement Learning with Proximal Policy Optimization (PPO)
  - Why needed here: The paper builds on PPO framework to implement fine-grained RL training with token-level rewards
  - Quick check question: What is the key difference between vanilla policy gradient methods and PPO in terms of stability and performance?

- **Concept**: Sequence-to-sequence modeling with minimum editing distance
  - Why needed here: The reward model needs to rewrite erroneous solutions while minimizing edits, which requires understanding sequence generation and edit distance algorithms
  - Quick check question: How does minimum editing distance constraint affect the training dynamics of a sequence-to-sequence model compared to unconstrained generation?

- **Concept**: Credit assignment in reinforcement learning
  - Why needed here: The paper addresses the challenge of identifying which tokens (actions) are responsible for incorrect solutions (outcomes), which is a fundamental credit assignment problem
  - Quick check question: Why is credit assignment particularly challenging in complex reasoning tasks compared to simpler sequential decision problems?

## Architecture Onboarding

- **Component map**: Question ‚Üí Policy Model ‚Üí Generated Solution ‚Üí Generative Reward Model ‚Üí Token-level Rewards ‚Üí RL Update ‚Üí Improved Policy Model
- **Critical path**: Question ‚Üí Policy Model ‚Üí Generated Solution ‚Üí Generative Reward Model ‚Üí Token-level Rewards ‚Üí RL Update ‚Üí Improved Policy Model
- **Design tradeoffs**:
  - Using generative vs discriminative reward model: Generative allows natural token-level probabilities but requires more complex training
  - Minimum editing constraint: Ensures focused learning but may miss broader solution patterns
  - Imitation regularization: Provides stability but adds computational overhead
- **Failure signatures**:
  - Policy model generates nonsensical outputs: Likely issue with reward model training or token-level rewards
  - Training becomes unstable with large gradients: Probably reward clipping thresholds are too permissive
  - Model overfits to training data: May need more diverse teacher model data or stronger regularization
- **First 3 experiments**:
  1. Train reward model on synthetic data with known errors and verify it correctly identifies error tokens
  2. Test token-level rewards on a simple arithmetic task to ensure they correlate with solution correctness
  3. Run ablation study removing imitation regularization to measure its impact on training stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RLMEC scale with the size of the training dataset?
- Basis in paper: [inferred] The paper mentions that RLMEC outperforms other methods with a "limited number of training data," but does not provide a detailed analysis of how performance changes with dataset size.
- Why unresolved: The paper does not conduct experiments varying the amount of training data to observe the scaling behavior of RLMEC.
- What evidence would resolve it: Experiments training RLMEC with different sizes of training datasets (e.g., 10%, 50%, 100% of the original data) and comparing the accuracy on test sets would show how RLMEC scales with data size.

### Open Question 2
- Question: What is the impact of the editing distance threshold (ùõº and ùõΩ) on the performance of RLMEC?
- Basis in paper: [explicit] The paper mentions using ùõº = -0.1, ùõΩ = 0 for negative samples and ùõº = 0, ùõΩ = 0.5 for positive samples, but does not explore the impact of different threshold values.
- Why unresolved: The paper does not experiment with different values of ùõº and ùõΩ to determine their optimal settings or how they affect performance.
- What evidence would resolve it: Experiments varying ùõº and ùõΩ values (e.g., ùõº = -0.2, ùõΩ = 0.2; ùõº = -0.05, ùõΩ = 0.1) and comparing the accuracy on test sets would show the impact of these thresholds on RLMEC's performance.

### Open Question 3
- Question: How does the quality of the teacher model (e.g., Claude 2) affect the performance of RLMEC?
- Basis in paper: [explicit] The paper uses Claude 2 as the teacher model for generating the distilled data, but does not explore the impact of using different teacher models or varying the quality of the teacher model.
- Why unresolved: The paper does not experiment with different teacher models or varying the quality of the teacher model to determine its impact on RLMEC's performance.
- What evidence would resolve it: Experiments using different teacher models (e.g., GPT-4, smaller models) or varying the quality of the teacher model (e.g., using a weaker version of Claude 2) and comparing the accuracy on test sets would show how the teacher model quality affects RLMEC's performance.

## Limitations

- The effectiveness of the reward model heavily depends on the quality of erroneous solutions and their rewritten counterparts, with potential biases propagating through the training process
- The minimum editing constraint may oversimplify complex reasoning errors that involve multiple interacting mistakes, potentially missing important pedagogical patterns
- The method's effectiveness on non-mathematical reasoning domains (code generation, multi-step planning, creative writing) remains untested

## Confidence

**High Confidence (5/5)**: The token-level RL objective with importance sampling and reward clipping effectively improves performance on mathematical tasks.

**Medium Confidence (3/5)**: The imitation-based regularization meaningfully stabilizes RL training and improves generalization to unseen tasks.

**Low Confidence (2/5)**: The minimum editing constraint in the reward model training is the critical factor enabling fine-grained supervision.

## Next Checks

1. **Reward model quality audit**: Generate 100 erroneous solutions and their rewritten versions from the trained reward model. Have human experts (or GPT-4 as a proxy) evaluate whether the minimum editing constraint correctly identifies and fixes only the truly error-causing tokens, or whether it sometimes makes unnecessary changes or misses critical errors.

2. **Error propagation analysis**: For a subset of problems where the base model fails, trace through the solution steps to identify where errors occur. Compare this with the token-level rewards assigned by the reward model to verify that error-causing tokens receive appropriately low rewards, and that the RL training successfully corrects these specific errors in the final model.

3. **Ablation of editing constraint**: Train an alternative reward model without the minimum editing constraint (allowing more extensive rewriting) and compare its effectiveness at identifying error-causing tokens and improving downstream RL performance. This would directly test whether the minimum editing constraint is truly beneficial or whether more flexible rewriting might capture complex error patterns better.