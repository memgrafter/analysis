---
ver: rpa2
title: Towards Accurate and Robust Architectures via Neural Architecture Search
arxiv_id: '2405.05502'
source_url: https://arxiv.org/abs/2405.05502
tags:
- search
- robust
- accuracy
- adversarial
- architectures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of designing accurate and robust
  neural architectures for adversarial training by proposing a method called ARNAS.
  The core idea involves constructing a novel search space that combines Accurate
  Cells, Robust Cells, and Reduction Cells, with specific placements and filter number
  proportions determined through experiments.
---

# Towards Accurate and Robust Architectures via Neural Architecture Search

## Quick Facts
- arXiv ID: 2405.05502
- Source URL: https://arxiv.org/abs/2405.05502
- Reference count: 40
- One-line primary result: Proposes ARNAS method that achieves superior accuracy-robustness tradeoff through novel search space design and multi-objective optimization

## Executive Summary
This paper addresses the challenge of designing neural architectures that are both accurate and robust to adversarial attacks. The authors propose ARNAS, a method that constructs a specialized search space combining Accurate Cells, Robust Cells, and Reduction Cells with specific placements. They introduce a differentiable multi-objective search strategy based on MGDA to optimize both natural and adversarial losses simultaneously. The method is evaluated on CIFAR-10, CIFAR-100, SVHN, and Tiny-ImageNet-200, demonstrating superior robustness and accuracy compared to state-of-the-art architectures under various attacks while maintaining competitive natural accuracy.

## Method Summary
ARNAS constructs a novel search space that combines Accurate Cells (placed before Reduction Cells), Robust Cells (placed after Reduction Cells), and Reduction Cells with specific filter number proportions. The method uses a differentiable multi-objective search strategy based on MGDA to simultaneously optimize natural and adversarial losses. The search process involves initializing a supernet based on the constructed search space and iteratively optimizing it through gradient descent towards directions beneficial for both objectives. After search completion, a discretization rule converts the supernet to the final architecture.

## Key Results
- Achieves highest adversarial accuracy under PGD 20, PGD 100, and AutoAttack while maintaining competitive natural accuracy
- Demonstrates superior robustness and accuracy compared to state-of-the-art architectures on CIFAR-10, CIFAR-100, SVHN, and Tiny-ImageNet-200
- Shows successful transferability to Tiny-ImageNet-200, breaking the traditional prejudice that NAS-based architectures cannot transfer well as task complexity increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed search space design, which places Accurate Cells before Reduction Cells and Robust Cells after, improves both accuracy and robustness simultaneously.
- Mechanism: Different cell types in different positions contribute to accuracy and robustness respectively, as supported by Proposition 1 and experimental validation in Section 4.4.5.
- Core assumption: The conjecture that cell structures in different positions play different roles for accuracy and robustness is valid and can be exploited through architectural design.
- Evidence anchors:
  - [abstract] "accurate and robust neural architectures tend to deploy different structures near the input and output"
  - [section] "we further conjecture that the architectures themselves in different positions also play different roles"
  - [corpus] No direct evidence, but related works on robust NAS support the importance of architecture design
- Break condition: If the conjecture is false or if the optimal cell placement does not follow the proposed pattern, the design would fail to improve both metrics.

### Mechanism 2
- Claim: The differentiable multi-objective search strategy based on MGDA effectively optimizes for both natural and adversarial losses.
- Mechanism: Dynamically determining weights for natural and adversarial losses through MGDA allows simultaneous optimization, finding a common descent direction beneficial for both objectives.
- Core assumption: Finding a common descent direction for multiple objectives is more effective than using fixed regularization coefficients for identifying the Pareto front.
- Evidence anchors:
  - [abstract] "we propose a differentiable multi-objective search strategy, performing gradient descent towards directions that are beneficial for both natural loss and adversarial loss"
  - [section] "scholars studying multi-objective optimization have shown that always finding a descent direction common to all criteria may be better for identifying the Pareto front"
  - [corpus] Limited direct evidence, but MGDA is a known method in multi-objective optimization literature
- Break condition: If the common descent direction approach does not lead to better Pareto front identification or if the dynamic weight determination fails to converge.

### Mechanism 3
- Claim: The proposed search space with a special proportional relationship of channels improves robustness without significantly sacrificing accuracy.
- Mechanism: Keeping the sum of channel numbers similar to conventional search space while significantly increasing the product of channel numbers results in architectures with larger FLOPs, which improves robustness.
- Core assumption: The number of parameters and FLOPs are both factors that affect robustness, and the proposed channel proportion design effectively leverages this relationship.
- Evidence anchors:
  - [section] "the number of parameters and the FLOPs are both the factors that affect the robustness"
  - [section] "the large FLOPs are essentially caused by the special proportional relationship of channels designed in the proposed search space"
  - [corpus] No direct evidence, but the relationship between model capacity and robustness is discussed in related works
- Break condition: If the increased FLOPs do not lead to improved robustness or if the accuracy is significantly sacrificed due to the special channel proportion.

## Foundational Learning

- Concept: Adversarial training and its role in improving robustness
  - Why needed here: ARNAS relies on adversarial training to improve the robustness of the searched architectures
  - Quick check question: What is the main difference between standard training and adversarial training?

- Concept: Neural Architecture Search (NAS) and its application to robust architectures
  - Why needed here: ARNAS is a method for searching accurate and robust architectures using NAS techniques
  - Quick check question: What are the main categories of NAS based on search strategy?

- Concept: Multi-objective optimization and Pareto front identification
  - Why needed here: ARNAS uses a multi-objective approach to optimize for both accuracy and robustness simultaneously
  - Quick check question: What is the advantage of finding a common descent direction for multiple objectives compared to using fixed regularization coefficients?

## Architecture Onboarding

- Component map:
  - Accurate Cell -> Reduction Cell -> Reduction Cell -> Robust Cell

- Critical path:
  1. Construct the accurate and robust search space
  2. Initialize a supernet based on the constructed search space
  3. Iteratively optimize the supernet using the differentiable multi-objective search strategy
  4. Apply the discretization rule of DARTS to obtain the final architecture

- Design tradeoffs:
  - Accuracy vs. Robustness: The proposed method aims to improve both metrics simultaneously, but there may still be a tradeoff
  - Computational cost: The special channel proportion design leads to larger FLOPs, which may increase computational cost
  - Search space complexity: The proposed search space is more complex than conventional search spaces, which may affect search efficiency

- Failure signatures:
  - Poor accuracy: If the searched architecture has significantly lower accuracy than peer competitors
  - Poor robustness: If the searched architecture has significantly lower robustness than peer competitors under various attacks
  - Inefficient search: If the search process takes significantly longer than conventional NAS methods or fails to converge

- First 3 experiments:
  1. Verify the effectiveness of the proposed search space by comparing the accuracy and robustness of architectures searched from the proposed space versus conventional spaces
  2. Evaluate the performance of the differentiable multi-objective search strategy by comparing the accuracy and robustness of architectures searched using this strategy versus fixed regularization coefficient approaches
  3. Test the transferability of the searched architecture to other datasets and tasks to validate the generalizability of the proposed method

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The search space design's effectiveness relies heavily on an unproven conjecture about cell placement that lacks strong theoretical justification
- The multi-objective optimization approach shows limited direct evidence in the NAS literature for achieving better Pareto front identification compared to fixed regularization methods
- The increased FLOPs from the special channel proportion design may limit practical deployment in resource-constrained settings

## Confidence
- Search space design and cell placement mechanism: Medium confidence - supported by Proposition 1 and experimental validation but based on an unproven conjecture
- Multi-objective optimization effectiveness: Medium confidence - MGDA is theoretically sound but lacks direct NAS-specific validation
- Transferability claims: Low-Medium confidence - based on limited experiments showing improved performance on Tiny-ImageNet-200

## Next Checks
1. Conduct ablation studies to verify the necessity of the specific cell placement pattern (Accurate before Reduction, Robust after) by comparing with alternative placement strategies
2. Compare the MGDA-based multi-objective optimization against strong baselines using fixed regularization coefficients on the same search space to isolate the optimization method's contribution
3. Test the searched architecture on additional datasets with varying complexity to better validate the transferability claims beyond the single Tiny-ImageNet-200 experiment