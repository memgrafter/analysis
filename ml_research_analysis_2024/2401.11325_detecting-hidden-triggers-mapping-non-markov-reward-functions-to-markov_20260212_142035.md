---
ver: rpa2
title: 'Detecting Hidden Triggers: Mapping Non-Markov Reward Functions to Markov'
arxiv_id: '2401.11325'
source_url: https://arxiv.org/abs/2401.11325
tags:
- reward
- learning
- armdp
- task
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel method for learning non-Markov reward
  functions as equivalent Markov representations using specialized reward automata,
  Reward Machines (RMs). The authors propose a framework that maps observed non-Markov
  rewards onto Abstract Reward Markov Decision Processes (ARMDPs) by solving an Integer
  Linear Programming problem.
---

# Detecting Hidden Triggers: Mapping Non-Markov Reward Functions to Markov

## Quick Facts
- **arXiv ID**: 2401.11325
- **Source URL**: https://arxiv.org/abs/2401.11325
- **Reference count**: 40
- **Primary result**: A novel method for learning non-Markov reward functions as equivalent Markov representations using specialized reward automata (Reward Machines) via Integer Linear Programming.

## Executive Summary
This paper presents a novel method for learning non-Markov reward functions as equivalent Markov representations using specialized reward automata, Reward Machines (RMs). The authors propose a framework that maps observed non-Markov rewards onto Abstract Reward Markov Decision Processes (ARMDPs) by solving an Integer Linear Programming problem. Unlike previous approaches, their method learns hidden triggers directly from data without requiring high-level propositional symbols or a labeling function. The ARMDP acts as a suitable proxy for maximizing reward expectations under non-Markov rewards. The authors validate their approach by demonstrating the ability to learn black-box RMs from the Officeworld domain and show the effectiveness of learning reward dependencies in a new domain, Breakfastworld. They also compare the performance of DQNs using ARMDP features against various sparse RDQN variants, showing improved learning profiles.

## Method Summary
The paper proposes a method for learning non-Markov reward functions as equivalent Markov representations using specialized reward automata (Reward Machines). The approach involves mapping observed non-Markov rewards onto Abstract Reward Markov Decision Processes (ARMDPs) by solving an Integer Linear Programming problem. The method learns hidden triggers directly from data without requiring high-level propositional symbols or a labeling function. The ARMDP acts as a proxy for maximizing reward expectations under non-Markov rewards. The authors validate their approach on Officeworld and Breakfastworld domains, comparing DQNs using ARMDP features against various sparse RDQN variants.

## Key Results
- Successfully learned black-box RMs from the Officeworld domain without requiring high-level propositional symbols or labeling functions
- Demonstrated the ability to model reward dependencies in a single automaton in the Breakfastworld domain
- Showed improved learning profiles for DQNs using ARMDP features compared to various sparse RDQN variants

## Why This Works (Mechanism)

### Mechanism 1
The ARMDP maps non-Markov rewards into Markov representations by learning hidden triggers through an ILP that resolves reward conflicts. Hidden triggers are detected by constructing binary variables Is,a,r,s′,i,j that capture transitions with unique reward outcomes. The ILP enforces determinism of both transition and reward functions by ensuring each observed transition or reward tuple maps to at most one state pair in the hidden space U. This works under the assumption that observed trajectories To are sufficiently representative of the true reward dynamics, and the underlying reward function can be modeled by a deterministic RM. If To contains insufficient or unrepresentative samples, the inferred ARMDP may not capture the full reward structure, leading to poor generalization.

### Mechanism 2
Learning RMs from H = (S × A × R)* instead of H = (S × A)* enables modeling of reward dependencies in a single automaton. By incorporating rewards into the history space, the ILP can detect upstream reward conflicts that inform downstream ones. This allows the learned RM to encode reward dependencies compactly, avoiding the need for multiple DFAs. This mechanism relies on the assumption that reward dependencies exist and can be expressed as regular patterns over (S × A × R)*. If rewards are independent or if the reward function cannot be expressed as a regular language, the benefit of this mechanism diminishes.

### Mechanism 3
ARMDPQ-Learning uses the inferred ARMDP as a hypothesis that is actively tested and refined by a Q-learning agent. The agent simulates trajectories under the current ARMDP hypothesis; when a reward conflict is detected, the conflicting trajectory is added to To and a new ARMDP is solved. This active interrogation expands To until no further conflicts arise, converging to an optimal policy. This mechanism assumes that Q-learning can effectively explore the environment to uncover reward conflicts not present in the initial To. If the exploration policy is too greedy or the environment is too large, conflicts may remain undetected, stalling convergence.

## Foundational Learning

- **Concept: Markov Decision Process (MDP)**
  - Why needed here: The ARMDP is an MDP over the product space S × U, enabling the use of standard RL algorithms once hidden triggers are learned.
  - Quick check question: What properties must hold for an MDP to guarantee convergence of Q-learning?

- **Concept: Deterministic Finite-State Automaton (DFA) vs. Reward Machine (RM)**
  - Why needed here: Understanding the difference explains why RMs can model reward dependencies in one automaton while DFAs cannot.
  - Quick check question: How does an RM's ability to emit rewards on transitions differ from a DFA's accept/reject behavior?

- **Concept: Integer Linear Programming (ILP) formulation**
  - Why needed here: The ILP encodes the constraints for mapping trajectories to an ARMDP while minimizing hidden triggers.
  - Quick check question: What constraints ensure that each transition in the ARMDP is deterministic and reward conflicts are resolved?

## Architecture Onboarding

- **Component map**: Trajectory Collector → ILP Solver → ARMDP Generator → Q-Learning Agent → Conflict Detector → (back to Trajectory Collector)
- **Critical path**: 1. Collect initial trajectories To from NMRDP. 2. Solve ILP to infer minimal ARMDP. 3. Train Q-learning agent on ARMDP. 4. Detect conflicts during exploration. 5. Expand To and re-solve ILP. 6. Repeat until convergence.
- **Design tradeoffs**: Using j ≥ i constraint reduces ILP complexity but may unroll cycles, increasing |U|. Passive ILP learning is fast but may miss dynamics; active Q-learning is thorough but slower. Discrete state assumption simplifies ARMDP learning but limits continuous applications.
- **Failure signatures**: ILP infeasibility → increase |U| or check data consistency. Agent fails to converge → check if To is representative or if exploration is too greedy. Large |U| → revisit j ≥ i assumption or check for excessive cycle unrolling.
- **First 3 experiments**: 1. Run ARMDPQ-Learning on a simple Officeworld task with known RM and verify |U| matches ground truth. 2. Compare ADQN vs. RDQN learning curves on a task with sparse rewards. 3. Test ILP scalability by increasing trajectory length and measuring solve time.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the limitations section, potential open questions include:
- How to handle stochastic or noisy reward functions where rewards are not deterministic for given histories
- How to scale the approach to larger state and action spaces where ILP solving becomes computationally intractable
- How to handle continuous state representations beyond the tabular setting

## Limitations
- The paper assumes R to be deterministic to some history, which may not hold in real-world scenarios
- The approach may not scale well to larger state and action spaces due to ILP complexity
- The paper only tests on relatively small domains (Officeworld and Breakfastworld) and doesn't address scalability concerns for larger problems

## Confidence
- **Core mechanism**: Medium - The ILP formulation is clearly specified and the ARMDP framework is theoretically sound, but empirical validation is limited to two domains with synthetic reward structures
- **Outperformance claim**: Low - While learning curves are shown, no statistical significance tests are reported, and baseline implementations are not fully described
- **Scalability**: Low - The paper mentions using out-of-the-box ILP solvers and discusses solve times for different tasks, noting that some tasks required thousands of seconds to solve, but doesn't provide a theoretical analysis of ILP complexity as state/action spaces grow

## Next Checks
1. Test the ILP scalability by generating trajectories of increasing length and measuring solve time and memory usage.
2. Conduct ablation studies comparing passive ILP learning (fixed To) against active ARMDPQ-Learning across multiple random seeds.
3. Apply the method to a domain with continuous states (e.g., Atari) using a learned state encoder and evaluate performance degradation.