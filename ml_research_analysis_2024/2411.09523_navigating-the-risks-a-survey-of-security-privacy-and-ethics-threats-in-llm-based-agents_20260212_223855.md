---
ver: rpa2
title: 'Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in
  LLM-Based Agents'
arxiv_id: '2411.09523'
source_url: https://arxiv.org/abs/2411.09523
tags:
- arxiv
- agents
- llm-based
- preprint
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive analysis of security, privacy,
  and ethics threats in LLM-based agents, which are AI systems that use large language
  models as their core decision-making component. The paper identifies six key features
  of these agents and proposes a novel taxonomy framework based on the sources and
  impacts of threats.
---

# Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents

## Quick Facts
- arXiv ID: 2411.09523
- Source URL: https://arxiv.org/abs/2411.09523
- Reference count: 40
- Key outcome: This survey provides a comprehensive analysis of security, privacy, and ethics threats in LLM-based agents, identifying six key features and proposing a novel taxonomy framework based on threat sources and impacts.

## Executive Summary
This survey comprehensively analyzes security, privacy, and ethics threats in LLM-based agents, which are AI systems that use large language models as their core decision-making component. The paper identifies six key features of these agents and proposes a novel taxonomy framework based on the sources and impacts of threats. Through detailed case studies of four representative agents (WebGPT, Voyager, PReP, and ChatDev), the survey demonstrates how different key features impact the severity of various threats. The work highlights limitations in current research and proposes future directions for data support, methodological improvements, and policy development.

## Method Summary
The survey conducts a comprehensive literature review from top conferences (IEEE S&P, ACM CCS, USENIX Security, NDSS, ACL, CVPR, NIPS, ICML, ICLR) and highly cited arXiv papers. The authors classify threats using a novel binary taxonomy framework based on sources (inputs, model, or input-model interaction) versus impacts (security/safety, privacy, or ethics). Four representative agents are analyzed as case studies to demonstrate how the six identified key features affect threat severity in practice.

## Key Results
- Proposes a novel taxonomy framework that effectively addresses cross-module and cross-stage threats by mapping threats to sources and impacts
- Identifies six key features of LLM-based agents: LLM-based controller, multi-modal inputs/outputs, multi-source inputs, multi-round interaction, memory mechanism, and tool invocation
- Demonstrates through case studies how different key features impact threat severity in four representative agents (WebGPT, Voyager, PReP, ChatDev)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed taxonomy effectively addresses cross-module and cross-stage threats by mapping threats to a binary table based on their sources (inputs, model, or input-model interaction) and impacts (security/safety, privacy, or ethics).
- Mechanism: By classifying threats according to their fundamental sources rather than their operational modules or stages, the taxonomy can accurately pinpoint threats that span multiple modules or stages. For example, privacy leakage is attributed to both model flaws (parametric memorization) and input-model interaction (contextual memorization), allowing for more precise categorization and analysis.
- Core assumption: The fundamental sources of threats are inputs, the model itself, and their interaction, rather than the operational modules or stages where the threats manifest.
- Evidence anchors:
  - [abstract]: "To address the challenges posed by previous taxonomies in handling cross-module and cross-stage threats, we propose a novel taxonomy framework based on the sources and impacts."
  - [section]: "For the sources of threats, we consider the operational nature of LLM-based agents: LLMs make decisions based on inputs from multiple sources... we attribute the threats to LLM-based agents to the inputs, the model, or a combination of both."
- Break condition: If a threat arises from a source not accounted for by the three categories (inputs, model, or input-model interaction), the taxonomy would fail to accurately classify it.

### Mechanism 2
- Claim: The case studies of four representative agents (WebGPT, Voyager, PReP, and ChatDev) effectively illustrate how different key features impact the severity of various threats.
- Mechanism: By selecting agents with diverse characteristics (e.g., WebGPT with complete components, Voyager with embodied gaming focus, PReP with multimodal inputs, and ChatDev with multi-agent framework), the case studies demonstrate how specific features like multi-source inputs, multi-modal interactions, and tool invocation can increase or decrease the severity of different threats. This provides concrete examples of how the theoretical taxonomy applies in practice.
- Core assumption: The selected agents represent a diverse enough range of LLM-based agent characteristics to effectively illustrate the impact of different key features on various threats.
- Evidence anchors:
  - [section]: "To help readers better understand the actual threats faced by agents, we present case studies of four different agents, representing four classic situations in Section 6."
  - [section]: "WebGPT [189], an agent with the complete components working for the general question-answering tasks; Voyager [263], an embodied agent working for playing games; PReP [310], an embodied agents working for the real-word tasks; ChatDev [212], a multi-agent framework."
- Break condition: If the selected agents do not adequately represent the diversity of LLM-based agent characteristics, the case studies may not effectively illustrate the impact of different key features on various threats.

### Mechanism 3
- Claim: The proposed taxonomy and case studies provide a comprehensive resource for researchers and practitioners to understand and address the risks associated with LLM-based agents.
- Mechanism: By offering a novel taxonomy that accurately categorizes threats and detailed case studies that illustrate their practical implications, the survey provides a comprehensive framework for understanding the risks of LLM-based agents. This framework can guide future research and development efforts in mitigating these risks.
- Core assumption: The combination of a novel taxonomy and detailed case studies provides a more comprehensive understanding of LLM-based agent risks than previous surveys that relied on module or stage-based categorizations.
- Evidence anchors:
  - [abstract]: "This survey serves as a valuable resource for researchers and practitioners to understand and address the risks associated with LLM-based agents."
  - [section]: "Compared with recent surveys [53, 61] on the security risks of LLM-based agents, there are three main advantages of our work."
- Break condition: If the taxonomy or case studies fail to accurately capture the full range of risks associated with LLM-based agents, the survey may not provide a comprehensive resource for researchers and practitioners.

## Foundational Learning

- Concept: Understanding of LLM-based agents and their key features (LLM-based controller, multi-modal inputs and outputs, multi-source inputs, multi-round interaction, memory mechanism, and tool invocation).
  - Why needed here: The paper's analysis of threats is based on how these key features impact the severity of different risks. Without understanding these features, it's difficult to grasp the paper's core arguments.
  - Quick check question: What are the six key features of LLM-based agents identified in the paper, and how do they differ from traditional AI agents?

- Concept: Familiarity with the different types of threats to LLM-based agents (security/safety, privacy, and ethics).
  - Why needed here: The paper's taxonomy categorizes threats based on their impacts. Understanding these impact categories is crucial for comprehending the taxonomy's structure and purpose.
  - Quick check question: How does the paper categorize threats based on their impacts, and what are some examples of each category?

- Concept: Knowledge of the different sources of threats to LLM-based agents (inputs, model, or input-model interaction).
  - Why needed here: The paper's taxonomy classifies threats based on their fundamental sources. Understanding these source categories is essential for grasping the taxonomy's logic and advantages over previous taxonomies.
  - Quick check question: What are the three sources of threats to LLM-based agents according to the paper's taxonomy, and how do they differ from the module or stage-based categorizations used in previous surveys?

## Architecture Onboarding

- Component map: Introduction -> Framework Overview -> Key Features Analysis -> Threat Categorization (Problematic Inputs, Model Flaws, Input-Model Interaction) -> Case Studies (WebGPT, Voyager, PReP, ChatDev) -> Future Directions
- Critical path: The critical path for understanding the survey's contributions involves grasping the limitations of previous taxonomies, comprehending the novel taxonomy based on sources and impacts, and appreciating how the case studies demonstrate the taxonomy's practical application.
- Design tradeoffs: The survey prioritizes a comprehensive and accurate categorization of threats over a simple module or stage-based approach. This tradeoff allows for more precise analysis of cross-module and cross-stage threats but may require more effort to understand the taxonomy's structure.
- Failure signatures: If the taxonomy fails to accurately categorize a threat or if the case studies do not effectively illustrate the taxonomy's practical application, the survey's contributions may be undermined.
- First 3 experiments:
  1. Apply the proposed taxonomy to a new threat not discussed in the survey and evaluate its effectiveness in categorization.
  2. Analyze a real-world LLM-based agent not covered in the case studies using the proposed taxonomy and identify potential risks.
  3. Propose a mitigation strategy for a specific threat based on the taxonomy and evaluate its effectiveness in addressing the threat's source and impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a comprehensive mathematical framework for quantifying and analyzing hallucinations in LLM-based agents?
- Basis in paper: [explicit] The paper states "Currently, there is a lack of mathematical definitions for hallucinations, and the methods for evaluating and mitigating them are primarily validated through experiments, lacking theoretical analysis."
- Why unresolved: The absence of rigorous mathematical definitions makes it difficult to compare hallucination severity across different agents and scenarios.
- What evidence would resolve it: A formal mathematical model that can predict hallucination probabilities based on input characteristics, agent architecture, and environmental factors.

### Open Question 2
- Question: What defense strategies can effectively protect LLM-based agents against multimodal jailbreaking attacks?
- Basis in paper: [explicit] The paper notes "Current research mainly focuses on text and image processing, but the capabilities of LLM-based agents to handle audio and video content are also growing quickly. These new modalities could introduce unique security risks."
- Why unresolved: Most current jailbreak defenses focus on single modalities or text-only scenarios, leaving agents vulnerable to sophisticated multimodal attacks.
- What evidence would resolve it: Demonstrated defense mechanisms that can detect and prevent jailbreak attempts across text, image, audio, and video modalities simultaneously.

### Open Question 3
- Question: How can we design collaborative defense strategies that protect the entire LLM-based agent framework rather than individual components?
- Basis in paper: [explicit] The paper states "Current defense strategies against backdoor attacks on LLM-based AI agents typically focus on safeguarding individual components, such as dataset sanitation and input data purification. However, there is a clear lack of systematic defenses that can protect against backdoor attacks originating from multiple sources and targeting various objectives."
- Why unresolved: Existing defenses treat each module in isolation rather than considering the interconnected nature of agent components.
- What evidence would resolve it: A unified defense framework that coordinates protection across input modules, decision modules, memory systems, and external tool interfaces.

## Limitations

- The survey may have missed emerging threats that materialized after the literature review period, given the rapidly evolving nature of LLM-based agents.
- The binary source-impact classification system, while novel, may not capture the full complexity of threat interactions, particularly for sophisticated multi-stage attacks.
- The selection of four case studies, though diverse, represents a small fraction of the growing ecosystem of LLM-based agents, potentially limiting generalizability.

## Confidence

- **High Confidence**: The identification of six key features of LLM-based agents is well-supported by literature review and represents a clear contribution.
- **Medium Confidence**: The taxonomy framework based on sources and impacts provides a logical structure, but its effectiveness in practice requires further validation across diverse agent architectures.
- **Medium Confidence**: The analysis of specific threats (adversarial examples, jailbreaking, hallucination, etc.) draws from established literature, though threat severity assessments may evolve as new attack techniques emerge.

## Next Checks

1. Apply the proposed taxonomy to classify threats in three additional LLM-based agents not covered in the survey and evaluate classification accuracy and completeness.
2. Conduct a systematic review of threats discovered in the six months following the survey's completion to assess whether the taxonomy captures emerging attack vectors.
3. Design and execute controlled experiments to measure the actual impact of the six key features on threat severity across multiple agent architectures.