---
ver: rpa2
title: Finding good policies in average-reward Markov Decision Processes without prior
  knowledge
arxiv_id: '2405.17108'
source_url: https://arxiv.org/abs/2405.17108
tags:
- algorithm
- policy
- complexity
- optimal
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of finding an \u03B5-optimal\
  \ policy in average-reward Markov Decision Processes (MDPs) without prior knowledge\
  \ of the MDP. The authors propose a novel algorithm called Diameter Free Exploration\
  \ (DFE) that combines diameter estimation with state-of-the-art policy identification\
  \ techniques."
---

# Finding good policies in average-reward Markov Decision Processes without prior knowledge

## Quick Facts
- **arXiv ID**: 2405.17108
- **Source URL**: https://arxiv.org/abs/2405.17108
- **Reference count**: 40
- **Primary result**: First algorithm for (ε,δ)-PAC policy identification in average-reward MDPs without prior knowledge of the MDP

## Executive Summary
This paper tackles the fundamental challenge of finding an ε-optimal policy in average-reward Markov Decision Processes (MDPs) without any prior knowledge of the MDP parameters. The authors identify that estimating the optimal bias span H, a key complexity measure, is computationally intractable in polynomial time. To overcome this barrier, they propose the Diameter Free Exploration (DFE) algorithm that first estimates the MDP diameter D (an upper bound on H) and then uses this estimate to identify an ε-optimal policy. The algorithm achieves near-optimal sample complexity of O(SAD/ε² log(1/δ)) in the generative model setting, establishing the first (ε,δ)-PAC algorithm without requiring prior knowledge.

## Method Summary
The method combines diameter estimation with policy identification techniques. First, Algorithm 2 estimates the MDP diameter D using an extended value iteration procedure inspired by SSP-MDP literature, with high probability bounds ensuring D ≤ ˆD ≤ 4D. Then, Algorithm 3 uses this diameter estimate as an upper bound on H to identify an ε-optimal policy through a reduction to discounted MDPs. The complete DFE algorithm (Algorithm 1) integrates these components with a data-dependent stopping rule that guarantees PAC correctness for any sampling strategy. The stopping rule monitors confidence intervals on quantities Is,a(n) = rs,a + ps,a bn - bn, where bn is a bias vector sequence, and terminates when the span of these quantities is sufficiently small.

## Key Results
- Proves that estimating the optimal bias span H tightly is not feasible in polynomial time, establishing fundamental hardness for average-reward MDP learning
- Proposes DFE, the first (ε,δ)-PAC policy identification algorithm without prior knowledge of the MDP, achieving near-optimal sample complexity O(SAD/ε² log(1/δ)) in the generative model
- Establishes a lower bound showing that no online algorithm can achieve sample complexity polynomial in S, A, and H
- Introduces a novel adaptive stopping rule with PAC guarantees that can be used with different sampling strategies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The sample complexity required to estimate the optimal bias span H tightly is not polynomial in H, S, and A.
- **Mechanism**: The hardness arises from a family of MDPs where distinguishing whether R > 1/2 or R < 1/2 requires an exponentially large number of samples when R is very close to 1/2.
- **Core assumption**: The MDP is weakly communicating with a fixed optimal bias span of 1/2.
- **Evidence anchors**:
  - [abstract]: "We first show that the sample complexity required to estimate H is not bounded by any function of S, A and H"
  - [section 3]: Proof using MDPMR where R < 1/2 makes full-line policy optimal with small H, while R > 1/2 makes dashed-line policy optimal with large H
  - [corpus]: No direct evidence in corpus neighbors, but related to hardness results in SSP-MDPs
- **Break condition**: If the MDP has a structure where R is bounded away from 1/2 or if prior bounds on H are available, this hardness may not apply.

### Mechanism 2
- **Claim**: Diameter estimation can replace bias span estimation for near-optimal policy identification.
- **Mechanism**: DFE first estimates the MDP diameter D (which is an upper bound on H) using a procedure inspired by SSP-MDP literature, then uses this estimate in an algorithm that requires H as input.
- **Core assumption**: The MDP is communicating, allowing diameter estimation to be performed.
- **Evidence anchors**:
  - [abstract]: "By relying instead on a diameter estimation procedure...we propose the first algorithm for (ε, δ)-PAC policy identification that does not need any form of prior knowledge on the MDP"
  - [section 4]: Algorithm 1 combines Algorithm 2 (diameter estimation) with Algorithm 3 (policy identification using H)
  - [corpus]: Related to diameter-based approaches in [9, 2, 3] for regret minimization
- **Break condition**: If the MDP is not communicating or if diameter estimation fails to provide a good enough upper bound on H.

### Mechanism 3
- **Claim**: A data-dependent stopping rule can guarantee PAC correctness for any sampling rule in AR-MDPs.
- **Mechanism**: The stopping rule uses confidence intervals on quantities Is,a(n) = rs,a + ps,a bn - bn, where bn is a bias vector sequence. When the span of these quantities is small enough, the associated policy is guaranteed to be ε-optimal.
- **Core assumption**: The MDP is weakly communicating, ensuring the existence of optimal policies satisfying Poisson equations.
- **Evidence anchors**:
  - [abstract]: "we further propose a novel approach based on a data-dependent stopping rule"
  - [section 6.2]: Theorem 6 proves PAC correctness for any sampling rule using stopping rule (4)
  - [corpus]: Related to value iteration stopping conditions and KL-based concentration from [1]
- **Break condition**: If the sampling rule fails to explore sufficiently or if the bias vector sequence bn is poorly chosen.

## Foundational Learning

- **Concept**: Average-reward MDPs and optimal bias span H
  - Why needed here: H is the key complexity measure that determines sample complexity bounds, but estimating it is hard
  - Quick check question: What is the relationship between H and the diameter D in an MDP?

- **Concept**: Poisson equations and bias vectors
  - Why needed here: The stopping rule and algorithm correctness rely on properties of bias vectors and Poisson equations
  - Quick check question: How does the optimal bias span H relate to the bias vector b⋆?

- **Concept**: KL divergence and concentration inequalities
  - Why needed here: Used to build confidence intervals for transition probabilities in the stopping rule
  - Quick check question: How does Pinsker's inequality relate to the relaxed confidence intervals in the stopping rule?

## Architecture Onboarding

- **Component map**: MDP parameters -> Diameter estimation (Algorithm 2) -> Policy identification (Algorithm 3) -> Stopping rule (Theorem 5) -> ε-optimal policy

- **Critical path**: 
  1. Estimate diameter D using Algorithm 2
  2. Use D as upper bound on H in Algorithm 3
  3. Collect samples and apply stopping rule (4)
  4. Output policy when stopping condition is met

- **Design tradeoffs**:
  - Using diameter D instead of H: Loses tightness but gains prior-knowledge-freeness
  - Generative vs online model: Generative allows more flexible sampling, online requires sequential decisions
  - Uniform vs adaptive sampling: Uniform is simpler but may be less efficient

- **Failure signatures**:
  - High sample complexity: May indicate diameter estimation is loose or MDP is hard to explore
  - Algorithm never stops: Could be due to poor choice of bias vector sequence bn or insufficient exploration
  - Output policy not ε-optimal: May indicate confidence intervals are too loose or MDP assumptions violated

- **First 3 experiments**:
  1. Test diameter estimation on small MDPs where D is known to verify Algorithm 2 correctness
  2. Compare sample complexity of DFE vs algorithm requiring H on MDPs with known H
  3. Implement and test the stopping rule (4) with uniform sampling on simple unichain MDPs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can we find an online algorithm with sample complexity scaling as SAD/ε² for (ε, δ)-PAC policy identification in average-reward MDPs?
- **Basis in paper**: [explicit] The paper establishes a lower bound showing that no online algorithm can achieve a sample complexity polynomial in S, A, and H, and proposes an online variant of DFE with SAD²/ε² sample complexity.
- **Why unresolved**: The gap between the lower bound SAD/ε² and the current best online algorithm's SAD²/ε² complexity remains open.
- **What evidence would resolve it**: Either a proof that no online algorithm can achieve SAD/ε² complexity, or an algorithm that achieves this bound.

### Open Question 2
- **Question**: How can we design more adaptive online sampling rules that can potentially reduce the sample complexity below SAD²/ε²?
- **Basis in paper**: [inferred] The paper discusses the potential of using adaptive stopping rules and mentions that clever online choices of the bias vector bn could minimize the stopping criterion.
- **Why unresolved**: While the paper introduces a novel adaptive stopping rule, it doesn't provide a complete online algorithm using it.
- **What evidence would resolve it**: An online algorithm that uses the proposed stopping rule with an efficient adaptive sampling strategy and achieves a sample complexity better than SAD²/ε².

### Open Question 3
- **Question**: What is the exact relationship between the diameter D and the optimal bias span H in terms of their impact on sample complexity for (ε, δ)-PAC policy identification?
- **Basis in paper**: [explicit] The paper shows that estimating H tightly is not feasible in finite time, leading to the use of diameter D as a complexity measure.
- **Why unresolved**: While the paper demonstrates that using D instead of H leads to near-optimal algorithms, the precise relationship between these measures in terms of sample complexity remains unclear.
- **What evidence would resolve it**: A comprehensive analysis comparing the sample complexities of algorithms using D versus H across various MDP classes, or a theoretical result quantifying the relationship between D and H in terms of sample complexity bounds.

## Limitations

- Fundamental hardness of H estimation: The paper proves that estimating H tightly is not feasible in polynomial time, which is a significant limitation for average-reward MDP learning
- Diameter approximation factor: Using diameter D instead of H introduces a potential looseness factor of 4 in the sample complexity bound
- Online algorithm gap: The online variant of DFE achieves SAD²/ε² complexity, which is worse than the optimal SAD/ε² bound by a factor of D

## Confidence

- Fundamental hardness of H estimation: High
- Diameter estimation approach: Medium
- Stopping rule correctness: Medium

## Next Checks

1. **Empirical validation**: Implement DFE on benchmark MDPs with known optimal bias spans to measure the practical impact of the diameter-over-approximation factor.

2. **Lower bound tightness**: Investigate whether the exponential lower bound for H estimation can be improved or if there exist MDP families where diameter estimation provides tighter bounds on H.

3. **Stopping rule robustness**: Test the data-dependent stopping rule with various sampling strategies (uniform, adaptive, optimistic) to verify Theorem 6's guarantee across different exploration patterns.