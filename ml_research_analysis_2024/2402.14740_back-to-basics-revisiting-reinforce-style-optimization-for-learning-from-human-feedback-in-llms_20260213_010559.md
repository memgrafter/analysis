---
ver: rpa2
title: 'Back to Basics: Revisiting REINFORCE Style Optimization for Learning from
  Human Feedback in LLMs'
arxiv_id: '2402.14740'
source_url: https://arxiv.org/abs/2402.14740
tags:
- reward
- rloo
- raft
- learning
- reinforce
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that simpler REINFORCE-style optimization
  methods outperform PPO and "RL-free" methods for LLM preference learning. By modeling
  full generations as single actions rather than partial completions, REINFORCE and
  its multi-sample extension RLOO achieve 3.2-20.3% higher win-rates than PPO across
  multiple datasets and models.
---

# Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs

## Quick Facts
- **arXiv ID**: 2402.14740
- **Source URL**: https://arxiv.org/abs/2402.14740
- **Reference count**: 33
- **Primary result**: Simpler REINFORCE-style methods outperform PPO and RL-free methods for LLM preference learning, achieving 3.2-20.3% higher win-rates across multiple datasets and models.

## Executive Summary
This paper challenges the conventional wisdom that PPO is the optimal method for reinforcement learning from human feedback (RLHF) in large language models. Through systematic experiments on TL;DR Summarize and Anthropic Helpful & Harmless Dialogue datasets, the authors demonstrate that simpler REINFORCE-style optimization methods consistently outperform PPO and RL-free alternatives. The key insight is that strong pre-training initialization and prompt conditioning naturally suppress variance in LLM RLHF, making PPO's bias-introducing variance reduction techniques unnecessary and potentially harmful. RLOO, a multi-sample extension of REINFORCE, achieves the best overall performance with superior sample efficiency and robustness to reward noise.

## Method Summary
The paper evaluates multiple optimization methods for fine-tuning pre-trained LLMs using human preference data. The three-stage pipeline includes: (1) SFT stage with cross-entropy loss for 2 epochs, (2) reward model training with binary classification loss for 1 epoch, and (3) RL stage using various optimization methods including PPO, REINFORCE, RLOO, RAFT, and DPO. The RL stage employs KL-shaped rewards to prevent reward hacking and maintain alignment with the base model. Key methodological innovations include modeling full generations as single actions rather than partial completions, and using leave-one-out baselines in RLOO for variance reduction without introducing bias.

## Key Results
- REINFORCE and RLOO achieve 3.2-20.3% higher win-rates than PPO across TL;DR and HH datasets
- RLOO shows better sample efficiency and robustness to reward noise compared to RAFT
- PPO's performance degrades when variance reduction is emphasized (λ < 1.0) in GAE
- Modeling full generations as single actions consistently outperforms partial completion modeling

## Why This Works (Mechanism)

### Mechanism 1
**Claim**: PPO's variance reduction techniques introduce unnecessary bias for LLM RLHF.
**Mechanism**: PPO uses Generalized Advantage Estimation (GAE) with λ parameter to trade variance for bias. In traditional RL, this reduces gradient variance but introduces bias. For LLM RLHF, strong pre-training and prompt conditioning create a stable optimization landscape where variance is naturally low, making bias introduction harmful.
**Core assumption**: Pre-trained LLMs have concentrated probability mass due to strong initialization and conditioning.
**Evidence anchors**: [abstract] "strong pre-training initialization and prompt conditioning reduce variance concerns"; [section 3.1] higher variance (λ=1.0) outperforms lower variance variants
**Break condition**: If pre-training is weak or prompt conditioning is ineffective, variance becomes problematic again.

### Mechanism 2
**Claim**: Modeling partial completions as states is unnecessary when rewards are only given at sequence completion.
**Mechanism**: PPO treats each token generation as an action in a state defined by partial completion. However, since rewards only exist at full sequence completion, this creates artificial intermediate states with zero reward. Modeling the full generation as a single action eliminates these unnecessary states and simplifies optimization.
**Core assumption**: The environment dynamics are deterministic (next state depends only on current action).
**Evidence anchors**: [section 3.3] "PD({y<t+1,x}|st, yt) = 1" indicating deterministic transitions; [section 5.1] REINFORCE and RLOO outperform PPO and Vanilla PG
**Break condition**: If reward models provide meaningful intermediate rewards or if environment becomes stochastic.

### Mechanism 3
**Claim**: Multi-sample REINFORCE (RLOO) provides better variance reduction than learned baselines while maintaining unbiasedness.
**Mechanism**: RLOO uses leave-one-out baselines constructed from other online samples, providing an on-the-fly, parameter-free value function estimate. This is more effective than moving average baselines and avoids the bias introduced by learned baselines in actor-critic methods.
**Core assumption**: Multiple online samples are available during training.
**Evidence anchors**: [abstract] "RLOO achieves 3.2-20.3% higher win-rates than PPO"; [section 2.3] RLOO uses remaining k-1 samples for unbiased expected return estimate
**Break condition**: If sample generation is too expensive or if samples become highly correlated.

## Foundational Learning

- **Concept: Policy gradient methods**
  - Why needed here: The paper compares REINFORCE-style methods with PPO, both of which are policy gradient approaches. Understanding the core idea of gradient ascent on expected reward is essential.
  - Quick check question: What is the fundamental difference between on-policy and off-policy policy gradient methods?

- **Concept: Variance-bias tradeoff in reinforcement learning**
  - Why needed here: The paper argues that PPO's emphasis on variance reduction introduces harmful bias in the LLM RLHF setting. Understanding this tradeoff is crucial for grasping the main argument.
  - Quick check question: How does Generalized Advantage Estimation (GAE) trade variance for bias, and why might this be problematic in stable environments?

- **Concept: Monte Carlo vs temporal difference methods**
  - Why needed here: REINFORCE uses Monte Carlo estimation while PPO uses TD-style bootstrapping. The paper argues that Monte Carlo is sufficient and preferable in this setting.
  - Quick check question: What are the key differences between Monte Carlo and TD methods in terms of bias, variance, and computational requirements?

## Architecture Onboarding

- **Component map**: Base model (Llama/Pythia) -> Reward model (binary classifier) -> Policy (generator) -> Sampling mechanism -> Evaluation pipeline
- **Critical path**: 1. Load pre-trained base model and initialize reward model 2. For each training step: sample prompts, generate k completions, score with reward model, compute gradients, update policy 3. Periodically evaluate on held-out test set
- **Design tradeoffs**: Single vs multiple action modeling (simpler but potentially less sample-efficient), Unbiased vs biased estimators (higher variance but no systematic error vs lower variance with potential bias), Online vs offline methods (more adaptive but computationally expensive vs cheaper but less responsive)
- **Failure signatures**: High variance in reward optimization, Degraded language quality, Poor win-rates despite high reward scores, Training instability
- **First 3 experiments**: 1. Reproduce the λ sweep experiment to verify that higher variance (λ=1.0) performs better 2. Compare REINFORCE with and without full trajectory modeling on a small dataset 3. Test RLOO with different values of k to find the optimal sample budget

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- The study focuses on relatively well-behaved preference learning tasks, leaving open questions about performance in more complex reward landscapes
- The analysis doesn't rigorously quantify how pre-training quality affects the variance-suppression dynamic
- Results may not generalize to RLHF tasks with intermediate rewards or weaker model initialization

## Confidence

- **High confidence**: REINFORCE-style methods outperforming PPO in tested settings (supported by multiple datasets and models with consistent results)
- **Medium confidence**: The mechanism explanation regarding variance-bias tradeoff (plausible but not exhaustively validated across different pre-training regimes)
- **Medium confidence**: RLOO's superiority over RAFT in noise robustness (demonstrated but with limited noise variation in experiments)

## Next Checks

1. Test the REINFORCE vs PPO comparison with varying degrees of pre-training quality to validate the variance-suppression hypothesis
2. Evaluate RLOO's performance when samples are correlated (e.g., with lower temperature or beam search) to test the independence assumption
3. Apply these methods to RLHF tasks with intermediate rewards (like helpfulness ratings at different response stages) to test the single-action modeling assumption