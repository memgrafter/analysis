---
ver: rpa2
title: 'Large language model for Bible sentiment analysis: Sermon on the Mount'
arxiv_id: '2401.00689'
source_url: https://arxiv.org/abs/2401.00689
tags:
- sentiment
- bible
- translations
- language
- sermon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applies BERT-based sentiment analysis to five English
  translations of the Bible's Sermon on the Mount. Using a pre-trained S-BERT model
  refined on the Senwave dataset, the authors perform chapter-by-chapter and verse-by-verse
  sentiment classification and polarity scoring.
---

# Large language model for Bible sentiment analysis: Sermon on the Mount

## Quick Facts
- arXiv ID: 2401.00689
- Source URL: https://arxiv.org/abs/2401.00689
- Reference count: 40
- This study applies BERT-based sentiment analysis to five English translations of the Bible's Sermon on the Mount

## Executive Summary
This study applies BERT-based sentiment analysis to five English translations of the Bible's Sermon on the Mount using a pre-trained S-BERT model refined on the Senwave dataset. The analysis performs chapter-by-chapter and verse-by-verse sentiment classification and polarity scoring, revealing varying sentiment profiles across translations. Results show "optimistic," "joking," and "annoyed" as the most common sentiments, with the KJV exhibiting notably less optimism than the NRSV. The study demonstrates the capability of large language models to compare biblical translations, though limitations include potential metaphor misclassification and training data bias.

## Method Summary
The study uses five English Bible translations (KJV, NIV, NRSV, Lamsa, Basic English) of the Sermon on the Mount, preprocessed with NLTK (stopword removal, lemmatization, UTF-8 normalization). S-BERT is fine-tuned on the Senwave dataset for multi-label sentiment classification across 10 sentiment categories. Verse-by-verse sentiment analysis and AFINN polarity scoring are performed, with results aggregated by chapter and compared across translations using n-gram frequency analysis.

## Key Results
- "Optimistic," "joking," and "annoyed" were the most common sentiments across all translations
- KJV exhibited notably less optimism than NRSV in sentiment profiles
- Jesus's overall speech polarity was neutral in Chapters 5 and 7, but strongly positive in Chapter 6

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERT-based sentence embeddings capture translation differences in biblical sentiment better than word-level methods.
- Mechanism: S-BERT transforms variable-length verses into fixed-dimensional semantic vectors, preserving context across translations even when vocabulary differs.
- Core assumption: Semantic meaning of verses is more stable than lexical form across translations.
- Evidence anchors:
  - [abstract] "We provide a verse-by-verse comparison using sentiment and semantic analysis"
  - [section 3.2] "The sentence-BERT (S-BERT) model improves the BERT model by reducing the computational time to derive semantically meaningful sentence embedding"
  - [corpus] Weak: corpus neighbors mostly focus on translation evaluation but not on verse-level sentiment comparison
- Break condition: If translations use completely different metaphors or cultural concepts, semantic similarity may collapse.

### Mechanism 2
- Claim: Multi-label sentiment classification captures emotional complexity in religious discourse better than single-label.
- Mechanism: Fine-tuned S-BERT on Senwave dataset allows verses to express multiple sentiments simultaneously, reflecting nuanced emotional content.
- Core assumption: Religious texts contain layered emotional meaning not reducible to one sentiment.
- Evidence anchors:
  - [abstract] "Our framework features S-BERT model, which is refined using the Senwave dataset for multi-label sentiment analysis"
  - [section 3.3] "We use the BERT model from our framework for verse-by-verse sentiment analysis"
  - [section 4.2] "in Figure 4, surprisingly joking is the sentiment detected the most"
- Break condition: If training data vocabulary and context differ too much from biblical text, multi-label predictions may be unreliable.

### Mechanism 3
- Claim: Bi-gram and tri-gram frequency analysis reveals translation style differences at vocabulary level.
- Mechanism: n-gram statistics provide a baseline comparison that complements semantic analysis, highlighting lexical choices and stylistic patterns.
- Core assumption: Different translation philosophies result in measurable vocabulary pattern differences.
- Evidence anchors:
  - [section 4.1] "We begin by analyzing the top 10 bi-grams and tri-grams for each translation"
  - [section 4.1] "This indicates a low degree of repetition and a significant difference in the vocabulary across the versions"
  - [corpus] Weak: corpus neighbors don't address n-gram style analysis in religious texts
- Break condition: If all translations use similar vocabulary despite different philosophies, n-gram analysis may miss deeper stylistic differences.

## Foundational Learning

- Concept: Pre-trained language models and transfer learning
  - Why needed here: The study relies on BERT/S-BERT as a foundation, fine-tuned for sentiment analysis rather than training from scratch
  - Quick check question: What advantage does fine-tuning a pre-trained BERT model offer compared to training a sentiment classifier from scratch on the Bible text?

- Concept: Multi-label classification vs multi-class classification
  - Why needed here: The sentiment framework allows verses to be tagged with multiple emotions simultaneously, which is critical for capturing the nuanced emotional content of religious discourse
  - Quick check question: How would the interpretation of Jesus's teachings change if we forced each verse into a single sentiment category?

- Concept: Sentiment polarity scoring and AFINN lexicon
  - Why needed here: Polarity scores provide a continuous measure of sentiment strength, allowing comparison of overall emotional tone across chapters and translations
  - Quick check question: Why might a chapter with mostly neutral words still receive a negative polarity score in AFINN?

## Architecture Onboarding

- Component map: Raw text -> UTF-8 normalization and stopword removal -> Lemmatization -> S-BERT embedding generation -> Multi-label sentiment classification -> AFINN polarity scoring -> n-gram frequency analysis -> Visualization and comparison
- Critical path: Raw text -> S-BERT -> Sentiment classification -> Results interpretation
- Design tradeoffs: Using Senwave dataset for training introduces vocabulary bias but provides robust multi-label sentiment labels; using AFINN provides polarity but may miss domain-specific sentiment
- Failure signatures: High "joking" classification on serious passages indicates metaphor misinterpretation; inconsistent sentiment across translations suggests model bias or translation quality issues
- First 3 experiments:
  1. Run S-BERT on a small set of parallel verses from all five translations and manually verify semantic similarity scores
  2. Test multi-label classification on verses with known emotional complexity to validate the model captures multiple sentiments
  3. Compare AFINN polarity scores for verses with known emotional content to check if the scoring aligns with human judgment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do metaphor detection techniques impact sentiment classification accuracy in religious texts compared to standard sentiment models?
- Basis in paper: [explicit] The paper identifies that the Bible's heavy use of metaphors led the sentiment model to incorrectly classify many verses as "joking," highlighting this as a key limitation.
- Why unresolved: The study used a standard BERT-based sentiment model without explicit metaphor detection, and acknowledged this limitation without exploring solutions.
- What evidence would resolve it: Comparing sentiment classification accuracy using models with and without explicit metaphor detection components on the same biblical text corpus.

### Open Question 2
- Question: How does training sentiment models on domain-specific corpora (e.g., religious texts) versus general-purpose datasets affect classification performance and bias?
- Basis in paper: [explicit] The authors note that their model was trained on COVID-19 tweets, which have a vocabulary significantly different from the Sermon on the Mount, potentially introducing bias.
- Why unresolved: The study acknowledged this training data bias but did not experiment with domain-specific training data or evaluate the impact of this mismatch.
- What evidence would resolve it: Training and evaluating the same sentiment model on religious text corpora versus general datasets, then comparing classification accuracy and bias metrics.

### Open Question 3
- Question: What are the implications of translation style and source language differences on sentiment analysis consistency across biblical translations?
- Basis in paper: [explicit] The study found significant vocabulary differences across translations and noted that the KJV exhibited notably less optimism than the NRSV, suggesting translation choices affect sentiment profiles.
- Why unresolved: While the paper identified these differences, it did not systematically analyze how specific translation choices or source language differences (e.g., Greek vs. Aramaic) impact sentiment consistency.
- What evidence would resolve it: A comparative analysis of sentiment scores across translations while controlling for or categorizing translation style choices and source language differences.

## Limitations
- Domain bias from training on COVID-19 tweets rather than religious text affects sentiment classification accuracy
- Metaphor-heavy verses misclassified as "joking" due to vocabulary and context mismatch
- AFINN lexicon may not capture nuanced sentiment in biblical passages

## Confidence
- High Confidence: The technical methodology (BERT/S-BERT framework, multi-label classification setup, n-gram analysis) is sound and well-documented. The finding that KJV exhibits less optimism than NRSV is likely robust.
- Medium Confidence: Chapter-level sentiment trends (Chapter 6 positivity, Chapters 5/7 neutrality) are methodologically valid but may be artifacts of domain bias.
- Low Confidence: The interpretation of Jesus's speech patterns and the overall sentiment profile of the Sermon on the Mount should be treated skeptically due to Senwave training data bias.

## Next Checks
1. Manual Annotation Validation: Select 50 verses across all translations and have three independent annotators assign sentiment labels. Compare human consensus with model predictions to quantify accuracy and identify systematic biases.
2. Domain-Specific Fine-tuning: Fine-tune the S-BERT model on a corpus of religious texts (e.g., sermons, theological writings) rather than COVID-19 tweets, then re-run the analysis to assess whether sentiment profiles change significantly.
3. Metaphor Detection Test: Create a test set of 20 metaphor-rich biblical verses (parables, symbolic language) and evaluate whether the model consistently misclassifies them as "joking" or other inappropriate sentiments.