---
ver: rpa2
title: 'LLMs are Superior Feedback Providers: Bootstrapping Reasoning for Lie Detection
  with Self-Generated Feedback'
arxiv_id: '2408.13915'
source_url: https://arxiv.org/abs/2408.13915
tags:
- feedback
- stage
- human
- gpt-4
- france
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a bootstrapping framework that uses self-generated
  feedback from large language models (LLMs) to enhance their reasoning capabilities
  for lie detection in the game of Diplomacy. The framework consists of three stages:
  suggestion, feedback collection, and modification.'
---

# LLMs are Superior Feedback Providers: Bootstrapping Reasoning for Lie Detection with Self-Generated Feedback

## Quick Facts
- arXiv ID: 2408.13915
- Source URL: https://arxiv.org/abs/2408.13915
- Authors: Tanushree Banerjee; Richard Zhu; Runzhe Yang; Karthik Narasimhan
- Reference count: 40
- Primary result: Achieves 39% improvement over zero-shot baseline in lying-F1 score without training data

## Executive Summary
This paper introduces a novel bootstrapping framework that uses self-generated feedback from large language models to enhance their reasoning capabilities for lie detection in the game of Diplomacy. The framework operates in three stages: suggestion, feedback collection, and modification. By leveraging a cost-effective LLM for initial predictions and then using a more advanced LLM to refine these predictions based on auto-generated feedback, the approach significantly improves performance. The authors demonstrate that LLM-generated feedback is superior to human feedback, achieving a 29% improvement over the best human feedback and rivaling state-of-the-art supervised learning results without requiring any training data.

## Method Summary
The method employs a three-stage bootstrapping framework for lie detection. In the suggestion stage, a cost-effective LLM (GPT-3) generates initial predictions about deceptive messages. The feedback collection stage involves a more advanced LLM (GPT-4) analyzing these predictions and generating natural language feedback on systematic errors without access to ground truth. Finally, in the modification stage, GPT-4 refines the initial predictions using the generated feedback. The framework is evaluated on the Diplomacy game dataset, using lying-F1 and macro-F1 scores as metrics, and compares performance against zero-shot baselines, human expert feedback, and supervised learning models.

## Key Results
- Achieves 39% improvement over zero-shot baseline in lying-F1 score without requiring training data
- LLM-generated feedback outperforms human feedback by 29%, particularly in cases where humans are unsure
- Zero-shot GPT-4 with self-generated feedback rivals previous state-of-the-art supervised LSTM models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-generated feedback from a more capable LLM can identify systematic errors made by a cheaper LLM in the suggestion stage.
- Mechanism: The feedback collection stage allows a more advanced LLM (GPT-4) to analyze the initial predictions and reasoning of the cheaper LLM (GPT-3) without access to ground truth. This analysis uncovers biases or errors in the cheaper model's reasoning, which are then used to refine predictions in the modification stage.
- Core assumption: A more capable LLM can effectively identify and articulate the reasoning errors of a less capable LLM.
- Evidence anchors:
  - [abstract]: "In the feedback-collection stage, a language model providing feedback on these predictions... the feedback provided is in natural language and contains information on 1) systematic errors made by the LLM in the suggestion stage..."
  - [section]: "The feedback-collection stage involves a language model providing feedback on these predictions... the feedback LLM in our setup has no access to the ground truth answers."
  - [corpus]: Found 25 related papers, average neighbor FMR=0.48, but no direct evidence on self-generated feedback quality comparison with human feedback.
- Break condition: If the more capable LLM cannot identify meaningful errors or if the feedback is not actionable for improving predictions.

### Mechanism 2
- Claim: LLM-generated feedback is more informative and longer than human feedback, leading to better performance improvements.
- Mechanism: GPT-4 generates feedback that is 5x-8x longer than human feedback, providing more detailed observations on systematic errors and suggestions for minimizing false negatives. This richer feedback leads to greater performance improvements in the modification stage.
- Core assumption: Longer, more detailed feedback is inherently more useful for refining LLM predictions.
- Evidence anchors:
  - [abstract]: "LLM-generated feedback significantly outperforms even the best human feedback by 29% and seems to be most useful in cases where humans are unsure or disagree in their assessment."
  - [section]: "Figure 2 demonstrates that LLM-generated feedback is notably longer (5x∼ 8x) compared to human feedback, with GPT-4 generating longer feedback than GPT-3.5."
  - [corpus]: No direct evidence in corpus on the relationship between feedback length and quality.
- Break condition: If the increased length does not translate to meaningful performance gains or if the feedback becomes redundant.

### Mechanism 3
- Claim: The bootstrapping framework allows zero-shot performance to rival supervised learning results without requiring any training data.
- Mechanism: By using a cheaper model for initial predictions and refining them with feedback from a more advanced model, the framework achieves significant improvements over the zero-shot baseline. This approach eliminates the need for annotated training data while still achieving state-of-the-art performance.
- Core assumption: Self-generated feedback can compensate for the lack of supervised training data.
- Evidence anchors:
  - [abstract]: "Our approach achieves a 39% improvement over the zero-shot baseline in lying-F1 without the need for any training data, rivaling state-of-the-art supervised learning results."
  - [section]: "Our bootstrapping method helps increase lying-F1 scores by 39% over the base LLM while also enabling zero-shot GPT-4 to rival the previous state-of-the-art supervised LSTM model..."
  - [corpus]: Found papers on bootstrapping language models and RLHF, but no direct comparison of zero-shot with supervised learning performance.
- Break condition: If the feedback mechanism fails to provide sufficient guidance for improving predictions beyond the zero-shot baseline.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The framework relies on generating predictions without any fine-tuning or training data, using only the base LLM's pre-trained knowledge.
  - Quick check question: What is the difference between zero-shot and few-shot learning in the context of LLMs?

- Concept: Feedback-driven refinement
  - Why needed here: The core mechanism of the framework is using feedback to improve initial predictions, similar to how humans learn from feedback.
  - Quick check question: How does the feedback collection stage differ from traditional supervised learning approaches?

- Concept: Diplomacy game mechanics
  - Why needed here: Understanding the game's rules, order types, and deception dynamics is crucial for interpreting the lie detection task and the model's predictions.
  - Quick check question: What are the four types of orders that can be placed in Diplomacy, and during which seasons?

## Architecture Onboarding

- Component map:
  - Suggestion stage: Base LLM (GPT-3) generates initial predictions
  - Feedback collection stage: Advanced LLM (GPT-4) provides feedback on initial predictions
  - Modification stage: Advanced LLM (GPT-4) refines initial predictions using feedback
  - Rule-based extractor: Extracts message numbers predicted as lies from model outputs

- Critical path: Suggestion stage → Feedback collection stage → Modification stage

- Design tradeoffs:
  - Using a cheaper LLM for initial predictions reduces computational costs but may introduce errors
  - Using a more advanced LLM for feedback generation improves quality but increases costs
  - Rule-based extraction is deterministic but may miss nuanced predictions

- Failure signatures:
  - Poor performance in suggestion stage: Base LLM fails to generate reasonable initial predictions
  - Ineffective feedback: Advanced LLM fails to identify meaningful errors or provide actionable suggestions
  - No improvement in modification stage: Refined predictions are no better than initial predictions

- First 3 experiments:
  1. Run suggestion stage with GPT-3 on a small subset of the dataset and evaluate zero-shot performance
  2. Generate feedback for the initial predictions using GPT-4 and compare feedback length and content with human feedback
  3. Run modification stage with GPT-4 using both human and GPT-4 generated feedback, and compare performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of feedback (e.g., focusing on false positives vs. false negatives) affect the model's performance in lie detection?
- Basis in paper: [explicit] The authors mention that feedback can contain observations on systematic errors and suggestions for minimizing false negatives.
- Why unresolved: The paper does not provide a detailed analysis of how different types of feedback specifically impact the model's performance.
- What evidence would resolve it: An ablation study comparing the effects of different feedback types on the model's lying-F1 score.

### Open Question 2
- Question: Can the bootstrapping framework be applied to other domains or languages beyond the game of Diplomacy?
- Basis in paper: [inferred] The authors discuss the potential of their framework for nuanced natural language tasks, but only evaluate it on Diplomacy data.
- Why unresolved: The paper does not explore the framework's applicability to other domains or languages.
- What evidence would resolve it: Experiments applying the framework to different domains (e.g., online conversations, news articles) or languages.

### Open Question 3
- Question: What is the optimal number of feedback rounds for improving the model's performance?
- Basis in paper: [explicit] The authors mention an experiment with successive rounds of feedback but found no significant improvement after the second round.
- Why unresolved: The paper does not provide a systematic analysis of the optimal number of feedback rounds.
- What evidence would resolve it: Experiments comparing the model's performance with varying numbers of feedback rounds.

## Limitations

- The framework's effectiveness may be specific to the Diplomacy lie detection task and may not generalize well to other domains or tasks that require different types of reasoning or feedback.
- The framework's performance heavily relies on the quality of the feedback generated by the advanced LLM (GPT-4). If the feedback is not accurate or actionable, the performance improvements may be limited.
- While the framework reduces the need for annotated training data, it still requires multiple calls to advanced LLMs (GPT-3.5 and GPT-4), which can be computationally expensive.

## Confidence

- **High**: The bootstrapping framework's ability to improve lie detection performance in Diplomacy using self-generated feedback from a more advanced LLM.
- **Medium**: The claim that LLM-generated feedback is more informative and useful than human feedback, leading to better performance improvements.
- **Low**: The assertion that the framework can rival state-of-the-art supervised learning results without any training data, as this claim is based on a single task and may not generalize to other domains.

## Next Checks

1. **Domain Transferability**: Test the bootstrapping framework on a different lie detection or reasoning task to assess its generalizability beyond the Diplomacy domain.
2. **Feedback Quality Analysis**: Conduct a detailed analysis of the feedback generated by the advanced LLM, comparing it with human feedback in terms of accuracy, relevance, and actionability.
3. **Computational Efficiency**: Measure the computational costs of the framework, including the number of API calls and the associated expenses, to determine its practical feasibility for real-world applications.