---
ver: rpa2
title: Correcting Large Language Model Behavior via Influence Function
arxiv_id: '2412.16451'
source_url: https://arxiv.org/abs/2412.16451
tags:
- influence
- samples
- data
- human
- lancet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of correcting undesirable behaviors
  in large language models (LLMs) that arise from outdated or inappropriate training
  data. The authors propose a novel approach called LANCET that uses influence functions
  to identify training samples that significantly impact undesirable model outputs,
  then applies a new technique called Influence-driven Bregman Optimization (IBO)
  to adjust the model's behavior based on these influence distributions.
---

# Correcting Large Language Model Behavior via Influence Function

## Quick Facts
- arXiv ID: 2412.16451
- Source URL: https://arxiv.org/abs/2412.16451
- Authors: Han Zhang; Zhuo Zhang; Yi Zhang; Yuanzhao Zhai; Hanyang Peng; Yu Lei; Yue Yu; Hui Wang; Bin Liang; Lin Gui; Ruifeng Xu
- Reference count: 17
- One-line primary result: LANCET effectively corrects inappropriate behaviors of LLMs while preserving model utility and improving interpretability

## Executive Summary
This paper addresses the challenge of correcting undesirable behaviors in large language models (LLMs) that stem from problematic training data. The authors propose LANCET, a novel two-phase approach that first identifies influential training samples using a computationally efficient influence function method called LinFAC, then applies Influence-driven Bregman Optimization (IBO) to correct model behavior based on these influence distributions. The method enables autonomous identification and correction of problematic training data without requiring costly human intervention.

## Method Summary
LANCET is a two-phase approach for correcting undesirable LLM behaviors. First, it uses LinFAC to efficiently calculate influence scores for training samples relative to undesirable model outputs, identifying which samples most significantly impact problematic behaviors. Second, it applies IBO, which uses pairwise learning with positive and negative influence samples to correct behavior while preserving model utility through Bregman divergence. The method is trained on datasets containing both safe and unsafe samples, with corrections evaluated on harmlessness and utility metrics.

## Key Results
- LANCET effectively corrects inappropriate behaviors of LLMs while preserving model utility
- Outperforms baseline methods that rely on collecting human preferences
- Demonstrates superior generalization ability on out-of-distribution harmful prompts
- LinFAC significantly reduces computational complexity compared to existing influence function methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LANCET identifies training samples that significantly impact undesirable model outputs through influence functions
- Mechanism: Uses LinFAC to efficiently calculate influence scores by approximating the Gauss-Newton Hessian using entire sequences and considering token interdependencies, then ranks samples by these scores
- Core assumption: Influence scores accurately reflect the contribution of training samples to undesirable model behaviors
- Evidence anchors:
  - [abstract] "using influence functions to identify the training data that significantly impact undesirable model outputs"
  - [section] "influence function of zm on θ∗ is defined as the gradient of the response function... Iθ∗(zm, zq) ≜ ∇θ∗ log p(zr|zp; θ∗)⊤Iθ∗(zm)"
  - [corpus] Weak - no direct corpus evidence of influence function effectiveness on LLMs

### Mechanism 2
- Claim: IBO uses pairwise learning with positive and negative influence samples to correct model behavior
- Mechanism: Constructs pairwise data using samples with high positive influence (increase undesirable behavior) and high negative influence (decrease undesirable behavior), then applies pairwise ranking loss to adjust model outputs
- Core assumption: Learning from the ranking information provided by influence scores can effectively correct undesirable behaviors
- Evidence anchors:
  - [abstract] "applying an novel Influence-driven Bregman Optimization (IBO) technique to adjust the model's behavior based on these influence distributions"
  - [section] "LCOR(πθ, πθs) = −Ez+∼DIF +z−∼DIF −[ϵ · log σ(...)]" showing pairwise learning objective
  - [corpus] Weak - no direct corpus evidence of IBO's effectiveness compared to other methods

### Mechanism 3
- Claim: LinFAC significantly reduces computational complexity compared to existing influence function methods
- Mechanism: Uses entire sequences instead of isolated tokens, employs Transformer sublayers as computational units instead of individual linear layers, and approximates the Gauss-Newton Hessian more efficiently
- Core assumption: The modular approximation of neural network blocks can capture the necessary information for influence calculation while reducing computation
- Evidence anchors:
  - [abstract] "introduces a new influence function calculation approach called LinFAC that significantly reduces computational complexity compared to existing methods"
  - [section] "LinFAC computes the Gauss-Newton Hessian using entire sequences, accounting for token interdependencies" and "uses the Transformer sublayer... as the computational unit"
  - [corpus] Moderate - case study shows LinFAC can recall relevant samples with high influence scores

## Foundational Learning

- Concept: Influence Functions
  - Why needed here: LANCET relies on influence functions to identify which training samples contribute most to undesirable model behaviors
  - Quick check question: How does an influence function measure the impact of removing a training sample on a model's output?

- Concept: Bregman Divergence
  - Why needed here: IBO uses Bregman divergence to preserve model utility on non-influential samples while correcting undesirable behaviors
  - Quick check question: What property of Bregman divergence makes it suitable for preventing catastrophic forgetting during model correction?

- Concept: Kronecker-Factored Approximate Curvature (KFAC)
  - Why needed here: Understanding KFAC helps explain why LinFAC improves upon existing influence function methods for LLMs
  - Quick check question: How does KFAC approximate the Fisher matrix, and why is this approximation useful for scaling influence functions to large models?

## Architecture Onboarding

- Component map: LinFAC (modular gradients → surrogate GNH → batch IHVP computation) → Sample ranking → Sample pairing → IBO (pairwise ranking loss + Bregman divergence)
- Critical path: Influence score calculation → Sample ranking → Sample pairing into positive/negative/non-influential sets → IBO training with pairwise loss and Bregman divergence
- Design tradeoffs: Computational efficiency (LinFAC) vs accuracy (EK-FAC), correction effectiveness (IBO) vs simplicity (PBO), batch query efficiency vs single query precision
- Failure signatures: Poor performance on unseen data (generalization failure), high perplexity (utility loss), low harmlessness scores (correction failure), high computational cost (efficiency failure)
- First 3 experiments:
  1. Implement LinFAC on a small dataset and verify influence scores correlate with known sample importance
  2. Test IBO with synthetic positive/negative influence samples to confirm pairwise learning corrects behavior
  3. Compare LinFAC vs EK-FAC on a medium-sized model to measure computational savings and accuracy trade-offs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the LinFAC method perform compared to EK-FAC on larger-scale models or datasets beyond those tested in the paper?
- Basis in paper: [explicit] The paper compares LinFAC and EK-FAC on specific datasets (Anthropic-HH, BeaverTails, Safe RLHF) and models (OPT-2.7B, Llama2-7B, Llama3.1-8B), showing LinFAC's advantages in computational efficiency and precision-recall curves.
- Why unresolved: The experiments are limited to specific model sizes and datasets. It's unclear if LinFAC maintains its advantages when scaled to significantly larger models or different types of datasets.
- What evidence would resolve it: Testing LinFAC on larger models (e.g., Llama3.1-70B, GPT-3) and diverse datasets would provide conclusive evidence of its scalability and general performance.

### Open Question 2
- Question: What is the long-term impact of using LANCET on the model's performance across different tasks and domains?
- Basis in paper: [inferred] The paper focuses on correcting specific undesirable behaviors and improving safety, but does not extensively explore the long-term effects on the model's overall performance and generalization across various tasks.
- Why unresolved: The experiments primarily assess immediate corrections to harmful outputs. There's a lack of analysis on how LANCET affects the model's performance over time and across diverse applications.
- What evidence would resolve it: Long-term studies tracking the model's performance on various tasks and domains after applying LANCET would reveal its broader impact.

### Open Question 3
- Question: How sensitive is LANCET to the choice of hyperparameters, such as the batch size for influence function queries and the α parameter in the Pareto distribution?
- Basis in paper: [explicit] The paper mentions using batch queries to improve performance and the Pareto rule for selecting influential samples, but does not extensively explore the sensitivity of LANCET to these hyperparameters.
- Why unresolved: The paper does not provide a detailed sensitivity analysis of LANCET to hyperparameter choices, leaving uncertainty about its robustness to different settings.
- What evidence would resolve it: Conducting experiments varying the batch size and α parameter to assess their impact on LANCET's performance would clarify its sensitivity to these hyperparameters.

## Limitations

- The scalability and robustness of LinFAC's influence function approximation trade-off between efficiency and accuracy is not fully characterized
- Claims about LANCET's generalization ability to out-of-distribution harmful prompts are supported by limited evidence
- The method's sensitivity to hyperparameter choices (particularly the α threshold for sample selection) could impact effectiveness across different domains

## Confidence

- **High Confidence**: The core architectural approach of combining influence function identification with pairwise learning is technically sound and well-grounded in existing literature
- **Medium Confidence**: The specific implementation details of LinFAC's modular approximation and the effectiveness of the Bregman divergence preservation mechanism require more extensive validation
- **Low Confidence**: Claims about LANCET's generalization ability to out-of-distribution harmful prompts are supported by limited evidence and would benefit from broader testing across diverse domains

## Next Checks

1. Conduct ablation studies varying the α threshold for sample selection to quantify its impact on correction effectiveness and model utility preservation
2. Test LANCET's performance on domain-specific datasets (e.g., medical, legal) where the definition of "undesirable behavior" may differ significantly from general toxicity
3. Implement a controlled experiment comparing LinFAC's influence score accuracy against exact influence function computation on a small model to measure the precision-cost trade-off