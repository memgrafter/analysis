---
ver: rpa2
title: HW-TSC's Submission to the CCMT 2024 Machine Translation Tasks
arxiv_id: '2409.14842'
source_url: https://arxiv.org/abs/2409.14842
tags:
- translation
- data
- machine
- training
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Huawei Translation Services Center's submission
  to the bilingual and multi-domain machine translation tasks at CCMT 2024. The team
  employed multiple strategies including regularized dropout, bidirectional training,
  data diversification, forward translation, back translation, alternated training,
  curriculum learning, and transductive ensemble learning to train NMT models based
  on the deep Transformer-big architecture.
---

# HW-TSC's Submission to the CCMT 2024 Machine Translation Tasks

## Quick Facts
- arXiv ID: 2409.14842
- Source URL: https://arxiv.org/abs/2409.14842
- Reference count: 40
- Achieved competitive results in CCMT 2024 machine translation tasks with significant improvements in BLEU and COMET scores

## Executive Summary
This paper presents Huawei Translation Services Center's submission to the bilingual and multi-domain machine translation tasks at CCMT 2024. The team employed a multi-strategy approach including regularized dropout, bidirectional training, data diversification, forward translation, back translation, alternated training, curriculum learning, and transductive ensemble learning to train deep Transformer-big architecture NMT models. For the multi-domain task, they explored using llama2-13b as an automatic post-editing model via supervised fine-tuning. The submission achieved competitive results across all language pairs and domains, demonstrating the effectiveness of their comprehensive approach.

## Method Summary
The submission utilizes a multi-strategy approach to enhance NMT performance. The team trained models based on deep Transformer-big architecture with regularized dropout (R-Drop) for regularization. They employed bidirectional training, data diversification through forward translation (FT) and back translation (BT), alternated training, and curriculum learning strategies. For the multi-domain task, they used LLM (llama2-13b) as an automatic post-editing model via supervised fine-tuning. The system also incorporated transductive ensemble learning to combine multiple model outputs. The approach showed significant improvements in translation quality across all language pairs and domains evaluated in CCMT 2024.

## Key Results
- Significant improvements in BLEU scores across all language pairs and domains
- Enhanced COMET scores demonstrating better translation quality
- LLM-based post-editing (llama2-13b) successfully improved translation outputs in the multi-domain task
- Multi-strategy approach outperformed single-method baselines

## Why This Works (Mechanism)
The multi-strategy approach works by addressing multiple aspects of NMT model training and optimization simultaneously. R-Drop regularization helps prevent overfitting and improves model generalization. Bidirectional training and data diversification expand the model's exposure to different linguistic patterns and contexts. Forward and back translation augment the training data with synthetic examples, particularly beneficial for low-resource scenarios. Curriculum learning helps the model learn from easier to harder examples, improving convergence. The LLM-based post-editing provides an additional refinement layer that can correct subtle translation errors. Transductive ensemble learning combines the strengths of multiple models to produce more robust outputs.

## Foundational Learning

**Transformer Architecture**: The base NMT model uses deep Transformer-big architecture, which provides strong representation learning capabilities through self-attention mechanisms. Needed for understanding the fundamental model structure; quick check: verify self-attention layers and feed-forward networks.

**Regularized Dropout (R-Drop)**: A regularization technique that minimizes the divergence between outputs from different dropout masks. Needed to prevent overfitting and improve model generalization; quick check: compare training with and without R-Drop on validation sets.

**Forward Translation (FT)**: Generates synthetic source sentences from target language data. Needed to augment training data, especially for low-resource language pairs; quick check: measure improvement in BLEU when adding FT data.

**Back Translation (BT)**: Translates target language sentences to source language to create additional training pairs. Needed to expand training data diversity; quick check: evaluate performance gains from BT data addition.

**Curriculum Learning**: Training strategy that presents examples in order of increasing difficulty. Needed to improve model convergence and stability; quick check: compare learning curves with and without curriculum ordering.

**Transductive Ensemble Learning**: Combines predictions from multiple models trained on the same data. Needed to leverage model diversity for improved robustness; quick check: measure ensemble performance versus individual model performance.

## Architecture Onboarding

**Component Map**: Data Augmentation (FT/BT) -> Model Training (Transformer-big + R-Drop) -> Ensemble Learning -> LLM Post-Editing -> Final Output

**Critical Path**: The critical path involves data preparation through augmentation, followed by multi-strategy model training, ensemble combination, and final LLM-based post-editing for the multi-domain task.

**Design Tradeoffs**: The approach balances computational cost (multiple training runs, ensemble methods) against translation quality improvements. Using LLM for post-editing adds significant computational overhead but provides quality gains in the multi-domain setting.

**Failure Signatures**: Potential failures include overfitting despite R-Drop regularization, degraded performance from poor-quality synthetic data in FT/BT, and computational infeasibility of the full multi-strategy pipeline in resource-constrained settings.

**First Experiments**:
1. Baseline Transformer-big model without any augmentation or regularization
2. R-Drop regularized model to measure regularization impact
3. Forward+Back translation augmented model to evaluate data diversification effects

## Open Questions the Paper Calls Out

**Open Question 1**: How does the performance of LLM-based APE compare to traditional APE methods across different language pairs and domains?
- Basis in paper: The paper mentions using LLM (llama2-13b) as an APE model via supervised fine-tuning to enhance translation quality, showing improvements in BLEU and COMET scores.
- Why unresolved: The paper does not provide a direct comparison between LLM-based APE and traditional APE methods, nor does it explore the performance across different language pairs and domains in detail.
- What evidence would resolve it: Conducting experiments comparing LLM-based APE with traditional APE methods across various language pairs and domains, measuring improvements in translation quality metrics.

**Open Question 2**: What are the limitations of using R-Drop regularization in low-resource translation tasks?
- Basis in paper: The paper applies R-Drop regularization and mentions setting dropout rates differently for high-resource and low-resource translation tasks, but does not explore the specific limitations or effects in low-resource settings.
- Why unresolved: The paper does not provide detailed analysis or results specific to low-resource translation tasks, nor does it discuss potential limitations of R-Drop in these contexts.
- What evidence would resolve it: Conducting experiments focusing on low-resource translation tasks with and without R-Drop, analyzing performance differences and identifying any limitations or challenges encountered.

**Open Question 3**: How does the choice of data diversification methods impact the overall translation quality in NMT models?
- Basis in paper: The paper mentions using data diversification as one of the strategies to improve NMT performance, but does not delve into how different methods of data diversification impact translation quality.
- Why unresolved: The paper does not compare different data diversification methods or explore their specific impacts on translation quality, leaving this aspect underexplored.
- What evidence would resolve it: Implementing and comparing various data diversification methods, such as DD, FT, and BT, and evaluating their impact on translation quality through comprehensive testing and analysis.

## Limitations

- The work focuses exclusively on the CCMT 2024 evaluation setting, limiting insights about performance on other benchmarks or real-world deployment scenarios.
- The technical report provides limited architectural details about the deep Transformer-big variants used, making exact replication challenging.
- The LLM-based post-editing approach using llama2-13b is evaluated only within the multi-domain task, with no analysis of computational overhead or latency impacts.
- The study lacks ablation studies isolating the contribution of individual strategies, making it difficult to determine which components drive performance improvements.

## Confidence

**High confidence**: The empirical improvements in BLEU and COMET scores are well-documented and verifiable through standard MT evaluation metrics

**Medium confidence**: The effectiveness of the multi-strategy approach is supported by results, though the relative contribution of each strategy remains unclear

**Medium confidence**: The LLM-based post-editing shows promise but is evaluated in a narrow context without broader validation

## Next Checks

1. Conduct ablation studies to isolate the impact of individual strategies (regularized dropout, bidirectional training, forward/back translation, etc.) on translation quality

2. Evaluate the LLM-based post-editing approach across additional domains and language pairs beyond the CCMT multi-domain task

3. Measure computational overhead and inference latency introduced by the ensemble methods and LLM post-editing to assess practical deployment viability