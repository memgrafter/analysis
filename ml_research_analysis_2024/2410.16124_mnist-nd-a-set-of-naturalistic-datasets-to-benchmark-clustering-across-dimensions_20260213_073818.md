---
ver: rpa2
title: 'MNIST-Nd: a set of naturalistic datasets to benchmark clustering across dimensions'
arxiv_id: '2410.16124'
source_url: https://arxiv.org/abs/2410.16124
tags:
- clustering
- datasets
- dimensions
- across
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MNIST-Nd, a synthetic benchmarking dataset
  for clustering algorithms in high-dimensional spaces. It is created by training
  mixture variational autoencoders with 2 to 64 latent dimensions on MNIST, producing
  datasets with comparable structure but varying dimensionality.
---

# MNIST-Nd: a set of naturalistic datasets to benchmark clustering across dimensions

## Quick Facts
- arXiv ID: 2410.16124
- Source URL: https://arxiv.org/abs/2410.16124
- Reference count: 27
- Primary result: Introduces MNIST-Nd, a synthetic high-dimensional clustering benchmark with realistic noise and overlap, showing Leiden clustering outperforms other methods in high dimensions.

## Executive Summary
This paper addresses the lack of realistic high-dimensional benchmarking datasets for clustering algorithms by introducing MNIST-Nd. The dataset is created by training mixture variational autoencoders (m-VAEs) with 2 to 64 latent dimensions on MNIST, producing datasets with comparable structure but varying dimensionality. The authors demonstrate that MNIST-Nd has realistic noise and overlapping clusters, making it suitable for evaluating clustering algorithms across dimensions. Benchmarking results show that Leiden clustering is particularly robust in high dimensions, while traditional methods like k-means and GMM degrade with increasing dimensionality.

## Method Summary
The authors create MNIST-Nd by training mixture VAEs with 2 to 64 latent dimensions on MNIST, using a β-VAE framework with inverse β scaling to maintain consistent signal-to-noise ratios across dimensions. The resulting datasets are high-dimensional embeddings with realistic noise and overlapping clusters. Clustering algorithms (k-means, GMM, TMM, Leiden) are benchmarked on these datasets using Adjusted Rand Index (ARI) as the primary metric, with additional analysis of stability and robustness through bootstrapping and cross-validated classification accuracy.

## Key Results
- MNIST-Nd datasets exhibit realistic noise and overlapping clusters across all dimensions (2-64)
- Leiden clustering consistently outperforms k-means, GMM, and TMM in high-dimensional settings
- Clustering performance (measured by ARI) generally decreases with increasing dimensionality for traditional methods
- Cross-validated classification accuracy remains stable (~90%) across all MNIST-Nd datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MNIST-Nd creates high-dimensional datasets with consistent signal-to-noise ratios across dimensions.
- Mechanism: The m-VAE model is trained with a β parameter inversely scaled to dimensionality, ensuring comparable regularization strength. This keeps KL loss balanced across dimensions, preserving structure while allowing realistic noise.
- Core assumption: Inverse scaling of β with dimensionality maintains similar prior regularization across datasets.
- Evidence anchors:
  - [abstract] "MNIST-Nd is obtained by training mixture variational autoencoders with 2 to 64 latent dimensions on MNIST, resulting in six datasets with comparable structure but varying dimensionality."
  - [section] "As the KL loss is unbounded and grows with the number of dimensions, we scale β inversely proportional to the dimensionality such that all datasets are similarly regularized to match the prior shape."
- Break condition: If KL loss scaling is not properly tuned, datasets could become over-regularized in high dimensions or under-regularized in low dimensions, breaking comparability.

### Mechanism 2
- Claim: High-dimensional embeddings in MNIST-Nd exhibit overlapping clusters similar to real-world data.
- Mechanism: The mixture VAE with categorical latent variables encourages multimodal distributions in latent space, while the learned decoder adds realistic noise. DISCO scores and density peak analysis confirm overlap.
- Core assumption: The m-VAE's mixture prior and noise injection produce embeddings that mirror the cluster overlap found in real biological data.
- Evidence anchors:
  - [abstract] "MNIST-Nd has realistic noise as it appears in learned embeddings and controllable dimensions while maintaining consistent signal-to-noise ratio across dimensions."
  - [section] "DISCO scores are bounded between −1 and 1, where negative values imply points being better connected to a different cluster than to the assigned one. The majority of points of our embeddings are scored around zero or below, suggesting a noticeable density overlap."
- Break condition: If the decoder noise model is too weak or the mixture components are too well-separated, overlap will be artificially reduced, invalidating the benchmark's realism.

### Mechanism 3
- Claim: Leiden clustering outperforms other methods in high dimensions due to its graph-based, community-detection approach.
- Mechanism: Leiden optimizes modularity in a neighborhood graph, making it less sensitive to distance concentration in high dimensions than centroid or density-based methods.
- Core assumption: Graph-based clustering is inherently more robust to the curse of dimensionality because it relies on local connectivity rather than absolute distance or density estimates.
- Evidence anchors:
  - [abstract] "Preliminary common clustering algorithm benchmarks on MNIST-Nd suggest that Leiden is the most robust for growing dimensions."
  - [section] "While Leiden clustering remains stable, the ARI decreases with dimensions for the centroid-based methods."
- Break condition: If the underlying graph construction fails to capture true cluster structure in very high dimensions, Leiden's advantage may disappear.

## Foundational Learning

- Concept: Variational Autoencoders and β-VAE regularization
  - Why needed here: Understanding how the m-VAE architecture and β scaling produce realistic, high-dimensional embeddings is essential for interpreting MNIST-Nd's properties.
  - Quick check question: What role does the β parameter play in β-VAE, and why must it be scaled inversely with dimensionality?

- Concept: Clustering evaluation metrics (ARI, DISCO, density peaks)
  - Why needed here: Proper interpretation of clustering performance and cluster overlap in MNIST-Nd depends on familiarity with these metrics and their implications.
  - Quick check question: How does ARI differ from other clustering metrics like homogeneity or completeness, and why is it used here?

- Concept: The curse of dimensionality and distance concentration
  - Why needed here: Recognizing why high-dimensional clustering is challenging explains the motivation for MNIST-Nd and the observed performance trends.
  - Quick check question: What is distance concentration, and how does it affect the performance of k-means and GMM in high dimensions?

## Architecture Onboarding

- Component map:
  - MNIST test set (784D) -> m-VAE encoder -> latent space (2-64D) -> m-VAE decoder -> MNIST-Nd embeddings
  - MNIST-Nd embeddings -> k-means/GMM/TMM/Leiden -> cluster assignments
  - Cluster assignments -> ARI calculation vs. ground truth

- Critical path:
  1. Train m-VAE per dimension with inverse-β scaling
  2. Encode MNIST test set → MNIST-Nd embeddings
  3. Run clustering algorithms with fixed seeds
  4. Compute ARI (vs. ground truth and across seeds)
  5. Assess robustness (bootstrapping)

- Design tradeoffs:
  - β scaling vs. reconstruction quality
  - Number of latent dimensions vs. computational cost
  - Choice of clustering algorithm vs. robustness to high-D

- Failure signatures:
  - KL loss diverges or collapses → poor embedding structure
  - Low cross-validated accuracy across dimensions → inconsistent signal-to-noise
  - DISCO scores near 0 or positive → lack of realistic overlap
  - Uniformly low ARI across all algorithms → poor separability

- First 3 experiments:
  1. Train m-VAE with 2 and 64 latent dims; compare KL/reconstruction curves
  2. Visualize t-SNE embeddings; check for overlap and cluster count
  3. Run Leiden and k-means on 2D and 64D MNIST-Nd; compare ARI trends

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of architecture (number of layers, units, activation functions) in the m-VAE affect the quality and dimensionality of the MNIST-Nd datasets?
- Basis in paper: [inferred] The paper mentions a specific m-VAE architecture but does not explore variations in architecture or their impact on the resulting datasets.
- Why unresolved: The paper uses a fixed architecture for all m-VAE models, making it unclear how sensitive the results are to this choice.
- What evidence would resolve it: Experiments comparing different m-VAE architectures and their effects on the resulting MNIST-Nd datasets, including metrics like cluster separability, noise levels, and dimensionality preservation.

### Open Question 2
- Question: How do the results of clustering algorithms on MNIST-Nd generalize to other high-dimensional datasets with different internal structures and noise characteristics?
- Basis in paper: [explicit] The authors state that future work may include generating embeddings for other datasets to explore varying internal structures and validate the findings on MNIST-Nd.
- Why unresolved: The study is limited to MNIST, which may not capture the full diversity of real-world high-dimensional datasets.
- What evidence would resolve it: Benchmarking clustering algorithms on MNIST-Nd and other high-dimensional datasets with varying internal structures and noise characteristics, comparing performance and robustness across different data types.

### Open Question 3
- Question: What is the impact of the β parameter in the β-VAE framework on the quality and structure of the MNIST-Nd datasets, and how does it affect the performance of clustering algorithms?
- Basis in paper: [explicit] The paper mentions using the β-VAE framework to scale the importance of the KL loss, but does not explore the effects of different β values on the resulting datasets or clustering performance.
- Why unresolved: The paper uses a fixed β value inversely proportional to dimensionality, without investigating the sensitivity of results to this choice.
- What evidence would resolve it: Experiments varying the β parameter and analyzing its effects on the MNIST-Nd datasets, including cluster separability, noise levels, and the performance of clustering algorithms across different dimensions.

## Limitations

- The exact architecture of the m-VAE (number of layers, neurons per layer) is not fully specified, which could affect reproducibility
- Specific training hyperparameters (learning rate, batch size, number of epochs, β values) are not provided, though the paper claims they are fixed
- The realism of noise and overlap compared to actual biological data remains to be empirically validated
- The choice of MNIST as a source dataset may limit generalizability to other data domains

## Confidence

- Mechanism 1 (Signal-to-noise ratio consistency): Medium confidence
- Mechanism 2 (Realistic overlap): Medium confidence
- Mechanism 3 (Leiden superiority in high dimensions): Medium confidence

## Next Checks

1. **Reproduce m-VAE training**: Train m-VAEs with 2 and 64 latent dimensions using the inverse-β scaling approach. Monitor KL/reconstruction loss curves and ensure the ratio of total loss to KL loss converges to similar values across dimensions. Verify cross-validated classification accuracy is approximately 90% for both.

2. **Validate embedding properties**: Generate t-SNE visualizations of MNIST-Nd embeddings for each dimensionality. Calculate DISCO scores and density peak analysis to confirm realistic overlap and noise levels. Compare these properties to embeddings from other real-world high-dimensional datasets.

3. **Benchmark clustering algorithms**: Run Leiden and k-means clustering on MNIST-Nd datasets across all dimensions. Compute ARI scores and compare performance trends. Test robustness by running multiple random seeds and bootstrapping. Validate that Leiden maintains stable performance while k-means performance degrades with increasing dimensionality.