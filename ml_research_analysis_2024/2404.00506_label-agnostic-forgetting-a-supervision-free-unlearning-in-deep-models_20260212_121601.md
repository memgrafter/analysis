---
ver: rpa2
title: 'Label-Agnostic Forgetting: A Supervision-Free Unlearning in Deep Models'
arxiv_id: '2404.00506'
source_url: https://arxiv.org/abs/2404.00506
tags:
- data
- unlearning
- forgetting
- remaining
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Label-Agnostic Forgetting (LAF), a novel
  approach to machine unlearning that removes information from forgotten data without
  requiring supervision labels during the unlearning process. LAF uses variational
  inference to approximate representation distributions of remaining data and introduces
  an extractor unlearning loss to eliminate information from forgetting data at the
  representation level.
---

# Label-Agnostic Forgetting: A Supervision-Free Unlearning in Deep Models

## Quick Facts
- arXiv ID: 2404.00506
- Source URL: https://arxiv.org/abs/2404.00506
- Reference count: 40
- Primary result: Introduces Label-Agnostic Forgetting (LAF), achieving competitive unlearning performance without requiring supervision labels

## Executive Summary
This paper presents Label-Agnostic Forgetting (LAF), a novel approach to machine unlearning that removes information from forgotten data without requiring supervision labels during the unlearning process. LAF uses variational inference to approximate representation distributions of remaining data and introduces an extractor unlearning loss to eliminate information from forgetting data at the representation level. A contrastive loss is then used to align representations between the remaining data and the original model, preserving predictive performance. Experimental results across various unlearning tasks demonstrate that LAF achieves comparable performance to state-of-the-art supervised methods while excelling in semi-supervised scenarios with limited supervision information.

## Method Summary
LAF operates by first training two separate VAEs to learn representation distributions for remaining and forgetting data. It then iteratively updates the representation extractor using an extractor unlearning loss that removes information associated with forgetting data, while simultaneously applying a representation alignment loss that preserves performance on remaining data through contrastive learning. The method alternates between these two loss functions and can optionally include a supervised repair stage if labels are available. The approach is evaluated on four datasets (DIGITS/MNIST, FASHION/MNIST, CIFAR10, SVHN) across three unlearning tasks (data removal, class removal, noisy label removal).

## Key Results
- Achieves accuracy levels around 97-99% on test data and 46-58% on attack success rates across various unlearning tasks
- Demonstrates comparable performance to state-of-the-art supervised methods without requiring label information
- Excels in semi-supervised scenarios with limited supervision information
- Shows effectiveness across multiple datasets and unlearning tasks (data removal, class removal, noisy label removal)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LAF removes information from forgetting data by shifting representation distributions while preserving remaining data performance through contrastive alignment.
- Mechanism: Uses two VAEs to model representation distributions of remaining and forgetting data separately. Then applies extractor unlearning loss (LU E) to push forgetting data representations away from their original distribution while pulling remaining data representations toward their distribution. Finally uses representation alignment loss (LRA) to match post-unlearning representations with pre-unlearning ones for the remaining data.
- Core assumption: Representation space contains sufficient information to distinguish forgetting vs remaining data, and shifting distributions can achieve forgetting without catastrophic forgetting.
- Evidence anchors: [abstract] "We utilize a variational inference approach (Kingma & Welling, 2014) to estimate the representation distribution of the remaining data and then design an extractor unlearning loss to adjust the representation extractor, aiming to eliminate the information associated with the forgetting data at the representation level."
- Break condition: If representation space lacks sufficient information to distinguish forgetting data, or if distribution shifts cause catastrophic forgetting that cannot be recovered by contrastive alignment.

### Mechanism 2
- Claim: Contrastive representation alignment preserves prediction performance by maintaining consistency between post-unlearning and pre-unlearning representations for remaining data.
- Mechanism: LRA uses cosine similarity to minimize distance between ge_U(x) and ge_D(x) for x in remaining data, while maximizing distance for x in forgetting data. This maintains classifier compatibility without requiring label information.
- Core assumption: Classifier gc_D remains effective when input representations maintain similar relative geometry to pre-unlearning state.
- Evidence anchors: [abstract] "To further address the issue of lacking supervision information, which hinders alignment with ground truth, we introduce a contrastive loss to facilitate the matching of representations between the remaining data and those of the original model, thus preserving predictive performance."
- Break condition: If classifier relies heavily on specific representation features that change during unlearning, or if remaining data distribution shifts significantly during unlearning.

### Mechanism 3
- Claim: VAE-based distribution modeling provides sufficient approximation of data representations for effective unlearning without requiring label information.
- Mechanism: Trains separate VAEs (h for remaining data, hf for forgetting data) to learn representation distributions. Uses these learned distributions to guide unlearning through reconstruction losses that measure how well extractor ge_U can reconstruct original representations.
- Core assumption: VAEs can effectively model complex representation distributions in deep networks, and reconstruction loss provides meaningful signal for distribution alignment.
- Evidence anchors: [section 3.1] "We utilize the variational inference and contrastive learning approaches and propose two novel loss functions for extractor unlearning and representation alignment."
- Break condition: If VAEs fail to capture meaningful structure in representation space, or if reconstruction loss becomes unstable during unlearning process.

## Foundational Learning

- Concept: Variational Autoencoders (VAEs) and their role in modeling probability distributions
  - Why needed here: LAF relies on VAEs to learn and approximate representation distributions for both remaining and forgetting data
  - Quick check question: What is the difference between a standard autoencoder and a variational autoencoder in terms of the latent space representation?

- Concept: Contrastive learning and similarity metrics in high-dimensional spaces
  - Why needed here: LAF uses contrastive loss (cosine similarity) to align representations before and after unlearning
  - Quick check question: How does cosine similarity differ from Euclidean distance when measuring similarity between high-dimensional representations?

- Concept: Catastrophic forgetting and its relationship to representation shifts in neural networks
  - Why needed here: LAF must avoid catastrophic forgetting while shifting representations to remove forgetting data information
  - Quick check question: What is catastrophic forgetting and why is it particularly challenging when modifying representation extractors in deep networks?

## Architecture Onboarding

- Component map: Input -> Original model gD with extractor ge_D and classifier gc_D -> VAE h (learns remaining data Dr) -> VAE hf (learns forgetting data Df) -> Extractor ge_U (updated extractor) -> LU E and LRA losses -> Optional supervised repair -> Output model gU

- Critical path:
  1. Train VAEs h and hf on D and Df respectively
  2. Iteratively update ge_U using LU E and LRA alternately
  3. (Optional) Apply supervised repair if labels available
  4. Output updated model gU

- Design tradeoffs:
  - VAE complexity vs. reconstruction accuracy: More complex VAEs may better capture representation distributions but increase computational cost
  - LU E vs. LRA balance: Too much emphasis on LU E may cause catastrophic forgetting; too much on LRA may limit forgetting effectiveness
  - Supervision vs. label-agnostic: Supervised repair improves performance but requires label availability

- Failure signatures:
  - Training instability: VAEs diverge or reconstruction loss becomes NaN
  - Performance degradation: Test accuracy drops significantly below baseline without corresponding improvement in forgetting metrics
  - Inconsistent behavior: Results vary wildly across random seeds or dataset splits

- First 3 experiments:
  1. Test VAE training stability on simple dataset (e.g., MNIST) with small subset of data
  2. Validate LU E alone on toy problem where forgetting target is known and measurable
  3. Test LRA alone by modifying representations and measuring alignment recovery on validation set

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but several important research directions emerge from the work, including extending LAF to more complex architectures like transformers, investigating the long-term effects of multiple unlearning operations, exploring applications in federated learning settings, and adapting the approach to work with non-image data types such as time-series or graph data.

## Limitations
- Performance claims lack comprehensive ablation studies showing the individual contribution of each component
- Hyperparameter sensitivity is not thoroughly explored across different datasets and tasks
- Claims about LAF's superiority in semi-supervised scenarios lack sufficient empirical validation with varying levels of supervision
- Technical details remain unspecified, including exact VAE architectures, hyperparameter settings, and precise training schedules

## Confidence
- **High confidence**: The core LAF framework combining variational inference with contrastive alignment is technically sound and builds on established methods
- **Medium confidence**: Performance claims relative to baselines are supported by experimental results, though hyperparameter sensitivity is not thoroughly explored
- **Low confidence**: Claims about LAF's superiority in semi-supervised scenarios lack sufficient empirical validation with varying levels of supervision

## Next Checks
1. Conduct hyperparameter sensitivity analysis across the three datasets to determine robustness of LU E and LRA components
2. Perform controlled experiments isolating the contribution of VAE-based distribution modeling versus contrastive alignment
3. Test LAF performance with varying levels of supervision (5%, 25%, 50% labeled data) to validate semi-supervised claims