---
ver: rpa2
title: 'SongGLM: Lyric-to-Melody Generation with 2D Alignment Encoding and Multi-Task
  Pre-Training'
arxiv_id: '2412.18107'
source_url: https://arxiv.org/abs/2412.18107
tags:
- alignment
- lyrics
- pre-training
- melodies
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SongGLM, a lyric-to-melody generation system
  that leverages 2D alignment encoding and multi-task pre-training based on the General
  Language Model (GLM). The system introduces a unified symbolic song representation
  with word-level and phrase-level (2D) alignment encoding to capture the lyric-melody
  alignment, and a multi-task pre-training framework with hierarchical blank infilling
  objectives to ensure the lyric-melody harmony.
---

# SongGLM: Lyric-to-Melody Generation with 2D Alignment Encoding and Multi-Task Pre-Training

## Quick Facts
- arXiv ID: 2412.18107
- Source URL: https://arxiv.org/abs/2412.18107
- Authors: Jiaxing Yu; Xinda Wu; Yunfei Xu; Tieyao Zhang; Songruoyao Wu; Le Ma; Kejun Zhang
- Reference count: 10
- One-line primary result: SongGLM achieves state-of-the-art lyric-to-melody generation with 96.83% alignment distribution similarity and significant improvements across all metrics.

## Executive Summary
SongGLM introduces a novel approach to lyric-to-melody generation that addresses two key challenges: lyric-melody alignment and harmony modeling. The system uses 2D alignment encoding (word ID + phrase ID) to capture the hierarchical relationship between lyrics and melodies, and a multi-task pre-training framework with hierarchical blank infilling objectives to ensure lyric-melody coherence. Trained on a large-scale dataset of 206,884 English MIDI songs, SongGLM demonstrates significant improvements over previous baseline methods in both objective and subjective evaluations.

## Method Summary
SongGLM employs a unified symbolic song representation with 2D alignment encoding to capture word-level and phrase-level lyric-melody alignments. The system uses a multi-task pre-training framework with three hierarchical blank infilling objectives (n-gram, phrase, and song-level) to learn multi-scale harmony. The model incorporates lyric-melody relationships through syllable stress and melodic peak (SMR) and syllable stress and rhythm skeleton (SRR) relationships to select harmonized n-grams for pre-training. The approach leverages the General Language Model (GLM) architecture with both content and alignment embeddings to generate coherent melodies from given lyrics.

## Key Results
- Achieves 96.83% alignment distribution similarity, outperforming best baseline by large margin
- Maintains high pitch (96.50%), duration (96.48%), and IOI (94.10%) distribution similarities
- Records melody distance of 3.85, significantly better than baseline methods
- Demonstrates improvements across subjective metrics including richness, consistency, and singability

## Why This Works (Mechanism)

### Mechanism 1: 2D Alignment Encoding
The Word ID + Phrase ID system directly links each note to its corresponding word/phrase in the lyric sequence, enabling the model to learn accurate one-to-many and one-to-one alignments. This explicit positional linkage replaces indirect alignment strategies, allowing the model to attend to correct lyric context regardless of syllable-to-note mapping patterns.

### Mechanism 2: Harmonized N-gram Extraction
The model computes t-statistic scores for lyric n-grams and melodic n-grams, then weights them by a concentration term that favors n-grams with stable syllable stress ↔ melodic peak or syllable stress ↔ rhythm skeleton correspondences. These harmonized n-grams become the sampling pool for blank infilling, ensuring musically coherent word-note patterns.

### Mechanism 3: Multi-Task Hierarchical Pre-training
The three-level blank infilling objectives (n-gram at 15%, phrase at 50%, song at 50% of note sequence) force the model to learn multi-scale lyric-melody coherence. Word-level spans enforce note-word matching, phrase-level spans encourage melodic phrase continuity, and song-level spans push global structural coherence.

## Foundational Learning

- **Concept**: Lyric-melody alignment modeling
  - Why needed here: Without explicit alignment, the model cannot map syllables to correct number of notes, leading to rhythmic/melodic mismatches
  - Quick check question: In the unified representation, what two attributes encode alignment, and how do they differ for Word vs Note tokens?

- **Concept**: Syllable stress and musical accent correspondence
  - Why needed here: Lyric-melody harmony depends on stressing right syllables at musically accented moments; ignoring this yields unnatural phrasing
  - Quick check question: How are the SMR and SRR relationships computed, and which melodic features do they link to syllable stress?

- **Concept**: Multi-task pre-training objectives
  - Why needed here: Single-task training often overfits to local patterns; hierarchical objectives enforce both fine-grained note-word mapping and large-scale structural consistency
  - Quick check question: What are the three span-level objectives, and what percentage of the note sequence does each cover?

## Architecture Onboarding

- **Component map**: 
  Tokenization -> Unified symbolic sequence -> 2D alignment embedding layer -> Content embedding layer -> GLM Transformer encoder-decoder -> Multi-task pre-training scheduler -> Fine-tuning head -> Output decoder

- **Critical path**:
  1. Tokenize lyrics and melody into unified sequence
  2. Generate 2D alignment embeddings
  3. Concatenate content and alignment embeddings
  4. Feed through Transformer layers
  5. Apply appropriate self-attention mask for pre-training or fine-tuning
  6. Predict masked spans (pre-training) or next token (fine-tuning/inference)

- **Design tradeoffs**:
  - Alignment granularity: 256 Word IDs and 128 Phrase IDs vs. larger vocabularies for more precise mapping but higher memory
  - Span sampling strategy: Fixed percentages (15%/50%/50%) vs. adaptive lengths based on actual phrase boundaries
  - Special token handling: Single compound token per note/word vs. separate attribute tokens (reduces sequence length but may lose attribute granularity)

- **Failure signatures**:
  - Misalignment: Generated melody places too many/few notes per syllable compared to ground truth
  - Poor harmony: High pitch/duration distribution similarity but low subjective singability scores
  - Overfitting to training style: Model generates melodies that match training statistics but lack creativity

- **First 3 experiments**:
  1. Ablation of 2D alignment encoding: Train baseline without alignment attributes and measure alignment distribution similarity drop
  2. Hyperparameter sweep on span sizes: Vary percentage of notes masked at each level and evaluate harmony metrics
  3. Stress-accent relationship removal: Train without SMR/SRR relationship scoring and compare n-gram selection quality and final harmony metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed 2D alignment encoding be extended to handle more complex alignments beyond one-syllable/word-to-multiple-notes, such as sub-syllable or phrase-level alignments?
- Basis in paper: [explicit] The paper mentions that previous works simplify alignment to one-syllable/word-to-one-note and that the proposed 2D encoding captures word and phrase level alignments, but does not explore more granular alignments
- Why unresolved: The current representation focuses on word and phrase level alignment, but more nuanced alignments (e.g., sub-syllable, phoneme-level) could potentially improve harmony and naturalness
- What evidence would resolve it: Comparative experiments evaluating models with sub-syllable or phoneme-level alignment encoding against the current word and phrase level approach on metrics like alignment distribution similarity and subjective singability

### Open Question 2
- Question: What is the impact of incorporating additional linguistic features, such as rhyme or semantic coherence, into the harmonized n-gram extraction process?
- Basis in paper: [inferred] The paper establishes relationships based on syllable stress, melodic peak, and rhythm skeleton, but does not explore other linguistic features that could enhance lyric-melody harmony
- Why unresolved: While the current approach captures stress-based relationships, other linguistic features might contribute to more coherent and musically pleasing results
- What evidence would resolve it: Objective and subjective evaluation comparing models with and without additional linguistic features in the n-gram extraction process

### Open Question 3
- Question: How does the proposed multi-task pre-training framework perform on other music generation tasks, such as melody harmonization or accompaniment generation?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of the framework for lyric-to-melody generation but does not explore its applicability to other tasks
- Why unresolved: The framework's ability to capture multi-scale harmony suggests potential for other music generation applications, but its performance on these tasks is unknown
- What evidence would resolve it: Experiments applying the framework to other music generation tasks and comparing results with state-of-the-art models

## Limitations

- Limited empirical validation of core mechanisms with weak evidence for alignment accuracy improvements from 2D encoding
- Dataset and generalization concerns regarding potential biases and model's ability to handle diverse musical styles
- High computational requirements (250,000 steps on NVIDIA A100 80GB GPU) may be prohibitive for reproduction

## Confidence

**High Confidence**: Overall architecture and methodology are clearly described with well-defined evaluation metrics and experimental setup.

**Medium Confidence**: Reported performance improvements are supported by quantitative metrics, but lack of ablation studies reduces confidence in which specific mechanisms drive improvements.

**Low Confidence**: Effectiveness of SMR/SRR lyric-melody relationship incorporation and sufficiency of 128 Phrase ID bins for diverse musical phrasing are not empirically validated.

## Next Checks

1. **Ablation Study of 2D Alignment Encoding**: Train and evaluate a baseline model without Word ID and Phrase ID alignment attributes to measure specific contribution to alignment distribution similarity improvements.

2. **Stress-Accent Relationship Validation**: Design experiment quantifying strength of SMR and SRR relationships in training data and testing model performance when these relationships are removed from n-gram extraction process.

3. **Cross-Genre Generalization Test**: Evaluate model on lyric-melody pairs from musical genres not well-represented in training data (e.g., jazz, classical art songs) to assess robustness of 2D alignment encoding across diverse musical styles.