---
ver: rpa2
title: 'SuperPos-Prompt: Enhancing Soft Prompt Tuning of Language Models with Superposition
  of Multi Token Embeddings'
arxiv_id: '2406.05279'
source_url: https://arxiv.org/abs/2406.05279
tags:
- prompt
- tuning
- prompts
- learning
- soft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of optimizing soft prompt tuning
  for language models, especially with smaller datasets. It introduces SuperPos-Prompt,
  a reparameterization technique that employs the superposition of multiple pretrained
  vocabulary embeddings to enhance the learning of soft prompts.
---

# SuperPos-Prompt: Enhancing Soft Prompt Tuning of Language Models with Superposition of Multi Token Embeddings

## Quick Facts
- arXiv ID: 2406.05279
- Source URL: https://arxiv.org/abs/2406.05279
- Authors: MohammadAli SadraeiJavaeri; Ehsaneddin Asgari; Alice Carolyn McHardy; Hamid Reza Rabiee
- Reference count: 21
- One-line primary result: SuperPos-Prompt outperforms Residual Prompt tuning by +6.4 (T5-Small) and +5.0 (T5-Base) on GLUE/SuperGLUE benchmarks

## Executive Summary
This paper addresses the challenge of optimizing soft prompt tuning for language models, particularly when working with smaller datasets. The authors introduce SuperPos-Prompt, a reparameterization technique that uses the superposition of multiple pretrained vocabulary embeddings to enhance soft prompt learning. By representing each prompt token as a weighted sum of multiple token embeddings, the method captures richer semantic information than traditional single-token approaches. Experiments across GLUE and SuperGLUE benchmarks demonstrate that SuperPos-Prompt consistently outperforms Residual Prompt tuning while achieving faster convergence.

## Method Summary
SuperPos-Prompt is a parameter-efficient fine-tuning technique that reparameterizes soft prompt tokens using a superposition of multiple pretrained token embeddings. Each prompt token is represented as a weighted sum of m randomly sampled token embeddings from the embedding layer, with the superposition weights jointly optimized during training. The method employs a T5v1.1-LM adapt model with 10 trainable prompt tokens, omitting dropout from the frozen network for improved stability. The approach is evaluated on GLUE and SuperGLUE benchmarks using T5-Small and T5-Base models, comparing performance against Residual Prompt tuning and full fine-tuning baselines.

## Key Results
- SuperPos-Prompt achieves an average score increase of +6.4 on T5-Small and +5.0 on T5-Base compared to Residual Prompt tuning
- The method demonstrates faster convergence than baseline approaches across all tested tasks
- SuperPos-Prompt occasionally outperforms full fine-tuning methods while using significantly fewer parameters
- Omitting dropout from the frozen network yields consistent improvements across all scenarios and tuning methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Superposition of multiple token embeddings improves soft prompt learning by leveraging diverse pretrained information.
- Mechanism: Each prompt token is represented as a weighted sum of m randomly sampled token embeddings from the embedding layer. This weighted sum is jointly optimized, allowing the prompt to capture richer semantic information than a single token embedding.
- Core assumption: Multiple diverse token embeddings provide complementary information that, when combined, form a more robust representation than any single embedding.
- Evidence anchors:
  - [abstract] "a new reparameterization technique employing the superposition of multiple pretrained vocabulary embeddings to improve the learning of soft prompts"
  - [section] "We propose a method called SuperPos-Prompt, which involves using a superposition, or a weighted sum of several chosen tokens, for each prompt embedding"
  - [corpus] Weak evidence - no direct mention of superposition mechanism in corpus neighbors
- Break condition: If sampled tokens are too similar or if the superposition weights collapse to one dominant token, the benefit diminishes.

### Mechanism 2
- Claim: Omitting dropout from the frozen network improves convergence speed and stability in prompt tuning.
- Mechanism: Dropout, typically used as regularization, may overly constrain the already limited parameter space of soft prompts. Removing dropout allows more stable learning of the few tunable parameters.
- Core assumption: The constraint imposed by dropout is unnecessary and harmful when only a small number of parameters (e.g., 10 prompts) are being tuned.
- Evidence anchors:
  - [abstract] "Additionally, we demonstrate enhanced performance and rapid convergence by omitting dropouts from the frozen network"
  - [section] "we observed that including weight decay in the optimizer reduced the norm of E, resulting in significant information loss. To address this issue, we exclude E from weight decay"
  - [corpus] No direct evidence in corpus neighbors about dropout omission
- Break condition: If the model overfits without dropout in scenarios with larger tuning parameter sets or noisier datasets.

### Mechanism 3
- Claim: Using pretrained token embeddings as initialization for prompt tokens provides a better starting point than random initialization.
- Mechanism: The prompt tokens are initialized by sampling unique token embeddings from the embedding layer, giving them semantic meaning from the start rather than starting from random vectors.
- Core assumption: Pretrained token embeddings contain meaningful semantic information that can be leveraged for downstream tasks, making them superior initializations compared to random vectors.
- Evidence anchors:
  - [section] "This approach is motivated by the observation that initializing prompts with token representations is generally more effective than starting with random vectors"
  - [section] "Prompts were initialized by sampling 10 unique token embeddings from the embedding layer"
  - [corpus] No direct mention in corpus neighbors, but aligns with general prompt tuning literature
- Break condition: If the task domain is vastly different from pretraining data, pretrained embeddings might not provide useful initialization.

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: The paper focuses on soft prompt tuning, a PEFT method that modifies only a small number of parameters (prompt tokens) rather than full fine-tuning
  - Quick check question: What distinguishes soft prompt tuning from full fine-tuning in terms of parameter modification?

- Concept: Transfer learning and foundation models
  - Why needed here: The study uses T5 models pretrained on large datasets as foundation models, then adapts them to new tasks via soft prompt tuning
  - Quick check question: Why is transfer learning particularly important when working with large language models and limited downstream data?

- Concept: Embedding spaces and token representations
  - Why needed here: The superposition mechanism relies on understanding that token embeddings live in a high-dimensional space and can be combined linearly
  - Quick check question: What property of embedding spaces makes it possible to combine multiple token embeddings to form new representations?

## Architecture Onboarding

- Component map:
  Frozen T5 encoder-decoder model -> Soft prompt layer (10 trainable prompt tokens) -> Superposition mechanism (each prompt = weighted sum of m sampled token embeddings) -> Optimizer (AdamW with specific learning rates and weight decay settings) -> Dropout removal from frozen network

- Critical path:
  1. Sample m token embeddings from the embedding layer
  2. Initialize superposition weights (pâ€²_i vectors)
  3. During training, optimize superposition weights while keeping sampled embeddings fixed
  4. Compute final prompt embeddings as weighted sums
  5. Concatenate prompts with input embeddings and pass through frozen T5 model

- Design tradeoffs:
  - Number of sampled tokens (m): More tokens provide richer information but increase computation
  - Prompt token count: 10 tokens used here balances expressiveness with parameter efficiency
  - Weight decay exclusion for E matrix: Prevents information loss but may lead to larger embedding norms
  - Dropout removal: Improves stability but may increase overfitting risk in some scenarios

- Failure signatures:
  - Poor performance with small m values (e.g., m=1) indicating insufficient diversity
  - Vanishing gradients if superposition weights become extremely small or large
  - Mode collapse where all prompt tokens converge to similar representations
  - Numerical instability if E matrix norm grows too large without weight decay

- First 3 experiments:
  1. Verify that removing dropout improves stability by training with and without dropout on a simple task
  2. Test the effect of different m values (e.g., m=16, 64, 128, 256) on a representative task to find optimal setting
  3. Compare SuperPos-Prompt against simple prompt tuning with random initialization on a held-out task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SuperPos-Prompt scale with model size, particularly for models in the billion-parameter range?
- Basis in paper: [inferred] The paper acknowledges that hardware limitations prevented testing with large-scale models and explicitly states "Future work includes a more extensive comparison of SUPER POS-P ROMPT with a broader range of prompting techniques in different dataset scenarios, an endeavor constrained in this study by computational resource limitations."
- Why unresolved: The authors were unable to test the method on larger models due to computational constraints, leaving the scalability question unanswered.
- What evidence would resolve it: Empirical results showing performance comparisons of SuperPos-Prompt across various model sizes (e.g., T5-Large, T5-3B, T5-11B) on the same benchmark tasks used in the paper.

### Open Question 2
- Question: What is the optimal number of sampled tokens (m) for SuperPos-Prompt across different model sizes and task complexities?
- Basis in paper: [explicit] The paper states "During our experiments, we observed that including weight decay in the optimizer reduced the norm of E, resulting in significant information loss" and "we exclude E from weight decay," indicating hyperparameter sensitivity. Additionally, they mention "we found no clear correlation between these patterns and the task descriptions" regarding learned prompts.
- Why unresolved: The authors only tested m=128 for their experiments and observed no clear correlation between prompt patterns and task descriptions, suggesting the need for more systematic investigation of optimal m values.
- What evidence would resolve it: A comprehensive study varying m across different model sizes, task complexities, and dataset sizes to identify optimal settings for each scenario.

### Open Question 3
- Question: How does the exclusion of dropout from the frozen network affect other parameter-efficient fine-tuning methods beyond soft prompt tuning?
- Basis in paper: [explicit] The paper states "Our research indicates that omitting dropout from the frozen network can yield more efficient and expedited convergence in prompt tuning" and notes this observation has not been addressed in prior studies.
- Why unresolved: The authors only tested dropout exclusion on prompt tuning methods and did not investigate its impact on other parameter-efficient fine-tuning techniques like LoRA or adapters.
- What evidence would resolve it: Systematic experiments applying dropout exclusion to various parameter-efficient fine-tuning methods (LoRA, adapters, prefix tuning) across different model architectures and task types.

## Limitations

- The study is limited to T5 models and English language tasks, leaving generalization to other architectures and languages untested
- Optimal hyperparameter m=128 is determined empirically without exploring sensitivity across different task complexities
- The mechanism by which superposition improves learning remains theoretically unclear, with no ablation studies on different m values

## Confidence

**High Confidence Claims**:
- SuperPos-Prompt outperforms Residual Prompt tuning on GLUE and SuperGLUE benchmarks with T5-Small and T5-Base models
- Removing dropout from the frozen network consistently improves performance across all tested scenarios
- The SuperPos-Prompt technique is implementable and reproducible with the described methodology

**Medium Confidence Claims**:
- SuperPos-Prompt occasionally outperforms full fine-tuning methods
- The optimal m value is 128 for the tested tasks
- Pretrained token embeddings provide better initialization than random vectors

**Low Confidence Claims**:
- The specific mechanism by which superposition improves learning (beyond the intuitive explanation)
- The general applicability of findings to other model architectures and domains
- The long-term stability and generalization of models trained with SuperPos-Prompt

## Next Checks

**Validation Check 1**: Conduct ablation studies across a broader range of m values (16, 32, 64, 128, 256, 512) on 3-4 representative tasks to understand the sensitivity of performance to the number of sampled tokens and identify whether the optimal m varies by task complexity.

**Validation Check 2**: Test SuperPos-Prompt on different model architectures (BERT, RoBERTa, GPT-2) and non-English GLUE-style benchmarks to evaluate whether the benefits generalize beyond T5 models and English language tasks.

**Validation Check 3**: Implement a controlled experiment comparing SuperPos-Prompt with and without dropout across different dataset sizes (small, medium, large) to quantify the trade-off between stability and overfitting risk in various scenarios.