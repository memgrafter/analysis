---
ver: rpa2
title: 'CBMAP: Clustering-based manifold approximation and projection for dimensionality
  reduction'
arxiv_id: '2404.17940'
source_url: https://arxiv.org/abs/2404.17940
tags:
- data
- cbmap
- datasets
- methods
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of preserving both global and local
  structures in dimensionality reduction, particularly when dealing with clusters
  in high-dimensional data. Existing methods like t-SNE, UMAP, TriMap, and PaCMAP
  tend to distort global structures and heavily rely on hyperparameters, making results
  sensitive to parameter settings.
---

# CBMAP: Clustering-based manifold approximation and projection for dimensionality reduction

## Quick Facts
- arXiv ID: 2404.17940
- Source URL: https://arxiv.org/abs/2404.17940
- Reference count: 25
- Key outcome: Introduces a clustering-based dimensionality reduction method that preserves both global and local structures while requiring minimal hyperparameters

## Executive Summary
CBMAP addresses the challenge of preserving both global and local structures in dimensionality reduction, particularly for clustered high-dimensional data. Existing methods like t-SNE, UMAP, TriMap, and PaCMAP often distort global structures and are sensitive to hyperparameter settings. CBMAP introduces a novel approach that first clusters the high-dimensional data, then embeds points in low-dimensional space while maintaining membership values to cluster centers, ensuring that clusters are preserved across dimensions.

The method demonstrates effectiveness through benchmark datasets, showing improved global structure preservation while maintaining local structure accuracy. CBMAP also enables projection of unseen test data without re-clustering, addressing a critical need in machine learning applications. The approach offers speed, scalability, and minimal reliance on hyperparameters compared to existing methods.

## Method Summary
CBMAP is a dimensionality reduction technique that uses k-means clustering to find high-dimensional cluster centers, computes Gaussian-based membership values between points and centers, then optimizes low-dimensional embeddings to minimize the difference between high-dimensional and low-dimensional membership matrices using Frobenius norm. The method saves cluster centers and sigma values from training data to enable projection of test data without re-clustering.

## Key Results
- Preserves both global and local structures better than existing methods through cluster-based membership preservation
- Requires minimal hyperparameter tuning compared to perplexity-based methods
- Enables projection of unseen test data using saved cluster parameters
- Demonstrates scalability and speed advantages on benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CBMAP preserves both global and local structures by using cluster-based membership values that are enforced across dimensions.
- Mechanism: The algorithm first clusters the high-dimensional data, computes membership values between each point and each cluster center, and then optimizes the low-dimensional embedding to minimize the difference between these membership matrices.
- Core assumption: Membership values based on Gaussian distance to cluster centers in both high- and low-dimensional spaces are a valid proxy for preserving structure.
- Evidence anchors:
  - [abstract] "CBMAP introduces a clustering-based approach that preserves both global and local structures by first clustering high-dimensional data, then embedding data points in low-dimensional space while maintaining membership values to cluster centers."
  - [section] "CBMAP aims to find the optimal projection... such that, the membership matrix... computed in the low-dimensional space... minimizes the Frobenius norm provided in Eq.2."
- Break condition: If the data does not have clear cluster structure, membership values may not be meaningful and the method may underperform.

### Mechanism 2
- Claim: CBMAP enables projection of unseen test data without re-clustering.
- Mechanism: CBMAP saves the cluster centers and sigma values from the training set, and uses them to compute membership values and optimize embeddings for test data.
- Core assumption: The cluster structure of the training data generalizes to test data.
- Evidence anchors:
  - [abstract] "Importantly, CBMAP enables low-dimensional projection of test data, addressing a critical need in machine learning applications."
  - [section] "With these parameters derived from the training data, the embeddings for the test data can be optimally obtained in the low-dimensional space."
- Break condition: If the test data distribution is significantly different from training data, the projections may be unreliable.

### Mechanism 3
- Claim: CBMAP is robust to hyperparameter choice and avoids the pitfalls of perplexity-based methods.
- Mechanism: By basing optimization on cluster memberships rather than nearest-neighbor relationships with a perplexity parameter, CBMAP reduces sensitivity to hyperparameter tuning.
- Core assumption: Clustering structure is more stable and interpretable than local neighborhood graphs with a perplexity parameter.
- Evidence anchors:
  - [abstract] "offering speed, scalability, and minimal reliance on hyperparameters."
  - [section] "these methods heavily rely on hyperparameters, making their results sensitive to parameter settings."
- Break condition: If the number of clusters is chosen poorly, performance may degrade even without explicit hyperparameters.

## Foundational Learning

- Concept: K-means clustering
  - Why needed here: CBMAP relies on clustering high-dimensional data to establish reference points for structure preservation.
  - Quick check question: What does the k-means algorithm optimize for, and how does it initialize cluster centers?

- Concept: Gaussian membership functions
  - Why needed here: CBMAP uses Gaussian-based membership values to measure the similarity between points and cluster centers in both high- and low-dimensional spaces.
  - Quick check question: How does the variance parameter (sigma) affect the spread of membership values?

- Concept: Frobenius norm as a loss function
  - Why needed here: CBMAP minimizes the Frobenius norm of the difference between membership matrices to ensure structure preservation.
  - Quick check question: What does the Frobenius norm measure, and why is it suitable for comparing membership matrices?

## Architecture Onboarding

- Component map: Input data → K-means clustering → Compute high-dimensional memberships → Project cluster centers (PCA or random) → Initialize low-dimensional points → Optimize membership alignment → Output low-dimensional embedding. Also saves cluster centers and sigma for test projection.
- Critical path: Clustering → Membership computation → Optimization loop. The bottleneck is usually the clustering step for large datasets.
- Design tradeoffs: Uses k-means (fast but assumes spherical clusters) vs. other clustering methods; uses Gaussian membership (smooth but may not fit skewed distributions); trades speed for structure preservation vs. methods like UMAP.
- Failure signatures: Poor cluster separation leads to ambiguous memberships; large datasets cause slow clustering; wrong number of clusters yields poor GS scores.
- First 3 experiments:
  1. Run CBMAP on the Iris dataset with default k=20; check GS and cluster preservation visually.
  2. Compare CBMAP embeddings of Swiss roll with and without PCA initialization of cluster centers.
  3. Project test data from MNIST using CBMAP trained on training split; compare GS to PCA.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of clusters (k) for CBMAP to balance computational efficiency and accuracy across diverse datasets?
- Basis in paper: [explicit] The paper mentions that CBMAP's performance improves with more clusters, but does not specify an optimal k for different dataset sizes and types.
- Why unresolved: The authors used arbitrary k values (20 for datasets <5000 samples, 40 for datasets >5000 samples) for comparison purposes, and experiments showed that higher k values generally improve global score (GS) but do not affect local accuracy (ACC).
- What evidence would resolve it: Systematic experiments varying k across different dataset sizes, types, and structures to determine a relationship between k and performance metrics (GS, ACC, computational time).

### Open Question 2
- Question: How would alternative membership functions affect CBMAP's performance with non-normal data distributions?
- Basis in paper: [inferred] The paper acknowledges that the Gaussian membership function may not optimally represent skewed data distributions, and suggests that data-oriented clustering methods and membership functions could improve performance.
- Why unresolved: The current implementation uses a fixed Gaussian membership function, and the paper only suggests the need for different membership functions without exploring alternatives.
- What evidence would resolve it: Comparative experiments using different membership functions (e.g., Student's t-distribution, power-law functions) on datasets with known non-normal distributions and measuring impact on CBMAP's performance.

### Open Question 3
- Question: What is the impact of CBMAP's clustering method on its performance with high-dimensional data containing varying cluster densities?
- Basis in paper: [explicit] The paper states that CBMAP relies on k-means clustering, which performs better with normally distributed or normalized data, and may not perform optimally for datasets with non-normal distributions.
- Why unresolved: The paper only mentions k-means as the default clustering method and suggests that alternative clustering methods could be selected based on data structure, but does not explore how different clustering methods affect CBMAP's performance.
- What evidence would resolve it: Experiments comparing CBMAP's performance using different clustering algorithms (e.g., DBSCAN, hierarchical clustering) on high-dimensional datasets with varying cluster densities and distributions.

## Limitations

- Performance on datasets without clear cluster structure is not evaluated
- Comparison with baseline methods uses default parameters which may not be optimal
- Scalability claims need validation on larger datasets beyond those tested

## Confidence

- High confidence in the core mechanism of using cluster membership values for structure preservation
- Medium confidence in the claims about minimal hyperparameter sensitivity, as only a limited range of parameter variations were tested
- Medium confidence in the speed and scalability claims, pending larger-scale experiments

## Next Checks

1. Test CBMAP on non-clustered synthetic datasets (e.g., continuous manifolds like S-curve without clusters) to evaluate performance when cluster structure is absent
2. Conduct a systematic hyperparameter sensitivity analysis varying both k (number of clusters) and max_iter to quantify robustness claims
3. Evaluate CBMAP on a dataset with 100K+ samples to validate scalability claims and benchmark runtime against other methods