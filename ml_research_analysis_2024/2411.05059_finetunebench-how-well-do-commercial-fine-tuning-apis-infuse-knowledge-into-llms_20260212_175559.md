---
ver: rpa2
title: 'FineTuneBench: How well do commercial fine-tuning APIs infuse knowledge into
  LLMs?'
arxiv_id: '2411.05059'
source_url: https://arxiv.org/abs/2411.05059
tags:
- knowledge
- fine-tuning
- question
- training
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study introduces FineTuneBench, a dataset and evaluation framework\
  \ for assessing commercial fine-tuning APIs' ability to infuse knowledge into LLMs.\
  \ It tests five models\u2014GPT-4o, GPT-4o mini, GPT-3.5 Turbo, Gemini 1.5 Pro,\
  \ and Gemini 1.5 Flash\u2014on learning new information (e.g., news events, fictional\
  \ people profiles) and updating existing knowledge (e.g., medical guidelines, code\
  \ frameworks)."
---

# FineTuneBench: How well do commercial fine-tuning APIs infuse knowledge into LLMs?

## Quick Facts
- arXiv ID: 2411.05059
- Source URL: https://arxiv.org/abs/2411.05059
- Authors: Eric Wu; Kevin Wu; James Zou
- Reference count: 9
- Primary result: Commercial fine-tuning APIs show limited knowledge infusion, with average generalization accuracy of 37% for new knowledge and 19% for updated knowledge.

## Executive Summary
This paper introduces FineTuneBench, a comprehensive dataset and evaluation framework for assessing commercial fine-tuning APIs' ability to infuse knowledge into large language models. The study tests five models—GPT-4o, GPT-4o mini, GPT-3.5 Turbo, Gemini 1.5 Pro, and Gemini 1.5 Flash—on learning new information (e.g., news events, fictional people profiles) and updating existing knowledge (e.g., medical guidelines, code frameworks). The results reveal significant limitations: while models can memorize training data with near-perfect accuracy, they struggle to generalize this knowledge to rephrased or modified questions, indicating that memorization does not translate into true knowledge acquisition.

The findings have important implications for the use of commercial fine-tuning APIs in real-world applications. Despite the promise of these services for knowledge injection, the study demonstrates that current implementations have substantial shortcomings in reliably adapting LLMs for knowledge infusion. The research highlights the gap between what these APIs can achieve in terms of data memorization versus genuine knowledge understanding and generalization.

## Method Summary
The study evaluates commercial fine-tuning APIs using a carefully constructed FineTuneBench dataset containing 625 training questions and 1075 test questions across four knowledge domains: news events, fictional people profiles, medical guidelines, and code frameworks. Models are fine-tuned on 50-150 unique facts per dataset using supervised instruction-response pairs, with training durations ranging from 1 to 30 epochs. The evaluation measures both memorization (accuracy on exact training questions) and generalization (accuracy on rephrased, modified, and derived questions). The researchers use an LLM-as-judge approach to assess model responses, and they test multiple fine-tuning configurations including varying numbers of epochs, learning rates, and batch sizes.

## Key Results
- Commercial fine-tuning APIs can achieve near-perfect memorization of training data (100% accuracy) but show limited generalization capability
- Average generalization accuracy was 37% for new knowledge infusion and only 19% for updating existing knowledge
- GPT-4o mini performed best among tested models, while Gemini models showed significant failure to learn effectively
- Models could recapitulate training data perfectly after 30 epochs but failed when questions were rephrased or modified
- Updated knowledge tasks (medical guidelines, code frameworks) were harder than learning new knowledge, with GPT-4o achieving only 10% accuracy on rephrased coding questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning commercial LLMs with supervised instruction-response pairs induces memorization of the training data.
- Mechanism: The models are exposed to a finite set of question-answer pairs across multiple epochs, leading to overfitting where the model learns to reproduce the exact answers for the exact questions.
- Core assumption: Commercial fine-tuning APIs use standard supervised fine-tuning approaches, possibly with limited hyperparameter control, making overfitting likely.
- Evidence anchors:
  - [abstract] "They can almost perfectly memorize QA pairs; in other words, they are able to recapitulate the training data with near 100% accuracy after training for 30 epochs."
  - [section] "we observe that strong memorization occurs as soon as 10 training epochs."
- Break condition: If the fine-tuning method includes regularization or dropout that prevents overfitting, or if the dataset size is significantly larger relative to model capacity.

### Mechanism 2
- Claim: Fine-tuned models fail to generalize knowledge beyond exact question-answer pairs.
- Mechanism: The models do not truly internalize the semantic content of the facts but instead learn surface-level mappings between specific questions and answers. When questions are rephrased or modified, the learned mapping no longer applies.
- Core assumption: The instruction fine-tuning paradigm used by commercial APIs does not encourage semantic understanding or robust reasoning.
- Evidence anchors:
  - [abstract] "They sometimes perform poorly on the rephrased or derivative questions, indicating that the memorization did not translate into true knowledge acquisition in many instances."
  - [section] "no models showed improvement after fine-tuning on the comparison questions, indicating a limited capability to synthesize knowledge across multiple training examples."
- Break condition: If the fine-tuning incorporates techniques that encourage generalization, such as contrastive learning or data augmentation.

### Mechanism 3
- Claim: Updating existing knowledge is harder than learning new knowledge due to the need to displace prior knowledge.
- Mechanism: When models are fine-tuned to update information, they must both unlearn the old information and learn the new information. This dual task is more complex than simply adding new information, leading to lower accuracy.
- Core assumption: The models' pretraining knowledge creates strong priors that resist modification, especially when the new information contradicts the old.
- Evidence anchors:
  - [abstract] "On our code dataset, OpenAI's fine-tuned models have an average accuracy of 10% on rephrased coding questions, the lowest among all four datasets."
  - [section] "we find that on average, commercial fine-tuning models on updated knowledge yields lower generalization performance compared to new knowledge."
- Break condition: If the fine-tuning method includes explicit mechanisms for knowledge editing or unlearning.

## Foundational Learning

- Concept: Supervised fine-tuning
  - Why needed here: The study evaluates how well commercial APIs can fine-tune models using supervised instruction-response pairs.
  - Quick check question: What is the difference between supervised fine-tuning and unsupervised pre-training in terms of the data used?

- Concept: Generalization vs. memorization
  - Why needed here: The core finding is that models can memorize training data but fail to generalize, so understanding this distinction is crucial.
  - Quick check question: How can you design a test to distinguish whether a model has memorized or truly learned a concept?

- Concept: Knowledge injection vs. retrieval-augmented generation (RAG)
  - Why needed here: The paper contrasts fine-tuning (knowledge injection) with RAG as two different approaches to incorporating new information.
  - Quick check question: What are the advantages and disadvantages of embedding knowledge directly in a model versus retrieving it at inference time?

## Architecture Onboarding

- Component map: User provides training data → Commercial fine-tuning API performs supervised fine-tuning → User receives model endpoint → User evaluates model on test data
- Critical path: Data preparation → API fine-tuning → Model deployment → Performance evaluation (memorization and generalization)
- Design tradeoffs: Commercial APIs offer convenience and accessibility but sacrifice control over the fine-tuning process, potentially leading to suboptimal knowledge infusion
- Failure signatures: High memorization accuracy but low generalization accuracy indicates overfitting; refusal to answer even after fine-tuning suggests safety filters or instruction priors are too strong
- First 3 experiments:
  1. Test the baseline model's accuracy on the FineTuneBench dataset before fine-tuning to establish the starting point
  2. Fine-tune the model on a subset of the training data and evaluate memorization accuracy to confirm the fine-tuning process is working
  3. Evaluate the fine-tuned model on rephrased and modified versions of the questions to assess generalization ability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different hyperparameter configurations (beyond just the number of epochs) impact the knowledge infusion capabilities of commercial fine-tuning APIs?
- Basis in paper: [inferred] The paper states "we used all default parameters except varying the number of epochs" and mentions limited evaluations of varying learning rates and batch sizes "did not observe significant model improvement," but acknowledges "performance may potentially improve with different sets of hyperparameters."
- Why unresolved: The paper only tested a limited set of hyperparameters and found no significant improvement, but did not perform a comprehensive hyperparameter search to find optimal configurations for knowledge infusion.
- What evidence would resolve it: A systematic study varying learning rates, batch sizes, adapter sizes, and other available hyperparameters across multiple models and datasets to identify configurations that maximize knowledge infusion performance.

### Open Question 2
- Question: What specific fine-tuning mechanisms (e.g., adapter-based, full fine-tuning, LoRA) do OpenAI and Google use in their commercial APIs, and how do these mechanisms affect knowledge infusion capabilities?
- Basis in paper: [explicit] The paper notes "OpenAI only allows specifying the learning rate, batch size, and number of training epochs, while Google additionally includes the adapter size" and states "it is not well understood whether these fine-tuning services enable knowledge infusion" partly because "the documentation provided by these companies do not detail the type of fine-tuning methods used."
- Why unresolved: The companies do not disclose their fine-tuning methods, and the paper's results show significant differences between providers (Gemini models performed much worse than OpenAI models) without understanding the underlying technical reasons.
- What evidence would resolve it: Direct disclosure from the companies about their fine-tuning architectures, or reverse engineering studies that identify the specific mechanisms used in each API.

### Open Question 3
- Question: How does the size and quality of training data affect knowledge infusion performance, and is there an optimal range for different types of knowledge (new vs. updated)?
- Basis in paper: [explicit] The paper mentions "we find that models perform best when given between 50-150 unique facts to learn" and tested with "625 training questions" but also notes this was based on "recommended range from fine-tuning documentation."
- Why unresolved: The paper tested within a specific range (50-150 facts) but did not explore whether this range is optimal or how performance scales with larger datasets or different quality levels of training examples.
- What evidence would resolve it: Systematic experiments varying training data size (from very small to very large) and quality (synthetic vs. real data) across different knowledge types to identify performance curves and optimal ranges.

### Open Question 4
- Question: How robust are commercially fine-tuned models to different types and degrees of input perturbations beyond those tested in this study?
- Basis in paper: [inferred] The paper tested "rephrased" and "date changed" questions for news data, "secondary" and "comparison" questions for fictional people, "clinical vignettes" for medical data, and "refactored" code, but notes these are "intuitive ways to test generalization" and "we expect that fine-tuned models perform variably depending on the degree of perturbation from the original question."
- Why unresolved: The paper tested specific types of modifications but acknowledges these are limited examples and did not systematically explore the full space of possible input perturbations or their varying degrees.
- What evidence would resolve it: Comprehensive perturbation studies testing various types of modifications (syntactic, semantic, contextual) with different degrees of change, measuring model performance across this spectrum to identify robustness boundaries.

## Limitations

- The study is limited to specific commercial APIs (OpenAI and Google Vertex AI) with their default hyperparameter settings, which may not represent the full potential of fine-tuning approaches.
- The FineTuneBench dataset, while comprehensive, covers only four knowledge domains and may not capture the full diversity of real-world knowledge infusion scenarios.
- The evaluation relies on LLM-as-a-judge, which introduces potential bias and inconsistency in accuracy measurement.
- The experiments are conducted in controlled settings without considering real-world deployment challenges such as domain shift, concept drift, or interactions between fine-tuned knowledge and base model training.

## Confidence

- **High confidence**: The finding that commercial fine-tuning APIs can achieve near-perfect memorization of training data (100% accuracy) is well-supported by multiple experiments across different models and datasets.
- **Medium confidence**: The observation that models struggle with generalization, particularly on rephrased and modified questions, is consistently observed across experiments but the exact reasons require further investigation.
- **Medium confidence**: The claim that updating existing knowledge is harder than learning new knowledge is supported by the data, but the underlying mechanisms are not fully explored.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Conduct experiments varying learning rates, batch sizes, and number of epochs across the fine-tuning APIs to determine if the observed limitations in knowledge infusion are inherent to the fine-tuning approach or artifacts of default settings.

2. **Cross-Domain Generalization Test**: Evaluate the fine-tuned models on knowledge infusion tasks from domains not represented in FineTuneBench (e.g., scientific literature, legal documents, or multilingual knowledge) to assess the broader applicability of the findings.

3. **Fine-tuning Method Comparison**: Compare commercial API fine-tuning with open-source fine-tuning approaches (e.g., LoRA, full fine-tuning) using the same datasets and evaluation metrics to isolate whether the limitations are specific to commercial APIs or more fundamental to supervised fine-tuning.