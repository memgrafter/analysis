---
ver: rpa2
title: 'SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning
  of Large Language Models'
arxiv_id: '2401.08295'
source_url: https://arxiv.org/abs/2401.08295
tags:
- learning
- task
- sapt
- tasks
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SAPT, a framework for continual learning
  of large language models that aligns the learning and selection of task-specific
  parameter-efficient tuning blocks through shared attention. The key innovation is
  the Shared Attentive Learning & Selection Module (SALS), which uses instance-level
  input-key attention to identify the optimal combination of existing PET blocks for
  the current task, and the Attentive Reflection Module (ARM) to maintain correct
  attention weights for previous tasks using generated pseudo samples.
---

# SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models

## Quick Facts
- arXiv ID: 2401.08295
- Source URL: https://arxiv.org/abs/2401.08295
- Authors: Weixiang Zhao; Shilong Wang; Yulin Hu; Yanyan Zhao; Bing Qin; Xuanyu Zhang; Qing Yang; Dongliang Xu; Wanxiang Che
- Reference count: 40
- Primary result: State-of-the-art performance on continual learning benchmarks addressing catastrophic forgetting and knowledge transfer simultaneously

## Executive Summary
This paper introduces SAPT, a framework for continual learning of large language models that aligns the learning and selection of task-specific parameter-efficient tuning blocks through shared attention. The key innovation is the Shared Attentive Learning & Selection Module (SALS), which uses instance-level input-key attention to identify the optimal combination of existing PET blocks for the current task, and the Attentive Reflection Module (ARM) to maintain correct attention weights for previous tasks using generated pseudo samples. Experiments on two benchmarks show SAPT achieves state-of-the-art performance in addressing catastrophic forgetting and knowledge transfer simultaneously, outperforming recent methods in average performance, forgetting rate, forward transfer, and backward transfer metrics.

## Method Summary
SAPT builds upon parameter-efficient tuning methods by introducing a shared attention mechanism that simultaneously learns and selects the optimal combination of existing PET blocks for each task. The framework consists of SALS for computing instance-level attention between inputs and PET key vectors to guide both learning and selection, and ARM for preventing forgetting through pseudo-sample generation and KL divergence loss. The method is PET-agnostic, working with both prompt tuning and LoRA, and demonstrates effectiveness across different model sizes (770M to 13B parameters) and architectures (T5 and LLaMA-2).

## Key Results
- Achieves state-of-the-art performance on SuperNI and Long Sequence benchmarks for continual learning
- Outperforms recent methods in average performance, forgetting rate, forward transfer, and backward transfer metrics
- Demonstrates effectiveness across different model sizes (770M to 13B parameters) and architectures (T5 and LLaMA-2)
- Successfully handles diverse NLP tasks ranging from sentiment analysis to question answering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shared attention aligns learning and selection modules to address catastrophic forgetting and knowledge transfer simultaneously.
- Mechanism: The SALS module uses instance-level input-key attention to calculate attention weights that guide both the weighted combination of PET blocks during learning and the selection of relevant PET blocks during inference.
- Core assumption: The optimal attention weights for learning a new task are also optimal for selecting PET blocks during inference for that task.
- Evidence anchors:
  - [abstract] "the Shared Attentive Learning & Selection Module (SALS), which uses instance-level input-key attention to identify the optimal combination of existing PET blocks for the current task"
  - [section] "the resulting attention weight a3 of task T3 is first obtained via the instance-level shared attention operation between the input x3 and PET key vectors {k1, k2, k3}, to perform weighted combination of all PET blocks {B1, B2, B3} for the attentive learning of the current task T3"
  - [corpus] Weak - no direct corpus evidence found supporting this specific shared attention mechanism
- Break condition: If the attention weights learned for task-specific knowledge acquisition do not generalize to optimal selection during inference, the alignment would fail.

### Mechanism 2
- Claim: Attentive Reflection Module prevents forgetting of previous tasks' attention weights.
- Mechanism: ARM generates pseudo-samples for previous tasks and uses KL divergence loss to pull the current attention weights back to their original states, ensuring correct PET block selection for old tasks.
- Core assumption: Pseudo-samples generated by a reference PET block can accurately represent the input distribution of previous tasks.
- Evidence anchors:
  - [abstract] "And the Attentive Reflection Module (ARM) to maintain correct attention weights for previous tasks using generated pseudo samples"
  - [section] "we employ generative replay to constrain the projection layer with pseudo-samples"
  - [corpus] Weak - no direct corpus evidence found for this specific pseudo-sample-based attention reflection approach
- Break condition: If generated pseudo-samples poorly represent the true input distribution of previous tasks, the KL divergence loss cannot effectively restore correct attention weights.

### Mechanism 3
- Claim: Instance-level attention enables PET-agnostic compatibility across different tuning methods.
- Mechanism: By computing attention between input embeddings and PET key vectors rather than directly on PET parameters, the framework works with both prompt tuning and LoRA.
- Core assumption: The attention mechanism can effectively capture task-specific relevance regardless of whether PET blocks are prompts or LoRA adapters.
- Evidence anchors:
  - [section] "This process is called shared attention because it will be repeated by the following attentive selection" and "Such input-key attention ensures the process of attentive learning to be PET-agnostic and compatible with both prompt tuning and LoRA in SAPT"
  - [corpus] Weak - no direct corpus evidence found for this specific PET-agnostic attention mechanism
- Break condition: If the attention mechanism fails to capture meaningful task-specific patterns for one PET method, the framework would lose effectiveness for that method.

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Understanding why previous knowledge is lost when learning new tasks is essential to appreciate the SAPT framework's solutions
  - Quick check question: What happens to the weights learned for Task 1 when the model is trained on Task 2 without any regularization or replay mechanism?

- Concept: Parameter-efficient tuning methods (LoRA and prompt tuning)
  - Why needed here: SAPT builds upon these methods by adding selection mechanisms; understanding their operation is crucial
  - Quick check question: How does LoRA modify the forward pass of a linear layer compared to standard fine-tuning?

- Concept: Attention mechanisms in transformer models
  - Why needed here: The shared attention operation is central to SAPT's alignment of learning and selection
  - Quick check question: What is the mathematical operation performed in a standard self-attention mechanism between query and key vectors?

## Architecture Onboarding

- Component map:
  LLM backbone (frozen) -> PET pool (collection of task-specific PET blocks) -> SALS module (shared attention computation, query projection layer, key vectors) -> ARM module (reference PET block generator, pseudo-sample generation) -> Loss functions (task loss + KL divergence loss)

- Critical path:
  1. Input passes through embedding layer
  2. Max-pool to create fixed-size embedding vector
  3. Query projection to create query vector
  4. Compute attention weights with all PET key vectors
  5. Weighted combination of PET blocks for learning
  6. During inference, same attention weights select PET blocks
  7. ARM generates pseudo-samples and applies KL loss to maintain previous task attention

- Design tradeoffs:
  - Storage vs. performance: Maintaining a growing PET pool vs. pruning/merging strategies
  - Quality vs. efficiency of pseudo-samples: Complete input-output pairs vs. input-only generation
  - Attention temperature: Higher temperature reduces overconfidence but may slow knowledge transfer

- Failure signatures:
  - Catastrophic forgetting: Performance on earlier tasks drops significantly as new tasks are learned
  - Knowledge transfer failure: New tasks learn poorly despite previous relevant tasks
  - Attention misalignment: Different attention weights during learning vs. selection phases

- First 3 experiments:
  1. Run SAPT on a simple two-task sequence (e.g., sentiment analysis followed by question answering) and visualize attention weights to verify alignment
  2. Test ARM by removing it and measuring performance degradation on previous tasks
  3. Compare SAPT with and without pseudo-sample generation to validate the effectiveness of the ARM approach

## Open Questions the Paper Calls Out

- How does SAPT's performance scale when the number of tasks increases beyond 15 to hundreds of tasks? The paper explicitly identifies computational and storage costs of maintaining a growing PET pool as a limitation for very long task sequences.

- Can SAPT be extended to handle continual learning without requiring task identification during training? The paper notes that SAPT still necessitates task identification during training to establish distinct PET parameters for each task, limiting its applicability to real-world scenarios where task boundaries may be unclear.

- What is the optimal balance between the attentive learning task loss and the KL divergence loss? The paper mentions that Î» is set to 1 for the SuperNI benchmark without providing a sensitivity analysis or justification for this specific value, using different values (0.5, 0.1, 2) for different model sizes and architectures without explanation.

## Limitations

- The framework's reliance on pseudo-sample generation for ARM introduces potential vulnerabilities if the reference PET block fails to capture the true distribution of previous tasks, with limited analysis of pseudo-sample quality or robustness to distributional shifts.

- The attention-based selection mechanism may struggle with tasks that have similar input patterns but require different PET blocks, as instance-level attention could incorrectly select suboptimal combinations.

- The computational overhead of maintaining and updating attention weights across an expanding PET pool is not thoroughly characterized, particularly for very long task sequences.

## Confidence

- **High Confidence**: SAPT's framework design for addressing catastrophic forgetting through shared attention alignment and ARM-based reflection. The experimental methodology and benchmark results demonstrate clear improvements over baselines.

- **Medium Confidence**: The PET-agnostic compatibility claim and the effectiveness of instance-level attention for task selection across different PET methods. While theoretically sound, these require more extensive empirical validation.

- **Low Confidence**: The robustness of pseudo-sample generation under distributional shifts and the framework's scalability to hundreds of tasks. The paper provides limited analysis of these aspects.

## Next Checks

1. **Attention Alignment Validation**: Implement a visualization tool to track attention weight distributions during training and inference phases across all tasks. Verify that attention weights remain stable for previous tasks after ARM application by computing KL divergence between attention distributions before and after learning new tasks.

2. **Pseudo-Sample Quality Assessment**: Conduct a controlled experiment where pseudo-samples are generated for a subset of tasks, then use these samples to train a classifier that distinguishes between real and generated samples. Evaluate the classifier's accuracy as a proxy for pseudo-sample quality, and correlate this with SAPT's performance metrics.

3. **Scalability Testing**: Extend the experimental evaluation to a 50-task sequence with varied task similarities. Measure the computational overhead of attention computation and PET block selection as the PET pool grows, and test whether attention weights become diluted or noisy when the number of tasks becomes very large.