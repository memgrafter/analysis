---
ver: rpa2
title: 'AutoHallusion: Automatic Generation of Hallucination Benchmarks for Vision-Language
  Models'
arxiv_id: '2406.10900'
source_url: https://arxiv.org/abs/2406.10900
tags:
- image
- object
- lvlms
- hallucination
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces AutoHallusion, the first automated benchmark
  generation approach for investigating hallucinations in large vision-language models
  (LVLMs). Inspired by cognitive science concepts of schema, the authors develop three
  principal strategies to create hallucination-inducing images: abnormal object insertion,
  paired object insertion, and correlated object removal.'
---

# AutoHallusion: Automatic Generation of Hallucination Benchmarks for Vision-Language Models

## Quick Facts
- **arXiv ID:** 2406.10900
- **Source URL:** https://arxiv.org/abs/2406.10900
- **Reference count:** 29
- **Primary result:** 97.7% and 98.7% success rates in inducing hallucinations on synthetic and real-world datasets respectively

## Executive Summary
AutoHallusion introduces the first automated approach for generating hallucination benchmarks for vision-language models (LVLMs). The method leverages schema theory from cognitive science to create image manipulations that exploit LVLMs' language priors, forcing conflicts between visual input and learned context associations. By automatically generating visual-question pairs whose ground truth answers contradict the models' priors, AutoHallusion reveals fundamental vulnerabilities in how LVLMs integrate visual and language information.

## Method Summary
AutoHallusion operates by probing LVLMs' language modules to identify strong object-context co-occurrence priors, then manipulating images to violate these priors through three strategies: abnormal object insertion, paired object deletion, and correlated object removal. The system automatically generates questions whose ground truth answers contradict the language prior, creating scenarios where models must either correctly use visual input or hallucinate. Hallucinations are detected through dual metrics measuring both correctness against ground truth and consistency across multiple questions with varying context levels.

## Key Results
- Achieved 97.7% Manipulation Attack Success Rate (MASR) on synthetic data
- Achieved 98.7% MASR on real-world COCO validation set
- GPT-4V showed superior resistance compared to LLaVA-1.5 and miniGPT4, validating the benchmark's discriminative power

## Why This Works (Mechanism)

### Mechanism 1: Language Prior Exploitation
The method successfully exploits language priors in LVLMs by creating visual-question pairs that directly contradict the model's learned context associations. It probes the language module to identify strong co-occurrence priors between objects and contexts, then manipulates images to violate these priors while generating questions whose ground truth answers contradict the language prior.

### Mechanism 2: Schema-Inspired Dissonance
Schema-inspired manipulation strategies create effective cognitive dissonance in LVLMs, leading to predictable hallucination patterns. The three manipulation strategies (abnormal object insertion, paired object insertion, correlated object removal) are based on cognitive science concepts of schema and expectancy violation, creating conflicts between visual input and language priors.

### Mechanism 3: Dual-Metric Evaluation
The dual-metric evaluation system (correctness + consistency) provides robust hallucination detection that doesn't rely on perfect ground truth generation. Correctness compares model output against ground truth derived from image manipulation, while consistency checks for internal contradictions across multiple questions with varying context levels.

## Foundational Learning

- **Language priors and statistical co-occurrence:** Understanding how LVLMs build statistical associations between objects and contexts is crucial for exploiting them through AutoHallusion. Quick check: How would an LVLM likely respond to seeing a keyboard and monitor together versus a keyboard and octopus together?

- **Schema theory and cognitive dissonance:** The manipulation strategies are directly inspired by schema theory - understanding this provides insight into why certain manipulations work better than others. Quick check: Why might removing a correlated object (like a coffee maker when coffee beans are present) cause more hallucinations than removing an uncorrelated object?

- **Multimodal model architecture:** Understanding the visual encoder + language model architecture helps explain why language priors can dominate over visual input. Quick check: In a typical LVLM architecture, which component (visual or language) would more likely cause hallucinations when there's a conflict?

## Architecture Onboarding

- **Component map:** Scene generation pipeline -> Object detection module -> Language prior probing interface -> Image manipulation engine -> Question construction module -> Hallucination detection system -> Evaluation framework

- **Critical path:** 1) Generate rich scene image with detected objects, 2) Probe language prior for target object selection, 3) Manipulate image based on selected strategy, 4) Construct question set with varying context levels, 5) Evaluate for hallucinations using dual metrics

- **Design tradeoffs:** Object insertion vs. removal (simpler vs. more subtle), question complexity vs. evaluation reliability (more detection vs. more complexity), ground truth generation vs. consistency checking (clear measurement vs. automatic detection)

- **Failure signatures:** Low MASR but high CASR (consistently wrong answers), high MASR but low CASR (learned patterns rather than hallucination), low values for both metrics (ineffective manipulation or too robust model)

- **First 3 experiments:** 1) Run abnormal object insertion on simple office scene with GPT-4V-Turbo, 2) Test paired object insertion with food-related scene, 3) Implement consistency checking on existing COCO dataset examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications to LVLMs would most effectively reduce hallucination susceptibility without significantly degrading performance on standard tasks?
- Basis: The paper identifies that hallucinations occur when LVLMs over-rely on language priors while ignoring visual inputs
- Why unresolved: The paper identifies the mechanism but doesn't propose specific architectural solutions or test modifications
- What evidence would resolve it: Controlled experiments testing various architectural modifications across multiple LVLM architectures

### Open Question 2
- Question: How do hallucinations in LVLMs correlate with model size, training data diversity, and instruction tuning methodology?
- Basis: The paper shows larger models like GPT-4V show superior resistance to hallucination attacks
- Why unresolved: Limited comparison of models without systematic analysis across model sizes, data diversity, or instruction tuning approaches
- What evidence would resolve it: Comprehensive benchmarking across models of varying sizes, trained on datasets with different diversity metrics

### Open Question 3
- Question: What are the real-world consequences of LVLM hallucinations in safety-critical applications, and how can we develop application-specific hallucination detection and mitigation strategies?
- Basis: The paper mentions applications in robotics, medical imaging, and autonomous driving but doesn't explore practical impact
- Why unresolved: While demonstrating induced hallucinations in controlled experiments, it doesn't investigate deployment scenarios or domain-specific safeguards
- What evidence would resolve it: Case studies of hallucination failures in deployed systems and evaluation of domain-tailored mitigation strategies

## Limitations
- Effectiveness may diminish as LVLMs evolve toward more balanced visual-language integration
- Schema-inspired strategies may not capture all hallucination mechanisms, particularly complex reasoning failures
- Computational overhead of image manipulation and multi-round evaluation requires further quantification

## Confidence

- **High Confidence:** The dual-metric hallucination detection system provides robust measurement across multiple LVLM architectures
- **Medium Confidence:** The three manipulation strategies effectively induce hallucinations, though success may vary with scene complexity
- **Medium Confidence:** The framework's cost-effectiveness claim is supported by automation, but computational overhead needs quantification

## Next Checks

1. Test AutoHallusion on emerging LVLM architectures with enhanced visual reasoning capabilities to assess robustness against evolving model designs
2. Conduct ablation studies removing the consistency metric to quantify its contribution to hallucination detection accuracy
3. Evaluate hallucination patterns across different cultural contexts and non-Western object associations to verify schema-based strategies generalize beyond training data distributions