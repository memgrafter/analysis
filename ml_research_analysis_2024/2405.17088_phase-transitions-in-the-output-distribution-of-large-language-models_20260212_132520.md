---
ver: rpa2
title: Phase Transitions in the Output Distribution of Large Language Models
arxiv_id: '2405.17088'
source_url: https://arxiv.org/abs/2405.17088
tags:
- transitions
- phase
- learning
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work adapts physics-based statistical methods to detect phase
  transitions in large language models (LLMs) by quantifying distributional changes
  in LLM outputs using f-divergences (e.g., total variation distance, Jensen-Shannon
  divergence). The method requires minimal prior system knowledge and works by computing
  dissimilarity scores between output distributions conditioned on different values
  of a control parameter (e.g., integer in prompt, temperature, or training epoch).
---

# Phase Transitions in the Output Distribution of Large Language Models

## Quick Facts
- arXiv ID: 2405.17088
- Source URL: https://arxiv.org/abs/2405.17088
- Reference count: 40
- This work adapts physics-based statistical methods to detect phase transitions in large language models (LLMs) by quantifying distributional changes in LLM outputs using f-divergences.

## Executive Summary
This paper introduces a physics-inspired method to detect phase transitions in large language models by quantifying distributional changes in generated outputs using f-divergences. The approach measures dissimilarities between output distributions conditioned on different values of control parameters (e.g., prompt integers, temperature, training epochs) and identifies critical points as local maxima in these dissimilarity scores. Applied to Pythia, Mistral, and Llama models, the method successfully reveals various transitions including integer ordering capability, tokenizer transitions, temperature-dependent phases, and training epoch transitions. The technique requires minimal prior knowledge of the system and enables black-box interpretability studies of LLMs.

## Method Summary
The method computes f-divergences (statistical distances) between output distributions generated by LLMs conditioned on different values of a control parameter. Specifically, it calculates a dissimilarity score D that quantifies the difference between distributions P(σ|x) for neighboring parameter values x and x+δx, where σ represents the generated text output. Critical points where phase transitions occur are identified as local maxima in D. The method uses a linear dissimilarity measure g(x)=2x-1 for numerical stability and requires generating outputs on a grid of parameter values. The approach works for various control parameters including prompt integers, temperature settings, and training epochs, and can be extended to any generative model with tractable output distributions.

## Key Results
- Successfully detected integer ordering capability transitions in instruction-tuned models, revealing that these models struggle with integer sequences but excel at word ordering
- Identified three distinct temperature-dependent phases in LLMs: deterministic (T<0.5), coherent (0.5<T<2.0), and disordered (T>2.0)
- Discovered outlier transitions at specific training epochs (20K, 40K, 80K) that may be linked to rapid changes in layer weights
- Demonstrated that tokenizer transitions occur at specific integer values, creating detectable discontinuities in output distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method detects abrupt distributional changes by measuring f-divergences between outputs conditioned on neighboring parameter values.
- Mechanism: By comparing output distributions on a grid of control parameter values and identifying local maxima in the dissimilarity score, the method pinpoints where the distribution shifts sharply, indicating a phase transition.
- Core assumption: Output distributions are smooth and slowly varying except at critical points, so local maxima in f-divergences correspond to true transitions.
- Evidence anchors:
  - [abstract] "we quantify distributional changes in the generated output via statistical distances, which can be efficiently estimated with access to the probability distribution over next-tokens"
  - [section] "Critical points where phase transitions occur can then be identified as local maxima in D"
  - [corpus] Weak: corpus papers focus on phase transitions in general, but not on this specific f-divergence approach
- Break condition: If the output distribution changes gradually over a wide parameter range, the method may produce spurious peaks or miss transitions.

### Mechanism 2
- Claim: The method works for multiple types of control parameters (prompt variables, temperature, training epochs) without requiring model internals.
- Mechanism: The same dissimilarity framework applies regardless of whether the control parameter is a prompt integer, temperature, or training epoch, because it only requires comparing output distributions for different parameter values.
- Core assumption: The output distribution is sufficiently sensitive to the control parameter to produce distinguishable distributions.
- Evidence anchors:
  - [abstract] "This versatile approach is capable of discovering new phases of behavior and unexplored transitions"
  - [section] "As a demonstration, we characterize transitions occurring as a function of three different control parameters in Pythia, Mistral, and Llama models"
  - [corpus] Weak: corpus lacks direct evidence for this multi-parameter adaptability claim
- Break condition: If the control parameter has no meaningful effect on output distribution, the method will yield flat dissimilarity scores.

### Mechanism 3
- Claim: The choice of g-function determines the specific statistical distance measured (e.g., linear dissimilarity bounds classification error, JS divergence measures information gain).
- Mechanism: Different g-functions correspond to different f-divergences via a transformation, allowing the method to target different aspects of distributional change.
- Core assumption: The mathematical relationship between g-functions and f-divergences is correctly implemented and stable numerically.
- Evidence anchors:
  - [section] "Specifically, we consider Dg = 1/(2L) * sum over segments and parameter values of g[P(σi|x)]"
  - [section] "these g-dissimilarities and the f-divergences [Eq. (1)] defined above correspond to each other in the following sense"
  - [corpus] Weak: corpus doesn't discuss this specific g-function correspondence
- Break condition: If the g-function is poorly chosen (e.g., too flat or too noisy), the resulting dissimilarity may be uninformative or unstable.

## Foundational Learning

- Concept: f-divergences and their properties (e.g., KL divergence, Jensen-Shannon divergence, total variation distance)
  - Why needed here: The method quantifies distributional changes using f-divergences, so understanding their properties is crucial for interpreting results
  - Quick check question: What is the key property of f-divergences that makes them suitable for detecting phase transitions?

- Concept: Autoregressive language model sampling and next-token probability distributions
  - Why needed here: The method relies on access to next-token probabilities to estimate dissimilarities efficiently
  - Quick check question: How does the autoregressive structure enable efficient computation of output probabilities?

- Concept: Fisher information and its relationship to f-divergences
  - Why needed here: The method reduces to Fisher information in the limit of small parameter changes, providing a theoretical justification
  - Quick check question: Why is it desirable that the dissimilarity measure reduces to Fisher information in the limit δT → 0?

## Architecture Onboarding

- Component map: Prompt generator → LLM → Next-token probability estimator → Dissimilarity calculator → Transition detector
- Critical path: Generate outputs for grid of parameter values → Estimate probabilities → Compute dissimilarities → Identify local maxima
- Design tradeoffs: Grid resolution vs. computational cost, L value vs. outlier sensitivity, g-function choice vs. numerical stability
- Failure signatures: Flat dissimilarity scores (no transitions), spurious peaks (gradual changes), numerical instability (poor g-function choice)
- First 3 experiments:
  1. Verify basic functionality on a known phase transition (e.g., temperature transition in a simple LLM)
  2. Test sensitivity to L value by comparing results for L=1 vs. L=5 on the same transition
  3. Compare different g-functions (linear vs. JS divergence) on a simple transition to understand their differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do transitions identified through f-divergence analysis compare to those found via neural network-based classifiers?
- Basis in paper: [explicit] The authors note that their method is inspired by physics approaches but can be approximated using NN-based classifiers when explicit probability distributions are unavailable.
- Why unresolved: The paper implements the f-divergence approach but doesn't compare it against NN-based classifier approaches for transition detection.
- What evidence would resolve it: Direct comparison of f-divergence and NN-based classifier results on the same LLM datasets, showing differences in sensitivity, specificity, and computational efficiency.

### Open Question 2
- Question: What is the relationship between outlier transitions observed at specific training epochs and the rapid changes in weight distributions?
- Basis in paper: [explicit] The authors observe "outliers where the output distribution changes severely only at a single point" around epochs 20K, 40K, and 80K, and note these might be "linked to the transitions observed in the layer weights."
- Why unresolved: The paper acknowledges the correlation but doesn't establish causation or investigate the mechanistic connection between weight changes and output distribution shifts.
- What evidence would resolve it: Analysis showing whether outlier transitions correspond to specific weight distribution changes, or if they represent independent phenomena.

### Open Question 3
- Question: How does the length of generated text (N_tokens) affect the detection of phase transitions?
- Basis in paper: [inferred] The methodology specifies a fixed number of generated tokens, but the authors acknowledge that computational constraints limited their analysis, suggesting this parameter wasn't fully explored.
- Why unresolved: The paper doesn't investigate how varying the number of generated tokens affects transition detection sensitivity or the characteristics of detected transitions.
- What evidence would resolve it: Systematic comparison of transition detection across different token generation lengths, showing how transition sharpness and detectability scale with text length.

### Open Question 4
- Question: Can the observed negative heat capacity phenomenon be explained within a proper statistical mechanical framework?
- Basis in paper: [explicit] The authors observe "the counterintuitive phenomenon of the mean energy of the system increasing with decreasing temperature corresponding to a negative 'heat capacity'" and note it arises from the sampling mismatch between Boltzmann distributions and LLM token-by-token sampling.
- Why unresolved: The paper presents this as an interesting observation but doesn't provide a theoretical explanation for why this phenomenon occurs.
- What evidence would resolve it: Derivation showing how the autoregressive sampling procedure mathematically leads to this apparent violation of thermodynamic principles, or experimental verification that this effect disappears under different sampling strategies.

## Limitations
- Method requires access to next-token probability distributions, limiting applicability to truly black-box settings
- Effectiveness depends on choosing appropriate g-functions and grid resolutions without clear theoretical guidance
- Limited validation across diverse LLM architectures and control parameters, raising questions about generalizability

## Confidence
- High confidence in the mathematical framework and its theoretical foundations (f-divergences, Fisher information connection)
- Medium confidence in the empirical demonstrations, particularly for temperature-dependent transitions and training epoch transitions
- Low confidence in the generalizability across all LLM types and control parameters

## Next Checks
1. Apply the method to synthetic data with known phase transitions at controlled noise levels to quantify false positive/negative rates and establish statistical thresholds for claiming phase transitions.

2. Apply the exact methodology to at least 5 additional LLM architectures (different sizes, training approaches, and domains) to test the method's generalizability and identify conditions under which it fails.

3. Systematically test the method on parameters known to have no effect on output distributions, and on parameters with gradual (non-abrupt) effects, to characterize the method's sensitivity and specificity boundaries.