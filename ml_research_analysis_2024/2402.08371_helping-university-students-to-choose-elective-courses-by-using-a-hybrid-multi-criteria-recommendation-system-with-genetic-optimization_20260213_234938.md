---
ver: rpa2
title: Helping university students to choose elective courses by using a hybrid multi-criteria
  recommendation system with genetic optimization
arxiv_id: '2402.08371'
source_url: https://arxiv.org/abs/2402.08371
tags:
- course
- each
- courses
- criteria
- students
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hybrid multi-criteria recommendation system
  for university course selection that combines Collaborative Filtering (CF) and Content-based
  Filtering (CBF). The system uses student information (ratings, grades, branches)
  and course information (competences, professors, contents, knowledge area) to recommend
  suitable courses.
---

# Helping university students to choose elective courses by using a hybrid multi-criteria recommendation system with genetic optimization

## Quick Facts
- arXiv ID: 2402.08371
- Source URL: https://arxiv.org/abs/2402.08371
- Authors: A. Esteban; A. Zafra; C. Romero
- Reference count: 31
- Primary result: Hybrid multi-criteria RS with GA optimization achieves RMSE 0.971, nDCG 0.682, and 100% reach

## Executive Summary
This paper presents a hybrid multi-criteria recommendation system for university course selection that combines Collaborative Filtering (CF) and Content-based Filtering (CBF). The system uses student information (ratings, grades, branches) and course information (competences, professors, contents, knowledge area) to recommend suitable courses. A Genetic Algorithm optimizes the system's configuration, including weights for each criterion, similarity measures, and neighborhood size. Experimental results using real data from 95 students and 63 courses over three academic years show that the hybrid approach significantly outperforms individual CF or CBF models.

## Method Summary
The system implements a hybrid multi-criteria recommendation approach that combines CF and CBF methods. Student information (ratings, grades, branch) and course information (professors, competences, knowledge area, contents) are processed through separate CF and CBF modules. A Genetic Algorithm (GA) with CHC variation optimizes the configuration by searching over a 14-dimensional chromosome encoding weights, similarity metrics, and neighborhood size. The fitness function uses RMSE between predicted and actual ratings, with the GA running for 1000 generations with population size 50 to find optimal parameter combinations.

## Key Results
- Hybrid approach significantly outperforms individual CF or CBF models (RMSE 0.971 vs higher baselines)
- Optimized system achieves 100% reach, ensuring all students receive recommendations
- Most relevant factors identified as student ratings, professor information, and course contents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid combination of CF and CBF with multi-criteria weighting yields better recommendations than either method alone.
- Mechanism: The system uses CF to capture user preference patterns from ratings/grades/branch, and CBF to capture course content/professor/competence similarity. A weighted linear combination of both outputs allows the model to leverage complementary strengths.
- Core assumption: Ratings and course attributes are both predictive of student satisfaction, and combining them captures orthogonal signal.
- Evidence anchors: [abstract] "experimental results show... the importance of using a hybrid model that combines both student information and course information to increase the reliability of the recommendations"; [section 4.2.2] "The results prove the relevance of using a hybrid approach with multiple criteria whose estimations are significantly better that the rest of the models"
- Break condition: If one modality (e.g., ratings) becomes sparse or highly correlated with the other, the hybrid advantage may vanish.

### Mechanism 2
- Claim: Genetic Algorithm optimizes weights and similarity metrics to adapt to the specific dataset structure.
- Mechanism: The GA searches over a 14-dimensional chromosome encoding weights for CF/CBF models, similarity metrics for each criterion, and neighborhood size. Fitness is RMSE on held-out data.
- Core assumption: The space of valid configurations is non-convex and cannot be tuned by hand effectively.
- Evidence anchors: [abstract] "A Genetic Algorithm (GA) has been developed to automatically discover the optimal RS configuration"; [section 3.3.3] "The fitness measures the goodness of each individual using the Root-Mean- Squared Error (RMSE) between the estimated ratings given by the RS and the real ratings"
- Break condition: If the dataset is too small or the signal too weak, the GA may overfit or converge to degenerate solutions.

### Mechanism 3
- Claim: Multiple similarity metrics per criterion (Euclidean, Pearson, Jaccard, etc.) enable flexible distance measurement tailored to data type.
- Mechanism: For each criterion, the GA selects among several similarity functions (e.g., Pearson for ratings, Jaccard for professors). This allows the model to respect the semantics of each feature.
- Core assumption: Different feature types benefit from different distance measures; a one-size-fits-all similarity is suboptimal.
- Evidence anchors: [section 3.2.1] "three similarity measures are considered: Similarity by ratings Rij... Similarity by grades Gij... Similarity by branch Bij"; [section 3.2.2] "four similarity measures are combined: Similarity considering professors, Pij... Similarity considering competences, Cmij... Similarity considering knowledge area, Sij... Similarity considering contents, Cnij"
- Break condition: If similarity measures are poorly calibrated, or if the chosen metric is inappropriate for the data distribution, performance may degrade.

## Foundational Learning

- Concept: Root Mean Squared Error (RMSE)
  - Why needed here: Used as the fitness function for the GA; quantifies recommendation accuracy.
  - Quick check question: If the RMSE of a recommender is 0.971, what does that say about the average prediction error in rating scale?

- Concept: Cross-validation with stratification
  - Why needed here: Ensures balanced representation of courses across folds; prevents bias from skewed rating distributions.
  - Quick check question: In a 5-fold CV, if each student has ratings for 10 courses, how many are used for training and how many for testing in each fold?

- Concept: Collaborative Filtering (CF) vs Content-Based Filtering (CBF)
  - Why needed here: Forms the basis of the hybrid model; understanding their differences explains why combination is useful.
  - Quick check question: Which method would you use if you had abundant user ratings but little item metadata, and why?

## Architecture Onboarding

- Component map: Data Ingestion → Preprocessor → CF Module → CBF Module → Hybrid Combiner → GA Optimizer → Evaluation
- Critical path: Data → Similarity computation → Rating prediction → Evaluation. GA optimization is offline; once weights are fixed, predictions are real-time.
- Design tradeoffs:
  - GA exploration vs. runtime: longer GA search yields better weights but delays deployment.
  - Number of criteria vs. sparsity: more criteria can improve accuracy but increase data sparsity and computational cost.
  - Neighborhood size vs. cold-start: larger neighborhoods improve coverage but may blur personalization.
- Failure signatures:
  - RMSE plateaus early in GA → local optimum reached; consider restarting with different seeds.
  - Reach < 100% in CF-only mode → high sparsity; hybrid helps but may still fail for outlier students.
  - High nDCG but low RMSE → ranking quality is good but absolute rating prediction is off; check calibration.
- First 3 experiments:
  1. Run CF-only with single criterion (ratings) and record RMSE, nDCG, Reach, time.
  2. Run CBF-only with all criteria and record same metrics.
  3. Run hybrid with GA-optimized weights and compare against both baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the recommendation system perform if extended to include data from multiple universities or different academic disciplines beyond Computer Science?
- Basis in paper: [inferred] The authors mention wanting to "generalize the obtained conclusions to other educational areas" and extend criteria to "more courses of other degrees" as future work.
- Why unresolved: The current system was tested only on Computer Science students at University of Cordoba, limiting generalizability to other contexts.
- What evidence would resolve it: Testing the system on datasets from multiple universities and academic disciplines, comparing performance metrics like RMSE and nDCG across different contexts.

### Open Question 2
- Question: What impact would incorporating social network analysis and trust mechanisms have on recommendation accuracy?
- Basis in paper: [explicit] Authors propose including "social network analysis to handle trust in social networks using reputation mechanism" as future work.
- Why unresolved: The current system doesn't account for social relationships or trust between students that might influence recommendation quality.
- What evidence would resolve it: Implementing social network features and comparing recommendation performance with and without trust mechanisms using metrics like RMSE and nDCG.

### Open Question 3
- Question: How sensitive is the system to different parameter configurations of the Genetic Algorithm, such as population size or number of generations?
- Basis in paper: [inferred] The authors used specific GA parameters (population size 50, 1000 generations) but note that GA optimization takes "5 hours and 16 minutes" which suggests computational expense.
- Why unresolved: The study used fixed GA parameters without exploring sensitivity to different configurations or efficiency trade-offs.
- What evidence would resolve it: Conducting experiments varying GA parameters and measuring both performance (RMSE, nDCG) and computational costs to identify optimal configurations.

## Limitations
- Limited generalizability to other universities and academic domains beyond Computer Science
- Substantial computational resources required for GA optimization (5+ hours)
- Content parsing methodology not thoroughly validated for keyword extraction quality
- Real-time recommendation performance and scalability not addressed

## Confidence
- High confidence: Hybrid model outperforms individual CF/CBF approaches (RMSE 0.971 vs baselines); GA optimization framework is technically sound; importance of combining multiple criteria is well-validated
- Medium confidence: Selection of specific similarity metrics and their optimal combinations; content parsing and keyword extraction methodology's effectiveness; practical utility for end users in real-world deployment scenarios

## Next Checks
1. Conduct cross-domain validation by applying the optimized model to student data from different academic programs or universities to assess generalizability beyond the original Computer Science dataset.
2. Perform ablation studies varying the number of criteria and similarity metrics to identify the minimum viable configuration that maintains performance while reducing computational overhead.
3. Implement a pilot deployment with actual students making course selections based on recommendations, collecting both satisfaction ratings and enrollment data to validate real-world utility beyond offline metrics.