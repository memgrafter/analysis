---
ver: rpa2
title: A Dialogue Game for Eliciting Balanced Collaboration
arxiv_id: '2406.08202'
source_url: https://arxiv.org/abs/2406.08202
tags:
- game
- round
- user
- collaboration
- player
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel 2D object placement game designed
  to study balanced collaboration in dialogue, contrasting with typical asymmetric
  task-oriented games. Players negotiate a shared goal state through chat without
  seeing each other's boards.
---

# A Dialogue Game for Eliciting Balanced Collaboration

## Quick Facts
- arXiv ID: 2406.08202
- Source URL: https://arxiv.org/abs/2406.08202
- Reference count: 14
- Primary result: Balanced "back and forth" collaboration strategies achieve higher task performance than dominant "leader" approaches in a novel 2D object placement game

## Executive Summary
This paper introduces a novel 2D object placement game designed to study balanced collaboration in dialogue, contrasting with typical asymmetric task-oriented games. Players negotiate a shared goal state through chat without seeing each other's boards. Human dyads exhibited diverse collaboration strategies, with balanced "back and forth" approaches yielding higher performance than dominant "leader" strategies. A simple LLM-based agent achieved lower scores than humans, demonstrating the game's potential as a testbed for collaborative AI. The dominance score metric captures collaboration imbalance, showing that balanced strategies correlate with better task outcomes.

## Method Summary
The game uses the Slurk interaction server framework with a web interface featuring chat and 2D drag-and-drop boards. Players negotiate object placements without seeing each other's boards, with five movable objects (pillow, pants, trash bag, flat cap, cowboy hat) on kitchen or living room backgrounds. The task performance metric calculates mean Manhattan distance between identical objects placed by two players, normalized to 0-100. Human participants (73 games) played two rounds via Prolific, with chat logs analyzed to classify collaboration strategies and calculate dominance scores. A baseline LLM agent using GPT 3.5 Turbo followed a Leader strategy, employing semantic parsing to extract spatial relationships and apply rule-based positioning.

## Key Results
- Balanced "back and forth" collaboration strategies achieved mean scores of 92.90, significantly outperforming dominant "leader" strategies at 84.17
- The dominance score metric effectively captures collaboration imbalance, with lower differences correlating with better task performance
- A simple LLM-based agent achieved mean scores of 84.17, demonstrating the challenge of natural collaboration for AI systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Balanced collaboration emerges because players must negotiate a goal state without predetermined roles.
- Mechanism: By removing asymmetric information, the game forces both players to contribute to goal definition through dialogue, creating natural role negotiation.
- Core assumption: Partial observability creates enough uncertainty that one player cannot dominate through positional knowledge alone.
- Evidence anchors:
  - [abstract]: "players must negotiate the goal state themselves" and "partial observability and negotiated goals effectively encourage natural role-taking"
  - [section 2]: "This approach places the players in fixed roles (instruction giver/follower), which is in contrast to the fluid and implicit negotiation of these roles in naturally occurring collaborative dialogue"
  - [corpus]: No direct corpus evidence for this specific mechanism, though related papers on LLM-based agents in games suggest negotiation is challenging without fixed roles
- Break condition: If players could see each other's boards, they could simply copy positions without negotiation, eliminating the need for collaborative dialogue.

### Mechanism 2
- Claim: The dominance score metric effectively captures collaboration imbalance and predicts task performance.
- Mechanism: By combining message volume and verbosity into a single metric, the dominance score quantifies which player controls decision-making, and lower dominance differences correlate with better scores.
- Core assumption: Message volume and length are reasonable proxies for decision-making control in collaborative tasks.
- Evidence anchors:
  - [abstract]: "The dominance score metric captures collaboration imbalance, showing that balanced strategies correlate with better task outcomes"
  - [section 4.2]: Detailed formula using volume and verbosity, with logistic dampening to emphasize smaller differences
  - [section 4.3]: Data showing Leader strategy (high dominance difference) has lower scores than Back and Forth (low dominance difference)
- Break condition: If players use very short messages but still collaborate effectively, or if one player types extensively but follows the other's lead, the metric would misrepresent actual collaboration quality.

### Mechanism 3
- Claim: LLM-based agents can perform the task but with significantly lower scores than humans, demonstrating the challenge of natural collaboration.
- Mechanism: The agent uses semantic parsing to extract spatial relationships from instructions and applies rule-based positioning, achieving partial success but lacking the nuanced coordination of human dyads.
- Core assumption: Simple semantic parsing and rule-based positioning can approximate human collaborative strategies, even if imperfectly.
- Evidence anchors:
  - [abstract]: "A simple LLM-based agent achieved lower scores than humans, demonstrating the game's potential as a testbed for collaborative AI"
  - [section 5]: Agent achieved mean score of 84.17 vs human Leader strategy at 84.17 and Back and Forth at 92.90
  - [corpus]: No direct corpus evidence for this specific agent approach, though related papers on LLM agents in games suggest they struggle with nuanced coordination
- Break condition: If the agent used more sophisticated language understanding or could see both boards, it might close the performance gap with humans.

## Foundational Learning

- Concept: Partial observability in multi-agent systems
  - Why needed here: The game's core innovation relies on players not seeing each other's boards, which forces negotiation rather than simple copying
  - Quick check question: What would happen to collaboration dynamics if players could see each other's boards?

- Concept: Dominance metrics and their relationship to team performance
  - Why needed here: The paper introduces a novel dominance score that predicts which collaboration strategies work best
  - Quick check question: How does the dominance score formula balance volume vs verbosity, and why use a logistic function?

- Concept: Semantic parsing for spatial reasoning in dialogue systems
  - Why needed here: The baseline agent relies on extracting spatial relationships from natural language to place objects
  - Quick check question: What spatial relations and landmark types does the agent handle, and what limitations does this create?

## Architecture Onboarding

- Component map: Slurk server -> Web interface (chat + 2D board) -> Logging system -> LLM agent component
- Critical path: Player A sends chat message → Agent parses message → Extracts (object, landmark, direction) triples → Applies positioning rules → Updates board → Player B sees updated state
- Design tradeoffs: The game prioritizes simplicity (5 objects, cartoon graphics) over realism to focus on collaboration mechanics. Fixed backgrounds prevent object overlap but limit spatial complexity.
- Failure signatures: Low scores typically indicate poor communication or coordination. High dominance score differences suggest one player dominating. LLM agent failures often involve misinterpreting spatial descriptions.
- First 3 experiments:
  1. Test if allowing players to see each other's boards changes collaboration strategies and eliminates balanced approaches
  2. Vary the number of movable objects to see how task complexity affects collaboration patterns
  3. Implement a more sophisticated LLM agent with better spatial reasoning to measure performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dominance score metric relate to actual collaboration quality beyond task performance?
- Basis in paper: [explicit] The authors acknowledge that dominance score is an approximation that doesn't consider message content
- Why unresolved: The paper only correlates dominance score with task performance but doesn't validate it against independent measures of collaboration quality
- What evidence would resolve it: Having human raters evaluate collaboration quality independently of task success, then comparing these ratings with dominance scores

### Open Question 2
- Question: What specific features of the game design make balanced collaboration strategies more effective than asymmetric ones?
- Basis in paper: [inferred] The authors suggest partial observability and negotiated goals encourage balanced play, but don't systematically test this
- Why unresolved: The paper presents correlations but doesn't experimentally isolate and test individual game features
- What evidence would resolve it: Comparative studies testing different game variants (e.g., with/without partial observability, with/without negotiated goals)

### Open Question 3
- Question: How do collaboration strategies evolve over extended gameplay beyond two rounds?
- Basis in paper: [inferred] The authors observe strategy evolution between rounds 1 and 2 but don't study longer-term dynamics
- Why unresolved: The dataset only includes two rounds per dyad, limiting understanding of long-term collaboration patterns
- What evidence would resolve it: Longitudinal studies tracking strategy evolution across many rounds or sessions

## Limitations

- Limited corpus validation: The paper introduces novel mechanisms without direct corpus evidence
- Agent design simplicity: The baseline LLM agent uses basic semantic parsing and rule-based positioning
- Strategy classification subjectivity: Manual analysis identifying collaboration strategies relies on human judgment

## Confidence

**High confidence**: The game design effectively creates asymmetric collaboration scenarios, the dominance score captures meaningful differences in communication patterns, and balanced strategies correlate with better performance.

**Medium confidence**: The mechanism that partial observability forces negotiation is theoretically sound but lacks direct empirical validation. The LLM agent's performance demonstrates the challenge of collaborative AI but doesn't fully explain why humans outperform.

**Low confidence**: The specific claims about why balanced collaboration emerges and how the dominance score formula optimally balances volume and verbosity lack detailed validation.

## Next Checks

1. Test the core mechanism by implementing a variant where players can see each other's boards to validate whether partial observability is truly necessary for balanced collaboration.

2. Conduct ablation studies on the dominance score formula by varying the weighting between volume and verbosity, or testing alternative formulations, to determine if the current metric is optimal for predicting task performance.

3. Implement a more sophisticated LLM agent with enhanced spatial reasoning capabilities (e.g., using vision models to understand board states) and compare its performance against both the current agent and human players to isolate which aspects of collaboration are most challenging for AI.