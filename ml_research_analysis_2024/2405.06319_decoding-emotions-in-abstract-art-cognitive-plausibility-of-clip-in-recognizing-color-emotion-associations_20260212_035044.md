---
ver: rpa2
title: 'Decoding Emotions in Abstract Art: Cognitive Plausibility of CLIP in Recognizing
  Color-Emotion Associations'
arxiv_id: '2405.06319'
source_url: https://arxiv.org/abs/2405.06319
tags:
- emotion
- images
- clip
- emotions
- abstract
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates CLIP's cognitive plausibility in recognizing
  emotions in abstract art, focusing on color-emotion associations. The authors use
  the FeelingBlue dataset of abstract images with emotion labels and rationales, analyzing
  CLIP's zero-shot classification, similarity-based prediction, and color-emotion
  interactions.
---

# Decoding Emotions in Abstract Art: Cognitive Plausibility of CLIP in Recognizing Color-Emotion Associations

## Quick Facts
- arXiv ID: 2405.06319
- Source URL: https://arxiv.org/abs/2405.06319
- Reference count: 9
- CLIP achieves above-chance but relatively low accuracy (11-29%) for emotion recognition in abstract art images

## Executive Summary
This paper investigates whether CLIP, a multimodal vision-language model, can recognize emotions in abstract art and whether its processing aligns with human cognitive patterns. The authors analyze CLIP's zero-shot classification, similarity-based prediction, and color-emotion associations using the FeelingBlue dataset of abstract images with emotion labels. While CLIP achieves above-chance performance (11-29% accuracy), it struggles to bridge the affective gap between low-level visual features and abstract emotional concepts. The model shows better performance with emotion rationales (34-45%) and improves to 47.5% accuracy using similarity-based encoding, but still falls short of human-level understanding.

## Method Summary
The study employs CLIP's zero-shot classification to predict emotions from abstract art images, using both single-word prompts and context-rich descriptions. The authors extract CLIP embeddings for images and emotion rationales, then implement a similarity-based encoding approach using weighted averages from the 10 most similar training images. They also analyze color-emotion associations using dominant color extraction and compare CLIP's predictions with human annotations and established psychological literature.

## Key Results
- CLIP achieves 11-29% accuracy for zero-shot emotion recognition in abstract art images, above chance (20%) but relatively low
- Performance improves to 47.5% using similarity-based encoding with contextual information
- CLIP's color-emotion associations align more closely with literature than human annotations, identifying patterns like red-anger and blue-sadness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP encodes emotional concepts in abstract images differently from naturalistic images due to lack of concrete visual cues
- Mechanism: CLIP relies on contrastive learning between image and text embeddings, but abstract art lacks recognizable objects/scenes that would provide clear semantic grounding
- Core assumption: CLIP's cross-modal alignment mechanism works best when images contain clear, describable visual elements
- Evidence anchors:
  - [abstract] CLIP demonstrated lower accuracy in emotion recognition for abstract art than for naturalistic images (Bondielli et al., 2021)
  - [section] CLIP's performance is fairly sensitive to prompt wording: Providing the to-be-predicted emotion label in context leads to superior performance over stating the emotion in isolation
  - [corpus] Weak evidence - no direct corpus support for this specific mechanism
- Break condition: If CLIP shows similar accuracy patterns on both abstract and naturalistic images, or if prompt context doesn't significantly affect performance

### Mechanism 2
- Claim: Similarity-based encoding approach improves CLIP's emotion prediction by leveraging contextual awareness from neighboring samples
- Mechanism: Using weighted averages of emotion labels from similar images provides richer contextual information than single-image zero-shot classification
- Core assumption: Images evoking similar emotions have similar representations in CLIP's feature space
- Evidence anchors:
  - [section] We utilize the ten most similar training images for each test image by using cosine similarity between CLIP-encoded images
  - [section] This resulted in an accuracy of 47.51% on the test set, demonstrating superiority over random guessing and zero-shot performance
  - [corpus] No direct corpus support for this mechanism
- Break condition: If similarity-based predictions don't improve accuracy over zero-shot methods, or if CLIP's embeddings don't cluster by emotion

### Mechanism 3
- Claim: CLIP's color-emotion associations align more closely with established literature than human annotations due to reliance on low-level visual features
- Mechanism: CLIP processes color distributions and basic visual features more consistently than humans who incorporate broader contextual understanding
- Core assumption: CLIP's visual processing focuses on low-level features while human emotion perception integrates sociocultural context
- Evidence anchors:
  - [section] Interestingly, CLIP's classification of images to emotions based on dominant colors aligns more closely with the theories from literature than the associations found between colors and human-annotated emotions
  - [section] CLIP's potential to recognize color-emotion associations in both images and text validates the relevance of color in emotional connotations
  - [corpus] Weak evidence - corpus doesn't directly support this mechanism
- Break condition: If CLIP's color-emotion associations diverge from established literature or human patterns

## Foundational Learning

- Concept: Zero-shot learning and contrastive training
  - Why needed here: Understanding how CLIP was trained and how it makes predictions without task-specific fine-tuning
  - Quick check question: How does CLIP's contrastive learning objective work, and what does it mean for a model to perform "zero-shot" classification?

- Concept: Cosine similarity and embedding spaces
  - Why needed here: The paper relies heavily on similarity measures between CLIP-encoded representations
  - Quick check question: How do you compute cosine similarity between two vectors, and why is it commonly used for comparing embeddings?

- Concept: Color theory and emotion psychology
  - Why needed here: The paper investigates color-emotion associations and their relationship to established psychological theories
  - Quick check question: What are some common color-emotion associations in psychological literature, and how might these vary across cultures?

## Architecture Onboarding

- Component map:
  CLIP model (ViT-B/32 and ViT-L/14) -> FeelingBlue dataset -> Text preprocessing pipeline -> ColorThief for dominant color extraction -> Similarity encoding module

- Critical path:
  1. Preprocess FeelingBlue dataset to create image-emotion dictionary
  2. Extract CLIP embeddings for images and rationales
  3. Perform zero-shot emotion classification using various prompts
  4. Implement similarity-based encoding approach
  5. Analyze color-emotion interactions in images and text

- Design tradeoffs:
  - Model size vs. computational efficiency (ViT-B/32 vs. ViT-L/14)
  - Number of similar images to use for context (10 vs. more/less)
  - Granularity of color categories (broad vs. specific)
  - Simplification of multi-label emotion problem to single-label assignment

- Failure signatures:
  - Consistently low accuracy across all experiments
  - CLIPScore doesn't correlate with human judgments
  - Similarity-based approach doesn't improve over zero-shot
  - Color analysis shows no discernible patterns

- First 3 experiments:
  1. Test zero-shot emotion classification on a small subset of images using both single-word and context-rich prompts
  2. Compare CLIP's performance on abstract art vs. naturalistic images from the same dataset
  3. Validate the similarity encoding approach by manually checking if the 10 most similar images actually evoke similar emotions

## Open Questions the Paper Calls Out
None

## Limitations
- The FeelingBlue dataset contains predominantly negative emotions (anger, disgust, fear, sadness), limiting generalizability to positive emotions
- The subjective nature of abstract art interpretation introduces inherent variability that is difficult to control
- Accuracy levels (11-29% for images, 34-45% for rationales) remain relatively low despite being above chance

## Confidence

**High Confidence**: CLIP's superior performance on emotion rationales compared to images (34-45% vs. 11-29%) and the improvement from similarity-based encoding (47.5%) are well-supported by the experimental results. The finding that CLIP's color-emotion associations align more closely with literature than human annotations is robust.

**Medium Confidence**: The claim that CLIP struggles with the affective gap between low-level features and abstract emotions is supported but requires further investigation. The assertion about CLIP's different processing of abstract versus naturalistic images is plausible but not definitively proven within this study alone.

**Low Confidence**: The broader implications about disparities between machine and human processing of abstract art are suggestive but require more extensive cross-cultural and cross-linguistic validation.

## Next Checks

1. **Cross-cultural validation**: Test CLIP's color-emotion associations on datasets annotated by participants from different cultural backgrounds to verify whether the observed patterns are universal or culturally specific.

2. **Controlled comparison with naturalistic images**: Conduct a direct, controlled experiment comparing CLIP's performance on abstract art versus naturalistic images from the same emotion categories, controlling for image complexity and color distribution.

3. **Fine-tuning experiment**: Evaluate whether task-specific fine-tuning on the FeelingBlue dataset significantly improves CLIP's emotion recognition accuracy compared to zero-shot methods, helping determine if the limitations are fundamental or addressable through adaptation.