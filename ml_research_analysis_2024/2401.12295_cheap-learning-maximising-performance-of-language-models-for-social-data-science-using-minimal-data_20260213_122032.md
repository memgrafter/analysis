---
ver: rpa2
title: 'Cheap Learning: Maximising Performance of Language Models for Social Data
  Science Using Minimal Data'
arxiv_id: '2401.12295'
source_url: https://arxiv.org/abs/2401.12295
tags:
- data
- learning
- text
- training
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper compares cheap learning techniques\u2014weak supervision,\
  \ transfer learning, and prompt engineering\u2014against conventional machine learning\
  \ for text classification in social science. It evaluates performance across binary\
  \ tasks (abuse detection, sentiment analysis) using minimal labelled data (16-1024\
  \ points) and studies systematic biases."
---

# Cheap Learning: Maximising Performance of Language Models for Social Data Science Using Minimal Data

## Quick Facts
- arXiv ID: 2401.12295
- Source URL: https://arxiv.org/abs/2401.12295
- Reference count: 40
- Primary result: Zero-shot prompting with GPT-4 achieves the highest macro F1 scores (up to 0.94) without any training data

## Executive Summary
This paper evaluates cheap learning techniques—weak supervision, transfer learning, and prompt engineering—against conventional machine learning for text classification in social science applications using minimal labeled data. The study tests binary classification tasks (abuse detection, sentiment analysis) across training sets of 16-1024 labeled examples and examines systematic biases. Results demonstrate that zero-shot prompting with GPT-4 outperforms other methods, achieving up to 0.94 macro F1 scores without any training data. Transfer learning and weak supervision also show strong performance with small budgets, while conventional methods like Naïve Bayes perform worse with unbalanced data. The study concludes that cheap techniques, particularly zero-shot prompting, offer high accuracy at low cost for social science applications, though API costs and potential biases require consideration.

## Method Summary
The study compares four approaches: weak supervision using Snorkel's LabelModel with heuristic labeling functions, transfer learning with DistilBERT fine-tuning, prompt engineering with both DistilBERT and GPT models, and conventional methods including Multinomial Naïve Bayes and logistic regression. Experiments use the Wikipedia Talk: Personal Attacks dataset (115,864 comments) and IMDb Movie Review Sentiment dataset (50,000 reviews). Seven training sets of increasing size (16, 32, 64, 128, 256, 512, 1024) are created from each dataset. Performance is measured using macro F1 scores across different labeling budgets, with evaluation of systematic biases and training time comparison. The codebase is available at https://github.com/Turing-Online-Safety-Codebase/cheap_learning.

## Key Results
- Zero-shot prompting with GPT-4 achieves the highest macro F1 scores (up to 0.94) without any training data
- Transfer learning and weak supervision perform well with small training budgets (16-1024 examples)
- Zero-shot models show lower bias than other techniques
- Conventional Naïve Bayes and logistic regression perform worse with unbalanced data
- Training time is negligible for all methods

## Why This Works (Mechanism)

### Mechanism 1
Large language models leverage their pretraining on massive text corpora to perform classification tasks via contextual understanding, without requiring labeled examples for fine-tuning. The core assumption is that the model's pretraining has exposed it to diverse linguistic patterns and task-relevant concepts. Evidence shows zero-shot prompting with GPT-4 achieves up to 0.94 macro F1 scores without training data. This may degrade if the pretraining corpus lacks sufficient diversity or contains significant bias.

### Mechanism 2
Transfer learning requires significantly less labeled data than conventional machine learning while achieving high performance by leveraging a model pretrained on a large corpus that is fine-tuned on a small task-specific dataset. The core assumption is that the source and target tasks share enough linguistic structure for knowledge transfer. Evidence shows transfer learning performs well with small budgets. This may not improve over training from scratch if source and target domains are too dissimilar.

### Mechanism 3
Weak supervision makes large training datasets cheaper to assemble without sacrificing evaluation rigor by encoding domain knowledge as labeling functions that generate noisy labels, which are then aggregated using a probabilistic model. The core assumption is that expert knowledge can be formalized into reliable labeling functions. Evidence shows weak supervision enables measuring coverage and overlap of labeling functions. This may produce unreliable labels if functions are too noisy or have poor coverage.

## Foundational Learning

- **Concept**: Understanding the distinction between supervised, unsupervised, and semi-supervised learning
  - **Why needed here**: This paper focuses on supervised learning techniques that require some labeled data, contrasting them with zero-shot approaches
  - **Quick check question**: Can you explain why unsupervised learning wouldn't directly solve the classification tasks described here?

- **Concept**: Basics of text classification and feature representation (e.g., TF-IDF, word embeddings)
  - **Why needed here**: All techniques discussed operate on text data and require understanding how text is represented for machine learning models
  - **Quick check question**: How would you convert a sentence like "This movie was great!" into a numerical representation suitable for a classifier?

- **Concept**: Evaluation metrics for classification (precision, recall, F1, macro vs micro averaging)
  - **Why needed here**: The paper extensively discusses performance using macro F1 scores and requires understanding what these metrics measure
  - **Quick check question**: In an imbalanced dataset where 90% of examples are negative, would a classifier that always predicts negative achieve high accuracy? What about high macro F1?

## Architecture Onboarding

- **Component map**: Data preprocessing (tokenization, vectorization) -> Model selection (pre-trained model choice) -> Training/inference pipeline -> Evaluation framework
- **Critical path**: For transfer learning: data → preprocessing → model loading → fine-tuning → evaluation. For zero-shot: data → prompt construction → API call → response parsing → evaluation.
- **Design tradeoffs**: Zero-shot offers speed and no training data but incurs API costs and potential bias; transfer learning balances cost and control; weak supervision requires domain expertise but is transparent.
- **Failure signatures**: Zero-shot: poor performance on niche topics, biased outputs; transfer learning: overfitting to small datasets, suboptimal hyperparameters; weak supervision: noisy labels from poor labeling functions.
- **First 3 experiments**:
  1. Implement zero-shot classification on a small text dataset using GPT-3.5 API to verify basic functionality
  2. Set up transfer learning pipeline with a pre-trained DistilBERT model on the same dataset with 100 labeled examples
  3. Create simple keyword-based labeling functions for weak supervision and run the LabelModel on the dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the size of the training data affect the over-prediction bias in zero-shot prompt engineering with LLMs?
- **Basis in paper**: [inferred] The paper shows that over-prediction varies across tasks and models, but does not explicitly test how training data size affects over-prediction in zero-shot prompting.
- **Why unresolved**: The paper focuses on comparing techniques with different training data sizes, but does not isolate the effect of training data size on over-prediction in zero-shot prompting specifically.
- **What evidence would resolve it**: Conduct experiments varying the amount of training data used to fine-tune prompts for zero-shot prompting and measure the resulting over-prediction bias.

### Open Question 2
- **Question**: How do different types of prompt engineering (e.g., few-shot, chain-of-thought) compare to zero-shot prompting in terms of performance and bias on social science text classification tasks?
- **Basis in paper**: [explicit] The paper focuses on zero-shot prompting but mentions few-shot learning as a related technique.
- **Why unresolved**: The paper does not directly compare zero-shot prompting to other prompt engineering methods.
- **What evidence would resolve it**: Conduct experiments comparing the performance and bias of different prompt engineering techniques on the same social science tasks.

### Open Question 3
- **Question**: Can the over-prediction bias in LLMs be mitigated through specific prompting strategies or model fine-tuning?
- **Basis in paper**: [explicit] The paper shows that GPT-4 has lower over-prediction than GPT-3 and 3.5, suggesting that model choice affects bias.
- **Why unresolved**: The paper does not explore strategies for mitigating over-prediction within the same model.
- **What evidence would resolve it**: Experiment with different prompting strategies (e.g., providing more context, adjusting prompt phrasing) or fine-tuning LLMs on balanced datasets to reduce over-prediction bias.

## Limitations

- The study focuses exclusively on binary classification tasks, leaving uncertainty about performance on multi-class problems
- The evaluation uses only two datasets (Wikipedia Talk and IMDb), which may not represent the full diversity of social science text data
- While zero-shot prompting shows high performance, API costs and potential biases remain practical concerns not fully quantified

## Confidence

- **High confidence**: The core finding that zero-shot prompting with GPT-4 achieves high macro F1 scores without training data is well-supported by experimental results
- **Medium confidence**: The comparison of different cheap learning techniques is methodologically sound, though specific labeling functions for weak supervision are not fully detailed
- **Medium confidence**: The bias analysis shows zero-shot models exhibit lower bias than other techniques, but bias metrics and computation methods are not specified

## Next Checks

1. Test the zero-shot prompting approach on a multi-class social science text classification task to assess generalizability beyond binary problems
2. Implement the weak supervision pipeline using the paper's examples to empirically determine the impact of labeling function quality on final performance
3. Conduct a cost-benefit analysis comparing API costs for zero-shot prompting versus computational costs for fine-tuning across different dataset sizes