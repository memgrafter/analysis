---
ver: rpa2
title: Understanding In-Context Learning of Linear Models in Transformers Through
  an Adversarial Lens
arxiv_id: '2411.05189'
source_url: https://arxiv.org/abs/2411.05189
tags:
- adversarial
- learning
- linear
- attacks
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the robustness of in-context learning of linear
  models in transformers to adversarial hijacking attacks, where an adversary manipulates
  in-context examples to force a model to output a specific target value. The authors
  first show that single-layer linear transformers and GPT-2 style transformers are
  vulnerable to such attacks.
---

# Understanding In-Context Learning of Linear Models in Transformers Through an Adversarial Lens

## Quick Facts
- arXiv ID: 2411.05189
- Source URL: https://arxiv.org/abs/2411.05189
- Reference count: 40
- Primary result: Adversarial training can significantly improve the robustness of transformers to hijacking attacks during in-context learning, even generalizing to stronger attack models

## Executive Summary
This paper investigates the adversarial robustness of transformers performing in-context learning on linear regression tasks. The authors demonstrate that both single-layer linear transformers and GPT-2 style transformers are vulnerable to hijacking attacks, where adversaries can manipulate in-context examples to force the model to output specific target values. Through a series of experiments, they show that adversarial training during pretraining or fine-tuning can substantially improve robustness, with the learned defenses generalizing to stronger attack models. The paper also conducts a comparative analysis of adversarial vulnerabilities across different transformer architectures and traditional linear solvers, finding that attacks transfer poorly between models trained from different seeds and between transformers and traditional methods, suggesting qualitatively different in-context learning algorithms are being implemented.

## Method Summary
The authors train transformer models on synthetic linear regression tasks, using d=20 dimensions with N=40 examples per task. They implement both GPT-2 style transformers (8 layers) and single-layer linear transformers, using standard training procedures with 5×10⁵ steps, batch size 64, and learning rate 5×10⁻⁴. For evaluation, they employ gradient-based hijacking attacks including feature attacks, label attacks, and joint attacks, measuring performance using Ground Truth Error (GTE) and Targeted Attack Error (TAE) metrics. Adversarial training is performed by generating adversarial examples during training using the same attack methods, with learning rates of 1 for feature attacks and 100 for label attacks. The study also compares transformers against traditional linear solvers like gradient descent and ordinary least squares, examining transferability of attacks across different models and seeds.

## Key Results
- Single-layer linear transformers are highly vulnerable to hijacking attacks, while GPT-2 style transformers show improved robustness
- Adversarial training during pretraining or fine-tuning significantly improves robustness, with defenses generalizing to stronger attack models
- Adversarial attacks transfer poorly between larger transformers trained from different seeds, suggesting different in-context learning algorithms are implemented
- Attacks do not transfer well between transformers and traditional solvers (OLS, gradient descent), indicating qualitative algorithmic differences

## Why This Works (Mechanism)
Transformers performing in-context learning implement a linear regression algorithm where the attention mechanism effectively computes weighted combinations of input examples. When vulnerable, the model's attention weights can be manipulated through carefully crafted adversarial examples that shift the decision boundary. Adversarial training works by exposing the model to these manipulated examples during training, forcing it to learn more robust attention patterns that are less susceptible to hijacking. The poor transferability of attacks between models suggests that different training runs converge to different local optima in the optimization landscape, each implementing slightly different algorithms for solving the in-context learning task.

## Foundational Learning
- **In-context learning**: The ability of transformers to learn from examples provided in the prompt without parameter updates. Needed to understand the core phenomenon being studied; check by verifying models can perform linear regression on novel tasks given only in-context examples.
- **Adversarial examples**: Inputs specifically designed to cause model misbehavior by exploiting vulnerabilities in the learned representation. Needed to frame the hijacking attack scenario; check by confirming small perturbations can significantly change model outputs.
- **Attention mechanisms**: The core operation in transformers that allows information to flow between tokens. Needed because attacks manipulate attention weights; check by examining how attention patterns change under adversarial examples.
- **Linear regression solvers**: Traditional methods like OLS and gradient descent for solving linear systems. Needed for comparative analysis; check by verifying these methods produce correct solutions on the synthetic data.
- **Gradient-based attacks**: Methods that use model gradients to construct adversarial examples. Needed to implement the hijacking attacks; check by ensuring attack loss decreases during optimization.
- **Robust optimization**: Training procedures that account for worst-case perturbations. Needed to understand adversarial training; check by verifying robust models maintain performance under attacks.

## Architecture Onboarding

**Component Map:**
Data Generation -> Model Training -> Attack Generation -> Evaluation -> Adversarial Training -> Transferability Analysis

**Critical Path:**
Synthetic data generation with linear regression tasks → Transformer training (GPT-2 or linear) → Gradient-based hijacking attacks → Evaluation using GTE/TAE metrics → Adversarial training → Re-evaluation of robustness

**Design Tradeoffs:**
The paper balances model complexity (single-layer vs 8-layer transformers) against vulnerability to attacks, and training stability (standard vs adversarial training) against computational cost. The choice of linear regression tasks provides clean analysis but may not generalize to more complex functions.

**Failure Signatures:**
- Attacks failing to converge: Learning rates too low or gradient computation incorrect
- Poor transferability results: Inconsistent prompt formatting or tokenization between experiments
- Adversarial training not improving robustness: Insufficient attack strength during training or improper gradient updates

**First Experiments to Run:**
1. Generate synthetic linear regression data with d=20, N=40 examples and verify ground truth solutions
2. Train a basic GPT-2 style transformer on the synthetic data and test in-context learning performance on held-out tasks
3. Implement a simple feature attack on the trained model and verify it can successfully hijack predictions

## Open Questions the Paper Calls Out
- Does adversarial training on more complex tasks (beyond linear regression) lead to similar robustness gains and transferability properties?
- What is the mechanistic explanation for why adversarial attacks transfer poorly between larger transformers trained from different seeds but transfer well between smaller ones?
- What specific algorithmic differences between transformers and traditional solvers (OLS, gradient descent) cause poor adversarial transferability?

## Limitations
- Results are limited to linear regression tasks and may not generalize to more complex function classes
- The study does not explore hyperparameter sensitivity or provide ablation studies
- The synthetic data generation process lacks full specification, particularly regarding task sampling distributions

## Confidence
- **High confidence**: Single-layer linear transformers are vulnerable to hijacking attacks (theoretical and empirical verification)
- **Medium confidence**: GPT-2 style transformers are more robust than linear transformers (empirical comparison sensitive to implementation)
- **Medium confidence**: Adversarial training generalizes to stronger attack models (demonstrated only on specific variants)
- **Low confidence**: Qualitative differences in in-context learning algorithms based on poor attack transferability (alternative explanations not ruled out)

## Next Checks
1. Test adversarial training robustness across a broader range of attack types and strengths to verify generalization claims
2. Conduct ablation studies varying key hyperparameters (learning rates, batch sizes, number of training steps) to assess result stability
3. Extend analysis to non-linear function classes to determine if findings transfer beyond linear regression tasks