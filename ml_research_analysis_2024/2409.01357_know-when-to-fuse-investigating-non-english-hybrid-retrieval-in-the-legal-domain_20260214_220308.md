---
ver: rpa2
title: 'Know When to Fuse: Investigating Non-English Hybrid Retrieval in the Legal
  Domain'
arxiv_id: '2409.01357'
source_url: https://arxiv.org/abs/2409.01357
tags:
- retrieval
- page
- bm25
- pages
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study explores hybrid retrieval combining diverse models\
  \ for legal question answering in French. While fusion improves performance in zero-shot\
  \ scenarios across all methods, in-domain training yields significant gains that\
  \ often render fusion unnecessary\u2014unless scores are carefully weighted."
---

# Know When to Fuse: Investigating Non-English Hybrid Retrieval in the Legal Domain

## Quick Facts
- arXiv ID: 2409.01357
- Source URL: https://arxiv.org/abs/2409.01357
- Authors: Antoine Louis; Gijs van Dijck; Gerasimos Spanakis
- Reference count: 40
- Primary result: Fusion improves zero-shot legal retrieval but often degrades in-domain performance unless weights are tuned

## Executive Summary
This study investigates hybrid retrieval combining diverse models for legal question answering in French. While fusion improves performance in zero-shot scenarios across all methods, in-domain training yields significant gains that often render fusion unnecessary—unless scores are carefully weighted. Models trained on domain-general data benefit from fusion, whereas specialized retrievers show limited complementarity due to similar learned relevance signals. Our findings highlight when fusion helps and when it may degrade performance, guiding optimal deployment in specialized domains.

## Method Summary
The study trains retrieval models on a domain-general French dataset (mMARCO-fr) with 8.8M passages, then optionally fine-tunes on a legal question-answering dataset (LLeQA) with 1,868 Belgian law questions. Models include BM25, DPR, SPLADE, ColBERT, and monoBERT using CamemBERT. Fusion techniques (BCF, RRF, NSF) combine scores using various normalization methods (min-max, z-score, percentile). Performance is evaluated using MRR@10, R-precision, and recall@k metrics, comparing zero-shot vs in-domain settings.

## Key Results
- Fusion consistently enhances zero-shot performance compared to standalone models
- In-domain training often makes fusion counterproductive unless scores are carefully weighted
- Score distribution misalignment across models harms fusion unless percentile scaling is used
- Specialized retrievers show limited complementarity, reducing fusion benefits in legal domain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fusion helps in zero-shot settings because domain-general models have complementary relevance signals that compensate for each other's weaknesses.
- Mechanism: When two retrieval models score the same document differently, one model may rank a relevant document highly while the other does not, so combining their scores increases the chance of surfacing relevant documents.
- Core assumption: The retrieval models capture different aspects of relevance (e.g., lexical vs semantic) and are not perfectly correlated in their predictions.
- Evidence anchors:
  - [abstract]: "in a zero-shot context, fusing different domain-general models consistently enhances performance compared to using a standalone model"
  - [section]: "We find that when one system scores high while the other does not, the higher-scoring system generally provides the correct signal, effectively compensating for the other’s error"
  - [corpus]: Weak – the corpus only contains 8 papers, all related to retrieval or legal search, so there's no direct evidence on fusion behavior in this dataset.
- Break condition: When models are trained on the same specialized domain, their learned relevance signals converge, reducing complementarity and making fusion counterproductive.

### Mechanism 2
- Claim: In-domain training improves retrieval effectiveness because the models learn domain-specific relevance patterns from labeled data.
- Mechanism: Fine-tuning on legal-domain questions and articles allows the retrievers to adjust their internal representations to match legal terminology, structure, and information needs.
- Core assumption: The small amount of in-domain labeled data contains enough signal to meaningfully adapt the model’s relevance matching.
- Evidence anchors:
  - [abstract]: "when models are trained in-domain, we find that fusion generally diminishes performance relative to using the best single system"
  - [section]: "our findings indicate that fusion almost always enhance performance on out-of-distribution data"
  - [corpus]: Weak – no corpus data directly supports in-domain training gains, but the cited papers are related to legal retrieval and would indirectly support the claim.
- Break condition: If the in-domain data is too small or unrepresentative, fine-tuning may overfit or fail to generalize, negating the benefit.

### Mechanism 3
- Claim: Score distribution misalignment across models harms fusion performance unless weights are tuned per domain.
- Mechanism: Different models produce scores on different scales or distributions; naive fusion treats them equally, so one model’s high relevance signal can be diluted by another’s lower relevance signal.
- Core assumption: Models’ score distributions are not naturally aligned and must be normalized or weighted to reflect their relative reliability in the target domain.
- Evidence anchors:
  - [section]: "traditional scaling methods lead to misaligned distributions among retrievers, particularly under min-max scaling" and "A score of 0.35 is adjusted to 0.5 for DPRFR-BASE and 0.95 for BM25"
  - [corpus]: Weak – corpus does not contain explicit discussion of score alignment issues.
- Break condition: If models are pre-aligned (e.g., via percentile scaling) or if domain tuning is skipped, fusion performance can degrade.

## Foundational Learning

- Concept: Understanding of lexical vs semantic matching
  - Why needed here: The paper contrasts BM25 (lexical) with neural models (semantic) to explain why hybrid retrieval can help.
  - Quick check question: Can you explain in one sentence why BM25 might miss relevant documents that a semantic model would catch?

- Concept: Zero-shot vs in-domain evaluation
  - Why needed here: The results hinge on comparing performance when models are trained on general data versus legal-specific data.
  - Quick check question: What is the key difference in the paper’s findings between zero-shot and in-domain settings?

- Concept: Score normalization and fusion techniques
  - Why needed here: Fusion effectiveness depends on how scores from different models are combined and normalized.
  - Quick check question: Which normalization method did the authors find ineffective despite seeming intuitive?

## Architecture Onboarding

- Component map:
  - Retriever models (BM25, DPR, SPLADE, ColBERT, monoBERT)
  - Fusion layer (NSF, BCF, RRF)
  - Normalization step (min-max, z-score, percentile)
  - Training pipeline (general → legal fine-tuning)
  - Evaluation harness (recall, MRR, R-precision)

- Critical path:
  1. Train or load base retrievers on general corpus
  2. Optionally fine-tune on in-domain data
  3. Index legal corpus with each retriever
  4. Run retrieval, collect scores
  5. Apply normalization and fusion
  6. Evaluate on held-out queries

- Design tradeoffs:
  - Fusion improves recall but increases memory and latency
  - Dense models need GPU; sparse models can run on CPU
  - Cross-encoders are accurate but too slow for end-to-end retrieval
  - Pre-finetuning helps dense models but may hurt sparse ones

- Failure signatures:
  - Performance drops when fusing in-domain models with equal weights
  - Fusion latency spikes when combining models with large index footprints
  - Low recall@10 indicates models are not retrieving relevant documents early enough

- First 3 experiments:
  1. Compare each retriever’s zero-shot performance on the legal corpus alone.
  2. Apply NSF fusion between BM25 and DPRFR-BASE and measure recall@10.
  3. Fine-tune DPRFR on the legal training set, then repeat experiment 2 in-domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do hybrid retrieval combinations perform across different legal domains (e.g., civil vs. criminal law) and how generalizable are the findings?
- Basis in paper: [explicit] The paper notes that findings are confined to the LLeQA dataset, which covers specific legal topics, and raises questions about generalizability across broader French legal resources and different jurisdictions.
- Why unresolved: The study's dataset is limited to one specific legal question-answering dataset, restricting the ability to test performance across diverse legal domains or jurisdictions.
- What evidence would resolve it: Testing hybrid retrieval combinations on datasets covering different legal domains (e.g., civil, criminal, administrative) and jurisdictions (e.g., France, Switzerland, Canada) would provide evidence of generalizability.

### Open Question 2
- Question: What is the impact of combining re-rankers with end-to-end retrievers in hybrid search for legal retrieval?
- Basis in paper: [inferred] The paper explicitly states that re-rankers are omitted due to their prohibitive inference costs for end-to-end retrieval, leaving open the question of their potential benefits when combined with retrievers.
- Why unresolved: The computational cost of using cross-encoders for end-to-end retrieval is prohibitive, preventing exploration of their potential benefits in hybrid combinations with other retrieval methods.
- What evidence would resolve it: Developing efficient methods to combine re-rankers with end-to-end retrievers, such as approximate inference techniques or model distillation, would allow testing their impact on hybrid search performance.

### Open Question 3
- Question: How do the findings on hybrid retrieval in French legal domains translate to other non-English languages and specialized domains?
- Basis in paper: [explicit] The paper acknowledges that it remains an open question whether the findings are applicable to other non-English languages within different highly specialized domains.
- Why unresolved: The study focuses specifically on French legal retrieval, and the unique characteristics of this language and domain may limit the applicability of findings to other contexts.
- What evidence would resolve it: Conducting similar studies in other non-English languages and specialized domains (e.g., German medical retrieval, Spanish financial retrieval) would provide evidence of the generalizability of the findings.

## Limitations
- Limited corpus size with only 8 relevant papers, making it difficult to fully validate claims about fusion behavior in specialized domains
- Experimental setup relies on a single legal domain (Belgian law) and French language, limiting generalizability
- Does not explore combination with re-rankers due to computational costs

## Confidence
- Zero-shot fusion benefits: High confidence - well-supported by multiple results across different model combinations
- In-domain fusion degradation: Medium confidence - observed consistently but limited to one domain
- Score distribution alignment importance: Medium confidence - demonstrated through specific examples but not extensively validated across all fusion methods

## Next Checks
1. Test fusion effectiveness on a second legal domain (e.g., French civil law) to verify domain-specific findings hold across different legal systems
2. Conduct ablation studies comparing percentile normalization against other scaling methods across multiple domains to quantify the impact of score distribution alignment
3. Measure computational overhead (latency, memory) of fusion approaches in production settings to validate the practical tradeoffs mentioned