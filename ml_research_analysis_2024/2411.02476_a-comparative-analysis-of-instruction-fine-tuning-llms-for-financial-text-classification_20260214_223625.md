---
ver: rpa2
title: A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial Text Classification
arxiv_id: '2411.02476'
source_url: https://arxiv.org/abs/2411.02476
tags:
- fine-tuned
- financial
- tasks
- fine-tuning
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates instruction fine-tuning on small LLMs for
  financial text classification tasks. The authors fine-tuned Mistral-7B, Llama3-8B,
  and Phi3-mini (both base and instruction-tuned variants) on four financial classification
  tasks: sentiment analysis, news headline classification, relation extraction, and
  hawkish-dovish classification.'
---

# A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial Text Classification

## Quick Facts
- arXiv ID: 2411.02476
- Source URL: https://arxiv.org/abs/2411.02476
- Reference count: 40
- Primary result: Instruction fine-tuning on small LLMs for financial tasks improves task-specific performance but risks catastrophic forgetting on unseen tasks, which model merging can mitigate

## Executive Summary
This paper investigates instruction fine-tuning on small language models (Mistral-7B, Llama3-8B, and Phi3-mini) for financial text classification tasks. The authors evaluate both base and instruction-tuned variants across four financial classification tasks, then test zero-shot performance on three unseen tasks. They find that base model fine-tuning leads to significant performance degradation on unseen tasks due to catastrophic forgetting, while instruction-tuned models maintain better generalization. To address this limitation, they employ model merging techniques to combine single-task fine-tuned models with base instruction models, achieving substantial improvements in zero-shot performance.

## Method Summary
The authors fine-tune three LLMs (Mistral-7B, Llama3-8B, and Phi3-mini) on four financial text classification tasks using both base and instruction-tuned variants. They employ LoRA for efficient fine-tuning and test zero-shot performance on three unseen tasks. When base models show catastrophic forgetting, they use MergeKit's Task Arithmetic to merge single-task fine-tuned models with vanilla instruction models. The evaluation compares zero-shot, few-shot, and fine-tuned performance across all tasks and models.

## Key Results
- Base model fine-tuning led to greater degradation on unseen tasks compared to instruction-tuned models
- Model merging with single-task fine-tuned models significantly enhanced zero-shot performance, often exceeding original zero-shot accuracy
- Instruction-tuned models showed better generalization across unseen tasks compared to base models after fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuned models maintain better generalization across unseen tasks compared to base models after fine-tuning.
- Mechanism: Instruction-tuned models already have exposure to diverse task instructions, creating a more robust underlying task understanding that persists even after domain-specific fine-tuning.
- Core assumption: The instruction-tuning process creates transferable task comprehension that isn't overwritten by domain-specific fine-tuning.
- Evidence anchors:
  - [abstract] "base model fine-tuning led to greater degradation, while instruction-tuned models maintained more robust performance"
  - [section] "our results on three unseen financial tasks... indicate that multi-task fine-tuned base models exhibit significantly greater performance declines compared to their instruct-tuned counterparts"
- Break condition: If fine-tuning completely overwrites the instruction-tuning layers or if the domain-specific data is so different that it conflicts with general task understanding.

### Mechanism 2
- Claim: Model merging with MergeKit framework effectively mitigates catastrophic forgetting while preserving task-specific performance.
- Mechanism: By arithmetically combining task-specific fine-tuned models with the original instruction model, the merged model retains both the domain knowledge and the general instruction-following capabilities.
- Core assumption: The arithmetic combination of model parameters can successfully integrate complementary knowledge without introducing conflicts.
- Evidence anchors:
  - [abstract] "model merging techniques... resulted in significant enhancements in zero-shot performance, even exceeding the original model's accuracy on certain datasets"
  - [section] "merging approach significantly enhances performance on unseen tasks, with results often surpassing the original zero-shot performance"
- Break condition: If the merged model weights create internal conflicts that degrade both task-specific and general performance.

### Mechanism 3
- Claim: Single-task fine-tuning provides better generalization for unseen tasks than multi-task fine-tuning when combined with model merging.
- Mechanism: Single-task models learn task-specific patterns without being influenced by the noise and interference from multiple tasks, making them better candidates for merging with the base instruction model.
- Core assumption: Specialized single-task models contain cleaner, more focused representations that can be successfully combined.
- Evidence anchors:
  - [section] "We utilized single-task fine-tuned models... to alleviate the degradation of zero-shot performance"
  - [section] "The results indicate that single-task fine-tuning performs similarly to multi-task fine-tuning" in appendix
- Break condition: If single-task models overfit too heavily to their specific tasks, making them incompatible with the base model.

## Foundational Learning

- Concept: Catastrophic forgetting in fine-tuning
  - Why needed here: The paper explicitly addresses this problem and its solution through model merging
  - Quick check question: What happens to a model's performance on original tasks when fine-tuned on new tasks without intervention?

- Concept: Model merging arithmetic techniques
  - Why needed here: The core innovation involves using MergeKit with Task Arithmetic to combine models
  - Quick check question: How does Task Arithmetic mathematically combine model parameters from different fine-tuned versions?

- Concept: Zero-shot vs few-shot vs fine-tuned performance evaluation
  - Why needed here: The paper compares all three approaches to understand the tradeoffs
  - Quick check question: What is the key difference between zero-shot and few-shot prompting in terms of model requirements?

## Architecture Onboarding

- Component map:
  - Input pipeline: Financial text classification datasets (sentiment, headline, relation extraction, hawkish-dovish)
  - Model zoo: Llama3-8B, Mistral-7B, Phi-3-mini (both base and instruction-tuned variants)
  - Fine-tuning pipeline: Single-task and multi-task fine-tuning with LoRA
  - Merging pipeline: MergeKit with Task Arithmetic
  - Evaluation pipeline: Seen tasks + three unseen tasks (argument unit, deal completeness, causal classification)

- Critical path:
  1. Prepare instruction datasets for both base and instruct models
  2. Fine-tune base and instruct models on four financial tasks
  3. Evaluate zero-shot performance on three unseen tasks
  4. Merge single-task fine-tuned models with vanilla instruct models
  5. Evaluate merged models on all tasks

- Design tradeoffs:
  - Model size vs performance: Smaller models (Phi-3) show competitive performance with less computational cost
  - Single-task vs multi-task fine-tuning: Single-task provides cleaner representations but requires more merging steps
  - Fine-tuning vs prompting: Fine-tuning gives better task-specific performance but risks catastrophic forgetting

- Failure signatures:
  - Performance degradation on unseen tasks indicates catastrophic forgetting
  - Merging failures show as either unchanged or decreased performance
  - Overfitting shows as high performance on training tasks but poor generalization

- First 3 experiments:
  1. Compare zero-shot performance of all models on the four financial tasks to establish baselines
  2. Fine-tune base models on each task individually and evaluate on unseen tasks to measure catastrophic forgetting
  3. Apply model merging to combine single-task fine-tuned models with vanilla instruct models and re-evaluate unseen tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of model merging techniques compare when using advanced merging methods like Dare and Tie versus the simpler arithmetic method used in this study?
- Basis in paper: [inferred] The paper mentions plans to explore advanced merging techniques like Dare and Tie in future work, suggesting potential performance differences compared to the arithmetic method used in this study.
- Why unresolved: The paper only implemented the arithmetic method for model merging and did not compare it to more advanced techniques.
- What evidence would resolve it: Experiments comparing the performance of different model merging techniques (arithmetic, Dare, Tie) on the same datasets and tasks used in this study would provide evidence for the relative effectiveness of each method.

### Open Question 2
- Question: How does the performance of smaller models like Phi-3-mini on unseen tasks compare to larger models like Llama3-8B and Mistral-7B, and what architectural or pre-training factors contribute to these differences?
- Basis in paper: [explicit] The paper notes that Phi-3-mini showed minimal performance decline on unseen tasks compared to larger models, despite its smaller size.
- Why unresolved: The paper does not investigate the underlying reasons for Phi-3-mini's superior generalization capabilities.
- What evidence would resolve it: Detailed architectural and pre-training analysis of Phi-3-mini compared to larger models, along with experiments isolating specific factors (e.g., model size, architecture, pre-training data), would provide insights into the causes of its better performance on unseen tasks.

### Open Question 3
- Question: What is the impact of incorporating generic instruction data into the fine-tuning process on the models' ability to handle unseen tasks, and how does this compare to the model merging approach used in this study?
- Basis in paper: [explicit] The paper mentions that previous studies have incorporated generic instruction data into fine-tuning, but chose not to do so to avoid increased computational costs.
- Why unresolved: The paper did not experiment with including generic instruction data in the fine-tuning process, so the potential benefits and drawbacks compared to model merging are unknown.
- What evidence would resolve it: Experiments comparing the performance of models fine-tuned with and without generic instruction data on both seen and unseen tasks would provide evidence for the effectiveness of this approach relative to model merging.

## Limitations
- Study focuses exclusively on financial text classification, limiting generalizability to other domains
- Only three small to medium-sized LLMs were evaluated, potentially missing behaviors of larger or different architectures
- Merging approach requires training multiple single-task models first, increasing computational overhead
- Only three unseen tasks were used for zero-shot evaluation, which may not fully capture generalization capabilities

## Confidence
High confidence in claims about instruction-tuned models maintaining better generalization - supported by clear quantitative comparisons showing consistent performance differences between base and instruction-tuned models across multiple tasks and datasets.

Medium confidence in model merging effectiveness - while results show improvements, the approach requires significant additional computational resources (training multiple single-task models) and the optimal merging weights may vary by task.

Medium confidence in single-task vs multi-task tradeoffs - the paper shows similar performance but doesn't deeply explore the computational or practical tradeoffs between these approaches.

## Next Checks
1. Test the merging approach on a non-financial domain (e.g., healthcare or legal text) to assess generalizability of the catastrophic forgetting mitigation technique.

2. Evaluate whether instruction-tuning on diverse non-financial tasks provides similar generalization benefits when fine-tuned on financial tasks, testing the robustness of the instruction-tuning advantage.

3. Compare the merging approach against continual learning techniques like elastic weight consolidation or learning without forgetting to determine if simpler methods could achieve similar results with less computational overhead.