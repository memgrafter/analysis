---
ver: rpa2
title: Task Addition in Multi-Task Learning by Geometrical Alignment
arxiv_id: '2409.16645'
source_url: https://arxiv.org/abs/2409.16645
tags:
- gate
- task
- tasks
- target
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces task addition for the Geometrically Aligned
  Transfer Encoder (GATE) to address computational scalability issues in multi-task
  molecular property prediction. The proposed two-stage approach performs supervised
  multi-task pre-training on source tasks, followed by addition and training of task-specific
  modules for target tasks.
---

# Task Addition in Multi-Task Learning by Geometrically Alignment

## Quick Facts
- arXiv ID: 2409.16645
- Source URL: https://arxiv.org/abs/2409.16645
- Reference count: 14
- This paper introduces task addition for the Geometrically Aligned Transfer Encoder (GATE) to address computational scalability issues in multi-task molecular property prediction

## Executive Summary
This paper presents a task addition framework for the Geometrically Aligned Transfer Encoder (GATE) that enables efficient addition of new tasks to pre-trained multi-task models. The approach addresses computational scalability challenges in molecular property prediction by allowing task-specific modules to be added to a pre-trained GATE model rather than retraining from scratch. The method consists of a two-stage process: supervised multi-task pre-training on source tasks followed by addition and training of task-specific modules for target tasks.

The proposed task-added GATE achieves significant computational efficiency gains while maintaining high prediction accuracy. Experimental results on molecular property prediction tasks demonstrate that the approach is 39.13× faster than training vanilla GATE from scratch while preserving 98.3% of the full model's correlation performance. The method shows robust performance across different task correlations and outperforms both single-task learning and task-added multi-task learning baselines.

## Method Summary
The task addition framework for GATE follows a two-stage approach to enable efficient multi-task learning. In the first stage, the model undergoes supervised multi-task pre-training on a set of source tasks using the original GATE architecture. This establishes a shared representation that captures common molecular properties across the source tasks. In the second stage, task-specific modules are added for target tasks and fine-tuned using the pre-trained weights from the first stage.

The key innovation lies in the geometrical alignment mechanism that allows new task modules to be integrated without disrupting the existing learned representations. The task-specific modules are designed to operate on the shared representation while maintaining the geometric properties of the molecular embeddings. This enables the model to leverage knowledge from source tasks when learning new target tasks, reducing the need for extensive additional training.

## Key Results
- Task-added GATE achieves average RMSE of 0.647 and Pearson correlation of 0.688 across 10 target tasks
- The approach is 39.13× faster than training vanilla GATE from scratch while maintaining 98.3% correlation recovery
- Task-added GATE shows robust performance independent of source task correlations, unlike task-added MTL which performs poorly when target and source tasks are less correlated

## Why This Works (Mechanism)
The task addition mechanism works by leveraging the geometric properties of molecular representations learned during pre-training. When new task modules are added, they can effectively utilize the shared representation space without requiring complete retraining. The geometrical alignment ensures that the added modules maintain compatibility with the existing representation structure, allowing for efficient knowledge transfer from source to target tasks.

The two-stage approach is particularly effective because the initial pre-training establishes a rich, task-agnostic representation of molecular properties. When target tasks are added, they can tap into this learned representation rather than starting from scratch. The geometrical alignment constraint prevents catastrophic forgetting and ensures that the addition of new tasks doesn't degrade performance on the source tasks.

## Foundational Learning
- Multi-task learning fundamentals: Understanding how multiple related tasks can be learned simultaneously through shared representations is crucial for grasping why pre-training on source tasks helps with target tasks
- Transfer learning principles: The approach relies on transferring knowledge from source to target tasks, requiring understanding of when and how transfer learning is effective
- Geometric deep learning: The method uses geometric alignment, which requires knowledge of how geometric properties of data can be preserved in learned representations
- Molecular representation learning: Understanding how molecules are represented in vector space and what properties are important for prediction tasks
- Efficient model extension techniques: Knowledge of how to add new capabilities to pre-trained models without full retraining is essential

Why needed: These concepts form the theoretical foundation that explains why task addition works effectively for molecular property prediction and under what conditions it can be expected to succeed.

Quick check: Can you explain how the geometric alignment constraint helps prevent catastrophic forgetting when adding new tasks?

## Architecture Onboarding

Component map: Input molecules -> Shared GATE encoder -> Source task heads (during pre-training) -> Target task heads (during task addition) -> Property predictions

Critical path: Molecular input features → Shared encoder → Task-specific modules → Property prediction

Design tradeoffs: The main tradeoff is between computational efficiency and model capacity. Task addition significantly reduces training time but may limit the model's ability to learn highly specialized representations for new tasks compared to full retraining.

Failure signatures: Poor performance when target tasks are highly dissimilar from source tasks, or when the pre-trained representation space doesn't capture relevant features for the new tasks. Also, potential degradation if the geometrical alignment constraint is too restrictive.

3 first experiments:
1. Test task addition with source and target tasks from completely different molecular property categories to assess transferability limits
2. Compare performance when adding single tasks versus multiple tasks simultaneously to understand scalability
3. Evaluate the impact of varying the number and diversity of source tasks on target task performance to determine optimal pre-training strategies

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- The computational efficiency gains and correlation recovery were demonstrated specifically on the ChEMBL dataset and may not translate to other domains with different data characteristics or task relationships
- The evaluation focuses primarily on quantitative performance metrics without exploring qualitative aspects of the learned representations or potential biases introduced by the two-stage training process
- The paper does not address scenarios where source and target tasks have fundamentally different feature distributions or when the pre-trained model architecture may not be well-suited for certain molecular properties

## Confidence
High: Claims about computational efficiency gains (39.13× faster) and performance preservation (98.3% correlation recovery) are well-supported by experimental evidence and rigorous comparisons with established baselines.

Medium: Claims about task independence and generalizability to other domains are supported by limited experimental evidence from a single molecular property prediction domain and require further validation.

Low: None identified.

## Next Checks
1. Test task addition on non-molecular datasets (e.g., vision, NLP) to assess cross-domain applicability and identify any domain-specific limitations.

2. Conduct ablation studies varying the number and diversity of source tasks to quantify the impact on target task performance and determine optimal source task selection strategies.

3. Evaluate the method's behavior when source and target tasks have minimal feature overlap or when source tasks are noisy/incorrectly labeled to assess robustness to task quality variations.