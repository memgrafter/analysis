---
ver: rpa2
title: 'Let''s Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum
  Learning'
arxiv_id: '2402.10738'
source_url: https://arxiv.org/abs/2402.10738
tags:
- learning
- curriculum
- iccl
- llms
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes In-Context Curriculum Learning (ICCL), a demonstration
  ordering method for in-context learning that arranges examples from simple to complex
  based on difficulty. The difficulty can be assessed by human experts or perplexity-based
  metrics.
---

# Let's Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning

## Quick Facts
- arXiv ID: 2402.10738
- Source URL: https://arxiv.org/abs/2402.10738
- Authors: Yinpeng Liu; Jiawei Liu; Xiang Shi; Qikai Cheng; Yong Huang; Wei Lu
- Reference count: 10
- Primary result: In-Context Curriculum Learning (ICCL) improves in-context learning performance by ordering demonstrations from simple to complex, achieving 9% improvement over random baseline at corpus level.

## Executive Summary
This paper introduces In-Context Curriculum Learning (ICCL), a demonstration ordering method for in-context learning that arranges examples from simple to complex based on difficulty assessment. The approach leverages the observation that LLMs can benefit from curriculum-like presentation of demonstrations, similar to how humans learn more effectively with structured progression. The method can use either human expert rankings or perplexity-based metrics to assess difficulty, and experiments show consistent improvements across three scientific NLP tasks and three open-source LLMs.

## Method Summary
ICCL reorders in-context learning demonstrations from simple to complex based on difficulty assessment, either by human experts or by perplexity calculations. The method applies to both corpus-level (fixed demonstration set) and instance-level (retrieved demonstrations per test sample) scenarios. The difficulty is quantified by measuring perplexity of labels given instructions, with lower perplexity indicating easier examples. The approach assumes that gradual complexity increase helps LLMs build understanding incrementally, and experimental results show this capability is primarily developed during the instruction-tuning stage rather than pre-training.

## Key Results
- ICCL achieves 9% improvement over random baseline and 7.8% over VoteK at corpus level
- ICCL shows 4.96% improvement over TopK baseline at instance level
- Instruction-tuned models benefit more from ICCL than base models (7.16% average decrement for base models)
- Both human-expert and perplexity-based difficulty assessment methods are effective

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradually increasing demonstration complexity enables LLMs to build understanding incrementally
- Mechanism: The model learns basic patterns from simple examples, then applies this foundation to tackle increasingly complex tasks
- Core assumption: LLMs can transfer knowledge between examples within the same inference session when demonstrations are ordered from simple to complex
- Evidence anchors: [abstract] "The ICCL implies gradually increasing the complexity of prompt demonstrations during the inference process."
- Break condition: If the model cannot establish meaningful connections between examples, or if complex examples are introduced too early

### Mechanism 2
- Claim: Perplexity-based difficulty assessment allows the LLM itself to identify complexity gradients
- Mechanism: By measuring perplexity of labels given instructions, the model quantifies how "surprising" or difficult each example is to process
- Core assumption: Lower perplexity indicates better model understanding, so examples with lower perplexity are "easier" for the model to process correctly
- Evidence anchors: [section] "We measure the complexity of a sample by calculating the perplexity on the label yi given the specified instruction"
- Break condition: If perplexity doesn't correlate with actual difficulty for the model

### Mechanism 3
- Claim: Instruction-tuning develops the model's ability to benefit from curriculum ordering
- Mechanism: During instruction-tuning, models learn to recognize instructional patterns and benefit from structured example presentation
- Core assumption: The curriculum learning capability is an emergent property of instruction-tuning rather than pre-training
- Evidence anchors: [section] "Comparative analysis indicates that the ICCL capability of LLMs is developed during the instruction-tuning stage"
- Break condition: If instruction-tuning doesn't consistently improve curriculum sensitivity across different model architectures

## Foundational Learning

- Concept: In-Context Learning (ICL)
  - Why needed here: ICCL builds directly on ICL principles - using demonstrations in prompts without parameter updates
  - Quick check question: How does ICL differ from fine-tuning, and why is this distinction important for ICCL's approach?

- Concept: Perplexity as a difficulty metric
  - Why needed here: The paper uses perplexity to quantify example complexity for automatic curriculum construction
  - Quick check question: What does a lower perplexity score indicate about a model's understanding of a given example?

- Concept: Curriculum Learning in Machine Learning
  - Why needed here: ICCL adapts curriculum learning principles from training to inference, requiring understanding of how curriculum order affects learning
  - Quick check question: How does the concept of gradually increasing task difficulty apply differently in training versus inference contexts?

## Architecture Onboarding

- Component map: Curriculum Constructor -> Demonstration Retriever -> Prompt Builder -> LLM Inference Engine -> Performance Evaluator

- Critical path:
  1. Select or retrieve candidate demonstrations
  2. Assess difficulty (human ranking or perplexity calculation)
  3. Order demonstrations from simple to complex
  4. Build prompt with ordered demonstrations
  5. Run inference and evaluate performance

- Design tradeoffs:
  - Human vs. automatic difficulty assessment: human provides domain expertise but doesn't scale; automatic is scalable but may miss nuanced difficulty factors
  - Corpus-level vs. instance-level ICCL: corpus-level is simpler but less personalized; instance-level is more targeted but requires retrieval overhead
  - Number of demonstrations: more examples provide better curriculum but increase prompt length and computational cost

- Failure signatures:
  - Performance degradation when using ICCL (suggests ordering is counterproductive)
  - High variance across different test samples (indicates inconsistent curriculum effectiveness)
  - No improvement over random ordering (suggests model isn't benefiting from curriculum structure)

- First 3 experiments:
  1. Compare ICCL ordering vs. random ordering on a single task with one model to establish baseline effectiveness
  2. Test human-expert ordering vs. perplexity-based ordering to validate automatic difficulty assessment
  3. Run ICCL on both base and instruction-tuned versions of the same model to confirm instruction-tuning dependency

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to scientific NLP tasks with small datasets, reducing generalizability to broader NLP applications
- Perplexity-based difficulty metric lacks theoretical grounding for why it correlates with optimal learning order
- Instruction-tuning dependency finding only tested on three model architectures, limiting confidence in broader applicability
- Computational overhead of instance-level ICCL isn't fully characterized, and scalability to larger demonstration sets remains unexplored

## Confidence
**High Confidence**: Core finding that curriculum ordering improves performance over random ordering is well-supported by consistent results across three tasks, three models, and multiple baseline comparisons.

**Medium Confidence**: Perplexity-based difficulty metric is shown to be effective, but theoretical justification for correlation with optimal learning order is weak.

**Low Confidence**: Claims about specific mechanism by which curriculum ordering improves performance (knowledge transfer between examples) are speculative.

## Next Checks
1. **Cross-domain validation**: Test ICCL on non-scientific NLP tasks to assess generalizability beyond the scientific domain.

2. **Alternative difficulty metrics**: Implement and compare ICCL using different difficulty assessment methods (e.g., entropy, model uncertainty) against the perplexity baseline.

3. **Extended demonstration analysis**: Systematically vary the number of demonstrations per prompt (2-10 examples) and measure how curriculum sensitivity changes with demonstration set size.