---
ver: rpa2
title: SHAP scores fail pervasively even when Lipschitz succeeds
arxiv_id: '2412.13866'
source_url: https://arxiv.org/abs/2412.13866
tags:
- feature
- shap
- scores
- regression
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHAP scores are widely used in explainable AI but can be misleading.
  This paper demonstrates that SHAP scores fail even for Lipschitz-continuous regression
  models, which are important for adversarial robustness.
---

# SHAP scores fail pervasively even when Lipschitz succeeds

## Quick Facts
- arXiv ID: 2412.13866
- Source URL: https://arxiv.org/abs/2412.13866
- Reference count: 40
- SHAP scores systematically fail to capture true feature importance even for Lipschitz-continuous regression models

## Executive Summary
This paper demonstrates that SHAP (SHapley Additive exPlanations) scores, widely used in explainable AI, can be fundamentally misleading even when applied to regression models that satisfy important mathematical properties like Lipschitz continuity. The authors construct specific regression functions that respect these properties but still produce SHAP scores that assign importance to irrelevant features while missing relevant ones. This finding challenges the reliability of SHAP scores for high-stakes AI applications and suggests the need for alternative explanation methods that can better capture feature importance in practical scenarios.

## Method Summary
The authors construct specific regression models that respect mathematical constraints like Lipschitz continuity and arbitrary differentiability, then compute SHAP scores using the standard Shapley value formula. They systematically verify that these constructed models produce problematic SHAP score patterns where irrelevant features receive non-zero importance scores and relevant features receive zero scores. The methodology includes polynomial interpolation techniques to create arbitrarily differentiable functions while preserving the problematic SHAP score properties, and extends formal explainability frameworks to handle both classification and regression domains.

## Key Results
- SHAP scores fail for Boolean classifiers with arbitrarily many variables, producing unsatisfactory explanations
- The same SHAP score failures occur for regression models with finite and uncountable codomains
- Lipschitz-continuous regression functions can still produce misleading SHAP scores
- Arbitrarily differentiable regression functions maintain the same SHAP score problems through polynomial interpolation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SHAP scores fail even when Lipschitz continuity is satisfied
- Mechanism: The paper constructs regression functions that respect Lipschitz continuity but still produce misleading SHAP scores by assigning non-zero importance to irrelevant features and zero importance to relevant ones
- Core assumption: The expected values of the Lipschitz-continuous regression function can be manipulated to produce the same problematic SHAP score patterns as non-continuous functions
- Evidence anchors:
  - [abstract] "the paper shows that the issues with SHAP scores occur even for regression models that respect Lipschitz continuity"
  - [section] "Given the above, this section proves the following result. Proposition 8. There exist regression models, respecting Lipschitz continuity, for which each feature i is either irrelevant and its SHAP score is non-zero or the feature is relevant and its SHAP score is zero."
- Break condition: If the Lipschitz constant constraint prevents the expected value manipulation needed to create the problematic SHAP score patterns

### Mechanism 2
- Claim: SHAP score failures extend to arbitrarily differentiable functions
- Mechanism: By using polynomial interpolation techniques, the paper shows how to create C^∞ regression functions that maintain the same SHAP score problems as their non-differentiable counterparts
- Core assumption: Polynomial interpolation can preserve both the expected value structure needed for problematic SHAP scores and the required differentiability properties
- Evidence anchors:
  - [abstract] "Finally, the paper shows that the same issues are guaranteed to exist for arbitrarily differentiable regression models"
  - [section] "The main insight is to take the rectangle [1 − ǫ, 1 + ǫ][0, 2] with (a very small) ǫ > 0. In this rectangle, we replace the function by polynomials on x1 with the same value on {1}[0, 2], on {1 − ǫ}[0, 2] and on {1 + ǫ}[0, 2]"
- Break condition: If the polynomial interpolation introduces derivative constraints that prevent the expected value manipulation

### Mechanism 3
- Claim: SHAP score failures are pervasive across classification and regression domains
- Mechanism: The paper generalizes the formal explainability framework to handle both classification and regression, showing that the fundamental issues with SHAP scores persist across these domains
- Core assumption: The similarity predicate framework can abstract away the differences between classification and regression while preserving the core explainability properties
- Evidence anchors:
  - [abstract] "The paper shows that for Boolean classifiers there are arbitrarily many examples for which the SHAP scores must be deemed unsatisfactory"
  - [section] "Given a sample (v, q), and a norm lp, a point x ∈ F is an adversarial example (AEx) if the prediction for x is distinguishable from that for v"
- Break condition: If the similarity predicate framework cannot adequately capture the distinction between classification and regression for explainability purposes

## Foundational Learning

- Concept: Shapley values and their axiomatic foundations
  - Why needed here: Understanding why SHAP scores are defined the way they are and what axioms they satisfy is crucial for understanding why they can fail
  - Quick check question: What are the four key axioms that make Shapley values unique, and how do they relate to feature importance?

- Concept: Lipschitz continuity and its relationship to adversarial robustness
  - Why needed here: The paper's main contribution is showing that even Lipschitz-continuous functions can produce misleading SHAP scores
  - Quick check question: How does Lipschitz continuity formally relate to adversarial robustness in machine learning models?

- Concept: Formal explainability frameworks (AXps, CXps, AExs)
- Why needed here: The paper uses these concepts to rigorously define when SHAP scores are unsatisfactory
  - Quick check question: What is the relationship between abductive explanations (AXps) and contrastive explanations (CXps) in formal explainability?

## Architecture Onboarding

- Component map:
  - Regression function construction module -> Expected value computation engine -> SHAP score calculation pipeline -> Formal explainability verification system

- Critical path:
  1. Define regression function with desired properties (Lipschitz, differentiable)
  2. Compute expected values for all feature subsets
  3. Calculate SHAP scores using the standard formula
  4. Verify that SHAP scores assign importance incorrectly
  5. Prove the construction can be generalized

- Design tradeoffs:
  - Simplicity vs. generality of examples
  - Continuity vs. ease of construction
  - Computational efficiency vs. mathematical rigor

- Failure signatures:
  - SHAP scores that don't match formal explainability criteria
  - Expected values that don't produce the desired SHAP patterns
  - Lipschitz constants that are too restrictive

- First 3 experiments:
  1. Implement the ρ3 function from Figure 2 and verify its Lipschitz continuity
  2. Compute expected values for the M2 example and verify SHAP scores
  3. Create a polynomial interpolation of ρ3 and verify C^∞ differentiability while preserving SHAP score properties

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do SHAP scores behave when comparing regression models with Lipschitz continuity to models without such constraints?
- Basis in paper: [explicit] The paper compares SHAP scores in Lipschitz-continuous regression models to other types of models.
- Why unresolved: The paper demonstrates that SHAP scores are unsatisfactory even in Lipschitz-continuous models, but does not quantify how performance differs between Lipschitz and non-Lipschitz models.
- What evidence would resolve it: Empirical studies comparing SHAP scores across various Lipschitz and non-Lipschitz regression models with different degrees of continuity.

### Open Question 2
- Question: Can the theoretical limitations of SHAP scores identified for boolean classifiers be extended to multi-class classification problems?
- Basis in paper: [explicit] The paper proves limitations for boolean classifiers but does not address multi-class scenarios.
- Why unresolved: The proofs for boolean classifiers rely on specific properties of binary outputs that may not generalize to multi-class settings.
- What evidence would resolve it: Formal proofs demonstrating whether the same SHAP score issues persist when extending from boolean to multi-class classification frameworks.

### Open Question 3
- Question: What alternative explanation methods could effectively address the limitations of SHAP scores in Lipschitz-continuous regression models?
- Basis in paper: [inferred] The paper mentions that alternative measures of feature importance are justified but does not specify which methods would work.
- Why unresolved: While the paper critiques SHAP scores, it does not provide specific alternatives or evaluate their effectiveness in these problematic cases.
- What evidence would resolve it: Comparative analysis of alternative explanation methods (like SHAP alternatives mentioned in the literature) applied to the same Lipschitz-continuous regression models that SHAP fails on.

## Limitations

- The constructed examples, while mathematically rigorous, may represent edge cases rather than typical model behaviors in practical applications
- The paper focuses on theoretical limitations without providing quantitative measures of how frequently SHAP score failures occur in real-world scenarios
- Alternative explanation methods are mentioned but not thoroughly evaluated against the problematic cases identified

## Confidence

- **High confidence**: The mathematical proofs showing SHAP score failures for Boolean classifiers with arbitrarily many variables are sound and follow directly from established Shapley value properties.
- **Medium confidence**: The extension to Lipschitz-continuous regression functions is well-constructed, but the practical significance depends on how common such functions are in real-world applications.
- **Medium confidence**: The polynomial interpolation technique for creating C^∞ functions preserves the problematic SHAP score patterns, though the complexity of these constructions may limit their practical relevance.

## Next Checks

1. Implement the M3 Lipschitz-continuous function and verify both the Lipschitz constant and the resulting SHAP score patterns match the paper's claims through numerical computation.
2. Create a comprehensive test suite that generates random regression functions satisfying Lipschitz continuity and measure the frequency of SHAP score failures compared to the specific constructed examples.
3. Compare SHAP score failures with alternative explainability methods (like LIME or integrated gradients) on the same Lipschitz-continuous functions to assess whether this is a fundamental limitation of additive feature attribution methods.