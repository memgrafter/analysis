---
ver: rpa2
title: 'Harnessing Language for Coordination: A Framework and Benchmark for LLM-Driven
  Multi-Agent Control'
arxiv_id: '2412.11761'
source_url: https://arxiv.org/abs/2412.11761
tags:
- units
- plan
- position
- attack
- behavior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HIVE, a framework that enables natural language-based
  control of large agent swarms (up to 2,000 units) through human-LLM collaboration
  in real-time strategy games. HIVE translates high-level human instructions into
  structured operational plans using behavior trees, allowing a single human to coordinate
  massive numbers of units.
---

# Harnessing Language for Coordination: A Framework and Benchmark for LLM-Driven Multi-Agent Control

## Quick Facts
- arXiv ID: 2412.11761
- Source URL: https://arxiv.org/abs/2412.11761
- Reference count: 40
- Primary result: HIVE framework enables natural language control of up to 2,000 units in real-time strategy games, with Claude Sonnet 3.5 achieving highest success rates across five multi-agent coordination tests

## Executive Summary
HIVE introduces a novel framework that bridges human strategic intent with large-scale multi-agent execution through LLM-driven translation. The system takes high-level human instructions and converts them into structured behavior tree plans that coordinate massive agent swarms in turn-based strategy games. The framework demonstrates that state-of-the-art LLMs can successfully manage complex coordination tasks like spatial navigation, terrain utilization, and tactical planning across up to 2,000 units. The benchmark reveals both promising capabilities and significant limitations, showing that while LLMs can handle structured coordination, they struggle with visual-spatial reasoning and long-term strategic planning when operating independently.

## Method Summary
The HIVE framework implements a turn-based, grid-based strategy game environment where human players provide high-level strategic goals that are translated into operational plans via behavior trees. The system uses nine state-of-the-art LLMs (including GPT-4o, Claude Sonnet 3.5, and Gemini models) to generate these plans, which are then executed by agent swarms of up to 2,000 units. The framework evaluates performance across five core multi-agent abilities: coordination, exploiting weaknesses, following spatial markers, terrain utilization, and strategic planning. Each ability test is run with ten different prompt variations, measuring success rates through wins, losses, ties, and early completions. The system employs a structured output format where LLMs must generate valid behavior trees that the game engine can parse and execute.

## Key Results
- Claude Sonnet 3.5 achieved the highest overall success rate across all five ability tests, demonstrating superior coordination and strategic planning capabilities
- HIVE successfully coordinated swarms of up to 2,000 units through LLM-driven plan generation, validating the framework's scalability
- Visual-spatial reasoning remains a significant bottleneck, with LLMs performing notably worse when processing raw game images compared to textual map descriptions
- The framework shows strong sensitivity to prompt variations and temperature settings, indicating the need for careful prompt engineering
- Human collaboration substantially improves performance, with HIVE performing significantly better when assisted by human players versus operating independently

## Why This Works (Mechanism)
HIVE works by creating a structured translation layer between human strategic intent and agent-level execution. The framework uses behavior trees as an intermediate representation that captures complex multi-agent coordination patterns in a format both humans and LLMs can understand. This structured approach allows LLMs to reason about large-scale coordination without needing to directly manage individual agent actions. The turn-based nature of the game provides sufficient time for LLM inference (4-12 seconds) while still maintaining strategic depth. The five ability tests are carefully designed to isolate specific coordination challenges, allowing systematic evaluation of LLM capabilities across different dimensions of multi-agent control.

## Foundational Learning

**Behavior Trees** - A hierarchical planning representation that encodes agent behaviors as tree structures with conditional branches and action nodes. Why needed: Provides a structured, executable format that LLMs can generate reliably. Quick check: Can the LLM produce syntactically valid behavior trees for simple coordination tasks?

**Multi-Agent Coordination** - The study of how multiple autonomous agents can work together to achieve shared objectives. Why needed: Forms the core challenge being addressed by HIVE's translation framework. Quick check: Does the system maintain coordination when scaling from 10 to 2,000 agents?

**Prompt Engineering** - The practice of crafting input prompts to elicit desired responses from LLMs. Why needed: HIVE's performance is highly sensitive to prompt variations and temperature settings. Quick check: Can success rates be improved through systematic prompt refinement?

**Visual-Spatial Reasoning** - The ability to understand and manipulate visual information about spatial relationships. Why needed: Current LLMs struggle significantly with processing game map images versus textual descriptions. Quick check: Compare performance with text vs. image map inputs.

**Structured Output Formats** - Predefined templates or schemas that constrain LLM responses to specific formats. Why needed: Enables reliable parsing of LLM-generated plans into executable behavior trees. Quick check: Does the LLM consistently follow the specified output format across different ability tests?

## Architecture Onboarding

**Component Map:** Human instructions -> System prompt -> LLM inference -> Behavior tree generation -> Plan parsing -> Agent execution -> Game state update -> Performance metrics

**Critical Path:** Human command → System prompt formatting → LLM plan generation → Behavior tree parsing → Agent execution → Victory condition check

**Design Tradeoffs:** The framework prioritizes interpretability and control over raw performance speed, choosing turn-based execution to accommodate LLM inference times (4-12 seconds) rather than pursuing real-time continuous planning. This decision enables reliable coordination but limits applicability to fast-paced scenarios.

**Failure Signatures:** Common failures include invalid plan generation (LLMs returning malformed behavior trees), no-plan returns (LLMs failing to generate any plan), and poor visual-spatial reasoning when processing map images. These failures typically manifest as agent inactivity or uncoordinated movement patterns.

**Three First Experiments:**
1. Test LLM plan generation with simple coordination tasks (5-10 agents) using the Coordinate ability test
2. Evaluate prompt sensitivity by running the same ability test with systematically varied temperature settings
3. Compare performance between textual map descriptions and grid-based representations to isolate visual reasoning capabilities

## Open Questions the Paper Calls Out

**Open Question 1:** How can we improve LLM visual-spatial reasoning for strategy game maps? The paper demonstrates that current LLMs struggle significantly when processing raw visual map data compared to textual descriptions, suggesting fundamental limitations in visual-spatial reasoning capabilities that need addressing for real-world deployment.

**Open Question 2:** Can HIVE's inference time be reduced to enable real-time continuous planning? Current LLM inference times (4-12 seconds) create bottlenecks for dynamic environments where average plan execution lasts only 10-20 seconds, raising questions about the feasibility of real-time adjustment capabilities.

**Open Question 3:** What is the optimal balance between human and LLM contributions in hybrid intelligence systems? The paper shows clear performance benefits from human collaboration but doesn't explore intermediate levels of human involvement or different types of human contributions beyond binary conditions.

## Limitations

- Performance is highly sensitive to prompt variations and temperature settings, requiring extensive prompt engineering for optimal results
- Visual-spatial reasoning remains a significant bottleneck, with LLMs performing notably worse when processing raw game images compared to textual descriptions
- The framework's focus on turn-based, grid-based environments limits generalizability to continuous or real-time control scenarios

## Confidence

High confidence: HIVE framework architecture and basic functionality across multiple LLMs
Medium confidence: Performance comparisons between specific LLM models
Low confidence: Generalizability to real-time, continuous control environments

## Next Checks

1. Replicate the benchmark with additional prompt variations and temperature settings to characterize sensitivity across the full range of LLM configurations
2. Extend evaluation to continuous control environments to test framework applicability beyond turn-based settings
3. Compare LLM performance when processing raw visual inputs versus alternative representation formats (grids, textual descriptions) to isolate visual reasoning capabilities