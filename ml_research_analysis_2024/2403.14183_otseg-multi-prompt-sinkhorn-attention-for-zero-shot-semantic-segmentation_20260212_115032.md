---
ver: rpa2
title: 'OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation'
arxiv_id: '2403.14183'
source_url: https://arxiv.org/abs/2403.14183
tags:
- otseg
- text
- segmentation
- semantic
- sinkhorn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OTSeg, a zero-shot semantic segmentation
  method that leverages a novel Multi-Prompts Sinkhorn Attention (MPSA) module to
  better align text prompts with image pixels. The key innovation is replacing standard
  cross-attention with MPSA, which uses Optimal Transport to allow multiple text prompts
  to selectively focus on different semantic features in the image.
---

# OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation

## Quick Facts
- arXiv ID: 2403.14183
- Source URL: https://arxiv.org/abs/2403.14183
- Authors: Kwanyoung Kim; Yujin Oh; Jong Chul Ye
- Reference count: 40
- One-line primary result: Achieves state-of-the-art zero-shot semantic segmentation performance using Multi-Prompts Sinkhorn Attention (MPSA) with significant improvements in mean IoU over previous methods.

## Executive Summary
OTSeg introduces a novel approach to zero-shot semantic segmentation by replacing standard cross-attention with Multi-Prompts Sinkhorn Attention (MPSA). This method leverages Optimal Transport to align multiple text prompts with image pixels, enabling each prompt to focus on distinct semantic regions rather than being conflated. The framework demonstrates strong performance across three benchmark datasets, achieving state-of-the-art results in both inductive and transductive settings.

The key innovation lies in using the Sinkhorn algorithm to create a doubly stochastic attention matrix, allowing multiple text prompts to selectively focus on different semantic features in the image. By combining this with a lightweight transformer decoder and an ensemble strategy, OTSeg significantly improves segmentation accuracy for unseen classes while maintaining strong performance on seen classes.

## Method Summary
OTSeg is a zero-shot semantic segmentation method that leverages CLIP's pre-trained vision-language model with a novel Multi-Prompts Sinkhorn Attention (MPSA) module. The method uses CLIP ViT-B/16 backbone with visual prompt tuning, multiple text prompts (6 for VOC, 8 for others), and three-layer transformer decoder with MPSA modules. It employs both focal and dice loss functions during training. The approach works by encoding images and multiple text prompts, computing text-pixel alignment via optimal transport, refining through a decoder with MPSA layers, and ensembling the decoder output with the MPS-refined score map to generate final pixel-wise class predictions.

## Key Results
- Achieves state-of-the-art performance on three benchmark datasets (VOC 2012, PASCAL Context, COCO-Stuff164K) in zero-shot semantic segmentation
- Demonstrates significant improvements in mean IoU (e.g., +5.5 points on COCO-Stuff164K) over previous methods
- Shows strong generalization in cross-dataset scenarios with balanced performance on both seen and unseen classes
- Validates effectiveness of MPSA through ablation studies showing consistent gains over standard cross-attention baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing SoftMax with Sinkhorn in cross-attention leads to better alignment between multiple text prompts and image pixels.
- Mechanism: Sinkhorn ensures a doubly stochastic attention matrix, allowing each text prompt to focus on distinct semantic regions of the image rather than being conflated.
- Core assumption: The optimal transport plan learned via Sinkhorn better matches the natural correspondence between text prompts and image regions than SoftMax does.
- Evidence anchors: [abstract] "we introduce the extension of MPS, called Multi-Prompts Sinkhorn Attention (MPSA), which effectively replaces cross-attention mechanisms within Transformer framework in multimodal settings." [section 4.2(b)] "Similar to the cross-attention mechanism Eq. (11), we define our proposed Multi-Prompts Sinkhorn Attention as follows: Multi-Prompts Sinkhorn Attention(Q, K, V) = MPS(QK⊤)V." [corpus] Weak: No direct citation; relies on analogy to Sinkformer in unimodal setting.
- Break condition: If the learned transport plan does not reflect semantic alignment, performance degrades to baseline SoftMax levels.

### Mechanism 2
- Claim: Using multiple hand-crafted text prompts with MPSA diversifies the score maps and improves segmentation accuracy.
- Mechanism: Each prompt focuses on different semantic attributes; the transport plan assigns different pixels to different prompts, leading to complementary score maps.
- Core assumption: Multiple prompts can capture distinct semantic facets, and optimal transport can map these to corresponding image regions.
- Evidence anchors: [section 4.1] "The multiple text prompts are created as a set of N text prompts denoted as P = {Pi|N i=1}. Each text prompt Pi can be defined as Pi = [P i 1, P i 2, · · · , P i l ]" [section 5.6] "With MPSA, we observe that each prompt-related score map is diversely dispersed, whereas the baseline method without MPSA shows significantly cohered score maps." [corpus] Moderate: Related works (Chen et al., 2022; PLOT) support prompt diversity but not in zero-shot segmentation.
- Break condition: If prompts are semantically redundant, transport cannot create diverse maps, reducing benefit.

### Mechanism 3
- Claim: Ensemble of decoder output and refined score map via MPS improves zero-shot segmentation performance.
- Mechanism: Decoder path learns to refine segmentation masks; MPS path directly aligns text and pixels; ensembling combines strengths of both.
- Core assumption: Both paths capture complementary information; balance factor λ can optimally weight them.
- Evidence anchors: [section 4.2(c)] "Y∗ = λ · Y + (1 − λ) · ˜Y" [section 5.6] "the proposed ensemble strategy yields the best performance in almost settings, validating the rationale of our ensemble approach." [corpus] Moderate: Ensemble strategies are common in segmentation but specific to this architecture is novel.
- Break condition: If one path dominates, ensembling may not improve or could degrade performance.

## Foundational Learning

- Concept: Optimal Transport (OT) and Sinkhorn algorithm
  - Why needed here: OT provides a principled way to align probability distributions, crucial for aligning text prompts with image pixels in a zero-shot setting.
  - Quick check question: What is the role of the regularization parameter ϵ in the Sinkhorn algorithm, and how does it affect convergence?

- Concept: Cross-attention in Transformers
  - Why needed here: Cross-attention allows the model to attend to image features conditioned on text prompts; replacing SoftMax with MPSA is the core innovation.
  - Quick check question: How does the attention score matrix QK⊤ differ between standard cross-attention and Multi-Prompts Sinkhorn Attention?

- Concept: Zero-shot semantic segmentation
  - Why needed here: The goal is to segment objects of unseen classes without pixel-level annotations, leveraging pre-trained vision-language models.
  - Quick check question: What is the difference between inductive and transductive settings in zero-shot semantic segmentation?

## Architecture Onboarding

- Component map: Image and text prompts -> CLIP text encoder -> text embeddings (multiple prompts) -> CLIP image encoder -> pixel embeddings -> MPS module -> MPSA module -> decoder with MPSA layers -> ensemble with MPS score map -> Output: pixel-wise class predictions

- Critical path:
  1. Encode image and text (multiple prompts)
  2. Compute text-pixel alignment score map
  3. Apply MPS to refine alignment
  4. Pass through decoder with MPSA layers
  5. Ensemble decoder output and MPS score map
  6. Generate final segmentation mask

- Design tradeoffs:
  - Multiple prompts increase diversity but add complexity
  - MPSA replaces cross-attention, improving alignment but increasing computation
  - Ensemble balances learned and direct alignment paths

- Failure signatures:
  - Degraded performance if MPS does not converge or aligns poorly
  - Overfitting if decoder learns spurious correlations
  - Poor generalization if prompts are not representative

- First 3 experiments:
  1. Ablation: Replace MPSA with standard cross-attention; compare mIoU.
  2. Ablation: Remove multiple prompts, use single prompt; measure diversity and accuracy.
  3. Ablation: Remove ensemble, use only decoder or only MPS path; compare performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of OTSeg change with different numbers of text prompts (N) beyond those tested in the paper?
- Basis in paper: [explicit] The paper tests N = 6 for VOC 2012 and N = 8 for PASCAL Context and COCO-Stuff164K, but does not explore values significantly larger or smaller than these.
- Why unresolved: The paper only explores a limited range of N values and does not provide a comprehensive study of how performance scales with the number of prompts.
- What evidence would resolve it: Experiments showing OTSeg's performance with a wider range of N values (e.g., N = 2, 4, 10, 12) on each dataset, along with an analysis of the trade-off between performance and computational cost.

### Open Question 2
- Question: Can the MPSA module be effectively applied to other multimodal tasks beyond zero-shot semantic segmentation, such as image-text retrieval or visual question answering?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of MPSA for zero-shot semantic segmentation, but does not explore its potential for other multimodal tasks.
- Why unresolved: The paper focuses solely on zero-shot semantic segmentation and does not provide evidence for the generalizability of MPSA to other tasks.
- What evidence would resolve it: Experiments applying MPSA to other multimodal tasks and comparing its performance to state-of-the-art methods in those domains.

### Open Question 3
- Question: How does the performance of OTSeg compare to fully supervised methods when trained on a larger number of seen classes?
- Basis in paper: [explicit] The paper compares OTSeg's performance to fully supervised methods, but only when trained on the same number of seen classes as in the zero-shot setting.
- Why unresolved: The paper does not explore how OTSeg's performance scales with the number of seen classes compared to fully supervised methods.
- What evidence would resolve it: Experiments training OTSeg on increasingly larger numbers of seen classes and comparing its performance to fully supervised methods with the same number of classes.

### Open Question 4
- Question: How sensitive is OTSeg's performance to the choice of the CLIP model architecture (e.g., ViT-B/16 vs. ViT-L/14)?
- Basis in paper: [explicit] The paper uses the CLIP ViT-B/16 model, but does not explore the impact of using different CLIP model architectures.
- Why unresolved: The paper does not provide evidence for the robustness of OTSeg to different CLIP model choices.
- What evidence would resolve it: Experiments training and evaluating OTSeg using different CLIP model architectures and comparing the results.

## Limitations
- Performance stability claims lack confidence intervals or variance metrics across different random seeds
- Method heavily depends on hand-crafted text prompts whose exact formulation is not specified, creating reproducibility barriers
- Computational overhead of Sinkhorn algorithm during inference is not discussed despite replacing standard SoftMax attention

## Confidence
**High Confidence**: The core mechanism of replacing SoftMax with Sinkhorn in cross-attention is well-supported by theoretical foundations in optimal transport and has precedent in the Sinkformer work. The implementation details provided are sufficient for technical validation.

**Medium Confidence**: The ensemble strategy combining decoder output with MPS-refined score maps shows consistent performance gains, but the specific weighting parameter λ=0.5 is not justified through ablation studies or theoretical analysis.

**Low Confidence**: Claims about the diversity of score maps produced by multiple prompts are primarily supported by qualitative visualizations (Fig. 5) rather than quantitative metrics measuring prompt diversity or semantic coverage.

## Next Checks
1. **Ablation Study with Confidence Intervals**: Run the OTSeg pipeline across 5 different random seeds for each benchmark dataset, reporting mean IoU with 95% confidence intervals. Compare against the baseline SoftMax attention with identical seeds to assess statistical significance of improvements.

2. **Prompt Sensitivity Analysis**: Systematically vary the number and content of text prompts (e.g., test with 2, 4, 6, 8 prompts using different semantic descriptors) and measure the impact on both seen and unseen class performance. This will reveal the method's robustness to prompt engineering choices.

3. **Runtime Efficiency Benchmark**: Profile the computational overhead of the Sinkhorn algorithm implementation during inference on representative hardware (e.g., A100 GPU). Compare FPS and memory usage against the baseline SoftMax attention method, and analyze the trade-off between accuracy gains and computational cost.