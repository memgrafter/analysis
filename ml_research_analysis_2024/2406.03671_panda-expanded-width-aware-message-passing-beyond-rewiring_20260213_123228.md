---
ver: rpa2
title: 'PANDA: Expanded Width-Aware Message Passing Beyond Rewiring'
arxiv_id: '2406.03671'
source_url: https://arxiv.org/abs/2406.03671
tags:
- graph
- panda
- nodes
- rewiring
- message
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PANDA, an expanded width-aware message passing\
  \ framework that addresses the over-squenching problem in Graph Neural Networks\
  \ (GNNs) without altering graph topology. The method selectively expands the hidden\
  \ dimension of high-centrality nodes\u2014potential bottlenecks\u2014to improve\
  \ their capacity for handling long-range signals."
---

# PANDA: Expanded Width-Aware Message Passing Beyond Rewiring
## Quick Facts
- arXiv ID: 2406.03671
- Source URL: https://arxiv.org/abs/2406.03671
- Reference count: 40
- Primary result: PANDA achieves 80.69% accuracy on REDDIT-BINARY (vs 68.25% for GCN+None), reducing over-squashing while maintaining original graph topology.

## Executive Summary
This paper introduces PANDA, a width-aware message passing framework that addresses over-squashing in Graph Neural Networks without altering graph topology. Unlike rewiring methods that modify graph structure, PANDA selectively expands the hidden dimension of high-centrality nodes—identified as bottlenecks for long-range information flow—while maintaining the original graph structure. The method enables message passing between nodes of different widths through specialized aggregation functions, achieving state-of-the-art performance across graph classification benchmarks while demonstrating robustness to increased effective resistance.

## Method Summary
PANDA is a width-aware message passing framework that identifies high-centrality nodes as bottlenecks for long-range information flow and selectively expands their hidden dimensions. The method stratifies nodes into low and high width groups based on centrality metrics (degree, betweenness, closeness, PageRank, load), then routes messages through four specialized aggregators depending on the width combination of source and target nodes. Low-to-high messages use projection, while high-to-low messages use selective gathering based on learned scores. The framework maintains the original graph topology while enabling more expressive message passing, reducing over-squashing without the complexity of rewiring approaches.

## Key Results
- PANDA achieves 80.69% accuracy on REDDIT-BINARY (vs 68.25% for GCN+None), outperforming rewiring methods
- Dirichlet energy increases with PANDA, indicating reduced over-smoothing compared to standard GNNs
- PANDA shows consistent signal propagation even as effective resistance increases, unlike rewiring methods that degrade
- Runtime increases are moderate (up to 35.85% on IMDB-BINARY) compared to the performance gains achieved

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selectively expanding the hidden dimension of high-centrality nodes improves feature sensitivity by increasing the representational capacity for incoming long-range signals.
- Mechanism: High-centrality nodes (e.g., high betweenness) are bottlenecks because they aggregate information from many distant nodes into fixed-size vectors. Expanding their hidden dimension allows more information to be retained without increasing the fixed output dimension, thus mitigating over-squashing.
- Core assumption: Bottlenecks in information flow are primarily caused by nodes with high centrality metrics (e.g., betweenness centrality).
- Evidence anchors:
  - [abstract]: "nodes with high centrality, a potential source of over-squashing, are selectively expanded in width to encapsulate the growing influx of signals from distant nodes."
  - [section 3.1]: "We consider node centrality metrics to identify a node as a potential source of the bottleneck in a graph. Betweenness centrality has been considered as a significant indicator of the bottleneck, which measures the number of shortest paths going through a node."
  - [corpus]: Weak. No corpus papers directly validate the centrality-bottleneck link for over-squashing.
- Break condition: If centrality does not correlate with over-squashing in the target graph domain, or if expanding width globally harms generalization as warned in Di Giovanni et al. (2023).

### Mechanism 2
- Claim: Message passing between nodes of different widths is enabled through dimension-aware aggregation functions, preserving original graph topology.
- Mechanism: Four distinct message types (low-to-low, high-to-high, low-to-high, high-to-low) use specialized aggregators ψ and transformation functions f, g to handle width mismatches. Low-to-high uses projection; high-to-low uses selective gathering.
- Core assumption: Width mismatches can be handled without topological rewiring if the aggregation functions are properly defined.
- Evidence anchors:
  - [section 3.2]: "Each aggregator ψ(ℓ) is tailored to handle a specific type of interactions among nodes."
  - [section 3.2]: "For ψ(ℓ)↣ (·), we define g(·) as a dimension selector... This score vector is then used to select the top p dimensions for the message aggregation."
  - [corpus]: Weak. No corpus papers explicitly describe message passing across width mismatches.
- Break condition: If aggregation functions cannot preserve expressivity, or if the width difference is too large to allow meaningful information exchange.

### Mechanism 3
- Claim: PANDA maintains consistent signal propagation even as effective resistance increases, unlike rewiring methods.
- Mechanism: By expanding widths at bottlenecks, PANDA reduces the compression of long-range signals into small vectors, maintaining sensitivity (Jacobian norm) and effective resistance correlation.
- Core assumption: Signal propagation is inversely proportional to total effective resistance, and width expansion can offset the negative effect of high resistance.
- Evidence anchors:
  - [abstract]: "PANDA shows consistent signal propagation even under large effective resistance without degradation in signal propagation."
  - [section 4]: "In Fig. 6, all methods except PANDA present decaying signal propagation trends as the resistance of graphs increases."
  - [corpus]: Weak. No corpus papers directly measure signal propagation w.r.t. effective resistance for width-aware methods.
- Break condition: If the relationship between resistance and signal propagation does not hold in the target domain, or if width expansion fails to preserve sensitivity.

## Foundational Learning

- Concept: Graph Neural Networks and Message Passing
  - Why needed here: PANDA builds on MPNNs; understanding the basic update equations and aggregation is essential to grasp the width-aware extensions.
  - Quick check question: In standard MPNNs, do all nodes share the same hidden dimension throughout the network?

- Concept: Over-squashing and Bottlenecks
  - Why needed here: PANDA is designed to mitigate over-squashing; knowing how bottlenecks cause exponential information compression is key to understanding the motivation.
  - Quick check question: What metric is commonly used to quantify over-squashing via sensitivity analysis?

- Concept: Graph Centrality Measures
  - Why needed here: PANDA selects nodes to expand based on centrality (degree, betweenness, closeness, PageRank, load); understanding these metrics is necessary to implement the node selection logic.
  - Quick check question: Which centrality measure counts the number of shortest paths passing through a node?

## Architecture Onboarding

- Component map: Input graph G=(V,E) -> Centrality computation -> Binary mask b -> Node stratification -> Four aggregators ψ(ℓ)↔, ψ(ℓ)⇔, ψ(ℓ)↠, ψ(ℓ)↣ -> Transformation functions f, g -> Standard update function ϕ -> Output representations

- Critical path:
  1. Compute centrality c ∈ Rn
  2. Sort and select top-k → mask b
  3. For each edge, route through correct aggregator based on bv, bu
  4. Apply f/g for cross-width messages
  5. Aggregate and update all nodes
  6. Repeat for ℓ layers

- Design tradeoffs:
  - Expanding more nodes → higher capacity but more parameters and runtime
  - Choosing centrality → different bottlenecks targeted; may affect generalization
  - Width difference (phigh - p) → larger gap increases expressivity but may destabilize training

- Failure signatures:
  - Accuracy plateaus or degrades as k increases (too many nodes expanded)
  - Training instability when phigh >> p
  - Sensitivity decreases despite expansion (wrong centrality chosen)
  - Runtime explosion on large graphs due to centrality computation

- First 3 experiments:
  1. Run PANDA-GCN on MUTAG with k=1,3,5,7; plot mean accuracy vs k.
  2. Compare Dirichlet energy of PANDA vs rewiring baselines on PROTEINS.
  3. Measure signal propagation vs normalized Rtot for GCN+None, GCN+FoSR, GCN+PANDA on REDDIT-BINARY.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of centrality metric affect PANDA's performance across different graph types (e.g., social networks vs. molecular graphs)?
- Basis in paper: [explicit] Table 3 shows performance comparisons using different centrality metrics on graph classification datasets.
- Why unresolved: The paper only evaluates centrality metrics on specific graph classification datasets, but doesn't systematically analyze how these metrics perform across different graph types or structural properties.
- What evidence would resolve it: Systematic experiments comparing centrality metric effectiveness across diverse graph types with varying properties (heterophily, community structure, scale-free characteristics) would clarify optimal metric selection.

### Open Question 2
- Question: What is the theoretical relationship between expanded width distribution and over-squashing mitigation beyond the sensitivity bound analysis?
- Basis in paper: [inferred] The paper provides sensitivity bound analysis but acknowledges it doesn't fully prove the effects of width expansion on over-squashing.
- Why unresolved: The paper relies on empirical evidence and the sensitivity bound theorem, but lacks a comprehensive theoretical framework explaining how selective width expansion directly addresses over-squashing mechanisms.
- What evidence would resolve it: A rigorous theoretical analysis connecting width expansion strategies to specific over-squashing mechanisms, possibly through information theory or dynamical systems approaches.

### Open Question 3
- Question: How does PANDA's computational complexity scale with graph size and width expansion, and what optimizations could make it more efficient?
- Basis in paper: [explicit] Section 3.3 mentions computational complexity depends on centrality calculation algorithms and runtime analysis is in Appendix H.
- Why unresolved: The paper provides runtime comparisons but doesn't offer a detailed complexity analysis or explore optimization strategies for large-scale applications.
- What evidence would resolve it: Detailed complexity analysis with asymptotic bounds, and benchmarking against optimized implementations or approximate centrality calculations for large graphs.

## Limitations
- Betweenness centrality computation scales poorly for very large graphs, limiting practical applicability
- The optimal choice of centrality metric varies by graph type, requiring domain knowledge or hyperparameter tuning
- Performance gains may diminish as width expansion increases, suggesting a sweet spot that must be identified empirically

## Confidence
- Mechanism 1 (centrality-based bottleneck identification): **Medium** - centrality metrics are well-established but their direct correlation with over-squashing is not universally validated
- Mechanism 2 (width-aware message passing): **High** - aggregation functions are mathematically sound but untested in extreme width differences
- Mechanism 3 (resistance-invariant propagation): **Medium** - empirical results show promise but theoretical guarantees are limited

## Next Checks
1. Measure training/inference time scaling with graph size to verify practical limits of betweenness centrality computation
2. Test PANDA with phigh/p ratios of 4:1, 8:1, and 16:1 to identify stability thresholds
3. Evaluate on non-TUDataset benchmarks (e.g., OGB) to assess domain generalization