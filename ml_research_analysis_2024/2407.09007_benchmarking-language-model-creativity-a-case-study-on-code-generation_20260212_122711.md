---
ver: rpa2
title: 'Benchmarking Language Model Creativity: A Case Study on Code Generation'
arxiv_id: '2407.09007'
source_url: https://arxiv.org/abs/2407.09007
tags:
- creativity
- human
- divergent
- convergent
- solutions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DENIAL PROMPTING, a method that iteratively
  imposes constraints on LLM-generated solutions to elicit creative responses, and
  NEOGAUGE, a metric quantifying both convergent (correctness and constraint adherence)
  and divergent (novelty vs human solutions) creativity. Evaluated on 199 Codeforces
  programming problems with 5.9K human solutions, NEOCODER dataset reveals that GPT-4
  achieves the highest creativity scores, though still below human levels.
---

# Benchmarking Language Model Creativity: A Case Study on Code Generation

## Quick Facts
- arXiv ID: 2407.09007
- Source URL: https://arxiv.org/abs/2407.09007
- Reference count: 40
- Primary result: Introduces DENIAL PROMPTING and NEOGAUGE to evaluate LLM creativity on programming problems, finding GPT-4 achieves highest scores but still below human levels

## Executive Summary
This paper addresses the challenge of measuring and eliciting creativity in large language models through a novel framework combining iterative constraint application and comprehensive evaluation metrics. The authors introduce DENIAL PROMPTING, which incrementally denies previously used techniques to push models toward more creative solutions, and NEOGAUGE, a metric quantifying both convergent (correctness and constraint adherence) and divergent (novelty vs human solutions) creativity. Tested on 199 Codeforces programming problems with 5.9K human solutions, the framework reveals that while GPT-4 achieves the highest creativity scores among evaluated models, it still falls short of human-level creativity. The work provides both a practical methodology for automated creativity evaluation and insights into the limitations of current approaches in fostering truly creative LLM outputs.

## Method Summary
The authors develop a two-part framework for benchmarking LLM creativity: DENIAL PROMPTING generates increasingly constrained problem instances by iteratively denying techniques used in previous solutions, while NEOGAUGE evaluates creativity across both convergent (correctness + constraint adherence) and divergent (novelty vs human solutions) dimensions. The method is applied to programming problems from Codeforces, with human solutions collected to serve as ground truth for novelty evaluation. The framework also tests various reasoning strategies (MCTS, self-correction, planning, sampling) to identify approaches that enhance creativity, revealing that only MCTS improves divergent creativity by exploring multiple solution paths.

## Key Results
- GPT-4 achieves the highest NEOGAUGE scores among evaluated models but remains below human creativity levels
- MCTS is the only reasoning strategy that improves divergent creativity by exploring multiple solution paths
- NEOGAUGE reveals that creativity evaluation should be state-aware, with creativity varying under different constraint contexts
- No tested reasoning strategy simultaneously enhances both convergent and divergent creativity dimensions

## Why This Works (Mechanism)

### Mechanism 1
DENIAL PROMPTING elicits more creative solutions by iteratively imposing constraints on previously used techniques. At each iteration, the model detects techniques used in the previous solution, samples one not previously used as a constraint, and updates the problem statement to prohibit that technique. This forces LLMs to seek novel solutions when conventional techniques are denied. The algorithm terminates when no new techniques can be detected (Tt \ Ct-1 = ∅).

### Mechanism 2
NEOGAUGE provides comprehensive creativity evaluation by combining convergent and divergent thinking metrics. It multiplies convergent creativity (correctness + constraint adherence) with divergent creativity (novelty vs human solutions), creating a joint probability of generating correct, constraint-following, and novel solutions. True creativity requires both generating novel solutions AND ensuring they solve the problem correctly. When either convergent or divergent creativity reaches zero, NEOGAUGE becomes zero regardless of the other dimension.

### Mechanism 3
State-aware evaluation captures the dynamic nature of creativity by measuring performance at different constraint levels. NEOGAUGE is computed at each state t (number of constraints), allowing comparison of creativity evolution as problems become more constrained. Creativity manifests differently under varying levels of constraint difficulty. When state progression stops (no more constraints can be applied), the state-aware evaluation becomes static.

## Foundational Learning

- **Difference between convergent and divergent thinking**: NEOGAUGE explicitly separates these two creativity dimensions for evaluation. Quick check: Can you explain why a solution might be convergent but not divergent, or vice versa?

- **Monte Carlo Tree Search (MCTS) basics**: MCTS is one of the reasoning strategies evaluated for creativity enhancement. Quick check: How does MCTS exploration differ from simple sampling approaches?

- **Prompt engineering techniques**: DENIAL PROMPTING relies on carefully crafted prompts for technique detection and constraint application. Quick check: What are the key differences between the problem-solving prompt and the technique detection prompt?

## Architecture Onboarding

- **Component map**: Codeforces problem retrieval → human solution extraction → DENIAL PROMPTING augmentation → NEOCODER dataset creation → model inference → NEOGAUGE computation → results aggregation
- **Critical path**: Problem retrieval → DENIAL PROMPTING iterations → human solution collection → NEOGAUGE computation
- **Design tradeoffs**: 
  - Technique-level comparison vs sentence-level similarity for human-grounded evaluation
  - Single conversation thread vs separate contexts for response generation vs technique detection
  - Maximum state limit (T=5) vs letting DENIAL PROMPTING run until constraint exhaustion
- **Failure signatures**:
  - NEOGAUGE = 0 when either convergent or divergent creativity is zero
  - Rapid NEOGAUGE decay across states indicates models struggle with constraint satisfaction
  - Techniques detection failures lead to premature DENIAL PROMPTING termination
- **First 3 experiments**:
  1. Run DENIAL PROMPTING with a simple baseline model (e.g., GPT-3.5) on 5-10 problems to verify technique detection and constraint application
  2. Compute NEOGAUGE for a single state (t=0) across all models to establish baseline creativity scores
  3. Test reasoning strategy (e.g., sampling with k=3) on a small subset to verify implementation and measure impact on NEOGAUGE

## Open Questions the Paper Calls Out

None

## Limitations

- **Technique detection reliability**: The DENIAL PROMPTING method relies heavily on accurate technique detection from LLM-generated solutions, but precision/recall metrics for technique detection aren't reported, creating uncertainty about whether constraints are actually novel.

- **Human solution coverage bias**: The evaluation compares LLM solutions against 5.9K human solutions, but this represents only a subset of possible solutions to 199 problems, potentially biasing novelty calculations.

- **Reasoning strategy implementation details**: While MCTS shows promise for divergent creativity, the paper lacks implementation specifics, making it unclear whether improvements come from MCTS specifically or general multi-path exploration.

## Confidence

- **High Confidence**: NEOGAUGE provides a more comprehensive creativity evaluation than previous metrics by explicitly measuring both convergent and divergent dimensions.
- **Medium Confidence**: DENIAL PROMPTING successfully elicits more creative solutions by forcing LLMs to abandon previously used techniques, though actual effectiveness depends heavily on reliable technique detection.
- **Low Confidence**: MCTS is the only reasoning strategy that improves divergent creativity among those tested, requiring more experimental evidence for validation.

## Next Checks

1. **Technique detection validation**: Run the technique detection prompt on a held-out set of human solutions with ground-truth technique annotations to measure precision and recall, establishing whether the constraint generation process produces novel constraints.

2. **State progression analysis**: Track how NEOGAUGE scores evolve across DENIAL PROMPTING states (t=0 to t=5) for each model. Plot NEOGAUGE vs state number to visualize whether scores decay rapidly, suggesting models struggle with constraint satisfaction rather than genuinely exploring creative solution spaces.

3. **MCTS ablation study**: Compare MCTS against a simple random sampling baseline that also explores multiple solution paths to determine whether MCTS's tree-based exploration specifically contributes to divergent creativity improvements.