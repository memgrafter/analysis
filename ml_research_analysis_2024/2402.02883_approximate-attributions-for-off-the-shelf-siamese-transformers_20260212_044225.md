---
ver: rpa2
title: Approximate Attributions for Off-the-Shelf Siamese Transformers
arxiv_id: '2402.02883'
source_url: https://arxiv.org/abs/2402.02883
tags:
- attributions
- exact
- attribution
- similarity
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of explaining the decision-making
  process of Siamese sentence transformers, which compare two inputs rather than processing
  a single one, making established attribution methods inapplicable. The authors propose
  two key extensions to their previous work: (1) a method to compute exact attributions
  using cosine similarity, retaining the original model''s predictive performance,
  and (2) a method to compute approximate attributions for off-the-shelf models without
  requiring adjustments.'
---

# Approximate Attributions for Off-the-Shelf Siamese Transformers

## Quick Facts
- arXiv ID: 2402.02883
- Source URL: https://arxiv.org/abs/2402.02883
- Authors: Lucas Möller; Dmitry Nikolaev; Sebastian Padó
- Reference count: 21
- Primary result: Methods for computing exact and approximate attributions for Siamese sentence transformers, with analysis of linguistic attention patterns

## Executive Summary
This paper addresses the challenge of explaining decision-making in Siamese sentence transformers, which compare two inputs rather than processing a single one. The authors propose two key extensions: a method for computing exact attributions using cosine similarity that maintains predictive performance, and a method for computing approximate attributions for off-the-shelf models without requiring adjustments. Through extensive comparison of approximate and exact attributions, the study analyzes how these models attend to different linguistic aspects including syntactic roles, negation, and adjectives.

The research reveals that approximate attributions are reliable for very deep intermediate representations and often agree with exact attributions. Key findings include insights into which syntactic roles Siamese transformers attend to, confirmation that they mostly ignore negation, exploration of how they judge semantically opposite adjectives, and identification of lexical bias in the models.

## Method Summary
The authors extend their previous work on attributions for Siamese transformers by developing methods for both exact and approximate attribution computation. The exact attribution method computes precise attributions using cosine similarity while preserving the original model's predictive performance. The approximate attribution method enables analysis of off-the-shelf models without requiring modifications to the underlying architecture. The approach involves analyzing intermediate representations at various depths and comparing attribution patterns between exact and approximate methods to validate reliability.

## Key Results
- Approximate attributions show high reliability for very deep intermediate representations and frequently align with exact attributions
- Siamese transformers exhibit selective attention to specific syntactic roles while largely ignoring negation
- Models demonstrate lexical bias in their judgments and process semantically opposite adjectives in predictable patterns
- The proposed attribution methods maintain model performance while enabling interpretability

## Why This Works (Mechanism)
The mechanism relies on the mathematical properties of cosine similarity in high-dimensional spaces and the structural characteristics of Siamese architectures. By analyzing intermediate representations, the methods capture how information flows through the model during comparison tasks. The exact method preserves the geometric properties of the original similarity computation, while the approximate method leverages the observation that deeper layers produce more stable and interpretable attribution patterns.

## Foundational Learning
- Cosine similarity computation: Essential for understanding how Siamese models measure input similarity; quick check: verify that similarity scores match expected geometric relationships
- Attribution methods for transformers: Needed to interpret model decisions; quick check: ensure attributions sum to predicted similarity scores
- Intermediate layer analysis: Critical for understanding model behavior at different depths; quick check: confirm that deeper layers produce more stable attributions
- Syntactic role tagging: Required for analyzing linguistic attention patterns; quick check: validate tagger accuracy on test data
- Negation processing: Important for understanding semantic handling; quick check: test model responses to negated vs. non-negated pairs
- Lexical bias identification: Necessary for evaluating fairness and robustness; quick check: examine attribution patterns across lexical variations

## Architecture Onboarding

**Component Map:**
Siamese Encoder -> Cosine Similarity Layer -> Prediction Output

**Critical Path:**
Input Pair → Individual Encoding → Intermediate Representation → Similarity Computation → Output Prediction

**Design Tradeoffs:**
- Exact attributions preserve performance but require model access
- Approximate attributions work on off-the-shelf models but may sacrifice precision in shallow layers
- Deeper layer analysis provides more reliable attributions but may miss early processing decisions

**Failure Signatures:**
- High disagreement between exact and approximate attributions in shallow layers
- Attribution patterns that don't correlate with linguistic expectations
- Performance degradation when applying exact attribution modifications

**First Experiments:**
1. Compute exact and approximate attributions for a simple sentence pair and compare correlation
2. Test model performance before and after applying exact attribution modifications
3. Analyze attribution patterns for sentence pairs with known syntactic differences

## Open Questions the Paper Calls Out
None

## Limitations
- Methods are restricted to cosine-similarity-based Siamese transformers
- Reliability of approximate attributions is lower for shallow layers
- Findings may not generalize across different transformer architectures and training regimes
- Analysis focuses on specific datasets that may contain inherent biases

## Confidence

**High confidence:**
- Mathematical framework for exact attributions
- General methodology for approximate attributions

**Medium confidence:**
- Reliability of approximate attributions for deep layers
- Analysis of syntactic role attention

**Low confidence:**
- Robustness of findings across different transformer variants
- Practical implications of attribution differences

## Next Checks
1. Conduct systematic error analysis comparing exact and approximate attributions across different layer depths, providing quantitative bounds on approximation error and identifying specific failure modes.

2. Test the attribution methods and analysis framework on additional transformer architectures (e.g., BERT, RoBERTa, T5) and non-English language pairs to assess generalizability.

3. Perform ablation studies removing specific training data subsets to isolate whether observed lexical biases stem from particular corpora or are inherent to the Siamese transformer architecture.