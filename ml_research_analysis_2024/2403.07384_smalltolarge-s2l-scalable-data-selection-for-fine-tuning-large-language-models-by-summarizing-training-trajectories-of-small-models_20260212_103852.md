---
ver: rpa2
title: 'SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language
  Models by Summarizing Training Trajectories of Small Models'
arxiv_id: '2403.07384'
source_url: https://arxiv.org/abs/2403.07384
tags:
- data
- training
- selection
- examples
- full
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SmallToLarge (S2L), a scalable data selection
  method for fine-tuning large language models in specialized domains. S2L uses loss
  trajectories from small models to cluster training examples, then samples uniformly
  from these clusters to guide data selection for larger models.
---

# SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models

## Quick Facts
- arXiv ID: 2403.07384
- Source URL: https://arxiv.org/abs/2403.07384
- Authors: Yu Yang; Siddhartha Mishra; Jeffrey N Chiang; Baharan Mirzasoleiman
- Reference count: 40
- Reduces required training data to 11% of MathInstruct dataset while matching full dataset performance

## Executive Summary
SmallToLarge (S2L) introduces a novel data selection method for fine-tuning large language models in specialized domains by leveraging loss trajectories from small models. The approach clusters training examples based on similarity in their loss trajectories and samples uniformly from these clusters to guide data selection for larger models. Theoretical analysis demonstrates that examples within the same cluster exhibit similar gradients, guaranteeing convergence to a neighborhood of the optimal solution. Experiments show S2L achieves state-of-the-art performance with significantly reduced training data requirements.

## Method Summary
S2L operates by first collecting loss trajectories of training examples using a small reference model, then applying clustering algorithms to group examples based on trajectory similarity. The method samples uniformly from these clusters to select a subset of data for training larger models. This approach enables effective data selection using a model 100x smaller than the target model, making it computationally efficient while maintaining strong performance. The theoretical foundation relies on the assumption that examples with similar loss trajectories will exhibit similar gradients during training, ensuring the selected subset maintains convergence properties.

## Key Results
- Reduces required training data to 11% of MathInstruct dataset while matching full dataset performance
- Achieves 32.7% accuracy on MATH benchmark using only 50K examples, improving Phi-2 by 16.6%
- Outperforms full dataset training in clinical text summarization using only 50% of the data
- Outperforms state-of-the-art methods by an average of 4.7% across benchmarks

## Why This Works (Mechanism)
S2L works by capturing the learning dynamics of small models through loss trajectories, which encode how each example contributes to the model's training progress. By clustering examples with similar trajectories, the method identifies groups of data points that the small model learns at similar rates or with similar difficulty. This clustering implicitly captures the underlying data distribution and difficulty patterns, allowing the selection of a representative subset that preserves the essential learning characteristics needed for effective fine-tuning of larger models.

## Foundational Learning
- **Loss trajectory analysis**: Understanding how loss values evolve during training for each example - needed to capture learning dynamics and difficulty patterns; quick check: verify trajectories are smooth and informative
- **Clustering algorithms**: Grouping similar examples based on trajectory similarity - needed to identify representative subsets; quick check: assess cluster homogeneity and separation
- **Gradient similarity**: Theoretical assumption that examples in same cluster have similar gradients - needed for convergence guarantees; quick check: measure gradient cosine similarity within clusters
- **Data selection theory**: Principles of subset selection that preserves learning efficiency - needed to justify uniform sampling from clusters; quick check: compare selected subset performance against random selection
- **Computational efficiency**: Using small models for selection to reduce costs - needed for practical scalability; quick check: measure computational overhead of trajectory collection

## Architecture Onboarding

**Component Map**: Small reference model -> Loss trajectory collection -> Clustering algorithm -> Data selection -> Large target model training

**Critical Path**: The method's success depends on the quality of loss trajectories collected by the small model, the effectiveness of the clustering algorithm in grouping similar examples, and the representativeness of the uniformly sampled subset for training the larger model.

**Design Tradeoffs**: The approach trades potential selection precision for computational efficiency by using a much smaller model for selection. The uniform sampling from clusters ensures diversity but may miss some rare but important examples. The method assumes the small model's learning patterns generalize to larger models.

**Failure Signatures**: Poor performance may result from uninformative loss trajectories (small model too different from target), ineffective clustering (clusters not homogeneous), or inadequate representation (selected subset missing important data patterns). Diagnostics include checking cluster quality, comparing selected vs random subsets, and validating gradient similarity assumptions.

**3 First Experiments**:
1. Collect loss trajectories using a small reference model on a subset of training data and visualize trajectory patterns
2. Apply clustering to the collected trajectories and analyze cluster quality and example distribution
3. Train the large model on uniformly sampled subsets from different cluster numbers and compare performance

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Theoretical assumptions about gradient similarity within clusters may not hold for all datasets or model architectures
- Effectiveness depends on the reference model being sufficiently similar to the target model in learning patterns
- Evaluation limited to specific benchmarks (mathematical reasoning and clinical summarization) without broader domain testing
- Computational costs of collecting loss trajectories and performing clustering are not fully detailed

## Confidence
High: Claims about improved data efficiency and performance on mathematical benchmarks are well-supported by experimental results
Medium: Claims about clinical text summarization performance are supported but with less detailed evaluation
Low: Theoretical convergence guarantees rely on idealized assumptions that may not fully translate to practice

## Next Checks
1. Test S2L's performance on diverse language tasks beyond mathematical reasoning and clinical summarization, including general language understanding, code generation, or creative writing, to evaluate its generalizability across different domains.

2. Conduct ablation studies to determine the sensitivity of S2L to key hyperparameters such as the number of clusters, the size of the reference model, and the clustering algorithm parameters, and assess how these affect performance across different datasets.

3. Compare S2L against alternative data selection methods (e.g., active learning, curriculum learning, or diversity-based sampling) on the same benchmarks to establish its relative effectiveness and identify scenarios where it performs best or worst.