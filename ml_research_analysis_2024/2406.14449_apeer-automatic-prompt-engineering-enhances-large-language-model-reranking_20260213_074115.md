---
ver: rpa2
title: 'APEER: Automatic Prompt Engineering Enhances Large Language Model Reranking'
arxiv_id: '2406.14449'
source_url: https://arxiv.org/abs/2406.14449
tags:
- prompt
- apeer
- prompts
- language
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "APEER addresses the challenge of automatic prompt engineering\
  \ for large language model reranking in information retrieval, where existing methods\
  \ struggle due to the complexity of integrating queries with long passages. The\
  \ method introduces a novel iterative approach that combines feedback optimization\u2014\
  generating refined prompts based on LLM feedback about current prompts\u2014with\
  \ preference optimization\u2014aligning prompts with high-performing examples while\
  \ avoiding poor ones."
---

# APEER: Automatic Prompt Engineering Enhances Large Language Model Reranking

## Quick Facts
- arXiv ID: 2406.14449
- Source URL: https://arxiv.org/abs/2406.14449
- Authors: Can Jin; Hongwu Peng; Shiyu Zhao; Zhenting Wang; Wujiang Xu; Ligong Han; Jiahui Zhao; Kai Zhong; Sanguthevar Rajasekaran; Dimitris N. Metaxas
- Reference count: 40
- Key outcome: APEER achieves 5.29 average nDCG@10 improvement on BEIR datasets compared to manual prompts on LLaMA3

## Executive Summary
APEER addresses the challenge of automatic prompt engineering for large language model reranking in information retrieval, where existing methods struggle due to the complexity of integrating queries with long passages. The method introduces a novel iterative approach that combines feedback optimization—generating refined prompts based on LLM feedback about current prompts—with preference optimization—aligning prompts with high-performing examples while avoiding poor ones. Extensive experiments across four LLMs (GPT4, GPT3.5, LLaMA3, Qwen2) and ten datasets show APEER consistently outperforms state-of-the-art manual prompts, achieving average nDCG@10 improvements of 5.29 on BEIR datasets compared to manual prompts on LLaMA3. The generated prompts also demonstrate strong transferability across diverse tasks and model architectures.

## Method Summary
APEER employs an iterative approach combining feedback optimization and preference optimization to automatically engineer prompts for LLM reranking. The method starts with an initial prompt and iteratively refines it through two mechanisms: feedback optimization where the LLM evaluates and provides specific feedback about current prompt performance, and preference optimization where the LLM aligns new prompts with high-performing examples while avoiding poor ones. The training uses 100 sampled queries from MS MARCO v1, with evaluation on TREC-DL19, TREC-DL20, and BEIR datasets using nDCG@1, nDCG@5, and nDCG@10 metrics. The approach trains for 3 epochs with batch size 1 in feedback optimization, using top 1 positive and bottom 1 negative prompt in preference optimization.

## Key Results
- APEER achieves 5.29 average nDCG@10 improvement on BEIR datasets compared to manual prompts on LLaMA3
- Outperforms existing manual prompts across four LLMs (GPT4, GPT3.5, LLaMA3, Qwen2) and ten datasets
- Generated prompts demonstrate strong transferability across diverse tasks and model architectures
- Performance scales with training dataset size, with 100 queries showing good balance between performance and training costs

## Why This Works (Mechanism)

### Mechanism 1: Feedback Optimization
- Claim: APEER's feedback optimization generates more effective prompts by iteratively refining based on LLM feedback about current prompts.
- Mechanism: The LLM evaluates the current prompt's performance and generates specific feedback about issues (e.g., "lack of specificity", "ambiguity in format"), then creates a refined prompt addressing these issues.
- Core assumption: LLM-generated feedback about prompt quality is meaningful and actionable for prompt improvement.
- Evidence anchors: [abstract] "APEER iteratively generates refined prompts through feedback and preference optimization"; [section] "Feedback Optimization: ... we infer it on a batch of data ... obtain the responses ... To attain the 'gradient' (i.e., feedback) on B, we utilize the LLM f to generate high-quality feedback on the current prompt based on the queries, passages, responses, and the relevance mapping"

### Mechanism 2: Preference Optimization
- Claim: APEER's preference optimization aligns prompts with high-performing examples while avoiding poor ones through demonstration-based learning.
- Mechanism: APEER maintains sets of positive and negative prompts, then uses the LLM to generate new prompts that preferentially align with positive examples and avoid negative ones.
- Core assumption: LLM can effectively learn prompt preferences from demonstration pairs and generalize this preference to new prompts.
- Evidence anchors: [abstract] "APEER iteratively generates refined prompts through feedback and preference optimization"; [section] "Preference Optimization: Direct Preference Optimization ... we have cataloged a collection of potential positive and negative responses within Hpos and Hneg ... Our objective is to refine the prompt p' such that it is biased towards the optimal prompt contained in Hpos"

### Mechanism 3: Iterative Combined Optimization
- Claim: APEER's iterative approach enables continuous improvement by combining local batch optimization with global preference alignment.
- Mechanism: Feedback optimization acts as local optimizer for each batch, while preference optimization extends this by globally aligning local optimized prompts towards superior global prompts via preference learning.
- Core assumption: Combining local optimization (batch-level) with global optimization (dataset-level) yields better results than either approach alone.
- Evidence anchors: [abstract] "APEER iteratively generates refined prompts through feedback and preference optimization"; [section] "Feedback Optimization acts as a local optimizer for the current batch B, whereas the Preference Optimization mechanism extends this local optimization by globally aligning the local optimized prompts towards superior global prompts"

## Foundational Learning

- Concept: Information Retrieval fundamentals (retriever/reranker pipeline)
  - Why needed here: APEER operates specifically on the reranking stage of IR systems, so understanding how retrieval works is essential
  - Quick check question: What are the two main stages in typical IR systems and what does each do?

- Concept: Prompt engineering and LLM interaction
  - Why needed here: APEER is fundamentally about optimizing prompts for LLMs in IR tasks
  - Quick check question: How do LLMs interpret and execute instructions provided in prompts?

- Concept: Reinforcement learning and preference optimization concepts
  - Why needed here: APEER uses preference optimization techniques similar to RLHF
  - Quick check question: What's the difference between reward-based and preference-based optimization in machine learning?

## Architecture Onboarding

- Component map: Training dataset builder -> LLM inference engine -> Feedback optimization module -> Preference optimization module -> Evaluation pipeline -> Best prompt selection

- Critical path: Training dataset → Feedback optimization (iterate) → Preference optimization → Evaluation → Best prompt selection

- Design tradeoffs:
  - Cost vs. performance: Using larger LLMs gives better feedback but increases computational cost
  - Batch size vs. quality: Smaller batches give more targeted feedback but may miss broader patterns
  - Training dataset size vs. generalization: More training data improves performance but increases cost

- Failure signatures:
  - Prompts stop improving after initial iterations
  - Performance degrades on out-of-domain datasets
  - Feedback generation becomes inconsistent or unhelpful
  - Preference optimization converges to degenerate solutions

- First 3 experiments:
  1. Test APEER on a single dataset with minimal training data to verify basic functionality
  2. Compare performance with and without preference optimization to validate its contribution
  3. Test transferability by training on one model and evaluating on another to measure cross-model effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of APEER scale with larger training dataset sizes beyond 100 queries?
- Basis in paper: [explicit] The paper mentions that increasing training dataset size improves performance and uses 100 queries as a default setting to balance performance and training costs.
- Why unresolved: The paper only provides results for up to 100 queries and suggests that performance may continue to improve with larger datasets, but does not empirically verify this.
- What evidence would resolve it: Systematic experiments training APEER with increasing dataset sizes (e.g., 100, 200, 500, 1000 queries) and measuring performance improvements on benchmark datasets.

### Open Question 2
- Question: How does APEER perform when applied to retrieval tasks beyond passage reranking, such as document retrieval or cross-lingual retrieval?
- Basis in paper: [inferred] The paper focuses exclusively on passage reranking and demonstrates transferability across different datasets and model architectures, but does not explore other retrieval tasks.
- Why unresolved: The paper's evaluation is limited to passage reranking benchmarks, leaving the effectiveness of APEER in other retrieval contexts unexplored.
- What evidence would resolve it: Experiments applying APEER to document retrieval, cross-lingual retrieval, or other information retrieval tasks and comparing performance against existing methods.

### Open Question 3
- Question: What is the impact of different feedback generation meta-prompts (c_fb) and prompt refinement meta-prompts (c_g) on APEER's performance?
- Basis in paper: [explicit] The paper mentions using meta-prompts for feedback generation and prompt refinement but does not provide ablation studies on different meta-prompt designs.
- Why unresolved: The paper treats meta-prompt design as fixed without exploring how different formulations affect the quality of feedback and refined prompts.
- What evidence would resolve it: Systematic ablation studies comparing different meta-prompt formulations for feedback generation and prompt refinement, measuring their impact on final reranking performance.

### Open Question 4
- Question: How does APEER's preference optimization compare to other preference learning methods like reinforcement learning from human feedback (RLHF) in the context of prompt engineering?
- Basis in paper: [explicit] The paper introduces a preference optimization method based on positive and negative prompt demonstrations and mentions RLHF as a related technique.
- Why unresolved: The paper does not directly compare APEER's preference optimization to alternative preference learning approaches like RLHF.
- What evidence would resolve it: Direct comparison experiments between APEER's preference optimization and RLHF-based prompt optimization, measuring both performance and computational efficiency.

## Limitations
- Exact meta-prompts for feedback generation, prompt refinement, and preference optimization are unspecified, making faithful reproduction difficult
- Claims about transferability across diverse tasks and model architectures are based on limited testing with passage reranking only
- The approach relies heavily on the quality of LLM-generated feedback, which may become inconsistent or unhelpful in edge cases

## Confidence
**High Confidence**: The core contribution of combining feedback optimization with preference optimization is well-specified and theoretically sound. The experimental methodology and evaluation metrics are clearly defined, and the results showing consistent improvements across multiple models and datasets are reproducible.

**Medium Confidence**: The effectiveness of APEER's iterative approach relies heavily on the quality of LLM-generated feedback, which is assumed to be meaningful and actionable. While the paper demonstrates positive results, the mechanism by which LLMs generate useful feedback about prompt quality is not extensively validated.

**Low Confidence**: Claims about transferability across diverse tasks and model architectures are based on limited testing. The paper shows some cross-model effectiveness, but the extent to which prompts generalize to completely different domains or task types remains uncertain.

## Next Checks
1. **Feedback Quality Validation**: Conduct ablation studies to quantify the impact of feedback quality on final prompt performance. Test APEER with artificially degraded feedback (e.g., random or contradictory feedback) to establish the sensitivity of the approach to feedback quality.

2. **Training Data Scaling Study**: Systematically vary the training dataset size (e.g., 10, 50, 100, 500 queries) to empirically validate the claim that performance scales with training data size, and identify the point of diminishing returns.

3. **Cross-Domain Transferability**: Test APEER-trained prompts on completely out-of-domain datasets (e.g., medical, legal, or technical domains not represented in BEIR) to rigorously evaluate the claimed transferability and identify limitations of the approach.