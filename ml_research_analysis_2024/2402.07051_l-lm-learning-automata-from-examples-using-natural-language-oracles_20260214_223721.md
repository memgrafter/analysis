---
ver: rpa2
title: '$L^*LM$: Learning Automata from Examples using Natural Language Oracles'
arxiv_id: '2402.07051'
source_url: https://arxiv.org/abs/2402.07051
tags:
- learning
- language
- examples
- demonstrations
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LLM, a novel algorithm for learning deterministic
  finite automata (DFA) from demonstrations, natural language descriptions, and labeled
  examples. The key idea is to use a large language model (LLM) as a membership oracle
  to answer queries about task specifications, combined with the Demonstration Informed
  Specification Search (DISS) algorithm to incorporate expert demonstrations.
---

# $L^*LM$: Learning Automata from Examples using Natural Language Oracles

## Quick Facts
- arXiv ID: 2402.07051
- Source URL: https://arxiv.org/abs/2402.07051
- Reference count: 26
- Primary result: L*LM significantly improves DFA learning data efficiency using LLM as membership oracle

## Executive Summary
This paper presents L*LM, an algorithm that learns deterministic finite automata (DFA) from expert demonstrations, natural language descriptions, and labeled examples by using a large language model (LLM) as a membership oracle. The key innovation is combining the Demonstration Informed Specification Search (DISS) algorithm with LLM-based querying to create a multi-modal learning approach that significantly improves data efficiency compared to using demonstrations alone. The authors show that allowing the LLM to respond "unsure" when uncertain and using a SAT-based version space learner outperforms traditional L* algorithm in terms of minimizing the energy (description complexity) of the learned DFA.

## Method Summary
L*LM learns DFAs by combining expert demonstrations with natural language task descriptions through a large language model acting as a membership oracle. The algorithm uses DISS to convert demonstrations into labeled examples, then iteratively queries the LLM with membership questions about sequences (yes/no/unsure). The LLM's responses are used to update a version space of consistent DFAs, either using the L* algorithm or a more efficient SAT-based approach. The process continues until convergence, with the goal of minimizing the energy (description complexity) of the learned DFA while maintaining consistency with all inputs.

## Key Results
- L*LM significantly improves data efficiency of DFA learning from expert demonstrations compared to demonstration-only methods
- Allowing LLM to respond "unsure" when uncertain improves performance by preventing incorrect feature additions
- SAT-based version space learner outperforms L* algorithm in minimizing DFA energy
- Natural language descriptions combined with demonstrations provide better learning outcomes than either modality alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using LLMs as membership oracles for DFA learning improves sample efficiency compared to traditional demonstration-only methods.
- Mechanism: The LLM provides labels for membership queries ("yes/no/unsure") about sequences, allowing the DFA learner to ask targeted questions rather than relying solely on demonstrations. This multi-modal approach combines the LLM's knowledge with DISS's demonstration analysis.
- Core assumption: The LLM's knowledge about the task is sufficient to provide useful membership answers, even if incomplete.
- Evidence anchors:
  - [abstract] "we observe a significant improvement in the data efficiency of learning DFAs from expert demonstrations"
  - [section 2] "LLMs are often better at answering simpler queries about a task than synthesizing a DFA whole cloth"
  - [corpus] Weak - no direct citations about LLM-based membership queries in automata learning

### Mechanism 2
- Claim: Allowing the LLM to respond "unsure" when uncertain improves DFA learning performance.
- Mechanism: By permitting "unsure" responses, the LLM avoids hallucinating membership labels for sequences it cannot confidently classify, preventing incorrect features from being added to the DFA.
- Core assumption: LLMs often hallucinate when forced to answer definitively, and this is worse than asking different questions.
- Evidence anchors:
  - [section 1] "we found in our experiments that the LLM would often state was unsure during chain of thought reasoning... followed by a hallucinated membership response"
  - [section 6] "we see a clear improvement in performance when allowing the LLM to respond unsure"
  - [corpus] Weak - no corpus evidence about "unsure" responses in automata learning context

### Mechanism 3
- Claim: DISS converts demonstration learning into labeled example learning problems, creating a bridge between LLM knowledge and demonstration analysis.
- Mechanism: DISS analyzes demonstrations to hypothesize new labeled examples, which are then provided to the LLM-based DFA learner. This iterative process combines the strengths of both modalities.
- Core assumption: DISS can effectively generate useful labeled examples from demonstrations that complement the LLM's knowledge.
- Evidence anchors:
  - [section 5] "DISS is a variant of maximum causal entropy inverse reinforcement learning that recasts learning a specification... as a series of grammatical inference from labeled example queries"
  - [section 6] "For each iteration of DISS we run our DFA extraction algorithm from Sec 4 with a fixed query budget"
  - [corpus] Weak - no direct corpus citations about DISS algorithm specifically

## Foundational Learning

- Concept: Deterministic Finite Automata (DFAs)
  - Why needed here: The entire paper is about learning DFAs from various inputs
  - Quick check question: Can you explain what makes a finite automaton "deterministic"?

- Concept: Membership queries in active learning
  - Why needed here: The LLM serves as a membership oracle answering yes/no/unsure questions about sequences
  - Quick check question: What is the difference between membership queries and equivalence queries in active learning?

- Concept: Version space learning
  - Why needed here: The algorithm maintains a version space of consistent DFAs and queries to eliminate inconsistent ones
  - Quick check question: How does version space learning differ from PAC learning?

## Architecture Onboarding

- Component map:
  LLM interface -> DFA learner -> DISS algorithm -> Example buffer -> Query generator

- Critical path:
  1. Start with task description and demonstrations
  2. DISS generates initial labeled examples from demonstrations
  3. DFA learner asks LLM membership queries
  4. LLM responds with yes/no/unsure
  5. DFA learner updates version space
  6. DISS analyzes current DFA to generate new examples
  7. Repeat until convergence

- Design tradeoffs:
  - L* vs SAT-based learner: L* asks more queries but SAT is more targeted
  - Query budget: More queries improve accuracy but increase cost
  - Grammar design: More expressive grammars allow better prompting but increase complexity

- Failure signatures:
  - LLM consistently responds "unsure": Need more demonstrations or better prompting
  - DFA energy doesn't converge: May need to adjust lambda parameter in DISS
  - Performance degrades with more queries: Could indicate LLM fatigue or prompt quality issues

- First 3 experiments:
  1. Run with 0 query budget to establish baseline DISS performance
  2. Run with all rules provided but no unsure responses to test individual mechanisms
  3. Run with minimal query budget (5 queries) to test performance with limited LLM access

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of L*LM scale with increasing complexity of the task specification (e.g., number of states in the DFA)?
- Basis in paper: [inferred] The paper mentions that DFA identification is NP-Hard in general and that the SAT-based implementation is used for efficiency. However, the experiments only use a relatively simple DFA with 4 states.
- Why unresolved: The paper does not provide any experiments or analysis of how L*LM performs as the complexity of the task specification increases.
- What evidence would resolve it: Experiments showing the performance of L*LM (e.g., number of queries needed, accuracy) as a function of the number of states in the DFA or other complexity measures.

### Open Question 2
- Question: How robust is L*LM to errors or ambiguities in the natural language task description?
- Basis in paper: [explicit] The paper mentions that the natural language description can have ambiguities and that demonstrations are used to resolve them. It also mentions that the LLM can respond "unsure" when uncertain.
- Why unresolved: The paper does not provide any experiments or analysis of how L*LM performs when the natural language description is incorrect or ambiguous.
- What evidence would resolve it: Experiments where the natural language description is intentionally made incorrect or ambiguous, and the performance of L*LM is measured.

### Open Question 3
- Question: Can L*LM be extended to learn other types of specifications beyond DFAs, such as non-deterministic automata or temporal logic formulas?
- Basis in paper: [explicit] The paper mentions that the techniques could potentially be extended to other concept classes learnable using a minimally adequate teacher, such as symbolic automata or non-deterministic residual automata.
- Why unresolved: The paper only demonstrates L*LM for learning DFAs and does not provide any experiments or analysis for other types of specifications.
- What evidence would resolve it: Experiments showing L*LM successfully learning other types of specifications, such as non-deterministic automata or temporal logic formulas, from demonstrations and natural language.

## Limitations

- Evaluation limited to grid-world navigation tasks with specific tile types (lava, water, charging, drying), limiting generalizability
- Several critical implementation details are underspecified, including exact prompt templates and grammar design
- Primary metric is "energy" (description complexity) which may not fully capture practical functional correctness

## Confidence

- High Confidence: The core mechanism of using LLMs as membership oracles is technically sound and well-grounded in existing literature on active learning and grammatical inference.
- Medium Confidence: Experimental results showing improved data efficiency appear valid for the specific grid-world domain tested, but sample size (5 environments) and domain specificity limit broader conclusions.
- Low Confidence: Claims about the specific contribution of allowing "unsure" responses and the comparative advantage of the SAT-based learner over L* are supported by experimental results but lack theoretical justification or ablation studies.

## Next Checks

1. Apply L*LM to a non-grid-world DFA learning task (e.g., protocol verification or text processing) to test generalizability beyond the current experimental domain.

2. Systematically disable individual components (natural language description, "unsure" responses, DISS algorithm) to quantify their specific contributions to the observed performance improvements.

3. Implement a second, independent DFA learner (e.g., using a different grammatical inference algorithm) and verify that the LLM-based membership oracle produces consistent results across different learning approaches.