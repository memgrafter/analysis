---
ver: rpa2
title: 'GraphLSS: Integrating Lexical, Structural, and Semantic Features for Long
  Document Extractive Summarization'
arxiv_id: '2410.21315'
source_url: https://arxiv.org/abs/2410.21315
tags:
- graph
- sentence
- document
- summarization
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GraphLSS presents a novel heterogeneous graph construction for
  long document extractive summarization that integrates lexical, structural, and
  semantic features. The model uses sentences and words as nodes with four edge types
  (sentence order, sentence similarity, word-in-sentence, and word similarity) without
  requiring external tools or additional machine learning models.
---

# GraphLSS: Integrating Lexical, Structural, and Semantic Features for Long Document Extractive Summarization

## Quick Facts
- arXiv ID: 2410.21315
- Source URL: https://arxiv.org/abs/2410.21315
- Reference count: 12
- Key outcome: GraphLSS achieves state-of-the-art ROUGE-1/2/L scores of 51.42/24.32/49.48 on PubMed and 55.14/23.00/50.83 on arXiv

## Executive Summary
GraphLSS presents a novel heterogeneous graph construction for long document extractive summarization that integrates lexical, structural, and semantic features. The model uses sentences and words as nodes with four edge types (sentence order, sentence similarity, word-in-sentence, and word similarity) without requiring external tools or additional machine learning models. Experiments on PubMed and arXiv datasets show GraphLSS significantly outperforms previous graph-based methods and recent non-graph models. The study highlights the importance of extractive label generation, showing substantial performance variations based on different labeling strategies. An ablation study reveals that word-in-sentence edges have the highest impact on performance, and adaptive class weights during training effectively handle the imbalanced nature of extractive summarization.

## Method Summary
GraphLSS constructs a heterogeneous graph with sentences and words as nodes, connected by four edge types: sentence order edges (Eso), sentence similarity edges (Ess) within a local window using cosine similarity, word-in-sentence edges (Ews) weighted by tf-idf, and word similarity edges (Eww). The model employs a GAT with 4 attention heads and 1-2 layers, using GloVe embeddings for words and SBERT embeddings for sentences. Classification uses weighted cross-entropy loss with adaptive class weights that update during training to handle the imbalanced nature of extractive summarization. The model is trained for 20 epochs with batch size 64 and Adam optimizer, stopping if validation loss doesn't improve for 7 epochs.

## Key Results
- GraphLSS achieves state-of-the-art ROUGE-1/2/L scores of 51.42/24.32/49.48 on PubMed
- GraphLSS achieves state-of-the-art ROUGE-1/2/L scores of 55.14/23.00/50.83 on arXiv
- Ablation study shows word-in-sentence edges have the highest impact on performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-granularity interactions between words and sentences are crucial for effective document representation
- Mechanism: The word-in-sentence edges (Ews) create direct associations between sentence nodes and their contained word nodes, weighted by tf-idf. This allows sentence representations to incorporate the semantic richness of their constituent words while word nodes benefit from their contextual sentence information.
- Core assumption: The semantic value of words and sentences is interdependent and can be captured through direct associations in the graph structure
- Evidence anchors:
  - [abstract]: "GraphLSS utilizes Lexical, Structural, and Semantic features, incorporating two types of nodes (sentences and words) and four types of edges (sentence order, sentences semantic similarity, words semantic similarity, and word–sentence associations)"
  - [section]: "Ablation Study We conducted an ablation study on PubMed to assess the contributions of each edge type (Table 4). The results indicate that word-in-sentence edges have the highest impact on GraphLSS performance"
  - [corpus]: No direct corpus evidence for this specific mechanism
- Break condition: If word nodes were defined too broadly (not limited to nouns, verbs, adjectives), or if tf-idf weighting was removed, the cross-granularity signal would degrade significantly

### Mechanism 2
- Claim: Adaptive class weights effectively handle the imbalanced nature of extractive summarization
- Mechanism: The model updates class weights during training using the formula λi+1 = λi - (τ - τ/log(τ)), where τ represents the proportion of sentences predicted as relevant. This dynamic adjustment prevents the model from being overwhelmed by the majority class (non-summary sentences).
- Core assumption: The imbalance in extractive labels (few relevant sentences vs many irrelevant ones) creates a significant learning challenge that static weights cannot address
- Evidence anchors:
  - [abstract]: "adaptive class weights during training effectively handle the imbalanced nature of extractive summarization"
  - [section]: "Since the extractive ground truth labels for long documents are highly imbalanced, we optimize the model using weighted cross-entropy loss"
  - [corpus]: No direct corpus evidence for this specific mechanism
- Break condition: If the initial class weights were poorly chosen or if τ values became unstable during training, the adaptive mechanism could fail to properly balance the learning signal

### Mechanism 3
- Claim: Sentence similarity edges with local windowing prevent graph density while capturing relevant relationships
- Mechanism: Sentence similarity edges (Ess) are added only within a predefined window size, weighting pairs by cosine similarity. This creates a sparse graph structure that focuses on local context while still capturing meaningful semantic relationships between nearby sentences.
- Core assumption: Relevant semantic relationships between sentences are primarily local rather than global, and dense graphs are computationally prohibitive
- Evidence anchors:
  - [abstract]: "four types of edges (sentence semantic similarity, sentence occurrence order, word in sentence, and word semantic similarity) without any need for auxiliary learning models"
  - [section]: "Ess includes sentence pair edges, weighted by cosine similarity, within a predefined window size to account for local similarity and prevent dense graphs"
  - [corpus]: No direct corpus evidence for this specific mechanism
- Break condition: If window size was set too small, important semantic relationships would be missed; if too large, graph density would negate computational advantages

## Foundational Learning

- Concept: Heterogeneous graph construction
  - Why needed here: The document contains multiple types of semantic units (sentences and words) that need different treatment and edge types to capture their relationships
  - Quick check question: What distinguishes a heterogeneous graph from a homogeneous one in the context of document summarization?

- Concept: Message passing in graph neural networks
  - Why needed here: The GAT model needs to propagate information between different node types (sentences and words) across multiple layers to build rich representations
  - Quick check question: How does message passing differ between a single-layer and multi-layer GAT in this heterogeneous graph?

- Concept: Class imbalance handling
  - Why needed here: Extractive summarization labels are highly imbalanced with few relevant sentences, requiring special treatment during training
  - Quick check question: What is the difference between static and adaptive class weights in binary classification?

## Architecture Onboarding

- Component map: Document preprocessing (tokenization, filtering, label generation) -> Graph construction (node creation with embeddings, edge creation with four types) -> GAT model (4 attention heads, 1-2 layers with dropout) -> Classifier (sigmoid output) -> Evaluation (ROUGE metrics)
- Critical path: Graph construction → GAT processing → Sentence classification → ROUGE evaluation
- Design tradeoffs: Using pre-trained embeddings (GloVe, SBERT) reduces bias and memorization compared to learning from scratch, but may miss domain-specific nuances; local windowing for sentence similarity edges balances computational efficiency with semantic coverage
- Failure signatures: Low ROUGE scores with high precision but low recall suggest over-conservative selection; high recall but low precision indicates excessive sentence selection; poor performance on arXiv vs PubMed may indicate dataset-specific issues
- First 3 experiments:
  1. Test single-layer vs two-layer GAT to verify the benefit of extended message passing
  2. Remove word-in-sentence edges to confirm their impact on performance
  3. Compare adaptive vs static class weights to validate the imbalance handling mechanism

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions.

## Limitations

- The study relies on pre-defined sentence similarity thresholds and window sizes for edge construction without thorough hyperparameter sensitivity analysis
- The adaptive class weight mechanism lacks comparison to other imbalance-handling techniques like focal loss or SMOTE
- The exclusive use of nouns, verbs, and adjectives as word nodes may exclude potentially informative word types (e.g., named entities, domain-specific terminology)

## Confidence

- **High Confidence**: The overall superiority of GraphLSS over baseline models (as evidenced by consistent ROUGE score improvements across both datasets)
- **Medium Confidence**: The specific claim that word-in-sentence edges have the highest impact (based on ablation study but with limited experimental variation)
- **Medium Confidence**: The effectiveness of adaptive class weights (mechanism described but not compared against alternatives)

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary the sentence similarity window size and threshold parameters to determine their impact on ROUGE scores and identify optimal settings
2. **Alternative Word Node Selection**: Test GraphLSS with expanded word node definitions (including named entities and domain-specific terms) to evaluate potential performance gains
3. **Class Imbalance Handling Comparison**: Replace the adaptive class weight mechanism with alternative approaches (focal loss, SMOTE) to benchmark effectiveness against established techniques