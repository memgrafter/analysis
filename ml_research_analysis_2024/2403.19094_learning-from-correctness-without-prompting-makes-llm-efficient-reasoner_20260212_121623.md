---
ver: rpa2
title: Learning From Correctness Without Prompting Makes LLM Efficient Reasoner
arxiv_id: '2403.19094'
source_url: https://arxiv.org/abs/2403.19094
tags:
- step
- reasoning
- leco
- arxiv
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LECO, a prompt-free self-correction framework
  for LLMs that improves reasoning by iteratively accumulating correct reasoning steps.
  It addresses limitations of prior self-correction methods that rely on handcrafted
  prompts and error feedback.
---

# Learning From Correctness Without Prompting Makes LLM Efficient Reasoner

## Quick Facts
- arXiv ID: 2403.19094
- Source URL: https://arxiv.org/abs/2403.19094
- Authors: Yuxuan Yao; Han Wu; Zhijiang Guo; Biyan Zhou; Jiahui Gao; Sichun Luo; Hanxu Hou; Xiaojin Fu; Linqi Song
- Reference count: 40
- This paper introduces LECO, a prompt-free self-correction framework for LLMs that improves reasoning by iteratively accumulating correct reasoning steps.

## Executive Summary
This paper presents LECO (Learning from Correctness), a novel self-correction framework for large language models (LLMs) that enhances reasoning performance without requiring handcrafted prompts or external feedback. The key insight is that providing the model with more correct reasoning steps helps narrow down the search space for the solution. LECO achieves this by iteratively refining reasoning paths through a unique method of measuring step confidence based on generation logits, enabling the identification and retention of correct reasoning steps while discarding potential errors.

## Method Summary
LECO is a prompt-free self-correction framework that improves LLM reasoning by iteratively accumulating correct reasoning steps. It operates by generating an initial reasoning path, calculating step confidence scores using three logit-based metrics (average token confidence, step divergence, and inter-step transition), identifying the earliest potential error step, and appending the preceding correct steps to the input for the next iteration. This process repeats until a stopping condition is met, such as maximum iterations or consecutive identical answers. The framework eliminates the need for handcrafted prompts or external feedback by leveraging the LLM's own generation process to identify and build upon correct reasoning.

## Key Results
- LECO consistently improves accuracy across arithmetic, commonsense, and logical reasoning tasks
- The framework reduces token consumption compared to baselines like self-consistency and RCI
- LECO works effectively for both closed-source (GPT-3.5, GPT-4) and open-source models (DeepSeek)
- Performance gains are particularly notable for complex problems in the AQuA and MATH datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning from correctness improves reasoning by progressively accumulating verified reasoning steps, reducing search space complexity.
- Mechanism: LECO identifies the earliest potential error step in the initial solution, treats preceding steps as "correctness," and appends them to the input for the next iteration. This iterative refinement narrows the reasoning path toward the correct answer.
- Core assumption: Correct reasoning steps can be reliably identified using step confidence scores derived from generation logits.
- Evidence anchors:
  - [abstract] "The proposed framework...improves reasoning performance without needing to learn from errors. This paradigm prioritizes learning from correct reasoning steps..."
  - [section] "Our core insight is that providing the model with more correct reasoning steps helps it narrow down the search space for the solution."
  - [corpus] Weak evidence: corpus does not provide explicit validation of this learning-from-correctness mechanism; only general reasoning improvements are discussed.
- Break condition: If step confidence scoring fails to identify correct steps reliably, the framework loses its ability to progressively accumulate correctness.

### Mechanism 2
- Claim: Step confidence scores accurately identify the earliest error step, enabling effective refinement.
- Mechanism: Confidence for each step is calculated using three logit-based scores: average token confidence, step divergence, and inter-step transition. The step with the lowest score is flagged as the earliest potential error.
- Core assumption: Logits from LLM token generation encode meaningful confidence information that correlates with step correctness.
- Evidence anchors:
  - [abstract] "...a unique method to measure confidence for each reasoning step based on generation logits."
  - [section] "We propose leveraging logits to estimate step confidence. We further design three logit-based scores that comprehensively evaluate confidence from both intra- and inter-step perspectives."
  - [corpus] No direct corpus evidence for logit-based confidence accuracy; mechanism is proposed but not independently validated.
- Break condition: If the logit-based scoring fails to correlate with actual correctness, error identification and refinement will be ineffective.

### Mechanism 3
- Claim: Iterative appending of correctness reduces token consumption compared to baselines.
- Mechanism: By focusing on learning from correctness rather than errors, LECO avoids extensive self-correction prompts and repeated error identification cycles, leading to fewer iterations and lower token usage.
- Core assumption: Error-focused methods like RCI require more tokens due to elaborate error analysis prompts, while correctness-focused methods are more direct.
- Evidence anchors:
  - [abstract] "...improves reasoning performance with reduced token consumption."
  - [section] "LECO reduces API consumption by alleviating prompting the model to identify and understand the errors and shortening the output length."
  - [corpus] Weak evidence: corpus mentions token efficiency but lacks detailed analysis comparing LECO directly to error-focused methods.
- Break condition: If correctness accumulation requires more iterations than error correction in practice, token savings may not materialize.

## Foundational Learning

- Concept: Logits and Softmax in LLM generation
  - Why needed here: LECO's step confidence scoring relies on logits from token generation; understanding their interpretation is essential.
  - Quick check question: What do logits represent before Softmax is applied in LLM token prediction?

- Concept: Kullback-Leibler Divergence (KLD)
  - Why needed here: Step divergence score uses KLD to measure the uniformity of token probability distribution within a step.
  - Quick check question: How does KLD between a token distribution and a uniform distribution indicate step confidence?

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: LECO operates on multi-step reasoning outputs; familiarity with CoT helps understand how reasoning steps are structured.
  - Quick check question: What distinguishes CoT reasoning from direct answer generation in LLMs?

## Architecture Onboarding

- Component map:
  - Input processing -> Initial generation -> Confidence scoring -> Error identification -> Correctness extraction -> Iterative refinement -> Output

- Critical path: Input → Initial generation → Confidence scoring → Error identification → Correctness extraction → Iterative refinement → Output

- Design tradeoffs:
  - Step confidence granularity vs. computational overhead
  - Early stopping vs. potential missed refinements
  - Prompt complexity (demonstrations) vs. reasoning quality

- Failure signatures:
  - Low accuracy improvement despite iterations
  - Token consumption similar to or higher than baselines
  - Step confidence scores not correlating with actual correctness

- First 3 experiments:
  1. Run LECO on a simple arithmetic task (GSM8K) with GPT-3.5, compare accuracy and token usage to SC baseline.
  2. Test LECO with different demonstration qualities (CoT vs. Complex) to evaluate impact on performance.
  3. Perform ablation on step confidence components (remove average token, divergence, or transition) to identify most critical factor.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LECO's performance scale with increasingly complex reasoning tasks beyond those tested in the paper?
- Basis in paper: [explicit] The paper notes LECO shows "remarkable improvements" on the AQuA and MATH datasets, which are described as having "more intricate problems and the need for more reasoning steps."
- Why unresolved: The paper only tests LECO on a limited set of reasoning tasks. It's unclear if the performance gains would hold for even more complex problems like multi-step mathematical proofs or real-world problem solving.
- What evidence would resolve it: Testing LECO on a wider range of reasoning tasks, particularly those with higher complexity and longer reasoning chains, would demonstrate its scalability and limitations.

### Open Question 2
- Question: What is the impact of different token selection strategies within LECO's step confidence calculation?
- Basis in paper: [inferred] The paper introduces three factors for calculating step confidence (average token score, step divergence score, inter-step transition score) but doesn't explore alternative strategies for token selection or weighting.
- Why unresolved: The current method may not optimally capture all relevant information for confidence estimation. Alternative token selection strategies could potentially improve performance.
- What evidence would resolve it: Experimenting with different token selection strategies, such as using attention weights or other contextual information, and comparing their impact on LECO's performance would provide insights into the optimal approach.

### Open Question 3
- Question: How does LECO's performance compare to other self-correction methods when applied to open-domain tasks?
- Basis in paper: [explicit] The paper focuses on multi-step reasoning tasks and doesn't explore LECO's effectiveness in open-domain scenarios.
- Why unresolved: LECO's performance on structured reasoning tasks may not translate directly to open-ended tasks that require more flexible reasoning and knowledge integration.
- What evidence would resolve it: Evaluating LECO on open-domain tasks like question answering, text summarization, or creative writing would reveal its strengths and weaknesses in broader applications.

## Limitations
- Lack of direct empirical validation for the core mechanism that learning from correctness drives improvements
- No independent validation of logit-based step confidence scoring accuracy in identifying correct reasoning steps
- Token efficiency claims lack detailed comparative analysis with error-focused methods

## Confidence
- **High Confidence**: Empirical results showing LECO's accuracy improvements across diverse reasoning tasks and model types
- **Medium Confidence**: Mechanism by which LECO achieves improvements is plausible but not directly validated
- **Low Confidence**: Reliability of logit-based step confidence scoring in accurately identifying correct versus incorrect reasoning steps

## Next Checks
1. Manually annotate a sample of reasoning steps from both correct and incorrect solutions to determine the correlation between the logit-based confidence scores and actual step correctness.
2. Perform an ablation study by removing each component of the step confidence score (average token confidence, step divergence, inter-step transition) to determine which contributes most to LECO's performance.
3. Conduct a detailed analysis comparing LECO's token usage to RCI and other baselines on the same tasks, measuring total tokens consumed per successful solution.