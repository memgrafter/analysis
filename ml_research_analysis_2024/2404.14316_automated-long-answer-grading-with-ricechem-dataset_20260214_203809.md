---
ver: rpa2
title: Automated Long Answer Grading with RiceChem Dataset
arxiv_id: '2404.14316'
source_url: https://arxiv.org/abs/2404.14316
tags:
- alag
- grading
- dataset
- ricechem
- automated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Automated Long Answer Grading (ALAG) and
  presents RiceChem, a new dataset of college-level chemistry responses. The authors
  address the challenge of grading long, fact-based answers by reformulating ALAG
  as a rubric entailment problem, where rubric items are treated as hypotheses and
  student responses as premises in a natural language inference framework.
---

# Automated Long Answer Grading with RiceChem Dataset

## Quick Facts
- arXiv ID: 2404.14316
- Source URL: https://arxiv.org/abs/2404.14316
- Reference count: 35
- Primary result: Introduced ALAG task with RiceChem dataset; rubric-based entailment formulation improves grading performance by 9.2% accuracy and 15.4% F1 over traditional methods

## Executive Summary
This paper introduces Automated Long Answer Grading (ALAG) as a new NLP task for educational assessment, presenting the RiceChem dataset of college-level chemistry responses. The authors reformulate ALAG as a rubric entailment problem, treating rubric items as hypotheses and student responses as premises in a natural language inference framework. By fine-tuning transformer models (BERT, RoBERTa, BART) on RiceChem and leveraging MNLI transfer learning, they demonstrate that their rubric-based approach significantly outperforms traditional score-based grading methods, achieving 9.2% accuracy and 15.4% F1 score improvements.

## Method Summary
The authors address the challenge of grading long, fact-based answers by reformulating ALAG as a rubric entailment problem. This approach treats rubric items as hypotheses and student responses as premises, applying natural language inference (NLI) techniques. They fine-tune transformer models (BERT, RoBERTa, BART) on the RiceChem dataset and leverage MNLI transfer learning to improve performance. The dataset contains 100 responses across 5 questions from a college-level chemistry course, with each response graded against a detailed rubric. Their experimental results show that this rubric-based approach outperforms traditional score-based methods, though LLMs still show lower performance compared to automated short answer grading (ASAG) tasks, highlighting the inherent difficulty of ALAG.

## Key Results
- Rubric-based entailment formulation achieves 9.2% accuracy improvement over traditional score-based methods
- Same approach shows 15.4% F1 score improvement compared to baseline grading techniques
- Despite improvements, ALAG remains significantly more challenging than ASAG tasks, with LLMs showing lower performance even with rubric-based approaches and transfer learning

## Why This Works (Mechanism)
The rubric entailment formulation works by reframing the grading task as a natural language inference problem. Instead of directly mapping responses to scores, the model evaluates whether a student's response entails or contradicts specific rubric criteria. This approach leverages the semantic understanding capabilities of transformer models while providing structured guidance through the rubric items. The transfer learning from MNLI helps the model generalize better to the specific grading context, as it already understands the nuances of entailment and contradiction in natural language.

## Foundational Learning
- Natural Language Inference (NLI): Understanding entailment relationships between premises and hypotheses is crucial for the rubric-based approach. Quick check: Verify the model correctly identifies entailment vs. contradiction in simple sentence pairs.
- Transfer Learning: MNLI pre-training provides semantic understanding that transfers to the grading task. Quick check: Compare performance with and without MNLI transfer learning.
- Rubric Design: Well-structured rubrics with clear criteria are essential for consistent grading. Quick check: Analyze rubric item clarity and coverage through inter-rater reliability tests.
- Transformer Fine-tuning: Adapting pre-trained models to specific tasks through additional training. Quick check: Monitor validation loss during fine-tuning to detect overfitting.

## Architecture Onboarding

Component Map: RiceChem Dataset -> Transformer Models (BERT/RoBERTa/BART) -> Rubric Entailment Framework -> Performance Metrics

Critical Path: Data preprocessing -> Model fine-tuning -> Rubric entailment evaluation -> Performance comparison with baselines

Design Tradeoffs: The rubric-based approach trades direct score prediction for semantic understanding, requiring more complex inference but potentially more nuanced grading. This increases computational cost but may improve consistency.

Failure Signatures: Poor performance on responses requiring synthesis of multiple concepts, difficulty with responses that partially satisfy rubric items, and potential bias toward rubric language in student responses.

First Experiments:
1. Baseline: Score-based grading without rubric entailment
2. Ablation: Model performance with and without MNLI transfer learning
3. Cross-validation: Performance across different rubric items and question types

## Open Questions the Paper Calls Out
The paper acknowledges that ALAG remains significantly more challenging than ASAG tasks, despite the proposed improvements. It raises questions about why LLMs show lower performance even with rubric-based approaches and transfer learning, suggesting the need for deeper analysis of the fundamental differences between short and long answer grading tasks.

## Limitations
- The RiceChem dataset is relatively small (100 responses across 5 questions), limiting generalizability
- Evaluation relies on limited metrics without deeper analysis of grading quality or consistency
- The approach may struggle with responses requiring synthesis of multiple concepts or creative answers

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| 9.2% accuracy and 15.4% F1 score improvement through rubric entailment | Medium |
| ALAG is more challenging than ASAG despite improvements | Medium |
| LLMs show lower performance despite rubric-based approaches and transfer learning | Medium |
| Generalizability of results beyond RiceChem dataset | Low |

## Next Checks
1. Test the rubric entailment approach on multiple courses and subjects to assess generalizability beyond the RiceChem dataset
2. Conduct human evaluation studies comparing rubric-based grading to traditional score-based grading methods in terms of inter-rater reliability and grading consistency
3. Perform ablation studies to isolate the impact of transfer learning (MNLI) versus the rubric formulation itself on performance improvements