---
ver: rpa2
title: 'MobileConvRec: A Conversational Dataset for Mobile Apps Recommendations'
arxiv_id: '2405.17740'
source_url: https://arxiv.org/abs/2405.17740
tags:
- user
- apps
- dataset
- recommendation
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MobileConvRec, a large-scale conversational
  recommendation dataset for mobile apps containing over 12K multi-turn dialogs with
  156K turns across 45 app categories. The dataset combines users' historical app
  interactions with multi-turn natural language conversations to capture both implicit
  preferences and explicit needs.
---

# MobileConvRec: A Conversational Dataset for Mobile Apps Recommendations

## Quick Facts
- arXiv ID: 2405.17740
- Source URL: https://arxiv.org/abs/2405.17740
- Reference count: 40
- 12K+ multi-turn dialogs, 156K+ turns across 45 app categories with rich metadata

## Executive Summary
This paper introduces MobileConvRec, a large-scale conversational recommendation dataset specifically designed for mobile apps. The dataset uniquely combines users' historical app interactions with multi-turn natural language conversations to capture both implicit preferences and explicit needs. By leveraging real Google Play Store interactions and simulating realistic user-system dialogs, MobileConvRec provides a rich resource containing over 12,000 conversations with 156,000 turns, enriched with detailed metadata including permissions, security information, and executable files. The paper establishes baseline results using pre-trained LLMs like GPT-2 and Flan-T5, demonstrating strong performance in both recommendation generation and response generation tasks.

## Method Summary
MobileConvRec is constructed by combining a large sequential recommendation dataset (MobileRec) containing 19.3M app reviews with a novel simulation framework that generates multi-turn conversations. The process begins with BERTopic modeling to extract app aspects from user reviews, followed by semantic dialog outline generation using a REQUEST-RESPONSE protocol. These outlines are then converted to natural language dialogs using GPT-3.5-turbo, with user profiles and interaction histories sampled to ground conversations in realistic scenarios. The dataset is enriched with comprehensive metadata including app permissions, security/privacy information, and binary executables. Baseline models including GPT-2 and Flan-T5 are fine-tuned on this dataset for recommendation generation, candidate ranking, and response generation tasks.

## Key Results
- 86% success rate in recommendation generation using Flan-T5 with context-aware candidate selection
- BLEU score of 0.30 for response generation, with context-aware selection showing 0.18 improvement
- Dataset contains 12,235 multi-turn dialogs across 45 app categories with rich metadata including permissions and security information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MobileConvRec's integration of historical user-item interactions with multi-turn conversational dialogs improves recommendation quality compared to using either modality alone.
- Mechanism: The dataset captures both implicit preferences (from sequential app usage patterns) and explicit needs (from conversational exchanges), allowing models to learn richer user representations.
- Core assumption: Historical interactions and conversational context contain complementary information that, when combined, provide more complete user preference signals than either source alone.
- Evidence anchors:
  - [abstract] "The proposed conversational recommendation dataset synergizes sequential user-item interactions, which reflect implicit user preferences, with comprehensive multi-turn conversations to effectively grasp explicit user needs"
  - [section 3.1] "At a high level, we develop a framework that ingests a sequential recommendation dataset containing user interactions over time (e.g., time-stamped reviews) with various apps. This framework then generates a conversational recommendation dataset as output."
  - [corpus] Weak - no direct corpus evidence comparing performance with/without historical interactions; corpus contains related papers but no evaluation data.
- Break condition: If the combination of modalities introduces noise that outweighs the benefits, or if the model cannot effectively learn to integrate the two information sources.

### Mechanism 2
- Claim: The dataset's rich metadata (permissions, security/privacy info, executables) enables follow-up question-answering and security/privacy analyses that enhance conversational recommender systems.
- Mechanism: By providing detailed app metadata beyond basic descriptions, the dataset allows models to answer user questions about specific app features, security practices, and privacy policies during conversations.
- Core assumption: Users ask follow-up questions about recommended apps, and models need detailed metadata to provide accurate, trustworthy answers.
- Evidence anchors:
  - [abstract] "MobileConvRec presents rich metadata for each app such as permissions data, security and privacy-related information, and binary executables of apps, among others"
  - [section 3.2.4] "the computer simulator provides answers to the user simulator's inquiries, drawing upon information from rich metadata about apps"
  - [corpus] Weak - corpus mentions related work on mobile app security but no direct evidence about follow-up question answering in conversational settings.
- Break condition: If the metadata provided doesn't align with actual user questions or if the volume of metadata overwhelms the conversational context.

### Mechanism 3
- Claim: The dataset's construction methodology, which grounds conversations in real user interactions, ensures realistic dialog simulation that better prepares models for real-world deployment.
- Mechanism: By sampling actual user interactions and using them to guide the simulation of conversations, the dataset creates dialogs that reflect genuine user needs and preferences rather than synthetic or arbitrary exchanges.
- Core assumption: Conversations grounded in real user behavior are more valuable for training than those generated from arbitrary or synthetic patterns.
- Evidence anchors:
  - [abstract] "MobileConvRec simulates conversations by leveraging real user interactions with mobile apps on the Google Play store, originally captured in large-scale mobile app recommendation dataset MobileRec"
  - [section 3.1] "It is crucial to emphasize that the simulated conversation remains grounded in the sampled interaction, ensuring that the simulation closely aligns with the user's actual interaction in hindsight"
  - [corpus] Weak - no direct corpus evidence comparing grounded vs ungrounded dialog simulation approaches.
- Break condition: If the sampling methodology introduces bias that limits the diversity of simulated conversations or if the grounding doesn't capture the full range of conversational scenarios.

## Foundational Learning

- Concept: Natural language processing fundamentals (tokenization, embeddings, transformers)
  - Why needed here: The dataset relies on LLMs (GPT-2, Flan-T5) for both conversation simulation and baseline evaluation, requiring understanding of how these models process and generate text.
  - Quick check question: How do pre-trained language models like GPT-2 and Flan-T5 differ in their architecture and training objectives?

- Concept: Recommendation systems (collaborative filtering, content-based filtering, sequential recommendations)
  - Why needed here: MobileConvRec combines elements of traditional recommendation systems (sequential user interactions) with conversational approaches, requiring understanding of both paradigms.
  - Quick check question: What are the key differences between implicit and explicit feedback in recommendation systems, and how does MobileConvRec capture both?

- Concept: Conversational AI and dialog systems
  - Why needed here: The dataset's primary contribution is enabling research in conversational app recommendations, requiring understanding of how dialog systems manage multi-turn conversations and maintain context.
  - Quick check question: What are the main challenges in maintaining conversational context across multiple turns, and how might MobileConvRec help address these challenges?

## Architecture Onboarding

- Component map: MobileRec -> BERTopic topic modeling -> User profile and interaction sampling -> Semantic dialog simulation -> LLM conversation generation -> Metadata enrichment -> Baseline model training and evaluation

- Critical path: MobileRec → BERTopic topic modeling → User profile and interaction sampling → Semantic dialog simulation → LLM conversation generation → Metadata enrichment → Baseline model training and evaluation

- Design tradeoffs: 
  - Real vs synthetic data: Using real user interactions provides authenticity but limits control over conversation diversity
  - Metadata depth vs usability: Rich metadata enables sophisticated analyses but increases dataset complexity
  - Simulation complexity vs naturalness: Detailed simulation protocols create realistic dialogs but require careful prompt engineering

- Failure signatures:
  - Low success rates in recommendation generation experiments indicate poor integration of conversational and historical data
  - High variance in BLEU scores suggests inconsistent response quality across different conversation types
  - Model bias toward popular apps reveals limitations in handling long-tail app recommendations

- First 3 experiments:
  1. Evaluate recommendation generation performance with only dialog context vs dialog context + historical interactions to quantify the value of combining modalities
  2. Test different candidate app selection strategies (sampled vs similar candidates) to understand impact on model performance
  3. Analyze response generation quality across different conversation lengths and topics to identify performance patterns and limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does incorporating executable file analysis improve recommendation quality and security compared to relying solely on developer-provided metadata?
- Basis in paper: [explicit] The paper mentions that executable files (.apk) are provided for free apps, enabling researchers to conduct security and privacy-related analyses and verify functional correctness.
- Why unresolved: The paper establishes the availability of executable files but does not empirically demonstrate how analyzing these files improves recommendation accuracy or security outcomes compared to metadata-only approaches.
- What evidence would resolve it: A controlled experiment comparing recommendation performance and security assessment accuracy between models using executable analysis versus those using only metadata would provide concrete evidence.

### Open Question 2
- Question: What is the optimal balance between historical interaction data and conversational context for achieving the best recommendation performance?
- Basis in paper: [explicit] The paper notes ambiguity about the impact of incorporating historical interactions, observing that simply passing historical interactions as input may not be optimal and warrants further investigation.
- Why unresolved: The experimental results show inconsistent improvements when adding historical interactions, suggesting that the current integration method may not be optimal. The paper identifies this as an area needing further exploration.
- What evidence would resolve it: Systematic experiments varying the weight and format of historical interaction data relative to conversational context, potentially using attention mechanisms or other sophisticated integration methods, would help identify optimal approaches.

### Open Question 3
- Question: How can conversational recommendation systems be designed to handle the cold-start problem when no user history is available?
- Basis in paper: [explicit] The paper mentions that cases with no user history have been sampled to simulate cold-start scenarios, noting that this approach ensures recommendation models cannot rely solely on historical interactions.
- Why unresolved: While the dataset includes cold-start scenarios, the paper does not present specific strategies or evaluate model performance in these situations, leaving the challenge of handling users without interaction history unaddressed.
- What evidence would resolve it: Evaluating different cold-start strategies (e.g., demographic-based recommendations, preference elicitation techniques, or transfer learning approaches) on the cold-start samples in the dataset would demonstrate effective solutions.

## Limitations
- Dataset construction relies heavily on simulation, which may not fully capture real-world conversation complexity and unpredictability
- Evaluation focuses primarily on technical metrics without comprehensive user studies to validate practical utility
- Dataset limited to English language data from Google Play Store, restricting multilingual and cross-platform applicability

## Confidence

**High Confidence**: The dataset construction methodology and technical specifications are well-documented and reproducible. The integration of historical user interactions with conversational data is clearly described, and the rich metadata provision is verifiable.

**Medium Confidence**: The performance metrics (86% success rate, BLEU score of 0.30) are based on established evaluation protocols, but the practical significance of these numbers in real-world deployment scenarios remains uncertain without user studies.

**Low Confidence**: The claim that this dataset "paves the way for building state-of-the-art conversational recommender systems" is aspirational and requires further validation through deployed systems and longitudinal studies.

## Next Checks

1. **Real-world deployment validation**: Conduct user studies comparing recommendations from systems trained on MobileConvRec against baseline systems in actual conversational settings to assess practical utility beyond laboratory metrics.

2. **Bias and fairness analysis**: Systematically evaluate model recommendations for biases related to app popularity, developer reputation, and user demographics to identify potential fairness issues in the recommendation generation process.

3. **Generalization testing**: Test model performance on out-of-distribution conversations and apps not well-represented in the training data to assess the dataset's coverage and the models' ability to handle novel recommendation scenarios.