---
ver: rpa2
title: Potential-Based Reward Shaping For Intrinsic Motivation
arxiv_id: '2402.07411'
source_url: https://arxiv.org/abs/2402.07411
tags:
- reward
- environment
- shaping
- optimal
- equation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Potential-Based Intrinsic Motivation (PBIM),
  a method that extends potential-based reward shaping to intrinsic motivation terms
  while preserving optimal policies. The key innovation is a boundary condition that
  allows arbitrary reward functions to be converted into potential-based form, preventing
  agents from exploiting intrinsic rewards to deviate from optimal behavior.
---

# Potential-Based Reward Shaping For Intrinsic Motivation

## Quick Facts
- arXiv ID: 2402.07411
- Source URL: https://arxiv.org/abs/2402.07411
- Reference count: 37
- Primary result: PBIM extends potential-based reward shaping to intrinsic motivation while preserving optimal policies through a boundary condition

## Executive Summary
This paper presents Potential-Based Intrinsic Motivation (PBIM), a method that extends potential-based reward shaping to intrinsic motivation terms while preserving optimal policies. The key innovation is a boundary condition that allows arbitrary reward functions to be converted into potential-based form, preventing agents from exploiting intrinsic rewards to deviate from optimal behavior. The method was tested on MiniGrid DoorKey and Cliff Walking environments, demonstrating that PBIM successfully prevents convergence to suboptimal policies while often accelerating training compared to standard intrinsic motivation approaches.

## Method Summary
PBIM converts arbitrary intrinsic rewards into potential-based form by expressing them as differences of cumulative returns, then applying a boundary condition to ensure optimal policy preservation. The method introduces a delayed adjustment term that makes it difficult for agents to discover that intrinsic motivation doesn't affect long-term returns. Two versions are proposed: normalized (Equation 34) and non-normalized (Equation 30), with the normalized version preventing short-term bias at the cost of computational overhead.

## Key Results
- PBIM successfully prevents convergence to suboptimal policies in DoorKey and Cliff Walking environments
- The method often accelerates training compared to baseline intrinsic motivation approaches
- Both normalized and non-normalized versions of PBIM show effective performance, with normalized version providing better long-term stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Potential-based reward shaping preserves optimal policies when a boundary condition holds
- Mechanism: Adding shaping reward ùêπùë° = ùõæŒ¶ùë°+1 - Œ¶ùë° preserves optimal policies if E[ùõæùëÅ-ùë°Œ¶ùëÅ - Œ¶ùë°] is constant with respect to action at time t
- Core assumption: The boundary condition E[ùõæùëÅ-ùë°Œ¶ùëÅ - Œ¶ùë°] = Œ¶'ùë° holds for all t ‚àà (0,1,...,N-1)
- Evidence anchors:
  - [abstract] "a boundary condition that allows arbitrary reward functions to be converted into potential-based form"
  - [section] "Theorem 1 (Sufficient Condition For Optimality). The addition of a shaping reward ùêπùë° = ùõæŒ¶ùë°+1 ‚àí Œ¶ùë° leaves the set of optimal policies unchanged if Equation 22 holds."
  - [corpus] "Potential-Based Reward Shaping For Intrinsic Motivation" with FMR score 0.616
- Break condition: The boundary condition fails when Œ¶ depends on future actions (a' > t) or when the environment is not episodic

### Mechanism 2
- Claim: Intrinsic rewards can be converted to potential-based form without altering optimal policies
- Mechanism: Any intrinsic reward ùêπùë° can be expressed as difference of cumulative returns: ùêπùë° = ùëàùúãùë° - ùõæùëàùúãùë°+1, allowing conversion to potential-based form
- Core assumption: Assumption 1 holds - ùêπùë° is constant with respect to a't > t for all t, t' ‚àà (0,1,...,N-1)
- Evidence anchors:
  - [abstract] "a method for converting IM rewards into a potential-based form that is useable without altering the set of optimal policies"
  - [section] "The trick is to realize that, in all time steps but the last, any arbitrary reward function (including IM) is already a difference of a potential function"
  - [corpus] "BAMDP Shaping: a Unified Framework for Intrinsic Motivation and Reward Shaping" with FMR score 0.618
- Break condition: Assumption 1 fails for IM methods that depend on future actions (e.g., empowerment)

### Mechanism 3
- Claim: Delayed adjustment term prevents agents from discovering IM futility
- Mechanism: The adjustment term at t=N-1 extends reward horizon, making it difficult for agents to learn IM has no long-term value
- Core assumption: Agents have limited planning horizon and cannot easily reverse-engineer the relationship between short-term IM and long-term returns
- Evidence anchors:
  - [abstract] "This makes it intentionally difficult for the agent to discover that IM doesn't ever actually affect the final return of an episode"
  - [section] "Equation 30 also has the advantage that it makes it particularly difficult for most agents to 'figure out' that optimizing intrinsic motivation does nothing to increase their value function in the long run"
  - [corpus] "On the Sample Efficiency of Abstractions and Potential-Based Reward Shaping in Reinforcement Learning" with FMR score 0.620
- Break condition: Agents discover the adjustment mechanism through extensive exploration or have very long planning horizons

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The entire framework is built on MDP theory and Q-learning
  - Quick check question: What are the five components of an MDP tuple (S, A, T, Œ≥, R)?

- Concept: Reward shaping theory
  - Why needed here: Understanding how reward shaping can alter optimal policies is central to the problem
  - Quick check question: What is the key difference between potential-based reward shaping and arbitrary reward shaping?

- Concept: Intrinsic motivation in RL
  - Why needed here: The paper specifically addresses converting IM methods to potential-based form
  - Quick check question: Name two common IM methods that can cause "reward hacking" in sparse-reward environments

## Architecture Onboarding

- Component map: IM reward calculation ‚Üí PBIM conversion (Equations 30/34) ‚Üí Standard RL algorithm ‚Üí Environment interaction
- Critical path: IM reward generation ‚Üí Potential-based transformation ‚Üí Q-value updates ‚Üí Policy improvement
- Design tradeoffs: Normalized vs non-normalized PBIM (Equation 30 vs 34) - trade-off between preventing short-term bias and computational overhead
- Failure signatures: Agent fails to converge, agent converges to suboptimal policy, training becomes unstable
- First 3 experiments:
  1. Implement PBIM with normalized version (Equation 34) on MiniGrid DoorKey with tabular exploration reward
  2. Compare convergence speed between PBIM and baseline IM on Cliff Walking environment
  3. Test PBIM with RND in a longer Cliff Walking variant to evaluate performance in sparse-reward settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the normalized variation of PBIM compare to the non-normalized version in terms of computational efficiency and training speed across different environments?
- Basis in paper: [explicit] The paper discusses the normalized variation of PBIM to mitigate potential issues with non-normalized PBIM, but does not provide a detailed comparison of computational efficiency and training speed.
- Why unresolved: The paper focuses on the theoretical guarantees and empirical effectiveness of both variations but does not explicitly compare their computational demands or training speed in various environments.
- What evidence would resolve it: A detailed empirical study comparing the computational efficiency and training speed of normalized versus non-normalized PBIM across a range of environments, including both simple and complex scenarios.

### Open Question 2
- Question: Can PBIM be effectively extended to handle intrinsic motivation terms that depend on future actions, such as empowerment?
- Basis in paper: [inferred] The paper assumes that the intrinsic motivation terms do not depend on future actions, but acknowledges that addressing such terms is left to future work.
- Why unresolved: The current formulation of PBIM does not accommodate intrinsic motivation terms that depend on future actions, and the paper does not provide a method for extending PBIM to handle such cases.
- What evidence would resolve it: A theoretical extension of PBIM that incorporates intrinsic motivation terms dependent on future actions, along with empirical validation in environments where such terms are relevant.

### Open Question 3
- Question: What are the long-term effects of using PBIM on the agent's ability to generalize to new tasks or environments?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of PBIM in specific environments but does not explore its impact on the agent's generalization capabilities.
- Why unresolved: The paper does not investigate whether the use of PBIM affects the agent's ability to adapt to new tasks or environments after being trained with PBIM.
- What evidence would resolve it: Longitudinal studies comparing the generalization performance of agents trained with PBIM versus traditional methods across a variety of tasks and environments.

## Limitations
- The PBIM framework's theoretical guarantees depend critically on the boundary condition holding throughout training, but empirical validation of this condition in complex environments remains limited
- While the method prevents convergence to suboptimal policies, it may introduce computational overhead through normalization and adjustment terms that could impact real-world deployment
- The conversion from arbitrary IM rewards to potential-based form assumes the agent cannot easily discover the adjustment mechanism, yet the paper provides limited evidence about agent learning dynamics in this regard

## Confidence
- High confidence: The theoretical foundation of potential-based reward shaping preserving optimal policies when boundary conditions are met (supported by established MDP theory)
- Medium confidence: The claim that PBIM prevents agents from discovering IM futility through delayed adjustment terms (based on theoretical argument but limited empirical validation)
- Medium confidence: The assertion that PBIM accelerates training compared to baseline IM methods (supported by experimental results but tested on limited environments)

## Next Checks
1. **Boundary condition validation**: Implement monitoring during training to empirically verify that the boundary condition E[ùõæùëÅ-ùë°Œ¶ùëÅ - Œ¶ùë°] remains constant across time steps and actions in both DoorKey and Cliff Walking environments.

2. **Agent learning dynamics analysis**: Design experiments to test whether agents with longer planning horizons or more sophisticated exploration strategies can discover the relationship between short-term IM and long-term returns, potentially invalidating the delayed adjustment mechanism.

3. **Computational overhead quantification**: Measure and compare the computational costs of PBIM (both normalized and non-normalized versions) against baseline IM methods across different environment complexities to assess practical deployment implications.