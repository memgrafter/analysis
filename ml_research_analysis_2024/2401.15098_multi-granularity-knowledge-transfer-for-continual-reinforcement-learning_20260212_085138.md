---
ver: rpa2
title: Multi-granularity Knowledge Transfer for Continual Reinforcement Learning
arxiv_id: '2401.15098'
source_url: https://arxiv.org/abs/2401.15098
tags:
- task
- tasks
- learning
- policy
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of knowledge transfer in continual
  reinforcement learning (CRL) across diverse tasks, which existing methods struggle
  with due to their focus on fine-grained knowledge transfer. The authors propose
  MT-Core, a novel framework that leverages a multi-granularity structure inspired
  by human cognitive control.
---

# Multi-granularity Knowledge Transfer for Continual Reinforcement Learning

## Quick Facts
- arXiv ID: 2401.15098
- Source URL: https://arxiv.org/abs/2401.15098
- Reference count: 10
- Primary result: MT-Core achieves 0.65 average performance vs 0.30-0.20 for baselines in continual RL

## Executive Summary
This paper addresses the challenge of knowledge transfer in continual reinforcement learning across diverse tasks by proposing a multi-granularity framework called MT-Core. Unlike existing methods that focus on fine-grained parameter transfer, MT-Core mimics human cognitive control by separating coarse-grained policy formulation using LLM reasoning from fine-grained policy learning via goal-oriented RL. The approach demonstrates significant performance improvements in MiniGrid environments, particularly for heterogeneous task transfer where traditional methods struggle.

## Method Summary
MT-Core employs a two-layer architecture where a large language model formulates coarse-grained policies by generating temporally extended goal sequences from environment descriptions. These goals guide a goal-oriented RL agent through intrinsic reward functions that provide denser feedback than sparse environmental rewards. A policy library stores successful policies using embedding-based retrieval for rapid transfer to new tasks. The framework is evaluated on MiniGrid environments with four tasks of increasing difficulty, comparing against EWC, L2P, and PPO baselines.

## Key Results
- MT-Core achieves average performance of 0.65 compared to 0.30-0.20 for baselines
- Positive forward transfer of 0.17 versus negative or zero transfer for other methods
- Effective transfer across heterogeneous tasks with different state spaces
- Demonstrates adaptability and generalization capability in CRL settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coarse-grained knowledge transfer using LLM reasoning improves cross-task adaptability
- Mechanism: LLM formulates temporally extended goal sequences that abstract away task-specific details, enabling transfer across heterogeneous environments
- Core assumption: LLM reasoning can capture task-agnostic abstractions that are transferable between different MDPs
- Evidence anchors:
  - [abstract]: "utilizing the powerful reasoning ability of the large language model (LLM) to set goals"
  - [section 3.1]: "MT-Core utilizes the powerful reasoning ability of a LLM to formulate a coarse-grained policy, which is represented as a temporally extended sequence of goals"
  - [corpus]: Weak - no direct evidence of LLM abstraction capabilities in CRL literature

### Mechanism 2
- Claim: Goal-oriented RL guided by coarse-grained policies improves fine-grained learning efficiency
- Mechanism: Intrinsic reward functions defined for each goal guide RL agent learning, providing more granular supervision than sparse environmental rewards
- Core assumption: Goal-specific intrinsic rewards accelerate learning by providing denser feedback signals
- Evidence anchors:
  - [section 3.2]: "This reward structure provides a more granular view into the effectiveness of goal-related actions, encouraging the agent to learn a policy that is aligned with the goal more efficiently"
  - [section 3.2]: "the agent's intrinsic reward function is defined as: R'(s, a, s', g) = rg, if vg(sg, s') = 1; 0, otherwise"
  - [corpus]: Weak - limited evidence on goal-oriented RL effectiveness in CRL contexts

### Mechanism 3
- Claim: Policy library enables both knowledge retention and rapid transfer through retrieval-based context
- Mechanism: Successful coarse-grained policies and corresponding environment descriptions are stored and retrieved via embedding similarity, providing context for new tasks
- Core assumption: Similar environmental descriptions have transferable policies that can be adapted with minimal fine-tuning
- Evidence anchors:
  - [section 3.3]: "The description of each coarse-grained policy d and the corresponding feedback is indexed by the embedding of the environment description e"
  - [section 3.3]: "Given a new environment description e, MT-Core can use embedding to retrieve the first k similar environment descriptions"
  - [corpus]: Weak - limited research on retrieval-based policy transfer in CRL

## Foundational Learning

- Concept: Multi-granularity hierarchical learning structure
  - Why needed here: Traditional CRL methods focus on fine-grained parameter sharing which fails across diverse tasks; multi-granularity structure mimics human cognitive control enabling both coarse planning and fine execution
  - Quick check question: How does separating coarse policy formulation from fine-grained learning address the stability-plasticity dilemma in CRL?

- Concept: Goal-oriented reinforcement learning
  - Why needed here: Provides intrinsic reward structure that guides learning toward specific subgoals, creating denser feedback signals than sparse environmental rewards
  - Quick check question: How do intrinsic reward functions for goals differ from extrinsic environmental rewards in terms of learning efficiency?

- Concept: Continual learning with catastrophic forgetting
  - Why needed here: Understanding forgetting mechanisms is crucial for designing policy library and transfer strategies that maintain performance across task sequences
  - Quick check question: What are the primary causes of catastrophic forgetting in sequential task learning, and how does parameter isolation differ from regularization approaches?

## Architecture Onboarding

- Component map: LLM reasoning layer -> Goal-oriented RL layer -> Policy library (embedding-based retrieval) -> Environment simulator -> Feedback loop
- Critical path: LLM → Goal formulation → RL learning → Policy evaluation → Library storage/retrieval → Next task adaptation
- Design tradeoffs:
  - LLM complexity vs. reasoning quality: More capable LLMs may provide better abstractions but increase computational cost
  - Library size vs. retrieval efficiency: Larger libraries provide more transfer opportunities but increase embedding computation time
  - Goal granularity vs. learning tractability: More detailed goals provide better guidance but may overwhelm the fine-grained RL learner
- Failure signatures:
  - LLM produces irrelevant or overly specific goals that don't generalize
  - RL agent fails to achieve goals despite reasonable goal formulation
  - Policy retrieval returns irrelevant policies due to poor embedding similarity
  - Catastrophic forgetting occurs despite policy library storage
- First 3 experiments:
  1. Single task learning with MT-Core vs. baseline to verify LLM reasoning benefits
  2. Two-task sequence with similar environments to test basic transfer capability
  3. Two-task sequence with different state spaces to validate heterogeneous task transfer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multi-granularity structure of MT-Core perform in environments with continuous state and action spaces compared to discrete spaces?
- Basis in paper: [inferred] The paper demonstrates MT-Core's effectiveness in MiniGrid environments with discrete state and action spaces, but does not explore continuous spaces.
- Why unresolved: The paper focuses on discrete environments, leaving the performance in continuous spaces unexplored, which is a common scenario in real-world applications.
- What evidence would resolve it: Experimental results comparing MT-Core's performance in continuous state and action space environments against existing CRL methods would provide clarity.

### Open Question 2
- Question: What are the limitations of the policy library in terms of scalability and retrieval efficiency as the number of tasks increases?
- Basis in paper: [explicit] The paper mentions that MT-Core uses a policy library to store and retrieve policies, but does not discuss scalability or efficiency concerns as the library grows.
- Why unresolved: The paper does not address how the policy library performs with a large number of tasks, which could affect retrieval times and storage efficiency.
- What evidence would resolve it: Empirical studies on the performance of the policy library with increasing task numbers, focusing on retrieval times and memory usage, would provide insights.

### Open Question 3
- Question: How does the integration of LLM in MT-Core affect the computational overhead and real-time applicability of the framework?
- Basis in paper: [inferred] The paper highlights the use of LLM for coarse-grained policy formulation, but does not discuss the computational costs or real-time constraints.
- Why unresolved: The paper does not explore the trade-offs between the benefits of using LLM and the potential increase in computational demands, which could limit real-time applicability.
- What evidence would resolve it: Comparative analysis of computational overhead between MT-Core and other CRL methods, along with real-time performance metrics, would clarify this issue.

## Limitations

- Evaluation limited to four MiniGrid environments with simplified state/action spaces
- LLM integration lacks detailed implementation specifications affecting reproducibility
- Does not address computational overhead considerations for large-scale deployments
- Missing ablation studies on individual components of the multi-granularity architecture

## Confidence

- Multi-granularity architecture effectiveness: Medium - supported by experimental results but limited to controlled environments
- LLM reasoning for coarse-grained policies: Medium - theoretical justification present but implementation details sparse
- Policy library transfer mechanism: Medium - concept is sound but empirical validation is limited to specific task types

## Next Checks

1. Test MT-Core on continuous control environments (e.g., MuJoCo) to evaluate scalability beyond discrete grid-world settings
2. Conduct ablation studies to isolate the contribution of each component (LLM, goal-oriented RL, policy library) to overall performance
3. Evaluate computational overhead and real-time applicability of the embedding-based policy retrieval mechanism in resource-constrained scenarios