---
ver: rpa2
title: Investigating LLM Applications in E-Commerce
arxiv_id: '2408.12779'
source_url: https://arxiv.org/abs/2408.12779
tags:
- tasks
- task
- product
- e-commerce
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the effectiveness of Large Language Models
  (LLMs) on e-commerce tasks by comparing fine-tuned LLMs with traditional models
  like BERT and T5 across classification, generation, summarization, and named entity
  recognition tasks. The study finds that while LLMs can outperform traditional models
  in text generation tasks, they often require more training data to achieve comparable
  performance in classification tasks.
---

# Investigating LLM Applications in E-Commerce

## Quick Facts
- arXiv ID: 2408.12779
- Source URL: https://arxiv.org/abs/2408.12779
- Reference count: 40
- Primary result: Fine-tuned smaller models often outperform zero-shot very large LLMs in e-commerce tasks

## Executive Summary
This paper evaluates Large Language Models (LLMs) on e-commerce tasks by comparing fine-tuned LLMs with traditional models like BERT and T5 across classification, generation, summarization, and named entity recognition tasks. The study finds that while LLMs can outperform traditional models in text generation tasks, they often require more training data to achieve comparable performance in classification tasks. Additionally, the research highlights that in-context learning with very large LLMs does not consistently surpass fine-tuned smaller models, emphasizing the importance of task-specific optimization.

## Method Summary
The study fine-tunes Llama-2-7b using LoRA on four e-commerce datasets: ESCI Multi-class Product Classification, QueryNER, Review Summarization, and Product Description Generation. The approach compares performance against baseline models (BERT, T5) and evaluates both fine-tuned and zero-shot performance using very large LLMs (Mixtral 8x22B). Experiments vary training data sizes and test LoRA weight merging across different task types to assess multitask training effectiveness.

## Key Results
- Fine-tuned smaller models often outperform zero-shot very large LLMs in e-commerce-specific tasks
- LLMs require more training data than traditional models to achieve comparable classification performance
- LoRA merging is effective for text generation tasks but degrades performance on tasks requiring rigid output formats

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning smaller models on domain-specific tasks can outperform zero-shot inference from much larger LLMs because fine-tuning optimizes model parameters to the task distribution and output format requirements, whereas zero-shot relies on general pretraining knowledge. The core assumption is that task-specific data contains sufficient signal for the smaller model to learn accurate mappings. Evidence shows fine-tuning consistently outperforms zero-shot on ESCI classification and QueryNER tasks. Break condition: insufficient or low-quality training data; model capacity too low for task complexity.

### Mechanism 2
Mixed LoRA merging is effective for text generation tasks but degrades performance on tasks requiring rigid output formats because averaging LoRA adapters works when tasks share similar output structure, but conflicts arise when output formats differ. The core assumption is that tasks with compatible output formats can share adaptation parameters without loss. Evidence shows merging maintains generation performance but degrades classification accuracy due to output formatting conflicts. Break condition: task output formats are too divergent to share LoRA parameters effectively.

### Mechanism 3
Training data volume is critical for LLM adaptation in classification tasks because more training samples allow the LLM to learn task-specific decision boundaries and output formatting patterns. The core assumption is that the model can generalize from more examples to unseen instances in the same task. Evidence demonstrates performance generally increases with training data volume, particularly showing clear boosts when increasing from 10k to 50k samples on ESCI tasks. Break condition: diminishing returns where additional data no longer improves performance.

## Foundational Learning

- Concept: Supervised fine-tuning (SFT) with LoRA adapters
  - Why needed here: Efficiently adapts large models to new tasks without full retraining
  - Quick check question: What is the primary benefit of using LoRA instead of full fine-tuning for LLM adaptation?

- Concept: Task-specific evaluation metrics (F1 for classification, ROUGE for generation)
  - Why needed here: Ensures fair comparison between models across different task types
  - Quick check question: Why can't we use the same metric (e.g., accuracy) for both classification and text generation tasks?

- Concept: Zero-shot vs. few-shot vs. fine-tuned performance differences
  - Why needed here: Understanding when each approach is appropriate for different e-commerce tasks
  - Quick check question: In what scenario would zero-shot inference potentially outperform fine-tuning despite the general findings?

## Architecture Onboarding

- Component map: Data preprocessing -> Prompt design -> Model loading (LLaMA-2-7B) -> LoRA fine-tuning -> Evaluation
- Critical path: Data -> Fine-tuning -> Evaluation -> Analysis
- Design tradeoffs: Model size vs. training efficiency vs. performance; task independence vs. multitask training
- Failure signatures: Output format mismatches, poor performance on specific tasks, inconsistent results across runs
- First 3 experiments:
  1. Fine-tune LLaMA-2-7b on ESCI classification with varying training set sizes (1k, 5k, 10k, 50k)
  2. Compare zero-shot Mixtral 8x22B performance vs. fine-tuned LLaMA-2-7b on QueryNER
  3. Test LoRA merging across classification and generation tasks to identify format conflicts

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal amount of training data required for LLMs to achieve competitive performance on classification tasks in e-commerce? The paper tested specific data sizes (1k, 5k, 10k, 50k) and found performance increased with more data, but didn't determine the exact threshold where LLM performance matches traditional models. Additional experiments testing wider ranges of data sizes with more granular increments would resolve this.

### Open Question 2
How does in-context learning with very large LLMs compare to fine-tuning smaller models for highly specialized or niche e-commerce tasks? The paper only tested Mixtral 8x22B and didn't explore diverse specialized tasks or compare with multiple smaller fine-tuned models. Extensive testing of various large LLMs on niche e-commerce tasks compared against multiple smaller models would resolve this.

### Open Question 3
What is the impact of task-specific formatting requirements on LoRA weight merging effectiveness across different e-commerce tasks? The paper noted performance degradation in classification tasks due to output formatting issues but didn't provide detailed analysis of the relationship between formatting complexity and merging effectiveness. In-depth analysis with various formatting requirements and potential mitigation techniques would resolve this.

## Limitations

- Lack of direct empirical evidence supporting mechanistic explanations for observed performance differences
- No detailed analysis of internal model changes driving differences between fine-tuning and zero-shot approaches
- Limited exploration of why traditional models like BERT and T5 perform differently than fine-tuned LLMs

## Confidence

- High Confidence: Fine-tuned smaller models outperform zero-shot very large LLMs; training data volume matters for classification tasks
- Medium Confidence: LoRA merging effectiveness varies by task type; performance patterns between model types are consistent
- Low Confidence: Mechanistic explanations for mixed-task training failures; underlying causes of performance differences remain speculative

## Next Checks

1. **Mechanistic Analysis of LoRA Degradation:** Design experiments that systematically vary output format constraints while keeping task similarity constant to isolate whether format conflicts or parameter interference drive performance degradation in mixed-task training.

2. **Training Data Efficiency Comparison:** Conduct controlled experiments varying both training data volume and model size across classification and generation tasks to quantify exactly how much additional data LLMs need versus traditional models for comparable performance.

3. **Cross-Domain Generalization Test:** Evaluate fine-tuned models on out-of-domain e-commerce data versus zero-shot very large LLMs to determine whether the fine-tuning advantage persists when task distribution shifts, validating the importance of task-specific optimization.