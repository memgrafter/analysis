---
ver: rpa2
title: 'SSET: Swapping-Sliding Explanation for Time Series Classifiers in Affect Detection'
arxiv_id: '2410.12996'
source_url: https://arxiv.org/abs/2410.12996
tags:
- time
- data
- sset
- explanations
- salient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SSET, a swapping-sliding explanation method
  for time series classifiers, specifically applied to affect detection. SSET addresses
  the challenge of providing accurate and comprehensible explanations for black-box
  models on multivariate time series data, where traditional methods often fail due
  to the conflation of time and variables.
---

# SSET: Swapping-Sliding Explanation for Time Series Classifiers in Affect Detection

## Quick Facts
- arXiv ID: 2410.12996
- Source URL: https://arxiv.org/abs/2410.12996
- Reference count: 35
- SSET outperforms LIME, integrated gradients, and Dynamask on precision and informativeness for affect detection time series explanations

## Executive Summary
This paper introduces SSET (Swapping-Sliding Explanation Technique), a novel method for explaining black-box time series classifiers specifically designed for affect detection tasks. Traditional explanation methods struggle with multivariate time series data because they conflate time and variable dimensions, treating time series as images. SSET addresses this by using a two-stage approach: swapping to identify salient variables by replacing values with similar training data, and sliding to detect important sub-sequences through temporal window analysis. The method demonstrates superior performance on WESAD and MAHNOB-HCI datasets, achieving 0.1 precision on WESAD compared to 0.02 for competing methods.

## Method Summary
SSET operates in two stages to explain multivariate time series classifiers. The swapping stage identifies salient variables by replacing each variable's values with corresponding values from training data of the target class, measuring prediction drops to determine importance. The sliding stage then detects salient sub-sequences by sliding a window over each time step and replacing entire sub-sequences with training data, calculating importance scores that combine prediction drop magnitude, window size (with exponential decay), and temporal roles (current vs neighbor steps). The method uses k-nearest neighbors to find similar training samples and applies thresholds to determine when swaps produce significant prediction changes.

## Key Results
- SSET achieves 0.1 precision on WESAD dataset versus 0.02 for LIME, 0.04 for integrated gradients, and 0.02 for Dynamask
- SSET achieves 0.07 precision on MAHNOB-HCI dataset versus 0.02 for LIME, 0.03 for integrated gradients, and 0.01 for Dynamask
- SSET explanations are more informative, focusing on smaller sub-sequences (average length 6) compared to LIME (20), IG (14), and Dynamask (8)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SSET improves explanation precision by using neighboring training data to create in-distribution perturbations.
- Mechanism: The swapping stage replaces signal values with close training samples from target classes, ensuring perturbations stay within the model's learned data distribution.
- Core assumption: Local perturbations within the training distribution preserve the model's decision boundaries and produce meaningful attribution scores.
- Evidence anchors:
  - [abstract] "The train data with target classes in Cl are then extracted from the dataset... To detect the important variables, the values of each signal s in xi is swapped with its counterparts in Xneighbors"
  - [section] "Using the train data in this process guarantees that the generated instances fall in an in-distribution space"
  - [corpus] Weak: No direct comparison of in-distribution vs out-of-distribution perturbations in related works.

### Mechanism 2
- Claim: SSET captures temporal context by sliding windows over sequences rather than analyzing individual time steps.
- Mechanism: The sliding stage replaces entire sub-sequences with corresponding training data, measuring prediction drops to identify salient intervals.
- Core assumption: Affect detection depends on contextual patterns across multiple time steps rather than isolated observations.
- Evidence anchors:
  - [abstract] "In the latter stage, the salient observations of these variables are explored by sliding a window over each time step"
  - [section] "we define a context or neighboring steps for each observation to scrutinize the variations and detect salient sub-sequences"
  - [corpus] Weak: While other works mention context, SSET is the only one explicitly using sliding windows over training data for attribution.

### Mechanism 3
- Claim: SSET's multi-factor importance scoring provides more accurate attribution by weighting prediction drops, window size, and temporal roles.
- Mechanism: Importance scores combine prediction drop magnitude, exponential decay of window size, and distinction between "current" vs "neighbor" time steps.
- Core assumption: Smaller, high-impact sub-sequences provide more informative explanations than larger, diffuse attributions.
- Evidence anchors:
  - [section] "we take the impact of the window size into account in each time step... smaller salient sub-sequences are expected to be more informative than larger ones"
  - [section] "distinguish the role of steps... The time step t′ over which the window is constructed takes the role of 'current', while other steps in the window are considered as 'neighbor'"
  - [corpus] Weak: No comparable multi-factor scoring approaches found in related works.

## Foundational Learning

- Concept: Multivariate time series data structure and conflation of time and variables
  - Why needed here: Understanding why traditional explanation methods fail on time series data
  - Quick check question: Why can't standard image-based explanation methods directly apply to time series data?

- Concept: Local explanation vs global explanation in machine learning
  - Why needed here: SSET is specifically designed for local, instance-level explanations
  - Quick check question: What's the difference between explaining why a model classified one instance versus explaining the model's overall behavior?

- Concept: Differentiability requirements in gradient-based explanation methods
  - Why needed here: Understanding why methods like IG and Dynamask fail on non-differentiable models like CN-Waterfall
  - Quick check question: Why do gradient-based explanation methods require the underlying model to be differentiable?

## Architecture Onboarding

- Component map: Data preprocessing → CN-Waterfall black-box classifier → SSET explanation pipeline (swapping stage → sliding stage → importance scoring)
- Critical path: Instance selection → Neighborhood sampling → Swapping → Sliding → Scoring → Explanation output
- Design tradeoffs: SSET trades computational efficiency for explanation accuracy by using multiple sampling iterations and window size adjustments
- Failure signatures: Explanations dominated by single time step (context too small), uniform importance scores across all variables (neighborhood too broad), or no salient sequences found (threshold too high)
- First 3 experiments:
  1. Run SSET on a single test instance from WESAD with default parameters and visualize the heatmap
  2. Compare SSET output with LIME on the same instance to demonstrate contextual advantage
  3. Test SSET with different threshold values (thrc) to find optimal precision-informativeness balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can SSET's computational time be reduced without sacrificing explanation quality?
- Basis in paper: [explicit] The paper states that SSET requires more computational time than benchmarks, especially for the WESAD dataset, taking about 3 hours compared to 1 minute for LIME and IG.
- Why unresolved: The paper identifies this as a limitation and suggests future work but does not provide concrete solutions.
- What evidence would resolve it: Experimental results demonstrating successful implementation of heuristics or efficient search algorithms that significantly reduce SSET's runtime while maintaining explanation quality.

### Open Question 2
- Question: How does SSET perform on time series data from domains other than affect detection?
- Basis in paper: [explicit] The paper suggests future work to "investigate SSET's quality on other time series and applications" but does not provide any results.
- Why unresolved: The current study is limited to affect detection using physiological time series data.
- What evidence would resolve it: Comparative evaluation of SSET against benchmarks on diverse time series datasets from various domains (e.g., finance, healthcare, IoT) showing consistent performance advantages.

### Open Question 3
- Question: What is the optimal threshold value for the prediction drop in the swapping stage that balances explanation precision and computational efficiency?
- Basis in paper: [explicit] The paper uses a threshold of 0.5 but mentions that "any desired value could be initialized" and suggests examining different values in future work.
- Why unresolved: The current study uses a single threshold value without exploring its sensitivity or optimality.
- What evidence would resolve it: Systematic analysis showing how different threshold values affect explanation precision, computational time, and the quality of generated explanations across multiple datasets.

### Open Question 4
- Question: How does SSET handle privacy-preserving settings where time series data cannot be fully disclosed?
- Basis in paper: [explicit] The paper raises this as a future research direction, noting that "explanability distorts privacy" and SSET should be examined under privacy-aware settings.
- Why unresolved: The current study assumes full access to time series data and does not address privacy constraints.
- What evidence would resolve it: Demonstration of SSET's performance and explanation quality when applied to differentially private time series data or when only partial data access is available.

## Limitations
- Computational inefficiency compared to benchmark methods, particularly for larger datasets like WESAD
- Reliance on training data availability for creating in-distribution perturbations
- Limited evaluation to affect detection domain with physiological time series data

## Confidence
- High confidence: SSET's overall superiority in precision and informativeness compared to benchmarks (0.1 vs 0.02 precision on WESAD)
- Medium confidence: The effectiveness of the swapping stage in maintaining in-distribution perturbations
- Medium confidence: The sliding window approach's ability to capture temporal context in affect detection

## Next Checks
1. Test SSET on instances where affect states change rapidly (< 1-second transitions) to evaluate sliding window limitations
2. Compare SSET explanations with human expert annotations on a subset of WESAD data to validate interpretability claims
3. Conduct ablation studies removing the exponential window size penalty to assess its actual contribution to explanation quality