---
ver: rpa2
title: Empirical Capacity Model for Self-Attention Neural Networks
arxiv_id: '2407.15425'
source_url: https://arxiv.org/abs/2407.15425
tags:
- capacity
- arxiv
- number
- transformer
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an empirical capacity model (ECM) for self-attention
  neural networks, specifically transformers. The problem addressed is the gap between
  theoretical and practical memory capacity of large transformer models, which have
  billions of parameters but fall short of their theoretical capacity due to optimization
  algorithms and data-dependent factors.
---

# Empirical Capacity Model for Self-Attention Neural Networks

## Quick Facts
- arXiv ID: 2407.15425
- Source URL: https://arxiv.org/abs/2407.15425
- Authors: Aki Härmä; Marcin Pietrasik; Anna Wilbik
- Reference count: 40
- Key outcome: Empirical capacity model (ECM) for transformers that predicts memorization capacity with 0.497 MAPE

## Executive Summary
This paper presents an empirical capacity model (ECM) for self-attention neural networks, specifically transformers. The model addresses the gap between theoretical and practical memory capacity of large transformer models by providing a computational method to measure capacity through training on synthetic data. The ECM enables principled hyperparameter selection for designing task-specific transformer architectures with optimal parameter counts.

## Method Summary
The method involves training transformer models of different sizes using synthetic autoregressive data and measuring their memorization capacity. The capacity is defined as the number of sequences fully memorized by the model. Using this data, the authors fit an empirical capacity model that predicts expected capacity based on hyperparameters: number of attention heads (H), sequence length (N), and token vector size (B). The model is formulated as C = max(f(H, N) * B, α * H + β), where f(H, N) is a saturation function of H and N, and α and β are learned parameters.

## Key Results
- ECM achieves MAPE of 0.497 compared to 1.0785 for fifth-order polynomial model
- Model can predict capacity of unseen transformer architectures
- Provides lower bound for storage capacity, enabling principled hyperparameter selection
- Outperforms polynomial function at fraction of trainable parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-attention networks memorize sequences by using weighted sum operation in attention circuit as associative memory
- Mechanism: Self-attention computes weighted sum of input vectors based on content similarity, mapping input sequences to memorized patterns. This can be rewritten as quadratic form enabling efficient memorization.
- Core assumption: Associative memory capacity of self-attention is primarily determined by number of parameters in attention heads
- Evidence anchors: [abstract] "The self-attention circuit can be considered as an associative memory that has a certain capacity determined by the number of parameters."
- Break condition: If input dimensionality exceeds parameter capacity, or if content is too diverse for learned attention weights to distinguish

### Mechanism 2
- Claim: ECM provides lower bound for storage capacity, allowing principled hyperparameter selection
- Mechanism: ECM models relationship between memorized sequences and hyperparameters (H, N, B) using max function of linear rise and saturation function
- Core assumption: Marginal effects of hyperparameters on capacity can be captured by low-order algebraic functions
- Evidence anchors: [abstract] "The empirical capacity model (ECM) for a generic transformer. The ECM can be used to design task-specific transformer models with an optimal number of parameters."
- Break condition: If training data or task characteristics significantly deviate from synthetic data used to derive ECM

### Mechanism 3
- Claim: Larger batch sizes improve memorization capacity up to saturation point
- Mechanism: Increasing batch size reduces gradient noise, allowing model to better learn and memorize training sequences
- Core assumption: Impact of batch size on capacity follows saturation curve
- Evidence anchors: [section] "The number of MAC memorized sequences of 32 or 128 tokens, as a function of batch size over a long training time, is shown in Fig. 6."
- Break condition: If batch size becomes too large, causing memory constraints or reduced gradient diversity

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanism
  - Why needed here: Understanding core components and their roles is crucial for interpreting capacity model and experimental results
  - Quick check question: What is the key operation in self-attention circuit that enables memorization?

- Concept: Associative memory and Hopfield networks
  - Why needed here: Paper draws parallels between transformer models and associative memories, providing context for capacity analysis
  - Quick check question: How does storage capacity of Hopfield network relate to its number of nodes?

- Concept: Empirical modeling and curve fitting
  - Why needed here: ECM is derived from experimental results using curve fitting techniques, requiring understanding of this process
  - Quick check question: What are key steps in developing empirical model from experimental data?

## Architecture Onboarding

- Component map: Input sequence -> self-attention computation -> weighted sum of input vectors -> memorization of patterns
- Critical path: Input sequence → self-attention computation → weighted sum of input vectors → memorization of patterns
- Design tradeoffs: More attention heads increase capacity but also parameter count; larger batch sizes improve memorization up to saturation point
- Failure signatures: Underfitting (low capacity) or overfitting (memorizing noise) due to inappropriate hyperparameter choices
- First 3 experiments:
  1. Train small transformer on synthetic data with varying sequence lengths and measure memorization capacity
  2. Compare capacity of models with trained vs. frozen feedforward layers to isolate contribution of self-attention
  3. Vary batch size and training time to observe impact on memorization capacity

## Open Questions the Paper Calls Out

- Question: How does ECM generalize to natural language data compared to synthetic data?
  - Basis in paper: [explicit] Authors plan to expand data to include natural language content in future work
  - Why unresolved: Current model validated on synthetic data only; authors acknowledge need to validate on natural language content
  - What evidence would resolve it: Empirical testing of ECM on variety of natural language datasets, comparing predicted capacity against actual performance metrics

- Question: What is impact of increasing number of layers in transformer model on empirical capacity?
  - Basis in paper: [explicit] Authors mention plans to introduce effects of number of network layers in future work
  - Why unresolved: Current experiments focus on single-layer networks; relationship between depth and capacity not explored
  - What evidence would resolve it: Experimental results showing empirical capacity of multi-layer transformer models, updated ECM incorporating layer count

- Question: How does choice of activation function (e.g., GELU vs. ReLU) affect empirical capacity?
  - Basis in paper: [inferred] Paper mentions GELU activation in context of experiments but does not explore impact of different activation functions
  - Why unresolved: Experiments use fixed activation function; authors do not discuss or test sensitivity of ECM to this choice
  - What evidence would resolve it: Comparative experiments using different activation functions, measuring empirical capacity and updating ECM

## Limitations

- Model derived exclusively from synthetic autoregressive data, generalizability to real-world datasets remains untested
- Capacity measurements rely on computationally expensive MAC method that may not scale well to larger models
- Experimental validation limited to relatively small models (up to 3M parameters), uncertainty about behavior at scale

## Confidence

**High Confidence:**
- Self-attention mechanism can be modeled as associative memory with capacity constraints
- Larger batch sizes improve memorization up to saturation point
- ECM provides reasonable lower bound for storage capacity within tested parameter ranges

**Medium Confidence:**
- Specific functional form of ECM is optimal
- Model's predictions will generalize to real-world datasets and larger architectures
- ECM will remain valid as transformer architectures evolve

**Low Confidence:**
- Exact parameter values will transfer to all transformer variants
- Capacity model will hold for non-autoregressive tasks or different training objectives
- Relationship between parameters and capacity will remain stable across different optimization algorithms

## Next Checks

1. **Real Dataset Validation**: Test ECM on established benchmarks like language modeling (WikiText, BookCorpus) and machine translation datasets. Measure whether predicted capacities align with actual memorization performance.

2. **Scaling Laws Verification**: Evaluate ECM's predictions for models significantly larger than tested (100M+ parameters). Check absolute capacity predictions and relative effects of hyperparameters at scale.

3. **Architecture Generalization**: Apply ECM to transformer variants with different attention mechanisms (Longformer, Performer, sparse attention). Test whether capacity model's assumptions hold for architectures designed to overcome standard transformer limitations.