---
ver: rpa2
title: Continual Deep Reinforcement Learning with Task-Agnostic Policy Distillation
arxiv_id: '2411.16532'
source_url: https://arxiv.org/abs/2411.16532
tags:
- learning
- tasks
- task
- phase
- task-agnostic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a task-agnostic policy distillation framework
  to enable continual reinforcement learning without clear task boundaries or extrinsic
  rewards. It extends the Progress & Compress algorithm with a new task-agnostic phase,
  where an agent explores environments using intrinsic motivation derived from self-supervised
  prediction error, and distills this exploratory behavior into a knowledge base.
---

# Continual Deep Reinforcement Learning with Task-Agnostic Policy Distillation

## Quick Facts
- **arXiv ID**: 2411.16532
- **Source URL**: https://arxiv.org/abs/2411.16532
- **Reference count**: 40
- **Primary result**: Introduces task-agnostic policy distillation for continual RL without task boundaries or extrinsic rewards, showing improved sample efficiency and forward transfer on five Atari games.

## Executive Summary
This paper presents a task-agnostic policy distillation framework that enables continual reinforcement learning without requiring explicit task boundaries or extrinsic rewards. The approach extends the Progress & Compress algorithm by introducing a task-agnostic exploration phase where agents maximize intrinsic motivation derived from self-supervised prediction error. The exploratory policies learned during this phase are distilled into a knowledge base, which is then used to accelerate learning in downstream tasks. Experiments on five Atari games demonstrate superior performance compared to Online EWC and Progressive Networks in terms of average score, entropy stability, and sample efficiency.

## Method Summary
The method alternates between task-agnostic exploration and progress phases. During the task-agnostic phase, an agent explores environments using intrinsic motivation from forward model prediction error, systematically seeking novel states without external rewards. After exploration periods, the active column's policy is distilled into a knowledge base using KL divergence while Online EWC protects previously learned parameters. During progress phases, lateral connections from the knowledge base enable positive forward transfer by reusing distilled exploratory features. The framework uses A2C for policy/value learning and processes 84x84 grayscale frames with 4-frame stacking from Atari 2600 games.

## Key Results
- Outperforms Online EWC and Progressive Networks on five Atari games in terms of average score and entropy stability
- Demonstrates improved sample efficiency through effective knowledge transfer from task-agnostic exploration
- Shows better scalability and adaptability without requiring explicit task labels compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-agnostic phase enables effective exploration without external rewards, leading to better initial knowledge that accelerates downstream learning
- Mechanism: Intrinsic motivation from self-supervised prediction error guides systematic exploration of novel states
- Core assumption: Forward model prediction error sufficiently correlates with true environmental novelty
- Break condition: Forward model may fail to detect novelty in predictable but suboptimal states

### Mechanism 2
- Claim: Policy distillation compresses multiple exploratory policies into a single knowledge base, enabling scalability
- Mechanism: Active column policies are distilled into knowledge base using KL divergence while EWC protects parameters
- Core assumption: Knowledge base can integrate multiple exploratory policies without interference
- Break condition: Distillation may fail to preserve most useful exploratory features

### Mechanism 3
- Claim: Lateral connections from knowledge base enable positive forward transfer by reusing learned features
- Mechanism: Active column receives feature inputs from knowledge base during progress phases
- Core assumption: Features from task-agnostic exploration are general enough for downstream tasks
- Break condition: Distilled features may be too task-specific or irrelevant for transfer

## Foundational Learning

- **Intrinsic motivation and curiosity-driven exploration**
  - Why needed: Task-agnostic phase operates without external rewards
  - Quick check: How does forward model prediction error serve as novelty proxy?

- **Policy distillation and knowledge transfer**
  - Why needed: Compress multiple exploratory behaviors into scalable knowledge base
  - Quick check: What loss ensures distilled policy matches active column while preserving old knowledge?

- **Catastrophic forgetting and regularization techniques**
  - Why needed: Retain knowledge across multiple exploration periods and tasks
  - Quick check: How does Online EWC balance learning new tasks while protecting previous knowledge?

## Architecture Onboarding

- **Component map**: Meta-Environment → Active Column → Knowledge Base → Forward Model (ICM) → Lateral Connections
- **Critical path**: Task-agnostic phase → Distillation into KB → Downstream progress phase with lateral connections → Evaluation
- **Design tradeoffs**: 
  - More frequent distillation vs. longer exploration periods
  - Higher intrinsic reward weight vs. distraction from downstream tasks
  - Larger knowledge base capacity vs. computational efficiency
- **Failure signatures**: 
  - Stagnant intrinsic reward indicates exploration plateaued
  - Decreasing downstream performance suggests poor distilled knowledge
  - High variance across tasks indicates poor knowledge retention
- **First 3 experiments**:
  1. Run task-agnostic phase on single environment to verify intrinsic reward generation
  2. Test distillation by comparing active column performance before/after compression
  3. Evaluate downstream transfer by measuring learning speed after task-agnostic pre-training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does task-agnostic phase scale with increasing number of tasks in terms of computational efficiency and memory usage?
- Basis: Paper mentions scalability advantages but lacks extensive analysis on large task sets
- Why unresolved: Experiments limited to five Atari games
- What evidence would resolve it: Testing on 20-50 tasks with varying configurations

### Open Question 2
- Question: How does forward model prediction accuracy affect exploration efficiency in task-agnostic phase?
- Basis: Paper uses prediction error for intrinsic reward but doesn't analyze model accuracy impact
- Why unresolved: No analysis of relationship between forward model performance and exploration effectiveness
- What evidence would resolve it: Comparative experiments varying forward model accuracy

### Open Question 3
- Question: Can the framework be effectively applied to domains with continuous action spaces like robotics?
- Basis: Paper focuses on discrete action spaces in Atari games
- Why unresolved: No discussion of adaptation for continuous action spaces
- What evidence would resolve it: Implementation on robotic control tasks with continuous actions

## Limitations
- Limited scalability analysis beyond five tested Atari games
- Lack of ablation studies on intrinsic reward weight sensitivity and distillation frequency
- Evaluation still requires task boundaries despite claims of task-agnosticism

## Confidence
- **High confidence**: Core mechanism of combining task-agnostic exploration with policy distillation
- **Medium confidence**: Empirical results demonstrating performance gains
- **Low confidence**: Claims about scalability and adaptability without task labels

## Next Checks
1. Run ablation studies varying intrinsic reward weight, distillation frequency, and knowledge base size
2. Test framework on larger set of diverse environments (e.g., ProcGen suite) to validate scalability
3. Implement truly task-agnostic evaluation protocol without predefined task boundaries