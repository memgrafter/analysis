---
ver: rpa2
title: 'Improved Emotional Alignment of AI and Humans: Human Ratings of Emotions Expressed
  by Stable Diffusion v1, DALL-E 2, and DALL-E 3'
arxiv_id: '2405.18510'
source_url: https://arxiv.org/abs/2405.18510
tags:
- emotional
- emotions
- alignment
- emotion
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a quantitative evaluation of emotional alignment
  in generative AI models (DALL-E 2, DALL-E 3, Stable Diffusion v1). Using human ratings,
  the study compares how well these models express 10 distinct emotions (5 positive,
  5 negative) in images of people versus robots.
---

# Improved Emotional Alignment of AI and Humans: Human Ratings of Emotions Expressed by Stable Diffusion v1, DALL-E 2, and DALL-E 3

## Quick Facts
- arXiv ID: 2405.18510
- Source URL: https://arxiv.org/abs/2405.18510
- Reference count: 23
- This paper presents a quantitative evaluation of emotional alignment in generative AI models (DALL-E 2, DALL-E 3, Stable Diffusion v1) using human ratings.

## Executive Summary
This study evaluates how well three leading generative AI models express emotions in images, comparing DALL-E 2, DALL-E 3, and Stable Diffusion v1. Using human ratings across 10 distinct emotions in both person and robot contexts, the research establishes a benchmarking approach for measuring emotional alignment. The key finding is that DALL-E 3 achieves significantly higher alignment with human perceptions than earlier models, with mean scores of 7.04 compared to 5.79 for DALL-E 2 and 2.13 for Stable Diffusion. The work demonstrates substantial improvements in newer models while identifying remaining gaps in expressing certain emotions and contexts.

## Method Summary
The study generated 240 images (4 per prompt) using 10 emotion prompts (5 positive, 5 negative) in person and robot contexts across three AI models. Twenty-four human raters from Prolific rated each image's alignment with its prompt on a 0-10 scale. The experimental design used a within-subjects approach with three-way repeated-measures ANOVA to analyze effects of AI model, context, and emotion on alignment scores. Participants completed training trials and attention checks before rating the main dataset.

## Key Results
- DALL-E 3 achieved mean alignment scores of 7.04, significantly outperforming DALL-E 2 (5.79) and Stable Diffusion (2.13)
- Emotional expressions involving people were rated more aligned than those involving robots
- Certain emotions like shock and surprise showed higher alignment than resentment or affection
- The study establishes a benchmarking approach for measuring emotional alignment in AI systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Improved emotional alignment stems from advances in text-to-image diffusion model architectures and training data quality.
- Mechanism: DALL-E 3's higher alignment scores result from architectural refinements and richer, more diverse training datasets that capture finer emotional nuances in facial expressions and body language.
- Core assumption: The diffusion model architecture inherently improves with more sophisticated pretraining data and model scale.
- Evidence anchors: DALL-E 3 achieves significantly higher alignment with human perceptions than earlier models, with mean scores of 7.04 compared to 5.79 for DALL-E 2 and 2.13 for Stable Diffusion.

### Mechanism 2
- Claim: Human raters' cultural and linguistic background significantly influences emotional alignment ratings.
- Mechanism: Participants from the US and UK, fluent in English, share cultural norms that make certain emotional expressions more interpretable, leading to higher alignment scores for emotions like shock or surprise versus resentment or affection.
- Core assumption: Emotional expression and interpretation are culturally mediated, and the study's participant pool reflects a narrow cultural lens.
- Evidence anchors: Participants recruited from the Prolific website... We limited participation to the US and UK and to fluent English speakers.

### Mechanism 3
- Claim: Emotion type and arousal level modulate the difficulty of accurate AI expression.
- Mechanism: High-arousal emotions (shock, surprise) are easier for AI to express because they have more distinct, universally recognizable visual cues, whereas low-arousal emotions (affection, resentment) require subtler, context-dependent cues that are harder to generate and align.
- Core assumption: Arousal level correlates with the distinctiveness of visual emotional cues, and AI models capture this difference in alignment.
- Evidence anchors: Emotional expressions involving people were rated more aligned than those involving robots, and certain emotions like shock and surprise were easier to align than resentment or affection.

## Foundational Learning

- Concept: Emotional granularity
  - Why needed here: The study's framework is grounded in measuring the ability of AI to express fine-grained emotional distinctions, which requires understanding what emotional granularity means in human psychology.
  - Quick check question: Can you define emotional granularity and explain why it matters for AI-generated emotional content?

- Concept: Human-computer interaction (HCI) and affective computing
  - Why needed here: The research intersects with designing AI systems that interact naturally with humans, requiring knowledge of how emotions are expressed and perceived in digital interfaces.
  - Quick check question: How do affective computing principles apply to evaluating the alignment of AI-generated emotional imagery?

- Concept: Experimental design in human-subject studies
  - Why needed here: The study uses a within-subjects experimental design with human ratings; understanding control, randomization, and statistical analysis is essential for interpreting results.
  - Quick check question: What are the advantages and potential pitfalls of using crowdworkers for subjective ratings in AI alignment studies?

## Architecture Onboarding

- Component map: Prompt generation -> Image synthesis (Stable Diffusion v1, DALL-E 2, DALL-E 3) -> Human rating collection -> Statistical analysis pipeline
- Critical path: Prompt generation → Image synthesis → Human rating collection → Statistical analysis → Interpretation
- Design tradeoffs: Using crowdworkers speeds data collection but introduces noise; limiting cultural background increases consistency but reduces generalizability; automated image generation ensures scale but may miss edge cases
- Failure signatures: Low inter-rater reliability, systematic bias toward certain emotions or contexts, failure to detect significant differences between models
- First 3 experiments:
  1. Run a pilot with a small, diverse group to check rating clarity and inter-rater reliability
  2. Test the same prompts across two different AI models to confirm the rating interface captures meaningful differences
  3. Replicate the within-subjects design with a larger sample to validate statistical power and effect sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors in the image generation prompts (beyond the emotion and subject context) influence the emotional alignment ratings, and how can these be systematically controlled?
- Basis in paper: The paper mentions that images were generated using "default settings" for DALL-E and Stable Diffusion, but doesn't analyze the impact of different prompt engineering techniques, style specifications, or parameter settings on emotional alignment.
- Why unresolved: The study used relatively simple prompts without exploring how variations in prompt structure, style keywords, or generation parameters might affect the resulting emotional expressions.
- What evidence would resolve it: Comparative studies testing different prompt engineering approaches (e.g., adding style descriptors, specifying artistic styles, adjusting generation parameters) while keeping the core emotion and subject constant would reveal which prompt elements most strongly influence emotional alignment.

### Open Question 2
- Question: How does the emotional alignment of AI-generated images compare to human-generated drawings of the same emotions and contexts?
- Basis in paper: The paper establishes that AI models vary in their emotional alignment capabilities but doesn't provide a human baseline for comparison, leaving unclear whether the alignment scores reflect genuine emotional understanding or simply mimicry of learned patterns.
- Why unresolved: Without a human-generated baseline, it's difficult to determine whether the observed alignment scores represent meaningful emotional expression or merely pattern matching to training data.
- What evidence would resolve it: A controlled study where human participants create drawings of the same emotions and contexts, followed by the same alignment rating procedure, would establish whether AI-generated emotional expressions reach or fall short of human performance levels.

### Open Question 3
- Question: What is the relationship between an individual's emotional granularity and their ratings of AI-generated emotional alignment, and does this relationship vary across different emotions or contexts?
- Basis in paper: The authors acknowledge this as a limitation, noting they "did not assess the emotional expertise of the human raters" and that "not all people have the same degree of emotional granularity."
- Why unresolved: The study collected ratings from participants without measuring their individual capacity for emotional differentiation, making it impossible to determine whether alignment scores reflect the AI's performance or the raters' own emotional perception abilities.
- What evidence would resolve it: Including validated measures of emotional granularity in future studies and analyzing how individual differences correlate with alignment ratings would reveal whether certain emotions or contexts are more susceptible to rater variability.

## Limitations
- The use of a narrow participant pool (US and UK, English speakers only) introduces potential cultural bias in emotional interpretation
- The specific image generation prompts were not fully specified, making exact replication difficult
- The study did not control for individual differences in emotional perception or measure inter-rater reliability beyond attention checks

## Confidence
- **High Confidence**: The comparative ranking of models (DALL-E 3 > DALL-E 2 > Stable Diffusion) is well-supported by the statistical analysis and consistent with broader trends in AI development.
- **Medium Confidence**: The claim that certain emotions (shock, surprise) are easier to align than others (resentment, affection) is supported by the data but requires further investigation.
- **Low Confidence**: The attribution of alignment improvements to specific architectural or training advances is speculative.

## Next Checks
1. **Cross-cultural validation**: Replicate the study with participants from diverse cultural backgrounds to test whether the observed alignment patterns hold across different cultural contexts.
2. **Prompt variation study**: Systematically vary the emotional prompts (word choice, context, specificity) to determine how prompt engineering affects alignment scores and whether improvements can be achieved through better prompting rather than model architecture alone.
3. **Model ablation analysis**: Compare emotional alignment scores across different versions of the same model family (e.g., multiple Stable Diffusion versions) to isolate whether improvements stem from architectural changes or other factors like training data curation.