---
ver: rpa2
title: 'Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal
  LLMs'
arxiv_id: '2410.11437'
source_url: https://arxiv.org/abs/2410.11437
tags:
- questions
- mllms
- question
- laziness
- llav
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates a phenomenon called "model laziness" in
  multimodal large language models (MLLMs), where models fail at simple visual question-answering
  tasks despite correctly describing the same visual content. To study this, the authors
  create LazyBench, a benchmark containing image-description pairs and related questions
  (Yes/No, multiple-choice, short answer, and description tasks).
---

# Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs

## Quick Facts
- **arXiv ID**: 2410.11437
- **Source URL**: https://arxiv.org/abs/2410.11437
- **Reference count**: 10
- **Key outcome**: MLLMs fail simple visual tasks despite correct descriptions, with >50% laziness rates on Yes/No questions

## Executive Summary
This paper investigates a phenomenon called "model laziness" in multimodal large language models (MLLMs), where models fail at simple visual question-answering tasks despite correctly describing the same visual content. To study this, the authors create LazyBench, a benchmark containing image-description pairs and related questions (Yes/No, multiple-choice, short answer, and description tasks). Testing advanced MLLMs (e.g., GPT-4o, Claude 3, LLaVA-1.5) shows significant laziness: models perform well on description tasks but poorly on simpler questions, with laziness rates exceeding 50% for Yes/No questions in most models. The authors also find that 41.15% of LLaVA-1.5-13B's failures on the VQA-v2 benchmark are due to laziness. They propose a chain-of-thought method to mitigate laziness, improving accuracy by ~40% on simpler tasks.

## Method Summary
The authors construct LazyBench by selecting image pairs with high CLIP similarity (0.96-0.99) but clear human-visible differences, then generate questions about these differences. They evaluate MLLMs on this benchmark across four task types: Yes/No, multiple-choice, short answer, and description tasks. Laziness is measured as the proportion of cases where models fail simple tasks but correctly describe the related image content. The chain-of-thought mitigation involves prompting models to first generate detailed descriptions before answering simple questions.

## Key Results
- MLLMs show significant performance gaps: excelling at description tasks while failing Yes/No questions (>50% lazy rate for most models)
- GPT-4o exhibits the most severe laziness, with 64.29% lazy rate on Yes/No questions
- 41.15% of LLaVA-1.5-13B's failures on VQA-v2 are attributable to laziness
- Chain-of-thought prompting improves Yes/No accuracy by 24.76% for GPT-4o

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: MLLMs exhibit "model laziness" by failing simple visual tasks despite correct image descriptions.
- **Mechanism**: The model allocates less cognitive resources to simple tasks (e.g., Yes/No questions) that can be answered with few tokens, while expending more effort on complex descriptive tasks that require multiple decoding steps and image lookups.
- **Core assumption**: The model's token generation process is optimized for quick responses on low-effort tasks, leading to insufficient visual processing.
- **Evidence anchors**:
  - [abstract] "models tend to err when answering easy questions (e.g. Yes/No questions) about an image, even though they can correctly describe it."
  - [section 3.3] "most of the MLLMs perform their best on description tasks and have the worst responses on Yes/No questions."
  - [corpus] Weak - no direct evidence of token-level decision-making; inferred from performance discrepancy.
- **Break condition**: If task complexity and token requirements are decoupled (e.g., through CoT prompting), the laziness pattern disappears.

### Mechanism 2
- **Claim**: CoT prompting mitigates laziness by forcing the model to "work harder" on simple tasks.
- **Mechanism**: By requiring the model to first generate a detailed description before answering a simple question, CoT increases the number of image lookups and token generation steps, effectively making the simple task complex.
- **Core assumption**: The model's visual processing depth is proportional to the number of decoding steps and image attention calls.
- **Evidence anchors**:
  - [section 4.2] "after employing this CoT method, MLLMs exhibit significant improvements in both Yes/No and multiple-choice questions."
  - [section 4.2] "GPT-4o's accuracy in Yes/No questions increases by 24.76%."
  - [corpus] No direct evidence of image lookup counts; inferred from performance gains.
- **Break condition**: If the model learns to bypass CoT through fine-tuning or shortcut patterns, laziness may re-emerge.

### Mechanism 3
- **Claim**: LazyBench construction exploits CLIP's similarity encoding to identify image pairs with obvious visual differences that models misclassify as similar.
- **Mechanism**: By selecting image pairs with high CLIP similarity (0.96-0.99) but clear human-visible differences, the benchmark ensures that models will fail on simple questions about these differences while succeeding on descriptions.
- **Core assumption**: CLIP's similarity encoding correlates with model failure on simple visual distinctions.
- **Evidence anchors**:
  - [section 3.2] "we focused on 'similar image pairs' with a cosine similarity greater than 0.96 but smaller than 0.99."
  - [section 3.2] "if two images are encoded as similar vectors by CLIP but have clear visual differences, it indicates that at least one of the images had certain features incorrectly encoded or neglected."
  - [corpus] Weak - assumes CLIP similarity predicts model behavior without direct validation.
- **Break condition**: If models learn to overcome CLIP-based similarity encoding or if human-visible differences don't correlate with model failures.

## Foundational Learning

- **Concept**: Visual question answering (VQA) task structure
  - Why needed here: Understanding the difference between simple (Yes/No, multiple-choice) and complex (description) tasks is fundamental to grasping the laziness phenomenon.
  - Quick check question: Can you explain why a Yes/No question might require fewer image lookups than a description task?

- **Concept**: Multimodal model architecture (CLIP + LLM fusion)
  - Why needed here: The laziness mechanism relies on understanding how visual features are encoded and processed in MLLMs.
  - Quick check question: How does CLIP's similarity encoding potentially lead to model failures on simple visual distinctions?

- **Concept**: Chain-of-thought prompting methodology
  - Why needed here: The mitigation strategy depends on understanding how CoT forces additional processing steps.
  - Quick check question: Why would requiring a description before a Yes/No answer force the model to "look at the image more times"?

## Architecture Onboarding

- **Component map**: CLIP visual encoder → similarity matching → LazyBench image pair selection → MLLM backbone (LLaVA, GPT-4V, etc.) → task-specific prompting → output generation → CoT prompting module → intermediate description task → final answer generation → Evaluation pipeline → binary classification of description accuracy → laziness rate calculation

- **Critical path**: Image selection (CLIP similarity) → question generation → model inference → description accuracy check → laziness determination

- **Design tradeoffs**: 
  - LazyBench size vs. diversity: Small dataset (101 images) ensures manual quality control but limits generalizability
  - Binary evaluation vs. nuanced scoring: Simple correct/incorrect classification is reproducible but may miss partial understanding
  - CoT effectiveness vs. efficiency: Improves accuracy but adds computational overhead

- **Failure signatures**: 
  - High accuracy on descriptions but low accuracy on simple questions indicates laziness
  - Option bias (always answering "yes" or "no") indicates different failure mode than laziness
  - High laziness rate on strong models suggests the phenomenon is independent of overall capability

- **First 3 experiments**:
  1. Replicate LazyBench evaluation on a new MLLM to verify laziness consistency across models
  2. Test CoT prompting on a subset of LazyBench questions to measure improvement magnitude
  3. Create variant LazyBench with reversed task difficulty (simple descriptions, complex Yes/No) to test if laziness is task-type dependent

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training factors cause MLLMs to exhibit laziness in simple visual tasks despite correctly describing the same content?
- Basis in paper: [inferred] The paper observes that MLLMs perform poorly on simple Yes/No questions while accurately describing the same visual content, suggesting architectural or training-related causes that remain unexplored.
- Why unresolved: The paper focuses on measuring and mitigating laziness but does not investigate the underlying causes, stating "we will leave the in-depth exploration of laziness for the future."
- What evidence would resolve it: Analysis of attention patterns, training data distribution, or architectural differences between models showing varying levels of laziness could identify specific causes.

### Open Question 2
- Question: Does the laziness phenomenon extend beyond visual tasks to other modalities or mixed-modal tasks in MLLMs?
- Basis in paper: [explicit] The paper focuses exclusively on visual question-answering tasks but acknowledges MLLMs handle "complex multimodal tasks" generally.
- Why unresolved: The LazyBench benchmark only tests visual perception, leaving open whether similar laziness occurs in audio, text-only, or mixed-modal scenarios.
- What evidence would resolve it: Testing MLLMs on analogous simple vs. complex tasks across different modalities (e.g., audio description vs. yes/no questions about sound) would reveal if laziness is modality-specific or universal.

### Open Question 3
- Question: How does model size and capacity relate to the severity of laziness in MLLMs?
- Basis in paper: [explicit] The paper notes that "stronger models" like GPT-4o and Gemini-1.5-pro exhibit the "most severe lazy rates," suggesting a potential correlation between model capability and laziness.
- Why unresolved: While the paper observes this pattern, it does not systematically analyze whether laziness decreases with smaller models or if there's a threshold effect.
- What evidence would resolve it: Comparative analysis of laziness rates across a wider range of model sizes and architectures, including smaller and larger variants than those tested, would clarify the relationship between capacity and laziness.

## Limitations

- Small dataset (101 images) may limit generalizability despite ensuring manual quality control
- Binary evaluation framework may miss nuanced understanding of model behavior
- No investigation into whether models can learn to bypass CoT prompting through fine-tuning

## Confidence

- **High**: Existence of laziness phenomenon (consistent performance gaps across multiple MLLMs)
- **Medium**: CoT mitigation effectiveness (accuracy improvements shown but lacks direct evidence of increased visual processing)
- **Low**: Underlying causes of laziness (inferred from performance patterns rather than measured through architectural analysis)

## Next Checks

1. **Image lookup validation**: Instrument LLaVA-1.5 to count actual image attention calls during CoT prompting vs. direct answering, verifying whether CoT truly increases visual processing depth as claimed.

2. **Task reversal experiment**: Create a variant benchmark where simple tasks are descriptions and complex tasks are Yes/No questions about subtle differences, testing whether laziness is truly task-type dependent or reflects broader capability gaps.

3. **Cross-similarity encoding test**: Evaluate the same image pairs using different visual encoders (not just CLIP) to determine if model failures correlate with specific similarity encoding approaches or represent a more fundamental visual processing issue.