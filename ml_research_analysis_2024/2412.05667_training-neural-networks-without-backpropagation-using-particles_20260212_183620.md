---
ver: rpa2
title: Training neural networks without backpropagation using particles
arxiv_id: '2412.05667'
source_url: https://arxiv.org/abs/2412.05667
tags:
- loss
- neural
- network
- best
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a novel approach for training neural networks
  without backpropagation by treating each neuron as an independent entity optimized
  using Particle Swarm Optimization (PSO). Instead of propagating errors backward
  through the network, each neuron is isolated, and PSO particles explore the weight
  space to find optimal values based on minimizing a loss function computed with fixed
  weights from other neurons.
---

# Training neural networks without backpropagation using particles

## Quick Facts
- arXiv ID: 2412.05667
- Source URL: https://arxiv.org/abs/2412.05667
- Reference count: 40
- Primary result: PSO-based neuron optimization achieves accuracy comparable to backpropagation on small datasets

## Executive Summary
This paper presents a novel approach for training neural networks without backpropagation by treating each neuron as an independent entity optimized using Particle Swarm Optimization (PSO). Instead of propagating errors backward through the network, each neuron is isolated, and PSO particles explore the weight space to find optimal values based on minimizing a loss function computed with fixed weights from other neurons. The method addresses backpropagation limitations like vanishing gradients while overcoming PSO's search space constraints by decomposing the problem into smaller subproblems.

## Method Summary
The proposed method treats each neuron in a neural network as an independent optimization problem solved using Particle Swarm Optimization. During training, PSO particles explore the weight space of individual neurons while keeping other neurons' weights fixed. The loss function is computed using the current network configuration, and the best-performing particles are selected to update the neuron's weights. This process is repeated for all neurons in the network, and the collective optimization aims to minimize the overall network loss. The method eliminates the need for gradient computation and backpropagation, instead relying on PSO's population-based search to find optimal weights.

## Key Results
- PSO-based training achieves accuracy, precision, recall, and F1-score comparable to backpropagation on Rice and Dry Bean datasets
- The method successfully avoids vanishing gradient problems inherent in traditional backpropagation
- Experimental results demonstrate the approach works on both synthetic (linearly/nonlinearly separable) and real-world datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method avoids vanishing gradients by isolating neuron-level optimization instead of global backpropagation.
- Mechanism: Each neuron's weights are optimized independently using PSO particles, with other neurons' weights fixed during each sub-optimization step. This eliminates the need for gradient computation through deep networks where gradients can vanish.
- Core assumption: Fixing other neurons' weights during a neuron's optimization is a valid approximation that still allows the network to learn.
- Evidence anchors:
  - [abstract]: "The problem of vanishing gradients appearing in the backpropagation is avoided."
  - [section 4.1]: "The differentiation required for backpropagation is avoided through the PSO approach."
- Break condition: If the fixed-weights approximation becomes too inaccurate in deep networks or highly coupled architectures, the method may fail to converge.

### Mechanism 2
- Claim: Decomposing the network into neuron-level subproblems circumvents PSO's limited search space limitation.
- Mechanism: Instead of applying PSO to the entire network weight space (which could be huge), the method applies PSO to each neuron's weight vector separately. This dramatically reduces the dimensionality of each optimization problem.
- Core assumption: The collective optimization of individual neurons can approximate global optimization of the entire network.
- Evidence anchors:
  - [abstract]: "In the proposed approach, we overcome the problem of gradient descent and the limitation of the PSO algorithm by training individual neurons separately."
  - [section 4.1]: "The problem of vanishing gradients appearing in the backpropagation is avoided. However, the neural network weights used in the PSO search space were limited due to the local minima within the explored area. So, we split the neural network into individual neuron nodes as a group of particles in the proposed method."
- Break condition: If neuron-level optimization leads to conflicting weight updates that cannot be reconciled, the network may fail to converge.

### Mechanism 3
- Claim: The collective behavior of independently optimized neurons can approximate backpropagation's coordinated weight updates.
- Mechanism: Each neuron independently optimizes its weights to minimize loss while other weights are held fixed. The network-level loss acts as a control signal to select the best collective configuration of neuron weights.
- Core assumption: Independent neuron optimization with periodic global validation can approximate the coordinated updates achieved by backpropagation.
- Evidence anchors:
  - [abstract]: "Our proposed approach, we overcome the problem of gradient descent and the limitation of the PSO algorithm by training individual neurons separately, capable of collectively solving the problem as a group of neurons forming a network."
  - [section 5]: "The control of the individual nodes is difficult. When all the nodes are updated, we select the best weights for each node and combine them. All the node weights combination forms the best network weights for the problem."
- Break condition: If the independence assumption is too strong, neurons may optimize in conflicting directions, preventing convergence.

## Foundational Learning

- Concept: Particle Swarm Optimization (PSO)
  - Why needed here: PSO provides the mechanism for weight optimization without gradients, using a population-based search that can escape local minima.
  - Quick check question: How does PSO update particle positions and velocities based on personal and global best values?

- Concept: Neural network forward pass computation
  - Why needed here: The method requires computing forward passes with fixed weights to evaluate loss for each neuron's optimization, which is fundamental to understanding how the method works.
  - Quick check question: What happens during a forward pass when some neuron weights are fixed and others are being optimized?

- Concept: Loss function evaluation and validation
  - Why needed here: The method uses loss function values as the objective for PSO optimization and validation loss to control the global weight selection process.
  - Quick check question: How does the method use validation loss to prevent runaway optimization in individual neurons?

## Architecture Onboarding

- Component map: Neural network -> Individual neurons -> PSO particles per neuron -> Loss function evaluator -> Global validator
- Critical path: For each batch: 1) For each neuron: update PSO particles, compute loss with fixed other weights, select best particle; 2) Combine best weights across neurons; 3) Compute validation loss; 4) Update global best weights if validation loss improves
- Design tradeoffs: The method trades computational redundancy (repeated loss evaluations) for gradient-free optimization and parallelizability. It also trades coordinated backpropagation updates for independent neuron optimization.
- Failure signatures: Poor convergence when neuron-level optimization leads to conflicting weight directions, high computational cost due to repeated loss evaluations, sensitivity to PSO parameters (particle count, iteration count)
- First 3 experiments:
  1. Test on a simple linearly separable dataset with a single-layer perceptron to verify basic functionality and compare with backpropagation
  2. Test on a nonlinearly separable dataset with a small MLP to verify the method can handle nonlinearity without backpropagation
  3. Test on a multi-class dataset with increasing complexity to evaluate scalability and identify computational bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can functional mappings between loss values and node weights be established to reduce redundant computations in the proposed PSO-based training method?
- Basis in paper: [explicit] The paper explicitly identifies redundant computation of loss values as a limitation and suggests exploring functional mappings between loss values and node weights as a potential solution.
- Why unresolved: No functional mapping was developed or tested in the paper. The authors only proposed it as a direction for future work.
- What evidence would resolve it: Empirical demonstration of a computational speedup in training time or reduction in the number of loss evaluations needed to achieve comparable accuracy.

### Open Question 2
- Question: How does the proposed method scale to deeper architectures and larger datasets compared to traditional backpropagation?
- Basis in paper: [inferred] The paper tested the method on small synthetic and two real datasets but did not explore scaling behavior or performance on large-scale benchmarks common in deep learning.
- Why unresolved: The experiments were limited in scope, and no analysis of computational complexity or resource requirements for scaling was provided.
- What evidence would resolve it: Benchmarking the method on standard deep learning datasets (e.g., ImageNet, CIFAR) and comparing training time, memory usage, and accuracy against backpropagation-based models.

### Open Question 3
- Question: What are the limitations and potential benefits of using arbitrary neural network architectures with non-standard connectivity in the proposed method?
- Basis in paper: [explicit] The paper suggests the method allows for arbitrary neural network architectures with individual connections between nodes, resembling biological neuronal activities, but acknowledges this needs further study.
- Why unresolved: No experiments or analysis were conducted on non-standard network topologies or their impact on learning.
- What evidence would resolve it: Experimental results showing how unconventional architectures affect learning dynamics, convergence, and performance on benchmark tasks compared to standard layered networks.

## Limitations

- Computational redundancy from repeated loss function evaluations during PSO particle updates, with no quantification of the overhead compared to backpropagation
- Limited experimental scope focused on small-scale problems, with no validation on deeper networks or larger datasets
- No theoretical guarantees for convergence when optimizing neurons independently

## Confidence

- **High Confidence**: PSO can optimize individual neuron weights without backpropagation and avoid vanishing gradients
- **Medium Confidence**: Neuron-level optimization can approximate backpropagation's coordinated updates, though convergence assumptions need validation
- **Low Confidence**: The method can scale to larger networks or more complex problems based on limited experimental evidence

## Next Checks

1. Conduct a detailed analysis of the computational cost of the PSO-based approach compared to backpropagation, including training time and memory usage
2. Evaluate the method on larger, deeper networks and datasets to assess its performance and scalability limitations
3. Investigate the convergence behavior of the method under different PSO parameters and network architectures