---
ver: rpa2
title: 'MMLongBench-Doc: Benchmarking Long-context Document Understanding with Visualizations'
arxiv_id: '2407.01523'
source_url: https://arxiv.org/abs/2407.01523
tags:
- questions
- documents
- page
- document
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMLongBench-Doc, a benchmark for evaluating
  large vision-language models (LVLMs) on long-document understanding tasks. The benchmark
  includes 1,082 questions across 135 lengthy, multi-page documents with rich layouts
  and multi-modal components.
---

# MMLongBench-Doc: Benchmarking Long-context Document Understanding with Visualizations

## Quick Facts
- arXiv ID: 2407.01523
- Source URL: https://arxiv.org/abs/2407.01523
- Authors: Yubo Ma; Yuhang Zang; Liangyu Chen; Meiqi Chen; Yizhu Jiao; Xinze Li; Xinyuan Lu; Ziyu Liu; Yan Ma; Xiaoyi Dong; Pan Zhang; Liangming Pan; Yu-Gang Jiang; Jiaqi Wang; Yixin Cao; Aixin Sun
- Reference count: 40
- Key outcome: This paper introduces MMLongBench-Doc, a benchmark for evaluating large vision-language models (LVLMs) on long-document understanding tasks. The benchmark includes 1,082 questions across 135 lengthy, multi-page documents with rich layouts and multi-modal components. Experiments on 14 LVLMs and 10 LLMs reveal that most LVLMs struggle with long-context understanding, with the best-performing model (GPT-4o) achieving only 44.9% F1 score. Notably, LVLMs perform worse than LLMs when processing lossy OCR-parsed documents, highlighting the challenges of long-context document understanding.

## Executive Summary
This paper introduces MMLongBench-Doc, a comprehensive benchmark for evaluating large vision-language models (LVLMs) on long-context document understanding tasks. The benchmark features 135 lengthy PDF documents with an average of 49.4 pages and 20,971 textual tokens, along with 1,082 expert-annotated questions covering various document types and evidence sources. Through extensive experiments on 14 LVLMs and 10 LLMs, the study reveals that current LVLMs significantly struggle with long-context document understanding, with the best model (GPT-4o) achieving only 44.9% F1 score. The research highlights the need for improved LVLMs capable of handling complex, multi-modal, and lengthy documents.

## Method Summary
The benchmark employs a three-stage annotation pipeline: document collection from diverse sources, question/answer collection through expert annotators, and quality control via a three-round review process. Documents are sourced from various domains and include text, layout, charts, tables, and images. Questions are categorized into single-page, cross-page (33.7%), and unanswerable types (20.6%). The evaluation protocol consists of three steps: response generation using model-specific prompts, answer extraction through a rule-based system, and score calculation using metrics like generalized accuracy and F1 score. Both LVLMs and LLMs are evaluated, with LVLMs processing document screenshots and LLMs using OCR-parsed text for comparison.

## Key Results
- GPT-4o achieves the highest F1 score of 44.9% among 14 evaluated LVLMs on long-context document understanding
- LVLMs perform worse than LLMs when processing OCR-parsed documents, suggesting challenges with visual information processing
- 33.7% of questions require cross-page evidence, highlighting the difficulty of long-context reasoning
- Models struggle most with chart-based questions, particularly in interpreting axes and colored elements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark's construction process ensures high-quality, diverse questions through a three-stage annotation pipeline with expert annotators and semi-automatic quality control.
- Mechanism: Expert annotators undergo training and iterative review, then modify existing questions and add new ones while adhering to quality standards. A three-round quality control process involving document-relevant detection, self-reflection, and cross-checking further improves annotation quality.
- Core assumption: Expert annotators can consistently create and edit questions that meet the defined quality standards, and the quality control process effectively identifies and removes problematic annotations.
- Evidence anchors:
  - [abstract]: "Experiments on 14 LVLMs demonstrate that long-context DU greatly challenges current models."
  - [section]: "We detail each round in the following components and leave the discussion of potential bias in Appendix A.6."
- Break condition: If annotators cannot maintain consistency in question quality or the quality control process fails to identify significant errors, the benchmark's validity would be compromised.

### Mechanism 2
- Claim: The benchmark's design, including lengthy documents with rich layouts and multi-modal components, effectively evaluates the long-context understanding capabilities of LVLMs.
- Mechanism: Documents are sourced from diverse domains and include various formats (text, tables, charts, images, layout structures). Questions are categorized into single-page, cross-page, and unanswerable types, requiring evidence from different sources and locations within the documents.
- Core assumption: The selected documents and question types adequately represent the challenges of long-context document understanding and provide a comprehensive evaluation of LVLMs' capabilities.
- Evidence anchors:
  - [abstract]: "Distinct from previous datasets, it is constructed upon 135 lengthy PDF-formatted documents with an average of 49.4 pages and 20,971 textual tokens."
  - [section]: "Being a multi-modal benchmark, the answer to each question requires evidence from one or more of these five in-document sources: text, layout, chart, table, and image."
- Break condition: If the documents and questions do not accurately reflect real-world long-context document understanding tasks, the benchmark may not effectively evaluate LVLMs' capabilities.

### Mechanism 3
- Claim: The experimental setup, including the use of both LVLMs and LLMs, provides a fair comparison of their long-context document understanding abilities.
- Mechanism: LVLMs are evaluated on document screenshots in an end-to-end approach, while LLMs are fed with OCR-parsed documents for comparison. The evaluation protocol involves response generation, answer extraction, and score calculation using a rule-based system.
- Core assumption: The experimental setup allows for a meaningful comparison between LVLMs and LLMs, and the evaluation protocol accurately measures their performance on long-context document understanding tasks.
- Evidence anchors:
  - [abstract]: "Experiments on 14 LVLMs demonstrate that long-context DU greatly challenges current models. Notably, the best-performing model, GPT-4o, achieves an F1 score of only 42.7%, while the second-best, GPT-4V, scores 31.4%."
  - [section]: "We follow MATHVISTA [56] to conduct a three-step evaluation protocol: response generation, answer extraction, and score calculation."
- Break condition: If the experimental setup introduces biases or the evaluation protocol does not accurately measure performance, the comparison between LVLMs and LLMs may be misleading.

## Foundational Learning

- Concept: Document Understanding (DU)
  - Why needed here: DU is the core task being evaluated by the benchmark, and understanding its components and challenges is crucial for interpreting the results.
  - Quick check question: What are the main components of document understanding, and how do they relate to the challenges faced by LVLMs in long-context scenarios?

- Concept: Multi-modal Learning
  - Why needed here: The benchmark involves documents with various modalities (text, images, tables, charts, layout), and LVLMs need to effectively integrate information from these modalities for understanding.
  - Quick check question: How do LVLMs handle multi-modal information, and what are the challenges in integrating information from different modalities in long-context scenarios?

- Concept: Long-context Processing
  - Why needed here: The benchmark focuses on long documents, and understanding how LVLMs process and reason over extended contexts is essential for evaluating their performance.
  - Quick check question: What are the key challenges in long-context processing for LVLMs, and how do these challenges manifest in document understanding tasks?

## Architecture Onboarding

- Component map: Document collection -> Question/Answer collection -> Quality control -> Evaluation
- Critical path: The critical path involves collecting documents, annotating questions and answers, performing quality control, and conducting evaluations. Ensuring the quality and diversity of annotations is crucial for the benchmark's validity.
- Design tradeoffs: The benchmark prioritizes annotation quality over quantity, leading to a smaller scale compared to previous datasets. However, this tradeoff ensures that the questions are challenging and representative of real-world long-context document understanding tasks.
- Failure signatures: Potential failure modes include inconsistent question quality, inadequate representation of long-context challenges, and biased or inaccurate evaluation results. These failures can compromise the benchmark's effectiveness in evaluating LVLMs.
- First 3 experiments:
  1. Evaluate the performance of different LVLMs on a subset of the benchmark to identify initial trends and potential issues.
  2. Conduct a fine-grained analysis of LVLMs' performance on different document types, evidence sources, and evidence positions to gain deeper insights into their strengths and weaknesses.
  3. Perform an error analysis on a sample of incorrect responses to understand the common failure modes and areas for improvement in LVLMs' long-context document understanding capabilities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can long-context LVLMs be improved to better handle perceptual errors in charts and images within lengthy documents?
- Basis in paper: [explicit] The paper identifies perceptual errors as a significant issue, with LVLMs misinterpreting axes, colored circles in charts, and miscounting elements.
- Why unresolved: While the paper acknowledges this as a bottleneck, it does not propose specific solutions or techniques to address these perceptual challenges.
- What evidence would resolve it: Development and evaluation of new LVLMs trained specifically on diverse chart and image data, with metrics showing reduced perceptual errors compared to current models.

### Open Question 2
- Question: What are the most effective strategies to mitigate hallucinations in LVLMs when processing long documents with unanswerable questions?
- Basis in paper: [explicit] The paper notes that GPT-4o and Claude-3 Opus tend to provide false-positive answers and fabricate evidence, highlighting the issue of hallucinations.
- Why unresolved: The paper identifies the problem but does not explore or test methods to reduce hallucinations, such as confidence thresholds or enhanced training data.
- What evidence would resolve it: Comparative studies of LVLMs using different hallucination mitigation techniques, showing improved accuracy on unanswerable questions.

### Open Question 3
- Question: How does the performance of LVLMs on long-context document understanding tasks scale with increasing document length and complexity?
- Basis in paper: [explicit] The paper demonstrates that current LVLMs struggle with lengthy documents, but does not systematically explore how performance degrades with document length or complexity.
- Why unresolved: While the paper shows a performance gap, it lacks a detailed analysis of how specific factors like page count or token number impact LVLM performance.
- What evidence would resolve it: A series of experiments varying document length and complexity, with performance metrics showing the relationship between these factors and LVLM accuracy.

## Limitations

- The benchmark's relatively small scale (135 documents, 1,082 questions) compared to previous datasets, though this was intentionally chosen to prioritize annotation quality
- The evaluation protocol's reliance on a rule-based scorer with specific thresholds may not capture nuanced answer quality variations
- The comparison between LVLMs and LLMs using OCR-parsed text introduces a confounding factor, as LLMs receive structured text while LVLMs process raw visual information

## Confidence

- High confidence: The experimental finding that LVLMs significantly underperform on long-context document understanding tasks, with GPT-4o achieving only 44.9% F1 score. This is well-supported by the systematic evaluation across 14 LVLMs and 10 LLMs.
- Medium confidence: The claim that LVLMs perform worse than LLMs on OCR-parsed documents. While supported by experimental results, the confounding factor of structured versus unstructured input format limits the conclusiveness of this comparison.
- Low confidence: The assertion that the three-stage annotation pipeline ensures high-quality questions. The paper provides limited empirical evidence about the effectiveness of the quality control process beyond the fact that 898 new questions were annotated.

## Next Checks

1. Conduct ablation studies on the evaluation protocol by varying the ANLS threshold and scoring rules to assess sensitivity of results to these parameters.
2. Perform controlled experiments where both LVLMs and LLMs receive the same input format (either both visual or both OCR-parsed) to isolate the effect of model architecture from input representation.
3. Analyze inter-annotator agreement metrics and conduct a statistical validation of the quality control process to empirically verify the effectiveness of the three-stage annotation pipeline.