---
ver: rpa2
title: Measuring Spiritual Values and Bias of Large Language Models
arxiv_id: '2410.11647'
source_url: https://arxiv.org/abs/2410.11647
tags:
- llms
- spiritual
- religious
- values
- hate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper evaluates spiritual values and biases in large language
  models (LLMs) using two assessments: SP-Typology and SP-10Axes. Contrary to expectations,
  results show most LLMs display diverse spiritual values rather than secular tendencies.'
---

# Measuring Spiritual Values and Bias of Large Language Models

## Quick Facts
- arXiv ID: 2410.11647
- Source URL: https://arxiv.org/abs/2410.11647
- Reference count: 37
- Primary result: LLMs exhibit diverse spiritual values and fine-tuning on religious texts improves religion-targeted hate speech detection

## Executive Summary
This study evaluates spiritual values and biases in large language models using two psychometric assessments: SP-Typology and SP-10Axes. Contrary to expectations that LLMs would display secular tendencies, results show most models exhibit varied spiritual orientations ranging from highly religious to somewhat religious. The research further demonstrates that fine-tuning LLMs on religious texts significantly improves their performance in detecting hate speech targeting religious groups, with the greatest improvement (14%) observed when fine-tuning on the Vedas.

## Method Summary
The study employs two psychometric assessments adapted for LLM evaluation: SP-Typology (16 questions, 7 classes) and SP-10Axes (133 statements, 20 classes). These assessments categorize LLMs into spiritual value groups and measure correlation between them. The research uses a hate-speech-identity dataset with 21,053 discourses, mapping identities to six religious groups. Models are evaluated for hate speech detection performance across these groups, then fine-tuned on religious texts (Bible, Quran, Pali, Vedas, Tanakh) to measure performance changes.

## Key Results
- Most LLMs display spiritual values ranging from highly religious to somewhat religious, contrary to secular expectations
- Higher spiritual value models show better performance in religion-targeted hate speech detection
- Fine-tuning on religious texts improves hate speech detection F1-scores across all religious groups, with Vedas providing the highest improvement (14%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning LLMs on religious texts reduces bias in hate speech detection.
- Mechanism: Religious text fine-tuning adjusts internal representations to encode moral/spiritual norms aligned with fairness toward religious groups.
- Core assumption: Religious canons contain consistent moral frameworks that generalize to hate speech detection across faiths.
- Evidence anchors:
  - [abstract] "Empirical results demonstrate the effectiveness of this approach in mitigating spiritual bias."
  - [section] "Empirical results demonstrate the effectiveness of this approach in mitigating spiritual biases."
  - [corpus] Weak: no quantitative analysis of moral norms in training texts.
- Break condition: If religious texts encode group-specific exclusivity, fine-tuning may worsen intergroup hate detection.

### Mechanism 2
- Claim: LLMs inherently reflect spiritual values present in pre-training corpora.
- Mechanism: Distributed representations capture cultural-linguistic patterns, including spiritual leanings, from raw web and publication data.
- Core assumption: Pre-training data contains sufficient spiritual/religious content to influence latent embeddings.
- Evidence anchors:
  - [abstract] "LLMs trained on general text can exhibit varied spiritual values."
  - [section] "The findings reveal that the majority of the evaluated LLMs display either a somewhat or highly religious inclination."
  - [corpus] Weak: no analysis of proportion or distribution of religious content in corpora.
- Break condition: If spiritual content is sparse or overshadowed by secular language, spiritual value variation will be minimal.

### Mechanism 3
- Claim: Spiritual value alignment correlates with hate speech detection sensitivity.
- Mechanism: Models with stronger religious orientation encode moral sensitivity to religious identity, improving task performance.
- Core assumption: Higher religious value models internalize moral rules favoring religious protection.
- Evidence anchors:
  - [section] "Empirical, more religious LLMs tend to perform better in hate speech detection."
  - [table] "Models fine-tuned on the Vedas exhibit the highest improvement, with an increase of approximately 14 percent."
  - [corpus] Weak: no study of how moral norms map to hate speech task outputs.
- Break condition: If hate speech detection relies more on syntactic cues than moral framing, correlation breaks down.

## Foundational Learning

- Concept: Spiritual value measurement via psychometric questionnaires.
  - Why needed here: Provides quantitative proxy for LLM spiritual orientation to compare across models.
  - Quick check question: How would you adapt a human questionnaire for model use without direct experiential grounding?

- Concept: Bias mitigation through continued pre-training.
  - Why needed here: Demonstrates that fine-tuning on targeted corpora can shift model behavior in desired direction.
  - Quick check question: What evidence would you need to prove that bias reduction is due to content, not just additional training steps?

- Concept: Correlation analysis between model attributes and task performance.
  - Why needed here: Establishes whether spiritual values drive hate speech detection accuracy differences.
  - Quick check question: What confounds might explain performance differences aside from spiritual values?

## Architecture Onboarding

- Component map: SP-Assessments -> LLM generation -> Hate speech classifier -> F1 metric aggregation
- Critical path: Generate responses -> classify hate speech -> compute F1 -> aggregate by religion group
- Design tradeoffs: In-context learning vs. full fine-tuning; automatic classification vs. human annotation; broad vs. narrow religious focus
- Failure signatures: Inconsistent responses to identical prompts; classification layer mislabeling; metrics dominated by majority class
- First 3 experiments:
  1. Run SP-Typology/SP-10Axes on held-out LLM and verify score distribution matches prior results.
  2. Validate hate speech detection pipeline with synthetic examples spanning all religion groups.
  3. Compare baseline hate speech F1 before and after fine-tuning on a single religious text to confirm expected improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs' spiritual values vary across different languages and cultural contexts beyond English-language assessments?
- Basis in paper: [explicit] The paper uses English-language assessments (SP-Typology and SP-10Axes) and primarily evaluates English-trained models, but acknowledges limitations in cultural context representation.
- Why unresolved: The study only tested English-language models and assessments, leaving open whether results would generalize to multilingual or culturally-specific contexts.
- What evidence would resolve it: Replicating the study with multilingual LLMs and culturally-adapted assessments across different regions would show whether spiritual value patterns hold cross-culturally.

### Open Question 2
- Question: What specific mechanisms in pre-training data cause LLMs to develop spiritual values rather than remaining secular as hypothesized?
- Basis in paper: [inferred] The paper finds LLMs are more religious than expected, contradicting their hypothesis about secular pre-training corpora, but doesn't investigate which data sources drive this.
- Why unresolved: The study identifies that pre-training corpora contain spiritual content but doesn't analyze which specific datasets or text types contribute most to spiritual value development.
- What evidence would resolve it: Corpus analysis identifying religious/spiritual text proportions and types in pre-training data, combined with ablation studies removing different religious content sources.

### Open Question 3
- Question: Do spiritual values in LLMs represent genuine understanding or superficial pattern matching from training data?
- Basis in paper: [inferred] The paper treats LLM responses as genuine spiritual value indicators but acknowledges limitations in using behavior-based questionnaires with non-conscious agents.
- Why unresolved: The study assumes LLM responses reflect internal spiritual values, but LLMs may simply mimic patterns from religious text without genuine comprehension.
- What evidence would resolve it: Comparative studies testing LLMs on tasks requiring deeper spiritual reasoning versus pattern matching, or experiments measuring consistency of spiritual values across varied contexts.

## Limitations

- The study uses human-designed psychometric assessments that may not accurately capture LLM internal representations or genuine spiritual understanding
- The correlation between spiritual values and hate speech detection performance could be driven by confounds such as general language proficiency or familiarity with religious vocabulary
- Without corpus analysis of pre-training data, it's unclear whether observed spiritual values reflect genuine cultural patterns or measurement artifacts

## Confidence

- **High confidence**: The methodological framework for evaluating LLMs using psychometric assessments is clearly described and reproducible. The correlation between fine-tuning on religious texts and improved hate speech detection is supported by quantitative results (F1 improvements up to 14% with Vedas).
- **Medium confidence**: The interpretation that LLMs display diverse spiritual values rather than secular tendencies. While results show variation, the underlying cause (genuine cultural encoding vs. measurement artifacts) remains uncertain.
- **Low confidence**: The claim that spiritual value alignment causes better hate speech detection performance. The correlation could reflect multiple confounding factors, and the mechanism linking moral sensitivity to detection accuracy is not empirically established.

## Next Checks

1. **Cross-cultural validation**: Evaluate the same LLMs using spiritual value assessments adapted for different cultural contexts (e.g., Eastern vs. Western frameworks) to test whether the measured values reflect universal patterns or measurement artifacts.

2. **Controlled fine-tuning experiment**: Compare hate speech detection performance after fine-tuning on religious texts versus secular texts matched for vocabulary complexity and topic diversity to isolate whether improvements stem from spiritual content or general training effects.

3. **Human benchmarking study**: Administer the SP-Typology and SP-10Axes assessments to human subjects with similar demographic profiles to typical pre-training data authors, then compare score distributions with LLM results to assess whether the measurements capture meaningful cultural patterns.