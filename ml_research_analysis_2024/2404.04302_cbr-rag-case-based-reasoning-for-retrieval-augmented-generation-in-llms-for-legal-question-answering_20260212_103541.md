---
ver: rpa2
title: 'CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs for
  Legal Question Answering'
arxiv_id: '2404.04302'
source_url: https://arxiv.org/abs/2404.04302
tags:
- legal
- case
- retrieval
- text
- cases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CBR-RAG, which integrates Case-Based Reasoning
  (CBR) into Retrieval-Augmented Generation (RAG) systems to enhance the performance
  of Large Language Models (LLMs) for legal question-answering. CBR-RAG uses CBR's
  retrieval stage, indexing vocabulary, and similarity knowledge to augment LLM queries
  with contextually relevant cases, creating a richer prompt.
---

# CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs for Legal Question Answering

## Quick Facts
- arXiv ID: 2404.04302
- Source URL: https://arxiv.org/abs/2404.04302
- Reference count: 25
- Best variant achieved 1.94% increase in semantic similarity over baseline

## Executive Summary
This paper introduces CBR-RAG, a novel framework that integrates Case-Based Reasoning (CBR) into Retrieval-Augmented Generation (RAG) systems to enhance Large Language Models (LLMs) for legal question-answering tasks. By leveraging CBR's retrieval stage and similarity knowledge containers, CBR-RAG augments LLM queries with contextually relevant legal cases, creating richer prompts that lead to more accurate and semantically similar answers. The study evaluates different representation methods (general and domain-specific embeddings) and comparison methods (inter, intra, and hybrid similarity) on a legal QA task using the Open Australian Legal Question-Answering dataset, demonstrating significant improvements over baseline models.

## Method Summary
CBR-RAG combines Case-Based Reasoning with Retrieval-Augmented Generation by using CBR's retrieval mechanisms to find relevant legal cases that augment LLM prompts. The system employs dual-embedding representation (intra and inter embeddings) to capture both local attribute similarities and broader relevance across different attributes. A hybrid similarity weighting approach combines multiple similarity metrics to improve retrieval accuracy. The framework uses a casebase of legal Q&A pairs, neural embedding models (BERT, LegalBERT, AnglEBERT), and a k-NN retrieval engine to identify top-k relevant cases. These retrieved cases are then formatted into augmented prompts for the LLM generator (Mistral-7B), which produces answers evaluated using semantic similarity metrics.

## Key Results
- CBR-RAG with hybrid AnglEBERT variant and 3 nearest neighbors achieved best performance with 1.94% increase in semantic similarity
- Dual-embedding representation significantly improved retrieval accuracy compared to single-embedding approaches
- Hybrid similarity weighting outperformed both intra and inter embedding methods individually
- LegalBERT and AnglEBERT domain-specific embeddings showed better performance than general BERT in legal QA tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CBR-RAG improves semantic similarity by providing relevant case context to LLM queries
- Mechanism: The retrieval stage uses similarity knowledge containers to find and retrieve cases that are contextually relevant to the query, augmenting the original LLM prompt with richer context
- Core assumption: Retrieved cases contain information that is semantically similar and relevant to the query, enhancing answer quality
- Evidence anchors:
  - [abstract] "CBR-RAG uses CBR's retrieval stage, indexing vocabulary, and similarity knowledge containers to augment LLM queries with contextually relevant cases, creating a richer prompt."
  - [section 4.2] "The best matching case identified from intra-embedding retrieval is defined by: βk = top-k ci∈C, k Sim(f(Q), f(Qi))"
- Break condition: If retrieved cases are not semantically relevant or contain contradictory information, augmentation could degrade answer quality

### Mechanism 2
- Claim: Dual-embedding representation enables more flexible and effective case retrieval
- Mechanism: Intra-embeddings optimized for attribute matching while inter-embeddings designed for information retrieval scenarios where matching isn't restricted to like attributes
- Core assumption: Dual-embedding approach captures both local similarities between same attribute types and broader relevance across different attributes
- Evidence anchors:
  - [section 4.1] "Intra Embeddings, f(·), optimised for attribute matching... Inter Embeddings, g(·), designed for information retrieval (IR) scenarios"
  - [section 5.4] "Dual-embedding case representation, accommodating both forms of retrieval tasks, is given by: c = < f(Q), g(Q), g(S), g(E), A >"
- Break condition: If distinction between intra and inter embeddings isn't meaningful for the domain, complexity may not provide benefits

### Mechanism 3
- Claim: Hybrid similarity weighting combines multiple metrics to improve retrieval accuracy
- Mechanism: Weighted combinations of intra and inter-embedding similarities identify best matching cases
- Core assumption: Combining multiple similarity measures with appropriate weights captures more comprehensive relevance than single measures
- Evidence anchors:
  - [section 4.2] "Hybrid embedding retrieval involves combination of both intra and inter-embeddings... βk = top-k ci∈C, k (w1 · Sim(f(Q), f(Qi)) + w2 · Sim(g(Q), g(Si))+ w3 · Sim(g(Q), g(Ei)))"
  - [section 6.1] "We calculated results using F1-score for retrieval@k and visualised using a heat-map"
- Break condition: If weights aren't properly tuned for the domain, hybrid approach could perform worse than simpler approaches

## Foundational Learning

- Concept: Case-Based Reasoning (CBR) cycle and 4R framework (Retrieve, Reuse, Revise, Retain)
  - Why needed here: CBR-RAG leverages retrieval stage of CBR to find relevant cases that can augment LLM queries
  - Quick check question: What are the four stages of the CBR cycle and how does CBR-RAG utilize them?

- Concept: Text embeddings and similarity measures (cosine similarity, Euclidean distance)
  - Why needed here: CBR-RAG uses neural embeddings to represent cases and queries, and similarity measures to find relevant cases
  - Quick check question: How do cosine similarity and Euclidean distance differ in measuring text similarity?

- Concept: Large Language Models (LLMs) and their limitations in knowledge-intensive tasks
  - Why needed here: CBR-RAG addresses LLM limitations by providing external knowledge context for improved performance
  - Quick check question: What are key limitations of LLMs in knowledge-intensive tasks and how does RAG address them?

## Architecture Onboarding

- Component map:
  - Casebase -> Embedding models (BERT, LegalBERT, AnglEBERT) -> Similarity computation -> Retrieval engine (k-NN) -> LLM generator (Mistral-7B) -> Evaluation

- Critical path:
  1. Query received by CBR-RAG system
  2. Embedding model generates representations of query
  3. Similarity computation finds top-k relevant cases
  4. Retrieved cases formatted into augmented prompt
  5. LLM generates answer using augmented prompt
  6. Answer evaluated against reference for quality assessment

- Design tradeoffs:
  - Embedding model choice: General-purpose (BERT) vs domain-specific (LegalBERT) vs contrastive learning (AnglEBERT)
  - Similarity measure: Single metric vs hybrid weighted combination
  - Retrieval strategy: Intra-embedding only, inter-embedding only, or hybrid approach
  - Context inclusion: Support text only vs full case representation

- Failure signatures:
  - Low semantic similarity scores despite high retrieval scores
  - Generated answers that don't address query directly
  - Retrieval of irrelevant cases that confuse rather than help LLM
  - Performance degradation when switching between embedding models

- First 3 experiments:
  1. Implement baseline RAG with BERT embeddings and cosine similarity, evaluate semantic similarity scores
  2. Compare retrieval performance of BERT vs LegalBERT vs AnglEBERT on legal QA dataset
  3. Test hybrid similarity weighting schemes with different weight combinations to optimize retrieval accuracy

## Open Questions the Paper Calls Out

- Question: How does fine-tuning AnglEBERT and LegalBERT on ALQA-specific legal corpus impact their performance in CBR-RAG systems?
  - Basis in paper: [inferred] Authors note none of embedding methods were fine-tuned to ALQA-specific legal corpus and express interest in exploring impact of fine-tuning using contrastive self-supervision methods
  - Why unresolved: Current study didn't include fine-tuning of embedding models on specific ALQA corpus, leaving potential performance improvements untested
  - What evidence would resolve it: Conducting experiments where AnglEBERT and LegalBERT are fine-tuned on ALQA corpus and comparing their performance against non-fine-tuned versions

- Question: What are optimal strategies for case aggregation in CBR-RAG systems to maintain coherent prompts while combining multiple nearest neighbors?
  - Basis in paper: [explicit] Authors mention combining multiple neighbors while maintaining coherent prompt is challenging and plan to explore case aggregation strategies
  - Why unresolved: Current study didn't explore case aggregation strategies, leaving question of how to effectively combine multiple cases into coherent prompt unanswered
  - What evidence would resolve it: Developing and testing various case aggregation strategies in CBR-RAG systems, evaluating impact on coherence and quality of generated prompts

- Question: How do different embedding methods beyond BERT, LegalBERT, and AnglEBERT perform in CBR-RAG systems for legal question answering?
  - Basis in paper: [explicit] Authors express interest in exploring further opportunities within representation and specifically alternative methods for text embeddings in future work
  - Why unresolved: Current study focused on three specific embedding methods, performance of other embedding techniques remains unexplored
  - What evidence would resolve it: Conducting experiments with variety of embedding methods (Sentence-BERT, RoBERTa, domain-specific embeddings) and comparing their performance in CBR-RAG systems

## Limitations

- Evaluation conducted on single legal domain dataset (Australian Open Legal Question-Answering), may not generalize to other legal jurisdictions or domains
- Semantic similarity metric (cosine similarity) captures surface-level similarity but may not fully reflect practical utility of generated answers
- Study doesn't address computational efficiency or runtime performance of CBR-RAG system

## Confidence

- **High Confidence**: Core mechanism of using CBR-style retrieval to augment LLM prompts is well-established and experimental methodology is sound
- **Medium Confidence**: Specific improvements in semantic similarity (1.94%) are meaningful but may be sensitive to evaluation dataset and metric choice
- **Medium Confidence**: Effectiveness of hybrid similarity weighting is demonstrated but requires careful tuning for optimal performance

## Next Checks

1. **Cross-Domain Validation**: Test CBR-RAG performance on legal question-answering datasets from different jurisdictions (US, UK) to assess generalizability beyond Australian legal domain

2. **Human Evaluation Study**: Conduct human evaluation of generated answers to complement semantic similarity metrics, assessing practical utility and coherence from legal experts' perspectives

3. **Ablation Study**: Perform ablation study removing each component (CBR retrieval, embedding models, similarity metrics) to quantify their individual contributions to performance improvements