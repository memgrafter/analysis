---
ver: rpa2
title: 'Putting People in LLMs'' Shoes: Generating Better Answers via Question Rewriter'
arxiv_id: '2408.10573'
source_url: https://arxiv.org/abs/2408.10573
tags:
- question
- questions
- points
- language
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving question answering
  performance by rewriting vague user questions into more LLM-friendly formats. The
  proposed method uses a question rewriter that is trained via direct preference optimization
  (DPO) using automatically generated feedback on answer quality, without requiring
  human annotations.
---

# Putting People in LLMs' Shoes: Generating Better Answers via Question Rewriter

## Quick Facts
- **arXiv ID:** 2408.10573
- **Source URL:** https://arxiv.org/abs/2408.10573
- **Reference count:** 12
- **Primary result:** A question rewriter trained via direct preference optimization (DPO) without human annotations consistently improves answer quality across multiple black-box LLMs and long-form QA datasets.

## Executive Summary
This paper addresses the challenge of improving question answering performance by automatically rewriting vague user questions into more LLM-friendly formats. The proposed method uses a question rewriter trained via direct preference optimization (DPO) using automatically generated feedback on answer quality, eliminating the need for human annotations. By sampling multiple rewritten questions and selecting those that yield better answers based on automatic evaluation criteria, the system learns to generate non-leading, concise, and professional questions. Experiments across multiple datasets and black-box LLMs demonstrate consistent improvements in answer quality metrics, with the rewriter showing strong generalizability to models it wasn't specifically trained on.

## Method Summary
The method employs a question rewriter (R) trained using direct preference optimization to transform vague user questions into formats that yield better answers from black-box LLMs. The training process involves sampling K rewritten questions per original question using top-p sampling, generating answers for each, and evaluating them using automatic criteria from long-form QA datasets. Better-worse question pairs are created based on evaluation scores, and the rewriter is trained to prefer questions leading to higher-quality answers. The approach leverages existing automatic evaluation metrics (Scomp, Scont, Struth, Sinfo, Soverall, Spref) to provide feedback without requiring human annotations, and demonstrates generalizability across different LLMs without model-specific fine-tuning.

## Key Results
- The question rewriter trained on one LLM (Llama3-8B-instruct) generalizes effectively to other black-box models including Mistral, Zephyr, Gemma, GPT-3.5, and GPT-4o
- The method shows consistent improvements across multiple long-form QA datasets (K-QA, TruthfulQA, OASST1QA) with significant gains in all evaluation metrics
- Analysis reveals the rewriter learns to generate questions with professional tone, non-leading phrasing, and conciseness, aligning with intuitive notions of good question formulation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The question rewriter learns to generate non-leading, concise, and professional questions that improve answer quality.
- Mechanism: The rewriter is trained using direct preference optimization (DPO) with pairs of better and worse questions, where the "better" questions are those whose answers score higher on automatic evaluation criteria. Through this training, the rewriter implicitly learns to generate questions with attributes like non-leadingness, conciseness, and professional tone, which correlate with higher evaluation scores.
- Core assumption: The automatic evaluation criteria (e.g., comprehensiveness, truthfulness, informativeness) are sensitive to the quality of the input question and can serve as a proxy for human judgment.
- Evidence anchors: [abstract] "Intiguingly, our analysis implies that the question rewriter learns to generate non-leading and concise questions in a professional tone"; [section] "Our importance and impact analysis unveils that our question rewriter learns to generate professional, non-leading, and concise questions, which align with our intuitions"

### Mechanism 2
- Claim: Sampling multiple rewritten questions and selecting the best ones allows the model to explore a diverse space of potential improvements.
- Mechanism: The method generates K different rewritten questions for each original question using top-p sampling, then evaluates all of them using automatic criteria. This allows the model to explore different ways of rephrasing the question and select the ones that lead to better answers. The diversity in the sampled questions prevents the model from getting stuck in local optima.
- Core assumption: Top-p sampling with a high p value (0.999) generates sufficiently diverse questions that cover a wide range of potential improvements.
- Evidence anchors: [section] "We use top-p sampling (Radford et al. 2019) to generate K different rewritten questions"; [section] "Our method demonstrates superior performance in most combinations of LLMs and datasets"

### Mechanism 3
- Claim: The method generalizes across different black-box LLMs without requiring model-specific fine-tuning.
- Mechanism: The question rewriter is trained once using a specific answer generation model (Llama3-8B-instruct), but the trained rewriter can then be applied to other black-box models (Mistral, Zephyr, Gemma, GPT-3.5, GPT-4o) without any additional training. This is possible because the rewriter is learning general principles of question formulation rather than model-specific patterns.
- Core assumption: The principles of good question formulation are largely independent of the specific LLM being used.
- Evidence anchors: [abstract] "The experiments across multiple black-box LLMs and long-form question answering (LFQA) datasets demonstrate the efficacy of our method"; [section] "our method excels in all metrics and all datasets, not only on the Llama3-8B-instruct used for training R but also on the other LLMs, demonstrating their generalizability"

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO allows training the question rewriter using pairs of better and worse questions without requiring human annotations or a differentiable reward model.
  - Quick check question: What is the key advantage of DPO over traditional RLHF in this context?

- Concept: Automatic evaluation criteria for generated answers
  - Why needed here: The method relies on automatic criteria (e.g., FActScore, truthfulness classifiers) to evaluate the quality of answers to rewritten questions, which in turn provides feedback for training the rewriter.
  - Quick check question: What are the three LFQA datasets used in the experiments and what automatic criteria do they provide?

- Concept: Top-p sampling for generating diverse rewritten questions
  - Why needed here: Top-p sampling with a high p value (0.999) generates a diverse set of rewritten questions, allowing the method to explore different ways of improving the original question.
  - Quick check question: How many rewritten questions are generated for each original question in the experiments?

## Architecture Onboarding

- Component map: Question Rewriter (R) -> Answer Generation LLM (L) -> Automatic Evaluation Criteria (C) -> DPO Training Loop

- Critical path:
  1. Sample K rewritten questions using R0
  2. Generate answers for all rewritten questions using L
  3. Evaluate answers using criteria in C
  4. Create better-worse question pairs based on evaluation scores
  5. Train R using DPO with these pairs
  6. Evaluate R on validation/test sets

- Design tradeoffs:
  - Sampling K questions vs. computational cost: Higher K provides more exploration but increases computation
  - Number of better-worse pairs (N+, N-) vs. training stability: More pairs provide better training but risk overfitting
  - Choice of automatic criteria vs. generalization: Different criteria may lead to different rewriter behaviors

- Failure signatures:
  - Rewriter generates questions that are too similar to the original (lack of exploration)
  - Training loss decreases but validation performance doesn't improve (overfitting)
  - Rewriter performs well on training LLM but poorly on others (lack of generalization)

- First 3 experiments:
  1. Train rewriter on K-QA dataset with N+=10, N-=20, evaluate on same dataset
  2. Train rewriter on TruthfulQA dataset, evaluate on both TruthfulQA and OASST1QA
  3. Train rewriter on OASST1QA dataset, evaluate on all three datasets to test cross-dataset generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the question rewriter vary across different domains or topics, such as medicine, law, and finance?
- Basis in paper: [explicit] The paper mentions that the rewriter is evaluated on a medical dataset (K-QA) and a multi-domain dataset (TruthfulQA), but does not explore performance across other domains like law or finance.
- Why unresolved: The paper focuses on a limited set of domains and does not provide insights into how the rewriter generalizes to other fields.
- What evidence would resolve it: Evaluating the rewriter on datasets from diverse domains (e.g., legal or financial questions) and comparing performance metrics across these domains would provide clarity.

### Open Question 2
- Question: What is the impact of the question rewriter on the interpretability and explainability of the generated answers?
- Basis in paper: [inferred] The paper focuses on improving answer quality through better questions but does not discuss how the rewriter affects the interpretability or explainability of the answers.
- Why unresolved: While the rewriter improves answer quality, it is unclear whether the rewritten questions lead to more interpretable or explainable answers.
- What evidence would resolve it: Analyzing the generated answers for interpretability and explainability, possibly through human evaluations or automated metrics, would address this question.

### Open Question 3
- Question: How does the question rewriter handle ambiguous or context-dependent questions, and what are its limitations in such scenarios?
- Basis in paper: [explicit] The paper mentions that the rewriter is designed to handle vague questions but does not provide specific examples or analysis of its performance on ambiguous or context-dependent questions.
- Why unresolved: The paper does not explore the rewriter's effectiveness in handling complex, context-dependent, or highly ambiguous questions.
- What evidence would resolve it: Testing the rewriter on datasets with ambiguous or context-dependent questions and analyzing its performance would provide insights into its limitations and capabilities.

## Limitations
- The method relies on automatic evaluation criteria rather than human judgment, introducing uncertainty about whether learned preferences truly align with human preferences for question quality
- The approach assumes that non-leading, concise, and professional questions universally correlate with better answers, which may not hold across all domains and question types
- While the method shows strong generalization across multiple LLMs, it still requires access to a specific answer generation model for training, limiting true black-box applicability

## Confidence

- High confidence: The core mechanism of using automatic evaluation criteria to train a question rewriter via DPO is technically sound and well-supported by experimental results
- Medium confidence: The claim that the rewriter learns to generate non-leading, concise, and professional questions is supported by analysis but could benefit from more direct human evaluation
- Medium confidence: The generalizability across different black-box LLMs is demonstrated but primarily tested on a specific set of models with similar capabilities

## Next Checks

1. Conduct human evaluation studies to validate whether the automatically learned question rewriting preferences align with human judgments of question quality and answer helpfulness
2. Test the method on a broader range of question types, including those requiring specialized domain knowledge or handling ambiguous/multi-intent questions
3. Evaluate the approach on smaller, more constrained language models to assess performance when computational resources are limited