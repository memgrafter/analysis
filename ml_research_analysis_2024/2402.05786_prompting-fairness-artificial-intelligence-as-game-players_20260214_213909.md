---
ver: rpa2
title: 'Prompting Fairness: Artificial Intelligence as Game Players'
arxiv_id: '2402.05786'
source_url: https://arxiv.org/abs/2402.05786
tags:
- theai
- therecipient
- fairness
- recipient
- theendof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored fairness in AI decision-making by having an
  AI model play 101 rounds of the dictator game under various conditions. The AI demonstrated
  a strong sense of fairness, influenced by trustworthiness of its partner, framing
  effects, and recipient identity.
---

# Prompting Fairness: Artificial Intelligence as Game Players

## Quick Facts
- arXiv ID: 2402.05786
- Source URL: https://arxiv.org/abs/2402.05786
- Reference count: 1
- Key result: AI model played 101 rounds of dictator game showing fairness influenced by trustworthiness, framing, and recipient identity

## Executive Summary
This study investigates AI decision-making fairness by having a single AI model play the dictator game under various conditions. The AI demonstrated human-like fairness preferences, with giving behavior influenced by perceived trustworthiness of partners, framing effects, and recipient characteristics. When acting as trustee, the AI averaged $44.58 given across 70 rounds, with regression analysis identifying recipient need and persona as the strongest predictors of giving behavior.

The research reveals that AI systems can exhibit inequality aversion and risk-taking behaviors similar to humans, though with distinct rationalization patterns. Notably, the AI gave more to recipients described as in need and less to millionaires, despite claiming recipient wealth shouldn't influence decisions. ANOVA tests confirmed statistically significant differences across framing conditions, suggesting AI fairness judgments are sensitive to contextual presentation.

## Method Summary
The study employed the dictator game framework with an AI model playing 101 rounds under various conditions. The AI was prompted to play as both dictator and trustee, with experiments varying framing conditions, recipient personas, and contextual information. Behavioral data was collected through direct play outcomes, while qualitative insights were gathered from the AI's stated rationales for decisions. Statistical analysis included regression modeling to identify predictors of giving behavior and ANOVA tests to assess framing effects.

## Key Results
- AI averaged $44.58 given across 70 trustee rounds, with regression showing recipient need and persona as strongest predictors
- ANOVA confirmed statistically significant differences in giving across framing conditions
- AI exhibited inequality aversion when receiving high sums, becoming more risk-taking, and showed differential giving based on recipient identity despite claiming wealth shouldn't matter

## Why This Works (Mechanism)
The AI's fairness behavior emerges from its training on human behavioral data and language patterns that encode social norms around fairness and reciprocity. The model's decision-making reflects learned associations between contextual cues (trustworthiness, need, framing) and appropriate responses. When processing the dictator game scenarios, the AI draws on its understanding of human fairness concepts, though it applies them through computational reasoning rather than emotional or intuitive processes.

## Foundational Learning
- Dictator Game Framework: Economic game where one player allocates resources between self and another player; needed to measure fairness preferences in controlled settings
- Prompt Engineering: Techniques for structuring AI inputs to elicit desired behaviors; quick check: test different prompt structures to see how they affect AI responses
- Regression Analysis: Statistical method for identifying predictors of outcomes; quick check: verify coefficient significance and multicollinearity
- ANOVA Testing: Method for comparing means across multiple groups; quick check: confirm assumptions of normality and homogeneity of variance

## Architecture Onboarding
**Component Map:** Prompt Design -> AI Model Processing -> Decision Output -> Behavioral Analysis -> Statistical Validation

**Critical Path:** The prompt enters the AI model, which processes the dictator game scenario and outputs a resource allocation decision. This decision is recorded and analyzed statistically to identify patterns and predictors of fairness behavior.

**Design Tradeoffs:** Single AI model vs. model comparison (depth vs. breadth), simplified dictator game vs. complex real-world scenarios (experimental control vs. ecological validity), qualitative rationales vs. quantitative behavioral measures (richness of insight vs. analytical rigor)

**Failure Signatures:** Overfitting to specific prompt patterns, lack of generalizability across different AI architectures, inability to distinguish between stated and actual reasoning processes, experimental design limitations affecting external validity

**3 First Experiments:**
1. Test different prompt formulations to identify optimal structure for eliciting consistent AI decision-making
2. Vary the complexity of recipient descriptions to map the AI's sensitivity to contextual information
3. Implement sequential rounds with feedback to assess learning and adaptation in AI fairness behavior

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Single AI model architecture limits generalizability of findings to other AI systems
- Simplified dictator game framework may not capture complexities of real-world AI decision contexts
- Text-based persona and framing manipulations may not fully reflect how AI systems process contextual information

## Confidence
- AI demonstrates fairness influenced by trustworthiness, framing, and recipient identity: High
- AI exhibits inequality aversion when receiving high sums: Medium
- AI rationalizes decisions differently from humans: Low

## Next Checks
1. Replicate the experiment across multiple AI model architectures to assess whether observed fairness patterns are model-specific or general to language models
2. Conduct blinded trials where researchers are unaware of the AI's persona or framing conditions to eliminate potential researcher bias
3. Implement real-time logging of token-level decision processes to identify whether the AI's stated rationales match its actual reasoning pathways during dictator game decisions