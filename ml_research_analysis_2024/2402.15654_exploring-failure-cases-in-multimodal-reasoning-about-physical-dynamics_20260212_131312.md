---
ver: rpa2
title: Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics
arxiv_id: '2402.15654'
source_url: https://arxiv.org/abs/2402.15654
tags:
- object
- objects
- language
- cylinder
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models show atomic knowledge about object properties
  but fail to compose this into correct solutions for physical reasoning tasks. This
  paper explores these failure cases in a simulated environment, finding that both
  text-only and multimodal LLMs struggle with object selection and physical feasibility
  in object manipulation and placement tasks.
---

# Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics

## Quick Facts
- arXiv ID: 2402.15654
- Source URL: https://arxiv.org/abs/2402.15654
- Authors: Sadaf Ghaffari; Nikhil Krishnaswamy
- Reference count: 7
- Large language models show atomic knowledge about object properties but fail to compose this into correct solutions for physical reasoning tasks

## Executive Summary
This paper investigates how large language models (LLMs) fail at physical reasoning tasks involving object manipulation and placement in a simulated environment. The study reveals that while models possess atomic knowledge about object properties like roundness and flatness, they struggle to compose this knowledge into physically feasible solutions. Both text-only and multimodal LLMs frequently generate unstable configurations when asked to solve simple stacking problems, demonstrating a critical gap between linguistic knowledge and physical reasoning capabilities.

## Method Summary
The research evaluates LLM performance on physical reasoning tasks using a simulated VoxWorld environment with objects of varying geometric properties. The method involves zero-shot evaluation of text-only models (ChatGPT, LLaMA 2) and multimodal models (LLaVA) using carefully crafted prompts. BLIP's cross-attention mechanism is analyzed to assess grounding of physical concepts. The paper proposes an exploration-based learning procedure where failure in initial LLM-generated plans triggers object interaction experiments to discover properties, with knowledge subsequently distilled back into the LLM through attention-based alignment.

## Key Results
- LLMs achieve only 55% stability in object placement tasks despite showing knowledge of individual object properties
- LLaVA and text-only models fail to select optimal objects for stacking, with stability rates ranging from 52% to 60%
- BLIP's sophisticated cross-modal attention fails to correctly ground abstract physical concepts like "flatness" and "roundness" to appropriate object instances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs possess atomic object knowledge but cannot compose this knowledge into physically feasible solutions
- Mechanism: The models encode object properties (roundness, flatness) as isolated tokens but lack causal reasoning about how these properties interact under physical dynamics like gravity and inertia
- Core assumption: Physical reasoning requires composition of atomic facts through a causal chain, which current LLMs lack
- Evidence anchors:
  - [abstract] "Large language models show atomic knowledge about object properties but fail to compose this into correct solutions for physical reasoning tasks."
  - [section] "The LLM is not grounded to the environmental dynamics" and "Solutions to simple problems that are intuitive to humans appear to not be considered by LLMs despite the level of world knowledge shown about individual items."
  - [corpus] No strong direct evidence; related work mentions sensitivity to environment changes but not causal composition failure specifically
- Break condition: If the model can successfully chain atomic facts into a physically coherent sequence, this mechanism fails

### Mechanism 2
- Claim: Visual grounding through cross-attention is insufficient for physical concept comprehension
- Mechanism: Even sophisticated cross-modal attention (as in BLIP) fails to associate abstract physical properties like "flat" and "round" with the correct object instances, leading to grounding errors in reasoning
- Core assumption: Visual grounding requires not just matching words to regions but understanding the physical significance of those regions in context
- Evidence anchors:
  - [abstract] "Even BLIP, a vision-language model with sophisticated cross-modal attention, fails to ground key physical concepts like flatness and roundness."
  - [section] "The sphere is highlighted for both [round and flat], indicating that 'round' is successfully localized to the sphere likely because of a significant presence in the training data. Despite the contrastive loss used for training BLIP, the antonym 'flat' is not localized to a non-round object."
  - [corpus] Weak evidence; related work mentions multimodal grounding but not this specific failure mode
- Break condition: If a model demonstrates correct grounding of abstract physical properties across diverse contexts, this mechanism fails

### Mechanism 3
- Claim: Failure in zero-shot reasoning can trigger a learning process that extracts physical properties through interaction and simulation
- Mechanism: When the LLM's solution fails in the simulation, the agent explores object interactions, uses trajectory data to infer properties, and distills this back into the LLM through attention-based alignment
- Core assumption: Embodied interaction provides richer physical cues than linguistic/visual input alone, enabling discovery of object properties
- Evidence anchors:
  - [abstract] "We develop a procedure by which an agent may exploit failure in the zero-shot outputs of LLMs as a trigger to investigate alternative solutions to the problem using object interactions and previously-encoded knowledge of the object semantics."
  - [section] "When the failure of the initial LLM-generated plan triggers exploration, the agent traverses the environment, selects objects, and attempts to stack each one on top of itself."
  - [corpus] No direct evidence; related work on embodied agents doesn't mention this specific failure-to-exploration pipeline
- Break condition: If exploration consistently fails to yield useful physical property information, this mechanism fails

## Foundational Learning

- Concept: Object affordances and stability
  - Why needed here: The task requires understanding which objects can support others based on their geometric properties (flat vs. round surfaces)
  - Quick check question: Given a cube and a sphere of the same size, which would you place at the base of a stack to maximize stability, and why?

- Concept: Cross-modal attention and grounding
  - Why needed here: The model needs to connect linguistic descriptions of objects to their visual representations and physical properties
  - Quick check question: If an image shows a cylinder lying on its curved side, what physical property (flat or round) would be most relevant for stacking another object on top of it?

- Concept: Trajectory-based object classification
  - Why needed here: The exploration phase uses how objects move and interact in the simulation to infer their properties
  - Quick check question: If you roll a cylinder along its curved side versus its flat side, how would the trajectory data differ, and what property could you infer from each?

## Architecture Onboarding

- Component map: LLM (generator) -> Simulation environment -> Trajectory encoder -> Attention alignment module -> Preference model
- Critical path: LLM output → Simulation execution → Failure detection → Object exploration → Property extraction → Knowledge distillation → Improved LLM output
- Design tradeoffs:
  - Between simulation fidelity and computational cost
  - Between exploration thoroughness and time efficiency
  - Between knowledge distillation complexity and model size constraints
- Failure signatures:
  - Unstable configurations after physics simulation
  - Incorrect object selection (e.g., choosing sphere over cube for base)
  - Grounding errors (e.g., associating "flat" with round objects)
  - Failure to compose atomic facts into coherent plans
- First 3 experiments:
  1. Run the base prompt through LLM → simulate → measure stability/IoU → analyze failure modes
  2. Test BLIP's grounding of "flat" vs "round" on controlled object images → measure accuracy
  3. Implement simple exploration (stack objects on themselves) → extract trajectory features → classify properties → compare to ground truth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively distill knowledge about physical object properties and environmental dynamics from a simulation into a large language model?
- Basis in paper: [explicit] The paper proposes a method to distill knowledge gained from object interactions in a simulation back into the LLM using attention-based alignment between object representations and language model embeddings
- Why unresolved: While the paper proposes a method, it does not empirically validate the effectiveness of this knowledge distillation approach. The proposed architecture and loss functions are theoretical and require experimental verification
- What evidence would resolve it: Empirical results showing that an LLM trained with this distillation method produces more physically plausible solutions to reasoning tasks compared to a baseline LLM without this training

### Open Question 2
- Question: To what extent do current multimodal models like LLaVA and BLIP fail to ground key physical concepts like flatness and roundness, and what are the underlying reasons for these failures?
- Basis in paper: [explicit] The paper demonstrates that LLaVA fails to correctly select objects and generate physically feasible solutions, and that BLIP struggles to ground concepts like flatness and roundness to the correct objects in the scene
- Why unresolved: The paper identifies these failures but does not provide a comprehensive analysis of the root causes. It is unclear whether these are fundamental limitations of current multimodal architectures or if they can be addressed with improved training data, model architectures, or other techniques
- What evidence would resolve it: Detailed ablation studies and experiments isolating the impact of different factors (e.g., training data, model architecture, attention mechanisms) on the grounding of physical concepts

### Open Question 3
- Question: How can we design prompts and controlled environments to effectively evaluate and improve the physical reasoning capabilities of large language models?
- Basis in paper: [explicit] The paper experiments with different prompt variants and environmental controls (e.g., removing distractors, providing partial solutions) to assess their impact on LLM performance
- Why unresolved: While the paper explores some prompt and environment variations, it does not systematically investigate the space of possible designs or provide general guidelines for effective evaluation and improvement of physical reasoning
- What evidence would resolve it: A comprehensive study comparing the impact of various prompt and environment designs on LLM performance across multiple physical reasoning tasks, leading to evidence-based recommendations for effective evaluation and improvement

## Limitations
- Evaluation relies heavily on a single simulated environment (VoxWorld), raising questions about generalization to real-world or alternative simulated settings
- The proposed distillation method remains theoretical, with no empirical demonstration of improved LLM performance after knowledge integration
- Study focuses on object placement tasks, limiting conclusions about broader physical reasoning capabilities

## Confidence
- High confidence: LLMs demonstrate atomic knowledge of object properties but struggle with physical reasoning composition
- Medium confidence: Cross-modal attention in BLIP fails to ground abstract physical concepts like flatness and roundness
- Medium confidence: The proposed exploration-based learning procedure could address LLM limitations, though unproven
- Low confidence: Generalizability of findings to other physical reasoning domains or real-world applications

## Next Checks
1. Test LLM performance across multiple simulated environments with varying physical parameters to assess generalization of failure patterns
2. Implement the proposed distillation method and measure its impact on LLM physical reasoning accuracy in a controlled experiment
3. Conduct human evaluation studies comparing LLM-generated solutions to human intuitive approaches for the same physical reasoning tasks