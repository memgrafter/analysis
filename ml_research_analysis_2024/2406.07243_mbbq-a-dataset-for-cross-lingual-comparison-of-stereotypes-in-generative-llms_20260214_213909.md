---
ver: rpa2
title: 'MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative
  LLMs'
arxiv_id: '2406.07243'
source_url: https://arxiv.org/abs/2406.07243
tags:
- bias
- languages
- language
- english
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MBBQ, a multilingual bias benchmark dataset
  for question answering, designed to measure cross-lingual differences in stereotypes
  exhibited by generative large language models (LLMs). The dataset extends the English
  BBQ dataset to Dutch, Spanish, and Turkish, covering six bias categories.
---

# MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs

## Quick Facts
- arXiv ID: 2406.07243
- Source URL: https://arxiv.org/abs/2406.07243
- Reference count: 40
- This paper introduces MBBQ, a multilingual bias benchmark dataset for question answering, designed to measure cross-lingual differences in stereotypes exhibited by generative large language models (LLMs).

## Executive Summary
This paper introduces MBBQ, a multilingual bias benchmark dataset for question answering, designed to measure cross-lingual differences in stereotypes exhibited by generative large language models (LLMs). The dataset extends the English BBQ dataset to Dutch, Spanish, and Turkish, covering six bias categories. A control set is also created to measure task performance independently of bias. Experiments with seven LLMs reveal that most models show significant cross-lingual differences in both accuracy and bias behavior, except for the most accurate models. Spanish generally exhibits the highest bias, while English or Turkish show the least. The study emphasizes the importance of controlling for cultural differences and task accuracy when evaluating model bias, and encourages further research on multilingual debiasing.

## Method Summary
The study extends the English BBQ dataset to Dutch, Spanish, and Turkish, creating MBBQ with six bias categories. A parallel control dataset (control-MBBQ) is developed to measure task performance independently of bias by replacing individuals with first names. Seven LLMs (Aya, Falcon 7b, GPT-3.5 Turbo, Llama 2-Chat 7b, Mistral 7b, WizardLM 7b, Zephyr 7b) are evaluated using five translated prompts per language. Answers are detected using a rule-based approach, and bias scores (BiasA for ambiguous contexts, BiasD for disambiguated contexts) are calculated. The study controls for cultural differences by focusing on stereotypes common across all languages and accounts for task accuracy when measuring bias.

## Key Results
- Most models show significant cross-lingual differences in both accuracy and bias behavior, except for the most accurate models.
- Spanish generally exhibits the highest bias levels, while English or Turkish show the least.
- Cross-lingual differences in model bias are primarily driven by inconsistent representation of non-English languages in training data.
- Ambiguous contexts increase reliance on stereotypes because models lack sufficient information to reason objectively.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual differences in model bias are primarily driven by inconsistent representation of non-English languages in training data.
- Mechanism: If a language is underrepresented or unevenly represented in training data, the model will exhibit more bias when prompted in that language due to weaker or noisier semantic associations.
- Core assumption: The observed bias differences are due to training data imbalance rather than deliberate model behavior or task performance variance.
- Evidence anchors:
  - [abstract] "We further complement MBBQ with a parallel control dataset to measure task performance on the question-answering task independently of bias."
  - [section 5.1] "Models perform best in English and worst in Turkish."
  - [corpus] Weak corpus coverage for specific demographic stereotypes across non-English languages.
- Break condition: If cross-lingual bias differences persist even after controlling for task accuracy, or if bias shifts are observed without corresponding accuracy changes.

### Mechanism 2
- Claim: Ambiguous contexts increase reliance on stereotypes because models lack sufficient information to reason objectively.
- Mechanism: In ambiguous contexts, models default to learned stereotypes when the correct answer ("unknown") is not obvious from the context, resulting in higher bias scores.
- Core assumption: Models are not trained to recognize when an answer cannot be inferred and instead select an option based on prior associations.
- Evidence anchors:
  - [section 5.2] "In ambiguous contexts, we find that models obtain higher bias scores compared to disambiguated contexts."
  - [section 5.1] "The fact that in ambiguous contexts the correct answer is always the 'unknown' option causes problems to some models."
  - [corpus] No explicit corpus evidence; inference based on experimental results.
- Break condition: If models can be retrained or prompted to recognize ambiguous contexts and consistently choose "unknown."

### Mechanism 3
- Claim: Bias is not uniform across bias categories; some stereotypes are more culturally entrenched or more consistently encoded across languages.
- Mechanism: Different bias categories (e.g., age, disability, SES) have varying degrees of stereotype consistency across languages, leading to uneven bias expression.
- Core assumption: Stereotypes that are common across all evaluated languages will be more consistently encoded, whereas language-specific stereotypes will show more variance.
- Evidence anchors:
  - [section 5.3] "Generally, a model's bias scores in a given language differ significantly across the different bias categories."
  - [section 3.2] "We only keep templates with stereotypes that are held in all languages."
  - [corpus] Limited corpus evidence; relies on native speaker judgments.
- Break condition: If all bias categories show similar bias scores regardless of cultural context.

## Foundational Learning

- Concept: Question-answering task accuracy as a control for bias measurement
  - Why needed here: To ensure that measured bias differences are not simply due to task performance variance across languages.
  - Quick check question: If a model performs poorly in a language, can we distinguish between poor reasoning ability and bias?

- Concept: Implicit vs. explicit stereotypes in model evaluation
  - Why needed here: MBBQ measures implicit stereotypes without requiring external classifiers, making it robust to introducing new biases.
  - Quick check question: Why might measuring explicit stereotypes (e.g., when a group name is mentioned) give a different result than implicit ones?

- Concept: Cross-lingual transfer of debiasing techniques
  - Why needed here: Understanding whether debiasing in one language affects bias in others is critical for building fair multilingual models.
  - Quick check question: If a model is debiased in English, should we expect the same reduction in bias in Spanish or Turkish?

## Architecture Onboarding

- Component map: MBBQ dataset (templates + control set) -> Model prompt translation -> Model inference -> Answer detection -> Bias scoring -> Statistical comparison across languages
- Critical path: Dataset creation -> Model evaluation -> Bias measurement -> Cross-lingual comparison
- Design tradeoffs: Hand-checked translations ensure quality but limit scalability; focusing on common stereotypes across languages enables comparison but excludes culturally specific biases.
- Failure signatures: If bias scores do not differ across languages after controlling for accuracy, the assumption about training data imbalance may be invalid; if models refuse to answer many questions, safety fine-tuning may interfere with bias measurement.
- First 3 experiments:
  1. Evaluate a multilingual model on MBBQ and control-MBBQ to verify accuracy differences across languages.
  2. Compare bias scores in ambiguous vs. disambiguated contexts to confirm the impact of context clarity.
  3. Test whether debiasing in one language reduces bias in other languages to assess cross-lingual transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does multilingual debiasing improve cross-lingual bias consistency in generative LLMs?
- Basis in paper: [explicit] The authors state that their findings "highlight the importance of controlling for cultural differences and task accuracy when measuring model bias" and hope to "encourage further work on bias in multilingual settings." They also mention potential extensions of MBBQ to other languages and stereotypes.
- Why unresolved: The paper investigates existing cross-lingual bias differences but does not evaluate any debiasing techniques or their cross-lingual effects. It's unclear whether techniques that reduce bias in one language would transfer to others or potentially introduce new biases.
- What evidence would resolve it: Experiments applying multilingual debiasing techniques (e.g., adversarial debiasing, counterfactual data augmentation) to models and measuring resulting bias scores across all languages in MBBQ, comparing cross-lingual bias consistency before and after debiasing.

### Open Question 2
- Question: What is the relationship between language representation in training data and cross-lingual bias differences?
- Basis in paper: [inferred] The authors note that "LLMs are being used by speakers of at least 150 different languages" but most models are trained predominantly in English. They hypothesize that "the language employed by a user to prompt a model could cause models to generate responses that exhibit varied harmful properties, possibly due to different languages being underrepresented to different degrees in the training data."
- Why unresolved: While the paper demonstrates that cross-lingual bias differences exist, it does not investigate the underlying causes or whether these differences correlate with the amount of training data available for each language.
- What evidence would resolve it: Analysis correlating cross-lingual bias differences with measures of language representation in model training data (e.g., amount of web text, parallel corpora, or instruction tuning data available for each language), potentially using MBBQ to measure bias across a broader range of languages.

### Open Question 3
- Question: How do cross-lingual bias differences generalize to other types of social biases and stereotypes not covered in MBBQ?
- Basis in paper: [explicit] The authors acknowledge that "MBBQ contains a non-exhaustive set of stereotypes" and "cannot possibly cover all stereotypes relevant for any of the languages we consider." They focused on stereotypes common across all four languages.
- Why unresolved: The paper demonstrates cross-lingual bias differences for a specific set of stereotypes but does not investigate whether these findings extend to other bias types, particularly those specific to individual cultures or languages.
- What evidence would resolve it: Extending MBBQ to include additional bias categories and language-specific stereotypes, then measuring cross-lingual bias differences across this expanded set to determine whether patterns observed in the current study generalize.

## Limitations
- The dataset covers only three non-English languages (Dutch, Spanish, Turkish), which may not represent the full spectrum of cross-lingual bias patterns.
- The requirement to find common stereotypes across all languages means culturally specific biases are excluded, potentially underestimating the true extent of cross-lingual differences.
- The rule-based answer detection approach introduces potential measurement errors, particularly for models that refuse to answer or produce unexpected output formats.

## Confidence

- **High confidence**: Cross-lingual differences in model bias exist and can be measured using the MBBQ framework. The experimental methodology for measuring bias and accuracy is sound and reproducible.
- **Medium confidence**: The ranking of languages by bias levels (Spanish highest, English/Turkish lowest) and the general finding that most models show cross-lingual bias differences are robust. However, the exact magnitude of these differences may vary with different model versions or evaluation conditions.
- **Low confidence**: The claim that training data imbalance is the primary mechanism driving cross-lingual bias differences. While this provides a plausible explanation, the study lacks direct evidence linking training data composition to observed bias patterns.

## Next Checks

1. **Controlled training data experiment**: Train identical model architectures on training data with systematically varied language proportions, then evaluate on MBBQ to establish causal links between training data composition and cross-lingual bias patterns.

2. **Fine-tuning transfer study**: Apply bias mitigation techniques in one language (e.g., English) and measure the resulting changes in bias across all languages in MBBQ to quantify cross-lingual transfer of debiasing effects.

3. **Extended language coverage**: Replicate the MBBQ evaluation framework with additional languages from different families (e.g., East Asian, Slavic, Semitic) to determine whether the observed patterns hold across a broader linguistic diversity and identify any language-family-specific effects.