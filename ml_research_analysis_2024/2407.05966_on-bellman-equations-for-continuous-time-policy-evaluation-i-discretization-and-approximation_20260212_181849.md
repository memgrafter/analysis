---
ver: rpa2
title: 'On Bellman equations for continuous-time policy evaluation I: discretization
  and approximation'
arxiv_id: '2407.05966'
source_url: https://arxiv.org/abs/2407.05966
tags:
- error
- function
- page
- proof
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a class of high-order discretization schemes
  for continuous-time policy evaluation using discretely-observed diffusion process
  trajectories. The authors construct higher-order Bellman operators by replacing
  the continuous-time reward integration with Lagrangian polynomial interpolation,
  achieving numerical accuracy that scales with step size to the power of the polynomial
  order.
---

# On Bellman equations for continuous-time policy evaluation I: discretization and approximation

## Quick Facts
- arXiv ID: 2407.05966
- Source URL: https://arxiv.org/abs/2407.05966
- Authors: Wenlong Mou; Yuhua Zhu
- Reference count: 40
- Primary result: High-order discretization schemes for continuous-time policy evaluation using Lagrangian polynomial interpolation achieve numerical accuracy that scales with step size to the power of the polynomial order

## Executive Summary
This paper develops a class of high-order discretization schemes for continuous-time policy evaluation using discretely-observed diffusion process trajectories. The authors construct higher-order Bellman operators by replacing the continuous-time reward integration with Lagrangian polynomial interpolation, achieving numerical accuracy that scales with step size to the power of the polynomial order. For function approximation, they show that the projected fixed-point errors have bounded approximation factors due to elliptic structure, avoiding the usual blow-up in effective horizon. The proposed methods are compatible with model-free RL algorithms and demonstrated through numerical simulations to outperform naive first-order schemes.

## Method Summary
The paper constructs high-order Bellman operators using Lagrangian polynomial interpolation to approximate the continuous-time reward integration. The value function is defined as an integral over continuous time, which is approximated by interpolating the observed rewards at discrete time points using Lagrangian polynomials. These polynomials are then integrated analytically against the exponential discount kernel. For function approximation, the infinite-dimensional Bellman fixed-point equation is projected onto a finite-dimensional subspace spanned by basis functions using Galerkin projection. The projected equations are solved using empirical samples from trajectory data, enabling model-free implementation compatible with existing RL algorithms.

## Key Results
- Higher-order Lagrangian polynomial interpolation achieves numerical accuracy that scales with step size to the power of the polynomial order
- Elliptic structure of the diffusion process enables uniformly bounded approximation factors in projected fixed-point methods
- The high-order Bellman operator can be estimated directly from trajectory data without requiring explicit estimation of dynamics coefficients

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Higher-order Lagrangian polynomial interpolation in time yields n-th order numerical accuracy for the Bellman operator, where n depends on the smoothness of the reward and dynamics.
- **Mechanism:** The value function is defined as an integral over continuous time, which is approximated by interpolating the observed rewards at discrete time points using Lagrangian polynomials. These polynomials are then integrated analytically against the exponential discount kernel.
- **Core assumption:** The coefficients b, Λ, and r satisfy sufficient smoothness (Lip(n) condition) so that the reward's conditional expectation has bounded n-th derivatives.
- **Evidence anchors:**
  - [abstract]: "construct higher-order Bellman operators by replacing the continuous-time reward integration with Lagrangian polynomial interpolation, achieving numerical accuracy that scales with step size to the power of the polynomial order."
  - [section 3.1]: "By generalizing the idea in Eq (8a), we are ready to construct a class of higher-order numerical approximations to the Bellman equation (7a)."
  - [corpus]: Weak evidence - corpus contains papers on continuous-time RL but none directly discuss Lagrangian polynomial interpolation for Bellman operators.
- **Break condition:** If the smoothness assumptions (Lip(n)) fail, the higher-order error bounds no longer hold and the method may not outperform naive first-order schemes.

### Mechanism 2
- **Claim:** Elliptic structure of the diffusion process enables uniformly bounded approximation factors in projected fixed-point methods, avoiding the usual effective horizon blow-up seen in discrete-time MDPs.
- **Mechanism:** The uniform ellipticity and regularity of the stationary distribution allow application of Cea-type error bounds for Galerkin methods, where the approximation factor is independent of the time discretization step size.
- **Core assumption:** The diffusion term Λ is uniformly elliptic (UE condition) and the linear subspace K satisfies reverse Poincaré inequalities (Reg condition).
- **Evidence anchors:**
  - [abstract]: "In contrast to discrete-time RL problems where the approximation factor depends on the effective horizon, we obtain a bounded approximation factor using the underlying elliptic structures, even if the effective horizon diverges to infinity."
  - [section 4.2]: "The approximation error bounds in Corollary 1 involves an approximation factor of order η−1/2, which may amplify the approximation error significantly when taking a small stepsize. Such a blow-up can be avoided under some stronger assumptions."
  - [corpus]: Weak evidence - corpus papers discuss continuous-time RL but do not specifically address bounded approximation factors due to elliptic structure.
- **Break condition:** If the diffusion becomes degenerate (violating UE) or the basis functions lack sufficient regularity (violating Reg), the approximation factor may blow up with step size.

### Mechanism 3
- **Claim:** The high-order Bellman operator can be estimated directly from trajectory data without requiring explicit estimation of the dynamics coefficients b and Λ.
- **Mechanism:** The Lagrangian polynomial interpolation only requires the observed rewards at discrete time points, and the expected value operator can be approximated using empirical averages from the trajectory.
- **Core assumption:** The observed trajectory is long enough to provide sufficient samples for accurate estimation of the empirical expectations in the projected fixed-point equations.
- **Evidence anchors:**
  - [abstract]: "The proposed methods are compatible with model-free RL algorithms and demonstrated through numerical simulations to outperform naive first-order schemes."
  - [section 4.4]: "Given a trajectory ( Xt)0≤t≤T observed at discrete time steps ( kη : k ≥ 0), we can approximate the solution to fixed-point equation (12) using empirical samples."
  - [corpus]: Weak evidence - corpus papers discuss continuous-time RL but none provide specific implementation details for data-driven estimation of high-order Bellman operators.
- **Break condition:** If the trajectory length is insufficient or the step size is too large, the empirical estimates may have high variance and the method may not converge to the true value function.

## Foundational Learning

- **Concept: Lagrangian Polynomial Interpolation**
  - Why needed here: Used to approximate the continuous-time reward integration by interpolating observed rewards at discrete time points, enabling higher-order numerical accuracy.
  - Quick check question: What is the error bound for Lagrangian polynomial interpolation when approximating a function with bounded n-th derivative?

- **Concept: Galerkin Projection**
  - Why needed here: Projects the infinite-dimensional Bellman fixed-point equation onto a finite-dimensional subspace spanned by basis functions, making the problem tractable for function approximation.
  - Quick check question: How does Cea's lemma relate the error in the projected solution to the best approximation error in the subspace?

- **Concept: Uniform Ellipticity**
  - Why needed here: Ensures the diffusion process has a well-behaved stationary distribution and allows application of regularity estimates for the value function.
  - Quick check question: What is the definition of uniform ellipticity for a diffusion coefficient matrix?

## Architecture Onboarding

- **Component map:** Lagrangian interpolation -> Galerkin projection -> Empirical estimation -> Error analysis
- **Critical path:**
  1. Verify smoothness assumptions (Lip(n)) for the problem
  2. Choose appropriate polynomial order n based on smoothness
  3. Select basis functions for Galerkin projection satisfying Reg condition
  4. Implement Lagrangian interpolation for reward approximation
  5. Construct empirical estimates from trajectory data
  6. Solve projected fixed-point equations
- **Design tradeoffs:**
  - Higher polynomial order n provides better accuracy but requires stronger smoothness assumptions and may increase computational cost
  - More basis functions improve approximation but increase the dimensionality of the linear system to solve
  - Smaller step size η improves discretization accuracy but may require more trajectory data for stable estimation
- **Failure signatures:**
  - If approximation error doesn't decrease with smaller step size, the smoothness assumptions may be violated
  - If numerical instability occurs in high-order schemes, the polynomial order may be too high for the given smoothness
  - If projected solution error is large despite many basis functions, the basis may not satisfy the reverse Poincaré inequality
- **First 3 experiments:**
  1. Implement the second-order Bellman operator (n=2) for a simple Ornstein-Uhlenbeck process with quadratic reward, verify the O(η²) error decay
  2. Compare the second-order method with naive first-order approach using the same basis functions, measure the accuracy improvement
  3. Test the method with different basis functions (Fourier vs Legendre polynomials) for a periodic reward function, verify the reverse Poincaré inequality holds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the global bounded/Lipschitz assumptions (Lip(n)) be replaced by local versions while maintaining the theoretical guarantees?
- Basis in paper: [inferred] The paper notes "we conjecture that the global bounded/Lipschitz assumptions (Lip(n)) can be replaced by local versions" as an interesting direction for future research.
- Why unresolved: The current proofs rely on global boundedness and Lipschitz constants throughout the entire state space, but many practical systems only exhibit these properties locally.
- What evidence would resolve it: A rigorous proof showing that the error bounds hold when the smoothness assumptions are only required to be satisfied in a neighborhood of the trajectory.

### Open Question 2
- Question: Can the elliptic assumption (UE(λmin, λmax)) be replaced by hypo-ellipticity for the improved approximation guarantees?
- Basis in paper: [explicit] "we conjecture that a hypo-elliptic H¨ ormander condition [H¨ or67] suffices, and we defer a detailed discussion to future works."
- Why unresolved: The current proof of Theorem 2 relies heavily on uniform ellipticity to establish the coercivity bound, but many important diffusion processes (like Langevin dynamics) are hypo-elliptic.
- What evidence would resolve it: A proof demonstrating that the bounded approximation factor still holds under the H¨ ormander condition, or a counterexample showing when it fails.

### Open Question 3
- Question: What are the optimal statistical methods and adaptive trade-offs for the sample-based algorithms?
- Basis in paper: [explicit] "the general question of optimal statistical methods and adaptive trade-offs remains open for a broad class of machine learning models used to fit the value function and/or the underlying dynamics."
- Why unresolved: While the paper establishes approximation error guarantees, it defers statistical analysis to a companion paper and notes that optimal methods are unknown for general function approximation classes.
- What evidence would resolve it: Sharp non-asymptotic statistical error bounds that match the approximation error rates, along with adaptive parameter selection rules that achieve these bounds without prior knowledge of problem constants.

## Limitations

- The smoothness assumptions (Lip(n) conditions) required for high-order accuracy may be restrictive for many realistic RL problems
- The empirical validation is limited to simple numerical examples without comparison to established continuous-time RL methods or real-world applications
- The paper does not provide quantitative guidance on how to verify the smoothness conditions empirically or select optimal parameters

## Confidence

- **Theoretical framework:** High - well-grounded in elliptic PDE theory and Galerkin approximation
- **Practical implementation:** Medium - idealized assumptions and limited empirical testing
- **Empirical validation:** Medium - simple numerical examples but no comparison to established methods

## Next Checks

1. Test the method on a non-smooth reward function to verify that the error bounds degrade appropriately when smoothness assumptions are violated
2. Implement the algorithm with noisy trajectory data to assess its robustness to observation errors and finite sample effects
3. Compare the computational efficiency and accuracy with model-based approaches that estimate the dynamics coefficients directly