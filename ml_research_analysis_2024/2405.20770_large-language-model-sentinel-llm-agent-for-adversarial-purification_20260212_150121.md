---
ver: rpa2
title: 'Large Language Model Sentinel: LLM Agent for Adversarial Purification'
arxiv_id: '2405.20770'
source_url: https://arxiv.org/abs/2405.20770
tags:
- adversarial
- defense
- agent
- sentence
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLAMOS, a novel defense technique that uses
  a large language model (LLM) agent to purify adversarial textual examples before
  they reach a target LLM classifier. LLAMOS operates as a pre-processing step without
  requiring fine-tuning of the target model.
---

# Large Language Model Sentinel: LLM Agent for Adversarial Purification

## Quick Facts
- arXiv ID: 2405.20770
- Source URL: https://arxiv.org/abs/2405.20770
- Authors: Guang Lin; Toshihisa Tanaka; Qibin Zhao
- Reference count: 40
- Key outcome: LLAMOS reduces attack success rates by up to 45.59% (GPT-3.5) and 37.86% (LLAMA-2) on GLUE datasets

## Executive Summary
This paper introduces LLAMOS, a novel defense technique that uses a large language model (LLM) agent to purify adversarial textual examples before they reach a target LLM classifier. LLAMOS operates as a pre-processing step without requiring fine-tuning of the target model. It consists of two components: an agent instruction that simulates an adversarial defense agent, and defense guidance that provides strategies for modifying inputs. Experiments on GLUE datasets with both GPT-3.5 and LLAMA-2 demonstrate that LLAMOS effectively defends against various types of adversarial attacks while maintaining high standard accuracy.

## Method Summary
LLAMOS is a pre-processing defense mechanism that employs an LLM-based agent to purify adversarial examples before they reach the target classifier. The system consists of two main components: an agent instruction module that defines the defense agent's role and goals, and a defense guidance module that provides specific strategies for modifying inputs. The defense agent operates without requiring fine-tuning of the target model, leveraging the LLM's inherent language understanding capabilities. The approach is enhanced with in-context learning (ICL) to improve performance against specific attack patterns, and the paper explores an adversarial system where defense and attack agents continuously counter each other.

## Key Results
- LLAMOS reduces attack success rates by up to 45.59% for GPT-3.5 and 37.86% for LLAMA-2 on GLUE datasets
- The method maintains high standard accuracy while defending against character, word, and sentence-level perturbations
- In-context learning significantly improves defense performance, achieving robust accuracies of 97.66% for C3 attacks and 92.19% for C3-FS attacks
- Adversarial system experiments show neither defense nor attack agent completely dominates, reaching a stalemate

## Why This Works (Mechanism)

### Mechanism 1
The LLM-based defense agent can simulate adversarial defense capabilities without requiring training on adversarial examples. The agent instruction component prompts the LLM to act as a defense agent with specific goals and guidance, leveraging the LLM's inherent understanding of language to identify and remove adversarial perturbations. Core assumption: The LLM possesses sufficient language understanding to distinguish between natural and adversarial text modifications.

### Mechanism 2
In-context learning significantly enhances the defense agent's ability to handle specific attack patterns. By providing specific guidance examples within the prompt, the defense agent learns to recognize and correct particular types of adversarial modifications. Core assumption: The LLM can effectively learn from in-context examples without parameter updates.

### Mechanism 3
The adversarial system dynamics create a balance where neither defense nor attack agent completely dominates. The continuous interaction between attack and defense agents through multiple rounds leads to a stalemate where each agent adapts to the other's strategies. Core assumption: The LLM-based agents can engage in meaningful adversarial interactions that lead to stable equilibria.

## Foundational Learning

- **Adversarial attacks on LLMs**: Understanding the nature of attacks (character, word, sentence level perturbations) is crucial for designing effective defense mechanisms. Quick check: What are the three levels at which adversarial attacks can modify text in the PromptAttack framework?

- **Prompt engineering and in-context learning**: The defense mechanism relies heavily on carefully crafted prompts and in-context learning to guide the LLM's behavior. Quick check: How does in-context learning differ from traditional fine-tuning in terms of parameter updates and computational cost?

- **Robustness metrics (ASR and RA)**: Evaluating the effectiveness of the defense mechanism requires understanding attack success rate and robust accuracy. Quick check: Given a dataset where the target LLM has accuracy 80% on clean examples and 40% on adversarial examples, what are the ASR and RA values?

## Architecture Onboarding

- **Component map**: Agent instruction module -> Defense guidance module -> In-context learning module -> Target LLM classifier

- **Critical path**: 1. Receive input text (clean or adversarial) 2. Apply defense agent purification using agent instruction and defense guidance 3. Pass purified text to target LLM for classification 4. (Optional) Engage in adversarial system dynamics with attack agent

- **Design tradeoffs**: Using an LLM as defense vs. fine-tuning the target model: Higher computational cost per inference but no need for retraining; Granularity of defense guidance: More specific guidance improves defense but may reduce generalization to unseen attacks; In-context learning vs. no ICL: Better performance on known attack patterns but requires more prompt tokens

- **Failure signatures**: High ASR with specific attack types indicates defense guidance gaps; Decreased standard accuracy suggests over-aggressive purification; Infinite loops in adversarial system indicate equilibrium but may not be optimal; Failure to defend obvious attacks suggests in-context learning is needed

- **First 3 experiments**: 1. Baseline evaluation: Test target LLM performance on clean examples and PromptAttack without defense 2. Simple defense test: Apply LLAMOS with agent instruction and basic defense guidance on PromptAttack 3. Enhanced defense evaluation: Add in-context learning to LLAMOS and test against the most challenging attack patterns (C3 and C3-FS)

## Open Questions the Paper Calls Out

1. How does LLAMOS perform against adversarial attacks that completely change the semantic meaning of the input while maintaining grammatical correctness?

2. Can LLAMOS be extended to handle multi-modal inputs, such as text-image pairs, for adversarial purification?

3. What is the impact of LLAMOS on the computational efficiency and latency of the target LLM's inference time?

## Limitations

- The scalability of LLAMOS to real-world deployment scenarios with diverse input patterns and sophisticated, adaptive attacks is uncertain
- Lack of evaluation on out-of-distribution data raises questions about generalization capability beyond GLUE datasets and PromptAttack framework
- Adversarial system dynamics reach a stalemate rather than meaningful improvement, suggesting potential limitations of LLM-based adversarial interactions

## Confidence

- **High Confidence**: The core mechanism of using an LLM agent for pre-processing purification is technically sound and well-supported by experimental results
- **Medium Confidence**: The effectiveness of in-context learning for enhancing defense capabilities shows strong results but may be sensitive to specific examples chosen
- **Low Confidence**: Claims about adversarial system dynamics leading to meaningful equilibrium states are the weakest, as agents reach stalemate without demonstrating useful defensive capabilities

## Next Checks

1. **Generalization Test**: Evaluate LLAMOS on out-of-distribution datasets (domain-specific text, different languages, or alternative attack frameworks) to assess whether the defense generalizes beyond GLUE datasets and PromptAttack

2. **Latency and Resource Analysis**: Measure computational overhead and inference latency of LLAMOS defense agent compared to target LLM, and assess practicality for production workloads with high request volumes

3. **Adaptive Attack Evaluation**: Implement adaptive attack strategy targeting LLAMOS weaknesses (e.g., exploiting defense guidance patterns) to determine whether the approach can withstand attacks designed to circumvent its specific purification strategies