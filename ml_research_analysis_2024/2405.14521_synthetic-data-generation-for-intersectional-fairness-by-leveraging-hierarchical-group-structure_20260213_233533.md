---
ver: rpa2
title: Synthetic Data Generation for Intersectional Fairness by Leveraging Hierarchical
  Group Structure
arxiv_id: '2405.14521'
source_url: https://arxiv.org/abs/2405.14521
tags:
- fairness
- data
- groups
- group
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a data augmentation approach to improve intersectional
  fairness in classification tasks. The method leverages the hierarchical structure
  of intersectional groups by generating additional data for smaller groups using
  a transformation function that combines data from parent groups.
---

# Synthetic Data Generation for Intersectional Fairness by Leveraging Hierarchical Group Structure

## Quick Facts
- arXiv ID: 2405.14521
- Source URL: https://arxiv.org/abs/2405.14521
- Reference count: 40
- Key outcome: Generative data augmentation approach that improves intersectional fairness by leveraging hierarchical group structure, showing superior fairness-accuracy trade-offs across four diverse datasets

## Executive Summary
This paper introduces a novel data augmentation method to address intersectional fairness in classification tasks by exploiting the hierarchical structure of intersectional groups. The approach generates synthetic data for underrepresented subgroups by combining and transforming examples from their parent groups, using Maximum Mean Discrepancy (MMD) loss to ensure the generated data resembles the true distribution. Evaluations on four diverse datasets (CelebA, Twitter Hate Speech, Numeracy, Anxiety) demonstrate that classifiers trained with this augmentation achieve superior intersectional fairness and are more robust to "leveling down" compared to traditional fairness methods.

## Method Summary
The method leverages the hierarchical structure of intersectional groups by viewing each group as the intersection of its parent categories. For each target group, generative models are trained using MMD loss to minimize the discrepancy between generated samples and true samples from the target group, plus an additional term with parent group samples to encourage diversity. During classifier training, equal sampling is used across all groups (including augmented data for smaller groups) to prevent bias toward larger groups. The approach is evaluated on four diverse datasets with text and image modalities, using α-Intersectional Fairness (IFα) and Differential Fairness (DF) metrics alongside balanced accuracy.

## Key Results
- Classifiers trained with augmented data achieve better fairness-accuracy trade-offs than unconstrained and traditional fairness methods
- Generated data are diverse and not mere replicas of real samples, as evidenced by distinguishability scores
- The approach is robust to "leveling down" where worst-off groups don't perform worse than unconstrained baselines
- Performance gains are consistent across four diverse datasets with different modalities and fairness metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hierarchical group structure enables targeted data generation for underrepresented subgroups by leveraging data from their parent groups.
- Mechanism: The method views each intersectional group as the intersection of its parent groups. For a target group g, it generates synthetic data by combining and transforming examples from parent groups g\i using a learned transformation function.
- Core assumption: The distribution of examples in a target group can be well-approximated by a combination of its parent groups' distributions.
- Evidence anchors:
  - [abstract] "Our method capitalizes on the hierarchical structure inherent to intersectionality, by viewing groups as intersections of their parent categories."
  - [section 4.1] "The intersection of immediate parent groups constitutes the target group g, with each parent group containing more examples than the target group itself."
  - [corpus] Weak - corpus neighbors do not directly discuss hierarchical group structure for data generation.
- Break condition: If the assumption that parent groups' distributions can approximate the target group's distribution fails, the generated data may not be representative of the target group, leading to poor performance.

### Mechanism 2
- Claim: Maximum Mean Discrepancy (MMD) loss ensures the generated data resembles the true distribution of the target group.
- Mechanism: The generative model is trained to minimize the MMD between the generated samples and samples from the target group, plus an additional term between generated samples and parent group samples to encourage diversity.
- Core assumption: MMD can effectively measure the similarity between the generated distribution and the true distribution of the target group.
- Evidence anchors:
  - [abstract] "Our generative models are trained using a loss based on Maximum Mean Discrepancy (MMD) to ensure the generated data resembles the true distribution."
  - [section 4.2] "Our loss function is the MMD between the generated samples and the samples from group g, to which we add the MMD between the generated samples and those from its parent groups."
  - [corpus] Weak - corpus neighbors do not discuss MMD loss for data generation.
- Break condition: If MMD fails to capture the relevant aspects of the distribution (e.g., higher-order moments), the generated data may not be sufficiently similar to the true distribution.

### Mechanism 3
- Claim: Equal sampling ensures the classifier treats all subgroups with equal importance, preventing bias towards larger groups.
- Mechanism: During training, the same number of examples are sampled from each group, including the augmented data for smaller groups, to ensure equal representation.
- Core assumption: Balancing the number of examples per group during training prevents the classifier from being biased towards larger groups.
- Evidence anchors:
  - [abstract] "The latter ensures that equal importance is given to all subgroups instead of focusing more on larger groups."
  - [section 4.2] "we train a classifier on the combination of original and generated examples, using equal sampling... ensuring equal number of examples for each group."
  - [corpus] Weak - corpus neighbors do not discuss equal sampling for fairness.
- Break condition: If the class distribution is highly imbalanced, equal sampling per group may not be sufficient to prevent bias, and additional techniques like class weighting may be needed.

## Foundational Learning

- Concept: Hierarchical group structure in intersectional fairness
  - Why needed here: The method relies on the assumption that intersectional groups can be represented as intersections of their parent groups, enabling targeted data generation.
  - Quick check question: Given a target group g = {Male, African American, under 45}, what are its parent groups according to the hierarchical structure?

- Concept: Maximum Mean Discrepancy (MMD) as a distribution similarity measure
  - Why needed here: The method uses MMD to train the generative model, ensuring the generated data resembles the true distribution of the target group.
  - Quick check question: What is the key idea behind using MMD to compare two distributions P and Q, given samples from each?

- Concept: Fairness metrics for intersectional fairness
  - Why needed here: The method is evaluated using intersectional fairness metrics (DF and IFα) to assess its effectiveness in improving fairness across subgroups.
  - Quick check question: How does α-Intersectional Fairness (IFα) differ from Differential Fairness (DF) in terms of the trade-off between relative and absolute performance?

## Architecture Onboarding

- Component map:
  Data -> Generative Model -> Classifier
  Original dataset with sensitive attributes and labels -> Learns to generate data for underrepresented groups by combining parent group data -> Trained on original + generated data using equal sampling

- Critical path:
  1. Identify hierarchical group structure in dataset
  2. Train generative models for each group and label using MMD loss
  3. Generate additional data for smaller groups by combining parent group data
  4. Train classifier on balanced dataset (original + generated data)

- Design tradeoffs:
  - Simple vs. complex generative model: Simpler models reduce overfitting risk but may limit expressiveness
  - MMD vs. other divergences: MMD is easier to implement but may not capture all aspects of the distribution

- Failure signatures:
  - Poor fairness metrics: Generated data may not be representative of target groups
  - Low classifier accuracy: Generated data may introduce noise or bias

- First 3 experiments:
  1. Evaluate the diversity and distinguishability of generated data compared to real data
  2. Assess the impact of generated data on fairness-accuracy trade-offs for different fairness methods
  3. Investigate the influence of intersectionality (number of sensitive axes) on the effectiveness of the approach

## Open Questions the Paper Calls Out
The paper explicitly acknowledges that its approach assumes accurate sensitive annotations and a static view of fairness, which may not hold in practice. It notes that temporal data drift where the distribution of sensitive attributes changes over time is an important limitation that requires further investigation.

## Limitations
- The approach assumes the hierarchical structure assumption holds - that parent group distributions can reliably approximate target group distributions
- MMD loss implementation details are underspecified, particularly regarding how generated examples are sampled from parent groups during training
- Evaluation focuses on synthetic data generation rather than testing whether the approach improves real data collection or annotation practices

## Confidence

**High confidence**: The fairness metrics used (DF and IFα) are well-established in the literature, and the basic experimental setup (comparing unconstrained vs constrained fairness methods on augmented data) is sound. The observation that generated data improves fairness-accuracy trade-offs is supported by quantitative results.

**Medium confidence**: The mechanism by which hierarchical structure enables effective data generation is theoretically plausible but relies on untested assumptions about distributional relationships between parent and child groups. The claim that MMD ensures generated data resembles true distributions needs more rigorous validation.

**Low confidence**: The assertion that generated data are "diverse and not mere replicas" lacks strong quantitative support beyond qualitative observations. The comparison to "leveling down" phenomena is interesting but not directly tested - we don't see whether unconstrained methods actually perform worse on worst-off groups.

## Next Checks

1. **Distribution similarity validation**: Implement quantitative measures (e.g., t-SNE visualization, classifier distinguishability scores) to rigorously assess how closely generated data matches target group distributions across all datasets.

2. **Parent-child distributional assumptions**: Systematically test the core assumption by measuring distributional differences between parent groups and their intersections, and evaluate how sensitive performance is to violations of this assumption.

3. **Leveling down comparison**: Run experiments comparing the proposed method against unconstrained fairness methods while specifically tracking worst-off group performance to directly validate claims about avoiding "leveling down" effects.