---
ver: rpa2
title: Scaling Laws for Forgetting When Fine-Tuning Large Language Models
arxiv_id: '2401.05605'
source_url: https://arxiv.org/abs/2401.05605
tags:
- forgetting
- fine-tuning
- scaling
- language
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates catastrophic forgetting during fine-tuning
  of pre-trained large language models (LLMs). The study examines the relationship
  between fine-tuning performance, the amount of forgetting, and scaling factors such
  as the number of parameters fine-tuned and training steps.
---

# Scaling Laws for Forgetting When Fine-Tuning Large Language Models

## Quick Facts
- arXiv ID: 2401.05605
- Source URL: https://arxiv.org/abs/2401.05605
- Authors: Damjan Kalajdzievski
- Reference count: 24
- Primary result: Forgetting in LLM fine-tuning follows inverse linear relationship with fine-tuning loss and power law scaling with parameters and steps

## Executive Summary
This paper investigates catastrophic forgetting during fine-tuning of pre-trained large language models (LLMs). Through experiments with LoRA adapters on Llama 2 7B chat, the study derives scaling laws showing that forgetting increases as a shifted power law in the number of parameters fine-tuned and training steps. The research reveals a strong inverse linear relationship between forgetting and fine-tuning loss, meaning higher performance on the fine-tuning dataset is associated with greater forgetting. The study demonstrates forgetting occurs across different dataset types including instruction-tuning, news articles, and observes forgetting of safety guardrails and reasoning capabilities, highlighting critical challenges for practical LLM deployment.

## Method Summary
The study conducts fine-tuning experiments using LoRA adapters on Llama 2 7B chat model across three datasets: OpenOrca, News, and WikiText-103. The experiments systematically vary the number of trainable parameters (2.5M to 40M) and training steps (25 to 1600) while measuring both fine-tuning performance and forgetting of pre-trained capabilities. The forgetting is quantified by comparing model performance on pre-training benchmarks before and after fine-tuning. The analysis establishes scaling relationships between forgetting and key variables including loss, parameters, and training steps.

## Key Results
- Forgetting exhibits a strong inverse linear relationship with fine-tuning loss
- Forgetting increases as a shifted power law in the number of parameters fine-tuned
- Forgetting also follows a shifted power law relationship with the number of training steps
- Safety guardrails and reasoning capabilities are subject to forgetting during fine-tuning
- Scaling laws hold across different dataset types (instruction-tuning, news, general text)

## Why This Works (Mechanism)
Catastrophic forgetting occurs when fine-tuning causes LLMs to lose previously acquired knowledge and capabilities. The mechanism involves optimization processes that prioritize fitting new training data while gradually erasing or overwriting older learned representations. As the model adapts to new tasks, the weights and parameters that encoded previous knowledge are modified, leading to degradation of performance on pre-existing tasks. The inverse relationship between forgetting and fine-tuning loss emerges because optimization pressure that improves fit to new data necessarily comes at the expense of maintaining old capabilities.

## Foundational Learning

**Catastrophic forgetting**: The phenomenon where neural networks lose previously learned information when trained on new tasks. Understanding this is crucial because it directly impacts the reliability and longevity of fine-tuned models in production.

**Parameter-efficient fine-tuning**: Methods like LoRA that update only a small subset of parameters rather than the full model. This is important because it allows for faster training and reduced computational costs while still achieving good performance.

**Scaling laws**: Mathematical relationships that describe how model performance or behavior changes with respect to model size, training compute, or other factors. These are essential for predicting and understanding model behavior across different scales.

**Inverse linear relationships**: Mathematical relationships where one variable decreases proportionally as another increases. Recognizing these patterns helps in understanding tradeoffs between competing objectives like fine-tuning performance and retention of prior knowledge.

**Quick check**: Verify that the inverse relationship holds by plotting forgetting versus fine-tuning loss across multiple experiments to confirm the linear trend.

## Architecture Onboarding

**Component map**: Pre-trained LLM -> LoRA adapters (2.5M-40M parameters) -> Fine-tuning process -> Performance evaluation (fine-tuning loss, forgetting metrics)

**Critical path**: Pre-trained model initialization → LoRA adapter attachment → Dataset loading → Fine-tuning optimization → Performance measurement → Forgetting quantification

**Design tradeoffs**: The study prioritizes parameter efficiency (LoRA) over full model fine-tuning to reduce computational costs, accepting that this may limit the maximum achievable performance but provides cleaner isolation of forgetting effects.

**Failure signatures**: Rapid degradation of pre-trained capabilities, unexpected performance drops on out-of-distribution data, and disproportionate forgetting of safety-related capabilities.

**First experiments**:
1. Fine-tune with minimal parameters (2.5M) for few steps (25) to establish baseline forgetting
2. Increase parameters to 40M while keeping steps constant to isolate parameter effect
3. Fix parameters at 10M and vary steps from 25 to 1600 to isolate step count effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does catastrophic forgetting in large language models vary across different types of fine-tuning tasks (e.g., knowledge acquisition, reasoning skills, safety alignment)?
- Basis in paper: Explicit - The paper discusses forgetting of knowledge, reasoning, and safety guardrails when fine-tuning Llama 2 7B chat on different datasets (OpenOrca, News, WikiText-103).
- Why unresolved: While the paper observes different levels of forgetting across these tasks, it does not provide a comprehensive analysis of how the nature of the fine-tuning task affects the extent and characteristics of forgetting.
- What evidence would resolve it: Systematic experiments fine-tuning on a wide variety of task types, quantifying and comparing the degree of forgetting for each, could reveal patterns in how different types of knowledge and skills are affected.

### Open Question 2
- Question: Can regularization techniques effectively mitigate catastrophic forgetting during fine-tuning of large language models?
- Basis in paper: Inferred - The paper highlights the need for techniques to mitigate forgetting and suggests this as a direction for future work.
- Why unresolved: The paper does not explore or evaluate any specific techniques for reducing forgetting, such as regularization methods that penalize deviation from the pre-trained model's behavior.
- What evidence would resolve it: Experiments applying regularization techniques during fine-tuning and measuring their impact on both fine-tuning performance and forgetting could demonstrate their effectiveness.

### Open Question 3
- Question: How do the scaling laws for forgetting change when using different fine-tuning methods beyond LoRA, such as full model fine-tuning or other parameter-efficient methods?
- Basis in paper: Explicit - The paper derives scaling laws for forgetting with LoRA fine-tuning and briefly examines generalization to other methods in the appendix.
- Why unresolved: While the paper shows that the scaling laws hold for some other methods, a comprehensive analysis across a wider range of fine-tuning approaches is needed to fully understand how the laws generalize.
- What evidence would resolve it: Deriving and comparing scaling laws for forgetting across multiple fine-tuning methods, including full model fine-tuning, could reveal how the choice of method impacts the relationship between forgetting, model size, and training steps.

## Limitations

- Limited model size (7B parameters) may not generalize to larger models where different forgetting dynamics could emerge
- Focus on single fine-tuning method (LoRA) restricts understanding of how other approaches affect forgetting
- Relatively small number of experimental trials (20) per condition limits statistical power
- Limited exploration of task-specific forgetting patterns across diverse capability types

## Confidence

**High**: The inverse linear relationship between forgetting and fine-tuning loss
**Medium**: The derived scaling laws for parameters and training steps
**Low**: Generalization to larger models and different fine-tuning methods

## Next Checks

1. Replicate the scaling laws experiments with larger models (70B+ parameters) and alternative parameter-efficient fine-tuning methods like prefix tuning or adapters
2. Conduct longitudinal studies to measure forgetting rates over extended fine-tuning periods and across multiple fine-tuning phases
3. Evaluate the practical impact of forgetting on safety-critical tasks by testing fine-tuned models on out-of-distribution safety benchmarks