---
ver: rpa2
title: 'ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language
  Models'
arxiv_id: '2408.08554'
source_url: https://arxiv.org/abs/2408.08554
tags:
- quantization
- abq-llm
- tops
- memory
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ABQ-LLM introduces a novel arbitrary-bit quantization framework
  for efficient LLM inference. It addresses performance degradation in low-bit quantization
  through distribution correction and a bit balance strategy, while enabling arbitrary-precision
  computation via a custom GPU kernel that reconstructs matrix multiplication using
  binary tensor cores.
---

# ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models

## Quick Facts
- arXiv ID: 2408.08554
- Source URL: https://arxiv.org/abs/2408.08554
- Reference count: 24
- Key outcome: W2*A8 configuration achieves WikiText2 perplexity of 7.59 (vs 9.76 in AffineQuant), 1.6× acceleration over SmoothQuant, and 2.7× memory compression

## Executive Summary
ABQ-LLM introduces a novel arbitrary-bit quantization framework for efficient LLM inference that addresses performance degradation in low-bit quantization through distribution correction and a bit balance strategy. The framework enables arbitrary-precision computation via a custom GPU kernel that reconstructs matrix multiplication using binary tensor cores, eliminating limitations of INT4/INT8 units and avoiding GEMV inefficiencies. Experiments on LLaMA models demonstrate significant improvements across various quantization settings, achieving state-of-the-art performance with 2.7× memory compression and 1.6× acceleration.

## Method Summary
ABQ-LLM combines distribution correction methods, a bit balance strategy, and a custom software engine for arbitrary-bit quantized inference. The distribution correction applies double cosine similarity distribution correction (DLC) loss and attention map distribution bootstrap to mitigate quantization degradation. The bit balance strategy extends INT2 symmetric quantization space from {-2, -1, 0, 1} to {-2, -1, 0, 1, 2} to preserve weight distributions at very low bit-widths. The custom software engine (ABQKernel) decomposes arbitrary quantization combinations into superposition of 1-bit matrix multiplications using binary tensor core equivalents, eliminating INT4/INT8 unit limitations and GEMV inefficiencies.

## Key Results
- W2*A8 configuration achieves WikiText2 perplexity of 7.59, outperforming AffineQuant (9.76) and SmoothQuant (8.07)
- Achieves 1.6× inference acceleration over SmoothQuant and 2.7× memory compression
- Maintains competitive performance across W4A4, W6A6 configurations with minimal perplexity degradation
- Enables arbitrary-precision combinations (W2A8, W4A4, W6A6) through custom GPU kernel

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distribution correction mitigates performance degradation from full quantization of weights and activations in transformer blocks.
- Mechanism: Applies double cosine similarity distribution correction (DLC) loss to the output of down proj linear layers and attention map distribution bootstrap to correct quantized model distribution differences.
- Core assumption: Distribution differences between full-precision and quantized outputs are the primary cause of performance degradation at low bit-widths.
- Evidence anchors:
  - [abstract] "distribution correction method for transformer blocks to mitigate distribution differences caused by full quantization of weights and activations"
  - [section 3.2] "we apply a double logarithm of cosine similarity loss on the output of down proj to correct the distribution of the quantized model"
  - [corpus] Weak evidence - no direct citations about distribution correction methods

### Mechanism 2
- Claim: Bit balance strategy restores model performance at very low bit-widths (e.g., INT2) by addressing asymmetric loss issues.
- Mechanism: Extends INT2 symmetric quantization space from {-2, -1, 0, 1} to {-2, -1, 0, 1, 2} to preserve original symmetric weight distribution.
- Core assumption: Standard INT2 quantization disrupts symmetric weight distributions, causing performance degradation.
- Evidence anchors:
  - [abstract] "bit balance strategy to counteract performance degradation from asymmetric distribution issues at very low bit-widths"
  - [section 3.3] "we adopted the bit balance strategy like (Li et al. 2016; Ma et al. 2024a), extending the INT2 symmetric quantization space to {-2, -1, 0, 1, 2}"
  - [corpus] Weak evidence - no direct citations about bit balance strategy in quantization

### Mechanism 3
- Claim: Custom software engine enables arbitrary-precision quantized inference by reconstructing quantization matrix multiplication using Binary TensorCore equivalents.
- Mechanism: Decomposes any combination of quantization into superposition of 1-bit matrix multiplications, eliminating INT4/INT8 unit limitations and GEMV inefficiencies.
- Core assumption: Arbitrary precision combinations can be mathematically decomposed into 1-bit operations and shift operations.
- Evidence anchors:
  - [abstract] "innovative quantization acceleration framework that reconstructs the quantization matrix multiplication of arbitrary precision combinations based on BTC (Binary TensorCore) equivalents"
  - [section 3.4] "we decompose the operation of arbitrary quantized combinations into a superposition of 1-bit matrix multiplications"
  - [corpus] Weak evidence - no direct citations about arbitrary-precision inference engines

## Foundational Learning

- Concept: Matrix multiplication decomposition into binary operations
  - Why needed here: Core to understanding how arbitrary-bit inference engine works
  - Quick check question: How would you decompose a 3-bit matrix multiplication into 1-bit operations?

- Concept: Distribution correction and similarity metrics
  - Why needed here: Understanding DLC loss and attention map correction
  - Quick check question: What is the difference between cosine similarity and KL divergence in distribution correction?

- Concept: Quantization bit-width tradeoffs
  - Why needed here: Understanding why different bit-widths are used for weights vs activations
  - Quick check question: Why might you choose W2A8 vs W4A4 for different deployment scenarios?

## Architecture Onboarding

- Component map: Distribution correction module -> Bit balance strategy -> Custom software engine (ABQKernel) -> Calibration system
- Critical path: Calibration → Distribution correction → Bit balance → Custom inference engine
- Design tradeoffs:
  - W2A8 provides best memory compression but requires bit balance strategy
  - W4A4 offers better accuracy but less compression
  - Custom engine enables arbitrary precision but adds implementation complexity
- Failure signatures:
  - Performance degradation without distribution correction applied
  - Asymmetric weight distributions in low-bit quantization
  - GEMV inefficiencies without engine optimizations
- First 3 experiments:
  1. Validate distribution correction: Compare W4A4 performance with and without DLC loss
  2. Test bit balance: Measure performance difference between W2A8 and W2*A8
  3. Benchmark engine: Compare ABQKernel throughput vs CUTLASS for various precision combinations

## Open Questions the Paper Calls Out
None

## Limitations
- Distribution correction effectiveness is claimed but specific component impacts are not isolated
- Bit balance strategy scope is limited to one specific configuration (W2A8)
- Engine decomposition assumptions rely on mathematical validity that is assumed but not exhaustively proven

## Confidence
- **High confidence**: Memory compression metrics (2.7× reduction with W2*A8) and inference latency improvements (1.6× over SmoothQuant)
- **Medium confidence**: Per-layer precision tuning and pipeline optimization contributions to performance
- **Low confidence**: Isolated impact of distribution correction mechanism on perplexity scores

## Next Checks
1. Ablation study on distribution correction: Implement W4A4 configuration with and without DLC loss and attention map bootstrap to quantify their individual contributions to perplexity improvement.
2. Cross-model validation: Apply ABQ-LLM to transformer architectures beyond LLaMA (e.g., OPT, Falcon) to test generalizability of bit balance strategy and distribution correction across different model families.
3. Precision decomposition verification: Mathematically verify that all claimed arbitrary-precision combinations (W2A8, W4A4, W6A6) can be correctly decomposed into binary tensor core operations without precision loss or computational overhead.