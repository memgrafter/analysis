---
ver: rpa2
title: Self-Supervised Speech Representations are More Phonetic than Semantic
arxiv_id: '2406.08619'
source_url: https://arxiv.org/abs/2406.08619
tags:
- word
- speech
- pairs
- representations
- s3ms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether self-supervised speech models (S3Ms)
  encode phonetic or semantic properties more effectively at the word level. The authors
  curate a novel dataset of near-homophone and synonym word pairs and measure cosine
  similarities between S3M word representations.
---

# Self-Supervised Speech Representations are More Phonetic than Semantic

## Quick Facts
- arXiv ID: 2406.08619
- Source URL: https://arxiv.org/abs/2406.08619
- Reference count: 0
- Self-supervised speech models encode more phonetic than semantic content at word level

## Executive Summary
This study investigates whether self-supervised speech models (S3Ms) encode phonetic or semantic properties more effectively at the word level. Through systematic analysis of word pair similarities across multiple S3M architectures, the authors demonstrate that phonetically similar words consistently exhibit higher similarity scores than semantically similar words. The research also challenges the adequacy of common intent classification benchmarks for measuring semantic understanding, showing that simple word identity baselines can outperform S3Ms on these tasks.

## Method Summary
The authors curate datasets of near-homophone and synonym word pairs from LibriSpeech and Multilingual Spoken Words corpus, extracting S3M representations using both feature and audio slicing methods with various pooling strategies. They measure cosine similarities between word pairs across different layers and architectures (wav2vec 2.0, HuBERT, WavLM, XLS-R). The study also evaluates intent classification performance using a simple bag-of-words baseline to assess semantic capabilities.

## Key Results
- Phonetically similar word pairs show significantly higher similarity than semantically similar pairs across all S3M layers
- Feature slicing causes representation squashing, making all word pairs appear more similar
- Bag-of-words baseline outperforms S3M-based models on intent classification tasks, questioning the semantic validity of these benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Phonetic similarity is encoded more strongly than semantic similarity in S3M representations. The model learns to represent speech at the acoustic-phonetic level because self-supervised training objectives focus on distinguishing phonetic units rather than higher-level semantics. Phonetic distinctions are more salient and easier to learn from raw audio than semantic distinctions, which require higher-level linguistic context.

### Mechanism 2
Feature slicing leads to representation squashing, making all word pairs appear more similar. The transformer architecture mixes contextual information from the entire utterance during feature slicing, causing the resulting representations to converge towards similar directions in the embedding space. This mixing effect reduces the discriminative power of word-level representations.

### Mechanism 3
Word identity information alone is sufficient for high performance on certain intent classification tasks, suggesting these tasks don't measure true semantic understanding. The evaluated intent classification datasets have biases where intent can be inferred from specific keywords or phrases, making the task more about pattern matching than understanding meaning.

## Foundational Learning

- **Self-supervised learning in speech models**: Why needed - Understanding how S3Ms learn representations without explicit labels is fundamental to interpreting phonetic vs. semantic encoding results. Quick check - What is the primary training objective used in wav2vec 2.0 and how does it differ from supervised phoneme classification?

- **Phonetic vs. semantic similarity**: Why needed - The paper's novel contribution is comparing these two types of similarity in S3M representations, so understanding the difference is crucial for interpreting results. Quick check - Can you give an example of two words that are phonetically similar but semantically unrelated, and vice versa?

- **Cosine similarity and embedding spaces**: Why needed - The paper uses cosine similarity to measure distances between word representations, so understanding this metric and anisotropic embeddings is important. Quick check - What does it mean if two word representations have a cosine similarity close to 1.0, and why might this not always indicate true semantic similarity?

## Architecture Onboarding

- **Component map**: S3M models (wav2vec 2.0, HuBERT, WavLM, XLS-R) -> Word pair datasets (near-homophones, synonyms) -> Similarity measurement (cosine similarity) -> Intent classification task setup
- **Critical path**: 1) Extract word-level representations from S3Ms, 2) Measure similarities between word pairs, 3) Analyze layer-wise trends, 4) Compare with intent classification performance
- **Design tradeoffs**: Feature slicing vs. audio slicing (context preservation vs. word-level focus), different pooling methods (mean, center, centroid), choice of similarity metric (cosine vs. others)
- **Failure signatures**: Overlapping confidence intervals in similarity scores, high performance of word identity baseline on intent tasks, squashing of representations with feature slicing
- **First 3 experiments**: 1) Replicate similarity analysis with different S3M architecture to verify phonetic dominance, 2) Test effect of different similarity metric (e.g., Euclidean distance), 3) Modify intent classification datasets to remove keyword biases and re-evaluate performance gap

## Open Questions the Paper Calls Out

- Do S3Ms exhibit semantic understanding when evaluated with more rigorous benchmarks beyond intent classification tasks? The paper challenges the adequacy of current intent classification datasets but doesn't explore whether S3Ms might perform better on more challenging semantic benchmarks.

- How do phonetic and semantic encoding capabilities of S3Ms vary across different languages and linguistic structures? The study is limited to a small set of languages and doesn't explore how linguistic differences affect the encoding of phonetic vs. semantic information.

- What specific mechanisms within S3M architectures contribute to the dominance of phonetic encoding over semantic encoding? The paper observes phonetic dominance but doesn't investigate the architectural or training-related factors driving this behavior.

## Limitations

- Analysis is confined to word-level representations without examining sub-word or sentence-level semantics
- Feature slicing methodology may introduce representation squashing due to transformer mixing
- Cross-lingual analysis is limited by availability of semantic resources in non-English languages

## Confidence

- Phonetic dominance in S3M representations: High
- Feature slicing causes representation squashing: Low
- Current intent classification tasks don't measure true semantic understanding: Medium
- Cross-lingual findings generalize: Low

## Next Checks

1. Test the phonetic dominance hypothesis using a transformer variant with temporal locality preservation (e.g., conformer) to determine if architecture affects the phonetic-semantic balance.

2. Conduct controlled experiments with artificially constructed word pairs varying in phonetic and semantic similarity independently to isolate the relative contributions of each property to representation similarity.

3. Evaluate S3M performance on more complex semantic tasks requiring compositional understanding (e.g., natural language inference with speech input) to test whether phonetic dominance persists at higher semantic levels.