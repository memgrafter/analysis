---
ver: rpa2
title: Evidence from counterfactual tasks supports emergent analogical reasoning in
  large language models
arxiv_id: '2404.13070'
source_url: https://arxiv.org/abs/2404.13070
tags:
- gpt-4
- sequence
- problems
- code
- interval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work responds to critiques suggesting that large language
  models' analogical reasoning abilities are due to memorization of training data.
  The authors tested models on counterfactual tasks with permuted alphabets and found
  degraded performance, which they attribute to models' difficulty with counting rather
  than analogical reasoning per se.
---

# Evidence from counterfactual tasks supports emergent analogical reasoning in large language models

## Quick Facts
- arXiv ID: 2404.13070
- Source URL: https://arxiv.org/abs/2404.13070
- Reference count: 40
- Large language models demonstrate emergent analogical reasoning capabilities but are constrained by auxiliary task demands like counting

## Executive Summary
This work addresses critiques that large language models' analogical reasoning abilities stem from memorization of training data. The authors tested models on counterfactual tasks using permuted alphabets and found degraded performance, which they attribute to counting difficulties rather than limitations in analogical reasoning per se. When GPT-4 was augmented with code execution to handle counting tasks, its performance matched human benchmarks. This suggests that while models can engage in analogical reasoning, they are limited by auxiliary task demands. The findings highlight the importance of carefully designed evaluations that isolate core competencies from confounding auxiliary demands when assessing artificial cognitive capacities.

## Method Summary
The authors developed a novel experimental paradigm using counterfactual tasks with permuted alphabets to test whether large language models could perform analogical reasoning without relying on memorized training data. They compared model performance on standard alphabet tasks versus counterfactual versions where letter orders were systematically permuted. Human participants completed the same tasks to establish baseline performance. Additionally, they augmented GPT-4 with code execution capabilities to test whether performance limitations stemmed from counting difficulties rather than analogical reasoning constraints. The experimental design aimed to isolate core analogical reasoning abilities from auxiliary task demands.

## Key Results
- Models showed degraded performance on counterfactual tasks with permuted alphabets compared to standard tasks
- When GPT-4 was augmented with code execution for counting, performance matched human benchmarks on counterfactual tasks
- The performance gap between vanilla and code-augmented models suggests auxiliary task demands, not analogical reasoning limitations, constrain performance

## Why This Works (Mechanism)
The mechanism underlying the observed analogical reasoning capabilities appears to rely on the models' ability to learn and apply structural mappings between different domains. The models can extract relational patterns and transfer them across contexts, which is the essence of analogical reasoning. However, auxiliary task components like precise counting operations can interfere with or overwhelm these core reasoning capabilities. By offloading these auxiliary demands to external computational resources (code execution), the models can better demonstrate their inherent analogical reasoning abilities.

## Foundational Learning
- **Analogical reasoning**: Understanding of how humans and models map relational structures between different domains; needed to interpret the core cognitive capacity being tested; quick check: can identify relational patterns across different contexts
- **Counterfactual reasoning**: Ability to reason about alternative scenarios that differ from reality; needed to understand the experimental paradigm; quick check: can evaluate "what if" scenarios
- **Transformer architecture**: Understanding of how attention mechanisms and positional embeddings work in language models; needed to contextualize model capabilities; quick check: can explain self-attention and token processing
- **In-context learning**: Understanding of how models learn from examples provided within prompts; needed to contextualize broader implications; quick check: can describe few-shot learning mechanisms

## Architecture Onboarding
**Component map**: Input prompt -> Token embedding -> Attention layers -> Output prediction
**Critical path**: The core analogical reasoning capability flows through the attention mechanisms and relational pattern extraction, while auxiliary demands like counting create bottlenecks that can be offloaded to external code execution
**Design tradeoffs**: The study highlights the tension between general-purpose language modeling and specialized cognitive capabilities, suggesting that hybrid approaches combining language models with external tools may better capture human-like reasoning
**Failure signatures**: Performance degradation on counterfactual tasks indicates either analogical reasoning limitations or auxiliary task constraints; the differential performance with code augmentation helps disambiguate these
**First experiments**: 1) Test additional counterfactual transformations beyond alphabet permutations, 2) Evaluate different model architectures with varying counting capabilities, 3) Manipulate the complexity of auxiliary task demands while holding analogical reasoning constant

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the relationship between the mechanisms supporting analogical reasoning in large language models and those that implement reasoning in the human brain?
- Basis in paper: [explicit] The authors state: "It remains an important priority for future work to determine whether and how the mechanisms that support this capacity in large language models relate to those that implement reasoning in the human brain."
- Why unresolved: This question is explicitly identified as an open priority for future work by the authors. While the paper discusses emergent analogical reasoning in LLMs, it does not explore the neural or cognitive mechanisms underlying this capacity or compare them to human reasoning mechanisms.
- What evidence would resolve it: Neuroimaging studies comparing brain activity during analogical reasoning tasks in humans and LLMs, or detailed computational models that explicitly map LLM mechanisms to cognitive processes.

### Open Question 2
- Question: How can auxiliary task demands be effectively isolated from core competencies when evaluating artificial cognitive capacities?
- Basis in paper: [explicit] The authors argue: "it is important to disambiguate domain-specific failures in processes such as physical reasoning – or, in this case, counting – from the evaluation of core competencies such as analogical reasoning" and cite concerns about auxiliary task demands [8].
- Why unresolved: While the paper demonstrates the importance of isolating auxiliary demands (using code execution to handle counting), it does not provide a general framework for identifying and controlling for such demands across different cognitive tasks and model architectures.
- What evidence would resolve it: A systematic methodology for decomposing cognitive tasks into core and auxiliary components, validated across multiple domains and model types.

### Open Question 3
- Question: To what extent does in-context learning in transformer-based language models depend on analogical reasoning or schema induction?
- Basis in paper: [explicit] The authors suggest: "in-context learning – the capacity of large language models to rapidly learn new tasks by conditioning their predictions on a small set of in-context task examples [13] – may itself depend on analogical reasoning (or more generally, schema induction)."
- Why unresolved: This is presented as a hypothesis based on the dependence of in-context learning on structured mechanisms for similarity-based inductive inference [14, 15], but the paper does not directly test this relationship.
- What evidence would resolve it: Experiments manipulating the similarity structure of in-context examples and measuring the impact on learning performance, or ablation studies identifying the specific mechanisms underlying in-context learning.

## Limitations
- The study focuses on specific task types and model architectures, which may limit generalizability to broader analogical reasoning domains
- The interpretation that counting limitations are the primary bottleneck for counterfactual task performance, while compelling, requires careful consideration of alternative explanations
- The exact relationship between counting proficiency and analogical reasoning remains somewhat speculative without direct manipulation of counting abilities independent of analogical reasoning demands

## Confidence
- **High confidence**: The core finding that models can perform analogical reasoning but struggle with auxiliary task components like counting
- **Medium confidence**: The interpretation that counting limitations are the primary bottleneck for counterfactual task performance
- **Medium confidence**: The generalizability of findings to broader analogical reasoning domains

## Next Checks
1. Conduct ablation studies that separately manipulate counting complexity and analogical reasoning demands within the same task framework to establish causal relationships
2. Test additional model architectures with varying degrees of symbolic reasoning capabilities to determine whether the counting-analogy relationship holds across different computational approaches
3. Evaluate model performance on counterfactual tasks with alternative auxiliary demands (e.g., spatial reasoning, temporal ordering) to determine whether counting is uniquely problematic or if other auxiliary components similarly constrain performance