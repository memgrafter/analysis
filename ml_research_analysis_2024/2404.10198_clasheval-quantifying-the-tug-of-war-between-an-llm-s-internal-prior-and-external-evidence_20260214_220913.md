---
ver: rpa2
title: 'ClashEval: Quantifying the tug-of-war between an LLM''s internal prior and
  external evidence'
arxiv_id: '2404.10198'
source_url: https://arxiv.org/abs/2404.10198
tags:
- prior
- information
- retrieved
- response
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how large language models (LLMs) handle
  conflicting information from their internal knowledge (prior) and retrieved documents
  (RAG). A dataset of over 1200 questions across six domains is created, with retrieved
  documents systematically perturbed to introduce varying degrees of incorrect information.
---

# ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence

## Quick Facts
- arXiv ID: 2404.10198
- Source URL: https://arxiv.org/abs/2404.10198
- Authors: Kevin Wu; Eric Wu; James Zou
- Reference count: 6
- Primary result: LLMs adopt incorrect retrieved content over 60% of the time, overriding their own correct prior knowledge

## Executive Summary
This study investigates how large language models handle conflicting information between their internal knowledge and retrieved documents in RAG systems. The researchers created a dataset of over 1200 questions across six domains, systematically perturbing retrieved documents to introduce varying degrees of incorrect information. Six top-performing LLMs including GPT-4o were benchmarked on this dataset. The results reveal that LLMs are susceptible to adopting incorrect retrieved content, with over 60% override rate, but the more unrealistic the retrieved content, the less likely adoption occurs. The study also demonstrates that model confidence in its initial response inversely correlates with willingness to adopt retrieved information.

## Method Summary
The researchers created a systematic evaluation framework by generating questions using GPT-4 across six domains (geographical, biographical, sports, company, movies, products). Retrieved documents were then perturbed using GPT-4 to create five levels of incorrect information ranging from 20% to 500% deviation from ground truth. Six LLMs (GPT-4, GPT-3.5, Mistral-7B, Llama-2, Claude-3, Gemini) were tested on this dataset with and without retrieved context. Model responses were evaluated by comparing answers with and without RAG to determine preference rates. Token-level confidence was measured using log probabilities of the prior response.

## Key Results
- LLMs adopt incorrect retrieved content over 60% of the time, overriding their own correct prior knowledge
- More unrealistic retrieved content (larger deviations from truth) is less likely to be adopted by models
- Lower model confidence in initial response correlates with higher likelihood of adopting retrieved content
- Simple methods can improve model accuracy when facing conflicting retrieved information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model's token-level confidence in its prior response inversely correlates with its willingness to adopt retrieved information
- Mechanism: Low log-probability tokens in the prior response signal uncertainty, making the model more receptive to externally provided facts that contradict its prior knowledge
- Core assumption: Token log-probabilities reflect the model's confidence in a given response
- Evidence anchors:
  - [abstract] "the less confident a model is in its initial response (via measuring token probabilities), the more likely it is to adopt the information in the retrieved content"
  - [section] "we observe a consistent negative relationship between the token probability of the model's prior answer and the associated RAG preference rate"
  - [corpus] Weak — related works measure confidence but don't explicitly link it to RAG preference in this manner
- Break condition: If token probabilities do not reliably indicate confidence, or if the model's confidence metric is manipulated by prompt phrasing

### Mechanism 2
- Claim: The degree of deviation between retrieved content and the model's prior determines the likelihood of adopting the retrieved information
- Mechanism: Small deviations are more likely to be accepted because they seem plausible extensions of the model's knowledge; large deviations are rejected as they appear unrealistic
- Core assumption: The model can internally assess realism of numerical/categorical values
- Evidence anchors:
  - [abstract] "the more unrealistic the retrieved content is (i.e. more deviated from truth), the less likely the model is to adopt it"
  - [section] "as the RAG value diverges from the model's prior, the model is less likely to adopt the RAG value over its own initial response"
  - [corpus] Moderate — studies on noise robustness exist but don't directly measure deviation realism thresholds
- Break condition: If the model's internal "realism check" is bypassed by carefully crafted misinformation

### Mechanism 3
- Claim: Prompt phrasing (strict vs loose adherence) modulates the strength of the prior-vs-RAG tension
- Mechanism: Strict prompts force literal alignment with retrieved context, suppressing prior influence; loose prompts allow the model to arbitrate, amplifying prior effects
- Core assumption: Prompt templates can control the model's degree of prior-vs-RAG preference
- Evidence anchors:
  - [section] "the choice of prompting technique (e.g. strictly adhere, loosely adhere) can influence both the baseline and strength of this relationship"
  - [section] "the strict prompt has uniformly higher RAG adherence than the standard prompt... the loose prompt results in much lower RAG adherence rates as prior probability increases"
  - [corpus] Moderate — prompt engineering literature supports this, but specific RAG preference modulation is not well established
- Break condition: If the model ignores prompt constraints due to strong priors or external context weight

## Foundational Learning

- Concept: Log probability interpretation in language models
  - Why needed here: Used to measure token-level confidence in the model's prior response
  - Quick check question: If a token has log probability -1.0, what is its probability in linear scale? (Answer: 0.368)

- Concept: Retrieval-augmented generation (RAG) architecture
  - Why needed here: Central to understanding how external context interacts with internal knowledge
  - Quick check question: In a RAG system, where does the model look for information first when answering a question? (Answer: Both internal knowledge and retrieved documents)

- Concept: Systematic perturbation of data for robustness testing
  - Why needed here: Method used to generate modified documents with varying degrees of incorrectness
  - Quick check question: If the original value is 50 and the perturbation multiplier is 0.2, what is the modified value? (Answer: 10)

## Architecture Onboarding

- Component map: Question generator (GPT-4) -> Document retriever -> Perturbation engine -> LLM inference (GPT-4, GPT-3.5, Mistral-7B) -> Log probability extractor -> Analysis pipeline

- Critical path: Question generation -> Perturbation creation -> LLM inference with/without context -> Log prob extraction -> Preference rate calculation -> Slope analysis

- Design tradeoffs:
  - Using GPT-4 for perturbations ensures realistic errors but introduces model-dependent bias; using simpler transformations could be more objective but less natural
  - Including multiple models (GPT-3.5, Mistral-7B) adds robustness but increases complexity and API costs

- Failure signatures:
  - Low concordance with or without RAG indicates poor model performance or retrieval failure
  - No correlation between prior probability and RAG preference suggests token probabilities don't reflect confidence
  - Inconsistent RAG preference across perturbation levels indicates flawed perturbation methodology

- First 3 experiments:
  1. Test a single question across all perturbation levels and all models to verify systematic behavior
  2. Swap the perturbation engine from GPT-4 to a deterministic multiplier and compare RAG preference trends
  3. Modify the prompt template to extreme strictness and measure changes in baseline RAG adherence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which models' confidence in their prior responses affects their susceptibility to incorrect retrieved information?
- Basis in paper: [explicit] The paper states "The less confident a model is in its initial response, the more likely it is to adopt the information in the retrieved content."
- Why unresolved: While the paper demonstrates a correlation between model confidence and susceptibility to incorrect retrieved information, it does not explain the underlying mechanism driving this relationship.
- What evidence would resolve it: Experiments that systematically manipulate model confidence through different prompting techniques or model architectures, while controlling for other factors, could shed light on the causal relationship between confidence and susceptibility.

### Open Question 2
- Question: How do different types of retrieval errors (e.g., typos, ambiguities, missing information) affect model behavior compared to the systematic perturbations used in this study?
- Basis in paper: [inferred] The paper acknowledges that "perturbations we produce are based on our priors for what would constitute a reasonable or unreasonable range of values" and that "more discrete types of errors (eg. typos, ambiguities, missing information, etc.) which are harder to simulate."
- Why unresolved: The study uses systematic perturbations to simulate retrieval errors, but real-world retrieval errors may have different characteristics and effects on model behavior.
- What evidence would resolve it: Experiments that introduce various types of realistic retrieval errors and measure their impact on model behavior could provide insights into how models handle different error types.

### Open Question 3
- Question: Can we develop more robust prompting techniques or model architectures that are less susceptible to incorrect retrieved information while still benefiting from correct information?
- Basis in paper: [explicit] The paper suggests that "simple methods are demonstrated to improve model accuracy in the presence of conflicting retrieved content."
- Why unresolved: While the paper demonstrates some simple methods to improve model accuracy, it does not explore the full potential of more sophisticated techniques or model architectures to address this issue.
- What evidence would resolve it: Developing and evaluating novel prompting techniques or model architectures specifically designed to handle conflicting information could lead to more robust and reliable RAG systems.

## Limitations

- The perturbation methodology using GPT-4 may introduce model-dependent biases that don't generalize to other error types
- Token log-probabilities may reflect fluency rather than true confidence in factual accuracy
- The study doesn't address more complex error types like typos, ambiguities, or missing information

## Confidence

- Mechanism 1 (Token confidence correlation): Medium - Strong experimental support but theoretical assumption needs validation
- Mechanism 2 (Deviation realism): Medium - Clear patterns observed but perturbation methodology introduces potential bias
- Mechanism 3 (Prompt effects): High - Well-established in literature with clear experimental demonstration
- Overall framework validity: High - Systematic approach well-executed with appropriate controls

## Next Checks

1. **Token probability validation**: Conduct controlled experiments where models answer questions with known ground truth, then measure whether token log-probabilities inversely correlate with actual error rates across multiple domains and model families.

2. **Perturbation methodology audit**: Replace GPT-4-generated perturbations with deterministic transformations (e.g., fixed percentage deviations) and compare RAG preference patterns to determine if the observed behavior is methodology-dependent.

3. **Real-world conflict scenarios**: Test the framework using naturally occurring conflicting information from real documents (e.g., news articles with contradictory claims) rather than synthetic perturbations to assess ecological validity.