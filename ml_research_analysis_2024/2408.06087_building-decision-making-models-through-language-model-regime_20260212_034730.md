---
ver: rpa2
title: Building Decision Making Models Through Language Model Regime
arxiv_id: '2408.06087'
source_url: https://arxiv.org/abs/2408.06087
tags:
- decision
- making
- data
- tasks
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces "Learning then Using" (LTU), a novel two-stage
  training approach for decision making models using large language models (LLMs).
  Unlike traditional supervised learning, LTU first performs continued pre-training
  on diverse decision-making data to build a foundational model, then refines it for
  specific tasks via supervised fine-tuning.
---

# Building Decision Making Models Through Language Model Regime

## Quick Facts
- arXiv ID: 2408.06087
- Source URL: https://arxiv.org/abs/2408.06087
- Reference count: 5
- LTU achieves 62.1% accuracy in CTR prediction versus 59.9% for SFT

## Executive Summary
This paper introduces a novel "Learning then Using" (LTU) training paradigm for decision-making models using large language models. Unlike traditional supervised learning, LTU employs a two-stage approach: first building a foundational model through continued pre-training on diverse decision-making data, then refining it for specific tasks via supervised fine-tuning. Experiments in e-commerce domains demonstrate that LTU outperforms standard supervised fine-tuning in both effectiveness and generalization, achieving 62.1% accuracy in CTR prediction versus 59.9% for SFT, and showing 3% higher accuracy on out-of-domain tasks.

## Method Summary
The LTU approach follows a two-stage training process: (1) Continued pre-training (CT) on diverse decision-making data using (state, action, reward) triplets to build a foundational model, and (2) Supervised fine-tuning (SFT) on specific tasks. The method uses Llama-2-13B as the base model, trained on 10B tokens of decision-making data across 64 A100 GPUs. Decision-making data is formatted as (s, a, r) triples where state represents context, action represents decisions, and reward represents outcomes. The approach is evaluated on e-commerce tasks including CTR, CPC, and impression prediction.

## Key Results
- LTU achieves 62.1% accuracy in CTR prediction versus 59.9% for standard supervised fine-tuning
- LTU demonstrates 3% higher accuracy on out-of-domain tasks compared to task-specific models
- The foundation model exhibits superior data efficiency, requiring less task-specific data for effective fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LTU improves generalization by injecting broad decision-making knowledge before task-specific refinement
- Mechanism: The continued pre-training phase builds a foundation model by learning from diverse (state, action, reward) triplets across domains, generalizing better than directly fine-tuning on a single task
- Core assumption: Patterns in decision-making are transferable across domains if the model is exposed to enough varied examples during CT
- Evidence anchors: [abstract] "learning phase develops a robust foundational decision making model by integrating diverse knowledge from various domains and decision making contexts"; [section] "we integrate this collective decision making intelligence into a LLM, transforming it into a comprehensive foundation decision making model which is suitable for various downstream tasks"
- Break condition: If the pre-training data lacks sufficient diversity or is biased toward a narrow subset of tasks, the foundation model will fail to generalize

### Mechanism 2
- Claim: LTU's data efficiency stems from the foundation model carrying over learned patterns to new tasks
- Mechanism: After CT, supervised fine-tuning only needs to adapt the foundation model to a specific task, not learn all patterns from scratch, reducing required training data
- Core assumption: The foundation model captures generalizable patterns that are relevant across related decision-making tasks
- Evidence anchors: [abstract] "LTU approach outperforms traditional supervised learning regimes in decision making capabilities and generalization"; [section] "we study whether this new paradigm can provide better decision making abilities than supervised learning and exhibit stronger generalization"
- Break condition: If tasks are too dissimilar, the foundation model's learned patterns may not transfer, and SFT will require nearly as much data as training from scratch

### Mechanism 3
- Claim: LTU mitigates knowledge forgetting by separating general pattern learning from task-specific refinement
- Mechanism: CT focuses on broad decision-making knowledge without overfitting to any specific task, while SFT fine-tunes for precision, preserving general knowledge while adapting to specifics
- Core assumption: LLMs can maintain general knowledge while adapting to specific tasks if the training phases are clearly separated
- Evidence anchors: [abstract] "LTU method embraces a versatile training methodology that combines broad pre-training with targeted fine-tuning"; [section] "we introduce a new training strategy called 'Learning then Using' (LTU), with separate learning and using phase"
- Break condition: If the SFT phase is too aggressive or the CT phase is too narrow, the model may forget general patterns or fail to specialize

## Foundational Learning

- Concept: Causal Language Modeling (CLM)
  - Why needed here: CLM is the core training paradigm used in LTU to predict the next token in sequences of (state, action, reward), enabling the model to learn decision-making patterns
  - Quick check question: How does CLM differ from other language modeling approaches, and why is it suited for decision-making tasks?
- Concept: (State, Action, Reward) triplet structure
  - Why needed here: This structure formalizes decision-making data, allowing the model to learn the relationship between context, decisions, and outcomes
  - Quick check question: What are the roles of state, action, and reward in decision-making, and how are they represented in the training data?
- Concept: Continued Pre-training (CT)
  - Why needed here: CT builds the foundation model by exposing it to diverse decision-making knowledge before task-specific refinement
  - Quick check question: How does CT differ from standard pre-training, and what are its benefits for decision-making models?

## Architecture Onboarding

- Component map: Base LLM (Llama-2-13B) -> Continued Pre-training (CT) module -> Supervised Fine-tuning (SFT) module -> Data pipeline for (state, action, reward) triplets -> Evaluation framework
- Critical path: 1. Prepare diverse decision-making data in (s, a, r) format; 2. Perform CT on the base LLM using the prepared data; 3. Fine-tune the foundation model with SFT on task-specific data; 4. Evaluate the model on the target decision-making task
- Design tradeoffs: Balance between CT data diversity and task-specific SFT data quality; tradeoff between model size and training efficiency; choice of evaluation metrics for decision-making tasks
- Failure signatures: Poor generalization (model performs well on training tasks but fails on unseen tasks); knowledge forgetting (model loses general decision-making patterns during SFT); overfitting (model performs well on training data but poorly on validation or test data)
- First 3 experiments: 1. Train LTU and SFT models on a simple decision-making task (e.g., CTR prediction) and compare performance; 2. Test LTU and SFT models on out-of-domain tasks to evaluate generalization; 3. Ablation study: Train LTU with and without common knowledge data to assess its impact

## Open Questions the Paper Calls Out

- Question: How does the LTU method perform in multi-step decision making tasks compared to single-step tasks?
  - Basis in paper: [explicit] The authors note that while LTU is designed to handle both single-step and multi-step decision making tasks, their experiments only tested single-step decision making problems, and further exploration and practical experiments on other domains are needed
  - Why unresolved: The paper's experiments focused solely on single-step decision making tasks, leaving the effectiveness of LTU in multi-step scenarios unverified
  - What evidence would resolve it: Conducting experiments on multi-step decision making tasks across various domains to compare the performance of LTU with traditional methods

- Question: What is the optimal balance between SEO data and common knowledge data in the LTU training process?
  - Basis in paper: [explicit] The authors found that incorporating common knowledge data negatively affects the training of LTU, as the model trained with a mix of SEO data and common knowledge performed worse than the model trained exclusively on SEO data
  - Why unresolved: The study only tested one specific mix of SEO and common knowledge data, and the impact of different proportions or types of common knowledge data remains unexplored
  - What evidence would resolve it: Experimenting with various ratios of SEO data to common knowledge data and analyzing the performance impact to determine the optimal balance

- Question: How does the LTU method generalize to decision making tasks outside the e-commerce domain?
  - Basis in paper: [explicit] The authors suggest that while LTU shows promise in e-commerce scenarios, its effectiveness in other domains needs exploration and practical experiments
  - Why unresolved: The experiments were limited to e-commerce-related tasks, and the generalization capabilities of LTU in other fields are not yet tested
  - What evidence would resolve it: Applying the LTU method to decision making tasks in diverse domains such as healthcare, finance, or autonomous systems and evaluating its performance compared to traditional methods

## Limitations

- Data quality and domain coverage remain unspecified, making it difficult to assess whether claimed generalization benefits would hold across different decision-making domains
- Experiments focus on e-commerce tasks, leaving unclear whether benefits extend to fundamentally different decision-making scenarios
- Resource intensity (64 A100 GPUs, 10B tokens) raises questions about practicality for organizations with limited computational resources

## Confidence

**High Confidence**: LTU outperforms standard supervised fine-tuning in CTR prediction accuracy (62.1% vs 59.9%) on tested e-commerce tasks; two-stage training approach is technically sound and feasible; knowledge forgetting can be mitigated through clear separation of training phases

**Medium Confidence**: LTU demonstrates 3% higher accuracy on out-of-domain tasks; foundation model exhibits superior data efficiency compared to task-specific models; patterns in decision-making are transferable across domains when exposed to varied examples

**Low Confidence**: LTU will generalize to fundamentally different decision-making domains beyond e-commerce; approach is resource-efficient and practical for organizations with limited computational resources; 3% out-of-domain improvement is statistically significant and robust across different task variations

## Next Checks

1. **Domain Transferability Test**: Implement LTU on a completely different decision-making domain (e.g., healthcare treatment recommendation or financial portfolio optimization) and compare performance against SFT baselines to validate whether claimed generalization benefits extend beyond e-commerce applications

2. **Data Efficiency Analysis**: Systematically vary the amount of task-specific fine-tuning data while keeping the foundation model constant, measuring how much data reduction LTU provides compared to SFT and identifying the point at which the foundation model's learned patterns no longer provide advantages

3. **Knowledge Forgetting Experiment**: Train LTU models with varying degrees of CT data diversity (narrow vs. broad) and measure knowledge retention during SFT using probing techniques to track how general decision-making patterns evolve through both training phases, identifying the optimal balance between foundation building and task specialization