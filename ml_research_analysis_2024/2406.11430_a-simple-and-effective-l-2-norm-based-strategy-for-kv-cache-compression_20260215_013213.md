---
ver: rpa2
title: A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache Compression
arxiv_id: '2406.11430'
source_url: https://arxiv.org/abs/2406.11430
tags:
- head
- would
- intheend
- snottheyearsinyourlifethatcount
- sthelifeinyouryears
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a simple and effective strategy for compressing
  the KV cache in large language models by leveraging the L2 norm of key embeddings.
  The authors observe a strong correlation between low L2 norms and high attention
  scores, indicating that key embeddings with lower L2 norms tend to have higher influence
  during decoding.
---

# A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache Compression

## Quick Facts
- **arXiv ID**: 2406.11430
- **Source URL**: https://arxiv.org/abs/2406.11430
- **Reference count**: 40
- **Primary result**: Up to 50% compression for language modeling tasks while maintaining performance

## Executive Summary
This paper introduces a simple and effective strategy for compressing the KV cache in large language models by leveraging the L2 norm of key embeddings. The authors observe a strong correlation between low L2 norms and high attention scores, indicating that key embeddings with lower L2 norms tend to have higher influence during decoding. Based on this insight, they propose a method that retains only the keys with the lowest L2 norms and their corresponding values, effectively reducing the KV cache size without requiring additional training or modifications to the model. Experiments on language modeling, needle-in-a-haystack, and passkey retrieval tasks show that this approach can compress the KV cache by up to 50% for language modeling and needle-in-a-haystack tasks, and up to 90% for passkey retrieval tasks, while maintaining model performance.

## Method Summary
The method compresses the KV cache by retaining only key-value pairs with the lowest L2 norms of key embeddings. During generation, the L2 norm is computed for each key embedding, and keys are ranked by their L2 norm values. The lowest L2 norm keys and their corresponding values are retained in the cache when it reaches a specified threshold. The method is compatible with FlashAttention and can be applied off-the-shelf to any transformer-based decoder-only LLM without requiring additional training or architectural modifications.

## Key Results
- Achieves up to 50% KV cache compression for language modeling tasks while maintaining model performance
- Maintains up to 90% compression for passkey retrieval tasks with negligible performance degradation
- Demonstrates strong correlation between low L2 norms and high attention scores across multiple experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The L2 norm of key embeddings correlates with their attention scores during decoding.
- Mechanism: Key embeddings with lower L2 norms tend to receive higher attention scores because they contain more salient information for the current query.
- Core assumption: The magnitude of the key embedding itself is a good proxy for its importance in the attention mechanism, independent of the query.
- Evidence anchors:
  - [abstract] "we find a clear correlation between the L2 and the attention scores over cached KV pairs, where a low L2 of a key embedding usually leads to a high attention score during decoding"
  - [section] "we observe that key embeddings with low L2 norm are often associated with higher attention scores"
  - [corpus] Weak evidence - no direct citations supporting this correlation found in the corpus
- Break Condition: If the correlation between L2 norm and attention score weakens or reverses in certain layers or contexts, the compression strategy would fail to retain the most important KV pairs.

### Mechanism 2
- Claim: By retaining only the keys with the lowest L2 norms, the most informative KV pairs are preserved.
- Mechanism: The compression method keeps the KV pairs that are most likely to be attended to based on their key embedding norms, effectively reducing cache size while maintaining model performance.
- Core assumption: The L2 norm of the key embedding is a reliable indicator of the KV pair's overall influence on the model's output.
- Evidence anchors:
  - [abstract] "we compress the KV cache based on the L2 norm of key embeddings"
  - [section] "we propose a simple and highly effective strategy for KV cache compression: keeping in memory only the keys with lowest L2 norm, and the corresponding values"
  - [corpus] No direct evidence found in corpus; the claim is based on the paper's internal analysis
- Break Condition: If the L2 norm becomes an unreliable predictor of KV pair importance (e.g., due to changes in model architecture or training data), the compression strategy would degrade model performance.

### Mechanism 3
- Claim: The compression method is compatible with FlashAttention and does not require additional training or modifications.
- Mechanism: By not relying on attention scores computed during the forward pass, the method can be applied to any transformer-based decoder-only LLM without architectural changes.
- Core assumption: FlashAttention's efficiency does not depend on the specific distribution of L2 norms in the key embeddings.
- Evidence anchors:
  - [abstract] "without relying on the attention scores, this approach remains compatible with FlashAttention, enabling broader applicability"
  - [section] "our method estimates the influence of cached key-value pairs without the need to compute the attention scores"
  - [corpus] No direct evidence found in corpus; the claim is based on the paper's description
- Break Condition: If FlashAttention's internal mechanisms or optimizations rely on the specific distribution of key embeddings, the compression method might interfere with its performance.

## Foundational Learning

- **Attention Mechanism in Transformers**
  - Why needed here: Understanding how attention scores are computed and how they relate to key-value pairs is crucial for grasping why L2 norm compression works.
  - Quick check question: How does the dot-product attention mechanism use key and query vectors to compute attention scores?

- **L2 Norm (Euclidean Norm)**
  - Why needed here: The L2 norm is the core metric used to determine which key-value pairs to retain or discard during compression.
  - Quick check question: What does the L2 norm of a vector represent, and why might a lower L2 norm indicate a more "concentrated" or "focused" embedding?

- **KV Cache in Autoregressive Generation**
  - Why needed here: Understanding how the KV cache grows and is used during generation is essential for appreciating the memory savings from compression.
  - Quick check question: What is the purpose of the KV cache in autoregressive language models, and how does its size scale with context length?

## Architecture Onboarding

- **Component map**:
  Key embeddings -> L2 norm computation -> Compression logic -> Retained KV pairs -> Attention mechanism

- **Critical path**:
  1. Token is processed, generating key and value embeddings
  2. L2 norm is computed for the key embedding
  3. Keys are ranked by L2 norm
  4. Lowest L2 norm keys and their values are retained in the cache
  5. During generation, only retained KV pairs are used for attention computation

- **Design tradeoffs**:
  - Compression ratio vs. model performance: Higher compression leads to more memory savings but may degrade accuracy
  - Layer-wise compression: Skipping compression in certain layers (e.g., first two) based on correlation strength
  - Real-time vs. pre-computed norms: Computing L2 norms on-the-fly vs. storing them for faster compression decisions

- **Failure signatures**:
  - Degradation in perplexity or next-token accuracy as compression ratio increases
  - Inconsistent performance across different tasks (e.g., language modeling vs. passkey retrieval)
  - Performance drops when compressing layers with low L2-norm/attention-score correlation

- **First 3 experiments**:
  1. Vary compression ratio (10%, 30%, 50%, 70%, 90%) and measure impact on perplexity for language modeling task
  2. Test compression on long-context tasks (needle-in-a-haystack, passkey retrieval) to evaluate retention of important information
  3. Compare performance when skipping compression in different layers to find optimal layer selection strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do key embeddings with low L2 norms correlate with high attention scores, and what underlying mechanism drives this relationship?
- Basis in paper: [explicit] The paper explicitly states this correlation was observed but does not provide a theoretical explanation for why it occurs.
- Why unresolved: The authors acknowledge they lack a comprehensive theoretical explanation for this phenomenon, despite observing it consistently across experiments.
- What evidence would resolve it: Empirical studies analyzing the mathematical relationship between L2 norms and attention mechanisms, or theoretical proofs demonstrating why low L2 norm keys attract higher attention scores in transformer architectures.

### Open Question 2
- Question: Does the L2 norm-based compression strategy generalize effectively to other transformer-based architectures beyond decoder-only LLMs like Llama2?
- Basis in paper: [inferred] The paper tested the method exclusively on Llama2 variants (7B, 7B-32k, 7B-80k) and acknowledges this as a limitation.
- Why unresolved: The experiments were limited to a single model family, leaving uncertainty about applicability to other architectures like BERT, GPT-3, or domain-specific transformers.
- What evidence would resolve it: Systematic testing of the compression strategy across diverse transformer architectures, including encoder-decoder models and specialized transformers for different tasks.

### Open Question 3
- Question: What is the optimal layer-skipping strategy for maximizing compression efficiency while maintaining performance, and how does this vary across different tasks?
- Basis in paper: [explicit] The paper mentions skipping compression in layers with low correlation between L2 norms and attention scores, but provides limited analysis on optimal layer selection strategies.
- Why unresolved: The authors conducted ablation studies but did not develop a principled method for determining which layers to skip based on task characteristics or model properties.
- What evidence would resolve it: Task-specific layer-skipping protocols derived from attention pattern analysis, or adaptive methods that dynamically determine layer-skipping based on input characteristics.

## Limitations

- The correlation between L2 norms and attention scores may not generalize across different model architectures or training regimes.
- The method assumes the L2 norm remains a reliable proxy for key importance across varying contexts and token positions, which may not hold for all use cases.
- The paper does not explore how the correlation might change during different stages of model fine-tuning or when using different tokenization strategies.

## Confidence

- **High Confidence**: The experimental results showing up to 50% compression for language modeling and needle-in-a-haystack tasks, and up to 90% for passkey retrieval tasks while maintaining performance. These results are directly supported by the paper's experiments and provide concrete evidence of the method's effectiveness.
- **Medium Confidence**: The claim about compatibility with FlashAttention and the assertion that no additional training or modifications are required. While the paper states this, the exact interaction between the compression method and FlashAttention's optimizations is not fully explored or validated.
- **Low Confidence**: The generalizability of the L2-norm/attention-score correlation across different model architectures, training datasets, and token positions. The paper provides evidence for Llama2 models but does not extensively validate the correlation's stability across diverse conditions.

## Next Checks

1. **Cross-Model Validation**: Test the L2-norm-based compression method on different transformer architectures (e.g., GPT, OPT, or Mistral) to verify if the correlation between L2 norms and attention scores holds consistently. This will help determine the method's generalizability.

2. **Dynamic Context Analysis**: Evaluate how the L2-norm/attention-score correlation changes during long-context generation, particularly for tokens near the beginning versus those recently added. This will reveal if the compression strategy remains effective throughout the generation process.

3. **Ablation Study on Layer Skipping**: Conduct a systematic ablation study to determine the optimal number of layers to skip compression based on the correlation strength in each layer. This will help refine the compression strategy for maximum efficiency without sacrificing performance.