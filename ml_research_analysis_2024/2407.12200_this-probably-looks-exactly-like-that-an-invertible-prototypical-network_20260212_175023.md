---
ver: rpa2
title: 'This Probably Looks Exactly Like That: An Invertible Prototypical Network'
arxiv_id: '2407.12200'
source_url: https://arxiv.org/abs/2407.12200
tags:
- prototypical
- prototypes
- image
- protoflow
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProtoFlow introduces a novel approach to interpretable image classification
  by learning prototypical distributions over latent space rather than individual
  prototype points. It combines normalizing flows with Gaussian mixture models to
  create an exactly invertible model that achieves both high predictive accuracy and
  generative performance.
---

# This Probably Looks Exactly Like That: An Invertible Prototypical Network

## Quick Facts
- arXiv ID: 2407.12200
- Source URL: https://arxiv.org/abs/2407.12200
- Authors: Zachariah Carmichael; Timothy Redgrave; Daniel Gonzalez Cedre; Walter J. Scheirer
- Reference count: 40
- Key outcome: ProtoFlow achieves 91.54% accuracy on CIFAR-10 with 3.95 BPD by learning prototypical distributions over latent space using normalizing flows

## Executive Summary
ProtoFlow introduces a novel approach to interpretable image classification by learning prototypical distributions over latent space rather than individual prototype points. It combines normalizing flows with Gaussian mixture models to create an exactly invertible model that achieves both high predictive accuracy and generative performance. The method demonstrates state-of-the-art results on joint generative-predictive modeling tasks while enabling richer prototype interpretation through faithful visualization of learned distributions.

## Method Summary
ProtoFlow learns prototypical distributions by composing normalizing flows with Gaussian mixture models in latent space. The DenseFlow architecture maps data to a structured latent space where class-conditional GMMs model the distributions. This enables both likelihood-based generation and classification through Bayes' rule. Consistency regularization encourages invariance to perturbations, while diversity loss prevents prototype redundancy. The invertible nature of the flow allows exact visualization of prototypical distributions through inverse mapping, providing interpretable explanations for predictions.

## Key Results
- Achieves 91.54% accuracy on CIFAR-10 with 3.95 BPD, outperforming baseline ProtoPNet (91.41%, 4.05 BPD)
- Demonstrates strong calibration metrics with ECE of 0.083 and MCE of 0.494
- Shows up to 78% prototype pruning capability without significant accuracy loss
- Enables exact visualization of prototypical distributions through inverse flow mapping

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning prototypical distributions instead of point prototypes enables richer, more faithful visualizations
- Mechanism: Normalizing flows provide exact inverses that map latent-space distributions back to data space, allowing direct visualization of what the model has learned
- Core assumption: The inverse flow mapping f⁻¹ is sufficiently well-behaved and smooth to preserve interpretable structure when transforming samples
- Evidence anchors:
  - [abstract] "enabling exact visualization of prototypical distributions through the inverse flow mapping"
  - [section 3] "we recover an intrinsic association between Z and X by learning these distributions invertibly with normalizing flows... providing us an implicit mapping f⁻¹: Z → X"
- Break condition: If the normalizing flow is poorly conditioned or the latent space dimensionality is too high relative to data complexity, the inverse mapping may not preserve meaningful structure

### Mechanism 2
- Claim: Composing normalizing flows with GMM classifiers enables joint generative-predictive modeling
- Mechanism: The flow maps complex data to a structured latent space where class-conditional GMMs can model the distributions, enabling both likelihood-based generation and classification
- Core assumption: The normalizing flow can sufficiently "normalize" the data distribution to make GMM modeling in latent space tractable
- Evidence anchors:
  - [section 3] "We base our normalizing flow on DenseFlow... We replace DenseFlow's unconditional latent distribution with conditional distributions"
  - [section 2.2] "Empirical information from data flows to the latent space through f, and density inferences are recovered by the inverse f⁻¹"
- Break condition: If the flow cannot adequately transform the data distribution, the GMMs in latent space will be poor models leading to degraded performance

### Mechanism 3
- Claim: Consistency regularization improves predictive performance by encouraging invariance to perturbations
- Mechanism: The consistency loss penalizes the model for making different predictions on perturbed versions of the same input, leading to smoother decision boundaries
- Core assumption: The perturbations used (augmentations) are meaningful and preserve class membership
- Evidence anchors:
  - [section 3] "we adapt the proposed consistency regularization loss from [37]... This encourages the model to be invariant to certain perturbations or augmentations"
  - [section 4] "Impact of Consistency Loss" section showing qualitative differences in learned prototypes
- Break condition: If perturbations are too aggressive or class-ambiguous, the regularization may hurt performance by smoothing over important decision boundaries

## Foundational Learning

- Normalizing flows and change of variables formula
  - Why needed here: The entire model architecture depends on invertible transformations between data and latent spaces
  - Quick check question: Can you derive the change of variables formula for density estimation under invertible transformations?

- Gaussian mixture models and parameter estimation
  - Why needed here: Prototypes are modeled as GMMs in the latent space, requiring understanding of how to parameterize and fit these distributions
  - Quick check question: How do you compute the likelihood of a point under a GMM and derive the EM update equations?

- Prototype-based neural networks and case-based reasoning
  - Why needed here: The interpretability framework builds on the idea of explaining predictions through similarity to learned prototypes
  - Quick check question: What is the key difference between prototype networks and standard classification in terms of how they make decisions?

## Architecture Onboarding

- Component map:
  - Input image → DenseFlow backbone → Latent space → Class-conditional GMMs → Classification probabilities
  - Class-conditional GMMs → Inverse flow mapping → Generated prototype samples

- Critical path:
  1. Transform input through DenseFlow to latent space
  2. Evaluate class-conditional GMM likelihoods
  3. Compute classification probabilities via Bayes' rule
  4. Backpropagate through both flow and GMM parameters

- Design tradeoffs:
  - More GMM components → richer prototypes but more parameters and potential overfitting
  - Stronger consistency regularization → smoother decision boundaries but may reduce model capacity
  - Truncated sampling for visualization → better-looking samples but reduced diversity

- Failure signatures:
  - Noisy/incoherent samples → flow not well-conditioned or GMMs poorly fitted
  - Poor calibration → consistency regularization too weak or data augmentation not appropriate
  - Low diversity scores → prototypical distributions too similar, consider increasing diversity loss weight

- First 3 experiments:
  1. Train ProtoFlow on MNIST with default settings, visualize prototype means and samples
  2. Compare predictive accuracy with and without consistency regularization
  3. Vary number of GMM components per class and measure impact on accuracy and diversity score

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of prototypical distributions per class (K) for balancing predictive accuracy and interpretability?
- Basis in paper: [inferred] The paper mentions that diversity scores (Sdiv) suggest ProtoFlow is learning redundant prototypes, and pruning experiments show up to 78% of prototypes can be removed without significant accuracy loss. However, the paper does not systematically explore how varying K affects performance.
- Why unresolved: The authors used a fixed K=10 for CIFAR-10 experiments but didn't explore a range of values or develop a principled method for selecting K.
- What evidence would resolve it: A systematic study varying K across multiple datasets and tasks, measuring accuracy, interpretability metrics, and computational efficiency to identify optimal values.

### Open Question 2
- Question: How can ProtoFlow's performance on highly imbalanced datasets be improved while maintaining interpretability?
- Basis in paper: [inferred] The paper does not discuss experiments on imbalanced datasets, and the authors mention that ProtoFlow is vulnerable to training data biases.
- Why unresolved: The paper focuses on balanced datasets like CIFAR-10 and MNIST, and does not address class imbalance which is common in real-world applications.
- What evidence would resolve it: Experiments on imbalanced datasets with evaluation of both accuracy and prototype quality, along with methods to handle class imbalance (e.g., class weights, data augmentation) that preserve interpretability.

### Open Question 3
- Question: What is the relationship between truncation levels and prototype interpretability across different datasets and training paradigms?
- Basis in paper: [explicit] The paper discusses truncated sampling for generating prototype visualizations and notes that the impact of truncation varies significantly based on dataset, training paradigm, and individual GMM components.
- Why unresolved: While the paper observes this variability, it does not provide a systematic analysis or guidelines for choosing appropriate truncation levels.
- What evidence would resolve it: A comprehensive study examining truncation effects across multiple datasets, training methods, and visualization tasks, with quantitative measures of prototype quality at different truncation levels.

## Limitations

- The scalability of the approach to larger datasets and higher resolution images remains unproven, as experiments are limited to relatively small-scale benchmarks.
- The computational overhead of invertible architectures versus standard classifiers is not discussed, raising questions about practical deployment.
- The paper assumes the normalizing flow inverse mapping preserves interpretable structure, but provides limited empirical validation of this critical assumption.

## Confidence

- **High confidence**: Joint generative-predictive performance metrics (accuracy, BPD) are well-supported by experimental results
- **Medium confidence**: Interpretability claims are supported qualitatively but lack rigorous quantitative validation
- **Medium confidence**: Calibration improvements are demonstrated but could benefit from additional analysis on out-of-distribution data

## Next Checks

1. Conduct a user study comparing human interpretability of ProtoFlow prototypes versus standard prototype networks, measuring time-to-understanding and explanation quality
2. Evaluate model performance and interpretability when scaling to higher resolution images (e.g., 128×128 or 256×256) to test practical scalability
3. Perform ablation studies systematically varying flow architecture depth, number of GMM components, and regularization strength to identify optimal configurations and failure modes