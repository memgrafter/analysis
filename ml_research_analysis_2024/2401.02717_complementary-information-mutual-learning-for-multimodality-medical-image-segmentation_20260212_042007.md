---
ver: rpa2
title: Complementary Information Mutual Learning for Multimodality Medical Image Segmentation
arxiv_id: '2401.02717'
source_url: https://arxiv.org/abs/2401.02717
tags:
- information
- modalities
- segmentation
- learning
- complementary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Complementary Information Mutual Learning
  (CIML) framework for multimodal medical image segmentation. CIML addresses the challenge
  of inter-modal redundant information in existing joint learning methods, which can
  lead to misjudging modality importance, ignoring specific modal information, and
  increasing cognitive load.
---

# Complementary Information Mutual Learning for Multimodality Medical Image Segmentation

## Quick Facts
- arXiv ID: 2401.02717
- Source URL: https://arxiv.org/abs/2401.02717
- Reference count: 26
- Key outcome: CIML framework achieves 88.21% mean dice score on BraTS2020, outperforming state-of-the-art methods by reducing inter-modal redundancy through task decomposition and complementary information extraction.

## Executive Summary
This paper addresses the challenge of inter-modal redundant information in multimodal medical image segmentation by introducing the Complementary Information Mutual Learning (CIML) framework. Traditional joint learning methods often misjudge modality importance and increase cognitive load due to redundant information. CIML decomposes the segmentation task into subtasks based on expert prior knowledge and extracts complementary information through message passing and variational inference. The framework employs cross-modal spatial attention to filter redundant information, achieving superior performance on BraTS2020, autoPET, and MICCAI HECKTOR 2022 datasets.

## Method Summary
The CIML framework addresses multimodal medical image segmentation by decomposing the task into single-modal subtasks using expert prior knowledge, then extracting complementary information through message passing between segmentors. Each segmentor serves as the primary modality for one subtask while using other modalities as auxiliary inputs. The framework transforms redundancy filtering into complementary information learning using variational information bottleneck principles, with cross-modal spatial attention identifying key voxels in auxiliary modalities. The method is trained using cross-entropy loss and KL divergence regularization on datasets including BraTS2020, autoPET, and MICCAI HECKTOR 2022, achieving state-of-the-art performance with a mean dice score of 88.21% on BraTS2020.

## Key Results
- CIML achieves 88.21% mean dice score on BraTS2020, outperforming the best baseline (Ensemble-UNet) at 87.72%
- The framework demonstrates consistent improvements across multiple datasets (autoPET, MICCAI HECKTOR 2022) and metrics
- CIML provides interpretable visualization of modality contributions through Grad-CAM, showing clear differentiation of modality importance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Task decomposition based on expert prior knowledge reduces information dependence between modalities.
- **Mechanism**: By decomposing the multimodal segmentation task into multiple single-modal subtasks, each modality serves as the primary modality for one subtask and as an auxiliary modality for others. This minimizes mutual influence of modal information.
- **Core assumption**: Expert knowledge about the correspondence between modalities and target regions is accurate and can be used as inductive bias.
- **Evidence anchors**:
  - [abstract] "CIML first decomposes the multimodal segmentation task into multiple subtasks based on expert prior knowledge, minimizing the information dependence between modalities."
  - [section 3.1] "CIML adopts a unique perspective of addition to eliminate inter-modal redundant information. The first step of addition is task decomposition, which is driven by the inductive bias extracted from expert prior knowledge."
  - [corpus] Weak evidence - no direct mention of task decomposition in related papers.
- **Break condition**: If expert prior knowledge is inaccurate or unavailable, task decomposition may not reduce information dependence and could even increase complexity.

### Mechanism 2
- **Claim**: Message passing-based redundancy filtering extracts complementary information from auxiliary modalities.
- **Mechanism**: The framework transforms the redundancy filtering problem into a complementary information learning problem using variational information bottleneck. It then employs variational inference and cross-modal spatial attention to efficiently solve this problem.
- **Core assumption**: Redundant information can be effectively filtered out while preserving complementary information that aids in segmentation.
- **Evidence anchors**:
  - [abstract] "To achieve non-redundancy of extracted information, the redundant filtering is transformed into complementary information learning inspired by the variational information bottleneck."
  - [section 3.2] "In the second phase of the addition process, message passing-based redundancy filtering is employed to eradicate inter-modality redundancy, thereby extracting supplementary information."
  - [corpus] Weak evidence - no direct mention of message passing or variational information bottleneck in related papers.
- **Break condition**: If the variational inference or cross-modal spatial attention fails to effectively filter out redundant information, the extracted complementary information may be insufficient or noisy.

### Mechanism 3
- **Claim**: Cross-modal spatial attention enables efficient extraction of complementary information.
- **Mechanism**: The cross-modality information gate (CIG) module utilizes cross-modal spatial attention to identify key voxels in auxiliary modalities, filtering complementary information via attention weights.
- **Core assumption**: Cross-modal spatial attention can effectively identify and weight important voxels across different modalities.
- **Evidence anchors**:
  - [section 3.2] "We employ cross-modal spatial attention as a parameterization backbone to obtain a practical implementation."
  - [section 3.2] "The CIG module utilizes cross-modal spatial attention as a parameterization backbone to achieve a practical implementation."
  - [corpus] Weak evidence - no direct mention of cross-modal spatial attention in related papers.
- **Break condition**: If cross-modal spatial attention fails to correctly identify important voxels, the extracted complementary information may be inaccurate or incomplete.

## Foundational Learning

- **Concept**: Variational Information Bottleneck (VIB)
  - Why needed here: VIB is used to transform the redundancy filtering problem into a complementary information learning problem, allowing for efficient optimization.
  - Quick check question: How does VIB help in extracting complementary information while minimizing redundancy?

- **Concept**: Cross-modal Spatial Attention
  - Why needed here: Cross-modal spatial attention is used in the CIG module to identify key voxels in auxiliary modalities, enabling efficient extraction of complementary information.
  - Quick check question: How does cross-modal spatial attention differ from standard spatial attention, and why is it important for multimodal segmentation?

- **Concept**: Message Passing
  - Why needed here: Message passing allows each modality to extract information from other modalities additively, facilitating the extraction of complementary information.
  - Quick check question: How does message passing help in reducing inter-modal redundancy and improving segmentation performance?

## Architecture Onboarding

- **Component map**: Segmentor1 (FLAIR→WT) <-> Segmentor2 (T1→TC) <-> Segmentor3 (T2→WT+TC) <-> Segmentor4 (T1CE→TC+ET)
- **Critical path**: Expert prior knowledge → Task decomposition → Message passing between segmentors → Complementary information filtering (VIB + cross-modal attention) → Segmentation prediction
- **Design tradeoffs**: The use of task decomposition and message passing increases model complexity but can improve segmentation performance by reducing inter-modal redundancy. The choice of expert prior knowledge for task decomposition is crucial and may limit the framework's applicability in cases where such knowledge is unavailable.
- **Failure signatures**: Poor segmentation performance may indicate: 1) Inaccurate expert prior knowledge for task decomposition, 2) Ineffective message passing or redundancy filtering, or 3) Failure of cross-modal spatial attention to identify important voxels.
- **First 3 experiments**:
  1. Test task decomposition on a simple multimodal segmentation task with known correspondence between modalities and target regions.
  2. Evaluate the effectiveness of message passing and redundancy filtering on a dataset with significant inter-modal redundancy.
  3. Assess the impact of cross-modal spatial attention on segmentation performance by comparing results with and without this component.

## Open Questions the Paper Calls Out
None

## Limitations
- Expert prior knowledge requirement: The framework depends on accurate expert knowledge for task decomposition, which may not be available for all multimodal problems
- Moderate performance gains: The 0.49% improvement over the best baseline on BraTS2020 represents a relatively modest gain for a complex framework with multiple new components
- Limited generalization: Experimental validation relies primarily on standard benchmarks where the proposed method's inductive biases may already align with dataset characteristics

## Confidence
- **Medium**: Claims about inter-modal redundancy reduction - supported by experimental results but dependent on correct task decomposition
- **Medium**: Effectiveness of variational information bottleneck for complementary information extraction - theoretically sound but with limited ablation study
- **High**: Basic segmentation performance improvements - well-demonstrated through multiple datasets and metrics

## Next Checks
1. Test CIML performance on multimodal datasets without clear expert knowledge about modality-region correspondence to assess robustness to incorrect task decomposition
2. Conduct systematic ablation studies isolating the contributions of task decomposition, message passing, and cross-modal attention to quantify their individual impacts
3. Evaluate computational overhead and memory requirements compared to simpler joint learning approaches to determine practical deployment trade-offs