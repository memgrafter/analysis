---
ver: rpa2
title: 'Lillama: Large Language Models Compression via Low-Rank Feature Distillation'
arxiv_id: '2412.16719'
source_url: https://arxiv.org/abs/2412.16719
tags:
- compression
- compressed
- student
- low-rank
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Lillama, a method for compressing large language
  models by combining low-rank decomposition with local feature distillation. It addresses
  the inefficiency of traditional pruning methods that require costly continued pretraining
  to recover performance.
---

# Lillama: Large Language Models Compression via Low-Rank Feature Distillation

## Quick Facts
- arXiv ID: 2412.16719
- Source URL: https://arxiv.org/abs/2412.16719
- Authors: Yaya Sy; Christophe Cerisara; Irina Illina
- Reference count: 40
- One-line primary result: Compresses Mixtral-8x7B by 20% within minutes on single GPU while retaining >95% performance

## Executive Summary
Lillama introduces a novel method for compressing large language models by combining low-rank decomposition with local feature distillation. The approach addresses the inefficiency of traditional pruning methods that require costly continued pretraining to recover performance. By leveraging the observation that pretrained transformer activations are more low-rank than weights, Lillama uses SVD initialization and a joint teacher-student loss to accelerate convergence and reduce memory usage. Experiments demonstrate significant compression ratios across multiple model architectures while maintaining high performance retention.

## Method Summary
Lillama compresses LLMs by first determining which layers to compress using a bottom-first strategy that minimizes memory overhead. For selected layers, it applies low-rank matrix decomposition initialized with SVD on the original weight matrices. During training, it uses a joint loss combining teacher and student activations to enable fast convergence while maintaining inference-time independence from the teacher model. The method requires only a small calibration dataset (13 million tokens) and can be applied to various architectures including transformers and Mamba models.

## Key Results
- Compresses Mixtral-8x7B by 20% within minutes on a single GPU while retaining over 95% performance
- Compresses Phi-2 3B by 40% using only 13 million tokens with competitive results
- Generalizes to non-transformer architectures like Mamba-3B with 99% performance retention

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-rank approximation of activations recovers performance better than approximating weights because activations are more compressible.
- Mechanism: Activations from pretrained models have lower intrinsic dimensionality than weights, making them amenable to low-rank approximation without significant loss of representational capacity.
- Core assumption: The rank of transformer activations is significantly lower than the rank of corresponding weight matrices.
- Evidence anchors:
  - [abstract] "Prior research suggests pretrained Transformer weights aren't inherently low-rank, unlike their activations, which may explain this drop."
  - [section 3.2] "Previous studies (Chen et al., 2021; Yu and Wu, 2023) have shown that the activations (i.e., features) of pretrained transformers, are more low-rank than the weights."
  - [corpus] Weak - corpus neighbors don't directly address activation vs weight rank differences
- Break condition: If activations are not low-rank (stable rank close to full rank), the low-rank approximation would destroy too much information and performance would collapse.

### Mechanism 2
- Claim: Initializing low-rank matrices with SVD provides better starting points than random initialization, accelerating convergence.
- Mechanism: SVD provides the optimal low-rank approximation under Frobenius norm, giving student networks a head start that random initialization lacks.
- Core assumption: The optimal low-rank decomposition of activations has a basin of attraction that gradient descent can reach from the SVD starting point.
- Evidence anchors:
  - [section 4] "We also show that initializing the Student's low-rank parameters with SVD, rather than randomly, improves convergence."
  - [section 8] "Figure 5 shows the perplexity curves for Phi-2 3B and Phi-3 14b when the low-rank matrices are initialized with SVD or randomly. SVD initialization enables faster convergence"
  - [corpus] Weak - no corpus evidence specifically about SVD initialization benefits
- Break condition: If the optimization landscape is too rugged or the SVD solution is far from the global optimum, initialization advantage may be minimal or negative.

### Mechanism 3
- Claim: Joint loss combining teacher and student activations enables faster convergence while maintaining inference-time independence from teacher.
- Mechanism: The teacher provides high-quality supervision signals for rapid learning while student-to-student connections ensure inference-time independence and robustness.
- Core assumption: Combining teacher and student supervision creates a more informative gradient signal than either alone.
- Evidence anchors:
  - [section 4] "To address both issues, we propose to combine the two losses: L(i)_T+S = L(i)_T + L(i)_S"
  - [section 8] "The joint loss depicted in Figure 2c generally produces the best results... Figure 3 shows that the Teacher+Student loss leads to faster convergence than the Student loss alone."
  - [corpus] Weak - corpus neighbors don't discuss joint loss strategies
- Break condition: If teacher and student activations become too misaligned, the joint loss may create conflicting gradient signals that slow or prevent convergence.

## Foundational Learning

- Concept: Low-rank matrix approximation and SVD
  - Why needed here: The entire compression approach relies on finding low-rank representations of activations and weight matrices
  - Quick check question: Given a matrix with singular values [10, 5, 1, 0.1], what rank approximation would preserve 99% of the energy?
- Concept: Knowledge distillation and loss functions
  - Why needed here: The method combines teacher and student supervision using a joint loss with ℓ1 and cosine similarity components
  - Quick check question: Why might combining ℓ1 and cosine similarity losses be more stable than using either alone for activation matching?
- Concept: Gradient-based optimization and local vs global objectives
  - Why needed here: The approach uses local layer-wise distillation objectives rather than global ones to reduce computational cost while maintaining effectiveness
  - Quick check question: What's the key computational advantage of local layer-wise distillation compared to global distillation?

## Architecture Onboarding

- Component map: Algorithm 1 (layer selection) -> SVD initialization (matrix decomposition) -> Joint loss training (gradient updates)
- Critical path: 1) Determine compression ratio and strategy (bottom-first recommended), 2) Run Algorithm 1 to assign ranks to weight matrices, 3) Initialize low-rank parameters using SVD on selected matrices, 4) Set up distillation training with joint loss, 5) Run local gradient updates until convergence
- Design tradeoffs: Bottom-first strategy trades off some potential performance (compressing early layers may hurt more) for memory efficiency and faster training. Uniform strategy is simpler but requires loading entire model. Joint loss trades off some convergence speed (compared to teacher-only) for inference-time independence.
- Failure signatures: Poor performance retention (>10% drop) indicates either too aggressive compression, wrong rank selection, or insufficient calibration data. OOM errors during training indicate memory inefficiency in the compression strategy. Slow convergence indicates problems with initialization or loss function design.
- First 3 experiments:
  1. Compress a small model (e.g., Phi-2 3B) with bottom-first strategy at 20% compression using default hyperparameters, verify performance retention and training time
  2. Test different minimum rank values (k=512, 1024, 2048) on the same model to understand rank-performance tradeoff
  3. Compare the three distillation loss variants (Teacher-only, Student-only, Teacher+Student) on the same setup to verify convergence differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the compressed models scale with the amount of fine-tuning data beyond the initial calibration dataset?
- Basis in paper: [explicit] The paper mentions that fine-tuning compressed models leads to performance recovery and that additional data could likely lead to even greater improvements.
- Why unresolved: The paper only provides results for fine-tuning on a fixed amount of data (e.g., 191 million tokens for Mistral 7B). The relationship between fine-tuning data size and performance recovery is not explored.
- What evidence would resolve it: Experiments showing performance curves as a function of fine-tuning data size for various compression ratios and model architectures.

### Open Question 2
- Question: How does the bottom-first compression strategy compare to other layer-wise pruning strategies (e.g., middle-out, mixed) in terms of performance and computational efficiency?
- Basis in paper: [explicit] The paper describes the bottom-first strategy and compares it to top-first and uniform strategies, but only in terms of performance and memory usage.
- Why unresolved: The paper does not explore other layer-wise pruning strategies or provide a comprehensive comparison of different approaches.
- What evidence would resolve it: Experiments comparing the performance and computational efficiency of different layer-wise pruning strategies across various model architectures and compression ratios.

### Open Question 3
- Question: What is the impact of using different initialization methods (e.g., random initialization, other low-rank approximations) on the convergence and final performance of the compressed models?
- Basis in paper: [explicit] The paper uses SVD for initialization and shows that it improves convergence compared to random initialization.
- Why unresolved: The paper does not explore other initialization methods or provide a comprehensive comparison of different approaches.
- What evidence would resolve it: Experiments comparing the performance of different initialization methods across various model architectures and compression ratios.

## Limitations

- The method's effectiveness on encoder-decoder architectures and vision transformers is not demonstrated
- The sensitivity to calibration data quality and quantity is not explored beyond the single 13 million token dataset
- The paper lacks comprehensive ablation studies to definitively prove the benefits of individual components like SVD initialization and joint loss

## Confidence

**High confidence (95%+)**: The core claim that Lillama can compress Mixtral-8x7B by 20% while retaining over 95% performance is well-supported by presented results. The memory efficiency claims (minutes on single GPU) are also directly demonstrated.

**Medium confidence (70-90%)**: The mechanism claims about why low-rank activation compression works better than weight compression are supported by prior literature but lack direct experimental validation within this paper. The benefits of SVD initialization and joint loss are shown empirically but could benefit from more rigorous ablation studies.

**Low confidence (below 70%)**: Claims about the method's generality to all transformer variants and non-transformer architectures are weakly supported, with only one non-transformer example (Mamba-3B) and no encoder-decoder models tested.

## Next Checks

1. **Ablation study on loss components**: Systematically test the three loss variants (Teacher-only, Student-only, Teacher+Student) across multiple models and compression ratios to quantify the exact contribution of each component and verify the claimed faster convergence of the joint approach.

2. **Cross-domain calibration validation**: Test the sensitivity of performance retention to calibration data domain by compressing models using calibration data from different domains (e.g., using code data to compress a general-purpose model) and measuring the degradation in downstream task performance.

3. **Architecture generalization test**: Apply Lillama to encoder-decoder architectures like T5 or BART and compare performance retention and convergence behavior to the decoder-only models presented, to validate the claimed generality across transformer variants.