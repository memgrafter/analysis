---
ver: rpa2
title: A Comparison of Language Modeling and Translation as Multilingual Pretraining
  Objectives
arxiv_id: '2407.15489'
source_url: https://arxiv.org/abs/2407.15489
tags:
- language
- computational
- association
- pretraining
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares different multilingual pretraining objectives
  in controlled settings, focusing on language modeling (LM) versus translation-based
  approaches. The authors train comparable models using five different objectives
  (LM, MT, CLM, MLM, TLM) on six languages with controlled data and architecture.
---

# A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives

## Quick Facts
- **arXiv ID**: 2407.15489
- **Source URL**: https://arxiv.org/abs/2407.15489
- **Reference count**: 29
- **Primary result**: Translation objectives outperform denoising objectives in multilingual pretraining under controlled conditions

## Executive Summary
This paper systematically compares different multilingual pretraining objectives in controlled experimental settings, focusing on language modeling versus translation-based approaches. The authors train comparable models using five different objectives (language modeling, machine translation, causal language modeling, masked language modeling, and translation language modeling) on six languages with controlled data and architecture. Through extensive evaluation on sentiment analysis, named entity recognition, POS tagging, and NLI tasks using both probing and fine-tuning approaches, the study reveals important insights about the relative effectiveness of different pretraining strategies.

The key finding is that machine translation-pretrained models consistently outperform denoising objectives like MLM and TLM, while double-stack encoder-decoder architectures (such as BART) show superior performance in probing tasks compared to single-stack models. Interestingly, most differences between pretraining objectives diminish when models are fine-tuned, suggesting that pretraining choices may be less critical than previously thought when sufficient fine-tuning data is available. These results provide valuable guidance for selecting pretraining objectives in multilingual settings.

## Method Summary
The authors conducted controlled experiments comparing five different pretraining objectives: language modeling (LM), machine translation (MT), causal language modeling (CLM), masked language modeling (MLM), and translation language modeling (TLM). They trained models on six languages (German, Spanish, French, Russian, Chinese, and English) using carefully controlled data and architecture settings. The study employed both single-stack and double-stack encoder-decoder architectures, with the latter following the BART design. Models were evaluated on four downstream tasks (sentiment analysis, named entity recognition, POS tagging, and natural language inference) using both probing and fine-tuning approaches. The controlled setup allowed direct comparison of pretraining objectives while holding other factors constant.

## Key Results
- Machine translation-pretrained models outperform denoising objectives (MLM, TLM) across most tasks
- Double-stack encoder-decoder models show better probing performance than single-stack models
- Differences between pretraining objectives largely disappear when models are fine-tuned
- Pretraining objectives have minimal impact on final task performance when sufficient fine-tuning data is available

## Why This Works (Mechanism)
The effectiveness of different pretraining objectives stems from how they shape the learned representations. Translation objectives force models to capture cross-lingual alignment and semantic equivalence, leading to more transferable representations. Double-stack architectures like BART provide additional capacity for denoising and reconstruction tasks. The study demonstrates that while pretraining shapes initial representations, fine-tuning can overcome many differences, suggesting that the quality of fine-tuning data and process may be more important than pretraining choices for downstream performance.

## Foundational Learning
- **Multilingual pretraining objectives**: Different objectives (LM, MT, CLM, MLM, TLM) shape learned representations in distinct ways - needed to understand why certain approaches work better for specific tasks
- **Encoder-decoder architectures**: Single-stack vs double-stack designs affect model capacity and learning dynamics - critical for understanding architectural trade-offs
- **Probing vs fine-tuning**: Different evaluation approaches reveal different aspects of model capabilities - essential for comprehensive model assessment
- **Controlled experiments**: Holding data and architecture constant while varying only the objective - necessary for isolating the impact of pretraining choices

## Architecture Onboarding
- **Component map**: Input data -> Pretraining objective (LM/MT/CLM/MLM/TLM) -> Encoder-decoder model (single-stack/double-stack) -> Downstream task evaluation (probing/fine-tuning)
- **Critical path**: Pretraining objective selection → Model architecture choice → Training configuration → Downstream evaluation
- **Design tradeoffs**: Single-stack models offer simplicity but may lack capacity; double-stack models provide more flexibility but increase complexity; translation objectives require parallel data but yield better cross-lingual alignment
- **Failure signatures**: Poor cross-lingual transfer when using denoising objectives; degraded performance with insufficient fine-tuning data; architectural mismatches between pretraining and downstream tasks
- **First experiments**: 1) Compare single MT objective against single LM objective on identical data; 2) Test double-stack vs single-stack with same pretraining objective; 3) Evaluate probing vs fine-tuning performance differences

## Open Questions the Paper Calls Out
None

## Limitations
- Controlled experimental setup may not reflect real-world pretraining scenarios with diverse data sources
- Five-language subset (German, Spanish, French, Russian, Chinese) may not capture full multilingual diversity
- Evaluation limited to four downstream tasks (sentiment analysis, NER, POS tagging, NLI)
- Single hyperparameter setting used for each objective, potentially suboptimal for individual approaches

## Confidence
- **High confidence**: MT-pretrained models outperform denoising objectives when comparing similar architectural complexity
- **Medium confidence**: Double-stack encoder-decoder superiority in probing tasks under specific experimental conditions
- **Medium confidence**: Pretraining objective differences diminish in fine-tuning scenarios with sufficient data

## Next Checks
1. **Expand language diversity**: Replicate with broader set of typologically diverse languages including lower-resource languages
2. **Test additional downstream tasks**: Evaluate on question answering, summarization, and document classification for generalizability
3. **Vary hyperparameter optimization**: Conduct experiments with individually optimized hyperparameters for each pretraining objective