---
ver: rpa2
title: Evaluating the Performance of LLMs on Technical Language Processing tasks
arxiv_id: '2403.15503'
source_url: https://arxiv.org/abs/2403.15503
tags:
- information
- spectrum
- llms
- https
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the performance of large language models (LLMs)
  on technical language processing tasks using Title 47 of the US Code of Federal
  Regulations as context. The study compares ChatGPT with several open-source models
  (Vicuna, MPT-Instruct, H2O GPT, GPT4All, and Snoozy-LLM) on 11 domain-specific questions
  about wireless spectrum regulations.
---

# Evaluating the Performance of LLMs on Technical Language Processing tasks

## Quick Facts
- arXiv ID: 2403.15503
- Source URL: https://arxiv.org/abs/2403.15503
- Reference count: 33
- This paper evaluates LLM performance on technical regulatory tasks using FCC regulations as context

## Executive Summary
This paper evaluates large language models on technical language processing tasks using Title 47 of the US Code of Federal Regulations as context. The study compares ChatGPT with several open-source models on 11 domain-specific questions about wireless spectrum regulations. Results show that while larger models generally perform better, all models struggle with accuracy and citability. Expert evaluations reveal significant limitations across all models, with less experienced evaluators tending to rate responses more positively despite inaccuracies.

## Method Summary
The study evaluated multiple LLMs (ChatGPT, Vicuna, MPT-Instruct, H2O GPT, GPT4All, Snoozy-LLM) on 11 technical questions about wireless spectrum regulations. Models were provided with Title 47 CFR Part 101 text as context. Human evaluators (novice, techie, and expert) rated responses on comprehensibility and perceived correctness using a 1-10 scale. The study compared performance across model sizes and evaluator expertise levels, with qualitative feedback collected on answer quality.

## Key Results
- ChatGPT achieved the highest average scores among all tested models
- Larger models generally performed better but showed diminishing returns
- Expert evaluators consistently identified more inaccuracies than novice evaluators
- All models struggled with providing accurate citations to specific CFR sections

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing context (CFR text) to LLMs improves performance on domain-specific tasks compared to zero-shot inference.
- Mechanism: Context retrieval grounds the model's output in relevant source material, reducing hallucination probability.
- Core assumption: The model can attend to and correctly interpret the context during inference.
- Evidence anchors:
  - [abstract] "Using a range of LLMs and providing the required CFR text as context we were able to quantify the performance..."
  - [section] "Context was provided as Part 101 of the Title 47 of the Code of Federal Regulations (47 CFR part 101)"
  - [corpus] Weak; no corpus neighbors directly address context grounding for LLMs in technical domains.
- Break condition: Context exceeds model's token limits or model fails to attend to context effectively.

### Mechanism 2
- Claim: Model size correlates with performance on technical regulatory questions, but with diminishing returns.
- Mechanism: Larger models have more parameters and training data, improving their ability to capture complex domain relationships.
- Core assumption: Larger models can better process and reason about technical regulatory text.
- Evidence anchors:
  - [abstract] "Results show that while larger models generally perform better..."
  - [section] "We also have data for other model sizes for some of the tools (Ex: H20 GPT was also tested with Falcon 7B and 40B and Vicuna 33B)"
  - [corpus] Weak; corpus neighbors are about unrelated topics like game theory and patent processing.
- Break condition: Additional parameters do not yield performance gains beyond a certain threshold.

### Mechanism 3
- Claim: Expert evaluators detect inaccuracies and off-topic information that novice evaluators miss, especially when models use confident but incorrect language.
- Mechanism: Domain expertise allows identification of subtle technical errors and contextual relevance issues.
- Core assumption: Expert knowledge is necessary to evaluate technical accuracy in specialized domains.
- Evidence anchors:
  - [abstract] "expert evaluations reveal significant limitations across all models" and "less experienced evaluators tend to rate responses more positively despite inaccuracies"
  - [section] "A common trend noticed was how the more confidence and higher level of language clarity used would push participants to rate the model higher. However, as the expertise level increases, participants begin to identify inaccuracies..."
  - [corpus] Weak; no corpus neighbors discuss expert vs novice evaluation of LLM outputs.
- Break condition: When model outputs are so clearly wrong that even novices can identify issues.

## Foundational Learning

- Concept: Technical domain knowledge (wireless spectrum regulations)
  - Why needed here: Questions require understanding of FCC regulations, frequency bands, and technical terminology.
  - Quick check question: What does HAAT stand for in wireless communications?

- Concept: CFR document structure and citation format
  - Why needed here: Models must cite specific sections and subsections accurately to be useful for regulatory work.
  - Quick check question: Which part of Title 47 covers Fixed Microwave Services?

- Concept: LLM context window and token limitations
  - Why needed here: Understanding how much context can be provided and how models process it is critical for experimental design.
  - Quick check question: What is the approximate token limit for GPT-3.5 Turbo?

## Architecture Onboarding

- Component map:
  - Input: Technical questions about wireless spectrum regulations
  - Context provider: CFR text (specifically Part 101)
  - LLM models: ChatGPT, Vicuna, MPT-Instruct, H2O GPT, GPT4All, Snoozy-LLM
  - Evaluators: Novice, techie, and expert human raters
  - Output: Rated responses with comprehensibility and accuracy scores

- Critical path: Question → Context retrieval → LLM inference → Human evaluation → Performance analysis

- Design tradeoffs:
  - Context vs. token limits: Providing more context improves grounding but may exceed model limits
  - Model size vs. cost: Larger models perform better but are more expensive to run
  - Expert vs. novice evaluation: Experts catch technical errors but are less available than novices

- Failure signatures:
  - High confidence but factually incorrect responses (hallucinations)
  - Responses that avoid the question or provide irrelevant information
  - Inability to cite specific CFR sections despite context being provided

- First 3 experiments:
  1. Test a simple question (e.g., "What is HAAT?") with and without context to measure grounding effect
  2. Compare responses from smallest and largest models on the same question to quantify size effect
  3. Have both novice and expert evaluators rate the same response to demonstrate expertise impact on evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on technical language processing tasks scale with model size beyond the tested parameters, and is there evidence of emergent abilities as claimed by other researchers?
- Basis in paper: [explicit] The paper explicitly states that results indicate performance improves with model size, with ChatGPT performing best, but notes there is not a consistent improvement in response quality. It also references recent work by Schaeffer et al. questioning the rapid scaling of LLM abilities and suggesting linear scaling without emergent capabilities.
- Why unresolved: The paper tested a limited range of model sizes and found inconsistent improvements. The authors reference external research questioning emergent abilities but do not conduct their own analysis of scaling beyond their tested models.
- What evidence would resolve it: Systematic testing of a wider range of model sizes with consistent evaluation metrics would determine if performance scales linearly or shows emergent capabilities. Comparative analysis using the same metrics across different model sizes would provide clarity.

### Open Question 2
- Question: Can knowledge graph-augmented LLMs improve the reliability and citability of responses for technical regulatory tasks compared to standard LLMs?
- Basis in paper: [explicit] The authors state their overall architecture depends on extensive usage of information modeling via knowledge graphs, but they have not found reasonably efficient and reliable KG creation tools. They mention GraphRAG uses LLM-processed KGs but note quality has not been evaluated and their experience shows quality is substandard.
- Why unresolved: The paper identifies KG-augmented LLMs as a promising direction but acknowledges the current inability to create reliable KGs with sub-document level citation information. No systematic evaluation of KG-augmented approaches has been conducted.
- What evidence would resolve it: Comparative evaluation of KG-augmented LLMs versus standard LLMs on technical regulatory tasks with metrics for accuracy, citability, and sub-document level source attribution would demonstrate if KG augmentation improves performance.

### Open Question 3
- Question: How do evaluator expertise levels affect the perceived quality of LLM responses, and what are the implications for deploying LLMs in technical domains?
- Basis in paper: [explicit] The paper finds that novice evaluators tend to rate responses more positively despite inaccuracies, while experts identify substantial issues with technical precision and relevance. The authors note this has implications for LLM usage in society and their overall information landscape.
- Why unresolved: While the paper identifies a correlation between evaluator expertise and response ratings, it does not explore the underlying mechanisms or develop frameworks for calibrating evaluator expectations or improving LLM responses for different expertise levels.
- What evidence would resolve it: Controlled studies varying evaluator expertise levels with detailed analysis of specific inaccuracies identified by different groups, combined with development of expertise-aware evaluation frameworks, would clarify how to deploy LLMs effectively across different user groups.

## Limitations

- Small sample size of questions (n=11) limits generalizability of findings
- Reliance on subjective human ratings rather than objective correctness metrics
- Limited range of model sizes tested, preventing comprehensive scaling analysis

## Confidence

- Claim that "larger models generally perform better" has Medium confidence based on limited data points
- Claim about expert vs novice evaluator differences has Low confidence due to informal feedback methodology
- Overall study confidence is Low to Medium due to subjective evaluation methods and small sample size

## Next Checks

1. Replicate with expanded question set: Test the same models on 30-50 domain-specific questions to verify whether the observed performance patterns hold across a broader sample.

2. Implement objective correctness scoring: Develop a rubric with ground-truth answers for each question and have independent raters assess responses against these benchmarks, not just perceived correctness.

3. Test context retrieval mechanisms: Compare model performance when context is provided at different positions (beginning, middle, end) and in different formats to determine optimal context ingestion methods.