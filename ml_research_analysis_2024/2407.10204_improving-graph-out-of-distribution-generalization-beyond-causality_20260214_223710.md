---
ver: rpa2
title: Improving Graph Out-of-distribution Generalization Beyond Causality
arxiv_id: '2407.10204'
source_url: https://arxiv.org/abs/2407.10204
tags:
- graph
- environments
- derog
- rationales
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of out-of-distribution (OOD)
  generalization in graph learning, particularly on real-world data where previous
  methods relying on causal assumptions often fail. The authors identify two key limitations:
  the assumption that environments and labels are independent, and the assumption
  that causal subgraphs are invariant across environments.'
---

# Improving Graph Out-of-distribution Generalization Beyond Causality

## Quick Facts
- arXiv ID: 2407.10204
- Source URL: https://arxiv.org/abs/2407.10204
- Authors: Can Xu; Yao Cheng; Jianxiang Yu; Haosen Wang; Jingsong Lv; Yao Liu; Xiang Li
- Reference count: 40
- Primary result: DEROG significantly outperforms state-of-the-art methods on real-world OOD graph datasets, achieving up to 6.03% improvement

## Executive Summary
This paper addresses out-of-distribution (OOD) generalization in graph learning by moving beyond traditional causal assumptions that often fail on real-world data. The authors identify two key limitations in existing methods: the assumption that environments and labels are independent, and the assumption that causal subgraphs are invariant across environments. To overcome these, they propose DEROG, a novel probabilistic graphical model that captures dependencies between environments, graph rationales, and labels using variational inference with generalized Bayesian inference. Extensive experiments demonstrate that DEROG significantly outperforms state-of-the-art methods on both real-world datasets under different distribution shifts and synthetic benchmarks.

## Method Summary
DEROG is a probabilistic graphical model that treats environments and graph rationales as latent variables. It uses variational inference with generalized Bayesian inference to handle unknown prior distributions of these latent variables. The model employs an EM-based optimization framework with four GNNs: one for pseudo-label generation, one for environment embedding generation, one for graph rationale extraction, and one for final classification. DEROG introduces environment alignment loss and contrastive loss terms to improve generalization. The method models the joint distribution of environments, graph rationales, and labels, allowing it to capture real-world dependencies that violate strict independence assumptions.

## Key Results
- DEROG achieves significant performance improvements over state-of-the-art methods on real-world molecular datasets (GOODHIV, DrugOOD-LBAP-core-IC50) and NLP datasets (GOODTwitter, GOODSST2)
- The method demonstrates robustness across different types of distribution shifts including scaffold, size, assay, and text length
- DEROG performs well on synthetic benchmarks while maintaining superior performance on real-world data where traditional causal methods fail
- Maximum improvement of 6.03% is achieved compared to the best baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Environment-label dependency allows DEROG to handle real-world distribution shifts better than methods assuming strict independence.
- **Mechanism:** DEROG models the joint distribution of environments, graph rationales, and labels instead of assuming independence. By capturing environment-label dependencies, the model can better predict labels under real-world shifts where spurious correlations exist.
- **Core assumption:** Real-world graphs exhibit non-negligible dependencies between environments and labels, violating strict independence assumptions.
- **Evidence anchors:**
  - [abstract] "This paper addresses the problem of out-of-distribution (OOD) generalization in graph learning, particularly on real-world data where previous methods relying on causal assumptions often fail."
  - [section] "Theorem 3.2. (Environment-label dependency) Let (Gi, Yi) and (Gj, Yj) denote the graph-label pairs sampled from the training and test sets, respectively. Further, QΦ ˆG represents the invariant rationale extractor optimized on the training set. Given the rationales ˆGi = QΦ ˆG(Gi) and ˆGj = QΦ ˆG(Gj), there exists an optimal QΦ ˆG such that Φ∗ ˆG = arg maxI( ˆGi; Yi) and I(Gj \ ˆGj; Yj) > 0."
  - [corpus] Weak evidence - no direct citation, but survey papers support that real-world data violates independence assumptions.
- **Break condition:** If real-world data actually satisfies strict independence assumptions, this mechanism would add unnecessary complexity.

### Mechanism 2
- **Claim:** Mutable rationale invariance captures the fact that causal subgraphs may not be strictly invariant across environments in real-world scenarios.
- **Mechanism:** DEROG constructs rationales based on both graphs and environments, allowing the importance of different rationale components to vary across environments rather than assuming strict invariance.
- **Core assumption:** Causal subgraphs exhibit loose invariance across environments in real-world data rather than strict invariance.
- **Evidence anchors:**
  - [abstract] "The authors identify two key limitations: the assumption that environments and labels are independent, and the assumption that causal subgraphs are invariant across environments."
  - [section] "Theorem 3.3. (Mutable rationale invariance) Given a graph G with rationale ˆG, ∃ Esub ⊆ E and Ek ∈ E \ E sub, ∀ Ei, Ej ∈ E sub, we have P(Y | ˆG, Ei) = P(Y | ˆG, Ej) ̸= P(Y | ˆG, Ek)."
  - [corpus] Weak evidence - no direct citation, but real-world examples like molecular learning support this claim.
- **Break condition:** If causal subgraphs are strictly invariant across all real-world environments, this mechanism would overcomplicate the model.

### Mechanism 3
- **Claim:** Generalized Bayesian inference handles unknown prior distributions of latent variables better than standard variational inference.
- **Mechanism:** DEROG substitutes KL-divergence terms in the objective function with negative entropy, avoiding the need to assume specific prior distributions for latent variables.
- **Core assumption:** Prior distributions of latent variables (environments, rationales) are unknown or unreliable in graph-level tasks.
- **Evidence anchors:**
  - [abstract] "To alleviate the adverse effect of unknown prior knowledge on environments and rationales, DEROG utilizes generalized Bayesian inference."
  - [section] "Due to the limited knowledge regarding the prior distributions of environments and rationales, we leverage the generalized Bayesian inference technique, which substitutes the KL-divergence terms in the objective function with negative entropy."
  - [corpus] Weak evidence - no direct citation, but generalized Bayesian inference is well-established in statistics for this exact purpose.
- **Break condition:** If reliable prior distributions for latent variables can be constructed, standard variational inference would be more theoretically sound.

## Foundational Learning

- **Concept: Variational Inference**
  - Why needed here: DEROG uses variational inference to approximate the intractable posterior distribution over latent variables (environments, rationales, pseudo-labels).
  - Quick check question: What is the relationship between the ELBO and the true log-likelihood in variational inference?

- **Concept: Out-of-Distribution Generalization**
  - Why needed here: The paper addresses the challenge of making predictions on graph data that follows different distributions than the training data.
  - Quick check question: How does covariate shift differ from concept shift in the context of OOD generalization?

- **Concept: Graph Neural Networks**
  - Why needed here: DEROG uses GNNs to extract node embeddings and graph-level representations for both environment generation and rationale extraction.
  - Quick check question: What is the role of the Readout function in transforming node embeddings to graph-level embeddings?

## Architecture Onboarding

- **Component map:** Graph → Pseudo-labels → Environments & Rationales → Final Classification
- **Critical path:** The model first generates pseudo-labels, then uses these along with the graph to infer environments and rationales, which are finally used for classification.
- **Design tradeoffs:**
  - Using latent variables adds expressiveness but increases computational cost (4 GNNs total)
  - Generalized Bayesian inference avoids prior assumptions but may be less theoretically grounded than standard VI
  - Environment-label dependency improves real-world performance but may hurt synthetic benchmark performance
- **Failure signatures:**
  - Poor performance on synthetic datasets with strict invariance: indicates the environment-label dependency may be overfit to real-world data
  - High variance across random seeds: suggests the model may be sensitive to initialization due to complex latent variable structure
  - Inconsistent performance across different distribution shift types: may indicate the model doesn't properly capture the specific shift mechanism
- **First 3 experiments:**
  1. Run DEROG-v1 (without environment alignment and contrastive loss) on a simple synthetic dataset to establish baseline performance
  2. Compare DEROG-v1 vs. DEROG-v2 on a real-world dataset with known environment labels to validate the effectiveness of the additional loss terms
  3. Perform ablation studies by removing individual components (environment generator, rationale extractor) to identify their individual contributions

## Open Questions the Paper Calls Out
None

## Limitations
- The claim that environment-label dependency is universally beneficial for real-world OOD generalization lacks comprehensive ablation studies demonstrating its necessity across different dataset types
- Theoretical contributions about mutable rationale invariance are mathematically correct but their practical implications depend on unverified assumptions about real-world data distributions
- The method's performance on synthetic datasets with strict invariance assumptions suggests potential overfitting to real-world data characteristics

## Confidence
- **High confidence**: The implementation of variational inference with generalized Bayesian inference is technically sound and well-established in the literature. The experimental methodology and baseline comparisons are rigorous.
- **Medium confidence**: The theoretical contributions (Theorems 3.2 and 3.3) are mathematically correct but their practical implications depend on unverified assumptions about real-world data distributions.
- **Low confidence**: The claim that environment-label dependency is universally beneficial for real-world OOD generalization lacks comprehensive ablation studies demonstrating its necessity across different dataset types.

## Next Checks
1. **Ablation Study on Dependency Mechanisms**: Systematically remove environment-label dependency and mutable rationale invariance from DEROG and test on a diverse set of real-world datasets to quantify their individual contributions.
2. **Synthetic Data Generation**: Create synthetic graph datasets with controlled environment-label dependencies and varying levels of rationale invariance to test DEROG's performance boundaries and failure modes.
3. **Case Study Analysis**: Select 2-3 real-world datasets where DEROG shows the largest improvements and conduct detailed case studies analyzing whether environment-label dependencies and mutable rationales actually exist and drive the performance gains.