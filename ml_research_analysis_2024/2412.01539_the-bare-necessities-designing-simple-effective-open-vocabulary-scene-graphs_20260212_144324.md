---
ver: rpa2
title: 'The Bare Necessities: Designing Simple, Effective Open-Vocabulary Scene Graphs'
arxiv_id: '2412.01539'
source_url: https://arxiv.org/abs/2412.01539
tags:
- object
- scene
- feature
- clip
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper revisits the design of open-vocabulary 3D scene graphs,
  focusing on simplifying computation while maintaining performance. The authors propose
  a general framework for analyzing key design choices: image pre-processing, multi-view
  feature fusion, and feature selection.'
---

# The Bare Necessities: Designing Simple, Effective Open-Vocabulary Scene Graphs

## Quick Facts
- **arXiv ID**: 2412.01539
- **Source URL**: https://arxiv.org/abs/2412.01539
- **Reference count**: 40
- **Primary result**: Minimal open-vocabulary 3D scene graph system achieves comparable accuracy (mIOU ≈ 0.07, mAcc ≈ 0.09) with threefold reduction in computation

## Executive Summary
This paper presents a systematic analysis of design choices for open-vocabulary 3D scene graphs, demonstrating that commonly used techniques like multi-scale cropping, SAM masking, and multi-view feature averaging provide minimal accuracy improvements while significantly increasing computational costs. Through ablation studies, the authors show that removing these components reduces computation by a factor of three without sacrificing segmentation performance. They propose entropy-based feature selection as an efficient alternative for choosing the most informative view from multiple observations, improving classification accuracy without additional computational overhead.

## Method Summary
The authors propose a general framework for analyzing key design choices in open-vocabulary 3D scene graphs: image pre-processing (multi-scale cropping and masking), multi-view feature fusion (averaging vs. selection), and feature selection strategies (entropy-based). They evaluate these components on ScanNet++ and Replica datasets using CLIP for feature extraction and region growing for 3D segmentation. The final minimal system uses single-scale cropping, geometry-based region growing, and entropy-based view selection to achieve state-of-the-art accuracy with significantly reduced computation.

## Key Results
- Multi-scale cropping and SAM masking provide minimal accuracy gains while tripling computation time
- Averaging CLIP features across multiple views degrades classification accuracy due to viewpoint variance
- Entropy-based feature selection improves classification without added computational cost
- The minimal system achieves mIOU ≈ 0.07 and mAcc ≈ 0.09, comparable to state-of-the-art methods

## Why This Works (Mechanism)

### Mechanism 1
CLIP feature averaging across multiple views degrades classification accuracy because CLIP's outputs are highly viewpoint-sensitive. When CLIP produces probability distributions over labels for each view, averaging these distributions fails to capture the best representative view, instead diluting correct predictions with noisy ones. The best feature for object classification comes from a single, informative view rather than an averaged combination.

### Mechanism 2
Entropy-based feature selection improves classification accuracy by identifying the most informative view without increasing computation. By calculating the entropy of the CLIP inter-concept similarity distribution for each view, the system can select the view with the lowest entropy (most confident, least ambiguous prediction) as the representative feature for the object. Lower entropy in the CLIP output distribution correlates with more accurate object classification.

### Mechanism 3
Removing multi-scale cropping and SAM masking significantly reduces computation while maintaining classification accuracy. These computationally expensive operations require multiple feature extractions per view but provide minimal accuracy gains. The computational cost of multi-scale processing and masking outweighs their minimal accuracy benefits, yielding a threefold reduction in computation with negligible performance loss.

## Foundational Learning

- **Concept**: Entropy as a measure of uncertainty in probability distributions
  - Why needed here: Used to select the most informative CLIP view by identifying the lowest-entropy (most confident) prediction
  - Quick check question: If a CLIP view produces a probability distribution [0.9, 0.05, 0.05] for three labels and another produces [0.33, 0.33, 0.34], which has lower entropy and why?

- **Concept**: Cosine similarity for comparing feature vectors
  - Why needed here: CLIP features are compared to text encodings using cosine similarity to obtain inter-concept similarity scores
  - Quick check question: Given two feature vectors A = [1, 0] and B = [0, 1], what is their cosine similarity and what does this value indicate?

- **Concept**: Region growing for 3D point cloud segmentation
  - Why needed here: Used to segment the 3D point cloud without requiring pre-trained class labels, avoiding bias toward specific object categories
  - Quick check question: What are the three main criteria typically used in region growing algorithms to group points together?

## Architecture Onboarding

- **Component map**: RGB-D → Point Cloud → Region Growing → View Association → CLIP Feature Extraction → Entropy Calculation → Feature Selection → Output

- **Critical path**: RGB-D → Point Cloud → Region Growing → View Association → CLIP Feature Extraction → Entropy Calculation → Feature Selection → Output

- **Design tradeoffs**:
  - Accuracy vs Computation: Single-scale cropping vs multi-scale fusion (78ms vs 314ms per view)
  - Segmentation Quality vs Bias: Geometry-based region growing vs class-based methods like Mask3D
  - Evaluation Flexibility vs Rigor: Open-vocabulary evaluation vs closed-vocabulary metrics

- **Failure signatures**:
  - Low mIOU/F-mIOU scores despite reasonable segmentation visualization
  - CLIP consistently choosing semantically similar but incorrect labels
  - Region growing over-segmenting complex objects into parts
  - Entropy-based selection failing when CLIP confidence doesn't correlate with accuracy

- **First 3 experiments**:
  1. Run the system on a simple scene (office0) with ground truth data to verify the full pipeline works and measure baseline accuracy
  2. Test the impact of crop scale factor (1.0, 1.5, 2.0) on a single object across multiple scenes to validate the optimal scale choice
  3. Compare entropy-based feature selection against random view selection on a scene with clear viewpoint differences to demonstrate the selection benefit

## Open Questions the Paper Calls Out

### Open Question 1
How does CLIP's performance vary across semantically similar labels (e.g., "sofa" vs "couch") in 3D open-vocabulary scene graphs? The paper highlights that CLIP tends to assign multiple semantically similar labels to the same object, which may obscure true performance when using strict, closed-vocabulary evaluation metrics. This remains unresolved without quantitative analysis of semantically similar vs. dissimilar label predictions.

### Open Question 2
Can automatic crop size adjustment based on object distance and size improve segmentation accuracy and reduce computation? The paper notes that using a single crop size across all scenes reduces performance and suggests that automatic adjustment could offer greater flexibility. This proposed approach remains unexplored, leaving its potential benefits and computational trade-offs unknown.

### Open Question 3
How do alternative class-free 3D segmentation methods (e.g., SAM3D) compare to region growing in terms of segmentation accuracy and computational efficiency for open-vocabulary scene graphs? While the paper mentions that region growing tends to over-segment objects and suggests SAM3D could provide potential solutions, it does not evaluate or compare these methods, leaving their relative strengths and weaknesses unknown.

## Limitations

- Entropy-based feature selection assumes CLIP's confidence scores reliably indicate classification accuracy, which may not hold for all object categories
- Findings are primarily validated on indoor scenes from ScanNet++ and Replica datasets, potentially limiting applicability to outdoor environments
- The study does not explore the impact of different CLIP model variants or vision-language models on the proposed simplifications

## Confidence

- **High**: Claims about multi-scale cropping and masking providing minimal accuracy gains while tripling computation
- **Medium**: Claims about entropy-based feature selection improving accuracy without added cost
- **Medium**: Claims about averaging CLIP features across views degrading performance

## Next Checks

1. Test the proposed system on outdoor scenes or different dataset distributions to verify generalizability beyond indoor environments
2. Experiment with different CLIP model variants (e.g., ViT-L/14, ResNet-50) to assess whether the simplification benefits transfer across model architectures
3. Evaluate the impact of prompt engineering strategies on entropy-based feature selection to determine optimal prompt list configurations for different object categories