---
ver: rpa2
title: Tracking the Median of Gradients with a Stochastic Proximal Point Method
arxiv_id: '2402.12828'
source_url: https://arxiv.org/abs/2402.12828
tags:
- median
- gradient
- sample
- where
- clipping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using median-based gradient estimators for
  robust stochastic optimization. The authors analyze SGD with sample median gradients
  and show it can converge even under heavy-tailed, state-dependent noise, where standard
  SGD fails.
---

# Tracking the Median of Gradients with a Stochastic Proximal Point Method

## Quick Facts
- **arXiv ID**: 2402.12828
- **Source URL**: https://arxiv.org/abs/2402.12828
- **Reference count**: 40
- **Primary result**: Median-based gradient estimators enable robust convergence under heavy-tailed noise where standard SGD fails.

## Executive Summary
This paper addresses the challenge of robust stochastic optimization in the presence of heavy-tailed noise, outliers, or corrupted data. The authors propose using sample median gradients instead of sample mean gradients, demonstrating that this approach can converge even when standard SGD fails. They develop efficient iterative methods based on stochastic proximal point (SPP) to compute median gradients, showing connections between clipping-based optimization algorithms and median estimation across iterations. The key insight is that clipping operations can be interpreted as estimating the median gradient, unifying various optimization techniques under a single theoretical framework.

## Method Summary
The proposed method replaces the sample mean gradient estimator in SGD with a sample median estimator to handle heavy-tailed noise. To efficiently compute medians, the authors employ stochastic proximal point methods that project sampled gradients onto ℓp-balls. For ℓ2-norm this is vectorwise clipping, and for ℓ1-norm this is componentwise clipping. The SPP framework allows for online median estimation with computational efficiency. The method updates parameters using: wt+1 = wt - ηtmt+1, where mt+1 is the median gradient estimate computed via the SPP update mt+1 = gt + proxτ D(mt - gt).

## Key Results
- Median-based SGD converges under heavy-tailed, state-dependent noise where standard SGD fails
- SPP-based methods efficiently approximate median gradients with good computational properties
- Clipping-based optimization algorithms (like momentum) secretly estimate the median gradient across iterations
- Online median estimators achieve good performance with lower computational cost than exact median computation

## Why This Works (Mechanism)

### Mechanism 1
Using median-based gradient estimators enables convergence under heavy-tailed, state-dependent noise where standard SGD fails. The sample median provides a more robust estimate of the true gradient than the sample mean when noise has heavy tails or outliers, because the median has a breakdown point of approximately 1/2 while the mean can be arbitrarily corrupted by a single outlier. Core assumption: The noise distribution has heavy tails (infinite variance) or contains significant outliers. Evidence anchors: [abstract] "SGD with sample median gradients and show it can converge even under heavy-tailed, state-dependent noise, where standard SGD fails." Break condition: If noise is light-tailed (finite variance) or the proportion of outliers exceeds 50%, the median may not provide significant advantages over the mean.

### Mechanism 2
Stochastic proximal point (SPP) methods with ℓp-norms (particularly ℓ1 and ℓ2) can efficiently approximate median gradients. SPP methods project sampled gradients onto ℓp-balls, which corresponds to clipping operations that estimate the median across iterations. For ℓ2-norm this is vectorwise clipping, for ℓ1-norm this is componentwise clipping. Core assumption: The proximal operator of the chosen norm has a closed-form solution or can be efficiently computed. Evidence anchors: [abstract] "They then develop efficient iterative methods based on stochastic proximal point (SPP) to compute medians, showing connections to clipping and momentum methods." Break condition: If the norm's proximal operator doesn't have a closed form or is computationally expensive, the efficiency advantage diminishes.

### Mechanism 3
Clipping-based optimization algorithms secretly estimate the median gradient across iterations. Methods like momentum (which estimates the mean) and various clipping techniques (which estimate the median) can be unified under the SPP framework, where the step size determines whether mean or median estimation occurs. Core assumption: The optimization algorithm can be expressed as an SPP update with an appropriate norm choice. Evidence anchors: [abstract] "The key insight is that clipping-based optimization algorithms secretly estimate the median gradient across iterations." Break condition: If the algorithm cannot be expressed in SPP form or requires non-norm-based regularization, the median estimation interpretation may not hold.

## Foundational Learning

- **Heavy-tailed distributions and their impact on statistical estimation**: Understanding why standard mean-based methods fail under heavy-tailed noise is crucial for appreciating the need for median-based approaches. Quick check: What is the breakdown point of the sample mean versus the sample median, and why does this matter for robust optimization?

- **Proximal operators and their closed-form solutions for common norms**: Efficiently computing median gradients requires understanding how proximal operators work for ℓ1, ℓ2, and other norms. Quick check: What is the proximal operator of the ℓ2-norm, and how does it relate to vectorwise clipping?

- **Stochastic optimization convergence theory under different noise assumptions**: Proving convergence of median-based methods requires different theoretical tools than standard SGD analysis. Quick check: How does the convergence rate of SGD change when moving from light-tailed to heavy-tailed noise assumptions?

## Architecture Onboarding

- **Component map**: Stochastic gradient oracle → Median estimator (SPP) → Parameter update
- **Critical path**: 1. Sample stochastic gradient gt 2. Update median estimate mt+1 using SPP: mt+1 = gt + proxτ D(mt - gt) 3. Update parameters: wt+1 = wt - ηtmt+1 4. Repeat
- **Design tradeoffs**: Accuracy vs. computational cost: Exact median computation is expensive; SPP provides approximation. Norm choice: ℓ1 gives componentwise robustness, ℓ2 gives vectorwise robustness. Step size tuning: Affects both convergence speed and robustness properties.
- **Failure signatures**: Divergence with light-tailed noise (mean might be better), slow convergence with very small step sizes, computational bottleneck if proximal operator is expensive.
- **First 3 experiments**: 1. Compare convergence of SGD with mean vs. median gradients on synthetic heavy-tailed data 2. Test different norm choices (ℓ1 vs. ℓ2) for median estimation on real-world datasets 3. Evaluate the impact of step size on both convergence speed and final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
Does the choice of p in the ℓp-median (e.g., p=1 vs p=2) significantly impact the robustness and convergence speed of SGD in practice? Basis in paper: The paper analyzes the ℓ1 and ℓ2 medians but does not provide a comprehensive comparison of their performance across diverse settings. Why unresolved: The theoretical analysis focuses on specific cases, and the experimental section only compares a limited number of methods. What evidence would resolve it: A large-scale empirical study comparing the performance of different ℓp-median choices in SGD for various tasks and noise models.

### Open Question 2
How does the sample size n (number of gradients per iteration) affect the trade-off between robustness and computational cost in sample median-based SGD? Basis in paper: The paper uses n=5 in some experiments but does not explore the impact of varying n on the convergence and robustness of the method. Why unresolved: The theoretical analysis assumes a fixed n, and the experiments do not systematically investigate the impact of this hyperparameter. What evidence would resolve it: An empirical study varying n and measuring the trade-off between robustness (e.g., resistance to outliers) and computational cost (e.g., runtime, memory usage) across different tasks and noise models.

### Open Question 3
Can the SPP-based methods for computing medians be extended to handle more complex noise models, such as those with dependencies across dimensions or non-symmetric distributions? Basis in paper: The theoretical analysis focuses on independent noise and symmetric distributions. Why unresolved: The current theoretical framework might not be directly applicable to more complex noise models. What evidence would resolve it: Theoretical analysis extending the convergence guarantees to more general noise models, accompanied by empirical validation on tasks with complex noise structures.

## Limitations
- Theoretical analysis primarily addresses convex problems and assumes access to exact median computation in some convergence proofs
- Connections between clipping methods and median estimation may not hold for all norm choices or optimization landscapes
- Experimental validation does not extensively test on diverse real-world scenarios with naturally occurring heavy-tailed noise

## Confidence
- **High Confidence**: Median-based gradient estimators provide robustness to heavy-tailed noise where standard SGD fails
- **Medium Confidence**: Stochastic proximal point methods can efficiently approximate median gradients with good computational properties
- **Low Confidence**: All clipping-based optimization algorithms secretly estimate the median gradient across iterations

## Next Checks
1. **Stress Test with Adversarial Noise**: Test the proposed methods on datasets with adversarially corrupted gradients (not just heavy-tailed noise) to evaluate robustness beyond the noise models considered in the paper.

2. **Non-Convex Landscape Analysis**: Extend the convergence analysis to non-convex optimization problems common in deep learning to verify if the median-based approaches maintain their advantages in realistic settings.

3. **Computational Overhead Benchmarking**: Conduct detailed runtime analysis comparing the SPP-based median computation to standard clipping and momentum methods across different problem sizes to quantify the practical efficiency gains.