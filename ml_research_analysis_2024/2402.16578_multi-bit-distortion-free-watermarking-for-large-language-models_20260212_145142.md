---
ver: rpa2
title: Multi-Bit Distortion-Free Watermarking for Large Language Models
arxiv_id: '2402.16578'
source_url: https://arxiv.org/abs/2402.16578
tags:
- watermarking
- text
- tokens
- token
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new method called DISC for watermarking
  text generated by large language models (LLMs). The main idea is to embed multiple
  bits of information (like model name, version, or generation time) into the watermark
  without distorting the text quality.
---

# Multi-Bit Distortion-Free Watermarking for Large Language Models

## Quick Facts
- arXiv ID: 2402.16578
- Source URL: https://arxiv.org/abs/2402.16578
- Reference count: 40
- The paper introduces DISC, a distortion-free watermarking method for LLMs that can embed multiple bits of information without degrading text quality.

## Executive Summary
This paper presents DISC, a novel watermarking algorithm for large language models that achieves multi-bit embedding without distorting the output text. The method uses a partition-based approach to map uniform random numbers to tokens in a way that preserves the original LLM distribution while embedding message information. DISC supports efficient decoding through an algorithm that exploits the structure of the score function, enabling reliable extraction of embedded information with low bit error rates.

## Method Summary
DISC extends zero-bit distortion-free watermarking to support multi-bit embedding by shifting token selection intervals based on the message to be embedded. The encoder generates uniform random numbers using a pseudorandom function (PRF) with a secret key, then maps these numbers to tokens through a partition of the sample space whose sizes match the original model's token probabilities. The decoder reconstructs random numbers from the text and calculates score functions to detect the watermark and extract the embedded message, using an efficient algorithm that avoids exhaustive search by exploiting the predictable pattern of the score function.

## Key Results
- Successfully embeds and extracts 1-bit watermarks with zero error over just 6 tokens
- Achieves zero error for 4-bit watermarks over 20 tokens
- Maintains distortion-free property where watermarked text has identical distribution to original LLM output

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Watermark embedding is distortion-free by partitioning the sample space into segments whose lengths equal the conditional probability of each token.
- Mechanism: For each token position, uniform random numbers are assigned to tokens by partitioning the interval [0,1] into segments whose lengths match the conditional probabilities of tokens given the context.
- Core assumption: The mapping rule can be constructed such that the probability of selecting each token equals its original probability in the LLM.
- Evidence anchors: The abstract states watermarked text has the same distribution as original LLM output; Proposition 2.5 provides the theoretical foundation.
- Break condition: If partition sizes deviate from true conditional probabilities due to rounding or implementation error.

### Mechanism 2
- Claim: Multi-bit information is embedded without distribution distortion by shifting token selection intervals.
- Mechanism: DISC uses parameter δM = M δ (where M is the message and δ = 2−m) to shift interval assignments so different messages map to different tokens while preserving probability mass.
- Core assumption: Shifted partitions maintain the same marginal probabilities as the original model.
- Evidence anchors: The abstract mentions embedding multiple bits without distorting text quality; section describes mapping rule with δM showing probability preservation.
- Break condition: If δM is too large relative to token probability mass, intervals may overlap or leave gaps.

### Mechanism 3
- Claim: Efficient decoding is achieved by exploiting the predictable structure of the score function.
- Mechanism: The decoder calculates scores at sparse, equally spaced δM′ values rather than exhaustive search, leveraging the known pattern where the score function has maximum at ∆ = 0 with predictable valleys.
- Core assumption: The score function's shape is consistent and exploitable for efficient search.
- Evidence anchors: The abstract states efficient algorithm extracts information with low BER; section explains using the pattern to calculate scores at a small set of δM′ values.
- Break condition: If score function pattern changes due to model updates or adversarial modifications.

## Foundational Learning

- Concept: PseudoRandom Functions (PRFs)
  - Why needed here: PRFs generate uniform random numbers used in watermarking, ensuring watermark cannot be easily detected or removed without the secret key.
  - Quick check question: What property of PRFs makes them suitable for generating random numbers used in watermarking?

- Concept: Entropy of Language Models
  - Why needed here: Entropy calculations determine minimum text length required for reliable watermark detection based on conditional entropy of token generation.
  - Quick check question: How does average conditional entropy per token affect number of tokens needed for watermark detection?

- Concept: Statistical Hypothesis Testing
  - Why needed here: Decoder uses statistical tests (p-values, critical regions) to determine watermark presence and extract embedded message.
  - Quick check question: What is the relationship between false positive rate, false negative rate, and threshold used in statistical test?

## Architecture Onboarding

- Component map: PRF -> Encoder -> Watermarked Text -> Decoder -> Score Function -> Statistical Test
- Critical path:
  1. Encoder receives prompt, secret key, and message
  2. PRF generates uniform random numbers for each token position
  3. Watermarking mapping rule assigns tokens based on random numbers and message
  4. Decoder reconstructs random numbers from text using same PRF and key
  5. Decoder calculates score functions for different message hypotheses
  6. Statistical test determines watermark presence and extracts message

- Design tradeoffs:
  - Message capacity vs. detection reliability: Higher bit capacity requires longer text for reliable detection
  - Efficiency vs. robustness: Efficient decoding relies on predictable score function patterns, which may be vulnerable to adversarial modifications
  - Distortion-free vs. detectability: Watermark must be embedded without changing output distribution, limiting signal strength

- Failure signatures:
  - High bit error rate: Indicates insufficient text length or adversarial interference
  - False negatives: Suggests threshold is too high or text length is insufficient
  - False positives: Indicates threshold is too low or statistical test is not properly calibrated

- First 3 experiments:
  1. Verify distortion-free property by comparing watermarked text distribution to original LLM output distribution
  2. Test multi-bit embedding by measuring bit error rate for messages of different lengths
  3. Evaluate detection efficiency by measuring time and resources required with and without efficient search optimization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does choice of ngram size h impact security and efficiency of watermarking scheme?
- Basis in paper: [explicit] Paper mentions h is chosen such that ⌊h/ log |V|⌋ ≤ 8, specifically setting ⌊h/ log |V|⌋ = 5, but impact on performance is not fully explored.
- Why unresolved: Paper does not provide detailed analysis of how different h values affect scheme's ability to resist attacks or computational efficiency.
- What evidence would resolve it: Experiments comparing performance with different ngram sizes h, measuring both security against attacks and computational efficiency.

### Open Question 2
- Question: What is impact of vocabulary size |V| on required number of tokens for successful watermark embedding and extraction?
- Basis in paper: [explicit] Paper uses vocabulary size |V| = 50272 and provides equations involving |V|, but does not explore how changing |V| affects watermarking performance.
- Why unresolved: Paper does not provide experiments or analysis showing how watermarking scheme performs with different vocabulary sizes.
- What evidence would resolve it: Experiments demonstrating performance with various vocabulary sizes and analyzing how this affects required number of tokens and error rates.

### Open Question 3
- Question: How does DISC algorithm compare to other multi-bit watermarking methods in terms of embedding capacity and robustness?
- Basis in paper: [inferred] Paper claims to be first to achieve multi-bit distortion-free watermarking with efficient decoding, but does not provide direct comparison with other multi-bit watermarking methods.
- Why unresolved: Paper does not benchmark DISC algorithm against other existing multi-bit watermarking techniques.
- What evidence would resolve it: Comprehensive comparison of DISC with other multi-bit watermarking methods evaluating embedding capacity, robustness against attacks, and computational efficiency.

## Limitations

- The numerical stability of the shifted partition scheme for rare tokens with extremely small probability masses is not explicitly addressed
- Performance under realistic scenarios with longer messages, diverse text genres, and adversarial attacks remains largely unexplored
- Security implications of secret key management and potential vulnerabilities to key recovery attacks are not discussed

## Confidence

**High Confidence:**
- The fundamental distortion-free property of the watermarking scheme
- The multi-bit embedding capability demonstrated through controlled experiments
- The basic structure of the efficient decoding algorithm and its theoretical foundation

**Medium Confidence:**
- The practical effectiveness of the efficient decoding algorithm in real-world conditions
- The scalability of the approach to longer messages and more complex scenarios
- The robustness of the watermark against various forms of text modifications

**Low Confidence:**
- The security guarantees against sophisticated adversarial attacks
- The performance under diverse linguistic contexts and text genres
- The practical implementation challenges related to numerical precision and floating-point arithmetic

## Next Checks

1. **Numerical Stability Verification**: Implement the watermarking scheme with controlled floating-point precision and systematically test whether the distortion-free property holds across a wide range of token probabilities, including very rare tokens.

2. **Adversarial Robustness Testing**: Design experiments where watermarked text is modified through common text editing operations that preserve human readability but may disrupt the watermark structure, measuring impact on detection accuracy and extraction reliability.

3. **Scalability Assessment**: Extend experimental validation to test watermark embedding and extraction for messages of increasing length across varying text lengths, measuring the relationship between message capacity, text length, and bit error rate.