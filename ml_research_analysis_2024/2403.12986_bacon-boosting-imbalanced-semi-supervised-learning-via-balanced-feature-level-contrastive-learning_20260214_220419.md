---
ver: rpa2
title: 'BaCon: Boosting Imbalanced Semi-supervised Learning via Balanced Feature-Level
  Contrastive Learning'
arxiv_id: '2403.12986'
source_url: https://arxiv.org/abs/2403.12986
tags:
- learning
- data
- contrastive
- distribution
- semi-supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method called BaCon to address the problem
  of class-imbalanced semi-supervised learning (CISSL). In CISSL, the feature representations
  learned by the backbone model are biased towards the majority classes, which limits
  the performance of downstream classification.
---

# BaCon: Boosting Imbalanced Semi-supervised Learning via Balanced Feature-Level Contrastive Learning

## Quick Facts
- arXiv ID: 2403.12986
- Source URL: https://arxiv.org/abs/2403.12986
- Reference count: 10
- Improves CISSL accuracy by up to 0.63% on CIFAR100-LT

## Executive Summary
BaCon addresses class-imbalanced semi-supervised learning by regularizing feature distributions through balanced contrastive learning. Unlike existing methods that handle imbalance at the instance level, BaCon directly improves backbone representations by using class-wise feature centers as positive anchors and a balanced temperature adjustment mechanism. The method shows strong performance improvements on benchmark datasets while maintaining better robustness against extreme imbalance degrees.

## Method Summary
BaCon combines semi-supervised learning with feature-level contrastive learning to address class imbalance. The method projects representations into a contrastive space, computes class-wise feature centers as positive anchors, and uses reliable negative selection to find appropriate negative samples. A balanced temperature adjustment mechanism dynamically controls the contrastive learning degree per class based on the imbalance distribution. The approach is integrated with existing semi-supervised learning frameworks like FixMatch, adding a projection head and memory banks while maintaining the original loss components.

## Key Results
- Achieves up to 0.63% accuracy improvement on CIFAR100-LT
- Demonstrates better robustness against more extreme imbalance degrees compared to other methods
- Outperforms state-of-the-art methods in CISSL across multiple benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
Most existing methods address imbalance at the instance level through reweighting or resampling, but performance is heavily limited by their reliance on biased backbone representation. The upstream feature extractor receives imbalanced gradients from the backbone classifier head, causing feature learning to push away from optimal directions.

### Mechanism 2
Using class-wise feature centers as positive anchors in contrastive learning encourages balanced clustering across all classes. By computing feature centers for each class and using them as positive anchors, the method pulls instances of each class toward their respective centers, promoting more balanced representation distributions.

### Mechanism 3
Balanced temperature adjustment dynamically modulates contrastive learning strength per class based on class size distribution. The temperature coefficient for each class is inversely correlated with the class size ratio, ensuring that minority classes are not overwhelmed by majority class gradients during contrastive learning.

## Foundational Learning

- **Contrastive learning fundamentals** - Understanding positive/negative pairs, temperature scaling, and InfoNCE loss is essential as BaCon builds on contrastive learning to regularize feature distributions.
  - Quick check: What role does the temperature parameter play in contrastive loss, and why might different classes need different temperatures under imbalance?

- **Semi-supervised learning with pseudo-labeling** - The method relies on pseudo-labels generated from the auxiliary classifier to identify reliable instances for feature memory banks and contrastive learning.
  - Quick check: How does the confidence threshold for pseudo-labels affect the reliability of the feature memory bank in BaCon?

- **Class imbalance handling strategies** - Understanding the limitations of instance-level imbalance methods is crucial for appreciating BaCon's design motivation.
  - Quick check: Why might resampling or reweighting at the instance level be insufficient when the backbone representation is already biased?

## Architecture Onboarding

- **Component map**: Backbone model -> Feature extractor -> Linear projection head -> Memory banks (Sb for features, SK for labels) -> Auxiliary classifier -> Reliable Negative Selection (RNS) -> Balanced Temperature Adjusting (BTA) -> Loss aggregation

- **Critical path**:
  1. Forward pass through backbone and projection head
  2. Pseudo-label generation and confidence filtering
  3. Memory bank updates (reliable instances only)
  4. Class-wise feature center computation
  5. RNS to identify reliable negatives
  6. Compute contrastive loss with BTA
  7. Backward pass and parameter update

- **Design tradeoffs**:
  - Memory overhead vs. stable feature centers: Larger memory banks give more stable centers but require more storage
  - Temperature sensitivity vs. imbalance robustness: BTA helps minority classes but may slow convergence if over-adjusted
  - RNS strictness vs. negative sample sufficiency: Stricter filtering yields cleaner negatives but may reduce batch diversity

- **Failure signatures**:
  - Degraded performance on minority classes → Check BTA calibration or memory bank reliability
  - High variance in results → Inspect pseudo-label confidence thresholds and RNS filtering
  - Slow convergence → Verify that temperature modulation isn't too aggressive or that projection head dimensionality is appropriate

- **First 3 experiments**:
  1. Run baseline FixMatch on CIFAR10-LT and measure class-wise accuracy to confirm imbalance impact
  2. Add BaCon's contrastive loss with fixed temperature and observe changes in feature distribution via t-SNE
  3. Enable BTA and compare minority vs. majority class accuracy gains; tune BTA sensitivity parameter η

## Open Questions the Paper Calls Out

### Open Question 1
How does BaCon perform when applied to more complex datasets or tasks beyond image classification, such as object detection or segmentation? The paper focuses on image classification tasks, leaving the performance on other tasks unexplored.

### Open Question 2
What is the impact of using different backbone architectures on BaCon's performance, and how does it compare to other methods with the same backbone? The paper uses Wide ResNet-28-2 as the backbone architecture but does not explore effects of using different backbones.

### Open Question 3
How does BaCon handle open-set scenarios, where the test data contains classes not seen during training? The paper focuses on closed-set classification tasks where test data is assumed to contain only classes seen during training.

## Limitations
- Feature center estimation becomes unreliable when classes have very few labeled examples
- Temperature calibration requires careful tuning and may be dataset-specific
- Inherits limitations from pseudo-labeling, where low-confidence predictions could corrupt the memory bank

## Confidence

- **High confidence**: The core observation that backbone bias limits downstream performance under class imbalance is well-supported by empirical evidence
- **Medium confidence**: The theoretical motivation for feature-level contrastive learning as a solution to representation bias is sound, but specific implementation details may require dataset-specific tuning
- **Low confidence**: The robustness claims under extreme imbalance conditions need further validation, as the paper does not extensively test scenarios with very few labeled examples per class

## Next Checks
1. Test BaCon on datasets with varying imbalance ratios (γ = 100, 200, 500) to quantify performance degradation as imbalance severity increases
2. Conduct ablation studies removing BTA to isolate its contribution versus the baseline contrastive learning component
3. Evaluate feature center stability across training epochs by measuring intra-class variance and center drift for minority classes