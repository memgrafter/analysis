---
ver: rpa2
title: 'MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language Models'
arxiv_id: '2407.02775'
source_url: https://arxiv.org/abs/2407.02775
tags:
- distillation
- student
- teacher
- knowledge
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MLKD-BERT proposes a two-stage knowledge distillation method for
  BERT compression that improves upon existing approaches by incorporating relation-level
  knowledge (token and sample similarities) alongside feature-level knowledge. The
  method uses KL-divergence losses to transfer token embedding similarities, self-attention
  relations, and sample similarities from teacher to student models.
---

# MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language Models

## Quick Facts
- arXiv ID: 2407.02775
- Source URL: https://arxiv.org/abs/2407.02775
- Reference count: 19
- Primary result: MLKD-BERT achieves 99.5% of teacher performance with 50% parameter reduction

## Executive Summary
MLKD-BERT introduces a novel two-stage knowledge distillation method for compressing BERT models that significantly outperforms existing approaches. The method incorporates relation-level knowledge through token embedding similarities, self-attention relations, and sample similarities, in addition to standard feature-level distillation. By distilling self-attention relations rather than distributions, MLKD-BERT enables flexible student attention head numbers that reduce inference time while maintaining performance. Experiments on GLUE benchmark and SQuAD tasks demonstrate state-of-the-art performance, achieving 99.5% of teacher performance with 50% parameter reduction.

## Method Summary
MLKD-BERT employs a two-stage distillation framework that transfers knowledge from a pre-trained BERT-base teacher to a smaller student model. The first stage focuses on embedding-layer and Transformer-layer distillation, using four loss functions to transfer token similarities, self-attention relations, and feed-forward network outputs. The second stage emphasizes prediction-layer distillation with three additional loss functions for sample similarities, sample contrastive relations, and soft label knowledge. A key innovation is the use of MHA-splits to distill self-attention relations rather than distributions, enabling flexible student attention head configurations that reduce inference time. The method achieves this through KL-divergence losses across seven distinct knowledge transfer pathways.

## Key Results
- MLKD-BERT achieves 99.5% of teacher performance with 50% parameter reduction on GLUE tasks
- Inference time reduction of up to 14% with flexible attention heads while maintaining accuracy
- Outperforms state-of-the-art BERT distillation methods including BERTTINY, DistilBERT, and TinyBERT
- Maintains strong performance on both GLUE benchmark (8 tasks) and SQuAD 1.1/2.0 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distilling self-attention relations rather than distributions enables flexible student attention head numbers while preserving performance
- Mechanism: Instead of transferring self-attention distributions (softmax outputs), the method transfers raw attention head outputs and computes similarities post-hoc. This decouples student architecture from teacher architecture
- Core assumption: Attention head outputs contain sufficient information for distillation even when not directly used as distributions
- Evidence anchors:
  - [abstract]: "Crucially, it distills self-attention relations rather than distributions, allowing flexible student attention head numbers that reduce inference time"
  - [section 3.2]: "In MHA distillation, MHA outputs (i.e., Ol,a defined in Eqn.(4)) are concatenated together and then split into a certain number of vector groups (named MHA-splits), in both teacher and student models. We suggest setting the number of MHA-splits as the number of student attention heads."
- Break condition: If attention head outputs are too sparse or if teacher attention patterns are highly idiosyncratic, the relation-based approach may fail to capture critical structural information

### Mechanism 2
- Claim: Incorporating relation-level knowledge (token and sample similarities) provides complementary information to feature-level knowledge
- Mechanism: Beyond standard feature matching, the method adds losses that capture semantic relationships - how tokens relate to each other and how samples relate to each other within batches
- Core assumption: Relational structures in the teacher model contain information not captured by individual feature representations
- Evidence anchors:
  - [abstract]: "The method uses KL-divergence losses to transfer token embedding similarities, self-attention relations, and sample similarities from teacher to student models"
  - [section 3.1]: "In embedding-layer distillation, token similarity relation is transferred from teacher to student by minimizing the KL-divergence of token embedding similarities"
- Break condition: If the teacher model's relational structures are noisy or if the student architecture cannot effectively utilize this relational information

### Mechanism 3
- Claim: Two-stage distillation allows different emphasis for representation learning versus prediction learning
- Mechanism: Stage 1 focuses on intermediate representations (embedding and transformer layers) while Stage 2 focuses on final predictions (sample similarities and soft labels)
- Core assumption: Different learning objectives require different optimization strategies and cannot be effectively combined in a single stage
- Evidence anchors:
  - [abstract]: "MLKD-BERT employs a two-stage distillation procedure for downstream task prediction"
  - [section 3.3]: "Stage 1 emphasizes distilling feature representation and transformation, while Stage 2 emphasizes distilling sample prediction"
- Break condition: If the tasks being distilled have simple decision boundaries where intermediate representations don't significantly impact final performance

## Foundational Learning

- Concept: Knowledge Distillation fundamentals (teacher-student training paradigm)
  - Why needed here: The entire method builds on transferring knowledge from a larger teacher model to a smaller student model
  - Quick check question: What is the difference between logits distillation and feature distillation in knowledge distillation?

- Concept: Transformer architecture (self-attention, multi-head attention, feed-forward networks)
  - Why needed here: The method operates on BERT's transformer layers and must understand how attention heads work
  - Quick check question: How do multi-head attention outputs get combined in a standard transformer layer?

- Concept: KL-divergence and loss functions for distribution matching
  - Why needed here: The method uses KL-divergence to match various distributions (token similarities, sample similarities, soft labels)
  - Quick check question: When would you use KL-divergence versus MSE for distillation losses?

## Architecture Onboarding

- Component map:
  - Teacher: BERT-base (12 layers, 12 attention heads, 768 hidden size)
  - Student: Configurable (4 or 6 layers, configurable attention heads, smaller hidden size)
  - Two-stage training pipeline with 6 distinct loss functions
  - MHA-split mechanism for flexible attention head mapping

- Critical path:
  1. Initialize student with TinyBERT initialization
  2. Stage 1: Embedding distillation (LEMB) + Transformer distillation (LMHA + LFFN)
  3. Stage 2: Prediction distillation (LSS + LSC + LKD)
  4. Fine-tune on downstream tasks

- Design tradeoffs:
  - Flexible attention heads vs. potential loss of fine-grained attention information
  - Two-stage training vs. training efficiency
  - Relation-level losses vs. increased computational overhead

- Failure signatures:
  - Performance degradation when reducing attention heads below certain thresholds
  - Training instability when balancing multiple loss functions
  - Suboptimal performance on tasks where relational information is less relevant

- First 3 experiments:
  1. Compare MLKD-BERT with standard distillation baseline on a single GLUE task
  2. Test flexible attention head configuration (e.g., 12 vs 6 vs 3 heads) on inference time vs accuracy
  3. Ablation study removing relation-level losses to measure their contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MLKD-BERT vary when applied to different pre-trained language models beyond BERT, such as RoBERTa or XLNet?
- Basis in paper: [inferred] The paper discusses MLKD-BERT's effectiveness in compressing BERT but does not explore its applicability to other pre-trained models.
- Why unresolved: The paper focuses solely on BERT, leaving the generalizability of MLKD-BERT to other models untested.
- What evidence would resolve it: Experimental results comparing MLKD-BERT's performance on RoBERTa, XLNet, and other models would clarify its broader applicability.

### Open Question 2
- Question: What is the impact of varying the mapping strategy for student to teacher layers in MLKD-BERT, beyond the uniform mapping strategy used in the experiments?
- Basis in paper: [explicit] The paper mentions using a uniform mapping strategy but does not explore alternative mapping strategies.
- Why unresolved: The uniform mapping strategy may not be optimal for all tasks or model architectures, and exploring alternatives could yield performance improvements.
- What evidence would resolve it: Comparative experiments using different layer mapping strategies, such as proportional or learned mappings, would indicate the best approach for various scenarios.

### Open Question 3
- Question: How does the inclusion of relation-level knowledge distillation affect the robustness of MLKD-BERT to adversarial attacks or noisy data?
- Basis in paper: [inferred] The paper emphasizes the role of relation-level knowledge but does not assess its impact on model robustness.
- Why unresolved: While relation-level knowledge improves performance, its effect on robustness to adversarial examples or data noise is not explored.
- What evidence would resolve it: Experiments testing MLKD-BERT's performance under adversarial attacks or with noisy data compared to models without relation-level distillation would provide insights into its robustness.

## Limitations

- Limited exploration of alternative layer mapping strategies beyond uniform mapping
- Complex two-stage training procedure with seven loss functions requiring careful hyperparameter tuning
- Potential overfitting to GLUE benchmark tasks without testing on diverse NLP tasks
- Uncertainty about generalization to other pre-trained language models beyond BERT

## Confidence

*High Confidence*: The fundamental concept of multi-level knowledge distillation (token, sample, and self-attention relations) is well-grounded in existing KD literature and the experimental methodology (GLUE benchmark, SQuAD datasets) is standard for this domain.

*Medium Confidence*: The specific claim about flexible attention heads enabling inference time reduction is supported by ablation studies but requires verification across different hardware and batch sizes.

*Low Confidence*: The exact implementation of the MHA-split mechanism and its impact on attention head flexibility across different student architectures needs independent reproduction.

## Next Checks

1. **Independent Reproduction**: Implement MLKD-BERT from scratch and verify the core claims on at least two GLUE tasks (MNLI and SST-2) and SQuAD, comparing against standard distillation baselines. Focus on reproducing the flexible attention head configuration and measuring actual inference time reduction across different GPU configurations.

2. **Loss Function Ablation**: Systematically remove each of the seven loss functions (LEMB, LMHA, LFFN, LSS, LSC, LKD) individually to quantify their relative contribution to final performance. This should be done across multiple tasks to understand which components are most critical for different task types.

3. **Architecture Sensitivity Analysis**: Test the method with different student architectures beyond the two configurations presented (4-layer vs 6-layer), varying hidden sizes, attention head counts, and intermediate sizes to understand the robustness of the approach across the design space. This should include testing whether the flexible attention head mechanism provides consistent benefits across different model sizes.