---
ver: rpa2
title: 'Large Language Models for Medical OSCE Assessment: A Novel Approach to Transcript
  Analysis'
arxiv_id: '2410.12858'
source_url: https://arxiv.org/abs/2410.12858
tags:
- medical
- grading
- history
- osce
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the use of large language models (LLMs)
  to assess medical students' ability to summarize patient medical history in Objective
  Structured Clinical Examinations (OSCEs). The research analyzed 2,027 video-recorded
  OSCE examinations, transcribing the audio using Whisper-v3 and applying various
  LLM-based grading approaches.
---

# Large Language Models for Medical OSCE Assessment: A Novel Approach to Transcript Analysis

## Quick Facts
- arXiv ID: 2410.12858
- Source URL: https://arxiv.org/abs/2410.12858
- Reference count: 34
- Key outcome: LLM-based OSCE assessment achieved 0.88 Cohen's kappa agreement with human graders using GPT-4, demonstrating strong potential for augmenting medical education assessment

## Executive Summary
This study presents a novel approach to automating Objective Structured Clinical Examination (OSCE) assessment using large language models. The research analyzed 2,027 video-recorded medical student OSCE examinations, applying advanced LLM techniques including zero-shot chain-of-thought prompting, retrieval augmented generation, and multi-model ensemble methods. Frontier models like GPT-4 demonstrated remarkable alignment with human graders (Cohen's kappa of 0.88), while open-source models also showed promising performance. The study not only validates the technical feasibility of LLM-based medical assessment but also identifies key failure modes and provides practical recommendations for deployment in medical education settings.

## Method Summary
The research transcribed audio from 2,027 video-recorded OSCE examinations using Whisper-v3, then applied various LLM-based grading approaches to assess students' ability to summarize patient medical history. The study employed frontier models including GPT-4 alongside open-source alternatives, exploring multiple prompting strategies and ensemble methods. Performance was evaluated against human grader assessments using Cohen's kappa agreement as the primary metric. The analysis included systematic investigation of model behaviors, identification of failure modes, and development of deployment recommendations based on observed performance patterns.

## Key Results
- GPT-4 achieved 0.88 Cohen's kappa agreement with human graders for OSCE assessment
- Multi-model ensemble approaches showed performance improvements when models agreed on assessments
- Open-source models demonstrated promising results, suggesting broader accessibility of LLM-based assessment tools
- Systematic identification of failure modes provided insights for practical deployment considerations

## Why This Works (Mechanism)
The success of LLM-based OSCE assessment stems from the models' ability to process complex clinical narratives and extract relevant medical information consistently. LLMs excel at pattern recognition across diverse patient presentations and can apply standardized assessment criteria systematically. The chain-of-thought prompting enables models to break down complex clinical reasoning tasks into manageable steps, while retrieval augmentation helps maintain consistency with established medical knowledge. Ensemble approaches leverage the complementary strengths of different models, reducing individual model biases and improving overall reliability.

## Foundational Learning
- **Cohen's kappa coefficient**: Measures inter-rater agreement while accounting for chance agreement - needed to properly evaluate LLM-human grader alignment in medical assessment
- **Zero-shot chain-of-thought prompting**: Enables complex reasoning without task-specific training - needed to adapt general-purpose LLMs to specialized medical assessment tasks
- **Retrieval augmented generation**: Combines LLMs with external knowledge bases - needed to ensure assessments align with current medical standards and guidelines
- **Ensemble learning in medical AI**: Uses multiple models to improve reliability - needed to reduce individual model biases and increase assessment consistency
- **OSCE assessment frameworks**: Standardized clinical skills evaluation methods - needed to ensure LLM assessments align with established medical education standards
- **Whisper-v3 transcription accuracy**: Automatic speech recognition performance - needed to ensure high-quality input data for LLM analysis

## Architecture Onboarding

**Component Map**: Whisper-v3 (transcription) -> LLM models (assessment) -> Ensemble aggregation -> Performance evaluation -> Failure mode analysis

**Critical Path**: Video input → Automated transcription → LLM-based assessment → Human grader comparison → Performance metrics calculation → Failure analysis → Deployment recommendations

**Design Tradeoffs**: 
- **Accuracy vs. Speed**: Higher-performing frontier models require more computational resources but achieve better alignment with human graders
- **Open-source vs. Proprietary**: Open models offer accessibility but may sacrifice some performance compared to frontier models
- **Single vs. Multi-model approaches**: Ensemble methods improve reliability but increase complexity and computational requirements

**Failure Signatures**: 
- Transcription errors from Whisper-v3 affecting assessment quality
- LLM hallucinations producing clinically implausible assessments
- Inconsistent application of grading rubrics across different model architectures
- Over-reliance on pattern matching without true clinical reasoning
- Sensitivity to prompt variations and formatting differences

**First Experiments**:
1. Compare Cohen's kappa agreement between single LLM assessments vs. ensemble approaches
2. Analyze failure mode distribution across different medical specialties and case complexity
3. Evaluate the impact of transcription quality on final assessment accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Single-institution study limits generalizability to other medical schools and assessment standards
- Focus on history-taking summaries excludes other critical OSCE components like physical examination and procedural skills
- Human grader variability not fully characterized, making it difficult to determine if LLM-human disagreements represent true errors
- No inter-rater reliability data for human assessors to establish baseline variance

## Confidence
- High confidence in core technical results (LLM performance metrics, kappa agreement values)
- Medium confidence in generalizability to broader medical education contexts
- Medium confidence in deployment recommendations based on identified failure modes

## Next Checks
1. External validation across multiple institutions with different OSCE formats and grading rubrics
2. Inter-rater reliability analysis of human graders within the current dataset
3. Extended validation incorporating other OSCE components beyond history-taking summaries