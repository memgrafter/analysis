---
ver: rpa2
title: 'HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution
  Vision-Language Models'
arxiv_id: '2408.10945'
source_url: https://arxiv.org/abs/2408.10945
tags:
- tokens
- visual
- token
- budget
- hired
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient inference for high-resolution
  Vision-Language Models (VLMs), which generate excessive visual tokens, leading to
  increased latency, memory usage, and computational demands. To solve this, the authors
  propose HiRED, a plug-and-play token-dropping framework that leverages attention
  patterns in Vision Transformers (ViTs).
---

# HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models

## Quick Facts
- **arXiv ID**: 2408.10945
- **Source URL**: https://arxiv.org/abs/2408.10945
- **Reference count**: 11
- **Primary result**: HiRED-20% achieves 4.7× token generation throughput, 78% latency reduction, and 14% GPU memory savings while preserving accuracy across multimodal benchmarks.

## Executive Summary
HiRED addresses the efficiency bottleneck in high-resolution Vision-Language Models (VLMs) where visual tokens constitute 80-90% of all tokens but receive significantly less attention than system and text tokens. The proposed plug-and-play framework leverages attention patterns in Vision Transformers to selectively drop less informative visual tokens without accuracy loss. By using CLS-attention from initial layers for budget allocation and final layers for token selection, HiRED achieves substantial efficiency gains while maintaining competitive accuracy across diverse multimodal benchmarks.

## Method Summary
HiRED is a two-phase token-dropping framework that operates on pre-trained ViT encoders and LLMs without requiring model training. Phase 1 (Token Budget Allocation) distributes a fixed token budget between full-image and sub-images based on visual content scores derived from CLS-attention in initial ViT layers. Phase 2 (Visual Token Dropping) selects the most informative tokens within each partition's allocated budget using feature importance scores from CLS-attention in final ViT layers. The method partitions high-resolution images dynamically, encodes them using ViT, and feeds the selected tokens to the LLM, achieving significant efficiency improvements while preserving accuracy.

## Key Results
- 4.7× increase in token generation throughput with HiRED-20% configuration
- 78% reduction in response latency on NVIDIA TESLA P40 GPU
- 14% GPU memory savings, with 30% reduction preventing out-of-memory errors at batch size 4
- Competitive accuracy preservation across VQAv2, ScienceQA, VQAT, DocVQA, OCRBench, MME, POPE, and ChartQA benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual tokens receive significantly less attention than system and text tokens during LLM generation, indicating sparsity.
- Mechanism: Only a small subset of visual tokens drives most of the attention in the LLM, enabling selective dropping without accuracy loss.
- Core assumption: Attention patterns in the LLM reliably indicate token importance for the downstream task.
- Evidence anchors:
  - [abstract] "visual tokens amount to 80-90% of all the tokens, they receive significantly less attention than system and text tokens."
  - [section] "The results reveal that a small subset of visual tokens brings most of the context from the image to the LLM."
  - [corpus] Weak - related works focus on token pruning but don't provide direct evidence for this sparsity claim.
- Break condition: If the attention distribution shifts dramatically for different tasks or model scales, the sparsity assumption may fail.

### Mechanism 2
- Claim: CLS-attention in early ViT layers correlates with visual content, allowing budget allocation based on image partition importance.
- Mechanism: Use CLS-attention scores from initial ViT layers to compute a visual content score for each image partition, then allocate token budgets proportionally.
- Core assumption: Higher CLS-attention in early layers directly indicates more visual content in that region.
- Evidence anchors:
  - [section] "CLS-attention maps in initial layers reveal the main content of the input image...higher attention corresponds to regions with more visual content."
  - [abstract] "we introduce the visual content score (which represents the amount of visual content a partition carries) as the token budget for each sub-image."
  - [corpus] Assumption: Related works use attention for pruning but don't explicitly validate this content-correlation claim.
- Break condition: If image content is uniformly distributed or attention patterns become decorrelated from content in deeper models.

### Mechanism 3
- Claim: CLS-attention in final ViT layers indicates token informativeness, enabling effective token selection within allocated budgets.
- Mechanism: Use CLS-attention scores from final ViT layers as feature importance scores to select top-k tokens within each partition's budget.
- Core assumption: Final layer CLS-attention highlights informative patches where ViT stores most image features.
- Evidence anchors:
  - [section] "attention maps in final layers indicate the informative areas...prioritize patches that are more informative than the others."
  - [abstract] "we use the CLS-attention (aggregated across multiple heads) from the final layer as feature importance score."
  - [corpus] Weak - similar approaches exist but don't provide direct evidence for this specific informativeness claim.
- Break condition: If feature importance shifts to earlier layers in different ViT architectures or if attention becomes too uniform in final layers.

## Foundational Learning

- Concept: Vision Transformer (ViT) attention mechanisms
  - Why needed here: Understanding how CLS-attention patterns work across layers is fundamental to HiRED's token selection and budget allocation.
  - Quick check question: What's the difference between CLS-attention in initial vs. final ViT layers, and why does this matter for token importance?

- Concept: Token sparsity in multimodal models
  - Why needed here: The core efficiency gain relies on recognizing that most visual tokens are unimportant for LLM generation.
  - Quick check question: How does the attention distribution between visual, text, and system tokens inform which visual tokens can be safely dropped?

- Concept: Budget allocation algorithms
  - Why needed here: HiRED needs to distribute limited tokens across image partitions based on content importance.
  - Quick check question: How does proportional allocation based on visual content scores differ from uniform allocation, and what are the trade-offs?

## Architecture Onboarding

- Component map: Vision Transformer (ViT) for image encoding -> CLS-attention pattern analysis module -> Token budget allocation calculator -> Token selection module (feature importance scoring) -> Integration layer for feeding tokens to LLM -> Memory and latency monitoring

- Critical path:
  1. Image partitioning and ViT encoding
  2. CLS-attention extraction from initial and final layers
  3. Visual content score computation and budget allocation
  4. Feature importance score calculation
  5. Token selection within budgets
  6. Token concatenation and LLM feeding

- Design tradeoffs:
  - Layer selection (initial vs. final) impacts both content detection accuracy and computational overhead
  - Head aggregation strategy (sum vs. no aggregation) affects token selection quality vs. complexity
  - Budget allocation ratio α between full-image and sub-images balances global vs. local context preservation

- Failure signatures:
  - Accuracy degradation indicates poor token selection or insufficient budget allocation
  - Out-of-memory errors suggest budget allocation is too aggressive or ViT layer selection is computationally heavy
  - Increased latency points to inefficient attention extraction or token selection processes

- First 3 experiments:
  1. Verify CLS-attention patterns correlate with visual content by visualizing attention maps on sample images and checking against human annotations
  2. Test different budget allocation ratios (α values) on a small subset of tasks to find optimal balance between accuracy and efficiency
  3. Compare token selection performance using different ViT layers and head aggregation strategies on a validation set to identify best configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of budget allocation ratio α impact the accuracy of HiRED across different tasks, and what is the optimal value of α?
- Basis in paper: [explicit] The paper discusses the impact of α on accuracy in Section 5.3, stating that a balanced budget distribution (i.e., α = 0.5) generally yields the highest accuracy.
- Why unresolved: While the paper provides evidence for α = 0.5, it does not explore the full range of possible α values or provide a rigorous justification for this specific value. The optimal α might vary depending on the specific task or dataset.
- What evidence would resolve it: Further experiments testing a wider range of α values and analyzing their impact on accuracy across different tasks and datasets would help determine the optimal value of α.

### Open Question 2
- Question: How does HiRED perform on tasks that require strong spatial understanding, such as ChartQA, compared to its performance on other tasks?
- Basis in paper: [explicit] The paper mentions that a limitation of HiRED is the potential loss of spatial information, which may impact tasks where spatial relationships are crucial, such as ChartQA.
- Why unresolved: The paper does not provide specific experimental results comparing HiRED's performance on ChartQA to its performance on other tasks. It only mentions this limitation without quantifying its impact.
- What evidence would resolve it: Conducting experiments specifically evaluating HiRED's performance on ChartQA and comparing it to its performance on other tasks would quantify the impact of spatial information loss on accuracy.

### Open Question 3
- Question: Can HiRED be extended to handle dynamic changes in token budgets during inference, allowing for more adaptive resource allocation?
- Basis in paper: [inferred] The paper discusses the fixed token budget approach of HiRED, but it does not explore the possibility of dynamic budget allocation during inference.
- Why unresolved: The paper focuses on a static token budget allocation strategy, which might not be optimal for all scenarios. Dynamic allocation could potentially improve efficiency and accuracy in certain cases.
- What evidence would resolve it: Developing and evaluating a dynamic token budget allocation strategy for HiRED, comparing its performance to the static approach, would demonstrate the feasibility and benefits of such an extension.

## Limitations

- The sparsity assumption (that visual tokens receive significantly less attention than system and text tokens) is observed but not rigorously validated across different model scales, tasks, or architectures.
- The method's effectiveness is demonstrated primarily on LLaVA-Next-7B with NVIDIA TESLA P40; results may not generalize to other hardware configurations or larger model variants.
- While HiRED shows strong efficiency gains, the accuracy preservation across benchmarks is reported without detailed error analysis to understand which token drops cause failures.

## Confidence

- **High Confidence**: Efficiency metrics (throughput increase, latency reduction, memory savings) on NVIDIA TESLA P40 hardware—these are directly measurable and reproducible.
- **Medium Confidence**: Accuracy preservation across benchmarks—reported but lacks detailed analysis of failure cases and robustness testing.
- **Medium Confidence**: The two-phase mechanism (budget allocation via initial-layer CLS-attention, token selection via final-layer CLS-attention)—the framework is plausible but assumptions about attention patterns are not exhaustively validated.
- **Low Confidence**: Generalization to different hardware, model scales, and image resolutions—insufficient evidence provided beyond the specific experimental setup.

## Next Checks

1. **Attention Pattern Validation**: Conduct controlled experiments where tokens identified as low-importance by HiRED are selectively restored to measure their actual impact on accuracy. This would test whether the attention-based importance scoring is truly reliable or if some "unimportant" tokens are actually critical for certain tasks.

2. **Cross-Architecture Generalization**: Test HiRED on different Vision Transformer variants (DeiT, Swin, PVT) and LLM architectures to verify that the CLS-attention patterns remain consistent indicators of content and importance across architectures, or identify which architectural features break the assumptions.

3. **Extreme Resolution Testing**: Evaluate HiRED's performance on ultra-high-resolution images (e.g., 4K medical scans or satellite imagery) where the current partitioning strategy may fail to capture global context effectively, and test adaptive partitioning strategies that maintain efficiency while preserving accuracy.