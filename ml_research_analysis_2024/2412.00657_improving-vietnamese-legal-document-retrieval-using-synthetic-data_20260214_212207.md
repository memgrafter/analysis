---
ver: rpa2
title: Improving Vietnamese Legal Document Retrieval using Synthetic Data
arxiv_id: '2412.00657'
source_url: https://arxiv.org/abs/2412.00657
tags:
- retrieval
- legal
- queries
- synthetic
- vietnamese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the scarcity of large annotated datasets for
  Vietnamese legal text retrieval by generating high-quality synthetic queries using
  a large language model. The authors employ Llama 3 to create 500,000 synthetic query-passage
  pairs, which are used to pre-train and fine-tune bi-encoder and ColBERT retrieval
  models.
---

# Improving Vietnamese Legal Document Retrieval using Synthetic Data

## Quick Facts
- arXiv ID: 2412.00657
- Source URL: https://arxiv.org/abs/2412.00657
- Authors: Son Pham Tien; Hieu Nguyen Doan; An Nguyen Dai; Sang Dinh Viet
- Reference count: 34
- One-line primary result: Synthetic data generation with Llama 3 and pre-training improves Vietnamese legal document retrieval accuracy

## Executive Summary
This paper addresses the scarcity of annotated data for Vietnamese legal text retrieval by generating high-quality synthetic queries using Llama 3 70B. The authors employ aspect-guided prompting to create diverse query-passage pairs, which are used to pre-train and fine-tune bi-encoder and ColBERT retrieval models. Through contrastive loss with mined hard negatives, the models achieve state-of-the-art performance on Vietnamese legal benchmarks, demonstrating the effectiveness of synthetic data in low-resource scenarios.

## Method Summary
The authors scrape 143,261 legal passages from thuvienphapluat.vn and generate 500,000 synthetic query-passage pairs using Llama 3 70B with aspect-guided prompting. The synthetic data is filtered using BGE-M3 and used for pre-training with CoT-MAE loss. The retrieval models (bi-encoder and ColBERT) are then fine-tuned on MS-MARCO, SQuAD 2.0, Legal Zalo 21, TVPL, and synthetic data using contrastive loss with hard negatives mined by BGE-M3. The approach is evaluated on TVPL and Legal Zalo 21 benchmarks, with additional out-of-domain testing on Vietnamese Wiki QA.

## Key Results
- ColBERT model achieves MRR@10 of 74.61% on TVPL benchmark and 84.08% on Legal Zalo 21
- Out-of-domain evaluation shows ColBERT achieves MRR@10 of 72.38% on Vietnamese Wiki QA dataset
- 1-bit compression achieves competitive results with minimal storage overhead
- Synthetic data and pre-training techniques significantly improve retrieval accuracy over baseline models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aspect-guided prompting with Llama 3 70B generates diverse, semantically rich queries
- Mechanism: Identifies 1-5 distinct aspects within passages and generates queries for each aspect
- Core assumption: Legal passages contain multiple distinct aspects that can be coherently identified
- Evidence anchors: Abstract mentions "high-quality, diverse synthetic queries"; section 3.3 describes aspect-guided prompting approach
- Break condition: If passages are too short or homogeneous, aspect identification may produce redundant aspects

### Mechanism 2
- Claim: CoT-MAE pre-training aligns query and passage representations more effectively than standard pre-training
- Mechanism: Dual reconstruction forces encoder to integrate query context when encoding passages
- Core assumption: Query-passage pairs share enough contextual information for meaningful reconstruction
- Evidence anchors: Abstract mentions pre-training; section 3.4 describes CoT-MAE reconstruction process
- Break condition: Poor quality synthetic queries could introduce noise rather than useful alignment

### Mechanism 3
- Claim: Hard negatives mined by BGE-M3 improve retrieval accuracy by sharpening distinction between similar passages
- Mechanism: Forces model to distinguish between semantically similar but irrelevant passages
- Core assumption: BGE-M3 can reliably identify challenging hard negatives
- Evidence anchors: Section 3.5 mentions hard negative mining; abstract discusses contrastive loss with mined hard negatives
- Break condition: If hard negatives are too easy or difficult, contrastive loss may not improve ranking precision

## Foundational Learning

- Concept: Dense vs sparse retrieval
  - Why needed here: The paper contrasts BM25 (sparse) with neural models like bi-encoder and ColBERT (dense)
  - Quick check question: What is the main difference in how BM25 and dense retrievers represent queries and documents for similarity matching?

- Concept: Contrastive loss and InfoNCE
  - Why needed here: Fine-tuning uses InfoNCE to pull positive pairs together while pushing negatives apart
  - Quick check question: In InfoNCE loss, what role does the temperature parameter τ play in shaping the similarity distribution?

- Concept: Multi-vector retrieval (ColBERT)
  - Why needed here: ColBERT uses multiple vectors per query/passage for fine-grained late interaction
  - Quick check question: How does ColBERT's late interaction differ from the single-vector similarity computation in a bi-encoder?

## Architecture Onboarding

- Component map: Legal corpus → Passage chunking → Llama 3 query generation → Synthetic dataset → CoT-MAE pre-training → Fine-tuning (hard negatives) → Retrieval evaluation
- Critical path: Passage → Synthetic query → Pre-training (CoT-MAE) → Fine-tuning (hard negatives) → Retrieval performance
- Design tradeoffs:
  - Synthetic data avoids costly annotation but depends on LLM quality and diversity
  - ColBERT offers higher accuracy at increased storage/computation cost vs bi-encoder
  - Compression (1-8 bits) trades storage savings against slight accuracy loss
- Failure signatures:
  - Low passage hit rate → synthetic queries are too generic or mismatched
  - Overfitting to synthetic data → poor generalization on real legal queries
  - Large storage spike → insufficient compression or excessive vector dimensionality
- First 3 experiments:
  1. Generate 100 synthetic queries with basic vs aspect-guided prompting; measure passage hit rate
  2. Pre-train small encoder with CoT-MAE vs standard MLM; compare retrieval accuracy
  3. Fine-tune bi-encoder with and without hard negatives; evaluate impact on MRR@10

## Open Questions the Paper Calls Out
- Question: How does the quality of synthetic queries compare to human-generated queries in terms of retrieval performance?
- Question: What is the impact of different pre-training techniques on retrieval models trained on synthetic data?
- Question: How does performance generalize to other domains beyond legal texts?
- Question: What are the potential risks of performance collapse when models are primarily trained on synthetic data?

## Limitations
- Exact aspect-guided prompt template is not provided, making reproducibility uncertain
- Ablation studies on hard negative impact and pre-training are limited
- Out-of-domain evaluation uses a different domain (general QA vs legal), which may not fully capture generalization capabilities

## Confidence
- **High Confidence**: Synthetic data generation with Llama 3 70B and contrastive loss with hard negatives
- **Medium Confidence**: CoT-MAE pre-training significantly improves retrieval accuracy
- **Low Confidence**: Aspect-guided prompting consistently produces diverse and relevant queries

## Next Checks
1. Replicate synthetic query generation using exact aspect-guided prompt template and compare diversity against baseline
2. Train bi-encoder with standard MLM pre-training and compare accuracy to CoT-MAE pre-trained model
3. Evaluate retrieval performance with and without hard negatives on held-out validation set to quantify impact