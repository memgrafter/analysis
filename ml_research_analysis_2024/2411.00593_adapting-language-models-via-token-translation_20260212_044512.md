---
ver: rpa2
title: Adapting Language Models via Token Translation
arxiv_id: '2411.00593'
source_url: https://arxiv.org/abs/2411.00593
tags:
- token
- tokenizer
- s2t2
- language
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: S2T2 addresses poor compression and semantic misalignment when
  applying a fixed tokenizer from one domain to a new target domain. It learns a tailored
  tokenizer for the target domain and translates between target and source tokens
  via a sparse optimal transport layer, enabling effective reuse of the pre-trained
  next-token predictor.
---

# Adapting Language Models via Token Translation

## Quick Facts
- arXiv ID: 2411.00593
- Source URL: https://arxiv.org/abs/2411.00593
- Reference count: 3
- Primary result: S2T2 improves perplexity (from ~151 to ~131) and compression (BpB from ~7.2 to ~3.8) on protein sequence adaptation

## Executive Summary
S2T2 (Sinkhorn Token Translation) addresses the challenge of adapting pre-trained language models to new domains where fixed tokenizers from the source domain perform poorly. The method learns a tailored tokenizer for the target domain and translates between source and target tokens via a sparse optimal transport layer. This enables effective reuse of the pre-trained next-token predictor while improving both perplexity and compression metrics on protein sequence adaptation tasks.

## Method Summary
S2T2 learns a sparse joint probability matrix P between source and target tokens using Sinkhorn iterations with SPARSEMAX projections. The method modifies a pre-trained model by translating embeddings and the language model head according to P, then finetunes on target data. This approach addresses poor compression and semantic misalignment when applying a fixed tokenizer from one domain to a new target domain, particularly effective for protein sequence modeling where standard tokenizers struggle.

## Key Results
- Improves perplexity from ~151 to ~131 on protein sequences
- Reduces bits-per-byte from ~7.2 to ~3.8 on UniRef50 dataset
- Enables transferring token translations from smaller to larger models at lower computational cost

## Why This Works (Mechanism)
The core insight is that domain-specific tokenization often outperforms fixed tokenizers when adapting to new domains. S2T2 bridges the gap by learning an optimal transport-based translation between source and target token spaces, preserving semantic alignment while enabling the reuse of pre-trained model parameters. The sparse optimal transport layer ensures computational efficiency while maintaining translation quality.

## Foundational Learning
1. **Optimal Transport Theory**: Why needed - Forms the mathematical foundation for token translation; Quick check - Verify understanding of Sinkhorn-Knopp algorithm and entropy regularization
2. **SPARSEMAX Operator**: Why needed - Enables sparse probability distributions in token translation; Quick check - Understand difference between softmax and sparsemax
3. **Byte-Pair Encoding**: Why needed - Standard tokenization method for protein sequences; Quick check - Know how BPE merges frequent symbol pairs
4. **Cross-Domain Adaptation**: Why needed - Framework for transferring knowledge between domains; Quick check - Understand domain shift and its implications
5. **Language Model Pretraining**: Why needed - Basis for model reuse in adaptation; Quick check - Know typical pretraining objectives and architectures
6. **Bits-Per-Byte Metric**: Why needed - Measures compression efficiency in sequence modeling; Quick check - Understand relationship between perplexity and compression

## Architecture Onboarding

**Component Map**: Source Tokenizer -> Optimal Transport Layer -> Target Tokenizer -> Pre-trained Model -> Finetuned Model

**Critical Path**: Tokenization → Optimal Transport Matrix Learning → Model Modification → Finetuning

**Design Tradeoffs**: 
- Sparsity vs accuracy in token translation (controlled by entropy regularizer α)
- Computational cost of Sinkhorn iterations vs translation quality
- Vocabulary size of target tokenizer vs model capacity

**Failure Signatures**: 
- High perplexity indicates poor token translation alignment
- Slow Sinkhorn convergence suggests inappropriate regularization parameters
- Compression degradation indicates tokenizer-tokenization mismatch

**First Experiments**:
1. Train BPE tokenizer on UniRef50 with vocabulary size 512
2. Implement 3-step Sinkhorn iteration with SPARSEMAX projections
3. Verify token translation matrix sparsity and semantic coherence

## Open Questions the Paper Calls Out
1. Can S2T2 be extended to handle multimodal data beyond text and proteins, such as images, audio, or video?
2. How does the entropy regularization parameter α affect the sparsity of the translation matrix P and downstream performance metrics?
3. Can the learned token translations be transferred between different model architectures (e.g., from OLMo to GPT)?

## Limitations
- Only validated on protein sequence dataset (UniRef50), limiting generalization claims
- Performance on non-biological domains (code, natural language) untested
- Hyperparameter sensitivity around entropy regularization parameter α
- Requires both source tokenizer and pre-trained model availability

## Confidence
- **High confidence**: S2T2 can be implemented and trained as described with verifiable improvements on protein sequences
- **Medium confidence**: Method's gains will transfer to other biologically relevant sequence domains
- **Low confidence**: S2T2 will yield similar improvements in natural language or code domains without domain-specific modifications

## Next Checks
1. Test S2T2 on diverse biological sequence datasets (DNA, RNA, enzyme families) to assess robustness
2. Evaluate S2T2 on non-biological domains (source code, non-Latin text) for cross-domain applicability
3. Perform ablation studies on optimal transport hyperparameters (α, iteration count) to identify robust defaults