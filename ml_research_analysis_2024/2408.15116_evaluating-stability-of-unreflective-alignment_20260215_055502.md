---
ver: rpa2
title: Evaluating Stability of Unreflective Alignment
arxiv_id: '2408.15116'
source_url: https://arxiv.org/abs/2408.15116
tags:
- llms
- which
- back
- planning
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether alignment problems stemming from
  reflective stability will arise in future LLMs before they can automate AI alignment
  research. The authors propose a specific threat model called Counterfactual Priority
  Change (CPC) destabilization, which could cause alignment failures if two conditions
  are met: 1) the LLM exhibits CPC-based stepping back behavior, and 2) the LLM has
  preference instability.'
---

# Evaluating Stability of Unreflective Alignment

## Quick Facts
- arXiv ID: 2408.15116
- Source URL: https://arxiv.org/abs/2408.15116
- Authors: James Lucassen; Mark Henry; Philippa Wright; Owen Yeung
- Reference count: 2
- Key outcome: The study finds that GPT-4 exhibits more CPC-based stepping back behavior and preference instability compared to GPT-3.5-turbo, suggesting that risk factors for reflective stability problems increase with LLM capabilities.

## Executive Summary
This paper investigates whether alignment problems stemming from reflective stability will arise in future LLMs before they can automate AI alignment research. The authors propose a specific threat model called Counterfactual Priority Change (CPC) destabilization, which could cause alignment failures if LLMs exhibit CPC-based stepping back behavior combined with preference instability. Through three experiments measuring CPC-based stepping back, dynamic planning capabilities, and preference stability, the study finds that GPT-4 shows more of these risk factors than GPT-3.5-turbo, suggesting that highly capable future LLMs may be more difficult to align using methods that don't account for reflective stability.

## Method Summary
The study evaluates three risk factors for CPC-destabilization in LLMs through controlled experiments. First, they measure CPC-based stepping back by observing how closely LLMs follow the CPC criterion when switching strategies during problem-solving tasks using quadratic equations. Second, they assess dynamic planning capabilities using a multi-armed bandit problem with different algorithmic instructions (random, intuitive, greedy, UCB) and prompt settings (one token vs chain of thought). Third, they measure preference stability by examining how often LLMs have cyclic preferences that admit money pumps using Dominion card game scenarios. The experiments are conducted across GPT-3.5-turbo, GPT-4, GPT-4-turbo, and GPT-4o models.

## Key Results
- GPT-4 exhibits significantly more CPC-based stepping back behavior compared to GPT-3.5-turbo, with agreement between actual switches and CPC predictions increasing from 44% to 63%
- GPT-4 outperforms GPT-3.5-turbo on multi-armed bandit tasks, achieving up to 96% accuracy compared to 75% with one-token prompting
- GPT-4 shows fewer cyclic preferences than GPT-3.5-turbo and is more likely to remove remaining cycles when prompted (66% vs 50% removal rate)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Counterfactual Priority Change (CPC) criterion provides a generalizable optimality criterion for when LLMs should step back during dynamic planning
- Mechanism: The CPC criterion formalizes stepping back behavior by asking whether an earlier point in the planning stack would lead to different object-level priorities if the stack were regenerated from that point with current information
- Core assumption: LLMs can be evaluated on their adherence to the CPC criterion by measuring whether they switch strategies at points where the criterion indicates they should
- Evidence anchors:
  - [abstract] "The authors propose a specific threat model called Counterfactual Priority Change (CPC) destabilization"
  - [section] "The CPC Criterion: Is there an item on the agent's current planning stack that, if the planning stack were regenerated from that point with all currently available information, would lead to a different object-level priority?"
  - [corpus] Weak evidence - corpus neighbors focus on LLM evaluation but don't directly address CPC-specific mechanisms
- Break condition: If LLMs don't follow the CPC criterion even approximately, or if the stepping back behavior doesn't correlate with the criterion being true

### Mechanism 2
- Claim: Preference instability in LLMs can be measured through cyclic preferences that admit money pumps
- Mechanism: The experiment creates pairwise preference matrices where cycles indicate preference instability, then tests whether LLMs recognize and want to remove these cycles
- Core assumption: Cyclic preferences are a measurable indicator of reflective preference instability that correlates with alignment risks
- Evidence anchors:
  - [abstract] "they assess dynamic planning capabilities using a multi-armed bandit problem" and "they measure preference stability by examining how often LLMs have cyclic preferences that admit money pumps"
  - [section] "We choose to focus on cyclic preferences as a particular case that may be unstable, as they admit the construction of very simple money-pumps"
  - [corpus] Moderate evidence - corpus includes papers on LLM alignment and preference stability evaluation
- Break condition: If cyclic preferences don't correlate with other measures of preference instability, or if LLMs don't recognize cycles as problematic

### Mechanism 3
- Claim: LLM capabilities scale correlates with increased risk factors for CPC-destabilization
- Mechanism: The experiments show that GPT-4 exhibits more CPC-based stepping back behavior and preference instability compared to GPT-3.5-turbo, suggesting scaling increases these risk factors
- Core assumption: The observed differences between GPT-3.5-turbo and GPT-4 generalize to future LLM scaling trends
- Evidence anchors:
  - [abstract] "The results show that GPT-4 exhibits more CPC-based stepping back behavior compared to GPT-3.5-turbo"
  - [section] "Our findings indicate that in current LLMs, increased scale and capability are associated with increases in both CPC-based stepping back and preference instability"
  - [corpus] Weak evidence - corpus doesn't provide comparative scaling studies between different LLM versions
- Break condition: If future LLM scaling doesn't follow the observed trend, or if the relationship is non-linear

## Foundational Learning

- Concept: Reflective stability in AI systems
  - Why needed here: The paper investigates whether alignment problems stemming from reflective stability will arise in future LLMs
  - Quick check question: What does it mean for an AI system's alignment to be reflectively stable?

- Concept: Dynamic planning and stepping back behavior
  - Why needed here: The CPC-destabilization threat model depends on understanding how LLMs adapt plans and when they abandon strategies
  - Quick check question: How does the CPC criterion define when an LLM should step back from its current approach?

- Concept: Preference cycles and money pumps
  - Why needed here: The preference stability experiment uses cyclic preferences as a measure of reflective preference instability
  - Quick check question: Why do cyclic preferences in decision-making admit the construction of money pumps?

## Architecture Onboarding

- Component map: Problem generation -> LLM prompting -> Response extraction -> Analysis pipeline
- Critical path: For CPC curves: generate problem dataset → prompt LLM to solve problems → detect strategy switches → interrupt at intervals to ask CPC questions → extract decisions → plot CPC curve
- Design tradeoffs: Using quadratic equation solving for CPC evaluation provides controlled strategy switching but may not generalize to other domains; using card game preferences for stability testing provides clear preference structures but may not capture all aspects of preference instability
- Failure signatures: If CPC curves don't show clear patterns even with optimal prompting, if multi-armed bandit performance plateaus regardless of algorithm choice, or if preference cycle detection methods fail to identify meaningful cycles
- First 3 experiments:
  1. Validate the switching detection method on synthetic data before applying to real problem-solving transcripts
  2. Test different CPC prompts to find which elicits the most consistent responses across different LLM sizes
  3. Run the multi-armed bandit experiment with varying difficulty levels to establish baseline performance curves

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the trend of increasing CPC-based stepping back behavior with model capability continue beyond GPT-4?
- Basis in paper: [explicit] The paper states "Our findings indicate that in current LLMs, increased scale and capability are associated with increases in both CPC-based stepping back and preference instability"
- Why unresolved: The study only compared GPT-3.5-turbo and GPT-4, leaving open whether the trend continues with more advanced models
- What evidence would resolve it: Testing more recent and larger models (GPT-4-turbo, GPT-4o, GPT-5) using the same CPC curve methodology would show if the trend continues

### Open Question 2
- Question: What is the relationship between preference stability and corrigibility in advanced LLMs?
- Basis in paper: [inferred] The paper notes GPT-4 has fewer cycles and stronger tendency to remove remaining inconsistencies than GPT-3.5-turbo, but states "It is not clear how to interpret this"
- Why unresolved: The paper observes a difference in preference stability between models but doesn't establish what this means for alignment safety
- What evidence would resolve it: Testing whether models with more stable preferences also show stronger alignment with human instructions and willingness to be shut down

### Open Question 3
- Question: Can LLMs develop robust intuitions for complex decision-making tasks without explicit algorithmic guidance?
- Basis in paper: [explicit] The paper found "GPT-4 and GPT-4-turbo achieved high accuracy using intuitive prompts in the one token setting, surpassing their own scores with chain of thought"
- Why unresolved: While the paper observed this capability, it doesn't explain whether this represents genuine understanding or superficial pattern matching
- What evidence would resolve it: Testing the same models on novel variants of the Multi-Armed Bandit problem to see if they can generalize their "intuitions" to new scenarios

## Limitations

- The study relies on controlled experimental settings that may not fully capture real-world LLM behavior, particularly the quadratic equation problems used for CPC evaluation
- The prompts for CPC questioning were chosen "arbitrarily" from several candidates without systematic optimization, which could affect the robustness of results
- The relationship between preference cycles and actual alignment failures remains theoretical rather than empirically demonstrated

## Confidence

- Medium confidence: GPT-4 exhibits more CPC-based stepping back behavior than GPT-3.5-turbo
- Medium confidence: Preference stability improves from GPT-3.5-turbo to GPT-4
- Low confidence: These risk factors will necessarily scale with future LLM capabilities

## Next Checks

1. Test whether the observed CPC stepping back behavior persists across diverse problem domains beyond quadratic equations, particularly in open-ended reasoning tasks.

2. Conduct ablation studies on prompt variations to determine how sensitive the CPC and preference stability measurements are to prompt wording and structure.

3. Implement longitudinal testing by running the same experiments on newer LLM versions as they become available to verify whether the observed trends continue.