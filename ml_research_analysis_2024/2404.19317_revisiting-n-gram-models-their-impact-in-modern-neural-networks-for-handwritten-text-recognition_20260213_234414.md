---
ver: rpa2
title: 'Revisiting N-Gram Models: Their Impact in Modern Neural Networks for Handwritten
  Text Recognition'
arxiv_id: '2404.19317'
source_url: https://arxiv.org/abs/2404.19317
tags:
- language
- recognition
- n-gram
- text
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether explicit n-gram language models
  can still improve the performance of modern neural networks in handwriting recognition.
  While recent transformer-based architectures have demonstrated strong implicit language
  modeling capabilities, their efficiency in low-resource languages and historical
  contexts is limited.
---

# Revisiting N-Gram Models: Their Impact in Modern Neural Networks for Handwritten Text Recognition

## Quick Facts
- arXiv ID: 2404.19317
- Source URL: https://arxiv.org/abs/2404.19317
- Reference count: 40
- Primary result: Character-level n-gram models significantly improve HTR performance on low-resource and historical datasets

## Executive Summary
This paper investigates whether explicit n-gram language models can enhance modern neural networks for handwritten text recognition (HTR), particularly for low-resource languages and historical contexts. While transformer-based architectures have demonstrated strong implicit language modeling capabilities, their performance degrades on datasets with limited training data and high out-of-vocabulary rates. The study explores integrating n-gram models with two state-of-the-art architectures (PyLaia and DAN) across three datasets: IAM, RIMES, and NorHand v2. Results demonstrate that character-based n-gram language models consistently outperform subword and word-level models, with the hybrid DAN + character LM combination achieving new benchmarks across all datasets.

## Method Summary
The study evaluates two architectures: PyLaia (CTC-based CNN+RNN) and DAN (CNN+Transformer with sequence-to-sequence learning). Both models are trained with and without n-gram language models at character, subword (using SentencePiece with vocab size 1000), and word levels. N-gram models (orders 2-6) are built using KenLM with Kneser-Ney smoothing. Integration occurs during beam search decoding, where neural network scores are combined with language model probabilities using optimized weights (1.5 for character/subword, 0.5 for word). Experiments run on IAM (English), RIMES (French), and NorHand v2 (Norwegian historical letters) at both line and paragraph/page levels.

## Key Results
- Character n-gram models achieve 13% relative WER reduction on IAM compared to models without language models
- DAN + character language model outperforms current benchmarks on IAM, RIMES, and NorHand v2 at paragraph and page levels
- Character-level LMs outperform subword and word-level LMs by up to 50% relative improvement on IAM dataset
- Page-level training improves DAN performance on IAM and RIMES compared to line-level training

## Why This Works (Mechanism)

### Mechanism 1
Character-level n-gram language models improve accuracy more than subword or word-level models in handwritten text recognition. Character n-grams capture fine-grained contextual dependencies that are robust to out-of-vocabulary words and spelling variations common in historical handwriting. Core assumption: Low-resource and historical datasets have high OOV rates, making character-level models more effective than higher-level tokenization. Evidence: Character LM shows 13% relative WER reduction on IAM; character-based LMs significantly improve performance on all datasets.

### Mechanism 2
Hybrid architectures combining explicit n-gram models with modern neural networks outperform either approach alone. Neural networks implicitly capture language patterns but may fail on ambiguous or rare sequences; explicit n-grams provide probabilistic corrections during decoding. Core assumption: Implicit language modeling in transformers is insufficient for low-resource historical data without explicit statistical support. Evidence: DAN with character LM outperforms current benchmarks; incorporating LMs significantly improves ATR performance on all datasets.

### Mechanism 3
Training on paragraphs/pages rather than lines improves recognition accuracy for transformer-based models. Larger context windows allow transformers to capture longer-range dependencies and better understand document structure. Core assumption: Transformers benefit from multi-line context for resolving ambiguities that require cross-line understanding. Evidence: DAN performs generally better when trained on pages rather than lines for IAM and RIMES.

## Foundational Learning

- **Connectionist Temporal Classification (CTC) loss function**: PyLaia uses CTC-based decoding which requires understanding how CTC aligns sequences during training and inference. Quick check: How does CTC handle variable-length input sequences compared to cross-entropy loss?

- **Beam search decoding with language model integration**: The study combines neural network outputs with n-gram probabilities using beam search, requiring understanding of how scores are combined. Quick check: What is the mathematical relationship between the neural network score, language model score, and the final combined score in beam search?

- **Tokenization granularity trade-offs (character vs subword vs word)**: The paper explores different tokenization levels for language models, requiring understanding of when each level is appropriate. Quick check: Why might character-level tokenization be preferred over word-level for historical handwriting datasets?

## Architecture Onboarding

- **Component map**: PyLaia (CNN + RNN + CTC) → KenLM n-gram model → Beam search decoder; DAN (CNN + Transformer + CE loss) → KenLM n-gram model → Modified CTC beam search
- **Critical path**: Image → Feature extraction → Sequence prediction → Language model rescoring → Final transcription
- **Design tradeoffs**: Memory vs accuracy (page-level training requires smaller batch sizes), speed vs accuracy (language models slow inference by ~10x for PyLaia), tokenization granularity vs OOV handling
- **Failure signatures**: Hallucinations in DAN on complex datasets, OOV issues with word-level models, GPU memory exhaustion during page-level training
- **First 3 experiments**:
  1. Baseline: Run PyLaia and DAN without language models on IAM to establish performance baseline
  2. Character LM integration: Add character-level KenLM model with weight 1.5 to both architectures on IAM
  3. Tokenization comparison: Test subword and word-level language models on same datasets to verify character-level superiority

## Open Questions the Paper Calls Out

- What is the optimal vocabulary size for subword tokenization in handwriting recognition tasks? The paper mentions that a vocabulary size of 1000 subwords was chosen as a compromise, but notes that further experiments could be performed to determine the optimal value.

- How does combining multiple tokenization levels (character, subword, and word) in a single language model affect handwriting recognition performance? The paper mentions this as a future research direction: "For future works, we plan to explore how different granularities can be combined in language models."

- What is the impact of n-gram order beyond 6-grams in handwriting recognition systems? The paper states that SRILM and KenLM do not support higher orders at the moment, and performance consistently improved with order increases up to 6 for character and subword models.

## Limitations

- Performance gains may be dataset-specific as evaluation focuses on three relatively small datasets (IAM: 13K lines, RIMES: 12K lines, NorHand: 18K lines)
- Computational overhead of integrating language models during inference is substantial but not fully quantified
- Analysis doesn't fully explain why character-level models outperform subword models by such large margins (up to 50% relative improvement)

## Confidence

- **High confidence**: Character n-gram language models improve recognition accuracy across all tested datasets and architectures
- **Medium confidence**: Character-level n-grams are superior to subword and word-level n-grams for historical and low-resource handwriting recognition
- **Low confidence**: The specific optimal hyperparameters (n-gram order 4, weight 1.5 for characters) generalize beyond the tested datasets

## Next Checks

1. **Cross-dataset hyperparameter validation**: Test whether the optimal n-gram order (4) and weight (1.5 for characters) that work on IAM and RIMES also perform optimally on additional datasets like Bentham or George Washington manuscripts.

2. **Computational overhead quantification**: Measure and report exact inference time increases when adding language models to PyLaia and DAN, providing concrete metrics (e.g., seconds per line, real-time factor).

3. **Language model ablation on larger datasets**: Evaluate the n-gram integration approach on larger modern datasets like CVL (500K+ lines) or IAM's full dataset to determine if observed benefits scale or diminish with increased training data volume.