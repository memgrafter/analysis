---
ver: rpa2
title: Evaluating Cost-Accuracy Trade-offs in Multimodal Search Relevance Judgements
arxiv_id: '2410.19974'
source_url: https://arxiv.org/abs/2410.19974
tags:
- relevance
- query
- search
- result
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the performance of several Large Language
  Models (LLMs) and Multimodal Language Models (MLLMs) as search relevance judges
  across three multimodal search domains. The authors compare model accuracy with
  human annotations and assess cost-accuracy trade-offs.
---

# Evaluating Cost-Accuracy Trade-offs in Multimodal Search Relevance Judgements

## Quick Facts
- **arXiv ID**: 2410.19974
- **Source URL**: https://arxiv.org/abs/2410.19974
- **Reference count**: 9
- **Primary result**: No single LLM consistently outperforms others across all domains; GPT-4o offers best balance of cost and accuracy, while prompt engineering and multimodal support show model-dependent effects.

## Executive Summary
This paper evaluates the performance of several Large Language Models (LLMs) and Multimodal Language Models (MLLMs) as search relevance judges across three multimodal search domains: Fashion, Hotel Supplies, and Design. The authors systematically compare model accuracy with human annotations while assessing cost-accuracy trade-offs. Their analysis reveals that model performance is highly use-case dependent, with no single model consistently excelling across all domains. The study finds that while larger models like GPT-4o benefit significantly from multimodal support, smaller models like Haiku may experience performance degradation when visual components are included. The research highlights the importance of prompt engineering and model-specific optimization, demonstrating that selecting an optimal model requires careful balancing of accuracy, cost, and domain-specific considerations.

## Method Summary
The study evaluates LLM and MLLM performance using three proprietary datasets (Fashion from Kaggle, Hotel Supplies, and Design) containing query-result pairs with text and images. A baseline retrieval system using BM25 with BGE M3 embeddings generates the search results. Human annotators grade relevance on a 0-2 scale, and multiple models (GPT-4V, GPT-4o, GPT-4o-mini, Claude 3.5 Sonnet, Claude 3 Haiku) are evaluated using carefully crafted prompts. The authors compare model judgments to human annotations using Cohen's kappa coefficient and analyze cost per million tokens. They conduct prompt engineering experiments to optimize model performance and perform error analysis on model disagreements, comparing multimodal versus text-only performance across use cases.

## Key Results
- GPT-4o achieves the best average performance across domains but at higher cost compared to smaller models
- Smaller models like Haiku benefit from prompt engineering but may show decreased performance with multimodal inputs
- Model performance varies significantly by domain, with no single model consistently outperforming others
- Multimodal support significantly benefits larger models but can hinder smaller models' performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM performance is use-case dependent.
- Mechanism: The model's ability to accurately judge relevance varies based on the specific characteristics of the dataset domain (e.g., Fashion vs. Hotel Supplies vs. Design), likely due to differences in data quality, visual importance, and feature complexity.
- Core assumption: Each domain presents unique challenges that affect the LLM's ability to interpret and integrate multimodal information.
- Evidence anchors:
  - [abstract] "Our analysis investigates the trade-offs between cost and accuracy, highlighting that model performance varies significantly depending on the context."
  - [section] "For example, GPT-4V shows higher performance in the Hotel Supplies use case but performs relatively worse in the other areas. We can observe a similar trend across the other models."
  - [corpus] Weak - no direct evidence in corpus neighbors about use-case dependency.

### Mechanism 2
- Claim: Multimodal support is not uniformly beneficial across model sizes.
- Mechanism: Larger models like GPT-4o and GPT-4V benefit from visual components because their architectures can effectively integrate and interpret visual information. Smaller models like Haiku may be overwhelmed by multimodal inputs, leading to decreased performance.
- Core assumption: Model architecture and capacity determine the ability to effectively process multimodal inputs.
- Evidence anchors:
  - [abstract] "Interestingly, in smaller models, the inclusion of a visual component may hinder performance rather than enhance it."
  - [section] "However, for smaller models, such as Haiku, the vision component appears to have a detrimental effect, decreasing the correlation from 0.433 (Text) to 0.368 (MM)."
  - [corpus] Weak - no direct evidence in corpus neighbors about multimodal support.

### Mechanism 3
- Claim: Prompt engineering significantly impacts performance and is model-specific.
- Mechanism: Tailoring prompts to specific datasets and models can improve grading performance. However, what works for one model may not work for another due to differences in model architecture and training.
- Core assumption: Different models respond differently to variations in prompt structure, complexity, and instructions.
- Evidence anchors:
  - [abstract] "prompt engineering significantly impacts performance, with model-specific optimization required."
  - [section] "We observe that tailoring a model's prompt to a specific domain can greatly improve its grading performance on the corresponding dataset. A notable example is the text-only Haiku LLM... However, we also note that using the Hotel Supplies dataset-adjusted prompt may lead to significant overfitting to other datasets."
  - [corpus] Weak - no direct evidence in corpus neighbors about prompt engineering.

## Foundational Learning

- **Multimodal Search Relevance**
  - Why needed here: The study focuses on evaluating how well LLMs and MLLMs can judge relevance in search results that include both text and images.
  - Quick check question: Can you explain the difference between unimodal and multimodal search relevance judgments?

- **Cohen's Kappa Coefficient**
  - Why needed here: Used to measure inter-annotator agreement between human judges and LLM-generated annotations.
  - Quick check question: What does a Cohen's kappa value of 0.7 indicate about the agreement between two annotators?

- **Cost-Accuracy Trade-off**
  - Why needed here: Central to the study's analysis of selecting the optimal model for practical applications.
  - Quick check question: How would you calculate the cost per relevance judgment if you know the model's pricing per million tokens and the average token usage per judgment?

## Architecture Onboarding

- **Component map**: Data Collection (retrieval system with BM25 and BGE M3 embeddings) -> Human Annotation (relevance grading by experts) -> Model Evaluation (LLMs and MLLMs generating relevance judgments)
- **Critical path**: The most critical path is the Model Evaluation stage, where the choice of model, prompt engineering, and multimodal support significantly impact the cost-accuracy trade-off.
- **Design tradeoffs**: The primary tradeoff is between model performance (accuracy) and cost. Larger, more capable models like GPT-4V offer higher accuracy but at a significantly higher cost compared to smaller models like Haiku.
- **Failure signatures**: Poor performance may manifest as low Cohen's kappa coefficients when comparing model judgments to human annotations. For smaller models, failure might be more pronounced in multimodal settings.
- **First 3 experiments**:
  1. Evaluate a single model (e.g., GPT-4o) across all three domains to confirm use-case dependency.
  2. Compare the performance of a model's multimodal version to its text-only version to assess the impact of visual components.
  3. Test different prompt variations for a smaller model like Haiku to identify optimal prompt engineering techniques.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do smaller MLLMs like Haiku achieve better performance through prompt engineering, and can this be systematized?
- Basis in paper: [explicit] The paper demonstrates that Haiku's performance improved significantly (Cohen's kappa from 0.36 to 0.40) through careful prompt engineering, and that smaller models are highly sensitive to prompt complexity.
- Why unresolved: The authors note that prompt engineering feels "more like art than science" and couldn't find a systematic way to optimize model accuracy across models. They achieved success with Haiku but couldn't replicate it with GPT-4V using the same prompt.
- What evidence would resolve it: Systematic testing of prompt variations across multiple model sizes and domains, with analysis of which prompt characteristics (length, specificity, structure) correlate with performance improvements for different model scales.

### Open Question 2
- Question: Under what specific conditions does multimodal support improve or harm performance for different model sizes?
- Basis in paper: [explicit] The paper shows that multimodal support significantly benefits larger models like GPT-4o and GPT-4V, but can be detrimental to smaller models like Haiku, with Haiku's performance dropping from 0.433 (text-only) to 0.368 (multimodal).
- Why unresolved: The authors conducted error analysis showing smaller models struggle with visual recognition tasks, but didn't systematically test what types of visual information or tasks benefit different model sizes.
- What evidence would resolve it: Controlled experiments varying image complexity, task types, and model sizes to identify the thresholds where multimodal input becomes beneficial or harmful.

### Open Question 3
- Question: What is the optimal cost-accuracy trade-off for domain-specific applications, and how does this vary across different use cases?
- Basis in paper: [explicit] The paper shows that GPT-4o offers the best current balance (50% cost reduction vs GPT-4V with similar performance), but also demonstrates that Haiku with optimized prompts can achieve good results at much lower cost.
- Why unresolved: The authors only tested three domains and didn't explore whether the cost-accuracy trade-offs vary significantly across different types of multimodal search tasks or whether certain models consistently perform better for specific domain characteristics.
- What evidence would resolve it: Testing across a broader range of domains with varying visual vs text importance, product complexity, and data quality to identify which models consistently offer the best value for specific domain characteristics.

## Limitations
- Proprietary datasets (Hotel Supplies and Design) limit reproducibility and require access to validate results
- Static evaluation framework may not capture how model performance and pricing evolve over time
- Focus on single retrieval baseline (BM25 + BGE M3) may not generalize to other search systems

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| GPT-4o achieves best average performance across domains | High |
| Smaller models show decreased performance with multimodal inputs | Medium |
| Optimal prompt engineering is model-specific | Low |

## Next Checks
1. Replicate the study with open-source datasets and models to verify the generalizability of the cost-accuracy findings
2. Conduct a longitudinal study to track how model performance and cost-effectiveness change over time as models are updated
3. Test the prompt engineering findings across a broader range of model architectures and sizes to validate the model-specific optimization hypothesis