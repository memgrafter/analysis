---
ver: rpa2
title: 'Learning by Surprise: Surplexity for Mitigating Model Collapse in Generative
  AI'
arxiv_id: '2410.12341'
source_url: https://arxiv.org/abs/2410.12341
tags:
- collapse
- generations
- generated
- which
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces surplexity as a measure to detect and mitigate
  model collapse in generative AI. The authors propose filtering training data by
  high surplexity to prevent models from training on "surprising" content, eliminating
  the need to distinguish between human- and AI-generated data.
---

# Learning by Surprise: Surplexity for Mitigating Model Collapse in Generative AI

## Quick Facts
- arXiv ID: 2410.12341
- Source URL: https://arxiv.org/abs/2410.12341
- Reference count: 31
- Authors: Daniele Gambetta; Gizem Gezici; Fosca Giannotti; Dino Pedreschi; Alistair Knott; Luca Pappalardo
- Primary result: Surplexity filtering prevents model collapse by removing data that does not surprise the model

## Executive Summary
This paper introduces surplexity as a measure to detect and mitigate model collapse in generative AI. The authors propose filtering training data by high surplexity to prevent models from training on "surprising" content, eliminating the need to distinguish between human- and AI-generated data. Experiments show this approach is at least as effective as human-data baselines and reduces distributional skewedness more effectively. The key insight is that model collapse occurs when models train on data that does not surprise them, linking this to the Free Energy Principle in cognitive science.

## Method Summary
The authors conducted recursive fine-tuning experiments using Llama2-chat-7b1 on 2050 randomly sampled Wikipedia articles. They generated summaries from these articles and prompted the model to create documents, then calculated surplexity for each generated document. Documents with high surplexity were retained for fine-tuning in subsequent generations. This process was repeated for 10 generations, with linguistic analysis (entropy, TTR, POS tags, n-grams, semantic networks) and qualitative evaluation performed on each generation.

## Key Results
- Surplexity filtering effectively prevents model collapse by removing data that does not surprise the model
- The approach is at least as effective as human-data baselines while reducing distributional skewedness more effectively
- Model collapse manifests as decreased entropy, reduced TTR, and increased Gini coefficient across generations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High surplexity filtering prevents model collapse by removing data that does not surprise the model.
- Mechanism: The model assigns higher probability to its own generated outputs during recursive training, leading to "surprising" data being underrepresented. Filtering by high surplexity ensures training data maintains unexpectedness.
- Core assumption: Model collapse occurs when training data matches the model's predictions too closely.
- Evidence anchors:
  - [abstract]: "Building on this insight, we propose a practical mitigation strategy: filtering training items by high surplexity, maximising the surprise of the model."
  - [corpus]: No direct corpus evidence found for surplexity filtering effectiveness.
- Break condition: If surplexity filtering removes too much data, model may lack sufficient training coverage.

### Mechanism 2
- Claim: Free Energy Principle explains model collapse through prediction error minimization.
- Mechanism: Cognitive systems minimize free energy by reducing prediction error. When models train on their own outputs, prediction error decreases, leading to loss of diversity.
- Core assumption: Model behavior follows Free Energy Principle from cognitive science.
- Evidence anchors:
  - [abstract]: "Our experiments prompt a new suggestion: that model collapse occurs when a model trains on data that does not 'surprise' it. We express this hypothesis in terms of the well-known Free Energy Principle in cognitive science."
  - [corpus]: No corpus evidence found for Free Energy Principle application to model collapse.
- Break condition: If prediction error minimization does not correlate with diversity loss.

### Mechanism 3
- Claim: Model collapse creates "rich-get-richer" effect in token distributions.
- Mechanism: As models train on synthetic data, frequent tokens become more frequent while rare tokens disappear, reducing vocabulary diversity.
- Core assumption: Autophagy amplifies existing frequency biases in token distributions.
- Evidence anchors:
  - [section]: "The phenomenon of the rich-get-richer or Matthew effect is also in line with the Pareto principle, which states that for many outcomes, roughly 80% of consequences come from 20% of causes."
  - [corpus]: Weak corpus evidence - no direct citations for rich-get-richer effect in model collapse.
- Break condition: If token frequency distributions remain stable across generations.

## Foundational Learning

- Concept: Entropy and information theory
  - Why needed here: Surplexity is defined using entropy-based measures of surprise in probability distributions.
  - Quick check question: How does entropy change when a model perfectly predicts its own outputs?

- Concept: Free Energy Principle
  - Why needed here: The paper connects model collapse to cognitive science principles about prediction error minimization.
  - Quick check question: What happens to a system's diversity when it minimizes prediction error too aggressively?

- Concept: Type-Token Ratio (TTR)
  - Why needed here: TTR measures lexical diversity, which decreases during model collapse according to experiments.
  - Quick check question: What does a decreasing TTR indicate about text generation over successive iterations?

## Architecture Onboarding

- Component map: Data collection pipeline (Wikipedia articles) -> Model inference engine (Llama2-chat) -> Fine-tuning loop with surplexity filtering -> Evaluation framework (linguistic metrics)

- Critical path: 1. Sample Wikipedia articles → 2. Generate summaries → 3. Prompt model for documents → 4. Calculate surplexity → 5. Filter by high surplexity → 6. Fine-tune model → 7. Repeat for 10 generations

- Design tradeoffs:
  - Surplexity threshold vs. data quantity
  - Generation length vs. diversity preservation
  - Model size vs. computational feasibility

- Failure signatures:
  - Rapid decrease in entropy and TTR values
  - Gini coefficient increase indicating inequality
  - HAPAX legomenon count decrease
  - Model outputs becoming repetitive or generic

- First 3 experiments:
  1. Measure surplexity distribution before and after filtering on sample dataset
  2. Compare diversity metrics (entropy, TTR) across generations with and without surplexity filtering
  3. Test Free Energy Principle hypothesis by correlating prediction probability with diversity loss

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does model collapse occur primarily due to the repetitive use of similar prompts across generations, or is it an inherent property of recursive fine-tuning on synthetic data?
- Basis in paper: [inferred] The paper suggests that using similar prompts across generations might interfere with the autophagy process and contribute to model collapse by forcing the model to repeat similar patterns. The authors propose using paraphrased prompts as a future direction to mitigate this effect.
- Why unresolved: The current study used the same prompt template for all generations, making it difficult to disentangle the effect of prompt repetition from the inherent model collapse caused by recursive fine-tuning on synthetic data.
- What evidence would resolve it: Comparing the results of two autophagy pipelines - one using the same prompt and another using paraphrased prompts - would help determine the primary cause of model collapse. If model collapse is significantly reduced with paraphrased prompts, it suggests that prompt repetition plays a major role. Otherwise, the inherent property of recursive fine-tuning on synthetic data is the primary cause.

### Open Question 2
- Question: How does the size of the initial training dataset affect the rate and severity of model collapse in the autophagy pipeline?
- Basis in paper: [inferred] The paper mentions that they used a relatively small dataset (2050 Wikipedia articles) and acknowledges that their results might be preliminary due to this limitation. They suggest replicating experiments with different dataset sizes as a future direction.
- Why unresolved: The current study used a specific dataset size, and it is unclear how the rate and severity of model collapse would change with larger or smaller initial training datasets.
- What evidence would resolve it: Conducting the same autophagy pipeline experiments with varying initial dataset sizes (e.g., 1000, 5000, 10000 articles) and comparing the results would reveal the relationship between dataset size and model collapse. If larger datasets lead to slower or less severe model collapse, it suggests that dataset size plays a crucial role in mitigating the phenomenon.

### Open Question 3
- Question: Does model collapse in the autophagy pipeline affect the performance of fine-tuned models on a wide range of NLP benchmark tasks, or is it limited to specific task domains?
- Basis in paper: [inferred] The paper presents a preliminary qualitative analysis of the models' performance on three specific prompts (story generation, factual question answering, and general knowledge). The authors suggest evaluating the models' performance on NLP benchmark tasks as a future direction to obtain more robust results.
- Why unresolved: The current study only examined the models' performance on a limited set of prompts, and it is unclear whether model collapse would have a similar impact on a broader range of NLP tasks.
- What evidence would resolve it: Fine-tuning the models in the autophagy pipeline and evaluating their performance on a comprehensive NLP benchmark suite (e.g., BIGBENCH) would provide insights into the generalizability of model collapse across different task domains. If the models show consistent degradation in performance across various tasks, it suggests that model collapse is a widespread issue affecting multiple aspects of NLP model capabilities.

## Limitations
- Limited empirical validation of surplexity filtering effectiveness with no direct corpus evidence
- Theoretical connection to Free Energy Principle lacks empirical grounding in the context of model collapse
- No analysis of potential limitations such as impact on dataset coverage or creation of new biases

## Confidence

- High Confidence: The observation that model collapse leads to distributional skewedness and reduced diversity (supported by linguistic metrics like entropy, TTR, and Gini coefficient).
- Medium Confidence: The surplexity filtering approach is at least as effective as human-data baselines (based on limited experimental evidence).
- Low Confidence: The Free Energy Principle provides the theoretical foundation for understanding model collapse (no direct corpus evidence found).

## Next Checks
1. Conduct experiments comparing model collapse progression with and without surplexity filtering across multiple model architectures and dataset sizes to verify the effectiveness claim.

2. Search for existing literature that connects prediction error minimization (Free Energy Principle) to diversity loss in generative models to strengthen the theoretical foundation.

3. Systematically vary surplexity thresholds to determine optimal filtering levels that prevent model collapse while maintaining sufficient training data coverage.