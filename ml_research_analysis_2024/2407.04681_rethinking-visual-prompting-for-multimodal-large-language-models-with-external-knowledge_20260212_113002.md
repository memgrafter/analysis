---
ver: rpa2
title: Rethinking Visual Prompting for Multimodal Large Language Models with External
  Knowledge
arxiv_id: '2407.04681'
source_url: https://arxiv.org/abs/2407.04681
tags:
- visual
- arxiv
- prompt
- image
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel visual prompting method for multimodal
  large language models (MLLMs) that addresses the challenge of fine-grained visual
  understanding. The approach leverages external knowledge from specialized vision
  models like panoptic segmentation and OCR to generate pixel-wise textual embeddings,
  which are then integrated directly into the visual prompt as a spatial embedding
  map.
---

# Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge

## Quick Facts
- arXiv ID: 2407.04681
- Source URL: https://arxiv.org/abs/2407.04681
- Authors: Yuanze Lin; Yunsheng Li; Dongdong Chen; Weijian Xu; Ronald Clark; Philip Torr; Lu Yuan
- Reference count: 40
- This paper introduces a novel visual prompting method that embeds pixel-wise textual embeddings from external knowledge sources (segmentation, OCR) directly into MLLMs as spatial embedding maps.

## Executive Summary
This paper addresses the challenge of fine-grained visual understanding in multimodal large language models (MLLMs) by introducing a novel visual prompting method. The approach leverages external knowledge from specialized vision models like panoptic segmentation and OCR to generate pixel-wise textual embeddings, which are then integrated directly into the visual prompt as a spatial embedding map. This differs from existing methods that transform external knowledge into text prompts requiring the model to learn coordinate correspondences. The proposed method can be easily incorporated into various MLLMs like LLaVA and Mipha, and experiments demonstrate significant improvements across nine benchmarks, with the 3-billion parameter model outperforming both 7-billion and 13-billion parameter MLLM variants.

## Method Summary
The method involves generating pixel-wise textual embeddings using panoptic segmentation and OCR models, creating an auxiliary visual prompt, and infusing it into MLLMs via feature addition. The approach leverages a pre-trained text encoder to map detected categories and OCR texts to embeddings, which are then filled into a zero-initialized tensor at the corresponding spatial locations. This bypasses the need for specialized ROI feature extractors and allows zero-shot integration without re-training external models. The feature addition of auxiliary visual prompt to image tokens preserves the original visual representation while injecting fine-grained semantic cues, enabling better alignment without distorting spatial features.

## Key Results
- Achieves state-of-the-art performance on 7 out of 9 benchmarks
- 3-billion parameter model outperforms both 7-billion and 13-billion parameter MLLM variants
- Demonstrates enhanced fine-grained context-aware capabilities without requiring additional training data
- Significant improvements across VQA-v2, GQA, ScienceQA-IMG, TextVQA, MME Perception, MME Cognition, MM-Bench, MM-Vet, and POPE benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Direct pixel-wise embedding of segmentation and OCR outputs into the visual prompt enables MLLMs to bypass learning coordinate-to-region correspondence, reducing spatial reasoning burden. Instead of encoding object coordinates and categories as text, the model receives a spatial embedding map where each pixel is assigned the embedding of the corresponding object class or OCR text. This embedding is fused with image tokens via addition or concatenation, allowing the model to directly access semantic context at the pixel level. If the external model's mask or OCR detection is noisy or the text encoder embeddings are misaligned with visual semantics, pixel-wise fusion can degrade performance or cause hallucinations.

### Mechanism 2
Using a pre-trained text encoder to map detected categories and OCR texts to embeddings allows zero-shot integration without re-training external models. The segmentation and OCR outputs (categories, bounding boxes, OCR strings) are passed through a frozen text encoder to produce embeddings that are then filled into a zero-initialized tensor at the corresponding spatial locations. This bypasses the need for specialized ROI feature extractors. If the text encoder is not fine-tuned on the domain-specific vocabulary (e.g., technical terms, proper nouns), the embeddings may be semantically inadequate for precise recognition tasks.

### Mechanism 3
Feature addition of auxiliary visual prompt to image tokens preserves the original visual representation while injecting fine-grained semantic cues, enabling better alignment without distorting spatial features. After processing the auxiliary prompt via a convolutional prompt embedding network, its features are added pixel-wise to the image token map. This maintains the spatial layout while enriching each token with contextual semantics. If the auxiliary prompt's feature distribution is very different from the image tokens', addition can cause gradient vanishing or exploding in the fused features, hurting convergence.

## Foundational Learning

- Concept: Multimodal alignment between vision and language embeddings
  - Why needed here: MLLMs must fuse heterogeneous modalities; understanding embedding space alignment is essential to integrate external semantic cues into the visual stream.
  - Quick check question: How do you ensure that the dimensions and normalization of visual tokens and auxiliary prompt embeddings match before fusion?

- Concept: Zero-shot transfer from pre-trained segmentation/OCR to MLLM fine-grained understanding
  - Why needed here: The approach leverages external model outputs without re-training; understanding the limits of zero-shot applicability is key to predicting robustness.
  - Quick check question: What happens to MLLM performance if the segmentation model fails to detect a salient object that the question refers to?

- Concept: Spatial resolution trade-offs in visual encoders
  - Why needed here: The pixel-wise embedding map's granularity depends on the visual encoder's output resolution; mismatches can degrade fine-grained reasoning.
  - Quick check question: If the visual encoder downsamples the image by a factor of 16, what is the effective pixel-level resolution of the auxiliary prompt embeddings?

## Architecture Onboarding

- Component map: Vision encoder → image tokens → Feature fusion → LLM decoder; Segmentation + OCR models → object categories & text → Text encoder → pixel-wise embeddings → Prompt embedding network → aligned spatial features → Feature fusion → LLM decoder
- Critical path: Segmentation/OCR → Text encoder → Prompt embedding network → Feature fusion → LLM inference
- Design tradeoffs:
  - Embedding fusion (add vs concat): Addition preserves spatial structure but risks feature interference; concatenation increases dimensionality but may better separate modalities.
  - Segmentation model choice: Panoptic segmentation yields fine-grained masks but is slower; object detectors are faster but coarser.
  - Text encoder size: Larger encoders produce richer embeddings but increase latency and memory.
- Failure signatures:
  - Degraded performance on OCR-heavy tasks → segmentation/OCR model failure or text encoder out-of-vocabulary
  - Hallucinations in fine-grained object counting → misalignment between pixel embeddings and actual image content
  - Training instability → feature distribution mismatch in addition fusion
- First 3 experiments:
  1. Replace panoptic segmentation with an object detector; compare accuracy drop on fine-grained benchmarks.
  2. Switch from feature addition to concatenation; measure change in VQAv2 and GQA scores.
  3. Freeze vs fine-tune the text encoder embeddings; evaluate impact on OCR-specific benchmarks (TextVQA, MM-Vet).

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed visual prompting method perform when the external knowledge sources (panoptic segmentation and OCR) have significant domain gaps with the evaluation benchmarks? The paper mentions that the performance of these models will significantly impact the performance of the proposed method, particularly when there is a substantial domain gap between the images from specific benchmarks and the training set of the segmentation or OCR models. The paper does not provide experimental results or analysis on the performance degradation when the external knowledge sources have domain gaps with the evaluation benchmarks.

### Open Question 2
What is the impact of using different fusion strategies (feature fusion vs. feature addition) on the performance of the proposed visual prompting method? The paper mentions two options for combining the image tokens and the processed auxiliary visual prompt, but does not provide a detailed comparison of the two strategies. The paper empirically observes that directly adding auxiliary visual prompts yields slightly better results than concatenation, but does not provide a thorough analysis of the impact of different fusion strategies on the performance of the proposed method.

### Open Question 3
How does the proposed visual prompting method compare to other methods that incorporate external knowledge, such as using bounding box coordinates or object detection features? The paper mentions concurrent works that transform external knowledge into additional text prompts, necessitating the model to indirectly learn the correspondence between visual content and text coordinates. It also mentions works that propose incorporating Region of Interest (ROI) features directly into model learning. However, the paper does not provide a direct comparison with these methods.

## Limitations

- The paper lacks detailed ablation studies on the relative importance of segmentation vs OCR contributions to performance gains
- No comparison with end-to-end trained models that incorporate similar external knowledge during pretraining
- The 1-epoch training schedule may not fully capture model convergence behavior or potential overfitting patterns

## Confidence

- **High**: The mechanism of using pixel-wise embeddings instead of coordinate-based text prompts is technically sound and addresses a real limitation in current MLLM prompting approaches
- **Medium**: The claim of state-of-the-art performance on 7/9 benchmarks is supported by experimental results, but the narrow margin over baseline models on some tasks suggests the improvements may not be uniformly robust
- **Low**: The assertion that this approach "significantly reduces the model's learning burden" lacks quantitative validation - no measurements of learning efficiency or convergence speed are provided

## Next Checks

1. Conduct ablation studies removing either panoptic segmentation or OCR components to quantify their individual contributions to performance improvements
2. Test model generalization on out-of-distribution data not seen during training to assess robustness of the external knowledge integration
3. Compare computational efficiency (FLOPs, inference latency) against baseline MLLMs to validate practical deployment advantages beyond accuracy gains