---
ver: rpa2
title: 'ConstrainedZero: Chance-Constrained POMDP Planning using Learned Probabilistic
  Failure Surrogates and Adaptive Safety Constraints'
arxiv_id: '2405.00644'
source_url: https://arxiv.org/abs/2405.00644
tags:
- failure
- policy
- probability
- mcts
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ConstrainedZero is a policy iteration algorithm for chance-constrained\
  \ POMDPs (CC-POMDPs) that combines neural network approximations of the value function,\
  \ policy, and failure probability with online Monte Carlo tree search (MCTS). The\
  \ algorithm uses an additional network head to estimate failure probability given\
  \ a belief and employs \u2206-MCTS with adaptive conformal inference to update failure\
  \ thresholds during planning."
---

# ConstrainedZero: Chance-Constrained POMDP Planning using Learned Probabilistic Failure Surrogates and Adaptive Safety Constraints

## Quick Facts
- arXiv ID: 2405.00644
- Source URL: https://arxiv.org/abs/2405.00644
- Reference count: 8
- Key outcome: ConstrainedZero achieved target safety levels without optimizing the balance between rewards and costs, outperforming BetaZero in safety-constrained scenarios while maintaining competitive returns.

## Executive Summary
ConstrainedZero is a policy iteration algorithm for chance-constrained POMDPs (CC-POMDPs) that combines neural network approximations of the value function, policy, and failure probability with online Monte Carlo tree search (MCTS). The algorithm uses an additional network head to estimate failure probability given a belief and employs ∆-MCTS with adaptive conformal inference to update failure thresholds during planning. Tested on three safety-critical CC-POMDP benchmarks—LightDark localization, aircraft collision avoidance, and safe CO2 storage—ConstrainedZero achieved target safety levels without optimizing the balance between rewards and costs. The algorithm consistently outperformed BetaZero in safety-constrained scenarios while maintaining competitive returns.

## Method Summary
ConstrainedZero solves CC-POMDPs by learning neural network surrogates for the value function, policy, and failure probability, then using these during online ∆-MCTS planning with adaptive safety thresholds. The network has three heads: policy, value, and failure probability, trained via policy iteration using data from ∆-MCTS episodes. During planning, CC-PUCT selects actions based on Q-values and policy priors, but only among actions whose failure probability estimate is below an adaptively updated threshold. This threshold is initialized to the target tolerance and updated online using adaptive conformal inference based on observed failure probabilities. The algorithm alternates between collecting data via ∆-MCTS and training the network until convergence.

## Key Results
- Achieved target safety levels (failure probability below threshold) across all three benchmark CC-POMDPs without optimizing the reward-cost balance
- Consistently outperformed BetaZero baseline in safety-constrained scenarios while maintaining competitive returns
- Demonstrated adaptive threshold mechanism effectively balanced safety and exploration across varying safety requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The neural network surrogate learns to predict both value and failure probability from belief states, allowing online MCTS to avoid expensive rollouts.
- Mechanism: The network has three heads: one for policy, one for value, and one for failure probability. During policy evaluation, these heads are trained on data collected from ∆-MCTS episodes, where failure events are marked per trajectory. The failure head learns to map belief inputs to failure probabilities, which are then used during online search instead of costly forward simulations.
- Core assumption: The belief representation (summary statistics like mean and standard deviation) is sufficient for the network to estimate future failure probabilities accurately.
- Evidence anchors:
  - [abstract]: "neural network approximations of the optimal value and policy with an additional network head that estimates the failure probability given a belief"
  - [section 3]: "minimize the binary cross-entropy loss LFθ(et, pt) to regress the failure probability function"
- Break condition: If the belief summary statistics fail to capture sufficient information about the underlying state uncertainty, the failure probability predictions will be inaccurate, causing the algorithm to misjudge safety.

### Mechanism 2
- Claim: Adaptive conformal inference updates the failure probability threshold during planning to balance safety and reward exploration.
- Mechanism: After each node expansion, the algorithm compares the observed failure probability F(b,a) to the current threshold ∆(b). If F(b,a) exceeds ∆(b), the threshold is increased; otherwise, it is decreased. This adaptation is clipped to the min/max observed failure probabilities for that belief, ensuring at least one action remains selectable.
- Core assumption: The sequence of failure probability estimates follows a distribution that can be tracked by the adaptive threshold without strong parametric assumptions.
- Evidence anchors:
  - [abstract]: "∆-MCTS, which uses adaptive conformal inference to update the failure threshold during planning"
  - [section 3]: "The adaptive threshold is initialized to the target tolerance ∆(b) = ∆0 where ∆0 = ∆ from the CC-BMDP... update the current acceptable safety threshold"
- Break condition: If the failure probability estimates are highly volatile or non-stationary, the adaptive threshold may oscillate or fail to converge to a useful value.

### Mechanism 3
- Claim: The CC-PUCT criterion ensures that action selection balances reward maximization with safety constraint satisfaction.
- Mechanism: During MCTS, actions are selected to maximize the sum of Q-value and policy prior, but only among actions whose failure probability estimate is below the adaptive threshold ∆′(b). This filters out unsafe actions while still allowing exploration of high-value safe actions.
- Core assumption: The Q-value estimates and failure probability estimates are sufficiently accurate to guide safe and rewarding action selection.
- Evidence anchors:
  - [abstract]: "The algorithm uses an additional network head to estimate failure probability given a belief and employs ∆-MCTS with adaptive conformal inference to update the failure thresholds during planning"
  - [section 3]: "πexplore(b) = arg max a∈A(b) ¯Q(b, a) + c ( Pθ(˜b, a) √ N (b) 1+N (b,a) ) s. t. F (b, a) ≤ ∆′(b)"
- Break condition: If the Q-value estimates are poor or the failure probability estimates are biased, the algorithm may select suboptimal or unsafe actions.

## Foundational Learning

- Concept: Belief space planning for POMDPs
  - Why needed here: The algorithm operates in belief space, treating the belief over states as the planning state. This allows handling partial observability without full state information.
  - Quick check question: What is the difference between a POMDP and a belief-state MDP, and why does converting to a BMDP help with planning?

- Concept: Monte Carlo tree search with progressive widening
  - Why needed here: MCTS builds a search tree incrementally, expanding nodes as they are visited. Progressive widening controls the number of children per node to manage computational cost, especially important for belief-state planning where belief updates are expensive.
  - Quick check question: How does progressive widening help balance exploration and computational efficiency in large or continuous action/observation spaces?

- Concept: Adaptive conformal inference for online threshold adaptation
  - Why needed here: ACI provides a way to adapt the failure probability threshold during planning based on observed outcomes, without requiring strong distributional assumptions. This allows the algorithm to adjust safety margins dynamically.
  - Quick check question: How does adaptive conformal inference differ from traditional conformal prediction, and why is it suitable for online planning?

## Architecture Onboarding

- Component map:
  Neural network: Input (belief summary stats) → Policy head, Value head, Failure probability head
  ∆-MCTS: Online search with CC-PUCT selection, belief expansion, backpropagation of Q and F values, adaptive threshold update
  Policy iteration loop: Collect data via ∆-MCTS → Train network on (belief, action, return, failure) tuples → Repeat
  Generative models: POMDP (s′, r, o) ~ G(s,a) and BMDP (b′, r, p) ~ Gb(b,a) for simulation

- Critical path:
  1. Initialize network and parameters
  2. For each iteration: run n parallel ∆-MCTS episodes, collect (belief, tree policy, return, failure) data
  3. Train network to minimize value, policy, and failure probability losses
  4. After training, use the learned network to guide online ∆-MCTS planning with adaptive safety thresholds

- Design tradeoffs:
  - Using a neural network surrogate reduces planning time but introduces approximation error
  - Adaptive threshold adjustment allows flexibility but may lead to threshold drift if failure estimates are noisy
  - Separating safety from reward simplifies constraint specification but requires careful tuning of the failure probability head

- Failure signatures:
  - High variance in failure probability estimates → unstable threshold adaptation
  - Poor value/policy predictions → suboptimal action selection even when safe
  - Belief representation insufficient → failure probability head cannot learn accurate predictions

- First 3 experiments:
  1. Run ConstrainedZero on LightDark with ∆0 = 0.01 and compare failure rate and returns to BetaZero with various λ penalties
  2. Sweep the ACI step size η on LightDark to observe its effect on threshold adaptation and performance
  3. Compare ConstrainedZero with and without adaptive threshold adaptation to confirm its importance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the adaptation step size η affect the trade-off between safety and performance in different CC-POMDP domains?
- Basis in paper: [explicit] The paper discusses empirical sensitivity analysis of η on the LightDark CC-POMDP, showing that larger η leads to more risky behavior with higher returns, while a smaller η provides more stability.
- Why unresolved: The paper only provides results for the LightDark domain. Different CC-POMDPs may have different characteristics that could make them more or less sensitive to η. The optimal η might also depend on the specific safety constraints and reward structures of each problem.
- What evidence would resolve it: A comprehensive empirical study across multiple CC-POMDP domains, varying η and analyzing the resulting safety-performance trade-offs. This could include sensitivity analysis, convergence studies, and comparison with theoretical bounds on η.

### Open Question 2
- Question: Can the adaptation mechanism in ∆-MCTS be improved to handle multiple failure modes or more complex safety constraints?
- Basis in paper: [inferred] The paper focuses on a single failure mode and a scalar failure probability threshold. Real-world safety-critical systems often have multiple failure modes or complex safety constraints that may not be easily captured by a single probability threshold.
- Why unresolved: The paper does not explore extensions of the adaptation mechanism to handle multiple failure modes or more complex safety constraints. Developing such extensions would require new theoretical insights and empirical validation.
- What evidence would resolve it: Theoretical analysis of how the adaptation mechanism could be extended to handle multiple failure modes or complex safety constraints. Empirical evaluation of the extended mechanism on CC-POMDPs with multiple failure modes or complex safety constraints, comparing its performance to the baseline ∆-MCTS.

### Open Question 3
- Question: How does the performance of ConstrainedZero compare to other state-of-the-art algorithms for solving CC-POMDPs, particularly in terms of scalability and sample efficiency?
- Basis in paper: [explicit] The paper compares ConstrainedZero to BetaZero and conducts an ablation study, but does not compare it to other state-of-the-art algorithms for solving CC-POMDPs. The paper mentions that ConstrainedZero may require more computing resources than existing POMDP solvers.
- Why unresolved: The paper does not provide a comprehensive comparison of ConstrainedZero to other state-of-the-art algorithms for solving CC-POMDPs. Such a comparison would be necessary to fully understand the strengths and weaknesses of ConstrainedZero and its potential for real-world applications.
- What evidence would resolve it: Empirical comparison of ConstrainedZero to other state-of-the-art algorithms for solving CC-POMDPs, including algorithms that use different approaches such as heuristic search, gradient-based methods, or reinforcement learning. The comparison should consider factors such as scalability, sample efficiency, and performance on various CC-POMDP domains.

## Limitations

- Critical hyperparameters like MCTS simulation counts, exploration constants, and progressive widening parameters are not specified, making faithful reproduction difficult
- The three benchmark problems have vastly different state dimensions (1D, 4D, and 510D) but the paper does not analyze how the algorithm scales with problem size
- Evaluation only compares against one baseline (BetaZero) and does not benchmark against other state-of-the-art CC-POMDP solvers

## Confidence

- High confidence: The neural network architecture with three heads and the overall policy iteration framework are well-specified and reproducible
- Medium confidence: The mechanism of adaptive conformal inference for threshold adaptation is described clearly, but the implementation details of how threshold clipping and updating are handled could vary
- Low confidence: The scaling behavior of the algorithm to larger problems and the sensitivity analysis to hyperparameters are not provided, limiting understanding of robustness

## Next Checks

1. Conduct a hyperparameter sensitivity analysis on the adaptive threshold update rate η across the three benchmark problems to determine its impact on safety-utility trade-offs
2. Implement and compare against at least one additional CC-POMDP baseline algorithm to validate the claimed superiority of ConstrainedZero
3. Analyze the computational complexity and wall-clock time scaling of ConstrainedZero as the state dimension increases from LightDark to CO2 storage to understand practical limitations