---
ver: rpa2
title: 'LLM-based speaker diarization correction: A generalizable approach'
arxiv_id: '2406.04927'
source_url: https://arxiv.org/abs/2406.04927
tags:
- diarization
- transcripts
- speaker
- https
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving speaker diarization
  accuracy in transcribed conversations. The authors propose using fine-tuned large
  language models (LLMs) as a post-processing step to correct speaker labels.
---

# LLM-based speaker diarization correction: A generalizable approach

## Quick Facts
- arXiv ID: 2406.04927
- Source URL: https://arxiv.org/abs/2406.04927
- Reference count: 40
- Primary result: Ensemble LLM model outperforms individual ASR-specific models in speaker diarization correction across different ASR tools

## Executive Summary
This paper addresses the challenge of improving speaker diarization accuracy in transcribed conversations by using fine-tuned large language models (LLMs) as a post-processing correction step. The authors train three separate ASR-specific models on transcripts from AWS, Azure, and WhisperX, then develop an ensemble model by combining these models' weights. Results show that the ensemble model achieves better overall diarization accuracy across different ASR tools compared to individual ASR-specific models, with a 32% reduction in deltaCP on the Fisher test set while maintaining inference speed.

## Method Summary
The approach uses fine-tuned Mistral 7b models to correct speaker labels in ASR transcripts. Three ASR-specific models are trained on transcripts from AWS, Azure, and WhisperX using prompt-completion pairs where the prompt is the ASR transcript and the completion is the oracle transcript. An ensemble model is created by combining these ASR-specific models using TIES-Merging from merge-kit. Speaker labels are transferred from reference transcripts to ASR transcripts using a Text-Parsing Speaker Transfer (TPST) algorithm that aligns word sequences between the two. The models are evaluated using deltaCP and deltaSA metrics that measure diarization accuracy improvement compared to baseline.

## Key Results
- The ensemble model outperformed individual ASR-specific models across all ASR tools tested
- On the Fisher test set, the ensemble model reduced deltaCP by 32% compared to baseline
- The ensemble model achieved better overall performance, though slightly worse than the AWS-specific model on AWS transcripts
- The approach maintained inference speed while improving diarization accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based diarization correction improves accuracy by aligning transcription text with reference speaker labels via TPST.
- Mechanism: The algorithm matches word sequences between ASR transcripts and reference transcripts, transferring reference speaker labels to ASR text without altering wording.
- Core assumption: The text content is sufficiently similar between ASR and reference transcripts for alignment to be possible.
- Evidence anchors:
  - [section] "The TPST algorithm accepts speaker-labeled source text and speaker-labeled target text. It aligns the two so that the source text speaker labels match the target text."
  - [abstract] "LLMs were fine-tuned using the Fisher corpus, a large dataset of transcribed conversations."
- Break condition: High transcription errors (WER) would prevent accurate word sequence alignment, making speaker label transfer unreliable.

### Mechanism 2
- Claim: ASR-specific models perform best on transcripts from the same ASR used during fine-tuning due to error pattern specialization.
- Mechanism: Fine-tuning on one ASR's error patterns allows the model to learn specific types of diarization mistakes that ASR tends to make, improving correction accuracy.
- Core assumption: Different ASR tools produce distinct and consistent error patterns in diarization.
- Evidence anchors:
  - [section] "we hypothesized that a model fine-tuned on one ASR would perform best at correcting diarization for transcripts produced by the same ASR compared to transcripts produced by a different ASR."
  - [section] "The nature of this variance is beyond the scope of this manuscript. However, qualitative observation showed that different ASR tools had different types of errors."
- Break condition: If ASR error patterns overlap significantly or change over time, specialization would not provide advantage.

### Mechanism 3
- Claim: Ensemble models generalize better by combining weights from multiple ASR-specific models, balancing different error correction approaches.
- Mechanism: TIES-Merging computes weighted averages of model parameters, selecting the most significant changes while resolving conflicts, creating a model that can handle multiple ASR error distributions.
- Core assumption: Different ASR-specific models capture complementary aspects of diarization correction that can be combined effectively.
- Evidence anchors:
  - [section] "To develop an ASR-agnostic ensemble model, TIES-Merging [45] from merge-kit [46] was used to combine parameters from each ASR-specific model."
  - [section] "The ensemble model achieved the best overall performance. In the AWS transcripts, it had marginally worse performance compared to the AWS model. However, in the Azure and WhisperX transcripts, it performed better than even the ASR-specific models for Azure and WhisperX."
- Break condition: If ASR-specific models are too similar or conflicts between them cannot be resolved effectively, ensemble performance would not exceed individual models.

## Foundational Learning

- Concept: Speaker diarization error types
  - Why needed here: Understanding how diarization errors manifest (speaker label swaps, missed speakers, etc.) is crucial for designing correction approaches
  - Quick check question: What are the two main types of errors measured by deltaCP and deltaSA?

- Concept: Transfer learning and fine-tuning
  - Why needed here: The approach relies on fine-tuning pre-trained LLMs for a specific task rather than training from scratch
  - Quick check question: Why might zero-shot performance be poor for this task compared to fine-tuned models?

- Concept: Model ensembling and weight merging
  - Why needed here: The ensemble approach uses TIES-Merging to combine model parameters, which requires understanding how this differs from simple averaging
  - Quick check question: What problem does TIES-Merging solve compared to naive ensemble averaging?

## Architecture Onboarding

- Component map: ASR transcription → LLM-based correction (fine-tuned or ensemble) → TPST-based label transfer → output transcript
- Critical path: Transcript preprocessing → prompt generation → LLM inference → completion parsing → TPST alignment → final output
- Design tradeoffs: Fine-tuning separate ASR-specific models vs. single generalizable model; computational cost vs. accuracy gains; zero-shot vs. fine-tuned performance
- Failure signatures: Poor diarization improvement (deltaCP/SA not decreasing); word substitutions in output; runtime errors during TPST alignment; model hallucinations introducing new errors
- First 3 experiments:
  1. Test diarization correction on a small subset of AWS transcripts with AWS-specific model and measure deltaCP/SA improvement
  2. Run the same transcripts through the ensemble model and compare performance to individual ASR-specific models
  3. Evaluate the ensemble model on Azure transcripts to verify generalization capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the specific error patterns of different ASR tools influence the effectiveness of LLM-based diarization correction models?
- Basis in paper: [explicit] The paper notes that different ASR tools exhibit different types of diarization errors, such as one tool frequently mislabeling speakers at the end of sentences while another mislabels small phrases in the middle of longer speech segments.
- Why unresolved: While the paper qualitatively observes these differences, it does not provide a detailed analysis of the specific error patterns or how they impact model performance.
- What evidence would resolve it: A comprehensive analysis of error patterns across different ASR tools, including quantitative metrics and case studies, would clarify how these patterns affect model effectiveness.

### Open Question 2
- Question: To what extent can multimodal data integration enhance the robustness of LLM-based diarization correction systems, particularly in noisy environments?
- Basis in paper: [inferred] The paper suggests that integrating multimodal data, such as acoustic information with semantic insights from LLMs, could enhance diarization systems' ability to label speech effectively, especially in complex and noisy environments.
- Why unresolved: The paper does not explore or test the integration of multimodal data, leaving the potential benefits and challenges unexplored.
- What evidence would resolve it: Empirical studies comparing diarization accuracy with and without multimodal data integration in various acoustic conditions would demonstrate the potential benefits.

### Open Question 3
- Question: How can prompt engineering techniques, such as few-shot learning or contextual prompts, improve the zero-shot performance of LLMs in diarization correction tasks?
- Basis in paper: [explicit] The paper acknowledges that zero-shot performance of LLMs was poor and suggests that using examples in the prompt, few-shot techniques, or other prompt engineering methods could potentially improve performance.
- Why unresolved: The paper does not explore these techniques, focusing instead on fine-tuning approaches.
- What evidence would resolve it: Experiments comparing zero-shot performance with and without advanced prompt engineering techniques would quantify their impact on diarization correction accuracy.

## Limitations

- Limited generalization testing with only one independent dataset (PriMock57) beyond the training corpus
- Critical failure point when WER exceeds ~30%, making speaker label transfer unreliable
- Lack of evaluation for varying audio quality, speaker overlap conditions, and non-conversational speech contexts

## Confidence

- **High Confidence:** The ensemble model's superior performance across multiple ASR tools on the Fisher corpus; the mechanism of TPST-based speaker label transfer; the negative correlation between WER and diarization improvement
- **Medium Confidence:** Generalization to PriMock57 dataset; the specific error pattern specialization of ASR-specific models; the effectiveness of TIES-Merging for ensemble creation
- **Low Confidence:** Cross-domain generalization beyond PriMock57; performance with WER >30%; effectiveness with overlapping speech; applicability to non-conversational contexts

## Next Checks

1. Evaluate the ensemble model on a third, completely independent dataset with different domain characteristics (e.g., medical consultations, customer service calls) to assess true generalization capability beyond the Fisher and PriMock57 datasets.

2. Systematically test the approach across a range of WER values (20%, 30%, 40%, 50%) to determine the precise threshold where speaker label transfer breaks down and identify the maximum usable error rate.

3. Conduct ablation studies comparing the ensemble model's performance when trained on different combinations of ASR tools (e.g., only AWS+Azure vs. all three) to quantify the contribution of each ASR-specific model to overall performance.