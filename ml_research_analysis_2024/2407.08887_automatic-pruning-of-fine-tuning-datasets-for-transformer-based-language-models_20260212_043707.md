---
ver: rpa2
title: Automatic Pruning of Fine-tuning Datasets for Transformer-based Language Models
arxiv_id: '2407.08887'
source_url: https://arxiv.org/abs/2407.08887
tags:
- data
- subset
- subsets
- fine-tuning
- points
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an automatic dataset pruning method for fine-tuning
  transformer-based language models, addressing the challenge of reducing fine-tuning
  dataset size while maintaining evaluation performance. The proposed method, called
  H-score, quantifies the difficulty of each data point based on the model's classification
  success rate across multiple fine-tuning runs and epochs.
---

# Automatic Pruning of Fine-tuning Datasets for Transformer-based Language Models

## Quick Facts
- arXiv ID: 2407.08887
- Source URL: https://arxiv.org/abs/2407.08887
- Reference count: 40
- Primary result: Automatic dataset pruning method that reduces fine-tuning dataset size by 3× while maintaining or improving evaluation performance

## Executive Summary
This paper introduces H-score, an automatic dataset pruning method for fine-tuning transformer-based language models. The method quantifies the difficulty of each data point based on the model's classification success rate across multiple fine-tuning runs and epochs. By removing data points with the smallest and largest H-scores, the authors create pruned subsets that are on average 3× smaller than the original training set. Experimental results on 5 downstream tasks and 2 language models show that fine-tuning on these pruned subsets results in an average 0.1% increase in evaluation performance compared to full fine-tuning, with some tasks showing up to 1.2% improvement while using only 21% of the original training data.

## Method Summary
The H-score method computes difficulty scores for each data point by tracking the model's classification success across multiple fine-tuning runs and epochs. The score is calculated by assigning rewards of 1 for each correctly classified epoch and summing across runs. Pruned subsets are created by removing data points with the smallest (most difficult) and largest (easiest) H-scores, retaining those with intermediate scores. The method automatically determines subset sizes for each model-task pair based on the distribution of H-scores rather than using fixed percentages. Fine-tuning is performed on these pruned subsets and compared to full fine-tuning baselines.

## Key Results
- H-score pruned subsets are on average 3× smaller than original training sets
- Fine-tuning on pruned subsets achieves an average 0.1% increase in evaluation performance
- Some tasks show up to 1.2% improvement while using only 21% of original training data
- Method automatically adapts subset sizes for different model-task combinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: H-score quantifies difficulty of each data point based on the model's classification success rate across multiple fine-tuning runs and epochs.
- Mechanism: The H-score assigns a reward of 1 for each epoch and fine-tuning run if the model correctly classifies the true label. These rewards are multiplied across epochs and summed across runs, resulting in scores that range from 0 (never correctly classified) to S (always correctly classified).
- Core assumption: The model's ability to correctly classify a data point across multiple runs and epochs is a reliable proxy for that data point's importance to downstream task performance.
- Evidence anchors: [abstract] "Our method is based on the model's success rate in correctly classifying each training data point." [section] "Equation 1 assigns a reward of 1 for each epoch and fine-tuning run if the model has correctly classified the true label."

### Mechanism 2
- Claim: Removing data points with the smallest H-scores (most difficult) while maintaining those with larger H-scores improves evaluation performance by eliminating confusing or destabilizing examples.
- Mechanism: The pruning process removes data points that are consistently misclassified (Hi=0) and those that are too easy (Hi=S), keeping data points with intermediate H-scores that are neither too difficult nor too easy.
- Core assumption: The most difficult data points destabilize training and the easiest data points are redundant for learning.
- Evidence anchors: [section] "We remove the most difficult data points as including them in the fine-tuning subset only confuses the model and destabilizes the training procedure." [section] "data points with Hi = 6 are too easy to learn and can be avoided without damaging the evaluation performance."

### Mechanism 3
- Claim: The proposed method automatically adapts subset size for each model-task pair, avoiding suboptimal performance from fixed-size pruning.
- Mechanism: By computing H-scores specifically for each model-task combination, the method determines which data points to include based on that model's actual learning behavior, rather than using a predetermined percentage.
- Core assumption: Different models have different learning patterns on the same task, requiring task-model-specific subset sizing.
- Evidence anchors: [abstract] "Our method automatically extracts training subsets that are adapted for each pair of model and fine-tuning task." [section] "different data points in the fine-tuning dataset have different contributions in achieving this goal"

## Foundational Learning

- Concept: Transfer learning in transformer-based language models
  - Why needed here: The paper builds on the observation that pre-trained transformers can be fine-tuned on downstream tasks, and this fine-tuning process can be made more efficient through dataset pruning.
  - Quick check question: What is the key difference between pre-training and fine-tuning in transformer-based language models?

- Concept: Dataset pruning and its relationship to model performance
  - Why needed here: The core contribution involves identifying and removing less useful data points from training sets while maintaining or improving evaluation performance.
  - Quick check question: How does dataset pruning potentially improve fine-tuning efficiency without sacrificing model performance?

- Concept: Scoring functions for data importance
  - Why needed here: H-score is the central scoring mechanism that determines which data points to keep or remove during pruning.
  - Quick check question: What properties should an effective data importance scoring function have in the context of fine-tuning?

## Architecture Onboarding

- Component map: H-score computation module → Subset selection engine → Fine-tuning pipeline → Evaluation framework
- Critical path: H-score computation → Subset creation → Model fine-tuning on pruned data → Performance evaluation
- Design tradeoffs:
  - H-score granularity vs. computational cost (more runs/epochs provide better scoring but increase computation)
  - Subset size vs. performance (smaller subsets save time but may reduce accuracy)
  - Automatic adaptation vs. user control (automatic sizing vs. manual specification)
- Failure signatures:
  - Consistently poor performance on pruned subsets indicates H-score is not capturing useful data
  - Very small subset sizes with minimal performance drop suggest over-aggressive pruning
  - Large performance drops when using pruned subsets indicate critical data was removed
- First 3 experiments:
  1. Compute H-scores for a simple dataset (e.g., SST-2) using a pre-trained model with 3 runs and 3 epochs each
  2. Create winning ticket subset and compare fine-tuning performance against full dataset
  3. Vary the number of runs/epochs used for H-score computation and measure impact on subset quality and size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the H-score method perform on more diverse NLP tasks beyond the 5 tasks studied, particularly in question answering and language generation tasks?
- Basis in paper: [explicit] The authors state "For future work, we will apply our method to a wider variety of natural language understanding tasks. Specifically, we will focus on question answering tasks similar to SQuAD."
- Why unresolved: The current study only evaluates on 5 specific tasks (SNLI, MNLI, SST-2, SQuAD v2, RACE), limiting generalizability to other NLP domains.
- What evidence would resolve it: Experiments applying H-score pruning to a broader range of tasks including language generation, summarization, and more diverse QA datasets, with comparative analysis of performance and subset sizes.

### Open Question 2
- Question: What is the impact of H-score pruned subsets on model performance under adversarial evaluation conditions?
- Basis in paper: [explicit] The authors note "However, these datasets may not be capable to fully capture the model's ability in learning the fine-tuning task. As a result, the performance of our fine-tuned models on adversarial tasks can be lower than full fine-tuning."
- Why unresolved: The study only evaluates on standard validation/test sets, not adversarial examples or out-of-distribution data.
- What evidence would resolve it: Comprehensive testing of pruned models on adversarial datasets, robustness benchmarks, and out-of-distribution samples, comparing performance to full fine-tuning baselines.

### Open Question 3
- Question: How does the H-score method scale to larger transformer models and datasets in terms of computational overhead and effectiveness?
- Basis in paper: [explicit] The authors mention "Our sensitivity analysis experiments provide information for the trade-off between the overhead of computing the H-score and the evaluation performance of the created subsets."
- Why unresolved: Experiments are limited to OPT 350M and RoBERTa LARGE; scalability to billion-parameter models and massive datasets is unexplored.
- What evidence would resolve it: Experiments with larger models (e.g., GPT-3, OPT 175B), analysis of computational costs versus performance gains, and evaluation of H-score effectiveness at different scales.

## Limitations
- The method's effectiveness across diverse model architectures and task types remains untested beyond the two language models and five tasks examined.
- Computational overhead of computing H-scores across multiple fine-tuning runs and epochs could offset some efficiency gains.
- The paper doesn't explore how the method performs with much larger datasets or more complex task distributions.

## Confidence

- **High confidence**: The core claim that H-score-based pruning can reduce dataset size by ~3× while maintaining evaluation performance is well-supported by the experimental results across multiple tasks.
- **Medium confidence**: The mechanism explanation for why difficult data points destabilize training is reasonable but not definitively proven; alternative explanations could exist.
- **Medium confidence**: The automatic adaptation of subset sizes for different model-task pairs is demonstrated but the sensitivity of this adaptation to different parameter choices (number of runs, epochs) is not explored.

## Next Checks

1. Replicate the H-score computation on SST-2 with RoBERTa-LARGE using 6 fine-tuning runs of 3 epochs each, verifying the distribution of scores and resulting subset sizes.
2. Test the method's sensitivity to parameter choices by varying the number of fine-tuning runs (2, 4, 6, 8) and epochs (2, 3, 4) to see how these affect subset quality and computational overhead.
3. Evaluate performance on a held-out task not used in the original experiments (e.g., QQP or CoLA) to assess generalization beyond the five tasks tested.