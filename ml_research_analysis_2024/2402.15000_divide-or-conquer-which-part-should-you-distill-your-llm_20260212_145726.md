---
ver: rpa2
title: Divide-or-Conquer? Which Part Should You Distill Your LLM?
arxiv_id: '2402.15000'
source_url: https://arxiv.org/abs/2402.15000
tags:
- decomposition
- arxiv
- solving
- question
- subquestions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how to efficiently distill reasoning capabilities
  from large language models (LLMs) by separating the problem decomposition and problem
  solving phases. The authors propose to distill only the decomposition capability
  into smaller models, hypothesizing that this is easier and more generalizable than
  distilling the knowledge-intensive solving capability.
---

# Divide-or-Conquer? Which Part Should You Distill Your LLM?

## Quick Facts
- arXiv ID: 2402.15000
- Source URL: https://arxiv.org/abs/2402.15000
- Authors: Zhuofeng Wu; He Bai; Aonan Zhang; Jiatao Gu; VG Vinod Vydiswaran; Navdeep Jaitly; Yizhe Zhang
- Reference count: 16
- Primary result: Distilling problem decomposition capability outperforms distilling problem solving capability for efficient LLM reasoning

## Executive Summary
This paper investigates how to efficiently distill reasoning capabilities from large language models (LLMs) by separating the problem decomposition and problem solving phases. The authors propose to distill only the decomposition capability into smaller models, hypothesizing that this is easier and more generalizable than distilling the knowledge-intensive solving capability. Experiments on mathematical reasoning, QA, and compositional tasks show that distilling the decomposition model (SD-T) achieves performance comparable to using GPT-3.5-turbo as decomposer while significantly reducing inference costs. The distilled decomposer also demonstrates good generalization across tasks, datasets, and solver models. In contrast, distilling the solving capability (SE-T/SE-A) results in substantial performance drops and poor generalization.

## Method Summary
The paper proposes a two-stage approach for reasoning tasks: first decomposing complex problems into simpler subquestions, then solving each subquestion. To create a cost-efficient inference pipeline, the authors distill the decomposition capability from a large teacher model (GPT-3.5-turbo) into a smaller student model (Vicuna-13B/Mistral-7B) using cross-entropy loss. The distilled decomposer generates subquestions that are then answered by the solver model. The approach can optionally include screening of subquestions using ground-truth answers to improve quality. The key insight is that decomposition requires semantic understanding and general problem-solving strategies rather than domain-specific knowledge, making it easier to distill and more generalizable than the solving capability.

## Key Results
- Distilling decomposition capability (SD-T) achieves performance comparable to GPT-3.5-turbo while reducing inference costs
- Distilled decomposition models generalize well across tasks, datasets, and solver models
- Distilling solving capability (SE-T/SE-A) results in substantial performance drops and poor generalization
- Static decomposition strategy provides better separation and cost efficiency compared to dynamic decomposition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing complex reasoning tasks into two stages (decomposition + solving) outperforms single-stage approaches
- Mechanism: The two-stage approach separates semantic understanding/parsing (decomposition) from knowledge-intensive execution (solving), allowing each capability to be optimized independently
- Core assumption: Decomposition and solving are sufficiently decoupled that separating them doesn't harm performance
- Evidence anchors:
  - [abstract] "we devise a similar strategy that breaks down reasoning tasks into a problem decomposition phase and a problem solving phase and show that the strategy is able to outperform a single stage solution"
  - [section 2] "Table 1 (Single-stage GPT/Vicuna vs Two-stage GPT/Vicuna), shows that in general such a static strategy leads to performance gains over a Single-stage approach"
- Break condition: Tasks where decomposition and solving are highly interdependent (interactive planning needed)

### Mechanism 2
- Claim: Decomposition capability is easier to distill than solving capability because it's less knowledge-intensive
- Mechanism: Decomposition primarily requires semantic understanding and query parsing, while solving requires memorizing vast amounts of domain knowledge from training data
- Core assumption: Abstract, logical decomposition skills are more compressible than knowledge-intensive solving skills
- Evidence anchors:
  - [abstract] "the latter requires large amounts of domain knowledge while the former only requires learning general problem solving strategies"
  - [section 5.2] "distilling the decomposer still yields better performance comparing with finetuning the solver"
- Break condition: When decomposition itself requires significant domain knowledge (e.g., highly specialized technical domains)

### Mechanism 3
- Claim: Distilled decomposition models generalize better across tasks, datasets, and solver models than distilled solving models
- Mechanism: Decomposition can be abstracted into symbolic principles making it more universally applicable, while solving is more task-specific
- Core assumption: Symbolic decomposition principles transfer better than knowledge-intensive solving patterns
- Evidence anchors:
  - [abstract] "the distilled query decomposition model exhibits good generalization across tasks, datasets, and models"
  - [section 5.3] "the distilled decomposer SD-R demonstrates good generalization and versatility to the other domain" vs "when substituting the solver with SE-A... the generalization to the other domain is poor"
- Break condition: When task decomposition patterns are domain-specific rather than symbolic

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: Understanding the baseline approach that the paper improves upon
  - Quick check question: What is the difference between single-stage CoT and the proposed two-stage decomposition approach?

- Concept: Knowledge distillation in language models
  - Why needed here: The core technique being applied to transfer decomposition capabilities
  - Quick check question: How does knowledge distillation differ when applied to decomposition vs solving capabilities?

- Concept: Cross-entropy loss optimization for sequence generation
  - Why needed here: The training objective used for distilling the decomposition model
  - Quick check question: Why is cross-entropy loss appropriate for distilling question decomposition capabilities?

## Architecture Onboarding

- Component map: Teacher model (GPT-3.5-turbo) -> Student decomposer (Vicuna-13B/Mistral-7B) -> Solver model (Vicuna-13B/GPT-3.5-turbo)
- Critical path: Decomposition generation → Subquestion screening (optional) → Student decomposer training → Inference pipeline deployment
- Design tradeoffs:
  - Static vs dynamic decomposition: Static offers better separation and cost efficiency but may miss context-dependent decompositions
  - With vs without oracle answers: Screening improves quality but reduces training data volume
  - Decomposing vs solving distillation: Decomposing is easier and more generalizable but requires a separate solver
- Failure signatures:
  - Performance degradation when switching from teacher to student decomposer indicates distillation issues
  - Poor generalization to new domains suggests decomposition patterns aren't sufficiently abstract
  - Solver confusion when subquestions are poorly formulated
- First 3 experiments:
  1. Implement single-stage vs two-stage comparison on GSM8K to verify baseline improvement
  2. Train student decomposer on DROP dataset and evaluate on held-out test set
  3. Cross-domain evaluation: Train on GSM8K, test on DROP (and vice versa) to measure generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of distilled problem decomposition models compare when trained on data from multiple tasks versus a single task?
- Basis in paper: Explicit - The paper mentions "it would be interesting to train universal decomposer models using data from various tasks" as future work
- Why unresolved: The paper only evaluates cross-domain generalization by training on one dataset and testing on another, not on multi-task training
- What evidence would resolve it: Experiments comparing single-task vs multi-task training of decomposition models across various combinations of tasks

### Open Question 2
- Question: What is the impact of different reasoning chain lengths on the effectiveness of decomposition vs solving capability distillation?
- Basis in paper: Inferred - The paper discusses decomposition being easier to distill due to lower information density, but doesn't examine how chain length affects this
- Why unresolved: The paper doesn't systematically vary reasoning chain length in its experiments
- What evidence would resolve it: Controlled experiments varying reasoning chain length while measuring distillation effectiveness for both capabilities

### Open Question 3
- Question: How does the effectiveness of static vs dynamic decomposition strategies vary across different types of reasoning tasks?
- Basis in paper: Explicit - The paper acknowledges "dynamic decomposition and its advantages in specific application scenarios" but only tests static approaches
- Why unresolved: The paper only compares static and dynamic approaches on a limited set of math and QA tasks
- What evidence would resolve it: Systematic comparison of static vs dynamic decomposition across diverse reasoning task types including planning, tool use, and multi-turn scenarios

## Limitations
- Experimental validation based on relatively few datasets (3) and task types
- Performance gains from static decomposition evaluated primarily through inference cost savings rather than qualitative assessment
- Screening mechanism using ground-truth answers may not be practically applicable in real-world scenarios
- Generalization benefits could be domain-dependent and need broader validation

## Confidence

- **High Confidence**: The empirical finding that distilling decomposition capability (SD-T) maintains performance while reducing costs compared to using GPT-3.5-turbo as decomposer. This is directly demonstrated across all three datasets with clear quantitative metrics.
- **Medium Confidence**: The claim that distilled decomposition generalizes better across tasks, datasets, and solver models. While supported by cross-domain experiments, the evidence is limited to three task types and the generalization benefits could be domain-dependent.
- **Medium Confidence**: The assertion that decomposition is easier to distill than solving because it's less knowledge-intensive. The reasoning is sound, but direct comparative analysis of distillation difficulty (e.g., training convergence rates, sample efficiency) is not provided.

## Next Checks

1. **Multi-Domain Generalization Test**: Evaluate the distilled decomposer (SD-T) on at least 3-5 additional reasoning task types (e.g., logical reasoning, code generation, scientific question answering) that were not seen during training to rigorously test the generalization claim.

2. **Dynamic vs Static Decomposition Comparison**: Implement a dynamic decomposition baseline where the teacher model generates decompositions conditioned on the specific question context, then compare both qualitative quality of subquestions and quantitative performance against the static approach.

3. **Cross-Architecture Solver Validation**: Test the distilled decomposer with multiple solver architectures beyond Vicuna-13B and GPT-3.5-turbo, including smaller open-source models (e.g., Llama-7B, Qwen-7B) to verify the claim that decomposition generalizes across different solver models.