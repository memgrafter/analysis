---
ver: rpa2
title: Adaptive Conformal Inference by Betting
arxiv_id: '2412.19318'
source_url: https://arxiv.org/abs/2412.19318
tags:
- conformal
- learning
- coverage
- sf-ogd
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adaptive conformal inference
  in non-stationary environments where exchangeability assumptions fail. The authors
  propose a novel approach based on parameter-free online convex optimization techniques,
  specifically coin betting strategies, to learn prediction set radii without requiring
  manual tuning of learning rates.
---

# Adaptive Conformal Inference by Betting

## Quick Facts
- arXiv ID: 2412.19318
- Source URL: https://arxiv.org/abs/2412.19318
- Reference count: 40
- Key outcome: Novel parameter-free online conformal inference method using coin betting strategies achieves provable coverage guarantees without learning rate tuning

## Executive Summary
This paper introduces a novel approach to adaptive conformal inference for non-stationary data streams where exchangeability assumptions fail. The method leverages parameter-free online convex optimization techniques, specifically coin betting strategies like the Krichevsky-Trofimov estimator, to learn prediction set radii without requiring manual learning rate tuning. The approach provably controls long-term miscoverage frequency at a pre-specified level α while demonstrating strong empirical performance across multiple datasets.

## Method Summary
The authors propose adaptive conformal predictors based on coin betting strategies that update prediction set radii using parameter-free online convex optimization. The method frames conformal inference as online quantile estimation, where the radius is updated based on the wealth from betting on the subgradient of the pinball loss. Unlike existing methods that require careful learning rate tuning, this approach uses the Krichevsky-Trofimov betting strategy to achieve adaptive coverage without parameter sensitivity. The theoretical framework ensures long-term coverage guarantees under bounded nonconformity scores.

## Key Results
- Betting-based conformal predictors achieve comparable or better empirical performance than tuned online gradient descent methods
- Method provably controls long-term miscoverage frequency at nominal level α without learning rate tuning
- Avoids sensitivity to learning rate choices that plagues existing online gradient descent methods while maintaining coverage and interval width properties

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The betting-based approach achieves parameter-free online convex optimization for conformal inference
- Mechanism: Uses coin betting strategies (Krichevsky-Trofimov estimator) to update prediction set radii without requiring pre-specified learning rates
- Core assumption: The nonconformity scores (St) are bounded by some constant D > 0
- Evidence anchors: [abstract] "leverages parameter-free online convex optimization techniques" [section] "we utilize optimization techniques that are based on coin betting"

### Mechanism 2
- Claim: The method provably controls long-term miscoverage frequency at the nominal level α
- Mechanism: The boundedness of the KT iterates is established through a careful analysis showing that if coverage deviates from α, the betting fraction would cause unbounded growth
- Core assumption: The nonconformity scores are bounded: St ∈ [0, D] for all t
- Evidence anchors: [abstract] "We prove that our method controls long-term miscoverage frequency at a nominal level"

### Mechanism 3
- Claim: The betting-based approach achieves comparable or better empirical performance than tuned online gradient descent methods
- Mechanism: By avoiding the need for learning rate tuning, the method achieves adaptive coverage without the sensitivity issues that plague OGD-based approaches
- Core assumption: The underlying forecasting model provides reasonable point predictions
- Evidence anchors: [abstract] "demonstrate its convincing empirical performance without any need of performing cumbersome parameter tuning"

## Foundational Learning

- Concept: Pinball loss and quantile estimation
  - Why needed here: The conformal inference problem is framed as online quantile estimation of the nonconformity scores
  - Quick check question: What is the subgradient of the pinball loss ℓβ(s, St) when s ≠ St?

- Concept: Online convex optimization and regret bounds
  - Why needed here: The method is analyzed through the lens of online convex optimization, with regret bounds ensuring sublinear cumulative loss
  - Quick check question: How does the regret bound for KT betting compare to that of online gradient descent with optimal learning rate?

- Concept: Coin betting and wealth dynamics
  - Why needed here: The betting strategies are the core mechanism for updating the prediction set radii without learning rates
  - Quick check question: What is the relationship between the betting fraction λt and the subgradient gt in Algorithm 1?

## Architecture Onboarding

- Component map:
  Forecasting model -> Point predictions ˆYt
  Nonconformity score computation -> St = |Yt - ˆYt|
  Pinball loss subgradient -> gt ∈ ∂ℓ1-α(s, St)|s=st
  Betting fraction update -> λt+1 based on KT formula
  Wealth update -> Wt = Wt-1 - gt·st
  Radius prediction -> st+1 = λt+1·Wt
  Prediction set output -> ˆCt(st) = [ˆYt - st, ˆYt + st]

- Critical path: Forecast → Nonconformity score → Subgradient → Betting fraction → Wealth → Radius → Prediction set

- Design tradeoffs:
  - Parameter-free vs. potentially sub-optimal regret (logarithmic factors)
  - Simplicity of implementation vs. potentially slower adaptation than tuned methods
  - Theoretical coverage guarantee vs. finite-sample coverage may be slightly below nominal level

- Failure signatures:
  - Coverage consistently below nominal level (indicates boundedness assumption may be violated)
  - Radius becoming zero or negative (indicates betting fraction becoming extreme)
  - Very wide prediction sets (indicates poor underlying forecasts or extreme nonconformity)

- First 3 experiments:
  1. Implement the KT-based Algorithm 1 on a simple synthetic dataset with known bounded nonconformity scores to verify coverage convergence
  2. Compare the betting-based method against OGD with different learning rates on a changepoint dataset to observe sensitivity to parameter tuning
  3. Test the method on a real-world time series dataset (like electricity demand) to evaluate practical performance and width characteristics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal betting strategy (KT vs ONS) for different types of non-stationary data distributions?
- Basis in paper: [explicit] The paper compares KT and ONS betting strategies in experiments but doesn't provide theoretical guidance on when one might outperform the other
- Why unresolved: The paper shows both methods work well empirically but doesn't establish conditions under which one would be superior to the other

### Open Question 2
- Question: How does the betting-based conformal predictor perform under extreme distribution shifts where the model accuracy drops significantly?
- Basis in paper: [inferred] The paper demonstrates coverage maintenance but doesn't explicitly test performance under severe model degradation scenarios
- Why unresolved: The experiments focus on moderate changes in data distribution, but don't push the boundaries to examine failure modes

### Open Question 3
- Question: Can the betting-based approach be extended to handle multivariate prediction sets or structured output spaces?
- Basis in paper: [explicit] The paper focuses on univariate prediction intervals but mentions that the approach "can be used to recalibrate prediction intervals based on conditional quantile regression models"
- Why unresolved: The paper doesn't explore how the betting mechanism would work with multivariate or structured outputs

## Limitations

- The theoretical coverage guarantee critically depends on the boundedness assumption for nonconformity scores, which may be violated in practice
- The comparison to OGD methods focuses on sensitivity to learning rate tuning but doesn't explore other OGD variants that might perform better with careful tuning
- The computational complexity of the betting-based methods versus simple fixed-radius approaches is not discussed

## Confidence

- Mechanism 1 (Parameter-free optimization): High confidence - well-established theoretical foundations in online learning literature
- Mechanism 2 (Coverage guarantee): Medium confidence - proof relies on boundedness assumption that requires empirical validation
- Mechanism 3 (Empirical performance): Medium confidence - results shown but limited to specific datasets and compared primarily against OGD variants

## Next Checks

1. Empirically test the boundedness assumption on real datasets by analyzing the distribution of nonconformity scores and their tails
2. Compare against adaptive conformal methods using different base online learning algorithms (beyond OGD) including parameter-free alternatives
3. Evaluate computational overhead and scalability of betting-based methods versus simpler approaches on large-scale streaming datasets