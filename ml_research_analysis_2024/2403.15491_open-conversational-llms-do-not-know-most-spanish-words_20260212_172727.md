---
ver: rpa2
title: Open Conversational LLMs do not know most Spanish words
arxiv_id: '2403.15491'
source_url: https://arxiv.org/abs/2403.15491
tags:
- words
- llms
- spanish
- word
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors evaluated the knowledge of open conversational LLMs
  regarding Spanish vocabulary. They tested 12 models with varying sizes and architectures
  on 100 randomly selected Spanish words from a reference dictionary.
---

# Open Conversational LLMs do not know most Spanish words

## Quick Facts
- arXiv ID: 2403.15491
- Source URL: https://arxiv.org/abs/2403.15491
- Reference count: 11
- Primary result: Most open conversational LLMs recognize less than half of randomly selected Spanish words and cannot use them correctly in context

## Executive Summary
This study evaluates the lexical knowledge of 12 open conversational LLMs in Spanish by testing their ability to recognize and use 100 randomly selected Spanish words from a reference dictionary. The results show that most models fail to recognize over half of the words and cannot use them meaningfully in sentences, with larger models generally performing better. The study also reveals that adaptation to Spanish does not significantly improve performance, and that automated evaluation methods using "Yes/No" prompts or ChatGPT checks are unreliable due to high error rates.

## Method Summary
The researchers evaluated 12 conversational LLMs (Llama-2, Mistral, Yi, Gemma, Solar, Bloomz, Flor, Bertin) with varying sizes (7B to 70B parameters) on their knowledge of 100 randomly selected Spanish words from the "Diccionario del español actual" (DEA). They used two manual evaluation prompts: one asking for word definitions and another asking for contextual sentence usage. Results were compared against three binary "Yes/No" prompts and automated checks using ChatGPT to assess the reliability of automated evaluation methods.

## Key Results
- Most models recognized less than 50% of tested Spanish words
- Larger models (70B parameters) performed significantly better than smaller ones (7B parameters)
- Spanish-adapted models did not outperform general multilingual models of the same size
- Automated "Yes/No" prompts were unreliable for evaluation, with accuracy below 70% for most models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger LLMs have better lexical knowledge of Spanish words.
- Mechanism: Increased parameter count allows the model to encode and retrieve a richer set of lexical representations, especially for less frequent or morphologically complex words.
- Core assumption: Model capacity directly translates into improved vocabulary recall and contextual usage accuracy.
- Evidence anchors:
  - [section] "Observation 3: Performance improves with model size. Both meaning and usage increase with model size for Llama, Mistral, and Yi, suggesting that larger models can better handle the lexicon."
  - [section] Results show a positive correlation between model size and performance in both word meaning and contextual usage.
- Break condition: If the training data for Spanish is too sparse, larger models may not gain proportional improvement, and the benefit plateaus or becomes negligible.

### Mechanism 2
- Claim: English-dominant training data leads to poor Spanish lexical performance.
- Mechanism: Models trained predominantly on English text internalize English linguistic patterns, which interfere with correct Spanish word usage, especially in morphology and agreement.
- Evidence anchors:
  - [section] "3. Structures are heavily influenced by English, which results in unusual constructions ('finalmente fue completado' [finally completed], 'socialmente responsable' [socially responsible], 'era frecuentemente invitado' [was frequently invited]...)."
  - [section] "2. Grammatical mistakes are rare, but when these occur, they are related exclusively to agreements (masculine/feminine, singular/plural: 'la olor' ['la' is feminine and 'olor' is masculine], 'que no se pierdan nadie' ['pierdan' is the verb form for plural, but for 'nadie' the singular form should be used], 'los sílabas' ['los' is masculine and 'sílabas' is feminine], 'el yema' ['el' is masculine and 'yema' is feminine]...)."
- Break condition: If a model is specifically fine-tuned or adapted for Spanish with sufficient high-quality data, this interference can be mitigated.

### Mechanism 3
- Claim: Manual evaluation is necessary because automated binary prompts are unreliable for lexical knowledge assessment.
- Mechanism: LLMs often produce false positives/negatives in simple "Yes/No" responses due to hallucinations or lack of self-awareness, making automated scoring inaccurate.
- Evidence anchors:
  - [section] "The accuracy for the ChatGPT-based checks is better, specially for check 2 that reaches in some cases 90%. However, even for these checks accuracy is below 70% for some models."
  - [section] "The 'Yes/No' prompts have poor accuracy for most models. In fact, the models that have the highest accuracy, for example Gemma with 80% on prompts 1 to 3, answer 'No' for all words which provides no useful information on the knowledge of words."
- Break condition: If automated evaluation techniques improve to account for model uncertainty and context, manual evaluation may become unnecessary.

## Foundational Learning

- Concept: Vocabulary frequency distribution in training data.
  - Why needed here: To understand why certain Spanish words are not recognized by models despite being present in the dictionary.
  - Quick check question: If a word has a frequency of 10^-6 in a corpus, how many times would it appear in a 1 billion word training set?

- Concept: Language interference in multilingual models.
  - Why needed here: To explain why models produce English-influenced Spanish sentences and incorrect grammatical agreements.
  - Quick check question: What linguistic feature is most affected when a model trained on English processes Spanish verbs?

- Concept: Lexical competence assessment methods.
  - Why needed here: To design effective evaluation prompts that test both word meaning and contextual usage.
  - Quick check question: What is the difference between testing word recognition and testing productive use in context?

## Architecture Onboarding

- Component map:
  - Data ingestion → Tokenization → Embedding layer → Transformer blocks → Output layer → Prompt interpretation
  - Evaluation pipeline: Prompt generation → Model inference → Response parsing → Manual/automated scoring

- Critical path:
  1. Load model with appropriate quantization.
  2. Generate deterministic responses using fixed seeds.
  3. Parse responses for meaning and usage.
  4. Compare against ground truth dictionary.
  5. Aggregate results by model and metric.

- Design tradeoffs:
  - Precision vs. memory: Lower bit quantization reduces memory usage but may degrade lexical accuracy.
  - Prompt complexity vs. evaluation speed: More detailed prompts improve assessment quality but increase processing time.
  - Manual vs. automated evaluation: Manual is accurate but slow; automated is fast but unreliable with current methods.

- Failure signatures:
  - High rate of "No" responses across all words suggests model defaulting to uncertainty.
  - English word insertions in Spanish responses indicate language interference.
  - Morphological errors in agreements point to insufficient Spanish grammar modeling.

- First 3 experiments:
  1. Test a small subset of high-frequency Spanish words across multiple models to establish baseline accuracy.
  2. Vary prompt phrasing (e.g., add context or examples) to see if response quality improves.
  3. Compare performance of multilingual vs. Spanish-adapted models on the same word set to quantify adaptation benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the actual knowledge of Spanish words by ChatGPT or other commercial LLMs compared to open-source models?
- Basis in paper: [explicit] The paper focuses on open-source models but mentions that commercial models like GPT-4 show similar performance for English and Spanish on MMLU, suggesting potential differences in lexical knowledge.
- Why unresolved: The paper does not directly compare open-source models with commercial ones like ChatGPT on Spanish vocabulary.
- What evidence would resolve it: Conduct a similar evaluation using ChatGPT and other commercial models on the same Spanish word set to compare performance directly.

### Open Question 2
- Question: How does the lexical knowledge of Spanish in open-source LLMs change with larger training datasets specifically focused on Spanish?
- Basis in paper: [inferred] The paper notes that larger models generally perform better but also mentions that adaptation to Spanish does not improve performance, suggesting the training data composition is crucial.
- Why unresolved: The paper evaluates existing models but does not experiment with training models on different proportions of Spanish data.
- What evidence would resolve it: Train open-source LLMs with varying proportions of Spanish data and evaluate their lexical knowledge using the same methodology.

### Open Question 3
- Question: Can prompt engineering techniques like chain-of-thought or few-shot learning improve the lexical knowledge of Spanish in open-source LLMs?
- Basis in paper: [explicit] The paper mentions that more advanced prompting techniques like chain of thoughts would be of interest, especially when the models are asked about their knowledge of words.
- Why unresolved: The paper only tests simple prompts and does not explore advanced prompting techniques.
- What evidence would resolve it: Apply chain-of-thought and few-shot learning prompts to the same Spanish word set and compare results with the basic prompts used in the paper.

### Open Question 4
- Question: What is the impact of fine-tuning open-source LLMs specifically on Spanish lexical knowledge versus general language adaptation?
- Basis in paper: [explicit] The paper evaluates models adapted for Spanish but finds they do not outperform general models of the same size, raising questions about the effectiveness of current adaptation methods.
- Why unresolved: The paper tests existing adapted models but does not explore different fine-tuning strategies focused on lexical knowledge.
- What evidence would resolve it: Fine-tune open-source models using different strategies (e.g., focusing on vocabulary, context, both) and evaluate their performance on Spanish lexical knowledge.

## Limitations

- Manual evaluation requirement makes large-scale validation computationally expensive and potentially subject to inter-rater variability
- Vocabulary test set may not represent the distribution of words encountered in actual conversational contexts
- Study doesn't account for potential domain-specific vocabulary or colloquial terms relevant for conversational AI applications

## Confidence

- **High Confidence**: The core finding that larger models perform better on Spanish lexical tasks is well-supported by the data showing consistent improvements across multiple model families (Llama, Mistral, Yi).
- **Medium Confidence**: The claim that adaptation to Spanish doesn't improve performance is based on limited comparisons and could benefit from more extensive testing across different adaptation strategies.
- **Low Confidence**: The specific performance thresholds and the precise impact of model size are difficult to verify without access to the exact evaluation prompts and methodology details.

## Next Checks

1. **Prompt Format Sensitivity Test**: Systematically vary prompt structure (add examples, change phrasing, include context) for a subset of models and words to quantify how much evaluation methodology affects measured performance, isolating whether improvements come from better model knowledge or better elicitation.

2. **Multilingual Interference Analysis**: Design targeted prompts that specifically test for English-to-Spanish interference patterns (e.g., asking for translations, then back-translations, or comparative constructions) to quantify the exact mechanisms and frequency of cross-linguistic influence in model outputs.

3. **Automated Evaluation Calibration**: Develop and test a calibration framework for automated evaluation by creating a small gold-standard dataset with human-annotated responses, then training a lightweight classifier to identify reliable vs. unreliable model responses, potentially enabling scalable assessment while maintaining accuracy.