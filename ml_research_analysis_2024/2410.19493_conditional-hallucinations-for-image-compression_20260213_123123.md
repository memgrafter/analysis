---
ver: rpa2
title: Conditional Hallucinations for Image Compression
arxiv_id: '2410.19493'
source_url: https://arxiv.org/abs/2410.19493
tags:
- image
- compression
- hific
- distortion
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel image compression method that dynamically
  balances the degree of hallucination based on image content. The key idea is to
  predict user preferences on hallucinations using a trained classifier and adjust
  the perceptual weight in the reconstruction loss accordingly.
---

# Conditional Hallucinations for Image Compression

## Quick Facts
- arXiv ID: 2410.19493
- Source URL: https://arxiv.org/abs/2410.19493
- Authors: Till Aczel; Roger Wattenhofer
- Reference count: 40
- One-line primary result: ConHa outperforms state-of-the-art compression models by dynamically adjusting hallucination levels based on image content, achieving higher user-preferred perceptual quality across all bitrates.

## Executive Summary
This paper introduces a novel image compression method that dynamically balances the degree of hallucination based on image content. The key idea is to predict user preferences on hallucinations using a trained classifier and adjust the perceptual weight in the reconstruction loss accordingly. This approach results in a Conditionally Hallucinating compression model (ConHa) that outperforms state-of-the-art image compression methods. The primary result is that ConHa consistently outperforms the previous state-of-the-art compression model, HiFiC, across all bitrates in a user study, as evidenced by higher bootstrapped Elo Scores.

## Method Summary
The method trains a preference model on DIV2K dataset pairwise comparisons to predict whether an image benefits more from hallucinated details or exact reconstruction. This prediction (w) scales the perceptual loss term during compression model training. The compression model uses a two-stage training procedure: first optimizing rate, MSE, and LPIPS for 1M iterations, then continuing with GAN component incorporation for another 1M iterations. The model is evaluated against baselines using both user studies and computational metrics on the CLIC 2024 test set.

## Key Results
- ConHa consistently outperforms HiFiC across all bitrates in user studies with higher bootstrapped Elo Scores
- At low bitrates, ConHa outperforms both Hyperprior and HiFiC baselines
- At medium and high bitrates, ConHa outperforms HiFiC alone
- Computational metrics fail to capture the performance gap between ConHa and other baselines as indicated by user studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The conditional hallucination model achieves better perceptual quality by dynamically adjusting the GAN discriminator weight based on image content.
- Mechanism: The model uses a trained preference model to predict whether an image benefits more from hallucinated details or exact reconstruction. This prediction scales the perceptual loss term, allowing emphasis on realism for texture-like content and fidelity for text/edges.
- Core assumption: A single scalar weight (w) is sufficient to capture the optimal hallucination level for an entire image, and this can be predicted from the uncompressed image.
- Evidence anchors: [abstract] "By using this prediction to adjust the perceptual weight in the reconstruction loss, we develop a Conditionally Hallucinating compression model (ConHa) that outperforms state-of-the-art image compression methods."
- Break condition: If the preference model's predictions do not correlate well with actual user preferences, or if the image context is too limited to make accurate predictions.

### Mechanism 2
- Claim: The two-stage training process allows the model to first learn basic reconstruction before optimizing for perceptual quality.
- Mechanism: The compression model is first trained with rate, MSE, and LPIPS losses for 1 million iterations, then continues with GAN component incorporation for another million iterations. This staged approach stabilizes training and prevents early collapse to unrealistic hallucinations.
- Core assumption: Separating the perceptual and GAN optimization into stages improves convergence compared to joint optimization from the start.
- Evidence anchors: [section 3.2] "We adopt the HiFiC approach [1] for our compression model training... First, we train the model for 1 million iterations with the rate, MSE, and LPIPS losses. Then we continue training for another 1 million iterations with the GAN component incorporated."
- Break condition: If the two-stage approach leads to mode collapse or fails to capture fine-grained perceptual details that could be learned jointly.

### Mechanism 3
- Claim: User studies provide more reliable evaluation than computational metrics for perceptual quality in image compression.
- Mechanism: The paper conducts a 2AFC user study with 40 participants, collecting 1531 comparisons. Bootstrapped Elo scores are used to aggregate results, which show ConHa outperforming HiFiC across all bitrates.
- Core assumption: Human perceptual preferences are the ultimate ground truth for evaluating compression quality, and computational metrics like PSNR, MS-SSIM, LPIPS, and FID are insufficient proxies.
- Evidence anchors: [section 4.2] "Computational distortion metrics often fail to predict human preferences accurately, highlighting the need for user studies... At all bitrates, our model consistently outperforms HiFiC, the previous state-of-the-art compression model."
- Break condition: If the user study sample size or methodology introduces bias, or if computational metrics improve significantly in correlating with human perception.

## Foundational Learning

- Concept: Rate-distortion-perception trade-off in learned image compression
  - Why needed here: The paper extends the standard rate-distortion trade-off by incorporating a perceptual term controlled by a GAN discriminator. Understanding this framework is essential to grasp how ConHa modifies the loss function.
  - Quick check question: What is the difference between optimizing for rate-distortion versus rate-distortion-perception in image compression?

- Concept: Variational Autoencoders (VAEs) and latent quantization for compression
  - Why needed here: The compression model uses an autoencoder architecture with learned quantization, similar to VAEs. Understanding how this differs from traditional codecs is crucial for implementing the model.
  - Quick check question: How does learned quantization in VAEs enable compression, and why is it necessary for this application?

- Concept: Generative Adversarial Networks (GANs) and their role in perceptual optimization
  - Why needed here: The GAN discriminator ensures the compressed images remain in-distribution, preventing out-of-distribution artifacts. Understanding GAN training dynamics is important for the two-stage training process.
  - Quick check question: What is the role of the GAN discriminator in the rate-distortion-perception loss, and how does it interact with the generator during training?

## Architecture Onboarding

- Component map:
  - Input image → Encoder → Quantizer → Generator → Output image
  - Preference Model (MP) predicts hallucination weight (w)
  - GAN Discriminator (D) ensures in-distribution outputs
  - Loss components: Rate (r), Distortion (d), Perceptual (dLPIPS), GAN (log(D))

- Critical path: Input image → Encoder → Quantizer → Generator → Output image
  - The preference model runs once per image during training to determine weight w
  - The GAN discriminator is trained alternately with the generator

- Design tradeoffs:
  - Fixed vs. conditional perceptual weight: ConHa-fixed uses average w across dataset, while ConHa adjusts per image
  - Two-stage vs. joint training: Separates basic reconstruction from perceptual optimization
  - Preference model complexity: Uses ResNet50 features vs. lighter alternatives for speed

- Failure signatures:
  - If w predictions are consistently wrong, the model may hallucinate inappropriately
  - If GAN training is unstable, the model may generate unrealistic artifacts
  - If preference model overfits to DIV2K, performance may degrade on other datasets

- First 3 experiments:
  1. Train ConHa-fixed (fixed w) and compare to ConHa (conditional w) to validate the importance of content-awareness
  2. Test ablation: Remove GAN discriminator to see impact on perceptual quality vs. hallucinations
  3. Evaluate preference model accuracy by testing its predictions against human judgments on held-out data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more accurate distortion metrics that align with human perception and capture subtle differences in compressed images?
- Basis in paper: [explicit] The paper states that "Distortion metrics often fail to accurately capture human perception" and that existing metrics do not align with human evaluations in user studies.
- Why unresolved: Current metrics like PSNR and MS-SSIM often poorly align with user studies and can even negatively correlate with human preferences among the best-performing models. The paper highlights that automated metrics do not capture the performance gap between ConHa and other baselines as indicated by user studies.
- What evidence would resolve it: Development of new distortion metrics that consistently correlate with human preferences in user studies across various image types and compression scenarios. Validation through extensive user studies comparing these new metrics against human judgments.

### Open Question 2
- Question: Can the conditional hallucination approach be extended to other domains beyond image compression, such as video or 3D data compression?
- Basis in paper: [inferred] The paper focuses on image compression but discusses the concept of balancing distortion and perception based on content, which could be applicable to other types of data.
- Why unresolved: The paper only demonstrates the effectiveness of the approach for image compression. Extending this to other domains would require adapting the methodology to handle different data structures and perceptual qualities specific to those domains.
- What evidence would resolve it: Successful implementation and user study validation of the conditional hallucination approach in video compression and 3D data compression, showing improved perceptual quality and user preferences compared to existing methods.

### Open Question 3
- Question: How does the conditional hallucination model perform in real-time or resource-constrained environments where computational efficiency is crucial?
- Basis in paper: [inferred] The paper does not address the computational