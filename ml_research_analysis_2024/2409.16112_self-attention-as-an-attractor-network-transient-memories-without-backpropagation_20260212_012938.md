---
ver: rpa2
title: 'Self-attention as an attractor network: transient memories without backpropagation'
arxiv_id: '2409.16112'
source_url: https://arxiv.org/abs/2409.16112
tags:
- transformer
- training
- block
- bare
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel interpretation of self-attention mechanisms
  in transformers as an attractor network with local energy functions, offering a
  new perspective on how transformers process information. The authors show that self-attention
  can be expressed as the derivative of local energy terms resembling pseudo-likelihood,
  allowing them to design a recurrent model that can be trained without backpropagation.
---

# Self-attention as an attractor network: transient memories without backpropagation

## Quick Facts
- **arXiv ID**: 2409.16112
- **Source URL**: https://arxiv.org/abs/2409.16112
- **Reference count**: 26
- **Key outcome**: Self-attention interpreted as attractor network with local energy functions, enabling training without backpropagation

## Executive Summary
This paper presents a novel theoretical framework interpreting self-attention mechanisms in transformers as attractor networks with local energy functions. The authors demonstrate that self-attention can be reformulated as the derivative of local energy terms resembling pseudo-likelihood, enabling a recurrent model architecture that operates without backpropagation during inference. Through experiments on MNIST, the model shows competitive performance with transformer blocks on masked prediction and denoising tasks while maintaining a simpler architecture. The work provides new insights into transformer dynamics by connecting them to physics-inspired concepts of attractor networks, suggesting that test examples can be stored as transient attractors in the system's dynamics.

## Method Summary
The authors reinterpret self-attention as an attractor network by expressing it through local energy functions that resemble pseudo-likelihood terms. This reformulation allows the construction of a recurrent neural network where the dynamics are governed by gradient descent on these local energy functions. The model iteratively updates its hidden states based on the self-attention mechanism until convergence to stable attractors. Training is performed using contrastive methods that do not require backpropagation through time, instead optimizing the energy landscape to ensure training examples become attractors. The architecture maintains the core self-attention mechanism but embeds it within a recurrent framework that naturally supports transient memory states and local energy minimization.

## Key Results
- Self-attention reformulated as gradient descent on local energy functions resembling pseudo-likelihood
- Model achieves competitive performance with transformer blocks on MNIST masked prediction and denoising tasks
- Test examples demonstrate transient states strongly correlated with both training and test patterns, suggesting attractor storage

## Why This Works (Mechanism)
The mechanism works by interpreting self-attention as performing gradient descent on a local energy landscape. Each attention head computes a local pseudo-likelihood energy term, and the overall attention computation becomes the sum of gradients from these local energies. This creates an energy-based recurrent dynamics where hidden states evolve toward stable attractors. The contrastive training procedure shapes the energy landscape so that training examples become low-energy states (attractors) while maintaining separation between different patterns. The transient nature of the memories emerges naturally from the iterative dynamics - patterns are stored as attractors that the system converges to during processing, with the attention mechanism serving as the gradient operator that drives this convergence.

## Foundational Learning
- **Attractor Networks**: Dynamical systems where states converge to stable patterns - needed to understand how memories can be stored as fixed points in recurrent dynamics; quick check: verify that the proposed energy function has stable fixed points corresponding to stored patterns.
- **Energy-based Models**: Models where probability distributions are defined through an energy function - needed to grasp how the local energy formulation relates to probabilistic interpretation of attention; quick check: confirm that the pseudo-likelihood form maintains proper normalization properties.
- **Pseudo-likelihood Approximation**: Technique for approximating joint distributions using conditional probabilities - needed to understand the mathematical form of the local energy terms; quick check: verify that the approximation becomes exact in the limit of the model's assumptions.
- **Contrastive Divergence**: Training method for energy-based models using positive and negative samples - needed to understand the backpropagation-free training approach; quick check: ensure that the contrastive objective properly shapes the energy landscape.
- **Recurrent Neural Networks**: Networks with cyclic connections enabling temporal dynamics - needed to understand the iterative nature of the proposed architecture; quick check: verify that the recurrent updates maintain stability and convergence.
- **Self-attention Mechanism**: Core transformer component computing weighted representations - needed as the foundation being reinterpreted; quick check: confirm that the reformulated attention preserves the essential properties of standard attention.

## Architecture Onboarding
- **Component Map**: Input -> Embedding Layer -> Recurrent Self-Attention Block (multiple heads) -> Output Layer; the self-attention block computes local energy gradients and updates states iteratively
- **Critical Path**: Embedding → Self-attention iteration → Output projection; the iterative self-attention computation is the core dynamic that determines performance
- **Design Tradeoffs**: Backpropagation-free training vs. computational efficiency of iterative dynamics; local energy formulation vs. global energy approaches; simplified architecture vs. full transformer expressiveness
- **Failure Signatures**: Oscillations in recurrent dynamics indicate unstable energy landscape; poor convergence suggests inadequate contrastive training; loss of pattern specificity indicates attractor basin overlap
- **First Experiments**: 1) Verify convergence properties on simple synthetic attractor patterns; 2) Test robustness to noise by adding perturbations to input patterns; 3) Examine interference between stored patterns by training on correlated examples

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical validation limited to MNIST, raising questions about scalability to complex vision or language tasks
- Claims about test examples as attractors lack rigorous quantitative metrics for basin sizes and generalization boundaries
- Simplified architecture may not capture full representational power of standard transformers, complicating performance comparisons

## Confidence
- Theoretical framework consistency: High
- Empirical validation on simple dataset: Medium
- Scalability to complex tasks: Low
- Quantitative characterization of attractor properties: Low

## Next Checks
1. Benchmark performance on CIFAR-10 or small language modeling task to assess scalability beyond MNIST
2. Conduct quantitative analysis of attractor basin properties including robustness to perturbations and interference between stored patterns
3. Perform ablation studies removing or modifying components of the energy function to isolate critical aspects for observed behavior