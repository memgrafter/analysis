---
ver: rpa2
title: Imitation Learning in Discounted Linear MDPs without exploration assumptions
arxiv_id: '2405.02181'
source_url: https://arxiv.org/abs/2405.02181
tags:
- learning
- expert
- policy
- algorithm
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes ILARL, a new algorithm for imitation learning
  in infinite horizon linear MDPs, and BRIG, an improved version for the finite horizon
  case. ILARL addresses the problem of learning a policy that competes with an expert
  policy without exploration assumptions and with improved accuracy dependence.
---

# Imitation Learning in Discounted Linear MDPs without exploration assumptions

## Quick Facts
- arXiv ID: 2405.02181
- Source URL: https://arxiv.org/abs/2405.02181
- Reference count: 40
- One-line primary result: ILARL algorithm achieves O(d³/(1-γ)⁸ε⁴) sample complexity for imitation learning in infinite horizon linear MDPs without exploration assumptions

## Executive Summary
This paper proposes ILARL, a novel algorithm for imitation learning in infinite horizon linear MDPs that removes the persistent excitation assumption required by previous works. The algorithm achieves a sample complexity of O(d³/(1-γ)⁸ε⁴) MDP interactions and O(d/(1-γ)²ε²) expert demonstrations. Additionally, the paper introduces BRIG, an improved algorithm for finite horizon cases achieving O(ε⁻²) sample complexity. The key innovation is connecting imitation learning with online learning in MDPs with adversarial losses, allowing the algorithm to bypass the need for exploration assumptions.

## Method Summary
The paper presents ILARL, an algorithm that connects imitation learning with online learning in MDPs with adversarial losses. Instead of relying on persistent excitation assumptions, ILARL uses an online learning framework where the cost vectors are treated as adversarial. The algorithm estimates the expert's occupancy measure from demonstrations and uses this to update cost estimates via gradient descent. A policy is then updated using an adversarial MDP algorithm. For finite horizon problems, the BRIG algorithm exploits known costs and greedy policy updates to achieve improved sample complexity. Both algorithms use optimistic evaluation with bonuses rather than ε-greedy exploration.

## Key Results
- ILARL achieves O(d³/(1-γ)⁸ε⁴) sample complexity for infinite horizon linear MDPs
- Removes exploration assumptions required by previous works
- Improves dependence on accuracy ε from O(ε⁻⁵) to O(ε⁻⁴)
- BRIG achieves O(ε⁻²) sample complexity for finite horizon problems
- Numerical experiments show ILARL outperforms existing algorithms with theoretical guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm achieves O(d³/(1-γ)⁸ε⁴) sample complexity without requiring persistent excitation.
- Mechanism: ILARL bypasses the persistent excitation assumption by connecting imitation learning with online learning in MDPs with adversarial losses. Instead of relying on policy visitation to ensure feature matrix invertibility, it uses an online learning framework where the cost vectors are treated as adversarial.
- Core assumption: The linear MDP structure allows the cost to be written as c = Φw, and the expert policy's occupancy measure can be estimated from demonstrations.
- Evidence anchors:
  - [abstract]: "We present a new algorithm for imitation learning in infinite horizon linear MDPs dubbed ILARL which greatly improves the bound on the number of trajectories that the learner needs to sample from the environment. In particular, we remove exploration assumptions required in previous works..."
  - [section]: "The design is different from [48] and it builds on a connection between imitation learning and online learning in MDP with full information."
- Break condition: If the linear MDP assumption fails or if the expert demonstrations do not provide sufficient coverage of state-action space for accurate occupancy measure estimation.

### Mechanism 2
- Claim: The algorithm improves the dependence on ε from O(ε⁻⁵) to O(ε⁻⁴).
- Mechanism: By using a no-regret algorithm for the adversarial MDP component and carefully controlling both the policy regret and the cost estimation error, the algorithm achieves tighter bounds on the number of samples needed for a given accuracy.
- Core assumption: The cost function is realizable (i.e., ctrue = Φwtrue for some wtrue) and the feature matrix Φ is known.
- Evidence anchors:
  - [abstract]: "...we improve the dependence on the desired accuracy ε from O(ε^(-5)) to O(ε^(-4))."
  - [section]: "Our result relies on a connection between imitation learning and online learning in MDPs with adversarial losses."
- Break condition: If the realizability assumption is violated or if the feature matrix does not capture the true cost structure adequately.

### Mechanism 3
- Claim: For finite horizon problems, the algorithm BRIG achieves O(ε⁻²) sample complexity.
- Mechanism: BRIG exploits the fact that in finite horizon settings, the cost function at each stage is known in advance, allowing the use of LSVI-UCB with best response policies. This combination of known costs and greedy policy updates leads to improved sample complexity.
- Core assumption: The finite horizon setting allows for stage-wise cost revelation and greedy policy updates without violating regret bounds.
- Evidence anchors:
  - [abstract]: "Moreover, we are able to provide a strengthen result for the finite horizon case where we achieve O(ε^(-2))."
  - [section]: "Key for this result is realizing that in the regret decomposition of [41] one of the two players can in fact play the best response rather than a more conservative no regret strategy."
- Break condition: If the cost function revelation assumption is violated or if the horizon H is too large relative to other problem parameters.

## Foundational Learning

- Concept: Linear MDP structure
  - Why needed here: The linear MDP assumption allows the cost and transition dynamics to be expressed linearly in feature vectors, which is crucial for both the algorithmic design and the theoretical analysis.
  - Quick check question: Can you express the cost function as c(s,a) = Φ(s,a)⊺w for some unknown vector w?

- Concept: Occupancy measure estimation
  - Why needed here: The algorithm relies on estimating the expert's occupancy measure from demonstrations to compute the difference between expert and learner policies.
  - Quick check question: How would you estimate dπE(s,a) from a dataset of expert trajectories?

- Concept: Online learning in adversarial MDPs
  - Why needed here: The algorithm uses techniques from online learning with adversarial losses to control the policy regret component of the overall error.
  - Quick check question: What is the difference between stochastic and adversarial costs in the context of MDPs?

## Architecture Onboarding

- Component map:
  - Expert data processor -> Cost updater -> Policy updater -> Optimistic evaluator

- Critical path:
  1. Process expert demonstrations to estimate Φ⊺dπE
  2. Initialize policy π0 uniformly
  3. For each iteration k:
     a. Update cost estimate wk+1 using gradient descent on (Φ⊺dπE - Φ⊺dπk)
     b. Run adversarial MDP algorithm with cost Φwk to update policy πk
     c. Collect on-policy data for next iteration
  4. Output policy sampled uniformly from {π1, ..., πK}

- Design tradeoffs:
  - Exploration vs exploitation: The algorithm uses optimistic evaluation with bonuses rather than ε-greedy exploration
  - Computational complexity vs sample complexity: Using full-information adversarial MDP algorithms trades computational efficiency for better sample complexity
  - Batch size τ: Larger batches reduce policy update frequency but increase per-iteration computational cost

- Failure signatures:
  - High estimation error in Φ⊺dπE leading to poor cost updates
  - Divergence in policy updates due to aggressive learning rates
  - Insufficient on-policy data collection leading to poor value function estimates

- First 3 experiments:
  1. Verify the algorithm works on a simple linear MDP with known dynamics and synthetic expert demonstrations
  2. Test the algorithm with varying numbers of expert trajectories to confirm the claimed dependence on τE
  3. Compare the algorithm against behavioral cloning and other IL methods on a benchmark continuous control task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the number of expert trajectories be further reduced for general stochastic experts in Linear MDPs without additional assumptions on features and expert occupancy measure?
- Basis in paper: [explicit] The paper mentions this as an interesting open question in the conclusion section, noting that the current work does not provide an answer.
- Why unresolved: The paper only provides a reduction in expert trajectories for the specific case of a deterministic expert satisfying the Linear Expert Assumption and uniform expert occupancy measure.
- What evidence would resolve it: A theoretical proof showing that the number of expert trajectories can be reduced for general stochastic experts in Linear MDPs without additional assumptions.

### Open Question 2
- Question: Can the sample complexity dependence on the effective horizon term (1-γ) be improved further for the infinite horizon case?
- Basis in paper: [explicit] The paper achieves O((1-γ)^-8) dependence, but the conclusion section suggests this could be a direction for future work.
- Why unresolved: The current analysis relies on a specific connection to online learning in MDPs with adversarial losses, which may not be the most efficient approach for the infinite horizon case.
- What evidence would resolve it: A new algorithm or analysis technique that achieves a better dependence on (1-γ) for the infinite horizon case.

### Open Question 3
- Question: Can the dependence on the dimension d and horizon H be improved for the finite horizon case?
- Basis in paper: [explicit] The paper mentions this as a potential improvement in the conclusion section, suggesting that bypassing the reduction to online learning in MDPs could help.
- Why unresolved: The current approach for the finite horizon case relies on the best response idea, which introduces a dependence on d and H.
- What evidence would resolve it: A new algorithm or analysis technique that achieves a better dependence on d and H for the finite horizon case, potentially by bypassing the reduction to online learning in MDPs.

### Open Question 4
- Question: Can the algorithm be extended to the Bilinear Classes setting with time-changing rewards?
- Basis in paper: [explicit] The paper discusses this as a potential extension in the conclusion section, providing an informal discussion of the proof technique that would be needed.
- Why unresolved: The current analysis relies on the specific structure of Linear MDPs, which may not directly extend to the more general Bilinear Classes setting.
- What evidence would resolve it: A theoretical proof showing that the algorithm can be extended to the Bilinear Classes setting with time-changing rewards, potentially with a polynomial sample complexity.

## Limitations

- The algorithm requires O(d/(1-γ)²ε²) expert demonstrations, which may be prohibitive in practice
- Theoretical guarantees depend on the realizability assumption that the true cost can be expressed as Φw
- The proposed algorithms appear computationally intensive, particularly for the finite horizon case
- Numerical experiments are limited in scope and do not demonstrate scalability to high-dimensional problems

## Confidence

- **High Confidence**: The sample complexity bounds for ILARL (O(d³/(1-γ)⁸ε⁴)) and the improvement over existing O(ε⁻⁵) dependence
- **Medium Confidence**: The BRIG algorithm's O(ε⁻²) bound for finite horizon, as the mechanism relies on specific structural properties of finite horizon problems
- **Medium Confidence**: The practical performance improvements shown in experiments, given the limited experimental scope

## Next Checks

1. Test the algorithm's sensitivity to violations of the linear MDP assumption by evaluating performance on problems with nonlinear dynamics or costs that cannot be expressed as Φw.

2. Verify the algorithm's performance with varying expert demonstration quality and quantity to confirm the O(d/(1-γ)²ε²) dependence on τE is tight and necessary.

3. Implement a runtime complexity analysis and evaluate scalability on problems with increasing feature dimension d and horizon H to assess practical feasibility.