---
ver: rpa2
title: 'CORM: Cache Optimization with Recent Message for Large Language Model Inference'
arxiv_id: '2404.15949'
source_url: https://arxiv.org/abs/2404.15949
tags:
- attention
- arxiv
- cache
- corm
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the memory bottleneck in large language model
  (LLM) inference caused by the key-value (KV) cache, which grows linearly with sequence
  length. The authors observe that (1) adjacent tokens' query vectors are highly similar,
  and (2) the attention calculation for the current query can rely primarily on the
  attention information from a small fraction of recent queries.
---

# CORM: Cache Optimization with Recent Message for Large Language Model Inference

## Quick Facts
- arXiv ID: 2404.15949
- Source URL: https://arxiv.org/abs/2404.15949
- Reference count: 40
- One-line primary result: CORM achieves up to 70% reduction in KV cache memory usage with negligible performance degradation across six tasks in LongBench

## Executive Summary
CORM addresses the memory bottleneck in large language model inference caused by the key-value (KV) cache, which grows linearly with sequence length. The authors observe that adjacent tokens' query vectors are highly similar and that current query attention can rely primarily on recent queries. Based on these insights, CORM proposes a dynamic KV cache eviction policy that retains essential key-value pairs using recent query attention messages without requiring model fine-tuning. The approach achieves significant memory savings while maintaining performance across multiple tasks and model architectures.

## Method Summary
CORM is a KV cache eviction policy that dynamically retains essential key-value pairs by leveraging the observation that current query attention can be approximated using recent query attention patterns. The method maintains a sliding window of recent queries and their attention scores, using this information to identify and evict unimportant keys without significant performance degradation. Unlike existing methods that use fixed cache budgets, CORM adapts to task-specific attention patterns and is compatible with grouped-query attention for additional compression.

## Key Results
- Achieves up to 70% reduction in KV cache memory usage
- Outperforms StreamingLLM, Scissorhands, and H2O across six tasks in LongBench
- Maintains negligible performance degradation while providing significant memory savings
- Compatible with grouped-query attention for additional compression

## Why This Works (Mechanism)

### Mechanism 1
CORM exploits the observation that current query is most similar to recent queries, enabling use of recent attention messages to approximate current attention without storing full KV cache. By maintaining a sliding window of recent queries and their attention scores, CORM can identify important keys based on recent attention patterns. When a key is deemed unimportant by all recent queries within the window, it can be evicted without significant performance loss. The core assumption is that similarity between adjacent query vectors is sufficiently high that recent attention patterns reliably indicate current importance.

### Mechanism 2
Attention sparsity across layers and heads allows selective eviction without degrading model performance. CORM leverages the empirical observation that deeper layers exhibit higher sparsity (over 90% in many cases). By dynamically adjusting eviction based on layer-specific and head-specific importance patterns, CORM preserves critical information while removing less important KV pairs. The degree of sparsity varies predictably across attention layers and heads, enabling targeted cache management.

### Mechanism 3
CORM's dynamic eviction policy without fixed budget outperforms fixed-budget approaches by adapting to task-specific attention patterns. Unlike StreamingLLM, Scissorhands, and H2O which use predetermined cache budgets, CORM dynamically determines which KV pairs to retain based on actual attention patterns. This adaptability allows CORM to achieve higher compression rates while maintaining performance, as different transformer layers and heads should be treated differently rather than applying uniform compression rates.

## Foundational Learning

- **Concept: Transformer self-attention mechanism and KV cache**
  - Why needed here: Understanding how KV cache grows linearly with sequence length and creates memory bottlenecks is fundamental to grasping CORM's value proposition
  - Quick check question: Why does KV cache size grow linearly with sequence length, and what computational problem does this solve?

- **Concept: Attention sparsity and its implications for cache optimization**
  - Why needed here: CORM's effectiveness relies on the empirical observation that attention matrices are sparse, particularly in deeper layers
  - Quick check question: How does attention sparsity enable KV cache compression without significant performance degradation?

- **Concept: Similarity metrics and their application to query vectors**
  - Why needed here: CORM uses cosine similarity between query vectors to identify recent queries that can serve as proxies for current attention patterns
  - Quick check question: Why does the paper hypothesize that similar queries have similar concerns for keys, and what mathematical property supports this?

## Architecture Onboarding

- **Component map**: CORM consists of three main components: (1) Recent query window management that stores query vectors and their attention messages, (2) Dynamic eviction policy that determines which KV pairs to retain based on recent attention patterns, and (3) Integration layer that modifies the standard generation loop to use CORM's eviction decisions.

- **Critical path**: During each generation step, CORM updates the recent query window with current attention scores, evaluates which keys are considered unimportant by all recent queries within the window, and evicts those KV pairs that have been minor for the entire window duration while always preserving the most recent r keys.

- **Design tradeoffs**: CORM trades increased computational overhead from maintaining recent query windows against reduced memory overhead from smaller KV caches. The window size (w) and recent size (r) parameters offer a performance-memory tradeoff curve where larger windows preserve more information but reduce compression.

- **Failure signatures**: Performance degradation occurs when (1) window size is too small causing premature eviction of important keys, (2) non-RoPE positional embeddings reduce query similarity making recent attention messages poor predictors, or (3) certain tasks exhibit uniform attention patterns across layers invalidating the sparsity assumption.

- **First 3 experiments**:
  1. Reproduce Figure 3 to verify that current query is most similar to recent queries on your target model architecture
  2. Implement Algorithm 1 and test KV cache compression rates on a small dataset while monitoring perplexity changes
  3. Compare CORM against StreamingLLM and Scissorhands on a single task to validate the performance-memory tradeoff claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CORM's performance vary across different model architectures (e.g., transformers with different attention mechanisms like sparse attention or local attention)?
- Basis in paper: The paper mentions that CORM works on the observation that similar queries have similar concerns for keys, but also notes that this phenomenon is not present in models using absolute position embedding or ALiBi.
- Why unresolved: The paper only tests CORM on a limited set of models (LLaMA2, Vicuna, Falcon, Qwen, LLaMA3, OPT, and Bloom) and does not explore its effectiveness on models with different attention mechanisms.
- What evidence would resolve it: Testing CORM on a diverse range of transformer architectures, including those with sparse attention or local attention, and comparing its performance to baseline methods.

### Open Question 2
- Question: Can CORM be extended to handle multi-modal inputs (e.g., text and images) effectively?
- Basis in paper: The paper focuses on optimizing KV cache for text-based LLMs, but does not address the potential for multi-modal inputs.
- Why unresolved: The paper does not explore the application of CORM to models that handle multi-modal inputs, which is an important area of research in the field.
- What evidence would resolve it: Implementing and testing CORM on multi-modal transformer models and evaluating its performance on tasks involving both text and image inputs.

### Open Question 3
- Question: What is the impact of CORM on the computational efficiency of LLM inference, beyond memory usage?
- Basis in paper: The paper mentions that CORM reduces memory usage by up to 70% but does not provide detailed analysis of its impact on computational efficiency.
- Why unresolved: The paper does not explore how CORM affects the speed of inference or the computational resources required for LLM generation.
- What evidence would resolve it: Conducting experiments to measure the inference time and computational resources (e.g., FLOPs) used by CORM compared to baseline methods across various tasks and model sizes.

## Limitations

- CORM's effectiveness relies heavily on the empirical observation that query vectors exhibit high similarity with recent queries, which may not hold for all model architectures and positional encoding schemes.
- The computational overhead from maintaining recent query windows is not fully characterized, with limited timing measurements provided.
- The evaluation scope is relatively narrow, focusing on six tasks in LongBench without comprehensive exploration of the performance-memory tradeoff curve across different parameter settings.

## Confidence

**High Confidence**: The core mechanism of using recent attention messages for KV cache eviction is technically sound and the mathematical formulation is clear. The experimental results showing up to 70% memory reduction with negligible performance degradation are convincing within the evaluated task scope.

**Medium Confidence**: The claim that CORM outperforms existing methods across all evaluated tasks is well-supported by the presented results, though the evaluation set is limited to six tasks. The assertion that CORM requires no model fine-tuning and is compatible with GQA is stated but lacks comprehensive validation.

**Low Confidence**: The paper's claims about CORM's robustness to different positional encoding schemes and its computational overhead characteristics are not adequately tested. The generalization to tasks outside LongBench and to model architectures beyond the evaluated set remains uncertain.

## Next Checks

1. **Cross-architecture validation**: Test CORM on models using different positional encoding schemes (non-RoPE) to verify query similarity assumptions hold across architectures.

2. **Computational overhead measurement**: Implement comprehensive timing analysis to measure the actual computational overhead from maintaining recent query windows and its impact on inference throughput.

3. **Parameter sensitivity analysis**: Conduct systematic experiments varying window size (w) and recent size (r) parameters across different task types to characterize the performance-memory tradeoff curve and identify optimal configurations for various use cases.