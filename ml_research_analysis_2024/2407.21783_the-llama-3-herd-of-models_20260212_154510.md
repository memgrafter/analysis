---
ver: rpa2
title: The Llama 3 Herd of Models
arxiv_id: '2407.21783'
source_url: https://arxiv.org/abs/2407.21783
tags:
- llama
- data
- language
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Llama 3 is a new set of foundation models developed by Meta that
  natively support multilinguality, coding, reasoning, and tool usage. The largest
  model is a dense Transformer with 405B parameters and a context window of up to
  128K tokens.
---

# The Llama 3 Herd of Models

## Quick Facts
- arXiv ID: 2407.21783
- Source URL: https://arxiv.org/abs/2407.21783
- Reference count: 40
- Primary result: Llama 3 is a set of foundation models (including a 405B parameter dense Transformer) achieving competitive performance on multilingual, coding, reasoning, and tool usage tasks

## Executive Summary
Llama 3 is a new set of foundation models developed by Meta that natively support multilinguality, coding, reasoning, and tool usage. The largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks, and is close to matching the state-of-the-art. The models are publicly released under an updated version of the Llama 3 Community License.

## Method Summary
Llama 3 uses a dense Transformer architecture with 405B parameters, trained on 15.6 trillion tokens following compute-optimal scaling laws. The models support extended context lengths through continued pre-training. Post-training includes supervised fine-tuning, direct preference optimization, and safety finetuning. Multimodal capabilities are added via a compositional approach using separate vision and speech adapters integrated through cross-attention.

## Key Results
- 405B parameter dense Transformer achieves competitive performance on MMLU, HumanEval, and MGSM benchmarks
- Extended 128K context window maintained through continued pre-training
- Multimodal integration via adapters preserves text-only performance
- Robust to MCQ design variations with 405B model showing particular resilience

## Why This Works (Mechanism)

### Mechanism 1
Dense Transformer architecture with 405B parameters and 128K context achieves competitive quality by balancing compute-optimal scaling with long-context capability. Scaling law optimization predicts that for the given pre-training compute budget (3.8×10^25 FLOPs), a model of ~405B parameters trained on 15.6T tokens is near-optimal. The extended context length to 128K is achieved via continued pre-training rather than joint optimization, preserving short-context quality.

### Mechanism 2
Compositional approach to multimodality (vision and speech) preserves text-only performance while adding capabilities. Pre-trained encoder models (image, speech) are integrated via adapters and cross-attention layers without updating the core language model. This avoids contention and tokenization complexity.

### Mechanism 3
Post-training with rejection sampling, supervised fine-tuning, and direct preference optimization aligns the model to human preferences while maintaining helpfulness. Preference data is collected via human annotations, used to train a reward model, and then applied in rejection sampling and DPO to fine-tune the model.

## Foundational Learning

- Concept: Scaling laws for foundation models
  - Why needed here: To determine the optimal model size and training compute budget for achieving competitive performance
  - Quick check question: What is the relationship between model size, training tokens, and performance according to scaling laws?

- Concept: Compositional model integration
  - Why needed here: To add multimodal capabilities (vision, speech) without degrading text-only performance
  - Quick check question: How does the adapter-based approach preserve the core language model's performance?

- Concept: Preference-based alignment
  - Why needed here: To align the model with human preferences and improve its helpfulness and safety
  - Quick check question: What are the key steps in the post-training process for aligning the model with human feedback?

## Architecture Onboarding

- Component map: Dense Transformer (405B parameters, 128K context) with cross-attention layers for multimodal integration. Separate speech and vision adapters.
- Critical path: Pre-training (language model) → continued pre-training (long context) → adapter training (vision/speech) → supervised fine-tuning → direct preference optimization → quality tuning
- Design tradeoffs: Dense architecture vs. mixture-of-experts (training stability vs. efficiency); compositional approach vs. joint training (simplicity vs. potential performance gains)
- Failure signatures: Degraded text-only performance (adapter integration issues); poor multimodal understanding (adapter training issues); misalignment with human preferences (post-training issues)
- First 3 experiments:
  1. Verify scaling law predictions by training a small model on a subset of the data and extrapolating performance
  2. Test adapter integration by evaluating text-only performance after adding vision or speech capabilities
  3. Validate preference-based alignment by comparing model outputs before and after post-training on human preference data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal trade-off between model size and inference efficiency for different use cases, considering the significant improvements in inference efficiency observed with FP8 quantization and pipeline parallelism?
- Basis in paper: [explicit] The paper demonstrates that FP8 quantization and pipeline parallelism can significantly improve inference efficiency, but the optimal trade-off for different use cases is not explored
- Why unresolved: The paper focuses on demonstrating the effectiveness of these techniques, but does not delve into the practical implications for different applications or deployment scenarios
- What evidence would resolve it: Empirical studies comparing the performance and efficiency of different model sizes and quantization/pipelining configurations across a range of tasks and deployment environments

### Open Question 2
- Question: How can the robustness of Llama 3 to different MCQ setups be further improved, especially for the 8B and 70B models which show some sensitivity to answer order and prompt format?
- Basis in paper: [explicit] The paper highlights the robustness of Llama 3 405B to MCQ design choices, but notes that the smaller models exhibit some sensitivity
- Why unresolved: The paper does not provide a detailed analysis of the underlying reasons for this sensitivity or propose specific methods to mitigate it
- What evidence would resolve it: Ablation studies isolating the factors contributing to sensitivity, and experiments testing different techniques for improving robustness, such as adversarial training or data augmentation

### Open Question 3
- Question: What are the long-term effects of the safety finetuning techniques employed in Llama 3, and how can potential unintended consequences be mitigated?
- Basis in paper: [inferred] The paper describes the safety finetuning process, but does not discuss the potential for unintended consequences or long-term effects on model behavior
- Why unresolved: The complex interplay between safety objectives and other capabilities is not fully understood, and the long-term impact of safety finetuning is unknown
- What evidence would resolve it: Longitudinal studies tracking model behavior over time, and experiments testing the model's ability to generalize safety knowledge to novel situations

## Limitations
- Limited transparency around data composition and quality thresholds
- Scaling law extrapolations may not hold across different model scales or data distributions
- Adapter-based multimodal integration lacks extensive empirical validation compared to joint training approaches

## Confidence
- High Confidence: Dense Transformer architecture with 405B parameters achieving competitive performance; 128K context window preservation; scaling law-based compute optimization methodology
- Medium Confidence: Multimodal integration via compositional approach; preference-based alignment effectiveness; safety evaluations and harmlessness metrics
- Low Confidence: Exact contribution of each component to overall performance; generalization across diverse use cases; long-term stability and robustness

## Next Checks
1. **Scaling Law Validation**: Replicate the small-scale scaling experiments to verify the predicted compute-optimal model size by training models at 10B, 30B, and 70B parameters on subsets of the training data, then extrapolating performance to validate the 405B parameter prediction.

2. **Adapter Integration Impact Assessment**: Conduct controlled experiments comparing text-only performance before and after adding vision/speech adapters by measuring performance degradation on standard language benchmarks (MMLU, BBH) when adapters are added versus a baseline with no adapters.

3. **Preference Data Quality Audit**: Perform an independent analysis of the preference data collection methodology by having external annotators rate a subset of model outputs before and after DPO fine-tuning, measuring inter-annotator agreement and identifying potential biases in the preference annotations.