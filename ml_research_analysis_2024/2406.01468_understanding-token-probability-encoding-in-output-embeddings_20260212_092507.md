---
ver: rpa2
title: Understanding Token Probability Encoding in Output Embeddings
arxiv_id: '2406.01468'
source_url: https://arxiv.org/abs/2406.01468
tags:
- output
- embedding
- probability
- token
- encoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the role of output embeddings in language
  models, focusing on their relationship with output token probabilities. It identifies
  a log-linear correlation between output token probabilities and specific sparse
  directions in the output embedding space, and demonstrates this correlation is stable
  and generalizable across different model sizes.
---

# Understanding Token Probability Encoding in Output Embeddings

## Quick Facts
- arXiv ID: 2406.01468
- Source URL: https://arxiv.org/abs/2406.01468
- Reference count: 20
- The paper demonstrates that output token probabilities in language models are approximately log-linearly encoded in sparse directions of the output embedding space.

## Executive Summary
This paper investigates the relationship between output embeddings and output token probabilities in language models. The authors discover that output token probabilities are encoded in a log-linear fashion within sparse directions of the output embedding space, and this encoding is stable and generalizable across different model sizes. They leverage this finding to develop a steering algorithm that can accurately control output probabilities by modifying the output embeddings, and demonstrate that more than 30% of output embedding dimensions can be pruned without significant performance impact. Additionally, they reveal that output embeddings capture token frequency information from the training corpus very early in the training process.

## Method Summary
The authors use multiple linear regression to fit the relationship between the negative logarithm of output token probabilities and output embeddings, identifying a log-linear encoding pattern. They employ principal component analysis to demonstrate the sparsity of this encoding, showing that only a few dimensions and principal components are highly correlated with output probabilities. To validate their findings, they develop a steering algorithm that modifies output embeddings along the identified encoding direction to control output probabilities. They also conduct pruning experiments using the regression slopes as saliency metrics, and analyze training dynamics to observe when frequency encoding emerges in the output embeddings.

## Key Results
- Output token probabilities are encoded in a log-linear fashion in sparse directions of the output embedding space
- More than 30% of output embedding dimensions can be pruned without significant impact on model performance
- Output embeddings capture token frequency information from the training corpus early in training, before other parameters converge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The softmax-based LM head naturally encodes output probabilities log-linearly in a common direction of the output embedding space when the output dimension is large and logits are concentrated.
- Mechanism: When the softmax function is applied to output logits, the resulting probabilities have a log-linear relationship with the output embeddings. This relationship becomes more accurate as the number of output dimensions increases and the logits become concentrated around their mean.
- Core assumption: The approximation of the log of the sum of exponentials (log-sum-exp) by a linear term is valid when the output logits are sufficiently concentrated.
- Evidence anchors:
  - [abstract] "We find an approximate common log-linear encoding of output token probabilities within the output embedding vectors and empirically demonstrate that it is accurate and sparse."
  - [section 2.1] "When we calculate the averaged output token probability αw,D,θ = Ex∈D[Pθ(w|x)] of token w on a dataset D (detecting dataset): − log αw,D,θ ≈ Ex∈D[− log[Pθ(w|x)]]"
  - [corpus] Weak evidence; the corpus neighbors mention "Following the Autoregressive Nature of LLM Embeddings" which relates to the distributional nature of embeddings but doesn't directly support the log-linear encoding claim.
- Break condition: If the output logits are not concentrated (e.g., very diverse logits), the log-sum-exp approximation becomes inaccurate and the log-linear relationship breaks down.

### Mechanism 2
- Claim: Only a few dimensions of the output embedding are highly correlated to output probability, indicating sparsity in the probability encoding.
- Mechanism: The output probabilities are encoded in a sparse manner, meaning that only a small subset of the output embedding dimensions contribute significantly to determining the output probabilities. This sparsity is evidenced by the fact that only the top principal components and a few original dimensions show strong correlation with output probabilities.
- Core assumption: The sparsity observed in the correlation between output embeddings and probabilities is a fundamental property of how the model encodes probability information.
- Evidence anchors:
  - [abstract] "Moreover, the sparsity we find in output probability encoding suggests that a large number of dimensions in the output embedding do not contribute to causal language modeling."
  - [section 2.2] "we find that only the top principal components and a few dimensions in the output embedding are highly correlated to the output token probabilities. That is, such probability encoding is sparse."
  - [corpus] No direct evidence in corpus neighbors supporting this specific sparsity claim.
- Break condition: If the model architecture or training process changes in a way that distributes probability information more uniformly across dimensions, the observed sparsity could diminish.

### Mechanism 3
- Claim: The output embeddings capture corpus token frequency information early in the training process, before other parameters converge.
- Mechanism: The output embeddings learn to encode the frequency of tokens in the training corpus at a very early stage of training, as evidenced by the log-linear correlation between the negative logarithm of token frequencies and the output embeddings. This occurs even before there is obvious convergence in other model parameters.
- Core assumption: The training objective forces the model to produce output distributions that match the corpus token frequency, leading the output embeddings to encode this frequency information.
- Evidence anchors:
  - [abstract] "Additionally, in the pre-training dynamics of language models, we find that the output embeddings capture the corpus token frequency information in early steps, even before an obvious convergence of parameters starts."
  - [section 5] "we find that the frequency encoding occurs at the very early steps in the training dynamics of LMs, even earlier than an obvious convergence trend is observed by the convergence rate."
  - [corpus] Weak evidence; corpus neighbors don't directly address early training dynamics or frequency encoding.
- Break condition: If the training process or model architecture is altered such that the output embeddings are not directly optimized to match output distributions, the early frequency encoding might not occur.

## Foundational Learning

- Concept: Softmax function and its properties
  - Why needed here: Understanding how the softmax function transforms logits into probabilities is crucial for grasping why the log-linear encoding emerges.
  - Quick check question: What is the relationship between the logits and the resulting probabilities when the softmax function is applied?

- Concept: Principal Component Analysis (PCA)
  - Why needed here: PCA is used to identify the directions (principal components) in the output embedding space that are most correlated with output probabilities, revealing the sparsity of the encoding.
  - Quick check question: How does PCA help in identifying the most important dimensions in a high-dimensional space?

- Concept: Multiple Linear Regression (MLR)
  - Why needed here: MLR is used to fit the relationship between the negative logarithm of output probabilities and the output embeddings, quantifying the log-linear encoding.
  - Quick check question: What does the goodness of fit (e.g., adjusted R-squared) tell us about the strength of the linear relationship in MLR?

## Architecture Onboarding

- Component map: Input sequence -> Hidden state computation -> Output embedding lookup -> LM head (softmax) -> Output probability distribution
- Critical path: Input sequence → Hidden state computation → Output embedding lookup → LM head (softmax) → Output probability distribution
- Design tradeoffs: Tying input and output embeddings reduces parameters but may harm performance; untied embeddings allow for specialized encoding but increase model size
- Failure signatures: If the log-linear encoding is not observed, it could indicate issues with model training, such as lack of concentration in logits or insufficient model capacity
- First 3 experiments:
  1. Compute the correlation between output embeddings and output probabilities using MLR to verify the log-linear encoding.
  2. Apply PCA to the output embedding matrix and analyze the correlation of principal components with output probabilities to assess sparsity.
  3. Modify the output embeddings along the identified encoding direction and measure the change in output probabilities to test causality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sparsity of probability encoding vary across different model architectures and sizes, and what architectural factors contribute to this variation?
- Basis in paper: [explicit] The paper observes that the sparsity of probability encoding differs between models like GPT2-XL and GPT-J, with GPT2-XL showing weaker sparsity. The authors speculate that differences in model architecture and training dynamics might explain these variations, but they do not provide a definitive explanation.
- Why unresolved: The paper identifies differences in sparsity across models but does not investigate the underlying causes. It is unclear whether these differences are due to architectural design, training procedures, or other factors.
- What evidence would resolve it: Comparative studies analyzing the sparsity of probability encoding across a diverse set of model architectures (e.g., different attention mechanisms, normalization techniques) and training configurations could identify the key factors influencing sparsity.

### Open Question 2
- Question: Can the log-linear correlation between output token probabilities and output embeddings be leveraged to develop more efficient and interpretable pruning methods for language models?
- Basis in paper: [explicit] The paper demonstrates that more than 30% of output embedding dimensions can be pruned without significant performance loss, using the MLR slopes as a saliency metric. However, the authors note that their method may not generalize to all tasks and that further research is needed to explore its broader applicability.
- Why unresolved: While the paper shows the potential of using the log-linear correlation for pruning, it does not explore its effectiveness in other contexts or its impact on model interpretability.
- What evidence would resolve it: Experiments applying the pruning method to a wider range of tasks (e.g., summarization, translation) and analyzing the resulting model behavior could provide insights into its generalizability and interpretability benefits.

### Open Question 3
- Question: How does the early emergence of token frequency encoding in output embeddings influence the overall training dynamics and convergence of language models?
- Basis in paper: [explicit] The paper finds that output embeddings capture token frequency information early in training, even before other parameters converge. The authors suggest this might be related to the model's need to produce consistent output distributions, but the broader implications for training dynamics are not explored.
- Why unresolved: The paper identifies the early emergence of token frequency encoding but does not investigate its impact on the subsequent training process or the model's ability to learn other linguistic features.
- What evidence would resolve it: Detailed analysis of training trajectories, including the relationship between token frequency encoding and the convergence of other model components, could clarify its role in the overall learning process.

## Limitations
- The findings rely on specific conditions for the log-linear approximation to hold, particularly requiring concentrated logits and large output dimensions
- The analysis focuses primarily on causal language models, leaving uncertainty about generalization to other architectures or tasks
- The pruning experiments don't fully explore downstream task performance or long-term stability of modified models

## Confidence

*High Confidence:* The empirical observations of log-linear correlation between output embeddings and token probabilities (supported by strong statistical evidence across multiple model sizes). The sparsity findings regarding probability encoding (demonstrated through consistent PCA and MLR analyses). The successful steering algorithm implementation showing controlled probability modification.

*Medium Confidence:* The stability and generalizability claims across model sizes (based on experiments with Llama models but limited to specific architectures). The early training dynamics findings regarding frequency encoding (observational but lacking mechanistic explanation).

*Low Confidence:* The theoretical justification for why this log-linear encoding emerges naturally (the mathematical derivation is suggestive but not conclusive). Claims about implications for model compression and editing (largely speculative based on the pruning results).

## Next Checks

1. **Cross-architecture validation:** Test the log-linear encoding hypothesis and steering algorithm on non-causal architectures (bidirectional models, encoder-decoder models) and non-language tasks to assess generalizability.

2. **Long-term stability analysis:** Evaluate the pruned models over extended training periods and across multiple fine-tuning scenarios to ensure the pruning doesn't introduce gradual performance degradation or instability.

3. **Mechanistic investigation of frequency encoding:** Design controlled experiments varying training data distribution and curriculum to determine whether the early frequency encoding is a fundamental property or dependent on specific training conditions.