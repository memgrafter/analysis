---
ver: rpa2
title: Optimized Quran Passage Retrieval Using an Expanded QA Dataset and Fine-Tuned
  Language Models
arxiv_id: '2412.11431'
source_url: https://arxiv.org/abs/2412.11431
tags: []
core_contribution: This work addresses the challenge of improving Quranic passage
  retrieval accuracy for question-answering systems. The approach involves expanding
  and diversifying an existing dataset of Quranic questions from 251 to 1895 through
  strategic rephrasing and categorization, while fine-tuning multiple transformer-based
  language models including AraBERT, RoBERTa, CAMeLBERT, AraELECTRA, and BERT.
---

# Optimized Quran Passage Retrieval Using an Expanded QA Dataset and Fine-Tuned Language Models

## Quick Facts
- arXiv ID: 2412.11431
- Source URL: https://arxiv.org/abs/2412.11431
- Authors: Mohamed Basem; Islam Oshallah; Baraa Hikal; Ali Hamdi; Ammar Mohamed
- Reference count: 29
- One-line primary result: AraBERT-base achieved MAP@10 of 0.36 and MRR of 0.59, improving over baseline by 63% and 59% respectively

## Executive Summary
This work addresses the challenge of improving Quranic passage retrieval accuracy for question-answering systems. The approach involves expanding and diversifying an existing dataset of Quranic questions from 251 to 1895 through strategic rephrasing and categorization, while fine-tuning multiple transformer-based language models including AraBERT, RoBERTa, CAMeLBERT, AraELECTRA, and BERT. The best-performing model, AraBERT-base, achieved a MAP@10 of 0.36 and MRR of 0.59, representing improvements of 63% and 59% respectively over baseline scores. The dataset expansion also significantly improved handling of "no answer" cases, with success rates increasing from 25% to 75%.

## Method Summary
The study expands an existing Quranic QA dataset from 251 to 1895 questions through strategic rephrasing and categorization. Multiple Arabic-specific transformer models (AraBERT, CAMeLBERT, AraELECTRA, Roberta, BERT) are fine-tuned on this expanded dataset. An ensemble learning approach combines predictions from multiple models, with thresholding mechanisms to filter low-confidence predictions for zero-answer cases. The system is evaluated using MAP@10, MRR, and recall/precision metrics across different question types.

## Key Results
- AraBERT-base achieved MAP@10 of 0.36 and MRR of 0.59, representing 63% and 59% improvements over baseline
- Dataset expansion improved zero-answer case handling from 25% to 75% success rate
- Ensemble learning with thresholding mechanisms enhanced overall answer accuracy and robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expanding the dataset from 251 to 1895 questions improves model robustness by increasing diversity in question phrasing and reducing overfitting.
- Mechanism: The dataset expansion involves strategic rephrasing and categorization, creating multiple question variants that capture different linguistic expressions of the same underlying intent. This allows the model to learn more generalized representations.
- Core assumption: More diverse training examples lead to better generalization across unseen question variations.
- Evidence anchors:
  - [abstract]: "expanding the original dataset by generating new questions through rephrasing and categorization, resulting in a significantly larger and more diverse set of 1895 questions"
  - [section]: "To enhance the dataset, the original dataset was manipulated 251-question dataset by rephrasing and generating additional questions, ultimately expanding it to 629 questions. All of these questions were rephrased twice, resulting in a robust dataset of 1895 questions"
- Break condition: If the rephrased questions don't maintain semantic equivalence to original questions, the expanded dataset could introduce noise and degrade performance.

### Mechanism 2
- Claim: Fine-tuning multiple transformer models on the expanded dataset improves passage retrieval accuracy by adapting pre-trained knowledge to Quranic context.
- Mechanism: Pre-trained models like AraBERT, CAMeLBERT, and AraELECTRA are fine-tuned on the expanded dataset, allowing them to adjust their learned representations to better handle the specific linguistic features of Quranic text and MSA questions.
- Core assumption: Pre-trained models contain generalizable knowledge that can be effectively adapted to specialized domains with appropriate fine-tuning.
- Evidence anchors:
  - [abstract]: "Extensive experiments fine-tuned transformer models, including AraBERT, RoBERTa, CAMeLBERT, AraELECTRA, and BERT. The best model, AraBERT-base, achieved a MAP@10 of 0.36 and MRR of 0.59"
  - [section]: "Fine-tuning pre-trained language models is a essential element of this study, as it greatly improves the model's capability to correctly recognize and understand Qur'anic verses when answering questions in Modern Standard Arabic (MSA)"
- Break condition: If the model capacity is insufficient to capture the domain-specific nuances, fine-tuning may not yield significant improvements.

### Mechanism 3
- Claim: Ensemble learning and thresholding mechanisms improve zero-answer case handling by combining multiple model predictions and filtering low-confidence results.
- Mechanism: Multiple fine-tuned models are combined through ensemble learning, with predictions aggregated to improve robustness. A thresholding mechanism filters out uncertain predictions, particularly for questions with no relevant answers in the Quran.
- Core assumption: Different models capture different aspects of the data, and combining them reduces individual model biases and errors.
- Evidence anchors:
  - [abstract]: "Additionally, the dataset expansion led to improvements in handling 'no answer' cases, with the proposed approach achieving a 75% success rate for such instances, compared to the baseline's 25%"
  - [section]: "implement ensemble learning techniques, combining predictions from multiple fine-tuned models. This approach improves overall answer accuracy and robustness by leveraging the strengths of each model while mitigating individual weaknesses"
- Break condition: If ensemble voting doesn't improve over the best individual model, or if thresholding is too aggressive, it may discard valid answers.

## Foundational Learning

- Concept: Transfer learning with pre-trained transformers
  - Why needed here: Pre-trained models provide a strong starting point for Arabic NLP tasks, reducing the need for extensive training data and computation
  - Quick check question: What is the primary advantage of using pre-trained models like AraBERT for Quranic QA instead of training from scratch?

- Concept: MAP@10 and MRR evaluation metrics
  - Why needed here: These metrics evaluate the quality of ranked retrieval results, crucial for assessing how well models surface relevant Quranic passages
  - Quick check question: How does MAP@10 differ from MRR in evaluating passage retrieval systems?

- Concept: Dataset expansion through rephrasing and augmentation
  - Why needed here: The original dataset is too small for effective training, and expansion techniques increase diversity without requiring new manual annotations
  - Quick check question: Why is it important to maintain semantic equivalence when rephrasing questions during dataset expansion?

## Architecture Onboarding

- Component map: Data collection → Dataset expansion → Model fine-tuning → Ensemble voting → Thresholding → Evaluation
- Critical path: Data collection → Dataset expansion → Model fine-tuning → Ensemble voting → Thresholding → Evaluation
- Design tradeoffs:
  - Model complexity vs. training time: Larger models like AraBERT-large offer better performance but require more resources
  - Dataset size vs. quality: Expanding through rephrasing increases size but risks introducing noise
  - Ensemble diversity vs. computational cost: More models improve robustness but increase inference time
- Failure signatures:
  - Low MAP@10/MRR with high Recall: Model is retrieving relevant passages but ranking them poorly
  - High MAP@10/MRR but poor zero-answer handling: Model is confident even when no answer exists
  - Performance degradation on expanded dataset: Rephrasing introduced semantic drift
- First 3 experiments:
  1. Fine-tune AraBERT-base on the original 251-question dataset to establish baseline performance
  2. Fine-tune AraBERT-base on the expanded 1895-question dataset to measure dataset expansion impact
  3. Implement ensemble voting with AraBERT-base and CAMeLBERT to evaluate ensemble benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would further increasing the dataset size beyond 1895 questions impact the performance of different language models, particularly for zero-answer cases?
- Basis in paper: [explicit] The paper notes that the dataset was expanded from 251 to 1895 questions, leading to significant improvements in handling "no answer" cases, with success rates increasing from 25% to 75%.
- Why unresolved: The paper does not explore the effects of further dataset expansion beyond 1895 questions, leaving the potential for additional improvements untested.
- What evidence would resolve it: Conducting experiments with datasets larger than 1895 questions and comparing performance metrics such as MAP, MRR, and zero-answer success rates would provide insights into the scalability of the approach.

### Open Question 2
- Question: How do different ensemble learning strategies affect the performance of Quranic QA systems, and which configurations yield the best results?
- Basis in paper: [explicit] The paper mentions the use of ensemble learning techniques to combine predictions from multiple fine-tuned models, which improved overall answer accuracy and robustness.
- Why unresolved: The paper does not detail specific ensemble learning strategies or compare their effectiveness, leaving the optimal configuration unexplored.
- What evidence would resolve it: Systematic experimentation with various ensemble methods (e.g., weighted averaging, stacking, voting) and their impact on performance metrics would identify the most effective strategies.

### Open Question 3
- Question: What are the limitations of current language models in handling the linguistic nuances of Classical Arabic, and how can these be addressed?
- Basis in paper: [inferred] The paper highlights the challenges posed by the linguistic differences between Modern Standard Arabic and Classical Arabic in Quranic texts, which existing models struggle to fully capture.
- Why unresolved: The paper does not investigate specific limitations or propose solutions for improving model handling of Classical Arabic nuances.
- What evidence would resolve it: Analyzing model performance on Classical Arabic-specific tasks and developing targeted adaptations or training techniques could address these limitations.

## Limitations
- Lack of detailed documentation regarding the specific methodology used for dataset expansion from 251 to 1895 questions
- No extensive qualitative analysis of model predictions or error analysis across different question types
- Does not address potential biases in the dataset expansion process or how the model handles linguistic variations between Classical and Modern Standard Arabic

## Confidence
- **High Confidence**: The overall approach of combining dataset expansion with fine-tuning transformer models for Quranic QA is sound and supported by the reported performance improvements.
- **Medium Confidence**: The claim that dataset expansion specifically improved zero-answer case handling from 25% to 75% success rate is supported by the results but lacks detailed analysis.
- **Low Confidence**: The exact mechanisms and quality assurance processes used during the dataset expansion phase are not fully documented.

## Next Checks
1. Conduct a thorough semantic equivalence analysis of the expanded questions compared to the original dataset, using human annotation or automated semantic similarity measures to ensure that rephrasing did not introduce noise or semantic drift.
2. Perform systematic ablation experiments to isolate the contributions of dataset expansion versus model fine-tuning to the overall performance improvements, testing each component independently.
3. Conduct comprehensive error analysis across different question types (single-answer, multi-answer, zero-answer) and investigate potential biases introduced during the dataset expansion process.