---
ver: rpa2
title: Can Large Language Models Create New Knowledge for Spatial Reasoning Tasks?
arxiv_id: '2405.14379'
source_url: https://arxiv.org/abs/2405.14379
tags:
- claude
- llms
- polygons
- figure
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether large language models (LLMs) can
  create new knowledge beyond their training data by testing them on two novel spatial
  reasoning tasks: a decidable combinatorial game and a family of 24-sided polygons
  with specific angle constraints. The authors found that Claude 3 performed notably
  well on both tasks, demonstrating the ability to generate new insights and properties
  of the problems.'
---

# Can Large Language Models Create New Knowledge for Spatial Reasoning Tasks?

## Quick Facts
- arXiv ID: 2405.14379
- Source URL: https://arxiv.org/abs/2405.14379
- Authors: Thomas Greatrix; Roger Whitaker; Liam Turner; Walter Colombo
- Reference count: 40
- Key outcome: This paper investigates whether large language models (LLMs) can create new knowledge beyond their training data by testing them on two novel spatial reasoning tasks: a decidable combinatorial game and a family of 24-sided polygons with specific angle constraints. The authors found that Claude 3 performed notably well on both tasks, demonstrating the ability to generate new insights and properties of the problems. For the game, Claude 3 discovered a dominant strategy for odd-numbered spaces and correctly identified the winner for 7 spaces. For the polygons, it proposed novel properties such as the ability to tile the plane and having an even number of right angles. While the results were not perfect and other LLMs showed varying performance, the findings suggest that state-of-the-art LLMs like Claude 3 can indeed contribute to knowledge creation in complex spatial reasoning tasks, supporting the idea of emergent properties in these models.

## Executive Summary
This paper investigates whether large language models (LLMs) can generate new knowledge by testing them on two novel spatial reasoning tasks: a decidable combinatorial game and a family of 24-sided polygons with specific angle constraints. The authors found that Claude 3 performed notably well on both tasks, demonstrating the ability to generate new insights and properties of the problems. For the game, Claude 3 discovered a dominant strategy for odd-numbered spaces and correctly identified the winner for 7 spaces. For the polygons, it proposed novel properties such as the ability to tile the plane and having an even number of right angles. While the results were not perfect and other LLMs showed varying performance, the findings suggest that state-of-the-art LLMs like Claude 3 can indeed contribute to knowledge creation in complex spatial reasoning tasks, supporting the idea of emergent properties in these models.

## Method Summary
The authors tested whether LLMs can generate new knowledge by solving two novel spatial reasoning tasks: a decidable combinatorial game and a family of 24-sided polygons with specific angle constraints. They provided exact prompts to Claude 3, ChatGPT-3.5-Turbo, and Bing Copilot, and evaluated the responses for correctness, novelty, and insightfulness. The study focused on identifying correct answers, novel properties, and reasoning quality, comparing results across models and documenting findings with supporting evidence.

## Key Results
- Claude 3 discovered a dominant strategy for odd-numbered spaces in the combinatorial game and correctly identified the winner for 7 spaces.
- For the polygons, Claude 3 proposed novel properties such as the ability to tile the plane and having an even number of right angles.
- While Claude 3 performed notably well, other LLMs showed varying performance, suggesting model-specific capabilities rather than a general LLM trait.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate new knowledge in spatial reasoning tasks beyond their training data.
- Mechanism: LLMs use their underlying model to deeply understand spatial relationships and apply mathematical reasoning to novel problems.
- Core assumption: The scale of data and features in LLMs allows them to develop sophisticated models for understanding and reasoning.
- Evidence anchors:
  - [abstract] "we observe that LLMs are able to perform sophisticated reasoning on problems with a spatial dimension, that they are unlikely to have previously directly encountered."
  - [section 2] "GPT-4 replied with an elegant mathematical reductionist approach that optimises for resources, demonstrating a powerful emergent property."
  - [corpus] Weak - corpus contains related but not directly applicable papers on LLM spatial reasoning.
- Break condition: If the problem requires domain-specific knowledge not present in the training data or if the spatial reasoning exceeds the model's inherent capabilities.

### Mechanism 2
- Claim: LLMs can discover optimal strategies in combinatorial games through emergent properties.
- Mechanism: LLMs can identify patterns and symmetries in game structures, leading to novel strategy discoveries.
- Core assumption: The game structure and rules can be analyzed by the LLM to find winning strategies without explicit training on that specific game.
- Evidence anchors:
  - [section 3.1] "Claude 3 was able to come up with a new, provably dominant strategy for if there is an odd number of spaces."
  - [section 6] "Claude 3 was able to find the optimal strategy for an odd number of spaces and elegantly use this result to correctly deduce winner of the case when there were 7 spaces."
  - [corpus] Weak - corpus doesn't contain direct evidence of LLM game strategy discovery.
- Break condition: If the game complexity exceeds the LLM's ability to analyze or if the optimal strategy requires domain-specific heuristics not inferable from the rules.

### Mechanism 3
- Claim: LLMs can propose novel properties of geometric shapes based on given constraints.
- Mechanism: LLMs can apply geometric principles and combinatorial reasoning to deduce properties of shapes meeting specific criteria.
- Core assumption: The LLM has sufficient geometric knowledge to reason about properties of shapes with given constraints.
- Evidence anchors:
  - [section 4.1] "Claude 3 additionally suggested that all the polygons from Figure 1 have an even number of right angles."
  - [section 4.1] "Both Claude 3 and Bing Copilot proposed that the polygons in Figure 1 could tile the plane."
  - [corpus] Weak - corpus contains related but not directly applicable papers on LLM geometric reasoning.
- Break condition: If the geometric properties require advanced mathematical theorems not inferable from basic principles or if the constraints are too complex for the LLM to analyze.

## Foundational Learning

- Concept: Spatial reasoning and geometric principles
  - Why needed here: The tasks involve understanding and reasoning about spatial relationships and geometric properties.
  - Quick check question: Can you explain why a polygon with an interior angle of 270 degrees cannot be convex?

- Concept: Combinatorial game theory and strategy
  - Why needed here: The game task requires understanding game structures and finding optimal strategies.
  - Quick check question: What is the Sprague-Grundy theorem and how does it relate to game strategy?

- Concept: Mathematical proof techniques
  - Why needed here: Some of the LLM's claims require mathematical justification and proof.
  - Quick check question: How would you prove that a tiling pattern covers the entire plane without gaps or overlaps?

## Architecture Onboarding

- Component map: Input (Problem description and constraints) -> Processing (LLM reasoning and analysis) -> Output (Proposed solutions, strategies, or properties) -> Validation (Mathematical verification of LLM claims)

- Critical path:
  1. Define problem and constraints
  2. Generate potential solutions/strategies using LLM
  3. Validate proposed solutions mathematically
  4. Analyze LLM reasoning process

- Design tradeoffs:
  - Model size vs. reasoning capability
  - Training data diversity vs. problem specificity
  - Computational resources vs. response time

- Failure signatures:
  - Incorrect mathematical reasoning
  - Overconfidence in unsupported claims
  - Inability to generalize from similar problems

- First 3 experiments:
  1. Test LLM on simpler spatial reasoning tasks with known solutions
  2. Vary problem complexity and observe performance changes
  3. Compare different LLM models on the same spatial reasoning tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs consistently generate genuinely novel mathematical insights beyond their training data?
- Basis in paper: Explicit - The paper investigates whether LLMs can create new knowledge by testing them on novel spatial reasoning tasks
- Why unresolved: The paper shows mixed results, with Claude 3 performing notably well but not perfectly. Other LLMs showed varying performance, and some generated incorrect or uninteresting ideas
- What evidence would resolve it: Systematic testing of LLMs on a large number of novel mathematical problems across different domains, with rigorous evaluation of the novelty and correctness of generated insights

### Open Question 2
- Question: What underlying mechanisms allow certain LLMs to solve novel spatial reasoning problems?
- Basis in paper: Explicit - The paper notes that Claude 3 performed notably well on both spatial tasks tested, suggesting emergent properties in LLMs
- Why unresolved: The paper doesn't investigate why Claude 3 performed better than other LLMs, or what specific capabilities enabled it to solve these novel problems
- What evidence would resolve it: Comparative analysis of different LLMs' architectures, training data, and performance on spatial reasoning tasks, combined with ablation studies to identify key factors

### Open Question 3
- Question: Can incorrect LLM suggestions still provide valuable insights that lead to correct solutions?
- Basis in paper: Explicit - The paper notes that Claude 3 incorrectly identified the game as Nim, but this prompted the authors to investigate an equivalence via the Sprague-Grundy theorem
- Why unresolved: The paper doesn't explore the frequency or mechanisms by which incorrect LLM suggestions can still be useful for problem-solving
- What evidence would resolve it: Systematic study of LLM suggestions on various problems, analyzing the relationship between suggestion accuracy and usefulness in guiding human problem-solving

## Limitations
- The sample size of spatial reasoning tasks is small, consisting of only two novel problems, raising questions about generalizability.
- The evaluation of "new knowledge creation" is somewhat subjective, relying on the authors' assessment of whether LLM responses represent novel insights.
- Performance differences between models suggest that results may be model-specific rather than demonstrating a general capability across LLMs.

## Confidence

*High Confidence:* The observation that Claude 3 performed well on both spatial reasoning tasks is well-supported by the evidence provided. The specific strategies and properties identified by the model are clearly documented.

*Medium Confidence:* The claim that LLMs can "create new knowledge" is plausible but requires careful interpretation. While the models demonstrated novel approaches to the specific problems tested, it's less clear whether this constitutes genuine knowledge creation versus sophisticated pattern matching and reasoning within the bounds of their training.

*Low Confidence:* The broader implication that these results demonstrate "emergent properties" in LLMs is not well-supported. The evidence shows capable performance on specific tasks but doesn't establish whether this reflects true emergence or simply the scaling of existing capabilities.

## Next Checks

1. **Replication with Diverse Spatial Tasks:** Test the same models on a broader range of spatial reasoning problems, including those involving different geometric concepts, topological reasoning, and multi-step spatial transformations to assess generalizability.

2. **Mathematical Verification Protocol:** Develop a rigorous framework for independently verifying whether LLM-proposed solutions and properties are genuinely novel or rediscoveries of known mathematical results, potentially involving consultation with domain experts.

3. **Model Comparison Under Controlled Conditions:** Conduct a systematic comparison of multiple LLM models on identical spatial reasoning tasks, controlling for model versions, prompt engineering, and evaluation criteria to isolate which aspects of performance are model-specific versus generalizable.