---
ver: rpa2
title: 'DomainLab: A modular Python package for domain generalization in deep learning'
arxiv_id: '2403.14356'
source_url: https://arxiv.org/abs/2403.14356
tags:
- domain
- generalization
- domainlab
- neural
- package
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DomainLab addresses the problem of poor generalization performance
  in deep learning models due to distribution shifts across domains. The package provides
  a modular Python framework for training neural networks with composable regularization
  loss terms, enabling the combination of different domain generalization methods.
---

# DomainLab: A modular Python package for domain generalization in deep learning

## Quick Facts
- arXiv ID: 2403.14356
- Source URL: https://arxiv.org/abs/2403.14356
- Reference count: 7
- Domain generalization methods suffer from poor generalization due to distribution shifts across domains

## Executive Summary
DomainLab is a Python package that addresses the challenge of domain generalization in deep learning by providing a modular framework for training neural networks with composable regularization loss terms. The package enables users to combine different domain generalization methods through a hierarchical configuration system, separating neural network implementations from regularization loss construction. DomainLab supports both HPC clusters and standalone machines, includes extensive documentation with over 95% test coverage, and provides powerful benchmarking functionality for evaluating generalization performance on out-of-distribution data.

## Method Summary
DomainLab implements domain generalization through a modular architecture that separates neural network models from regularization loss construction. Users specify experimental setups in YAML configuration files that define hierarchical combinations of models (which construct instance-wise regularization losses using auxiliary networks) and trainers (which apply additional domain invariance losses and update parameters). The package supports systematic benchmarking with hyperparameter sampling and visualization, powered by Snakemake for job scheduling across different computing environments. Built-in tasks include Color-MNIST, subsampled PACS, and VLCS datasets.

## Key Results
- Modular design allows separation of neural networks from regularization loss construction
- Single YAML configuration file enables reproducible benchmarking with hierarchical method combinations
- Supports both HPC clusters and standalone machines through Snakemake pipeline
- Over 95% test coverage with extensive documentation for user-friendly implementation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling neural networks from regularization loss construction enables hierarchical combinations of models, methods, and hyperparameters
- Mechanism: By separating the model component from the trainer component, users can freely combine different domain generalization techniques without modifying core source code
- Core assumption: The regularization loss from models and trainers can be mathematically combined as ℓ(·) + Σ μmk Rmk(·) + Σ μtj Rtj(·) without introducing conflicts
- Evidence anchors:
  - [abstract]: "Its decoupled design allows the separation of neural networks from regularization loss construction"
  - [section 2.1]: "We formalize this concept in Equation (1) as Tθ: ˆθ = Tθ[Eξ∼Dtr[ℓ(b(θ); ξ) + μT R(b(θ); ξ)]; Oθ, θ(0)]"
  - [section 2.2]: "Such combination and decoration can be done recursively to have the form ℓ(·)+P k μmk Rmk(·)+P j μtj Rtj(·)"
- Break condition: If the mathematical combination of regularization losses from different sources creates optimization conflicts or incompatible gradient flows

### Mechanism 2
- Claim: Single configuration file specification enables reproducible and systematic benchmarking of domain generalization methods
- Mechanism: Hierarchical combinations of neural networks, different domain generalization methods, and associated hyperparameters can all be specified together in a single YAML configuration file
- Core assumption: A single configuration file can capture all necessary experimental variations while maintaining reproducibility
- Evidence anchors:
  - [abstract]: "Hierarchical combinations of neural networks, different domain generalization methods, and associated hyperparameters, can all be specified together with other experimental setup in a single configuration file"
  - [section 2.2]: "By simply editing a YAML configuration file, users can specify the domain generalization task, define the hierarchical combination of Trainer, Model and neural net along with their respective hyperparameter ranges"
  - [section 3.2]: "Hyperparameters can be sampled either randomly or with fixed grids, according to specified distributions"
- Break condition: If the YAML configuration becomes too complex to manage or if certain experimental variations cannot be captured in the hierarchical specification

### Mechanism 3
- Claim: Support for both HPC clusters and standalone machines through Snakemake enables scalable benchmarking
- Mechanism: The package uses Snakemake pipeline to schedule and execute benchmark jobs, allowing the same benchmark configuration to run on different computing environments without modification
- Core assumption: Snakemake provides sufficient abstraction to hide differences between HPC clusters and standalone machines
- Evidence anchors:
  - [abstract]: "The package supports running the specified benchmark on an HPC cluster or on a standalone machine"
  - [section 2.3]: "Powered by Snakemake (Mölder et al., 2021), DomainLab supports job submission to an HPC cluster or a standalone machine"
  - [section 3.2]: "The following command runs the benchmark on a Slurm HPC cluster: bash run_benchmark_slurm.sh examples/benchmark/demo_shared_hyper_grid.yaml"
- Break condition: If the underlying computing environment has specific requirements that Snakemake cannot abstract away

## Foundational Learning

- Concept: Structural Risk Minimization with domain invariant regularization
  - Why needed here: Domain generalization methods rely on adding regularization terms to the standard ERM loss to enforce domain invariance
  - Quick check question: What is the mathematical form of the loss function used in domain generalization methods according to the paper?

- Concept: Software design patterns (open/closed principle)
  - Why needed here: The package follows design principles where it is "closed to modification but open to extension" to allow users to add new methods without changing core code
  - Quick check question: How does DomainLab's design allow extension without modification of source code?

- Concept: YAML configuration and hierarchical parameter specification
  - Why needed here: The benchmarking functionality requires users to specify complex experimental setups including shared hyperparameters and individual method parameters
  - Quick check question: What are the two ways hyperparameters can be sampled in DomainLab's benchmarking system?

## Architecture Onboarding

- Component map:
  - Task -> Model -> Trainer -> Benchmark
  - Task: Handles dataset loading and domain specification (TaskDset, TaskFolder, TaskPathFile)
  - Model: Constructs instance-wise regularization losses using auxiliary neural networks
  - Trainer: Directs data flow, applies additional regularization, and updates model parameters
  - Benchmark: Manages systematic benchmarking with hyperparameter sampling and visualization

- Critical path:
  1. User creates YAML configuration specifying task, model combinations, trainer decorations, and hyperparameters
  2. DomainLab loads the task and creates dataset iterators
  3. Models construct their regularization losses
  4. Trainers apply additional regularization and manage training loop
  5. For benchmarking, Snakemake schedules jobs across computing resources
  6. Results are aggregated and visualized

- Design tradeoffs:
  - Modularity vs. performance: Decoupling components provides flexibility but may introduce overhead
  - Abstraction vs. control: High-level configuration hides implementation details but may limit fine-grained control
  - Flexibility vs. complexity: Support for many combinations increases complexity of the API

- Failure signatures:
  - Configuration errors: Incorrect YAML structure or missing required fields
  - Gradient conflicts: Incompatible regularization losses causing training instability
  - Resource issues: Memory constraints when running on standalone machines vs. HPC clusters
  - Sampling problems: Insufficient hyperparameter coverage or poor random seed distribution

- First 3 experiments:
  1. Run the basic ERM baseline using the provided Color-MNIST task to verify installation
  2. Combine DIVA model with MLDG trainer using the VLCS dataset to test hierarchical combination
  3. Execute a small benchmark comparing ERM, DIVA, and MLDG on PACS with 2-3 hyperparameter samples each

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DomainLab's decoupled design specifically impact the performance of domain generalization methods compared to coupled implementations like DomainBed?
- Basis in paper: [explicit] The paper contrasts DomainLab's modular design with DomainBed's coupled approach, noting that DomainLab allows for hierarchical combinations of models, methods, and hyperparameters.
- Why unresolved: The paper describes the design advantages but does not provide empirical comparisons showing performance differences between DomainLab and DomainBed implementations.
- What evidence would resolve it: Benchmarking results comparing the same domain generalization methods implemented in both DomainLab and DomainBed would demonstrate performance differences.

### Open Question 2
- Question: What is the impact of different hyperparameter sampling strategies (random vs. grid search) on the effectiveness of domain generalization methods in DomainLab?
- Basis in paper: [explicit] The paper mentions that DomainLab supports both random and grid search hyperparameter sampling, with an example of grid search implementation by CF.
- Why unresolved: While the paper describes the sampling options, it does not provide comparative results showing how different sampling strategies affect method performance.
- What evidence would resolve it: Empirical studies comparing domain generalization performance using random vs. grid search hyperparameter sampling within DomainLab.

### Open Question 3
- Question: How does the hierarchical combination of regularization terms from different models and trainers affect the generalization performance in DomainLab?
- Basis in paper: [explicit] The paper describes DomainLab's ability to combine regularization terms from both Models and Trainers in a hierarchical manner, extending the SRM loss function.
- Why unresolved: The paper explains the theoretical framework but does not provide empirical results demonstrating the practical impact of different hierarchical combinations on performance.
- What evidence would resolve it: Comparative benchmarking results showing performance differences between single-method approaches and various hierarchical combinations of methods in DomainLab.

## Limitations
- Claims about composable domain generalization methods are primarily validated through example configurations rather than comprehensive empirical studies
- No theoretical guarantees are provided for arbitrary combinations of regularization losses
- The paper does not address how to handle the combinatorial explosion of possible method combinations in large-scale experiments

## Confidence
- High Confidence: The basic framework architecture (modular separation of models, trainers, and tasks) is well-documented and supported by clear code examples
- Medium Confidence: The YAML configuration system for specifying hierarchical combinations is feasible based on provided examples, though scalability to complex combinations remains unverified
- Medium Confidence: The benchmarking functionality using Snakemake is technically sound, but real-world performance on diverse HPC environments is not demonstrated

## Next Checks
1. Test compatibility constraints: Systematically attempt to combine all available Models with all available Trainers to identify mathematical or implementation conflicts in regularization loss combinations
2. Benchmark scalability evaluation: Run the benchmarking system with increasing numbers of hyperparameter samples and method combinations to measure performance degradation and resource utilization patterns
3. Reproducibility audit: Attempt to exactly reproduce the results shown in the provided examples using only the package documentation and configuration files, without referencing the source code