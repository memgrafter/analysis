---
ver: rpa2
title: 'Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal
  Training'
arxiv_id: '2407.09121'
source_url: https://arxiv.org/abs/2407.09121
tags:
- safety
- harmful
- response
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces Decoupled Refusal Training (DeRTa) to address
  a refusal position bias in safety tuning data, which limits LLMs'' ability to appropriately
  refuse unsafe content. The method employs two novel components: Maximum Likelihood
  Estimation with Harmful Response Prefix, which trains models to recognize and avoid
  unsafe content by prepending harmful response segments to safe responses, and Reinforced
  Transition Optimization, which reinforces transitions from potential harm to safety
  refusal throughout harmful response sequences.'
---

# Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training

## Quick Facts
- arXiv ID: 2407.09121
- Source URL: https://arxiv.org/abs/2407.09121
- Reference count: 40
- Surpasses GPT-4 in defending against advanced attack methods including CodeAttack and CompletingAttack

## Executive Summary
This paper addresses a critical vulnerability in large language model safety training: the refusal position bias. Standard safety tuning forces models to make refusal decisions only at the beginning of responses, creating exploitable weaknesses. The authors introduce Decoupled Refusal Training (DeRTa), which trains models to refuse unsafe content at any position through two novel components: Maximum Likelihood Estimation with Harmful Response Prefix and Reinforced Transition Optimization. Evaluated on LLaMA3 and Mistral models across six attack scenarios, DeRTa significantly improves safety while maintaining helpfulness, with one model showing attack success rate reduction from 79.1% to just 8.7%.

## Method Summary
DeRTa addresses safety tuning limitations by implementing two key components. First, MLE with Harmful Response Prefix trains models by prepending random-length segments of harmful responses to safe responses, teaching contextual awareness of unsafe content. Second, Reinforced Transition Optimization reinforces transitions from harmful to safe refusal at every position within harmful response sequences. The method is applied through fine-tuning on LLaMA3 and Mistral model families using safety and helpfulness datasets, with evaluation across multiple attack scenarios including CodeAttack, PAIR, JailbreakChat, SelfCipher, CompletingAttack, and AutoDAN.

## Key Results
- Reduces attack success rates from 79.1% to 8.7% on Mistral-MoE models
- Maintains high helpfulness scores on GSM8K, MMLU, and AlpacaEval benchmarks
- Surpasses GPT-4 and LLaMA3-70B-Instruct in defending against advanced attacks

## Why This Works (Mechanism)

### Mechanism 1
Standard safety tuning creates a "refusal position bias" that forces LLMs to refuse only at the beginning of responses, leaving them vulnerable to attacks that exploit continuation of harmful content. By prepending segments of harmful responses to safe responses during training, DeRTa teaches the model to recognize and refuse unsafe content at any position rather than just the start.

### Mechanism 2
Reinforcing transitions from potential harm to safety refusal at every position within harmful response sequences improves the model's ability to recognize and stop generating unsafe content. RTO trains the model to transition from harmful content to refusal at each token position in harmful responses, creating stronger associations between harmful content and refusal.

### Mechanism 3
The combination of harmful prefix training and reinforced transitions creates a more robust safety mechanism that outperforms models like GPT-4 on advanced attack methods. MLE with harmful prefix provides contextual awareness while RTO ensures consistent refusal capability, together creating a defense that handles both black-box and white-box attacks.

## Foundational Learning

- **Concept**: Safety alignment in LLMs
  - **Why needed here**: Understanding how safety is typically implemented in LLMs is crucial for recognizing why the refusal position bias exists and how DeRTa addresses it
  - **Quick check question**: How do standard safety-tuned LLMs typically handle refusal decisions during response generation?

- **Concept**: Reinforcement learning from human feedback (RLHF)
  - **Why needed here**: DeRTa uses reinforcement learning principles in RTO, so understanding RLHF concepts helps in grasping how the model learns safety transitions
  - **Quick check question**: What is the key difference between standard supervised fine-tuning and RLHF in the context of safety training?

- **Concept**: Adversarial attack methods on LLMs
  - **Why needed here**: Understanding the attack methods (CodeAttack, CompletingAttack, etc.) that DeRTa defends against helps in evaluating the effectiveness of the approach
  - **Quick check question**: What makes completion-type attacks particularly challenging for safety-tuned LLMs to defend against?

## Architecture Onboarding

- **Component map**: Training data generation → MLE with Harmful Prefix training → RTO training → Evaluation on attack scenarios
- **Critical path**: Training data generation → MLE with Harmful Prefix training → RTO training → Evaluation on attack scenarios
- **Design tradeoffs**: The harmful prefix approach adds complexity to training but provides contextual awareness; RTO increases training time significantly but improves safety robustness; the combination may create tension between the two training objectives
- **Failure signatures**: High attack success rates on any attack method indicate training failure; significant drop in helpfulness scores suggests safety training is too restrictive; inconsistent refusal behavior across different positions indicates incomplete training
- **First 3 experiments**:
  1. Test the harmful prefix component alone on a subset of attacks to verify it improves contextual awareness
  2. Test RTO alone to verify it improves position-flexible refusal capability
  3. Test the combined approach on the most challenging attacks (CodeAttack and CompletingAttack) to verify the synergistic effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of DeRTa vary across different types of harmful content (e.g., financial crimes vs. physical harm)?
- Basis in paper: [explicit] The paper mentions testing across six attack scenarios but does not provide a detailed breakdown of performance across different categories of harmful content
- Why unresolved: The paper focuses on overall attack success rates without analyzing the nuanced performance differences when dealing with various types of harmful content
- What evidence would resolve it: A detailed analysis showing the attack success rates for each type of harmful content, such as financial crimes, physical harm, and misinformation, would clarify how DeRTa performs across different categories

### Open Question 2
- Question: What are the long-term effects of DeRTa on the model's general language understanding and generation capabilities?
- Basis in paper: [inferred] The paper mentions that DeRTa improves safety without compromising performance, but it does not discuss the long-term impact on the model's general capabilities
- Why unresolved: The paper does not provide data on the sustained impact of DeRTa on the model's ability to perform general tasks over time
- What evidence would resolve it: Longitudinal studies tracking the model's performance on various tasks over an extended period would provide insights into the long-term effects of DeRTa

### Open Question 3
- Question: How does DeRTa handle ambiguous or borderline harmful content where the context is not clear?
- Basis in paper: [inferred] The paper does not address how DeRTa deals with ambiguous content that may not be clearly harmful or safe
- Why unresolved: The paper focuses on clear-cut harmful content but does not explore the model's ability to navigate ambiguous scenarios
- What evidence would resolve it: Experiments involving ambiguous prompts that require nuanced judgment would demonstrate how well DeRTa can handle such cases

## Limitations

- Limited empirical evidence for the extent and impact of refusal position bias across different model families
- Evaluation focuses primarily on six attack scenarios without addressing generalization to real-world applications
- Claims of superiority over GPT-4 are not directly substantiated with head-to-head comparisons

## Confidence

**High Confidence**: The core technical methodology of DeRTa is clearly specified and reproducible. The two-component approach (MLE with Harmful Prefix and RTO) is well-defined, and the implementation details provided are sufficient for replication.

**Medium Confidence**: The empirical results showing reduced Attack Success Rates across multiple attack scenarios appear robust. The consistent improvements across both LLaMA3 and Mistral model families suggest the approach has broad applicability.

**Low Confidence**: Claims of superiority over GPT-4 are not fully substantiated. While the paper states that DeRTa "surpasses notable models including GPT-4," there is no direct comparison or detailed analysis showing GPT-4's performance on the same attack scenarios.

## Next Checks

1. **Direct GPT-4 Benchmark**: Implement the exact same attack scenarios (CodeAttack, CompletingAttack, etc.) on GPT-4 to verify the claimed superiority. This requires controlled testing environment and careful attack implementation to ensure fair comparison.

2. **Position-Flexible Refusal Analysis**: Conduct ablation studies removing either the MLE with Harmful Prefix component or the RTO component to quantify their individual contributions. Additionally, analyze refusal behavior at different response positions to empirically validate the existence and impact of the refusal position bias.

3. **Real-World Safety Transfer**: Test DeRTa-enhanced models on safety benchmarks that reflect practical deployment scenarios, such as the HELM benchmark or custom evaluations involving complex multi-turn conversations where safety decisions must be made contextually rather than in response to single queries.