---
ver: rpa2
title: 'LLaVA Steering: Visual Instruction Tuning with 500x Fewer Parameters through
  Modality Linear Representation-Steering'
arxiv_id: '2412.12359'
source_url: https://arxiv.org/abs/2412.12359
tags:
- visual
- mores
- arxiv
- steering
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of modality imbalance in multimodal
  large language models (MLLMs), where textual representations often dominate visual
  ones during visual instruction tuning, even when using parameter-efficient fine-tuning
  (PEFT) methods. The authors propose Modality Linear Representation-Steering (MoReS),
  a method that rebalances intrinsic modalities by steering visual representations
  through linear transformations in a visual subspace across each model layer.
---

# LLaVA Steering: Visual Instruction Tuning with 500x Fewer Parameters through Modality Linear Representation-Steering

## Quick Facts
- arXiv ID: 2412.12359
- Source URL: https://arxiv.org/abs/2412.12359
- Authors: Jinhe Bi; Yujun Wang; Haokun Chen; Xun Xiao; Artur Hecker; Volker Tresp; Yunpu Ma
- Reference count: 24
- Key outcome: MoReS achieves comparable performance to LoRA across three visual benchmarks and six visual question-answering tasks while requiring 287 to 1,150 times fewer trainable parameters.

## Executive Summary
This paper addresses the problem of modality imbalance in multimodal large language models (MLLMs), where textual representations often dominate visual ones during visual instruction tuning, even when using parameter-efficient fine-tuning (PEFT) methods. The authors propose Modality Linear Representation-Steering (MoReS), a method that rebalances intrinsic modalities by steering visual representations through linear transformations in a visual subspace across each model layer. The approach freezes the entire LLM during visual instruction tuning and only updates the steering parameters, significantly reducing the number of trainable parameters. Experiments show that MoReS achieves comparable performance to LoRA across three visual benchmarks and six visual question-answering tasks while requiring 287 to 1,150 times fewer trainable parameters. Additionally, MoReS outperforms other PEFT methods in mitigating hallucinations and maintaining pre-trained world knowledge. The authors also introduce the LLaVA Steering Factory, a framework for developing and evaluating MLLMs with minimal coding effort.

## Method Summary
The paper proposes Modality Linear Representation-Steering (MoReS), a parameter-efficient method for visual instruction tuning that addresses modality imbalance in MLLMs. MoReS works by applying linear transformations to visual representations in a compressed subspace, effectively amplifying visual features in the attention distribution without modifying LLM parameters. The method freezes the entire LLM during visual instruction tuning and only updates the steering parameters, significantly reducing the number of trainable parameters. The approach is integrated into LLaVA models of different scales (3B, 7B, 13B) and evaluated using multitask supervised fine-tuning, task-specific fine-tuning, and multi-scale data fine-tuning. MoReS is compared against baseline methods including LoRA, Adapter, OFT, and IA3 across multiple visual benchmarks and question-answering tasks.

## Key Results
- MoReS achieves comparable performance to LoRA across three visual benchmarks and six visual question-answering tasks
- MoReS requires 287 to 1,150 times fewer trainable parameters than baseline methods
- MoReS outperforms other PEFT methods in mitigating hallucinations and maintaining pre-trained world knowledge
- A rank of 1 for the MoReS subspace transformation achieves the highest average performance across four visual tasks while requiring the fewest parameters (0.164M)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MoReS achieves intrinsic modality balance by steering visual representations in a low-dimensional subspace, reducing reliance on textual modality during output generation.
- Mechanism: MoReS applies a linear transformation to visual tokens in a compressed subspace, effectively amplifying visual features in the attention distribution without modifying LLM parameters.
- Core assumption: Visual information can be effectively encoded and manipulated in a lower-dimensional subspace while preserving semantic richness.
- Evidence anchors:
  - [abstract] "re-balance intrinsic modalities by steering visual representations through linear transformations in the visual subspace across each model layer"
  - [section 4] "We define our steering function MoReS as: MoReS(h) = Wup · ϕ(h)" with ϕ(h) = Linear(h) − Wdownh
  - [corpus] Weak evidence - no direct corpus support for the specific steering mechanism
- Break condition: If the visual subspace cannot capture essential semantic information, the steering transformation will fail to effectively balance modalities.

### Mechanism 2
- Claim: MoReS requires significantly fewer trainable parameters by freezing the LLM and only updating steering parameters.
- Mechanism: By preserving pre-trained LLM parameters and only learning a small set of transformation matrices (Wup, Wdown, and linear layer), MoReS maintains capability while reducing parameter count.
- Core assumption: The LLM's pre-trained capabilities in textual modality are sufficient and need not be modified during visual instruction tuning.
- Evidence anchors:
  - [abstract] "MoReS focuses solely on steering the visual representations... freezes the entire LLM during visual instruction tuning"
  - [section 4] "MoReS method can be formulated as follows: Let H = {hi}N i=1 ⊂ RD denote the set of visual representations... MoReS(h) = Wup · ϕ(h)"
  - [corpus] Weak evidence - no direct corpus support for the parameter efficiency claim
- Break condition: If visual instruction tuning requires significant modification of LLM parameters, freezing them will limit performance gains.

### Mechanism 3
- Claim: MoReS improves visual modality utilization while reducing hallucination in MLLMs.
- Mechanism: By redistributing attention towards visual representations, MoReS ensures outputs are more grounded in visual context rather than relying on language priors.
- Core assumption: The attention distribution directly influences the model's reliance on visual vs. textual information for output generation.
- Evidence anchors:
  - [abstract] "MoReS outperforms other PEFT methods in mitigating hallucinations and maintaining pre-trained world knowledge"
  - [section 5.6] "MoReS method significantly outperforms existing tuning approaches in mitigating hallucinations"
  - [corpus] Weak evidence - no direct corpus support for the hallucination mitigation mechanism
- Break condition: If the visual information is insufficient or noisy, steering attention towards it may increase errors rather than reduce hallucinations.

## Foundational Learning

- Concept: Attention mechanism in Transformers
  - Why needed here: Understanding how attention scores are computed and distributed between modalities is critical for grasping MoReS's steering approach
  - Quick check question: How is the attention score between two tokens computed in a Transformer layer?

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: MoReS is a PEFT method, so understanding the space of PEFT approaches (LoRA, Adapter, etc.) provides context for its innovations
  - Quick check question: What distinguishes LoRA from full fine-tuning in terms of parameter updates?

- Concept: Linear algebra and subspace transformations
  - Why needed here: MoReS operates by projecting visual representations into and out of a lower-dimensional subspace
  - Quick check question: What is the mathematical relationship between a matrix and its low-rank approximation?

## Architecture Onboarding

- Component map:
  - Vision encoder (CLIP/ViT or SigLIP) → Connector (MLP) → LLM (Vicuna/Phi-2) → MoReS layers (in each transformer block)
  - MoReS components: Downsampling matrix Wdown, linear transformation, upsampling matrix Wup

- Critical path:
  - Visual token → MoReS transformation (ϕ(h) = Linear(h) - Wdownh) → Wup projection → attention computation → output generation
  - The steering operation occurs at each transformer layer for a subset of visual tokens

- Design tradeoffs:
  - MoReS trades computational overhead during inference (keeping transformation layers) for parameter efficiency during training
  - The choice of subspace rank (1 vs. higher) balances representational capacity against parameter count
  - Sparse steering (1% of tokens) reduces computation but may miss important visual information

- Failure signatures:
  - If visual modality attention scores remain low despite MoReS, the subspace transformation may be ineffective
  - If text-only performance degrades, the MoReS layers may be interfering with textual token processing
  - If parameter count savings are not realized, the MoReS implementation may be incorrectly configured

- First 3 experiments:
  1. Implement MoReS with rank=1 and 1% token steering on a small LLaVA model, measure attention distribution change
  2. Compare performance and parameter count against LoRA baseline on VQAv2 dataset
  3. Test hallucination mitigation by evaluating on POPE benchmark with and without MoReS

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal rank for the MoReS subspace transformation across different model scales and tasks?
- Basis in paper: [explicit] The paper shows that a rank of 1 achieves the highest average performance across four visual tasks while requiring the fewest parameters (0.164M).
- Why unresolved: The experiments only tested ranks of 1, 2, 4, and 8, leaving uncertainty about whether higher ranks could yield better performance on specific tasks or model scales.
- What evidence would resolve it: Systematic testing of MoReS with a wider range of ranks (e.g., 16, 32, 64) across multiple model scales (3B, 7B, 13B) and diverse tasks to identify the optimal rank for each scenario.

### Open Question 2
- Question: How does the MoReS method perform when integrated with other vision encoders beyond CLIP and SigLIP?
- Basis in paper: [inferred] The paper only evaluates MoReS with CLIP ViT-L/14 336px and SigLIP-SO400M-Patch14-384, suggesting potential limitations in generalizability to other vision encoders.
- Why unresolved: The paper does not provide evidence of MoReS's effectiveness with other popular vision encoders like DINOv2 or OpenCLIP, which could impact its broader applicability.
- What evidence would resolve it: Testing MoReS with a variety of vision encoders across different datasets and tasks to assess its performance and robustness.

### Open Question 3
- Question: What is the impact of varying the steered visual token ratio beyond the tested 1% on model performance and efficiency?
- Basis in paper: [explicit] The paper tests steered visual token ratios of 1%, 5%, 50%, and 100%, finding 1% to be optimal, but does not explore intermediate values or values below 1%.
- Why unresolved: The optimal ratio may depend on the specific task or dataset, and exploring a finer granularity of ratios could reveal more nuanced insights into the trade-off between performance and efficiency.
- What evidence would resolve it: Conducting experiments with a range of steered visual token ratios (e.g., 0.1%, 0.5%, 2%, 10%) to determine the optimal ratio for different tasks and datasets.

### Open Question 4
- Question: How does the MoReS method compare to other representation steering techniques in terms of effectiveness and efficiency?
- Basis in paper: [inferred] The paper mentions other representation steering approaches like those by Singh et al. (2024) and Avitan et al. (2024), but does not provide a direct comparison with MoReS.
- Why unresolved: Without a direct comparison, it is unclear whether MoReS offers unique advantages or if similar results could be achieved with alternative steering methods.
- What evidence would resolve it: Implementing and comparing MoReS with other representation steering techniques on the same tasks and datasets to evaluate their relative effectiveness and efficiency.

## Limitations

- The paper's claims about MoReS achieving intrinsic modality balance lack mechanistic validation through controlled experiments that isolate the effect of subspace transformations
- The 287-1,150x parameter reduction claim depends heavily on specific implementation details of MoReS and baseline methods that aren't fully specified
- Hallucination mitigation results, while promising, are based on benchmark evaluations rather than controlled experiments that isolate MoReS's effect from other factors

## Confidence

- **High Confidence**: Claims about MoReS achieving parameter efficiency through freezing LLM parameters and updating only steering parameters are well-supported by experimental results
- **Medium Confidence**: Claims about MoReS improving visual modality utilization and reducing hallucinations are supported by benchmark results but could benefit from more mechanistic analysis
- **Low Confidence**: Claims about the specific mechanism by which MoReS achieves modality balance through subspace transformations lack direct validation

## Next Checks

1. Conduct controlled experiments that isolate the effect of MoReS by comparing models with and without MoReS layers while keeping all other components identical, measuring attention distribution changes at each layer

2. Implement a detailed parameter accounting system that tracks trainable parameters throughout training, ensuring that the claimed 287-1,150x reduction is accurate and not influenced by implementation artifacts

3. Evaluate MoReS on out-of-distribution visual tasks and datasets not seen during training to verify that the modality balancing generalizes beyond the specific benchmarks used in the paper