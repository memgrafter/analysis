---
ver: rpa2
title: Prompting Large Language Models with Knowledge Graphs for Question Answering
  Involving Long-tail Facts
arxiv_id: '2405.06524'
source_url: https://arxiv.org/abs/2405.06524
tags:
- knowledge
- triples
- llms
- long-tail
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces LTGen, a fully automatic pipeline for generating
  benchmarks that assess LLMs'' ability to handle long-tail facts. LTGen includes
  two datasets: LTGen-QA for simple question answering and LTGen-Conv for conversational
  QA.'
---

# Prompting Large Language Models with Knowledge Graphs for Question Answering Involving Long-tail Facts

## Quick Facts
- arXiv ID: 2405.06524
- Source URL: https://arxiv.org/abs/2405.06524
- Reference count: 40
- State-of-the-art LLMs struggle with long-tail facts but show significant improvement when prompted with non-parametric knowledge sources, particularly KG triples.

## Executive Summary
This paper introduces LTGen, a fully automatic pipeline for generating benchmarks that assess Large Language Models' (LLMs) ability to handle long-tail facts in question answering tasks. The authors evaluate state-of-the-art LLMs on two datasets: LTGen-QA for simple question answering and LTGen-Conv for conversational QA. The study compares the effectiveness of prompting LLMs with unstructured textual passages versus structured knowledge graph (KG) triples, finding that KG triples are more effective and efficient. The research also demonstrates that combining both knowledge sources reduces hallucination tendencies in LLM-generated responses.

## Method Summary
The authors propose a comprehensive evaluation framework that prompts LLMs with different non-parametric knowledge sources for long-tail question answering. The method involves retrieving relevant knowledge from Wikipedia passages using Contriever and from Wikidata using a KG triple retrieval pipeline that employs TAGME for entity linking, SPARQL queries for triple retrieval, and AMR-based or LLM-based ranking for selecting the most relevant triples. LLMs (GPT-3.5 and LLaMA2 variants) are prompted in a zero-shot generation setting with questions and knowledge sources, and their outputs are evaluated using knowledge matching scores and NLI-based metrics to assess both factual accuracy and hallucination reduction.

## Key Results
- KG triples significantly outperform textual passages in improving LLM performance on long-tail question answering tasks.
- Combining KG triples and passages achieves the highest NLI-based metric scores while reducing hallucination tendencies.
- AMR-based relation linking approach achieves higher recall scores compared to LLM-based ranking for KG triple retrieval.
- KG triples are more efficient than passages, consuming fewer tokens while providing more focused knowledge.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prompting LLMs with knowledge graph (KG) triples is more effective than prompting with unstructured textual passages for long-tail question answering.
- **Mechanism:** KG triples provide structured, concise knowledge that is less noisy and more aligned with the triple-based representation of knowledge in the LTGen benchmark. The compact format reduces token consumption and improves focus.
- **Core assumption:** LLMs can effectively parse and utilize structured triple representations when prompted, and the retrieval pipeline yields relevant triples for long-tail entities.
- **Evidence anchors:**
  - [abstract] "KG triples are more effective and efficient than passages, and combining both sources reduces hallucinations."
  - [section] "By prompting LLMs with KG triples, we achieve much better performance in all metrics as opposed to prompting LLMs with passages which further leads to higher inference costs 12, showing both the effectiveness and efficiency of using structural knowledge from KGs."
- **Break condition:** If the KG retrieval pipeline fails to retrieve relevant triples (low recall) or if the LLM cannot parse the triple format, prompting with KG triples will not help and may degrade performance.

### Mechanism 2
- **Claim:** Combining KG triples and textual passages reduces hallucination in LLM-generated responses compared to using either source alone.
- **Mechanism:** The dual-source setup provides complementary information: KG triples give precise relational facts, while passages offer contextual background. This redundancy allows the LLM to cross-check facts, reducing the likelihood of fabricating information.
- **Core assumption:** The LLM can effectively integrate and reconcile information from both structured and unstructured sources without being overwhelmed.
- **Evidence anchors:**
  - [abstract] "Moreover, the amalgamation of both structural knowledge from KGs and unstructured knowledge from passages can reduce hallucination tendencies of the involved LLMs, leading to the highest NLI-based metric scores across various settings."
  - [section] "We also note that the performance gap between different LLMs prompted with KG triples is more clear than prompted with passages." (This suggests that KG triples provide more reliable grounding.)
- **Break condition:** If the LLM struggles to reconcile conflicting information from the two sources or if the additional context from passages introduces noise, hallucination reduction may not occur and performance could degrade.

### Mechanism 3
- **Claim:** The AMR-based relation linking approach outperforms LLM-based ranking for selecting relevant KG triples.
- **Mechanism:** AMR captures fine-grained semantic relations in the question, enabling more accurate matching of question semantics to KG relations. This structured semantic representation improves precision over purely textual ranking.
- **Core assumption:** The AMR encoder can effectively capture the semantic structure of the question and align it with KG relations, and the entity tagging step is sufficiently accurate.
- **Evidence anchors:**
  - [section] "Compared with using LLMs, the AMR relation linking approach achieves a higher Recall score in both datasets of the LTGen benchmark with only less than 0.2B parameters."
  - [section] "The best AMR relation linking approach received close to perfect Recall on the LTGen-QA dataset and over 84% Recall on the LTGen-Conv dataset."
- **Break condition:** If the AMR parser fails to generate accurate graphs for complex questions or if the entity linking step introduces errors, the AMR-based approach will not outperform LLM-based ranking.

## Foundational Learning

- **Concept:** Knowledge Graphs (KGs) and their triple structure (subject-predicate-object).
  - **Why needed here:** The paper's core innovation is prompting LLMs with KG triples; understanding the triple format and how it encodes knowledge is essential to grasp why this approach works.
  - **Quick check question:** Given the triple ("Zsolt Hirling", "participant in", "rowing at the 2004 Summer Olympics"), what is the subject, predicate, and object?
- **Concept:** Abstract Meaning Representation (AMR) and its use in semantic parsing.
  - **Why needed here:** The paper uses an AMR-based approach for ranking KG triples. Understanding how AMR represents sentence semantics is key to understanding why it improves relation linking.
  - **Quick check question:** How does an AMR graph represent the semantic relations in the sentence "Was he part of any Olympic events?"?
- **Concept:** Long-tail entities and their challenges for knowledge-intensive NLP tasks.
  - **Why needed here:** The paper focuses on long-tail facts; understanding what makes an entity "long-tail" and why LLMs struggle with them is fundamental to the problem being addressed.
  - **Quick check question:** How is the long-tail level of an entity determined in this paper, and why do LLMs perform worse on higher long-tail levels?

## Architecture Onboarding

- **Component map:** LTGen benchmark generation pipeline (entity sampling → triple retrieval → sample generation) → KG triple retrieval pipeline (entity tagging → triple retrieval → triple ranking) → LLM prompting with different knowledge sources (no knowledge, passages, KG triples, both) → Evaluation metrics (knowledge matching, NLI-based scores)
- **Critical path:** For evaluating an LLM on LTGen: generate/retrieve test samples → retrieve relevant knowledge (passages and/or KG triples) → prompt LLM with knowledge and sample → generate response → evaluate against reference using knowledge matching and NLI scores
- **Design tradeoffs:**
  - KG triples are more effective but require accurate relation linking; passages are noisier but easier to retrieve
  - Using both sources reduces hallucination but increases token usage and complexity
  - AMR-based ranking is more accurate but adds computational overhead compared to LLM-based ranking
- **Failure signatures:**
  - Low recall in KG triple retrieval (entity tagging errors or SPARQL query issues)
  - LLM ignoring provided knowledge and hallucinating answers
  - AMR parser failing to generate graphs for complex questions
- **First 3 experiments:**
  1. Evaluate an LLM on LTGen-QA without any external knowledge to establish a baseline
  2. Evaluate the same LLM with KG triples (using AMR ranking) to measure the effectiveness of structured knowledge
  3. Evaluate the same LLM with both KG triples and passages to assess hallucination reduction

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of LLMs on long-tail facts vary when using different knowledge graph triple ranking methods beyond AMR-based approaches?
- **Basis in paper:** [explicit] The paper compares AMR-based relation linking with LLM-based relation ranking but does not explore other potential methods.
- **Why unresolved:** The study focuses on AMR-based and LLM-based approaches, leaving open the question of whether other ranking methods could yield better results.
- **What evidence would resolve it:** Conducting experiments with alternative ranking methods such as graph neural networks or transformer-based models specifically trained for relation linking tasks.

### Open Question 2
- **Question:** What is the impact of different entity linking techniques on the accuracy of KG triple retrieval in long-tail question answering?
- **Basis in paper:** [inferred] The paper mentions using TAGME for entity tagging but does not explore other entity linking methods or their effects on retrieval accuracy.
- **Why unresolved:** The study uses a single entity linking approach, which may not be optimal for all types of long-tail entities or question formats.
- **What evidence would resolve it:** Comparing the performance of different entity linking techniques (e.g., BLINK, REL, or custom-trained models) on the same benchmark.

### Open Question 3
- **Question:** How does the inclusion of qualifier knowledge in KG triples affect the performance of LLMs in long-tail question answering tasks?
- **Basis in paper:** [explicit] The paper discusses relational and literal knowledge but does not investigate the role of qualifier knowledge in improving LLM performance.
- **Why unresolved:** Qualifier knowledge is mentioned as a form of knowledge in KBs, but its potential impact on LLM performance is not explored.
- **What evidence would resolve it:** Designing experiments that incorporate qualifier knowledge into the KG triples and measuring the impact on LLM performance in long-tail question answering.

## Limitations

- The effectiveness claims for KG triples over passages are based on a specific benchmark and retrieval setup, raising questions about generalizability to other domains or knowledge sources.
- The AMR-based relation linking approach requires accurate entity linking and AMR parsing, which may not generalize well to complex or ambiguous questions.
- The reduction in hallucination through dual-source prompting is inferred from NLI-based scores but not directly validated against human judgments.

## Confidence

- **High Confidence:** KG triples are more effective than passages for prompting LLMs on long-tail facts (supported by multiple experiments and clear performance gaps)
- **Medium Confidence:** Combining KG triples and passages reduces hallucination (supported by NLI scores but not directly validated)
- **Medium Confidence:** AMR-based relation linking outperforms LLM-based ranking (supported by recall scores but requires more robust evaluation across diverse question types)

## Next Checks

1. **Replicate the core experiments:** Reproduce the evaluation of GPT-3.5 and LLaMA2 models on LTGen-QA and LTGen-Conv with different knowledge sources (no knowledge, passages, KG triples, both) to verify the reported performance differences.

2. **Test generalization:** Evaluate the prompting approach with KG triples on a different long-tail QA benchmark (e.g., HotpotQA or WikiHop) to assess if the effectiveness holds beyond LTGen.

3. **Analyze failure cases:** Conduct error analysis on LLM outputs that ignore provided knowledge or show high hallucination. Identify patterns in questions/types of knowledge that lead to failures and assess if dual-source prompting mitigates these specific cases.