---
ver: rpa2
title: Transformer Circuit Faithfulness Metrics are not Robust
arxiv_id: '2407.08734'
source_url: https://arxiv.org/abs/2407.08734
tags:
- circuit
- ablation
- faithfulness
- circuits
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Circuit faithfulness metrics are highly sensitive to methodological
  choices, meaning measured faithfulness reflects both actual circuit components and
  experimental setup. Small changes in ablation methodology (e.g., node vs edge ablation,
  mean vs resample ablation, token position selection) can produce dramatically different
  faithfulness scores.
---

# Transformer Circuit Faithfulness Metrics are not Robust

## Quick Facts
- arXiv ID: 2407.08734
- Source URL: https://arxiv.org/abs/2407.08734
- Authors: Joseph Miller; Bilal Chughtai; William Saunders
- Reference count: 29
- Key outcome: Circuit faithfulness scores are highly sensitive to methodological choices, making measured faithfulness reflect both actual circuit components and experimental setup

## Executive Summary
Circuit faithfulness metrics, used to evaluate how well a proposed circuit explains model behavior, are highly sensitive to seemingly minor methodological choices. The paper demonstrates that changing ablation granularity (node vs edge), ablation value (mean vs resample), token positions, or ablation direction can produce dramatically different faithfulness scores for the same circuit. This sensitivity has significant implications for automated circuit discovery, as faithfulness scores may reward circuits that optimize for specific methodological choices rather than capturing true model behavior. The authors release AutoCircuit, a library with efficient implementations of various ablation methodologies and circuit discovery algorithms.

## Method Summary
The paper systematically evaluates circuit faithfulness across different ablation methodologies including node vs edge ablation, mean vs resample ablation values, token position selection, and ablation direction. Using GPT-2 models and known circuits (IOI, Docstring, Sports Players), they measure logit difference recovery and correct answer percentage under various experimental conditions. They also develop Tracr models with known ground truth circuits to study how optimal circuits depend on the ablation methodology used. The analysis reveals that faithfulness scores vary widely not just across methodologies but also across individual datapoints within a distribution.

## Key Results
- IOI circuit logit difference recovery ranges from negative to over 100% depending on methodology
- Optimal circuits cannot be defined without specifying the ablation methodology
- Faithfulness variance across individual datapoints reveals circuits balance extreme scores rather than faithfully reproducing behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Circuit faithfulness scores reflect both the methodological choices of researchers and the actual components of the circuit.
- Mechanism: The paper demonstrates that faithfulness scores vary significantly when researchers change their ablation methodology, including choices about granularity (node vs edge), ablation value (mean vs resample), token positions, and ablation direction. These variations in methodology lead to dramatically different faithfulness scores for the same circuit, indicating that the measured faithfulness is not solely determined by the circuit's components but also by how it is tested.
- Core assumption: The ablation methodology fundamentally shapes what the circuit is being asked to do, making the task definition dependent on the ablation approach.
- Evidence anchors:
  - [abstract] "we find existing methods are highly sensitive to seemingly insignificant changes in the ablation methodology"
  - [section 4] "we empirically demonstrate that evaluations of a given circuit's faithfulness are highly sensitive to the experimental choices outlined in Section 3 made at evaluation time"
  - [corpus] "Circuit Faithfulness Metrics Are Not Robust" - paper title directly supports this mechanism

### Mechanism 2
- Claim: Optimal circuits cannot be defined without specifying the ablation methodology.
- Mechanism: The paper shows through case studies on Tracr models that the "ground truth" circuit depends on whether zero or resample ablation is used. For the X-Proportion task, the circuit that includes the edge from Input to Attn 1.0 is optimal with zero ablation (since it destroys positional encoding), but not optimal with resample ablation (since positional encoding is constant across clean and corrupt distributions). This demonstrates that the optimal circuit is task-methodology specific.
- Core assumption: The circuit's purpose is defined by the ablation methodology, not just by the input distribution.
- Evidence anchors:
  - [section 5] "The optimal circuit for some distribution cannot be defined unless we also specify the ablation methodology and metric that we are using to measure it"
  - [section 5] "This case study illustrates that the optimal circuit with respect to only a set of prompts is undefined. The ablation partly determines the task."
  - [corpus] "X-Proportion model performs the task of outputting at each token position the proportion of previous characters that are 'x's" - shows the task is well-defined but circuit depends on ablation method

### Mechanism 3
- Claim: Faithfulness variance across individual datapoints reveals that circuits are optimized to balance extreme scores rather than faithfully reproducing behavior.
- Mechanism: The paper shows that even with a fixed methodology, faithfulness scores vary widely across individual prompts in the distribution. For IOI circuit, the inter-quartile range stretches up to 50% across the dataset, with extreme outliers in the tens of thousands of percent. This variance suggests circuits are balancing extremely high (>100%) and extremely low (<0%) faithfulness scores rather than faithfully reproducing behavior across the entire distribution.
- Core assumption: An ideal circuit should have low faithfulness variance over the task input distribution.
- Evidence anchors:
  - [section 4] "There is a large range of logit difference recovered... The inter-quartile range (IQR) is also large, stretching up to 50% across the dataset"
  - [section 4] "It is concerning: while the circuit matches the behavior on average, it does not match it for many examples"
  - [corpus] "faithfulness scores may reward circuits that optimize for specific methodological choices rather than capturing true model behavior" - supports concern about optimization behavior

## Foundational Learning

- Concept: Transformer architecture fundamentals (residual connections, attention mechanisms, MLP layers)
  - Why needed here: Understanding how transformers process information is essential for grasping why different ablation methodologies produce different results. The paper discusses node vs edge ablation, which requires understanding the factorized vs residual view of transformers.
  - Quick check question: What is the difference between node ablation and edge ablation in terms of which causal pathways are affected?

- Concept: Causal inference and intervention concepts (do-calculus, counterfactuals)
  - Why needed here: The paper uses causal language to describe ablations and interventions on model activations. Understanding these concepts is crucial for following the discussion of how ablations isolate causal effects.
  - Quick check question: How does an ablation intervention differ from simply observing model behavior on different inputs?

- Concept: Statistical measurement and evaluation concepts (variance, inter-quartile range, significance testing)
  - Why needed here: The paper extensively discusses variance in faithfulness scores across different conditions and individual datapoints. Understanding these concepts is essential for interpreting the empirical results.
  - Quick check question: What does it mean for a measurement to have high inter-quartile range, and why is this concerning for circuit evaluation?

## Architecture Onboarding

- Component map: Circuit specification parser -> Ablation methodology selector -> Circuit evaluation engine -> Faithfulness metric calculator -> Dataset manager -> Visualization tools
- Critical path: The main workflow is: specify circuit → choose ablation methodology → run ablation → calculate faithfulness metric → analyze variance. The critical path is ensuring the ablation methodology correctly implements the intended intervention and that the faithfulness metric accurately captures the desired comparison.
- Design tradeoffs: The paper highlights tradeoffs between different ablation methodologies. Edge ablation is more specific but computationally expensive compared to node ablation. Mean ablation preserves constant information while resample ablation doesn't. The choice affects both computational cost and what aspects of model behavior are measured.
- Failure signatures: Key failure modes include: high variance in faithfulness scores across individual datapoints (suggesting the circuit doesn't faithfully reproduce behavior for all examples), methodology-dependent optimal circuits (suggesting the circuit is optimized for the ablation method rather than the task), and methodology-specific faithfulness scores that don't generalize across reasonable variations.
- First 3 experiments:
  1. Reproduce the IOI circuit evaluation using different ablation methodologies (node vs edge, mean vs resample) to observe faithfulness score variations.
  2. Implement the X-Proportion Tracr model and test how the optimal circuit changes between zero and resample ablation methodologies.
  3. Analyze variance in faithfulness scores across individual datapoints for a given circuit and methodology to quantify the consistency of circuit behavior.

## Open Questions the Paper Calls Out

- **Question 1**: What is the optimal ablation methodology for measuring circuit faithfulness that balances sensitivity to true circuit components versus methodological artifacts?
  - Basis in paper: [explicit] The paper demonstrates that different ablation methodologies (node vs edge, mean vs resample, token position selection) produce dramatically different faithfulness scores for the same circuit
  - Why unresolved: The paper shows that no single methodology is universally "correct" and that the optimal circuit depends on the ablation methodology used. Different methodologies reveal different aspects of model behavior.
  - What evidence would resolve it: Systematic comparison across many circuits and tasks using standardized metrics to determine which methodological choices consistently identify true circuit components versus methodological artifacts.

- **Question 2**: How can automated circuit discovery algorithms be evaluated in a way that accounts for the sensitivity of faithfulness metrics to ablation methodology?
  - Basis in paper: [explicit] The authors show that optimal circuits cannot be defined without specifying the ablation methodology, and that automated discovery algorithms may optimize for specific methodological choices rather than true circuit components
  - Why unresolved: Current evaluation approaches compare discovered circuits to "ground truth" circuits found using specific methodologies, but this may be misleading if the methodologies differ
  - What evidence would resolve it: Development of evaluation frameworks that account for methodological dependencies, perhaps by requiring circuits to perform well across multiple ablation methodologies or by establishing methodology-independent evaluation criteria.

- **Question 3**: What is the relationship between circuit variance across individual data points and the quality or completeness of the circuit explanation?
  - Basis in paper: [explicit] The authors observe high variance in faithfulness scores across individual prompts, even for circuits that perform well on average
  - Why unresolved: The paper notes this variance is concerning but does not establish whether high variance indicates incomplete circuits, task complexity, or other factors
  - What evidence would resolve it: Analysis of circuits with varying levels of variance to determine if low-variance circuits consistently provide better explanations, or if high-variance circuits reveal important aspects of model behavior that average-case metrics miss.

## Limitations

- The paper focuses primarily on sensitivity to methodological choices without establishing which methodologies provide the most accurate reflection of true circuit behavior
- The analysis is limited to specific model families (GPT-2) and may not generalize to other transformer architectures or scales
- High variance in faithfulness scores across individual datapoints is observed but not fully explained - could reflect circuit incompleteness or inherent task complexity

## Confidence

- Circuit faithfulness metrics are not robust (High): Well-supported by empirical evidence showing dramatic score variations across methodologies
- Optimal circuits cannot be defined without specifying ablation methodology (Medium): Compelling given X-Proportion case study but may reflect task specification issues
- Circuits balance extreme scores rather than faithfully reproducing behavior (Medium): Supported by variance analysis but alternative explanations exist

## Next Checks

1. **Methodological Convergence Test**: Apply a standardized set of ablation methodologies to multiple well-studied circuits (IOI, induction, copy) and measure the correlation between faithfulness scores across different approaches. This would help determine whether some methodologies consistently produce similar rankings despite absolute score differences.

2. **Variance Decomposition Analysis**: For a fixed circuit and methodology, decompose the observed variance in faithfulness scores into components attributable to: individual example difficulty, random seed effects in ablation, and measurement noise. This would clarify whether high variance reflects fundamental circuit limitations or experimental artifacts.

3. **Cross-Model Generalization**: Test whether the observed sensitivity patterns hold across different model families (GPT-2, Pythia, BERT) and model sizes. This would help determine if the methodological sensitivity is a universal property of transformer circuits or specific to certain architectures or scales.