---
ver: rpa2
title: 'Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals'
arxiv_id: '2402.11655'
source_url: https://arxiv.org/abs/2402.11655
tags:
- attention
- factual
- last
- layer
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of competition of mechanisms,
  which focuses on the interplay of multiple mechanisms within large language models
  (LLMs) rather than analyzing individual mechanisms in isolation. The authors investigate
  how two mechanisms - factual knowledge recall and in-context adaptation to counterfactual
  statements - compete within GPT-2 and Pythia-6.9B models.
---

# Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals

## Quick Facts
- arXiv ID: 2402.11655
- Source URL: https://arxiv.org/abs/2402.11655
- Authors: Francesco Ortu; Zhijing Jin; Diego Doimo; Mrinmaya Sachan; Alberto Cazzaniga; Bernhard Schölkopf
- Reference count: 28
- Primary result: This paper introduces the concept of competition of mechanisms, which focuses on the interplay of multiple mechanisms within large language models (LLMs) rather than analyzing individual mechanisms in isolation.

## Executive Summary
This paper investigates how language models handle competing mechanisms through the lens of factual knowledge recall versus in-context adaptation to counterfactual statements. The authors propose a framework for analyzing mechanism competition rather than studying mechanisms in isolation. Using logit inspection and attention modification methods, they trace how factual and counterfactual mechanisms compete across different model components in GPT-2 and Pythia-6.9B. The study reveals that while factual information primarily flows from the subject position in early layers, counterfactual information comes from the attribute position, with both mechanisms and their competition intensifying in later layers.

## Method Summary
The authors analyze mechanism competition using two primary methods: logit inspection and attention modification. Logit inspection projects model component outputs to vocabulary space using an unembedding matrix to trace information flow across layers and positions. Attention modification intervenes on attention weights to test hypotheses about which components control mechanism strength. The experiments use the COUNTER FACT 2 dataset with 10K data points containing factual statements and corresponding counterfactual statements. The analysis focuses on GPT-2 small and Pythia-6.9B models, examining how the competition between factual recall and counterfactual adaptation manifests across different layers, attention heads, and token positions.

## Key Results
- Information flows from different token positions for different mechanisms, with subject position contributing most to factual information in early layers and attribute position to counterfactual information
- Both mechanisms and their competition primarily occur in late layers of the models
- A few specialized attention heads contribute most to the competition, with some promoting the counterfactual mechanism and others suppressing it
- Modifying attention weights in a few localized positions can substantially increase factual recall
- More similar factual and counterfactual tokens lead to greater confusion and dominance of the factual mechanism, especially in larger models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The competition of mechanisms within LLMs is observable through the interplay of factual knowledge recall and in-context adaptation to counterfactual statements.
- Mechanism: Different mechanisms activate different token positions in early layers (subject for factual, attribute for counterfactual), and competition intensifies in later layers where attention heads dominate.
- Core assumption: LLMs can simultaneously activate multiple mechanisms, and one will prevail in the final prediction based on learned weights.
- Evidence anchors:
  - [abstract] "We propose a formulation of competition of mechanisms, which focuses on the interplay of multiple mechanisms instead of individual mechanisms and traces how one of them becomes dominant in the final prediction."
  - [section] "Our results reveal the prevalence of each mechanism by varying the layer l and position i."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.511, average citations=0.0."
- Break condition: If mechanisms cannot be distinguished through logit inspection or attention modification, the competition model breaks down.

### Mechanism 2
- Claim: Attention heads are the primary drivers of mechanism competition, with a few specialized heads contributing most to the outcome.
- Mechanism: Certain attention heads promote counterfactual information while others suppress it, with most information flowing from the attribute position.
- Core assumption: Attention heads can be isolated and their contributions quantified through logit differences.
- Evidence anchors:
  - [abstract] "Our findings show traces of the mechanisms and their competition across various model components and reveal attention positions that effectively control the strength of certain mechanisms."
  - [section] "We find that we can up-weight the value of a few very localized values of the attention head matrix to strengthen factual mechanisms substantially."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.511, average citations=0.0."
- Break condition: If modifying attention weights doesn't affect factual recall, the attention head hypothesis is weakened.

### Mechanism 3
- Claim: The similarity between factual and counterfactual tokens influences which mechanism dominates.
- Mechanism: More similar tokens confuse the model, making it more likely to default to the factual mechanism, especially in larger models.
- Core assumption: Token similarity can be quantified and correlated with mechanism dominance.
- Evidence anchors:
  - [abstract] "More similar factual and counterfactual tokens lead to greater confusion and dominance of the factual mechanism, especially in larger models."
  - [section] "Finding 1: Similar tokens confuse the model more easily. Consistently across all the models, the more similar the two tokens are, the more likely the model is to be confused and mistakenly let the factual mechanism dominate."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.511, average citations=0.0."
- Break condition: If token similarity doesn't correlate with mechanism dominance, the confusion hypothesis fails.

## Foundational Learning

- Concept: Logit inspection
  - Why needed here: To trace how different mechanisms activate across layers and positions within the model.
  - Quick check question: Can you explain how projecting residual streams to vocabulary space reveals token preferences?
- Concept: Attention modification
  - Why needed here: To intervene on model activations and test hypotheses about which components control mechanism strength.
  - Quick check question: What happens to factual recall when you scale up attention weights from the attribute position?
- Concept: Mechanism competition
  - Why needed here: To understand how multiple mechanisms interact rather than analyzing them in isolation.
  - Quick check question: How would you distinguish between a model failing to recognize a mechanism versus successfully suppressing it?

## Architecture Onboarding

- Component map: GPT-2 small (12 layers, 12 attention heads each, residual stream dimension 768) and Pythia-6.9B (32 layers, 32 attention heads each, dimension 4096)
- Critical path: Token embedding → attention blocks → MLP blocks → unembedding matrix → vocabulary logits
- Design tradeoffs: GPT-2 is simpler for initial analysis but may not generalize; larger models show stronger factual recall but are harder to interpret
- Failure signatures: If logit inspection shows no clear mechanism separation, if attention modification has no effect, or if token similarity doesn't correlate with outcomes
- First 3 experiments:
  1. Run logit inspection across layers and positions for a small set of factual/counterfactual pairs
  2. Identify attention heads with largest ∆cofa values and test their individual contributions
  3. Modify attention weights for top contributors and measure change in factual recall rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do larger models (e.g., GPT-4) handle the competition between factual and counterfactual mechanisms compared to GPT-2 and Pythia-6.9B?
- Basis in paper: [explicit] The paper discusses the findings on GPT-2 and Pythia-6.9B, noting that larger models suffer more from confusion between similar factual and counterfactual tokens.
- Why unresolved: The study only includes GPT-2 and Pythia-6.9B, and does not extend to larger models like GPT-4.
- What evidence would resolve it: Experiments on larger models like GPT-4 showing their behavior in the competition of mechanisms, particularly how they handle similar factual and counterfactual tokens.

### Open Question 2
- Question: Can the logit inspection method reliably reflect the importance of vocabulary items in all layers of the network?
- Basis in paper: [explicit] The paper mentions that the logit inspection method can occasionally fail to reflect the actual importance of vocabulary items, especially in the early layers of the network.
- Why unresolved: The paper acknowledges the limitation of the logit inspection method but does not provide a comprehensive analysis of its reliability across all layers.
- What evidence would resolve it: A detailed study comparing the logit inspection method's accuracy across different layers and alternative methods to validate its reliability.

### Open Question 3
- Question: How does the structure of prompts affect the competition of mechanisms within language models?
- Basis in paper: [inferred] The paper suggests that the prompts used have a relatively simple structure for controllability, indicating that more complex prompts might affect the information flow differently.
- Why unresolved: The study uses simple prompt structures, and the impact of more complex or varied prompt structures on the competition of mechanisms is not explored.
- What evidence would resolve it: Experiments with a variety of prompt structures to analyze how they influence the competition of mechanisms and the flow of information within the models.

## Limitations

- The analysis relies on the assumption that logit differences accurately reflect underlying mechanisms, though empirical validation remains limited
- The focus on two specific model architectures (GPT-2 and Pythia-6.9B) may limit generalizability to other model families or sizes
- The attention modification method may have unintended side effects on other model capabilities that weren't thoroughly explored
- The reliance on single-token attributes in the COUNTER FACT 2 dataset constrains the scope of the findings

## Confidence

**High Confidence**: The observation that attention heads contribute differently to factual versus counterfactual mechanisms, and that modifying attention weights can strengthen factual recall. These findings are supported by direct experimental interventions with measurable outcomes.

**Medium Confidence**: The claim that similar factual and counterfactual tokens lead to greater confusion, particularly in larger models. While the experimental results support this, the underlying reasons for why larger models show stronger factual dominance remain speculative.

**Low Confidence**: The broader generalizability of the competition of mechanisms framework across different model architectures and tasks. The paper demonstrates the concept in a specific context but doesn't extensively validate it beyond the studied models and dataset.

## Next Checks

1. **Cross-model validation**: Apply the competition of mechanisms framework to different model architectures (e.g., Llama, Mistral, or Claude) to test whether the observed patterns of mechanism competition hold across diverse model families.

2. **Attention modification ablation**: Systematically test the effects of attention weight modifications on downstream tasks beyond factual recall, such as reasoning, generation quality, and task-specific benchmarks, to ensure the modifications don't introduce harmful side effects.

3. **Token similarity threshold analysis**: Conduct a more granular analysis of how different degrees of token similarity affect mechanism competition, including experiments with controlled token pairs that vary systematically in semantic and surface similarity to better understand the confusion threshold.