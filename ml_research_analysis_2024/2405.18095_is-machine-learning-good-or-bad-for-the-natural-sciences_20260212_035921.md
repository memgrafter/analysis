---
ver: rpa2
title: Is machine learning good or bad for the natural sciences?
arxiv_id: '2405.18095'
source_url: https://arxiv.org/abs/2405.18095
tags:
- data
- sciences
- natural
- learning
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the epistemic tensions between machine learning
  and natural science practices, focusing on ontology (what exists) and epistemology
  (what counts as knowledge). ML prioritizes data-driven prediction over understanding
  latent physical mechanisms, while natural science seeks to explain the world through
  underlying structures and theories.
---

# Is machine learning good or bad for the natural sciences?

## Quick Facts
- arXiv ID: 2405.18095
- Source URL: https://arxiv.org/abs/2405.18095
- Reference count: 36
- Primary result: ML prioritizes data-driven prediction over understanding latent physical mechanisms, creating epistemic tensions with natural science practices

## Executive Summary
This paper examines the fundamental tensions between machine learning approaches and traditional natural science practices, focusing on the clash between ML's data-driven prediction paradigm and science's quest for mechanistic understanding. The authors identify two major statistical biases that arise when ML is applied to scientific problems: confirmation bias from emulator failures and amplified bias in joint analyses. They argue that ML is most valuable for operational tasks requiring real-time decisions and modeling confounders in causal inference, but should be avoided when precise physical understanding is required or when labels will be used in downstream joint analyses.

## Method Summary
The paper takes a conceptual and theoretical approach, examining the epistemic foundations of both machine learning and natural science practices. Rather than presenting empirical studies or experimental results, the authors develop a framework for understanding when ML applications are appropriate versus problematic in scientific contexts. They analyze the statistical properties of ML predictions and their implications for scientific inference, identifying systematic biases that can arise in specific use cases. The methodology relies on logical argumentation and mathematical reasoning about the statistical behavior of ML models rather than empirical validation.

## Key Results
- ML emulators can introduce confirmation bias when they replace physics-based simulations, potentially causing scientists to accept flawed results as physical reality
- When ML regression labels are used in joint analyses, population-level biases remain fixed while variance-based errors average out, leading to strongly biased population estimates
- ML is valuable for operational tasks (real-time decisions, outlier detection) and modeling confounders in causal inference where expressive models provide conservative approaches

## Why This Works (Mechanism)
The paper works by clearly articulating the fundamental philosophical and practical differences between machine learning's prediction-focused approach and natural science's explanation-focused methodology. It identifies specific mechanisms by which ML can introduce systematic errors into scientific workflows, particularly when ML models are used as surrogates for physical understanding or when their outputs are aggregated in ways that preserve certain biases while eliminating others. The framework provides actionable guidance by distinguishing between operational and explanatory scientific tasks, allowing practitioners to make informed decisions about ML deployment based on the nature of their scientific questions.

## Foundational Learning
1. **Epistemological distinction**: Understanding that ML prioritizes prediction accuracy while natural science prioritizes mechanistic understanding. This is needed to grasp why certain ML applications may be philosophically incompatible with scientific goals. Quick check: Can you identify which scientific questions require explanation versus those that only need accurate prediction?

2. **Statistical bias propagation**: Learning how biases in ML labels propagate differently in joint analyses versus individual predictions. This is needed to understand when ML outputs can safely be used versus when they might systematically distort scientific conclusions. Quick check: What happens to population-level versus variance-based errors when averaging multiple ML predictions?

3. **Emulator behavior**: Understanding how ML surrogates for physical simulations can fail in ways that are difficult to distinguish from genuine physical phenomena. This is needed to recognize situations where computational efficiency might come at the cost of scientific validity. Quick check: How can you distinguish between an emulator error and a genuine physical anomaly?

## Architecture Onboarding
Component map: Physical System -> ML Emulator -> Scientific Inference -> Validation
Critical path: Physical System -> Scientific Inference (for understanding mechanisms)
Design tradeoffs: Speed vs accuracy (emulators are faster but may introduce biases), Expressiveness vs interpretability (complex models may capture more but explain less)
Failure signatures: Confirmation of expected results when physical reality suggests anomalies, Systematic biases that persist across ensemble analyses, Inability to distinguish emulator artifacts from physical phenomena
First experiments: 1) Test emulator predictions against known physical scenarios with controlled anomalies, 2) Analyze bias propagation in synthetic joint analysis scenarios, 3) Compare operational versus explanatory ML applications on identical datasets

## Open Questions the Paper Calls Out
None

## Limitations
- The paper identifies statistical biases but lacks quantitative evidence for their prevalence or severity in actual scientific practice
- The distinction between operational and explanatory tasks may oversimplify the spectrum of scientific applications
- No empirical validation is provided for the theoretical claims about confirmation bias from emulators

## Confidence
High confidence in the conceptual framework distinguishing ML's prediction focus from science's explanation focus. Medium confidence in the identified biases as real phenomena, but low confidence in their quantitative impact without empirical validation. Medium confidence in the operational vs explanatory distinction, though this could benefit from more granular categories.

## Next Checks
1. Empirical study measuring the frequency and impact of emulator confirmation bias in published scientific papers that use ML surrogates for physical simulations
2. Systematic review of ML applications in natural sciences to quantify how often joint analysis scenarios occur and whether bias amplification is documented in practice
3. Case study comparison of ML approaches versus traditional statistical methods on identical scientific problems, measuring both prediction accuracy and preservation of physical interpretability