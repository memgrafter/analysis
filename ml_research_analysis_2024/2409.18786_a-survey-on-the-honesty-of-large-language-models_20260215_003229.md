---
ver: rpa2
title: A Survey on the Honesty of Large Language Models
arxiv_id: '2409.18786'
source_url: https://arxiv.org/abs/2409.18786
tags:
- arxiv
- llms
- language
- knowledge
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys research on the honesty of large language models
  (LLMs), focusing on self-knowledge (recognizing known and unknown knowledge) and
  self-expression (faithfully expressing knowledge). The survey reviews evaluation
  methods including recognition of known/unknown, calibration, and selective prediction
  for self-knowledge, and identification-based and identification-free approaches
  for self-expression.
---

# A Survey on the Honesty of Large Language Models

## Quick Facts
- arXiv ID: 2409.18786
- Source URL: https://arxiv.org/abs/2409.18786
- Reference count: 28
- Primary result: This paper surveys research on the honesty of large language models, focusing on self-knowledge (recognizing known and unknown knowledge) and self-expression (faithfully expressing knowledge).

## Executive Summary
This paper provides a comprehensive survey of research on the honesty of large language models, addressing two key aspects: self-knowledge (recognizing known and unknown knowledge) and self-expression (faithfully expressing knowledge). The survey categorizes existing evaluation methods into recognition of known/unknown, calibration, and selective prediction for self-knowledge, and identification-based and identification-free approaches for self-expression. It also reviews improvement strategies, distinguishing between training-free approaches (predictive probability, prompting, sampling and aggregation) and training-based approaches (supervised fine-tuning, reinforcement learning, probing). The paper highlights challenges such as unclear definitions of honesty, difficulties in knowledge identification, and limited evaluation in instruction-following scenarios, while identifying future research directions including subjective vs. objective honesty and extending research to multimodal and long-context models.

## Method Summary
The survey systematically reviews existing literature on LLM honesty through a comprehensive categorization of evaluation methods and improvement strategies. It organizes the research landscape into self-knowledge and self-expression dimensions, then further classifies evaluation approaches into recognition of known/unknown, calibration, and selective prediction, while improvement strategies are divided into training-free (predictive probability, prompting, sampling/ aggregation) and training-based (supervised fine-tuning, reinforcement learning, probing) methods. The paper draws from multiple benchmarks and experimental results across the literature to synthesize current understanding and identify research gaps.

## Key Results
- LLMs exhibit dishonest behaviors including confidently presenting wrong answers and failing to express known knowledge
- Self-knowledge evaluation focuses on recognition of known/unknown, calibration of confidence scores, and selective prediction
- Improvement strategies span training-free methods (prompting, sampling) and training-based approaches (SFT, RL)
- Current research limitations include unclear definitions of honesty and insufficient evaluation in instruction-following scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-knowledge in LLMs can be improved through training-free methods that leverage internal uncertainty signals.
- Mechanism: Methods like predictive probability, prompting, and sampling/ aggregation use the model's own uncertainty estimates to detect knowledge gaps and calibrate confidence.
- Core assumption: The model's internal representations contain signals about its knowledge state that can be extracted without retraining.
- Evidence anchors:
  - [abstract] "recognizing known and unknown knowledge and faithfully expressing knowledge"
  - [section] "The predictive probability of LLM is well-calibrated on multiple-choice tasks... but poorly calibrated for free-form generation tasks"
  - [corpus] Weak; no direct neighbor evidence for training-free uncertainty extraction.
- Break condition: If the model's internal uncertainty signals are unreliable or biased, these methods will fail to improve honesty.

### Mechanism 2
- Claim: Supervised fine-tuning can teach LLMs to verbalize "I don't know" when lacking relevant knowledge.
- Mechanism: By providing examples of correct responses and cases where the model should refuse, SFT aligns the model's outputs with human expectations of honesty.
- Core assumption: LLMs can learn to recognize their knowledge boundaries through examples of known and unknown questions.
- Evidence anchors:
  - [abstract] "current LLMs still frequently exhibit dishonest behaviors... confidently presenting wrong answers or failing to express what they know"
  - [section] "Yang et al. (2023); Zhang et al. (2024a); Cheng et al. (2024) sample multiple candidate answers for each question and compare them with the ground-truth answer, classifying a question as known if the accuracy exceeds a certain threshold"
  - [corpus] Weak; no direct neighbor evidence for SFT-based honesty improvements.
- Break condition: If the training data is noisy or the model cannot generalize knowledge boundaries, SFT may not improve honesty.

### Mechanism 3
- Claim: Reinforcement learning can optimize LLMs for honesty by rewarding accurate knowledge expression and refusal to answer unknown questions.
- Mechanism: RL-based approaches use feedback signals to align the model's outputs with human preferences for honesty, such as accepting correct answers and rejecting incorrect ones.
- Core assumption: The model's outputs can be optimized through trial and error to better reflect its internal knowledge state.
- Evidence anchors:
  - [abstract] "current models still frequently exhibit dishonest behaviors... may provide biased information influenced by human input"
  - [section] "Cheng et al. (2024); Xu et al. (2024a) teach LLMs to abstain from responding to questions they do not know and apply DPO (Rafailov et al., 2024) or PPO (Schulman et al., 2017) for optimization"
  - [corpus] Weak; no direct neighbor evidence for RL-based honesty improvements.
- Break condition: If the reward signals are poorly designed or the model overfits to them, RL may not improve honesty.

## Foundational Learning

- Concept: Calibration
  - Why needed here: Calibration measures how well the model's confidence scores match the actual accuracy of its responses, which is crucial for evaluating and improving honesty.
  - Quick check question: What is the difference between a well-calibrated model and a poorly calibrated model?

- Concept: Knowledge Identification
  - Why needed here: Knowledge identification methods are essential for distinguishing between what the model knows and doesn't know, which is a key aspect of honesty.
  - Quick check question: How do supervised and unsupervised approaches differ in knowledge identification?

- Concept: Self-awareness
  - Why needed here: Self-awareness refers to the model's ability to recognize its own knowledge state, which is a prerequisite for honest behavior.
  - Quick check question: What are some signs that a model lacks self-awareness?

## Architecture Onboarding

- Component map: Evaluation methods (self-knowledge and self-expression) -> Improvement strategies (training-free and training-based) -> Future research directions
- Critical path: The most critical path for improving LLM honesty is likely a combination of knowledge identification methods and supervised fine-tuning to teach the model to refuse unknown questions.
- Design tradeoffs: Training-free methods are simpler but may be less effective than training-based methods, which require more data and computational resources.
- Failure signatures: If the model's honesty does not improve after applying the methods, it may indicate that the methods are not well-suited to the model's architecture or training data.
- First 3 experiments:
  1. Test the effectiveness of knowledge identification methods on a held-out dataset.
  2. Fine-tune the model on a small dataset of known and unknown questions to see if it learns to refuse unknown questions.
  3. Evaluate the model's honesty on a benchmark task before and after applying the improvement methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective methods for eliciting self-knowledge from large language models that accurately reflects the model's internal state rather than being influenced by the training data?
- Basis in paper: [explicit] The paper discusses prompting strategies like P(True), fact-and-reflection, and verbalized confidence, but notes that these may mimic human expressions of confidence rather than genuinely assessing the model's knowledge.
- Why unresolved: While prompting-based methods have shown promise, there is concern about whether the external output accurately represents the model's internal representation. The paper highlights the discrepancy between LLMs' internal knowledge and what they express.
- What evidence would resolve it: Comparative studies between different prompting strategies and their alignment with the model's internal knowledge, using probing techniques to validate the accuracy of elicited self-knowledge.

### Open Question 2
- Question: How can we effectively evaluate the honesty of large language models in instruction-following scenarios that require long-form generation, beyond the current focus on short-form question answering?
- Basis in paper: [explicit] The paper explicitly states that current evaluations predominantly focus on short-form question answering, leaving long-form instruction following scenarios underexplored.
- Why unresolved: Instruction-following tasks differ significantly from question answering in terms of scope and complexity. The paper notes that this gap in evaluation methods limits our understanding of LLM honesty in real-world applications.
- What evidence would resolve it: Development and validation of new benchmarks specifically designed for instruction-following tasks, with metrics that capture both the accuracy and honesty of long-form responses.

### Open Question 3
- Question: What are the most effective strategies for improving the honesty of in-context knowledge in large language models, particularly in retrieval-augmented and long-context scenarios?
- Basis in paper: [explicit] The paper identifies honesty on in-context knowledge as an important future research direction, noting that while most existing research emphasizes parametric knowledge, in-context knowledge also plays a vital role in generation.
- Why unresolved: The paper highlights that current research on honesty focuses primarily on parametric knowledge, leaving the honesty of in-context knowledge underexplored despite its importance in real-world applications.
- What evidence would resolve it: Empirical studies comparing different methods for improving in-context knowledge honesty, such as fine-tuning strategies that incorporate reference knowledge or decoding-time interventions that emphasize contextual information.

## Limitations

- No unified definition of honesty exists in the LLM context, making cross-study comparisons difficult
- Most evaluation methods focus on factual knowledge rather than subjective domains like ethics or personal opinions
- Limited research on honesty in instruction-following scenarios that require long-form generation

## Confidence

- High: The categorization of evaluation methods (recognition, calibration, selective prediction) and improvement strategies (training-free, training-based) is well-supported by the literature
- Medium: Claims about the effectiveness of specific approaches like SFT and RL for improving honesty are plausible but lack direct experimental validation in the survey
- Low: Predictions about future directions, particularly regarding multimodal and long-context models, are speculative given the current state of research

## Next Checks

1. Conduct a controlled experiment comparing knowledge identification performance across multiple LLMs using both model-agnostic and model-specific benchmarks to assess generalizability
2. Implement a small-scale SFT experiment on a dataset of known/unknown questions to verify whether LLMs can learn to refuse unknown questions as claimed
3. Design a calibration test suite that evaluates both multiple-choice and free-form generation tasks to identify whether the "poor calibration in free-form tasks" observation holds across different model architectures