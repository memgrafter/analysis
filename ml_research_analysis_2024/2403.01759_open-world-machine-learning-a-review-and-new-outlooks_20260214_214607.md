---
ver: rpa2
title: 'Open-world machine learning: A review and new outlooks'
arxiv_id: '2403.01759'
source_url: https://arxiv.org/abs/2403.01759
tags:
- learning
- classes
- conference
- pages
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive review of open-world machine
  learning, addressing the limitations of closed-world assumptions in real-world applications.
  The authors systematically examine three key tasks: unknown rejection, novel class
  discovery, and class-incremental learning.'
---

# Open-world machine learning: A review and new outlooks

## Quick Facts
- arXiv ID: 2403.01759
- Source URL: https://arxiv.org/abs/2403.01759
- Reference count: 40
- Primary result: Comprehensive review of open-world machine learning covering unknown rejection, novel class discovery, and class-incremental learning with analysis of current methods and future directions

## Executive Summary
This paper provides a systematic review of open-world machine learning (OWL), addressing the limitations of closed-world assumptions in real-world applications. The authors examine three core tasks: unknown rejection (identifying samples from unknown classes), novel class discovery (grouping unknown samples into new classes), and class-incremental learning (incrementally learning these discovered classes). The review covers existing methodologies, benchmarks, metrics, and identifies key challenges and future research directions for building AI systems capable of continual learning and adaptation in dynamic environments.

## Method Summary
The paper reviews open-world machine learning through a comprehensive analysis of three interconnected tasks: unknown rejection, novel class discovery, and class-incremental learning. For each task, the authors examine discriminative, generative, and hybrid approaches, along with their strengths and limitations. The review covers widely used benchmarks and evaluation metrics across these areas. The methodology involves synthesizing existing literature to identify common patterns, challenges, and gaps in current OWL approaches, while proposing future research directions including unified frameworks, foundation model extensions, and applications to structured data.

## Key Results
- Identified three key tasks in OWL: unknown rejection, novel class discovery, and class-incremental learning
- Analyzed current methodologies and limitations across these areas
- Provided comprehensive overview of benchmarks and metrics used in OWL research
- Highlighted promising future directions including unified frameworks and extension to foundation models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Open-world machine learning (OWL) succeeds by integrating three sequential tasks—unknown rejection, novel class discovery, and class-incremental learning—to handle dynamic environments.
- Mechanism: The system first detects and rejects unknown samples, then clusters them into novel classes, and finally incrementally learns these classes without catastrophic forgetting.
- Core assumption: The semantic similarity between known and unknown classes determines the transferability of knowledge, enabling effective clustering and learning.
- Evidence anchors:
  - [abstract] "rejecting unknowns, discovering novelties, and then incrementally learning them"
  - [section 4.3] "how much knowledge could be transferred essentially influences the performance of category discovery"
  - [corpus] Weak correlation; corpus lacks direct evidence for the three-task integration claim.
- Break condition: If semantic similarity is low, knowledge transfer fails, degrading novel class discovery performance.

### Mechanism 2
- Claim: Exemplar replay and feature replay methods effectively mitigate catastrophic forgetting in class-incremental learning.
- Mechanism: By storing and replaying old data or features during new class learning, the model maintains performance on previously learned classes.
- Core assumption: Saved exemplars or features remain representative despite model updates.
- Evidence anchors:
  - [section 5.2] "exemplar replay strategy... learns both new data and stored old data at each incremental stage"
  - [section 5.3] "feature replay strategies... store and replay features of old classes"
  - [corpus] Weak evidence; corpus doesn't directly address catastrophic forgetting mitigation.
- Break condition: If stored data becomes outdated or unrepresentative, the model suffers from catastrophic forgetting.

### Mechanism 3
- Claim: Unified frameworks for OWL are needed to handle the interactions between unknown rejection, novel class discovery, and class-incremental learning.
- Mechanism: A single model architecture that can perform all three tasks simultaneously, adapting to open-world environments.
- Core assumption: The tasks are interdependent and benefit from joint optimization.
- Evidence anchors:
  - [section 6.1] "It is worth studying the relation or interaction among those three tasks in OWL and developing a unified framework"
  - [section 6.1] "when a model continually adapts and learn in the open environment... incremental learning could profoundly affect the rejection ability"
  - [corpus] No direct evidence; corpus lacks information on unified frameworks.
- Break condition: If tasks are optimized separately, interactions may lead to suboptimal performance.

## Foundational Learning

- Concept: Out-of-distribution (OOD) detection
  - Why needed here: OOD detection is crucial for unknown rejection, identifying samples that don't belong to known classes.
  - Quick check question: How does OOD detection differ from anomaly detection in terms of the semantic distance between OOD and ID samples?

- Concept: Catastrophic forgetting
  - Why needed here: Understanding catastrophic forgetting is essential for class-incremental learning, ensuring the model retains knowledge of old classes.
  - Quick check question: What are the three main issues contributing to catastrophic forgetting in class-incremental learning?

- Concept: Semantic similarity
  - Why needed here: Semantic similarity between known and unknown classes influences the transferability of knowledge in novel class discovery.
  - Quick check question: How does semantic similarity impact the difficulty of novel class discovery compared to unknown rejection?

## Architecture Onboarding

- Component map: Input → Feature extraction → Unknown rejection → Novel class discovery (if rejected) → Class-incremental learning (if novel classes are discovered) → Final classification
- Critical path: Input → Feature extraction → Unknown rejection → Novel class discovery (if rejected) → Class-incremental learning (if novel classes are discovered) → Final classification
- Design tradeoffs: Balancing between the complexity of the model and its ability to handle open-world scenarios; choosing between exemplar replay and feature replay for class-incremental learning
- Failure signatures: High false positive rate in unknown rejection, poor clustering in novel class discovery, and significant performance drop on old classes in class-incremental learning
- First 3 experiments:
  1. Test unknown rejection on a dataset with known and unknown classes to evaluate OOD detection performance.
  2. Evaluate novel class discovery on a dataset with labeled and unlabeled data to assess clustering accuracy.
  3. Assess class-incremental learning on a dataset with sequential tasks to measure catastrophic forgetting and adaptation to new classes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a unified framework that simultaneously handles unknown rejection, novel class discovery, and class-incremental learning in a seamless, end-to-end manner?
- Basis in paper: [explicit] The paper explicitly identifies this as a key future direction, noting that existing methods focus on isolated tasks rather than the entire open-world learning process.
- Why unresolved: Current methods treat these three tasks separately, with no integrated approach that can manage the complete learning cycle from detecting unknowns to incrementally learning them while maintaining performance on known classes.
- What evidence would resolve it: A unified framework that demonstrates superior performance across all three tasks compared to specialized, isolated approaches on standard benchmarks.

### Open Question 2
- Question: How can open-world learning be effectively extended to structured data (e.g., graphs, networks) where traditional methods focusing on individual samples are insufficient?
- Basis in paper: [explicit] The paper identifies this as a promising direction, noting that most existing OWL methods assume independently sampled inputs and neglect topological structures and interconnections among samples.
- Why unresolved: Current OWL techniques are primarily designed for independent samples (like images) and don't account for the complex relationships and dependencies inherent in structured data formats.
- What evidence would resolve it: Novel OWL methods specifically designed for structured data that demonstrate improved performance over adapting traditional methods to graph-based tasks.

### Open Question 3
- Question: What architectural and training innovations are needed to enable foundation models like GPT and CLIP to handle open-world learning scenarios effectively?
- Basis in paper: [explicit] The paper highlights this as a critical challenge, noting that while foundation models show remarkable zero-shot learning, they lack open-world learning abilities and struggle with continual adaptation.
- Why unresolved: Foundation models currently exhibit overconfidence in incorrect predictions, poor incremental learning capabilities, and cannot adapt to new knowledge without catastrophic forgetting or extensive retraining.
- What evidence would resolve it: Modified foundation models that demonstrate reliable unknown rejection, continual learning on new classes, and maintained performance on existing knowledge without full retraining.

## Limitations

- Weak evidence supporting the integration of the three tasks in OWL
- Lack of direct evidence for unified frameworks in the corpus
- Potential outdatedness of stored exemplars or features in class-incremental learning

## Confidence

- The necessity of integrating unknown rejection, novel class discovery, and class-incremental learning: Low confidence
- The effectiveness of exemplar and feature replay methods in mitigating catastrophic forgetting: Low confidence
- The need for unified frameworks in OWL: Low confidence

## Next Checks

1. Replicate the experimental setup and implement the key methods discussed in the paper to evaluate their performance and compare the results with the reported values.
2. Analyze the feature embeddings and check for overlap between known and unknown classes to diagnose potential issues in unknown rejection.
3. Monitor the performance on old classes during incremental learning and compare it with the initial performance to assess the effectiveness of class-incremental learning methods in mitigating catastrophic forgetting.