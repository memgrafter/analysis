---
ver: rpa2
title: 'JMA: a General Algorithm to Craft Nearly Optimal Targeted Adversarial Example'
arxiv_id: '2401.01199'
source_url: https://arxiv.org/abs/2401.01199
tags:
- attack
- case
- adversarial
- target
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Jacobian-induced Mahalanobis distance
  Attack (JMA), a novel targeted adversarial attack method designed to be effective
  across various output encoding schemes, including one-hot encoding, error correction
  output coding (ECOC), and multi-label classification. The core idea of JMA is to
  minimize a Jacobian-induced Mahalanobis distance term, which accounts for the effort
  required to move the latent space representation of an input sample in a given direction.
---

# JMA: a General Algorithm to Craft Nearly Optimal Targeted Adversarial Example

## Quick Facts
- **arXiv ID**: 2401.01199
- **Source URL**: https://arxiv.org/abs/2401.01199
- **Reference count**: 40
- **One-line primary result**: JMA achieves attack success rates close to or equal to 1 while requiring significantly fewer iterations than state-of-the-art attacks across multiple encoding schemes and datasets.

## Executive Summary
This paper introduces the Jacobian-induced Mahalanobis distance Attack (JMA), a novel targeted adversarial attack method designed to be effective across various output encoding schemes, including one-hot encoding, error correction output coding (ECOC), and multi-label classification. The core idea of JMA is to minimize a Jacobian-induced Mahalanobis distance term, which accounts for the effort required to move the latent space representation of an input sample in a given direction. By exploiting the Wolfe duality theorem, the problem is reduced to solving a Non-Negative Least Square (NNLS) problem, providing an optimal solution under a linear approximation of the network's behavior.

Experimental results demonstrate that JMA outperforms state-of-the-art attacks in terms of attack success rate (ASR), distortion (measured by Mean Square Error, MSE), and computational efficiency (measured by average number of iterations and attack time). Notably, JMA achieves ASR values close to or equal to 1 in various classification tasks, including ECOC-based classification on datasets like GTSRB, CIFAR-10, and MNIST, as well as multi-label classification on VOC2012, MS-COCO, and NUS-WIDE. JMA requires significantly fewer iterations compared to existing methods, making it more efficient.

## Method Summary
The JMA algorithm operates by computing the Jacobian matrix of the network output with respect to the input, then using this to determine the minimal perturbation needed to reach a target output. The key innovation is formulating the adversarial attack as minimizing a Mahalanobis distance induced by the Jacobian matrix, which accounts for the varying difficulty of moving the network output in different directions. By exploiting the Wolfe duality theorem, this constrained optimization problem is transformed into a Non-Negative Least Squares (NNLS) problem that can be solved efficiently. The algorithm works at the logits level rather than probabilities, enabling it to generalize across different encoding schemes including one-hot, ECOC, and multi-label classification.

## Key Results
- JMA achieves ASR values close to or equal to 1 on multiple datasets including CIFAR-10, GTSRB, MNIST, VOC2012, MS-COCO, and NUS-WIDE.
- JMA requires significantly fewer iterations (average 10-15) compared to baseline methods like C&W (100+ iterations) to achieve comparable or better attack success rates.
- JMA demonstrates capability to induce targeted modifications of up to half the labels simultaneously in complex multi-label classification scenarios, surpassing existing attacks.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** JMA achieves lower computational complexity than existing attacks by solving the adversarial optimization problem via a single constrained quadratic programming step under a first-order linear approximation.
- **Mechanism:** The Jacobian matrix of the network output with respect to the input is computed once per iteration. Using the Wolfe duality theorem, the problem is reformulated as a Non-Negative Least Squares (NNLS) problem, which is computationally simpler than iterative gradient descent used by other attacks. This exploits the local linearity of the network to compute the optimal perturbation in closed form.
- **Core assumption:** The network function can be approximated as linear in a small neighborhood around the input image, allowing the use of Jacobian-based methods.
- **Evidence anchors:**
  - [abstract] "By exploiting the Wolfe duality theorem, the problem is reduced to solving a Non-Negative Least Square (NNLS) problem, providing an optimal solution under a linear approximation of the network's behavior."
  - [section] "Under the assumption that the input perturbation is small (which should always be the case with adversarial examples), we can consider a first order approximation of the effect of the perturbation δ on the network output: f(x0 + δ) ≃ f(x0) + Jx0 δ."
  - [corpus] No direct corpus evidence for Wolfe duality theorem application in adversarial attacks; this appears to be novel to this paper.
- **Break condition:** If the perturbation required to reach the target region is large, the linear approximation breaks down, and the Jacobian-based closed-form solution becomes inaccurate, requiring iterative refinement.

### Mechanism 2
- **Claim:** JMA generalizes effectively across different output encoding schemes (one-hot, ECOC, multi-label) by working directly at the logits level rather than the probability level.
- **Mechanism:** By operating on logits, JMA avoids the non-linear softmax transformation that couples all output classes in one-hot encoding. This allows simultaneous modification of all logits in a way that accounts for their correlations, which is essential for ECOC and multi-label classification where classes are not mutually exclusive.
- **Core assumption:** The logits contain sufficient information to reconstruct the desired target output under any encoding scheme, and modifying logits directly is more effective than modifying probabilities.
- **Evidence anchors:**
  - [abstract] "Most attacks create the adversarial examples by minimizing a cost function subject to a constraint on the maximum perturbation introduced in the image. Moreover, they focus on single-label classifiers based on one-hot encoding."
  - [section] "In contrast to LOTS [21], JMA defines the target point in the feature space that minimizes perturbation necessary to move the feature representation of the input sample to the target point. It does so, by relying on the Mahalanobis distance induced by the Jacobian matrix of the neural network function so to take into account the different effort, in terms of input perturbation, required to move the sample in different directions."
  - [corpus] Weak evidence - no direct corpus examples of logit-level attacks generalizing across encoding schemes; this appears to be a key contribution.
- **Break condition:** If the encoding scheme requires non-linear transformations that cannot be adequately captured by logit modifications alone, or if the logits are not well-calibrated for the specific encoding scheme.

### Mechanism 3
- **Claim:** JMA achieves higher attack success rates by minimizing the Mahalanobis distance induced by the Jacobian matrix, which accounts for the varying difficulty of moving the network output in different directions in the input space.
- **Mechanism:** Instead of minimizing Euclidean distance in the output space, JMA uses a weighted distance metric where the weights come from the Jacobian matrix. This means it prioritizes directions in output space that require less perturbation in the input space, leading to more efficient attacks that are more likely to succeed.
- **Core assumption:** The Jacobian matrix accurately captures the sensitivity of the network output to input perturbations, and directions with lower Jacobian-induced distance are easier to achieve in practice.
- **Evidence anchors:**
  - [abstract] "The proposed algorithm (referred to as JMA) provides an optimal solution to a linearised version of the adversarial example problem originally introduced by Szegedy et al."
  - [section] "The optimal target point r∗, and hence the optimal distance term d∗, is then determined (Step 2) by solving the following minimization: min d:dT (ci−ct)≤gti,∀i̸=t dT (J JT )−1 d. Note that the above formulation corresponds to minimizing the Mahalanobis distance induced by the Jacobian matrix, between the current output of the network and the target point."
  - [corpus] No direct corpus evidence for Mahalanobis distance optimization in adversarial attacks; this appears to be novel methodology.
- **Break condition:** If the Jacobian matrix is rank-deficient or ill-conditioned, the Mahalanobis distance becomes undefined or unstable, leading to failed attacks.

## Foundational Learning

- **Concept:** Jacobian matrix and its role in sensitivity analysis
  - Why needed here: The Jacobian matrix captures how small changes in the input image affect each output logit, which is essential for determining the minimal perturbation needed to reach a target output.
  - Quick check question: If the Jacobian of a network output with respect to its input has a very small value in a particular direction, what does this imply about the sensitivity of that output to changes in that input direction?

- **Concept:** Non-Negative Least Squares (NNLS) problem formulation
  - Why needed here: The optimization problem in JMA reduces to an NNLS problem, which has efficient numerical solvers and guarantees non-negative solutions that are required for the perturbation calculation.
  - Quick check question: What is the key difference between a standard least squares problem and a non-negative least squares problem, and why is the non-negativity constraint important in this context?

- **Concept:** Wolfe duality theorem in optimization
  - Why needed here: The Wolfe duality theorem is used to transform the constrained quadratic programming problem into an equivalent dual problem that can be solved more efficiently as an NNLS problem.
  - Quick check question: In optimization, what is the relationship between a primal problem and its dual problem, and under what conditions can solving the dual problem provide the solution to the primal problem?

## Architecture Onboarding

- **Component map:** Input image -> Forward pass (logits) -> Jacobian computation -> Constraint matrix construction -> NNLS solver -> Perturbation calculation -> Binary search refinement -> Output adversarial image

- **Critical path:**
  1. Forward pass to get logits f(x)
  2. Backward pass to compute Jacobian J
  3. Construct constraint matrices A and b
  4. Solve NNLS problem for λ*
  5. Compute perturbation δ* = -J^T (A^T λ*)
  6. Apply perturbation and clip
  7. Check if target class reached, otherwise iterate

- **Design tradeoffs:**
  - Linear approximation vs. accuracy: The first-order approximation enables efficient computation but may fail for large perturbations
  - Logits vs. probabilities: Working at logits level provides generality but may require careful handling of different encoding schemes
  - Single vs. iterative solution: One-shot solution is efficient but may require iterative refinement in practice

- **Failure signatures:**
  - Jacobian rank deficiency: If J loses full rank during iterations, the algorithm cannot find a solution
  - Constraint violation: If no perturbation satisfies all constraints, the attack fails
  - Vanishing gradients: If the Jacobian becomes too small, the perturbation becomes ineffective

- **First 3 experiments:**
  1. Implement JMA on a simple MNIST classifier with one-hot encoding and verify it can successfully attack a small set of test images
  2. Test JMA on an ECOC-encoded CIFAR-10 classifier and compare ASR and MSE with baseline C&W attack
  3. Implement the multi-label version of JMA on VOC2012 dataset and verify it can modify multiple labels simultaneously compared to baseline methods

## Open Questions the Paper Calls Out

The paper identifies several future research directions, including extending JMA to black-box attack scenarios to develop powerful targeted attacks with certain transferability properties, exploring JMA's potential in adversarial training for multi-label classifiers, and investigating its use in training provably robust multi-label classifiers using randomized smoothing. The paper also suggests that JMA could inspire new defenses beyond adversarial training, though it does not provide specific details on what these defenses might entail.

## Limitations

- The paper's claims rely heavily on the validity of the first-order linear approximation of neural network behavior, which may break down for large perturbations or complex network architectures with skip connections or attention mechanisms.
- The paper provides limited evidence that the logit-level approach works equally well for all encoding types, particularly for complex multi-label scenarios, with most experiments focusing on binary and multi-class ECOC.
- The claim that JMA can "induce targeted modifications of up to half the labels" in multi-label classification appears to be an extrapolation from limited experiments without detailed analysis of the semantic coherence of these modifications.

## Confidence

- **High Confidence**: JMA's superiority in computational efficiency (fewer iterations) and attack success rates on the tested datasets and models.
- **Medium Confidence**: The generalization claims across different encoding schemes (one-hot, ECOC, multi-label).
- **Low Confidence**: The claim that JMA can "induce targeted modifications of up to half the labels" in multi-label classification.

## Next Checks

1. **Jacobian Sensitivity Analysis**: Conduct systematic experiments varying perturbation magnitudes and measuring the breakdown point where linear approximation error becomes significant. Compare JMA's performance degradation against baseline attacks as perturbations grow larger.

2. **Cross-Architecture Generalization**: Test JMA on a diverse set of architectures including vision transformers, residual networks with attention mechanisms, and models with non-standard layer compositions to identify architectures where the Jacobian computation or Wolfe duality transformation may fail.

3. **Multi-Label Quality Assessment**: Beyond measuring attack success rate, implement human or automated evaluation of the semantic coherence of multi-label modifications. Verify that simultaneous label changes produce realistic images rather than artifacts that fool the classifier but would be obvious to human observers.