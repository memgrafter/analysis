---
ver: rpa2
title: 'OpenFactCheck: Building, Benchmarking Customized Fact-Checking Systems and
  Evaluating the Factuality of Claims and LLMs'
arxiv_id: '2405.05583'
source_url: https://arxiv.org/abs/2405.05583
tags:
- claims
- factuality
- fact-checking
- evaluation
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces OpenFactCheck, a unified framework designed
  to build customized fact-checking systems, benchmark their accuracy, evaluate the
  factuality of large language models (LLMs), and verify claims in documents. The
  framework consists of three modules: CUSTCHECKER, which allows users to customize
  and deploy automatic fact-checkers; LLMEVAL, which evaluates LLM factuality across
  multiple datasets; and CHECKEREVAL, which assesses the reliability of fact-checkers
  using human-annotated benchmarks.'
---

# OpenFactCheck: Building, Benchmarking Customized Fact-Checking Systems and Evaluating the Factuality of Claims and LLMs

## Quick Facts
- arXiv ID: 2405.05583
- Source URL: https://arxiv.org/abs/2405.05583
- Reference count: 15
- Over 90% of LLM-generated claims are factually correct, but challenges remain in detecting false claims and handling rapidly changing information.

## Executive Summary
OpenFactCheck is a unified framework for building customized fact-checking systems, benchmarking their accuracy, and evaluating the factuality of large language models (LLMs). The framework consists of three modular components: CUSTCHECKER for customizing automatic fact-checkers, LLMEVAL for evaluating LLM factuality across multiple datasets, and CHECKEREVAL for assessing fact-checker reliability using human-annotated benchmarks. Experiments demonstrate that while over 90% of LLM-generated claims are factually correct, significant challenges persist in detecting false claims and handling rapidly changing information. The framework's modular design enables extensibility, allowing developers to integrate new modules and improve factuality evaluation.

## Method Summary
The OpenFactCheck framework provides a three-module approach to factuality assessment. CUSTCHECKER allows users to build customized fact-checking pipelines by selecting and combining claim processors, retrievers, and verifiers through a configuration file. LLMEVAL evaluates LLM responses using seven factuality-specific datasets and standardized metrics including precision, recall, and F1-score. CHECKEREVAL assesses the accuracy of automatic fact-checkers by comparing their outputs against human-annotated benchmarks and maintaining a leaderboard of performance. The framework is implemented in Python and features a Streamlit-based web interface for user interaction.

## Key Results
- Over 90% of claims generated by LLMs are factually correct across tested models.
- Factcheck-GPT achieves the highest accuracy but at significant cost and latency.
- Automatic fact-checking systems struggle with detecting false claims, particularly when retrieval of pertinent evidence is challenging.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The modular design of OpenFactCheck enables customization and extensibility of fact-checking pipelines.
- **Mechanism**: The framework abstracts fact-checking into three task solvers—claim processor, retriever, and verifier—that can be individually selected, combined, or replaced through a configuration file. This decouples implementation from pipeline structure.
- **Core assumption**: As long as task solvers adhere to the defined interface (input/output format), they can be plugged into the pipeline regardless of their internal implementation.
- **Evidence anchors**:
  - [abstract]: "CUST CHECKER allows users to easily customize an automatic fact-checker and verify the factual correctness of documents and claims"
  - [section]: "The instances of the three classes are sequentially linked into a pipeline, solving the following tasks: (i) decomposing a document into atomic claims, (ii) collecting relevant evidence passages given a claim, and (iii) making a true/false judgment given both the claim and the evidence as input"
  - [corpus]: Weak; no direct corpus evidence supporting the claim, though related work (RARR, FactScore, etc.) shows diverse implementations.
- **Break condition**: If task solvers fail to produce outputs matching the expected format, the message-passing mechanism breaks and the pipeline halts with an error.

### Mechanism 2
- **Claim**: OpenFactCheck provides a unified evaluation framework that enables fair comparison of LLM factuality across different models and datasets.
- **Mechanism**: By collecting multiple factuality-specific datasets (Snowball, SelfAware, FreshQA, etc.) and standardizing evaluation metrics (precision, recall, F1), the framework ensures that different LLMs are assessed under the same conditions.
- **Core assumption**: The selected datasets collectively cover diverse error types (knowledge errors, over-commitment, fast-changing info) and domains, making the evaluation comprehensive.
- **Evidence anchors**:
  - [abstract]: "LLMEVAL, a unified evaluation framework assesses LLM’s factuality ability from various perspectives fairly"
  - [section]: "We collect a dataset FactQA by gathering a large number of factual questions that probe diverse factual errors and span across a spectrum of domains, to fairly evaluate LLMs’ factuality under the same criteria"
  - [corpus]: Weak; no direct corpus evidence, though the cited datasets (Snowball, SelfAware, FreshQA) support the approach.
- **Break condition**: If the datasets fail to cover emerging error types or domains, the evaluation becomes incomplete or biased.

### Mechanism 3
- **Claim**: Automatic fact-checking systems can serve as proxies for evaluating the factuality of LLM-generated content.
- **Mechanism**: The framework uses human-annotated benchmarks (Factcheck-Bench, FacTool-QA, FELM-WK) to evaluate the accuracy of automatic fact-checkers, assuming their outputs correlate with human judgment.
- **Core assumption**: The verification results of automatic fact-checkers are statistically significantly correlated with human-annotated labels, as demonstrated in prior work (Wei et al., 2024).
- **Evidence anchors**:
  - [abstract]: "CHECKER EVAL is an extensible solution for gauging the reliability of automatic fact-checkers' verification results using human-annotated datasets"
  - [section]: "We gather four LLM factuality benchmarks with human-annotated factual labels... We refer to them as FactBench"
  - [corpus]: Weak; no direct corpus evidence, though the approach is supported by the cited prior work.
- **Break condition**: If the correlation between automatic fact-checker outputs and human labels weakens or breaks down for new domains or error types, the proxy evaluation becomes unreliable.

## Foundational Learning

- **Concept**: Modular software design
  - **Why needed here**: Enables flexible combination of claim processors, retrievers, and verifiers, allowing users to tailor fact-checking systems to specific needs.
  - **Quick check question**: Can you swap out a retriever implementation without modifying the rest of the pipeline?

- **Concept**: Benchmark standardization
  - **Why needed here**: Ensures fair comparison of LLM factuality across different models and datasets by using consistent metrics and evaluation criteria.
  - **Quick check question**: Are all evaluated models using the same datasets and metrics in the LLM factuality evaluation?

- **Concept**: Human-in-the-loop evaluation
  - **Why needed here**: Provides ground truth labels for claims, enabling the assessment of automatic fact-checker accuracy and reliability.
  - **Quick check question**: How are the human-annotated labels used to compute precision, recall, and F1 for fact-checkers?

## Architecture Onboarding

- **Component map**:
  - CUST CHECKER: Customizable fact-checking pipeline (claim processor, retriever, verifier)
  - LLMEVAL: LLM factuality evaluation framework (dataset collection, metric computation)
  - CHECKER EVAL: Fact-checker accuracy evaluation (human-annotated benchmarks, leaderboard)
  - Web interface: User interaction layer (Streamlit-based UI for each module)

- **Critical path**:
  1. User configures fact-checking pipeline via YAML
  2. Pipeline processes input document/claim through claim processor → retriever → verifier
  3. LLMEVAL evaluates LLM responses using standardized datasets and metrics
  4. CHECKER EVAL compares fact-checker outputs to human-annotated labels and updates leaderboard

- **Design tradeoffs**:
  - Flexibility vs. complexity: Modular design allows customization but increases configuration complexity
  - Accuracy vs. cost: Higher accuracy fact-checkers (e.g., GPT-4-based) are more expensive than cheaper alternatives
  - Comprehensiveness vs. scalability: Multiple datasets ensure thorough evaluation but increase computational overhead

- **Failure signatures**:
  - Claim processor fails: Pipeline halts with "decomposition error" message
  - Retriever fails: Pipeline continues but produces "no evidence found" results
  - Verifier fails: Pipeline outputs "verification failed" with error details
  - LLMEVAL fails: Evaluation report generation fails, no results returned
  - CHECKER EVAL fails: Leaderboard not updated, error logged

- **First 3 experiments**:
  1. Test claim processor with a simple document to verify decomposition into atomic claims
  2. Test retriever with a known claim and Wikipedia to verify evidence collection
  3. Test verifier with a claim and evidence to verify true/false judgment

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the accuracy of automated fact-checking systems be significantly improved for detecting false claims?
- **Basis in paper**: [explicit] The paper states that "automatic fact-checking systems struggle to detect false claims" and identifies "retrieval of pertinent evidence" as a significant bottleneck.
- **Why unresolved**: The paper identifies the problem but doesn't propose specific solutions beyond mentioning that "the efficacy of automated fact-checking systems is fundamentally dependent on implementation factors."
- **What evidence would resolve it**: Empirical studies comparing different retrieval strategies, evidence collection methods, and verification approaches specifically targeting false claim detection with measurable improvements in F1-score for false claims.

### Open Question 2
- **Question**: What are the most effective methods for evaluating implicit reasoning and irrelevant error types in LLM factuality assessment?
- **Basis in paper**: [inferred] The paper explicitly states they "do not consider reasoning errors that arise when a claim employs flawed reasoning or faulty logic, and irrelevant error concerning that the content is unrelated to the question" and leaves this for future work.
- **Why unresolved**: The authors deliberately excluded these error types from their evaluation framework, acknowledging their importance but not addressing them.
- **What evidence would resolve it**: Development and validation of evaluation datasets and metrics specifically designed to measure implicit reasoning errors and irrelevant content generation, with demonstrated correlation to real-world model failures.

### Open Question 3
- **Question**: How can the latency and cost of high-accuracy fact-checking systems be reduced without sacrificing performance?
- **Basis in paper**: [explicit] The paper notes that "Factcheck-GPT exhibits superior effectiveness, it is associated with considerable latency and substantial costs" and that latency and cost "primarily hinge on implementation strategies."
- **Why unresolved**: While the paper identifies the trade-off between accuracy and resource consumption, it doesn't explore optimization strategies or architectural modifications to address this limitation.
- **What evidence would resolve it**: Comparative studies of alternative architectures (e.g., hybrid retrieval-augmented generation, model distillation, or asynchronous processing) that maintain high accuracy while reducing time and cost by at least 50%.

### Open Question 4
- **Question**: How can fact-checking systems be made more robust to rapidly changing information and time-sensitive claims?
- **Basis in paper**: [explicit] The paper identifies that LLMs "struggle to answer with up-to-date information" and that "disability error happens when the model is unable to search up-to-date information to correctly answer questions whose answers change over time."
- **Why unresolved**: The paper acknowledges this as a limitation but doesn't propose specific mechanisms for handling fast-changing information beyond general suggestions about external knowledge retrieval.
- **What evidence would resolve it**: Development and evaluation of fact-checking systems that incorporate real-time information sources, temporal reasoning capabilities, or confidence calibration specifically for time-sensitive claims with demonstrated improvements in accuracy over static knowledge bases.

## Limitations
- The framework's accuracy heavily depends on the quality of external evidence retrieval, which can fail due to API limitations or content changes.
- The evaluation framework relies on existing human-annotated benchmarks rather than newly collected data, potentially missing emerging error types.
- High-accuracy fact-checking systems (e.g., Factcheck-GPT) incur significant computational costs and latency.

## Confidence
- High confidence: The modular framework design and its three-component architecture (claim processor, retriever, verifier) are well-specified and theoretically sound.
- Medium confidence: The LLM factuality evaluation methodology is robust given the multiple datasets used, but coverage gaps may exist for rapidly evolving information domains.
- Medium confidence: The automatic fact-checker evaluation approach is valid based on prior work, but the statistical significance of correlation with human judgment needs explicit validation.

## Next Checks
1. Test the correlation between automatic fact-checker outputs and human annotations on a new, independently collected dataset to verify the proxy evaluation approach.
2. Evaluate the framework's performance on claims involving fast-changing information (e.g., recent events) to assess its handling of temporal limitations.
3. Implement and integrate at least two additional fact-checking systems beyond the examples provided to validate the claimed extensibility of the framework.