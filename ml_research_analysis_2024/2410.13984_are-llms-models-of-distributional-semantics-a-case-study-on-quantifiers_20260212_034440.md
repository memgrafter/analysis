---
ver: rpa2
title: Are LLMs Models of Distributional Semantics? A Case Study on Quantifiers
arxiv_id: '2410.13984'
source_url: https://arxiv.org/abs/2410.13984
tags:
- than
- quantifiers
- threshold
- half
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines whether large language models (LLMs) can be
  considered models of distributional semantics by testing their performance on vague
  and exact quantifiers. Using human judgments as a benchmark, the authors find that
  LLMs align more closely with humans on exact quantifiers (e.g., "more than half")
  than on vague ones (e.g., "many").
---

# Are LLMs Models of Distributional Semantics? A Case Study on Quantifiers

## Quick Facts
- arXiv ID: 2410.13984
- Source URL: https://arxiv.org/abs/2410.13984
- Authors: Zhang Enyan; Zewei Wang; Michael A. Lepori; Ellie Pavlick; Helena Aparicio
- Reference count: 40
- Key outcome: LLMs align better with human judgments on exact quantifiers than vague ones, challenging distributional semantics assumptions

## Executive Summary
This study examines whether large language models can be considered models of distributional semantics by testing their performance on vague and exact quantifiers. Using human judgments as a benchmark, the authors find that LLMs align more closely with humans on exact quantifiers (e.g., "more than half") than on vague ones (e.g., "many"). This counterintuitive result challenges the assumption that distributional semantics models excel at capturing graded meaning. Further analysis reveals that instruction-tuned models perform better than base models, and that negative polarity quantifiers (e.g., "few") are harder for models to process. The findings suggest that LLMs may rely on mechanisms beyond pure distributional semantics, prompting a reevaluation of what these models can capture.

## Method Summary
The study compared LLM judgments on quantifiers to human judgments using a controlled experiment with 1000 questions per quantifier type. Questions varied total_number, number, and noun parameters systematically. Five prompt variations were used per question, with responses aggregated to compute agreement thresholds. Human judgments came from 362 participants on Prolific after attention filtering. Model thresholds were calculated where agreement crossed 50%, then compared to human thresholds using absolute difference as the primary metric. Linear regression on log(total_number) was used to test scaling effects.

## Key Results
- LLMs show better alignment with human judgments on exact quantifiers than vague quantifiers
- Instruction-tuned models outperform base models on both quantifier types, with larger improvements on exact quantifiers
- Negative polarity quantifiers ("few", "less than half") are consistently harder for models to process across all model types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs better align with human judgments on exact quantifiers because they may be learning numerical cardinal thresholds rather than proportional reasoning.
- Mechanism: The model may be computing a fixed numeric threshold (e.g., "many" means at least 200 items) rather than a relative proportion, which explains why performance on exact quantifiers is better—these have well-defined thresholds in the training data.
- Core assumption: Models do not fully capture the context-sensitive, proportional nature of vague quantifiers and instead default to simple count-based heuristics.
- Evidence anchors:
  - [abstract] "the opposite of the prediction holds true: LLMs’ performance on vague quantifiers is consistently worse than their performance on exact quantifiers"
  - [section] "one possibility that is consistent with the linear regression results is that models might be following a cardinal threshold, where the threshold value is fixed regardless of total_number"
  - [corpus] Weak: No direct corpus evidence of this mechanism; requires further behavioral or probing analysis.
- Break condition: If future probing shows that models can recover context-dependent proportional reasoning when prompted explicitly, the cardinal threshold hypothesis weakens.

### Mechanism 2
- Claim: Instruction-tuned models outperform base models on both quantifier types due to fine-tuning on mathematical and logical reasoning tasks.
- Mechanism: Post-training instruction tuning introduces additional distribution shifts that bias models toward handling exact numeric thresholds, which benefits exact quantifier tasks more than vague ones.
- Core assumption: Instruction tuning datasets contain more examples involving precise numerical comparisons than graded linguistic judgments.
- Evidence anchors:
  - [abstract] "instruction-tuned models perform better than base models"
  - [section] "instruction tuning has an overall higher effect on boosting human alignment on exact quantifiers than vague quantifiers"
  - [corpus] Weak: No corpus evidence quantifying the content of instruction tuning datasets; assumed based on typical fine-tuning pipelines.
- Break condition: If future analysis shows that instruction tuning datasets are balanced across numeric and vague linguistic tasks, this mechanism is weakened.

### Mechanism 3
- Claim: Negative polarity quantifiers ("few", "less than half") are harder for models because negation processing is underrepresented in training data.
- Mechanism: Language models may have fewer negative polarity examples in training corpora, leading to weaker semantic representations for negated quantifiers compared to positive ones.
- Core assumption: The distributional hypothesis implies that less frequent linguistic constructions (like negative quantifiers) are learned less robustly.
- Evidence anchors:
  - [abstract] "negative polarity quantifiers (e.g., "few") are harder for models to process"
  - [section] "it's not uncommon that language models struggle with negative statements" and "LLMs struggle with negative statements"
  - [corpus] Moderate: [corpus] includes references to prior work showing LLMs struggle with negation, supporting this mechanism.
- Break condition: If future training data augmentation shows that increasing negative polarity examples improves performance on these quantifiers, the mechanism is validated.

## Foundational Learning

- Concept: Distributional semantics and its contrast with formal semantics
  - Why needed here: Understanding why LLMs were expected to excel at vague quantifiers requires grasping the core assumptions of distributional semantics.
  - Quick check question: What is the fundamental difference between distributional and formal semantics approaches to meaning?

- Concept: Proportional vs cardinal reasoning
  - Why needed here: The study hinges on whether models use proportional (context-sensitive) or cardinal (fixed threshold) reasoning for quantifiers.
  - Quick check question: How would a model's performance differ if it used cardinal thresholds versus proportional reasoning for quantifiers?

- Concept: Instruction tuning and post-training effects
  - Why needed here: The differential performance between instruction-tuned and base models requires understanding how fine-tuning changes model behavior.
  - Quick check question: What aspects of instruction tuning might specifically improve exact quantifier processing more than vague quantifier processing?

## Architecture Onboarding

- Component map: Data pipeline -> Model inference layer -> Analysis engine -> Regression module
- Critical path:
  1. Generate 1000 questions per quantifier type with controlled parameters
  2. Query each model with 5 prompt variations per question
  3. Aggregate responses to compute P(Yes)/(P(Yes)+P(No)) per question
  4. Calculate thresholds as proportions where agreement crosses 50%
  5. Compare model thresholds to human thresholds
- Design tradeoffs:
  - Using fixed noun sets vs. random sampling: Fixed nouns control for noun-specific effects but may introduce bias
  - Log-scaling vs. linear regression: Log-scaling handles the sparse high-total-number region but may obscure some patterns
  - Single-token response assumption: Works for current models but may break with multi-token outputs
- Failure signatures:
  - Large standard deviations across prompt variations suggest prompt sensitivity
  - Systematic bias toward positive quantifiers indicates training data imbalance
  - Inverse scaling effects (worse with larger models) suggest fundamental architectural limitations
- First 3 experiments:
  1. Replicate the main analysis with additional quantifiers (e.g., "some", "most") to test generalizability
  2. Probe model internals (attention patterns, hidden states) when processing exact vs. vague quantifiers
  3. Fine-tune models on balanced datasets containing both positive and negative polarity quantifier examples to test if performance improves

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do language models exhibit a fixed cardinal threshold for vague quantifiers, or do they adjust thresholds based on total-number?
- Basis in paper: [explicit] The authors observe that vague quantifier thresholds decrease as total-number increases across models, suggesting potential cardinal threshold behavior
- Why unresolved: While the negative regression slopes suggest this pattern, the underlying mechanism is not directly examined. Models could be using various heuristics or patterns
- What evidence would resolve it: Direct probing of model internals to identify if fixed numerical thresholds are being applied, or controlled experiments varying total-number and number combinations systematically

### Open Question 2
- Question: How does instruction tuning affect the distributional semantics capabilities of language models?
- Basis in paper: [explicit] Instruction-tuned models consistently outperform base models on both exact and vague quantifiers, with larger improvements on exact quantifiers
- Why unresolved: The paper notes this effect but doesn't explore the underlying mechanisms. The improvement could stem from various factors in the instruction tuning process
- What evidence would resolve it: Comparative analysis of training data, architectural changes, or internal representations between base and instruction-tuned models

### Open Question 3
- Question: Why do language models perform worse on negative polarity quantifiers compared to positive ones?
- Basis in paper: [explicit] The authors observe consistent performance gaps for "few" and "less than half" compared to "many" and "more than half" across all model types
- Why unresolved: While the authors note human difficulty with negative quantifiers, they don't explain why this pattern emerges in trained models or its relationship to training data
- What evidence would resolve it: Analysis of negation handling in training corpora, controlled experiments with negated statements, or examination of attention patterns in negative quantifier contexts

### Open Question 4
- Question: What is the fundamental nature of distributional semantics that LLMs are capturing?
- Basis in paper: [explicit] The authors conclude that LLMs either are not distributional semantics models, or distributional semantics does not have the features it has been traditionally associated with
- Why unresolved: The paper identifies a paradox but doesn't resolve whether the issue lies with the definition of DSMs or the nature of what LLMs learn
- What evidence would resolve it: Theoretical work defining distributional semantics more precisely, or empirical studies mapping LLM representations to traditional DSM characteristics

### Open Question 5
- Question: How does the scale of language models affect their ability to capture graded versus exact meanings?
- Basis in paper: [inferred] The authors compare small (<10B parameters) and large (>10B parameters) models, finding different patterns in their performance
- Why unresolved: While some size-related trends are observed, the relationship between model scale and semantic capabilities is not fully characterized
- What evidence would resolve it: Systematic scaling studies across multiple semantic phenomena, or analysis of how representational capacity relates to semantic precision

## Limitations

- English-only data limits generalizability to other languages with different quantifier systems
- Reliance on specific LLM versions tested may not represent the broader model landscape due to rapid architectural changes
- Weak corpus evidence for several mechanisms, particularly regarding instruction tuning datasets and negative polarity frequency

## Confidence

- High confidence: The finding that LLMs align better with human judgments on exact versus vague quantifiers is well-supported by the data
- Medium confidence: The mechanism involving cardinal vs. proportional reasoning requires further probing to validate
- Low confidence: The instruction tuning mechanism lacks direct corpus evidence about training data composition

## Next Checks

1. **Cross-linguistic validation**: Replicate the study with non-English languages to test whether the exact/vague quantifier pattern holds across linguistic contexts.

2. **Probing experiments**: Conduct targeted interventions on model representations to test whether models can recover proportional reasoning when explicitly prompted or fine-tuned on balanced datasets.

3. **Negative polarity augmentation**: Systematically increase negative quantifier examples in training data and measure whether performance on "few" and "less than half" improves, validating the underrepresentation hypothesis.