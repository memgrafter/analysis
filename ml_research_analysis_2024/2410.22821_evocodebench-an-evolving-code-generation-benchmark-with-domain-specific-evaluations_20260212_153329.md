---
ver: rpa2
title: 'EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific Evaluations'
arxiv_id: '2410.22821'
source_url: https://arxiv.org/abs/2410.22821
tags:
- code
- llms
- evocodebench
- domain
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces EvoCodeBench, an evolving code generation
  benchmark designed to address data leakage and lack of domain-specific evaluation
  in existing benchmarks. The key innovations are: (1) Dynamic updates every 6 months
  to avoid data leakage, with the first version containing 275 samples from 25 repositories
  created between October 2023 and March 2024.'
---

# EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific Evaluations

## Quick Facts
- arXiv ID: 2410.22821
- Source URL: https://arxiv.org/abs/2410.22821
- Reference count: 40
- 8 LLMs evaluated with significant performance gaps between previous benchmarks and EvoCodeBench

## Executive Summary
This paper introduces EvoCodeBench, an evolving code generation benchmark designed to address data leakage and lack of domain-specific evaluation in existing benchmarks. The key innovations are: (1) Dynamic updates every 6 months to avoid data leakage, with the first version containing 275 samples from 25 repositories created between October 2023 and March 2024. (2) A domain taxonomy covering 10 popular programming domains based on PyPI repository statistics, with automatic domain labeling. (3) Domain-specific evaluations including Domain-Specific Improvement (DSI) to identify LLMs' comfort and strange domains. The benchmark evaluates 8 popular LLMs and reveals their actual abilities in real-world repositories, showing significant performance differences compared to previous benchmarks.

## Method Summary
EvoCodeBench collects code samples from newly created repositories (October 2023-March 2024) to avoid training data leakage. The benchmark uses automatic annotation to extract function signatures, reference code, dependencies, and test cases from real-world repositories. It evaluates LLMs in three settings: without context, local file completion, and local file infilling. The evaluation metrics include Pass@k for functional correctness and Recall@k for dependency recall. A 10-domain taxonomy based on PyPI statistics enables domain-specific evaluation, computing Domain-Specific Improvement (DSI) to identify each model's comfort and strange domains.

## Key Results
- gpt-4 achieves only 20.74% Pass@1 on EvoCodeBench-2403, significantly lower than on previous benchmarks
- Data leakage reduction from 41.47% (HumanEval) to 2.18% (EvoCodeBench-2403) using CDD detection
- StarCoder 2-15B shows unexpectedly strong performance in Database domain despite lower overall ranking
- Domain-specific evaluation reveals performance patterns not visible in general benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic updates every 6 months significantly reduce data leakage by ensuring test data remains outside model training windows
- **Mechanism:** By collecting new repositories created after the cutoff date of existing training data (September 2023), the benchmark ensures that test samples have not been seen during model training
- **Core assumption:** The training data cutoff date for LLMs is fixed and known, and new repositories are sufficiently distinct from pre-training data
- **Evidence anchors:**
  - [abstract] "EvoCodeBench will be dynamically updated every period (e.g., 6 months) to avoid data leakage"
  - [section 2.4] "the 25 repositories in EvoCodeBench-2403 were created between October 2023 and March 2024 and are not included in the training data"
  - [corpus] Weak - corpus shows related papers on evolving benchmarks but doesn't directly confirm training data dates
- **Break condition:** If training data extends beyond September 2023 or includes repositories from October 2023-March 2024, the leakage prevention fails

### Mechanism 2
- **Claim:** Domain-specific evaluations enable practitioners to select optimal LLMs for their particular programming domain
- **Mechanism:** By annotating each sample with domain labels from a 10-domain taxonomy and computing Domain-Specific Improvement (DSI), the benchmark reveals which LLMs excel in specific domains versus their overall performance
- **Core assumption:** Domain-specific performance differs meaningfully from general performance and correlates with real-world needs
- **Evidence anchors:**
  - [abstract] "compute the Domain-Specific Improvement (DSI) and define LLMs' comfort and strange domains"
  - [section 2.4] "We evaluate LLMs in different domains and discover their comfort and strange domains"
  - [corpus] Moderate - corpus includes papers on domain-specific code benchmarks that support this approach
- **Break condition:** If domain boundaries are too fuzzy or if DSI doesn't correlate with actual developer needs

### Mechanism 3
- **Claim:** High-quality, repository-aligned data provides realistic evaluation of LLMs in real-world development scenarios
- **Mechanism:** The benchmark aligns with real-world repositories in code distributions, dependency distributions, and includes comprehensive annotations like test cases, dependencies, and repository context
- **Core assumption:** Repository-level context (code distributions, dependencies, test cases) significantly impacts LLM performance and reflects real development scenarios
- **Evidence anchors:**
  - [section 2.4] "EvoCodeBench aligns with real-world repositories in multiple aspects, e.g., code distributions and dependency distributions"
  - [section 2.2] "Repo-level code generation takes a requirement and a repository as inputs"
  - [corpus] Weak - corpus mentions related work but doesn't directly validate repository alignment claims
- **Break condition:** If repository context doesn't improve LLM performance or if distributions differ significantly from actual development

## Foundational Learning

- **Concept:** Repository-level code generation vs snippet-level evaluation
  - **Why needed here:** Understanding the difference between generating standalone functions versus functions that integrate with existing repository context
  - **Quick check question:** What are the key challenges in repository-level code generation that don't exist in snippet-level tasks?

- **Concept:** Domain taxonomy design and statistical grounding
  - **Why needed here:** The benchmark's domain taxonomy is based on PyPI repository statistics, requiring understanding of how to create statistically-grounded taxonomies
  - **Quick check question:** How would you validate that the 10-domain taxonomy covers most real-world programming tasks?

- **Concept:** Evaluation metrics for code generation
  - **Why needed here:** Understanding Pass@k (functional correctness) and Recall@k (dependency recall) as evaluation metrics
  - **Quick check question:** Why might a model achieve high Pass@k but low Recall@k, and what would that indicate about its capabilities?

## Architecture Onboarding

- **Component map:** Data Collection Pipeline -> Execution Filtering -> Automatic Annotation -> Benchmark Construction -> Model Evaluation
- **Critical path:** Repository selection -> Function scraping -> Execution filtering -> Automatic annotation -> Benchmark construction -> Model evaluation
- **Design tradeoffs:**
  - Static vs dynamic benchmarks: Tradeoff between consistency and leakage prevention
  - Manual vs automatic annotation: Tradeoff between quality and scalability
  - Context inclusion levels: Tradeoff between evaluation realism and computational cost
- **Failure signatures:**
  - High leakage rates indicate repository selection or timing issues
  - Unbalanced domain distribution suggests taxonomy or collection bias
  - Low correlation between repository alignment and performance indicates evaluation methodology issues
- **First 3 experiments:**
  1. Test leakage detection by running CDD [6] on the benchmark to verify claimed 2.18% leakage rate
  2. Evaluate the same models on both EvoCodeBench and a leaked benchmark to quantify performance differences
  3. Compare model performance across different context settings to validate the importance of repository context

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- The 6-month update cycle effectiveness depends on unknown LLM training data boundaries
- Domain taxonomy coverage may not represent all real-world programming domains or emerging fields
- Repository alignment claims require validation against actual development practices

## Confidence

- **High confidence:** Data collection methodology and execution filtering process
- **Medium confidence:** Domain taxonomy design and statistical grounding
- **Low confidence:** Long-term effectiveness of dynamic update mechanism without knowing training data boundaries

## Next Checks

1. Run the CDD [6] leakage detection tool on the benchmark to verify the claimed 2.18% leakage rate and identify potential blind spots
2. Compare model performance on EvoCodeBench against established benchmarks to quantify the practical impact of data leakage prevention
3. Conduct a domain coverage analysis by surveying developers across different programming fields to validate the 10-domain taxonomy comprehensiveness