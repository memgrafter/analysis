---
ver: rpa2
title: Want to train KANS at scale? Now UKAN!
arxiv_id: '2408.11200'
source_url: https://arxiv.org/abs/2408.11200
tags:
- ukan
- grid
- b-spline
- learning
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents UKAN (Unbounded Kolmogorov-Arnold Networks),
  which removes the need for bounded grids in traditional KANs by using a coefficient-generator
  (CG) model that produces B-spline coefficients on the fly. The key innovation is
  feeding positional encoding of grid groups into the CG model, enabling function
  approximation on unbounded domains without data normalization.
---

# Want to train KANS at scale? Now UKAN!

## Quick Facts
- arXiv ID: 2408.11200
- Source URL: https://arxiv.org/abs/2408.11200
- Reference count: 6
- UKAN removes need for bounded grids in KANs using coefficient-generator model, achieving 3-30x speed-up and up to 1000x memory reduction

## Executive Summary
This paper introduces UKAN (Unbounded Kolmogorov-Arnold Networks), which addresses the key limitation of traditional KANs requiring bounded grids and data normalization. The core innovation is a coefficient-generator (CG) model that dynamically produces B-spline coefficients for an unbounded symmetric grid extending from negative to positive infinity. The authors also present warpKAN, a GPU-accelerated library that reduces B-spline evaluation complexity from O(Kd gdindout) to O(Kd indout), where K is grid size, d is B-spline order, ind is input dimension, and out is output dimension.

## Method Summary
UKAN extends KANs to unbounded domains by replacing static B-spline coefficient storage with a CG model that generates coefficients on-the-fly based on positional encodings of grid groups. The method groups K adjacent grid indices together and uses a single MLP call per group to predict K coefficients. The warpKAN library optimizes B-spline evaluation by exploiting their local support property, using pre-computed basis matrices instead of recursive formulas across the entire grid. This enables training on large-scale problems without data normalization while maintaining or improving upon KAN accuracy.

## Key Results
- 3-30x speed-up compared to vanilla KANs on benchmark tasks
- Up to 1000x memory reduction by eliminating dense grid storage
- UKAN matches or surpasses KAN accuracy on regression, classification, and generative tasks
- Successful training on unbounded domains without data normalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The coefficient-generator (CG) model enables KANs to work on unbounded domains without data normalization.
- Mechanism: The CG model takes positional encodings of grid groups as input and outputs B-spline coefficients on-the-fly. This allows KANs to generate coefficients dynamically for any input location within an infinite symmetric grid extending from negative to positive infinity.
- Core assumption: B-spline coefficients can be accurately predicted from positional encodings of grid groups.
- Evidence anchors:
  - [abstract]: "The key innovation of this method is a coefficient-generator (CG) model that produces, on the fly, only the B-spline coefficients required locally on an unbounded symmetric grid."
  - [section]: "To achieve an unbounded domain for the KAN grid, we use an MLP to generate B-spline coefficients, called a coefficient-generator (CG) MLP."
- Break condition: The CG model fails to accurately predict B-spline coefficients for grid positions far from training data or when the grid groups are too large to capture local variations.

### Mechanism 2
- Claim: The warpKAN library reduces B-spline evaluation complexity by a factor proportional to grid size.
- Mechanism: By exploiting the local nature of B-splines and using basis matrices instead of recursive formulas over the entire grid, the library achieves O(Kd indout) complexity instead of O(Kd gdindout).
- Core assumption: Local matrix representations of B-splines can accurately approximate the full recursive evaluation.
- Evidence anchors:
  - [abstract]: "To reduce the computational cost of both UKANs and KANs, we introduce a GPU-accelerated library that lowers B-spline evaluation complexity by a factor proportional to the grid size"
  - [section]: "The solution to compute and memory issues of KANs lies in exploring the local nature of B-splines instead of the recursive formula over the entire grid."
- Break condition: For B-splines with very high degrees or when the local approximation becomes insufficient to capture the function's behavior.

### Mechanism 3
- Claim: Grouping K adjacent grid indexes together and using a single MLP call per group improves coefficient prediction accuracy.
- Mechanism: Instead of predicting coefficients for each grid index separately, grouping K adjacent indexes and using their concatenated embeddings as input allows the CG model to learn better representations of local coefficient patterns.
- Core assumption: Adjacent grid indexes share similar coefficient patterns that can be captured by a single MLP call.
- Evidence anchors:
  - [section]: "To address this issue, we group K adjacent grid indexes together, called a group index, and use the CG model to predict K coefficients for each group index."
  - [section]: "The input to the CG MLP is obtained by concatenating the embedding of the feature index and the sinusoidal positional encoding of the group index as used in transformers."
- Break condition: When grid groups become too large, local variations are lost and the MLP cannot capture fine-grained coefficient patterns.

## Foundational Learning

- Concept: Kolmogorov-Arnold Representation Theorem
  - Why needed here: The KAN architecture is based on this theorem, which states that any multivariate function on a bounded domain can be represented as a finite composition of continuous univariate functions and summation.
  - Quick check question: How does the Kolmogorov-Arnold representation theorem justify using B-splines as activation functions in KANs?

- Concept: B-spline basis functions and their local support property
  - Why needed here: Understanding B-splines is crucial because they are the core component of KANs, and their local support property enables the computational optimizations in warpKAN.
  - Quick check question: Why does the local support property of B-splines allow for O(Kd indout) complexity instead of O(Kd gdindout)?

- Concept: Positional encoding in transformer models
  - Why needed here: The UKAN architecture uses positional encodings of grid groups as input to the CG model, similar to how transformers use positional encodings for sequence data.
  - Quick check question: How does positional encoding help the CG model distinguish between different grid positions when generating B-spline coefficients?

## Architecture Onboarding

- Component map:
  Input layer → CG MLP → B-spline basis matrices → Output layer
  CG MLP: Takes positional encodings of grid groups and feature embeddings
  B-spline basis matrices: Pre-computed matrices for each B-spline order
  warpKAN library: GPU-accelerated B-spline evaluation using local matrix representations

- Critical path:
  1. Input data arrives
  2. Grid groups are determined based on input positions
  3. Positional encodings and feature embeddings are computed for each group
  4. CG MLP generates B-spline coefficients for each group
  5. warpKAN evaluates B-splines using local matrix representations
  6. Outputs are computed and passed to the next layer

- Design tradeoffs:
  - Group size vs. accuracy: Larger groups reduce the number of CG MLP calls but may lose local information
  - Grid density vs. memory: Denser grids provide better approximation but require more memory for basis matrices
  - CG MLP complexity vs. training time: More complex CG MLPs can capture better patterns but increase training time

- Failure signatures:
  - Poor performance on inputs far from training data distribution (CG model extrapolation failure)
  - Memory issues with very large grid sizes (basis matrix storage)
  - Slow training convergence (insufficient CG MLP capacity or poor initialization)

- First 3 experiments:
  1. Compare UKAN vs. KAN performance on a simple regression task with unbounded input domain
  2. Benchmark warpKAN vs. PyTorch KAN implementation on varying grid sizes and B-spline orders
  3. Test UKAN performance on a classification task with non-normalized input data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of UKAN scale with increasing dimensionality of the input space, particularly in comparison to traditional MLPs and KANs?
- Basis in paper: [inferred] The paper mentions a high-dimensional function experiment (function III) but does not provide a comprehensive analysis of how UKAN performs as dimensionality increases.
- Why unresolved: The paper only provides results for a 16-dimensional function and does not explore the scaling behavior of UKAN with respect to input dimensionality.
- What evidence would resolve it: Conducting experiments with input dimensions ranging from low (e.g., 2D) to high (e.g., 100D or more) and comparing the performance of UKAN, KAN, and MLPs across these dimensions would provide insights into the scalability of UKAN.

### Open Question 2
- Question: What is the impact of the coefficient-generator (CG) model architecture on the performance and generalization capabilities of UKAN?
- Basis in paper: [explicit] The paper mentions that the CG model uses an MLP with SiLU nonlinearity and that the MLP component might slightly hurt generalization compared to KAN.
- Why unresolved: The paper does not explore alternative architectures for the CG model or provide a detailed analysis of how different CG model designs affect UKAN's performance.
- What evidence would resolve it: Experimenting with various CG model architectures (e.g., different activation functions, depths, widths) and comparing their impact on UKAN's accuracy and generalization would help determine the optimal CG model design.

### Open Question 3
- Question: How does UKAN perform in tasks beyond regression, classification, and generative modeling, such as reinforcement learning or time-series forecasting?
- Basis in paper: [inferred] The paper focuses on regression, classification, and generative tasks but does not explore other potential applications of UKAN.
- Why unresolved: The authors do not provide any experiments or discussions on the applicability of UKAN to other domains like reinforcement learning or time-series forecasting.
- What evidence would resolve it: Applying UKAN to tasks in reinforcement learning (e.g., policy approximation) or time-series forecasting and comparing its performance to existing methods would demonstrate the versatility and effectiveness of UKAN in these domains.

## Limitations
- Performance claims based on limited benchmark tasks, generalizability to real-world datasets unclear
- warpKAN optimizations assume specific B-spline properties that may not hold for all function classes
- Positional encoding mechanism lacks theoretical justification for capturing coefficient patterns across unbounded domains

## Confidence
- **High Confidence**: Computational complexity reduction claims for warpKAN are mathematically sound given local support property of B-splines
- **Medium Confidence**: Accuracy comparisons between UKAN and KAN are promising but based on limited experimental suite
- **Low Confidence**: Extrapolation capability to truly unbounded domains is demonstrated only implicitly, not rigorously tested

## Next Checks
1. **Extrapolation Stress Test**: Systematically evaluate UKAN performance on inputs drawn from distributions that extend 10-100x beyond the training data bounds to quantify the true limits of the unbounded domain claim.

2. **Cross-Domain Generalization**: Test UKAN on a diverse suite of real-world datasets (e.g., from UCI repository) with varying input dimensionality, noise levels, and function complexity to assess whether the observed performance gains generalize beyond the current benchmark set.

3. **Hyperparameter Sensitivity Analysis**: Conduct ablation studies varying grid group sizes (K), B-spline degrees, and CG MLP architectures to identify the regimes where UKAN's advantages are most pronounced and where they diminish.