---
ver: rpa2
title: 'TOWER: Tree Organized Weighting for Evaluating Complex Instructions'
arxiv_id: '2410.06089'
source_url: https://arxiv.org/abs/2410.06089
tags:
- aspect
- instruction
- complex
- level
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating large language
  models on complex instructions, where different aspects of the instruction have
  varying levels of importance. To solve this, the authors introduce TOWER, a novel
  evaluation metric that assigns weights to individual aspects based on their importance
  within the context of the entire instruction.
---

# TOWER: Tree Organized Weighting for Evaluating Complex Instructions

## Quick Facts
- arXiv ID: 2410.06089
- Source URL: https://arxiv.org/abs/2410.06089
- Reference count: 3
- Primary result: Tree-based weighting metric (TOWER) achieves 0.72 Spearman correlation with human preferences for evaluating complex instruction following

## Executive Summary
This paper addresses the challenge of evaluating large language models on complex instructions, where different aspects of the instruction have varying levels of importance. To solve this, the authors introduce TOWER, a novel evaluation metric that assigns weights to individual aspects based on their importance within the context of the entire instruction. They propose three methods for determining aspect importance: Direct Scoring, Ranking, and Tree-based Weighting. Through experiments, they find that human annotators strongly prefer tree-based representations of complex instructions, with tree-based weighting (TOWER) achieving a Spearman correlation of 0.72 with human preferences, nearly matching the 0.74 correlation among human annotators themselves. TOWER provides insights into model performance, revealing that some models excel at addressing higher-level aspects while struggling with lower-level ones. The authors release their tree-based annotations of the InFoBench dataset and the corresponding evaluation code to facilitate future research.

## Method Summary
The TOWER method evaluates complex instructions by first decomposing them into individual aspect questions, then assigning importance weights based on hierarchical relationships. The paper proposes three approaches: Direct Scoring (rating each aspect on a 1-5 scale), Ranking (relative ordering of aspects), and Tree-based Weighting (organizing aspects into a hierarchical tree where weight = 1/level). For tree-based weighting, aspects closer to the root receive higher importance, reflecting their relative significance within the instruction. The method uses GPT-4-Turbo for automated tree generation and aspect evaluation, reducing manual annotation costs while maintaining high agreement with human preferences.

## Key Results
- TOWER achieves a Spearman correlation of 0.72 with human preferences, nearly matching the 0.74 correlation among human annotators themselves
- Tree-based representations of complex instructions align better with human preferences than ranking or direct scoring methods
- GPT-4-Turbo's performance differences across tree levels reveal varying capabilities in addressing different instruction aspects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tree-based representations of complex instructions align better with human preferences than ranking or direct scoring methods.
- Mechanism: The tree structure captures hierarchical relationships between instruction aspects, where parent nodes represent more important high-level aspects and child nodes represent lower-level modifications or details. This hierarchical organization mirrors how humans naturally prioritize instruction components.
- Core assumption: Human judgment of instruction aspect importance follows a hierarchical structure rather than a flat ranking or linear scoring system.
- Evidence anchors:
  - [abstract] "We show that human annotators agree with tree-based representations of these complex instructions nearly as much as they agree with other human annotators."
  - [section 4.4] "Surprisingly, with an average correlation of 0.72, Tree-Based Weighting aligns with human annotators almost as closely as the annotators align with each other."
  - [corpus] Weak - related work focuses on LLM-as-a-judge approaches but doesn't specifically address hierarchical tree structures for aspect weighting.
- Break condition: If human judgment of aspect importance is better represented by flat rankings or if multiple aspects have equal importance that cannot be captured in a tree structure.

### Mechanism 2
- Claim: Weighting instruction aspects based on their position in the tree (1/level) provides a more nuanced evaluation than equal weighting.
- Mechanism: The inverse level weighting scheme gives higher importance to aspects closer to the root (level 1) and progressively less importance to deeper aspects, reflecting the natural hierarchy of instruction requirements.
- Core assumption: Higher-level aspects in the instruction tree are more critical to overall instruction following success than lower-level aspects.
- Evidence anchors:
  - [section 3.3] "the weight for an aspect question is derived from the level it occurs within the labeled tree, where a parent has higher importance than the children. Specifically the weight is calculated by: 1/level(v)"
  - [section 4.5] "GPT-4-Turbo has a difference of only 0.92 between the Tree-Based Weighting metric and DRFR, indicating that it addresses all aspects equally"
  - [corpus] Weak - related papers discuss instruction following but don't explore hierarchical weighting schemes.
- Break condition: If lower-level aspects are equally or more important than higher-level ones, or if the inverse level weighting doesn't accurately reflect human importance judgments.

### Mechanism 3
- Claim: Using LLMs to generate tree structures for instruction aspect importance reduces manual annotation costs while maintaining high agreement with human preferences.
- Mechanism: Automated tree generation through LLM prompting creates consistent, reproducible aspect importance weights without the need for expensive human annotation of each aspect's importance.
- Core assumption: LLMs can accurately capture the hierarchical structure of human-written instructions and generate appropriate tree representations.
- Evidence anchors:
  - [abstract] "we opt to use automated annotation and measure agreement with human evaluators"
  - [section 4.2] "We also use GPT-4-Turbo for our experiments involving automated ranking, direct scoring, and tree-based labeling"
  - [section 4.4] "Our results show human annotators have an average correlation of 0.74 with each other" and "Tree-Based Weighting aligns with human annotators almost as closely as the annotators align with each other"
  - [corpus] Moderate - papers like "On Evaluating LLM Alignment by Evaluating LLMs as Judges" discuss LLM-as-judge approaches but not specifically for tree-based importance weighting.
- Break condition: If LLM-generated trees consistently misrepresent human judgment of aspect importance or if the LLM has inherent biases that affect the tree generation.

## Foundational Learning

- Concept: Spearman rank correlation
  - Why needed here: Used to measure agreement between human annotators and automated weighting approaches for instruction aspect importance.
  - Quick check question: If human annotator A ranks aspects as [1,2,3,4] and human annotator B ranks them as [1,2,3,4], what is their Spearman correlation?

- Concept: Hierarchical tree structures
  - Why needed here: The core mechanism for representing instruction aspect importance relationships in TOWER.
  - Quick check question: In a tree with root node A, children B and C, and C's child D, what are the levels of each node?

- Concept: Inverse weighting schemes
  - Why needed here: The specific weighting formula (1/level) used to calculate aspect importance in the tree structure.
  - Quick check question: If an aspect is at level 3 in the tree, what weight does it receive using the TOWER weighting scheme?

## Architecture Onboarding

- Component map: Instruction → Tree generation → Aspect weighting → Model evaluation → Performance analysis
- Critical path: Instruction → Tree generation → Aspect weighting → Model evaluation → Performance analysis
- Design tradeoffs: Automated tree generation reduces costs but may introduce LLM biases; inverse level weighting captures hierarchy but may oversimplify importance relationships.
- Failure signatures: Low agreement between LLM-generated trees and human preferences; models showing large performance gaps between different tree levels; inconsistent tree structures across similar instructions.
- First 3 experiments:
  1. Compare Spearman correlation between LLM-generated trees and human rankings on a small subset of instructions.
  2. Evaluate model performance differences between TOWER and baseline DRFR metrics on the same instructions.
  3. Analyze model performance degradation across different tree levels to identify patterns in instruction following capabilities.

## Open Questions the Paper Calls Out
None

## Limitations
- The inverse level weighting scheme (1/level) may oversimplify importance relationships by assuming linear decay of importance with tree depth.
- Automated tree generation using LLMs introduces potential biases from the underlying model's understanding of instruction hierarchies.
- The method's performance on domain-specific instructions outside the InFoBench dataset composition remains unverified.

## Confidence
- Tree structure alignment with human preferences: Medium-High
- Inverse level weighting effectiveness: Medium
- LLM-based tree generation reliability: Medium

## Next Checks
1. Test TOWER's performance on instructions from different domains (legal, medical, technical) to assess generalizability beyond the InFoBench dataset composition.
2. Compare TOWER against human expert evaluations on a subset of instructions to measure real-world alignment beyond annotator agreement.
3. Evaluate whether alternative weighting schemes (exponential decay, weighted combinations of levels) outperform the current inverse level approach for specific instruction types.