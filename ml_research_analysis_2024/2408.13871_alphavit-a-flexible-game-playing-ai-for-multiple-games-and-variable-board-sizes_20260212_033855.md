---
ver: rpa2
title: 'AlphaViT: A flexible game-playing AI for multiple games and variable board
  sizes'
arxiv_id: '2408.13871'
source_url: https://arxiv.org/abs/2408.13871
tags:
- alphavit
- alphavid
- alphavda
- alphazero
- board
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces three transformer-based game-playing agents
  (AlphaViT, AlphaViD, AlphaVDA) that overcome AlphaZero's fixed board size limitation
  by using Vision Transformers. These agents can play multiple board games (Connect
  4, Gomoku, Othello) with varying board sizes using a single neural network with
  shared weights.
---

# AlphaViT: A flexible game-playing AI for multiple games and variable board sizes

## Quick Facts
- arXiv ID: 2408.13871
- Source URL: https://arxiv.org/abs/2408.13871
- Reference count: 7
- Three transformer-based game-playing agents overcome AlphaZero's fixed board size limitation

## Executive Summary
This paper introduces AlphaViT, AlphaViD, and AlphaVDA - transformer-based game-playing agents that overcome AlphaZero's fixed board size limitation by using Vision Transformers. The agents can play multiple board games (Connect 4, Gomoku, Othello) with varying board sizes using a single neural network with shared weights. The key innovation replaces AlphaZero's residual blocks with ViT architecture, incorporating transformer encoders and decoders to handle variable action spaces and board sizes. Experimental results show these agents outperform traditional algorithms and approach AlphaZero's performance across games, with multitask training performing comparably or better than single-game training.

## Method Summary
The authors replace AlphaZero's residual blocks with Vision Transformer architecture to enable variable board size handling. Three agents are proposed: AlphaViT uses only a transformer encoder, while AlphaViD and AlphaVDA add decoders with different input strategies. The architecture employs learnable tokens (value, game, pass) to provide flexibility for multiple games and board sizes. The models are trained using self-play reinforcement learning combined with Monte Carlo Tree Search, with the transformer predicting both value and policy for move selection.

## Key Results
- AlphaViT with L8 encoder outperforms AlphaZero by 20-30% in Elo ratings for Connect 4 and Othello
- Multitask training performs comparably to or better than single-game training across all games
- Fine-tuning from small board games (5x4 Connect 4) accelerates convergence, particularly for Gomoku (6x6→15x15)
- Deeper transformer encoders (L8 for AlphaViT, L5 for AlphaViD/AlphaVDA) improve performance by 20-30% in Elo ratings
- Agents outperform traditional algorithms (Minimax, MCTS) across all tested games and board sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision Transformer architecture replaces fixed-size residual blocks to enable variable board size handling
- Mechanism: ViT divides input into fixed-size patches and processes them with self-attention, making the model independent of overall input dimensions
- Core assumption: Patch embeddings can encode spatial information adequately for game board states
- Evidence anchors:
  - [abstract] "Vision Transformers (ViT) into the AlphaZero framework"
  - [section 1] "ViT is an image-classification DNN based on the transformer architecture. ViT divides an image into patches and infers its class from the patches"
  - [corpus] Weak - neighbors focus on general game-playing frameworks, not ViT-specific architecture

### Mechanism 2
- Claim: Learnable tokens (value, game, pass) provide flexibility for multiple games and board sizes
- Mechanism: Special embeddings prepended/appended to patch sequence allow single network to handle different game types and rules (like pass moves in Othello)
- Core assumption: Game-specific tokens can be learned to represent distinct game mechanics within shared architecture
- Evidence anchors:
  - [section 3.1] "AlphaViT employs three special learnable embeddings (tokens) to enable flexibility for different games and board sizes"
  - [abstract] "AlphaViT, AlphaViD (AlphaViT with a transformer decoder), and AlphaVDA (AlphaViD with learnable action embeddings)"
  - [corpus] Weak - no corpus evidence about token-based architectural flexibility

### Mechanism 3
- Claim: Multitask training achieves performance comparable to single-game training
- Mechanism: Shared transformer encoder learns common game-playing patterns while maintaining game-specific capabilities through token embeddings
- Core assumption: There exist transferable representations across Connect 4, Gomoku, and Othello that can be learned simultaneously
- Evidence anchors:
  - [abstract] "simultaneous training on multiple games yields performance comparable to, or even surpassing, that of single-game training"
  - [section 5.1] "Multitask AlphaViT L4 (AlphaViT L4 Multi) demonstrates competitive performance on large boards and closely approaches AlphaZero"
  - [corpus] Weak - neighbors discuss general game frameworks but not multitask learning performance

## Foundational Learning

- Concept: Transformer self-attention mechanisms
  - Why needed here: Core of ViT architecture that enables variable input processing
  - Quick check question: How does self-attention allow transformers to process sequences of different lengths without changing the architecture?

- Concept: Game tree search and Monte Carlo Tree Search
  - Why needed here: How the agent selects moves using value/policy predictions from the neural network
  - Quick check question: What role do the value and policy heads play in guiding MCTS exploration?

- Concept: Transfer learning and fine-tuning
  - Why needed here: Explains how pre-training on small boards accelerates learning on larger boards
  - Quick check question: Why would weights learned on a 5x4 Connect 4 board help with a 7x6 board?

## Architecture Onboarding

- Component map: Input → Convolutional patch embedding → Learnable tokens (value/game/pass) → Position embeddings → Transformer encoder/decoder → Value/policy heads → MCTS
- Critical path: Patch embedding quality → Token learning → Position encoding → Encoder depth → Decoder flexibility (for AlphaViD/AlphaVDA)
- Design tradeoffs: Fixed patch size vs. variable board dimensions, encoder-only (AlphaViT) vs. encoder-decoder (AlphaViD/AlphaVDA), multitask vs. single-task training
- Failure signatures: Poor performance on new board sizes (patch/padding mismatch), inability to handle pass moves (missing token), catastrophic forgetting in multitask training
- First 3 experiments:
  1. Test patch embedding with varying board sizes to verify spatial encoding
  2. Evaluate token learning by training on single game then testing on different game
  3. Compare multitask vs. single-task training convergence on same architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do AlphaViT, AlphaViD, and AlphaVDA perform on games with more complex rules or stochastic elements compared to their performance on deterministic board games?
- Basis in paper: [explicit] The authors mention plans to extend these architectures to games with more complex rules and stochastic elements in future work.
- Why unresolved: The paper only evaluates performance on deterministic board games (Connect 4, Gomoku, Othello), so the agents' capabilities with more complex games remain untested.
- What evidence would resolve it: Experiments showing Elo ratings and performance metrics when applying these transformer-based agents to games like Poker, Backgammon, or other stochastic board games with varying rules.

### Open Question 2
- Question: What is the optimal number of transformer encoder layers for AlphaViT, AlphaViD, and AlphaVDA across different game types and board sizes?
- Basis in paper: [explicit] The paper tests L4 and L8 for AlphaViT, and L1 and L5 for AlphaViD/AlphaVDA, showing performance improvements with deeper encoders but noting diminishing returns.
- Why unresolved: While the paper shows that deeper encoders generally improve performance, it doesn't identify the optimal depth for each game type or board size configuration.
- What evidence would resolve it: Systematic testing of a wider range of encoder depths (e.g., L2, L3, L6, L7) across all games and board sizes to identify performance peaks and optimal trade-offs between depth and computational efficiency.

### Open Question 3
- Question: Can the transformer-based architectures be further optimized to reduce parameter count while maintaining or improving performance compared to AlphaZero?
- Basis in paper: [inferred] The paper notes that AlphaZero achieves top performance with fewer parameters in some cases, suggesting a trade-off between transformer flexibility and efficiency.
- Why unresolved: The current transformer-based agents have significantly more parameters than AlphaZero, and the paper doesn't explore architectural optimizations that might reduce this gap.
- What evidence would resolve it: Experiments comparing parameter-efficient transformer variants (e.g., using depthwise separable convolutions, pruning, or quantization) against AlphaZero while measuring performance and parameter count.

## Limitations

- Cross-game generalization limits: Demonstrated only on three similar board games, not tested on substantially different game types
- Architectural scalability constraints: Deeper transformers improve performance but computational efficiency versus parameter count trade-off remains unclear
- Fine-tuning transfer assumptions: Acceleration benefits demonstrated only for one game pair, may not generalize across all game combinations

## Confidence

- High confidence: Transformer-based variable input handling (ViT patch embeddings demonstrably work for different board sizes)
- Medium confidence: Multitask training performance (results show comparable performance but limited game diversity)
- Medium confidence: Fine-tuning acceleration (shown for one game pair but mechanism needs broader validation)

## Next Checks

1. **Cross-domain generalization test**: Evaluate the trained models on completely different game types (e.g., card games like Poker, or asymmetric games like chess vs. checkers) to test the claimed "multiple games" flexibility beyond similar board games.

2. **Scaling limit analysis**: Systematically test deeper transformer configurations (L10, L12, L15 encoders) to identify performance plateaus and computational efficiency breakpoints, particularly comparing parameter count vs. Elo gains against AlphaZero baselines.

3. **Transfer robustness study**: Conduct fine-tuning experiments across all game pairs in both directions (small→large and large→small boards) and with varying scale ratios (2x, 3x, 5x board dimensions) to validate the claimed transfer learning benefits and identify failure conditions.