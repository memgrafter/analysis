---
ver: rpa2
title: 'GUIDEQ: Framework for Guided Questioning for progressive informational collection
  and classification'
arxiv_id: '2411.05991'
source_url: https://arxiv.org/abs/2411.05991
tags:
- information
- question
- partial
- guide
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces GUIDE Q, a framework that combines a classifier\u2019\
  s explainability with large language model (LLM) reasoning to generate targeted\
  \ questions for incomplete information classification. The method uses occlusion-based\
  \ keyword extraction for each label and leverages the top-3 predicted labels to\
  \ prompt an LLM (LLaMa-3 8B) to generate questions that elicit the most distinguishing\
  \ missing details."
---

# GUIDEQ: Framework for Guided Questioning for progressive informational collection and classification

## Quick Facts
- arXiv ID: 2411.05991
- Source URL: https://arxiv.org/abs/2411.05991
- Reference count: 9
- This paper introduces GUIDE Q, a framework that combines a classifier's explainability with large language model (LLM) reasoning to generate targeted questions for incomplete information classification.

## Executive Summary
This paper introduces GUIDE Q, a framework that combines a classifier's explainability with large language model (LLM) reasoning to generate targeted questions for incomplete information classification. The method uses occlusion-based keyword extraction for each label and leverages the top-3 predicted labels to prompt an LLM (LLaMa-3 8B) to generate questions that elicit the most distinguishing missing details. Evaluated across six text classification datasets, GUIDE Q consistently outperformed baselines—achieving up to 22% higher F1-scores and generating more contextually relevant questions, as shown by win-rate comparisons. The approach also showed flexibility with n-gram keyword strategies and effectiveness in multi-turn interactions. Limitations include dependency on classifier quality and computational demands of LLMs.

## Method Summary
GUIDE Q addresses incomplete information classification by combining classifier explainability with LLM reasoning. The method involves training a classifier on complete labeled data, then using occlusion techniques to extract significant keywords for each label. For a given partial input, the top-3 predicted labels and their associated keywords are fed to an LLM (Llama-3 8B) to generate targeted questions. A QA model extracts answers from the reference text based on these questions, and the extracted information is appended to the partial input for reclassification. The framework was evaluated across six datasets using various keyword strategies (unigrams, bigrams, trigrams) and demonstrated effectiveness in both single-turn and multi-turn interactions.

## Key Results
- GUIDE Q achieved up to 22% higher F1-scores compared to baselines across six text classification datasets
- The framework generated more contextually relevant questions, as evidenced by win-rate comparisons against LLM-only and LLM-nk baselines
- Multi-turn interactions showed effectiveness, particularly on the CNEWS dataset, with dynamic keyword pool updates improving classification accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Classifier model explanations improve LLM question quality.
- Mechanism: The GUIDE Q framework uses a trained classifier to identify the top-3 most probable labels for incomplete input, then leverages occlusion techniques to extract the most significant keywords for each label. These keywords act as semantic anchors that guide the LLM to ask questions targeting the most distinguishing features between labels, reducing hallucination and irrelevant questions.
- Core assumption: The classifier's top-3 predictions contain at least one correct label, and the occlusion-derived keywords are semantically representative of the label.
- Evidence anchors:
  - [abstract] "GUIDE Q derives the most significant key-words representative of a label using occlusions… based on the top-3 classifier label outputs and the significant words…"
  - [section 3.3] "We employ the occlusion method to identify the top-i significant words or word pairs for each label li…"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.435, average citations=0.0. Weak evidence of direct corpus support for occlusion-based keyword extraction.
- Break condition: If the classifier confidence is low or if occlusion fails to extract meaningful keywords, the guided questions will lose relevance.

### Mechanism 2
- Claim: Multi-turn guided questioning progressively refines classification accuracy.
- Mechanism: By iteratively updating the pool of guiding keywords (removing those already used) and generating follow-up questions, the framework gathers more distinguishing information over multiple turns, enabling better discrimination between closely related labels.
- Core assumption: The partial input contains enough initial context to start the questioning loop, and the QA model can extract relevant snippets from the reference text.
- Evidence anchors:
  - [section 7] "We specifically report multi-turn results on the CNEWS dataset… This method involves dynamically updating the pool of guiding words, removing those already used in previous turns."
  - [abstract] "We also show that GUIDE Q generates more accurate and targeted questions in relation to the user query."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.435, average citations=0.0. No strong corpus evidence for multi-turn effectiveness.
- Break condition: If the initial partial input is too vague, the questioning loop may not converge or may ask irrelevant follow-ups.

### Mechanism 3
- Claim: Classifier model choice influences keyword quality and thus question relevance.
- Mechanism: Larger classifier models (e.g., DeBERTa vs. BERT) capture richer semantic relationships, leading to better occlusion-derived keywords, which in turn produce more focused and relevant questions.
- Core assumption: The larger model has higher classification accuracy on full-text inputs and more expressive embeddings for keyword extraction.
- Evidence anchors:
  - [section 6.1] "DeBERTa being a larger parameter model compared to BERT is able to capture more relevant explainable keywords for a given label using occlusions…"
  - [abstract] "GUIDE Q derives the most significant key-words representative of a label using occlusions…"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.435, average citations=0.0. Weak corpus evidence linking model size to keyword quality.
- Break condition: If the larger model overfits or if keyword extraction becomes noisy, question quality may degrade.

## Foundational Learning

- Concept: Occlusion-based feature importance
  - Why needed here: To identify the most semantically significant words/phrases that represent each classification label, enabling the LLM to ask questions targeting the distinguishing concepts.
  - Quick check question: If you mask the word "fever" in a medical symptom text, and the classifier's confidence for "flu" drops sharply, what does that tell you about "fever"?
- Concept: In-context learning (ICL) with demonstrations
  - Why needed here: The LLM is prompted with few-shot examples to generate questions in a consistent, targeted format; ICL allows the model to adapt to the guided questioning task without fine-tuning.
  - Quick check question: If you give the LLM two examples of partial input + top-3 labels → question, can it generate a third question correctly?
- Concept: Confidence-based filtering in QA models
  - Why needed here: After the LLM generates a question, a QA model extracts answers from the reference text; filtering by confidence (e.g., 20% threshold) ensures only high-quality, relevant answers are used to augment the input.
  - Quick check question: If the QA model's confidence is below 0.2, should you accept that answer or discard it?

## Architecture Onboarding

- Component map: Input preprocessor -> Classifier (BERT/DeBERTa) -> Occlusion keyword extractor -> LLM (Llama-3 8B) with prompt templates -> QA model (RoBERTa/DistilBERT) -> Output aggregator -> Updated input for next turn (optional)
- Critical path: Classifier prediction -> Keyword extraction -> LLM question generation -> QA answer extraction -> Classification with augmented input
- Design tradeoffs:
  - Classifier choice: Larger models give better keywords but cost more compute
  - LLM size: Smaller LLMs (8B) are faster and cheaper but may produce less nuanced questions
  - Keyword granularity: Unigrams capture broad concepts; bigrams/trigrams capture compound concepts; trade-off between specificity and coverage
- Failure signatures:
  - Low classifier confidence -> irrelevant keywords -> off-topic questions
  - QA model confidence below threshold -> no answer -> no information gain
  - LLM hallucination -> questions not grounded in keywords -> useless follow-ups
- First 3 experiments:
  1. Run GUIDE Q on a simple dataset (e.g., Symptom2Disease) with partial input split at sentence boundary; verify F1 improvement over "partial" baseline
  2. Swap BERT for DeBERTa; compare keyword quality and question relevance via win-rate against LLM baseline
  3. Enable multi-turn mode on CNEWS; measure F1 improvement per turn and observe keyword pool update behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GUIDE Q scale with larger datasets or more complex classification tasks beyond the six evaluated datasets?
- Basis in paper: [inferred] The paper evaluates GUIDE Q on six diverse datasets but does not explore its scalability to larger or more complex datasets
- Why unresolved: The paper focuses on specific datasets and does not provide evidence or analysis on how GUIDE Q would perform with significantly larger datasets or tasks with higher complexity
- What evidence would resolve it: Conducting experiments with larger datasets or more complex classification tasks, and analyzing the performance trends, would provide insights into GUIDE Q's scalability

### Open Question 2
- Question: What is the impact of using different LLM models (e.g., larger models like GPT-4) on the performance of GUIDE Q?
- Basis in paper: [explicit] The paper uses Llama-3 8B due to its computational efficiency but does not explore the impact of using larger or different LLM models
- Why unresolved: The choice of Llama-3 8B was driven by computational considerations, but the paper does not investigate whether larger or different LLM models could enhance performance
- What evidence would resolve it: Comparing GUIDE Q's performance using different LLM models, including larger ones, would reveal the impact of model size and architecture on the framework's effectiveness

### Open Question 3
- Question: How does GUIDE Q handle multi-label classification tasks where a single input might belong to multiple categories?
- Basis in paper: [inferred] The paper focuses on single-label classification tasks and does not address the scenario of multi-label classification
- Why unresolved: The framework is designed for single-label classification, and the paper does not provide insights into its adaptability or performance in multi-label scenarios
- What evidence would resolve it: Adapting GUIDE Q to handle multi-label classification and evaluating its performance in such tasks would clarify its applicability to more complex classification scenarios

### Open Question 4
- Question: What are the long-term effects of iterative questioning in multi-turn interactions on the overall classification accuracy?
- Basis in paper: [explicit] The paper explores multi-turn interactions but does not analyze the long-term effects of iterative questioning on classification accuracy
- Why unresolved: While multi-turn interactions are discussed, the paper does not provide a detailed analysis of how repeated questioning impacts classification accuracy over time
- What evidence would resolve it: Conducting longitudinal studies on multi-turn interactions and analyzing the trends in classification accuracy would provide insights into the long-term effects of iterative questioning

### Open Question 5
- Question: How does the quality of generated questions vary with different levels of initial information completeness?
- Basis in paper: [inferred] The paper evaluates GUIDE Q with partially complete information but does not explore how question quality varies with different levels of initial completeness
- Why unresolved: The paper does not investigate whether the quality of generated questions changes based on how much information is initially provided
- What evidence would resolve it: Testing GUIDE Q with inputs of varying levels of completeness and analyzing the quality of generated questions would reveal how initial information affects question generation

## Limitations
- The framework depends heavily on classifier quality and the effectiveness of occlusion-based keyword extraction
- Computational overhead from running both large classifiers and LLMs may limit practical deployment
- The evaluation focuses on single-label classification and does not address multi-label scenarios

## Confidence
- Classifier-explainability + LLM reasoning mechanism: Medium confidence - core idea is sound, but exact implementation details and sensitivity to hyperparameters are uncertain
- Multi-turn questioning effectiveness: Medium confidence - demonstrated on one dataset only, with unclear generalization
- Classifier model choice impact: Medium confidence - shown for BERT vs DeBERTa, but no broader model family analysis

## Next Checks
1. **Ablation study on component contributions**: Run GUIDE Q with (a) keywords only (no classifier top-3), (b) classifier top-3 only (no keywords), (c) full system. Measure F1 changes to isolate each component's impact.
2. **Prompt sensitivity analysis**: Systematically vary the few-shot examples in the LLM prompt (change wording, order, number) and measure question quality via win-rate. Determine if results are robust to prompt engineering.
3. **Cross-dataset classifier generalization**: Train the classifier on complete data from one domain (e.g., Crypto News) and test GUIDE Q on a different domain (e.g., Symptom2Disease). Assess whether classifier explainability transfers across domains or if domain-specific training is required.