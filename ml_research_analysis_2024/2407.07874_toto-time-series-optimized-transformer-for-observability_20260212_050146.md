---
ver: rpa2
title: 'Toto: Time Series Optimized Transformer for Observability'
arxiv_id: '2407.07874'
source_url: https://arxiv.org/abs/2407.07874
tags:
- time
- series
- data
- forecasting
- datadog
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This technical report introduces Toto, a state-of-the-art foundation
  model for time series forecasting developed by Datadog. Toto addresses the challenge
  of forecasting observability metrics, which often exhibit high time resolution,
  sparsity, extreme right skew, and dynamic non-stationary behavior.
---

# Toto: Time Series Optimized Transformer for Observability

## Quick Facts
- arXiv ID: 2407.07874
- Source URL: https://arxiv.org/abs/2407.07874
- Authors: Ben Cohen; Emaad Khwaja; Kan Wang; Charles Masson; Elise Ramé; Youssef Doubli; Othmane Abou-Amal
- Reference count: 40
- Primary result: State-of-the-art zero-shot performance on time series forecasting with macro-averaged MAE of 0.312 on LSF datasets

## Executive Summary
Toto is a foundation model for time series forecasting specifically designed for observability metrics, addressing challenges like high time resolution, sparsity, extreme right skew, and dynamic non-stationary behavior. The model employs a decoder-only transformer architecture with a novel proportional factorized space-time attention mechanism, enabling efficient modeling of both temporal and spatial dependencies. Trained on a dataset of one trillion time series points (75% from Datadog platform, 25% from LOTSA dataset), Toto demonstrates superior performance on both public benchmarks and Datadog-specific observability data.

The model achieves state-of-the-art zero-shot performance on the LSF datasets with a macro-averaged MAE of 0.312 and MSE of 0.265, significantly outperforming other models. On the Datadog benchmark, Toto achieves a sMAPE of 0.672 and sMdAPE of 0.318, highlighting its effectiveness for real-world observability scenarios where other models struggle with the unique characteristics of monitoring data.

## Method Summary
Toto utilizes a decoder-only transformer architecture with a novel proportional factorized space-time attention mechanism that balances temporal and spatial modeling through configurable ratios of time-wise to space-wise attention blocks. The model processes input sequences using a patch size of 32, with embedding dimension 512, 24 layers, and 8 attention heads. For probabilistic forecasting, Toto employs a Student-T mixture model head with 16 components, providing robust handling of complex distributions and outliers common in observability data. The model is trained using the AdamW optimizer on a massive dataset containing one trillion time series points, with three-quarters coming from the Datadog platform and the remainder from the LOTSA dataset.

## Key Results
- Achieves state-of-the-art zero-shot performance on LSF datasets with macro-averaged MAE of 0.312 and MSE of 0.265
- Significantly outperforms other zero-shot models on Datadog benchmark with sMAPE of 0.672 and sMdAPE of 0.318
- Demonstrates effectiveness for observability data characterized by high resolution, sparsity, and dynamic non-stationary behavior
- Shows robustness to outliers through Student-T mixture model prediction head

## Why This Works (Mechanism)
Toto's effectiveness stems from its specialized architecture designed for observability time series. The proportional factorized space-time attention mechanism allows the model to dynamically balance temporal and spatial dependencies, crucial for handling the unique characteristics of monitoring data. The decoder-only transformer design enables autoregressive forecasting while maintaining computational efficiency through patch-based processing. The Student-T mixture model head provides robust probabilistic predictions that can handle the heavy-tailed distributions and outliers common in observability metrics, making the model particularly well-suited for real-world monitoring scenarios.

## Foundational Learning

**Transformer Architecture**
- Why needed: Enables sequence modeling with attention mechanisms that can capture long-range dependencies
- Quick check: Verify positional encoding implementation and multi-head attention computation

**Attention Mechanisms**
- Why needed: Allows selective focus on relevant parts of input sequences for both temporal and spatial relationships
- Quick check: Validate attention weight distributions and sparsity patterns

**Student-T Distribution**
- Why needed: Provides heavy-tailed distribution suitable for modeling outliers in observability data
- Quick check: Confirm mixture model parameters converge to reasonable values during training

**Space-Time Factorization**
- Why needed: Efficiently models both temporal progression and cross-series relationships in time series data
- Quick check: Test different ratios of time-wise vs space-wise attention blocks

## Architecture Onboarding

**Component Map**
Input Patch -> Embedding Layer -> Proportional Factorized Space-Time Attention -> Feed-Forward Network -> Layer Norm -> Student-T Mixture Head

**Critical Path**
Patch embedding → Space-time attention → Feed-forward network → Layer normalization → Prediction head

**Design Tradeoffs**
The model trades computational efficiency for modeling capability by using patch-based processing (size 32) rather than raw time series, enabling handling of high-resolution data while maintaining manageable memory requirements. The proportional factorized attention balances temporal and spatial modeling, but requires careful tuning of the ratio parameter.

**Failure Signatures**
- Training instability when mixture model components collapse or diverge
- Poor generalization to new time series if synthetic data lacks diversity
- Attention mechanism fails to capture relevant patterns if ratio parameters are poorly chosen

**First Experiments**
1. Implement and test proportional factorized space-time attention with synthetic time series data
2. Validate Student-T mixture model head on simple synthetic distributions
3. Benchmark transformer architecture on LOTSA dataset before scaling to full model

## Open Questions the Paper Calls Out

**Open Question 1**
How does the performance of Toto compare to other models when trained on observability data specifically, versus general-purpose time series data? The paper mentions that Toto is specifically trained on observability data and outperforms other models on the Datadog benchmark. However, it does not explicitly compare the performance of Toto when trained on observability data versus general-purpose data.

**Open Question 2**
What is the impact of the patch size on Toto's performance? The paper mentions that Toto was trained using a fixed patch size of 32, but does not explore the impact of different patch sizes on performance.

**Open Question 3**
How does Toto's performance scale with the size of the dataset? The paper mentions that Toto was trained on a dataset of one trillion time series points, but does not explore how its performance scales with dataset size.

## Limitations

- Evaluation methodology on public datasets lacks clear documentation of experimental setup and hyperparameter optimization
- Proprietary Datadog dataset prevents independent validation of real-world performance claims
- Missing ablation studies showing individual contributions of architectural components
- No analysis of computational requirements for training and inference

## Confidence

**High Confidence**: Architectural description of decoder-only transformer with space-time attention is clearly specified

**Medium Confidence**: Performance claims on LSF datasets, as methodology details are limited

**Low Confidence**: Claims about superiority on proprietary Datadog benchmark data due to lack of reproducibility

## Next Checks

1. Replicate the space-time attention mechanism by implementing proportional factorized attention with configurable time-space ratios and validating its behavior on synthetic time series data

2. Benchmark against public datasets using the same hyperparameters and preprocessing pipeline as described to verify zero-shot performance claims on LSF datasets

3. Analyze computational requirements by measuring training time, memory usage, and inference latency for different model sizes to assess practical deployment feasibility