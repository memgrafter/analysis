---
ver: rpa2
title: 'Small Language Models: Survey, Measurements, and Insights'
arxiv_id: '2409.15790'
source_url: https://arxiv.org/abs/2409.15790
tags:
- arxiv
- slms
- language
- memory
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first comprehensive survey of small language
  models (SLMs), focusing on transformer-based, decoder-only models with 100M-5B parameters.
  The authors analyze 70 state-of-the-art open-source SLMs across three axes: architectures,
  training datasets, and training algorithms, while benchmarking their capabilities
  in commonsense reasoning, mathematics, in-context learning, and long-context retrieval.'
---

# Small Language Models: Survey, Measurements, and Insights

## Quick Facts
- arXiv ID: 2409.15790
- Source URL: https://arxiv.org/abs/2409.15790
- Reference count: 40
- Primary result: First comprehensive survey of 70 small language models (100M-5B parameters) analyzing architectures, training datasets, and benchmarking capabilities across multiple tasks

## Executive Summary
This paper presents the first comprehensive survey of small language models (SLMs), focusing on transformer-based, decoder-only models with 100M-5B parameters. The authors analyze 70 state-of-the-art open-source SLMs across three axes: architectures, training datasets, and training algorithms, while benchmarking their capabilities in commonsense reasoning, mathematics, in-context learning, and long-context retrieval. They also measure on-device runtime costs including inference latency and memory footprints.

Key findings include: (1) SLMs have shown significant performance improvements from 2022-2024, with models like Phi-3-mini rivaling larger LLaMA 3.1 8B in capability; (2) Model architecture innovations (group-query attention, gated FFN with SiLU activation) and high-quality synthetic datasets (DCLM, FineWeb-Edu) are driving SLM advancement; (3) Recent SLMs are "over-trained" with excessive training tokens (typically >1.5T) compared to Chinchilla scaling laws; (4) Larger models demonstrate superior long-context capabilities, while smaller models experience significant performance drops; (5) Runtime performance depends heavily on architecture choices and hardware compatibility, with quantization methods providing substantial speedups.

## Method Summary
The study collected 70 open-source SLMs with decoder-only transformer architecture and 100M-5B parameters, evaluating them using llama.cpp on edge devices (Jetson Orin NX 16GB, Pixel 7 Pro, Xiaomi 12S, Meizu 18Pro). The evaluation used 12 benchmark datasets across commonsense reasoning, mathematics, problem-solving, and long-context retrieval tasks, with default prompt length of 50 and generation length of 50. Models were tested with 4-bit quantization for larger variants. The analysis covered architectural trends, training dataset characteristics, and runtime performance metrics including accuracy, inference latency, memory footprint, and energy consumption.

## Key Results
- SLMs from 2024 rival larger models in capability, with Phi-3-mini matching LLaMA 3.1 8B performance
- Group-query attention and gated FFN with SiLU activation are dominant architectural patterns in recent SLMs
- SLMs are over-trained with >1.5T tokens versus Chinchilla scaling laws suggesting 20:1 token-to-parameter ratios
- Larger SLMs show significantly better long-context capabilities, with smaller models experiencing substantial performance degradation
- Runtime performance is heavily hardware-dependent, with quantization providing substantial speed improvements

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Group-query attention (GQA) and gated FFN with SiLU activation improve runtime performance more than vanilla transformers
- Mechanism: GQA reduces the number of key-value (KV) cache entries per query, lowering memory bandwidth pressure; gated FFN adds nonlinearity control to stabilize training and improve convergence; SiLU activation is smoother and often accelerates convergence
- Core assumption: Memory bandwidth and compute intensity dominate runtime costs on edge devices
- Evidence anchors:
  - [section] "As of August 2024, a typical SLM architecture tends to use group-query attention, gated FFN with SiLU activation..."
  - [section] "The correlation is likely hardware-dependent."
  - [corpus] Weak evidence

## Foundational Learning
- **Chinchilla scaling laws**: Relationship between model parameters and training tokens (20:1 ratio optimal) - why needed: Provides theoretical framework for efficient model training; quick check: Compare actual training token counts to parameter counts in surveyed models
- **Group-query attention**: Attention mechanism where queries are divided into groups sharing KV cache - why needed: Reduces memory footprint while maintaining performance; quick check: Measure KV cache size reduction vs standard attention
- **SiLU activation function**: Sigmoid-weighted Linear Unit (x * sigmoid(x)) - why needed: Smooth activation that often accelerates convergence; quick check: Compare training stability and convergence speed with ReLU
- **4-bit quantization**: Reducing model precision from 16/32-bit to 4-bit - why needed: Enables deployment on memory-constrained devices; quick check: Measure accuracy drop vs memory savings ratio
- **Rotary position embeddings**: Encodes absolute position information in attention patterns - why needed: Improves long-context performance; quick check: Compare performance on long-sequence tasks vs learned embeddings
- **Gated FFN**: Feed-forward network with gating mechanism - why needed: Adds nonlinearity control for stable training; quick check: Monitor training stability metrics during convergence

## Architecture Onboarding
### Component Map
Input -> Embedding Layer -> Transformer Blocks (Multi-Head Attention + Gated FFN + SiLU) -> Group-Query Attention -> Rotary Position Embeddings -> Output Layer

### Critical Path
Embedding → Multi-Head Attention → Feed-Forward Network → Output (for inference latency)

### Design Tradeoffs
- Depth vs width: Deeper models show better performance but higher latency
- Attention type: Standard vs group-query (memory vs speed tradeoff)
- Quantization level: 4-bit vs 8-bit (deployment size vs accuracy)
- Context length: Rotary vs learned embeddings (long-context vs training efficiency)

### Failure Signatures
- Memory overflow: Architecture too deep for device constraints
- Slow inference: Excessive KV cache size or inefficient attention patterns
- Training instability: Poor activation choice or insufficient gating

### 3 First Experiments
1. Compare inference latency of group-query vs standard attention on target hardware
2. Measure accuracy drop when applying 4-bit vs 8-bit quantization
3. Test long-context performance with rotary vs learned position embeddings

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the optimal parameter-token ratio for small language models when considering device deployment constraints versus pure performance scaling?
- Basis in paper: [explicit] The paper notes that SLMs are "over-trained" with excessive training tokens (typically >1.5T) compared to Chinchilla scaling laws, which suggest a 20:1 token-to-parameter ratio.
- Why unresolved: The paper observes this discrepancy but doesn't establish a new scaling law that accounts for deployment constraints like memory and compute on devices.
- What evidence would resolve it: Systematic experiments varying training token counts relative to parameter sizes while measuring both final model performance and device deployment efficiency metrics.

### Open Question 2
- Question: How can sparse architectures like mixture-of-experts be effectively adapted for small language models on resource-constrained devices?
- Basis in paper: [explicit] The paper notes that sparse SLMs are understudied and that MoE models typically sacrifice memory usage for computational efficiency, which conflicts with device constraints.
- Why unresolved: The paper identifies the potential but highlights challenges including memory overhead and compatibility with hardware accelerators.
- What evidence would resolve it: Benchmarking sparse SLM variants on actual edge hardware showing memory-CPU/GPU utilization tradeoffs and demonstrating practical deployment scenarios.

### Open Question 3
- Question: What is the optimal architecture for small language models when co-designed with specific hardware accelerators?
- Basis in paper: [explicit] The paper notes that model architecture choices significantly impact runtime performance and that co-design could push SLMs toward optimal accuracy-speed tradeoffs.
- Why unresolved: While the paper identifies this as a promising direction, it doesn't provide concrete co-design methodology or demonstrate specific architecture-hardware pairings.
- What evidence would resolve it: Comparative benchmarking of differently architected SLMs (varying depth-width ratios, attention types, etc.) deployed on the same hardware with systematic measurement of latency and accuracy trade-offs.

## Limitations
- Hardware-dependent measurements may not generalize across different deployment environments
- Training dataset analysis relies on publicly reported information that may be incomplete or inconsistent
- Focus on decoder-only transformer models excludes other architectural innovations
- "Over-training" hypothesis depends on accurate reporting of training procedures by various research groups

## Confidence
- **High Confidence**: Architectural trends analysis showing group-query attention and gated FFN with SiLU activation as common patterns
- **Medium Confidence**: Benchmarking results for inference latency and memory footprints due to hardware dependency
- **Low Confidence**: "Over-training" hypothesis regarding excessive training tokens due to reliance on self-reported training data

## Next Checks
1. **Cross-platform benchmarking**: Replicate latency and memory footprint measurements on diverse hardware platforms to validate hardware-dependency claims and establish generalizable performance patterns
2. **Training dataset verification**: Conduct detailed audit of actual training data composition used in recent SLMs by analyzing model outputs and controlled experiments to verify claimed benefits of synthetic datasets
3. **Long-context capability testing**: Perform systematic ablation studies on model size versus context length performance using standardized datasets, isolating effects of architectural choices from scaling effects