---
ver: rpa2
title: Phasic Diversity Optimization for Population-Based Reinforcement Learning
arxiv_id: '2403.11114'
source_url: https://arxiv.org/abs/2403.11114
tags:
- diversity
- algorithm
- agents
- learning
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the Phasic Diversity Optimization (PDO) algorithm
  for population-based reinforcement learning, addressing the challenge of balancing
  reward and diversity in small-scale populations. The key idea is to separate reward
  and diversity training into distinct phases within a Population-Based Training framework,
  using an archive to maintain high-performing agents and an auxiliary phase to optimize
  diversity without harming performance.
---

# Phasic Diversity Optimization for Population-Based Reinforcement Learning

## Quick Facts
- arXiv ID: 2403.11114
- Source URL: https://arxiv.org/abs/2403.11114
- Reference count: 37
- One-line primary result: PDO achieves superior performance compared to baselines in both reward optimization and quality diversity, with a maximum fitness of 972 in dogfight (vs. 968 for PBT) and coverage of 50.8 (vs. 20.2 for PBT)

## Executive Summary
This paper proposes the Phasic Diversity Optimization (PDO) algorithm for population-based reinforcement learning, addressing the challenge of balancing reward and diversity in small-scale populations. The key idea is to separate reward and diversity training into distinct phases within a Population-Based Training framework, using an archive to maintain high-performing agents and an auxiliary phase to optimize diversity without harming performance. The method introduces a differentiable similarity estimation and surrogate determinant regularization to model diversity via determinants, ensuring numerical stability. Experiments on a novel dogfight task and MuJoCo environments demonstrate that PDO achieves superior performance compared to baselines, with a maximum fitness of 972 in dogfight (vs. 968 for PBT), coverage of 50.8 (vs. 20.2 for PBT), and quality diversity scores of 39,201 (vs. 21,032 for PBT).

## Method Summary
PDO uses a two-phase optimization approach within a Population-Based Training framework. In the first phase, agents are trained in parallel to optimize reward, and the best-performing agents are stored in an archive. In the second phase, the top M agents from the archive are selected to optimize diversity via determinant-based regularization. The method uses differentiable similarity estimation to compute policy similarities efficiently, and surrogate determinant regularization to ensure numerical stability when computing determinants. The algorithm is tested on a novel dogfight task and MuJoCo environments, comparing performance against baselines like PBT, DvD, and DSE-UCB.

## Key Results
- PDO achieves maximum fitness of 972 in dogfight task vs. 968 for PBT
- PDO achieves coverage of 50.8 vs. 20.2 for PBT in dogfight
- PDO achieves quality diversity score of 39,201 vs. 21,032 for PBT in dogfight

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling reward and diversity into distinct phases avoids the performance-degrading trade-offs inherent in multi-objective optimization.
- Mechanism: By separating the optimization into a reward phase (exploiting high-performing agents) and an auxiliary diversity phase (diversifying via determinants without harming the archive), PDO prevents diversity optimization from interfering with reward maximization.
- Core assumption: The archive reliably preserves high-performing agents so that diversity phase changes do not affect overall performance.
- Evidence anchors:
  - [abstract]: "The decoupling of reward and diversity allows us to use an aggressive diversity optimization in the auxiliary phase without performance degradation."
  - [section]: "The idea of PDO is simple. It separates the training into two phases... In the auxiliary phase, we elect the best M agents from the archive, improve their population diversity via determinants, and then evaluate them to decide whether to store them in the archive."
  - [corpus]: Weak/no direct evidence. The paper cites related work (DvD, EDO-CS) that suffers from performance degradation when diversity is optimized jointly with reward, supporting PDO's design choice indirectly.
- Break condition: If the archive fails to maintain high-performing agents (e.g., due to poor archiving policy or instability), the diversity phase may inadvertently reduce overall performance.

### Mechanism 2
- Claim: Differentiable similarity estimation via sampled states allows efficient computation of the similarity matrix K without intractable integrals.
- Mechanism: Instead of computing the exact integral over the state space for similarity, PDO samples states from trajectories and uses a differentiable kernel function f to approximate similarity. This makes the gradient computation tractable.
- Core assumption: Finite sampling from trajectories provides a sufficiently accurate approximation of the true similarity between policies.
- Evidence anchors:
  - [section]: "The Differentiable Similarity Estimation (DSE) allows us to sample the visited states from trajectories of mixed average policies, and estimate the similarity by finite sampling instead of the intractable integral in (4)."
  - [section]: "We can easily map the value to [0, 1] by the function f if D is a symmetric and bounded metric. Then the policy can be updated by applying the chain rule to (5) w.r.t. the parameter θ."
  - [corpus]: No direct evidence. The method relies on the validity of the approximation, which is stated but not empirically validated in the provided text.
- Break condition: If the sampled states do not adequately represent the policy's behavior across the state space, the similarity estimation may be inaccurate, leading to poor diversity optimization.

### Mechanism 3
- Claim: Surrogate determinant regularization with a smoothed kernel matrix ensures numerical stability and avoids zero determinants during Cholesky decomposition.
- Mechanism: When the original kernel matrix K is singular (e.g., due to policy replication in PBT), PDO uses a surrogate matrix K̃ = βK + (1-β)I to ensure it is positive definite, allowing stable determinant computation and gradient calculation.
- Core assumption: The smoothing parameter β is chosen such that K̃ remains positive definite and the determinant is non-zero.
- Evidence anchors:
  - [section]: "To meet the conditions in Theorem 1, we use surrogate matrix K̃... Lemma 2 implies that if all the pairs of different policies in the population are not perfectly similar, and the kernel matrix is positive definite the condition of Theorem 1 is satisfied."
  - [section]: "The gradient of objective in (13) w.r.t. parameter θi ∈ Θ is given by: ∇θi JD(Θ) = (∇θiΘ)∇Θ det(K̃)"
  - [corpus]: No direct evidence. The method is theoretically justified but not empirically tested for edge cases in the provided text.
- Break condition: If β is too small, K̃ may still be singular; if too large, the diversity signal may be weakened excessively.

## Foundational Learning

- Concept: Determinantal Point Processes (DPPs)
  - Why needed here: DPPs provide a principled way to model diversity by using the determinant of a similarity matrix, which naturally repels similar policies.
  - Quick check question: Why does maximizing the determinant of a similarity matrix encourage diversity among policies?

- Concept: Population-Based Training (PBT)
  - Why needed here: PBT is the framework PDO builds upon, using parallel learners and periodic exploitation to balance exploration and exploitation in a population of agents.
  - Quick check question: How does PBT's exploitation mechanism (copying weights from high-performing agents) potentially lead to loss of diversity?

- Concept: Jensen-Shannon Divergence (JSD) and Wasserstein Distance
  - Why needed here: These are used as distance metrics between stochastic policies, which are then transformed into similarity measures via differentiable kernels.
  - Quick check question: What is the advantage of using Wasserstein distance over JSD for continuous action spaces in policy similarity estimation?

## Architecture Onboarding

- Component map:
  - Population of agents (Θ) -> Archive (A) -> Reward optimization phase -> Diversity optimization phase -> Behavior Descriptor (BD) -> Kernel function K -> Surrogate matrix K̃

- Critical path:
  1. Initialize population and archive
  2. Reward optimization phase: Train agents in parallel, evaluate, and store in archive
  3. Exploitation: Sample from archive to replace worst agents
  4. Diversity optimization phase: Select top M agents from archive, optimize diversity via determinants, evaluate, and update archive
  5. Repeat until convergence

- Design tradeoffs:
  - Using BD vs. reward-priority queue for archiving: BD allows behavior-based diversity but requires expert knowledge; priority queue is simpler but may not capture behavioral diversity.
  - Number of diversity iterations per epoch: More iterations increase diversity but may slow training; fewer iterations risk insufficient diversity.
  - Smoothing parameter β: Larger values ensure numerical stability but may weaken diversity optimization; smaller values risk instability.

- Failure signatures:
  - Archive becomes dominated by similar high-performing agents: Diversity optimization is ineffective.
  - Performance drops after diversity phase: Archive is not preserving high-performing agents effectively.
  - Training stalls: Kernel matrix K is singular, and β is not chosen properly.
  - High variance in results: Insufficient sampling in differentiable similarity estimation.

- First 3 experiments:
  1. Implement a simple 2-agent dogfight environment and run PDO vs. PBT to verify improved coverage and maximum fitness.
  2. Test different values of β in the surrogate determinant regularization to find the optimal balance between stability and diversity.
  3. Compare BD-based archiving vs. reward-priority queue archiving to evaluate the impact on behavioral diversity and performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PDO scale with population size, and what is the optimal population size for different tasks?
- Basis in paper: [inferred] The paper uses a population size of M=5 for experiments, but does not explore the impact of different population sizes on performance.
- Why unresolved: The paper does not provide a systematic analysis of the effect of population size on PDO's performance across different tasks and environments.
- What evidence would resolve it: Conducting experiments with varying population sizes (e.g., M=3, 5, 10, 20) on the dogfight and MuJoCo tasks, and comparing the performance metrics (max fitness, QD-score, coverage) to determine the optimal population size for each task.

### Open Question 2
- Question: How does the choice of kernel function in the differentiable similarity estimation (DSE) affect the performance of PDO?
- Basis in paper: [explicit] The paper mentions using the Jensen-Shannon divergence and the 2-Wasserstein distance as kernel functions in the DSE, but does not provide a detailed comparison of their performance.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of different kernel functions on PDO's performance and diversity optimization.
- What evidence would resolve it: Conducting experiments with different kernel functions (e.g., JSD, WDs, RBF) in the DSE and comparing their performance on the dogfight and MuJoCo tasks, measuring metrics such as max fitness, QD-score, and coverage.

### Open Question 3
- Question: How does the choice of archive implementation (MAP-Elites grid vs. fitness priority queue) affect the performance of PDO?
- Basis in paper: [explicit] The paper introduces two implementations of PDO archive: MAP-Elites grid and fitness priority queue without a Behavior Descriptor (BD).
- Why unresolved: The paper does not provide a detailed comparison of the performance of PDO with different archive implementations.
- What evidence would resolve it: Conducting experiments with both archive implementations (MAP-Elites grid and fitness priority queue) on the dogfight and MuJoCo tasks, and comparing their performance metrics (max fitness, QD-score, coverage) to determine which implementation is more effective for different tasks.

## Limitations

- The paper's theoretical guarantees rely on assumptions about kernel matrix properties that may not hold in all practical scenarios
- Empirical validation of the differentiable similarity estimation's accuracy across diverse state spaces is limited
- Ablation studies for key design choices (like β and diversity iteration counts) are not fully explored

## Confidence

- Mechanism 1 (Phase decoupling): High - well-supported by ablation and baseline comparisons
- Mechanism 2 (Differentiable similarity): Medium - theoretically justified but limited empirical validation
- Mechanism 3 (Surrogate regularization): Medium - theoretically sound but no edge-case testing

## Next Checks

1. Test PDO with varying β values (0.1, 0.5, 0.9) to empirically determine the optimal balance between numerical stability and diversity strength.

2. Implement an ablation study comparing PDO with and without the archive mechanism to quantify the contribution of each component to overall performance.

3. Evaluate PDO's performance on a wider range of environments (e.g., Atari, continuous control tasks) to assess generalizability beyond the dogfight and MuJoCo domains.