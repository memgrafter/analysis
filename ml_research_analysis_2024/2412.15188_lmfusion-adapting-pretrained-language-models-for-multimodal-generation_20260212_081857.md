---
ver: rpa2
title: 'LMFusion: Adapting Pretrained Language Models for Multimodal Generation'
arxiv_id: '2412.15188'
source_url: https://arxiv.org/abs/2412.15188
tags:
- image
- text
- generation
- lmfusion
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LMFusion, a framework that adapts pretrained
  language models (LLMs) for multimodal generation. The key idea is to preserve the
  strong language capabilities of LLMs like Llama-3 while introducing dedicated transformer
  modules for image processing, enabling the model to understand and generate both
  text and images in arbitrary sequences.
---

# LMFusion: Adapting Pretrained Language Models for Multimodal Generation

## Quick Facts
- **arXiv ID**: 2412.15188
- **Source URL**: https://arxiv.org/abs/2412.15188
- **Authors**: Weijia Shi; Xiaochuang Han; Chunting Zhou; Weixin Liang; Xi Victoria Lin; Luke Zettlemoyer; Lili Yu
- **Reference count**: 12
- **Key outcome**: LMFusion achieves 20% improvement in image understanding and 3.6% improvement in image generation while using 50% of the FLOPs compared to Transfusion, and maintains 11.6% better language performance than Transfusion.

## Executive Summary
This paper introduces LMFusion, a framework that adapts pretrained language models (LLMs) for multimodal generation by preserving strong language capabilities while adding visual understanding and generation. The key innovation is freezing text-specific modules during training while introducing dedicated transformer modules for image processing, enabling the model to understand and generate both text and images in arbitrary sequences. Compared to Transfusion which trains multimodal models from scratch, LMFusion achieves superior performance with significantly fewer computational resources while maintaining the language capabilities of the base LLM.

## Method Summary
LMFusion adapts pretrained LLMs for multimodal generation by freezing text-specific modules and training only image-specific modules. The architecture uses modality-specific feedforward layers, query-key-value projections, and normalization layers for text and image processing, while maintaining shared self-attention layers for cross-modal interactions. The model is initialized from pretrained Llama-3 weights, with text modules frozen (ηtext = 0) and image modules trained using AdamW optimizer with learning rate ηimage = 1×10^-4. Training uses cosine decay schedule with 4000-step warmup on 380M Shutterstock image-caption pairs.

## Key Results
- 20% improvement in image understanding (CIDEr score) compared to Transfusion
- 3.6% improvement in image generation (FID and CLIP scores) with 50% of the FLOPs
- 11.6% better language performance on HellaSwag, SIQA, and WinoGrande benchmarks compared to Transfusion
- Maintains strong language capabilities while developing visual understanding and generation abilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Freezing text-specific modules preserves language capabilities while allowing image modules to learn
- Mechanism: By setting ηtext = 0, pretrained language parameters remain static, preventing catastrophic forgetting while image-specific modules develop visual capabilities independently
- Core assumption: Text and image processing paths are sufficiently modular that freezing one doesn't prevent the other from learning effectively
- Evidence anchors: [abstract] "By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities." [section] "To preserve the model's performance on text-only benchmarks, we use ηtext = 0 (freezing text modules) for our main experiments"

### Mechanism 2
- Claim: Deep modality separation with both FFN and attention-specific modules outperforms shallow separation
- Mechanism: Modality-specific QKV projections, FFNs, and normalization layers process each modality independently while shared self-attention layers enable cross-modal interaction
- Core assumption: Different modalities benefit from specialized processing layers rather than shared ones
- Evidence anchors: [abstract] "modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently" [section] "We employ modality-specific QKV projections and feed-forward networks (FFNs) to process text and image data separately while still allowing for cross-modal interactions in the joint self-attention layer"

### Mechanism 3
- Claim: Parallel development of language and vision capabilities through initialization from pretrained Llama-3
- Mechanism: Initializing both text and image modules from pretrained Llama-3 weights provides strong language processing while image modules adapt these weights for visual processing