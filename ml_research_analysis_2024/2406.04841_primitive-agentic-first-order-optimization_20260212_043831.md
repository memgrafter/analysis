---
ver: rpa2
title: Primitive Agentic First-Order Optimization
arxiv_id: '2406.04841'
source_url: https://arxiv.org/abs/2406.04841
tags:
- optimization
- learning
- state
- update
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a proof-of-concept reinforcement learning
  (RL) approach for improving first-order optimization under computational resource
  budget constraints. The method formulates optimization as an agent-environment interaction,
  where an RL agent sequentially selects update rules and parameters from a set of
  gradient-based algorithms.
---

# Primitive Agentic First-Order Optimization

## Quick Facts
- arXiv ID: 2406.04841
- Source URL: https://arxiv.org/abs/2406.04841
- Authors: R. Sala
- Reference count: 40
- Primary result: RL-based Sequential Update Selection (SUS) method outperforms hyperoptimized Nesterov accelerated gradient on quadratic optimization under fixed iteration budgets.

## Executive Summary
This paper presents a proof-of-concept reinforcement learning approach for first-order optimization under computational resource constraints. The method frames optimization as an agent-environment interaction where an RL agent sequentially selects update rules and parameters from a set of gradient-based algorithms. Using low-dimensional state representations that track progress and resource use, the agent is trained via SARSA updates with epsilon-greedy exploration. Experiments on quadratic optimization problems demonstrate that SUS achieves up to 40% relative objective improvement over conventional hyperoptimized Nesterov accelerated gradient with fixed iteration budgets.

## Method Summary
The approach formulates optimization as an RL agent-environment interaction, where the agent selects from a set of first-order update rules at each iteration. The agent uses primitive state representations combining objective value classes and iteration fraction to track progress under resource constraints. Training uses SARSA updates with epsilon-greedy exploration, where rewards are based on immediate objective improvements. The method is tested on quadratic optimization problems, comparing against hyperoptimized Nesterov accelerated gradient across various problem dimensions and condition numbers.

## Key Results
- SUS outperformed hyperoptimized Nesterov accelerated gradient with up to 40% relative objective improvement on fixed iteration budgets
- Performance improvements were consistent across different problem dimensions (up to 500) and condition numbers
- Results suggest that elementary RL methods with succinct partial state representations can manage complexity in optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using primitive state representations with epsilon-greedy SARSA enables learning better update policies than static hyperparameter tuning.
- Mechanism: The agent dynamically selects from a small set of first-order updates at each iteration, using a low-dimensional state space that captures progress and resource use. SARSA updates Q-values based on observed rewards from objective improvements, refining the policy over episodes.
- Core assumption: The partial observability of the optimization process can be summarized in low-dimensional states that still allow the agent to distinguish useful updates from non-useful ones.
- Evidence anchors:
  - [abstract] "optimal policies for sequential update selection of algorithmic iteration steps are approximated in generally formulated low-dimensional partial state representations that consider aspects of progress and resource use."
  - [section] "primitive state representation concepts can be combined and extended, with state dimensions that consider properties of the gradients."
  - [corpus] No direct evidence found for SARSA + primitive states achieving better performance; claim is based on the paper's own results.
- Break condition: If the state space is too coarse to capture meaningful distinctions between iteration states, the policy will converge to suboptimal or random actions.

### Mechanism 2
- Claim: Training on a class of quadratic problems allows the agent to generalize to unseen instances of the same class.
- Mechanism: The RL agent is trained on a training set of quadratic instances drawn from a problem class parameterized by dimension and condition number. Because the structure of the problem class is consistent, the learned policy captures general heuristics for progress tracking and update selection.
- Core assumption: The training set is sufficiently diverse to cover the variability within the problem class, enabling transfer to unseen instances.
- Evidence anchors:
  - [section] "For the investigated case studies, deployment of the trained agents to unseen instances of the quadratic optimization problem classes outperformed conventional optimal algorithms with optimized hyperparameters."
  - [corpus] No external evidence; this is based on the paper's experimental setup and results.
- Break condition: If the test instances differ too much in structure from the training set, the learned policy may fail to generalize, resulting in performance no better than random selection.

### Mechanism 3
- Claim: The combination of low-dimensional states and small action sets reduces the complexity of the RL problem, making learning tractable.
- Mechanism: By limiting the action set to a handful of known first-order updates and discretizing the state space into a small number of bins, the Q-table remains small and learning can converge within reasonable training episodes.
- Core assumption: The problem structure allows meaningful policies to be expressed with a small number of discrete states and actions.
- Evidence anchors:
  - [abstract] "elementary RL methods combined with succinct partial state representations can be used as heuristics to manage complexity in RL-based optimization."
  - [section] "despite the relative simplicity of this heuristic approach, the results from the case studies showed that better performance than theoretically optimal hyperoptimized NAG could be achieved."
  - [corpus] No external validation; this is the paper's own claim about complexity management.
- Break condition: If the problem requires fine-grained state distinctions or a large action set for effective optimization, the coarse discretization will limit policy quality.

## Foundational Learning

- **Reinforcement Learning fundamentals** (states, actions, rewards, policies, Q-learning/SARSA)
  - Why needed here: The core algorithm is an RL agent that learns to select update rules based on state and reward signals.
  - Quick check question: What is the difference between Q-learning and SARSA, and why is SARSA used in this paper?

- **First-order optimization methods** (gradient descent, momentum, Nesterov accelerated gradient)
  - Why needed here: The agent selects from a set of first-order update rules; understanding these methods is essential to design the action set.
  - Quick check question: How does Nesterov accelerated gradient differ from standard momentum in terms of the update rule?

- **Partially observable Markov decision processes (POMDPs)**
  - Why needed here: The optimization process is modeled as a POMDP where the agent only observes partial information about the true state of convergence.
  - Quick check question: Why is the state space described as "partial" rather than fully observable in this optimization setting?

## Architecture Onboarding

- **Component map**: Environment (executes updates, computes state/reward) -> Agent (selects actions, updates Q-table) -> Training loop (episodes with decay schedule)
- **Critical path**: Initialize environment with random problem instance and starting point. For each iteration: Agent selects action → Environment applies update → Evaluates f and ∇f → Computes state and reward → Agent updates Q-values. Repeat until budget K is reached; record final objective value. Repeat for N episodes; decay epsilon and alpha exponentially.
- **Design tradeoffs**:
  - State resolution vs. learning complexity: Finer discretization improves policy quality but increases Q-table size and training time.
  - Action set size vs. expressiveness: Larger action sets allow more sophisticated policies but require more training data and may overfit.
  - Immediate vs. delayed rewards: Immediate rewards (objective improvement) are easier to learn but may miss long-term strategy benefits.
- **Failure signatures**:
  - Q-values plateau at low values: Likely due to insufficient exploration (epsilon too low) or poor reward signal.
  - Performance matches random: Likely state representation too coarse or action set too small to express useful policies.
  - High variance across runs: Likely insufficient training episodes or unstable learning rate schedule.
- **First 3 experiments**:
  1. Baseline: Hyperoptimized NAG on training set; record average final objective.
  2. Minimal RL: 2-action set (NAG vs. GD), 1D state (objective percentile), 100 episodes; compare to baseline.
  3. Full RL: 4-action set (GURU, GD variants, NAG), 2D state (objective + iteration fraction), 10k episodes; compare to baseline and experiment 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SUS with primitive state representations compare to conventional methods when the problem dimensionality increases significantly beyond the tested range?
- Basis in paper: [explicit] The paper mentions that results mildly depend on problem dimension with a mildly decreasing trend, but further investigations on a larger variety of problems are planned.
- Why unresolved: The current study only tested up to 500 dimensions and the trend was mild, leaving uncertainty about scalability.
- What evidence would resolve it: Experimental results comparing SUS performance to conventional methods across a much broader range of problem dimensions, especially in the high-dimensional regime.

### Open Question 2
- Question: Can more sophisticated RL methods (e.g., deep RL) with higher-dimensional state representations significantly outperform the elementary RL approach presented in this paper?
- Basis in paper: [explicit] The paper explicitly suggests that future work can explore alternative update operators, state representations, and hierarchical agentic architectures.
- Why unresolved: The paper deliberately used simple RL methods and low-dimensional states to manage complexity, but did not test whether more complex approaches could yield better results.
- What evidence would resolve it: Direct comparison studies using advanced RL methods (like deep RL) with complex state representations on the same problem classes.

### Open Question 3
- Question: How robust is the SUS approach when applied to non-convex optimization problems, as opposed to the convex quadratic problems studied in this paper?
- Basis in paper: [inferred] The paper focuses exclusively on convex quadratic optimization problems, with no discussion of non-convex cases, suggesting this as a potential direction for future research.
- Why unresolved: The current study's positive results are limited to convex problems, and it's unclear if the same benefits would transfer to non-convex landscapes.
- What evidence would resolve it: Experimental validation of SUS on a diverse set of non-convex optimization problems, comparing performance against conventional non-convex optimizers.

## Limitations
- Results are based solely on synthetic quadratic problems with no validation on real-world or non-convex optimization tasks
- The claim of generalization to unseen instances relies entirely on experiments within the same problem class
- No ablation study is provided to quantify the impact of state representation granularity or action set size on performance

## Confidence
- **High**: The agent-environment formulation, SARSA learning algorithm, and use of epsilon-greedy exploration are standard RL techniques with well-understood behavior.
- **Medium**: The claim that primitive state representations enable effective learning is supported by the paper's results, but lacks external validation or theoretical justification.
- **Low**: The generalization claim to unseen instances is based solely on the paper's own experiments and has not been independently verified.

## Next Checks
1. Conduct ablation studies to assess the impact of state representation granularity and action set size on policy performance.
2. Test the trained agent on non-quadratic, non-convex optimization problems to evaluate robustness beyond the training distribution.
3. Compare the learning efficiency and final performance of SUS with other RL-based optimization methods (e.g., policy gradient, actor-critic) under the same computational budget.