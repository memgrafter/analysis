---
ver: rpa2
title: Hierarchical Classification Auxiliary Network for Time Series Forecasting
arxiv_id: '2405.18975'
source_url: https://arxiv.org/abs/2405.18975
tags:
- hcan
- forecasting
- series
- time
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses over-smoothing in time series forecasting by
  reformulating the task as a hierarchical classification problem. The authors introduce
  HCAN (Hierarchical Classification Auxiliary Network), a model-agnostic component
  that integrates multi-granularity high-entropy features through a hierarchy-aware
  attention module.
---

# Hierarchical Classification Auxiliary Network for Time Series Forecasting

## Quick Facts
- arXiv ID: 2405.18975
- Source URL: https://arxiv.org/abs/2405.18975
- Reference count: 40
- Primary result: Hierarchical Classification Auxiliary Network (HCAN) improves time series forecasting by reformulating it as a hierarchical classification problem, achieving 9.1%-35.5% MSE improvements across multiple datasets.

## Executive Summary
This paper addresses the over-smoothing problem in time series forecasting by reformulating the task as hierarchical classification. The authors introduce HCAN (Hierarchical Classification Auxiliary Network), a model-agnostic component that integrates multi-granularity high-entropy features through a hierarchy-aware attention module. The method includes uncertainty-aware classifiers using evidence theory and a hierarchical consistency loss to mitigate boundary effects. Experiments integrating HCAN with state-of-the-art forecasting models show substantial improvements across multiple real-world datasets.

## Method Summary
HCAN reformulates time series forecasting as hierarchical classification by discretizing continuous values into class intervals and training with cross-entropy loss instead of MSE. The architecture includes a hierarchy-aware attention module that fuses multi-granularity features, uncertainty-aware classifiers using evidence theory to mitigate over-confidence, and a hierarchical consistency loss that maintains prediction consistency across levels. The method can be integrated with any backbone forecasting model and trained end-to-end using ADAM optimizer with grid-searched hyperparameters.

## Key Results
- HCAN integration with baseline models (Informer, Autoformer, FEDformer, PatchTST, DLinear, NBEATS) shows consistent performance improvements
- Average MSE improvements of 9.1%-35.5% across all tested datasets (ETT, Weather, Exchange-Rate, ILI, Electricity, Traffic, Solar Wind)
- Optimal performance achieved with 3 hierarchical levels (H=3) on DLinear backbone
- HCAN maintains model-agnostic properties while providing substantial accuracy gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reformulating forecasting as hierarchical classification reduces over-smoothing by encouraging high-entropy feature representations.
- Mechanism: Converting continuous targets into discrete class intervals and training with cross-entropy loss instead of MSE promotes more diverse and complex feature representations, counteracting the feature compression that leads to over-smoothing.
- Core assumption: Cross-entropy loss inherently produces higher-entropy features than MSE loss when applied to discretized continuous targets.
- Evidence anchors:
  - [abstract] "However, training these models with the Mean Square Error (MSE) loss often results in over-smooth predictions, making it challenging to handle the complexity and learn high-entropy features from time series data with high variability and unpredictability."
  - [section] "In this work, we reformulate time series forecasting as a classification problem. Specifically, we tokenize time series values into different categories based on their magnitude and leverage the cross-entropy loss to train a classifier on these tokenized values."
  - [corpus] Weak evidence - no direct comparison of MSE vs cross-entropy feature entropy in related work.
- Break condition: If discretization leads to loss of important continuous information that is critical for accurate forecasting, or if cross-entropy training fails to maintain ordinal relationships between classes.

### Mechanism 2
- Claim: The uncertainty-aware classifier with evidence theory mitigates over-confidence in predictions and improves reliability.
- Mechanism: Replacing softmax with evidence-based uncertainty estimation using Dirichlet distributions produces belief masses and uncertainty weights that down-weight confident but potentially incorrect predictions, leading to more calibrated uncertainty estimates.
- Core assumption: Evidence-based uncertainty estimation via Dirichlet distributions provides more reliable uncertainty quantification than standard softmax approaches.
- Evidence anchors:
  - [abstract] "This classifier mitigates the over-confidence in softmax loss via evidence theory."
  - [section] "To address this issue and improve the robustness of classification across various hierarchical levels, we implement an evidence-based uncertainty estimation technique, which is meant to enhance the precision of uncertainty assessments."
  - [corpus] Weak evidence - no direct comparison of evidence-based vs softmax uncertainty calibration in related work.
- Break condition: If the evidence theory assumptions about data distribution don't hold for the specific time series data, or if the computational overhead of uncertainty estimation outweighs its benefits.

### Mechanism 3
- Claim: Hierarchical consistency loss alleviates boundary effects by maintaining prediction consistency across hierarchical levels.
- Mechanism: Enforcing consistency between fine-grained and coarse-grained classifier predictions through KL divergence penalizes discrepancies near class boundaries, effectively smoothing out boundary discontinuities.
- Core assumption: Predictions near class boundaries in fine-grained classification should be consistent with the corresponding coarse-grained class, and enforcing this consistency reduces boundary effects.
- Evidence anchors:
  - [abstract] "We also implement a Hierarchical Consistency Loss to maintain prediction consistency across hierarchy levels."
  - [section] "Due to the continuous nature of time series data, directly classifying timestep values may result in misclassified values near the inter-class boundaries, known as boundary effects. Therefore, we propose the Hierarchical Consistency Loss (HCL), which aims to keep the values near the boundary of a fine-grained class within the correct coarse-grained category."
  - [corpus] Weak evidence - no direct demonstration of boundary effect mitigation through hierarchical consistency in related work.
- Break condition: If the hierarchical structure doesn't align well with the natural structure of the time series data, or if the consistency enforcement is too strict and prevents the model from capturing genuine boundary phenomena.

## Foundational Learning

- Concept: Time series discretization and quantization
  - Why needed here: The method requires converting continuous time series values into discrete class intervals for hierarchical classification.
  - Quick check question: How would you implement a discretization scheme that preserves important temporal patterns while creating meaningful class boundaries?

- Concept: Evidence theory and Dirichlet distribution for uncertainty quantification
  - Why needed here: The uncertainty-aware classifier relies on evidence theory to produce calibrated uncertainty estimates rather than standard softmax probabilities.
  - Quick check question: What are the key differences between evidence-based uncertainty estimation and traditional softmax-based approaches, and when would each be preferable?

- Concept: Hierarchical classification and multi-granularity feature learning
  - Why needed here: The approach explicitly leverages multiple classification levels with different granularities to capture patterns at various scales.
  - Quick check question: How does hierarchical classification differ from standard multi-class classification, and what are the benefits and challenges of incorporating multiple hierarchical levels?

## Architecture Onboarding

- Component map: Backbone model → Hierarchy-Aware Attention (HAA) module → Uncertainty-Aware Classifiers (UAC) at fine-grained and coarse-grained levels → Hierarchical Consistency Loss (HCL) → Prediction output
- Critical path: Feature extraction from backbone → Multi-granularity feature generation (θ, ϕ, η) → Classification training with UAC → Consistency enforcement with HCL → Feature fusion with HAA → Final prediction
- Design tradeoffs: The hierarchical classification approach trades continuous prediction precision for improved feature diversity and reduced over-smoothing, while the uncertainty-aware components add computational overhead but improve reliability.
- Failure signatures: Over-smoothing may persist if discretization is too coarse; boundary effects may remain if hierarchical consistency is insufficient; model may become unstable if uncertainty estimation doesn't converge properly.
- First 3 experiments:
  1. Replace MSE loss with cross-entropy on discretized targets and compare feature entropy and prediction smoothness
  2. Implement uncertainty-aware classifier with evidence theory and compare calibration metrics against standard softmax
  3. Add hierarchical consistency loss and measure reduction in boundary effect artifacts compared to single-level classification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of HCAN scale when extending beyond three hierarchical levels (e.g., K1=1, K2=2, K3=4, K4=8, K5=16)?
- Basis in paper: [explicit] The authors conducted experiments with 1-5 hierarchical levels on DLinear, finding H=3 (HCAN's configuration) achieved optimal performance.
- Why unresolved: The paper only tested up to 5 levels on one backbone model (DLinear) and one dataset pair (ETTh1, ETTh2). It's unclear if the diminishing returns observed would hold across different backbone architectures and datasets.
- What evidence would resolve it: Comprehensive experiments testing 3-7 hierarchical levels across all backbone models and all datasets used in the main experiments.

### Open Question 2
- Question: What is the impact of HCAN on model interpretability, particularly regarding the hierarchy-aware attention mechanism's ability to reveal temporal patterns?
- Basis in paper: [explicit] The authors mention that the HAA mechanism enhances prediction accuracy by fusing features at different granular levels, but do not provide interpretability analysis of what patterns are captured at each hierarchy level.
- Why unresolved: The paper focuses on quantitative performance improvements but does not analyze what specific temporal patterns or relationships the hierarchical attention mechanism learns at each level.
- What evidence would resolve it: Interpretability studies visualizing attention weights across hierarchies, analysis of what temporal patterns are captured at each level, and correlation between attention patterns and known temporal structures in the data.

### Open Question 3
- Question: How does HCAN's performance compare to traditional ensemble methods that combine multiple forecasting models with different granularities?
- Basis in paper: [inferred] The paper introduces HCAN as a model-agnostic component that can be integrated with any forecasting model, but does not compare its performance against ensemble approaches that might achieve similar multi-granularity benefits.
- Why unresolved: The paper demonstrates HCAN's effectiveness when added to single models but doesn't explore whether traditional ensemble methods could achieve comparable or better results with less architectural complexity.
- What evidence would resolve it: Direct comparison between HCAN-enhanced models and ensemble methods (e.g., weighted combinations of models with different look-back windows or prediction granularities) on the same benchmarks.

## Limitations
- Weak empirical evidence supporting the core mechanism of cross-entropy loss producing higher-entropy features compared to MSE
- Insufficient validation of evidence theory uncertainty estimation against standard softmax approaches
- Limited analysis of when hierarchical classification is most beneficial versus simpler approaches

## Confidence
- **High confidence**: Experimental results showing MSE/MAE improvements across multiple datasets
- **Medium confidence**: Mechanism explanations linking hierarchical classification to reduced over-smoothing
- **Low confidence**: Claims about uncertainty-aware classifier improvements without direct validation

## Next Checks
1. **Cross-entropy vs MSE feature entropy ablation**: Conduct controlled experiments comparing the entropy of learned features when training with cross-entropy loss on discretized targets versus MSE loss on continuous targets to validate the core mechanism.

2. **Evidence theory uncertainty calibration**: Implement ablation studies comparing the evidence-based uncertainty estimator against standard softmax approaches using calibration metrics like Expected Calibration Error (ECE) and reliability diagrams.

3. **Boundary effect quantitative analysis**: Design experiments that specifically measure boundary effect artifacts before and after applying hierarchical consistency loss, analyzing prediction errors near class boundaries and quantifying the reduction in boundary discontinuities.