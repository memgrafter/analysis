---
ver: rpa2
title: AI-Driven Agents with Prompts Designed for High Agreeableness Increase the
  Likelihood of Being Mistaken for a Human in the Turing Test
arxiv_id: '2411.13749'
source_url: https://arxiv.org/abs/2411.13749
tags:
- human
- agreeableness
- agents
- personality
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study demonstrates that AI agents designed with high agreeableness\
  \ traits are significantly more likely to be perceived as human in the Turing Test.\
  \ Three GPT agents with varying agreeableness levels (disagreeable, neutral, highly\
  \ agreeable) were tested, with the highly agreeable agent achieving a 63.7% confusion\
  \ rate\u2014surpassing Turing's 30% threshold and previous benchmarks."
---

# AI-Driven Agents with Prompts Designed for High Agreeableness Increase the Likelihood of Being Mistaken for a Human in the Turing Test

## Quick Facts
- arXiv ID: 2411.13749
- Source URL: https://arxiv.org/abs/2411.13749
- Reference count: 0
- Highly agreeable AI agents achieved 63.7% confusion rate in Turing Test, surpassing 30% threshold

## Executive Summary
This study demonstrates that AI agents designed with high agreeableness traits are significantly more likely to be perceived as human in the Turing Test. Three GPT agents with varying agreeableness levels (disagreeable, neutral, highly agreeable) were tested, with the highly agreeable agent achieving a 63.7% confusion rate—surpassing Turing's 30% threshold and previous benchmarks. All three agents exceeded 50% confusion rates, with the highly agreeable agent also receiving the most votes for human-like characteristics. The findings suggest that incorporating agreeableness traits through personality engineering enhances AI humanization, highlighting the importance of psychological modeling in improving Human-AI collaboration.

## Method Summary
The study tested three GPT-4o agents with different agreeableness levels (disagreeable, neutral, highly agreeable) in Turing Test conversations with 102 participants. Participants engaged in five-minute conversations via Discord with each agent, then identified whether they believed the agent was human or AI and rated their confidence. The agents were configured using personality prompts based on Big Five Inventory items, with agreeableness levels manipulated through prompt engineering. Conversations were manually transcribed to allow prompt re-issuance when agents deviated from instructions. Statistical analysis examined confusion rates and human-like perception across the three agreeableness conditions.

## Key Results
- Highly agreeable agent achieved 63.7% confusion rate (participants identified it as human)
- All three agents exceeded 50% confusion rates, with no significant differences between conditions
- Highly agreeable agent received the most votes for human-like characteristics
- Results surpassed Turing's 30% threshold and previous benchmarks from June 2024

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agreeableness traits in AI agents make them appear more human-like in Turing Tests.
- Mechanism: Agreeableness traits such as empathy, cooperation, and warmth create conversational behaviors that align with human social expectations, leading participants to anthropomorphize the AI.
- Core assumption: Human perception of AI as human is influenced by the degree to which the AI exhibits personality traits similar to those valued in human interaction.
- Evidence anchors:
  - [abstract]: "AI agents designed with high agreeableness traits are significantly more likely to be perceived as human in the Turing Test."
  - [section]: "The trait of agreeableness enhances empathy, rapport, and a sense of security... These psychological attributes, when implemented in AI systems, can significantly improve human-AI collaboration."
  - [corpus]: "Weak evidence: only one related paper on LLMs with personalities in multi-issue negotiation games, no direct study of agreeableness in Turing Tests."
- Break condition: If agreeableness traits are not aligned with participants' cultural or personal values, the effect may reverse or be neutralized.

### Mechanism 2
- Claim: High agreeableness reduces the "uncanny effect," making AI appear more natural.
- Mechanism: By reducing behaviors perceived as cold, rude, or uncooperative, agreeableness decreases the psychological distance between human and AI, enhancing perceived authenticity.
- Core assumption: Uncanny effect is driven by mismatches between expected human warmth and actual AI behavior.
- Evidence anchors:
  - [section]: "it appears that the 'uncanny effect' might be reduced if, during human-machine interaction, the human perceives the robot's behavior as highly agreeable, emotionally stable, and conscientious."
  - [abstract]: "the highly agreeable agent achieving a 63.7% confusion rate—surpassing Turing's 30% threshold."
  - [corpus]: "Weak evidence: no corpus studies directly address uncanny effect reduction via agreeableness."
- Break condition: If the agreeableness prompts are too generic or over-optimized, they may feel scripted and trigger suspicion.

### Mechanism 3
- Claim: Participants project their own agreeable traits onto the AI, increasing perceived humanness.
- Mechanism: The "looking-glass self" effect causes people to see the AI as human because it mirrors traits they value in themselves, such as trustworthiness and empathy.
- Core assumption: Human identity validation influences AI-human attribution in conversational contexts.
- Evidence anchors:
  - [section]: "The witness's level of agreeableness may have led the interrogator to perceive themselves as agreeable in the eyes of the witness... they could have humanized the witness."
  - [abstract]: "The highly agreeable agent... also receiving the most votes for human-like characteristics."
  - [corpus]: "Weak evidence: no corpus studies directly link looking-glass self to AI perception."
- Break condition: If the participant does not value agreeableness, the effect will be absent or reversed.

## Foundational Learning

- Concept: Personality engineering for AI
  - Why needed here: The study relies on engineering personality traits (agreeableness) into AI prompts to influence human perception.
  - Quick check question: How does personality engineering differ from traditional prompt engineering?

- Concept: Anthropomorphism in AI
  - Why needed here: Understanding why humans attribute human traits to non-human agents is central to interpreting Turing Test results.
  - Quick check question: What psychological mechanisms drive anthropomorphism of AI systems?

- Concept: Turing Test methodology
  - Why needed here: The experimental design and interpretation depend on understanding how confusion rates are measured and what thresholds indicate human-like performance.
  - Quick check question: Why is 30% confusion considered the threshold for passing the Turing Test?

## Architecture Onboarding

- Component map:
  - ChatGPT-4o + custom prompts -> Discord chat interface -> Manual transcription -> Statistical analysis

- Critical path:
  1. Prompt configuration (agreeableness level)
  2. Participant interaction (5 min per agent)
  3. Data capture (human/machine identification + confidence)
  4. Aggregation and analysis (Chi-square tests, frequency counts)

- Design tradeoffs:
  - Real-time manual transcription vs. API automation: Manual allows prompt re-issuance but is slower and error-prone.
  - Single vs. multi-round conversations: Single exchanges reduce drift but may miss deeper behavioral cues.

- Failure signatures:
  - GPTs fail to maintain trait consistency over conversation duration.
  - Participants recognize patterns (e.g., scripted empathy) and downgrade human-likeness scores.
  - Statistical tests show no significant differences between agreeableness levels.

- First 3 experiments:
  1. Vary agreeableness intensity across three agents (disagreeable, neutral, highly agreeable) and measure confusion rates.
  2. Introduce a fourth agent with low agreeableness to test if negative correlation exists.
  3. Test alternate personality traits (e.g., openness) to compare effects on human-likeness perception.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific mediating variables influence the attribution of human characteristics to AI agents with varying levels of agreeableness?
- Basis in paper: [explicit] The authors state "questions remain, such as the role of potential mediating variables in the assignment of human traits"
- Why unresolved: The study did not assess potential mediating variables that could influence how participants attribute human-like characteristics to AI agents
- What evidence would resolve it: Future studies should measure and control for variables such as prior experience with AI, individual differences in anthropomorphism tendencies, and social connection needs to determine their influence on human-likeness attribution

### Open Question 2
- Question: What is the relative importance of human-likeness (expression) versus cognitive anthropomorphism (personality) in determining whether an AI agent is perceived as human?
- Basis in paper: [explicit] The authors note "questions remain... the relative importance of human-likeness (expression) versus cognitive anthropomorphism (personality)"
- Why unresolved: The study collected data on both factors but did not analyze their relative contributions to the overall perception of humanness
- What evidence would resolve it: Experimental designs that systematically manipulate expression and personality traits independently could determine which factor has greater impact on human-likeness perception

### Open Question 3
- Question: How does the agreeableness trait in AI agents affect human-AI collaboration outcomes in real-world applications beyond the Turing Test context?
- Basis in paper: [explicit] The authors suggest "implementing high levels of agreeableness traits in AI agents may help humanize them, thereby positively influencing interaction and collaboration"
- Why unresolved: The study only measured perception of humanness, not actual collaboration performance or outcomes
- What evidence would resolve it: Longitudinal studies examining team performance, user satisfaction, and task completion rates when collaborating with AI agents varying in agreeableness levels across different domains (healthcare, education, customer service)

## Limitations

- Results based on a homogeneous sample of Spanish-speaking university students aged 18-24, limiting generalizability
- Manual transcription process introduces observer effects and potential transcription errors
- Self-reported BFI measurements for participant agreeableness may introduce measurement bias

## Confidence

- **High Confidence**: The statistical significance of the highly agreeable agent achieving a 63.7% confusion rate
- **Medium Confidence**: The claim that agreeableness is the primary driver of human-like perception
- **Low Confidence**: The broader implication that agreeableness should be universally incorporated into AI systems for humanization

## Next Checks

1. Replicate the experiment with a more diverse participant pool across different age groups, cultures, and languages to test the robustness of the agreeableness effect.
2. Conduct a controlled study isolating agreeableness from other personality traits to determine whether the observed effect is specific to agreeableness or generalizable to other traits.
3. Implement automated conversation logging and response analysis to eliminate transcription errors and observer bias while testing the stability of trait-consistent responses over longer conversation durations.