---
ver: rpa2
title: Towards Temporal Change Explanations from Bi-Temporal Satellite Images
arxiv_id: '2407.09548'
source_url: https://arxiv.org/abs/2407.09548
tags:
- prompting
- change
- changes
- image
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the ability of Large-scale Vision-Language
  Models (LVLMs) to explain temporal changes between satellite images, addressing
  the need for comprehensive and consistent explanations in urban planning and environmental
  monitoring. To handle the input of paired satellite images, three prompting methods
  are proposed: All-at-Once, Step-by-Step, and Hybrid.'
---

# Towards Temporal Change Explanations from Bi-Temporal Satellite Images

## Quick Facts
- arXiv ID: 2407.09548
- Source URL: https://arxiv.org/abs/2407.09548
- Authors: Ryo Tsujimoto; Hiroki Ouchi; Hidetaka Kamigaito; Taro Watanabe
- Reference count: 7
- One-line primary result: Step-by-Step prompting with LLaVA-1.5 outperforms All-at-Once prompting in generating temporal change explanations from satellite images.

## Executive Summary
This study investigates how Large-scale Vision-Language Models (LVLMs) can explain temporal changes between satellite images, a critical capability for urban planning and environmental monitoring. The research addresses the challenge that most LVLMs only accept single images as input by proposing three prompting methods—All-at-Once, Step-by-Step, and Hybrid—to handle paired bi-temporal satellite images. Human evaluation using metrics of Coverage, Truthfulness, and Informativeness reveals that Step-by-Step prompting with LLaVA-1.5 outperforms All-at-Once prompting across all metrics, while GPT-4V with All-at-Once prompting achieves the best results in Truthfulness and Informativeness. The study also identifies a significant over-explanation problem where LVLMs attempt to explain changes even when none exist.

## Method Summary
The study proposes three prompting methods to enable LVLMs to process bi-temporal satellite images and generate temporal change explanations. The All-at-Once method concatenates both images and sends them to an LVLM (LLaVA-1.5 or GPT-4V) for direct comparison and explanation. The Step-by-Step method first generates separate captions for each image using spatial concepts (places, entities, topological relations), then combines these captions and sends them to an LLM (GPT-3.5-turbo or GPT-4-turbo) to synthesize the change explanation. The Hybrid method combines elements of both, using separate captioning followed by concatenated image processing. The methods are evaluated on 100 bi-temporal image pairs from the Levir-CC dataset using human annotators who rate the generated explanations on Coverage, Truthfulness, and Informativeness metrics.

## Key Results
- Step-by-Step prompting with LLaVA-1.5 outperformed All-at-Once prompting across all evaluation metrics
- GPT-4V with All-at-Once prompting achieved the best results in Truthfulness and Informativeness
- LVLMs often attempt to explain changes even when none exist, highlighting the over-explanation problem
- The study demonstrates the effectiveness of chain-of-thought reasoning for visual tasks in generating temporal change explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Step-by-Step prompting with LLaVA-1.5 outperforms All-at-Once prompting because it allows the model to first capture detailed spatial information before synthesizing the change explanation.
- Mechanism: By generating separate captions for each image using spatial concepts (places, entities, topological relations), the model can better identify and articulate the changes between images rather than trying to process both simultaneously.
- Core assumption: The model's ability to process detailed spatial information in isolation is superior to processing both images at once.
- Evidence anchors:
  - [abstract]: "we found the effectiveness of our step-by-step reasoning based prompting"
  - [section]: "Step-by-Step prompting outperformed All-at-Once prompting across all evaluation metrics"
  - [corpus]: Weak - no direct corpus evidence found
- Break condition: If the model fails to properly incorporate the separate captions into a coherent change explanation, or if spatial concepts are not adequately captured in individual captions.

### Mechanism 2
- Claim: All-at-Once prompting with GPT-4V achieves best Truthfulness and Informativeness because GPT-4V has superior visual processing capabilities that can handle simultaneous image comparison better than LLaVA-1.5.
- Mechanism: GPT-4V's advanced vision-language integration allows it to directly compare two images and identify changes without needing intermediate caption generation, leading to more accurate and detailed explanations.
- Core assumption: GPT-4V has superior visual processing capabilities compared to LLaVA-1.5.
- Evidence anchors:
  - [abstract]: "All-at-Once prompting with GPT-4V achieves the best results in Truthfulness and Informativeness"
  - [section]: "In terms of Truthfulness and Informativeness, GPT-4V with All-at-Once Prompting was the best"
  - [corpus]: Weak - no direct corpus evidence found
- Break condition: If the concatenated image exceeds token limits or if GPT-4V's vision processing fails to identify subtle changes.

### Mechanism 3
- Claim: The over-explanation problem occurs because LVLMs attempt to generate changes even when none exist, driven by their training bias toward describing visible differences.
- Mechanism: LVLMs are trained on datasets where changes are expected, leading them to fabricate changes when presented with similar-but-unchanged images, particularly in Step-by-Step prompting where separate captions may highlight minor differences.
- Core assumption: LVLMs have a bias toward describing changes due to their training data composition.
- Evidence anchors:
  - [abstract]: "LVLMs often attempt to explain changes even when none exist, highlighting the over-explanation problem"
  - [section]: "Many low-scored outputs were from bi-temporal SIs with little to no change. This suggests that the models attempted to explain changes even when there were none"
  - [corpus]: Weak - no direct corpus evidence found
- Break condition: If the model is explicitly trained or prompted to recognize and handle unchanged cases, or if additional verification steps are added.

## Foundational Learning

- Concept: Spatial concept integration in image captioning
  - Why needed here: The study uses spatial concepts (places, entities, topological relations) to constrain captions for better change detection
  - Quick check question: What are the seven spatial concept categories used in the Step-by-Step prompting method?

- Concept: Vision-language model input limitations
  - Why needed here: Most LVLMs only accept single images as input, requiring prompting methods to handle bi-temporal images
  - Quick check question: What is the main technical challenge addressed by the three prompting methods proposed in this study?

- Concept: Chain-of-thought reasoning in visual tasks
  - Why needed here: The success of generating final temporal change explanations using inferred information suggests chain-of-thought is effective for visual reasoning
  - Quick check question: How does the Step-by-Step prompting method relate to the concept of chain-of-thought prompting?

## Architecture Onboarding

- Component map: Bi-temporal satellite images -> LVLM component (LLaVA-1.5 or GPT-4V) -> LLM component (GPT-3.5-turbo or GPT-4-turbo) -> Change explanation text

- Critical path:
  - All-at-Once: Image concatenation → LVLM processing → Change explanation
  - Step-by-Step: Separate image captioning → Caption combination → LLM change explanation
  - Hybrid: Separate captioning → Concatenated image + captions → LVLM/LM change explanation

- Design tradeoffs:
  - All-at-Once offers simplicity but may miss details; Step-by-Step captures details but may lose context; Hybrid attempts to balance both
  - Using LLaVA-1.5 throughout maintains consistency but may limit capability; GPT-4V offers superior performance but at higher cost
  - Longer explanations increase coverage but may introduce errors affecting Truthfulness and Informativeness

- Failure signatures:
  - Over-explanation: Generating changes when none exist, particularly for unchanged images
  - Coverage-Truthfulness trade-off: High coverage with low Truthfulness indicates fabricated information
  - Token limit issues: Concatenated images may exceed model input limits
  - Spatial concept omission: Missing key spatial details in captions

- First 3 experiments:
  1. Implement All-at-Once prompting with LLaVA-1.5 on a small subset of images to verify basic functionality
  2. Implement Step-by-Step prompting with LLaVA-1.5 for captioning and GPT-3.5-turbo for change explanation
  3. Test Hybrid prompting using LLaVA-1.5 for both captioning and final explanation on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LVLMs handle bi-temporal satellite images with minimal or no changes, and what strategies can be implemented to mitigate over-explanation in such cases?
- Basis in paper: [explicit] The analysis reveals that LVLMs often attempt to explain changes even when none exist, highlighting the over-explanation problem and the need for improved handling of unchanged cases.
- Why unresolved: The paper identifies the over-explanation problem but does not provide specific strategies or solutions to address it. This remains a significant challenge for improving the accuracy of change explanations.
- What evidence would resolve it: Developing and testing new prompting strategies or model architectures that can accurately detect and report minimal or no changes, and comparing their performance against current methods using metrics like Truthfulness and Informativeness.

### Open Question 2
- Question: How do different LVLM architectures (e.g., GPT-4V vs. LLaVA-1.5) compare in terms of their ability to generate comprehensive and accurate change explanations, and what are the underlying factors contributing to their performance differences?
- Basis in paper: [explicit] The paper compares the performance of GPT-4V and LLaVA-1.5 in generating change explanations, finding that GPT-4V with All-at-Once Prompting achieves the best results in Truthfulness and Informativeness.
- Why unresolved: While the paper highlights performance differences between models, it does not delve into the specific architectural or training factors that contribute to these differences, leaving room for further investigation.
- What evidence would resolve it: Conducting a detailed analysis of the internal mechanisms and training processes of different LVLMs, and correlating these factors with their performance in generating change explanations.

### Open Question 3
- Question: What preprocessing techniques can be applied to improve the accuracy of noun coverage metrics in change explanations, and how do these techniques impact the overall quality of the explanations?
- Basis in paper: [explicit] The paper suggests that preprocessing tailored to the task, such as setting stopwords and focusing on nouns directly related to observed changes in satellite images, should be applied before calculating coverage to accurately reflect the relevance of the generated explanations to the ground truth.
- Why unresolved: The paper identifies the need for preprocessing but does not explore or test specific techniques, leaving this as an open area for research.
- What evidence would resolve it: Implementing and evaluating various preprocessing techniques on the noun coverage metric and assessing their impact on the overall quality of change explanations using human evaluation metrics like Truthfulness and Informativeness.

## Limitations

- The study only tested on satellite images from the Levir-CC dataset, limiting generalizability to other domains or image types
- Human evaluation metrics introduce subjectivity that may affect reproducibility and comparison across studies
- The prompt templates and constraints are not fully specified, making exact reproduction challenging
- The analysis of over-explanation is qualitative rather than quantitative, lacking systematic measurement

## Confidence

- High confidence: The comparative performance between prompting methods (Step-by-Step outperforming All-at-Once) is well-supported by the human evaluation data
- Medium confidence: The over-explanation problem is identified, but the underlying mechanisms and potential solutions require further investigation
- Low confidence: Claims about GPT-4V's superior capabilities are based on a single dataset and may not generalize to other domains or image types

## Next Checks

1. Test the prompting methods on a different satellite imagery dataset (e.g., DFC2020 or S2Looking) to verify generalizability
2. Conduct automated evaluation using ground-truth change masks to quantify the over-explanation problem
3. Implement a verification step in the prompting methods to explicitly check for unchanged cases before generating explanations