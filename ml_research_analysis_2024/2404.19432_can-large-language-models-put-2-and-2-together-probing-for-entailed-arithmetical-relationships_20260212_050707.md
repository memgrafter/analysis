---
ver: rpa2
title: Can Large Language Models put 2 and 2 together? Probing for Entailed Arithmetical
  Relationships
arxiv_id: '2404.19432'
source_url: https://arxiv.org/abs/2404.19432
tags:
- reasoning
- llms
- knowledge
- data
- than
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether large language models (LLMs) can
  perform arithmetical reasoning by comparing implicit numerical knowledge, termed
  "Entailed Arithmetical Relationships" (EAR) probing. The authors create a dataset
  of 6003 quad-tuples from 280 subject-element pairs, representing numerical facts
  and their relationships.
---

# Can Large Language Models put 2 and 2 together? Probing for Entailed Arithmetical Relationships

## Quick Facts
- arXiv ID: 2404.19432
- Source URL: https://arxiv.org/abs/2404.19432
- Reference count: 26
- Primary result: LLMs struggle with relational reasoning despite strong factual knowledge, revealing limitations in genuine logical inference

## Executive Summary
This study investigates whether large language models can perform arithmetical reasoning by probing their ability to infer relationships between numerical facts, termed "Entailed Arithmetical Relationships" (EAR). Using a dataset of 6003 quad-tuples derived from 280 subject-element pairs, the authors test GPT-3.5-turbo and GPT-4 on factual recall, relational comparisons, and EAR tasks. Results show that while both models excel at factual knowledge, they struggle significantly with relational reasoning, especially when combining multiple numerical facts. The findings suggest that LLMs rely on statistical inference rather than genuine logical reasoning, with performance improvements stemming from richer data distributions rather than enhanced reasoning capabilities.

## Method Summary
The authors create a dataset of 6003 quad-tuples from 280 subject-element pairs, representing numerical facts and their relationships. They prompt GPT-3.5-turbo and GPT-4 with various formulations to extract numerical facts and compare relationships. The models are evaluated on factual knowledge (single fact recall), relational reasoning (comparing two facts), and EAR (inferring relationships between implicit numerical facts). Performance is measured using exact match, partial match, and entailment metrics across different prompt types and settings.

## Key Results
- GPT-4 outperforms GPT-3.5 on factual knowledge but both models struggle with relational reasoning tasks
- Performance drops significantly when combining two numerical facts, indicating limited genuine reasoning capabilities
- Models exhibit biases such as "null value blindness" where they struggle with zero values and subject/element biases in hallucinations
- LLMs rely on statistical inference rather than logical deduction, with performance improvements attributed to richer data distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs perform poorly on relational reasoning because their training data rarely contains explicit negative statements about numerical attributes (e.g., "a bird has zero wheels").
- Mechanism: During pretraining, the absence of zero-value examples means the model lacks learned patterns for inferring null relationships. This leads to "null value blindness," where models hallucinate non-zero numbers or default to biased values (e.g., 1, 2, or 3 for wheels).
- Core assumption: The training corpus follows a reporting bias where only positive or unusual facts are stated explicitly.
- Evidence anchors:
  - [abstract] The authors note "lack of explicit data regarding the number 0" and refer to the "null value problem."
  - [section] Discussion of "null value blindness" and how "people typically 'not stating the obvious'" affects model training.
  - [corpus] No direct corpus evidence provided; claim inferred from general reporting bias literature.
- Break condition: If a model is fine-tuned on a dataset that explicitly includes zero-value examples and negative statements about numerical attributes, performance on relational reasoning would improve.

### Mechanism 2
- Claim: LLMs rely on statistical inference and pattern matching rather than genuine reasoning, leading to correct answers by chance rather than understanding.
- Mechanism: The model's predictions are based on learned token probabilities from the training data. When asked to compare implicit numerical facts, it often retrieves the most likely answer based on statistical patterns, not logical deduction. This explains why performance improves with larger models (more data, richer distributions) but not due to reasoning ability.
- Core assumption: The model's internal representations are optimized for prediction accuracy, not for capturing logical relationships between concepts.
- Evidence anchors:
  - [abstract] "LLMs make steady progress in knowledge acquisition and (pseudo)reasoning... their capabilities are limited to statistical inference only."
  - [section] "LLMs can not reason, they only fake it" and discussion of models "circumvent[ing] relational reasoning" by using task-irrelevant information.
  - [corpus] Neighbor papers on LLM reasoning limitations support this mechanism.
- Break condition: If a model demonstrates consistent logical inference across novel combinations of concepts not seen in training, this mechanism would be invalidated.

### Mechanism 3
- Claim: Subject/element biases in the training data lead to entangled concepts, causing the model to hallucinate numbers based on related subjects rather than actual knowledge.
- Mechanism: When the model lacks explicit data about a subject-element pair, it relies on related concepts that are more frequently mentioned together. For example, it might assign 4 legs to birds because "legs" and "4" are strongly associated with quadrupeds in the training data.
- Core assumption: The training data contains imbalanced representations of different subjects and their attributes, leading to concept entanglement.
- Evidence anchors:
  - [abstract] Discussion of "subject/element biases in hallucinations" and "concept entanglement."
  - [section] "Number of elements predicted by GPT-3.5... answers are biased towards the defining characteristic number of each subject" and examples of biased predictions for unicycle, bicycle, and tricycle.
  - [corpus] No direct corpus evidence; inferred from the discussion of biases in the paper.
- Break condition: If a model's predictions for unseen subject-element pairs show no correlation with related but irrelevant concepts, this mechanism would be weakened.

## Foundational Learning

- Concept: Statistical learning vs. symbolic reasoning
  - Why needed here: The paper distinguishes between statistical pattern matching (LLMs) and logical inference (symbolic solvers). Understanding this difference is crucial for interpreting why LLMs struggle with relational reasoning.
  - Quick check question: Can you explain why a model that memorizes "birds have 2 legs" might still fail to correctly answer "do birds have more legs than tricycles have wheels?" even if it knows both facts?

- Concept: Reporting bias in natural language
  - Why needed here: The paper relies on the idea that people don't state obvious facts (like "birds have no wheels"), which affects the training data and model performance.
  - Quick check question: Why might a model be more likely to hallucinate "4 legs" for a bird than "0 wheels" for the same bird?

- Concept: Prompt sensitivity and in-context learning
  - Why needed here: The paper shows that model performance varies significantly with prompt phrasing, highlighting the importance of understanding how LLMs respond to different formulations.
  - Quick check question: How might changing "fewer than" to "less than" in a prompt affect the model's answer, even if the meaning is the same?

## Architecture Onboarding

- Component map: Data generation module -> Prompt generation module -> LLM API -> Answer extraction -> Evaluation module -> Metrics computation
- Critical path: Data generation → Prompt generation → LLM API call → Answer extraction → Evaluation → Metrics computation
- Design tradeoffs: Using a simple template-based approach allows for clear isolation of reasoning capabilities but may not capture the full complexity of natural language queries. The combinatorial explosion in data generation ensures coverage but increases computational cost.
- Failure signatures: If the model consistently returns the same answer regardless of input (e.g., always "2"), it may be relying on a default pattern. If performance is near chance on all tasks, the model may not be engaging with the prompt at all.
- First 3 experiments:
  1. Run the probing task with GPT-3.5 and GPT-4 using the same prompts to compare performance and identify model-specific biases.
  2. Test the effect of adding a 1-shot demonstration by including an example query-answer pair in the prompt.
  3. Evaluate the "null value blindness" by creating a subset of queries with zero-value ground truths and analyzing the model's predictions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific biases in LLMs' numerical knowledge contribute to "null value blindness" and how can these be mitigated?
- Basis in paper: [explicit] The paper identifies "null value blindness" where models struggle with zero values due to scarcity of explicit data stating what subjects lack.
- Why unresolved: While the paper identifies the problem and suggests it's due to data scarcity, it doesn't explore specific biases or propose mitigation strategies.
- What evidence would resolve it: Analysis of training data to identify specific patterns contributing to null value blindness, followed by experiments testing targeted interventions to improve zero value predictions.

### Open Question 2
- Question: How does the performance of LLMs on relational reasoning tasks scale with model size and training data volume?
- Basis in paper: [explicit] The paper argues that larger models with more data don't necessarily improve reasoning capabilities, suggesting a plateau or diminishing returns.
- Why unresolved: The study only compares GPT-3.5 and GPT-4, not exploring a wider range of model sizes or training data volumes.
- What evidence would resolve it: Systematic experiments varying model size and training data volume, measuring performance on relational reasoning tasks to identify scaling patterns.

### Open Question 3
- Question: Can neuro-symbolic approaches effectively address the limitations of LLMs in relational reasoning, particularly for entailed arithmetical relationships?
- Basis in paper: [explicit] The paper mentions neuro-symbolic approaches as a potential solution but doesn't test their effectiveness on the specific task of entailed arithmetical relationships.
- Why unresolved: While the paper suggests neuro-symbolic approaches might be promising, it doesn't provide empirical evidence of their effectiveness for this particular task.
- What evidence would resolve it: Implementation and testing of neuro-symbolic approaches on the EAR task, comparing performance to pure LLM approaches.

## Limitations

- The paper's core claim that LLMs "do not reason" is based on performance on a specific task type, which may not generalize to all forms of logical reasoning
- The mechanisms proposed (null value blindness, statistical inference, concept entanglement) are inferred from performance patterns but lack direct causal evidence
- The dataset creation process relies on human annotations for 4.5% of examples, introducing potential subjectivity that could affect the ground truth quality

## Confidence

- **High confidence**: LLMs show significant performance drops when combining numerical facts versus recalling single facts. The empirical results on factual knowledge vs. relational reasoning are clearly demonstrated.
- **Medium confidence**: The interpretation that poor relational reasoning indicates LLMs are "statistical search engines rather than genuine reasoners." While the evidence supports this view, alternative interpretations exist (e.g., models may use different reasoning strategies than humans).
- **Medium confidence**: The mechanisms of null value blindness, statistical inference, and concept entanglement. These are plausible explanations but not definitively proven by the experimental design.

## Next Checks

1. Test whether fine-tuning the models on datasets that explicitly include zero-value examples and negative statements about numerical attributes improves performance on relational reasoning tasks.
2. Design controlled experiments that isolate whether models are using logical inference or pattern matching by testing with novel concept combinations that cannot be answered through simple statistical retrieval.
3. Evaluate whether alternative prompt formulations (beyond those tested) can elicit better relational reasoning performance, which would indicate the current formulations may be suboptimal rather than revealing fundamental reasoning limitations.