---
ver: rpa2
title: Geometric Interpretation of Layer Normalization and a Comparative Analysis
  with RMSNorm
arxiv_id: '2409.12951'
source_url: https://arxiv.org/abs/2409.12951
tags:
- vector
- hidden
- layer
- layernorm
- uniform
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a novel geometric interpretation of Layer Normalization
  (LayerNorm), showing it can be understood as removing the component of a vector
  along a uniform vector, normalizing the remainder, and scaling by the square root
  of the dimensionality. It reveals that the "mean subtraction" step in LayerNorm
  is theoretically redundant, as LLM hidden representations naturally operate orthogonal
  to the uniform vector at inference time.
---

# Geometric Interpretation of Layer Normalization and a Comparative Analysis with RMSNorm

## Quick Facts
- arXiv ID: 2409.12951
- Source URL: https://arxiv.org/abs/2409.12951
- Authors: Akshat Gupta; Atahan Ozdemir; Gopala Anumanchipalli
- Reference count: 8
- Primary result: LayerNorm's mean subtraction step is theoretically redundant for LLM hidden representations that naturally operate orthogonal to the uniform vector

## Executive Summary
This paper provides a novel geometric interpretation of Layer Normalization (LayerNorm), showing it can be understood as removing the component of a vector along a uniform vector, normalizing the remainder, and scaling by the square root of the dimensionality. The authors reveal that the "mean subtraction" step in LayerNorm is theoretically redundant, as LLM hidden representations naturally operate orthogonal to the uniform vector at inference time. Through experiments across seven transformer models (ranging from 1.3B to 8B parameters), they demonstrate that both LayerNorm and RMSNorm models have hidden vectors already orthogonal to the uniform vector before normalization, making the mean subtraction step unnecessary. These findings advocate for using RMSNorm over LayerNorm, which is more computationally efficient while achieving comparable performance.

## Method Summary
The study processes 1 million Wikipedia tokens through seven transformer models (GPT-2 XL, GPT-Neo 1.3B, Pythia-1.4B, GPT-J 6B, Pythia-6.9B, Llama-2-7B, Llama-3-8B) ranging from 1.3B to 8B parameters. For each model, the authors capture hidden representations before and after each normalization layer, then compute L2 norms and cosine similarities. They analyze norm stabilization effects, rotation angles between pre/post-normalization vectors, and angles between hidden vectors and the uniform vector (1 = [1,1,...,1]ᵀ). The geometric interpretation of LayerNorm as removing the uniform vector component is derived mathematically and validated empirically across different model architectures.

## Key Results
- LayerNorm can be geometrically decomposed into three steps: remove uniform vector component, normalize remainder, scale by sqrt(d)
- Hidden vectors in both LayerNorm and RMSNorm models are naturally orthogonal to the uniform vector at inference time (~90 degree angles)
- The mean subtraction step in LayerNorm is redundant for LLM hidden representations
- RMSNorm achieves comparable performance to LayerNorm while being more computationally efficient
- LayerNorm stabilizes growing vector norms while introducing consistent rotations across layers

## Why This Works (Mechanism)

### Mechanism 1
LayerNorm removes the component of a vector along the uniform vector, normalizes the remainder, and scales by sqrt(d). The LayerNorm operation can be decomposed into three geometric steps: (i) remove the projection of the input vector onto the uniform vector 1, (ii) normalize the remaining orthogonal component to unit norm, (iii) scale by sqrt(d). This process inherently aligns hidden vectors orthogonal to the uniform vector. The mean vector µ1 is a projection of the input vector along the uniform vector, and its norm equals the scalar projection of the input onto the uniform direction. Evidence from the abstract and section 2.2 supports this mechanism, though it's primarily internal to the paper.

### Mechanism 2
The mean subtraction step in LayerNorm is theoretically redundant because LLMs naturally operate orthogonal to the uniform vector during inference. Empirical measurements show that hidden vectors before normalization already have near-zero projection onto the uniform vector (angles ~90 degrees). Thus, removing this component changes nothing functionally. The uniform vector is an arbitrary direction in high-dimensional space, and random high-dimensional vectors are nearly orthogonal to any fixed vector. Evidence from the abstract and section 4 shows angle measurements between hidden vectors and uniform vector are ~90 degrees both pre- and post-normalization across multiple models.

### Mechanism 3
RMSNorm achieves comparable performance to LayerNorm because it preserves the natural orthogonality of hidden vectors to the uniform vector while being computationally more efficient. Since hidden vectors naturally align orthogonal to the uniform vector, omitting the mean subtraction step (RMSNorm) doesn't harm representation quality but reduces computation by avoiding explicit mean calculation and subtraction. The computational savings from omitting mean calculation outweigh any marginal performance differences, and model performance is not sensitive to the specific normalization variant when hidden vectors are naturally orthogonal. Evidence from the abstract and section 4 shows Llama models (RMSNorm-based) also have hidden vectors orthogonal to the uniform vector with even smaller variance than LayerNorm models.

## Foundational Learning

- **Concept: Vector projection and orthogonal decomposition**
  - Why needed here: Understanding how to decompose a vector into components parallel and perpendicular to another vector is essential for interpreting LayerNorm as removing the uniform vector component
  - Quick check question: Given vector v = [3, 4] and uniform vector u = [1, 1], what is the component of v along u and the orthogonal remainder?

- **Concept: High-dimensional geometry and random vector properties**
  - Why needed here: The paper relies on the property that random high-dimensional vectors are nearly orthogonal to any fixed direction, explaining why hidden vectors naturally align orthogonal to the uniform vector
  - Quick check question: Why are two randomly chosen vectors in a 1000-dimensional space almost always nearly orthogonal?

- **Concept: Normalization and its effects on vector norms and orientations**
  - Why needed here: LayerNorm modifies both the norm (scales to sqrt(d)) and orientation (projects to orthogonal subspace) of vectors, which are key to understanding its geometric interpretation
  - Quick check question: After LayerNorm, what is the norm of the standardized vector in d-dimensional space, and what subspace does it lie in?

## Architecture Onboarding

- **Component map:** Input vector x ∈ R^d → Compute mean µ → Form mean vector µ1 = µ * 1 → Compute orthogonal component x1_perp = x - µ1 → Normalize x1_perp to unit norm → Scale by sqrt(d) → Apply learned scale-and-shift α, β

- **Critical path:**
  1. Compute mean µ = (1/d) * 1^T x
  2. Form mean vector µ1 = µ * 1
  3. Compute orthogonal component x1_perp = x - µ1
  4. Normalize x1_perp to unit norm
  5. Scale by sqrt(d)
  6. Apply learned scale-and-shift

- **Design tradeoffs:**
  - LayerNorm vs RMSNorm: LayerNorm removes uniform vector component (theoretically redundant), RMSNorm omits this step (more efficient)
  - Computational cost: LayerNorm requires mean calculation and subtraction; RMSNorm only requires variance calculation
  - Reversibility: LayerNorm is irreversible (loses information along uniform vector); RMSNorm preserves all information
  - Stability: Both stabilize growing norms, but LayerNorm also enforces orthogonality to uniform vector

- **Failure signatures:**
  - If hidden vectors develop significant components along the uniform vector, LayerNorm becomes necessary
  - If scale-and-shift parameters α, β become unstable, normalization may fail to stabilize norms
  - If dimensionality d is very small, the orthogonality property may not hold, affecting both normalization methods

- **First 3 experiments:**
  1. Measure angle between hidden vectors and uniform vector before and after normalization across multiple layers and models to verify orthogonality claim
  2. Compare norm stabilization effectiveness between LayerNorm and RMSNorm on the same model architecture
  3. Train identical models with LayerNorm vs RMSNorm and compare downstream performance to verify computational efficiency claim

## Open Questions the Paper Calls Out

### Open Question 1
How do the theoretical findings about LayerNorm's redundancy apply to larger models beyond 8 billion parameters? The paper explicitly states that testing larger models was beyond their computational capacity due to data storage requirements, leaving this as an open area for future research. The authors were limited by computational resources and could only test models up to 8 billion parameters, despite acknowledging the importance of understanding how these findings scale. Testing the uniformity and redundancy of LayerNorm's mean subtraction step on models with parameters exceeding 8 billion, while measuring hidden vector components along the uniform vector across multiple layers, would resolve this question.

### Open Question 2
What specific architectural or training factors cause GPT2-XL to deviate from the pattern where hidden representations are orthogonal to the uniform vector, unlike other LayerNorm-based models? The authors note that GPT2-XL is the only LayerNorm-based model among the five studied that does not have hidden vectors orthogonal to the uniform vector before normalization. The paper observes this anomaly but does not investigate the underlying reasons, such as differences in architecture, training procedure, or initialization that might explain this behavior. Comparative analysis of GPT2-XL's architecture, training parameters, and initialization against other LayerNorm models, combined with layer-by-layer examination of hidden vector components along the uniform vector, would resolve this question.

### Open Question 3
What are the precise computational efficiency gains of using RMSNorm over LayerNorm in practice, beyond the theoretical savings from removing the mean subtraction step? While the paper advocates for RMSNorm due to its computational efficiency and comparable performance, it doesn't quantify the exact computational savings or provide benchmarking data. The paper mentions RMSNorm is "more computationally efficient" but doesn't provide concrete metrics or benchmarks comparing the two methods in terms of FLOPs, memory usage, or inference speed. Detailed benchmarking of both normalization methods across the same models, measuring actual computational costs including FLOPs per forward pass, memory requirements, and inference latency, would resolve this question.

### Open Question 4
Does the redundancy of the mean subtraction step in LayerNorm extend to other transformer-based architectures beyond decoder-only LLMs, such as encoder-only or encoder-decoder models? The study focuses exclusively on decoder-only LLMs, leaving open the question of whether these findings generalize to other transformer architectures. The paper's experimental scope is limited to decoder-only models, and the authors don't discuss whether their theoretical insights about LayerNorm's geometric interpretation apply to other transformer variants. Applying the same geometric analysis and empirical measurements to encoder-only models (like BERT) and encoder-decoder models (like T5), examining hidden vector components along the uniform vector before and after normalization, would resolve this question.

## Limitations
- The orthogonality claim relies on observational data from inference-time activations across 7 models, but doesn't explain why this property emerges during training
- The efficiency comparison assumes computational equivalence beyond normalization, but doesn't account for downstream effects on training dynamics
- The sample size (1M tokens) may not capture rare activation patterns that could develop significant uniform vector components

## Confidence
- **Geometric interpretation and mathematical framework (High):** The three-step decomposition of LayerNorm is mathematically sound and the uniform vector analysis is rigorous
- **Orthogonality observation at inference time (Medium):** Strong empirical support across 7 models, but limited to specific model families and scales
- **Redundancy claim and RMSNorm advocacy (Medium):** Supported by current observations but doesn't establish causal mechanisms or guarantee generalization

## Next Checks
1. **Training dynamics validation:** Monitor the angle between hidden vectors and uniform vector throughout training to determine when and how orthogonality emerges, and whether this property is preserved across different learning rates, batch sizes, and optimization strategies.

2. **Architecture and scale generalization:** Test the orthogonality property across diverse model architectures (CNNs, RNNs) and scales (beyond 8B parameters) to verify whether the uniform vector alignment is a general property of deep learning or specific to transformers.

3. **Task-specific performance evaluation:** Conduct controlled experiments training identical architectures with LayerNorm vs RMSNorm on multiple tasks to quantify performance differences, particularly for tasks requiring fine-grained control over mean activations or operating in low-dimensional spaces where orthogonality properties break down.