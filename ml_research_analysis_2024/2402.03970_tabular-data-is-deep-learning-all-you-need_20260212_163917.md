---
ver: rpa2
title: 'Tabular Data: Is Deep Learning all you need?'
arxiv_id: '2402.03970'
source_url: https://arxiv.org/abs/2402.03970
tags:
- performance
- datasets
- methods
- learning
- roc-auc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether deep learning (DL) methods outperform
  classical machine learning (ML) approaches on tabular data. A large-scale empirical
  evaluation was conducted using 68 datasets from the OpenMLCC18 benchmark and 17
  state-of-the-art methods, including neural networks, classical ML, and AutoML techniques.
---

# Tabular Data: Is Deep Learning all you need?

## Quick Facts
- **arXiv ID**: 2402.03970
- **Source URL**: https://arxiv.org/abs/2402.03970
- **Reference count**: 40
- **Primary result**: Deep learning methods outperform classical ML approaches on tabular data across all dataset regimes, with meta-learned foundation models and feed-forward neural networks achieving superior performance.

## Executive Summary
This study challenges the conventional wisdom that gradient-boosted decision trees (GBDTs) remain the gold standard for tabular data classification. Through a comprehensive empirical evaluation of 68 datasets from the OpenMLCC18 benchmark using 17 state-of-the-art methods, the research demonstrates a paradigm shift toward deep learning approaches. The findings reveal that simple feed-forward neural networks, when properly tuned, can outperform sophisticated GBDT architectures. Meta-learned foundation models and in-context learning approaches show particular promise, suggesting new directions for tabular data modeling that leverage pretraining and transfer learning.

## Method Summary
The study employs a rigorous 10-fold nested cross-validation protocol with 9-fold inner cross-validation for hyperparameter optimization using Optuna's Tree-structured Parzen Estimator (TPE) algorithm. Seventeen methods were evaluated, including classical ML approaches (CatBoost, XGBoost, LightGBM), neural network architectures (MLP, TabNet, FT-Transformer), foundation models (TP-BERTa, TabPFN, TabPFNv2, TabICL), and AutoML frameworks (AutoGluon, XTab). A critical innovation is the refitting step, where models are retrained on the combined training and validation sets after hyperparameter optimization. Performance is measured using ROC-AUC scores, and statistical significance is assessed through Wilcoxon signed-rank tests.

## Key Results
- Deep learning methods achieve superior performance across all dataset regimes, outperforming GBDTs regardless of dataset size or complexity
- Meta-learned foundation models and simple feed-forward networks consistently outperform dataset-specific architectures
- Refitting models on combined training and validation sets after hyperparameter optimization significantly improves predictive quality and affects overall model rankings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep learning methods outperform classical gradient boosting methods across all dataset regimes.
- Mechanism: Deep learning architectures can better capture complex interactions and patterns in tabular data when given sufficient data and proper hyperparameter tuning, especially with modern techniques like attention mechanisms, residual connections, and appropriate embedding strategies.
- Core assumption: The experimental setup provides fair comparisons with comprehensive hyperparameter optimization and refitting on combined training/validation sets.
- Evidence anchors:
  - [abstract]: "Our empirical results over 68 diverse datasets from a well-established benchmark indicate a paradigm shift, where Deep Learning methods outperform classical approaches."
  - [section 5]: "The results provided in Figure 2 (Left) indicate that DL methods outperform the previous state-of-the-art GBDTs approaches."
  - [corpus]: Weak evidence - only 5 related papers with minimal citations, suggesting this may be a novel finding.
- Break condition: If the hyperparameter optimization budget is insufficient or the refitting step is omitted, classical methods may retain their advantage.

### Mechanism 2
- Claim: Meta-learned foundation models and non-fine-tuned architectures outperform dataset-specific architectures and fine-tuned counterparts.
- Mechanism: Foundation models pretrained on diverse tabular data learn general representations that transfer well to new tasks without requiring dataset-specific fine-tuning, while in-context learning methods leverage pretraining without additional parameter updates.
- Core assumption: Pretraining data is sufficiently diverse and representative of the target datasets, and in-context learning provides adequate task adaptation.
- Evidence anchors:
  - [abstract]: "Meta-learned foundation models and simple feed-forward neural networks outperform gradient-boosted decision trees (GBDTs)."
  - [section 5]: "Figure 2 (Right) plot illustrates that in-context learning models are very competitive, with TabICL and TabPFNv2 having the best overall rank."
  - [corpus]: Limited evidence - only mentions related foundation model work without detailed comparative analysis.
- Break condition: If pretraining data lacks diversity or target datasets differ significantly from pretraining distribution, fine-tuning may become necessary.

### Mechanism 3
- Claim: Refitting on combined training and validation sets after hyperparameter optimization significantly improves predictive quality and affects overall model rankings.
- Mechanism: Training on the full available data (training + validation) provides more examples for learning, allowing models to better utilize the hyperparameter configuration found through cross-validation.
- Core assumption: The validation set contains useful information that can improve generalization when incorporated into training.
- Evidence anchors:
  - [abstract]: "Refitting models on combined training and validation sets after hyperparameter optimization significantly improves predictive quality and affects overall model rankings."
  - [section 5]: "Figure 7 (Left), where, as observed, all the methods that incorporate refitting feature a lower rank and outperform their non-refitting counterparts."
  - [corpus]: No direct evidence found in related papers, suggesting this may be an original contribution.
- Break condition: If the validation set is very small or unrepresentative, refitting may lead to overfitting rather than improvement.

## Foundational Learning

- Concept: Cross-validation methodology and nested cross-validation
  - Why needed here: The study uses 10-fold cross-validation with nested inner folds for hyperparameter optimization, requiring understanding of how to properly estimate model performance while avoiding overfitting to validation data.
  - Quick check question: What is the difference between using a single validation split versus nested cross-validation for hyperparameter tuning?

- Concept: Hyperparameter optimization techniques (TPE vs random search)
  - Why needed here: The study uses Tree-structured Parzen Estimator (TPE) algorithm for HPO, which requires understanding how it differs from simpler methods like random search in terms of efficiency and effectiveness.
  - Quick check question: Why might TPE be preferred over random search for tuning neural network hyperparameters on tabular data?

- Concept: Meta-feature analysis and correlation with model performance
  - Why needed here: The study analyzes how dataset characteristics (meta-features) correlate with model performance, requiring understanding of what meta-features measure and how they relate to learning difficulty.
  - Quick check question: How might the number of equivalent attributes (eq_num_attr) affect the performance of gradient boosting versus deep learning methods?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline (quantile transformation, ordinal encoding) -> Nested cross-validation framework with 10 outer folds and 9 inner folds -> Hyperparameter optimization using Optuna with TPE algorithm -> Model training and evaluation pipeline -> Refitting step on combined training/validation data -> Performance aggregation and ranking system

- Critical path:
  1. Preprocess dataset consistently across all methods
  2. Set up nested cross-validation structure
  3. Configure hyperparameter search spaces for each method
  4. Run HPO for each method on each fold
  5. Select best hyperparameters and refit on full training data
  6. Evaluate on test fold and aggregate results
  7. Compute rankings and statistical significance

- Design tradeoffs:
  - Memory vs. completeness: Some methods excluded due to memory constraints on large datasets
  - Search space comprehensiveness vs. computational feasibility: Fixed search spaces vs. dataset-specific tuning
  - Training time vs. performance: Longer training often improves performance but increases computational cost
  - Foundation models vs. dataset-specific models: Trade-off between generalization and task-specific optimization

- Failure signatures:
  - Memory errors: Often occur with large transformer models on datasets with many features
  - Convergence issues: May indicate inappropriate learning rates or architectural choices
  - Overfitting: High validation performance but poor test performance suggests insufficient regularization
  - Underfitting: Consistently poor performance across all configurations suggests architectural limitations

- First 3 experiments:
  1. Run CatBoost with default hyperparameters on a small dataset to verify basic pipeline functionality
  2. Test hyperparameter optimization on a simple MLP to ensure Optuna integration works correctly
  3. Verify refitting step by comparing performance with and without refitting on a single dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific dataset characteristics determine when deep learning methods outperform gradient boosting trees?
- Basis in paper: [explicit] The paper mentions a dataset landscape showing winning method families across different dataset sizes, but doesn't provide detailed analysis of which characteristics matter most.
- Why unresolved: The study identifies general trends but doesn't isolate the specific features that determine method superiority.
- What evidence would resolve it: A detailed correlation analysis between method performance and individual dataset meta-features like class separability, feature redundancy, or sample complexity.

### Open Question 2
- Question: How do meta-learned foundation models compare to fine-tuned models when both use the same pretraining data?
- Basis in paper: [explicit] The paper shows in-context learning models outperform fine-tuned ones, but doesn't explore whether this holds when both use identical pretraining.
- Why unresolved: The current study compares different pretraining strategies, making it impossible to isolate the effect of fine-tuning vs in-context learning.
- What evidence would resolve it: A controlled experiment where both approaches use the same pretraining data and only differ in the adaptation strategy.

### Open Question 3
- Question: Does refitting on combined training and validation sets always improve model performance, or are there cases where it harms generalization?
- Basis in paper: [explicit] The paper shows refitting generally improves performance, but doesn't explore edge cases where it might be detrimental.
- Why unresolved: The study demonstrates overall benefits but doesn't investigate scenarios where refitting could lead to overfitting.
- What evidence would resolve it: A systematic analysis of refitting effects across different dataset regimes, model architectures, and noise levels.

## Limitations
- Computational constraints led to the exclusion of some methods on larger datasets, particularly fine-tuned foundation models and transformer architectures
- Fixed hyperparameter search spaces may not be optimal for all methods and could underestimate the true potential of certain approaches
- The study focuses solely on classification tasks, leaving open questions about deep learning's effectiveness for regression and other tabular data problems

## Confidence

- **High confidence**: Deep learning methods outperform classical GBDTs across all dataset regimes when using comprehensive hyperparameter optimization and refitting on combined training/validation sets
- **Medium confidence**: Meta-learned foundation models and non-fine-tuned architectures consistently outperform fine-tuned counterparts, though this finding is sensitive to computational constraints affecting fine-tuned model performance
- **Medium confidence**: Refitting on combined training and validation sets significantly improves predictive quality, though this may not hold for all dataset sizes and characteristics

## Next Checks

1. **Replicate on regression tasks**: Test whether deep learning superiority extends to regression problems using the same experimental protocol on regression benchmarks like the OpenMLCC18 regression datasets.

2. **Vary computational budget**: Conduct experiments with different hyperparameter optimization time limits to assess how computational constraints affect the relative performance of computationally expensive methods like fine-tuned foundation models.

3. **Dataset-specific tuning**: Compare fixed search spaces with dataset-specific hyperparameter tuning to determine whether current results underestimate methods that benefit from more tailored optimization.