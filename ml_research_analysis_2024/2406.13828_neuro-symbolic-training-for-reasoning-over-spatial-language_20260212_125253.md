---
ver: rpa2
title: Neuro-symbolic Training for Reasoning over Spatial Language
arxiv_id: '2406.13828'
source_url: https://arxiv.org/abs/2406.13828
tags:
- square
- triangle
- medium
- block
- blue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a neuro-symbolic training approach for improving
  spatial reasoning in language models by incorporating spatial logical rules as constraints
  during fine-tuning. The method automatically creates chains of reasoning for each
  training example, derives consistency constraints from spatial logical rules, and
  integrates these constraints into the loss function using differentiable soft logic.
---

# Neuro-symbolic Training for Reasoning over Spatial Language

## Quick Facts
- arXiv ID: 2406.13828
- Source URL: https://arxiv.org/abs/2406.13828
- Authors: Tanawan Premsri; Parisa Kordjamshidi
- Reference count: 31
- Improves spatial reasoning in language models through neuro-symbolic training with logical constraints

## Executive Summary
This paper introduces a neuro-symbolic training approach that improves spatial reasoning in language models by incorporating spatial logical rules as differentiable soft constraints during fine-tuning. The method automatically generates reasoning chains (Q-Chains) for each training example and integrates spatial logical rules into the loss function using differentiable soft logic. Evaluated on spatial question-answering benchmarks including SpartQA-Human, ReSQ, and StepGame, the approach consistently improves performance over standard fine-tuning, particularly for multi-hop reasoning tasks. The method achieves 1-4% higher accuracy for deeper reasoning steps (k=6 to k=10) compared to large language models, while avoiding reliance on external reasoning tools at inference time.

## Method Summary
The approach fine-tunes language models for spatial reasoning by integrating spatial logical rules as differentiable soft constraints. For each training example, the method automatically creates Q-Chains through resolution tree building using forward chaining, derives consistency constraints from spatial logical rules, and incorporates these constraints into the loss function using differentiable soft logic via the DomiKnowS framework. The training process jointly optimizes both the task loss and constraint violation through a primal-dual program. This allows the model to learn spatial reasoning patterns while adhering to logical consistency rules, without requiring formal knowledge representation or external reasoning tools during inference.

## Key Results
- Consistently improves performance over standard fine-tuning on spatial QA benchmarks (SpartQA-Human, ReSQ, StepGame)
- Achieves 1-4% higher accuracy for deeper reasoning steps (k=6 to k=10) compared to large language models
- Shows significant improvements on Flan-T5 models while large language models still outperform on commonsense reasoning subsets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating spatial logical rules as differentiable soft constraints during fine-tuning improves the model's ability to capture abstract spatial patterns needed for multi-hop reasoning.
- Mechanism: The training process creates Q-Chains for each example, derives consistency constraints from spatial logical rules, and incorporates these constraints into the loss function using differentiable soft logic. This guides the model to minimize violations of spatial reasoning rules while learning the primary task.
- Core assumption: Supervision from high-level logical knowledge allows the model to capture more abstract patterns, thereby improving generalization to other domains.
- Evidence anchors:
  - [abstract] "Training language models to adhere to spatial reasoning rules guides them in making more effective and general abstractions for transferring spatial knowledge to various domains."
  - [section 3.3] "Our main hypothesis is that supervision from high-level logical knowledge allows the model to capture more abstract patterns, thereby improving generalization to various domains."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.5, average citations=0.0. Weak evidence for direct spatial reasoning methods.

### Mechanism 2
- Claim: The Q-Chain creation process enables the model to learn the intermediate reasoning steps needed for complex multi-hop spatial reasoning.
- Mechanism: For each training example, the method automatically augments the example with a chain of questions (Q-Chain) by building a resolution tree using forward chaining. This creates explicit intermediate facts that connect initial facts to target facts.
- Core assumption: Learning the intermediate reasoning steps explicitly helps the model understand the multi-hop reasoning process rather than just memorizing direct patterns.
- Evidence anchors:
  - [section 3.3] "We exploit Spatial Logic by automatically augmenting each training example with a chain of questions, denoted as Q-Chain."
  - [section 3.3] "We build the resolution tree using the forward chaining algorithm to infer a specific target fact given a set of initial facts."
  - [corpus] Weak evidence for Q-Chain specific methods in related work.

### Mechanism 3
- Claim: The approach's independence from external reasoning tools at inference time makes it more practical for real-world applications compared to pipeline-based neuro-symbolic methods.
- Mechanism: By incorporating logical constraints only during training (not inference), the model learns to reason spatially without requiring formal representations or external solvers at test time.
- Core assumption: Training with logical constraints is sufficient for the model to internalize the reasoning patterns, making external tools unnecessary during inference.
- Evidence anchors:
  - [abstract] "Our approach also avoids reliance on external reasoning tools, which can add computational complexity in real-time applications."
  - [section 1] "Additionally, formal knowledge representation is not needed at inference time, which is crucial because generating formal representations during inference is a challenging task in itself."
  - [corpus] Weak evidence for inference-time independence in related work.

## Foundational Learning

- Concept: Differentiable soft logic conversion
  - Why needed here: Standard logical expressions are not differentiable, so they cannot be directly incorporated into neural network loss functions. The soft logic conversion allows logical constraints to be used in gradient-based optimization.
  - Quick check question: How does the t-norm Product convert the logical implication A ⇒ B into a differentiable form?

- Concept: Forward chaining algorithm for resolution tree building
  - Why needed here: The method needs to automatically create Q-Chains by inferring intermediate facts from initial facts and target facts. Forward chaining provides a systematic way to build these reasoning chains.
  - Quick check question: What is the difference between forward chaining and backward chaining in terms of how they build reasoning paths?

- Concept: Constraint-based training with dual formulation
  - Why needed here: The approach needs to optimize both the task loss and the constraint violation simultaneously. The primal-dual program provides a framework for this joint optimization.
  - Quick check question: How does the primal-dual formulation differ from simply adding weighted constraint losses to the main loss?

## Architecture Onboarding

- Component map: Input → BERT/Flan-T5 backbone → Q-Chain generation → Constraint derivation → Loss calculation → Optimization → Output
- Critical path: Input → Backbone → Q-Chain generation → Constraint derivation → Loss calculation → Optimization → Output
- Design tradeoffs:
  - Using synthetic data (SpaRTUN) for Q-Chain supervision vs. needing realistic domain data
  - Computational cost of constraint calculation vs. performance improvement
  - Model size selection (BERT vs. Flan-T5) vs. training/inference efficiency
- Failure signatures:
  - Poor performance on multi-hop reasoning despite good single-hop performance suggests Q-Chain generation issues
  - Degradation on commonsense reasoning tasks suggests over-reliance on logical constraints
  - High constraint violation loss suggests incorrect logical rule formulation
- First 3 experiments:
  1. Train BERT with SpaRTUN only, evaluate on ReSQ k=2 split to establish baseline improvement
  2. Train BERT with SpaRTUN + Q-Chain constraints, compare performance on synthetic vs. realistic domains
  3. Train Flan-T5 with SpaRTUN + Q-Chain constraints, evaluate on StepGame to test multi-hop reasoning limits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of neuro-symbolic training scale with increasing reasoning depth beyond k=10?
- Basis in paper: [inferred] The paper reports performance on reasoning depths up to k=10, noting that improvements are "1-4% on a higher number of reasoning hops (k=6 to k=10)" compared to LLMs
- Why unresolved: The evaluation stops at k=10, but spatial reasoning tasks could require even deeper reasoning chains
- What evidence would resolve it: Testing the model on datasets with reasoning depths >10 and reporting performance degradation rates compared to baseline models

### Open Question 2
- Question: What is the computational overhead of incorporating spatial logical constraints during fine-tuning compared to standard fine-tuning?
- Basis in paper: [explicit] The paper mentions using "8 A6000 GPUs, requiring approximately 100 GPU hours" but doesn't provide comparative timing with standard fine-tuning
- Why unresolved: The paper focuses on accuracy improvements but doesn't quantify the training efficiency trade-offs
- What evidence would resolve it: Detailed timing comparisons between standard fine-tuning and neuro-symbolic fine-tuning across different model sizes and dataset scales

### Open Question 3
- Question: How does the quality of spatial logical rules affect the effectiveness of neuro-symbolic training?
- Basis in paper: [explicit] The paper uses "79 reasoning rules taken from Mirzaee and Kordjamshidi (2022)" but doesn't explore the impact of rule quality or completeness
- Why unresolved: The paper assumes logical rules are available but doesn't investigate how rule quality affects learning outcomes
- What evidence would resolve it: Experiments varying rule quality (e.g., incomplete rules, incorrect rules) and measuring impact on model performance across different reasoning tasks

## Limitations
- Effectiveness heavily depends on quality and completeness of the 79 spatial logical rules used
- Modest absolute performance gains (1-4%) suggest limited impact on simpler reasoning scenarios
- Large language models still outperform on commonsense reasoning subsets, indicating trade-off between logical consistency and intuitive reasoning

## Confidence
- **High confidence** in the mechanism that integrating differentiable spatial logical constraints during training can improve multi-hop reasoning performance, as evidenced by consistent improvements across synthetic benchmarks and specific improvements on ReSQ k=3 split (60.1% vs 57.1%).
- **Medium confidence** in the approach's ability to generalize across domains, given that improvements are observed on realistic domains (SpartQA-Human) but with more modest gains (56.5% vs 55.3%) compared to synthetic domains.
- **Low confidence** in the scalability of the Q-Chain generation process to more complex reasoning scenarios or larger datasets, as the evaluation focuses primarily on reasoning depths up to k=10 and uses datasets with limited scale.

## Next Checks
1. **Constraint sensitivity analysis**: Systematically remove subsets of the 79 spatial logical rules to identify which rule categories (converse, inverse, transitive, etc.) contribute most to performance improvements, and test whether the approach remains effective with reduced rule sets.

2. **Cross-domain generalization test**: Evaluate the trained models on out-of-distribution spatial reasoning tasks that were not present in any of the four training datasets to assess the claimed generalization capabilities beyond the reported domain transfers.

3. **Inference-time efficiency benchmark**: Measure the actual inference latency and computational overhead of the fine-tuned models compared to baseline approaches, particularly focusing on whether the training-time constraint integration provides measurable efficiency benefits during deployment.