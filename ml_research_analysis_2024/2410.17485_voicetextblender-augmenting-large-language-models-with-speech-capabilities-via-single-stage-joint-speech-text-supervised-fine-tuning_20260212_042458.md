---
ver: rpa2
title: 'VoiceTextBlender: Augmenting Large Language Models with Speech Capabilities
  via Single-Stage Joint Speech-Text Supervised Fine-Tuning'
arxiv_id: '2410.17485'
source_url: https://arxiv.org/abs/2410.17485
tags:
- speech
- data
- text
- training
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes a single-stage joint speech-text supervised
  fine-tuning approach to augment large language models with speech capabilities.
  The method combines text-only SFT data with three types of speech-related data:
  speech recognition and translation, speech-based question answering, and mixed-modal
  SFT generated using TTS.'
---

# VoiceTextBlender: Augmenting Large Language Models with Speech Capabilities via Single-Stage Joint Speech-Text Supervised Fine-Tuning

## Quick Facts
- **arXiv ID**: 2410.17485
- **Source URL**: https://arxiv.org/abs/2410.17485
- **Reference count**: 31
- **Primary result**: 3B model outperforms previous 7B or 13B SpeechLMs on most speech benchmarks while maintaining comparable text-only performance

## Executive Summary
This paper introduces VoiceTextBlender (VTBlender), a method to augment large language models with speech capabilities through single-stage joint speech-text supervised fine-tuning. The approach combines text-only SFT data with three types of speech-related data: speech recognition and translation, speech-based question answering, and mixed-modal SFT generated using TTS. Using LoRA adapters for efficient fine-tuning while preserving text performance, the 3B model demonstrates superior performance across various speech benchmarks while maintaining comparable text-only task performance. The model shows emergent abilities to handle previously unseen prompts and multi-turn mixed-modal conversations.

## Method Summary
The method employs single-stage joint speech-text SFT by combining text-only SFT data with three types of speech-related data: ASR/AST data for basic speech recognition, speech-based QA data for comprehension, and mixed-modal SFT generated via TTS for handling interleaving speech-text inputs. The model uses LoRA adapters to efficiently update parameters while preserving the frozen pre-trained LLM backbone. Training involves mixing these datasets with specific sampling ratios during mini-batch construction, allowing the model to learn to handle both modalities simultaneously while maintaining text capabilities.

## Key Results
- 3B model outperforms previous 7B and 13B SpeechLMs on most speech benchmarks
- Achieves superior ASR WER and AST BLEU scores compared to prior models
- Demonstrates strong performance on speech-based QA tasks (SPGI, SQuAD 2.0)
- Shows emergent abilities for handling previously unseen prompts and multi-turn mixed-modal inputs
- Maintains comparable text-only performance on GSM8K, IFEval, BBH, and MMLU benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Single-stage joint SFT with LoRA adapters preserves text performance while adding speech capabilities. By combining text-only and speech-related SFT data during training, the model learns to handle both modalities simultaneously. LoRA adapters update only a small subset of parameters, reducing interference with pre-trained text knowledge.

### Mechanism 2
Mixed-modal SFT data generation with TTS enables handling of interleaving speech-text inputs. By randomly replacing sentences in text SFT data with synthesized speech, the model learns to handle flexible input formats where speech and text can appear in any order.

### Mechanism 3
Three types of speech-related SFT data provide comprehensive speech understanding capabilities. ASR/AST data teaches basic recognition and translation, speech-based QA enables comprehension, and mixed-modal SFT teaches handling of flexible input formats.

## Foundational Learning

**Catastrophic forgetting in fine-tuning LLMs**: Why needed here - Understanding why standard fine-tuning degrades text performance is crucial for designing effective speech augmentation strategies. Quick check question: What happens to a model's performance on its original task when fine-tuned on a new task with standard optimization?

**Low-rank adaptation (LoRA) for parameter-efficient fine-tuning**: Why needed here - LoRA is the key mechanism that allows updating speech capabilities while preserving text knowledge by modifying only a small subset of parameters. Quick check question: How does LoRA differ from standard fine-tuning in terms of which parameters are updated?

**Curriculum learning and task ordering in multi-stage training**: Why needed here - Understanding why single-stage training can be more effective than multi-stage approaches for this problem. Quick check question: What are the potential benefits and drawbacks of updating model components in stages versus simultaneously?

## Architecture Onboarding

**Component map**: Speech encoder (Canary 609M) -> Modality adapter (Conformer 52M) -> LLM (Gemma 2.5B) -> LoRA adapters (36M)

**Critical path**: Speech encoder → Modality adapter → LLM → LoRA adapters
- Speech encoder and modality adapter are fully fine-tuned
- LLM backbone is frozen except for LoRA parameters
- Data mixing determines balance between text and speech capabilities

**Design tradeoffs**:
- Single-stage vs multi-stage training: Simplicity and efficiency vs potential for better task-specific optimization
- LoRA rank selection: Higher rank captures more speech patterns but risks forgetting text knowledge
- Speech encoder choice: Canary provides good quality but limits flexibility compared to using the model's own encoder

**Failure signatures**:
- Poor text performance: LoRA rank too high or insufficient text SFT data
- Poor speech performance: Insufficient speech SFT data or ineffective modality adapter
- Mode collapse: Model consistently ignores speech or text input
- Overfitting to TTS: Poor performance on real speech despite good TTS results

**First 3 experiments**:
1. Ablation study: Train with only speech SFT data (no text) to confirm catastrophic forgetting
2. LoRA rank sweep: Test different LoRA ranks to find optimal balance between speech performance and text preservation
3. Data mixing ratio sweep: Test different sampling ratios between text-only and speech SFT data to find optimal balance

## Open Questions the Paper Calls Out

**Open Question 1**: How does performance scale with model size (3B, 7B, 13B)?
- Basis: Paper states 3B outperforms previous 7B/13B models but only evaluates 3B
- Resolution: Systematic evaluation across multiple model sizes on identical benchmarks

**Open Question 2**: What is the impact of different TTS models on mixed-modal SFT quality?
- Basis: Paper uses single TTS model and suggests exploring multiple models
- Resolution: Comparative experiments with multiple TTS models varying speaker characteristics

**Open Question 3**: How does the model perform on specialized speech tasks (speaker recognition, emotion detection, speech enhancement)?
- Basis: Paper notes limitations in training data scope
- Resolution: Evaluation on benchmarks for specialized speech processing tasks

## Limitations
- Heavy reliance on synthesized speech data (TTS) may limit acoustic diversity and naturalness
- Parameter efficiency claims are overstated as most trainable parameters are fully fine-tuned
- Generalization to truly novel prompts and tasks is demonstrated but not thoroughly validated

## Confidence

**Single-Stage Joint SFT Effectiveness**: Medium - Superior benchmark performance demonstrated, but mechanism not fully explained
**Mixed-Modal Capability**: Medium - Improved performance shown but evaluation limited to specific benchmarks  
**Parameter Efficiency**: Low - Good performance achieved but characterization as "parameter-efficient" is misleading

## Next Checks

**Check 1: LoRA Rank Sensitivity Analysis**
Conduct systematic study varying LoRA rank from 4 to 64 while keeping other factors constant. Measure both speech task performance and text task preservation across this range to identify optimal tradeoff point.

**Check 2: Real Speech Generalization Test**
Replace all TTS-synthesized data in mixed-modal SFT with real speech recordings from diverse speakers covering different accents, speaking styles, and background conditions. Retrain and evaluate performance.

**Check 3: Novel Prompt Evaluation Protocol**
Develop benchmark of truly novel speech prompts and tasks not represented in training data, including complex multi-turn conversations and domain-specific instructions. Compare model's ability to handle these novel inputs against baseline SpeechLMs and original text-only Gemma model.