---
ver: rpa2
title: 'Learning Mixtures of Experts with EM: A Mirror Descent Perspective'
arxiv_id: '2411.06056'
source_url: https://arxiv.org/abs/2411.06056
tags:
- page
- where
- function
- experts
- convex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the theoretical properties of the Expectation
  Maximization (EM) algorithm for training Mixture of Experts (MoE) models, which
  partition the input space into subsets processed by specialized expert models. The
  authors establish a formal connection between EM for MoE and projected Mirror Descent
  with unit step size and Kullback-Leibler divergence regularizer.
---

# Learning Mixtures of Experts with EM: A Mirror Descent Perspective

## Quick Facts
- **arXiv ID:** 2411.06056
- **Source URL:** https://arxiv.org/abs/2411.06056
- **Reference count:** 40
- **Primary result:** Establishes EM for MoE as equivalent to projected Mirror Descent with unit step size, enabling new convergence analysis

## Executive Summary
This paper provides a novel theoretical framework for understanding the Expectation Maximization (EM) algorithm when applied to Mixture of Experts (MoE) models. The authors establish a formal connection between EM and projected Mirror Descent with unit step size and Kullback-Leibler divergence regularizer. This equivalence enables new convergence analysis for EM applied to general MoE models where the conditional distribution belongs to an exponential family. The paper derives sufficient conditions for convergence to stationary points or true parameters, with explicit rates, and characterizes convergence for symmetric mixtures of 2 linear or logistic experts in terms of signal-to-noise ratio.

## Method Summary
The paper studies EM for training MoE models by first establishing its equivalence to projected Mirror Descent with unit step size and KL divergence regularizer when the conditional distribution belongs to an exponential family. The authors then derive sufficient conditions for convergence to stationary points or true parameters, with explicit rates characterized by the Missing Information Matrix eigenvalues. For symmetric mixtures of 2 linear or logistic experts, they show EM is equivalent to Mirror Descent without projection, enabling simpler convergence analysis based on signal-to-noise ratio. Experimental results on synthetic and real-world data demonstrate EM's superior convergence rate and final accuracy compared to gradient descent.

## Key Results
- EM for MoE is mathematically equivalent to projected Mirror Descent with unit step size and KL divergence regularizer
- Under certain initialization conditions, EM converges linearly to true parameters with rate determined by Missing Information Matrix eigenvalues
- For symmetric mixtures of 2 linear or logistic experts, EM is equivalent to Mirror Descent without projection
- Experiments show EM outperforms gradient descent in both convergence rate and final accuracy on synthetic and real-world data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** EM for MoE is mathematically equivalent to a single step of projected Mirror Descent with unit step size and KL divergence regularizer.
- **Mechanism:** The EM surrogate objective Q(θ|θt) can be decomposed into an expected Bregman divergence term plus a linear term in the natural parameter space. This decomposition exactly matches the Mirror Descent subproblem formulation when the complete data distribution belongs to an exponential family.
- **Core assumption:** The conditional distribution p(y,z|x,θ) belongs to an exponential family of distributions, enabling the natural parameterization and log-partition decomposition.
- **Evidence anchors:**
  - [abstract]: "show its equivalence to projected Mirror Descent with unit step size and Kullback-Leibler divergence regularizer"
  - [section 4]: Theorem 4.1 formalizes this equivalence with proof showing Q(θ|θt) − Q(θt|θt) = EX[⟨∇L(θt_x), θ_x − θt_x⟩ + DA(θ_x, θt_x)]
  - [corpus]: Missing - no direct citations found supporting this specific equivalence

### Mechanism 2
- **Claim:** Under certain initialization conditions, EM iterates converge linearly to the true parameters with rate determined by the Missing Information Matrix (MIM) eigenvalues.
- **Mechanism:** The MIM captures the information gap between observed data (x,y) and latent expert labels z. When MIM eigenvalues are bounded away from 1, the loss function becomes strongly convex relative to the mirror map in a neighborhood of the true parameters, enabling linear convergence.
- **Core assumption:** θ1 is initialized in a locally strongly convex region of L(θ) relative to A(θ) containing θ∗.
- **Evidence anchors:**
  - [abstract]: "characterize convergence in terms of signal-to-noise ratio" and "sufficient conditions for local linear convergence"
  - [section 5]: Theorem B.2 shows L(θ) is α-strongly convex relative to A(θ) iff λmax(M(θ)) ≤ (1 − α)
  - [corpus]: Missing - no direct citations found connecting MIM to EM convergence rates

### Mechanism 3
- **Claim:** For symmetric mixtures of 2 linear or logistic experts, EM is equivalent to Mirror Descent without projection, enabling simpler convergence analysis.
- **Mechanism:** The symmetric structure (β∗ = −β∗−1, z ∈ {−1,1}) simplifies the complete data distribution such that it no longer requires exponential family structure while maintaining the MD equivalence. This allows direct application of MD convergence theory.
- **Core assumption:** The mixture is symmetric with exactly 2 experts, enabling the simplified parameterization.
- **Evidence anchors:**
  - [abstract]: "In the special case of mixture of 2 linear or logistic experts, we additionally provide guarantees for linear convergence based on the signal-to-noise ratio"
  - [section 5]: Theorem 5.1 establishes MD equivalence for SymMoLinE and SymMoLogE with explicit mirror maps
  - [corpus]: Missing - no direct citations found for this specific symmetric case analysis

## Foundational Learning

- **Concept:** Exponential family distributions and natural parameterization
  - **Why needed here:** The entire MD equivalence proof relies on the ability to write p(y,z|x,θ) = exp{⟨S(y,z),θ_x⟩ + A(θ_x)} where S is the sufficient statistic and A is the log-partition function
  - **Quick check question:** Can you write the Gaussian distribution in exponential family form and identify its natural parameters and sufficient statistics?

- **Concept:** Bregman divergence and mirror maps
  - **Why needed here:** The KL divergence regularizer in MD is a specific Bregman divergence induced by the log-partition function A(θ), and understanding its properties is crucial for the convergence analysis
  - **Quick check question:** What properties must a function satisfy to be a valid mirror map, and how does the KL divergence fit this definition?

- **Concept:** Fisher information matrix and Missing Information Matrix
  - **Why needed here:** The MIM quantifies the information gap about latent variables and directly determines the convergence rate through its eigenvalues' relationship to strong convexity
  - **Quick check question:** How does the Missing Information Matrix differ from the standard Fisher information matrix, and what does its spectral structure tell us about identifiability?

## Architecture Onboarding

- **Component map:** Data generation -> EM algorithm -> Mirror Descent equivalence -> Convergence analysis
- **Critical path:** 1) Verify data follows exponential family conditional distribution 2) Implement EM updates for gating (w) and expert (β) parameters separately 3) Monitor convergence via objective function and parameter distance metrics 4) Check initialization is within locally convex region (use multiple random starts)
- **Design tradeoffs:** EM vs GD: EM requires no step-size tuning but needs inner optimization loops; GD is simpler but sensitive to learning rate; Symmetric vs general MoE: Symmetric case enables simpler analysis but may not capture all real-world scenarios; Projection requirement: General MoE needs expectation moment projection, adding computational overhead
- **Failure signatures:** Divergence or oscillation: Likely due to poor initialization outside convex region; Slow sub-linear convergence: MIM eigenvalues too close to 1 (high SNR regime); Numerical instability: KL divergence terms becoming extreme when distributions are nearly deterministic
- **First 3 experiments:** 1) Synthetic SymMoLinE with known parameters: Test linear convergence and compare EM vs GD step counts 2) Fashion MNIST with random inversion: Validate EM's ability to recover partitioning in real data 3) Varying SNR scenarios: Demonstrate transition from linear to sub-linear convergence as MIM eigenvalues approach 1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EM perform compared to gradient descent for deep and sparse MoE models on large-scale datasets?
- Basis in paper: [explicit] The paper mentions formulating EM for deep and sparse MoE in Appendix D.4 and suggests this as an open direction for future research.
- Why unresolved: The paper only provides theoretical analysis and experiments for standard MoE models, not for deep and sparse variants.
- What evidence would resolve it: Experimental results comparing EM, Gradient EM, and gradient descent on deep and sparse MoE models trained on large-scale datasets like ImageNet or large language models.

### Open Question 2
- Question: What is the optimal batch size for EM in mini-batch settings, and how does it compare to the optimal batch size for gradient descent?
- Basis in paper: [inferred] The paper discusses that EM is equivalent to MD with a fixed learning rate of 1, which differs from gradient descent's sensitivity to step size. It mentions that mini-batch training could benefit large-scale EM implementations but doesn't explore this.
- Why unresolved: The paper doesn't investigate how batch size affects EM's performance or compare it to gradient descent's optimal batch size scaling.
- What evidence would resolve it: Empirical studies varying batch sizes for EM and gradient descent on MoE models, measuring convergence rates and final performance.

### Open Question 3
- Question: Under what conditions does EM achieve linear convergence for symmetric mixtures of more than 2 experts?
- Basis in paper: [explicit] The paper shows EM achieves linear convergence for symmetric mixtures of 2 linear or logistic experts based on SNR, but notes that the S(x,y,z) relationship becomes non-linear for k ≥ 3, making analysis difficult.
- Why unresolved: The paper only provides linear convergence guarantees for k=2 experts, and the non-linear relationship for k≥3 prevents direct extension of the analysis.
- What evidence would resolve it: Theoretical analysis characterizing sufficient conditions for linear convergence of EM for symmetric mixtures with k ≥ 3 experts, potentially relating to generalized SNR measures or other model properties.

## Limitations

- The exponential family assumption is critical yet restrictive, excluding many practical distributions
- Convergence guarantees depend heavily on initialization being in a locally strongly convex region
- Analysis focuses on batch settings with limited discussion of how results extend to stochastic or mini-batch regimes

## Confidence

- **High confidence:** The formal equivalence between EM and mirror descent (Theorem 4.1) and the basic convergence framework
- **Medium confidence:** The specific convergence rates and conditions for linear convergence, as these depend on problem-specific quantities that are difficult to verify in practice
- **Low confidence:** The practical implications for very high-dimensional problems where the dimensionality affects the spectral properties of the Missing Information Matrix

## Next Checks

1. **Robustness to initialization:** Systematically test EM convergence from multiple random initializations across different SNR regimes to empirically validate the theoretical initialization requirements.

2. **Scalability assessment:** Evaluate EM performance on larger-scale MoE architectures (more experts, higher dimensional inputs) to identify potential computational bottlenecks or breakdown of theoretical assumptions.

3. **Alternative regularizers:** Test whether the mirror descent equivalence and convergence properties hold with different Bregman divergences beyond KL divergence, potentially improving robustness to initialization.