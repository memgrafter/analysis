---
ver: rpa2
title: 'CLMIA: Membership Inference Attacks via Unsupervised Contrastive Learning'
arxiv_id: '2411.11144'
source_url: https://arxiv.org/abs/2411.11144
tags:
- attack
- data
- learning
- membership
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CLMIA, a membership inference attack that leverages
  unsupervised contrastive learning to train an attack model without requiring labeled
  membership information. The method generates positive samples using dropout layers
  on shadow models and trains the attack model to distinguish member and non-member
  data features in an unsupervised manner, requiring only a small amount of labeled
  data for fine-tuning.
---

# CLMIA: Membership Inference Attacks via Unsupervised Contrastive Learning

## Quick Facts
- arXiv ID: 2411.11144
- Source URL: https://arxiv.org/abs/2411.11144
- Reference count: 40
- Outperforms existing attack methods on CIFAR-10, CIFAR-100, and STL-10 datasets

## Executive Summary
CLMIA introduces a novel membership inference attack that leverages unsupervised contrastive learning to train attack models without requiring labeled membership information. The method generates positive samples using dropout layers on shadow models and trains the attack model to distinguish member and non-member data features in an unsupervised manner, requiring only a small amount of labeled data for fine-tuning. Experiments show CLMIA achieves superior performance compared to baseline attacks on multiple datasets and model architectures.

## Method Summary
CLMIA uses unsupervised contrastive learning to train an attack model without labeled membership data. The method involves training shadow models with dropout layers, generating positive samples by applying different dropout rates, and using contrastive loss to train the attack model on positive/negative pairs. After contrastive training, an MLP layer is added and fine-tuned with minimal labeled data to enable explicit membership predictions. The approach is tested on CIFAR-10, CIFAR-100, and STL-10 datasets with CNN, VGG-19, and ResNet-18 architectures.

## Key Results
- Achieves 66.8% balanced accuracy on CIFAR-10 with ResNet-18 (vs 65.9% baseline)
- Achieves 89.4% balanced accuracy on CIFAR-100 with ResNet-18 (vs 89.7% baseline)
- Achieves 64.9% balanced accuracy on STL-10 with ResNet-18 (vs 63.8% baseline)
- Optimal temperature parameter is 0.05 for contrastive learning loss function
- Better performance with higher proportions of labeled member data and lower dropout rates (0.1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLMIA improves attack performance by using contrastive learning to train the attack model without labeled membership data.
- Mechanism: Contrastive learning maximizes agreement between positive samples (generated via dropout) and pushes negative samples apart, allowing the attack model to extract distinguishing features between members and non-members without requiring explicit labels.
- Core assumption: The target model exhibits different behavior for member vs non-member data, and this difference can be captured through unsupervised contrastive learning.
- Evidence anchors:
  - [abstract]: "CLMIA uses unsupervised contrastive learning to train an attack model without using extra membership status information"
  - [section]: "CLMIA can be applied to more realistic scenarios and achieve better attack performance, especially in the case of insufficient data of labeled identity information"
  - [corpus]: Weak - no direct evidence found about contrastive learning's effectiveness for membership inference attacks
- Break condition: If the target model does not exhibit sufficient behavioral differences between members and non-members, or if the contrastive learning fails to capture these differences, the attack performance will degrade significantly.

### Mechanism 2
- Claim: Dropout layers on shadow models generate positive samples that capture membership-relevant features.
- Mechanism: By applying different dropout rates to shadow models, CLMIA creates pairs of outputs (positive samples) that reflect the same input but with different internal representations, allowing the attack model to learn which features are consistent across dropout variations for member data.
- Core assumption: Dropout creates meaningful variations in model output that preserve membership information while providing diversity for contrastive learning.
- Evidence anchors:
  - [section]: "we train a shadow model...The shadow model and the target model have the same model structure, and the only difference is that the shadow model has more dropout layers"
  - [section]: "by adding a dropout layer to the shadow model, a pair of positive samples is obtained"
  - [corpus]: Missing - no corpus evidence about dropout-based positive sample generation for MIA
- Break condition: If dropout rates are too high or too low, the positive samples may lose membership information or become too similar, reducing the effectiveness of contrastive learning.

### Mechanism 3
- Claim: Fine-tuning with minimal labeled data allows the attack model to output explicit membership predictions while preserving the learned feature extraction capabilities.
- Mechanism: After unsupervised contrastive learning, CLMIA adds an MLP layer that is trained with a small amount of labeled data while keeping the feature extraction layers frozen, allowing the model to map learned features to explicit membership predictions.
- Core assumption: The features learned through contrastive learning are transferable to supervised membership prediction tasks.
- Evidence anchors:
  - [abstract]: "require only a small amount of data with known membership status to fine-tune the attack model"
  - [section]: "the attacker then uses a supervised method to fine-tune the attack model via a small sample with known data identification information"
  - [corpus]: Weak - no direct evidence about fine-tuning contrastive learning models for membership inference
- Break condition: If the feature space learned through contrastive learning is not aligned with the membership prediction task, fine-tuning may not improve performance despite having labeled data.

## Foundational Learning

- Concept: Contrastive Learning
  - Why needed here: CLMIA relies on contrastive learning to train the attack model without labeled membership data, which is crucial for real-world scenarios where labeled data is scarce.
  - Quick check question: How does contrastive learning create positive and negative samples without explicit labels?

- Concept: Membership Inference Attacks
  - Why needed here: Understanding the fundamentals of MIA helps explain why CLMIA's approach of distinguishing member vs non-member data is effective and what assumptions it relies on.
  - Quick check question: What behavioral differences between member and non-member data does CLMIA exploit?

- Concept: Dropout as Data Augmentation
  - Why needed here: CLMIA uses dropout layers to generate positive samples, so understanding how dropout affects model behavior is essential for grasping the attack mechanism.
  - Quick check question: How does applying different dropout rates to the same input create useful positive samples for contrastive learning?

## Architecture Onboarding

- Component map:
  - Target model -> Shadow models (with dropout layers) -> Positive samples generation -> Contrastive learning training -> MLP fine-tuning -> Attack model

- Critical path:
  1. Train shadow models with dropout layers
  2. Generate positive samples by running inputs through shadow models
  3. Use contrastive loss to train attack model on positive/negative pairs
  4. Add MLP layer and fine-tune with small labeled dataset
  5. Use trained attack model to infer membership from target model outputs

- Design tradeoffs:
  - More dropout layers vs less: More dropout creates more diverse positive samples but may lose membership information
  - Larger vs smaller labeled fine-tuning dataset: More labeled data improves accuracy but reduces the advantage of unsupervised training
  - Higher vs lower temperature parameter: Lower temperature makes the model more sensitive to negative samples but may lead to overfitting

- Failure signatures:
  - Attack accuracy near 50%: Model failed to learn distinguishing features
  - Large gap between training and validation accuracy: Overfitting to the small labeled dataset
  - Performance doesn't improve with more labeled data: Feature space learned through contrastive learning may not be suitable for membership prediction

- First 3 experiments:
  1. Test CLMIA on a simple dataset (like CIFAR-10) with a basic CNN to verify the basic mechanism works
  2. Vary the dropout rate in shadow models to find the optimal setting for positive sample generation
  3. Test CLMIA with different proportions of labeled data to understand the minimum required for effective fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different data augmentation techniques on the performance of CLMIA?
- Basis in paper: [inferred] The paper discusses using dropout layers to generate positive samples for contrastive learning, but does not explore other data augmentation techniques.
- Why unresolved: The paper focuses on using dropout layers for positive sample generation and does not investigate other potential data augmentation methods that could improve CLMIA's performance.
- What evidence would resolve it: Experimental results comparing CLMIA's performance using various data augmentation techniques, such as rotation, scaling, or color jittering, would provide insights into their impact on attack accuracy.

### Open Question 2
- Question: How does the choice of temperature parameter Ï„ affect CLMIA's performance across different datasets and model architectures?
- Basis in paper: [explicit] The paper mentions that the optimal temperature parameter is 0.05 for the contrastive learning loss function but does not provide a comprehensive analysis of its impact across different scenarios.
- Why unresolved: The paper only presents results for a specific temperature parameter value (0.05) and does not explore how varying this parameter affects CLMIA's performance on different datasets and model architectures.
- What evidence would resolve it: Experimental results showing CLMIA's performance with different temperature parameter values for each dataset and model architecture would provide insights into the optimal parameter choice for various scenarios.

### Open Question 3
- Question: Can CLMIA be adapted to work with other types of machine learning models beyond image classification, such as natural language processing or recommendation systems?
- Basis in paper: [inferred] The paper focuses on image classification tasks using CNN, VGG-19, and ResNet-18 architectures, but does not explore the applicability of CLMIA to other domains.
- Why unresolved: The paper demonstrates CLMIA's effectiveness on image classification tasks but does not investigate its potential for other machine learning domains that may have different data characteristics and model architectures.
- What evidence would resolve it: Experimental results showing CLMIA's performance on natural language processing or recommendation system tasks, using appropriate model architectures and datasets, would provide insights into its applicability to other domains.

## Limitations
- Core claims about contrastive learning's effectiveness for MIA lack direct corpus support
- Mechanism by which dropout-generated positive samples capture membership information remains theoretical
- Claims of state-of-the-art performance without requiring labeled membership data contradicts common understanding

## Confidence
- **High Confidence**: Experimental methodology is sound with appropriate metrics and multiple datasets/models tested
- **Medium Confidence**: Contrastive learning framework is technically feasible though its specific application to MIA is novel
- **Low Confidence**: Claims of significantly outperforming existing methods without labeled membership data

## Next Checks
1. Test CLMIA on additional datasets (ImageNet, Places365) to verify if performance gains hold beyond CIFAR and STL datasets
2. Perform ablation study by removing contrastive learning component and comparing performance with direct supervised fine-tuning
3. Measure mutual information between dropout-generated samples and membership status to verify dropout preserves membership-relevant features