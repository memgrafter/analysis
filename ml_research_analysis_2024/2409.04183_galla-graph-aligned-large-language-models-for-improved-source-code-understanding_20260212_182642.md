---
ver: rpa2
title: 'GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding'
arxiv_id: '2409.04183'
source_url: https://arxiv.org/abs/2409.04183
tags:
- node
- graph
- code
- type
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of enhancing large language models
  for source code understanding by incorporating structural graph information, such
  as Abstract Syntax Trees (AST) and Data Flow Graphs (DFG), which are typically ignored
  by text-based models. The proposed method, GALLa (Graph Aligned Large Language Models),
  uses graph neural networks and cross-modal alignment to inject structural code information
  into LLMs as an auxiliary task during finetuning.
---

# GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding

## Quick Facts
- arXiv ID: 2409.04183
- Source URL: https://arxiv.org/abs/2409.04183
- Reference count: 35
- Primary result: GALLa improves code understanding by incorporating structural graph information (AST/DFG) into LLMs, achieving up to 36% performance gains on weaker models and 9% on stronger ones, with cross-language generalization.

## Executive Summary
GALLa addresses the challenge of enhancing large language models for source code understanding by incorporating structural graph information that text-based models typically ignore. The method uses graph neural networks to process Abstract Syntax Trees and Data Flow Graphs, then aligns these representations with LLM embeddings through a cross-modal adapter during finetuning. This approach is model-agnostic and task-agnostic, requiring graph data only at training time with no additional inference cost. Experiments with seven LLMs (350M to 14B parameters) across five code tasks demonstrate consistent improvements, including successful generalization to programming languages not present in the training data.

## Method Summary
GALLa enhances LLMs for code understanding through a two-stage training process that incorporates structural graph information. First, a GNN encoder processes ASTs and DFGs extracted from code, and an adapter module projects these graph representations into the LLM's embedding space. During stage 1, the LLM is frozen while the GNN and adapter are trained on Graph2Code reconstruction tasks using 626K code-graph pairs. In stage 2, all components are unfrozen and jointly trained on a mixture of downstream task data and graph alignment tasks (Graph2Code and GraphQA with 770K question-answer pairs). The method requires graph data only during training from an unrelated corpus, with no additional cost at inference time, and demonstrates cross-language generalization by learning universal structural patterns.

## Key Results
- Consistent improvements over baseline models across five code tasks with seven different LLMs
- Performance gains up to 36% on weaker models (350M-1B parameters) and 9% on stronger models like LLaMA3 and Qwen2.5-Coder
- Successful generalization to programming languages not present in graph alignment data (JavaScript, C)
- Model-agnostic approach works across diverse LLM architectures from 350M to 14B parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GALLa improves code understanding by aligning LLM representations with structural graph information without modifying the base LLM architecture.
- Mechanism: GALLa uses a GNN to process ASTs and DFGs, then projects the resulting node representations into the LLM's embedding space via an adapter module. This alignment happens during finetuning as an auxiliary task, allowing the LLM to learn structural semantics while preserving its pretrained language capabilities.
- Core assumption: The adapter can effectively bridge the modality gap between graph-structured code representations and the text-based embedding space of LLMs.
- Evidence anchors:
  - [abstract]: "GALLa utilizes graph neural networks and cross-modal alignment technologies to inject the structural information of code into LLMs as an auxiliary task during finetuning."
  - [section 3.1]: "The adapter's outputs Xg ∈ R^{ng×dlm} are ng embedding vectors, which we dub 'graph tokens'...The LLM's output logits Y ∈ R^{(ng+nt)×dlm} are then used to compute cross-entropy loss with next token prediction"
  - [corpus]: Weak evidence - corpus shows related works like "CGP-Tuning: Structure-Aware Soft Prompt Tuning for Code Vulnerability Detection" but not direct validation of GALLa's specific alignment approach
- Break condition: The adapter fails to map graph features into the LLM embedding space, or the cross-modal alignment introduces noise that degrades performance on downstream tasks.

### Mechanism 2
- Claim: GALLa generalizes structural knowledge across programming languages, improving performance even on tasks involving languages not present in the graph alignment data.
- Mechanism: By aligning to universal structural patterns (UAST) rather than language-specific constructs, the model learns generalizable code semantics that transfer across languages. The graph alignment data from Python and Java provides structural patterns that apply to other languages like JavaScript and C.
- Core assumption: Code structural patterns (like control flow and data dependencies) are sufficiently similar across programming languages that knowledge transfers effectively.
- Evidence anchors:
  - [abstract]: "The method also demonstrates generalization to programming languages not present in the graph alignment data."
  - [section 4.3]: "from Table 3 we observe that they can even improve six of the seven models' performance on tasks in other languages - code summarization in JavaScript, and defect detection in C"
  - [corpus]: Moderate evidence - corpus includes works like "BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning" suggesting graph-based approaches can generalize across domains
- Break condition: Structural patterns differ significantly between source languages (Python/Java) and target languages, or the universal abstraction loses critical language-specific information.

### Mechanism 3
- Claim: GALLa's two-stage training scheme prevents disruption of LLM's pretrained representations while enabling effective graph alignment.
- Mechanism: Stage 1 freezes the LLM and pretrains only the GNN and adapter on Graph2Code reconstruction, creating a stable graph-to-text mapping. Stage 2 unfreezes all components and jointly trains on graph alignment and downstream tasks, allowing the LLM to adapt to structural information without catastrophic forgetting.
- Core assumption: Freezing the LLM during initial graph alignment prevents the adapter from introducing noise that could corrupt the LLM's pretrained representations.
- Evidence anchors:
  - [section 3.2.1]: "Thus, to prevent the newly initialized GNN and adapter from disrupting the LLM's pretrained representations, we fix the LLM's weights in stage 1, and update only the GNN and the adapter."
  - [section 3.2.2]: "In the second stage, we aim to align the LLM's pretrained representations of source code to the structural graphs and deepen their understanding of code structures. In this stage, the LLM is unfrozen, and all three modules are updated together."
  - [corpus]: Limited evidence - corpus doesn't directly validate this specific two-stage approach but shows related work on transfer learning and knowledge preservation
- Break condition: The two-stage approach is too rigid, preventing necessary adaptation of the LLM to structural information, or the adapter learns to bypass the LLM entirely.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their application to code structures
  - Why needed here: Understanding how GNNs process ASTs and DFGs is essential for grasping how structural code information is extracted and represented
  - Quick check question: What's the difference between how GNNs process ASTs (tree structures) versus DFGs (graphs with cycles), and why does this matter for the adapter design?

- Concept: Cross-modal alignment and adapter-based transfer learning
  - Why needed here: The core innovation relies on projecting graph features into LLM embedding space, which requires understanding how cross-modal alignment works in practice
  - Quick check question: How does the cross-attention adapter in GALLa differ from simpler approaches like direct MLP projection, and what are the trade-offs?

- Concept: Universal AST (UAST) and language-agnostic code representation
  - Why needed here: The generalization across programming languages depends on abstracting away language-specific details, which UAST enables
  - Quick check question: Why might language-specific ASTs limit cross-language generalization compared to UAST, and what information might be lost in the abstraction?

## Architecture Onboarding

- Component map: GNN Encoder → Adapter (Cross-attention) → LLM Decoder → Loss computation on text tokens
- Critical path: GNN → Adapter → LLM → Loss computation on text tokens only
  The graph tokens flow through the adapter into the LLM, but gradients only flow through the LLM on text tokens, preserving the LLM's pretrained capabilities.
- Design tradeoffs:
  - Cross-attention vs MLP adapter: Cross-attention allows node-to-node information exchange but is computationally heavier; MLP is simpler but may lose structural relationships
  - Graph type choice: AST provides simpler, more code-surface-aligned information; DFG provides richer structural semantics including loops and data flow
  - Training stage separation: Prevents catastrophic forgetting but may limit flexibility in adaptation
- Failure signatures:
  - No improvement over baseline: Adapter may not be learning useful projections, or GNN representations may be too noisy
  - Performance degradation: Graph alignment may be introducing conflicting signals or the adapter may be disrupting LLM representations
  - Overfitting to graph tasks: Model may focus too much on graph alignment at expense of downstream task performance
- First 3 experiments:
  1. Run GALLa with only Graph2Code task on a small LLM to verify basic functionality and measure improvement over baseline
  2. Test different adapter types (cross-attention vs MLP) on the same task to understand impact on performance
  3. Evaluate cross-language generalization by training on Python/Java graphs and testing on JavaScript/C tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GALLa scale with the size and complexity of the structural graphs (ASTs and DFGs) beyond what was tested in the paper?
- Basis in paper: [inferred] The paper mentions using 626K code-graph pairs for alignment, but does not explore scaling the graph complexity or the volume of graph data.
- Why unresolved: The experiments used a fixed dataset size and graph types (AST and DFG), but did not test the impact of varying graph complexity or data volume on performance.
- What evidence would resolve it: Experiments with larger, more complex graphs or increased graph data volume to measure performance changes.

### Open Question 2
- Question: Can GALLa be extended to support additional types of structural graphs, such as Control Flow Graphs (CFGs) or Call Graphs, and how would this affect downstream task performance?
- Basis in paper: [inferred] The paper focuses on ASTs and DFGs but does not explore other graph types like CFGs or Call Graphs.
- Why unresolved: The study did not test the inclusion of additional graph types, leaving their potential impact on model performance unexplored.
- What evidence would resolve it: Experiments incorporating CFGs or Call Graphs into the graph alignment process and measuring their effect on downstream tasks.

### Open Question 3
- Question: How does GALLa perform when applied to code generation tasks, such as code completion or code synthesis, compared to its performance on understanding tasks?
- Basis in paper: [inferred] The paper evaluates GALLa on understanding and translation tasks but does not explicitly test code generation tasks.
- Why unresolved: The experiments focused on understanding and translation, leaving the model's effectiveness in code generation tasks unverified.
- What evidence would resolve it: Testing GALLa on code generation benchmarks and comparing results with baseline models.

## Limitations
- Cross-language generalization claims rely on inference from experimental results rather than controlled ablation studies specifically designed to test transfer between source and target languages
- The exact contribution of each component (GNN type, adapter design, two-stage training) to overall performance gains is not isolated through systematic ablation studies
- GraphQA component's contribution is not clearly delineated from Graph2Code alignment task, making it difficult to assess relative importance of each graph alignment objective

## Confidence

**High Confidence**: The core mechanism of using GNN to process code graphs and aligning them with LLM embeddings through an adapter is technically sound and well-supported by experimental results. The two-stage training approach is clearly specified and implemented.

**Medium Confidence**: The cross-language generalization claims are supported by experimental evidence but lack controlled studies to definitively prove the transfer mechanism. The specific contribution of each component to performance gains is not fully isolated.

**Low Confidence**: The exact implementation details of GraphQA prompts and the specific hyperparameters for LoRA training of larger models are not fully specified, which could impact reproducibility.

## Next Checks

1. **Component Ablation Study**: Systematically remove or replace individual components (GNN type, adapter design, GraphQA task, two-stage training) to isolate their specific contributions to performance improvements.

2. **Controlled Cross-Language Transfer**: Design targeted experiments that explicitly test the transfer of structural knowledge from source languages (Python/Java) to target languages (JavaScript/C) by controlling for graph alignment data and measuring performance changes.

3. **Scalability Analysis**: Evaluate GALLa's performance on intermediate-sized LLMs (1-5B parameters) to better understand the scaling behavior between the smaller models (350M-1B) and the largest models (14B) where different improvements are observed.