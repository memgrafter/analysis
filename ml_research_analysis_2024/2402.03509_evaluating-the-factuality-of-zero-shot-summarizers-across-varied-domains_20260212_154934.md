---
ver: rpa2
title: Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains
arxiv_id: '2402.03509'
source_url: https://arxiv.org/abs/2402.03509
tags:
- news
- domains
- summaries
- errors
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the factuality of zero-shot summarizers across
  specialized domains, including biomedical articles and legal bills, in addition
  to standard news benchmarks. The authors focus on the prevalence and types of factual
  inconsistencies in model-generated summaries, using domain experts to annotate errors.
---

# Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains

## Quick Facts
- **arXiv ID**: 2402.03509
- **Source URL**: https://arxiv.org/abs/2402.03509
- **Reference count**: 16
- **Primary result**: Zero-shot summarizers show higher factual inconsistency rates in news articles compared to specialized domains, with domain representation in pretraining data correlating with factuality

## Executive Summary
This study evaluates the factuality of zero-shot summarizers across three domains: news, biomedical, and legal. Using domain experts to annotate factual inconsistencies in summaries generated by GPT-3.5 and Flan-T5-XL, the authors find that error rates vary considerably across domains, with news articles showing higher rates of extrinsic factual errors (hallucinations). The study also reveals that the prevalence of articles from a given domain in pretraining data may correlate with the factuality of summaries in that domain. Additionally, existing automatic systems for factual evaluation struggle when applied to niche domains.

## Method Summary
The study uses three datasets: XSUM and CNN-DM (news), PubMed (biomedical), and BillSum (legal), sampling approximately 50 articles from each domain. Zero-shot summaries are generated using GPT-3.5 and Flan-T5-XL with a general prompt ("Article: [article]. Summarize the above article."). Domain experts (linguists for news, attorneys for legal, MDs for biomedical) manually annotate inconsistencies in the summaries and categorize errors into intrinsic, extrinsic factual, extrinsic nonfactual, and extrinsic outdated. The study compares these human annotations with automated factuality metrics including QAFactEval, QuestEval, and SummaC variations.

## Key Results
- Factual inconsistency rates vary across domains, with news showing 9-11% inconsistent sentences versus 1-4% in specialized domains
- Extrinsic factual errors (hallucinations) are more prevalent in news summaries (4-5%) compared to biomedical (1-2%) and legal (0-1%) domains
- Automated factuality metrics show weak correlation with human annotations and struggle on niche domains
- Higher ROUGE-L recall correlates with lower error rates, suggesting extractiveness improves factuality

## Why This Works (Mechanism)

### Mechanism 1: Domain Representation in Pretraining Data
Higher domain representation in pretraining data correlates with lower factuality errors in summaries. When a domain is well-represented in pretraining, the model has more exposure to its specific terminology, style, and factual patterns. This enables it to generate summaries that are more extractive (copying source content) and less prone to hallucinations (introducing unsupported content).

### Mechanism 2: Implicit Knowledge in News Domains
The proportion of extrinsic errors (hallucinations) is higher in news articles compared to specialized domains. News articles often contain information that is implicitly known to the general public and may be present in the model's weights. When summarizing news, the model may introduce such implicit information, leading to extrinsic errors.

### Mechanism 3: Domain-Specific Evaluation Challenges
Existing automatic systems for factual evaluation struggle when applied to niche domains. Automated factuality metrics are typically trained and evaluated on news summarization datasets. When applied to specialized domains like medicine and law, these metrics may not capture the unique characteristics and factual requirements of these domains, leading to lower performance.

## Foundational Learning

- **Concept**: Domain representation in pretraining data
  - **Why needed here**: Understanding the relationship between domain representation and factuality is crucial for explaining the observed differences in error rates across domains.
  - **Quick check question**: How can we measure the representation of a domain in a model's pretraining data?

- **Concept**: Extrinsic vs. intrinsic errors
  - **Why needed here**: Distinguishing between extrinsic (hallucinations) and intrinsic (misrepresentations) errors is essential for characterizing the types of errors observed in different domains.
  - **Quick check question**: What is the difference between extrinsic and intrinsic errors in summarization?

- **Concept**: Automated factuality metrics
  - **Why needed here**: Evaluating the performance of existing automated factuality metrics across domains helps identify the limitations of current evaluation methods and the need for domain-specific metrics.
  - **Quick check question**: What are some common automated factuality metrics used in summarization, and how are they typically evaluated?

## Architecture Onboarding

- **Component map**: Domain expert annotation → Manual inconsistency categorization → Zero-shot summarizer generation → Automated metric evaluation
- **Critical path**: Generate summaries using zero-shot summarizer → Manually annotate summaries for factual consistency → Categorize errors → Analyze error patterns across domains → Evaluate automated metrics on annotated data
- **Design tradeoffs**: Domain expert annotation provides high-quality annotations but is expensive and time-consuming, while automated metrics are faster but may not generalize well to niche domains
- **Failure signatures**: High proportion of extrinsic errors in news summaries, lower performance of automated metrics in niche domains, potential bias in manual annotations due to domain expertise
- **First 3 experiments**:
  1. Evaluate zero-shot summarizer on small sample of news articles and manually annotate summaries for factual consistency
  2. Compare error patterns in news summaries to those in a specialized domain (e.g., medicine) using same manual annotation process
  3. Evaluate performance of automated factuality metrics on annotated data from both news and specialized domains

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the prevalence of news articles in pretraining data causally increase extrinsic factual errors in zero-shot summaries?
- **Basis in paper**: The paper suggests a correlation between domain representation in pretraining data and the likelihood of model hallucinations, but does not establish causation.
- **Why unresolved**: The study uses ROUGE-L recall as a proxy for domain representation and correlation with error types, but does not perform controlled experiments to isolate the effect of pretraining data composition.
- **What evidence would resolve it**: Controlled experiments varying the proportion of domain-specific articles in pretraining data and measuring the resulting error rates in zero-shot summaries.

### Open Question 2
- **Question**: Are existing automated factuality metrics fundamentally limited for niche domains, or can they be adapted through domain-specific training?
- **Basis in paper**: The paper finds that automated metrics like QAFactEval, QuestEval, and SummaC struggle when applied to specialized domains compared to news.
- **Why unresolved**: The study evaluates off-the-shelf versions of these metrics without exploring domain adaptation or fine-tuning approaches.
- **What evidence would resolve it**: Training or adapting existing factuality metrics on domain-specific annotated data and comparing performance to human evaluations.

### Open Question 3
- **Question**: What is the relationship between extractiveness and factuality across different zero-shot summarization models?
- **Basis in paper**: The paper finds that higher 3-gram overlap (extractiveness) correlates with lower error rates, particularly in the PubMed domain.
- **Why unresolved**: The study only examines Spearman correlations and does not investigate whether this relationship holds across different model architectures or prompting strategies.
- **What evidence would resolve it**: Systematic experiments varying extractiveness through different decoding strategies or model architectures while measuring factual consistency.

## Limitations

- The evidence linking factuality patterns to pretraining data representation relies on proxy measures rather than direct analysis of model weights or training data composition
- The study focuses on only two specific models (GPT-3.5 and Flan-T5-XL) and a limited sample size per domain (50 articles), constraining generalizability
- Manual annotation introduces potential subjectivity that isn't fully characterized through inter-annotator agreement metrics

## Confidence

- **High confidence**: The comparative analysis of factual inconsistency rates across domains and error type distributions
- **Medium confidence**: The correlation between domain representation in pretraining data and summary factuality
- **Medium confidence**: The performance limitations of automated factuality metrics on niche domains

## Next Checks

1. Verify the pretraining data representation hypothesis by analyzing model weights or obtaining more direct evidence of domain-specific training data composition
2. Replicate the study with a larger sample size (e.g., 200+ articles per domain) to assess the stability of the observed error patterns
3. Test the same summarization models on additional specialized domains (e.g., scientific, technical) to determine if the observed patterns generalize beyond the three studied domains