---
ver: rpa2
title: Logic Contrastive Reasoning with Lightweight Large Language Model for Math
  Word Problems
arxiv_id: '2409.00131'
source_url: https://arxiv.org/abs/2409.00131
tags:
- reasoning
- similarity
- language
- problem
- mathematical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Logic Contrastive Reasoning (LCR), a method
  that enhances lightweight Large Language Models' mathematical reasoning by leveraging
  retrieval-augmented generation and contrastive learning. LCR measures logical similarity
  between problems using algebraic expression similarity and retrieves reference examples
  that guide the model toward correct reasoning paths.
---

# Logic Contrastive Reasoning with Lightweight Large Language Model for Math Word Problems

## Quick Facts
- arXiv ID: 2409.00131
- Source URL: https://arxiv.org/abs/2409.00131
- Authors: Ding Kai; Ma Zhenguo; Yan Xiaoran
- Reference count: 36
- Key outcome: LCR achieves 15.8% improvement over Chain of Thought on SVAMP and 21.5% on GSM8K datasets for lightweight LLM mathematical reasoning

## Executive Summary
This paper introduces Logic Contrastive Reasoning (LCR), a method that enhances mathematical reasoning in lightweight Large Language Models by combining retrieval-augmented generation with contrastive learning. LCR measures logical similarity between problems using algebraic expression similarity and retrieves reference examples that guide the model toward correct reasoning paths. The approach significantly reduces comprehension and logical errors while improving reasoning accuracy by 15.8% on SVAMP and 21.5% on GSM8K datasets compared to standard Chain of Thought methods.

## Method Summary
LCR improves lightweight LLM mathematical reasoning through a four-component pipeline: problem preprocessing extracts known conditions and structures problems logically; algebraic transformation converts reasoning steps to algebraic expressions; similarity retrieval uses a hybrid semantic-algebraic metric to find reference examples; and contrastive prompting constructs prompts with positive and negative examples for final reasoning. The method employs a weighted combination of semantic similarity (via SentBERT) and algebraic similarity using normalized tree edit distance to retrieve problems with similar logical structures, then guides the model through carefully crafted contrastive prompts containing both correct and incorrect reasoning examples.

## Key Results
- LCR achieves 15.8% improvement over Chain of Thought on SVAMP dataset
- LCR achieves 21.5% improvement over Chain of Thought on GSM8K dataset
- Method validated on both lightweight models (Mistral-7B, LLaMA2-7B) and 175B parameter models, achieving state-of-the-art performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation with algebraic similarity retrieval improves lightweight LLM reasoning accuracy by guiding models toward structurally similar problem-solving logic.
- Mechanism: LCR uses a custom similarity metric that measures logical similarity between problems by transforming reasoning steps into algebraic expressions, then retrieving reference examples that match the algebraic structure of the current problem.
- Core assumption: Mathematical problem similarity is better captured by algebraic expression structure than by semantic similarity alone.
- Evidence anchors: [abstract] "LCR measures logical similarity between problems using algebraic expression similarity and retrieves reference examples that guide the model toward correct reasoning paths."
- Break condition: If algebraic expression similarity fails to capture meaningful problem structure differences, or if retrieved examples are irrelevant despite high similarity scores.

### Mechanism 2
- Claim: Contrastive learning with positive and negative examples improves lightweight LLM reasoning by exposing the model to correct reasoning patterns while explicitly avoiding common errors.
- Mechanism: LCR constructs prompts containing both correct (positive) and incorrect (negative) reasoning examples, retrieved via logical similarity. The model learns to follow the positive examples' logic while avoiding the errors shown in negative examples.
- Core assumption: Lightweight LLMs can learn from contrastive examples without fine-tuning, simply by prompt engineering.
- Evidence anchors: [abstract] "By employing carefully crafted positive and negative example prompts, we guide the model towards adopting sound reasoning logic."
- Break condition: If the model cannot distinguish between correct and incorrect reasoning patterns from examples, or if negative examples reinforce rather than deter incorrect reasoning.

### Mechanism 3
- Claim: Hybrid similarity metric combining semantic and algebraic similarity improves retrieval quality for mathematical problem-solving.
- Mechanism: LCR uses a weighted combination of semantic similarity (via SentBERT) and algebraic similarity to retrieve reference examples, ensuring both contextual understanding and structural alignment.
- Core assumption: Mathematical problems require both semantic understanding (what the problem is about) and logical structure (how to solve it), making hybrid similarity necessary.
- Evidence anchors: [section] "We use the Logical Similarity Function TLS() to retrieve samples from the example set that are similar to the problem to be solved"
- Break condition: If either component dominates the similarity score inappropriately, leading to retrieval of irrelevant examples.

## Foundational Learning

- Concept: Algebraic expression similarity
  - Why needed here: Core to measuring logical similarity between problems for retrieval-augmented generation
  - Quick check question: Can you explain how the normalized tree edit distance is modified to handle commutative and associative properties in mathematical expressions?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: Foundation for understanding how LCR retrieves and uses reference examples to improve reasoning
  - Quick check question: What distinguishes LCR's RAG approach from standard RAG methods in terms of retrieval criteria?

- Concept: Contrastive learning
  - Why needed here: Essential for understanding how positive and negative examples guide the model toward correct reasoning
  - Quick check question: How does LCR's use of contrastive examples differ from standard contrastive learning approaches in NLP?

## Architecture Onboarding

- Component map: Problem → Preprocessing → Algebraic Transformation → Similarity Retrieval → Contrastive Prompt Construction → Final Reasoning
- Critical path: Problem → Preprocessing → Algebraic Transformation → Similarity Retrieval → Contrastive Prompt Construction → Final Reasoning
- Design tradeoffs: The method trades increased computational cost (similarity calculations, multiple model calls) for improved accuracy on lightweight models. The hybrid similarity metric adds complexity but aims for better retrieval quality. The contrastive approach requires careful curation of positive/negative examples.
- Failure signatures: Low accuracy improvements despite retrieval (similarity metric not capturing meaningful structure), inconsistent results across runs (retrieval not deterministic enough), accuracy drops when increasing few-shot examples (overfitting to reference examples).
- First 3 experiments:
  1. Implement basic algebraic similarity metric and test on SVAMP dataset with fixed reference examples to validate retrieval quality
  2. Add hybrid semantic-algebraic similarity and compare retrieval effectiveness against pure semantic or pure algebraic approaches
  3. Implement contrastive prompting with positive/negative examples and measure accuracy improvement over standard CoT on GSM8K dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method's performance scale when applied to models with parameters between the lightweight models (under 10B) and the 175B model tested?
- Basis in paper: [explicit] The paper tests on models under 10B parameters and a 175B model, but does not explore intermediate sizes
- Why unresolved: The paper does not provide performance data for models in the range between lightweight models and the 175B parameter model, leaving uncertainty about how performance scales with model size in this range
- What evidence would resolve it: Experiments testing the LCR method on models with parameters in the 10B-100B range, showing performance trends as model size increases

### Open Question 2
- Question: What is the long-term generalization capability of the LCR method when applied to new mathematical domains or problem types not seen during training?
- Basis in paper: [inferred] The paper demonstrates effectiveness on GSM8K and SVAMP datasets but does not test on new mathematical domains
- Why unresolved: The experiments are limited to specific datasets, and there is no information on how the method performs on entirely new types of mathematical problems or different mathematical domains
- What evidence would resolve it: Testing the LCR method on diverse mathematical problem sets from different domains (e.g., geometry, calculus, real-world applications) to assess generalization capabilities

### Open Question 3
- Question: How does the LCR method perform in terms of computational efficiency compared to baseline methods, particularly in terms of inference time and memory usage?
- Basis in paper: [inferred] While the paper mentions using 4-bit quantization for lightweight models, it does not provide detailed comparisons of computational efficiency or inference speed
- Why unresolved: The paper focuses on accuracy improvements but does not discuss the trade-offs in terms of computational resources, inference speed, or memory usage compared to baseline methods
- What evidence would resolve it: Benchmark studies comparing the LCR method's inference time, memory usage, and overall computational efficiency against baseline methods (CoT, SC, etc.) across different model sizes and hardware configurations

## Limitations
- The method's effectiveness depends heavily on the quality of algebraic expression similarity calculations, which may not generalize well to all mathematical domains
- The hybrid similarity metric requires careful tuning of the α parameter (set to 0.7), and performance could be sensitive to this hyperparameter
- The approach adds computational overhead from similarity calculations and multiple model calls, potentially offsetting benefits for some deployment scenarios

## Confidence
- **High Confidence**: The core mechanism of using algebraic expression similarity for retrieval-augmented generation is well-specified and grounded in mathematical reasoning principles. The experimental results showing 15.8% improvement on SVAMP and 21.5% on GSM8K are clearly reported with appropriate baselines.
- **Medium Confidence**: The effectiveness of contrastive learning through positive/negative example prompts assumes lightweight LLMs can learn from these examples without fine-tuning. While the approach is intuitive, the mechanism by which LLMs distinguish correct from incorrect reasoning patterns from examples isn't fully explained.
- **Low Confidence**: The generalizability of the hybrid similarity metric (combining semantic and algebraic similarity) to problems beyond the tested datasets remains uncertain. The paper doesn't provide ablation studies showing the relative contribution of each similarity component.

## Next Checks
1. **Ablation Study on Similarity Components**: Run experiments comparing pure semantic similarity, pure algebraic similarity, and the hybrid approach to quantify the contribution of each component to overall performance. This will validate whether the hybrid metric provides significant advantages over simpler approaches.
2. **Hyperparameter Sensitivity Analysis**: Systematically vary the α parameter controlling the semantic-algebraic similarity trade-off across the full range [0,1] to identify optimal values for different problem types and assess stability of performance across this range.
3. **Generalization Test on New Dataset**: Apply LCR to a previously unseen mathematical reasoning dataset (such as MAWPS or MathQA) to evaluate whether the algebraic similarity retrieval generalizes beyond the GSM8K and SVAMP datasets, testing the method's robustness across mathematical domains.