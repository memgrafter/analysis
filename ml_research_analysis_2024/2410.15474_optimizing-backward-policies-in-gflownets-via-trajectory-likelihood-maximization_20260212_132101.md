---
ver: rpa2
title: Optimizing Backward Policies in GFlowNets via Trajectory Likelihood Maximization
arxiv_id: '2410.15474'
source_url: https://arxiv.org/abs/2410.15474
tags:
- learning
- backward
- policy
- iterations
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces Trajectory Likelihood Maximization (TLM),
  a method for optimizing backward policies in Generative Flow Networks (GFlowNets)
  by maximizing the likelihood of trajectories sampled from the forward policy. TLM
  alternates between two steps: (1) updating the backward policy to maximize trajectory
  likelihood, and (2) optimizing the forward policy via entropy-regularized reinforcement
  learning.'
---

# Optimizing Backward Policies in GFlowNets via Trajectory Likelihood Maximization

## Quick Facts
- arXiv ID: 2410.15474
- Source URL: https://arxiv.org/abs/2410.15474
- Reference count: 40
- Key outcome: TLM improves backward policy optimization by maximizing trajectory likelihood sampled from forward policy, showing significant gains especially in less structured environments like QM9

## Executive Summary
This paper introduces Trajectory Likelihood Maximization (TLM), a method for optimizing backward policies in Generative Flow Networks (GFlowNets) by maximizing the likelihood of trajectories sampled from the forward policy. TLM alternates between two steps: (1) updating the backward policy to maximize trajectory likelihood, and (2) optimizing the forward policy via entropy-regularized reinforcement learning. This approach is compatible with any existing GFlowNet or soft RL algorithm. Experiments across hypergrid, bit sequence, and molecule design tasks show that TLM consistently improves convergence speed and mode discovery, especially in less structured environments like QM9. TLM represents the first principled backward policy optimization method for soft RL-based GFlowNets, demonstrating significant performance gains and supporting the hypothesis that backward policy optimization benefits less structured tasks.

## Method Summary
TLM optimizes backward policies in GFlowNets through an alternating optimization procedure. In the first step, the backward policy is updated to maximize the log-likelihood of trajectories generated by the forward policy, creating alignment between forward and backward distributions. In the second step, the forward policy is optimized within an entropy-regularized MDP that uses rewards derived from the updated backward policy. This creates a non-stationary soft RL problem that adapts to backward policy updates. The method incorporates stability techniques including lower learning rates for backward updates, target networks for forward policy training, and uniform initialization to ensure stable optimization dynamics. TLM is compatible with any existing GFlowNet or soft RL algorithm and represents the first principled approach to backward policy optimization for soft RL-based GFlowNets.

## Key Results
- TLM consistently improves convergence speed and mode discovery across hypergrid, bit sequence, and molecule design tasks
- Significant performance gains observed in less structured environments like QM9, with minimal improvement on more structured tasks like sEH
- Stability techniques (learning rate decay, target networks) are crucial for successful TLM implementation
- TLM is compatible with multiple base GFlowNet algorithms including DB, TB, SubTB, and MunchausenDQN

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TLM improves backward policy optimization by maximizing trajectory likelihood sampled from the forward policy
- Mechanism: The backward policy is updated to maximize the log-likelihood of trajectories generated by the forward policy, creating a tighter alignment between forward and backward distributions
- Core assumption: The forward policy generates meaningful trajectories that contain information about the reward structure
- Evidence anchors:
  - [abstract] "maximizing the likelihood of trajectories sampled from the forward policy"
  - [section 3] "trajectory likelihood maximization objective: θt+1B ≈ arg minθ Eτ ∼P tF[LTLM(θ; τ)], LTLM(θ; τ) := −Pnτi=1 log PB(si−1|si, θ)"
- Break condition: If the forward policy fails to explore high-reward regions, TLM cannot improve backward policy quality

### Mechanism 2
- Claim: TLM creates a non-stationary soft RL problem that adapts to backward policy updates
- Mechanism: The forward policy is optimized within an entropy-regularized MDP that uses rewards derived from the updated backward policy, creating adaptive learning dynamics
- Core assumption: The connection between GFlowNets and entropy-regularized RL holds even when the backward policy changes
- Evidence anchors:
  - [abstract] "alternates between two steps: (1) updating the backward policy to maximize trajectory likelihood, and (2) optimizing the forward policy via entropy-regularized reinforcement learning"
  - [section 3] "P t+1F ≈ arg maxPF∈ΠF V PFλ=1(s0; rP t+1B)"
- Break condition: If the non-stationary RL algorithm fails to adapt quickly enough, convergence may be slower than with fixed backward policies

### Mechanism 3
- Claim: TLM provides stability through controlled backward policy updates and target networks
- Mechanism: Using lower learning rates for backward updates, target networks for forward policy training, and uniform initialization creates stable optimization dynamics
- Core assumption: Stability techniques are necessary for convergence in non-stationary RL settings
- Evidence anchors:
  - [section 4] "enforcing stability in backward updates, particularly by using a decaying learning rate and a target network, significantly improves convergence in practice"
  - [section A.2] "using a target model and a lower learning rate is crucial"
- Break condition: If stability techniques are removed, training may become unstable or diverge

## Foundational Learning

- Concept: Trajectory Balance Constraint
  - Why needed here: TLM is fundamentally about satisfying this constraint through joint optimization of forward and backward policies
  - Quick check question: What happens if the trajectory balance constraint is violated in GFlowNets?

- Concept: Entropy-Regularized Reinforcement Learning
  - Why needed here: TLM relies on the equivalence between GFlowNet training and entropy-regularized RL when the backward policy is fixed
  - Quick check question: How does entropy regularization affect exploration in standard RL problems?

- Concept: KL Divergence Minimization
  - Why needed here: Both the trajectory likelihood maximization and the forward policy optimization steps can be viewed as minimizing KL divergences between trajectory distributions
  - Quick check question: What is the relationship between KL divergence minimization and maximum likelihood estimation?

## Architecture Onboarding

- Component map: Forward policy network (PF) -> Backward policy network (PB) -> Target network for PB -> Optional replay buffer
- Critical path: Forward policy generates trajectories → Backward policy is updated via trajectory likelihood maximization → Forward policy is updated using entropy-regularized RL with rewards from updated backward policy → Repeat
- Design tradeoffs: TLM trades increased computational complexity (two separate network updates per iteration) for potentially faster convergence and better mode discovery. The stability techniques add overhead but are crucial for convergence.
- Failure signatures: If TLM is failing, common symptoms include: backward policy learning rate too high causing instability, forward policy not adapting to updated backward policy rewards, or insufficient exploration by the forward policy leading to poor backward policy updates.
- First 3 experiments:
  1. Implement TLM on a simple hypergrid environment (e.g., 2x2x2) with a known reward structure to verify basic functionality
  2. Compare TLM against uniform backward policy baseline on bit sequence generation task with small n and k
  3. Test stability techniques (learning rate decay, target network) by disabling them one at a time on QM9 task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TLM scale with the complexity and structure of the state space in GFlowNet tasks, and what theoretical insights can be derived about the relationship between environment structure and the benefits of backward policy optimization?
- Basis in paper: [explicit] The paper explicitly states that TLM shows significant improvement on QM9 (less structured) but less improvement or even degradation on sEH (more structured), supporting the hypothesis that TLM benefits less structured environments.
- Why unresolved: The paper provides empirical evidence but does not offer a rigorous theoretical analysis explaining why environment structure affects the performance of TLM. The relationship between structure and backward policy optimization benefits remains a hypothesis.
- What evidence would resolve it: A theoretical framework quantifying the impact of environment structure (e.g., tree-like vs. graph-like) on backward policy optimization performance, validated with systematic experiments across environments with varying degrees of structure.

### Open Question 2
- Question: What is the precise impact of stability techniques (such as learning rate scheduling, target networks, and uniform initialization) on the convergence and performance of TLM, and can these techniques be optimized or combined in a principled way?
- Basis in paper: [explicit] The paper mentions stability techniques motivated by Theorem 3.1 and shows ablation results indicating their importance, but does not provide a comprehensive analysis of their individual or combined effects.
- Why unresolved: While the paper identifies key stability techniques, it does not systematically explore their interactions or determine optimal configurations for different tasks and environments.
- What evidence would resolve it: A systematic ablation study varying each stability technique independently and in combination, across multiple tasks, to determine their relative contributions and optimal configurations.

### Open Question 3
- Question: How sensitive is TLM to the choice of the base GFlowNet algorithm (e.g., DB, TB, SubTB, MunchausenDQN), and are there specific characteristics of these algorithms that make them more or less compatible with TLM?
- Basis in paper: [explicit] The paper evaluates TLM with multiple base algorithms and observes varying degrees of improvement, but does not analyze the underlying reasons for these differences or provide guidance on selecting compatible algorithm pairs.
- Why unresolved: The paper demonstrates that TLM's effectiveness varies with the base algorithm but does not investigate the algorithmic properties (e.g., credit assignment mechanisms, exploration strategies) that influence this compatibility.
- What evidence would resolve it: An analysis identifying the algorithmic characteristics that enhance or hinder TLM compatibility, supported by experiments varying these characteristics systematically across base algorithms.

## Limitations

- The stability mechanisms required (lower learning rates, target networks) suggest TLM may be sensitive to hyperparameter tuning
- Experimental validation focuses primarily on synthetic and molecular design tasks, leaving uncertainty about performance on more complex, real-world distributions
- The hypothesis about why TLM works better for less structured tasks is intuitive but not rigorously tested

## Confidence

**High Confidence**: The core mechanism of alternating between trajectory likelihood maximization and entropy-regularized RL is well-defined and theoretically grounded. The relationship between TLM and existing GFlowNet frameworks is clearly established.

**Medium Confidence**: The empirical improvements shown across different tasks are convincing, but the magnitude of gains varies significantly by task type. The claim that TLM particularly benefits less structured environments is supported by QM9 results but requires more diverse testing.

**Low Confidence**: The paper's hypothesis about why TLM works better for less structured tasks is intuitive but not rigorously tested. The claim that TLM represents "the first principled backward policy optimization method" for soft RL-based GFlowNets needs broader comparison with alternative approaches.

## Next Checks

1. **Ablation Study**: Remove the stability mechanisms (target network, learning rate decay) one at a time to quantify their contribution to TLM's performance and identify the minimal requirements for stable training.

2. **Structured vs. Unstructured Test**: Design a controlled experiment with environments that vary in structural complexity while keeping other factors constant, to directly test the hypothesis that TLM benefits less structured tasks more.

3. **Scaling Analysis**: Test TLM on progressively larger state spaces and more complex reward structures to evaluate whether the performance gains scale with problem difficulty or if there are fundamental limitations to the approach.