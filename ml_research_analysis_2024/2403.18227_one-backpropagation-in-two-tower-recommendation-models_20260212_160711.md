---
ver: rpa2
title: One Backpropagation in Two Tower Recommendation Models
arxiv_id: '2403.18227'
source_url: https://arxiv.org/abs/2403.18227
tags:
- items
- user
- backpropagation
- recommendation
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper challenges the equal training assumption in two tower
  recommendation models, which use symmetric backpropagation for both user and item
  encoding updates. It proposes One Backpropagation (OneBP), an asymmetric training
  strategy that applies gradient backpropagation only to the item encoding tower,
  while updating user encodings through a moving-aggregation strategy.
---

# One Backpropagation in Two Tower Recommendation Models

## Quick Facts
- arXiv ID: 2403.18227
- Source URL: https://arxiv.org/abs/2403.18227
- Authors: Erjia Chen; Bang Wang
- Reference count: 40
- One-line primary result: Proposes asymmetric backpropagation for two-tower recommendation models, achieving 5.57%-6.72% improvement in recommendation metrics while reducing computation time by 6.72%-17.5%.

## Executive Summary
This paper challenges the conventional symmetric backpropagation approach in two-tower recommendation models, proposing an asymmetric training strategy called One Backpropagation (OneBP). The method applies gradient backpropagation only to the item encoding tower while updating user encodings through a moving-aggregation strategy. The authors argue that user interests are diverse and distributed across multiple latent item types, while batch-based gradient updates tend to be dominated by majority item types. Experiments on four public datasets demonstrate that OneBP outperforms state-of-the-art models with significant improvements in recommendation performance metrics and computational efficiency.

## Method Summary
The paper introduces One Backpropagation (OneBP), an asymmetric training strategy for two-tower recommendation models in one-class collaborative filtering settings. The method uses simple projection matrices for user and item encoding, random negative sampling, and InfoNCE loss. Unlike traditional symmetric backpropagation that updates both user and item encoders, OneBP computes gradients only for item encodings while updating user encodings through a moving-aggregation formula: u_i^(t+1) = βu_i^(t) + (1-β)v_j^(t+1). This approach treats users and items differently, leveraging the insight that items have intrinsic attributes that can be learned as latent types, and that user interests are diverse. The method is evaluated on four datasets (MovieLens-100k, MovieLens-1M, Gowalla, and Yelp2018) and shows improvements of 5.57%-6.72% in Precision, Recall, F1, and NDCG metrics while reducing computation time by 6.72%-17.5%.

## Key Results
- Achieves 5.57%-6.72% improvement in Precision, Recall, F1, and NDCG metrics compared to state-of-the-art models
- Reduces computation time by 6.72%-17.5% per epoch, with savings becoming more significant on larger datasets
- Outperforms traditional two-tower models and other competitors like MF, NGCF, and LightGCN across all four tested datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Asymmetric backpropagation improves user representation diversity by avoiding gradient domination from majority-item types
- Mechanism: Traditional two-tower models apply symmetric backpropagation to both user and item encoders. In each batch, the gradient direction is dominated by the most frequent item types a user interacts with, causing the user representation to be biased toward those types. OneBP removes this gradient for users and instead uses a moving-aggregation strategy that blends the previous user encoding with the current interacted item encoding. This allows minority-item types to contribute equally to the user representation.
- Core assumption: User interests are diverse and distributed across multiple latent item types, and batch-based gradient updates bias toward majority types
- Evidence anchors:
  - [abstract] "We also argue that a user's interests could be diverse, embodied in that his/her interacted items normally are distributed in different latent types. We note that in the batch-based training phase, the gradient direction would be dominated by those items with a same type."
  - [section] "Fig. 4 illustrates the clustering phenomenon based on items' encodings. One cluster can be regarded as one latent type of items."
- Break condition: If user interests are not diverse or if item type clusters are not well-separated, the asymmetric update may not provide significant benefit

### Mechanism 2
- Claim: Decoupling user updates from negative sampling reduces the impact of false negative samples
- Mechanism: In contrastive learning for OCCF, negative samples may include items the user would actually like but hasn't interacted with yet (false negatives). Traditional two-tower models update user representations based on the contrastive loss, which includes these false negatives, potentially pushing user representations away from truly relevant items. OneBP updates users only using positive interactions via moving-aggregation, completely decoupling user updates from negative sampling.
- Core assumption: False negatives in OCCF settings can significantly distort user representations when included in gradient updates
- Evidence anchors:
  - [abstract] "We next take a close comparison on the traditional TwoBP and our OneBP updating strategy... the interacted items by a user in the training dataset are largely distributed in one cluster, so-called majority items; While few are distributed in other clusters, so-called minority items"
  - [section] "In contrast, our OneBP only applies his/her interacted items' encodings to update a user encoding, which can effectively decouple the impact of false negative samples."
- Break condition: If negative sampling is extremely accurate and false negatives are negligible, the decoupling benefit diminishes

### Mechanism 3
- Claim: Asymmetric backpropagation reduces computational overhead by eliminating user-side gradient computation
- Mechanism: Traditional two-tower models compute gradients for both user and item encoders, doubling the backpropagation computation. OneBP computes gradients only for item encodings, significantly reducing computation time per epoch. This efficiency gain becomes more pronounced in larger datasets.
- Core assumption: Gradient computation for user encodings contributes significantly to training time and can be eliminated without harming performance
- Evidence anchors:
  - [abstract] "Experiments on four public datasets validate its superiority over representative state-of-the-art models in terms of large improvement margins on recommendation performance, as well as its computation efficiency."
  - [section] "Table 2 also compares the computation time per epoch on the four datasets, where our OneBP requires the least ones. The savings of computation time become more significant in larger datasets."
- Break condition: If the user encoder architecture is extremely simple or if computation is not a bottleneck, the efficiency gain may be less critical

## Foundational Learning

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: The paper uses InfoNCE loss for training the two-tower model, which requires understanding how positive and negative samples are used to pull similar items closer and push dissimilar items apart
  - Quick check question: What is the mathematical form of InfoNCE loss and how does it differ from BPR loss?

- Concept: Moving-aggregation update strategy
  - Why needed here: OneBP uses a moving-aggregation strategy to update user encodings, which is crucial to understanding how user representations evolve without gradient backpropagation
  - Quick check question: How does the moving-aggregation formula (u^(t+1)_i = βu^(t)_i + (1-β)v^(t+1)_j) balance between previous and current information?

- Concept: Latent type clustering in item representations
  - Why needed here: The paper argues that items can be clustered into latent types based on their learned representations, which is fundamental to understanding why asymmetric updates help with diverse user interests
  - Quick check question: How would you visualize and validate that items cluster into distinct latent types in the embedding space?

## Architecture Onboarding

- Component map:
  - User encoder: Simple projection matrix W_u (M × k)
  - Item encoder: Simple projection matrix W_v (N × k)
  - Negative sampler: Random negative sampling
  - Loss function: InfoNCE loss with dot product similarity
  - Backpropagation strategy: Asymmetric - gradients only for items, moving-aggregation for users

- Critical path:
  1. Forward pass: User and item embeddings computed via projection matrices
  2. Negative sampling: Random selection of N_s uninteracted items
  3. Loss computation: InfoNCE loss with dot product similarity
  4. Backward pass: Gradients computed only for item embeddings
  5. Parameter update: Item embeddings updated via gradients, user embeddings updated via moving-aggregation

- Design tradeoffs:
  - Simplicity vs. sophistication: Uses simple projection matrices instead of complex neural encoders
  - Random vs. sophisticated negative sampling: Uses random negatives for speed vs. curated negatives for quality
  - Asymmetric vs. symmetric updates: Asymmetric updates for diversity vs. symmetric updates for balanced learning

- Failure signatures:
  - Poor performance on datasets with homogeneous user interests (no diversity to capture)
  - Instability when β is too large (overweighting new item information)
  - Suboptimal results when item type clusters are not well-separated

- First 3 experiments:
  1. Ablation study: Compare OneBP vs. traditional TwoBP on a small dataset to validate the asymmetric update benefit
  2. Hyperparameter sweep: Test different values of β (1-β from 10^-1 to 10^-6) to find optimal balance
  3. Dataset scaling: Measure computation time reduction on increasingly large datasets to validate efficiency claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed OneBP approach perform on large-scale datasets with millions of users and items?
- Basis in paper: [explicit] The paper experiments on four public datasets, including MovieLens-100k, MovieLens-1M, Gowalla, and Yelp2018, but does not explore the performance on larger datasets
- Why unresolved: The paper does not provide evidence of the scalability and effectiveness of OneBP on larger datasets, which is crucial for real-world applications
- What evidence would resolve it: Experimental results on larger datasets with millions of users and items would provide evidence of the scalability and effectiveness of OneBP in real-world scenarios

### Open Question 2
- Question: How does the OneBP approach handle cold-start users or items?
- Basis in paper: [inferred] The paper does not explicitly address the cold-start problem, which is a common challenge in recommendation systems
- Why unresolved: The paper does not provide any insights into how OneBP can handle users or items with limited or no interaction history
- What evidence would resolve it: Experimental results or theoretical analysis on how OneBP performs for cold-start users or items would provide insights into its robustness and generalization capabilities

### Open Question 3
- Question: How does the OneBP approach compare to other state-of-the-art recommendation algorithms that use advanced neural encoders and sophisticated negative sampling techniques?
- Basis in paper: [explicit] The paper compares OneBP to various state-of-the-art algorithms, but does not explore the performance of OneBP when combined with advanced neural encoders and sophisticated negative sampling techniques
- Why unresolved: The paper does not provide evidence of the potential improvements in recommendation performance when OneBP is combined with more advanced techniques
- What evidence would resolve it: Experimental results comparing the performance of OneBP with and without advanced neural encoders and sophisticated negative sampling techniques would provide insights into the potential improvements in recommendation performance

## Limitations
- Limited to four specific datasets, raising questions about generalization across diverse recommendation domains
- Does not address cold-start scenarios for new users or items with limited interaction history
- Hyperparameter sensitivity, particularly for the moving-aggregation parameter β, is not thoroughly explored

## Confidence
- High confidence: The computational efficiency improvement claim is well-supported by empirical timing results across multiple datasets
- Medium confidence: The recommendation performance improvements are demonstrated but could be influenced by specific dataset characteristics and hyperparameter choices
- Low confidence: The theoretical explanation of why asymmetric updates improve user diversity representation needs more rigorous validation

## Next Checks
1. Hyperparameter robustness test: Systematically vary the β parameter across multiple orders of magnitude (10^-1 to 10^-6) to identify the sensitivity of performance to this critical hyperparameter
2. Cross-domain generalization: Apply OneBP to datasets from different recommendation domains (e.g., music, e-commerce, news) to validate its effectiveness beyond the tested domains
3. Cold-start scenario evaluation: Design experiments specifically testing OneBP's performance with new users/items having 1-5 interactions to assess its practical deployment limitations