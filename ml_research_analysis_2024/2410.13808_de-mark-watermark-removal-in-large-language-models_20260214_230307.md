---
ver: rpa2
title: 'De-mark: Watermark Removal in Large Language Models'
arxiv_id: '2410.13808'
source_url: https://arxiv.org/abs/2410.13808
tags:
- watermark
- n-gram
- token
- tokens
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DE-MARK, a framework for removing n-gram-based
  watermarks from large language models. The core method uses a random selection probing
  strategy to identify watermark parameters (strength, red-green token lists, and
  prefix n-gram length) without requiring knowledge of the underlying hash function.
---

# De-mark: Watermark Removal in Large Language Models

## Quick Facts
- arXiv ID: 2410.13808
- Source URL: https://arxiv.org/abs/2410.13808
- Reference count: 40
- Key outcome: DE-MARK reduces true positive watermark detection rates from over 60% to below 20% while maintaining generation quality

## Executive Summary
DE-MARK presents a novel framework for removing n-gram-based watermarks from large language models without requiring knowledge of the underlying hash function. The approach uses a random selection probing strategy to identify watermark parameters including strength, red-green token lists, and prefix n-gram length. Experiments demonstrate significant reduction in watermark detection rates on popular models like Llama3 and Mistral while preserving generation quality, with theoretical guarantees on distribution preservation after removal.

## Method Summary
DE-MARK employs a five-algorithm framework to reverse-engineer and remove n-gram watermarks from LLMs. The core approach uses random selection probing to estimate relative probability ratios between token pairs, then iteratively identifies the watermark's prefix n-gram length h by detecting consistency rate drops, estimates the watermark strength δ through probability ratio analysis, and identifies the red-green token lists. Watermark removal is achieved by applying inverse weighting based on the estimated parameters. The method operates in both gray-box (with top-k probabilities) and black-box settings, requiring multiple queries to the watermarked model for accurate parameter estimation.

## Key Results
- Reduces true positive detection rates from over 60% to below 20% across multiple datasets and models
- Maintains generation quality with minimal GPT score degradation (0.96 vs 0.97 for watermark-free models)
- Provides theoretical guarantees on distribution preservation after watermark removal

## Why This Works (Mechanism)

### Mechanism 1: Prefix n-gram length identification
The algorithm iteratively computes token scores for decreasing context lengths. When context length equals the watermark's n-gram length h, scores change significantly because the watermark key depends on h previous tokens, manifesting as a drop in consistency rate between scores at h and h-1.

### Mechanism 2: Watermark strength estimation
The algorithm identifies token pairs where one has highly positive scores (likely green) and another has highly negative scores (likely red). The relative probability ratio between these pairs estimates δ, as green tokens have logits increased by δ while red tokens remain unchanged.

### Mechanism 3: Red-green token list identification
After estimating δ, the algorithm computes adjusted relative probability ratios between all token pairs and aggregates these to compute final scores for each token. Tokens with positive final scores are classified as green, while those with negative scores are classified as red.

## Foundational Learning

- **Probability ratios and log-odds**: DE-MARK relies heavily on computing relative probability ratios between tokens to detect watermark patterns. Quick check: Why does the algorithm use log(P(Ti|x,Ti,Tj)/P(Tj|x,Ti,Tj)) instead of just P(Ti|x,Ti,Tj)/P(Tj|x,Ti,Tj)?

- **Statistical hypothesis testing**: The watermark detection relies on statistical tests to determine if text is watermarked. Quick check: What statistical test is used to determine if text is watermarked in the n-gram watermarking scheme?

- **Token scoring and aggregation**: DE-MARK computes token scores by aggregating relative probability ratios across multiple token pairs. Quick check: How does the algorithm ensure that token scores are robust to noise in the probability estimates?

## Architecture Onboarding

- **Component map**: Random Selection Probing -> Relative Probability Ratio Calculation -> Token Score Computation -> Prefix n-gram Length Identification -> Watermark Strength Estimation -> Green List Identification -> Watermark Removal

- **Critical path**: Identify prefix n-gram length h → Identify watermark strength δ → Identify watermark green list → Apply watermark removal formula

- **Design tradeoffs**: Trades accuracy for computational cost by using multiple queries to estimate probabilities, with hyperparameter choices (α1, α2, β, γ) affecting sensitivity and specificity of watermark detection

- **Failure signatures**: Incorrect parameter identification (h, δ, or red-green lists) leads to ineffective watermark removal; diagnose by checking intermediate outputs of each algorithm

- **First 3 experiments**:
  1. Test Algorithm 1 on simple watermarking scheme with known n-gram length to verify correct identification of h
  2. Test Algorithm 2 on watermarking scheme with known strength to verify accurate estimation of δ
  3. Test Algorithm 3 on watermarking scheme with known red-green lists to verify correct identification of lists

## Open Questions the Paper Calls Out

### Open Question 1
How does DE-MARK perform on black-box watermark removal without access to top-k probabilities? The paper mentions black-box capabilities but provides limited experimental validation and performance metrics for this setting.

### Open Question 2
What is the computational overhead and query efficiency of DE-MARK when applied to large-scale models? While efficiency constraints are acknowledged, the paper lacks concrete measurements of query counts, processing time, or resource usage across different model sizes.

### Open Question 3
How does DE-MARK handle cases where watermarking parameters are partially known or guessed incorrectly? The theoretical analysis assumes perfect parameter estimation, but doesn't validate robustness to estimation errors or examine performance degradation with imperfect parameter knowledge.

## Limitations
- Assumes knowledge of watermarking scheme's general structure (n-gram based with red-green lists)
- Requires multiple queries to watermarked models, introducing computational overhead and privacy concerns
- Evaluation limited to specific n-gram watermarking schemes with fixed parameters, not validated against adaptive countermeasures

## Confidence

**High Confidence**: Core algorithmic approach for identifying watermark parameters through probability ratio analysis is mathematically sound with clearly presented experimental results.

**Medium Confidence**: Theoretical guarantees on distribution preservation rely on assumptions about original and watermarked distributions; practical implications for downstream tasks remain underexplored.

**Low Confidence**: Claims of robustness across different watermarking configurations and model architectures are not empirically validated beyond specific n-gram schemes with fixed parameters.

## Next Checks

1. **Parameter Sensitivity Analysis**: Systematically vary watermarking parameters (prefix n-gram length h from 2-5, strength δ from 0.5-3) and measure accuracy of DE-MARK's parameter identification across the full range of practical configurations.

2. **Downstream Task Performance**: Evaluate impact of watermark removal on task-specific performance metrics beyond general generation quality, testing on benchmarks like question answering, summarization, or code generation.

3. **Adaptive Watermark Resistance**: Design an adaptive watermarking scheme that modifies parameters based on detection attempts, and test whether DE-MARK can still successfully remove the watermark to validate robustness against countermeasures.