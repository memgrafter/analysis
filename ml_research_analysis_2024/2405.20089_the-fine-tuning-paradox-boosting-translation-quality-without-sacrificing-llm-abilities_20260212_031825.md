---
ver: rpa2
title: 'The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing
  LLM Abilities'
arxiv_id: '2405.20089'
source_url: https://arxiv.org/abs/2405.20089
tags:
- data
- translation
- fine-tuning
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Fine-tuning LLMs for machine translation improves overall translation\
  \ quality but causes degradation in key LLM abilities like formality steering, technical\
  \ translation via few-shot examples, and document-level translation. The study evaluates\
  \ LLaMA and Falcon models (7B\u201365B parameters) across 6 translation directions,\
  \ showing that larger fine-tuning datasets worsen these degradations."
---

# The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities

## Quick Facts
- **arXiv ID**: 2405.20089
- **Source URL**: https://arxiv.org/abs/2405.20089
- **Reference count**: 18
- **Primary result**: Fine-tuning improves translation quality but degrades LLM abilities; adding monolingual data preserves abilities while enhancing translation.

## Executive Summary
This study reveals a critical trade-off in fine-tuning large language models (LLMs) for machine translation: while translation quality improves, specialized LLM abilities like formality steering, technical translation via few-shot examples, document-level translation, and non-literal idiomatic translation degrade. The research systematically evaluates LLaMA and Falcon models (7B-65B parameters) across 6 translation directions, showing that larger fine-tuning datasets worsen this degradation. A novel solution involves incorporating monolingual data during fine-tuning, which preserves these abilities while simultaneously improving overall translation quality. The study also introduces IdiomsInCtx-MT, a new dataset for evaluating idiomatic translation quality.

## Method Summary
The researchers fine-tuned LLaMA and Falcon models (7B-65B parameters) on parallel translation data from WMT17-20 (89K examples) and filtered OPUS datasets (up to 1.4M examples). Fine-tuning used AdamW optimizer with cosine learning rate scheduler, batch size 128, learning rate 2e-5 for 1 epoch. For LLaMA-65B, QLoRA with 8-bit quantization was employed. Models were evaluated on WMT22, CoCoA-MT, Law/Medical/TICO-19, ctxpro, and the novel IdiomsInCtx-MT dataset using COMET scores, formality accuracy, technical domain COMET, document-level contextualization accuracy, and idiomatic translation metrics (LitTER, MWEScore).

## Key Results
- Fine-tuning improves general translation quality (COMET scores) but degrades LLM-specific abilities including formality steering, technical translation via few-shot examples, and document-level translation
- Larger fine-tuning datasets (up to 1.4M examples) worsen degradation of LLM abilities, with negative correlation (ρ = -0.55, p < 0.001) between dataset size and ability preservation
- Incorporating monolingual data during fine-tuning mitigates ability degradation (reducing formality steering degradation from 0.1 to 0.025-0.007) while enhancing translation quality
- Fine-tuning on parallel data alone produces less literal translations but degrades other LLM-specific abilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fine-tuning on parallel data improves general translation quality but degrades specialized LLM abilities.
- **Mechanism**: The model adapts to maximize general translation metrics (COMET) at the cost of fine-grained control and contextual reasoning, leading to catastrophic forgetting of non-literal translation and formality control.
- **Core assumption**: The training objective and data distribution prioritize literal accuracy over stylistic or contextual nuances.
- **Evidence anchors**: Abstract states "While fine-tuning improves the general translation quality of LLMs, several abilities degrade." Section 3.2 shows "fine-tuning on only 18K examples results in a decline of this ability... even though German-English COMET on WMT22 continues to improve up to 36K examples."
- **Break condition**: If the fine-tuning dataset includes balanced examples of both literal and non-literal translation, or if the loss function explicitly rewards stylistic control.

### Mechanism 2
- **Claim**: Including monolingual data in fine-tuning mitigates ability degradation.
- **Mechanism**: Monolingual data preserves pre-training behaviors (e.g., context awareness, stylistic control) by maintaining exposure to natural language distributions while still adapting to translation tasks.
- **Core assumption**: The model's emergent abilities are rooted in its pre-training on monolingual corpora and can be reinforced during fine-tuning.
- **Evidence anchors**: Abstract states "We show that by including monolingual data as part of the fine-tuning data we can maintain the abilities while simultaneously enhancing overall translation quality." Section 4.2 shows "Incorporating monolingual data during fine-tuning enhances performance... mitigates this degradation, reducing it to just 0.025 for formal and 0.007 for informal steering."
- **Break condition**: If monolingual data is too domain-specific or low-quality, it may introduce noise without preserving abilities.

### Mechanism 3
- **Claim**: Larger fine-tuning datasets exacerbate degradation of LLM-specific abilities.
- **Mechanism**: Extended exposure to parallel-only data shifts the model's distribution toward literal, sentence-level translation, suppressing document-level and stylistic reasoning.
- **Core assumption**: The model's ability to generalize across contexts and styles is fragile and can be overwritten by large-scale parallel fine-tuning.
- **Evidence anchors**: Abstract notes "On the other hand, we observe that the model produces less literal translations after fine-tuning on parallel data." Section 3.2 reports "The decline can be summarized by a negative correlation between accuracy and the size of the dataset used for fine-tuning (ρ = −0.55, p < 0.001)"
- **Break condition**: If the fine-tuning dataset is curated to include diverse, non-literal examples, or if the model architecture includes mechanisms to protect learned abilities.

## Foundational Learning

- **Concept: Catastrophic forgetting in continual learning**
  - **Why needed here**: Explains why fine-tuning on new tasks (translation) can overwrite previously learned abilities (formality control, context reasoning).
  - **Quick check question**: What happens to a model's performance on task A when it is fine-tuned on task B without any preservation strategy?

- **Concept: Domain adaptation via few-shot learning**
  - **Why needed here**: The paper evaluates whether fine-tuning impairs the model's ability to adapt to new domains using minimal examples.
  - **Quick check question**: How does few-shot performance differ between base and fine-tuned models on technical domains?

- **Concept: Idiomatic translation and non-literalness**
  - **Why needed here**: One of the key LLM advantages being measured is the ability to produce non-literal translations, especially for idioms.
  - **Quick check question**: How can we measure whether a translation is literal or captures the intended meaning of an idiom?

## Architecture Onboarding

- **Component map**: Pre-trained LLM (7B-65B parameters) -> Fine-tuning dataset (parallel + optional monolingual) -> Evaluation suite (COMET, formality accuracy, idiom metrics, document-level accuracy) -> Hyperparameter tuning (batch size, learning rate, epochs) -> Inference pipeline (beam search, few-shot prompting)

- **Critical path**: 1) Load base model and tokenizer, 2) Prepare parallel (and optionally monolingual) data, 3) Fine-tune with AdamW + cosine LR scheduler, 4) Evaluate on WMT22, CoCoA-MT, IdiomsInCtx-MT, ctxpro, 5) Compare base vs fine-tuned performance across metrics

- **Design tradeoffs**:
  - Monolingual vs parallel data ratio: more monolingual preserves abilities but may slow general translation gains
  - Fine-tuning dataset size: larger improves general quality but risks greater ability degradation
  - Context window size: larger allows better document-level translation but increases compute cost

- **Failure signatures**:
  - Sharp drop in formality accuracy despite improved COMET
  - Decreased performance on technical domains with few-shot examples
  - Lower animacy contextualization accuracy on document-level input
  - Overfitting to training domain, poor generalization

- **First 3 experiments**:
  1. Fine-tune LLaMA-7B on 89K parallel data, evaluate WMT22 and formality steering accuracy
  2. Fine-tune LLaMA-7B on 89K parallel + 89K monolingual data, compare to experiment 1
  3. Vary fine-tuning dataset size (18K, 36K, 89K, 178K) and measure degradation of few-shot technical translation ability

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the degradation of LLM abilities during fine-tuning follow a predictable pattern that could be used to develop a mathematical model for catastrophic forgetting?
  - **Basis in paper**: [inferred] The paper observes consistent negative correlations between dataset size and degradation of abilities like formality steering, technical translation, and document-level translation across multiple model scales and architectures.
  - **Why unresolved**: While correlations are identified, the paper does not explore whether these degradations follow a predictable mathematical pattern or could be modeled to predict degradation based on dataset characteristics.
  - **What evidence would resolve it**: A systematic study varying dataset characteristics (size, domain, similarity to pretraining data) while measuring ability degradation, followed by mathematical modeling of the relationship between these variables.

- **Open Question 2**: Would fine-tuning strategies that preserve LLM abilities perform differently on truly multilingual LLMs (trained on diverse multilingual data from the start) compared to English-centric models like LLaMA?
  - **Basis in paper**: [explicit] The authors note their method of mixing monolingual and parallel data is tailored for English-centric LLMs and suggest that future work should explore fine-tuning strategies for LLMs designed with multilingual data in mind.
  - **Why unresolved**: The paper only tests their fine-tuning approach on English-centric models and acknowledges this limitation, leaving open whether the same strategies would work for truly multilingual LLMs.
  - **What evidence would resolve it**: Replicating the fine-tuning experiments with the same strategies on multilingual LLMs (like those mentioned in the paper such as PaLM 2 or PolyLM) and comparing the preservation of abilities.

- **Open Question 3**: What is the optimal ratio of monolingual to parallel data during fine-tuning for maximizing both translation quality and preservation of LLM abilities?
  - **Basis in paper**: [explicit] The authors test a 1:1 ratio of monolingual to parallel data but do not explore whether this is optimal or how different ratios might affect the trade-off between translation quality and ability preservation.
  - **Why unresolved**: The paper only tests one ratio (1:1) and shows it works better than pure parallel fine-tuning, but does not systematically explore the parameter space of different ratios.
  - **What evidence would resolve it**: A comprehensive ablation study testing multiple ratios of monolingual to parallel data during fine-tuning, measuring both translation quality and preservation of various LLM abilities to identify optimal trade-offs.

## Limitations

- The study tested only 6 translation directions, which may not capture the full spectrum of degradation patterns across different language pairs and domains.
- Document-level translation evaluation relies on a single benchmark (ctxpro), which may not fully represent real-world document translation complexity.
- The study doesn't explore the long-term stability of preserved abilities after fine-tuning, leaving questions about whether improvements are temporary or sustained.

## Confidence

- **High confidence**: The core finding that fine-tuning improves general translation quality while degrading LLM abilities (formality steering, technical translation via few-shot examples, document-level translation) is well-supported by multiple evaluation metrics across different model sizes and datasets.
- **Medium confidence**: The claim that incorporating monolingual data mitigates ability degradation while improving translation quality is supported by experimental results, but the optimal ratio of monolingual to parallel data remains unclear and may vary by language pair and domain.
- **Low confidence**: The assertion that larger fine-tuning datasets exacerbate degradation is based on correlation analysis (ρ = -0.55) across a limited range of dataset sizes, and the mechanism for why this occurs is speculative rather than empirically validated.

## Next Checks

1. **Dataset size scaling experiment**: Systematically vary fine-tuning dataset sizes across a broader range (e.g., 5K, 20K, 50K, 100K, 200K examples) for multiple language pairs to establish more robust correlation patterns and identify the critical threshold where degradation accelerates.

2. **Monolingual data ratio optimization**: Conduct a grid search over different monolingual-to-parallel data ratios (e.g., 0:1, 0.25:1, 0.5:1, 1:1, 2:1) to determine the optimal balance that maximizes both translation quality and ability preservation for each language pair and domain.

3. **Long-term stability test**: After fine-tuning, evaluate models periodically (e.g., after 1 week, 1 month, 3 months) on all ability metrics to determine whether preserved abilities degrade over time or remain stable, and whether fine-tuning with monolingual data provides lasting protection against catastrophic forgetting.