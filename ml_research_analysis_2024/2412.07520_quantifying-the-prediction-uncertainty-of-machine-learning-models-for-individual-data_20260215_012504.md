---
ver: rpa2
title: Quantifying the Prediction Uncertainty of Machine Learning Models for Individual
  Data
arxiv_id: '2412.07520'
source_url: https://arxiv.org/abs/2412.07520
tags: []
core_contribution: This thesis develops the predictive normalized maximum likelihood
  (pNML) framework for quantifying prediction uncertainty in machine learning models,
  particularly for individual data settings where no assumptions are made about data
  distributions. The core method idea centers on designing pNML learners for linear
  regression and neural networks, where the learner competes with a reference "genie"
  that knows the true test label.
---

# Quantifying the Prediction Uncertainty of Machine Learning Models for Individual Data

## Quick Facts
- arXiv ID: 2412.07520
- Source URL: https://arxiv.org/abs/2412.07520
- Reference count: 40
- One-line primary result: pNML regret provides a theoretically justified confidence measure without distributional assumptions, achieving up to +15.2% AUROC improvement in OOD detection and 33.8% fewer labeled samples in active learning

## Executive Summary
This thesis develops the predictive normalized maximum likelihood (pNML) framework for quantifying prediction uncertainty in machine learning models, particularly for individual data settings where no assumptions are made about data distributions. The core method involves designing pNML learners for linear regression and neural networks, where the learner competes with a reference "genie" that knows the true test label. The pNML regret serves as a measure of prediction uncertainty, with low regret indicating reliable predictions and high regret flagging potential errors or out-of-distribution samples.

The framework is applied to three main tasks: out-of-distribution detection (achieving up to +15.2% AUROC improvement on 74 benchmarks), adversarial robustness (up to 8.8% accuracy gain against PGD attacks), and active learning (requiring 22.8% fewer labeled samples on MNIST). The approach provides a theoretically justified confidence measure without requiring distributional assumptions or additional data, making it particularly valuable for safety-critical applications where uncertainty quantification is essential.

## Method Summary
The pNML framework computes prediction uncertainty by having the learner compete with a genie that knows the true test label, using the pNML regret as the uncertainty measure. For linear regression, the method derives analytical expressions for pNML regret, showing it is low when test samples lie in subspaces spanned by large eigenvalues of training data. For neural networks, pNML regret is computed on top of pretrained DNN embeddings, treating the last layer as a single-layer network. The regret is then used as a confidence score for downstream tasks including OOD detection, adversarial defense, and active learning. The approach requires no distributional assumptions and works directly on individual test samples, making it suitable for real-world deployment where data distributions are unknown or shifting.

## Key Results
- Achieved up to +15.2% AUROC improvement in out-of-distribution detection across 74 benchmarks
- Demonstrated 8.8% accuracy gain against PGD adversarial attacks using Adversarial pNML defense
- Reduced active learning sample requirements by 22.8% on MNIST, 11.0% on EMNIST, and 33.8% on CIFAR10 compared to leading methods
- Provided theoretical generalization bounds for over-parameterized linear regression matching double-descent behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: pNML regret quantifies prediction uncertainty without distributional assumptions.
- Mechanism: pNML computes log-normalization factor of genie predictions over all possible test labels; high regret indicates unreliable predictions, low regret indicates reliable ones.
- Core assumption: Test label is unknown but can be any value; no generative model assumed.
- Evidence anchors:
  - [abstract]: "pNML regret serves as a measure of prediction uncertainty, with low regret indicating reliable predictions and high regret flagging potential errors or out-of-distribution samples."
  - [section]: "The pNML probability assignment and regret are qpNML(y|x) = pˆθ(DN;x,y)(y|x) / ∑y′∈Y pˆθ(DN;x,y′)(y′|x), Γ = log ∑y′∈Y pˆθ(DN;x,y′)(y′|x)."
  - [corpus]: Weak evidence; no direct citations to pNML regret quantification in neighbors.

### Mechanism 2
- Claim: Over-parameterized linear regression generalizes when test samples lie in subspaces spanned by large eigenvalues of training data correlation matrix.
- Mechanism: pNML regret is low when x lies in large eigenvalue subspace; regret upper bound derived for minimum norm solution matches this behavior.
- Core assumption: Training data matrix XN is known; SVD decomposition available.
- Evidence anchors:
  - [section]: "the pNML regret is minimal when test samples lie in subspaces spanned by large eigenvalues of training data."
  - [section]: "Γ = logK = log 1 + 1/N ∑M i=1 (x⊤ui)² / η²i ."
  - [corpus]: No direct evidence in neighbors; mechanism is original to this work.

### Mechanism 3
- Claim: LpNML with luckiness function improves prediction by shifting toward zero for samples in small eigenvalue subspace.
- Mechanism: LpNML uses luckiness function proportional to model norm; prediction deviates from Ridge ERM toward zero when test sample is in small eigenvalue subspace.
- Core assumption: Ridge regression hypothesis class with sphere constraint; luckiness function is Gaussian prior.
- Evidence anchors:
  - [abstract]: "LpNML prediction differs from the ridge regression: When the test sample lies within the subspace associated with the small eigenvalues... the prediction is shifted toward 0."
  - [section]: "ˆµLpNML = λKλ ˆθ⊤λ Pλx / (1 + λx⊤P²λx); when x lies in smallest eigenvalue subspace, LpNML prediction is shifted to 0."
  - [corpus]: Weak evidence; no direct citations to LpNML in neighbors.

## Foundational Learning

- Concept: Linear regression with minimum norm solution
  - Why needed here: Over-parameterized regression requires minimum norm solution to avoid infinite norm; pNML regret analysis depends on MN solution properties.
  - Quick check question: What is the Moore-Penrose inverse formula for over-parameterized case where M > N?

- Concept: Eigenvalue decomposition of training data matrix
  - Why needed here: pNML regret depends on test sample's alignment with eigenvectors; large eigenvalue subspace indicates learnable region.
  - Quick check question: How does the SVD of training matrix XN relate to the pNML regret formula?

- Concept: Neural network feature extraction and last layer analysis
  - Why needed here: pNML can be applied to pretrained DNN by treating last layer as single-layer NN; regret computed on embeddings.
  - Quick check question: How does MC-Dropout approximate the posterior distribution for deep individual active learning?

## Architecture Onboarding

- Component map: Data preprocessing -> Feature extraction and normalization -> SVD decomposition -> pNML regret computation -> Confidence scoring -> Downstream applications

- Critical path: 1. Extract training embeddings and compute correlation matrix 2. Perform SVD decomposition to obtain eigenvectors/eigenvalues 3. For each test sample, compute x⊤g term and pNML regret 4. Use regret as confidence score for downstream tasks

- Design tradeoffs:
  - Computational cost vs accuracy: Full SVD vs approximate methods
  - Hypothesis class size: Constrained vs unconstrained pNML
  - Regularization strength: λ affects LpNML behavior in small eigenvalue subspace

- Failure signatures:
  - High regret for all test samples: Indicates poor generalization or inappropriate hypothesis class
  - Regret insensitive to test sample: May indicate uniform eigenvalue spectrum
  - Computational overflow: Large hypothesis classes without constraints

- First 3 experiments:
  1. Linear regression on synthetic polynomial data: Compare pNML regret across different polynomial degrees
  2. OOD detection on CIFAR10: Apply pNML regret to pretrained DenseNet and evaluate AUROC
  3. Active learning on MNIST: Compare DIAL vs random sampling in presence of OOD samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the pNML framework be extended to other hypothesis classes beyond linear regression and neural networks, such as logistic regression, decision trees, or support vector machines?
- Basis in paper: [explicit] The authors state in the future research section that "A straight forward research direction is to calculate the pNML for additional hypothesis class. Linear regression with L1 regularization, decision trees, and logistic regression are all popular machine learning models that are widely used in many applications, but their generalization performance is not yet well understood."
- Why unresolved: While the pNML framework has been successfully applied to linear regression and neural networks, the authors have not yet explored its applicability to other common machine learning models. Each model has its own unique properties and challenges that would need to be addressed when extending the pNML framework.
- What evidence would resolve it: Developing and evaluating the pNML framework for additional hypothesis classes, such as logistic regression, decision trees, or support vector machines, would provide evidence for its broader applicability. Comparing the performance of these models using pNML to traditional methods would demonstrate the potential benefits and limitations of the approach.

### Open Question 2
- Question: How does the choice of the hypothesis class affect the performance of the pNML framework in the "twice universality" approach?
- Basis in paper: [explicit] The authors mention that "When dealing with real world data, it can be difficult to determine what hypothesis class should be used. In fact, the selection of the hypothesis class itself can have a significant impact on the accuracy and effectiveness of the model." They suggest the "twice universality" approach as a potential solution, where the pNML algorithm is executed over a number of hierarchical model families.
- Why unresolved: While the "twice universality" approach is proposed as a potential solution, the authors do not provide specific guidance on how to select the appropriate hypothesis classes or how the choice of hypothesis class affects the performance of the pNML framework. This is an important consideration for practical applications.
- What evidence would resolve it: Conducting experiments that compare the performance of the pNML framework using different sets of hypothesis classes in the "twice universality" approach would provide insights into the impact of hypothesis class selection. Analyzing the trade-offs between model complexity, generalization performance, and computational efficiency would help guide the choice of hypothesis classes for specific applications.

### Open Question 3
- Question: Can the pNML framework be used to detect adversarial examples in a more general sense, beyond the specific defense mechanism proposed in Chapter 6?
- Basis in paper: [explicit] The authors mention in the future research section that "The pNML regret can form an adversarial attack detector." They also propose the Adversarial pNML scheme as a specific defense mechanism against adversarial attacks.
- Why unresolved: While the Adversarial pNML scheme is proposed as a specific defense mechanism, it is unclear whether the pNML regret can be used more generally to detect adversarial examples. This would require understanding the relationship between the pNML regret and the properties of adversarial examples.
- What evidence would resolve it: Conducting experiments that evaluate the effectiveness of the pNML regret as an adversarial attack detector on various datasets and attack scenarios would provide evidence for its general applicability. Comparing the performance of the pNML regret to other adversarial detection methods would help assess its strengths and limitations.

## Limitations
- pNML requires computing normalization factors over all possible test labels, which becomes computationally expensive for continuous output spaces or high-dimensional problems
- Framework assumes training data is representative and test samples lie in similar subspaces, which may not hold for extreme distribution shifts
- Extending pNML to deeper architectures requires treating them as single-layer networks on top of pretrained embeddings, potentially limiting performance

## Confidence
- High confidence: pNML regret as uncertainty measure (strong theoretical foundation, validated on multiple OOD detection benchmarks)
- Medium confidence: Generalization bounds for over-parameterized linear regression (theoretical derivation solid, but empirical validation limited to synthetic and UCI datasets)
- Medium confidence: Deep pNML for OOD detection (good performance on standard benchmarks, but relies on pretrained DNN features)
- Low confidence: LpNML improvements over standard pNML (mechanism is theoretically justified but empirical gains are modest and specific to ridge regression context)

## Next Checks
1. Evaluate pNML-based OOD detection under covariate shift versus semantic shift scenarios to understand when regret reliably indicates uncertainty
2. Measure computational complexity of pNML regret computation for different hypothesis class sizes and develop approximations for high-dimensional problems
3. Test whether pNML regret correlates with actual prediction error across different data distributions, particularly for heavy-tailed or multimodal distributions where pNML assumptions may break down