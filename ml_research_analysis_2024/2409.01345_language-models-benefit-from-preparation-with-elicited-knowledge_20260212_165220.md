---
ver: rpa2
title: Language Models Benefit from Preparation with Elicited Knowledge
arxiv_id: '2409.01345'
source_url: https://arxiv.org/abs/2409.01345
tags:
- answer
- question
- made
- magnifying
- material
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PREP, a dual-instance prompting method for
  language models that improves performance on question answering tasks requiring
  factual knowledge rather than step-by-step reasoning. The method uses one model
  instance to elicit relevant information and a second to answer the question using
  that information, leveraging instruction-following capabilities.
---

# Language Models Benefit from Preparation with Elicited Knowledge

## Quick Facts
- arXiv ID: 2409.01345
- Source URL: https://arxiv.org/abs/2409.01345
- Authors: Jiacan Yu; Hannah An; Lenhart K. Schubert
- Reference count: 40
- Primary result: Dual-instance prompting method (PREP) improves QA accuracy by 2-4% across multiple datasets and models

## Executive Summary
This paper introduces PREP, a dual-instance prompting method for language models that improves performance on question answering tasks requiring factual knowledge. The method uses one model instance to elicit relevant information and a second to answer the question using that information, leveraging instruction-following capabilities. Tested on a curated dataset of 100 binary-choice questions about shared materials between artifacts, as well as CommonsenseQA, StrategyQA, and OpenBookQA, PREP achieved consistently higher accuracy than baselines including zero-shot Chain of Thought, Plan-and-Solve, and direct prompting.

## Method Summary
PREP uses two separate instances of language models: LM1 elicits relevant knowledge by listing specific facts related to the question, while LM2 answers using the provided information. This design aims to better utilize the model's instruction-following capability by explicitly providing relevant facts rather than requiring the model to infer them from context. The method is tested on three models (Phi-3, Aya 23, Command-R) across multiple QA datasets, with temperature set to 0 for deterministic outputs.

## Key Results
- PREP consistently outperforms baselines by 2-4% average accuracy across all tested datasets
- The method shows general applicability without requiring domain-specific prompt engineering
- Performance improvements are consistent across different model sizes (14B-35B parameters)
- PREP is particularly effective for questions requiring factual knowledge rather than step-by-step reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PREP improves accuracy by leveraging the instruction-following capability of language models through explicit knowledge elicitation before reasoning.
- Mechanism: The dual-instance setup separates knowledge elicitation (LM1) from question answering (LM2), ensuring that relevant facts are explicitly provided to the answering model rather than requiring it to infer them from context alone.
- Core assumption: Language models have a stronger ability to follow instructions when information is explicitly provided rather than inferred from surrounding context.
- Evidence anchors:
  - [abstract] "This design is intended to make better use of the LM's instruction-following capability."
  - [section 3] "It involves copying the information provided by LM1 and sending it to LM2 as part of the user's instruction, rather than asking the same instance of LM to answer the question after it retrieves the information."
  - [corpus] Weak evidence - corpus contains papers on data preparation and multi-agent frameworks but no direct evidence about instruction-following improvements.
- Break condition: The mechanism breaks if the knowledge elicited by LM1 is inaccurate, incomplete, or irrelevant to the question, or if the instruction-following capability of the model is insufficient for the task complexity.

### Mechanism 2
- Claim: PREP reduces cognitive load on the answering model by pre-loading relevant knowledge, allowing it to focus on reasoning rather than information retrieval.
- Mechanism: By having LM1 generate relevant facts first, LM2 receives a knowledge-primed context that eliminates the need to search for or infer relevant information during the reasoning process.
- Core assumption: The cognitive load of retrieving relevant knowledge interferes with the model's ability to perform reasoning tasks effectively.
- Evidence anchors:
  - [section 4.2] "The results demonstrate the efficacy of our dual-instance prompting approach" and comparison with methods that don't separate knowledge elicitation.
  - [section 5] "We observe that repeating information in the context negatively impacts accuracy. This demonstrates the necessity of creating an additional instance of the LM."
  - [corpus] Weak evidence - corpus contains papers on mathematical reasoning and question-answering but no direct evidence about cognitive load reduction.
- Break condition: The mechanism breaks if the pre-loaded knowledge actually introduces noise or if the model's reasoning capacity is not the bottleneck in performance.

### Mechanism 3
- Claim: PREP provides consistent performance improvements across different model sizes and question types by standardizing the knowledge elicitation process.
- Mechanism: The method's independence from domain-specific prompt engineering means it can be applied uniformly across various QA tasks and model architectures, leading to consistent accuracy gains.
- Core assumption: Standardization of knowledge elicitation leads to more reliable performance than task-specific prompt engineering approaches.
- Evidence anchors:
  - [abstract] "PREP is applicable across various QA tasks without domain-specific prompt engineering."
  - [section 4.2] Testing on three different models (Phi-3, Aya 23, Command-R) and multiple datasets shows consistent improvements.
  - [section 5] "The average accuracy of our dual-instance methods (averaged across the three models) consistently matches or exceeds the accuracy of all other tested methods."
  - [corpus] Weak evidence - corpus contains papers on multi-agent frameworks and data preparation but no direct evidence about cross-task consistency.
- Break condition: The mechanism breaks if certain question types or model architectures respond poorly to standardized knowledge elicitation, or if task-specific knowledge is actually required for optimal performance.

## Foundational Learning

- Concept: Chain of Thought (CoT) reasoning
  - Why needed here: Understanding the baseline method being compared against, which uses step-by-step reasoning for complex tasks.
  - Quick check question: What is the key difference between zero-shot CoT and PREP in terms of their approach to knowledge utilization?

- Concept: Instruction fine-tuning in language models
  - Why needed here: The method specifically relies on models that have been instruction fine-tuned, which affects how they process and follow user instructions.
  - Quick check question: How does instruction fine-tuning differ from standard pre-training in terms of model behavior and capabilities?

- Concept: Binary-choice question answering
  - Why needed here: The evaluation method uses binary-choice questions, which have specific characteristics that affect how knowledge is elicited and applied.
  - Quick check question: What are the advantages and disadvantages of binary-choice questions compared to multiple-choice or open-ended questions in evaluating language model knowledge?

## Architecture Onboarding

- Component map: LM1 -> Knowledge Elicitation -> Knowledge Transfer -> LM2 -> Answer Generation
- Critical path: Receive question → Send to LM1 with knowledge elicitation prompt → Receive and validate knowledge → Format knowledge with original question → Send to LM2 with question answering prompt → Receive and validate answer → Return final answer
- Design tradeoffs:
  - Single vs. dual instances: Dual instances provide better performance but require more computational resources
  - Information copying vs. reference: Copying ensures consistency but increases context length
  - Knowledge elicitation vs. direct prompting: Elicitation provides more relevant information but adds an extra step
- Failure signatures:
  - LM1 fails to provide relevant knowledge: Knowledge transfer layer receives incomplete or irrelevant information
  - LM2 fails to answer despite relevant knowledge: Indicates reasoning capacity limitations
  - Performance degradation with information copying: Suggests context length or attention mechanism limitations
- First 3 experiments:
  1. Test PREP on a simple binary-choice question with known correct answer to validate basic functionality
  2. Compare PREP performance against zero-shot CoT on the same question set to measure baseline improvement
  3. Test PREP across all three model sizes (Phi-3, Aya 23, Command-R) on a subset of questions to validate cross-model consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does PREP work effectively with models that have not undergone instruction fine-tuning?
- Basis in paper: [explicit] The authors state "Our method is specifically designed for and has been tested on instruction fine-tuned language models. We are uncertain about its effectiveness on models that have not undergone instruction fine-tuning."
- Why unresolved: The authors explicitly acknowledge uncertainty about performance on non-instruction-tuned models but did not test this scenario.
- What evidence would resolve it: Testing PREP on a range of base models (without instruction fine-tuning) across multiple QA tasks and comparing performance to baseline prompting methods.

### Open Question 2
- Question: What is the optimal prompt length and specificity for the information elicitation step (LM1) to maximize PREP's effectiveness?
- Basis in paper: [inferred] The authors use general prompts like "Please list specific facts that seem most relevant to answering the question" without exploring variations in prompt specificity or length.
- Why unresolved: The paper uses a single prompt format for LM1 without exploring whether more specific or differently structured prompts might improve performance.
- What evidence would resolve it: Systematic experimentation with different prompt formulations (varying specificity, length, and structure) to identify optimal prompt design for the elicitation step.

### Open Question 3
- Question: How does PREP's performance scale with model size beyond the medium-sized LMs tested (14B-35B parameters)?
- Basis in paper: [explicit] The authors note "Larger LMs (70B+) are not in the scope of this study because of our hardware constraints" and their experiments were limited to medium-sized models.
- Why unresolved: The authors acknowledge hardware limitations prevented testing larger models, leaving the scaling behavior of PREP with model size unknown.
- What evidence would resolve it: Testing PREP on models ranging from small (7B) to frontier scale (70B+) to determine whether performance improvements continue, plateau, or reverse at larger scales.

## Limitations

- The method requires two instances of language models, increasing computational costs and latency
- Performance on non-instruction-tuned models is unknown, limiting generalizability
- The dual-instance approach may not scale well to very large models (70B+) due to hardware constraints

## Confidence

**High confidence**: PREP consistently improves accuracy over baselines (2-4% average improvement across all tested datasets and models).

**Medium confidence**: The three proposed mechanisms explain why PREP works, though direct empirical evidence for each mechanism varies in strength.

**Low confidence**: Claims about PREP's performance on tasks requiring step-by-step reasoning or its behavior with significantly smaller or larger models than those tested.

## Next Checks

1. **Mechanism isolation test**: Run PREP with LM1 providing irrelevant knowledge (random facts) to measure how much accuracy depends on the quality of knowledge elicitation versus the dual-instance architecture itself.

2. **Resource efficiency analysis**: Measure and compare computational resources (tokens processed, latency) between PREP and baselines to quantify the cost of the accuracy improvements and test the cognitive load reduction claim.

3. **Edge case robustness**: Test PREP on questions requiring temporal reasoning, negation, or complex inference patterns to determine whether the method's performance gains hold for tasks beyond simple factual recall.