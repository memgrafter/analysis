---
ver: rpa2
title: 'DOMAINEVAL: An Auto-Constructed Benchmark for Multi-Domain Code Generation'
arxiv_id: '2408.13204'
source_url: https://arxiv.org/abs/2408.13204
tags:
- code
- llms
- domain
- generation
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents DOMAIN EVAL, a multi-domain code generation
  benchmark designed to evaluate Large Language Models (LLMs) across six specific
  domains: computation, network, basic operations, system, visualization, and cryptography.
  Unlike existing benchmarks focused on common coding tasks, DOMAIN EVAL targets domain-specific
  coding challenges.'
---

# DOMAINEVAL: An Auto-Constructed Benchmark for Multi-Domain Code Generation

## Quick Facts
- arXiv ID: 2408.13204
- Source URL: https://arxiv.org/abs/2408.13204
- Authors: Qiming Zhu; Jialun Cao; Yaojie Lu; Hongyu Lin; Xianpei Han; Le Sun; Shing-Chi Cheung
- Reference count: 38
- Key outcome: Automated pipeline creates DOMAIN EVAL benchmark revealing significant LLM performance gaps across six coding domains

## Executive Summary
DOMAINEVAL is a multi-domain code generation benchmark constructed through an automated pipeline to evaluate Large Language Models across six specific domains: computation, network, basic operations, system, visualization, and cryptography. Unlike existing benchmarks focused on common coding tasks, DOMAIN EVAL targets domain-specific coding challenges. The benchmark comprises 2,454 subjects with 5,892 test cases, revealing that LLMs perform well on computation tasks but struggle significantly with cryptography and system tasks, with performance gaps reaching 68.94% for some models.

## Method Summary
The authors propose a fully automated pipeline to construct DOMAIN EVAL by collecting code from GitHub repositories, matching functions with corresponding test cases, filtering for executable and significant code (20-100 lines), and generating natural language instructions using an LLM. The benchmark evaluates 12 representative LLMs across six domains, revealing significant performance disparities. While generating more samples improves overall performance, domain biases persist or even increase, highlighting the need for specialized training strategies to enhance LLMs' capabilities in specific vertical domains.

## Key Results
- LLMs achieve 82.44% Pass@1 on computation tasks but only 33.08% on cryptography and 37.50% on system tasks
- Performance gaps between domains reach 68.94% for some models
- Automated benchmark construction successfully identifies domain-specific challenges
- Increasing sample generation improves overall performance but domain biases persist

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its focus on domain-specific coding challenges rather than general programming tasks, revealing fundamental limitations in LLM capabilities across different vertical domains.

## Foundational Learning
- **Automated benchmark construction** - Why needed: Enables large-scale, diverse evaluation without manual curation; Quick check: Pipeline reproducibility across different GitHub repositories
- **Domain-specific task representation** - Why needed: Reveals specialized LLM weaknesses not apparent in general benchmarks; Quick check: Task complexity comparison across domains
- **Pass@1 metric evaluation** - Why needed: Provides standardized performance measurement across models and domains; Quick check: Metric consistency with human programmer baselines

## Architecture Onboarding

Component map: GitHub repository collection -> Function-test case matching -> Code filtering -> LLM instruction generation -> Model evaluation

Critical path: Automated pipeline ensures consistent benchmark construction and evaluation across all domains, enabling systematic performance comparison.

Design tradeoffs: Automated construction enables scale but introduces potential quality issues through LLM-generated instructions; manual verification limited to 12.2% of samples.

Failure signatures: Poor performance in cryptography and system domains may indicate either fundamental LLM limitations or instruction quality issues rather than true capability gaps.

First experiments:
1. Manual review of instruction quality across all domains to assess impact on performance metrics
2. Human programmer baseline evaluation on DOMAIN EVAL tasks
3. Fine-tuning experiments on domain-specific subsets to test generalization capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on LLM-generated instructions with limited manual verification (12.2% of samples)
- Filtering criteria may exclude important domain-specific patterns, particularly in cryptography and system domains
- Results may reflect benchmark-specific challenges rather than fundamental LLM limitations

## Confidence

High: Domain-specific performance measurements based on systematic evaluation of 12 models across 2,454 subjects with verifiable pass@1 metrics

Medium: Automated benchmark construction methodology with detailed pipeline but unverified instruction quality for most samples

Medium: Generalization of domain-specific challenges with variable sample sizes and potential selection bias

## Next Checks

1. Conduct comprehensive manual review of instruction quality across all domains to assess whether poor performance stems from instruction clarity issues

2. Evaluate a subset of DOMAIN EVAL tasks on human programmers to establish baseline performance and determine if LLM gaps reflect genuine capability limitations

3. Test whether fine-tuning on domain-specific subsets of DOMAIN EVAL improves cross-domain generalization, distinguishing between knowledge transfer and domain-specific memorization effects