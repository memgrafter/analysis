---
ver: rpa2
title: Privacy-preserving Universal Adversarial Defense for Black-box Models
arxiv_id: '2408.10647'
source_url: https://arxiv.org/abs/2408.10647
tags:
- certified
- defense
- adversarial
- target
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of defending against adversarial
  attacks on black-box deep neural networks (DNNs), where the defender lacks access
  to the target model's parameters or architecture. The authors propose DUCD (Distillation-based
  Universal Certified Defense), a novel method that generates a surrogate model through
  knowledge distillation using query-based techniques, then applies randomized smoothing
  with optimized noise selection to create a robust certified classifier.
---

# Privacy-preserving Universal Adversarial Defense for Black-box Models

## Quick Facts
- arXiv ID: 2408.10647
- Source URL: https://arxiv.org/abs/2408.10647
- Reference count: 40
- Outperforms existing black-box defenses by >20% in certified accuracy

## Executive Summary
This paper addresses the challenge of defending against adversarial attacks on black-box deep neural networks (DNNs), where the defender lacks access to the target model's parameters or architecture. The authors propose DUCD (Distillation-based Universal Certified Defense), a novel method that generates a surrogate model through knowledge distillation using query-based techniques, then applies randomized smoothing with optimized noise selection to create a robust certified classifier. DUCD achieves universal defense (model-agnostic and norm-universal), privacy preservation, and high certified radius.

## Method Summary
The method involves two main stages: (1) knowledge distillation using query access to create a white-box surrogate model that approximates the black-box model's behavior, and (2) applying randomized smoothing with optimized noise selection to this surrogate model. The distillation process iteratively trains the surrogate model using the black-box model's outputs as targets while preserving data privacy. The randomized smoothing component uses Gaussian noise with optimized parameters to maximize certified radius across all ℓp norms. The defense is evaluated on MNIST, SVHN, and CIFAR10 datasets, showing significant improvements over existing black-box defenses.

## Key Results
- Achieves >20% improvement in certified accuracy over existing black-box defenses
- Matches white-box defense performance while maintaining privacy preservation
- Reduces membership inference attack success rates to near-random guessing levels
- Provides certified robustness across all ℓp norms with optimized noise selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation transfers the black-box model's predictive behavior to a surrogate white-box model while preserving privacy.
- Mechanism: The surrogate model is trained using the black-box model's outputs (logits) as targets, iteratively aligning its predictions with the target model's without accessing internal parameters.
- Core assumption: The black-box model's decision boundaries can be approximated sufficiently well through output queries alone.
- Evidence anchors:
  - [abstract] "Our approach involves distilling the target model by querying it with data, creating a white-box surrogate while preserving data privacy."
  - [section] "The surrogate model is iteratively trained on the dataset, using the predicted output of the black-box model as the target."
  - [corpus] No direct evidence for distillation-based black-box defenses found in corpus.
- Break condition: If the black-box model's decision boundaries are too complex or non-smooth to approximate through queries, or if query budget is insufficient.

### Mechanism 2
- Claim: Randomized smoothing with optimized noise selection provides certified robustness across all ℓp norms.
- Mechanism: Gaussian noise is applied during inference, and the noise distribution parameters are optimized via grid search to maximize certified radius while maintaining accuracy.
- Core assumption: The noise distribution can be tuned to provide better certified radii than standard Gaussian noise for all ℓp norms.
- Evidence anchors:
  - [abstract] "We further enhance this surrogate model using a certified defense based on randomized smoothing and optimized noise selection, enabling robust defense against a broad range of adversarial attacks."
  - [section] "To maximize the certified radius, we consider the noise PDF as a variable to be optimized."
  - [corpus] Limited evidence - only general references to randomized smoothing exist, no specific optimization methods found.
- Break condition: If the optimized noise distribution fails to provide meaningful improvements over standard Gaussian noise for certain ℓp norms.

### Mechanism 3
- Claim: Scalar and direction optimization maximizes the certified radius by finding the perturbation that exactly touches the robustness boundary.
- Mechanism: Two-stage optimization - first scales the perturbation to the robustness boundary using binary search, then optimizes the perturbation direction using PSO to minimize ℓp norm.
- Core assumption: The robustness boundary can be precisely located through iterative optimization procedures.
- Evidence anchors:
  - [section] "We utilize two-stage optimization [26] to derive precise certified radii."
  - [section] "The first stage utilizes scalar optimization to find a scaling factor λ that adjusts the perturbation δ to the robust boundary."
  - [corpus] No direct evidence for two-stage optimization in black-box settings found in corpus.
- Break condition: If the optimization procedures fail to converge or if the computational cost becomes prohibitive for real-time applications.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: Enables transfer of black-box model behavior to a white-box surrogate without accessing internal parameters.
  - Quick check question: How does the student model's loss function ensure it mimics the teacher model's outputs rather than learning from ground truth labels?

- Concept: Randomized Smoothing
  - Why needed here: Provides certified robustness guarantees by making predictions based on noisy input samples.
  - Quick check question: What is the relationship between noise variance σ and certified radius R in randomized smoothing?

- Concept: Zero-Order Optimization
  - Why needed here: Enables gradient estimation for black-box models through query-based techniques.
  - Quick check question: How does the query complexity of zero-order optimization scale with input dimensionality?

## Architecture Onboarding

- Component map: Query Interface -> Distillation Engine -> Noise Optimizer -> Smoothing Module -> Certification Engine
- Critical path: Query → Distillation → Noise Optimization → Smoothing → Certification
- Design tradeoffs:
  - Query budget vs. surrogate model accuracy
  - Noise variance vs. certified radius vs. accuracy
  - Optimization precision vs. computational cost
- Failure signatures:
  - Surrogate model accuracy drops significantly below target model
  - Certified radius becomes too small to be practical
  - Query budget exhausted before model convergence
- First 3 experiments:
  1. Train surrogate model with varying query budgets on MNIST to establish accuracy scaling
  2. Test randomized smoothing with different noise distributions on simple linear models
  3. Implement two-stage optimization on synthetic data to verify radius maximization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can certified defenses be made more effective against ℓ∞ norm adversarial attacks, given that randomized smoothing currently shows suboptimal performance in this domain?
- Basis in paper: [explicit] The paper explicitly states that while DUCD performs well against ℓ1 and ℓ2 attacks, it shows suboptimal performance against ℓ∞ attacks, which remains a critical issue for future exploration and resolution in certified defenses.
- Why unresolved: Despite theoretical guarantees, randomized smoothing fails to effectively defend against ℓ∞ norm attacks in practice. The paper acknowledges this as a significant limitation that requires further research.
- What evidence would resolve it: Successful development and empirical validation of a certified defense method that demonstrates robust performance against ℓ∞ norm attacks, with quantitative metrics showing comparable effectiveness to its performance on other ℓp norms.

### Open Question 2
- Question: What is the optimal noise distribution for maximizing certified radius across different ℓp norms, and how does this vary with input data characteristics?
- Basis in paper: [explicit] The paper discusses that Gaussian noise generally yields the largest certified radii for most ℓp norms and pA values, but the optimal noise distribution may vary depending on the specific ℓp norm and input characteristics.
- Why unresolved: While the paper explores different noise distributions and their impact on certified radius, it doesn't provide a definitive answer on the universally optimal noise distribution across all scenarios.
- What evidence would resolve it: Comprehensive experimental results comparing various noise distributions (including non-standard ones) across multiple datasets, model architectures, and attack scenarios, identifying the optimal distribution for each combination.

### Open Question 3
- Question: How does the distribution shift between the distillation
- Basis in paper: [explicit] The paper discusses that while DUCD performs well against ℓ1 and ℓ2 attacks, it shows suboptimal performance against ℓ∞ attacks, which remains a critical issue for future exploration and resolution in certified defenses.
- Why unresolved: Despite theoretical guarantees, randomized smoothing fails to effectively defend against ℓ∞ norm attacks in practice. The paper acknowledges this as a significant limitation that requires further research.
- What evidence would resolve it: Successful development and empirical validation of a certified defense method that demonstrates robust performance against ℓ∞ norm attacks, with quantitative metrics showing comparable effectiveness to its performance on other ℓp norms.

## Limitations

- Query efficiency requirements may limit practical deployment in real-world scenarios with rate-limited query access
- Computational complexity of two-stage optimization may become prohibitive for high-dimensional inputs or large-scale models
- Performance validation limited to standard vision datasets, with unclear generalizability to other domains

## Confidence

**High Confidence**: The core mechanism of using knowledge distillation to create a white-box surrogate from a black-box model is well-established and the experimental results showing improved certified accuracy are reproducible.

**Medium Confidence**: The claim of achieving "universal" defense across all ℓp norms is supported by experimental results but relies on assumptions about noise distribution optimization that may not hold for all attack scenarios.

**Low Confidence**: The privacy preservation claims, particularly regarding membership inference attack resistance, are based on reported success rates but lack detailed analysis of attack scenarios.

## Next Checks

1. **Query Budget Analysis**: Conduct experiments measuring certified accuracy as a function of query budget on MNIST and CIFAR10 to establish the relationship between query access and defense performance.

2. **Cross-Domain Generalization**: Test the DUCD framework on non-vision datasets (e.g., text classification using BERT, graph neural networks) to validate the universal defense claims across different data modalities.

3. **Optimization Efficiency Benchmark**: Compare the computational cost of the two-stage optimization procedure against alternative methods like Frank-Wolfe algorithm or gradient-free optimization techniques on high-dimensional inputs.