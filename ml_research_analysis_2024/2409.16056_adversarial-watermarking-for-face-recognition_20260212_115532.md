---
ver: rpa2
title: Adversarial Watermarking for Face Recognition
arxiv_id: '2409.16056'
source_url: https://arxiv.org/abs/2409.16056
tags:
- watermarking
- face
- recognition
- adversarial
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces adversarial watermarking attacks on face
  recognition systems, revealing that adversarial perturbations can exploit watermarking
  to degrade recognition performance. The proposed threat model remains stealthy without
  watermarking but triggers recognition failures once watermarking is applied.
---

# Adversarial Watermarking for Face Recognition

## Quick Facts
- **arXiv ID**: 2409.16056
- **Source URL**: https://arxiv.org/abs/2409.16056
- **Authors**: Yuguang Yao, Anil Jain, Sijia Liu
- **Reference count**: 38
- **Primary result**: Adversarial perturbations can exploit watermarking to degrade face recognition performance by 67.2% (ℓ∞=2/255) to 95.9% (ℓ∞=4/255)

## Executive Summary
This paper introduces a novel adversarial attack that exploits the interaction between adversarial perturbations and watermarking in face recognition systems. The attack crafts perturbations that remain stealthy when watermarking is absent but trigger recognition failures when watermarking is applied. The threat model demonstrates that the watermarking process amplifies the adversarial effect of perturbations, creating a previously unrecognized vulnerability. Evaluated on the CASIA-WebFace dataset, the attack significantly degrades face matching accuracy while maintaining perceptual quality.

## Method Summary
The method uses a joint optimization framework that simultaneously optimizes an adversarial perturbation δ and a watermark message m. The attack satisfies two conditions: (1) when watermarking is absent, the perturbed image should still be correctly recognized, and (2) when watermarking is applied, the combination of δ and m should cause recognition failure. The optimization uses PGD-10 with relaxed binary variables for the watermark message to make the problem computationally tractable. The watermarking system is based on the HiDDeN framework, and the face recognition model is AdaFace trained on MS-Celeb-1M.

## Key Results
- Face matching accuracy drops by 67.2% with ℓ∞ norm perturbation strength of 2/255
- Face matching accuracy drops by 95.9% with ℓ∞ norm perturbation strength of 4/255
- The attack remains stealthy (recognition succeeds) when watermarking is absent
- The attack specifically exploits the watermarking process to amplify adversarial effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial perturbations crafted for face recognition exploit the watermarking process to amplify their adversarial effect.
- Mechanism: The attack jointly optimizes an input perturbation δ and a watermark message m. When watermarking is absent, the perturbation δ is designed to keep the face recognized correctly (condition 1). Once watermarking is applied, the combination of δ and m causes recognition failure (condition 2). This interaction is captured in the joint optimization (4) which maximizes the original similarity s(z′_p, z_r) while minimizing the watermarked similarity s(z′_w, z_r).
- Core assumption: The watermarking process introduces additional transformations that interact with the adversarial perturbation in a way that disrupts feature extraction for recognition.
- Evidence anchors:
  - [abstract] "Our study reveals a previously unrecognized vulnerability: adversarial perturbations can exploit the watermark message to evade face recognition systems."
  - [section] "This design reveals a unique adversarial challenge in face recognition with watermarking, where the optimization of the watermark message in condition 2 interacts synergistically with the input perturbations δ to amplify the adversarial effect."
  - [corpus] Weak: Corpus contains papers on watermarking attacks and fragility, but none directly address the specific adversarial watermarking interaction described here.
- Break condition: If the watermarking encoder is modified to be robust against adversarial perturbations, or if the face recognition model becomes invariant to the watermarking transformations.

### Mechanism 2
- Claim: The proposed threat model remains stealthy in the absence of watermarking, allowing images to be correctly recognized initially.
- Mechanism: The attack uses a two-stage approach where adversarial perturbations are optimized to maintain high similarity scores when watermarking is not present. Only when the watermark is applied does the attack trigger recognition failures. This is formalized in condition 1 of the joint optimization (4).
- Core assumption: The face recognition model's feature extractor h_ψ is sensitive to the watermarking transformations when adversarial perturbations are present.
- Evidence anchors:
  - [abstract] "Our proposed adversarial watermarking attack reduces face matching accuracy by 67.2% with an ℓ∞ norm-measured perturbation strength of 2/255 and by 95.9% with a strength of 4/255."
  - [section] "Our rationale has two key aspects. First, satisfying both conditions 1 and 2 ensures that the adversarial attack (δ) stays stealthy when watermarking is absent, but is triggered upon watermark application, leading to recognition failures."
  - [corpus] Weak: While the corpus includes papers on watermarking attacks, none specifically discuss stealthy attacks that only activate upon watermarking application.
- Break condition: If watermarking is always applied in the system, the stealthy property becomes irrelevant. Alternatively, if the recognition system can detect the presence of adversarial perturbations before watermarking.

### Mechanism 3
- Claim: The alternative optimization procedure using relaxed binary variables for the watermark message allows efficient joint optimization of perturbations and watermark messages.
- Mechanism: Direct optimization over binary variables m is computationally intractable for large dimensionality L. The paper relaxes m to be continuous in [0,1]^L during optimization, then projects back to binary values. This allows efficient use of PGD while maintaining valid watermark messages.
- Core assumption: The relaxation and projection process does not significantly degrade the quality of the found adversarial watermarking attack.
- Evidence anchors:
  - [section] "To solve the optimization in (4), we then adopt an alternative optimization procedure to jointly optimize δ and m. Specifically, we use the PGD (projected gradient descent) method [14] to iteratively minimize one variable while keeping the other fixed."
  - [section] "Direct optimization over binary variables is computationally intractable for large dimensionality L. To address this, we relax m to be continuous in the range [0, 1]^L during the optimization."
  - [corpus] Weak: The corpus doesn't provide specific evidence about optimization techniques for binary variables in adversarial watermarking contexts.
- Break condition: If the dimensionality L becomes very large, the relaxation approach may become insufficient and a different optimization strategy would be needed.

## Foundational Learning

- Concept: Adversarial examples and evasion attacks
  - Why needed here: Understanding how small, carefully crafted perturbations can fool machine learning models is fundamental to grasping this attack methodology.
  - Quick check question: What is the difference between white-box and black-box adversarial attacks?

- Concept: Watermarking techniques in digital media
  - Why needed here: The paper relies on understanding how watermarks are embedded in images and how they affect downstream tasks like face recognition.
  - Quick check question: What are the key differences between spatial domain and frequency domain watermarking?

- Concept: Face recognition systems and feature extraction
  - Why needed here: The attack specifically targets face recognition systems, so understanding how these systems extract and compare facial features is crucial.
  - Quick check question: How does cosine similarity work in the context of face recognition feature comparison?

## Architecture Onboarding

- Component map: Input image -> Adversarial perturbation δ -> Watermark encoder f_θ -> Face recognition model h_ψ -> Watermark decoder g_ϕ -> Output recognition result

- Critical path:
  1. Apply adversarial perturbation δ to probe image I_p
  2. Watermark the perturbed image to create I'_w = f_θ(I'_p, m)
  3. Extract features z'_p and z'_w using h_ψ
  4. Compare features with reference using cosine similarity
  5. Trigger recognition failure when watermarking is applied

- Design tradeoffs:
  - Perturbation strength vs. imperceptibility: Higher perturbation strength leads to better attack success but may be more visible
  - Watermark message length vs. effectiveness: Longer messages may provide more attack surface but could be harder to embed
  - Optimization time vs. attack quality: More optimization iterations generally yield better attacks but require more computation

- Failure signatures:
  - Recognition accuracy drops significantly when watermarking is applied but remains stable without watermarking
  - Similarity scores between probe and reference images remain high without watermarking but drop sharply with watermarking
  - The attack specifically fails when either the watermarking or face recognition components are modified

- First 3 experiments:
  1. Baseline: Test face recognition accuracy without any adversarial perturbations or watermarking
  2. Individual component testing: Test recognition accuracy with only adversarial perturbations (no watermarking) and with only watermarking (no perturbations)
  3. Full attack testing: Test recognition accuracy with both adversarial perturbations and watermarking applied together at different perturbation strengths (ϵ = 2/255 and 4/255)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do other watermarking techniques (e.g., frequency domain methods) compare to neural network-based approaches in their vulnerability to adversarial watermarking attacks?
- Basis in paper: [inferred] The paper uses neural network-based HiDDeN watermarking and shows robustness against common transformations but doesn't test against frequency domain methods or other watermarking approaches.
- Why unresolved: The study focuses exclusively on one watermarking technique (HiDDeN), leaving open whether different watermarking methods have varying levels of vulnerability to adversarial attacks.
- What evidence would resolve it: Systematic comparison of adversarial watermarking attacks across multiple watermarking techniques (DCT, DWT, neural network-based) using identical attack parameters and datasets.

### Open Question 2
- Question: What is the minimum perturbation strength required to successfully execute an adversarial watermarking attack across different face recognition models?
- Basis in paper: [explicit] The paper tests perturbation strengths from 0.0 to 4.0/255 but notes that even small perturbations (0.5/255) can cause recognition failures when watermarking is applied.
- Why unresolved: The study uses specific perturbation ranges and a single face recognition model (AdaFace), but doesn't systematically determine the absolute minimum perturbation needed or compare across different recognition architectures.
- What evidence would resolve it: Fine-grained perturbation strength testing across multiple face recognition models (ArcFace, CosFace, etc.) to identify the minimum successful attack threshold for each model.

### Open Question 3
- Question: How do adversarial watermarking attacks generalize across different datasets and demographic groups?
- Basis in paper: [explicit] The paper evaluates the attack on CASIA-WebFace dataset with 1,000 individuals but doesn't test on other datasets or analyze performance across demographic subgroups.
- Why unresolved: The evaluation is limited to a single dataset and doesn't examine whether the attack effectiveness varies based on factors like age, gender, or ethnicity of the subjects.
- What evidence would resolve it: Testing the same adversarial watermarking attack across multiple datasets (LFW, VGGFace, MS-Celeb-1M) and performing subgroup analysis to identify differential attack effectiveness.

## Limitations

- The attack assumes full white-box access to both face recognition and watermarking systems, which may not be realistic in practice
- The study only evaluates one specific watermarking framework (HiDDeN), leaving uncertainty about effectiveness against other watermarking techniques
- Limited analysis of perceptual quality and human studies on image differences between clean and attacked images

## Confidence

- Joint optimization framework effectiveness: **High** - Well-specified mathematical formulation with clear experimental validation showing 67.2-95.9% accuracy degradation
- Stealthiness property: **Medium** - Theoretical justification provided but limited ablation studies on perturbation behavior under varying watermarking conditions
- Alternative optimization procedure: **Medium** - Standard relaxation approach used but lacks detailed analysis of binary projection effects on attack quality

## Next Checks

1. **Cross-watermarking robustness**: Evaluate the attack against multiple watermarking algorithms (spatial vs frequency domain, fragile vs robust) to determine if the adversarial amplification effect is specific to the HiDDeN framework or generalizes across different embedding techniques.

2. **Defense mechanism testing**: Implement and test defensive strategies including adversarial training on watermarked images, detection of adversarial perturbations before watermarking, and watermarking encoders robust to adversarial inputs to establish practical security boundaries.

3. **Black-box transferability analysis**: Conduct experiments with limited model access (score-only, query-based) and different face recognition architectures to assess how well the attack transfers across systems and what level of access is required for practical exploitation.