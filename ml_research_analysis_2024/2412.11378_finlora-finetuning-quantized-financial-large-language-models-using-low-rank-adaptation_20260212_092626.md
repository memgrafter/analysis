---
ver: rpa2
title: 'FinLoRA: Finetuning Quantized Financial Large Language Models Using Low-Rank
  Adaptation'
arxiv_id: '2412.11378'
source_url: https://arxiv.org/abs/2412.11378
tags:
- xbrl
- nancial
- memory
- tasks
- netuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates the use of Quantized Low-Rank Adaptation
  (QLoRA) for finetuning large language models on financial tasks such as sentiment
  analysis, named entity recognition, news headline classification, and XBRL tagging
  and extraction. The method leverages low-rank matrix decomposition and quantization
  to significantly reduce computational requirements while maintaining high model
  performance.
---

# FinLoRA: Finetuning Quantized Financial Large Language Models Using Low-Rank Adaptation

## Quick Facts
- arXiv ID: 2412.11378
- Source URL: https://arxiv.org/abs/2412.11378
- Authors: Dannong Wang; Daniel Kim; Bo Jin; Xingjian Zhao; Tianfan Fu; Steve Yang; Xiao-Yang Liu
- Reference count: 9
- Primary result: FinLoRA achieves up to 48% improvement in accuracy on financial tasks compared to base models, with Llama-3.1-8B finetuned with QLoRA surpassing Llama-3.1-70B base model on Financial Phrasebank

## Executive Summary
FinLoRA presents a parameter-efficient method for finetuning large language models on financial tasks by combining Quantized Low-Rank Adaptation (QLoRA) with distributed training. The approach leverages low-rank matrix decomposition to reduce trainable parameters and quantization to minimize memory usage, enabling finetuning on commodity GPUs. Experiments demonstrate significant performance improvements across sentiment analysis, named entity recognition, news headline classification, and XBRL tagging tasks, with finetuned smaller models outperforming much larger base models.

## Method Summary
FinLoRA combines QLoRA quantization with LoRA adapters for efficient finetuning of large language models on financial tasks. The method quantizes pretrained weights to 8-bit or 4-bit, stores LoRA adapters in 16-bit, and employs distributed data parallelism and pipeline parallelism for scalable training and inference. Low-rank decomposition reduces trainable parameters by over 99% while maintaining model capacity, and quantization further reduces GPU memory consumption. The approach enables finetuning of Llama-3.1-8B and 70B models on financial datasets using cost-effective GPU setups.

## Key Results
- Up to 48% improvement in accuracy compared to base models across financial tasks
- Llama-3.1-8B finetuned with QLoRA achieves 86.30% accuracy on Financial Phrasebank, surpassing Llama-3.1-70B base model's 74.50%
- QLoRA reduces GPU memory usage by 60-80% compared to full fine-tuning
- Finetuned models achieve comparable or better performance than much larger base models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-rank decomposition in LoRA significantly reduces trainable parameters while preserving model capacity.
- Mechanism: LoRA replaces full weight matrices W with W0 + BA, where W0 is frozen pretrained weights and BA is a low-rank update. Since rank r ≪ n (e.g., n=4096, r=8), BA has far fewer parameters (65,536 vs 16 million).
- Core assumption: The update needed for finetuning lies in a low-rank subspace of the original parameter space.
- Evidence anchors:
  - [abstract] "leveraging low-rank matrix decomposition and quantization to significantly reduce computational requirements while maintaining high model performance"
  - [section] "As an example, setting n = 4,096, and r = 8, then W0 has approximately 16 million parameters, while A and B together have 65,536 parameters, which is approximately only 0.039% the size of W0."
  - [corpus] Weak - related papers mention LoRA but do not quantify parameter savings.

### Mechanism 2
- Claim: Quantization reduces GPU memory without significantly harming model quality.
- Mechanism: QLoRA quantizes all pretrained weights to 8-bit or 4-bit, storing only 16-bit LoRA adapters. During computation, weights are dequantized to 16-bit, so inference remains accurate.
- Core assumption: Quantization noise is negligible for downstream task performance.
- Evidence anchors:
  - [abstract] "quantization compresses the model size, further reducing GPU memory consumption"
  - [section] "Table 1 illustrates GPU memory usage with QLoRA during finetuning... The reductions in GPU memory with quantization displayed practical benefits of resource-efficient finetuning and inference for large-scale models."
  - [corpus] Weak - corpus mentions quantization but not specific memory savings or accuracy trade-offs.

### Mechanism 3
- Claim: Distributed data parallelism and pipeline parallelism together enable scalable finetuning and inference on commodity GPUs.
- Mechanism: DDP distributes training data across GPUs for faster training; pipeline parallelism partitions the model into layers across GPUs to fit large models in memory during inference.
- Core assumption: Memory and compute overhead of parallelization is outweighed by gains in throughput and scalability.
- Evidence anchors:
  - [abstract] "We also employ data and pipeline parallelism to enable local finetuning using cost-effective, widely accessible GPUs"
  - [section] "We employed Distributed Data Parallel (DDP)... We also opted to use Brain Floating Point (BF16)... We use pipeline parallelism, where the model is partitioned at the layer level and distributed across multiple GPUs"
  - [corpus] Weak - corpus mentions parallelism but not detailed experimental results.

## Foundational Learning

- Concept: Low-rank matrix factorization
  - Why needed here: Core to LoRA's parameter efficiency; understanding how rank affects expressiveness.
  - Quick check question: If W ∈ Rⁿˣⁿ is approximated as BA where B ∈ Rⁿˣʳ and A ∈ Rʳˣⁿ, how many parameters are in BA vs W when r ≪ n?

- Concept: Quantization schemes (e.g., 8-bit, 4-bit)
  - Why needed here: QLoRA relies on quantization to reduce memory; knowing trade-offs is essential.
  - Quick check question: What is the memory saving ratio when moving from 16-bit to 8-bit storage? What about 4-bit?

- Concept: Distributed training (DDP, pipeline parallelism)
  - Why needed here: Enables scaling to large models on multiple GPUs; necessary for Llama-3.1-70B finetuning.
  - Quick check question: In DDP, how are gradients synchronized across GPUs? In pipeline parallelism, what determines the partitioning of layers?

## Architecture Onboarding

- Component map: Base LLM (Llama-3.1-8B or 70B) -> QLoRA quantization (4-bit/8-bit) -> LoRA adapters (rank 4 or 8) -> Training loop with DDP + BF16 + 0/1 Adam -> Pipeline parallelism for inference
- Critical path: Data loading -> Quantization -> LoRA forward/backward pass -> Gradient sync (DDP) -> Optimizer step -> Checkpointing
- Design tradeoffs:
  - Higher quantization (4-bit) -> more memory savings but potentially lower accuracy
  - Higher LoRA rank -> more expressiveness but larger adapters and slower training
  - More GPUs -> faster training but higher communication overhead
- Failure signatures:
  - Memory OOM -> likely quantization or batch size too high
  - Slow convergence -> learning rate or optimizer choice (0/1 Adam) may need tuning
  - Low accuracy -> rank too low for task complexity, or quantization causing precision loss
- First 3 experiments:
  1. Finetune Llama-3.1-8B on Financial Phrasebank with rank 4, 4-bit QLoRA, batch size 16, 4 epochs
  2. Compare memory usage and accuracy with rank 8, 8-bit QLoRA on same dataset
  3. Scale to Llama-3.1-70B on XBRL tagging with rank 4, 4-bit QLoRA using DDP across 4 GPUs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of QLoRA-finetuned models on financial tasks compare to models finetuned with other parameter-efficient finetuning methods like Adapter or Prefix Tuning?
- Basis in paper: [explicit] The paper focuses on QLoRA and demonstrates its effectiveness, but does not compare it to other parameter-efficient finetuning methods.
- Why unresolved: The paper only evaluates QLoRA and does not provide a direct comparison with other methods.
- What evidence would resolve it: A comparative study evaluating QLoRA against other parameter-efficient finetuning methods (e.g., Adapter, Prefix Tuning) on the same financial datasets and tasks.

### Open Question 2
- Question: How does the choice of LoRA rank (r) affect the tradeoff between model performance and computational efficiency for different financial tasks?
- Basis in paper: [explicit] The paper experiments with different LoRA ranks (4 and 8) and quantization levels (4-bit and 8-bit), but does not provide a systematic analysis of the impact of rank on different tasks.
- Why unresolved: The paper only provides results for a few combinations of rank and quantization, without exploring the full range of possible values or their impact on different tasks.
- What evidence would resolve it: A comprehensive study varying the LoRA rank and quantization level for each financial task to identify the optimal tradeoff between performance and efficiency.

### Open Question 3
- Question: How well do QLoRA-finetuned models generalize to out-of-distribution financial data or data from different time periods?
- Basis in paper: [inferred] The paper evaluates model performance on held-out test sets from the same datasets used for finetuning, but does not assess generalization to new or unseen data.
- Why unresolved: The paper does not address the potential for overfitting or lack of generalization in finetuned models.
- What evidence would resolve it: Evaluating finetuned models on financial datasets from different time periods or with different characteristics than the training data to assess their ability to generalize.

### Open Question 4
- Question: What is the impact of using different base models (e.g., Llama 3.1 vs. other LLMs) on the performance of QLoRA-finetuned models for financial tasks?
- Basis in paper: [explicit] The paper uses Llama 3.1-8B and Llama 3.1-70B as base models, but does not compare their performance to other LLMs.
- Why unresolved: The paper does not explore the potential benefits of using different base models for finetuning.
- What evidence would resolve it: A comparative study using different base models (e.g., Llama, GPT, BERT) for finetuning on the same financial tasks to identify the optimal base model for each task.

## Limitations

- Quantization-induced accuracy loss: The paper lacks detailed ablation studies on accuracy trade-offs between different quantization levels across financial tasks
- Limited generalization assessment: Evaluation focuses on specific financial tasks without demonstrating broader domain generalization or sequential task fine-tuning capabilities
- Unreported distributed training overhead: Missing metrics on communication overhead, scaling efficiency, and wall-clock time comparisons with alternative distributed training approaches

## Confidence

**High Confidence**: The core claim that LoRA significantly reduces trainable parameters while maintaining model performance is well-established in literature and supported by parameter count comparisons.

**Medium Confidence**: Specific performance improvements (48% accuracy gain, surpassing larger models) are based on experiments but lack comparison with full fine-tuning baselines and cross-validation across diverse financial domains.

**Low Confidence**: The assertion that QLoRA enables finetuning on "commodity GPUs" without qualification regarding model size, batch size, or acceptable training times.

## Next Checks

1. **Quantization-Ablation Study**: Conduct controlled experiments comparing 4-bit vs 8-bit QLoRA fine-tuning across all four financial tasks, measuring both memory savings and accuracy differences. Include a full fine-tuning baseline to isolate LoRA's contribution from quantization effects.

2. **Cross-Dataset Generalization Test**: Fine-tune QLoRA adapters on one financial dataset (e.g., Financial Phrasebank), then evaluate zero-shot or few-shot performance on unseen financial datasets (e.g., different NER corpora or XBRL schemas) to assess domain generalization.

3. **Distributed Training Efficiency Analysis**: Measure strong and weak scaling efficiency for DDP + pipeline parallelism across 2, 4, and 8 GPUs. Report communication overhead, memory usage per GPU, and wall-clock time comparisons with alternative distributed training configurations.