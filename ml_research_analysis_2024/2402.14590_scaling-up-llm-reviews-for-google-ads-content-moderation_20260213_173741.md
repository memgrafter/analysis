---
ver: rpa2
title: Scaling Up LLM Reviews for Google Ads Content Moderation
arxiv_id: '2402.14590'
source_url: https://arxiv.org/abs/2402.14590
tags:
- google
- images
- content
- prompt
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of scaling LLM-based content
  moderation for large ad repositories, where inference costs and latency are prohibitive.
  The proposed method combines candidate selection through heuristics and clustering,
  LLM review of representative ads, and label propagation back to clusters.
---

# Scaling Up LLM Reviews for Google Ads Content Moderation

## Quick Facts
- arXiv ID: 2402.14590
- Source URL: https://arxiv.org/abs/2402.14590
- Reference count: 8
- Primary result: Reduces LLM reviews by 3+ orders of magnitude while achieving 2x recall vs baseline

## Executive Summary
This paper addresses the challenge of scaling large language model (LLM) content moderation for massive ad repositories, where direct LLM inference on hundreds of millions of ads is computationally prohibitive. The authors propose a funneling approach that combines candidate selection through heuristics and clustering, LLM review of representative ads, and label propagation back to clusters. This method achieves a 1000x reduction in reviews while maintaining high recall, demonstrating that cross-modal similarity representations are crucial for effective clustering and label propagation in ad moderation systems.

## Method Summary
The proposed method employs a multi-stage funneling approach to reduce the computational burden of LLM-based content moderation. First, candidate selection filters and clusters ads using content and actor similarity heuristics, reducing the dataset from 400 million to approximately 400,000 candidates. Next, an LLM reviews representative ads from each cluster after prompt engineering and soft-prompt tuning for the specific moderation policy. Finally, label propagation uses cross-modal similarity embeddings to assign labels from reviewed ads to their clusters, with a feedback loop to improve candidate selection in subsequent rounds.

## Key Results
- Achieves 3+ orders of magnitude reduction in LLM reviews while maintaining high recall
- Demonstrates 2x recall improvement compared to baseline non-LLM model
- Shows cross-modal similarity representations outperform uni-modal approaches for clustering and label propagation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using representative ad sampling per cluster dramatically reduces LLM reviews needed while maintaining coverage
- Mechanism: Ads are grouped into clusters based on similarity, with only one representative ad per cluster reviewed by the LLM, then labels are propagated back to all ads in the cluster
- Core assumption: Ads within a cluster share sufficient similarity that one review can represent the entire cluster
- Evidence anchors: [abstract] selection of one representative ad per cluster; [section 3.1.2] duplicate/near-duplicate content wastes resources
- Break condition: If clustering fails to group truly similar ads or if ads within clusters have heterogeneous content

### Mechanism 2
- Claim: Cross-modal similarity representations improve clustering and label propagation accuracy compared to uni-modal representations
- Mechanism: Embeddings capturing both text and image content are used to measure similarity between ads, leading to better clustering and more accurate label propagation
- Core assumption: Cross-modal embeddings capture semantic relationships between ads better than uni-modal embeddings
- Evidence anchors: [abstract] cross-modal similarity representations yield better results; [section 3.1.1] graph-based label propagation from known violating images
- Break condition: If cross-modal embeddings fail to capture relevant semantic similarities or if uni-modal representations are sufficient

### Mechanism 3
- Claim: Prompt engineering and soft-prompt tuning significantly improve LLM performance on ad content moderation tasks
- Mechanism: Carefully designed prompts guide the LLM to make correct moderation decisions, while soft-prompt tuning adapts the model with minimal parameters to the specific policy
- Core assumption: LLMs can be effectively guided through prompt design and soft-prompt tuning to perform well on specialized tasks
- Evidence anchors: [section 3.2] combination of prompt engineering and parameter efficient tuning; evaluation of various prompts on small labeled dataset
- Break condition: If prompt engineering fails to improve LLM performance or if soft-prompt tuning overfits to training data

## Foundational Learning

- Concept: Cross-modal embeddings and similarity measures
  - Why needed here: Clustering and label propagation rely on measuring similarity between ads using embeddings that capture both text and image content
  - Quick check question: What is the difference between cross-modal and uni-modal embeddings, and why are cross-modal embeddings preferred for this task?

- Concept: Prompt engineering and soft-prompt tuning for LLMs
  - Why needed here: LLM performance on ad moderation depends on carefully designed prompts and soft-prompt tuning to adapt the model to the specific policy
  - Quick check question: How does prompt engineering differ from soft-prompt tuning, and why are both used in this approach?

- Concept: Label propagation and feedback loops
  - Why needed here: Labels assigned to representative ads are propagated back to their clusters, with feedback loop using labeled ads to improve candidate selection in future rounds
  - Quick check question: How does the feedback loop work in this system, and what is its purpose?

## Architecture Onboarding

- Component map: Review Candidate Selection Funneling -> Large Language Model Tuning and Labeling -> Label Propagation and Feedback Loop
- Critical path: Candidate selection → LLM review of representatives → Label propagation → Feedback loop
- Design tradeoffs:
  - Clustering vs. individual review: Clustering reduces LLM reviews but may miss edge cases
  - Cross-modal vs. uni-modal embeddings: Cross-modal embeddings are more accurate but computationally expensive
  - Prompt complexity vs. LLM performance: More complex prompts may improve performance but increase latency
- Failure signatures:
  - Low recall: Clustering or label propagation may be too aggressive, missing policy violations
  - High false positives: LLM may be too sensitive to certain content, flagging non-violating ads
  - Slow processing: Computational bottlenecks in candidate selection, LLM inference, or label propagation
- First 3 experiments:
  1. Compare clustering performance using cross-modal vs. uni-modal embeddings on a held-out dataset
  2. Evaluate the impact of different prompt designs on LLM performance for ad moderation
  3. Measure the effectiveness of the feedback loop by comparing candidate selection quality before and after multiple iterations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of cross-modal similarity representations versus uni-modal representations impact the scalability and accuracy trade-offs in LLM-based content moderation across different ad formats (e.g., text, video, landing pages)?
- Basis in paper: [explicit] Cross-modal similarity representations yield better results than uni-modal representations for image ads
- Why unresolved: The paper focuses on image ads and does not explore performance across other ad formats or modalities
- What evidence would resolve it: Empirical studies comparing cross-modal and uni-modal representations across various ad formats including text, video, and landing pages

### Open Question 2
- Question: What are the optimal strategies for balancing the trade-off between the number of LLM reviews and the recall rate in content moderation, especially when dealing with diverse and evolving ad content?
- Basis in paper: [inferred] Reduction of reviews by 3+ orders of magnitude while achieving 2x recall implies need to balance review volume and recall
- Why unresolved: The paper does not provide detailed analysis of how different strategies affect the balance between review volume and recall
- What evidence would resolve it: Systematic experiments varying review selection and label propagation parameters and measuring their impact on the trade-off

### Open Question 3
- Question: How can the feedback loop from labeled images to the initial funneling step be optimized to improve the efficiency and effectiveness of LLM-based content moderation?
- Basis in paper: [explicit] Feedback loop uses labeled images to identify similar images as potential candidates for next round of LLM review
- Why unresolved: The paper does not explore mechanisms or strategies for optimizing the feedback loop
- What evidence would resolve it: Studies evaluating different feedback loop designs including variations in feedback frequency, selection criteria, and incorporation mechanisms

## Limitations
- Effectiveness heavily depends on quality of clustering and label propagation, which are not fully validated
- Cross-modal similarity representations claimed superior but direct comparisons with uni-modal baselines not provided
- Feedback loop mechanism mentioned but lacks detailed evaluation of its contribution to overall performance
- Focus on single "Non-Family Safe" policy limits generalizability to other moderation policies

## Confidence
**High Confidence**: The core claim that representative sampling with label propagation can reduce LLM reviews by 3+ orders of magnitude is well-supported by described methodology and reasonable given scale of ad repositories.

**Medium Confidence**: The 2x recall improvement over baseline models is reported but lacks detailed comparative analysis. Effectiveness of cross-modal representations is claimed but not empirically validated against alternatives.

**Low Confidence**: Specific contributions of individual components (prompt engineering, soft-prompt tuning, feedback loop) to overall performance are not clearly isolated or measured.

## Next Checks
1. Implement and compare clustering and label propagation performance using both cross-modal and uni-modal embeddings on the same dataset to empirically validate claimed superiority

2. Systematically remove or replace individual components (prompt engineering, soft-prompt tuning, feedback loop) to measure their individual contributions to overall performance and identify critical dependencies

3. Apply the methodology to a different content moderation policy (e.g., hate speech detection) and evaluate whether same performance gains and component effectiveness generalize across policy domains