---
ver: rpa2
title: 'How not to Stitch Representations to Measure Similarity: Task Loss Matching
  versus Direct Matching'
arxiv_id: '2412.11299'
source_url: https://arxiv.org/abs/2412.11299
tags:
- similarity
- layer
- matching
- representations
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of measuring similarity between
  internal representations of deep neural networks. The authors argue that task loss
  matching (TLM), a commonly used method for model stitching, can be misleading as
  a similarity index because it often creates out-of-distribution (OOD) representations
  to improve task-specific performance.
---

# How not to Stitch Representations to Measure Similarity: Task Loss Matching versus Direct Matching

## Quick Facts
- arXiv ID: 2412.11299
- Source URL: https://arxiv.org/abs/2412.11299
- Reference count: 40
- Primary result: Task loss matching (TLM) can produce misleading similarity scores by creating out-of-distribution representations, while direct matching (DM) avoids this issue and better identifies corresponding layers.

## Executive Summary
This paper investigates the problem of measuring similarity between internal representations of deep neural networks. The authors argue that task loss matching (TLM), a commonly used method for model stitching, can be misleading as a similarity index because it often creates out-of-distribution (OOD) representations to improve task-specific performance. They propose direct matching (DM) as an alternative, which minimizes the distance between stitched representations. Through experiments with ResNet and Vision Transformer architectures on CIFAR-10, SVHN, and ImageNet datasets, the authors demonstrate that TLM fails two important sanity checks: identifying corresponding layers between networks and recognizing that a layer is most similar to itself.

## Method Summary
The authors compare two methods for measuring representational similarity through model stitching: task loss matching (TLM) and direct matching (DM). TLM optimizes the stitching transformation by minimizing task loss while freezing the two halves of the networks, potentially creating OOD representations. DM directly matches source and target representations through a linear least-squares problem, inherently promoting in-distribution mappings. The evaluation uses three datasets (CIFAR-10, SVHN, ImageNet) and two architectures (ResNet-18, ViT-Ti), with OOD detection based on energy-based methods and functional similarity measured through linear probing.

## Key Results
- TLM often produces out-of-distribution representations, failing to correctly identify corresponding layers and violating the expectation that a layer should be most similar to itself.
- DM avoids OOD issues by directly minimizing representational distance, achieving higher rank correlations with functional similarity in sensitivity and specificity tests.
- The results indicate that DM strikes a good balance between structural and functional requirements for a good similarity index.

## Why This Works (Mechanism)

### Mechanism 1
Task Loss Matching (TLM) can produce misleading similarity scores because it optimizes for task performance rather than representational alignment, often generating out-of-distribution (OOD) internal representations. TLM prioritizes functional performance, which can lead the stitching layer to transform source representations into target-like forms that are not naturally occurring in the target network's input distribution. This results in high task accuracy but OOD internal activations that violate structural similarity expectations.

### Mechanism 2
Direct Matching (DM) avoids OOD issues because its objective function directly minimizes the distance between source and target representations, inherently promoting in-distribution mappings. DM solves a linear least-squares problem to align source and target activations without reference to the classification task. This structural alignment naturally preserves the distributional characteristics of the target representation space, since the transformation is computed to minimize representational distance under the constraint of an affine mapping.

### Mechanism 3
OOD detectors based on energy-based methods can effectively identify when TLM has produced out-of-distribution representations, providing a diagnostic tool for similarity measures. Energy-based OOD detection assigns an energy score (negative LogSumExp of logits) to each input sample and learns a threshold that separates in-distribution from OOD samples. By training dedicated OOD detectors on internal activations of stitched layers, one can measure the separability (AUROC) between original target activations and stitched activations, revealing whether TLM has created OOD representations.

## Foundational Learning

- Concept: Out-of-Distribution (OOD) Detection
  - Why needed here: To diagnose whether similarity measures like TLM produce representations that deviate from the natural input distribution, which is crucial for validating the reliability of similarity indices.
  - Quick check question: What is the primary metric used in the paper to quantify OOD separability between original and stitched activations?

- Concept: Canonical Correlation Analysis (CCA) and Centered Kernel Alignment (CKA)
  - Why needed here: These are structural similarity indices used as baselines to compare against task loss matching and direct matching, helping to evaluate the trade-offs between structural and functional similarity measures.
  - Quick check question: Which structural similarity index was found to perform best in identifying architecturally corresponding layers between different networks?

- Concept: Model Stitching Framework
  - Why needed here: Understanding how model stitching works (connecting two half-networks via a stitching layer) is fundamental to grasping the problem of measuring representational similarity and the differences between TLM and DM approaches.
  - Quick check question: In the context of model stitching, what is the difference between task loss matching and direct matching in terms of their optimization objectives?

## Architecture Onboarding

- Component map: Input images -> Source network -> Stitching layer (TLM/DM) -> Receiver network or OOD detector
- Critical path: For evaluating a similarity measure: 1) Train two networks independently, 2) Select layers to stitch, 3) Compute stitching transformation (TLM or DM), 4) Measure functional similarity (accuracy) and OOD separability (AUROC), 5) Compare results to structural similarity indices
- Design tradeoffs: TLM offers potentially higher functional compatibility but risks OOD representations; DM ensures in-distribution mappings but may be less flexible functionally; structural indices are computationally cheaper but may not reflect functional similarity
- Failure signatures: TLM shows high functional similarity scores between distant layers and fails intra-network sanity checks (a layer not being most similar to itself); OOD detectors show high AUROC values for TLM-stitched representations, indicating distributional shift
- First 3 experiments:
  1. Implement self-stitching within a single network using TLM and verify if the similarity matrix shows the identity layer as most similar to itself; expect failure as shown in the paper
  2. Apply direct matching to the same self-stitching scenario and confirm that the identity layer is correctly identified as most similar; expect success
  3. Train an OOD detector on the activations of a target layer and use it to measure AUROC for both TLM and DM stitched representations; expect high AUROC for TLM and low for DM

## Open Questions the Paper Calls Out

### Open Question 1
How does task loss matching (TLM) behave with more complex models or tasks beyond image classification? The authors note that their evaluation methodology is "rather expensive" and suggest verifying their claims "on a wider set of models and applications, including ones outside the image processing domain." This remains unresolved because the study focused on ResNet and Vision Transformer architectures on CIFAR-10, SVHN, and ImageNet datasets, with limited computational resources restricting exploration of more complex models or tasks.

### Open Question 2
Can task loss matching be modified to avoid creating out-of-distribution (OOD) representations while maintaining its functional benefits? The authors argue that TLM's main issue is creating OOD representations, but acknowledge it's "still a suitable method to provide evidence for the lack of functional similarity." This is unresolved because the paper demonstrates the problem but doesn't explore potential modifications to TLM to address the OOD issue.

### Open Question 3
How does direct matching (DM) perform in scenarios where functional similarity is critical but structural similarity is less important? The authors conclude that "DM strikes a good balance between the structural and functional requirements for a good similarity index," but acknowledge this needs further investigation. This remains unresolved because the study focused on demonstrating DM's advantages over TLM, but didn't extensively explore scenarios where functional similarity is paramount.

## Limitations
- The analysis reveals significant uncertainty regarding the paper's claims due to limited direct evidence in the corpus
- The energy-based OOD detection mechanism, while theoretically sound, lacks validation from related work in the corpus
- The assumption that DM inherently produces in-distribution representations may not hold for vastly different network architectures or modalities

## Confidence
- High confidence: The structural distinction between TLM (task-focused) and DM (representation-focused) optimization objectives is clearly articulated and theoretically sound
- Medium confidence: The claim that TLM can produce OOD representations is supported by the paper's experimental results, but lacks external validation from related work
- Low confidence: The assertion that DM consistently produces in-distribution representations across all scenarios, given that this depends heavily on the similarity of source and target distributions

## Next Checks
1. **OOD Detection Calibration**: Validate the energy-based OOD detectors by testing them on known OOD datasets (e.g., CIFAR-10.1, ImageNet-A) to ensure they can reliably distinguish between in-distribution and out-of-distribution internal representations before applying them to TLM vs DM comparisons

2. **Architectural Transferability**: Test whether the OOD issues with TLM persist when stitching between different architectures (e.g., ResNet to ViT) and between different modalities (e.g., vision to language models), to assess the generalizability of the findings beyond CIFAR-10 and SVHN

3. **Task Sensitivity Analysis**: Systematically vary the task difficulty and loss function (e.g., using cross-entropy vs. margin-based losses) in TLM to determine whether the OOD behavior is task-dependent or an inherent property of the matching approach