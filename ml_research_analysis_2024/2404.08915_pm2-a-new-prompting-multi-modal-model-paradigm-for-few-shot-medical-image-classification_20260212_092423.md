---
ver: rpa2
title: 'PM2: A New Prompting Multi-modal Model Paradigm for Few-shot Medical Image
  Classification'
arxiv_id: '2404.08915'
source_url: https://arxiv.org/abs/2404.08915
tags:
- image
- visual
- prompt
- medical
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new prompting multi-modal model paradigm
  for few-shot medical image classification called PM2. PM2 combines image and text
  modalities, using prompt engineering to better describe medical images and classes.
---

# PM2: A New Prompting Multi-modal Model Paradigm for Few-shot Medical Image Classification

## Quick Facts
- arXiv ID: 2404.08915
- Source URL: https://arxiv.org/abs/2404.08915
- Reference count: 40
- Key outcome: Achieves state-of-the-art performance on three medical datasets with up to 84.58% accuracy on brain tumor classification

## Executive Summary
PM2 introduces a novel multi-modal approach for few-shot medical image classification that combines image and text prompts to improve classification accuracy. The method leverages pre-trained CLIP models with frozen encoders and employs linear probing on both visual tokens and class tokens. By using global covariance pooling with matrix power normalization to aggregate visual tokens, PM2 captures richer feature relationships than traditional first-order pooling methods. Experiments on three medical datasets demonstrate significant improvements over baseline approaches, achieving up to 84.58% accuracy on brain tumor classification.

## Method Summary
PM2 uses a pre-trained CLIP model with frozen image and text encoders to extract features from medical images and associated text prompts. A linear classifier is trained on top of these features, with the approach exploring five distinct text prompt schemes including classname, vanilla, hand-crafted, GPT, and CoOp methods. The model combines two classification heads - one for image features and one for text features - with the visual classification head employing covariance pooling with matrix power normalization to capture second-order statistics. The method is evaluated on three medical datasets (BACH, MRI brain tumor, and Diabetic Retinopathy) across various shot settings.

## Key Results
- Achieves 84.58% classification accuracy on brain tumor dataset
- Achieves 76.11% classification accuracy on breast cancer dataset
- Achieves 72.17% classification accuracy on diabetic retinopathy dataset
- Outperforms baseline methods across all three datasets and shot settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text prompts provide supplementary context that improves few-shot classification accuracy
- Mechanism: Text prompts encode semantic descriptions of image classes, enriching the feature space when few labeled examples exist
- Core assumption: Text encoder can map class descriptions into same embedding space as visual features
- Evidence anchors: Abstract states PM2 "introduces another supplementary text input, known as prompt, to further describe corresponding image or concept classes"
- Break condition: If prompts are ambiguous or unrelated to image content, they may introduce noise

### Mechanism 2
- Claim: Covariance pooling captures richer feature relationships than first-order pooling
- Mechanism: Aggregates pairwise feature interactions across spatial locations, modeling feature distribution
- Core assumption: Feature distribution contains discriminative information beyond mean
- Evidence anchors: Abstract mentions "global covariance pooling with efficient matrix power normalization"
- Break condition: If covariance estimate is unstable due to high dimensionality and limited samples

### Mechanism 3
- Claim: Combining class token and visual token statistics gives more complete representation
- Mechanism: Fuses global image summary (class token) and local feature distribution (covariance of visual tokens)
- Core assumption: Both global and local statistics are complementary and jointly discriminative
- Evidence anchors: Abstract states model "classification on feature distribution of visual tokens"
- Break condition: If fusion weighting is not optimized, representations may cancel each other

## Foundational Learning

- Concept: Contrastive learning and alignment of image and text embeddings
  - Why needed here: PM2 relies on CLIP-style pre-training where images and text are projected into shared space
  - Quick check question: How does CLIP ensure that "a photo of a dog" and an actual dog image are close in embedding space?

- Concept: Covariance pooling and matrix power normalization
  - Why needed here: Visual classification head uses covariance pooling with Newton-Schulz iteration
  - Quick check question: Why is covariance pooling preferred over simple mean pooling in this context?

- Concept: Prompt engineering and learnable prompts (CoOp)
  - Why needed here: Paper investigates multiple prompt schemes including learnable vectors
  - Quick check question: What is the difference between hand-crafted prompts and learnable prompts in terms of training?

## Architecture Onboarding

- Component map:
  Input: Medical image + text prompt
  → Image encoder (ViT/ResNet from CLIP) → Class token + visual tokens
  ↓
  Text encoder (CLIP) → [EOS] token
  ↓
  Visual classification head (Cov pooling on visual tokens + class token) → Shared head → Prediction
  ↓
  Text classification head (Shared FC layer) → Prediction

- Critical path: Image → Image encoder → Visual tokens → Cov pooling → Visual head → Prediction
                            ↓
                         Text prompt → Text encoder → [EOS] token → Shared head → Prediction

- Design tradeoffs:
  - Frozen CLIP vs fine-tuning: Efficiency and generalization vs potential overfitting
  - Second-order pooling vs first-order: Richer statistics vs higher computational cost
  - Learnable prompts vs fixed prompts: Adaptability vs added complexity

- Failure signatures:
  - Overfitting: Check training vs validation gap
  - Unstable covariance estimates: High variance in second-order pooling results
  - Poor prompt alignment: Text and image predictions diverge significantly

- First 3 experiments:
  1. Baseline: Replace covariance pooling with global average pooling; compare accuracy
  2. Prompt ablation: Remove text prompts; measure drop in performance
  3. Shot sensitivity: Test across {1, 2, 4, 8, 16} shots; plot accuracy vs shot count

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance scale with different numbers of learnable context vectors (M) in CoOp prompt method?
- Basis in paper: Paper mentions CoOp evaluated with M = 4 and M = 16 context vectors
- Why unresolved: No systematic comparison of different M values across datasets
- What evidence would resolve it: Experiments with varying M values (2, 4, 8, 16) on each dataset

### Open Question 2
- Question: Can visual classification head be applied to other multi-modal tasks beyond few-shot classification?
- Basis in paper: Paper focuses on few-shot medical image classification
- Why unresolved: Does not explore applicability to other tasks like image-text retrieval
- What evidence would resolve it: Implement head in other multi-modal architectures and evaluate performance

### Open Question 3
- Question: How does PM2 perform with larger numbers of classes or more complex classification tasks?
- Basis in paper: Evaluates on datasets with limited number of classes (4 for BACH, 3 for brain tumor, 5 for DR)
- Why unresolved: Does not explore performance in scenarios with more classes or complex tasks
- What evidence would resolve it: Apply to datasets with larger number of classes or multi-label classification

## Limitations

- Specific implementations of five prompt schemes are underspecified, making performance attribution unclear
- Covariance pooling with matrix power normalization lacks detailed mathematical specification
- Claims of "state-of-the-art performance" lack proper baseline comparison with recent few-shot methods

## Confidence

- **High confidence** in general multi-modal architecture combining image and text features
- **Medium confidence** in second-order statistics mechanism - theoretical soundness but practical benefits not fully demonstrated
- **Low confidence** in prompt engineering claims - without seeing actual prompt formulations or impact analysis

## Next Checks

1. Implement and compare all five prompt schemes in isolation to determine which contributes most to performance gains
2. Replace covariance pooling with global average pooling while keeping all other components fixed to isolate impact of second-order statistics
3. Conduct ablation studies removing either text modality or second-order visual statistics to quantify individual contributions to reported performance