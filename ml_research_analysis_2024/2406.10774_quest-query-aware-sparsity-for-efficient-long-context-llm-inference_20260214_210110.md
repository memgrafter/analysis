---
ver: rpa2
title: 'Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference'
arxiv_id: '2406.10774'
source_url: https://arxiv.org/abs/2406.10774
tags:
- quest
- cache
- tokens
- attention
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Quest, a query-aware KV cache selection algorithm
  for efficient long-context LLM inference. The key insight is that the criticality
  of tokens in the KV cache depends on the current query, rather than being static.
---

# Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference

## Quick Facts
- arXiv ID: 2406.10774
- Source URL: https://arxiv.org/abs/2406.10774
- Authors: Jiaming Tang; Yilong Zhao; Kan Zhu; Guangxuan Xiao; Baris Kasikci; Song Han
- Reference count: 12
- Key outcome: Achieves up to 7.03x self-attention speedup and 2.23x end-to-end latency reduction while maintaining high accuracy with minimal KV cache budgets (256-4K tokens for 32K context length)

## Executive Summary
Quest introduces a query-aware KV cache selection algorithm for efficient long-context LLM inference. The key insight is that token criticality depends on the current query rather than being static, enabling selective loading of only the most relevant cache pages. By maintaining metadata (min/max values) for each KV cache page and using this along with the current query vector, Quest estimates page criticality and performs sparse self-attention on only the top-K critical pages. This approach significantly reduces memory movement and latency while maintaining high accuracy.

## Method Summary
Quest addresses the challenge of efficient long-context LLM inference by implementing a query-aware KV cache selection mechanism. The method maintains metadata for each page of the KV cache, specifically the minimum and maximum Key values, which are used to estimate the criticality of each page relative to the current query vector. During inference, only the top-K most critical pages are loaded for attention computation, significantly reducing memory movement and latency. The approach is evaluated on language modeling (PG19 dataset), passkey retrieval, and LongBench tasks, demonstrating substantial speedups while maintaining accuracy with sparse KV caches.

## Key Results
- Achieves up to 7.03x self-attention speedup compared to baselines
- Reduces end-to-end latency by up to 2.23x
- Maintains high accuracy with sparse KV caches (256-4K tokens for 32K context length), reaching 99% sparsity
- Effective across various long-context tasks and model sizes

## Why This Works (Mechanism)
Quest's effectiveness stems from its query-aware approach to KV cache selection, which recognizes that token importance is dynamic and context-dependent rather than static. By maintaining page-level metadata (min/max Key values) and using this information along with the current query vector, the algorithm can accurately estimate which cache pages are most critical for the current inference step. This allows selective loading of only the most relevant pages, significantly reducing memory movement and computational overhead while preserving accuracy.

## Foundational Learning
- KV cache mechanism: Why needed - Enables efficient attention computation by storing previously computed Key/Value pairs; Quick check - Verify understanding of how KV cache is used in self-attention
- Sparse attention: Why needed - Reduces computational complexity by focusing only on relevant tokens; Quick check - Understand how sparsity patterns affect attention computation
- Metadata maintenance: Why needed - Critical for efficient estimation of token/page criticality; Quick check - Verify correct implementation of min/max value tracking
- Page-based cache organization: Why needed - Enables efficient memory management and selective loading; Quick check - Confirm proper page size selection and boundary handling
- Query-aware criticality estimation: Why needed - Allows dynamic selection of relevant cache pages; Quick check - Validate accuracy of criticality estimation algorithm
- Upper bound calculation: Why needed - Enables safe approximation of attention scores for sparse computation; Quick check - Verify correctness of upper bound formulas

## Architecture Onboarding
**Component map:** Input query -> Query vector processing -> KV cache with page metadata -> Criticality estimation -> Top-K page selection -> Sparse self-attention computation

**Critical path:** Query vector processing -> KV cache metadata lookup -> Criticality estimation -> Top-K selection -> Sparse attention computation

**Design tradeoffs:** 
- Memory vs. accuracy: Smaller KV cache budgets improve efficiency but may impact accuracy
- Page size selection: Larger pages reduce metadata overhead but may include less relevant tokens
- Criticality estimation accuracy: More complex estimation improves accuracy but increases computational overhead

**Failure signatures:** 
- Poor accuracy due to inaccurate criticality estimation
- Insufficient speedup from suboptimal page size or token budget selection
- Memory inefficiencies from improper metadata management

**3 first experiments:**
1. Verify correct implementation of KV cache metadata maintenance and criticality estimation
2. Profile memory usage patterns with different page sizes and token budgets
3. Test sparse attention computation with synthetic data to validate upper bound calculations

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Precise implementation details of criticality estimation algorithm remain unclear
- Optimal configuration of parameters (page size, token budget) varies across tasks
- Generalizability to all long-context LLM applications not fully established
- Method's effectiveness heavily depends on accurate query-aware criticality estimation

## Confidence
- High confidence: The core concept of query-aware KV cache selection using page-level metadata is clearly presented and the general approach is reproducible
- Medium confidence: The reported speedups and accuracy trade-offs are plausible given the method's design, but exact reproduction may vary based on implementation details
- Low confidence: The generalizability of results across all long-context tasks and the robustness of the method to different parameter configurations

## Next Checks
1. Implement the exact criticality estimation algorithm with proper min/max Key value metadata maintenance and verify the attention score upper bound calculations
2. Conduct ablation studies varying page size and token budget parameters to identify optimal configurations for different context lengths and tasks
3. Evaluate Quest's performance on additional long-context datasets beyond those presented to assess generalizability and identify potential failure modes