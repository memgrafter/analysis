---
ver: rpa2
title: 'Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs
  for Information Retrieval'
arxiv_id: '2406.12169'
source_url: https://arxiv.org/abs/2406.12169
tags:
- distillation
- retriever
- llms
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Intermediate Distillation, a data-efficient
  knowledge distillation method that trains retriever models from black-box LLMs using
  an intermediate ranker model. Unlike previous methods that rely on LLM output probabilities,
  this approach uses LLM-generated ranking orders as supervision signals.
---

# Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs for Information Retrieval

## Quick Facts
- arXiv ID: 2406.12169
- Source URL: https://arxiv.org/abs/2406.12169
- Authors: Zizhong Li; Haopeng Zhang; Jiawei Zhang
- Reference count: 14
- One-line primary result: Achieves strong retrieval performance with only 1,000 training instances - 100-1,000x less data than previous approaches

## Executive Summary
This paper introduces Intermediate Distillation, a novel knowledge distillation method that trains retriever models from black-box LLMs using an intermediate ranker model. Unlike previous approaches that rely on LLM output probabilities, this method uses LLM-generated ranking orders as supervision signals. The approach employs a two-stage pipeline: first distilling knowledge from LLMs to a ranker model using list-wise ranking orders, then transferring this knowledge to the retriever via KL-divergence minimization. Experiments demonstrate that the method achieves strong performance with only 1,000 training instances while improving retrieval performance on question-answering tasks within the RAG framework.

## Method Summary
The Intermediate Distillation method uses a two-stage knowledge transfer pipeline. In Stage 1, an LLM generates relevance-based ranking orders for document subsets retrieved by a base retriever. A ranker model is then trained to mimic these LLM rankings using ListMLE loss, which optimizes for the correct permutation of documents based on their relative ordering. In Stage 2, the trained ranker enhances the original retriever by minimizing the KL-divergence between their similarity likelihood distributions. This approach is compatible with closed-source LLMs and demonstrates significant computational efficiency gains by requiring only 1,000 training instances compared to previous methods that need 100,000-1,000,000 instances.

## Key Results
- Achieves strong retrieval performance with only 1,000 training instances
- Improves retrieval performance on question-answering tasks within RAG framework
- Demonstrates compatibility with closed-source LLMs
- Shows significant computational efficiency gains compared to previous methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: List-wise ranking orders from LLMs provide more consistent and interpretable supervision signals than output probabilities
- Mechanism: The LLM generates a relevance-based ranking order (π) that directly captures relative document importance without ambiguous probability scores
- Core assumption: Ranking orders are more stable and informative than similarity scores for training retrievers
- Evidence anchors: [abstract]: "solely using LLMs' ranking generation as the supervision signal"; [section]: "Using LLMs to generate a relevance-based ranking order is more suitable for retriever training than depending on LLMs output probabilities"

### Mechanism 2
- Claim: The two-stage distillation pipeline (LLM→ranker→retriever) creates a stable knowledge transfer pathway
- Mechanism: First trains a ranker to mimic LLM ranking behavior, then uses KL-divergence to align retriever with ranker's learned similarity distribution
- Core assumption: The intermediate ranker acts as a stable knowledge bridge that can be trained reliably
- Evidence anchors: [abstract]: "employs a two-stage pipeline: first distilling knowledge from LLMs to a ranker model using list-wise ranking orders, then transferring this knowledge to the retriever via KL-divergence minimization"; [section]: "In Stage 2, this distilled ranker enhances the original retriever by minimizing the KL-divergence between their similarity likelihood"

### Mechanism 3
- Claim: ListMLE loss function effectively captures relative ordering information from LLM rankings
- Mechanism: ListMLE optimizes the ranker by maximizing the probability of the correct permutation given similarity scores, rather than just pointwise ranking
- Core assumption: List-wise ranking losses better preserve ordering relationships than pairwise or pointwise alternatives
- Evidence anchors: [section]: "We use ListMLE (Xia et al., 2008), a list-wise loss function for distillation training from LLMs to the ranker model"; [section]: "ListMLE considers the similarity likelihood PRAN K(Dn) as the predicted list and πDn as the ground truth"

## Foundational Learning

- Concept: KL-divergence minimization
  - Why needed here: Used to align retriever similarity distribution with ranker's learned distribution in Stage 2
  - Quick check question: What does minimizing KL-divergence between PRANK and PRETR accomplish in the distillation process?

- Concept: List-wise ranking vs pointwise ranking
  - Why needed here: ListMLE operates on entire ranking permutations rather than individual document comparisons
  - Quick check question: How does ListMLE differ from pointwise ranking loss when optimizing for ranking order?

- Concept: Dual-encoder architecture
  - Why needed here: Both ranker and retriever use dual-encoder structure to separately encode queries and documents
  - Quick check question: Why is the dual-encoder structure particularly suitable for retrieval tasks compared to encoder-decoder models?

## Architecture Onboarding

- Component map: Query → Retriever → Document Subset → LLM → Ranking Order → Ranker Training → Retriever Training

- Critical path: Query → Retriever → Document Subset → LLM → Ranking Order → Ranker Training → Retriever Training

- Design tradeoffs:
  - Two-stage vs direct distillation: Two-stage provides stability but adds complexity
  - ListMLE vs other ranking losses: Better for ordering but potentially harder to optimize
  - Size of document subset (Dn): Larger subsets provide more supervision but increase computational cost

- Failure signatures:
  - Poor ranker performance: LLM rankings not being learned effectively
  - Retriever degradation: KL-divergence alignment not working as expected
  - Overfitting: Model memorizing training instances rather than learning general patterns

- First 3 experiments:
  1. Baseline: Train retriever directly on retrieval task without any LLM supervision
  2. Single-stage: Train retriever directly from LLM similarity scores (Direct Distillation variant)
  3. Two-stage validation: Train ranker first, then measure its ability to reproduce LLM rankings before proceeding to retriever training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Intermediate Distillation approach scale when trained on significantly larger datasets (tens of thousands or millions of instances) rather than the 1,000 instances used in the experiments?
- Basis in paper: [explicit] The paper explicitly mentions that they did not evaluate their proposed distillation scheme with larger scales of training data due to budget limitations on accessing responses from closed-source LLMs and insufficient computational resources to utilize high-quality open-source LLMs like Llama-70B. They note this as a limitation and state that in future work, they will focus on extending this study to larger-scale training data.
- Why unresolved: The authors acknowledge budget and computational resource constraints prevented them from testing larger-scale training. They recognize this as an important area for future research.
- What evidence would resolve it: Conducting experiments with training datasets of 10,000, 50,000, and 100,000+ instances using both closed-source and open-source LLMs, comparing performance metrics across different dataset sizes to identify scaling patterns and potential diminishing returns.

### Open Question 2
- Question: Why does the Rule-Based supervision signal (placing documents containing answers at the top) perform worse than text similarity-based re-ranking signals, and what does this reveal about the nature of effective retrieval supervision?
- Basis in paper: [explicit] The paper states that Rule-Based supervision signals are "ineffective and detrimentally impacting the retriever's performance" and that "considering the semantic similarity of the text is far more important than arranging documents containing the answers to the top for re-ranking in distillation training."
- Why unresolved: While the paper demonstrates the inferiority of Rule-Based signals through experimental results, it doesn't deeply explore the underlying reasons why semantic similarity provides better supervision or what this reveals about the fundamental differences between retrieval and answer extraction tasks.
- What evidence would resolve it: Detailed analysis of where Rule-Based approaches fail (e.g., false positives, semantic mismatches) compared to similarity-based approaches, potentially through error analysis, qualitative case studies, and investigation of whether this pattern holds across different domains and question types.

### Open Question 3
- Question: What is the optimal re-ranking list size for the Intermediate Distillation approach, and how does this optimal size vary with different dataset characteristics or LLM capabilities?
- Basis in paper: [inferred] The paper conducts experiments varying the re-ranking list size from 3 to 10 documents and observes that performance improves with larger lists, but doesn't identify an optimal point or investigate whether this relationship is linear, logarithmic, or has diminishing returns.
- Why unresolved: The experiments show a positive trend with larger re-ranking lists but don't explore the upper bounds of this relationship or whether there's a point of diminishing returns. The paper also doesn't investigate whether optimal list size depends on factors like dataset complexity, query diversity, or LLM quality.
- What evidence would resolve it: Comprehensive experiments testing re-ranking list sizes from 2 to 50+ documents, analyzing performance curves to identify optimal points, and conducting ablation studies to determine whether optimal size varies by dataset characteristics (e.g., NQ vs. TriviaQA) or LLM capabilities (GPT-3.5 vs. GPT-4).

## Limitations
- The claim that ranking orders are superior to probability scores lacks direct empirical validation
- Computational efficiency claims are based on reduced data requirements rather than actual training time measurements
- The method's generalizability to domains beyond question-answering is untested
- Reliance on LLM-generated rankings introduces potential brittleness if LLM behavior changes

## Confidence

**High Confidence**: The experimental methodology is sound, with proper dataset splits and evaluation metrics. The retrieval and QA performance improvements on NQ and TriviaQA datasets are well-documented and reproducible.

**Medium Confidence**: The two-stage distillation architecture is theoretically justified and the implementation details are clear. The use of ListMLE and KL-divergence for knowledge transfer is appropriate, though the necessity of both stages could be questioned.

**Low Confidence**: The claim that ranking orders are inherently superior to probability scores for supervision lacks direct empirical support. The computational efficiency claims are indirect, and the method's robustness to LLM behavior changes is untested.

## Next Checks

1. **Direct comparison experiment**: Implement a one-stage direct distillation approach where the retriever is trained directly from LLM similarity scores (bypassing the ranker), then compare performance against the two-stage Intermediate Distillation method to validate the necessity of the intermediate ranker.

2. **Ranking order stability test**: Measure the consistency of LLM-generated ranking orders across multiple generations with the same input. Calculate inter-annotator agreement or ranking stability metrics to empirically validate the assumption that ranking orders are more stable than probability scores.

3. **Cross-domain generalization study**: Apply the Intermediate Distillation method to a different domain (e.g., biomedical literature retrieval or legal document search) using the same 1,000 sample training strategy. This would test whether the method's data efficiency generalizes beyond question-answering tasks.