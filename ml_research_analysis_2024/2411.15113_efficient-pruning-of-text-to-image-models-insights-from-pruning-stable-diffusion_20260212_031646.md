---
ver: rpa2
title: 'Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable Diffusion'
arxiv_id: '2411.15113'
source_url: https://arxiv.org/abs/2411.15113
tags:
- pruning
- diffusion
- sparsity
- text
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates post-training pruning of Stable Diffusion
  2, a large text-to-image model with 1.2 billion parameters. We examine the impact
  of pruning on the textual component (CLIP text encoder) and image generation component
  (U-Net diffusion generator) separately.
---

# Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable Diffusion

## Quick Facts
- arXiv ID: 2411.15113
- Source URL: https://arxiv.org/abs/2411.15113
- Authors: Samarth N Ramesh; Zhixue Zhao
- Reference count: 40
- This study investigates post-training pruning of Stable Diffusion 2, finding that magnitude pruning outperforms advanced techniques like Wanda and SparseGPT, with optimal pruning at 47.5% text encoder sparsity and 35% diffusion generator sparsity.

## Executive Summary
This study investigates post-training pruning of Stable Diffusion 2, a large text-to-image model with 1.2 billion parameters, examining the impact on its textual component (CLIP text encoder) and image generation component (U-Net diffusion generator) separately. The research reveals that contrary to established trends in language model pruning, simple magnitude pruning outperforms more advanced techniques like Wanda and SparseGPT in this text-to-image context. The findings show that the text encoder can be pruned to 47.5% sparsity and the diffusion generator to 35% sparsity with minimal quality loss, achieving a total sparsity of 38.5%. Beyond these thresholds, models experience sudden performance drops characterized by unreadable images, suggesting that specific weights encode critical semantic information.

## Method Summary
The study applies magnitude pruning, Wanda pruning, SparseGPT, and OWL to Stable Diffusion 2's CLIP text encoder (340M parameters) and U-Net diffusion generator (860M parameters) components independently at various sparsity levels (10% increments, with finer intervals at critical thresholds). The pruned models are evaluated using FID (Fréchet Inception Distance) and CLIP Score metrics on 10,000 generated images from MSCOCO 2017 captions. The research determines optimal full model configuration by combining the best-performing component sparsities (47.5% text encoder, 35% diffusion generator).

## Key Results
- Magnitude pruning outperforms advanced techniques like Wanda and SparseGPT specifically for text-to-image model compression
- Text encoder can be pruned to 47.5% sparsity while diffusion generator can be pruned to 35% sparsity with minimal quality loss
- Pruning beyond these thresholds causes sudden performance drops with unreadable images, suggesting critical weights encode essential semantic information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Magnitude pruning outperforms Wanda and SparseGPT specifically for text-to-image model compression.
- Mechanism: Text-to-image models encode different semantic relationships between text and image components than language models, making simpler pruning criteria more effective at preserving these cross-modal dependencies.
- Core assumption: The critical weights for cross-modal understanding are more uniformly distributed and less dependent on outlier activations than in pure language models.
- Evidence anchors:
  - [abstract]: "contrary to established trends in language model pruning, we discover that simple magnitude pruning outperforms more advanced techniques in text-to-image context"
  - [section]: "the finding that Wanda pruning is outperformed by magnitude pruning is quite surprising and underscores the need for pruning techniques tailored specifically to text-to-image models"
  - [corpus]: Weak evidence - corpus contains no directly comparable pruning comparisons for text-to-image models

### Mechanism 2
- Claim: The text encoder and diffusion generator components have different pruning resilience thresholds (47.5% vs 35% sparsity).
- Mechanism: The components process fundamentally different types of information - the text encoder handles semantic encoding while the diffusion generator handles spatial generation - leading to different redundancy patterns and critical weight distributions.
- Core assumption: Information redundancy and criticality are component-specific rather than uniform across the model architecture.
- Evidence anchors:
  - [abstract]: "we propose an optimal pruning configuration that prunes the text encoder to 47.5% and the diffusion generator to 35%"
  - [section]: "Both the text encoder and the diffusion generator exhibit identifiable drop-off points in performance"
  - [corpus]: No direct corpus evidence for component-specific pruning thresholds in text-to-image models

### Mechanism 3
- Claim: Pruning beyond certain thresholds causes sudden performance drops characterized by unreadable images.
- Mechanism: Specific weights encode critical semantic information that cannot be reconstructed from remaining weights, and their removal causes catastrophic failure in cross-modal alignment.
- Core assumption: Text-to-image models rely on precise semantic-image mappings that are fragile to weight removal beyond certain thresholds.
- Evidence anchors:
  - [abstract]: "pruning beyond certain thresholds leads to sudden performance drops (unreadable images), suggesting that specific weights encode critical semantics information"
  - [section]: "At 62.5% sparsity, the model struggles to correctly interpret the prompt, as evidenced by a misframed dog and the absence of a field"
  - [corpus]: No corpus evidence for sudden drop mechanisms in text-to-image pruning

## Foundational Learning

- Concept: Diffusion models and denoising process
  - Why needed here: Understanding how Stable Diffusion generates images through iterative denoising is crucial for grasping why pruning affects different components differently
  - Quick check question: What is the key difference between training diffusion models in latent space versus pixel space, and how does this affect pruning strategies?

- Concept: Cross-attention mechanisms in multimodal models
  - Why needed here: The text encoder and diffusion generator interact through cross-attention layers, making their pruning interdependencies critical to understand
  - Quick check question: How do cross-attention layers facilitate the connection between textual semantics and image generation, and why might this make them particularly sensitive to pruning?

- Concept: FID and CLIP Score evaluation metrics
  - Why needed here: These metrics provide complementary views of model quality - FID measures image distribution similarity while CLIP Score measures semantic alignment
  - Quick check question: Why might text encoder pruning affect CLIP Score more than FID, while diffusion generator pruning affects FID more than CLIP Score?

## Architecture Onboarding

- Component map: CLIP text encoder (340M parameters, 28%) → cross-attention → U-Net diffusion generator (860M parameters, 72%) → VAE decoder → final image
- Critical path: Text prompt → CLIP text encoder → cross-attention → U-Net diffusion generator → latent space → VAE decoder → final image
- Design tradeoffs: Component size disparity (text encoder much smaller) affects pruning distribution; text component more sensitive to pruning but smaller size limits overall impact
- Failure signatures: Sudden image degradation beyond component-specific thresholds; text component failures show semantic misalignment; diffusion generator failures show spatial/structural degradation
- First 3 experiments:
  1. Test magnitude pruning on text encoder alone at 10% intervals to identify drop-off threshold
  2. Test magnitude pruning on diffusion generator alone at 10% intervals to identify gradual degradation point
  3. Apply optimal component sparsities (47.5% text, 35% image) to full model and evaluate FID/CLIP Score compared to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does magnitude pruning outperform Wanda and SparseGPT in text-to-image models when these advanced techniques typically perform better in language models?
- Basis in paper: [explicit] The paper states that "contrary to established trends in language model pruning, we discover that simple magnitude pruning outperforms more advanced techniques in text-to-image context."
- Why unresolved: The paper demonstrates this unexpected result but does not explain the underlying reasons for this performance difference between model types.
- What evidence would resolve it: Comparative analysis of weight distributions and outlier patterns in text-to-image versus language models, along with theoretical explanations for why magnitude-based importance correlates better with performance in this domain.

### Open Question 2
- Question: What specific information do the weights encode that causes sudden performance drops when pruning beyond certain thresholds?
- Basis in paper: [explicit] The paper observes that "pruning beyond certain thresholds leads to sudden performance drops (unreadable images), suggesting that specific weights encode critical semantics information."
- Why unresolved: While the paper identifies this phenomenon, it does not investigate which semantic information is encoded in these critical weights or why their removal causes such abrupt failures.
- What evidence would resolve it: Analysis of weight activation patterns, correlation with specific semantic features, and ablation studies targeting the identified critical weight regions.

### Open Question 3
- Question: Are the identified pruning thresholds (47.5% for text encoder, 35% for diffusion generator) generalizable to other text-to-image models like Stable Diffusion 3 or larger architectures?
- Basis in paper: [inferred] The paper notes that "these unexpected results highlight the unique challenges and opportunities in pruning text-to-image models" and suggests applying findings to "more recent models" in future work.
- Why unresolved: The study only tested on Stable Diffusion 2, so it's unclear whether these thresholds apply to different model architectures or sizes.
- What evidence would resolve it: Systematic pruning experiments across multiple text-to-image model architectures and sizes to determine if similar thresholds exist and what factors influence their values.

## Limitations
- The study focuses exclusively on Stable Diffusion 2.0, which may not generalize to other text-to-image models or different versions
- Evaluation uses only MSCOCO 2017 captions, representing a relatively narrow domain of image generation tasks
- The study does not explore fine-tuning after pruning, which could potentially recover some lost performance

## Confidence
**High Confidence:** The finding that text encoder and diffusion generator components have different pruning resilience thresholds (47.5% vs 35%) is well-supported by systematic experimentation with multiple sparsity levels.

**Medium Confidence:** The superiority of magnitude pruning over Wanda and SparseGPT in this context is demonstrated but needs replication across different model architectures and datasets to establish as a general principle.

**Medium Confidence:** The characterization of sudden performance drops beyond threshold sparsities is observed in the experiments, though the exact mechanisms causing these drops require further investigation.

## Next Checks
1. **Cross-model validation:** Test the optimal pruning configuration (47.5% text encoder, 35% diffusion generator) on Stable Diffusion 1.x and other latent diffusion models to assess generalizability.

2. **Fine-tuning recovery:** Apply task-specific fine-tuning to pruned models and measure whether performance can be recovered beyond the identified thresholds, potentially enabling higher sparsity levels.

3. **Ablation of pruning techniques:** Conduct more granular comparisons between magnitude pruning, Wanda, and SparseGPT across different layers and attention mechanisms to better understand why magnitude pruning outperforms in this context.