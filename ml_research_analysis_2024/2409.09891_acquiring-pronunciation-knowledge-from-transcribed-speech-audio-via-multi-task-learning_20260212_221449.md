---
ver: rpa2
title: Acquiring Pronunciation Knowledge from Transcribed Speech Audio via Multi-task
  Learning
arxiv_id: '2409.09891'
source_url: https://arxiv.org/abs/2409.09891
tags:
- speech
- pronunciation
- training
- audio
- frontend
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel multi-task learning (MTL) approach to
  improve Seq2Seq text-to-speech (TTS) frontends by leveraging transcribed speech
  audio. The method addresses the limitation of fixed lexical coverage in bootstrapping
  training data for TTS frontends.
---

# Acquiring Pronunciation Knowledge from Transcribed Speech Audio via Multi-task Learning

## Quick Facts
- arXiv ID: 2409.09891
- Source URL: https://arxiv.org/abs/2409.09891
- Reference count: 40
- The paper proposes a multi-task learning approach to improve Seq2Seq TTS frontends by leveraging transcribed speech audio

## Executive Summary
This paper addresses the limitation of fixed lexical coverage in bootstrapping training data for Seq2Seq text-to-speech (TTS) frontends. The authors propose a novel multi-task learning (MTL) approach that jointly learns pronunciation prediction and acoustic feature regression using a shared Text Encoder representation. By leveraging transcribed speech audio, the method transfers pronunciation knowledge to improve accuracy for words not covered in the initial bootstrapping data. Experiments demonstrate significant improvements in phone error rate for exclusive word types while maintaining simpler implementation compared to previous approaches.

## Method Summary
The proposed method integrates an acoustic decoder into a Seq2Seq frontend architecture, creating a multi-task learning framework. The Text Encoder is shared between the main pronunciation prediction task and the auxiliary acoustic feature regression task. The acoustic decoder attends to intermediate representations of the Seq2Seq frontend to regress acoustic features (mel-spectrograms) from text. Training involves bootstrapping data from LibriSpeech text for the main task and transcribed speech from Hi-Fi TTS for the extra task, with L1 loss on mel-spectrograms. The shared representation allows knowledge transfer from acoustic features to improve pronunciation accuracy for words exclusive to the transcribed speech data.

## Key Results
- Phone error rate (PER) reduced from 2.5% to 1.6% for word types covered exclusively in transcribed speech audio
- Method achieves similar performance to previous Hidden Pronunciation approach with simpler implementation
- Significant improvements observed for extra-exclusive words while maintaining performance on main-covered words

## Why This Works (Mechanism)

### Mechanism 1
The multi-task learning framework transfers pronunciation knowledge from transcribed speech audio to the Seq2Seq frontend by sharing the Text Encoder parameters between main and auxiliary tasks. The shared Text Encoder extracts linguistic representations beneficial for both pronunciation prediction and acoustic feature regression, allowing the model to learn acoustic-phonetic correspondences that improve pronunciation accuracy for words not seen during initial bootstrapping.

### Mechanism 2
The auxiliary acoustic feature regression task acts as a regularizer preventing overfitting to bootstrapping data and improving generalization to unseen words. Training on transcribed speech audio forces the model to develop more robust and generalizable linguistic representations that can handle a wider range of vocabulary by mapping text to acoustic features for words not covered in the bootstrapping data.

### Mechanism 3
The multi-task model architecture allows the auxiliary task to attend to different levels of the Seq2Seq frontend's intermediate representations, enabling flexible integration of acoustic information. By attending to Text Encoder output, Pronunciation Decoder hidden states, or sampled tokens' embeddings, the acoustic decoder can integrate acoustic information at various stages of the linguistic processing pipeline, potentially capturing different aspects of the pronunciation-acoustic relationship.

## Foundational Learning

- **Multi-task learning (MTL)**: Why needed - Core framework enabling transfer of pronunciation knowledge from transcribed speech audio; Quick check - What is the main difference between multi-task learning and transfer learning, and how does this distinction apply to the proposed method?

- **Sequence-to-sequence (Seq2Seq) models**: Why needed - Primary model being improved; Quick check - In a standard Seq2Seq model for text-to-speech, what are the inputs and outputs, and how does the attention mechanism facilitate the mapping between them?

- **Acoustic feature extraction**: Why needed - Auxiliary task involves regressing acoustic features from text; Quick check - What is the difference between mel-spectrograms and MFCCs, and which one is used in the proposed method? Why might one be preferred over the other for this task?

## Architecture Onboarding

- **Component map**: Text Encoder (Bi-directional LSTM) -> Pronunciation Decoder (LSTM with attention) -> Pronunciation sequence; Text Encoder -> Acoustic Decoder (LSTM with attention) -> Acoustic features (mel-spectrograms)

- **Critical path**: 1) Text is encoded by Text Encoder; 2) Pronunciation Decoder uses encoding to generate pronunciation sequence; 3) Acoustic Decoder uses encoding to regress acoustic features; 4) Both decoders' outputs compared to targets to compute multi-task loss; 5) Gradients backpropagated to update shared Text Encoder and respective decoder parameters

- **Design tradeoffs**: Level of integration offers flexibility but increases complexity; Mel-spectrograms vs. MFCCs involves tradeoff between feature richness and computational efficiency; Weighting factor (Î») needs careful tuning to balance pronunciation and acoustic tasks

- **Failure signatures**: Degradation in pronunciation accuracy for bootstrapping words indicates harmful interference; No improvement for extra-exclusive words suggests ineffective transfer; Overfitting to transcribed speech manifests as poor generalization to new words

- **First 3 experiments**: 1) Ablation study with only pronunciation task to establish baseline; 2) Level comparison training acoustic decoder at different intermediate representation levels; 3) Feature comparison using mel-spectrograms vs. MFCCs

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions, but raises implicit ones about generalization to other languages, handling of transcription errors, and optimal acoustic feature types

## Limitations
- Evaluation focuses on controlled test splits rather than truly unseen vocabulary in real-world deployment
- Specific mechanisms by which acoustic feature regression improves pronunciation prediction remain underspecified
- Limited sensitivity analysis showing robustness to architectural choices and hyperparameter settings

## Confidence
- **High confidence** in core finding that multi-task learning improves pronunciation accuracy for exclusive word types (PER reduction from 2.5% to 1.6%)
- **Medium confidence** in claim of similar performance to previous work with simpler implementation
- **Medium confidence** in general mechanism that shared representations facilitate transfer between tasks

## Next Checks
1. **Ablation of acoustic decoder components**: Systematically remove or modify acoustic decoder components to determine which are essential for performance improvements

2. **Cross-domain generalization test**: Evaluate trained model on completely different corpus (different accent, domain, or recording conditions) to assess generalization beyond training dataset

3. **Phonetic feature sensitivity analysis**: Analyze which specific phonetic features (vowels, consonants, stress patterns, syllable boundaries) benefit most from multi-task learning approach