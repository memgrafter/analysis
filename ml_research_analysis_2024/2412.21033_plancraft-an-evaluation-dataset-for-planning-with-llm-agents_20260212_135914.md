---
ver: rpa2
title: 'Plancraft: an evaluation dataset for planning with LLM agents'
arxiv_id: '2412.21033'
source_url: https://arxiv.org/abs/2412.21033
tags:
- llama
- crafting
- quantity
- plancraft
- inventory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Plancraft is a new multi-modal dataset for evaluating LLM agents
  in planning tasks based on Minecraft crafting. It includes both text-only and image-based
  interfaces, a Minecraft Wiki for retrieval-augmented generation, and a handcrafted
  planner for benchmarking.
---

# Plancraft: an evaluation dataset for planning with LLM agents

## Quick Facts
- arXiv ID: 2412.21033
- Source URL: https://arxiv.org/abs/2412.21033
- Authors: Gautier Dagan; Frank Keller; Alex Lascarides
- Reference count: 40
- Key outcome: New multi-modal dataset for evaluating LLM agents in Minecraft crafting tasks, showing larger models perform better but struggle with image inputs

## Executive Summary
Plancraft is a new dataset for evaluating large language model (LLM) agents on multi-modal planning tasks based on Minecraft crafting. The dataset includes both text-only and image-based interfaces, a Minecraft Wiki for retrieval-augmented generation, and a handcrafted planner for benchmarking. The paper evaluates various open-source and closed-source LLMs on tasks ranging from simple crafting to complex multi-step planning, including intentionally unsolvable tasks to test agents' ability to assess task feasibility. Results show that larger models perform better, search and think actions improve success rates, but models struggle with image inputs and fine-tuning limits their ability to use new actions.

## Method Summary
The Plancraft dataset was created by sampling craftable items from Minecraft's 976 total items and generating tasks with random inventories containing 4-16 distractor items. A handcrafted planner using memoized depth-first search over the crafting dependency graph generated expert trajectories for benchmarking. The evaluation includes text-only and multi-modal environments, with the latter using a bounding box detection model to convert visual observations into text format. Agents can use move, smelt, think, search, and impossible actions. The search action queries the Minecraft Wiki knowledge base, while the think action allows reasoning traces. The impossible action enables agents to declare tasks unsolvable. The dataset includes 1,145 expert trajectories and 50 test tasks.

## Key Results
- Llama 70B achieved 36% success rate with move and smelt actions, improving to 41% when think and search actions were enabled
- Closed-source models (GPT-4o, Claude 3.5 Sonnet) significantly outperformed open-source models on text-only tasks
- Models struggled with image inputs, with Llama 70B achieving only 4% success rate on multi-modal tasks
- Fine-tuning improved success rates but severely limited models' ability to use new actions like "impossible"

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The handcrafted planner provides a reliable baseline for evaluating LLM agents by representing the optimal solution path for each task
- Mechanism: The planner uses memoized depth-first search over the crafting dependency graph, exploring all possible crafting recipes from the initial inventory to the target item
- Core assumption: The crafting system is deterministic and fully observable with finite items (976) and recipes (859)
- Evidence anchors:
  - [abstract] "as well as a handcrafted planner and Oracle Retriever, to ablate the different components of a modern agent architecture"
  - [section 3.4] "We implement the solver as a memoized Depth-First search over all possible crafting recipes given an inventory"
  - [corpus] Weak - no direct corpus evidence about handcrafted planners in this specific domain
- Break condition: If the crafting system becomes non-deterministic, introduces hidden states, or exceeds computational limits for exhaustive search

### Mechanism 2
- Claim: Retrieval-Augmented Generation (RAG) with the Minecraft Wiki significantly improves agent performance by providing accurate crafting recipes
- Mechanism: The search action queries the Minecraft Wiki knowledge base, returning gold-label recipe instantiations for requested items
- Core assumption: The Minecraft Wiki contains complete and accurate recipes for all craftable items
- Evidence anchors:
  - [abstract] "We include the Minecraft Wiki to evaluate tool use and Retrieval Augmented Generation (RAG)"
  - [section 3.5] "we design the search operator to return a gold-label instantiation of a recipe required to craft the particular item"
  - [corpus] Weak - limited evidence about RAG effectiveness in crafting domains specifically
- Break condition: If the knowledge base becomes incomplete, contains errors, or agents cannot effectively parse and apply retrieved information

### Mechanism 3
- Claim: Fine-tuning smaller models on expert plans significantly improves task success rates while limiting their ability to use new actions
- Mechanism: LoRA fine-tuning on 1145 planning trajectories constrains the model's behavior to match expert solutions
- Core assumption: Expert trajectories contain sufficient diversity and coverage to generalize across the task distribution
- Evidence anchors:
  - [abstract] "We also fine-tune a bounding-box detection model on our environment to provide an interface through which text-only LLMs can interact with the multi-modal environment"
  - [section 4] "As mentioned in Section 4, we also fine-tune an LLM on expert plans (Llama 8B FT)"
  - [corpus] Weak - limited evidence about fine-tuning impacts on agent action flexibility
- Break condition: If expert trajectories are biased or limited, or if the fine-tuned model becomes too rigid to handle novel situations

## Foundational Learning

- Concept: Depth-First Search (DFS) with memoization
  - Why needed here: Efficiently explores the large crafting dependency space while avoiding redundant computation
  - Quick check question: How does memoization prevent the planner from recalculating the same crafting paths multiple times?

- Concept: Retrieval-Augmented Generation (RAG) systems
  - Why needed here: Provides external knowledge access to supplement the agent's limited context window and pretraining knowledge
  - Quick check question: What are the key differences between RAG and traditional retrieval methods in agent decision-making?

- Concept: Multi-modal input processing
  - Why needed here: Enables agents to work with both text descriptions and visual observations of the crafting interface
  - Quick check question: How does the bounding box detection model convert visual observations into the same format as text observations?

## Architecture Onboarding

- Component map:
  Environment -> Observation (text/image) -> Agent Model -> Action (move/smelt/think/search/impossible) -> Environment Feedback -> Plan Update -> Task Completion/Failure

- Critical path:
  1. Agent receives observation (text/image) and task goal
  2. Agent generates action (move, smelt, think, search, impossible)
  3. Environment processes action and returns feedback
  4. Agent updates plan based on new observation
  5. Repeat until task completion or failure

- Design tradeoffs:
  - Text-only vs multi-modal: Simpler implementation vs richer information access
  - Expert planner vs agent autonomy: Reliable baseline vs learning generalization
  - Fine-tuning vs few-shot: Better performance vs greater flexibility
  - Impossible action: More realistic evaluation vs potential premature task termination

- Failure signatures:
  - Consistent failure on shaped recipes: Indicates spatial reasoning limitations
  - High token usage with low success: Suggests inefficient planning strategies
  - Failure to use search action after fine-tuning: Shows over-constraint from expert trajectories
  - Poor performance on image inputs: Reveals multi-modal processing limitations

- First 3 experiments:
  1. Test Llama 70B with only move/smelt actions on easy tasks to establish baseline performance
  2. Add think action to evaluate impact of reasoning traces on plan quality
  3. Enable search action to measure knowledge base utilization effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would different RAG retrieval strategies (e.g., dense vs. sparse) affect the performance of LLM agents on Plancraft tasks?
- Basis in paper: The paper uses an Oracle Retriever with exact match for recipes, noting this provides a best-case scenario baseline
- Why unresolved: The paper only evaluates one retrieval method and acknowledges that real-world RAG systems would need to handle partial matches and ambiguous queries
- What evidence would resolve it: Comparing agent performance using different retrieval strategies (BM25, dense retrieval, hybrid approaches) while keeping other factors constant

### Open Question 2
- Question: What is the impact of inventory complexity (number of distractor items) on agent performance across different model sizes?
- Basis in paper: The paper samples 4-16 distractor items to make identifying correct crafting paths more challenging, but doesn't analyze how this affects performance
- Why unresolved: The paper reports overall success rates but doesn't break down performance by inventory complexity levels or compare model size effects
- What evidence would resolve it: Systematic evaluation of agent performance across varying numbers of distractor items for each model size

### Open Question 3
- Question: How would fine-tuning on impossible tasks affect an agent's ability to correctly identify unsolvable problems without compromising performance on solvable tasks?
- Basis in paper: The paper finds that fine-tuning improves task success but severely decreases the model's ability to use new actions like "impossible"
- Why unresolved: The paper only fine-tunes on successful trajectories and doesn't explore how training on both solvable and unsolvable examples would affect the impossible action's effectiveness
- What evidence would resolve it: Fine-tuning experiments that include both successful and failed trajectories, measuring the trade-off between general task success and impossible prediction accuracy

## Limitations

- Task Generation Bias: The handcrafted planner may introduce systematic biases toward certain solution patterns
- Knowledge Base Dependence: Evaluation assumes the Minecraft Wiki contains complete and accurate recipes for all craftable items
- Multi-modal Interface Limitations: The bounding box detection model introduces an additional abstraction layer with limited evaluation of its accuracy

## Confidence

**High Confidence**: Larger models (70B+ parameters) outperform smaller models on planning tasks
**Medium Confidence**: Think and search actions improve success rates, though experimental design makes it difficult to isolate individual contributions
**Low Confidence**: Fine-tuning significantly improves success rates while limiting action flexibility based on limited experimental evidence

## Next Checks

**Validation Check 1**: Replicate the task generation process with an independent planner or random task sampling to assess whether the handcrafted planner introduces systematic biases

**Validation Check 2**: Conduct ablation studies specifically isolating the think action from the search action by testing models with only one enabled at a time

**Validation Check 3**: Evaluate the multi-modal interface reliability by measuring the accuracy of the bounding box detection model in converting visual observations to text format