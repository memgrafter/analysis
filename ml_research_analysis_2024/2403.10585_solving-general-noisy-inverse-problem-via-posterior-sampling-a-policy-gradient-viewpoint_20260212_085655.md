---
ver: rpa2
title: 'Solving General Noisy Inverse Problem via Posterior Sampling: A Policy Gradient
  Viewpoint'
arxiv_id: '2403.10585'
source_url: https://arxiv.org/abs/2403.10585
tags:
- image
- diffusion
- function
- input
- deblurring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Diffusion Policy Gradient (DPG), a method
  for solving general noisy inverse problems using posterior sampling in diffusion
  models. The key innovation is viewing intermediate noisy images as policies and
  the target image as states, enabling a policy gradient approach to estimate the
  guidance score function.
---

# Solving General Noisy Inverse Problem via Posterior Sampling: A Policy Gradient Viewpoint

## Quick Facts
- **arXiv ID**: 2403.10585
- **Source URL**: https://arxiv.org/abs/2403.10585
- **Reference count**: 21
- **Primary result**: DPG achieves superior image restoration quality compared to state-of-the-art methods like DPS, DDNM+, and DDRM, with better FID and LPIPS scores across tasks including inpainting, super-resolution, and deblurring under both Gaussian and Poisson noise.

## Executive Summary
This paper introduces Diffusion Policy Gradient (DPG), a method for solving general noisy inverse problems using posterior sampling in diffusion models. The key innovation is viewing intermediate noisy images as policies and the target image as states, enabling a policy gradient approach to estimate the guidance score function. Unlike existing methods that rely on SVD decomposition or direct plug-in of input images, DPG handles both linear and non-linear inverse problems without task-specific fine-tuning. Experiments on FFHQ, ImageNet, and LSUN datasets show DPG achieves superior image restoration quality compared to state-of-the-art methods like DPS, DDNM+, and DDRM, with better FID and LPIPS scores across tasks including inpainting, super-resolution, and deblurring under both Gaussian and Poisson noise.

## Method Summary
DPG solves inverse problems by reframing posterior sampling as a reinforcement learning problem where noisy images are policies and clean images are states. The method uses Monte Carlo sampling from a Gaussian approximation of the posterior to compute the score function, incorporating reward shaping to reduce variance. Unlike existing methods that rely on SVD decomposition or direct plug-in of input images, DPG handles both linear and non-linear inverse problems without task-specific fine-tuning. The method uses Tweedie's formula to estimate the posterior mean and a leave-one-out bias subtraction technique to reduce variance in the Monte Carlo policy gradient estimate.

## Key Results
- DPG achieves superior FID and LPIPS scores compared to DPS, DDNM+, and DDRM across inpainting, super-resolution, and deblurring tasks
- DPG works for both linear and non-linear inverse problems without task-specific fine-tuning
- The method is effective under both Gaussian and Poisson noise models
- DPG shows consistent performance improvements across FFHQ, ImageNet, and LSUN datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Viewing intermediate noisy images as policies and the target image as states enables Monte Carlo estimation of the guidance score function without needing the true posterior density.
- **Mechanism:** The paper reframes posterior sampling as a reinforcement learning problem where each noisy image xi acts as a policy, and the clean image x0 is the state. Using policy gradient methods, they estimate the expected cost E[p0|i(x0|xi)[p0(y|x0)] by sampling x0 from a Gaussian approximation q0|i(x0|xi) and computing the gradient via Monte Carlo.
- **Core assumption:** The Gaussian approximation q0|i(x0|xi) = N(µi(xi), r²ᵢI) closely matches the true posterior p0|i(x0|xi) in shape, so MC sampling is representative.
- **Break condition:** If the Gaussian approximation poorly matches the true posterior (e.g., highly multimodal posteriors), the Monte Carlo estimates will be biased and lead to poor guidance.

### Mechanism 2
- **Claim:** Reward shaping (leave-one-out bias subtraction) reduces variance in the Monte Carlo policy gradient estimate.
- **Mechanism:** For each MC sample x(m)₀, a bias term b(m)ᵢ is computed by averaging the costs of all other samples. Subtracting this bias before computing the gradient reduces the variance of the estimate, similar to baseline subtraction in policy gradient RL.
- **Core assumption:** The bias computed via leave-one-out is a good estimate of the expected cost for each sample, reducing variance without introducing bias.
- **Break condition:** If the number of MC samples is too small, the leave-one-out estimate becomes noisy and may not effectively reduce variance.

### Mechanism 3
- **Claim:** DPG's score function estimate is more accurate than DPS in early diffusion steps because it uses multiple samples rather than a single MMSE estimate.
- **Mechanism:** DPS approximates the posterior with a single sample at the MMSE estimate µi(xi), leading to poor gradient estimates when xi is far from x0. DPG samples multiple points from q0|i(x0|xi) and averages, providing a better estimate of the expected gradient.
- **Core assumption:** Multiple samples from q0|i(x0|xi) provide a better approximation of the expected gradient than a single point estimate.
- **Break condition:** If Nmc is too small, the MC estimate will have high variance and may not outperform the single-sample DPS approach.

## Foundational Learning

- **Concept:** Diffusion models and reverse-time SDEs
  - Why needed here: The method relies on the reverse diffusion process to generate images while incorporating guidance from the input. Understanding the forward SDE, marginal distributions, and the reverse process is essential.
  - Quick check question: What is the role of the score function ∇xi log pi(xi) in the DDIM sampling equation?

- **Concept:** Reinforcement learning policy gradients
  - Why needed here: The paper frames posterior sampling as a policy gradient problem, where noisy images are policies and clean images are states. Understanding REINFORCE and variance reduction techniques is crucial.
  - Quick check question: How does reward shaping (baseline subtraction) reduce variance in policy gradient estimates?

- **Concept:** Bayesian inference and posterior sampling
  - Why needed here: The goal is to sample from p0(x0|y) ∝ p0(x0)p0(y|x0). Understanding how to incorporate the likelihood term via guidance is key.
  - Quick check question: Why is it difficult to compute the exact posterior p0|i(x0|xi) in diffusion models?

## Architecture Onboarding

- **Component map:** Pretrained diffusion model -> Gaussian posterior approximator -> Monte Carlo sampler -> Reward shaper -> Score function rescaler -> Diffusion sampler
- **Critical path:**
  1. Start with noisy image xN ~ N(0,I)
  2. For i = N to 1:
     - Compute µi(xi) via Tweedie's formula
     - Set ri based on reconstruction error
     - Sample Nmc points from q0|i(x0|xi)
     - Compute leave-one-out biases b(m)ᵢ
     - Estimate score function via MC
     - Rescale and plug into DDIM/DDPM
  3. Return x0
- **Design tradeoffs:**
  - Nmc vs. accuracy: More samples → better score estimate but slower
  - ri selection: Small ri → better approximation but risk of numerical instability
  - B scaling: Must be tuned for stable sampling
  - DDPM vs. DDIM: DDPM more stable, DDIM faster
- **Failure signatures:**
  - High FID/LPIPS scores → poor score estimation or bad Gaussian approximation
  - Noisy outputs → too few MC samples or poorly tuned B
  - Divergence → ri too small or B too large
  - Blurry outputs → guidance too weak (ri too large or B too small)
- **First 3 experiments:**
  1. Run DPG with Nmc=1 (reduces to DPS) and compare FID/LPIPS to verify MC sampling helps
  2. Vary ri (e.g., set to 0.1×, 1×, 10× the default) and observe reconstruction error evolution
  3. Compare DDPM vs. DDIM solvers for runtime vs. quality tradeoff on a small dataset

## Open Questions the Paper Calls Out
- The paper mentions future work would like to explore applying the method on latent diffusion models and solving deblurring problems with unknown kernels, but does not provide specific open questions.

## Limitations
- The Gaussian posterior approximation may break down for highly multimodal posteriors, leading to biased score estimates
- The leave-one-out bias subtraction assumes the bias estimate is accurate enough to reduce variance without introducing bias
- The claim that DPG works for "any linear/non-linear inverse problem" without task-specific tuning is based on a limited set of experiments

## Confidence
- **High**: DPG framework correctly reframes posterior sampling as policy gradient; experimental results show consistent FID/LPIPS improvements over baselines
- **Medium**: The mechanism by which reward shaping reduces variance is theoretically sound but relies on unverified assumptions about the bias estimator quality
- **Low**: The claim that DPG works for "any linear/non-linear inverse problem" without task-specific tuning is based on a limited set of experiments and may not generalize to all problem types

## Next Checks
1. Test DPG on inverse problems with known multimodal posterals (e.g., structured sparsity or segmentation-like tasks) to assess Gaussian approximation breakdown
2. Compare variance reduction from leave-one-out bias subtraction against simpler baselines (e.g., constant baseline) to isolate its contribution
3. Evaluate DPG on out-of-distribution corruptions or extreme noise levels not seen in training to test robustness claims