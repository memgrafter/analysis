---
ver: rpa2
title: How Does Bayes Error Limit Probabilistic Robust Accuracy
arxiv_id: '2405.14923'
source_url: https://arxiv.org/abs/2405.14923
tags:
- accuracy
- probabilistic
- robustness
- robust
- vicinity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how Bayes error limits probabilistic robust
  accuracy in neural networks. The authors model probabilistic robustness as a Maximum
  A Posteriori decision rule over the vicinity of each input, then derive an optimal
  voting classifier.
---

# How Does Bayes Error Limit Probabilistic Robust Accuracy

## Quick Facts
- arXiv ID: 2405.14923
- Source URL: https://arxiv.org/abs/2405.14923
- Reference count: 40
- One-line primary result: This paper establishes theoretical upper bounds on probabilistic robust accuracy and proves that voting improves robustness, with empirical validation showing the bounds are respected by state-of-the-art training methods.

## Executive Summary
This paper studies how Bayes error limits probabilistic robust accuracy in neural networks. The authors model probabilistic robustness as a Maximum A Posteriori decision rule over the vicinity of each input, then derive an optimal voting classifier. They prove that any probabilistically robust input is also deterministically robust within a smaller vicinity, establishing an upper bound on probabilistic robust accuracy based on this reduced vicinity size. Experiments show that (1) voting always improves probabilistic robust accuracy, (2) the theoretical upper bound is respected by state-of-the-art training methods, and (3) the bound monotonically increases as the tolerance κ grows.

## Method Summary
The paper derives theoretical bounds on probabilistic robust accuracy by modeling it as a Maximum A Posteriori decision rule over a vicinity distribution. The authors establish that probabilistic robustness admits a higher upper bound than deterministic robustness because Bayes error for probabilistic robustness operates within a smaller vicinity. They prove the optimal decision rule and demonstrate that voting within the vicinity always improves accuracy. The framework is validated across four datasets (Moons, Chan, FashionMNIST, CIFAR-10) using three training methods (DA, RS, CVaR) and shows the theoretical bounds are respected empirically.

## Key Results
- Voting within the vicinity consistently improves probabilistic robust accuracy across all tested datasets and training methods
- The theoretical upper bound on probabilistic robust accuracy is respected by state-of-the-art training methods
- The upper bound monotonically increases as the tolerance κ grows, allowing significantly higher accuracy than deterministic robustness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Probabilistic robustness admits a higher upper bound than deterministic robustness because Bayes error for probabilistic robustness operates within a much smaller vicinity.
- **Mechanism:** The authors model probabilistic robustness as a Maximum A Posteriori (MAP) decision rule over a vicinity distribution. By analyzing the optimal decision rule, they show that any probabilistically robust input is also deterministically robust within a smaller vicinity, reducing the Bayes error contribution.
- **Core assumption:** The vicinity distribution is smooth (quasiconcave) and translation invariant, ensuring that the optimal classifier achieves continuity in predictions.
- **Evidence anchors:**
  - [abstract]: "We prove that with optimal probabilistic robustness, each probabilistically robust input is also deterministically robust in a smaller vicinity."
  - [section]: Theorem 3.4 formalizes that "there is a lower bound on the distance between an input x and any of its adversarial examples if probabilistic consistency is satisfied on x."
  - [corpus]: Weak. The neighbor paper "Certified Robust Accuracy of Neural Networks Are Bounded due to Bayes Errors" overlaps conceptually but lacks direct empirical support for the vicinity reduction claim.
- **Break condition:** If the vicinity distribution is not quasiconcave or if the decision rule is not optimal, the continuity argument fails and the upper bound may collapse to that of deterministic robustness.

### Mechanism 2
- **Claim:** Voting within the vicinity always improves probabilistic robust accuracy.
- **Mechanism:** By aggregating predictions over the vicinity distribution, voting approximates the MAP decision rule, which is proven to be optimal for probabilistic robustness.
- **Core assumption:** The sample size for voting is sufficiently large to approximate the expected value of predictions within the vicinity.
- **Evidence anchors:**
  - [abstract]: "We also show that voting within the vicinity always improves probabilistic robust accuracy."
  - [section]: Theorem 3.2 proves the optimal decision rule is arg max_k μ_k(x), and experiments in Table 1 confirm voting consistently improves accuracy.
  - [corpus]: Weak. No direct neighbor evidence, but the theoretical claim is supported by the paper's experiments.
- **Break condition:** If the vicinity is too small or the sample size is too low, voting may not capture the true MAP decision and could introduce noise.

### Mechanism 3
- **Claim:** The upper bound of probabilistic robust accuracy monotonically increases as the tolerance level κ grows.
- **Mechanism:** As κ increases, the consistency requirement relaxes, allowing more inputs to be classified probabilistically robustly, thereby increasing the achievable accuracy.
- **Core assumption:** The relationship between κ and the vicinity size follows a predictable scaling (e.g., V_↓κ size decreases with 1/n in high dimensions).
- **Evidence anchors:**
  - [abstract]: "We also show that the bound monotonically increases as κ grows."
  - [section]: Theorem 3.8 proves the monotonicity and bounds the probabilistic robust accuracy between deterministic and vanilla accuracy.
  - [corpus]: Weak. The neighbor papers do not provide direct evidence for the monotonicity claim.
- **Break condition:** If κ approaches 1/2 too quickly, the distinction between probabilistic and vanilla robustness may blur, invalidating the bound.

## Foundational Learning

- **Concept: Bayes Error**
  - Why needed here: Bayes error represents the irreducible error in any classifier due to inherent uncertainty in the data distribution, which directly limits the achievable accuracy for both vanilla and robust classifications.
  - Quick check question: What is the relationship between Bayes error and the minimum achievable error in a classification task?

- **Concept: Vicinity Distribution**
  - Why needed here: The vicinity distribution models the imperceptible perturbations around an input, which is central to defining both deterministic and probabilistic robustness.
  - Quick check question: How does the choice of vicinity function (e.g., L∞ vs. Gaussian) affect the robustness guarantees?

- **Concept: Maximum A Posteriori (MAP) Decision Rule**
  - Why needed here: The MAP decision rule is proven to be optimal for maximizing probabilistic robust accuracy by considering the most probable label within the vicinity.
  - Quick check question: Why is the MAP decision rule preferred over other decision rules in the context of probabilistic robustness?

## Architecture Onboarding

- **Component map:**
  - Data Input -> Model Training (DA/RS/CVaR) -> Probabilistic Robust Accuracy Calculation -> Voting Mechanism -> Theoretical Bound Comparison

- **Critical path:**
  1. Load and preprocess dataset
  2. Train classifier using one of the three algorithms
  3. Compute probabilistic robust accuracy
  4. Apply voting mechanism to improve accuracy
  5. Compare empirical results with theoretical bounds
  6. Analyze the effect of varying κ on the upper bound

- **Design tradeoffs:**
  - Vicinity size (ϵ): Larger ε increases robustness but may reduce accuracy; smaller ε tightens bounds but may be less practical
  - Sample size for voting: Larger sample sizes improve approximation of the MAP decision but increase computational cost
  - Tolerance level (κ): Higher κ allows more flexibility in robustness but may reduce the strictness of guarantees

- **Failure signatures:**
  - Overfitting: Classifier performance on clean data is much higher than on perturbed data
  - Inconsistent bounds: Empirical probabilistic robust accuracy exceeds the theoretical upper bound
  - Non-monotonic κ behavior: Upper bound does not increase as expected with κ

- **First 3 experiments:**
  1. Synthetic Data (Moons): Train an MLP using DA, compute probabilistic robust accuracy, and apply voting. Compare with the theoretical bound.
  2. Real-world Data (FashionMNIST): Train a CNN-7 using CVaR, evaluate robustness, and test the effect of varying κ on the upper bound.
  3. High-dimensional Data (CIFAR-10): Use CVaR training, analyze the gap between empirical and theoretical bounds, and study the impact of dimensionality on the vicinity size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the voting sample size affect the probabilistic robust accuracy improvement?
- Basis in paper: [explicit] The paper shows that voting improves probabilistic robust accuracy, with experiments using sample sizes up to 10,000, and notes that performance improvement becomes more noticeable when sample size exceeds 100.
- Why unresolved: The paper does not provide a detailed analysis of how different sample sizes impact the magnitude of improvement, or whether there is an optimal sample size beyond which additional samples provide diminishing returns.
- What evidence would resolve it: A systematic study varying sample sizes and measuring the corresponding improvements in probabilistic robust accuracy would clarify the relationship between sample size and performance gains.

### Open Question 2
- Question: What is the relationship between the upper bound of probabilistic robust accuracy and the actual performance of state-of-the-art training methods?
- Basis in paper: [explicit] The paper establishes an upper bound on probabilistic robust accuracy and compares it to the performance of state-of-the-art methods (DA, RS, CVaR), finding that actual performance never exceeds the theoretical bound but may approach it closely.
- Why unresolved: The paper does not investigate why there is a gap between the upper bound and actual performance, or what factors contribute to this gap in different datasets.
- What evidence would resolve it: Further experiments analyzing the factors contributing to the gap, such as dataset characteristics, model architecture, or training methodology, would help understand why the upper bound is not fully achieved.

### Open Question 3
- Question: How does the tolerance level κ affect the upper bound of probabilistic robust accuracy in high-dimensional distributions?
- Basis in paper: [explicit] The paper shows that the upper bound of probabilistic robust accuracy increases monotonically as κ grows, and notes that in high-dimensional distributions, the upper bound is close to that of vanilla accuracy but significantly higher than that of deterministic robust accuracy.
- Why unresolved: The paper does not provide a detailed analysis of how the upper bound scales with κ in high-dimensional spaces, or how the dimensionality of the data affects the rate of increase of the upper bound.
- What evidence would resolve it: A detailed study varying κ and dimensionality, and measuring the corresponding upper bounds, would clarify how these factors interact and affect the achievable probabilistic robust accuracy.

## Limitations

- Theoretical guarantees rely on strong assumptions about quasiconcave vicinity distributions that may not hold in practice
- Empirical validation is limited to specific datasets and training methods, leaving questions about generalizability
- The relationship between κ and vicinity size reduction is theoretically derived but lacks extensive empirical verification across different data distributions

## Confidence

- Mechanism 1 (Vicinity reduction enables higher bounds): Medium - theoretically sound but depends on strong distributional assumptions
- Mechanism 2 (Voting always improves accuracy): Medium - supported by experiments but not proven for all distributions
- Mechanism 3 (Monotonicity with κ): Medium - theoretically proven but empirical validation is limited

## Next Checks

1. Test the monotonicity claim across a wider range of κ values (0.05 to 0.45) on multiple datasets to verify the theoretical bound holds in practice
2. Experiment with non-quasiconcave vicinity distributions to determine when the theoretical guarantees break down
3. Implement the theoretical framework on additional datasets (e.g., SVHN, ImageNet subsets) with different classifier architectures to assess generalizability