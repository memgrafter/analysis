---
ver: rpa2
title: Visual Prompt Tuning in Null Space for Continual Learning
arxiv_id: '2406.05658'
source_url: https://arxiv.org/abs/2406.05658
tags:
- prompt
- learning
- projection
- forgetting
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to eliminate catastrophic forgetting
  in continual learning for vision transformers by tuning prompts in a direction orthogonal
  to previous tasks' features. The key innovation is deriving two consistency conditions
  that guarantee interference-free prompt updates, accounting for the self-attention
  mechanism and LayerNorm effects.
---

# Visual Prompt Tuning in Null Space for Continual Learning

## Quick Facts
- arXiv ID: 2406.05658
- Source URL: https://arxiv.org/abs/2406.05658
- Authors: Yue Lu; Shizhou Zhang; De Cheng; Yinghui Xing; Nannan Wang; Peng Wang; Yanning Zhang
- Reference count: 40
- Primary result: Eliminates catastrophic forgetting in continual learning for vision transformers by tuning prompts in orthogonal directions to previous tasks' features

## Executive Summary
This paper addresses catastrophic forgetting in continual learning for vision transformers by introducing a method that tunes visual prompts in the null space of previous tasks' features. The approach derives two sufficient consistency conditions that guarantee interference-free prompt updates by accounting for self-attention mechanism and LayerNorm effects. Through null-space projection and distribution stability constraints, the method achieves significant improvements over sequential fine-tuning baselines, demonstrating 4-10% accuracy gains and 9-17% forgetting reductions across four class-incremental benchmarks.

## Method Summary
The method projects prompt gradients into the null space of concatenated query-key transformation matrices and attention maps from previous tasks, ensuring that new task learning does not interfere with previously learned representations. By splitting self-attention into Affinity and Aggregation operations, the approach derives conditions that, when jointly satisfied, ensure output features remain unchanged despite prompt updates. An additional LayerNorm drift constraint maintains prompt distribution invariance, enabling simpler consistency conditions. The solution uses SVD-based nullity determination and adaptive null-space projection matrices to balance stability and plasticity.

## Key Results
- Achieves 4-10% accuracy improvements over sequential fine-tuning baselines on CIFAR-100, ImageNet-R, and DomainNet benchmarks
- Reduces forgetting by 9-17% compared to state-of-the-art continual learning methods
- Demonstrates consistent performance across different pre-trained models (ViT-B/16 and CLIP variants)
- Ablation studies confirm the effectiveness of both null-space projection components and distribution consistency loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt gradient updates orthogonal to previous tasks' feature subspace prevent catastrophic forgetting
- Mechanism: The method projects prompt gradients into the null space of concatenated query-key transformation matrices and attention maps from previous tasks, ensuring that new task learning does not interfere with previously learned representations
- Core assumption: The self-attention mechanism's non-linear operations can be decomposed into sufficient linear conditions that, when satisfied, guarantee feature consistency across tasks
- Evidence anchors:
  - [abstract] "tuning the prompts in the direction orthogonal to the subspace spanned by previous tasks' features"
  - [section 4.1] "if the prompt update can be orthogonal to (1) the normalized previous input image tokens projected with the second-order qkv-transformation matrices... and (2) the activated attention map generated by image queries and prompt keys, the interference in visual prompt tuning can be eliminated theoretically"
  - [corpus] "Gradient Null Space Projection for Preserving Cross-Modal Alignment in VLMs Continual Learning" - shows null space projection is effective in VLMs for preserving learned knowledge

### Mechanism 2
- Claim: Two sufficient consistency conditions derived from self-attention decomposition ensure theoretical guarantee of interference elimination
- Mechanism: By splitting self-attention into Affinity (query-key similarity) and Aggregation (value-weighted sum) operations, the method derives conditions that, when jointly satisfied, ensure the output features remain unchanged despite prompt updates
- Core assumption: The softmax operation's non-injectivity can be circumvented by ensuring consistency in both Affinity and Aggregation stages independently
- Evidence anchors:
  - [section 4.1] "we split the process of self-attention into two primary stages... We can achieve Eq. (11) by ensuring the consistency of each stage"
  - [section 4.1] "we are able to derive the following two equations, respectively: (QXtW⊤k LN(Pt)⊤ = QXtW⊤k LN(Pt+1)⊤, SPtLN(Pt)Wv = SPtLN(Pt+1)Wv)"
  - [corpus] "Gradient Projection For Continual Parameter-Efficient Tuning" - demonstrates gradient projection can work for parameter-efficient tuning scenarios

### Mechanism 3
- Claim: LayerNorm drift constraint maintains prompt distribution invariance, enabling simpler consistency conditions
- Mechanism: By penalizing the L1 distance between prompt mean and standard deviation distributions across tasks, the method ensures that LayerNorm operations do not introduce distribution shifts that would violate the derived consistency conditions
- Core assumption: Maintaining similar prompt statistics across tasks is sufficient to linearize the relationship between LN(Pt) and LN(Pt+∆P), simplifying the consistency derivation
- Evidence anchors:
  - [section 4.1] "we introduce an additional constraint on the distribution of prompts... we require that the updated prompts Pt + ∆P retain the same distribution as Pt"
  - [section 4.2] "LLN = ||µPt+1 − µPt ||1 + ||σPt+1 − σPt ||1" - explicitly shows the distribution consistency loss
  - [corpus] "CODE-CL: Conceptor-Based Gradient Projection for Deep Continual Learning" - shows distribution preservation techniques can help in continual learning

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) for null space computation
  - Why needed here: The method relies on computing null spaces of matrices QXtW⊤k and SPt to project prompt gradients orthogonally, which requires SVD decomposition
  - Quick check question: Given a matrix A, how would you use SVD to find a basis for its null space?

- Concept: Layer Normalization and its distributional effects
  - Why needed here: Understanding how LayerNorm affects prompt distributions is crucial for deriving the invariant distribution constraint and its impact on consistency conditions
  - Quick check question: How does LayerNorm transform the mean and standard deviation of its input, and why might this create challenges for maintaining consistency across prompt updates?

- Concept: Self-attention mechanism decomposition
  - Why needed here: The method's theoretical foundation relies on decomposing self-attention into Affinity and Aggregation operations to derive sufficient conditions for feature consistency
  - Quick check question: What are the three main operations in self-attention, and how can they be conceptually separated to analyze their individual effects on feature transformation?

## Architecture Onboarding

- Component map:
  ViT layer with prompts -> LayerNorm -> qkv-transformations -> self-attention (Affinity + Aggregation) -> LayerNorm -> MLP

- Critical path:
  1. Forward pass: Input tokens → LayerNorm → qkv-transformations → self-attention → LayerNorm → MLP
  2. Gradient computation: Compute gradients for prompts
  3. Projection: Project gradients using B1 and B2 to ensure orthogonality to previous tasks' subspaces
  4. Update: Apply projected gradients to update prompts
  5. Distribution constraint: Apply LLN loss to maintain prompt distribution consistency

- Design tradeoffs:
  - Stability vs. plasticity: The weight η in projection matrices controls the balance between maintaining old knowledge and learning new tasks
  - Approximation vs. exactness: Using approximate null spaces rather than exact null spaces trades computational efficiency for potential small approximation errors
  - Distribution constraint strength: The coefficient of LLN loss trades off between strict distribution preservation and flexibility in prompt updates

- Failure signatures:
  - Degrading performance on previous tasks while learning new tasks indicates insufficient orthogonality in gradient projection
  - Inconsistent performance across runs suggests instability in null space computation or approximation
  - Slow learning on new tasks may indicate overly strict projection constraints or distribution preservation

- First 3 experiments:
  1. Baseline comparison: Implement VPT-Seq (sequential fine-tuning without projection) and compare accuracy and forgetting metrics on CIFAR-100 10-split
  2. Component ablation: Test variants with only B1 projection, only B2 projection, and no LLN loss to isolate the contribution of each component
  3. Projection weight sensitivity: Vary η from 0.9 to 1.0 in increments of 0.01 to find the optimal stability-plasticity tradeoff on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mathematical relationship between the prompt update ΔP and the output image tokens when considering the full self-attention mechanism, including softmax and aggregation?
- Basis in paper: [explicit] The paper states that the non-linear softmax operation and the quadratic term from K⊤ZVZ complicate the direct resolution of keeping FZt = FZt+1
- Why unresolved: The paper simplifies this by deriving sufficient conditions for Affinity and Aggregation separately, but does not provide the exact relationship for the complete self-attention
- What evidence would resolve it: A mathematical proof showing the exact relationship between ΔP and the output image tokens for the complete self-attention mechanism, or experimental validation of this relationship

### Open Question 2
- Question: How does the null-space projection method perform when the null space of QXtW⊤k or SPt does not exist in practice?
- Basis in paper: [explicit] The paper mentions that the null spaces of QXtW⊤k and SPt may not always exist in practice and uses an adaptive nullity strategy to determine the nullities R1 and R2
- Why unresolved: The paper does not provide a comprehensive analysis of the performance of the null-space projection method when the null space does not exist, only mentioning the use of an adaptive strategy
- What evidence would resolve it: Experimental results comparing the performance of the null-space projection method with and without the adaptive nullity strategy, or a theoretical analysis of the method's performance in the absence of a null space

### Open Question 3
- Question: What is the impact of the invariant prompt distribution constraint on the final performance, and how does it affect the stability-plasticity trade-off?
- Basis in paper: [explicit] The paper introduces an invariant prompt distribution constraint and shows its importance in the ablation study, but does not provide a detailed analysis of its impact on the stability-plasticity trade-off
- Why unresolved: The paper does not explore the relationship between the invariant prompt distribution constraint and the stability-plasticity trade-off in depth, only mentioning its importance in the ablation study
- What evidence would resolve it: A detailed analysis of the impact of the invariant prompt distribution constraint on the stability-plasticity trade-off, including experimental results showing the performance with and without the constraint under different trade-off settings

## Limitations
- The core assumption that self-attention operations can be linearized through sufficient consistency conditions may not hold for all model architectures or task distributions
- Null-space projection introduces computational overhead and potential numerical instability when feature space dimensionality approaches prompt update dimensionality
- The distribution consistency constraint may overly restrict prompt updates in scenarios requiring significant distributional shifts for optimal performance

## Confidence
- High confidence in the empirical results showing 4-10% accuracy improvements and 9-17% forgetting reductions
- Medium confidence in the theoretical derivation of consistency conditions, as the approximation quality depends on specific architectural details
- Low confidence in the generalization to arbitrary ViT architectures beyond the tested ViT-B/16 and CLIP variants

## Next Checks
1. Test the method on ViT-Small and ViT-Large variants to verify performance scaling and identify if null-space projection becomes unstable with larger models
2. Conduct ablation studies varying the LLN loss weight across several orders of magnitude to quantify the tradeoff between distribution preservation and learning flexibility
3. Track the evolution of attention maps across tasks to empirically verify that derived consistency conditions maintain feature invariance