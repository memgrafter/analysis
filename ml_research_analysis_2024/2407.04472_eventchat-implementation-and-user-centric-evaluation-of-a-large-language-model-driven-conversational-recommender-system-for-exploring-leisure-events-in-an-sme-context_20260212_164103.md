---
ver: rpa2
title: 'EventChat: Implementation and user-centric evaluation of a large language
  model-driven conversational recommender system for exploring leisure events in an
  SME context'
arxiv_id: '2407.04472'
source_url: https://arxiv.org/abs/2407.04472
tags:
- user
- https
- eventchat
- systems
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study details the implementation and evaluation of EventChat,
  an LLM-driven conversational recommender system for event discovery, within an SME
  context. Using a revised ResQue framework, the system was tested through both subjective
  user surveys and objective performance metrics.
---

# EventChat: Implementation and user-centric evaluation of a large language model-driven conversational recommender system for exploring leisure events in an SME context

## Quick Facts
- arXiv ID: 2407.04472
- Source URL: https://arxiv.org/abs/2407.04472
- Authors: Hannes Kunstmann; Joseph Ollier; Joel Persson; Florian von Wangenheim
- Reference count: 0
- Key outcome: 85.5% recommendation accuracy with median 5.7s latency and 3.6 cents per message cost

## Executive Summary
This study presents EventChat, an LLM-driven conversational recommender system for event discovery within an SME context. The system was implemented using a stage-based dialog architecture with ChatGPT as the core LLM, evaluated through a field study with 83 participants. Results demonstrate that LLM-driven CRS are feasible for SMEs, achieving high recommendation accuracy and generally positive user experience. However, significant challenges remain around operational costs and latency, which emerged as critical barriers to long-term viability.

## Method Summary
The study implemented EventChat using a stage-based dialog architecture with five action modules (Chat, Refusal, Search, Recommendation, Targeted Inquiry) integrated with ChatGPT API. The system was evaluated through a field study with 83 participants who used the chat interface to discover events. Evaluation combined subjective user surveys based on a revised ResQue framework with objective performance metrics including latency, token usage, and operational costs. The backend architecture processed user queries through prompt-based learning techniques, while the Flutter-based frontend provided a conversational interface with event cards and time selection capabilities.

## Key Results
- 85.5% recommendation accuracy achieved through RAG with ChatGPT reranking
- Median latency of 5.7 seconds per message and median operational cost of 3.6 cents per message
- Positive user experience scores across most ResQue dimensions, with concerns about latency and cost
- Stage-based architecture successfully balanced cost/latency constraints while maintaining user experience quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-based learning in LLM-driven CRS enables rapid SME deployment without requiring large labeled datasets.
- Mechanism: Uses few-shot CoT and Few-Shot ICL techniques within prompts to guide LLM responses, avoiding the need for extensive training data or fine-tuning.
- Core assumption: The LLM can understand and execute the business logic embedded in prompts without model adaptation.
- Evidence anchors:
  - [abstract]: "Prompt-based learning does not require large amounts of training data from past user interactions or generated synthetic data, both of which are costly for SMEs to obtain in terms of time, finances, or engineering efforts."
  - [section]: "We used techniques like Few-Shot CoT and Few-Shot ICL to help the LLM perform specific business tasks [54]."
  - [corpus]: Weak — no direct citations of ICL effectiveness in production CRS in corpus.
- Break condition: If LLM fails to interpret prompt context correctly, leading to inconsistent or hallucinated outputs.

### Mechanism 2
- Claim: RAG with ChatGPT as a ranker achieves high recommendation accuracy but incurs prohibitive costs and latency.
- Mechanism: Candidate set filtering followed by LLM reranking over up to 10 items per prompt; ranking distinguishes match/no-match, reducing granularity but increasing token consumption.
- Core assumption: Reordering by LLM is necessary for personalized relevance, despite high cost.
- Evidence anchors:
  - [abstract]: "One major driver of these costs is the use of an advanced LLM as a ranker within the retrieval-augmented generation (RAG) technique."
  - [section]: "we used ChatGPT as a ranker to determine whether the shown events matched the user’s query."
  - [corpus]: Weak — no direct cost/latency analysis of LLM rerankers in production CRS.
- Break condition: If operational cost per message exceeds business margins or latency exceeds user patience threshold.

### Mechanism 3
- Claim: Stage-based dialog architecture balances cost/latency constraints while maintaining user experience quality.
- Mechanism: Each conversation turn follows predefined stages (Chat, Search, Recommendation, Targeted Inquiry) instead of agent-based continuous reasoning, reducing API calls.
- Core assumption: Business-critical constraints (cost, latency, stability) outweigh the flexibility benefits of agent-based systems.
- Evidence anchors:
  - [abstract]: "with a median cost of $0.04 per interaction and a latency of 5.7s, cost-effectiveness and response time emerge as crucial areas for achieving a more user-friendly and economically viable LLM-driven CRS for SME settings."
  - [section]: "we chose the low design complexity offered by the stage-based architecture [72]."
  - [corpus]: Moderate — corpus includes agent-based CRS research but no direct comparison with stage-based for SME contexts.
- Break condition: If user queries require complex multi-turn reasoning that cannot be mapped to discrete stages.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Provides mechanism to combine external data retrieval with LLM-based ranking for personalized recommendations.
  - Quick check question: What are the two main phases of RAG in this implementation, and how do they differ from standard recommendation pipelines?
- Concept: Prompt-based learning vs. Fine-tuning
  - Why needed here: Determines feasibility of deployment for SMEs with limited resources.
  - Quick check question: Which trade-off between quality and cost is highlighted when using longer prompts for more context?
- Concept: Stage-based vs. Agent-based dialog architecture
  - Why needed here: Influences system complexity, cost, and latency.
  - Quick check question: Why did the authors reject agent-based architecture despite its potential for flexibility?

## Architecture Onboarding

- Component map:
  - User input -> Action Detection module -> appropriate module (Search/Recommendation/Targeted Inquiry) -> LLM processing -> response generation -> UI update
  - External APIs: startup's relational DB, vector DB, Amazon Personalize, ChatGPT API
  - Frontend: Flutter-based chat UI with event cards, time selector, and mode buttons

- Critical path: User input → Action Detection → appropriate module (Search/Recommendation/Targeted Inquiry) → LLM processing → response generation → UI update

- Design tradeoffs:
  - Stage-based vs. Agent-based: Reduced cost/latency but less flexibility
  - Long prompts vs. multiple short prompts: Better context but higher latency and cost
  - LLM reranking vs. simpler filtering: Higher accuracy but prohibitive cost

- Failure signatures:
  - High latency (>10s) → Check Reduction module token usage
  - Low recommendation accuracy → Verify candidate set quality and LLM reranking logic
  - Inconsistent responses → Examine prompt structure and context window handling

- First 3 experiments:
  1. Measure token usage and latency per module with sample prompts to identify bottlenecks
  2. Test different prompt lengths for Action Detection to find optimal balance of context and cost
  3. Compare recommendation accuracy with and without LLM reranking to quantify value vs. cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the use of fine-tuned LLMs with fewer parameters (as opposed to ChatGPT) affect latency, cost, and recommendation accuracy in LLM-driven CRS for SMEs?
- Basis in paper: [inferred] The paper mentions that fine-tuning adds complexity but is less suitable for SMEs, while using ChatGPT as a ranker is prohibitively expensive and contributes significantly to latency.
- Why unresolved: The paper did not directly compare the performance of fine-tuned LLMs with fewer parameters to ChatGPT in terms of latency, cost, and recommendation accuracy.
- What evidence would resolve it: A comparative study implementing both approaches and measuring latency, cost, and recommendation accuracy for each.

### Open Question 2
- Question: What are the most effective strategies for generating training data and mitigating fine-tuning costs for SMEs implementing LLM-driven CRS?
- Basis in paper: [explicit] The paper suggests that generating training data is often challenging for SMEs and recommends exploring easy and resourceful methods for doing so.
- Why unresolved: The paper does not provide specific strategies or methods for generating training data and mitigating fine-tuning costs.
- What evidence would resolve it: A study exploring and evaluating various strategies for generating training data and mitigating fine-tuning costs, with a focus on their applicability and effectiveness for SMEs.

### Open Question 3
- Question: How can LLM-driven CRS be designed to effectively incorporate fine-grained and often subtle knowledge about the item corpus when formulating responses to users?
- Basis in paper: [explicit] The paper identifies that relying solely on prompt-based learning revealed limitations in capturing detailed information, leading to inconsistent answers and lower user satisfaction.
- Why unresolved: The paper does not propose specific solutions for addressing this limitation and incorporating fine-grained knowledge into LLM-driven CRS.
- What evidence would resolve it: A study exploring and evaluating various approaches for incorporating fine-grained knowledge into LLM-driven CRS, with a focus on their impact on user satisfaction and recommendation accuracy.

## Limitations

- Evaluation relies on self-reported user satisfaction metrics through survey instruments, which may be subject to response bias and social desirability effects
- The operational cost analysis assumes current ChatGPT API pricing, which may change and affect the business case
- The system's performance with more complex or ambiguous user queries was not extensively tested

## Confidence

- **High Confidence**: The technical implementation details of the stage-based dialog architecture and prompt engineering approaches are well-documented and verifiable through code analysis
- **Medium Confidence**: The user experience metrics (ResQue scores) and recommendation accuracy (85.5%) are supported by survey data but may be influenced by selection bias in the participant pool
- **Low Confidence**: Long-term economic viability predictions based on current operational costs may not hold if API pricing changes or user behavior patterns evolve

## Next Checks

1. Conduct A/B testing comparing stage-based architecture against agent-based alternatives with identical user populations to quantify the flexibility-cost tradeoff empirically
2. Implement a comprehensive cost model that accounts for variable token usage patterns across different user query types to identify specific optimization opportunities
3. Perform longitudinal analysis of user satisfaction and recommendation accuracy over extended periods to validate initial positive results against potential novelty effects or usage fatigue