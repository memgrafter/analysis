---
ver: rpa2
title: Cost-Effective Proxy Reward Model Construction with On-Policy and Active Learning
arxiv_id: '2407.02119'
source_url: https://arxiv.org/abs/2407.02119
tags:
- query
- data
- reward
- training
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the high cost of human feedback in RLHF by proposing
  a method to train a proxy reward model with limited expert queries. The approach
  uses on-policy data generation to avoid distribution shift and active learning to
  select informative samples for labeling.
---

# Cost-Effective Proxy Reward Model Construction with On-Policy and Active Learning

## Quick Facts
- arXiv ID: 2407.02119
- Source URL: https://arxiv.org/abs/2407.02119
- Reference count: 18
- Primary result: With 1.7K expert queries, trains reward model that labels 9x more preference pairs, achieving >1% improvement on AlpacaEval2 and MMLU benchmarks

## Executive Summary
This paper addresses the high cost of human feedback in RLHF by proposing a method to train a proxy reward model with minimal expert queries. The approach uses on-policy data generation to avoid distribution shift and active learning to select informative samples for labeling. With just 1.7K expert queries, the method trains a reward model that labels 9x more preference pairs, achieving over 1% improvement on AlpacaEval2 and MMLU benchmarks compared to a baseline. The method is orthogonal to other RLHF techniques and can be combined with them to further reduce query costs.

## Method Summary
The method involves fine-tuning Llama-2 7B on high-reward data to get M1, generating on-policy responses using M1, selecting informative samples via coreset selection, querying experts for evaluations to create EFT data, training a weak evaluator M_eval on EFT data, using M_eval to label remaining responses and form DPO pairs, and finally training DPO model M2. The approach introduces on-policy query to avoid out-of-distribution issues in seed data and active learning to select the most informative data for preference queries.

## Key Results
- Achieved >1% improvement on AlpacaEval2 and MMLU benchmarks
- Trained proxy reward model with just 1.7K expert queries
- Proxy reward model labeled 9x more preference pairs than expert-labeled data
- Method is orthogonal to other RLHF techniques and can be combined with them

## Why This Works (Mechanism)

### Mechanism 1
Training a weak proxy reward model with a small budget of on-policy queries enables labeling 9x more preference pairs than the original budget. On-policy query generation ensures the data distribution matches the policy model being trained, avoiding out-of-distribution issues that arise from using seed data with different characteristics. This allows the proxy reward model to generalize better to the responses it needs to evaluate.

### Mechanism 2
Active learning (coreset selection) improves proxy reward model quality by selecting the most informative samples for labeling. Coreset selection chooses prompts and responses that are diverse in the representation space, ensuring the proxy reward model sees a broad range of behaviors and avoids redundancy. This is particularly effective because reward feedback is a discriminative task, which active learning strategies are well-suited for.

### Mechanism 3
Training the proxy reward model on on-policy data with a naturally balanced reward distribution mitigates bias issues present in seed data. Seed data is often biased toward high rewards because it comes from human-annotated, high-quality entries. On-policy generation naturally produces a more diverse reward distribution, and balanced training further enforces this by ensuring equal representation of each reward class.

## Foundational Learning

- **Reinforcement Learning from Human Feedback (RLHF) pipeline**: The paper builds on RLHF but focuses on the reward model training stage, so understanding the full pipeline (SFT → Reward Model → DPO) is essential. Quick check: What are the three main stages of a typical RLHF pipeline, and where does this paper intervene?

- **Distribution shift and its impact on model generalization**: The paper emphasizes avoiding OOD issues by using on-policy data, so understanding how distribution shift affects model performance is critical. Quick check: Why might a reward model trained on seed data fail when applied to on-policy generated responses?

- **Active learning and coreset selection**: The paper uses coreset-based active learning to select informative samples for labeling, so familiarity with these concepts is necessary. Quick check: What is the objective of coreset selection in active learning, and how does it differ from random sampling?

## Architecture Onboarding

- **Component map**: Pretrained model (M0) → Supervised Fine-Tuning (M1) → On-policy response generation → Active learning (coreset selection) → Expert query → Evaluation Fine-Tuning (EFT) data → Weak evaluation model (M_eval) → Reward labeling of remaining responses → DPO preference pairs → Policy model (M2)

- **Critical path**: 
  1. Generate on-policy responses using M1
  2. Select informative samples via coreset selection
  3. Query expert for evaluations to create EFT data
  4. Train M_eval on EFT data
  5. Use M_eval to label remaining IFT data
  6. Train M2 using DPO

- **Design tradeoffs**:
  - Query budget vs. proxy reward model quality: Higher budget improves M_eval but increases cost
  - CoresetEFT vs. coresetIFT: CoresetEFT considers response diversity but may be noisier; coresetIFT is more stable but may miss important response variations
  - Balanced vs. unbalanced training: Balanced avoids bias but reduces effective training data; unbalanced uses more data but risks overfitting

- **Failure signatures**:
  - M2 performance degrades as query budget increases (overfitting to noisy proxy rewards)
  - AL strategies increase validation loss of M_eval (poor embedding representation)
  - Performance gains only on certain metrics (MMLU-5shot) but not others (AlpacaEval2, MMLU-0shot) (metric-specific biases)

- **First 3 experiments**:
  1. Compare random on-policy vs. coresetEFT vs. coresetIFT at low query budget (n=400) to see initial performance differences
  2. Test balanced vs. unbalanced training for M_eval to observe impact on proxy reward quality and downstream performance
  3. Evaluate correlation between M_eval validation loss and downstream metrics to understand when active learning helps

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal trade-off between the size of the initial labeled seed data and the query budget for training an effective proxy reward model? The paper explores cost-effective proxy reward oracle construction with extremely limited labeled data and expert query budgets, but does not systematically explore the full trade-off space to identify the optimal balance.

### Open Question 2
How do the proposed on-policy and active learning strategies generalize to different model architectures beyond Llama-2 7B, such as larger models or different model families? The paper focuses on Llama-2 7B and mentions that the methodology should be applicable to any model, but does not extensively test on different architectures.

### Open Question 3
What is the impact of using the proxy reward model on the diversity and quality of the generated responses compared to direct expert labeling? The paper discusses using the proxy reward model to label nine times more preference pairs than the expert-labeled data, but does not directly analyze the impact on response diversity and quality.

## Limitations

- Reliance on GPT-4 as an expert feedback oracle may not generalize to human expert preferences and could introduce systematic biases
- Performance gains are measured against a relatively weak baseline (1.7K queries), and it's unclear how the method performs when compared to more sophisticated active learning or proxy reward model approaches
- The method shows mixed results across different evaluation metrics, with improvements on AlpacaEval2 and MMLU-5shot but no gains on MMLU-0shot and BBH

## Confidence

- **High confidence**: The core mechanism of using on-policy data to avoid distribution shift is well-established and logically sound. The claim that active learning can improve sample efficiency for reward model training has strong theoretical support.
- **Medium confidence**: The claim of achieving 1% improvement on benchmarks with 1.7K queries is supported by the experimental results, but the comparison baseline and the potential for metric-specific biases reduce confidence in the generalizability of these gains.
- **Low confidence**: The claim that the method is "orthogonal" to other RLHF techniques and can be combined with them is stated but not empirically validated in the paper.

## Next Checks

1. Conduct ablation studies comparing random on-policy selection vs. coreset selection at various budget levels to isolate the contribution of active learning to performance gains.

2. Test the method with human expert feedback instead of GPT-4 to validate the generalizability of the approach and identify potential biases in the current evaluation setup.

3. Evaluate the correlation between proxy reward model validation loss and downstream policy performance across different query budgets to identify the optimal trade-off between query cost and model quality.