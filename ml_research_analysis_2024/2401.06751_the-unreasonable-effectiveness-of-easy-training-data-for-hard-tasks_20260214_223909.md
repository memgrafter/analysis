---
ver: rpa2
title: The Unreasonable Effectiveness of Easy Training Data for Hard Tasks
arxiv_id: '2401.06751'
source_url: https://arxiv.org/abs/2401.06751
tags:
- data
- hard
- easy
- test
- hardness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that pretrained language models can surprisingly
  generalize well from easy to hard data, often matching the performance of models
  trained directly on hard data. Across multiple datasets, training methods, and hardness
  measures, easy-to-hard generalization closes 70-100% of the supervision gap between
  unsupervised and hard-supervised models.
---

# The Unreasonable Effectiveness of Easy Training Data for Hard Tasks

## Quick Facts
- arXiv ID: 2401.06751
- Source URL: https://arxiv.org/abs/2401.06751
- Reference count: 40
- Key outcome: Easy-to-hard generalization closes 70-100% of supervision gaps across multiple datasets and hardness measures

## Executive Summary
This paper reveals a surprising phenomenon in language model training: models can generalize effectively from easy-to-annotate data to perform hard tasks, often matching models trained directly on hard data. Across multiple datasets and tasks, easy-to-hard generalization consistently closes 70-100% of the performance gap between unsupervised and fully supervised models. The findings suggest that collecting easy data may be more effective than hard data when easy data is cheaper or less noisy to obtain, potentially simplifying approaches to scalable oversight in AI systems.

## Method Summary
The paper conducts systematic experiments across multiple NLP tasks including natural language inference, fact checking, and reasoning. For each task, the authors define "hardness" either through human annotation difficulty or by measuring model performance gaps. They then train models on easy subsets of data and evaluate performance on hard test sets, comparing against models trained directly on hard data. The experiments span different model architectures, training methods, and definitions of hardness to test the robustness of easy-to-hard generalization.

## Key Results
- Easy-to-hard generalization closes 70-100% of supervision gaps across multiple datasets
- Models trained on easy data often match performance of models trained on hard data
- The effect holds across different model architectures, training methods, and definitions of hardness
- Easy data collection may be more cost-effective than hard data collection for certain tasks

## Why This Works (Mechanism)
The paper doesn't provide a definitive mechanistic explanation for why easy-to-hard generalization works so effectively. However, the authors suggest several potential factors including the implicit structure learned during pretraining, the distribution of task difficulties in natural data, and the ability of models to leverage patterns from easier examples to solve harder instances. The phenomenon appears to stem from the combination of pretrained representations and the particular structure of NLP tasks rather than from any specific architectural feature.

## Foundational Learning
- Pretrained language models: Why needed - provide general linguistic knowledge and reasoning capabilities; Quick check - verify pretraining objectives and dataset sizes
- Hardness measures: Why needed - operationalize what makes tasks difficult; Quick check - validate hardness definitions through human studies
- Generalization bounds: Why needed - understand theoretical limits of easy-to-hard transfer; Quick check - compare empirical results to theoretical predictions

## Architecture Onboarding

### Component Map
Pretrained LM -> Easy Data Training -> Hard Task Performance

### Critical Path
The critical path involves pretraining, followed by fine-tuning on easy data, then evaluation on hard tasks. Success depends on the quality of pretraining and the relevance of easy examples to hard instances.

### Design Tradeoffs
The main tradeoff is between annotation ease and data quality. Easy data may be noisier or less comprehensive, while hard data is more expensive but potentially more targeted. The paper suggests that for many tasks, the ease advantage outweighs quality concerns.

### Failure Signatures
Easy-to-hard generalization may fail when easy data is too dissimilar from hard instances, when tasks require specialized knowledge not captured in easy examples, or when hard tasks involve reasoning patterns absent from easy data.

### First 3 Experiments
1. Train on easy NLI examples, test on hard NLI examples
2. Train on easy fact-checking examples, test on hard fact-checking examples  
3. Vary the degree of hardness difference between training and test sets

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Results may be specific to GPT-style models and NLP tasks, limiting generalizability
- Relationship between annotation ease and annotation noise wasn't systematically studied
- Scalability to real-world scenarios with vastly different data collection costs remains uncertain

## Confidence
- **High confidence**: Core empirical finding of easy-to-hard generalization
- **Medium confidence**: Implications for data collection strategies
- **Medium confidence**: Robustness across different hardness measures

## Next Checks
1. Test easy-to-hard generalization on model architectures beyond GPT-style transformers
2. Systematically vary annotation noise levels while holding difficulty constant
3. Evaluate cost-effectiveness in real-world data collection scenarios with large cost differentials