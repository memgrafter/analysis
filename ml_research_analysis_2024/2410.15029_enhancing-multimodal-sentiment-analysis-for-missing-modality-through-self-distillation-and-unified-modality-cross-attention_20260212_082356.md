---
ver: rpa2
title: Enhancing Multimodal Sentiment Analysis for Missing Modality through Self-Distillation
  and Unified Modality Cross-Attention
arxiv_id: '2410.15029'
source_url: https://arxiv.org/abs/2410.15029
tags:
- modality
- text
- missing
- multimodal
- sentiment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multimodal sentiment analysis with missing
  text modality by proposing a Double-Flow Self-Distillation Framework that combines
  Unified Modality Cross-Attention (UMCA) and Modality Imagination Autoencoder (MIA).
  The method uses LLM-based models to simulate missing text representations from audio
  and visual modalities while MIA supplements information to align these simulated
  representations with real text.
---

# Enhancing Multimodal Sentiment Analysis for Missing Modality through Self-Distillation and Unified Modality Cross-Attention

## Quick Facts
- arXiv ID: 2410.15029
- Source URL: https://arxiv.org/abs/2410.15029
- Authors: Yuzhe Weng; Haotian Wang; Tian Gao; Kewei Li; Shutong Niu; Jun Du
- Reference count: 39
- Key outcome: Proposed Double-Flow Self-Distillation Framework achieves MAE of 0.506 and accuracy of 87.6% with complete modalities, and significantly outperforms other approaches when text is missing (MAE of 0.550, accuracy of 84.2%), with only 0.044 MAE increase and 3.4% accuracy decrease compared to complete modality scenarios.

## Executive Summary
This paper addresses multimodal sentiment analysis with missing text modality by proposing a Double-Flow Self-Distillation Framework that combines Unified Modality Cross-Attention (UMCA) and Modality Imagination Autoencoder (MIA). The method uses LLM-based models to simulate missing text representations from audio and visual modalities while MIA supplements information to align these simulated representations with real text. The framework employs Rank-N Contrast loss to capture the continuous nature of sentiment valence regression tasks. Experiments on CMU-MOSEI dataset show that the model achieves MAE of 0.506 and accuracy of 87.6% with all modalities, and significantly outperforms other approaches when text is missing (MAE of 0.550, accuracy of 84.2%), with only 0.044 MAE increase and 3.4% accuracy decrease compared to complete modality scenarios.

## Method Summary
The framework uses a double-flow architecture where one path processes complete multimodal data and another handles missing text scenarios. For missing text, the Vicuna LLM simulates text representations from audio modality, while MIA refines these using residual information from audio and visual modalities. UMCA employs modality-specific cross-attention with shared query projections to fuse information effectively. The model is trained with multiple loss functions including task loss, modality knowledge distillation loss, representation similarity loss, and rank-N contrast loss to align complete and missing-text data flows.

## Key Results
- Achieves MAE of 0.506 and accuracy of 87.6% with complete modalities on CMU-MOSEI
- Outperforms other approaches when text is missing with MAE of 0.550 and accuracy of 84.2%
- Only 0.044 MAE increase and 3.4% accuracy decrease compared to complete modality scenarios
- Significantly improves upon baseline methods in text-missing scenarios

## Why This Works (Mechanism)

### Mechanism 1
The LLM-based text simulation (Vicuna) combined with MIA allows robust sentiment prediction when text modality is missing by approximating the semantic space of missing text from audio-visual modalities. When text is missing, audio representation is projected through WavLM and Vicuna to simulate text embeddings. MIA then refines these by incorporating residual information from audio and visual modalities to align with real text representation distribution. This works because simulated text embeddings preserve sufficient semantic structure to be compatible with downstream cross-attention fusion when text is missing.

### Mechanism 2
Unified Modality Cross-Attention (UMCA) enables effective multimodal fusion by learning modality-specific keys/values while using shared query projections for sentiment valence regression. UMCA first maps each modality to a shared embedding space, computes cross-attention with modality-specific keys/values, then uses an attention-guided feature gathering module to produce a multi-view query representing all modality combinations for final regression. This captures complementary sentiment cues better than simple concatenation or single-stream attention.

### Mechanism 3
The combination of Modality Knowledge Distillation Loss (LMKD), Representation Similarity Loss (LRS), and Rank-N Contrast Loss (LRNC) aligns representations across complete and missing-text data flows while preserving sentiment ordering. LMKD aligns simulated text representations with real ones during missing-text training, LRS minimizes the difference between final regression inputs across data flows, and LRNC enforces continuous label-distance relationships via contrastive ranking. These three losses jointly ensure that the model learns a unified representation space where complete and missing-text flows produce similar outputs for the same sentiment.

## Foundational Learning

- Concept: Multimodal sentiment analysis with complete and missing modalities
  - Why needed here: The method must handle two distinct inference scenarios—full modality input and text-absent input—without retraining separate models.
  - Quick check question: What is the difference between a missing modality scenario and a low-quality ASR scenario in multimodal sentiment analysis?

- Concept: Cross-attention mechanism in multimodal fusion
  - Why needed here: UMCA uses modality-specific cross-attention to extract modality-aligned features before weighted fusion, which is central to the model's ability to integrate audio, visual, and text cues.
  - Quick check question: How does cross-attention differ from self-attention in terms of query-key-value relationships?

- Concept: Knowledge distillation in multimodal settings
  - Why needed here: The self-distillation framework transfers knowledge from the complete-modality teacher to the missing-text student by aligning intermediate representations and final predictions.
  - Quick check question: In what way does modality knowledge distillation differ from traditional model distillation?

## Architecture Onboarding

- Component map: Encoders (MANet visual, WavLM audio, Vicuna text simulation) -> UMCA (cross-attention + AFG) -> MIA (residual autoencoder) -> Regression head -> Losses (LTask + LMKD + LRS + LRNC)

- Critical path:
  1. Input → Encoder → Modality-specific embeddings
  2. UMCA: shared query → modality cross-attention → multi-view query → final representation
  3. MIA: residual correction on simulated text only during missing-text training
  4. Regression head: valence prediction
  5. Losses: task + distillation + similarity + contrastive

- Design tradeoffs:
  - Using LLM-based text simulation trades inference latency for modality robustness; Vicuna 7B is large and slow.
  - MIA adds parameter overhead but only activates during missing-text training, minimizing runtime cost.
  - Self-distillation requires twice the forward passes per batch, increasing memory and compute.

- Failure signatures:
  - High MAE on complete modality but low on missing-text → UMCA cross-attention is misaligned for complete modality.
  - Low MAE on complete modality but high on missing-text → MIA or LMKD insufficiently aligned simulated text.
  - Degraded performance when switching MIA on/off → distillation loss weights need tuning.

- First 3 experiments:
  1. Baseline ablation: remove MIA, LMKD, LRS, LRNC one at a time; measure MAE/ACC on both data flows.
  2. MIA ablation: compare simulated text vs. original text representation alignment using L2 distance heatmaps.
  3. Distillation sensitivity: sweep α, β, γ, δ loss weights; plot MAE vs. weight ratios to find sweet spot.

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed framework perform when different modalities are missing, such as audio or visual, instead of just text? The framework's robustness and performance with other missing modalities remain untested. Testing the framework with missing audio or visual modalities and comparing performance metrics to the text-missing scenario would resolve this.

### Open Question 2
What is the impact of using different LLM-based models for text representation simulation on the framework's performance? The choice of LLM model could significantly affect the quality of simulated text representations and overall performance. Conducting experiments with various LLM-based models and analyzing their impact on the framework's performance metrics would resolve this.

### Open Question 3
How does the framework's performance scale with larger datasets or more diverse multimodal data? The framework's generalization and scalability to different datasets and modalities are not assessed. Evaluating the framework on larger datasets or datasets with more diverse modalities and comparing performance metrics would resolve this.

## Limitations
- Relies heavily on Vicuna 7B LLM which is computationally expensive and may not generalize well to domains outside CMU-MOSEI
- MIA's effectiveness depends on the assumption that audio-visual modalities contain sufficient information to reconstruct meaningful text representations
- Self-distillation framework requires twice the computational resources per training batch, potentially limiting scalability

## Confidence
**High Confidence**: Experimental results showing MAE of 0.506 and accuracy of 87.6% with complete modalities, and comparative performance improvement over existing methods when text is missing.

**Medium Confidence**: Effectiveness of Unified Modality Cross-Attention mechanism, though more ablation studies could strengthen this.

**Low Confidence**: Generalizability of the LLM-based text simulation approach to other multimodal sentiment analysis datasets or domains remains uncertain.

## Next Checks
1. Conduct additional ablation experiments removing individual components (MIA, LMKD, LRS, LRNC) to quantify their specific contributions to performance improvements, particularly focusing on the trade-off between complexity and accuracy gains.

2. Test the framework on at least one additional multimodal sentiment analysis dataset (e.g., MELD or IEMOCAP) to assess generalizability and identify potential domain-specific limitations of the LLM-based text simulation approach.

3. Measure and report the inference latency and memory usage of the complete framework versus simpler baselines, particularly highlighting the overhead introduced by Vicuna-based text simulation and the double-flow training approach.