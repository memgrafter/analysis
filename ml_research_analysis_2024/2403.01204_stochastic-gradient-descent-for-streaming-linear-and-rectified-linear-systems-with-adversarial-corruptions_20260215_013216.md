---
ver: rpa2
title: Stochastic gradient descent for streaming linear and rectified linear systems
  with adversarial corruptions
arxiv_id: '2403.01204'
source_url: https://arxiv.org/abs/2403.01204
tags:
- corruption
- sgd-exp
- regression
- linear
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes SGD-exp, a stochastic gradient descent method\
  \ with exponential decay step size, for linear and ReLU regression under Massart\
  \ noise (adversarial semi-random corruption) in the fully streaming setting. The\
  \ method aims to solve \u21131 minimization problems efficiently with corrupted\
  \ data."
---

# Stochastic gradient descent for streaming linear and rectified linear systems with adversarial corruptions

## Quick Facts
- arXiv ID: 2403.01204
- Source URL: https://arxiv.org/abs/2403.01204
- Reference count: 40
- Primary result: SGD-exp achieves nearly linear convergence for linear and ReLU regression under Massart noise in fully streaming settings

## Executive Summary
This paper introduces SGD-exp, a stochastic gradient descent method with exponentially decaying step size for robust regression in streaming settings under Massart noise (adversarial semi-random corruption). The method addresses ℓ1 minimization problems where up to 50% of responses can be adversarially corrupted. The key innovation is using exponential decay λ⁻ᵏ instead of constant or root-decay step sizes, which provides improved convergence rates and the first convergence guarantees for robust ReLU regression in the streaming setting.

## Method Summary
SGD-exp processes streaming measurements (aⱼ, yⱼ) pairs where yⱼ may be corrupted with probability p < 0.5 under the Massart noise model. For each iteration, it computes the residual yⱼ - f(⟨x, aⱼ⟩) where f is either the identity (linear regression) or ReLU activation, then updates the parameter estimate xₖ₊₁ = xₖ + λ⁻ᵏ·sign(yⱼ - f(⟨x, aⱼ⟩))·∇f(⟨x, aⱼ⟩)·aⱼ. The exponentially decaying step size λ⁻ᵏ provides variance reduction while maintaining sufficient progress toward the true parameter. The method tolerates any corruption probability under symmetric oblivious noise due to sign cancellation effects.

## Key Results
- SGD-exp achieves nearly linear convergence rates for any corruption probability less than 0.5 under Massart noise
- The method demonstrates improved convergence rates over previous robust methods for L1 linear regression due to the exponentially decaying step size
- SGD-exp outperforms GLM-Tron in convergence speed and robustness to corruption on synthetic and real datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SGD-exp achieves near-linear convergence for any corruption probability below 0.5 under Massart noise.
- Mechanism: Exponentially decaying step size λ⁻ᵏ reduces variance in noisy gradient estimates while maintaining sufficient progress toward the true parameter.
- Core assumption: The step size parameter λ must satisfy λ² ≤ 1 + ˜C²(1-2p)²/(9d) to balance noise suppression and convergence speed.
- Evidence anchors:
  - [abstract]: "SGD-exp... provides the first convergence guarantee result for robust ReLU regression in the streaming setting... shows the improved convergence rate over previous robust methods for L1 linear regression due to a choice of an exponentially decaying step size"
  - [section]: "Lemma 7... for any step size decay parameter λ > 0 that satisfies 1 < λ² ≤ 1 + ˜C²(1-2p)²/(9d) < 50/49... we have E[eη(Yk+1-Yk)/{a≤Yk<b} |Fk] ≤ 1 - ˜C²(1-2p)²/(60d)"
  - [corpus]: Weak - related works focus on mirror descent or Krylov methods, not exponential decay SGD under Massart noise
- Break condition: If corruption probability exceeds 0.5, the adversary can flip signs consistently to make x and -x indistinguishable.

### Mechanism 2
- Claim: The drift analysis technique proves bounded growth of ∥uₖ∥² with high probability.
- Mechanism: Transform residual equation into stochastic process Yₖ = ∥uₖ∥² and show expected exponential decay when process stays in safe region [a,b).
- Core assumption: The measurement vectors satisfy a lower bound on E[|⟨u, a⟩| | u] via Assumption 2.
- Evidence anchors:
  - [section]: "Theorem 5... Suppose that for some a, b, η, ρ, D ∈ R such that a < b, η > 0, 0 < ρ < 1 and D ≥ 1, the process satisfies (C0) Y₀ ∈ [0, a) a.s., (C1) E[eη(Yk+1-Yk)/{a≤Yk<b} |Fk] ≤ ρ a.s. for any k ≥ 1"
  - [section]: "Lemma 7... E[eη(Yk+1-Yk)/{a≤Yk<b} |Fk] ≤ 1 - ˜C²(1-2p)²/(60d)" where ρ = 1 - ˜C²(1-2p)²/(60d)
  - [corpus]: Weak - most corpus works don't use drift analysis for robust streaming regression
- Break condition: If the lower bound in Assumption 2 fails, the process can grow unbounded.

### Mechanism 3
- Claim: The method tolerates any corruption probability under symmetric oblivious noise.
- Mechanism: For symmetric noise, sign flips are equally likely in either direction, so their expected contribution to gradient cancels out.
- Core assumption: The corruption noise is symmetric around zero (P(noise positive) = P(noise negative)).
- Evidence anchors:
  - [section]: "Remark 7... For the random symmetric oblivious noise... the inequality (ii) in the proof of Lemma 7 is refined to... we have the factor (1-p) instead of (1-2p) in Theorem 3"
  - [section]: "This allows the recovery of the signal for SGD-exp for any corruption probability p < 1 under the random symmetric oblivious corruption model"
  - [corpus]: Weak - corpus neighbors don't discuss symmetric oblivious corruption tolerance
- Break condition: If noise is asymmetric, the expected gradient bias prevents convergence.

## Foundational Learning

- Concept: Massart noise model
  - Why needed here: The corruption model where each response has probability p of being adversarially corrupted, but the adversary doesn't choose which samples
  - Quick check question: In Massart noise, can the adversary choose which measurements to corrupt before seeing them?

- Concept: Drift analysis of discrete stochastic processes
  - Why needed here: The convergence proof relies on showing that the process Yₖ = ∥uₖ∥² has bounded expected exponential growth
  - Quick check question: What two conditions (C1) and (C2) must hold for the drift analysis to apply?

- Concept: Subgradient methods for non-smooth optimization
  - Why needed here: The ReLU activation function is non-smooth, requiring subgradient computation in SGD-exp iteration
  - Quick check question: What is the subgradient of |x| at x=0?

## Architecture Onboarding

- Component map: Data stream processor -> Corruption detector -> Gradient estimator -> Parameter updater -> Convergence monitor
- Critical path: For each iteration: receive (aⱼ, yⱼ) → compute residual yⱼ - f(⟨xₖ, aⱼ⟩) → compute sign → update xₖ₊₁ → check convergence → repeat
- Design tradeoffs: Exponential decay step size vs. constant/square-root decay - exponential provides better noise robustness but requires tuning λ; memory vs. accuracy - streaming setting forbids storing past data; computational cost of subgradient vs. smoothness - ReLU requires subgradient computation
- Failure signatures: If λ is too large, method diverges; if λ is too small, convergence is extremely slow; if corruption exceeds theoretical bounds, method fails to converge; if Assumption 2 fails, convergence guarantees break
- First 3 experiments:
  1. Test SGD-exp on synthetic linear system with p=0.4 sign-flip corruption, verify near-linear convergence in log-error vs. iteration plot
  2. Compare SGD-exp with SGD-root on same data, confirm SGD-exp achieves lower final error
  3. Test SGD-exp on ReLU regression with p=0.4 corruption, verify it outperforms GLM-Tron

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SGD-exp converge linearly for Massart noise corruption probabilities up to 0.5 in high-dimensional settings where the condition $\tilde{C}(1-2p)\sqrt{d} < 3/7$ is violated?
- Basis in paper: [explicit] The paper states that convergence is guaranteed when $\tilde{C}(1-2p)\sqrt{d} < 3/7$ for linear regression and $\tilde{C}(1-2p)\sqrt{d} < 1$ for ReLU regression, but these conditions may be violated in high-dimensional settings.
- Why unresolved: The paper's analysis relies on these dimensional conditions to bound the error terms, and it is unclear if alternative analysis techniques could relax these requirements.
- What evidence would resolve it: Empirical or theoretical results demonstrating linear convergence of SGD-exp for Massart corruption probabilities up to 0.5 in high-dimensional settings where the stated dimensional conditions are violated.

### Open Question 2
- Question: How does the convergence rate of SGD-exp compare to other robust streaming methods for linear and ReLU regression under the Massart noise model in terms of both sample complexity and computational efficiency?
- Basis in paper: [explicit] The paper shows that SGD-exp achieves nearly linear convergence rates for any corruption probability less than 0.5 under the Massart noise model, but does not provide a direct comparison with other methods.
- Why unresolved: While the paper demonstrates the convergence properties of SGD-exp, a comprehensive comparison with other robust streaming methods is needed to fully understand its advantages and limitations.
- What evidence would resolve it: Empirical studies comparing the convergence rates, sample complexity, and computational efficiency of SGD-exp with other state-of-the-art robust streaming methods for linear and ReLU regression under the Massart noise model.

### Open Question 3
- Question: Can the convergence analysis of SGD-exp be extended to other noise models beyond Massart noise, such as Huber noise or Cauchy noise, and what are the resulting convergence rates and conditions?
- Basis in paper: [explicit] The paper focuses on the Massart noise model and does not explore the performance of SGD-exp under other noise models.
- Why unresolved: The convergence analysis of SGD-exp relies on specific properties of the Massart noise model, and it is unclear if the same techniques can be applied to other noise models.
- What evidence would resolve it: Theoretical analysis and empirical results demonstrating the convergence properties of SGD-exp under various noise models, including Huber noise and Cauchy noise, and the resulting convergence rates and conditions.

## Limitations
- The method requires careful tuning of the exponential decay parameter λ, which may be sensitive to problem-specific constants
- Theoretical convergence guarantees depend on dimensional conditions that may be violated in high-dimensional settings
- The method's performance beyond the two tested real datasets (Red Wine Quality and Lending Club Loan Data) remains unverified

## Confidence
- **High Confidence**: The core convergence guarantee for SGD-exp under Massart noise with p < 0.5 is well-supported by the drift analysis framework
- **Medium Confidence**: The claim of "nearly linear convergence" depends on problem-specific constants that aren't fully characterized
- **Low Confidence**: The tolerance to arbitrary corruption probability under symmetric oblivious noise relies on strong symmetry assumptions

## Next Checks
1. **Convergence Sensitivity Analysis**: Systematically vary the corruption probability p from 0.1 to 0.49 and measure how the convergence rate degrades to reveal practical limits of the theoretical p < 0.5 bound.

2. **Parameter Tuning Study**: Implement an automated λ tuning procedure that adjusts the decay parameter based on observed gradient variance, comparing this adaptive approach against the theoretical fixed-λ approach.

3. **Robustness to Assumption Violations**: Test SGD-exp on data where Assumption 2 (lower bound on E[|⟨u, a⟩| | u]) is intentionally violated by using correlated or non-Gaussian measurement vectors, measuring how quickly convergence degrades as the assumption becomes less valid.