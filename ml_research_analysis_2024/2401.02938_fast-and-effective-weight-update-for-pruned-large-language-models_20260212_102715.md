---
ver: rpa2
title: Fast and Effective Weight Update for Pruned Large Language Models
arxiv_id: '2401.02938'
source_url: https://arxiv.org/abs/2401.02938
tags:
- pruning
- update
- weight
- sparsegpt
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of pruning large language models
  (LLMs) by introducing a fast and effective weight update algorithm for pruned layers
  based on the Alternating Direction Method of Multipliers (ADMM). The key innovation
  is an iterative method that computes accurate weight updates for a given pruning
  mask without requiring many gradient descent iterations or heuristic approximations.
---

# Fast and Effective Weight Update for Pruned Large Language Models

## Quick Facts
- arXiv ID: 2401.02938
- Source URL: https://arxiv.org/abs/2401.02938
- Reference count: 7
- Primary result: ADMM-based iterative weight update achieves state-of-the-art pruning performance on LLaMA and LLaMA2 models

## Executive Summary
This paper addresses the challenge of pruning large language models by introducing a fast and effective weight update algorithm based on Alternating Direction Method of Multipliers (ADMM). The key innovation is an iterative method that computes accurate weight updates for a given pruning mask without requiring many gradient descent iterations or heuristic approximations. The algorithm achieves state-of-the-art pruning performance across a wide range of LLMs, including LLaMA and LLaMA2 variants, while imposing minimal computational overhead.

## Method Summary
The method frames layer-wise pruning as a constrained optimization problem where fixed pruned weights form a convex set. ADMM splits this into alternating updates of the weight matrix and a projection onto the valid set, converging quickly because each step solves a well-conditioned ridge regression subproblem. The approach includes gradual pruning with iterative mask selection and weight updates, improving final performance compared to one-shot pruning. The weight update step acts like a pull towards zero for pruned weights while limiting changes to unpruned weights through an augmented Lagrangian penalty term.

## Key Results
- ADMM method achieves superior perplexity results compared to previous approaches like Wanda and SparseGPT on WikiText benchmark
- At 50% sparsity, ADMM achieves perplexity of 7.06 on LLaMA-7B, outperforming Wanda (7.26) and SparseGPT (7.22)
- Method generalizes well to larger models like LLaMA2-70B, maintaining competitive performance at various sparsity levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ADMM solves the layer-wise pruning weight update problem more efficiently than gradient descent by converging in far fewer iterations.
- Mechanism: The problem is formulated as a constrained optimization where fixed pruned weights form a convex set. ADMM splits this into alternating updates of the weight matrix and a projection onto the valid set, converging quickly because each step solves a well-conditioned ridge regression subproblem.
- Core assumption: The reconstruction error ||XW - XW||² is convex in W, and the set of valid weight matrices under a fixed mask is convex.
- Evidence anchors: Abstract states algorithm produces state-of-the-art results with minimal computational overhead; section claims ADMM sidesteps problems of previous solutions without needing many gradient descent iterations.

### Mechanism 2
- Claim: Gradual pruning with ADMM achieves better final performance than one-shot pruning by allowing more informed weight updates as sparsity increases.
- Mechanism: In each sparsification step, the algorithm prunes additional weights based on current weight estimates and then updates remaining weights with ADMM. This incremental approach reduces reconstruction error more effectively than pruning all weights at once.
- Core assumption: The optimal pruning mask for a given sparsity level depends on current weight values, so updating weights before pruning further leads to better masks.
- Evidence anchors: Section states gradual mask selection and weight updates improve pruning performance and allow state-of-the-art results.

### Mechanism 3
- Claim: The ADMM weight update step acts like a pull towards zero for pruned weights while limiting changes to unpruned weights.
- Mechanism: The augmented Lagrangian term ρ||W - (Z - U)||² acts as a quadratic penalty. For unpruned weights, this only limits step size; for pruned weights, the sign difference between W and (Z - U) causes strong attraction towards zero.
- Core assumption: The penalty term effectively regularizes the solution, and the sign difference for pruned weights is maintained across iterations.
- Evidence anchors: Section explains updating W is similar to ridge regression, with pruned weights pulled towards zero due to sign difference.

## Foundational Learning

- Concept: Convex optimization and ADMM
  - Why needed here: The paper frames the weight update as a convex optimization problem solvable by ADMM, so understanding convexity and ADMM mechanics is essential.
  - Quick check question: Why does convexity of the reconstruction error and the mask constraint set guarantee ADMM convergence?

- Concept: Layer-wise structured pruning
  - Why needed here: The approach processes each layer independently, so understanding how layer-wise pruning works and its limitations is crucial.
  - Quick check question: How does layer-wise pruning differ from unstructured pruning in terms of computational complexity and performance?

- Concept: Ridge regression and matrix inversion
  - Why needed here: The weight update step involves solving a ridge regression problem, which requires matrix inversion; understanding this is key to grasping the computational efficiency.
  - Quick check question: Why is precomputing (X^T X + ρI)^{-1} beneficial for the ADMM update step?

## Architecture Onboarding

- Component map: Calibration input capture -> Mask selection -> ADMM weight update -> Forward pass verification
- Critical path: Calibration input capture → Mask selection → ADMM weight update → Forward pass verification
- Design tradeoffs:
  - ADMM vs. gradient descent: ADMM converges faster but requires matrix inversion; gradient descent is more memory-friendly but slower.
  - Gradual vs. one-shot: Gradual pruning yields better results but increases computational cost.
  - Mask selection criteria: Wanda rule is simple but may be suboptimal; iterative methods like SparseGPT can be better but more complex.
- Failure signatures:
  - Divergence: ADMM iterations fail to converge (possibly due to non-convexity or poor penalty parameter)
  - Performance drop: Pruned model performs worse than expected (possibly due to suboptimal mask or insufficient update iterations)
  - Memory issues: Unable to cache necessary matrices (possibly due to model size or limited resources)
- First 3 experiments:
  1. Run ADMM weight update on a single layer with a fixed mask and compare reconstruction error vs. gradient descent.
  2. Test gradual pruning vs. one-shot pruning on a small model to verify performance gains.
  3. Vary the ADMM penalty parameter ρ and observe its effect on convergence speed and final performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the ADMM-based pruning algorithm compare to other state-of-the-art methods when applied to even larger language models, such as those with hundreds of billions of parameters?
- Basis in paper: The paper demonstrates effectiveness on LLaMA and LLaMA2 models but does not explore performance on models with hundreds of billions of parameters.
- Why unresolved: The paper focuses on models up to LLaMA2-70B, and there is a lack of empirical evidence on the scalability of the ADMM-based method to much larger models.
- What evidence would resolve it: Conducting experiments with the ADMM-based pruning algorithm on models with hundreds of billions of parameters and comparing the results with other state-of-the-art methods would provide the necessary evidence.

### Open Question 2
- Question: How does the gradual pruning mask selection in the ADMM-based method affect the overall pruning performance compared to other mask selection strategies?
- Basis in paper: The paper introduces a gradual pruning mask selection strategy and shows its effectiveness, but does not compare it extensively with other mask selection strategies.
- Why unresolved: The paper focuses on the ADMM-based method with gradual pruning mask selection, and there is a lack of direct comparison with other mask selection strategies.
- What evidence would resolve it: Conducting experiments with the ADMM-based method using different mask selection strategies and comparing the results would provide the necessary evidence.

### Open Question 3
- Question: How does the choice of the penalty factor (ρ) in the ADMM-based method impact the convergence and quality of the weight updates?
- Basis in paper: The paper mentions that the ADMM-based method works well with the default setting of ρ=1, but does not explore the impact of different choices of ρ.
- Why unresolved: The paper uses a fixed value of ρ=1, and there is a lack of empirical evidence on the sensitivity of the ADMM-based method to the choice of ρ.
- What evidence would resolve it: Conducting experiments with the ADMM-based method using different values of ρ and analyzing the impact on convergence and weight update quality would provide the necessary evidence.

## Limitations
- Convergence guarantees are unproven for non-linear layers with activation functions like ReLU or GELU
- Computational overhead characterization is incomplete, focusing on iteration count rather than total runtime or memory usage
- Generalization beyond WikiText is limited, with unclear performance on downstream fine-tuning tasks

## Confidence

**High confidence (8-10/10)**: The core ADMM formulation for linear layers is mathematically sound, and the empirical superiority over baseline methods (Wanda, SparseGPT) on WikiText is well-demonstrated with clear numerical results.

**Medium confidence (5-7/10)**: The extension to non-linear layers through iterative linearization is plausible but lacks rigorous convergence analysis. The claim of "state-of-the-art" performance across all sparsity levels is supported by experiments but may not generalize to all LLM architectures or tasks.

**Low confidence (2-4/10)**: Claims about computational efficiency relative to gradient descent are primarily theoretical; actual runtime comparisons are absent. The assertion that ADMM "sidesteps all problems of previous solutions" overstates the evidence, as some limitations (like memory requirements for matrix inversions) remain.

## Next Checks

1. **Convergence verification on non-linear layers**: Implement the iterative ADMM approach on a layer with non-linear activation (e.g., a single transformer block) and empirically verify that reconstruction error decreases monotonically across iterations. Track both reconstruction error and actual downstream task performance.

2. **Runtime and memory benchmarking**: Compare wall-clock time and peak memory usage between ADMM-based weight updates and gradient descent approaches on the same pruning task. Include both single-layer and multi-layer experiments to capture scaling behavior.

3. **Fine-tuning task sensitivity analysis**: Evaluate pruned models on a suite of fine-tuning benchmarks (e.g., GLUE, SuperGLUE) at multiple sparsity levels to assess whether perplexity improvements translate to downstream task performance, particularly at higher sparsity rates where pruning typically degrades fine-tuning capability.