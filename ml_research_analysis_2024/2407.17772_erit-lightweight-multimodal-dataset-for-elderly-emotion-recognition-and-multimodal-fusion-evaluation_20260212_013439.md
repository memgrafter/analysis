---
ver: rpa2
title: ERIT Lightweight Multimodal Dataset for Elderly Emotion Recognition and Multimodal
  Fusion Evaluation
arxiv_id: '2407.17772'
source_url: https://arxiv.org/abs/2407.17772
tags:
- emotion
- recognition
- dataset
- erit
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ERIT is a novel multimodal dataset designed for lightweight emotion
  recognition in elderly individuals and multimodal fusion evaluation. It combines
  text transcriptions and image frames from videos of elderly people reacting to various
  stimuli, with seven emotion labels (anger, disgust, fear, happiness, sadness, surprise,
  neutral) assigned to each sample.
---

# ERIT Lightweight Multimodal Dataset for Elderly Emotion Recognition and Multimodal Fusion Evaluation

## Quick Facts
- arXiv ID: 2407.17772
- Source URL: https://arxiv.org/abs/2407.17772
- Reference count: 7
- ERIT is a novel multimodal dataset designed for lightweight emotion recognition in elderly individuals and multimodal fusion evaluation.

## Executive Summary
ERIT is a novel multimodal dataset designed for lightweight emotion recognition in elderly individuals and multimodal fusion evaluation. It combines text transcriptions and image frames from videos of elderly people reacting to various stimuli, with seven emotion labels (anger, disgust, fear, happiness, sadness, surprise, neutral) assigned to each sample. The dataset is built from the ElderReact video dataset, with emotion labels validated using DeepFace. ERIT is publicly available and contains 2788 samples split into training, validation, and test sets. Experiments using large multimodal models (GPT-4V, GPT-4o, LLaMA with adapter) show that fusion of text and image outperforms single modalities, achieving 42.3% accuracy on the test set. ERIT also addresses the underrepresentation of elderly individuals in emotion recognition research, providing a lightweight resource for both emotion recognition and multimodal fusion evaluation.

## Method Summary
ERIT is constructed from the ElderReact video dataset, which contains videos of elderly individuals reacting to various stimuli. The dataset extracts text transcriptions and image frames from these videos. Emotion labels are assigned using DeepFace, a deep learning-based facial expression recognition tool, and validated through manual inspection. The dataset is split into training, validation, and test sets, with 2788 samples in total. Experiments are conducted using large multimodal models (GPT-4V, GPT-4o, LLaMA with adapter) to evaluate the performance of emotion recognition using text, image, and fused modalities. The fusion approach combines text and image features to improve emotion recognition accuracy.

## Key Results
- ERIT contains 2788 samples with seven emotion labels (anger, disgust, fear, happiness, sadness, surprise, neutral).
- Fusion of text and image modalities outperforms single modalities, achieving 42.3% accuracy on the test set.
- The dataset addresses the underrepresentation of elderly individuals in emotion recognition research.

## Why This Works (Mechanism)
The effectiveness of ERIT stems from its focus on a specific demographic (elderly individuals) and the combination of multimodal data (text and image) for emotion recognition. By leveraging both visual and textual cues, the models can capture a more comprehensive representation of emotions, leading to improved performance compared to single-modality approaches. The use of DeepFace for emotion labeling provides a scalable and automated method for assigning labels, although manual validation ensures accuracy.

## Foundational Learning
1. **Multimodal Fusion**: Combining text and image features to improve emotion recognition accuracy.
   - Why needed: Single modalities may miss important contextual or visual cues.
   - Quick check: Compare accuracy of fused models vs. single-modality models.

2. **Elderly Emotion Recognition**: Addressing the underrepresentation of elderly individuals in emotion recognition research.
   - Why needed: Existing datasets often focus on younger populations, leading to biases.
   - Quick check: Evaluate model performance on elderly-specific datasets vs. general datasets.

3. **DeepFace Validation**: Using automated tools for emotion labeling with manual validation.
   - Why needed: Manual labeling is time-consuming and may introduce human biases.
   - Quick check: Compare DeepFace-labeled emotions with human-labeled emotions.

## Architecture Onboarding
**Component Map**: Text extraction -> Image extraction -> Emotion labeling (DeepFace) -> Manual validation -> Dataset split -> Model training (GPT-4V, GPT-4o, LLaMA with adapter) -> Fusion evaluation.

**Critical Path**: Text and image extraction -> Emotion labeling -> Dataset split -> Model training and evaluation.

**Design Tradeoffs**: The use of automated emotion labeling (DeepFace) speeds up the process but may introduce biases. Manual validation ensures accuracy but is time-consuming. The focus on elderly individuals addresses underrepresentation but limits generalizability to other age groups.

**Failure Signatures**: Low accuracy on the test set (42.3%) suggests challenges in emotion recognition, possibly due to the complexity of elderly expressions or the limitations of the models. Biases in the emotion labels could also affect model performance.

**First Experiments**:
1. Train and evaluate models on the ERIT dataset using text, image, and fused modalities.
2. Compare the performance of fused models with single-modality models.
3. Conduct a human-in-the-loop validation of the emotion labels to ensure accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- The dataset's small size (2788 samples) may limit the robustness of the findings.
- The reliance on automated emotion labeling tools (DeepFace) introduces potential biases.
- The focus on elderly individuals may limit the generalizability of the findings to other age groups.

## Confidence
- High: The dataset's construction and validation processes are well-described.
- Medium: The reliance on automated emotion labeling tools introduces uncertainty.
- Low: The 42.3% accuracy on the test set suggests room for improvement.

## Next Checks
1. **Manual Validation of Emotion Labels**: Conduct a human-in-the-loop validation of the emotion labels to ensure accuracy, particularly for elderly facial expressions, and compare the results with the DeepFace validation.
2. **Cross-Demographic Testing**: Test the models trained on ERIT on datasets containing younger or more diverse age groups to assess the generalizability of the findings and identify any biases in the emotion recognition models.
3. **Contextual Expansion**: Expand the dataset by including a wider range of stimuli and contexts to capture a more comprehensive set of emotional expressions and improve the robustness of the emotion recognition models.