---
ver: rpa2
title: 'Analyzing Images of Legal Documents: Toward Multi-Modal LLMs for Access to
  Justice'
arxiv_id: '2412.15260'
source_url: https://arxiv.org/abs/2412.15260
tags:
- legal
- information
- forms
- llms
- justice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates using multi-modal LLMs to extract structured
  information from images of legal documents, specifically rental agreements. The
  researchers tested GPT-4o on images of filled-out forms under various conditions:
  typed PDFs, neatly handwritten forms, sloppily handwritten forms, and low-quality
  photos.'
---

# Analyzing Images of Legal Documents: Toward Multi-Modal LLMs for Access to Justice

## Quick Facts
- arXiv ID: 2412.15260
- Source URL: https://arxiv.org/abs/2412.15260
- Authors: Hannes Westermann; Jaromir Savelka
- Reference count: 38
- Key outcome: GPT-4o achieved 73% accuracy in extracting 14 fields from rental agreement images, with performance varying by field type and image quality

## Executive Summary
This paper investigates using multi-modal LLMs to extract structured information from images of legal documents, specifically rental agreements. The researchers tested GPT-4o on images of filled-out forms under various conditions: typed PDFs, neatly handwritten forms, sloppily handwritten forms, and low-quality photos. They found that GPT-4o achieved 73% accuracy overall in extracting 14 different fields from the forms. Accuracy varied significantly by field type and image quality, with typed documents yielding near-perfect results while handwritten forms and poor-quality photos showed decreased performance.

## Method Summary
The researchers used GPT-4o (gpt-4o-2024-08-06) via OpenAI API with temperature=0 to extract specific fields from images of filled-out Ontario Residential Tenancy Agreement forms. They created 15 samples across 3 scenarios with increasing difficulty and 5 different image formats (typed PDF screenshot, neat handwritten HD, sloppy handwritten HD, neat handwritten SD, sloppy handwritten SD). The model was prompted to extract 14 fields in JSON format from base64-encoded images, with accuracy measured as percentage of exact matches against ground truth labels.

## Key Results
- GPT-4o achieved 73% overall accuracy in extracting 14 fields from rental agreement images
- Typed documents yielded near-perfect results while handwritten forms and poor-quality photos showed decreased performance
- The model performed better with common names and addresses it could infer from training data
- Accuracy varied significantly by field type, with some fields being extracted nearly perfectly while others showed much lower success rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal LLMs can reliably locate and extract information from images of legal forms even when handwriting quality is poor.
- Mechanism: The model leverages its pre-trained vision-language capabilities to recognize text regions in images, understand field labels, and map extracted text to structured fields based on contextual understanding of the document layout.
- Core assumption: The LLM has sufficient visual acuity and contextual understanding to process handwritten text variations and noisy images while maintaining spatial awareness of form fields.
- Evidence anchors:
  - [abstract] "Our initial results are promising, but reveal some limitations (e.g., when the image quality is low)"
  - [section] "it always picked up at least some matching letters or numbers... the model sometimes struggled to extract the correct values from the fields, it had no trouble locating the information on the page"
  - [corpus] Weak evidence - no direct corpus support for vision-language form extraction mechanisms
- Break condition: When image quality deteriorates beyond recognition of text and field boundaries, or when handwriting is so irregular that character recognition fails completely.

### Mechanism 2
- Claim: The model can leverage prior knowledge from training data to improve extraction accuracy for common names and predictable fields.
- Mechanism: The LLM uses its pretraining on web-scale data to recognize common names, cities, and address patterns, effectively "filling in" expected values even when image quality is suboptimal.
- Core assumption: The training corpus contained sufficient examples of common legal form fields and geographic data to allow the model to recognize and complete these patterns.
- Evidence anchors:
  - [section] "The perfect accuracy here may indicate that the model is able to benefit from its pre-training to correctly 'guess' the value even if the image quality is lacking"
  - [section] "Names such as Robert and Michelle were easier for the model to spot than, e.g., Jame or Joon"
  - [corpus] Weak evidence - no corpus support for how pretraining specifically helps with form field extraction
- Break condition: When encountering uncommon names, addresses, or specialized terminology not well-represented in the pretraining data.

### Mechanism 3
- Claim: Multi-modal LLMs can process structured form extraction tasks by following explicit JSON formatting instructions in prompts.
- Mechanism: The model uses its instruction-following capabilities to parse image content and structure the output according to specified JSON schemas, maintaining consistency across multiple extraction tasks.
- Core assumption: The LLM has been trained with sufficient instruction-tuning data to understand and follow complex output formatting requirements.
- Evidence anchors:
  - [section] "We provided the model with a system prompt: Analyze the provided image. Extract the values exactly as they appear, and return them in the json format specified below"
  - [section] "For ease of analysis, we asked the model to provide every field as a textual value"
  - [corpus] Weak evidence - no corpus support for structured output capabilities in vision-language models
- Break condition: When instructions become too complex or when the model encounters ambiguous field mappings that cannot be resolved through prompt engineering.

## Foundational Learning

- Concept: Vision-Language Model Architecture
  - Why needed here: Understanding how multi-modal models process both image and text inputs simultaneously is crucial for debugging extraction failures and optimizing prompts
  - Quick check question: How does a vision-language model typically tokenize and process image inputs differently from text inputs?

- Concept: OCR (Optical Character Recognition) Fundamentals
  - Why needed here: While the model performs end-to-end extraction, understanding basic OCR principles helps in preprocessing images and interpreting model behavior
  - Quick check question: What are the key differences between traditional OCR and vision-language model-based text extraction from images?

- Concept: Prompt Engineering for Structured Outputs
  - Why needed here: The accuracy of field extraction heavily depends on how well the prompt guides the model to follow the desired output format and extraction criteria
  - Quick check question: How do different prompt structures (few-shot examples vs. direct instructions) affect the consistency of structured output generation?

## Architecture Onboarding

- Component map:
  Image preprocessing pipeline -> Vision encoder for feature extraction -> Text encoder for prompt processing -> Cross-modal attention layers for integration -> Output decoder with JSON formatting logic -> Post-processing validation layer

- Critical path:
  1. Image ingestion and preprocessing
  2. Vision feature extraction
  3. Prompt integration and context processing
  4. Cross-modal reasoning and field location
  5. Text extraction and value mapping
  6. JSON formatting and validation
  7. Output delivery

- Design tradeoffs:
  - Model size vs. inference speed (larger models may be more accurate but slower)
  - Temperature setting (affects consistency vs. creativity in extraction)
  - Image preprocessing intensity (aggressive preprocessing may help but could lose context)
  - Prompt complexity vs. model instruction-following capacity

- Failure signatures:
  - Systematic errors on specific field types (indicates model bias or training data imbalance)
  - Complete failure on low-quality images (suggests vision component limitations)
  - Inconsistent output formats (indicates prompt clarity issues)
  - Hallucinations or fabricated data (indicates temperature or confidence threshold issues)

- First 3 experiments:
  1. Test model performance on high-quality typed forms vs. handwritten forms to establish baseline accuracy differences
  2. Vary prompt structure (few-shot examples vs. direct instructions) to measure impact on extraction consistency
  3. Test different image preprocessing techniques (contrast enhancement, noise reduction) to measure impact on low-quality image performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model performance vary when extracting information from forms in languages other than English?
- Basis in paper: [inferred] The paper focuses on English names and forms, but notes that LLMs may benefit from pre-training data. The performance with "common names" suggests language-specific patterns may affect accuracy.
- Why unresolved: The experiments only used English forms and names. The authors note that names like "Robert" and "Michelle" were easier to extract than less common names, suggesting potential language-specific biases.
- What evidence would resolve it: Testing the same extraction pipeline on forms in multiple languages with varied name frequencies and comparing accuracy rates.

### Open Question 2
- Question: What is the impact of form layout variations on extraction accuracy, such as different font sizes, colors, or page orientations?
- Basis in paper: [explicit] The authors mention investigating image quality variations but only tested standard layout with different handwriting quality and camera resolutions.
- Why unresolved: The experiments used consistent form layouts. The paper notes that "forms may not be filled out in the expected ways" but doesn't explore layout variations beyond handwriting quality.
- What evidence would resolve it: Testing extraction accuracy across forms with varied layouts, fonts, colors, and orientations while controlling for handwriting quality.

### Open Question 3
- Question: How does the accuracy of multi-modal LLMs compare to traditional OCR systems when extracting structured data from legal documents?
- Basis in paper: [explicit] The authors note that "LLMs may exhibit somewhat different behaviors from traditional OCR approaches" and mention the model's preference for common data.
- Why unresolved: The paper only tests GPT-4o without comparison to other extraction methods. The authors acknowledge this is an important distinction but don't evaluate it.
- What evidence would resolve it: Head-to-head comparison of multi-modal LLM extraction versus traditional OCR systems on the same dataset, measuring accuracy and error patterns.

## Limitations

- Dataset size is relatively small (15 samples across 5 image conditions), which may not capture full variability in real-world legal documents
- Evaluation uses binary "exact match" metric that doesn't account for near-misses or semantic equivalence
- Study doesn't explore temporal aspects like extraction speed or cost per document, which are critical for real-world deployment

## Confidence

**High Confidence**: The core finding that GPT-4o can successfully extract information from high-quality typed legal documents (near 100% accuracy) is well-supported by the data and consistent with known capabilities of modern vision-language models.

**Medium Confidence**: The accuracy figures for handwritten documents (73% overall) are reasonably supported but may not generalize well beyond the specific document type tested.

**Low Confidence**: The claim that pretraining specifically helps with form field extraction (Mechanism 2) lacks direct evidence in the paper and relies on indirect inference from observed patterns in the results.

## Next Checks

1. **Scale Test**: Expand the evaluation to 100+ document samples across diverse legal form types to verify if the 73% accuracy holds across broader document variability and to identify any systematic biases.

2. **Quality Gradient Analysis**: Systematically test intermediate image quality levels between HD and SD to determine the precise quality threshold where performance begins to degrade, helping establish practical minimum requirements for real-world deployment.

3. **Semantic Match Evaluation**: Re-run the accuracy assessment using a semantic matching metric that considers near-misses (e.g., "St" vs "Street") as partial successes to better reflect real-world utility and to understand how many failures are due to strict matching criteria versus actual model limitations.