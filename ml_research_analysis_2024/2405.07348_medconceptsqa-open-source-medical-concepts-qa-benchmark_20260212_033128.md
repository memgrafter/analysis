---
ver: rpa2
title: 'MedConceptsQA: Open Source Medical Concepts QA Benchmark'
arxiv_id: '2405.07348'
source_url: https://arxiv.org/abs/2405.07348
tags:
- medical
- learning
- benchmark
- gpt-4
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MedConceptsQA, a large-scale open-source\
  \ benchmark designed to evaluate large language models' understanding of medical\
  \ concepts. The benchmark includes over 800,000 multiple-choice questions across\
  \ three vocabularies\u2014diagnoses, procedures, and drugs\u2014with three difficulty\
  \ levels each."
---

# MedConceptsQA: Open Source Medical Concepts QA Benchmark
## Quick Facts
- arXiv ID: 2405.07348
- Source URL: https://arxiv.org/abs/2405.07348
- Authors: Ofir Ben Shoham; Nadav Rappoport
- Reference count: 40
- Primary result: Large-scale medical concepts QA benchmark with 800K+ questions shows clinical LLMs perform near random guessing while GPT-3.5/4 significantly outperform them

## Executive Summary
MedConceptsQA introduces a comprehensive open-source benchmark designed to evaluate large language models' understanding of medical concepts across diagnoses, procedures, and drugs. The benchmark features over 800,000 multiple-choice questions spanning three difficulty levels. Despite being trained on medical data, clinical LLMs showed surprisingly poor performance, barely exceeding random guessing rates. In contrast, GPT-3.5 and GPT-4 demonstrated substantial improvements of 27-37% absolute accuracy over clinical models, suggesting fundamental gaps in how current clinical LLMs process medical knowledge.

## Method Summary
The benchmark was constructed using medical ontologies and vocabularies to generate multiple-choice questions across three medical concept categories. Questions were categorized into three difficulty levels based on the complexity of relationships between concepts. The evaluation compared commercial LLMs (GPT-3.5, GPT-4) against clinical LLMs using standardized metrics. The dataset and evaluation code were made publicly available to enable community validation and improvement of medical AI models.

## Key Results
- Clinical LLMs performed near random guessing (25% baseline) across all difficulty levels
- GPT-3.5 and GPT-4 achieved 27-37% absolute accuracy improvements over clinical LLMs
- Benchmark includes 800K+ questions across diagnoses, procedures, and drugs vocabularies
- Three difficulty levels per vocabulary category tested model reasoning capabilities

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its systematic construction using standardized medical ontologies, ensuring comprehensive coverage of medical concepts while maintaining consistent difficulty progression. The multiple-choice format enables precise measurement of concept understanding without ambiguity in responses. The large scale (800K+ questions) provides statistical power to detect meaningful performance differences between models, while the three difficulty levels reveal specific weaknesses in medical reasoning capabilities.

## Foundational Learning
- Medical ontologies and vocabularies: Essential for understanding the structured knowledge base used to generate questions; quick check by reviewing SNOMED CT or similar standards
- Multiple-choice question design: Critical for creating unambiguous evaluation metrics; quick check by examining distractor quality and difficulty calibration
- Large-scale benchmarking methodology: Necessary for establishing reliable performance baselines; quick check by analyzing sample size calculations and statistical power
- Clinical LLM training paradigms: Important context for understanding why specialized models may underperform general models; quick check by comparing training datasets and objectives
- Medical concept relationships: Fundamental for assessing reasoning capabilities; quick check by examining cross-category question performance

## Architecture Onboarding
- Component map: Question generator -> Difficulty classifier -> Model evaluator -> Performance analyzer
- Critical path: Ontology extraction → Question generation → Difficulty assessment → Model evaluation → Statistical analysis
- Design tradeoffs: Multiple-choice format ensures consistency but may not capture full reasoning complexity; large scale enables robust statistics but requires significant computational resources
- Failure signatures: Random performance on intermediate difficulty questions suggests fundamental gaps in concept integration; consistent underperformance across categories indicates systemic issues
- Three first experiments: 1) Evaluate same models on open-ended medical questions; 2) Test larger clinical models (70B+ parameters); 3) Conduct human evaluation of benchmark relevance to clinical practice

## Open Questions the Paper Calls Out
None

## Limitations
- Multiple-choice format may not fully capture clinical reasoning complexity
- Clinical LLMs tested were relatively small (7B parameters) compared to frontier models
- Benchmark may not adequately assess real-world medical decision-making contexts

## Confidence
- Clinical LLMs perform near random guessing: High
- GPT-4 outperforms clinical LLMs by 27-37% absolute accuracy: High
- Clinical LLMs struggle with medical concept understanding and reasoning: Medium

## Next Checks
1. Evaluate clinical LLMs using open-ended responses and clinical vignettes to assess performance outside multiple-choice constraints
2. Test larger clinical models (70B+ parameters) to determine if scale correlates with improved medical concept understanding
3. Conduct human evaluation studies to validate benchmark relevance to actual clinical reasoning tasks and identify gaps between benchmark performance and real-world medical decision-making