---
ver: rpa2
title: Text-only Synthesis for Image Captioning
arxiv_id: '2405.18258'
source_url: https://arxiv.org/abs/2405.18258
tags:
- data
- toca
- text
- token
- captioning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a text-only synthesis approach for image captioning
  that eliminates the need for paired image-text data by deconstructing captions into
  lexical pairs and structure templates, which are then recombined and input into
  a large language model to generate diverse, domain-specific captions. Experiments
  show the method improves zero-shot captioning performance by nearly 5 CIDEr points
  in cross-domain settings and over 20 CIDEr points in data-efficient scenarios, while
  also demonstrating strong generalizability and flexibility across tasks like style
  captioning and video captioning.
---

# Text-only Synthesis for Image Captioning

## Quick Facts
- arXiv ID: 2405.18258
- Source URL: https://arxiv.org/abs/2405.18258
- Authors: Qing Zhou; Junlin Huang; Qiang Li; Junyu Gao; Qi Wang
- Reference count: 40
- Primary result: Improves zero-shot captioning performance by nearly 5 CIDEr points in cross-domain settings

## Executive Summary
This paper introduces ToCa (Text-only Synthesis for image Captioning), a novel approach that generates synthetic image captions without requiring paired image-text data. The method deconstructs existing captions into lexical pairs and structure templates, then uses a large language model to recombine these elements into diverse, domain-specific captions. ToCa demonstrates significant improvements in zero-shot captioning performance, achieving nearly 5 CIDEr points gains in cross-domain settings and over 20 CIDEr points in data-efficient scenarios.

## Method Summary
ToCa extracts lexical pairs (relationships between tokens) and structure templates (POS patterns with function words) from a caption corpus. These components are recombined and fed into a quantized LLM (Mistral-7B) to generate synthetic captions. The method operates in three scenarios: cross-domain (using captions from one domain to caption another), in-domain (improving data efficiency within the same domain), and data-efficient (augmenting limited target data). The generated captions are then used to train ViECap models, with optional fine-tuning on accessible target data.

## Key Results
- Achieves 5+ CIDEr points improvement in cross-domain zero-shot captioning
- Delivers over 20 CIDEr points gains in data-efficient settings
- Demonstrates strong generalizability across style captioning and video captioning tasks

## Why This Works (Mechanism)

### Mechanism 1: Lexical Pair Extraction
The method extracts entity relationships as lexical pairs rather than treating captions as flat sequences, capturing domain-specific co-occurrence patterns more effectively than generic syntax modeling.

### Mechanism 2: Structure Template Preservation
By isolating POS patterns and function words into templates, the approach ensures generated captions maintain grammatical coherence even when lexical words are swapped, preserving syntactic diversity.

### Mechanism 3: LLM-Based Masked Completion
Masked sentence templates force the LLM to fill in contextually appropriate lexical words, producing captions that match target style while introducing novel entity combinations through controlled recombination.

## Foundational Learning

- **Lexical vs function words distinction**: Enables decomposition of captions into semantic content (lexical) and grammatical scaffolding (function). Quick check: Can you identify which of these words would be lexical vs function: "the", "dog", "running", "on", "quickly"?

- **Conditional probability approximation in text synthesis**: Allows efficient generation of large caption datasets by approximating complex dependencies between lexical choices. Quick check: If "dog" appears with "running" 30 times and "sleeping" 10 times in corpus, what's the approximate conditional probability of "sleeping" given "dog"?

- **CLIP space as shared representation**: Provides bridge between generated text and image features for zero-shot captioning. Quick check: Why does using CLIP embeddings for both text and images enable zero-shot transfer between domains?

## Architecture Onboarding

- **Component map**: Corpus preprocessing → Lexical pair extraction → Structure template construction → LLM prompt generation → Text synthesis → Model training
- **Critical path**: Prompt generation → LLM inference → Text filtering → Dataset construction → Model training (bottleneck: LLM inference time 1-2 seconds per caption on RTX 3090)
- **Design tradeoffs**: Recombination vs generation (favors recombination of existing patterns), template coverage vs diversity (more templates provide better coverage but increase complexity), quantization vs quality (8-bit quantization enables GPU inference but may reduce output quality)
- **Failure signatures**: Repetitive captions (insufficient template diversity or lexical variety), grammatically incorrect captions (template construction errors or LLM completion failures), off-topic captions (LLM misalignment with target domain style)
- **First 3 experiments**: 1) Verify lexical pair extraction correctly identifies entity relationships in small corpus sample, 2) Test template construction by reconstructing original captions from extracted components, 3) Generate 100 captions with minimal templates to validate LLM completion quality before scaling

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions but leaves several unresolved issues including the optimal corpus size for different domains, strategies for handling rare entities not well-represented in corpus, and the impact of different LLM choices on synthesis quality.

## Limitations

- Heavy dependence on LLM performance, with quantization potentially reducing output quality
- Assumes corpus contains sufficient structural diversity to capture all necessary caption patterns
- Zero-shot evaluation may not fully validate whether synthetic captions correspond to actual visual content

## Confidence

- **High Confidence**: Core mechanism of lexical pair extraction and structure template construction is technically sound with substantial quantitative improvements
- **Medium Confidence**: Generalizability claims across different tasks demonstrated but with limited ablation studies
- **Low Confidence**: Exact prompt engineering details and synthesis hyperparameters are underspecified, creating uncertainty about reproducibility

## Next Checks

1. **Template Coverage Analysis**: Systematically evaluate template extraction coverage by measuring percentage of unique syntactic patterns captured and test performance degradation as coverage decreases.

2. **Quantization Quality Assessment**: Compare caption quality and downstream model performance between full-precision and 8-bit quantized LLM outputs using standardized metrics.

3. **Visual Grounding Validation**: Implement validation step checking whether synthetic captions correspond to visual content in target domain using CLIP-based similarity scores or human evaluation.