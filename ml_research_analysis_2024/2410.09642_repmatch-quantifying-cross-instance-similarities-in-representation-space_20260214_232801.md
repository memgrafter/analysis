---
ver: rpa2
title: 'RepMatch: Quantifying Cross-Instance Similarities in Representation Space'
arxiv_id: '2410.09642'
source_url: https://arxiv.org/abs/2410.09642
tags:
- repmatch
- dataset
- instances
- similarity
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RepMatch introduces a method for quantifying similarity between
  subsets of training data by comparing the knowledge encoded in models trained on
  them. Unlike prior approaches focused on individual instances, RepMatch enables
  dataset-to-dataset and instance-to-dataset analyses.
---

# RepMatch: Quantifying Cross-Instance Similarities in Representation Space

## Quick Facts
- arXiv ID: 2410.09642
- Source URL: https://arxiv.org/abs/2410.09642
- Reference count: 10
- Primary result: Introduces a method to quantify similarity between training data subsets by comparing LoRA-adapted models trained on them

## Executive Summary
RepMatch addresses the challenge of quantifying similarity between subsets of training data by comparing the knowledge encoded in models trained on each subset. The method leverages LoRA (Low-Rank Adaptation) to efficiently capture model updates in low-rank matrices, then uses Grassmann similarity to measure the alignment of learned representation subspaces. Experiments across multiple NLP tasks and models demonstrate that RepMatch can identify more representative subsets than random sampling, uncover heuristics in challenge datasets like HANS, and reliably measure task similarities.

## Method Summary
RepMatch quantifies similarity between datasets by training LoRA-adapted models on each subset and comparing their learned representations. The method fine-tunes models using LoRA with rank-1 matrices to capture task-specific adaptations, then computes Grassmann similarity between the subspaces formed by the LoRA matrices across all layers. The RepMatch score is the average of these similarity values. This approach enables efficient dataset-to-dataset and instance-to-dataset analyses by focusing on the essential changes made during fine-tuning rather than full model parameters.

## Key Results
- RepMatch successfully identifies more representative subsets than random sampling, leading to better performance with the same number of instances
- The method uncovers known heuristics in the HANS challenge dataset, showing high similarity between instances with similar heuristics
- RepMatch reliably measures task similarities, with SST-2 and SST-5 showing high similarity (0.78) while SST-2 and SNLI show low similarity (0.28)
- The approach is robust across different model architectures (BERT, ELECTRA, LLaMA2) and LoRA ranks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA matrices capture task-specific feature extraction patterns in a low-rank format, enabling efficient similarity comparisons.
- Mechanism: LoRA constrains weight updates to low-rank matrices (Ar_i * Br_i) that capture the essential task-specific transformations while freezing pre-trained weights. These low-rank adaptation matrices encode the features the model learns from the training data.
- Core assumption: Task-specific features are compressible into low-rank matrices without significant information loss.
- Evidence anchors:
  - [abstract]: "LoRA efficiently captures changes in a weight matrix through a low-rank matrix"
  - [section]: "LoRA restricts these ∆Wi matrices to be low-rank...effectively reducing the number of parameters from d² to 2rd"
  - [corpus]: Weak - no direct mention of LoRA's compression efficiency in neighbor papers
- Break condition: If task-specific features require full-rank matrices to be represented accurately, similarity measurements will be unreliable.

### Mechanism 2
- Claim: Grassmann similarity between LoRA matrices quantifies the alignment of learned representation subspaces.
- Mechanism: Grassmann similarity measures the similarity between subspaces formed by the LoRA matrices by computing the norm of the product of their right singular vectors. Higher similarity indicates better alignment of the learned features.
- Core assumption: Models trained on similar data learn similar feature subspaces that can be compared via Grassmann similarity.
- Evidence anchors:
  - [section]: "we adopt the Grassmann similarity. The metric was used by Hu et al. (2021) to discern subspace similarities"
  - [abstract]: "we measure the similarity between two subsets...by comparing the models trained exclusively on each subset"
  - [corpus]: Weak - neighbor papers focus on contrastive learning rather than subspace similarity
- Break condition: If the learned subspaces are fundamentally different in structure (e.g., orthogonal), Grassmann similarity will fail to capture meaningful relationships.

### Mechanism 3
- Claim: The average RepMatch score across layers provides a robust measure of dataset similarity that is stable across training seeds.
- Mechanism: By averaging the maximum Grassmann similarity values across all layers, RepMatch creates a stable metric that is less sensitive to layer-specific variations and training randomness.
- Core assumption: The similarity of LoRA matrices is consistent across layers when trained on similar data.
- Evidence anchors:
  - [section]: "The RepMatch score for the whole model is simply computed as the average of RepMatch scores across all layers"
  - [abstract]: "RepMatch quantifies the similarity between subsets of training instances by comparing the knowledge encoded in models trained on them"
  - [corpus]: Weak - neighbor papers don't discuss layer-wise averaging for similarity measures
- Break condition: If certain layers capture more task-relevant information than others, averaging may dilute important signals.

## Foundational Learning

- Concept: Linear algebra - matrix rank and subspace theory
  - Why needed here: Understanding how low-rank approximations preserve essential information and how subspace similarity is measured
  - Quick check question: What is the relationship between a matrix's rank and the dimensionality of its column space?

- Concept: Representational similarity analysis
  - Why needed here: The core idea that models trained on similar data develop similar internal representations that can be compared
  - Quick check question: Why would models trained on different datasets develop different internal representations even with the same architecture?

- Concept: Stochastic optimization and training dynamics
  - Why needed here: Understanding why RepMatch must be robust to training seed variations and how training randomness affects learned representations
  - Quick check question: How does changing the random seed affect the initialization and training trajectory of a neural network?

## Architecture Onboarding

- Component map:
  - Input: Two subsets of training data (S1, S2)
  - LoRA fine-tuning: Train two models on S1 and S2 using LoRA to obtain Ar_i and Br_i matrices
  - Subspace extraction: Compute the right singular vectors from LoRA matrices via SVD
  - Similarity computation: Calculate Grassmann similarity between subspaces for each layer
  - Aggregation: Average maximum similarity values across layers to get RepMatch score

- Critical path: LoRA fine-tuning → SVD computation → Grassmann similarity → RepMatch aggregation
- Design tradeoffs:
  - LoRA rank selection: Higher rank captures more information but increases computation; lower rank is efficient but may lose information
  - Layer selection: Using all layers provides comprehensive measurement but increases computation; using select layers may miss important signals
  - Aggregation method: Averaging is simple but may not capture layer-specific importance

- Failure signatures:
  - Low RepMatch scores despite known dataset similarity: May indicate insufficient LoRA rank or inappropriate layer selection
  - High variance across runs: May indicate instability in fine-tuning or need for more training epochs
  - Unexpected similarity patterns: May indicate bias in the datasets or artifacts in the LoRA matrices

- First 3 experiments:
  1. Compare RepMatch between two models fine-tuned on the same dataset with different seeds to establish baseline similarity
  2. Compare RepMatch between models fine-tuned on datasets known to be similar (e.g., SST-2 vs SST-5) vs different (e.g., SST-2 vs SNLI)
  3. Test RepMatch's ability to identify representative subsets by comparing models trained on top-k instances vs random subsets of same size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the rank of LoRA matrices affect the effectiveness of RepMatch in identifying representative subsets across different task complexities?
- Basis in paper: Explicit - The paper mentions that rank 1 was used for efficiency, but rank 4 showed improvements in some datasets, with SST-5 showing decreased performance due to label imbalance in top-100 instances.
- Why unresolved: The paper only briefly mentions rank 4 experiments on a small subset of the dataset and leaves further analysis to future work.
- What evidence would resolve it: Comprehensive experiments varying LoRA rank across multiple datasets and task complexities, including analysis of label distribution in selected subsets.

### Open Question 2
- Question: Can RepMatch be effectively applied to non-Transformer architectures and non-textual modalities?
- Basis in paper: Inferred - The paper acknowledges this as a limitation, stating "Although our focus was on Transformer models with a textual modality...we believe this method is applicable to other modalities and settings."
- Why unresolved: All experiments were conducted on Transformer models with textual data, leaving applicability to other architectures and modalities unexplored.
- What evidence would resolve it: Experiments applying RepMatch to architectures like CNNs, RNNs, or Vision Transformers, and to non-textual data like images or audio.

### Open Question 3
- Question: How does the stochastic nature of training batches affect RepMatch scores at the instance level?
- Basis in paper: Explicit - The paper notes that "the relationship between instances within a training batch is not taken into account" and that "randomness of training and instances in a batch can have a non-negligible effect."
- Why unresolved: The paper acknowledges this as a limitation but does not explore how batch composition affects RepMatch reliability.
- What evidence would resolve it: Experiments varying batch compositions and analyzing how RepMatch scores change with different batch arrangements or with repeated measurements.

## Limitations

- The paper only tests RepMatch on Transformer models with textual data, leaving applicability to other architectures and modalities unexplored
- The stochastic nature of training batches and their effect on RepMatch scores at the instance level is not fully characterized
- The choice of LoRA rank (rank 1 vs rank 4) and its impact on RepMatch effectiveness across different task complexities is not thoroughly investigated

## Confidence

**High confidence** in the empirical demonstration that RepMatch can distinguish between different types of datasets and identify representative subsets. The experimental results across multiple tasks and models are consistent and show clear patterns.

**Medium confidence** in the general mechanism claims about LoRA capturing task-specific features and Grassmann similarity measuring subspace alignment. While the approach is theoretically sound, the paper doesn't provide extensive ablations to test these assumptions directly.

**Low confidence** in the robustness of RepMatch across all possible model architectures and dataset types. The experiments focus on specific NLP tasks and models, leaving open questions about generalization to other domains.

## Next Checks

1. **Information preservation test**: Compare the performance of models trained with LoRA vs full fine-tuning on the same datasets to verify that the low-rank approximation doesn't lose critical information needed for similarity measurement.

2. **Layer sensitivity analysis**: Investigate how RepMatch scores vary when using different subsets of layers (e.g., only lower layers, only upper layers, or specific layer ranges) to understand which layers contribute most to dataset similarity signals.

3. **Cross-architecture comparison**: Test RepMatch between models with different architectures (e.g., BERT vs RoBERTa) trained on the same data to determine if the similarity metric is architecture-agnostic or if normalization is needed.