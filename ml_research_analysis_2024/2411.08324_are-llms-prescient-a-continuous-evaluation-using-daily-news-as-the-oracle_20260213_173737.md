---
ver: rpa2
title: Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle
arxiv_id: '2411.08324'
source_url: https://arxiv.org/abs/2411.08324
tags:
- question
- questions
- will
- article
- daily
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Daily Oracle, a continuous evaluation benchmark
  for Large Language Models (LLMs) using automatically generated question-answer pairs
  from daily news articles. The benchmark tests LLMs' ability to forecast future events,
  addressing limitations of static benchmarks that quickly become outdated.
---

# Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle
## Quick Facts
- **arXiv ID**: 2411.08324
- **Source URL**: https://arxiv.org/abs/2411.08324
- **Authors**: Hui Dai; Ryan Teehan; Mengye Ren
- **Reference count**: 40
- **Primary result**: LLM performance degrades significantly over time, with average accuracy dropping by 21.55% on True/False questions and 11.33% on Multiple Choice questions from January 2020 to December 2024

## Executive Summary
This paper introduces Daily Oracle, a continuous evaluation benchmark for Large Language Models (LLMs) using automatically generated question-answer pairs from daily news articles. The benchmark tests LLMs' ability to forecast future events, addressing limitations of static benchmarks that quickly become outdated. The study finds that LLM performance degrades significantly over time, with average accuracy dropping by 21.55% on True/False questions and 11.33% on Multiple Choice questions from January 2020 to December 2024. While Retrieval Augmented Generation (RAG) can improve accuracy by providing recent news articles, the degradation pattern persists even with access to relevant information, indicating the need for continuous model updates to maintain forecasting capabilities.

## Method Summary
The Daily Oracle benchmark automatically generates QA pairs from daily news articles using LLM prompting and filtering. The pipeline extracts hot topics from news clusters, generates questions and answers, and filters them based on seven principles. The evaluation uses three settings: closed-book (no external information), constrained open-book (RAG with news articles up to different cutoff dates), and gold article (direct access to the article the question was generated from). The dataset consists of 1,246,973 English news articles from January 2019 to December 2024, with questions covering True/False and Multiple Choice formats.

## Key Results
- LLM performance degrades significantly over time, with average accuracy dropping by 21.55% on True/False questions and 11.33% on Multiple Choice questions from January 2020 to December 2024
- All models except Claude-3.5-Sonnet struggle with True/False questions, with degradation trends towards the random baseline accuracy of 50%
- RAG improves accuracy by providing recent news articles, but performance degradation persists even with gold article access, suggesting outdated internal representations are the primary cause

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs experience performance degradation over time due to outdated internal representations, even when provided with relevant external information.
- **Mechanism**: As time passes beyond a model's knowledge cutoff, its internal representations become less aligned with current world knowledge. This misalignment causes errors in reasoning and prediction, which cannot be fully corrected by retrieval-augmented generation (RAG) because the fundamental understanding of relationships between concepts is outdated.
- **Core assumption**: The degradation is primarily due to outdated internal representations rather than missing factual knowledge.
- **Evidence anchors**:
  - [abstract]: "While Retrieval Augmented Generation (RAG) has the potential to enhance prediction accuracy, the performance degradation pattern persists"
  - [section 4.3]: "However, accuracy still declines over time. Notably, the gold article setting provides an 'upper bound' of open-book retrieval. The remaining performance drop despite full access to relevant information suggests that the models' internal representations are outdated."

### Mechanism 2
- **Claim**: LLMs show different degradation patterns for different question types due to the nature of temporal generalization required.
- **Mechanism**: True/False questions require more open-ended reasoning about future events, making them harder for LLMs than Multiple Choice questions where distractor options provide contextual cues. The degradation is more pronounced for TF questions as they approach random baseline accuracy.
- **Core assumption**: The complexity of the forecasting task varies significantly between question types.
- **Evidence anchors**:
  - [section 4.3]: "All models except for Claude-3.5-Sonnet struggle with TF questions, where the degradation trends towards the random baseline accuracy of 50%"
  - [section 4.3]: "In contrast, on MC questions, models tend to perform much better than the random baseline at 25%"

### Mechanism 3
- **Claim**: The pre-training corpus distribution affects model performance over time, with certain periods being overrepresented.
- **Mechanism**: LLMs trained on corpora with imbalanced temporal representation (overrepresentation of earlier periods) show accelerated degradation when encountering events from underrepresented time periods. This explains the observed performance drop around September 2021.
- **Core assumption**: Pre-training corpora have temporal biases that affect downstream performance on time-sensitive tasks.
- **Evidence anchors**:
  - [section 4.3]: "Interestingly, Figure 3 reveals a higher rate of performance decline around September 2021, which is the knowledge cutoff date of GPT-3.5, across all models, particularly for MC questions"
  - [section 4.3]: "We hypothesize that this trend arises because the period up to September 2021 may be overrepresented in many pre-training corpora"

## Foundational Learning

- **Concept**: Temporal generalization in language models
  - Why needed here: Understanding how LLMs perform on data from time periods beyond their training window is central to this work's contribution
  - Quick check question: What distinguishes temporal generalization from domain generalization in LLMs?

- **Concept**: Retrieval Augmented Generation (RAG)
  - Why needed here: RAG is used as a method to mitigate performance degradation by providing access to external knowledge sources
  - Quick check question: How does RAG fundamentally differ from fine-tuning in addressing knowledge gaps?

- **Concept**: Knowledge cutoff dates in LLMs
  - Why needed here: The study explicitly differentiates between pre-knowledge cutoff and post-knowledge cutoff performance, making understanding cutoff dates essential
  - Quick check question: What factors determine an LLM's knowledge cutoff date?

## Architecture Onboarding

- **Component map**: News article corpus -> LLM-based question generation -> Filtering -> Evaluation framework -> Performance tracking
- **Critical path**: Daily article → Question generation → Filtering → Evaluation → Performance analysis
- **Design tradeoffs**:
  - LLM-based generation vs. human annotation: Automation enables daily updates but may introduce biases
  - Closed-book vs. open-book settings: Isolates model knowledge vs. tests retrieval capabilities
  - Question type balance: Different degradation patterns require separate analysis
- **Failure signatures**:
  - Rapid performance decline immediately after knowledge cutoff
  - Refusal to answer questions increases over time
  - Performance degradation persists even with gold articles
- **First 3 experiments**:
  1. Compare TF vs MC question degradation rates across all models
  2. Test different RAG cutoff dates to find optimal retrieval timing
  3. Evaluate the impact of model size on temporal generalization performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum frequency of model updates required to maintain forecasting accuracy within a specified threshold (e.g., <5% degradation per year)?
- Basis in paper: Explicit - The paper demonstrates that even with RAG access to recent articles, performance degradation persists, and asks whether continuous pre-training is needed
- Why unresolved: The paper shows that performance degrades over time but doesn't empirically test different frequencies of model updates to determine the optimal update cadence
- What evidence would resolve it: A controlled experiment varying update frequencies (monthly, quarterly, annually) while measuring forecasting accuracy degradation rates, identifying the point of diminishing returns

### Open Question 2
- Question: How does the quality of LLM-generated forecasting questions compare to human-written questions in terms of predictive validity and robustness to temporal generalization tests?
- Basis in paper: Explicit - The paper acknowledges that generated questions may contain biases from outdated LLMs and could be less reliable in the long run
- Why unresolved: While the paper conducts human evaluation of question quality, it doesn't compare the predictive validity of LLM-generated versus human-written forecasting questions
- What evidence would resolve it: A head-to-head comparison where both LLM-generated and human-written questions are used to evaluate the same models' temporal generalization, measuring which type better predicts actual future events

### Open Question 3
- Question: What specific aspects of model internal representations become outdated, and can targeted fine-tuning on recent temporal patterns restore forecasting performance without full continuous pre-training?
- Basis in paper: Explicit - The paper suggests that even with full access to relevant articles, performance drops due to outdated internal representations, and asks whether continuous pre-training is still needed
- Why unresolved: The paper identifies that internal representations are outdated but doesn't analyze which specific aspects (e.g., temporal reasoning, world knowledge integration, uncertainty calibration) are most affected
- What evidence would resolve it: A detailed analysis decomposing model performance on different temporal reasoning tasks, followed by targeted fine-tuning experiments on specific temporal patterns to measure recovery in forecasting accuracy

### Open Question 4
- Question: How does the performance degradation pattern differ between forecasting questions about short-term events (days/weeks) versus long-term events (months/years), and what does this reveal about model temporal reasoning capabilities?
- Basis in paper: Inferred - The paper observes general performance degradation but doesn't analyze how this varies with temporal distance between question resolution and model knowledge cutoff
- Why unresolved: The paper treats all forecasting questions uniformly without distinguishing between different temporal horizons of the events being predicted
- What evidence would resolve it: A stratified analysis grouping questions by temporal distance (e.g., <30 days, 30-90 days, 90-180 days, >180 days) and measuring differential degradation rates, revealing whether models handle near-term versus far-term forecasting differently

## Limitations
- The evaluation relies entirely on automatically generated question-answer pairs using GPT-4, which may introduce systematic biases in question difficulty and style
- Human evaluation of 200 randomly selected questions showed only 81.6% agreement with the LLM-generated labels, suggesting potential quality issues
- The study focuses primarily on English-language news from Common Crawl, limiting generalizability to other languages and domains

## Confidence
- **High confidence**: The observed performance degradation patterns are robust across multiple models and time periods
- **Medium confidence**: The mechanisms explaining why degradation occurs (outdated internal representations vs. missing factual knowledge) are plausible but not definitively proven
- **Medium confidence**: The temporal distribution hypothesis for the September 2021 performance drop is speculative and requires further validation

## Next Checks
1. Conduct human evaluation on a larger sample of questions (minimum 1,000) across all time periods to validate the quality of automatically generated questions and identify systematic biases
2. Test the degradation patterns with models trained on corpora with explicitly balanced temporal representation to confirm whether temporal distribution affects performance
3. Implement a longitudinal study tracking individual models over time rather than cross-sectional comparisons to better isolate the effects of temporal generalization from model architectural differences