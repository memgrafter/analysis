---
ver: rpa2
title: Linguistic Intelligence in Large Language Models for Telecommunications
arxiv_id: '2402.15818'
source_url: https://arxiv.org/abs/2402.15818
tags:
- llms
- tasks
- performance
- text
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper conducts a zero-shot evaluation of four prominent large
  language models (Llama-2, Falcon, Mistral, and Zephyr) on three key NLP tasks within
  the telecommunications domain: text classification, summarization, and question
  answering. The authors aim to assess the capabilities of these models without fine-tuning,
  using benchmark datasets (SPEC5G-Classification, SPEC5G-Summarization, and TeleQnA).'
---

# Linguistic Intelligence in Large Language Models for Telecommunications

## Quick Facts
- arXiv ID: 2402.15818
- Source URL: https://arxiv.org/abs/2402.15818
- Authors: Tasnim Ahmed; Nicola Piovesan; Antonio De Domenico; Salimur Choudhury
- Reference count: 12
- One-line primary result: Zero-shot evaluation shows Mistral and Zephyr outperform Llama-2 and Falcon on telecom NLP tasks, but all remain below fine-tuned model performance

## Executive Summary
This paper evaluates four large language models (LLMs) - Llama-2, Falcon, Mistral, and Zephyr - on three key natural language processing tasks within the telecommunications domain without any fine-tuning. Using benchmark datasets SPEC5G-Classification, SPEC5G-Summarization, and TeleQnA, the study finds that Mistral and Zephyr generally outperform the other models across all tasks. While zero-shot LLMs achieve performance levels approaching fine-tuned models in some cases, they still fall short of current state-of-the-art results. The authors identify key failure modes including inability to follow instructions and overly verbose responses, and call for further research on prompt optimization and fine-tuning techniques.

## Method Summary
The study conducts zero-shot evaluation of four 7B parameter LLMs on three NLP tasks using three benchmark datasets. The models were tested on text classification (SPEC5G-Classification with 2,401 samples), summarization (SPEC5G-Summarization with 713 samples), and question answering (TeleQnA with 10,000 samples). For each task, carefully designed prompts guided the models' responses without any parameter updates. Performance was measured using accuracy for classification, Rouge scores for summarization, and accuracy/Rouge/Bleu scores for question answering. The evaluation pipeline involved loading pretrained models, generating prompts, running inference, collecting responses, and computing metrics using standard libraries.

## Key Results
- Zephyr achieved 57.93% accuracy on classification while Mistral reached 60.93% on multiple-choice question answering, approaching GPT-3.5 performance
- Zero-shot LLM performance remains below state-of-the-art fine-tuned models across all tasks
- Mistral and Zephyr consistently outperformed Llama-2 and Falcon, with the latter two struggling particularly with instruction adherence
- Error analysis revealed issues including failure to follow prompts, overly long responses, and random token generation for multiple-choice questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot LLMs can achieve performance comparable to fine-tuned models due to extensive pretraining on diverse corpora.
- Mechanism: Pretraining on large text corpora provides LLMs with broad language understanding, enabling them to perform tasks without task-specific fine-tuning.
- Core assumption: The pretraining corpus contains sufficient domain-relevant information to enable reasonable performance on specialized tasks.
- Evidence anchors:
  - [abstract] "Our evaluation reveals that zero-shot LLMs can achieve performance levels comparable to the current state-of-the-art fine-tuned models. This indicates that pretraining on extensive text corpora equips LLMs with a degree of specialization, even within the telecommunications domain."
  - [section] "We also observe that no single LLM consistently outperforms others, and the performance of different LLMs can fluctuate."
- Break condition: If the domain-specific terminology or concepts are not well-represented in the pretraining corpus, performance will degrade significantly.

### Mechanism 2
- Claim: Mistral and Zephyr outperform other LLMs due to their architectural advantages and fine-tuning.
- Mechanism: Mistral's advanced features like Grouped-Query Attention and sliding window attention improve efficiency and performance. Zephyr's fine-tuning on UltraChat dataset enhances its alignment with human preferences.
- Core assumption: The architectural improvements and fine-tuning strategies effectively translate to better performance on telecommunications tasks.
- Evidence anchors:
  - [abstract] "Mistral and Zephyr generally outperform Llama-2 and Falcon across the tasks. For example, Zephyr achieved 57.93% accuracy on classification, while Mistral reached 60.93% on multiple-choice question answering, close to much larger models like GPT-3.5."
  - [section] "Zephyr's performance closely follows that of Mistral. However, the performances of Llama-2 and Falcon were found to be subpar."
- Break condition: If the telecommunications domain requires specific knowledge not covered by the fine-tuning dataset or architectural improvements do not generalize well.

### Mechanism 3
- Claim: LLMs struggle with tasks requiring strict adherence to instructions due to their generative nature.
- Mechanism: LLMs generate text based on patterns learned during pretraining, which can lead to deviations from specific instructions, especially in zero-shot settings.
- Core assumption: The LLMs' tendency to generate coherent text overrides strict instruction following in zero-shot scenarios.
- Evidence anchors:
  - [section] "Upon manual inspection of the responses generated by these LLMs, it was found that the decline in performance of Falcon and Llama-2 was primarily due to their inability to adhere to prompt instructions."
  - [section] "In a traditional question-answering setting where the options are not provided in the prompt, we observed that the length of answers for the LLMs is generally longer than the ground truth."
- Break condition: If task-specific fine-tuning or prompt engineering techniques are employed to improve instruction adherence.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The study evaluates LLMs' performance without any task-specific fine-tuning, relying solely on their pretraining.
  - Quick check question: What is the difference between zero-shot, few-shot, and fine-tuned learning in the context of LLMs?

- Concept: Natural Language Processing (NLP) tasks
  - Why needed here: The study focuses on three key NLP tasks (text classification, summarization, and question answering) within the telecommunications domain.
  - Quick check question: How do text classification, summarization, and question answering differ in terms of input and output formats?

- Concept: Prompt engineering
  - Why needed here: The study relies on carefully crafted prompts to guide the LLMs' responses in the zero-shot setting.
  - Quick check question: What are the key elements of an effective prompt for guiding LLM responses in zero-shot learning?

## Architecture Onboarding

- Component map: Pretrained LLM -> Prompt generator -> Dataset -> Evaluation metrics -> Error analysis module
- Critical path:
  1. Load pretrained LLM weights
  2. Generate prompts based on task and sample
  3. Feed prompts to LLM for inference
  4. Collect and format LLM responses
  5. Evaluate responses using appropriate metrics
  6. Perform error analysis on failure cases
- Design tradeoffs:
  - Model size vs. performance: Larger models like GPT-3.5/4 outperform smaller models but require more resources
  - Zero-shot vs. fine-tuned: Zero-shot is faster and more generalizable but may underperform on specialized tasks
  - Automated vs. human evaluation: Automated metrics are faster but may miss nuances that human evaluation catches
- Failure signatures:
  - Inability to follow instructions (generating extra text or incorrect formats)
  - Overly long responses for summarization tasks
  - Random token generation instead of correct answers for MCQs
  - Poor performance on domain-specific terminology or concepts
- First 3 experiments:
  1. Test each LLM on a small subset of the SPEC5G-Classification dataset to identify instruction-following issues
  2. Compare the summarization performance of Llama-2 with other models on a few samples to understand length and repetition issues
  3. Evaluate all models on a small set of TeleQnA questions in both MCQ and traditional settings to identify response format problems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt optimization techniques affect the performance of LLMs in telecommunications-specific NLP tasks?
- Basis in paper: [explicit] The authors mention that future research should explore prompt optimization and efficient fine-tuning techniques.
- Why unresolved: The paper only uses basic prompts without optimization, and does not explore how different prompt engineering approaches might improve results.
- What evidence would resolve it: Systematic comparison of multiple prompt optimization techniques (chain-of-thought, few-shot examples, instruction tuning) across the same tasks and datasets.

### Open Question 2
- Question: What is the relative performance of larger LLMs (GPT-3.5, GPT-4) compared to the 7B parameter models studied in this work for telecommunications tasks?
- Basis in paper: [explicit] The authors explicitly note they excluded larger LLMs and state that "larger models such as GPT-3.5 or GPT-4 significantly surpass these smaller models."
- Why unresolved: The study intentionally focused on resource-constrained models and did not evaluate larger models despite acknowledging their superior performance.
- What evidence would resolve it: Direct performance comparison of larger LLMs against the 7B models on the same benchmark tasks using identical evaluation protocols.

### Open Question 3
- Question: How do human evaluations compare to automated metrics (Rouge, Bleu) for assessing LLM-generated responses in telecommunications question-answering tasks?
- Basis in paper: [explicit] The authors note that "for tasks like summarization or question-answering, human evaluation proves to be more effective than automated evaluation using Rouge or Bleu scores."
- Why unresolved: The study relies solely on automated metrics and explicitly states human evaluation is needed but was not incorporated.
- What evidence would resolve it: Side-by-side human evaluation study where experts rate LLM responses on the same question-answering tasks, compared against automated metric scores.

## Limitations
- Zero-shot performance remains significantly below fine-tuned models across all tasks, with classification accuracy reaching only ~58%
- LLM performance can fluctuate significantly with no single model consistently outperforming others across all tasks
- Results may be limited by prompt quality and lack of task-specific fine-tuning, suggesting potential for improvement with optimized prompts or domain adaptation

## Confidence

- **High confidence**: LLMs can perform zero-shot telecom NLP tasks without fine-tuning; Mistral and Zephyr outperform Llama-2 and Falcon; zero-shot performance remains below fine-tuned models
- **Medium confidence**: Pretraining on extensive corpora provides sufficient domain knowledge for telecom tasks; architectural features like GQA and sliding window attention contribute to Mistral's performance
- **Low confidence**: The extent to which prompt optimization could bridge the performance gap between zero-shot and fine-tuned models; whether current results represent the true capability ceiling for zero-shot telecom applications

## Next Checks

1. **Prompt optimization study**: Systematically test different prompt templates and instructions for each task to quantify the impact of prompt engineering on performance
2. **Cross-dataset generalization**: Evaluate the same models on alternative telecom datasets (beyond SPEC5G and TeleQnA) to assess robustness and generalizability
3. **Hybrid approach validation**: Test whether combining zero-shot inference with lightweight parameter-efficient fine-tuning (like LoRA) achieves better performance than either pure zero-shot or full fine-tuning approaches