---
ver: rpa2
title: 'Logic Distillation: Learning from Code Function by Function for Decision-making
  Tasks'
arxiv_id: '2407.19405'
source_url: https://arxiv.org/abs/2407.19405
tags:
- s-llms
- l-llms
- decision-making
- function
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Logic Distillation (LD), a novel framework to
  improve smaller language models' (S-LLMs) decision-making capabilities by learning
  the logical reasoning process of larger language models (L-LLMs) function by function.
  Instead of having S-LLMs mimic L-LLMs' outputs (as in Knowledge Distillation), LD
  decomposes decision-making logic into discrete functions instantiated by L-LLMs
  and creates a function base with usage examples.
---

# Logic Distillation: Learning from Code Function by Function for Decision-making Tasks

## Quick Facts
- arXiv ID: 2407.19405
- Source URL: https://arxiv.org/abs/2407.19405
- Reference count: 11
- S-LLMs with Logic Distillation achieve 100% success rate in pursuit games, outperforming traditional KD methods (88-89%)

## Executive Summary
Logic Distillation (LD) is a novel framework that improves smaller language models' (S-LLMs) decision-making capabilities by learning the logical reasoning process of larger language models (L-LLMs) function by function. Instead of having S-LLMs mimic L-LLMs' outputs as in traditional Knowledge Distillation, LD decomposes decision-making logic into discrete functions instantiated by L-LLMs and creates a function base with usage examples. S-LLMs are then fine-tuned to select and invoke relevant functions based on instructions and current states, transforming generation into selection for better stability. Experiments in pursuit games and 21-point card games show that S-LLMs with LD achieve success rates of 100% and performance comparable to or better than L-LLMs, while traditional KD methods only achieve 88-89% success rates.

## Method Summary
Logic Distillation decomposes complex decision-making tasks into discrete functions using L-LLMs, creating a function base with user manuals. A retriever identifies the top-K relevant functions for the current state and instructions, and S-LLMs are fine-tuned to select and invoke these functions rather than generating responses directly. This transforms generation into selection, reducing output entropy and improving stability. The framework demonstrates superior ability to handle emergencies by adding new rules to the function library without retraining, enabling better generalization than traditional KD approaches that simply mimic L-LLM outputs.

## Key Results
- S-LLMs with LD achieve 100% success rate in pursuit games, compared to 88-89% for traditional KD methods
- LD achieves performance comparable to or better than L-LLMs while using smaller models
- The framework shows superior emergency handling capabilities by adding new rules to the function library without retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing decision-making into discrete functions reduces cognitive load on S-LLMs and enables more reliable reasoning.
- Mechanism: L-LLMs break complex decision tasks into smaller, manageable functions (e.g., distance calculation, valid move filtering). Each function encapsulates a single logical step. S-LLMs then invoke these pre-defined functions instead of generating reasoning from scratch.
- Core assumption: Complex reasoning can be reliably decomposed into discrete, reusable logical functions without losing task fidelity.
- Evidence anchors: [abstract] "LD decomposes decision-making logic into discrete functions instantiated by L-LLMs and creates a function base with usage examples."

### Mechanism 2
- Claim: Switching from generation to selection improves output stability by constraining the output space.
- Mechanism: Instead of generating tokens from the full vocabulary, S-LLMs select from a smaller set of candidate functions provided by a retriever. This reduces entropy and limits the probability of producing invalid or out-of-distribution outputs.
- Core assumption: The retriever can consistently provide relevant candidate functions, and the function set covers most needed reasoning steps.
- Evidence anchors: [section] "S-LLMs will select and invoke functions based on instructions and current states, transforming generation into selection for better stability."

### Mechanism 3
- Claim: Learning the logic of L-LLMs (not just outputs) enables S-LLMs to generalize better to unseen scenarios.
- Mechanism: L-LLMs generate function definitions and usage examples. S-LLMs learn the underlying logic by studying these examples and the corresponding usage manual, not by memorizing L-LLM outputs.
- Core assumption: Function usage patterns capture the essence of L-LLM decision logic, allowing S-LLMs to generalize without overfitting to specific outputs.
- Evidence anchors: [abstract] "Instead of having S-LLMs mimic L-LLMs' outputs (as in Knowledge Distillation), LD decomposes decision-making logic into discrete functions instantiated by L-LLMs..."

## Foundational Learning

- Concept: Function decomposition and modular reasoning
  - Why needed here: Complex decision-making tasks are broken into smaller, reusable logical units to simplify reasoning and improve reliability.
  - Quick check question: If you had to split a pathfinding task into functions, what core functions would you define?

- Concept: Retrieval-augmented generation (RAG) for function selection
  - Why needed here: The retriever identifies the most relevant functions for the current state, constraining the S-LLM's choices and improving stability.
  - Quick check question: How does retrieving candidate functions differ from generating tokens directly?

- Concept: Entropy reduction through constrained output spaces
  - Why needed here: By reducing the set of possible outputs (from tokens to functions), the model's prediction uncertainty is lowered, leading to more stable results.
  - Quick check question: Why would selecting from 10 functions have lower entropy than generating from a vocabulary of 50,000 tokens?

## Architecture Onboarding

- Component map: L-LLM -> Function base creation -> Retriever -> S-LLM function selection -> Task completion
- Critical path: 1) L-LLM generates function base from instructions, 2) Retriever selects candidate functions for current state, 3) S-LLM chooses and executes a function, 4) State is updated; loop continues until task completion
- Design tradeoffs:
  - Function granularity: Too coarse → insufficient flexibility; too fine → too many functions, hard to manage
  - Retriever quality: Poor retrieval → irrelevant functions → wrong decisions
  - S-LLM size: Too small → cannot learn function usage; too large → defeats purpose of using S-LLMs
- Failure signatures:
  - Frequent function retrieval errors → retriever or function base issues
  - S-LLM repeatedly picks wrong functions → fine-tuning or architecture problems
  - Model fails to handle emergencies → function generation or library update mechanism broken
- First 3 experiments:
  1. Test retriever with static state to verify top-K selection accuracy
  2. Run S-LLM function selection in isolation to check if it can choose correct functions
  3. Validate end-to-end logic distillation on a simple rule-based task (e.g., number guessing game)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Logic Distillation perform when applied to multimodal tasks involving both text and images?
- Basis in paper: [inferred] The paper focuses on text-based decision-making tasks and mentions "vision anomaly detection" only in the acknowledgments, suggesting potential for multimodal applications.
- Why unresolved: The current experiments are limited to text-based games and decision-making tasks, without exploring multimodal capabilities.
- What evidence would resolve it: Experiments demonstrating LD's effectiveness on multimodal tasks, such as visual question answering or multimodal decision-making games.

### Open Question 2
- Question: What is the scalability limit of the function base in Logic Distillation, and how does it impact performance as the number of functions increases?
- Basis in paper: [explicit] The paper mentions two retrieval solutions based on function base scale and notes that S-LLMs only need to remember function usage rather than massive outputs.
- Why unresolved: The paper doesn't explore how performance changes with very large function bases or provide theoretical limits on scalability.
- What evidence would resolve it: Systematic experiments varying function base sizes and analyzing performance degradation points, along with computational complexity analysis.

### Open Question 3
- Question: How does Logic Distillation handle tasks that require continuous adaptation versus discrete function-based decisions?
- Basis in paper: [inferred] The paper mentions "continuous decision-making tasks" in the abstract but focuses on discrete function-based decision-making.
- Why unresolved: The framework is designed around discrete functions, but real-world decision-making often involves continuous adjustments.
- What evidence would resolve it: Experiments comparing LD performance on tasks requiring continuous adaptation versus discrete decisions, and potential modifications to handle continuous spaces.

### Open Question 4
- Question: What are the long-term generalization capabilities of S-LLMs trained with Logic Distillation when encountering entirely new types of decision-making tasks?
- Basis in paper: [explicit] The paper demonstrates that S-LLMs with LD can handle emergencies by adding new rules without retraining, suggesting generalization potential.
- Why unresolved: The experiments focus on specific game scenarios and don't test the framework's ability to generalize to completely novel task domains.
- What evidence would resolve it: Transfer learning experiments where S-LLMs trained with LD on one domain are tested on entirely different decision-making tasks, measuring performance retention.

## Limitations
- Dependency on quality and completeness of L-LLM function decomposition
- Assumption that decision-making tasks can be adequately represented as sequences of function invocations
- Experiments limited to two specific games with well-defined rules, raising questions about generalization
- No runtime comparison with baseline approaches or computational overhead analysis

## Confidence

**High confidence**: The claim that transforming generation into selection reduces output instability is well-supported by the entropy reduction argument (K tokens vs M tokens). The experimental results showing LD outperforming traditional KD on success rates (100% vs 88-89%) provide strong empirical evidence.

**Medium confidence**: The assertion that learning L-LLM logic rather than outputs enables better generalization to unseen scenarios is plausible but under-explored. While the paper demonstrates emergency handling by adding new functions, it doesn't test whether S-LLMs can generalize to completely novel reasoning patterns not captured in the function library.

**Low confidence**: The scalability claims for real-world applications remain largely speculative. The paper provides limited evidence about how LD would perform on tasks with significantly larger state spaces or more complex decision hierarchies than the tested games.

## Next Checks

1. **Function decomposition robustness test**: Systematically evaluate whether different L-LLMs produce equivalent function bases for the same task, and measure the impact on S-LLM performance when using suboptimal function decompositions.

2. **Retrieval quality assessment**: Conduct ablation studies removing the retriever component to determine the marginal benefit of function selection versus direct generation, and measure the precision@k of the retriever in returning relevant functions.

3. **Cross-task generalization experiment**: Apply the trained S-LLM models to a different decision-making task (e.g., a pathfinding problem) without retraining to assess whether function-based reasoning transfers across domains, or requires domain-specific function libraries.