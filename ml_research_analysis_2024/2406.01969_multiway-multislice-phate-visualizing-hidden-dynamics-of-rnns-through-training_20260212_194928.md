---
ver: rpa2
title: 'Multiway Multislice PHATE: Visualizing Hidden Dynamics of RNNs through Training'
arxiv_id: '2406.01969'
source_url: https://arxiv.org/abs/2406.01969
tags:
- hidden
- training
- units
- epochs
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Multiway Multislice PHATE (MM-PHATE) is a novel visualization
  framework for understanding RNNs'' hidden dynamics during training. It extends the
  previous Multislice PHATE method to capture dynamics across three dimensions: time,
  training epochs, and hidden units.'
---

# Multiway Multislice PHATE: Visualizing Hidden Dynamics of RNNs through Training

## Quick Facts
- arXiv ID: 2406.01969
- Source URL: https://arxiv.org/abs/2406.01969
- Authors: Jiancheng Xie; Lou C. Kohler Voinov; Noga Mudrik; Gal Mishne; Adam Charles
- Reference count: 40
- Multiway Multislice PHATE (MM-PHATE) is a novel visualization framework for understanding RNNs' hidden dynamics during training

## Executive Summary
Multiway Multislice PHATE (MM-PHATE) is a novel visualization framework for understanding RNNs' hidden dynamics during training. It extends the previous Multislice PHATE method to capture dynamics across three dimensions: time, training epochs, and hidden units. MM-PHATE uses a structured graph kernel to model the community structure and temporal relationships between hidden units. Applied to LSTM and GRU networks on Area2Bump and HAR datasets, MM-PHATE uniquely preserves hidden representation community structure and identifies information processing and compression phases during training. It outperforms traditional dimensionality reduction methods like PCA, t-SNE, and Isomap in capturing the evolution of hidden states and their correlations.

## Method Summary
MM-PHATE is a visualization framework that extends Multislice PHATE to capture dynamics across three dimensions: time, training epochs, and hidden units. It uses a structured graph kernel to model the community structure and temporal relationships between hidden units. The method takes as input the hidden states of RNNs at each time step and training epoch, constructs a graph representation of these states, and applies PHATE (Potential of Heat-diffusion for Affinity-based Trajectory Embedding) to generate low-dimensional embeddings that preserve both local and global structure. The resulting visualizations reveal the evolution of hidden representations during training, including the formation of communities and the emergence of information processing and compression phases.

## Key Results
- MM-PHATE uniquely preserves hidden representation community structure during training
- Identifies distinct information processing and compression phases in RNNs
- Outperforms traditional dimensionality reduction methods (PCA, t-SNE, Isomap) in capturing hidden state evolution

## Why This Works (Mechanism)
MM-PHATE works by constructing a structured graph kernel that captures the temporal and community relationships between hidden units across training epochs. The graph kernel incorporates both local neighborhood structure and global manifold structure, allowing the method to preserve community formations while maintaining temporal continuity. By extending PHATE's diffusion geometry approach to the multiway setting, MM-PHATE can reveal the intrinsic low-dimensional structure of RNN hidden states that emerges during training. The entropy analysis component further reveals how different hidden units contribute to learning at various stages, with some units showing higher sensitivity to input changes and contributing to complex dependency learning.

## Foundational Learning
- **PHATE (Potential of Heat-diffusion for Affinity-based Trajectory Embedding)**: A nonlinear dimensionality reduction technique that preserves both local and global structure; needed to maintain trajectory information in RNN hidden states
- **Graph kernels**: Mathematical constructs for measuring similarity between graphs; needed to capture community structure in hidden representations
- **Diffusion maps**: A dimensionality reduction technique based on random walks; needed for the underlying geometry preservation in PHATE
- **Multislice networks**: Graph representations that capture temporal evolution; needed to model hidden state changes across training epochs
- **Community detection**: Algorithms for identifying clusters in network structures; needed to reveal functional groups of hidden units
- **Information bottleneck principle**: Theoretical framework for understanding compression in neural networks; needed to interpret the observed compression phases

## Architecture Onboarding
- **Component map**: Input data -> Graph construction (structured kernel) -> PHATE embedding -> Visualization
- **Critical path**: Hidden state extraction → Graph kernel construction → Diffusion geometry computation → Low-dimensional embedding
- **Design tradeoffs**: Computational complexity vs. visualization quality; local vs. global structure preservation; temporal resolution vs. community detection
- **Failure signatures**: Loss of community structure in visualization; temporal discontinuities; inability to distinguish training phases
- **First experiments**:
  1. Apply MM-PHATE to a simple LSTM on synthetic sequential data to verify basic functionality
  2. Compare visualizations across different RNN architectures (LSTM vs. GRU) on the same dataset
  3. Test sensitivity to graph kernel hyperparameters by varying neighborhood sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Computational complexity may not scale efficiently to larger networks with thousands of hidden units
- Effectiveness demonstrated primarily on relatively small datasets (Area2Bump and HAR)
- Limited statistical validation of identified information processing phases

## Confidence
- Visualization effectiveness and community structure preservation: Medium
- Identification of information processing and compression phases: Low

## Next Checks
1. Apply MM-PHATE to larger-scale RNN architectures (e.g., language models with 100+ hidden units) to assess computational scalability and visualization clarity
2. Conduct quantitative benchmark comparisons using established metrics for visualization quality (trustworthiness, continuity, and precision-recall curves) against traditional methods
3. Validate the identified information processing phases by correlating MM-PHATE visualizations with external performance metrics (e.g., validation loss, gradient norms) across multiple training runs and architectures