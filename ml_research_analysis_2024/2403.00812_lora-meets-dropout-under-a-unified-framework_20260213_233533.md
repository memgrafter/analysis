---
ver: rpa2
title: LoRA Meets Dropout under a Unified Framework
arxiv_id: '2403.00812'
source_url: https://arxiv.org/abs/2403.00812
tags:
- dropout
- hiddenkey
- methods
- lora
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates a potential contradiction between the limited
  trainable parameters in LoRA-based parameter-efficient fine-tuning and the effectiveness
  of dropout methods, which are typically designed for full fine-tuning scenarios
  with all parameters updated. The authors first confirm that LoRA is also prone to
  overfitting.
---

# LoRA Meets Dropout under a Unified Framework

## Quick Facts
- arXiv ID: 2403.00812
- Source URL: https://arxiv.org/abs/2403.00812
- Authors: Sheng Wang; Liheng Chen; Jiyue Jiang; Boyang Xue; Lingpeng Kong; Chuan Wu
- Reference count: 23
- One-line primary result: HiddenKey consistently outperforms alternatives, highlighting its effectiveness as a preferred approach for high-performance and parameter-efficient fine-tuning of large language models.

## Executive Summary
This work investigates the effectiveness of dropout methods in LoRA-based parameter-efficient fine-tuning of large language models. The authors confirm that LoRA is prone to overfitting despite its limited trainable parameters, and analyze three transformer-specific dropout methods (DropKey, DropAttention, HiddenCut) under a unified framework. Based on this analysis, they propose HiddenKey, a novel dropout method that combines the strengths of different approaches and consistently outperforms baselines and existing dropout methods across multiple models and tasks.

## Method Summary
The paper proposes a unified framework to analyze dropout methods in LoRA-based fine-tuning, identifying three key dimensions: dropping position, structural pattern, and compensation measure. The authors mathematically and empirically analyze existing dropout methods, revealing that DropKey and DropAttention have equivalent forward passes but differ in back-propagation due to gradient noise. Based on this framework, they derive HiddenKey, which combines optimal aspects of existing methods. The method is evaluated through extensive experiments on multiple transformer models (RoBERTa-large, GPT2-Medium, LLaMA2-7B) and various NLU/NLG datasets with LoRA rank 8 and scalar 16.

## Key Results
- LoRA-based fine-tuning suffers from overfitting despite limited trainable parameters
- DropKey and DropAttention share equivalent forward passes but differ in back-propagation due to gradient noise
- HiddenKey consistently outperforms DropKey, DropAttention, and HiddenCut across multiple tasks and datasets
- The unified framework reveals dropping position preferences and structural patterns for LoRA scenarios

## Why This Works (Mechanism)

### Mechanism 1
LoRA-based parameter-efficient fine-tuning still suffers from overfitting due to excessive model capacity despite limited trainable parameters. The frozen base model parameters coupled with the expressive LoRA low-rank adaptation matrices create a high-capacity solution space that can overfit the training data. This occurs even with only ~0.2% of parameters being trained, as the interaction between the low-rank matrices and the large frozen model creates sufficient representational power to memorize training data.

### Mechanism 2
DropKey and DropAttention have mathematically equivalent forward passes but differ in back-propagation due to the NoGrad() operator. During forward propagation, both methods produce identical attention weights through different computational paths. However, DropAttention's NoGrad() operator introduces gradient noise during back-propagation, while DropKey adaptively scales gradients based on the dropped attention logit values, affecting training stability and performance.

### Mechanism 3
The unified framework reveals that different dropping positions and structural patterns have varying effectiveness in LoRA scenarios compared to full fine-tuning. The framework identifies three key dimensions (dropping position, structural pattern, compensation measure) that determine dropout method effectiveness. In LoRA scenarios, the limited trainable parameters create different preferences for these dimensions compared to full fine-tuning, where the frozen weight matrices create a different optimization landscape.

## Foundational Learning

- **Concept**: Low-Rank Adaptation (LoRA) and its mathematical formulation
  - Why needed here: Understanding how LoRA decomposes weight updates into low-rank matrices AB is crucial for grasping why dropout methods interact differently with LoRA compared to full fine-tuning.
  - Quick check question: In LoRA, if the original weight matrix is W and the low-rank decomposition produces matrices A and B, what is the mathematical expression for the adapted weight during fine-tuning?

- **Concept**: Transformer attention mechanism and its components (queries, keys, values)
  - Why needed here: The paper analyzes dropout methods that operate on different components of the attention mechanism (logits, weights, representations), so understanding these components is essential.
  - Quick check question: In multi-head self-attention, what are the dimensions of the query, key, and value matrices, and how do they interact to produce attention weights?

- **Concept**: Dropout regularization and its variants (element-wise, column-wise, span-wise)
  - Why needed here: The paper introduces a unified framework for different dropout patterns and their effectiveness, requiring understanding of how different dropout strategies affect model training.
  - Quick check question: How does element-wise dropout differ from column-wise dropout in terms of which units are deactivated, and what are the implications for information flow?

## Architecture Onboarding

- **Component map**: Input -> Base model (frozen) -> Attention with dropout -> Feed-forward with HiddenCut -> LoRA adaptation -> Output layer
- **Critical path**: Input passes through base model layers, attention computation with dropout applied based on method, feed-forward with HiddenCut dropout, LoRA adaptation applied to frozen weights, output classification/regression, loss computation with regularization
- **Design tradeoffs**: LoRA rank vs. model capacity (higher rank increases capacity but also overfitting risk), dropout rate vs. training stability (higher rates provide more regularization but can destabilize training), dropping position selection (attention components vs. hidden representations have different impacts), compensation measure choice (KL divergence vs. JS consistency have different effects)
- **Failure signatures**: Training loss converges to near zero while validation performance degrades (overfitting), sudden performance drops when increasing dropout rate beyond optimal threshold, no improvement over baseline despite adding dropout (wrong dropping position or pattern), gradient explosion or vanishing when using DropAttention with NoGrad()
- **First 3 experiments**: 1) Verify LoRA overfitting by training with varying ranks and plotting training vs. validation accuracy, 2) Compare dropout methods by implementing DropKey, DropAttention, and HiddenCut on same LoRA setup, 3) Test dropping positions by applying each dropout method to different positions and measuring improvement

## Open Questions the Paper Calls Out

### Open Question 1
Does the effectiveness of HiddenKey generalize to other PEFT methods beyond LoRA? The paper only tests HiddenKey on LoRA but mentions other PEFT methods like Prefix-tuning and Adapter in related work. This remains unresolved as the authors focus on LoRA specifically without comparing to other PEFT methods.

### Open Question 2
How does the performance of HiddenKey vary with different model architectures beyond transformers? The paper focuses on transformers and mentions the effectiveness of dropout in general neural networks in related work. The authors do not test HiddenKey on other model architectures like RNNs or CNNs.

### Open Question 3
Is there an optimal dropout rate for HiddenKey that generalizes across different tasks and datasets? The authors iterate over a range of dropout rates and find the best one for each method and dataset, but do not analyze the relationship between dropout rate and performance across tasks and datasets.

### Open Question 4
How does the computational cost of HiddenKey compare to other dropout methods and the baseline LoRA? The authors mention the potential longer training duration due to KL loss in HiddenKey, but do not provide a detailed comparison of computational costs or quantitative comparison of training time, memory usage across different tasks and datasets.

## Limitations

- The analysis relies heavily on theoretical derivations without extensive empirical validation of the unified framework's generality across different LoRA configurations
- The practical impact of gradient noise from the NoGrad() operator remains somewhat qualitative rather than quantified with specific metrics
- The claim that the unified framework can predict optimal dropping positions and structural patterns for any given LoRA configuration appears somewhat task-specific and may not generalize to all transformer architectures

## Confidence

**High Confidence**: The empirical confirmation that LoRA-based fine-tuning suffers from overfitting despite limited trainable parameters. Experimental results across multiple datasets and models consistently demonstrate this phenomenon.

**Medium Confidence**: The mathematical analysis showing DropKey and DropAttention have equivalent forward passes but diverge in back-propagation due to the NoGrad() operator. While the theoretical proof is sound, practical implications for training stability could benefit from more rigorous quantitative analysis.

**Low Confidence**: The claim that the unified framework can predict optimal dropping positions and structural patterns for any given LoRA configuration. The analysis appears somewhat task-specific and may not generalize to all transformer architectures or fine-tuning scenarios.

## Next Checks

1. **Gradient Noise Quantification**: Measure and compare the actual gradient variance introduced by DropAttention's NoGrad() operator versus DropKey's adaptive scaling across multiple training runs to provide concrete evidence for gradient noise affecting training stability.

2. **Ablation Study on HiddenKey Components**: Systematically remove or modify individual components of HiddenKey (dropping position, structural pattern, compensation measure) to isolate which design choices contribute most to its superior performance and validate the framework's claim about combining favorable aspects.

3. **Cross-Architecture Generalization Test**: Apply the unified framework and HiddenKey to non-standard transformer architectures (e.g., Swin Transformers for vision tasks or hybrid architectures) to verify whether the identified dropping position preferences and structural patterns generalize beyond the tested NLP models.