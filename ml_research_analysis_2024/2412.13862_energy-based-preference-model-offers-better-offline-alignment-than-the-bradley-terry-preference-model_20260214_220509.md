---
ver: rpa2
title: Energy-Based Preference Model Offers Better Offline Alignment than the Bradley-Terry
  Preference Model
arxiv_id: '2412.13862'
source_url: https://arxiv.org/abs/2412.13862
tags:
- loss
- preference
- reward
- weak
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a fundamental flaw in the Bradley-Terry
  preference model underlying DPO: it may have multiple maximum likelihood estimators,
  only one of which satisfies the required linear relationship with the true reward
  for RLHF optimization. To address this, the authors propose an energy-based model
  (EBM) called the Infinite Preference Model (IPM) that always has a unique MLE equivalent
  to the RLHF minimizer.'
---

# Energy-Based Preference Model Offers Better Offline Alignment than the Bradley-Terry Preference Model

## Quick Facts
- arXiv ID: 2412.13862
- Source URL: https://arxiv.org/abs/2412.13862
- Authors: Yuzhong Hong; Hanshan Zhang; Junwei Bao; Hongfei Jiang; Yang Song
- Reference count: 40
- Primary result: Energy Preference Alignment (EPA) consistently outperforms Direct Preference Optimization (DPO) on MT-Bench and AlpacaEval 2.0 benchmarks

## Executive Summary
This paper identifies a fundamental flaw in the Bradley-Terry preference model underlying Direct Preference Optimization (DPO): it may have multiple maximum likelihood estimators, only one of which satisfies the required linear relationship with the true reward for RLHF optimization. To address this, the authors propose an energy-based model called the Infinite Preference Model (IPM) that always has a unique MLE equivalent to the RLHF minimizer. They introduce a practical contrastive loss named Energy Preference Alignment (EPA) that approximates this MLE by contrasting positive samples with both strong and weak negatives. Empirically, EPA consistently outperforms DPO on open benchmarks including MT-Bench and AlpacaEval 2.0, with metrics showing 19.20/19.26% AlpacaEval win-rate versus DPO's 17.43/15.24%.

## Method Summary
The authors propose replacing the Bradley-Terry preference model with an energy-based Infinite Preference Model (IPM) that guarantees a unique maximum likelihood estimator. The key innovation is the Energy Preference Alignment (EPA) loss, which implements a contrastive objective that contrasts positive samples with both strong and weak negative samples. This approach addresses the theoretical flaw in DPO where the Bradley-Terry model can have multiple MLEs, only one of which satisfies the linear reward constraint needed for RLHF. The EPA loss is designed to approximate the IPM's MLE through this contrastive formulation, providing more stable training dynamics and better alignment between KL divergence and reward optimization compared to DPO.

## Key Results
- EPA achieves 19.20% win-rate on MT-Bench compared to DPO's 17.43%
- EPA achieves 19.26% win-rate on AlpacaEval 2.0 compared to DPO's 15.24%
- EPA demonstrates more stable training dynamics and better KL-reward balance than DPO
- The method shows consistent improvement across multiple preference alignment benchmarks

## Why This Works (Mechanism)
The Bradley-Terry preference model used in DPO can have multiple maximum likelihood estimators, creating ambiguity in the learned reward function. Only one of these estimators satisfies the linear relationship required for proper RLHF optimization. The energy-based IPM guarantees a unique MLE that inherently satisfies this constraint. EPA implements a contrastive loss that approximates this unique solution by carefully balancing the influence of strong and weak negative samples, effectively regularizing the optimization landscape to avoid the multiple-solution problem inherent in DPO.

## Foundational Learning
**Preference Learning**: Understanding how models learn from pairwise comparisons rather than absolute labels is fundamental to this work. The Bradley-Terry model is the standard approach, but its limitations motivate the energy-based alternative. Why needed: All alignment methods rely on preference data, so understanding preference modeling is essential. Quick check: Can you explain why pairwise comparisons are used instead of direct reward learning?

**Energy-Based Models**: EBMs define probability distributions through energy functions rather than explicit probabilistic models. This allows for flexible modeling of complex distributions and unique optimization properties. Why needed: The IPM uses EBM framework to guarantee unique MLE. Quick check: What distinguishes EBMs from standard probabilistic models?

**Contrastive Learning**: The EPA loss uses contrastive objectives to learn effective representations by pulling together similar samples and pushing apart dissimilar ones. Why needed: Contrastive objectives provide the mechanism for approximating the IPM's unique MLE. Quick check: How does contrastive learning differ from standard supervised learning?

**KL Divergence in Alignment**: KL divergence measures the difference between probability distributions and is central to both DPO and EPA for measuring preference deviation. Why needed: Understanding KL divergence is crucial for interpreting the alignment objective and training dynamics. Quick check: Why is KL divergence a natural measure for preference alignment?

## Architecture Onboarding

**Component Map**: Data -> Preference Pairs -> Energy Function -> EPA Loss -> Model Parameters -> Aligned Model

**Critical Path**: The critical path is the computation of the energy function and its gradients through the EPA loss. The energy function takes preference pairs as input and produces scalar energies that are then transformed into probabilities via softmax. The EPA loss computes the contrastive objective using these probabilities, and backpropagation flows through the energy function to update model parameters. This path must be numerically stable and efficient for large-scale training.

**Design Tradeoffs**: The choice between Bradley-Terry (DPO) and energy-based (EPA) modeling involves a fundamental tradeoff between simplicity and theoretical guarantees. Bradley-Terry is simpler and more interpretable but can have multiple MLEs, while the energy-based approach is more complex but guarantees uniqueness. The EPA implementation must balance the strength of negative samples - too strong and the model may become overly conservative, too weak and the contrastive signal may be insufficient. Temperature scaling in the energy function is another critical hyperparameter that affects the sharpness of the probability distribution.

**Failure Signatures**: Training instability or divergence can occur if the energy function produces extreme values that cause numerical overflow in the softmax computation. Poor performance relative to DPO might indicate that the negative sampling strategy is not effectively capturing the preference structure. If the model learns to assign similar energies to all samples regardless of preference, this suggests the contrastive signal is too weak or the temperature is poorly calibrated. Sudden drops in performance during training often indicate that strong negatives are overwhelming the positive examples.

**First Experiments**:
1. Compare EPA and DPO training curves on a small preference dataset to observe stability differences
2. Vary the temperature parameter in EPA to find the optimal setting for a given dataset
3. Test EPA with different negative sampling strategies (uniform vs. learned vs. margin-based) to evaluate their impact on performance

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical analysis relies on idealized assumptions that may not hold in practice, particularly regarding infinite data and perfect pairwise preference capture
- EPA's performance improvements could be influenced by implementation details and hyperparameter choices rather than fundamental superiority
- The empirical evaluation compares different training recipes with varying hyperparameters, making it difficult to isolate the effect of the loss function itself

## Confidence
High confidence: The mathematical derivation showing that Bradley-Terry preference models can have multiple MLEs that do not satisfy the linear reward constraint. This is a well-established result in preference learning theory.

Medium confidence: The empirical superiority of EPA over DPO on the tested benchmarks. While the results are consistent across multiple datasets, the performance differences could be influenced by implementation details and hyperparameter choices.

Medium confidence: The claim that EPA provides more stable training dynamics. The evidence for this is primarily qualitative observations of training curves rather than quantitative stability metrics.

## Next Checks
1. Conduct systematic ablation studies on EPA's key hyperparameters (temperature, positive-to-negative ratio, weak vs strong negative weighting) to isolate which components contribute most to performance improvements and to establish optimal configurations.

2. Test EPA on additional preference learning scenarios beyond the current RLHF setup, including different reward modeling architectures, varying preference dataset sizes, and non-language domains to assess generalizability.

3. Design controlled experiments that keep all training parameters identical between EPA and DPO except for the loss function itself, including matching learning rates, batch sizes, and training durations, to definitively attribute performance differences to the loss formulation.