---
ver: rpa2
title: Characterizing Dynamical Stability of Stochastic Gradient Descent in Overparameterized
  Learning
arxiv_id: '2407.20209'
source_url: https://arxiv.org/abs/2407.20209
tags:
- have
- such
- theorem
- will
- stochastic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper rigorously characterizes which global minima SGD can\
  \ find in overparameterized neural networks. The authors introduce a new notion\
  \ of linear stability via a characteristic Lyapunov exponent \u03BB(x) for SGD,\
  \ analogous to the classical stability measure \u03BC(x) for gradient descent."
---

# Characterizing Dynamical Stability of Stochastic Gradient Descent in Overparameterized Learning

## Quick Facts
- arXiv ID: 2407.20209
- Source URL: https://arxiv.org/abs/2407.20209
- Reference count: 28
- This paper rigorously characterizes which global minima SGD can find in overparameterized neural networks through a new notion of linear stability via Lyapunov exponents.

## Executive Summary
This paper establishes a rigorous theoretical framework for understanding which global minima stochastic gradient descent (SGD) can find in overparameterized neural networks. The authors introduce a characteristic Lyapunov exponent λ(x*) that determines whether SGD will converge to a given global minimum x* when initialized nearby. They prove that convergence occurs if and only if λ(x*) < 0, analogous to the classical stability measure μ(x*) < 0 for gradient descent. The work connects these stability measures to the neural tangent kernel, providing theoretical insight into SGD's implicit bias and generalization properties in deep learning.

## Method Summary
The authors employ random dynamical systems theory to analyze SGD's behavior in overparameterized networks. They define a linear stability measure λ(x*) for SGD and prove that a global minimum x* is stable under SGD if and only if λ(x*) < 0, meaning SGD converges to x* when initialized nearby. For gradient descent, they show stability is determined by μ(x*) < 0. The proofs establish a connection between the Lyapunov exponent and the neural tangent kernel, creating a bridge between random dynamical systems theory and practical deep learning applications.

## Key Results
- A global minimum x* is stable under SGD if and only if the Lyapunov exponent λ(x*) < 0
- SGD converges to x* when initialized nearby if λ(x*) < 0
- The Lyapunov exponent λ(x*) is connected to the neural tangent kernel, providing theoretical insight into SGD's implicit bias

## Why This Works (Mechanism)
The stability of SGD in overparameterized networks is determined by the characteristic Lyapunov exponent λ(x*), which captures the exponential growth rate of perturbations around a global minimum x*. When λ(x*) < 0, small perturbations decay exponentially, ensuring convergence to x*. The connection to the neural tangent kernel provides a concrete mathematical object that characterizes this stability, bridging abstract dynamical systems theory with practical neural network behavior.

## Foundational Learning

**Random Dynamical Systems Theory**: Needed to analyze SGD's stochastic behavior over time. Quick check: Can describe how random perturbations affect system stability.

**Lyapunov Exponents**: Measure the exponential growth rate of perturbations in dynamical systems. Quick check: Can calculate λ for simple systems and interpret its sign.

**Neural Tangent Kernel**: Characterizes the training dynamics of infinitely wide neural networks. Quick check: Can explain how NTK relates to gradient flow in overparameterized networks.

**Implicit Bias of SGD**: Understanding why SGD prefers certain solutions over others. Quick check: Can describe how optimization dynamics influence which minima SGD finds.

## Architecture Onboarding

Component map: SGD initialization -> Loss landscape -> Global minimum x* -> Lyapunov exponent λ(x*) -> Convergence behavior

Critical path: The key sequence is (1) define loss landscape, (2) identify global minimum x*, (3) compute Lyapunov exponent λ(x*), (4) determine stability based on sign of λ(x*).

Design tradeoffs: The framework assumes nearby initialization and specific technical conditions for random dynamical systems theory, which may limit applicability to practical SGD variants with momentum, weight decay, or batch normalization.

Failure signatures: When λ(x*) ≥ 0, SGD will not converge to x* despite it being a global minimum. The framework may fail to capture stability for non-standard SGD variants or networks that don't satisfy overparameterization assumptions.

First experiments:
1. Compute λ(x*) for a simple convex problem and verify convergence behavior matches predictions
2. Test stability predictions on a small overparameterized linear network with synthetic data
3. Validate the NTK connection by comparing computed Lyapunov exponents with NTK-based predictions

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis assumes a specific technical framework for random dynamical systems that may not capture all practical SGD variants
- The connection between Lyapunov exponent and neural tangent kernel requires empirical validation on realistic network architectures
- Results focus on local convergence properties and don't address global optimization dynamics or full generalization behavior

## Confidence
- High confidence in mathematical framework for defining and computing stability measures λ(x*) and μ(x*)
- Medium confidence in theoretical connection to neural tangent kernels
- Medium confidence in practical implications for SGD implicit bias and generalization
- Low confidence in direct applicability to all practical SGD variants and network architectures

## Next Checks
1. Empirically verify predicted stability regions on standard benchmark architectures (MLP, CNN, Transformer) across different dataset regimes
2. Test stability predictions when SGD uses practical modifications like momentum, weight decay, or batch normalization
3. Compare predicted convergence behavior with actual SGD trajectories on real overparameterized networks, measuring both stability and generalization performance