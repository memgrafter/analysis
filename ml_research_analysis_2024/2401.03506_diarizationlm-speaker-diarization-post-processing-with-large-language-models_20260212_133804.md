---
ver: rpa2
title: 'DiarizationLM: Speaker Diarization Post-Processing with Large Language Models'
arxiv_id: '2401.03506'
source_url: https://arxiv.org/abs/2401.03506
tags:
- speaker
- diarization
- diarizationlm
- word
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DiarizationLM, a framework that uses large
  language models (LLMs) to post-process speaker diarization outputs to reduce word
  diarization errors. The method involves representing ASR transcripts and speaker
  diarization results in a compact textual format, which is used as a prompt to an
  optionally fine-tuned LLM.
---

# DiarizationLM: Speaker Diarization Post-Processing with Large Language Models

## Quick Facts
- **arXiv ID:** 2401.03506
- **Source URL:** https://arxiv.org/abs/2401.03506
- **Reference count:** 40
- **Primary result:** Fine-tuned PaLM 2-S reduces WDER by 55.5% on Fisher and 44.9% on Callhome datasets

## Executive Summary
This paper introduces DiarizationLM, a framework that uses large language models (LLMs) to post-process speaker diarization outputs, significantly reducing word diarization errors. The approach involves creating compact textual prompts from ASR transcripts and speaker diarization results, which are then processed by an optionally fine-tuned LLM to produce corrected speaker labels. The framework demonstrates substantial improvements in WDER on both Fisher and Callhome datasets, with a finetuned PaLM 2-S model achieving 55.5% and 44.9% relative reductions respectively.

## Method Summary
The DiarizationLM framework orchestrates ASR and speaker diarization outputs, converting them into a compact textual representation used as prompts for an LLM. The LLM, optionally fine-tuned on hyp2ora data, generates corrected speaker labels which are then parsed and transferred back to the original ASR transcript using the Transcript-Preserving Speaker Transfer (TPST) algorithm. This process preserves the original transcript while improving speaker attribution accuracy.

## Key Results
- Fine-tuned PaLM 2-S model reduces WDER by 55.5% on Fisher dataset
- Fine-tuned PaLM 2-S model reduces WDER by 44.9% on Callhome dataset
- Framework can be applied to existing ASR and diarization systems without retraining

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs use semantic context to fix speaker attribution errors caused by timing misalignment.
- **Mechanism:** The LLM leverages language coherence to reassign words to speakers, correcting timing-based errors from separate ASR and diarization models.
- **Core assumption:** Semantic coherence is a strong signal for correct speaker attribution.
- **Evidence:** Abstract mentions improving diarized transcript readability and reducing WDER.
- **Break condition:** If LLM lacks semantic understanding or transcript is ambiguous, corrections may fail.

### Mechanism 2
- **Claim:** Fine-tuning LLM on hypothesis-to-oracle data dramatically improves correction ability.
- **Mechanism:** Training on paired prompts (incorrect labels) and completions (corrected labels) teaches the LLM to map error patterns to correct forms.
- **Core assumption:** LLM can generalize from finetuned examples to unseen error patterns.
- **Evidence:** Abstract shows 55.5% WDER reduction on Fisher after finetuning.
- **Break condition:** If finetuning data is too narrow, LLM may not generalize to other domains.

### Mechanism 3
- **Claim:** TPST algorithm ensures speaker corrections don't alter original ASR transcript.
- **Mechanism:** After LLM generates corrected speaker sequence, TPST aligns it back to original ASR sequence using Levenshtein alignment, transferring labels without changing words.
- **Core assumption:** Word-level alignment between LLM output and original transcript is possible and accurate.
- **Evidence:** Section describes treating completion sequences as source and original as target for label transfer.
- **Break condition:** If LLM heavily edits transcript or alignment fails, TPST may alter original words.

## Foundational Learning

- **Concept:** Speaker diarization and common error modes (e.g., word misattribution around speaker turns)
  - **Why needed:** Understanding error types helps appreciate why LLM post-processing can help
  - **Quick check:** What is the difference between WDER and WER, and why is WDER relevant for evaluating speaker diarization?

- **Concept:** Large language model prompting and fine-tuning basics (prefix/suffix, tokenization, instruction tuning)
  - **Why needed:** The approach relies on carefully structured prompts and optional fine-tuning
  - **Quick check:** How does the prompt format in DiarizationLM differ from a typical free-form LLM prompt?

- **Concept:** Alignment algorithms (Levenshtein distance, Hungarian assignment)
  - **Why needed:** TPST uses these algorithms to map corrected speakers back to original transcript
  - **Quick check:** Why is it important to preserve the original ASR word sequence when correcting speaker labels?

## Architecture Onboarding

- **Component map:** ASR system → Word sequence + timing → Orchestration module → Prompt builder → LLM (optionally fine-tuned) → Completion parser → Final diarized transcript
- **Critical path:** ASR → Diarization → Orchestration → Prompt builder → LLM → Completion parser → Output
- **Design tradeoffs:**
  - Fine-tuning vs. zero-shot: Fine-tuning improves accuracy but requires labeled data; zero-shot is easier but less accurate
  - Prompt segmentation: Longer prompts risk truncation; shorter prompts may lose context
  - TPST vs. direct output: TPST preserves WER but adds complexity; direct output is simpler but may alter transcripts
- **Failure signatures:**
  - LLM output deviates significantly from input transcript → TPST fails or alters words
  - Zero-shot/ one-shot LLM introduces deletions or hallucinations → Worse WDER than baseline
  - Prompt builder creates overly long segments → LLM truncation or incomplete correction
- **First 3 experiments:**
  1. Run baseline ASR + diarization pipeline and measure WDER and cpWER
  2. Apply DiarizationLM with zero-shot prompting and compare WDER/cpWER
  3. Apply DiarizationLM with fine-tuned LLM (hyp2ora) and compare improvements

## Open Questions the Paper Calls Out

- **Question:** How does performance vary with different LLM sizes and architectures when finetuned on speaker diarization?
  - **Basis:** Paper suggests larger models like PaLM 2-M or PaLM 2-L might perform even better
  - **Why unresolved:** Only experimented with PaLM 2-S, Llama 2, and Llama 3
  - **What would resolve:** Finetuning and evaluating larger LLM models and other architectures on the task

- **Question:** How does DiarizationLM perform on speaker diarization tasks in languages other than English?
  - **Basis:** Paper notes PaLM 2 models are multilingual, suggesting natural applicability to other languages
  - **Why unresolved:** Experiments limited to English datasets (Fisher and Callhome)
  - **What would resolve:** Finetuning and evaluating on speaker diarization datasets in various languages

- **Question:** How does DiarizationLM perform on speaker diarization tasks with varying numbers of speakers?
  - **Basis:** Paper mentions including more diverse datasets for finetuning and evaluation across domains with unknown speaker counts
  - **Why unresolved:** Experiments limited to datasets with exactly 2 speakers
  - **What would resolve:** Finetuning and evaluating on datasets with varying numbers of speakers

## Limitations

- Exact preprocessing steps for creating training data (prompt-completion pairs) are not fully specified
- Specific hyperparameters for LLM fine-tuning are not provided
- Experiments limited to English datasets with exactly 2 speakers

## Confidence

- **High confidence:** General framework and its potential effectiveness
- **Medium confidence:** Specific implementation details and hyperparameter choices
- **Low confidence:** Ability to perfectly reproduce results without additional clarification

## Next Checks

1. Verify exact preprocessing pipeline for creating prompt-completion pairs from ASR and diarization outputs
2. Experiment with different LLM fine-tuning hyperparameters to optimize performance
3. Test the approach on additional datasets to assess generalization beyond Fisher and Callhome