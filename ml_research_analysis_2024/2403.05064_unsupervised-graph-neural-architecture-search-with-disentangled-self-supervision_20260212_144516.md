---
ver: rpa2
title: Unsupervised Graph Neural Architecture Search with Disentangled Self-supervision
arxiv_id: '2403.05064'
source_url: https://arxiv.org/abs/2403.05064
tags:
- graph
- architecture
- neural
- search
- architectures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DSGAS, the first method for unsupervised
  graph neural architecture search. The core challenge addressed is discovering optimal
  GNN architectures without labeled data, by capturing latent graph factors that drive
  data formation.
---

# Unsupervised Graph Neural Architecture Search with Disentangled Self-supervision

## Quick Facts
- **arXiv ID**: 2403.05064
- **Source URL**: https://arxiv.org/abs/2403.05064
- **Reference count**: 40
- **Key outcome**: DSGAS significantly outperforms state-of-the-art baselines in both unsupervised (3.1% improvement on PROTEINS) and semi-supervised settings.

## Executive Summary
This paper introduces DSGAS, the first method for unsupervised graph neural architecture search. The core challenge addressed is discovering optimal GNN architectures without labeled data by capturing latent graph factors that drive data formation. DSGAS employs three key innovations: a disentangled graph super-network with multiple architectures optimized simultaneously, self-supervised training that jointly disentangles architectures and graph factors, and contrastive search using architecture augmentations to find factor-specific experts. Experiments on 11 real-world datasets show DSGAS significantly outperforms state-of-the-art baselines, demonstrating its effectiveness in discovering powerful architectures without labels.

## Method Summary
DSGAS addresses unsupervised GNAS by constructing a disentangled graph super-network that incorporates K different architectures simultaneously, each specializing in capturing a distinct latent factor. The method employs self-supervised training with joint architecture-graph disentanglement, using Bayesian inference to estimate factor probabilities and route appropriate architectures for factor-specific training. A contrastive search mechanism with architecture augmentations (operation choice, weight, and embedding perturbations) discovers architectures with distinct capabilities for different factors. The approach is evaluated on 11 real-world graph datasets across both unsupervised and semi-supervised settings.

## Key Results
- DSGAS achieves 3.1% improvement over state-of-the-art unsupervised method DGNAS on PROTEINS dataset
- DSGAS outperforms supervised GNAS methods like MiLeNAS in semi-supervised settings with limited labeled data (10%, 5%, 1% splits)
- DSGAS demonstrates consistent performance gains across diverse graph datasets including graph-level (PROTEINS, DD, MUTAG, IMDB-B) and node-level (Coauthor CS/Physics, Amazon Computers/Photos) tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The disentangled graph super-network enables simultaneous optimization of multiple architectures with factor-wise disentanglement.
- **Mechanism**: By incorporating K different architectures into a single super-network with separate operation choices, each architecture can specialize in capturing a distinct graph factor without interference from others.
- **Core assumption**: The graph contains multiple latent factors that can be independently captured by different architectural choices.
- **Evidence anchors**:
  - [abstract] "disentangled graph super-network capable of incorporating multiple architectures with factor-wise disentanglement, which are optimized simultaneously"
  - [section] "we propose a disentangled graph architecture super-network to incorporate K different architectures to be estimated and searched w.r.t factors simultaneously"
- **Break condition**: If the graph factors are not truly disentangled (e.g., strongly correlated), the architectures may interfere with each other and fail to specialize.

### Mechanism 2
- **Claim**: Self-supervised training with joint architecture-graph disentanglement provides accurate performance estimation without labels.
- **Mechanism**: The method uses Bayesian inference to estimate the probability of each latent factor given the graph, then routes the appropriate architecture to conduct factor-specific self-supervised training, creating a feedback loop between architecture selection and factor discovery.
- **Core assumption**: The architectures can be used to infer latent factors, and different architectures perform better under different factors.
- **Evidence anchors**:
  - [abstract] "self-supervised training with joint architecture-graph disentanglement"
  - [section] "we estimate the performance of architectures under different factors by our proposed self-supervised training with joint architecture-graph disentanglement"
- **Break condition**: If the architectures cannot distinguish between factors (e.g., if all architectures perform similarly regardless of factor), the disentanglement will fail.

### Mechanism 3
- **Claim**: Contrastive search with architecture augmentations discovers architectures with distinct capabilities for different factors.
- **Mechanism**: By creating multiple views of each architecture through perturbations and contrasting them, the method encourages architectural diversity while maintaining semantic similarity, allowing discovery of architectures that excel at specific factors.
- **Core assumption**: Architectures that are similar in structure capture similar factors, and dissimilar architectures capture different factors.
- **Evidence anchors**:
  - [abstract] "contrastive search with architecture augmentations to discover architectures with factor-specific expertise"
  - [section] "we propose a contrastive search with architecture augmentations, where a novel architecture-level instance discrimination task is introduced to discover architectures with distinct capabilities"
- **Break condition**: If the augmentations are too weak (architectures remain similar) or too strong (architectures lose semantic meaning), the contrastive learning will not effectively distinguish factor-specific architectures.

## Foundational Learning

- **Concept**: Graph Neural Networks and their operation choices (GCN, GAT, GIN, etc.)
  - **Why needed here**: The paper builds on understanding that different GNN operations capture different graph properties, which is fundamental to the architecture search process
  - **Quick check question**: Why might a GAT operation be better than GCN for certain graph datasets?

- **Concept**: Self-supervised learning and pretext tasks
  - **Why needed here**: The method replaces supervised labels with self-supervised objectives to evaluate architecture performance, requiring understanding of how to design meaningful pretext tasks
  - **Quick check question**: How does the factor-aware graph self-supervised learning differ from standard graph self-supervised learning?

- **Concept**: Disentangled representation learning
  - **Why needed here**: The core innovation involves separating graph factors and architectural capabilities, which requires understanding how disentanglement works in representation learning
  - **Quick check question**: What is the key difference between joint architecture-graph disentanglement and standard graph disentanglement?

## Architecture Onboarding

- **Component map**: Super-network construction -> Self-supervised training (factor discovery + architecture evaluation) -> Contrastive search (architecture selection) -> Downstream evaluation

- **Critical path**: Disentangled super-network construction → Self-supervised training (factor discovery + architecture evaluation) → Contrastive search (architecture selection) → Downstream evaluation

- **Design tradeoffs**:
  - K (number of factors): Too small loses specialization benefits; too large increases computational cost and may overfit
  - Architecture augmentations: Balance between maintaining semantic similarity and creating diversity
  - Operation pool size: Larger pools offer more flexibility but increase search space complexity

- **Failure signatures**:
  - All architectures converge to similar structures (insufficient disentanglement)
  - Performance similar to random search (self-supervised objectives not meaningful)
  - Worsening performance with increased K (over-complexity or interference)

- **First 3 experiments**:
  1. Run with K=1 (baseline single architecture) vs K=2 to verify disentanglement benefits
  2. Test with and without contrastive search to measure architectural diversity impact
  3. Compare different architecture augmentation strategies (operation vs weight vs embedding) to find most effective combination

## Open Questions the Paper Calls Out

The paper mentions that it focuses on homogeneous graphs and leaves extending to heterogeneous graphs for future work, as heterogeneous graphs with different types of nodes and edges are common in many real-world applications.

## Limitations
- The assumption that graph data contains K truly disentangled latent factors may not hold for all datasets, potentially limiting the method's effectiveness when factors are correlated
- Performance heavily depends on the quality of self-supervised objectives and their ability to distinguish between architectures without labels
- Computational cost scales with K, creating a tradeoff between specialization benefits and resource requirements

## Confidence

**Confidence Levels:**
- **High Confidence**: The experimental results showing DSGAS outperforming baselines on multiple datasets, particularly the consistent improvements in both unsupervised and semi-supervised settings
- **Medium Confidence**: The theoretical framework of disentangled super-network and joint architecture-graph training, as some implementation details are not fully specified
- **Medium Confidence**: The contrastive search mechanism's ability to discover factor-specific architectures, as the effectiveness depends heavily on augmentation strategies that may vary by dataset

## Next Checks
1. Conduct ablation studies with varying K values on a single dataset to identify the optimal number of factors and verify the tradeoff between specialization and computational cost
2. Test the method on synthetic graphs with known latent factors to validate whether DSGAS can correctly identify and separate the expected factors
3. Compare different architecture augmentation strategies (operation vs weight vs embedding) on a subset of datasets to determine which approach provides the best disentanglement performance