---
ver: rpa2
title: 'Beyond algorithm hyperparameters: on preprocessing hyperparameters and associated
  pitfalls in machine learning applications'
arxiv_id: '2412.03491'
source_url: https://arxiv.org/abs/2412.03491
tags:
- prediction
- error
- tuning
- data
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper reviews and empirically illustrates pitfalls in machine
  learning model evaluation, focusing on preprocessing hyperparameters (HPs) often
  overlooked by applied ML users. It distinguishes between preprocessing HPs (e.g.,
  missing value handling, feature selection) and algorithm HPs (e.g., tree depth),
  showing that manual experimentation with preprocessing options constitutes a form
  of hyperparameter tuning.
---

# Beyond algorithm hyperparameters: on preprocessing hyperparameters and associated pitfalls in machine learning applications

## Quick Facts
- **arXiv ID**: 2412.03491
- **Source URL**: https://arxiv.org/abs/2412.03491
- **Reference count**: 40
- **Primary result**: Manual experimentation with preprocessing hyperparameters leads to substantial optimistic bias in model evaluation, often going undetected without nested cross-validation.

## Executive Summary
This paper examines the critical role of preprocessing hyperparameters (HPs) in machine learning model evaluation and the pitfalls that arise when they are overlooked. The authors distinguish between preprocessing HPs (e.g., missing value handling, feature selection) and algorithm HPs (e.g., tree depth), demonstrating that manual experimentation with preprocessing options constitutes a form of hyperparameter tuning that requires special evaluation procedures. Using a real-world palliative care prediction problem, the study compares different model generation and evaluation procedures, finding that the apparent error leads to substantial optimistic bias, especially when preprocessing HPs are tuned manually. While cross-validation reduces bias compared to the apparent error, it can still be optimistically biased due to undetected overtuning and clustering effects. The study emphasizes the importance of transparent reporting and appropriate evaluation procedures, such as nested resampling, to avoid exaggerated performance claims in ML applications.

## Method Summary
The authors used the COMPANION dataset (1,449 palliative care phases from 705 episodes across 9 teams) to evaluate prediction models for palliative care resource needs. They split the data into Dtrain (724 observations) and Dnew (725 observations), ignoring clustering structure. Models were generated using 96 different analysis settings combining sample size, performance measures, learning algorithms (CART or CIT decision trees), and model generation/evaluation procedures. The learning pipeline included preprocessing steps (cost correction, outlier removal, IPOS score calculation, feature modifications) followed by algorithm training. Evaluation methods included apparent error, 10-fold cross-validation, and 10-2-fold nested cross-validation to compare prediction error estimates (cPEtrain vs. cPEnew).

## Key Results
- Manual tuning of preprocessing HPs leads to substantial optimistic bias, often undetected by simple cross-validation
- The apparent error for evaluation produces the most optimistic bias, particularly with manual preprocessing HP tuning
- Nested cross-validation mitigates data leakage from HP tuning but is computationally expensive and rarely used in practice
- Clustered data structures can introduce additional optimistic bias even with proper train/test splits

## Why This Works (Mechanism)

### Mechanism 1
Tuning preprocessing hyperparameters without realizing it leads to optimistic bias because the model is overfit to the specific quirks of the training data. When users experiment manually with preprocessing choices (e.g., which features to include, how to handle missing values), they are implicitly optimizing hyperparameters. If this tuning is not accounted for during evaluation, the test set is used both for tuning and final evaluation, causing data leakage.

### Mechanism 2
Nested cross-validation prevents data leakage from hyperparameter tuning by ensuring the test set is never used during the tuning process. In nested cross-validation, the outer loop splits the data into training and test sets. The inner loop performs hyperparameter tuning on the training set only. The final model is evaluated on the outer test set, which was never involved in tuning.

### Mechanism 3
Manual tuning of preprocessing HPs often leads to more optimistic bias than automated tuning of algorithm HPs because manual tuning typically uses the apparent error for evaluation. When tuning manually, users often evaluate each configuration using the apparent error (training and testing on the full dataset). This leads to selecting configurations that overfit the training data. Automated tuning typically uses cross-validation, which is less optimistic but can still be affected by overtuning.

## Foundational Learning

- **Hyperparameter tuning and its impact on model evaluation**
  - Why needed here: Understanding that tuning any hyperparameter (algorithm or preprocessing) requires special evaluation procedures to avoid optimistic bias.
  - Quick check question: What is the difference between tuning preprocessing HPs and algorithm HPs in terms of their impact on model evaluation?

- **Data leakage and its different forms**
  - Why needed here: Recognizing that data leakage can occur not only from overlapping train/test sets but also from tuning procedures and non-i.i.d. data structures.
  - Quick check question: How does clustered data structure lead to data leakage even with proper train/test splits?

- **Nested cross-validation and its implementation**
  - Why needed here: Knowing how to properly implement nested cross-validation to account for hyperparameter tuning during model evaluation.
  - Quick check question: What is the difference between simple cross-validation and nested cross-validation in the context of hyperparameter tuning?

## Architecture Onboarding

- **Component map**: Data preprocessing steps (with associated HPs) -> Learning algorithm (with associated HPs) -> Hyperparameter tuning procedure (manual or automated) -> Model evaluation procedure (apparent error, cross-validation, nested cross-validation) -> Model generation process (training with pre-specified HPs or tuned HPs)

- **Critical path**: Specify preprocessing steps and their HPs -> Specify learning algorithm and its HPs -> Choose tuning procedure (manual or automated) -> Choose evaluation procedure (accounting for tuning) -> Generate and evaluate model

- **Design tradeoffs**: Manual vs. automated tuning: Manual is simpler but more prone to optimistic bias; automated is more robust but requires more expertise. Simple vs. nested cross-validation: Simple is faster but can lead to data leakage; nested is more robust but computationally expensive. Tuning preprocessing HPs vs. algorithm HPs: Both can improve model performance, but preprocessing HPs are often overlooked.

- **Failure signatures**: Overly optimistic performance estimates, poor generalization to new data, inconsistent results across different data splits, long training times without performance improvement

- **First 3 experiments**:
  1. Implement a simple preprocessing pipeline with one tunable HP (e.g., imputation method) and evaluate using both apparent error and cross-validation to observe the difference.
  2. Implement nested cross-validation for a model with multiple tunable HPs (both preprocessing and algorithm) and compare the results to simple cross-validation.
  3. Manually tune preprocessing HPs using the apparent error and then evaluate the final model on a held-out test set to demonstrate the optimistic bias.

## Open Questions the Paper Calls Out

### Open Question 1
How do different levels of clustering (e.g., within-team vs. between-team similarity) quantitatively affect the optimistic bias in prediction error estimates? The paper notes that the COMPANION data set exhibits nested clustering (phases within episodes within teams) and that ignoring this clustering leads to optimistic bias, but does not quantify the effect of different clustering structures.

### Open Question 2
What are the most effective automated tuning procedures for preprocessing hyperparameters in practice? The paper states that integrating preprocessing hyperparameters into automated tuning requires advanced programming expertise and is not yet standard practice, but does not identify which automated procedures are most effective.

### Open Question 3
How does the choice of evaluation procedure (e.g., nested CV vs. permanent holdout) affect the trade-off between computational cost and bias in prediction error estimates? The paper discusses the computational expense of nested CV and the potential bias in simpler evaluation procedures, but does not provide a quantitative analysis of this trade-off.

## Limitations
- The study uses a single medical prediction task, limiting generalizability to other domains
- Manual tuning experiments rely on subjective human decisions, making exact replication difficult
- The real-world dataset has unknown clustering structure that may introduce optimistic bias in error estimates
- The computational expense of nested cross-validation limits its practical adoption

## Confidence
- **High confidence**: Claims about data leakage from manual preprocessing HP tuning and the benefits of nested CV for mitigating this issue
- **Medium confidence**: Specific magnitude of bias reduction across different evaluation procedures, given the single dataset constraint
- **Low confidence**: Generalizability of results to non-medical domains or different ML algorithms beyond decision trees

## Next Checks
1. Replicate the study on multiple synthetic datasets with known clustering structures to quantify the impact of data structure on bias
2. Compare manual vs. automated preprocessing HP tuning on the same dataset to measure relative bias magnitudes
3. Implement a simplified nested CV framework and measure computational overhead vs. bias reduction trade-off on larger datasets