---
ver: rpa2
title: 'CoMM: Collaborative Multi-Agent, Multi-Reasoning-Path Prompting for Complex
  Problem Solving'
arxiv_id: '2404.17729'
source_url: https://arxiv.org/abs/2404.17729
tags:
- scenario
- wrong
- answer
- reasoning
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoMM, a collaborative multi-agent, multi-reasoning-path
  prompting framework that significantly improves the reasoning capabilities of large
  language models (LLMs) on complex science problems. The key innovation is to prompt
  LLMs to play different roles (e.g., physicist, mathematician, summarizer) in a problem-solving
  team and apply different reasoning paths for different roles, enabling effective
  few-shot learning in multi-agent scenarios.
---

# CoMM: Collaborative Multi-Agent, Multi-Reasoning-Path Prompting for Complex Problem Solving

## Quick Facts
- arXiv ID: 2404.17729
- Source URL: https://arxiv.org/abs/2404.17729
- Reference count: 11
- Key outcome: CoMM achieves absolute average improvements of 3.84% at zero-shot and 8.23% at few-shot settings on college-level science benchmarks

## Executive Summary
CoMM introduces a collaborative multi-agent prompting framework that significantly improves LLM reasoning on complex science problems. The framework assigns different roles (physicist, mathematician, summarizer) to separate agents, each with specialized reasoning paths, enabling effective few-shot learning in multi-agent scenarios. Experiments on two college-level science benchmarks demonstrate that CoMM outperforms strong baselines by substantial margins.

## Method Summary
CoMM uses three LLM instances with distinct roles and reasoning paths: a physicist expert, a mathematician expert, and a summarizer. Each agent receives specialized prompts with different reasoning exemplars appropriate to their role. The framework implements multi-turn discussions where outputs circulate between agents for collaborative problem-solving. The approach leverages role specialization to reduce cognitive interference and applies different reasoning strategies to different agents for comprehensive problem analysis.

## Key Results
- CoMM outperforms strong baselines with absolute average improvements of 3.84% at zero-shot and 8.23% at few-shot settings
- Multi-agent collaboration significantly outperforms single-agent approaches across all benchmarks and settings
- Multi-expert settings with different expertise outperform single-expert settings with the same expertise

## Why This Works (Mechanism)

### Mechanism 1
Role specialization reduces cognitive interference in LLMs during complex problem solving by preventing context-switching confusion that occurs when a single LLM must switch between diverse reasoning modes. Each agent focuses on its specialized reasoning path without interference from competing cognitive demands.

### Mechanism 2
Multi-path reasoning in few-shot settings improves knowledge transfer by exposing agents to complementary reasoning strategies. The physicist agent learns to apply domain knowledge while the mathematician focuses on computational accuracy, creating diverse solution approaches that can be synthesized.

### Mechanism 3
Iterative multi-turn discussions enable error correction and knowledge refinement between specialized agents. Multiple discussion rounds improve solution accuracy by allowing agents to build on and correct each other's work through collaborative reasoning.

## Foundational Learning

- **Concept**: Chain-of-thought prompting
  - **Why needed here**: CoMM builds on CoT by distributing the reasoning steps across multiple specialized agents rather than requiring a single model to maintain the entire reasoning chain.
  - **Quick check question**: How does CoMM's approach to intermediate reasoning steps differ from traditional chain-of-thought prompting?

- **Concept**: Role-based prompting
  - **Why needed here**: CoMM relies on clear role definitions (physicist, mathematician, summarizer) to create specialized reasoning paths that complement each other.
  - **Quick check question**: What are the three roles defined in CoMM and what expertise does each bring to problem-solving?

- **Concept**: Multi-agent collaboration frameworks
  - **Why needed here**: Understanding how multiple LLM instances can work together through structured communication patterns is fundamental to CoMM's architecture.
  - **Quick check question**: What is the difference between single-agent role-playing and CoMM's multi-agent approach?

## Architecture Onboarding

- **Component map**: Task input → System message generation → Physicist agent reasoning → Mathematician agent reasoning → Synthesizer agent synthesis → Final answer output
- **Critical path**: Task input → System message generation → Agent 1 reasoning → Agent 2 reasoning → Agent 3 synthesis → Final answer output
- **Design tradeoffs**: Single vs multiple LLM instances (complexity vs performance), number of discussion turns (accuracy vs efficiency), role specialization granularity (expertise vs overhead)
- **Failure signatures**: Agents producing contradictory outputs, discussion loops without convergence, role confusion in agent responses, computational errors propagating through synthesis
- **First 3 experiments**:
  1. Single-agent vs multi-agent comparison on College Physics benchmark
  2. Single-expert vs multi-expert comparison with same expertise vs different expertise
  3. One-turn vs two-turn discussion comparison on both benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of CoMM compare to traditional multi-agent frameworks that use the same reasoning path for all agents? The paper discusses applying different reasoning paths for different roles but doesn't compare this to using the same reasoning path for all agents.

### Open Question 2
What is the optimal number of discussion turns for different types of problems, and can this be predicted before solving? The paper shows different results for one-turn vs two-turn discussions on different benchmarks, but doesn't provide a systematic way to determine optimal turns.

### Open Question 3
How does CoMM's performance scale with the number of agents beyond three, and what are the diminishing returns? The paper restricts to three agents but doesn't explore whether more agents would improve or harm performance.

### Open Question 4
How sensitive is CoMM to the choice of few-shot examples, and can the framework automatically select optimal demonstrations? The paper states that CoMM "still requires task-specific design to define the experts and reasoning examples."

## Limitations

- Limited ablation studies prevent isolation of which components contribute most to performance gains
- Role assignments may not generalize effectively across different problem domains beyond science
- Computational costs and inference time comparisons between single-agent and multi-agent approaches are not reported

## Confidence

**High confidence**: Core claim that multi-agent collaboration with role specialization improves reasoning performance on college-level science benchmarks
**Medium confidence**: Specific mechanisms proposed (role specialization reducing cognitive interference, multi-path reasoning improving knowledge transfer)
**Low confidence**: Scalability and efficiency claims due to lack of computational cost analysis

## Next Checks

1. Conduct component ablation study by systematically removing role specialization, multi-path reasoning, and iterative discussion to quantify individual contributions
2. Apply CoMM framework to non-science domains (legal reasoning, medical diagnosis, creative writing) with domain-appropriate role definitions
3. Measure and compare inference times, token usage, and cost per query between single-agent and multi-agent approaches across different model sizes