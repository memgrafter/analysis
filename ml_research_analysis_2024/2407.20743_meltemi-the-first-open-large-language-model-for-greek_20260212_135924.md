---
ver: rpa2
title: 'Meltemi: The first open Large Language Model for Greek'
arxiv_id: '2407.20743'
source_url: https://arxiv.org/abs/2407.20743
tags:
- greek
- language
- meltemi
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Meltemi 7B, the first open large language model
  for the Greek language, developed by continuous pretraining of Mistral 7B on a 54.5B
  token Greek corpus and instruction-tuning on a translated Greek preference dataset.
  The model achieves state-of-the-art performance in Greek, showing a 20.2% average
  improvement over Mistral 7B on Greek benchmarks while experiencing a moderate 6%
  drop in English tasks due to distribution shift.
---

# Meltemi: The first open Large Language Model for Greek

## Quick Facts
- arXiv ID: 2407.20743
- Source URL: https://arxiv.org/abs/2407.20743
- Reference count: 20
- Primary result: First open LLM for Greek, 20.2% improvement over Mistral 7B on Greek benchmarks

## Executive Summary
Meltemi 7B is the first open large language model specifically developed for the Greek language, created through continuous pretraining of Mistral 7B on a 54.5B token Greek corpus. The model achieves state-of-the-art performance in Greek, showing a 20.2% average improvement over Mistral 7B on Greek benchmarks while maintaining moderate English capabilities with only a 6% drop. The work includes tokenizer expansion from 32K to 61K tokens for efficient Greek processing and comprehensive evaluation through translated and curated benchmarks.

## Method Summary
The development of Meltemi 7B involved two-stage continuous pretraining of Mistral 7B on a Greek corpus (43.4B Greek tokens, 10.5B English tokens, 0.6B parallel data) with an expanded tokenizer (61K tokens). The model was then instruction-tuned using the ORPO algorithm on a high-quality translated Greek preference dataset (89,730 preference triplets in Greek, 7,342 in English). The process included deduplication, quality filtering, and systematic evaluation against both Greek and English benchmarks.

## Key Results
- 20.2% average improvement over Mistral 7B on Greek language benchmarks
- 6% performance drop on English tasks due to distribution shift
- 54.5B token pretraining corpus with 79.5% Greek content
- Tokenizer expansion reduced Greek text fertility from 6.8 to 1.52 tokens per word

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continual pretraining on a high-quality Greek corpus enables the model to develop native Greek fluency while retaining English capabilities.
- Mechanism: The model is first adapted through continual pretraining on a large, diverse Greek corpus, which shifts its distribution toward Greek language patterns. The use of both Greek and English data in the mix helps prevent catastrophic forgetting.
- Core assumption: The continual pretraining process can effectively balance the shift toward Greek without erasing the English capabilities learned during initial training.
- Evidence anchors:
  - [abstract]: "adapt Mistral, by continuous pretraining on the Greek Corpus"
  - [section]: "Our continual pretraining approach adapts an existing model for a new natural language using data which induce a strong distribution shift"
  - [corpus]: The corpus includes 43.4B Greek tokens, 10.5B English tokens, and 0.6B parallel data, providing a balanced mix for the pretraining process.
- Break condition: If the pretraining data mix becomes too skewed toward Greek, the model may experience catastrophic forgetting of English capabilities, as evidenced by the moderate 6% drop in English tasks.

### Mechanism 2
- Claim: Expanding the tokenizer vocabulary for Greek improves computational efficiency and model performance on Greek texts.
- Mechanism: The original tokenizer is expanded from 32K to 61K tokens, reducing the fertility rate for Greek texts from 6.8 to 1.52 tokens per word, which decreases training and inference costs.
- Core assumption: A larger vocabulary size for Greek tokens will lead to more efficient tokenization and better model performance on Greek texts.
- Evidence anchors:
  - [section]: "increasing the vocabulary size of the original tokenizer proved to be a vital step in significantly reducing training costs and increasing inference speed"
  - [section]: "The fertility of a tokenizer is an extremely important quantity, as higher values directly correlate with higher training and inference costs"
  - [corpus]: The expanded tokenizer is specifically designed to handle Greek textual data more efficiently, as evidenced by the reduction in fertility rate.
- Break condition: If the expanded tokenizer is not properly trained on the Greek corpus, it may not effectively reduce the fertility rate, leading to inefficient tokenization and increased computational costs.

### Mechanism 3
- Claim: Instruction tuning using translated preference datasets aligns the model with human preferences for Greek language tasks.
- Mechanism: The model is fine-tuned using the ORPO algorithm on a high-quality translated Greek preference dataset, which includes 89,730 preference triplets in Greek and 7,342 in English.
- Core assumption: Translating and curating preference datasets for Greek will effectively align the model with human preferences for Greek language tasks.
- Evidence anchors:
  - [abstract]: "translated and curated a Greek instruction corpus, which has been used for the instruction-tuning of a chat model"
  - [section]: "we translated a mix of 12 preference datasets from Huggingface and carefully curated them"
  - [corpus]: The translated preference dataset is specifically designed to align the model with human preferences for Greek language tasks, as evidenced by the high-quality curation process.
- Break condition: If the translated preference datasets are not properly curated or do not accurately reflect human preferences for Greek language tasks, the instruction tuning process may not effectively align the model with human preferences.

## Foundational Learning

- Concept: Continual pretraining
  - Why needed here: To adapt an existing model to a new language (Greek) while retaining some capabilities from the original model.
  - Quick check question: What is the primary goal of continual pretraining in the context of Meltemi?
- Concept: Tokenization and vocabulary expansion
  - Why needed here: To efficiently handle Greek textual data and reduce computational costs.
  - Quick check question: How does expanding the tokenizer vocabulary improve the model's performance on Greek texts?
- Concept: Instruction tuning and preference optimization
  - Why needed here: To align the model with human preferences for Greek language tasks.
  - Quick check question: What is the purpose of instruction tuning in the context of Meltemi?

## Architecture Onboarding

- Component map: Greek corpus (54.5B tokens) -> Tokenizer expansion (32K→61K tokens) -> Mistral 7B base model -> ORPO instruction tuning -> Greek evaluation suite
- Critical path: Pretraining corpus → Tokenizer expansion → Base model adaptation → Instruction tuning → Evaluation
- Design tradeoffs: Balancing the pretraining data mix to prevent catastrophic forgetting vs. achieving native Greek fluency
- Failure signatures: Significant drop in English task performance, inefficient tokenization, misalignment with human preferences
- First 3 experiments:
  1. Evaluate the fertility rate of the expanded tokenizer on a sample Greek corpus.
  2. Measure the model's performance on Greek and English benchmarks after continual pretraining.
  3. Assess the model's alignment with human preferences using the instruction-tuned version.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Meltemi 7B's performance compare to native Greek models trained from scratch on the same data?
- Basis in paper: [inferred] The paper discusses continual pretraining of Mistral 7B but doesn't compare to models trained from scratch on Greek data
- Why unresolved: The authors only compare to Mistral 7B and other multilingual models, not to Greek-specific models built from scratch
- What evidence would resolve it: Training a comparable Greek model from scratch and benchmarking against Meltemi 7B

### Open Question 2
- Question: What is the optimal balance between Greek and English data in continual pretraining to maximize Greek performance while minimizing English performance degradation?
- Basis in paper: [explicit] The paper mentions using both Greek and English data but doesn't explore different ratios or their effects
- Why unresolved: The paper uses a fixed 79.5% Greek to 19.3% English ratio without exploring alternatives
- What evidence would resolve it: Systematic experiments varying the Greek-English data ratio in pretraining

### Open Question 3
- Question: How sustainable is Meltemi 7B's development approach compared to training from scratch when considering long-term maintenance and updates?
- Basis in paper: [explicit] The authors discuss sustainability and mention energy consumption of 2300 kWh for development
- Why unresolved: The paper acknowledges sustainability concerns but doesn't provide a detailed analysis comparing different development approaches
- What evidence would resolve it: Comparative lifecycle analysis of continual pretraining versus training from scratch approaches

### Open Question 4
- Question: What specific factors in the pretraining data selection and filtering process most significantly impact Meltemi 7B's final performance?
- Basis in paper: [explicit] The paper describes extensive data preprocessing and filtering but doesn't analyze which steps are most critical
- Why unresolved: While the paper details the filtering pipeline, it doesn't systematically evaluate the impact of individual filtering decisions
- What evidence would resolve it: Ablation studies removing or modifying specific filtering steps and measuring performance impact

## Limitations

- Limited analysis of cross-lingual interference and its practical implications for bilingual applications
- Insufficient evaluation across diverse Greek domains and specialized vocabulary areas
- Lack of comprehensive robustness testing against adversarial inputs and out-of-distribution examples

## Confidence

**High Confidence**: The core methodology of continuous pretraining and tokenizer expansion is well-established in the literature. The quantitative improvements on Greek benchmarks (20.2% average improvement) are clearly demonstrated through systematic evaluation.

**Medium Confidence**: The effectiveness of the instruction tuning approach using translated preference data is supported by the results, but the potential limitations of this approach (translation artifacts, cultural misalignment) are not fully explored.

**Low Confidence**: Claims about the model's performance across diverse real-world Greek applications, its robustness to adversarial inputs, and its ability to handle specialized domains are not thoroughly validated.

## Next Checks

1. **Domain-Specific Performance Analysis**: Evaluate Meltemi's performance across diverse Greek domains (technical, legal, literary, colloquial) to assess its versatility and identify potential weaknesses in specialized vocabulary or domain-specific reasoning.

2. **Robustness and Safety Testing**: Conduct comprehensive adversarial testing to evaluate the model's robustness to out-of-distribution inputs, prompt injection attacks, and potential generation of harmful content in Greek.

3. **Cross-Lingual Interference Analysis**: Perform detailed analysis of the 6% English performance drop to understand the nature of cross-lingual interference and assess the practical implications for bilingual Greek-English applications.