---
ver: rpa2
title: Safe Exploration Using Bayesian World Models and Log-Barrier Optimization
arxiv_id: '2405.05890'
source_url: https://arxiv.org/abs/2405.05890
tags:
- learning
- safe
- policy
- exploration
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of safe reinforcement learning
  in partially observable environments. The proposed method, CERL, combines Bayesian
  world models with log-barrier optimization to ensure safety during learning.
---

# Safe Exploration Using Bayesian World Models and Log-Barrier Optimization

## Quick Facts
- arXiv ID: 2405.05890
- Source URL: https://arxiv.org/abs/2405.05890
- Authors: Yarden As; Bhavya Sukhija; Andreas Krause
- Reference count: 5
- Primary result: Zero constraint violations during training while learning near-optimal policies

## Executive Summary
This work addresses safe reinforcement learning in partially observable environments by combining Bayesian world models with log-barrier optimization. The proposed method, CERL, uses a probabilistic ensemble of Recurrent State Space Models to capture epistemic uncertainty and employs an interior-point method to optimize policies within a pessimistic safe set. This approach maintains safety throughout training and outperforms prior methods on the Safety-Gym benchmark suite, achieving zero constraint violations while learning near-optimal policies in high-dimensional settings with image observations.

## Method Summary
CERL tackles safe exploration in constrained MDPs by building a Bayesian ensemble of RSSM models to capture epistemic uncertainty about the environment dynamics. The method uses pessimistic constraint evaluation by selecting the worst-case (highest cost) model from the ensemble for safety checks. Policy optimization is performed using log-barrier stochastic gradient descent (LBSGD), which maintains feasibility by adaptively adjusting step sizes based on constraint violations. The approach scales to high-dimensional partial-observable environments by leveraging RSSM's ability to learn compact latent state representations from image observations.

## Key Results
- Achieved zero constraint violations during training across Safety-Gym benchmark tasks
- Outperformed prior safe RL methods on the Safety-Gym benchmark suite
- Successfully scaled to high-dimensional image observation settings
- Maintained safety guarantees throughout the learning process

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pessimistic policy selection with respect to epistemic uncertainty ensures safety during exploration
- Mechanism: CERL samples multiple models from the posterior over RSSM parameters and selects the worst-case (highest cost) model for constraint evaluation, ensuring the policy is feasible for the true model with high probability.
- Core assumption: The true transition dynamics p* is within the support of the Bayesian ensemble model distribution P
- Evidence anchors: [abstract] pessimistic w.r.t. model's epistemic uncertainty; [section 4] ensures constraint satisfaction for p* by picking a policy that satisfies constraints for all transition distributions in P
- Break condition: If p* falls outside the support of P, pessimistic selection may be overly conservative or fail to ensure safety

### Mechanism 2
- Claim: Log-barrier optimization with interior-point guarantees maintains safety during learning iterations
- Mechanism: LBSGD uses log-barrier function Bη(πn) = J(πn,pθ)−ηlog (−JcP (πn)) where JcP (πn) = max pθi∈PJc(πn,pθi), with gradient terms that push iterates away from constraint boundaries.
- Core assumption: Smoothness of J and Jc functions, and unbiased gradient estimation of ∇Bη(πn)
- Evidence anchors: [section 4] LBSGD ensures distance is always kept from boundaries of the safe set from its interior; [abstract] leverages RSSM and scales CERL to high-dimensional real-world setting
- Break condition: If smoothness assumptions are violated or gradient estimates become biased, interior-point guarantees may fail

### Mechanism 3
- Claim: RSSM-based world models enable efficient planning in high-dimensional partial-observable environments
- Mechanism: RSSM learns a latent state representation that captures both observation and temporal dynamics, allowing the agent to plan over compressed latent states rather than raw images.
- Core assumption: The latent state space learned by RSSM is sufficient to represent all relevant information for planning
- Evidence anchors: [section 4] To handle partial-observability, we choose to base our world model on the Recurrent State Space Model (RSSM); [abstract] maintains safety throughout training, outperforming prior methods
- Break condition: If the latent representation fails to capture critical information for safety, the model may make unsafe decisions

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: CERL operates on CMDPs, which extend MDPs with safety constraints
  - Quick check question: What is the difference between the reward function and cost function in a CMDP?

- Concept: Bayesian inference and uncertainty quantification
  - Why needed here: CERL uses Bayesian ensembles to capture epistemic uncertainty in the world model
  - Quick check question: How does epistemic uncertainty differ from aleatoric uncertainty in model-based RL?

- Concept: Interior-point methods and log-barrier optimization
  - Why needed here: LBSGD uses log-barrier optimization to ensure feasibility during policy updates
  - Quick check question: What is the role of the barrier parameter η in log-barrier optimization?

## Architecture Onboarding

- Component map: Observation encoder → RSSM latent dynamics → Ensemble of transition models → Pessimistic constraint evaluation → Log-barrier optimizer → Policy network

- Critical path: 1. Collect trajectories with current policy 2. Train RSSM on collected data 3. Sample ensemble of models from posterior 4. Evaluate worst-case constraints across ensemble 5. Update policy using LBSGD with log-barrier 6. Repeat

- Design tradeoffs:
  - RSSM vs. simpler dynamics models: RSSM handles partial-observability but is more complex
  - Ensemble size vs. computational cost: Larger ensembles give better uncertainty estimates but are slower
  - Barrier parameter η vs. conservatism: Larger η enforces stricter safety but may slow learning

- Failure signatures:
  - Policy never improves: Barrier parameter too large or constraints too conservative
  - Safety violations occur: Model ensemble doesn't capture true dynamics or RSSM underfits
  - Training instability: Gradient estimates become biased or smoothness assumptions violated

- First 3 experiments:
  1. Train RSSM on static dataset to verify latent dynamics learning
  2. Evaluate pessimistic constraint selection with known ground truth dynamics
  3. Test LBSGD on simple constrained optimization problem with known landscape

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can CERL's safety guarantees be extended to state-wise safety constraints rather than just expected cost bounds?
- Basis in paper: [explicit] The authors note that CERL satisfies constraints in the classical CMDP setting with expected cost bounds, but many real applications require state-wise safety
- Why unresolved: The current formulation focuses on expected cost constraints over the episode horizon, which may not be sufficient for applications requiring safety at every state
- What evidence would resolve it: Experimental results showing CERL's performance on tasks with explicit state-wise safety constraints, or theoretical extensions proving safety at each timestep

### Open Question 2
- Question: What is the impact of using a stronger black-box optimizer like LBSGD on CERL's performance compared to other constrained optimization methods?
- Basis in paper: [explicit] The authors attribute CERL's empirical safety improvements to using LBSGD, a stronger black-box optimizer compared to LAMBDA's Augmented Lagrangian approach
- Why unresolved: While the authors suggest LBSGD's importance, they don't provide direct comparisons with other optimizers or ablation studies
- What evidence would resolve it: Comparative experiments using different constrained optimization methods within CERL's framework, showing performance differences in safety and optimality

### Open Question 3
- Question: How does CERL's performance scale with increasing state dimensionality and partial observability in real-world robotic applications?
- Basis in paper: [inferred] The authors demonstrate CERL's effectiveness in high-dimensional Safety-Gym tasks with image observations, but real-world applications may present greater challenges
- Why unresolved: The experiments are limited to simulated environments, and real-world scenarios may introduce additional complexities like sensor noise and dynamic environments
- What evidence would resolve it: Deployment of CERL on real robotic systems with varying levels of sensor noise and environmental complexity, measuring safety and performance across different domains

## Limitations

- The safety guarantee relies on the assumption that the true dynamics p* is within the support of the Bayesian ensemble, which may not hold in practice
- Performance depends on the quality of the RSSM's latent representation, which may fail to capture safety-critical features in complex environments
- The log-barrier method's effectiveness depends on smoothness assumptions that may be violated in real-world applications

## Confidence

- Safety guarantees through pessimistic selection: High confidence (strong theoretical foundation)
- LBSGD maintains safety during learning: Medium confidence (relies on smoothness assumptions)
- RSSM enables safe exploration in high-dimensions: Medium confidence (empirical validation shown but limited scope)

## Next Checks

1. Test CERL with models where p* is deliberately excluded from the ensemble support to quantify conservatism when safety assumptions fail
2. Evaluate RSSM's latent representation quality on safety-critical features (obstacle proximity, agent state) to identify potential blind spots
3. Compare CERL against non-Bayesian ensemble methods to isolate the contribution of epistemic uncertainty quantification to safety performance