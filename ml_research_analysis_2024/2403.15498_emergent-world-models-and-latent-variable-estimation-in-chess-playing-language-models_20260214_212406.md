---
ver: rpa2
title: Emergent World Models and Latent Variable Estimation in Chess-Playing Language
  Models
arxiv_id: '2403.15498'
source_url: https://arxiv.org/abs/2403.15498
tags:
- board
- intervention
- layer
- skill
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether language models can learn world
  models and extract latent variables from data, extending prior research on Othello
  to the more complex domain of chess. The authors train a GPT model on real chess
  games and use linear probes to uncover internal representations of board state and
  player skill (Elo rating).
---

# Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models

## Quick Facts
- arXiv ID: 2403.15498
- Source URL: https://arxiv.org/abs/2403.15498
- Authors: Adam Karvonen
- Reference count: 19
- Key outcome: Language models trained on chess PGN strings learn internal representations of board state and player skill, with interventions showing up to 2.6x improvement in win rate when skill vectors are added.

## Executive Summary
This work investigates whether language models can learn world models and extract latent variables from data, extending prior research on Othello to the more complex domain of chess. The authors train a GPT model on real chess games and use linear probes to uncover internal representations of board state and player skill (Elo rating). They find that the model learns to estimate player skill to improve predictions, achieving up to 2.6x improvement in win rate when this skill vector is added to the model. Interventions using these representations successfully edit the model's internal board state and chess-playing ability, demonstrating that the model forms a world model of chess and can estimate latent variables like player skill.

## Method Summary
The authors train GPT models (8 or 16 layers, 512 hidden dim, 8 attention heads) on real chess PGN strings from Lichess database. They use linear probes to analyze activations for board state representation and player skill estimation. Interventions are performed by adding/subtracting vectors derived from these probes to the model's residual stream. The models are evaluated by their ability to generate legal moves and win games against Stockfish.

## Key Results
- Linear probes achieve 99.6% accuracy in classifying board state across test games
- Model learns to estimate player skill, with skill vectors improving win rate by up to 2.6x when added
- Interventions successfully edit internal board state and chess-playing ability
- 16-layer model achieves 64% win rate against Stockfish level 0, 8-layer achieves 46%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model learns an internal board state representation through sequential prediction on chess PGN strings.
- Mechanism: The model processes chess moves as text tokens and builds an internal representation of the board state in its activations. Linear probes trained on these activations can recover the board state with high accuracy.
- Core assumption: The model's next-token prediction task forces it to track board state implicitly, even without explicit knowledge of chess rules.
- Evidence anchors:
  - [abstract] "The model is given no a priori knowledge of the game and is solely trained on next character prediction, yet we find evidence of internal representations of board state."
  - [section 3.1] "The most accurate probe achieves a 99.6% accuracy in classifying the state of each square across the test games."
  - [corpus] Found related work on emergent world representations in transformers trained on games, supporting the general mechanism.
- Break condition: If the dataset is too small or too synthetic (like uniform sampling from game trees), the model may not learn robust board state representations.

### Mechanism 2
- Claim: The model learns to estimate player skill (Elo rating) as a latent variable to improve next-move prediction.
- Mechanism: Since the model is trained on real human games with varying skill levels, it learns to estimate player skill from game context to better predict which moves players of different skill levels would make.
- Core assumption: Real games contain meaningful variations in skill that affect move selection, and the model can extract this information from the text.
- Evidence anchors:
  - [abstract] "Unlike Li et al's prior synthetic dataset approach, our analysis finds that the model also learns to estimate latent variables like player skill to better predict the next character."
  - [section 3.2] "We find that to better predict the next character, the model learns to estimate the Elo rating of the players in the game."
  - [corpus] Related work on latent variable estimation in language models supports this mechanism.
- Break condition: If all games in the dataset were between players of similar skill, or if skill differences didn't significantly affect move patterns, the model might not learn to estimate skill.

### Mechanism 3
- Claim: Linear probes can be used to intervene on the model's activations to edit its internal board state and skill estimation.
- Mechanism: By adding or subtracting vectors derived from linear probes to the model's activations, we can causally manipulate its internal representations and observe corresponding changes in behavior.
- Core assumption: The linear probe directions correspond to meaningful directions in the activation space that can be manipulated to change the model's internal state.
- Evidence anchors:
  - [abstract] "We validate these internal representations by using them to make interventions on the model's activations and edit its internal board state."
  - [section 4.1] "Linear probes enable a simple intervention approach of vector addition... we can simply add or subtract vectors derived from our linear probe from the model's residual stream."
  - [corpus] Found related work on intervention techniques using linear probes in language models.
- Break condition: If the model's representations are highly non-linear or distributed across multiple layers in complex ways, simple linear interventions may not work effectively.

## Foundational Learning

- Concept: Linear probes for representation analysis
  - Why needed here: To measure what information the model has learned and where it's stored in the network
  - Quick check question: If a linear probe achieves 99.6% accuracy on board state classification, what does this tell us about the linearity of the representation?

- Concept: Contrastive activation analysis
  - Why needed here: To identify meaningful directions in activation space (like high vs low skill) by comparing different conditions
  - Quick check question: How do you obtain a "skill vector" using contrastive activations from high and low skill games?

- Concept: Vector arithmetic for model editing
  - Why needed here: To causally manipulate the model's internal state by adding/subtracting learned directions
  - Quick check question: What happens if you add the "high skill" vector to a model's activations during inference?

## Architecture Onboarding

- Component map: GPT model (8 or 16 layers, 512 dim hidden, 8 attention heads) → Linear probes for board state and skill → Intervention module for activation editing
- Critical path: Model training → Probe training → Validation of probe accuracy → Intervention testing → Win rate measurement
- Design tradeoffs: Smaller models (25M params) train faster but may learn less accurate representations; larger models (50M params) are more accurate but computationally expensive
- Failure signatures: Low probe accuracy suggests poor internal representations; illegal moves after intervention suggest incorrect intervention vectors or scaling
- First 3 experiments:
  1. Train linear probes on board state and measure accuracy per layer to find where representations emerge
  2. Train Elo classification probes to verify skill estimation capability
  3. Test board state interventions by removing a piece and measuring legal move rate improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the upper limit of latent variable estimation accuracy that can be achieved by language models trained on chess games?
- Basis in paper: [explicit] The paper demonstrates that a probe can classify player skill (Elo rating) with high accuracy, but it does not explore the theoretical limits of this capability.
- Why unresolved: The paper provides evidence of the model's ability to estimate latent variables but does not systematically investigate the maximum accuracy achievable or the factors that might limit this accuracy.
- What evidence would resolve it: Systematic testing of probe accuracy across different model architectures, training durations, and dataset sizes to determine the peak performance and identify bottlenecks in latent variable estimation.

### Open Question 2
- Question: How do interventions on model activations scale with the complexity of the latent variables being estimated?
- Basis in paper: [inferred] The paper discusses interventions on board state and player skill but does not explore whether the effectiveness of these interventions changes with the complexity or abstractness of the latent variables.
- Why unresolved: The interventions described are limited to relatively straightforward variables (board state, player skill), and the paper does not address whether similar techniques would be effective for more complex or abstract latent variables.
- What evidence would resolve it: Experimental results showing the success rates of interventions on a range of latent variables, from simple (board state) to complex (strategic understanding), to determine if there is a correlation between variable complexity and intervention success.

### Open Question 3
- Question: Can the techniques used to estimate latent variables in chess-playing models be generalized to other domains, such as natural language processing?
- Basis in paper: [explicit] The paper suggests that understanding world models in chess could have applications in natural language processing but does not provide evidence of such generalization.
- Why unresolved: The paper focuses on a specific domain (chess) and does not explore whether the methods for estimating latent variables and performing interventions are applicable to other, more complex domains like natural language.
- What evidence would resolve it: Successful application of similar probing and intervention techniques to language models trained on diverse natural language tasks, demonstrating the transferability of the methods.

## Limitations

- The exact mechanism by which the model learns to estimate skill from text remains unclear, with correlation shown but not causal pathway explained.
- Generalizability to other domains and model scales is unknown, as results are specific to chess and relatively small GPT models.
- Intervention techniques may not capture full complexity of model's internal representations, particularly for multi-hop reasoning about game state.

## Confidence

- **High confidence**: The existence of board state representations (99.6% probe accuracy) and the success of board state interventions (up to 92% improvement) are well-established. These findings are directly measurable and reproducible.
- **Medium confidence**: The model's ability to estimate player skill is supported by probe accuracy metrics and win rate improvements, but the causal relationship between skill estimation and prediction quality requires further validation through ablation studies.
- **Medium confidence**: The claim that skill estimation "improves predictions" is demonstrated through win rate improvements but could benefit from more direct measurements of prediction quality on individual moves.

## Next Checks

1. **Ablation study on skill estimation**: Train models on datasets with controlled skill distributions (uniform skill vs. realistic distribution) and measure the impact on both skill probe accuracy and win rate against Stockfish.

2. **Cross-layer intervention analysis**: Systematically test skill interventions at different layers and with different scaling parameters to establish the most effective intervention strategy and better understand the skill representation's location in the network.

3. **Transfer to other games**: Apply the same methodology to a simpler board game (like Othello, as in the referenced work) and a more complex game (like Go) to test the robustness of emergent world model formation across different game complexities.