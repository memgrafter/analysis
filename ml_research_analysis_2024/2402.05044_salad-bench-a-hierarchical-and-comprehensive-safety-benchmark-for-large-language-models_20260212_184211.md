---
ver: rpa2
title: 'SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language
  Models'
arxiv_id: '2402.05044'
source_url: https://arxiv.org/abs/2402.05044
tags:
- safety
- questions
- llms
- attack
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SALAD-Bench is a large-scale safety benchmark for evaluating Large
  Language Models (LLMs), attack, and defense methods. It features a hierarchical
  taxonomy of 6 domains, 16 tasks, and 66 categories, with 21k base questions, 5k
  attack-enhanced questions, 200 defense-enhanced questions, and 4k multiple-choice
  questions.
---

# SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2402.05044
- Source URL: https://arxiv.org/abs/2402.05044
- Authors: Lijun Li; Bowen Dong; Ruohui Wang; Xuhao Hu; Wangmeng Zuo; Dahua Lin; Yu Qiao; Jing Shao
- Reference count: 40
- One-line primary result: SALAD-Bench introduces a hierarchical safety benchmark with 21k base questions, 5k attack-enhanced questions, and 200 defense-enhanced questions, featuring dual LLM-based evaluators achieving up to 99.77% safety rate on Claude2.

## Executive Summary
SALAD-Bench is a large-scale safety benchmark designed to comprehensively evaluate Large Language Models (LLMs) across multiple dimensions of safety. The benchmark features a hierarchical taxonomy structure with 6 domains, 16 tasks, and 66 categories, covering 21k base questions along with enhanced subsets for attack and defense evaluation. It introduces two specialized evaluators - MD-Judge for QA-pairs and MCQ-Judge for multiple-choice questions - to provide both comprehensive and efficient safety assessment. Experiments demonstrate significant variations in LLM safety performance, with Claude2 achieving the highest safety rate of 99.77% on the base set, while attack methods like human-designed jailbreaks show the highest success rates.

## Method Summary
SALAD-Bench employs a three-level hierarchical taxonomy to categorize safety concerns across 6 domains, 16 tasks, and 66 categories. The benchmark includes 21k base questions, enhanced with 5k attack-enhanced and 200 defense-enhanced questions. Two evaluators are introduced: MD-Judge, a fine-tuned Mistral-7B model for QA-pair evaluation, and MCQ-Judge, a regex-based system with in-context learning for multiple-choice questions. The benchmark tests 14 LLMs across base, attack-enhanced, and defense-enhanced subsets using various attack methods (TAP, AutoDAN, GPTFuzz, GCG, CoU, human-designed) and defense methods (GPT-paraphrasing, Self-Reminder, Safe/XSafe prompts). Safety rates, attack success rates, and Elo ratings are used as primary metrics.

## Key Results
- Claude2 achieves the highest safety rate of 99.77% on the base set among tested models
- GPT-4 and Claude2 maintain strong performance (80.28% and 88.02%) on the attack-enhanced subset
- Human-designed jailbreak attacks achieve the highest success rates across all attack methods
- GPT-paraphrasing and Self-Reminder prompts show the strongest effectiveness in reducing attack success rates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The hierarchical taxonomy structure enables precise safety evaluation by breaking down safety concerns into 6 domains, 16 tasks, and 66 categories.
- **Mechanism**: The three-level hierarchy allows evaluators to identify not just overall safety rates but also specific vulnerabilities within subcategories. This granularity helps pinpoint which areas of LLM safety need improvement.
- **Core assumption**: Safety concerns can be meaningfully decomposed into a structured hierarchy that captures the full spectrum of potential harms.
- **Evidence anchors**:
  - [abstract]: "SALAD-Bench introduces a structured hierarchy with three levels, comprising 6 domains, 16 tasks, and 66 categories"
  - [section 2.1]: "SALAD-Bench includes six domain-level harmfulness areas" with detailed breakdown
  - [corpus]: Weak evidence - related papers focus on jailbreak attacks but don't discuss hierarchical taxonomy benefits
- **Break condition**: If safety concerns cannot be effectively categorized or if the hierarchy becomes too rigid to capture emerging safety threats, the evaluation precision will degrade.

### Mechanism 2
- **Claim**: The dual-evaluator system (MD-Judge and MCQ-Judge) provides both comprehensive and efficient evaluation across different question types.
- **Mechanism**: MD-Judge uses fine-tuned LLM to evaluate question-answer pairs with multi-dimensional safety assessment, while MCQ-Judge leverages regex parsing with in-context learning for multiple-choice questions, balancing accuracy with efficiency.
- **Core assumption**: Different question formats require different evaluation approaches, and specialized evaluators can outperform generic ones.
- **Evidence anchors**:
  - [abstract]: "we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs" and "MCQ-Judge for multiple-choice questions"
  - [section 4]: Detailed explanation of both evaluators' construction and methodology
  - [section 5.2]: MD-Judge achieves 0.818 F1 score on SALAD-Base-Test vs 0.785 for GPT-4
- **Break condition**: If either evaluator fails to generalize beyond the training data or if the cost-benefit tradeoff shifts unfavorably, the system's effectiveness will diminish.

### Mechanism 3
- **Claim**: The enhanced question subsets (attack-enhanced and defense-enhanced) create realistic stress tests for LLM safety capabilities.
- **Mechanism**: By applying various attack methods to base questions and filtering for questions that challenge all models, SALAD-Bench creates difficult test cases. The defense-enhanced subset ensures attack methods are properly evaluated.
- **Core assumption**: Safety evaluation requires not just baseline capability assessment but also testing against sophisticated attack strategies.
- **Evidence anchors**:
  - [abstract]: "By infusing our questions with attack methods, we obtain enhanced questions that significantly heightens the evaluation's challenge"
  - [section 3]: Detailed methodology for creating attack-enhanced and defense-enhanced subsets
  - [section 5.4]: "Claude2 achieves the top safety score at 99.77%, while GPT-3.5 scores the lowest at 88.62% among black-box LLMs"
- **Break condition**: If attack methods become obsolete or if defense strategies evolve faster than the benchmark, the stress test will no longer reflect current threats.

## Foundational Learning

- **Concept**: Hierarchical categorization of safety concerns
  - **Why needed here**: The benchmark needs to cover the full spectrum of LLM safety issues systematically
  - **Quick check question**: Can you explain the difference between domain-level, task-level, and category-level taxonomies in SALAD-Bench?

- **Concept**: Adversarial attack methodology for LLMs
  - **Why needed here**: Understanding how attacks work is crucial for creating effective stress tests
  - **Quick check question**: What are the main types of attack methods used to create the attack-enhanced subset?

- **Concept**: Safety evaluation metrics and benchmarking
  - **Why needed here**: Proper evaluation requires understanding metrics like safety rate, attack success rate, and Elo ratings
  - **Quick check question**: How does the attack success rate differ from the safety rate in SALAD-Bench evaluation?

## Architecture Onboarding

- **Component map**: Data collection and taxonomy definition -> Question enhancement pipeline -> Dual-evaluator system (MD-Judge and MCQ-Judge) -> Experiment framework for evaluating LLMs, attacks, and defenses
- **Critical path**: Data collection → Taxonomy definition → Question enhancement → Evaluator training → Model evaluation → Analysis
- **Design tradeoffs**: Using LLM-based evaluators provides flexibility but introduces dependency on model availability; attack enhancement increases benchmark difficulty but requires significant computational resources; hierarchical taxonomy provides precision but adds complexity.
- **Failure signatures**: Poor evaluator performance on out-of-distribution data; attack methods failing to generate meaningful challenges; defense methods showing inconsistent results across different models.
- **First 3 experiments**:
  1. Evaluate a simple LLM on the base set using the keyword evaluator to establish baseline performance
  2. Apply a basic attack method (like human-designed jailbreak) to the base questions and re-evaluate
  3. Test the MCQ-Judge evaluator on a small subset of multiple-choice questions to verify the regex parsing works correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal frequency for updating the hierarchical taxonomy in SALAD-Bench to keep pace with evolving safety threats?
- Basis in paper: [inferred] The paper acknowledges that the defined hierarchical taxonomy may become outdated as new safety threats emerge and evolve.
- Why unresolved: The paper does not provide specific guidance on how often the taxonomy should be reviewed and updated.
- What evidence would resolve it: Empirical studies tracking the emergence of new safety threats over time and correlating them with the effectiveness of existing taxonomies.

### Open Question 2
- Question: How does the performance of MD-Judge compare to human evaluators on attack-enhanced questions across different domains and tasks?
- Basis in paper: [explicit] The paper introduces MD-Judge as an LLM-based evaluator specifically designed for attack-enhanced queries but does not provide direct comparison with human evaluators on this subset.
- Why unresolved: The paper focuses on comparing MD-Judge with other automated methods but lacks human evaluation benchmarks for attack-enhanced questions.
- What evidence would resolve it: A large-scale human evaluation study specifically focused on attack-enhanced questions across all domains and tasks in SALAD-Bench.

### Open Question 3
- Question: What is the relative effectiveness of different defense methods against specific types of attack methods in the attack-enhanced subset?
- Basis in paper: [explicit] The paper evaluates various defense methods but presents aggregate results rather than detailed breakdowns by attack type.
- Why unresolved: The paper shows overall defense effectiveness but doesn't analyze which defenses work best against which attack strategies.
- What evidence would resolve it: Detailed experimental results showing defense success rates against individual attack methods and attack types.

## Limitations
- The evaluation framework relies heavily on LLM-based judges, which may introduce bias and limit reproducibility
- The attack enhancement process may not fully capture the creativity of human adversaries or emerging attack techniques
- Performance metrics indicate reasonable but not perfect accuracy, suggesting room for improvement in evaluator systems

## Confidence
- **High confidence** in the hierarchical taxonomy design and its coverage of safety domains, as evidenced by the detailed breakdown and extensive corpus of 21k base questions across 66 categories.
- **Medium confidence** in the attack and defense evaluation results, given that the benchmark tests against specific methods (TAP, AutoDAN, GPTFuzz, GCG, CoU) which may not generalize to all potential attack vectors.
- **Medium confidence** in the evaluator performance metrics, as the F1 scores indicate reasonable but not perfect accuracy, and the multiple-choice evaluator's performance (90.29% overall accuracy) suggests room for improvement.

## Next Checks
1. **Evaluator Robustness Test**: Evaluate MD-Judge and MCQ-Judge performance on an out-of-distribution safety question set not used in training or benchmark construction to assess generalization capabilities.

2. **Attack Transferability Analysis**: Test the same attack methods across a broader range of LLM architectures (beyond the current set) to verify if attack success rates are consistent or model-dependent.

3. **Defense Method Ablation Study**: Systematically disable individual defense components (e.g., test Self-Reminder without GPT-paraphrasing) to quantify the contribution of each method to overall defense effectiveness.