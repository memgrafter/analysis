---
ver: rpa2
title: An Efficient Rehearsal Scheme for Catastrophic Forgetting Mitigation during
  Multi-stage Fine-tuning
arxiv_id: '2402.08096'
source_url: https://arxiv.org/abs/2402.08096
tags:
- prior
- damage
- collateral
- samples
- rehearsal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work studies catastrophic forgetting during multi-stage fine-tuning\
  \ of foundational models and proposes mix-cd, a computationally efficient rehearsal\
  \ scheme that prioritizes \"collateral damage\" samples\u2014those predicted correctly\
  \ by the prior model but forgotten by the fine-tuned one. The method estimates the\
  \ density of such samples in different data partitions using only samples already\
  \ seen during training, avoiding extra inference costs."
---

# An Efficient Rehearsal Scheme for Catastrophic Forgetting Mitigation during Multi-stage Fine-tuning

## Quick Facts
- arXiv ID: 2402.08096
- Source URL: https://arxiv.org/abs/2402.08096
- Authors: Andrew Bai; Chih-Kuan Yeh; Cho-Jui Hsieh; Ankur Taly
- Reference count: 38
- This work studies catastrophic forgetting during multi-stage fine-tuning of foundational models and proposes mix-cd, a computationally efficient rehearsal scheme that prioritizes "collateral damage" samples—those predicted correctly by the prior model but forgotten by the fine-tuned one.

## Executive Summary
This paper addresses catastrophic forgetting during multi-stage fine-tuning of foundational models by introducing mix-cd, an efficient rehearsal scheme that prioritizes "collateral damage" samples—those predicted correctly by the prior model but forgotten by the fine-tuned one. The method estimates the density of such samples in different data partitions using only samples already seen during training, avoiding extra inference costs. By selectively rehearsing from partitions with higher collateral damage, mix-cd improves the Pareto frontier of prior and new task performance without additional computation compared to random uniform rehearsal. Across text classification, QA, and multilingual translation tasks, mix-cd consistently outperforms several strong baselines in compute-constrained settings.

## Method Summary
The mix-cd algorithm partitions prior data into bins and estimates the conditional probability of collateral damage for each bin. It then samples more heavily from bins with higher estimated collateral damage rates, focusing computational budget on the most problematic regions. The method uses samples already seen during training (mixed in from previous iterations) to estimate collateral damage density without additional inference costs. Multiple partition strategies can be combined to form finer-grained partitions, and the sampling scheduler uses these estimates to prioritize rehearsal. The approach requires caching prior model predictions on all prior data samples but avoids additional forward passes during fine-tuning.

## Key Results
- mix-cd improves the Pareto frontier of prior and new task performance without additional computation compared to random uniform rehearsal
- The method samples 2x or more collateral damage samples than random approaches while maintaining similar computational budgets
- Across text classification, QA, and multilingual translation tasks, mix-cd consistently outperforms several strong baselines in compute-constrained settings
- mix-cd achieves better prior task preservation while maintaining comparable new task performance to baselines

## Why This Works (Mechanism)

### Mechanism 1
Estimating collateral damage density using samples already seen during training avoids additional inference costs. At each fine-tuning iteration, the method uses prior data samples mixed into the current training batch from the previous iteration to estimate the probability that a sample suffers collateral damage. Since these samples have already been processed by the fine-tuned model, their predictions are available without extra computation.

### Mechanism 2
Prioritizing rehearsal of samples with higher collateral damage likelihood improves Pareto tradeoff between new and prior task performance. The method partitions prior data into bins and estimates the conditional probability of collateral damage for each bin. It then samples more heavily from bins with higher estimated collateral damage rates, focusing computational budget on the most problematic regions.

### Mechanism 3
Combining multiple partition strategies provides more accurate collateral damage estimation than single partition approaches. The method can combine different partitioning schemes (e.g., loss-based and label-based) to create finer-grained bins. When partitions are conditionally independent, the method estimates collateral damage likelihood by factoring across partitions, requiring fewer samples to maintain accurate estimates.

## Foundational Learning

- **Catastrophic forgetting**: The phenomenon where models forget previously learned tasks during multi-stage fine-tuning. Why needed: The entire paper addresses this problem when incrementally learning new tasks while preserving old knowledge.
- **Continual learning**: The setting where the model incrementally learns new tasks while preserving old knowledge. Why needed: The paper operates in this framework to study how to maintain performance across multiple fine-tuning stages.
- **Pareto optimization**: A comparison method that evaluates the tradeoff between prior and new task performance across different mix ratios. Why needed: The evaluation metric compares methods based on their ability to balance old and new task performance.

## Architecture Onboarding

- **Component map**: Base model (f) -> Fine-tuned model (f') -> Prior dataset (Zp) -> New dataset (Zf) -> Partition strategy -> Collateral damage estimator -> Sampling scheduler
- **Critical path**: Initialize collateral damage estimates (αk = 0.5 for all bins) -> For each training iteration: Sample from prior dataset using current αk estimates -> Train on mixed batch of prior and new task samples -> Update uk and nk counts based on observed collateral damage -> Recalculate αk estimates for next iteration
- **Design tradeoffs**: More bins provide better granularity but require more samples to estimate accurately; complex partition strategies may capture better signals but increase implementation complexity; unbiased estimation requires additional inference but provides more accurate estimates
- **Failure signatures**: If prior task performance degrades despite using mix-cd, the partition strategy may not capture relevant collateral damage signals; if new task performance is significantly worse than random baseline, the prioritization may be too aggressive; if training becomes unstable, the mix ratio β may need adjustment
- **First 3 experiments**: Run with single partition strategy (e.g., loss-based only) and compare to random uniform rehearsal; test different bin granularities (e.g., 5 vs 10 bins) to find optimal balance; compare unbiased vs biased collateral damage estimation to measure impact of estimation bias

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the choice of partition granularity (number of bins K) affect the performance of mix-cd in different tasks and datasets? The paper mentions that partitions can be combined to form finer-grained partitions and discusses the trade-off between having fewer partitions versus more accurate collateral damage prediction, but doesn't systematically explore how different granularities affect performance.

- **Open Question 2**: Can the collateral damage estimation procedure be extended to handle non-stationary data distributions where the underlying task characteristics change over time? The paper assumes that the prior data distribution remains static and that cached predictions from the base model remain valid throughout fine-tuning, but doesn't address scenarios where the data distribution shifts.

- **Open Question 3**: What is the theoretical limit of how much collateral damage can be mitigated through rehearsal alone, and how does this compare to weight regularization approaches? The paper discusses how mix-cd compares to weight regularization methods like LoRA and WiSE-FT, but does not provide theoretical analysis of the fundamental limits of rehearsal-based approaches.

## Limitations

- The method requires caching prior model predictions on all prior data samples, which may be impractical for very large foundational models or datasets
- The effectiveness of mix-cd depends heavily on the quality of partition selection, and the paper doesn't provide clear guidance on choosing between different partition strategies for specific task types
- The claims about computational efficiency hinge on the assumption that collateral damage patterns remain stable enough across training iterations for accurate density estimation

## Confidence

- **High confidence**: The core algorithmic approach and its theoretical soundness
- **Medium confidence**: The empirical results showing consistent improvements across tasks
- **Low confidence**: The generalizability to extremely large-scale scenarios and different model architectures

## Next Checks

1. **Partition Stability Test**: Track how collateral damage density estimates change across training iterations to validate the stability assumption. Compare estimation accuracy when using only previously seen samples versus including additional inference passes.

2. **Scalability Assessment**: Implement mix-cd with increasingly large prior datasets and measure both memory requirements and computational overhead. Test whether the 1/3 sampling cost assumption holds at scale.

3. **Cross-Architecture Generalization**: Apply mix-cd to different foundational model architectures (e.g., GPT-style vs BERT-style) and task combinations to verify that the method's effectiveness isn't tied to specific model families.