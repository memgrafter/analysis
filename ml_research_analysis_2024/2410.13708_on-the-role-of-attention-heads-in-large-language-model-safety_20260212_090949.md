---
ver: rpa2
title: On the Role of Attention Heads in Large Language Model Safety
arxiv_id: '2410.13708'
source_url: https://arxiv.org/abs/2410.13708
tags:
- safety
- attention
- head
- heads
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the role of attention heads in large language
  model (LLM) safety. The authors propose a novel metric, Safety Head Important Score
  (Ships), to assess the importance of individual attention heads in rejecting harmful
  queries.
---

# On the Role of Attention Heads in Large Language Model Safety

## Quick Facts
- **arXiv ID**: 2410.13708
- **Source URL**: https://arxiv.org/abs/2410.13708
- **Reference count**: 40
- **Key outcome**: Ablating a single safety head increases attack success rate from 0.04 to 0.64 on Llama-2-7b-chat and from 0.27 to 0.55 on Vicuna-7b-v1.5, while only modifying 0.006% of parameters.

## Executive Summary
This paper investigates the role of attention heads in large language model safety by proposing a novel metric called Safety Head Important Score (Ships) to assess individual attention heads' contributions to rejecting harmful queries. The authors introduce a heuristic algorithm called Sahara to identify groups of safety-critical attention heads and demonstrate that ablating these heads significantly degrades model safety. Experiments show that models fine-tuned from the same base model exhibit overlapping safety heads, suggesting that safety capabilities are primarily established during pre-training and reinforced during alignment.

## Method Summary
The authors propose a novel metric called Safety Head Important Score (Ships) to evaluate the importance of individual attention heads for safety by measuring changes in rejection probability distributions when heads are ablated. They introduce the Sahara algorithm to identify groups of safety-critical attention heads and perform ablation experiments using two methods: undifferentiated attention (scaling Wq or Wk matrices by epsilon) and scaling contribution. The study evaluates safety degradation using attack success rate (ASR) on harmful query datasets including Advbench, Jailbreakbench, and Malicious Instruct.

## Key Results
- Ablating a single safety head allows aligned models to respond to 16 times more harmful queries
- Safety heads primarily function as feature extractors for harmful query detection
- Models fine-tuned from the same base model exhibit overlapping safety heads
- Only 0.006% of parameters need to be modified to significantly degrade safety

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ablating safety attention heads via undifferentiated attention increases ASR by collapsing attention weights to a uniform matrix
- Mechanism: Multiplying Wq or Wk matrices by a very small coefficient epsilon causes the softmax to approximate a uniform distribution, effectively preventing the head from extracting task-specific features
- Core assumption: The model's safety capability relies on extracting specific harmful query features through attention weights
- Evidence anchors:
  - [abstract]: "Ablating a single safety head allows aligned model (e.g., Llama-2-7b-chat) to respond to 16 times more harmful queries"
  - [section]: "Undifferentiated Attention achieves ablation by hindering the head from extracting the critical information from the input sequence"
  - [corpus]: "Focusing on Language: Revealing and Exploiting Language Attention Heads in Multilingual Large Language Models" (weak evidence for attention head role in task-specific extraction)
- Break condition: If attention weights do not degenerate to mean values with epsilon scaling, or if other mechanisms compensate for feature extraction

### Mechanism 2
- Claim: Safety heads function as feature extractors for harmful query detection
- Mechanism: Specific attention heads learn to identify harmful query patterns and route this information through the residual stream to trigger rejection responses
- Core assumption: Safety capability is mediated by specific attention heads that extract and process harmful features
- Evidence anchors:
  - [abstract]: "attention heads primarily function as feature extractors for safety"
  - [section]: "Certain safety heads within the attention mechanism are crucial for feature integration in safety tasks"
  - [corpus]: "Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models" (weak evidence for attention heads in specific pattern detection)
- Break condition: If safety capability persists when all identified safety heads are ablated, or if safety can be localized to other components

### Mechanism 3
- Claim: Pre-training establishes safety attention heads that alignment fine-tuning builds upon
- Mechanism: The base model develops attention heads that can detect harmful patterns, and alignment tuning reinforces these patterns without creating new safety heads
- Core assumption: Safety attention heads originate during pre-training and are preserved/modified during alignment
- Evidence anchors:
  - [abstract]: "models fine-tuned from the same base model exhibit overlapping safety heads"
  - [section]: "the safety effect of the attention mechanism is primarily derived from the pre-training phase"
  - [corpus]: "Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models" (weak evidence for safety mechanism persistence across fine-tuning)
- Break condition: If safety heads differ significantly between models fine-tuned from the same base, or if safety capability can be entirely reconstructed through alignment alone

## Foundational Learning

- Concept: Multi-Head Attention mechanism and self-attention computation
  - Why needed here: Understanding how attention heads process input sequences is fundamental to interpreting their role in safety
  - Quick check question: What happens to the attention distribution when you scale Wq or Wk by a very small epsilon value?

- Concept: Kullback-Leibler divergence and probability distribution comparison
  - Why needed here: Ships metric uses KL divergence to measure changes in rejection probability distributions when heads are ablated
  - Quick check question: If the rejection probability distribution shifts toward affirmative responses after ablation, what does this indicate about the ablated head?

- Concept: Principal angles and singular value decomposition for representation comparison
  - Why needed here: Generalized Ships uses SVD and principal angles to measure changes in harmful query representations at the dataset level
  - Quick check question: What does a large principal angle between UÎ¸ and UA indicate about the safety representation changes?

## Architecture Onboarding

- Component map: Multi-head attention layers (with Query/Key/Value matrices) -> Residual stream for feature propagation -> Top layer for representation extraction via SVD -> Safety evaluation pipeline (generation and ASR computation)

- Critical path: 1. Compute Ships for individual heads on harmful queries 2. Generalize Ships to dataset level using SVD of top layer activations 3. Apply Sahara algorithm to find groups of safety heads 4. Perform ablation experiments to verify safety impact 5. Analyze overlap patterns across different models

- Design tradeoffs:
  - Fine-grained ablation (head-level) vs. coarse-grained (layer/neuron-level) for precision
  - Undifferentiated attention vs. scaling contribution for ablation effectiveness
  - Dataset-level attribution vs. query-specific attribution for generalizability

- Failure signatures:
  - ASR improvement without corresponding representation changes
  - High Ships values but no safety impact when ablated
  - Inconsistent safety head identification across different datasets

- First 3 experiments:
  1. Compute Ships for all attention heads on a small harmful query dataset and verify top-scoring heads increase ASR when ablated
  2. Compare undifferentiated attention vs. scaling contribution ablation methods on the same model and dataset
  3. Test whether identified safety heads overlap between Llama-2-7b-chat and Vicuna-7b-v1.5 (both fine-tuned from Llama-2-7b)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Ships effectively identify safety-critical attention heads across different model architectures and sizes?
- Basis in paper: [explicit] The authors demonstrate the effectiveness of Ships on Llama-2-7b-chat and Vicuna-7b-v1.5, showing significant improvements in ASR after ablating identified safety heads.
- Why unresolved: The experiments were limited to two specific models, and it is unclear if Ships generalizes to other architectures or larger models.
- What evidence would resolve it: Conducting experiments on a diverse set of models, including different architectures (e.g., GPT, BERT) and sizes, would demonstrate the broader applicability of Ships.

### Open Question 2
- Question: How do safety-critical attention heads interact with other model components, such as feed-forward networks and layer normalization?
- Basis in paper: [inferred] The authors focus on multi-head attention but acknowledge that components within LLMs often have synergistic effects.
- Why unresolved: The paper does not explore the interactions between safety heads and other components, leaving their combined impact on safety unexplored.
- What evidence would resolve it: Analyzing the interactions between safety heads and other components through ablation studies or mechanistic interpretability techniques would provide insights into their combined effects.

### Open Question 3
- Question: Can the safety-critical attention heads identified by Sahara be leveraged to improve model safety without compromising helpfulness?
- Basis in paper: [explicit] The authors find that ablating safety heads significantly degrades safety while causing little helpfulness compromise, suggesting a potential trade-off.
- Why unresolved: The paper does not explore methods to enhance safety using the identified heads without negatively impacting helpfulness.
- What evidence would resolve it: Experimenting with techniques such as fine-tuning or activation engineering on the identified safety heads to improve safety while maintaining or enhancing helpfulness would address this question.

## Limitations
- The Ships metric relies on KL divergence between rejection probability distributions but doesn't fully address potential confounding factors like adversarial training data bias
- The ablation method using undifferentiated attention assumes uniform degradation across all positions, which may not reflect real-world attention patterns
- The analysis focuses on two models fine-tuned from the same base, limiting generalizability to other architectures or pre-training approaches

## Confidence
- Safety head importance for ASR degradation: High
- Safety heads as feature extractors: Medium
- Pre-training origin of safety heads: Medium
- Ships metric reliability: Low

## Next Checks
1. **Cross-architecture validation**: Test Ships metric and Sahara algorithm on models from different base architectures (e.g., GPT-3.5-turbo, Claude) to assess generalizability of safety head identification.

2. **Ablation method comparison**: Compare undifferentiated attention ablation against alternative methods like head pruning or masking to verify that observed safety degradation isn't method-specific.

3. **Temporal stability analysis**: Track Ships scores and safety head overlap across multiple fine-tuning checkpoints to determine if safety head patterns emerge early and remain stable during alignment.