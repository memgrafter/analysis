---
ver: rpa2
title: System 2 Reasoning Capabilities Are Nigh
arxiv_id: '2410.03662'
source_url: https://arxiv.org/abs/2410.03662
tags:
- arxiv
- reasoning
- learning
- system
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that neural networks are close to achieving human-like
  System 2 reasoning capabilities. It reviews the psychology of human thought processes
  and current neural reasoning approaches, concluding that the remaining steps to
  achieve System 2 reasoning are relatively small.
---

# System 2 Reasoning Capabilities Are Nigh

## Quick Facts
- arXiv ID: 2410.03662
- Source URL: https://arxiv.org/abs/2410.03662
- Authors: Scott C. Lowe
- Reference count: 18
- Primary result: Neural networks are close to achieving human-like System 2 reasoning capabilities through a pipeline of step-level feedback, verifiers, bootstrapping, and RL fine-tuning

## Executive Summary
This paper argues that neural networks are approaching human-like System 2 reasoning capabilities through a combination of existing techniques. The proposed pipeline leverages step-level feedback for training, verifiers to validate reasoning steps, bootstrapping through self-distillation, and reinforcement learning fine-tuning focused on reasoning accuracy. The key insight is that current transformer-based models already perform reasoning through hidden layer computations, even when not explicitly shown in chain-of-thought outputs. The paper contends that neural reasoning models are either already here or will be soon, as most technical hurdles have been cleared and the remaining challenges are primarily around data quality and model efficiency.

## Method Summary
The paper proposes a multi-stage pipeline to achieve System 2 reasoning in neural networks. It begins with fine-tuning pretrained language models on mathematical reasoning datasets using chain-of-thought prompting to generate reasoning traces. A verifier model is trained to classify valid reasoning steps, which is then used to filter and validate generated chain-of-thought data, creating high-quality datasets for self-distillation. The model is subsequently fine-tuned using reinforcement learning to maximize verifier confidence on reasoning steps, with on-policy training to handle incorrect reasoning steps during inference. The approach also suggests incorporating multi-modal data from video and sensorimotor sources to better capture real-world complexity.

## Key Results
- Current transformer models perform reasoning through hidden layer computations, not just chain-of-thought outputs
- The remaining steps to achieve System 2 reasoning are relatively small, primarily requiring better data quality and model efficiency
- Multi-modal models incorporating video and sensorimotor data will better capture the richness needed for robust reasoning
- Neural reasoning models are likely already here or will be soon as technical hurdles have been cleared

## Why This Works (Mechanism)
The mechanism works by leveraging the existing reasoning capabilities of transformer models while providing structured feedback and validation at each reasoning step. The verifier acts as a teacher to guide the model toward more accurate reasoning patterns, while reinforcement learning fine-tuning reinforces successful reasoning strategies. Multi-modal integration provides richer world representations that enable more robust reasoning across diverse scenarios.

## Foundational Learning

**System 1 vs System 2 Reasoning**
- Why needed: Understanding the distinction between fast, intuitive thinking (System 1) and slow, deliberate reasoning (System 2) is crucial for evaluating whether neural networks truly achieve human-like reasoning
- Quick check: Can you explain the difference between a gut reaction and a carefully worked-out solution?

**Chain-of-Thought Reasoning**
- Why needed: Chain-of-thought prompting enables models to show their reasoning process, which is essential for training verifiers and understanding model behavior
- Quick check: Can you generate a step-by-step solution to a math problem using chain-of-thought prompting?

**Verifier Models**
- Why needed: Verifiers classify valid reasoning steps, enabling quality control in the self-distillation pipeline
- Quick check: Can you train a simple classifier to distinguish between correct and incorrect reasoning steps?

**Reinforcement Learning Fine-tuning**
- Why needed: RL fine-tuning allows models to optimize for reasoning accuracy rather than just next-token prediction
- Quick check: Can you implement a basic RL algorithm to optimize model performance on a reasoning task?

## Architecture Onboarding

**Component Map**
- Pretrained model -> Chain-of-thought training -> Verifier training -> Self-distillation -> RL fine-tuning -> Multi-modal integration

**Critical Path**
The critical path involves verifier accuracy: if the verifier cannot reliably classify valid reasoning steps, the entire pipeline's effectiveness becomes questionable. The quality of the verifier directly impacts the success of self-distillation and subsequent RL fine-tuning.

**Design Tradeoffs**
The paper balances between using verifiers versus generating additional reasoning data. Verifiers provide quality control but create dependencies on their accuracy, while generating more data increases coverage but may include more errors. The optimal balance likely depends on the specific task and model scale.

**Failure Signatures**
- Verifier incorrectly classifies reasoning steps, leading to poor self-distillation quality
- Model fails to backtrack when encountering incorrect reasoning steps during inference
- Domain shift between training and inference causes performance degradation

**3 First Experiments**
1. Fine-tune a pretrained model on GSM8K with step-level labels and train a verifier to classify valid steps
2. Use the verifier to filter generated chain-of-thought data and create a high-quality dataset for self-distillation
3. Implement on-policy RL fine-tuning to maximize verifier confidence on reasoning steps

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How can we empirically verify that current transformer models are performing hidden System 2 reasoning processes rather than just producing more accurate outputs through increased sequence length and computation?
- Basis in paper: [explicit] The paper notes that recent work shows chain-of-thought gains can be matched by filler tokens, implying hidden computations in transformer layers, and compares this to human System 2 reasoning being primarily non-symbolic but projectable to language.
- Why unresolved: The paper acknowledges this is challenging due to difficulties in interpreting deep transformer representations, and there's a fundamental tension between the observed outputs and the hidden internal processes.
- What evidence would resolve it: Direct evidence would include interpretability studies showing the internal computations match human-like reasoning steps, or ablation studies demonstrating that removing certain hidden layer capabilities degrades reasoning performance beyond what chain-of-thought outputs would suggest.

**Open Question 2**
- Question: What is the optimal balance between using verifiers versus generating additional reasoning data when training reasoning models, and how does this balance change as models scale?
- Basis in paper: [explicit] The paper proposes a pipeline using verifiers to validate reasoning steps and bootstrap more data, but notes this creates a dependency on verifier accuracy and raises questions about the relative difficulty of verification versus generation.
- Why unresolved: The paper suggests this is an active area of research with multiple proposed approaches, but doesn't provide empirical comparisons or theoretical guarantees about when each approach is preferable.
- What evidence would resolve it: Systematic experiments comparing different combinations of verifier accuracy thresholds, data generation rates, and final model performance across multiple reasoning tasks would clarify the optimal balance.

**Open Question 3**
- Question: How can we design neural architectures that maintain efficient reasoning performance while scaling to handle an exponentially growing number of entities and their interactions, similar to human working memory constraints?
- Basis in paper: [explicit] The paper discusses the challenge of exponential scaling with entity interactions, compares it to human working memory limits of 7Â±2 objects, and suggests hierarchical models and memory-augmented transformers as potential solutions.
- Why unresolved: The paper notes this is an open research direction with some promising approaches but acknowledges there's still work to be done in this area, particularly in combining compactness with selective memory retention.
- What evidence would resolve it: Empirical demonstrations of reasoning models that can maintain performance while scaling to handle thousands of entities, or theoretical proofs showing that certain architectural constraints can bound the computational complexity of reasoning.

## Limitations
- The paper conflates computational processing with human-like reasoning capabilities without adequately addressing what constitutes genuine System 2 reasoning
- Heavy reliance on verifier quality creates a potential bottleneck in the pipeline
- Multi-modal integration with video and sensorimotor data remains largely speculative without concrete implementation details

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Neural networks are close to achieving human-like System 2 reasoning | Medium |
| Technical feasibility of the proposed pipeline components | High |
| Timeline claim that "neural reasoning models are either already here or will be soon" | Low |

## Next Checks
1. Implement the proposed pipeline on a mathematical reasoning benchmark (e.g., GSM8K or MATH) to empirically evaluate whether the combination of verifiers and reinforcement learning significantly improves reasoning accuracy compared to standard fine-tuning approaches.

2. Conduct controlled experiments comparing verifier-based filtering against random sampling of reasoning steps to quantify the actual contribution of the verifier component to model performance.

3. Test the model's ability to handle domain shifts by measuring performance degradation when reasoning steps deviate from training distributions, validating the on-policy training approach's effectiveness.