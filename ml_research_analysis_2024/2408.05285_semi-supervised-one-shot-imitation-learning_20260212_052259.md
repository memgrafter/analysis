---
ver: rpa2
title: Semi-Supervised One-Shot Imitation Learning
arxiv_id: '2408.05285'
source_url: https://arxiv.org/abs/2408.05285
tags:
- learning
- task
- osil
- dataset
- semi-supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces semi-supervised one-shot imitation learning
  (OSIL), addressing the challenge of requiring large numbers of paired expert demonstrations
  for OSIL training. The proposed method leverages a small labeled dataset of paired
  demonstrations alongside a large unlabeled dataset to improve label efficiency.
---

# Semi-Supervised One-Shot Imitation Learning

## Quick Facts
- arXiv ID: 2408.05285
- Source URL: https://arxiv.org/abs/2408.05285
- Authors: Philipp Wu, Kourosh Hakhamaneshi, Yuqing Du, Igor Mordatch, Aravind Rajeswaran, Pieter Abbeel
- Reference count: 9
- One-line primary result: Achieves near-equivalent OSIL performance with only 15% of labeled data using semi-supervised self-training

## Executive Summary
This paper addresses the challenge of data efficiency in one-shot imitation learning (OSIL) by introducing a semi-supervised approach that leverages a small labeled dataset alongside a large unlabeled dataset. The method employs a teacher-student self-training paradigm where a teacher encoder learns a structured embedding space that clusters similar tasks, enabling pseudo-label generation through nearest-neighbor retrieval. A student model is then trained on both labeled and pseudo-labeled data, achieving competitive performance with significantly less labeled data.

## Method Summary
The method trains a teacher encoder on a small labeled dataset using imitation and contrastive losses to learn task-specific embeddings. This teacher is then used to generate pseudo-pairs from a large unlabeled dataset by retrieving k nearest neighbors in embedding space. A student policy is trained on both the original labeled dataset and the pseudo-labeled dataset. The approach is evaluated on simulated control tasks including semantic goal navigation (MuJoCo pointmass) and sequential goal navigation (pinpad world), demonstrating significant reduction in labeled data requirements while maintaining performance.

## Key Results
- Achieved near-equivalent performance with only 15% of labeled data in semantic goal navigation task
- Approached full performance with just 5% of labeled data in sequential goal navigation task
- High trajectory retrieval scores (>95%) confirmed effective pseudo-label generation
- Iterative self-training showed mixed results with no significant gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Teacher encoder learns structured embedding space clustering trajectories by semantic task
- Mechanism: Contrastive InfoNCE loss pushes embeddings of same-task trajectories closer while separating different tasks
- Core assumption: Small labeled dataset is diverse enough to expose meaningful task variations
- Evidence anchors: Abstract mentions structured embedding space despite low task success; Section 4.1 details contrastive loss for task clustering

### Mechanism 2
- Claim: Pseudo-labels from nearest-neighbor retrieval are sufficiently accurate
- Mechanism: k-NN retrieval in teacher's embedding space pairs trajectories from unlabeled dataset
- Core assumption: Retrieval accuracy remains high (measured by TR score)
- Evidence anchors: Abstract describes self-generating pairings; Section 5.2 defines retrieval accuracy; Section 5.3 shows consistent high retrieval scores

### Mechanism 3
- Claim: Iterative self-training doesn't significantly improve performance
- Mechanism: Student-as-teacher paradigm could refine pseudo-labels through additional iterations
- Core assumption: Student's embedding space is at least as good as teacher's
- Evidence anchors: Section A.2 shows mixed results with no significant gains from additional iterations

## Foundational Learning

- Concept: Contrastive learning (InfoNCE loss)
  - Why needed here: Creates task-specific clustering in embedding space for accurate pseudo-label generation
  - Quick check question: What role does the InfoNCE loss play in teacher training and why is it crucial for semi-supervised OSIL?

- Concept: k-Nearest Neighbors (k-NN) retrieval
  - Why needed here: Generates pseudo-pairs from unlabeled dataset by finding trajectories with similar task embeddings
  - Quick check question: How does k-NN retrieval in the teacher's embedding space enable pseudo-label generation?

- Concept: Teacher-student self-training paradigm
  - Why needed here: Teacher generates pseudo-labels for unlabeled data while student learns from both labeled and pseudo-labeled data
  - Quick check question: What is the purpose of the teacher-student self-training paradigm in semi-supervised OSIL?

## Architecture Onboarding

- Component map: Teacher encoder -> Embedding space -> k-NN retrieval -> Pseudo-labels -> Student policy
- Critical path:
  1. Train teacher encoder on labeled dataset (imitation + contrastive loss)
  2. Generate pseudo-labels via k-NN retrieval in embedding space
  3. Train student on labeled + pseudo-labeled datasets
  4. (Optional) Iterate with student as new teacher

- Design tradeoffs:
  - Final frame vs. full trajectory encoder: Simpler/faster but only works if final frame specifies task
  - Value of k: Larger k gives more pairs but may introduce noise
  - Iterative self-training: Potentially refines pseudo-labels but may overfit to noise

- Failure signatures:
  - Low trajectory retrieval scores: Poor embedding structure makes pseudo-labeling ineffective
  - Student worse than teacher: Pseudo-labels too noisy or student overfits to them
  - No improvement: Poor retrieval or unlabeled dataset lacks useful variations

- First 3 experiments:
  1. Train teacher with only imitation loss (no contrastive) to confirm contrastive loss necessity
  2. Compare student trained on labeled only vs. labeled + pseudo-labeled data
  3. Vary k values (10, 50, 100) and observe effects on performance and retrieval scores

## Open Questions the Paper Calls Out

- Question: Optimal k value for pseudo-labeling across different task complexities
  - Basis: Mixed results for different k values suggest significance for more difficult tasks
  - Evidence needed: Systematic experiments varying k across wider task complexities

- Question: Quality degradation of embedding space below 5% labeled data
  - Basis: Tests down to 5% but doesn't explore lower bounds
  - Evidence needed: Experiments with 1-3% labeled data and embedding quality analysis

- Question: Robustness to noisy or incorrect expert demonstrations
  - Basis: Assumes clean demonstrations but doesn't address real-world noise
  - Evidence needed: Experiments introducing varying noise levels into unlabeled dataset

## Limitations

- The approach relies heavily on contrastive learning creating sufficiently structured embedding spaces, which may not generalize to more complex or visually diverse environments
- The "scripted policy" for data collection limits real-world applicability and introduces uncertainty about performance in less controlled settings
- Limited experimentation with iterative self-training (only one additional iteration tested) leaves open questions about potential benefits in more challenging tasks

## Confidence

- High confidence: The semi-supervised framework is sound and well-motivated by established self-training literature
- Medium confidence: Empirical results are convincing for the specific environments tested (MuJoCo pointmass and pinpad world)
- Low confidence: The claim that iterative self-training doesn't improve performance is based on limited experimentation

## Next Checks

1. Test the approach on a more visually diverse environment requiring full trajectory encoding rather than final-frame task specification
2. Conduct ablation studies varying labeled data proportions (5%, 10%, 15%, 25%) to better understand scaling relationships
3. Experiment with multiple iterations of self-training (2-3 cycles) to rigorously test whether iterative refinement provides benefits in more challenging task distributions