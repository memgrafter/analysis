---
ver: rpa2
title: Safe Reinforcement Learning with Learned Non-Markovian Safety Constraints
arxiv_id: '2405.03005'
source_url: https://arxiv.org/abs/2405.03005
tags:
- safety
- safe
- learning
- non-markovian
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles safe RL with unknown, non-Markovian constraints
  by inferring them from labeled trajectory data. The key novelty is a recurrent safety
  model that outputs per-timestep log-probabilities, enabling safe RL via an inference-based
  method called SafeSAC-H.
---

# Safe Reinforcement Learning with Learned Non-Markovian Safety Constraints

## Quick Facts
- arXiv ID: 2405.03005
- Source URL: https://arxiv.org/abs/2405.03005
- Authors: Siow Meng Low; Akshat Kumar
- Reference count: 40
- Primary result: SafeSAC-H achieves high constraint compliance on MuJoCo, Bullet Safety Gym, and RDDL Gym tasks while maintaining competitive reward.

## Executive Summary
This paper tackles safe RL with unknown, non-Markovian constraints by inferring them from labeled trajectory data. The key novelty is a recurrent safety model that outputs per-timestep log-probabilities, enabling safe RL via an inference-based method called SafeSAC-H. A dual formulation is used to automatically tune the trade-off between reward and constraint satisfaction. Experiments show SafeSAC-H achieves high constraint compliance on MuJoCo, Bullet Safety Gym, and RDDL Gym tasks while maintaining competitive reward. Using the safety model's hidden state improves safety, and automatic Lagrange multiplier tuning is critical for success.

## Method Summary
The method trains a safety model (RNN with GRU) to predict trajectory safety from labeled data, then uses SafeSAC-H algorithm combining reward maximization with safety compliance via dual formulation and automatic Lagrange multiplier adjustment. The safety model outputs per-timestep log-probabilities which are summed to get trajectory-level safety. The RL agent optimizes a safe policy using these predictions while automatically adapting the tradeoff coefficient λ between reward and safety.

## Key Results
- SafeSAC-H achieves 97-99% constraint compliance on MuJoCo tasks while maintaining competitive reward compared to SAC
- Including safety model's hidden state in policy and critics significantly improves safety compliance
- Automatic Lagrange multiplier adjustment outperforms fixed λ baselines in balancing reward and constraint satisfaction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The recurrent safety model encodes trajectory history into hidden states, enabling credit assignment for non-Markovian constraints.
- **Mechanism**: GRU-based RNN processes state-action trajectories and maintains hidden state `h_t` summarizing history. The model outputs per-timestep log-probability `log P(ψ_t = 1|s_t, h_t, a_t)`, which when summed gives trajectory-level safety probability.
- **Core assumption**: Safety labels depend on trajectory segments, not just individual state-action pairs, and the history can be compressed into fixed-dimensional hidden states.
- **Evidence anchors**:
  - [abstract]: "design a safety model that specifically performs credit assignment to assess contributions of partial state-action trajectories on safety"
  - [section]: "The novelty of this architecture is that it decomposes the predicted log-probability score... into a sum of multiple log-probability scores, one for each timestep"
  - [corpus]: Weak - no directly comparable mechanism found
- **Break condition**: If safety depends on long-term dependencies beyond GRU's memory capacity, or if history cannot be meaningfully compressed.

### Mechanism 2
- **Claim**: The RL-as-inference reformulation allows policy optimization while respecting learned non-Markovian constraints.
- **Mechanism**: Two optimality variables Ot (reward) and ψt (safety) create a probabilistic graphical model. Variational inference maximizes the negative KL divergence between optimal and policy-induced trajectory distributions, yielding a safe RL objective.
- **Core assumption**: The safety model's output probabilities can be treated as soft constraints in the inference framework.
- **Evidence anchors**:
  - [abstract]: "using RL-as-inference strategy we derive an effective algorithm for optimizing a safe policy using the learned safety model"
  - [section]: "To solve the CRL problem in (4), we derive an iterative policy improvement algorithm using probabilistic inference"
  - [corpus]: Weak - inference-based safe RL exists but not with learned non-Markovian constraints
- **Break condition**: If the variational approximation breaks down or the inference distribution cannot represent the true constraint satisfaction.

### Mechanism 3
- **Claim**: Automatic Lagrange multiplier adjustment via dual formulation ensures constraint compliance without manual tuning.
- **Mechanism**: The non-Markovian CRL constraint is rewritten using Jensen's inequality to get a per-timestep bound. The dual problem is solved by gradient descent on λ using on-policy samples approximated from replay buffer.
- **Core assumption**: The lower bound constraint is tight enough to guide λ learning, and the gradient estimate is accurate.
- **Evidence anchors**:
  - [abstract]: "we devise a method to dynamically adapt the tradeoff coefficient between reward maximization and safety compliance"
  - [section]: "we propose a method to automatically learn the appropriate value of λ during safe RL"
  - [corpus]: Weak - automatic Lagrange tuning exists but not in this non-Markovian context
- **Break condition**: If the lower bound is too loose, leading to poor λ estimates, or if the gradient estimate variance is too high.

## Foundational Learning

- **Concept**: Markov Decision Processes and Constrained MDPs
  - Why needed here: The problem formulation starts from CMDPs and extends them to handle non-Markovian constraints
  - Quick check question: What is the key difference between MDP and CMDP formulations?

- **Concept**: Variational Inference and Probabilistic Graphical Models
  - Why needed here: The safe RL algorithm is derived by reformulating CRL as variational inference over a PGM with optimality variables
  - Quick check question: How does the introduction of optimality variables enable the inference-based approach?

- **Concept**: Recurrent Neural Networks and Credit Assignment
  - Why needed here: The safety model uses GRU to assign credit to trajectory segments for safety prediction
  - Quick check question: Why is a recurrent architecture necessary for learning non-Markovian safety constraints?

## Architecture Onboarding

- **Component map**: Safety model (GRU + decoder) → RL agent (SafeSAC-H) with two critics (QR, Qψ) + automatic λ tuner → enriched replay buffer
- **Critical path**: Train safety model on labeled trajectories → Initialize SafeSAC-H with safety model → Run RL loop with λ adaptation → Evaluate constraint compliance
- **Design tradeoffs**: 
  - Fixed vs. adaptive λ (manual tuning vs. automatic)
  - Including vs. excluding hidden state h in policy and critics (history awareness vs. simplicity)
  - Using original vs. lower-bound constraint for λ learning (accuracy vs. tractability)
- **Failure signatures**:
  - λ diverging or oscillating → Check gradient estimation and learning rate
  - Poor constraint compliance despite high safety model accuracy → Verify history encoding and policy dependence on h
  - Training instability → Check critic learning rates and entropy coefficient
- **First 3 experiments**:
  1. Train safety model on synthetic labeled trajectories with known non-Markovian patterns, evaluate classification accuracy
  2. Run SafeSAC-H on a simple environment (e.g., GridWorld) with hand-designed non-Markovian constraints, check if h improves performance
  3. Compare SafeSAC-H with fixed λ vs. adaptive λ on a continuous control task, measure constraint satisfaction and reward tradeoff

## Open Questions the Paper Calls Out

- **Question**: How does the performance of SafeSAC-H scale with the complexity of the non-Markovian safety constraints (e.g., longer dependency windows, more intricate patterns)?
  - Basis in paper: [inferred] The paper mentions "sophisticated non-Markovian safety constraints" in the abstract and discusses the ability to handle "variable-length trajectory segments" but does not systematically evaluate how constraint complexity affects performance.
  - Why unresolved: The experiments use relatively simple non-Markovian constraints (e.g., moving average velocity thresholds, zone occupancy over short time windows) and do not explore a range of constraint complexities.
  - What evidence would resolve it: Experiments varying the length of dependency windows, the number of variables involved in the constraint, and the complexity of the patterns that trigger safety violations, with systematic evaluation of SafeSAC-H's performance across this spectrum.

- **Question**: What is the impact of the quality and diversity of the labeled safety dataset on the safety model's accuracy and the subsequent performance of SafeSAC-H?
  - Basis in paper: [explicit] The paper states "The dataset is also further augmented by labeling sub-trajectories contained within these 400K trajectories" and mentions the safety model's high accuracy (97-99%), but does not explore how dataset characteristics affect performance.
  - Why unresolved: The experiments use a fixed dataset size and do not investigate how variations in dataset size, diversity of scenarios included, or labeling noise affect the safety model and SafeSAC-H's ability to learn safe policies.
  - What evidence would resolve it: Experiments systematically varying the size of the labeled dataset, the diversity of scenarios included, and the amount of labeling noise, with evaluation of the safety model's accuracy and SafeSAC-H's constraint compliance under each condition.

- **Question**: How does SafeSAC-H's performance compare to other safe RL methods when safety constraints can be expressed in a Markovian form?
  - Basis in paper: [explicit] The paper focuses on non-Markovian constraints and states "To the best of our knowledge, existing safe RL algorithms are unable to handle unknown non-Markovian constraints," but does not compare to Markovian safe RL methods.
  - Why unresolved: The paper does not provide empirical comparisons between SafeSAC-H and traditional safe RL methods (e.g., CPO, Lagrangian methods) on tasks where constraints can be expressed in a Markovian form, leaving the relative performance in this regime unknown.
  - What evidence would resolve it: Experiments applying SafeSAC-H and traditional safe RL methods (e.g., CPO, Lagrangian methods) to tasks with Markovian constraints, comparing constraint compliance, reward, and sample efficiency.

## Limitations
- Reliance on trajectory labeling quality - errors in safety annotation directly impact the safety model's ability to capture non-Markovian patterns
- Effectiveness depends on assumption that trajectory histories can be compressed into fixed-dimensional hidden states, which may not hold for very long-term dependencies
- Requires significant hyperparameter tuning for the automatic Lagrange multiplier adjustment to work properly

## Confidence
- **High confidence**: Safety model's ability to learn from trajectory data and improve constraint compliance when history is included (supported by experimental results showing SafeSAC-H outperforms SafeSAC-NoH)
- **Medium confidence**: Automatic Lagrange multiplier tuning, as it relies on a lower-bound approximation that may introduce bias
- **Low confidence**: Generalization to environments with very long-term non-Markovian dependencies, given the fixed GRU architecture

## Next Checks
1. Test the safety model's robustness to noisy or inconsistent safety labels by introducing label corruption and measuring the degradation in constraint compliance
2. Evaluate the method on environments with known non-Markovian constraints of varying temporal scales to assess the GRU's capacity to capture long-term dependencies
3. Compare the automatic λ adjustment with a grid search baseline to quantify the benefits and potential drawbacks of the dual formulation approach