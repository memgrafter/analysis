---
ver: rpa2
title: 'Previously on the Stories: Recap Snippet Identification for Story Reading'
arxiv_id: '2402.07271'
source_url: https://arxiv.org/abs/2402.07271
tags:
- snippet
- snippets
- recap
- target
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the novel task of Recap Snippet Identification,
  which aims to find story snippets from previous context that are directly plot-related
  to the current snippet, helping readers recall important events. To support this
  task, the authors present RECIDENT, a dataset containing 2 classic novels, 1 Wuxia
  novel, 1 TV series, and 1 anime, with human annotations on which snippets serve
  as recaps.
---

# Previously on the Stories: Recap Snippet Identification for Story Reading

## Quick Facts
- **arXiv ID:** 2402.07271
- **Source URL:** https://arxiv.org/abs/2402.07271
- **Reference count:** 27
- **Key outcome:** Introduces RECIDENT dataset and recap snippet identification task; shows fine-tuning LLMs or using Line2Note training achieves best performance but with large gap to human performance.

## Executive Summary
This paper introduces the novel task of Recap Snippet Identification, which aims to find story snippets from previous context that are directly plot-related to the current snippet, helping readers recall important events. To support this task, the authors present RECIDENT, a dataset containing 2 classic novels, 1 Wuxia novel, 1 TV series, and 1 anime, with human annotations on which snippets serve as recaps. The task is challenging for PLMs, LLMs, and proposed methods, requiring a deep understanding of plot correlation. Results show that fine-tuning LLMs with supervised data or using unsupervised Line2Note training can achieve the best performance, but there is still a large gap compared to human performance.

## Method Summary
The paper introduces RECIDENT, a dataset containing 2 classic novels, 1 Wuxia novel, 1 TV series, and 1 anime with human annotations identifying recap snippets. The task requires identifying previous story snippets that are plot-relevant to a current target snippet. The authors propose several methods including zero-shot approaches (RoBERTa, SBERT, ChatGPT Listwise), fine-tuning approaches (supervised fine-tuning of various models, unsupervised Line2Note training), and a pipeline system (SBERT + Line2Note + ChatGPT Pairwise). The Line2Note method leverages reader notes as a bridge to connect snippets discussing similar plot events.

## Key Results
- Fine-tuning LLMs with supervised data or using unsupervised Line2Note training achieves the best performance on recap snippet identification
- There remains a large performance gap between current methods and human performance
- Character-based filtering and LLM pairwise prompting can significantly improve identification accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Line2Note training can build a bridge between story snippets and plot-relevant information by leveraging reader notes.
- **Mechanism:** Reader notes attached to story snippets often comment on the same plot events. By training a model to align snippet embeddings with note embeddings when they discuss the same event, the model learns to associate snippets with their plot context.
- **Core assumption:** Reader notes contain meaningful plot-related information and are attached to story snippets discussing similar events.
- **Evidence anchors:**
  - [abstract] "we designed an auxiliary unsupervised training called Line2Note training, which shows that easily obtained resources like reader notes can be a bridge to connect two snippets with the plot associations."
  - [section 5.2] "To this end, we propose a Line2Note Learning method... notes can be a bridge to connect two story snippets if the notes tagged on them talk about a similar plot"
- **Break condition:** Reader notes are sparse, irrelevant to the plot, or inconsistently attached to story snippets.

### Mechanism 2
- **Claim:** Character-based filtering can significantly reduce noise in recap snippet identification by focusing on snippets containing relevant characters.
- **Mechanism:** Characters are the primary drivers of plot. By identifying characters in the target snippet and filtering candidate snippets to only those containing those characters, the search space for relevant recaps is narrowed.
- **Core assumption:** The appearance of characters in both the target and candidate snippets is a strong indicator of plot relevance.
- **Evidence anchors:**
  - [abstract] "appearing characters can provide some information for recap identification."
  - [section 5] "We claim that characters are the main carriers and prompters of the plot. Therefore, appearing characters can provide some information for recap identification."
- **Break condition:** The target snippet contains characters who appear in many unrelated contexts, or the recap snippet focuses on a different set of characters relevant to the plot.

### Mechanism 3
- **Claim:** Pairwise prompting with LLMs can effectively identify recap snippets by analyzing the causal and temporal relationship between snippets.
- **Mechanism:** For each candidate snippet, the LLM is prompted to analyze whether it causally or temporally leads to the target snippet. By ranking the identified recaps by distance, the most relevant are selected.
- **Core assumption:** LLMs can understand the nuanced causal and temporal relationships between story snippets.
- **Evidence anchors:**
  - [abstract] "ChatGPT with Pairwise Prompts... can achieve good performance as other models."
  - [section 5.1] "For each candidate snippet of a target snippet, LLM is requested to analyze whether it qualifies as a recap snippet for the target snippet."
- **Break condition:** The LLM struggles with long input contexts, the cost of multiple prompts is prohibitive, or the LLM fails to grasp the nuanced plot relationships.

## Foundational Learning

- **Concept:** Plot correlation and temporal/causal relationships between story snippets.
  - **Why needed here:** The task requires understanding how events in one snippet lead to or are related to events in another, beyond simple textual similarity.
  - **Quick check question:** Given two story snippets, can you identify the causal or temporal relationship between them (e.g., "Event A causes Event B")?

- **Concept:** Character identification and their role in driving plot.
  - **Why needed here:** Characters are the primary drivers of plot, and their appearance in snippets is a key indicator of relevance.
  - **Quick check question:** Given a story snippet, can you identify the main characters and their roles in the plot?

- **Concept:** Reader note analysis and its relation to story content.
  - **Why needed here:** Reader notes are used as a bridge to connect snippets discussing similar plot events in the Line2Note training.
  - **Quick check question:** Given a reader note and a story snippet, can you determine if the note discusses the same plot event as the snippet?

## Architecture Onboarding

- **Component map:** Data preprocessing -> Line2Note training -> Character filtering -> LLM inference -> Recap snippet selection
- **Critical path:** Data preprocessing → Line2Note training → Character filtering → LLM inference → Recap snippet selection
- **Design tradeoffs:** The use of reader notes for unsupervised training reduces the need for labeled data but relies on the availability and quality of notes. Character filtering reduces noise but may miss relevant snippets without the target characters. LLM prompting is powerful but costly.
- **Failure signatures:** Poor performance on identifying recaps for snippets with sparse character mentions, failure to capture nuanced plot relationships, high computational cost for LLM prompting.
- **First 3 experiments:**
  1. Evaluate the impact of character filtering on recall and precision.
  2. Compare the performance of Line2Note training with supervised fine-tuning on a small labeled dataset.
  3. Test the effectiveness of different LLM prompting strategies (listwise vs. pairwise) on a sample of the dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal number of reasoning hops to consider when identifying recap snippets, and how does this impact the quality and relevance of the recaps?
- **Basis in paper:** [explicit] The paper mentions that some snippets can be plot-related to the target snippet by several reasoning hops, and annotators found it difficult to reach an agreement on the proper number of reasoning hops.
- **Why unresolved:** The paper does not provide a clear answer on the optimal number of reasoning hops, and it is a subjective decision that may vary depending on the story and the annotators' preferences.
- **What evidence would resolve it:** Conducting experiments with different numbers of reasoning hops and evaluating the quality and relevance of the recaps using human judgment or automated metrics could help determine the optimal number.

### Open Question 2
- **Question:** How does the performance of the Line2Note training method compare to other unsupervised training methods, such as contrastive learning or masked language modeling, for recap snippet identification?
- **Basis in paper:** [inferred] The paper proposes the Line2Note training method and shows that it improves the performance of RoBERTa and SBERT for recap snippet identification. However, it does not compare Line2Note to other unsupervised training methods.
- **Why unresolved:** The paper does not provide a comprehensive comparison of different unsupervised training methods for recap snippet identification, making it unclear how Line2Note performs relative to other approaches.
- **What evidence would resolve it:** Conducting experiments comparing Line2Note to other unsupervised training methods, such as contrastive learning or masked language modeling, using the same evaluation dataset and metrics would provide insights into their relative performance.

### Open Question 3
- **Question:** How does the performance of the recap snippet identification models vary across different genres of stories, such as novels, TV series, and anime, and what factors contribute to these differences?
- **Basis in paper:** [explicit] The paper presents a dataset (RECIDENT) that includes three novels and two TV productions (GOT and AOT), and it shows that the performance of the models varies across these different genres.
- **Why unresolved:** The paper does not provide a detailed analysis of the factors that contribute to the performance differences across genres, such as the narrative structure, character development, or event density.
- **What evidence would resolve it:** Conducting experiments that systematically vary the genres of stories and analyze the factors that contribute to the performance differences, such as the narrative structure, character development, or event density, would provide insights into how the models perform across different genres.

## Limitations
- The RECIDENT dataset is relatively small, comprising only 5 literary works, which may not fully capture the diversity of plot structures and narrative styles across different genres and mediums.
- The evaluation metrics (Recall@5, Precision@5, F1@5) may not adequately reflect the practical utility of identified recap snippets for readers, as the quality and relevance of top-5 results could vary significantly.
- The unsupervised Line2Note training approach relies heavily on the availability and quality of reader notes, which may not be consistently present across all literary works.

## Confidence

**High Confidence Claims:**
- The Recap Snippet Identification task is novel and addresses a genuine need for readers to recall important plot events
- Fine-tuning LLMs with supervised data or using unsupervised Line2Note training can achieve better performance than zero-shot approaches
- There remains a significant performance gap between current methods and human performance on this task

**Medium Confidence Claims:**
- Reader notes can serve as an effective bridge to connect story snippets with plot associations
- Character appearance in both target and candidate snippets is a strong indicator of plot relevance
- Pairwise prompting with LLMs can effectively identify recap snippets by analyzing causal and temporal relationships

**Low Confidence Claims:**
- The performance of specific methods (RoBERTa, SBERT, LLaMA2, InternLM2) on this task
- The generalizability of findings across different literary genres and mediums
- The practical utility of identified recap snippets for readers based on the evaluation metrics used

## Next Checks

1. **Dataset Expansion Validation:** Validate the robustness of the proposed methods by testing them on an expanded dataset that includes a wider variety of literary genres, mediums, and plot structures. This will help assess the generalizability of the findings beyond the current 5 works.

2. **Reader Study for Practical Utility:** Conduct a reader study to evaluate the practical utility of identified recap snippets. This could involve asking readers to use the identified snippets to recall plot events and then assessing their comprehension and recall accuracy, providing more direct evidence of the method's effectiveness.

3. **Alternative Filtering Mechanisms:** Explore alternative filtering mechanisms beyond character-based filtering, such as event-based filtering or semantic similarity measures, to determine if they can improve the identification of relevant recap snippets, especially in cases where character-based filtering may be insufficient.