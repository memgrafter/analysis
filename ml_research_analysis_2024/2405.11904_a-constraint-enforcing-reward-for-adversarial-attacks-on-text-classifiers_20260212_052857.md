---
ver: rpa2
title: A Constraint-Enforcing Reward for Adversarial Attacks on Text Classifiers
arxiv_id: '2405.11904'
source_url: https://arxiv.org/abs/2405.11904
tags:
- adversarial
- have
- examples
- original
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a reinforcement learning approach to generate
  adversarial examples for text classifiers. Instead of using traditional token-modification
  attacks, it fine-tunes a pre-trained paraphrase model with a constraint-enforcing
  reward that promotes adversarial examples while preserving semantic meaning and
  linguistic acceptability.
---

# A Constraint-Enforcing Reward for Adversarial Attacks on Text Classifiers

## Quick Facts
- arXiv ID: 2405.11904
- Source URL: https://arxiv.org/abs/2405.11904
- Authors: Tom Roth; Inigo Jauregi Unanue; Alsharif Abuadbba; Massimo Piccardi
- Reference count: 13
- This paper introduces a reinforcement learning approach to generate adversarial examples for text classifiers using a constraint-enforcing reward function.

## Executive Summary
This paper proposes a novel approach to generating adversarial examples for text classifiers by fine-tuning a pre-trained paraphrase model using reinforcement learning with a constraint-enforcing reward. The method addresses limitations of token-modification attacks by generating more diverse and semantically coherent adversarial examples. Experiments on two sentiment analysis datasets show significant improvements over both the original paraphrase model and established adversarial attack methods, achieving up to 88% attack success rate while maintaining linguistic acceptability.

## Method Summary
The approach fine-tunes a pre-trained T5 paraphrase model using REINFORCE with baseline algorithm and a custom reward function that encourages adversarial success while enforcing constraints on semantic consistency, grammaticality, and label invariance. The reward function includes a KL divergence term to maintain coherence with the pre-trained distribution. The model is trained on examples with up to 32 tokens and evaluated using beam search decoding to generate diverse adversarial candidates.

## Key Results
- Achieved 88% attack success rate on the Financial PhraseBank dataset
- Generated an average of 27 successful adversarial examples per original input
- Outperformed both the original paraphrase model and established adversarial attack methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reinforcement learning with a constraint-enforcing reward can train a paraphrase model to generate adversarial examples while maintaining linguistic acceptability.
- Mechanism: The model is fine-tuned using REINFORCE with baseline, optimizing a reward function that encourages label changes in the victim model while penalizing violations of constraints like semantic consistency and grammaticality. The KL divergence term ensures the generated text remains coherent by limiting drift from the pre-trained paraphrase distribution.
- Core assumption: The reward function and constraints accurately capture the trade-off between adversarial effectiveness and text quality.
- Evidence anchors:
  - [abstract] "propose a constraint-enforcing reward that promotes the generation of valid adversarial examples"
  - [section 3.3] "paraphrase reward to use in (4) is: r (x, x') = max(0, min(α, η δ(x, x') V(x, x'))"
  - [corpus] Weak evidence - no explicit comparison of constraint types or their effectiveness is provided in the corpus
- Break condition: If the reward function fails to balance adversarial success with constraint satisfaction, the model may generate either ineffective attacks or ungrammatical text.

### Mechanism 2
- Claim: Beam search decoding during evaluation generates more diverse and successful adversarial examples than sampling or nucleus sampling.
- Mechanism: Beam search explores multiple candidate sequences in parallel, maintaining a set of top-k hypotheses. This allows the model to find adversarial examples that might be missed by sampling methods which can get stuck in local optima or generate less diverse outputs.
- Core assumption: The beam search width (set to 48) is sufficient to capture the diversity of successful adversarial examples.
- Evidence anchors:
  - [section 5.1] "beam search as the decoding method has reported the highest success rates on both datasets (61.9% on Rotten Tomatoes and 82.0% on Financial PhraseBank)"
  - [section 7.2] "Beam search and low-diversity beam search perform best, on average"
  - [corpus] Weak evidence - the corpus does not provide comparative analysis of different decoding strategies
- Break condition: If the beam search width is too small, it may miss diverse successful adversarial examples. If too large, it may become computationally expensive without significant gains.

### Mechanism 3
- Claim: The per-example baseline reward stabilizes training by providing a reference point that is highly correlated with the potential for generating adversarial examples for a given input.
- Mechanism: The baseline is updated during validation as the average reward of the set of adversarial example candidates generated for each original example. This adaptive baseline helps the REINFORCE algorithm by reducing variance and focusing updates on examples where the model has higher potential for success.
- Core assumption: The average reward of candidate sets is a good proxy for the difficulty of generating adversarial examples for a particular input.
- Evidence anchors:
  - [section 3.4] "we use a per-example baseline, b(x), and define it as the average reward of the set of adversarial example candidates generated for each x in the training set"
  - [section 3.2] "provided b is highly correlated with r"
  - [corpus] Weak evidence - no explicit validation of the baseline's correlation with success rates is provided in the corpus
- Break condition: If the baseline becomes poorly correlated with success rates (e.g., due to distributional shift), it may destabilize training or prevent the model from improving on difficult examples.

## Foundational Learning

- Concept: Reinforcement learning with policy gradients
  - Why needed here: The task requires optimizing a model to generate text that maximizes a non-differentiable reward (adversarial success + constraint satisfaction), which cannot be achieved through standard supervised learning.
  - Quick check question: What is the key difference between REINFORCE and standard supervised learning when training a text generation model?

- Concept: KL divergence regularization
  - Why needed here: To prevent the fine-tuned model from drifting too far from the pre-trained paraphrase distribution, which could lead to ungrammatical or incoherent text.
  - Quick check question: Why is it important to include a KL divergence term when fine-tuning a pre-trained language model with reinforcement learning?

- Concept: Adversarial example constraints
  - Why needed here: To ensure generated text maintains semantic meaning, grammaticality, and label invariance while successfully attacking the classifier.
  - Quick check question: What are the three main types of constraints used to validate adversarial examples in this approach?

## Architecture Onboarding

- Component map:
  Pre-trained T5 paraphrase model -> Victim classifier -> Constraint validation models -> REINFORCE optimizer with baseline -> KL divergence regularizer

- Critical path:
  1. Generate paraphrase candidates using fine-tuned model
  2. Validate candidates against constraints
  3. Compute reward based on adversarial success and constraint satisfaction
  4. Update model parameters using REINFORCE with baseline and KL regularization
  5. Repeat until convergence

- Design tradeoffs:
  - Beam search width vs. diversity: Larger beams find more successful examples but may reduce diversity
  - KL penalty coefficient vs. model drift: Higher penalties maintain coherence but may limit adversarial effectiveness
  - Temperature during training vs. exploration: Higher temperatures increase diversity but may reduce valid sentence generation

- Failure signatures:
  - High variance across random seeds (indicates unstable training)
  - Low attack success rate despite long training (indicates poor reward shaping)
  - High perplexity in generated text (indicates loss of linguistic coherence)
  - Many candidates failing constraint validation (indicates poor constraint integration)

- First 3 experiments:
  1. Compare attack success rates of fine-tuned vs. original paraphrase model on a small validation set
  2. Test different decoding methods (sampling vs. beam search) on the same fine-tuned model
  3. Vary the KL penalty coefficient to find the optimal balance between coherence and adversarial effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the fine-tuned model perform on datasets with longer sequences beyond the 32-token limit used in the experiments?
- Basis in paper: [inferred] The paper mentions that the model was tested on examples with up to 32 tokens, and that extending to 48 tokens showed no noticeable difference, but does not explore performance on significantly longer sequences.
- Why unresolved: The paper does not provide results or analysis for datasets with longer sequences, leaving uncertainty about the model's scalability.
- What evidence would resolve it: Experiments showing attack success rates and diversity metrics on datasets with sequences longer than 48 tokens.

### Open Question 2
- Question: What is the impact of using different pre-trained seq2seq models, such as those for style transfer or dialogue generation, on the effectiveness of adversarial attacks?
- Basis in paper: [explicit] The paper suggests experimenting with different pre-trained seq2seq models as future work but does not provide any results or comparisons.
- Why unresolved: The paper only uses a T5 transformer as the base model and does not explore the potential benefits or drawbacks of other models.
- What evidence would resolve it: Comparative studies showing attack success rates, diversity, and fluency metrics using different pre-trained seq2seq models.

### Open Question 3
- Question: How do human evaluators perceive the quality and naturalness of adversarial examples generated by the fine-tuned model compared to those generated by token-modification attacks?
- Basis in paper: [explicit] The paper includes a human validation section that compares label invariance but does not address the overall quality and naturalness of the generated text.
- Why unresolved: The human validation focuses solely on label invariance and does not assess the fluency or naturalness of the adversarial examples.
- What evidence would resolve it: Human evaluations specifically designed to rate the fluency, naturalness, and overall quality of adversarial examples generated by different methods.

## Limitations

- Dataset size and representativeness: The experiments rely on two sentiment analysis datasets with only 1,024 test examples each, which may not capture the full diversity of adversarial examples needed to validate the approach's robustness.
- Constraint validation assumptions: The paper assumes that NLI, STS, and acceptability constraints adequately capture semantic preservation and linguistic quality, but these may not perfectly align with human judgments.
- Reproducibility challenges: The paper provides limited implementation details for critical components like the constraint-enforcing reward function and KL divergence regularization parameters.

## Confidence

- High confidence: The claim that reinforcement learning can be used to fine-tune a paraphrase model for adversarial example generation.
- Medium confidence: The claim that the constraint-enforcing reward function effectively balances adversarial success with semantic preservation and linguistic acceptability.
- Medium confidence: The claim that beam search decoding generates more diverse and successful adversarial examples than sampling methods.

## Next Checks

1. Conduct human evaluation studies to assess whether the generated adversarial examples maintain semantic meaning and grammaticality as perceived by human annotators.
2. Test the approach on a wider variety of classifier architectures and NLP tasks beyond sentiment analysis.
3. Perform ablation studies on the constraint-enforcing reward components to determine the relative importance of each constraint and the KL divergence regularization.