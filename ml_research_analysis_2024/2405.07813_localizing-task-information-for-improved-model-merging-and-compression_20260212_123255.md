---
ver: rpa2
title: Localizing Task Information for Improved Model Merging and Compression
arxiv_id: '2405.07813'
source_url: https://arxiv.org/abs/2405.07813
tags:
- task
- tasks
- merging
- performance
- consensus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of performance degradation in model
  merging and compression due to task interference. The core method, TALL-masks, constructs
  binary masks to localize task-specific information in the multi-task vector, allowing
  for efficient model merging and compression.
---

# Localizing Task Information for Improved Model Merging and Compression

## Quick Facts
- arXiv ID: 2405.07813
- Source URL: https://arxiv.org/abs/2405.07813
- Reference count: 40
- One-line primary result: Achieves up to 4.9% and 6.3% absolute accuracy gains on vision and NLP benchmarks while reducing storage from 57Gb to 8.2Gb with 99.7% performance retention

## Executive Summary
This work addresses the problem of performance degradation in model merging and compression due to task interference. The authors propose TALL-masks, a method to identify task-specific parameter subsets in multi-task vectors, enabling efficient model merging and compression. By eliminating catastrophic and selfish weights through Consensus Merging, the approach consistently improves over existing methods across vision and NLP benchmarks.

## Method Summary
The method constructs binary masks to localize task-specific information in multi-task vectors, enabling efficient model merging and compression. TALL-masks identify relevant parameter subsets by comparing each task vector to the multi-task vector, selecting parameters where the task vector's magnitude exceeds the difference, scaled by λ. The Consensus Merging algorithm then eliminates weights important exclusively to one task (selfish) or irrelevant to all tasks (catastrophic), keeping only general weights used by multiple tasks. This compression scheme reduces storage requirements significantly while retaining performance.

## Key Results
- Up to 4.9% and 6.3% absolute accuracy gains on vision and NLP benchmarks, respectively
- Storage reduction from 57Gb to 8.2Gb while retaining 99.7% of original performance
- Consistent improvements over existing merging approaches across multiple benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific information is preserved in the multi-task vector after merging, but not properly utilized due to task interference.
- Mechanism: When multiple task vectors are merged via simple summation, the discriminant information for each task remains embedded but becomes inaccessible because parameter interactions obscure task-specific relevance.
- Core assumption: The weight space contains task-specific subspaces that do not overlap, so merging does not erase information but creates interference.
- Evidence anchors:
  - [abstract] "Instead, in this work we show that the information required to solve each task is still preserved after merging as different tasks mostly use non-overlapping sets of weights."
  - [section 3] "we show that task arithmetic fails to properly utilize the relevant information to restore the fine-tuned performance. It hints that task interference is the culprit for the performance decline of task arithmetic rather than weight interference."
  - [corpus] Weak evidence - no related papers directly support this mechanism.
- Break condition: If tasks heavily overlap in parameter usage, the assumption of non-overlapping task subspaces fails.

### Mechanism 2
- Claim: TALL-masks can localize task-specific information by identifying relevant parameter subsets in the multi-task vector.
- Mechanism: Binary masks are constructed by comparing each task vector to the multi-task vector and selecting parameters where the task vector's magnitude exceeds the difference, scaled by λ.
- Core assumption: The optimal mask for task t can be derived from the ℓ1 distance minimization between the reconstructed and original task vectors.
- Evidence anchors:
  - [abstract] "We propose TALL-masks, a method to identify these task supports given a collection of task vectors and show that one can retrieve > 99% of the single task accuracy by applying our masks to the multi-task vector"
  - [section 4] "We formulate the problem of localization of task-specific knowledge as extracting relevant weight subsets from the multi-task vector with binary masks, such that the extracted weights approximate the original task vector τt."
  - [corpus] Weak evidence - no related papers directly support this mechanism.
- Break condition: If task vectors are highly correlated, the mask construction may fail to isolate task-specific parameters.

### Mechanism 3
- Claim: Consensus Merging improves performance by eliminating selfish and catastrophic weights.
- Mechanism: By analyzing mask agreements, weights important to only one task (selfish) or no tasks (catastrophic) are removed, keeping only general weights used by multiple tasks.
- Core assumption: Removing weights that are task-exclusive or universally irrelevant reduces interference and improves generalization.
- Evidence anchors:
  - [abstract] "We study the statistics of intersections among constructed masks and reveal the existence of selfish and catastrophic weights, i.e., parameters that are important exclusively to one task and irrelevant to all tasks but detrimental to multi-task fusion."
  - [section 5.2] "We find that many parameters in the multi-task vector are relevant to only a subset of tasks... We term the rest of weights as general weights, as they are relevant to at least two tasks and their importance grows with the number of relevant tasks."
  - [corpus] Weak evidence - no related papers directly support this mechanism.
- Break condition: If all tasks require unique parameters, removing selfish weights may degrade performance.

## Foundational Learning

- Concept: Weight interpolation and model merging
  - Why needed here: Understanding how simple arithmetic operations on model weights can combine task-specific knowledge is fundamental to grasping the problem and proposed solutions.
  - Quick check question: What is the difference between weight averaging and task arithmetic in model merging?

- Concept: Parameter-efficient fine-tuning
  - Why needed here: The work builds on the premise that many fine-tuned models exist, and efficient ways to combine them are needed.
  - Quick check question: How does parameter-efficient fine-tuning reduce the number of trainable parameters compared to full fine-tuning?

- Concept: Binary mask operations on weight vectors
  - Why needed here: TALL-masks uses binary masks to selectively activate/deactivate parameters, which is central to both compression and merging improvements.
  - Quick check question: How does a Hadamard product with a binary mask affect a weight vector?

## Architecture Onboarding

- Component map:
  Pre-trained model θ0 -> Fine-tuned task vectors {τt} -> Multi-task vector τMTL (sum of task vectors) -> TALL-masks (binary mask construction) -> Task-specific masks {mt} -> Consensus mask mconsensus -> Applied masks for compression or merging

- Critical path:
  1. Fine-tune individual models to get {τt}
  2. Compute τMTL = Στt
  3. Construct task-specific masks {mt}
  4. Optionally construct consensus mask mconsensus
  5. Apply masks for compression or merging

- Design tradeoffs:
  - Storage vs performance: Compression saves space but requires mask storage
  - Task interference vs task specificity: Removing selfish weights may hurt some tasks but help overall merging
  - Mask sparsity vs accuracy: More parameters in mask may recover more performance but reduce compression

- Failure signatures:
  - Performance degradation when task overlap is high
  - Masks becoming too sparse to recover task information
  - Consensus threshold k too high, removing useful general weights

- First 3 experiments:
  1. Verify mask construction recovers >99% task accuracy on a small benchmark
  2. Test compression ratio and performance retention on 8-task vision benchmark
  3. Validate Consensus Merging improves Task Arithmetic and TIES on NLP tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different model merging strategies (e.g., Task Arithmetic, TIES, Fisher-weighted averaging) affect the profile of mask agreements and the optimal weight-pruning threshold k?
- Basis in paper: [explicit] The paper discusses how different merging strategies affect mask agreement profiles, noting that Task Arithmetic and TIES have different optimal k values.
- Why unresolved: The paper only provides preliminary observations on this, comparing two methods. A comprehensive study across more merging strategies is needed to understand the full impact.
- What evidence would resolve it: A systematic comparison of mask agreement profiles and optimal k values across a wider range of model merging strategies, using consistent benchmarks and analysis methods.

### Open Question 2
- Question: Can TALL-masks be effectively applied to other types of model merging tasks beyond vision and NLP, such as reinforcement learning or graph neural networks?
- Basis in paper: [inferred] The paper demonstrates TALL-masks on vision and NLP tasks, but does not explore other domains. The core concept of identifying task-specific parameters is potentially generalizable.
- Why unresolved: The paper focuses on vision and NLP due to their prevalence in multi-task learning. Extending the method to other domains requires validation and potential adaptation.
- What evidence would resolve it: Applying TALL-masks to model merging tasks in other domains (e.g., reinforcement learning, graph neural networks) and demonstrating its effectiveness in localizing task-specific information and improving merging performance.

### Open Question 3
- Question: How does the performance of TALL-masks and Consensus Merging scale with the number of tasks beyond 20, and what are the computational limitations?
- Basis in paper: [explicit] The paper evaluates the method on benchmarks with up to 20 tasks and shows good performance. However, the scalability to larger numbers of tasks is not explored.
- Why unresolved: The paper's experiments are limited to 20 tasks. Scaling to larger numbers of tasks may introduce new challenges, such as increased computational cost or changes in the effectiveness of the method.
- What evidence would resolve it: Evaluating TALL-masks and Consensus Merging on benchmarks with significantly more than 20 tasks (e.g., 50 or 100 tasks) and analyzing the computational cost and performance trends. Investigating potential optimizations or modifications needed for scaling.

## Limitations
- The method relies on the assumption of minimal task overlap in parameter usage, which may not hold for tasks with high semantic similarity
- Binary mask construction depends on the λ hyperparameter, requiring per-task tuning and potentially being sensitive to task vector magnitudes
- The approach has not been validated on extremely large-scale multi-task scenarios or other domains beyond vision and NLP

## Confidence
- High confidence: The experimental results showing compression from 57Gb to 8.2Gb with 99.7% performance retention are well-supported by the reported methodology and benchmarks
- Medium confidence: The mechanism claims about task interference and mask effectiveness are supported by ablation studies, but the underlying assumptions about non-overlapping task subspaces remain theoretically unproven
- Medium confidence: The Consensus Merging improvements over existing methods are demonstrated across multiple benchmarks, but the specific gains may depend on the particular task combinations used

## Next Checks
1. Test mask construction and recovery performance on tasks with known high parameter overlap to evaluate method robustness
2. Validate the compression scheme with varying λ parameters to establish sensitivity and optimal selection strategies
3. Benchmark Consensus Merging on a new multi-task combination not seen in the original experiments to test generalizability