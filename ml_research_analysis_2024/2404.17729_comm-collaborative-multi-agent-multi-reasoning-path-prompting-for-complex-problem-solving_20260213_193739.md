---
ver: rpa2
title: 'CoMM: Collaborative Multi-Agent, Multi-Reasoning-Path Prompting for Complex
  Problem Solving'
arxiv_id: '2404.17729'
source_url: https://arxiv.org/abs/2404.17729
tags:
- scenario
- wrong
- answer
- reasoning
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoMM is a collaborative multi-agent prompting framework that enhances
  LLM reasoning by assigning distinct domain-expert roles (e.g., physicist, mathematician)
  to different agents and combining their outputs through a summarizer. Each agent
  applies a reasoning path tailored to its role, and few-shot examples are routed
  accordingly, enabling few-shot learning in a multi-agent setup.
---

# CoMM: Collaborative Multi-Agent, Multi-Reasoning-Path Prompting for Complex Problem Solving

## Quick Facts
- arXiv ID: 2404.17729
- Source URL: https://arxiv.org/abs/2404.17729
- Reference count: 11
- CoMM improves accuracy over strong baselines (e.g., CoT, Thought) by absolute gains of 2.78–12.74% in zero-shot and 4.91–8.23% in few-shot settings on college-level science problems.

## Executive Summary
CoMM is a collaborative multi-agent prompting framework that enhances LLM reasoning by assigning distinct domain-expert roles (e.g., physicist, mathematician) to different agents and combining their outputs through a summarizer. Each agent applies a reasoning path tailored to its role, and few-shot examples are routed accordingly, enabling few-shot learning in a multi-agent setup. On college-level science problems (College Physics, Moral Scenarios), CoMM improves accuracy over strong baselines (e.g., CoT, Thought) by absolute gains of 2.78–12.74% in zero-shot and 4.91–8.23% in few-shot settings. Analysis shows multiple independent agents and diverse expert roles are crucial for performance.

## Method Summary
CoMM implements a three-agent system where domain experts (e.g., physicist, mathematician) apply specialized reasoning paths to problems, with few-shot examples routed by expertise. Agents collaborate through multiple discussion turns, and a summarizer agent consolidates their outputs. The framework uses separate LLM instances for each role rather than having one agent switch roles, avoiding self-consistency bias. Each agent receives prompts defining its role and appropriate few-shot demonstrations, with the number of discussion turns optimized per dataset.

## Key Results
- CoMM improves accuracy over strong baselines (CoT, Thought) by absolute gains of 2.78–12.74% in zero-shot settings
- In few-shot settings, CoMM achieves improvements of 4.91–8.23% over baselines
- Multiple independent agents outperform single agents playing multiple roles across all benchmarks and settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent collaboration corrects individual knowledge and computation errors
- Mechanism: Assigning distinct domain-expert roles allows each agent to focus on specialized knowledge, with errors in one domain being corrected by another agent's expertise
- Core assumption: Different domain experts have complementary strengths and weaknesses in problem-solving
- Evidence anchors:
  - [abstract]: "Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines"
  - [section]: "In detail, it improves with absolute average improvements of 3.84% at zero-shot setting and 8.23% at few-shot setting"
  - [corpus]: "Found 25 related papers (using 8). Average neighbor FMR=0.411, average citations=0.0." (weak corpus support for this specific mechanism)
- Break condition: When agents share the same error pattern or when domain boundaries are unclear

### Mechanism 2
- Claim: Different reasoning paths for different roles enable effective few-shot learning in multi-agent settings
- Mechanism: Each agent receives few-shot examples routed according to their expertise, allowing them to learn specialized reasoning patterns rather than generic ones
- Core assumption: Domain-specific reasoning patterns are more effective than generic reasoning patterns
- Evidence anchors:
  - [abstract]: "we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios"
  - [section]: "Empirical results on multiple complicated college-level science problems show that our method significantly outperforms strong baselines"
  - [corpus]: Weak support - no specific citations about routing few-shot examples by expertise
- Break condition: When few-shot examples are too similar across domains or when agents cannot differentiate between reasoning paths

### Mechanism 3
- Claim: Multiple independent agents outperform single agents playing multiple roles
- Mechanism: Separate LLM instances playing distinct roles avoid the self-consistency bias that occurs when one agent switches between roles
- Core assumption: Self-consistency in a single agent causes confusion when switching between different reasoning styles
- Evidence anchors:
  - [section]: "Apparently, the performance of multiple agents (CoMM) significantly outperforms the single-agent approach, across all benchmarks and settings"
  - [abstract]: "Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently"
  - [corpus]: "Two Heads are Better Than One: Test-time Scaling of Multi-agent Collaborative Reasoning" suggests similar findings in related work
- Break condition: When communication overhead between agents exceeds the benefits of independence

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: CoT serves as the baseline comparison and understanding it helps grasp why CoMM improves upon it
  - Quick check question: What is the key difference between standard prompting and chain-of-thought prompting?

- Concept: Multi-agent systems and role specialization
  - Why needed here: CoMM relies on assigning different roles to different agents, which is fundamental to its architecture
  - Quick check question: Why might assigning one LLM to play multiple roles simultaneously be less effective than using separate LLMs?

- Concept: Few-shot learning and demonstration routing
  - Why needed here: CoMM implements few-shot learning by routing demonstrations to agents based on their expertise
  - Quick check question: How does CoMM handle few-shot examples differently from standard few-shot prompting approaches?

## Architecture Onboarding

- Component map:
  - Expert agents (e.g., physicist, mathematician) - receive domain-specific prompts and reasoning paths
  - Summarizer agent - consolidates outputs from expert agents
  - Prompt routing system - assigns few-shot examples to appropriate agents based on expertise
  - Interaction manager - controls discussion turns and message passing

- Critical path:
  1. Receive problem input
  2. Generate system message defining roles
  3. Route problem to first expert agent
  4. Route first agent's output to second expert agent
  5. Route both expert outputs to summarizer
  6. Generate final answer from summarizer

- Design tradeoffs:
  - Single vs. multiple LLM instances: multiple instances avoid self-consistency bias but increase resource usage
  - Number of discussion turns: more turns allow refinement but risk confusion and hallucination
  - Domain expertise granularity: more specialized roles capture more nuance but require more prompt engineering

- Failure signatures:
  - All agents converge on the same incorrect answer (suggests shared blind spots)
  - Summarizer outputs inconsistent or incoherent synthesis (suggests poor agent communication)
  - Performance degrades with additional discussion turns (suggests over-discussion causing confusion)

- First 3 experiments:
  1. Compare single expert vs. multiple experts on college physics problems
  2. Test one-turn vs. two-turn discussions on moral scenarios
  3. Evaluate expert-only vs. non-expert agents on cross-domain problems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CoMM's performance vary with different numbers of agents (e.g., 2 vs 4 vs 6 agents) beyond the fixed three-agent setup?
- Basis in paper: [inferred] The paper shows CoMM uses three agents and mentions "multiple agents" are necessary, but doesn't systematically explore optimal agent count.
- Why unresolved: The paper only compares single-agent vs three-agent setups, leaving the optimal number of agents unknown.
- What evidence would resolve it: Experiments varying agent count while keeping other parameters constant, measuring accuracy gains.

### Open Question 2
- Question: Can CoMM's role-assignment strategy be automated rather than requiring manual task-specific design?
- Basis in paper: [explicit] "However, this is a common limitation for all the CoT-style approaches... We leave the automatic prompting design for the CoMM framework as future work."
- Why unresolved: The paper acknowledges this as a limitation but doesn't propose solutions or test automated approaches.
- What evidence would resolve it: Implementation and evaluation of an automated role-assignment mechanism compared to manual design.

### Open Question 3
- Question: How does CoMM perform on domains outside of college-level physics and moral scenarios?
- Basis in paper: [inferred] Experiments are limited to two specific benchmarks, suggesting generalization needs testing.
- Why unresolved: The paper doesn't test CoMM on other scientific domains or problem types.
- What evidence would resolve it: Experiments on diverse domains like biology, chemistry, or mathematical proofs showing performance patterns.

### Open Question 4
- Question: What is the impact of discussion depth (number of discussion turns) on different problem types?
- Basis in paper: [explicit] "It turns out that the turns of discussions depend on the benchmark or dataset" with different optimal turns for different tasks.
- Why unresolved: The paper observes this dependency but doesn't explain the underlying reasons or provide guidelines.
- What evidence would resolve it: Analysis correlating discussion depth with problem complexity, knowledge requirements, or solution structure.

## Limitations
- Limited evaluation scope: Only two college-level science benchmarks (College Physics and Moral Scenarios) were tested, both from MMLU dataset
- Architectural choices lack rigorous ablation studies: Direct comparison between multiple agents and single agent with role-switching is missing
- No comparison against state-of-the-art multi-agent or specialized prompting techniques that have emerged since baseline methods were established

## Confidence

**High confidence**: The core mechanism of using multiple agents with distinct domain expertise roles is technically sound and well-supported by the empirical results. The observation that multiple independent agents outperform single-agent approaches is consistently demonstrated across benchmarks.

**Medium confidence**: The claim that routing few-shot examples by domain expertise significantly improves performance has moderate support, but the paper lacks direct comparisons to standard few-shot prompting approaches with the same examples.

**Low confidence**: The generalizability of CoMM to problems outside the tested domains (college physics and moral scenarios) remains uncertain. The paper provides limited evidence for the framework's effectiveness on problems requiring different types of reasoning or knowledge domains.

## Next Checks

1. **Ablation study on agent count**: Implement a direct comparison between CoMM's three-agent system and a single agent prompted to play all three roles sequentially. Measure performance differences on both College Physics and Moral Scenarios to quantify the exact contribution of agent independence versus role definition.

2. **Cross-domain generalization test**: Evaluate CoMM on at least two additional problem domains from MMLU or other benchmark datasets (e.g., college mathematics, humanities) to assess whether the framework's improvements transfer beyond science problems.

3. **Discussion turn optimization**: Systematically vary the number of discussion turns (0, 1, 2, 3+) in the CoMM framework and measure the impact on accuracy and computational efficiency. This would help identify whether the two-turn discussion used in the main experiments represents an optimal tradeoff between refinement quality and over-discussion risks.