---
ver: rpa2
title: 'Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative
  Multi-Agent Reinforcement Learning'
arxiv_id: '2410.06101'
source_url: https://arxiv.org/abs/2410.06101
tags:
- reward
- cory
- task
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper
---

# Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2410.06101
- Source URL: https://arxiv.org/abs/2410.06101
- Reference count: 40
- This paper proposes CORY, a sequential cooperative multi-agent RL framework for fine-tuning LLMs that outperforms single-agent baselines on IMDB and GSM8K tasks.

## Executive Summary
This paper introduces CORY, a novel approach to fine-tuning large language models using sequential cooperative multi-agent reinforcement learning. The method involves creating two LLM agents that work together: a pioneer that generates responses from queries alone, and an observer that generates responses using both the query and the pioneer's output. Through periodic role exchange and a shared reward structure, the agents coevolve to achieve better performance than single-agent approaches while maintaining better control over KL divergence. The framework demonstrates consistent improvements over PPO baselines on both IMDB sentiment classification and GSM8K math problem tasks.

## Method Summary
CORY duplicates a pre-trained LLM into two agents (pioneer and observer) that work cooperatively. The pioneer generates responses from queries alone, while the observer receives both the query and the pioneer's response to generate its own output. Both agents share a collective reward (sum of individual task rewards) and are trained simultaneously using PPO. Periodic role exchange ensures both agents experience both input formats, preventing prompt bias and fostering skill transfer. The method addresses key challenges in LLM fine-tuning including large action spaces, sparse rewards, and distribution collapse.

## Key Results
- CORY outperforms single-agent PPO baselines on both IMDB Review and GSM8K tasks
- The method achieves better trade-offs between task reward and KL divergence
- Role exchange and knowledge transfer mechanisms contribute to improved stability and performance
- CORY demonstrates consistent improvements across different model sizes (GPT-2 and Llama-2)

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Transfer
- Claim: The observer leverages the pioneer's response as an in-context reference, improving policy quality while reducing optimization pressure
- Core assumption: The observer can effectively use the pioneer's response to guide its own generation
- Break condition: If the observer cannot effectively utilize the pioneer's response or if the pioneer generates poor responses

### Mechanism 2: Role Exchange
- Claim: Periodic role swapping prevents prompt bias and ensures both agents develop unified representations
- Core assumption: Role exchange can effectively transfer skills learned by the observer back to the pioneer
- Break condition: If role exchange frequency is inappropriate or skill transfer fails

### Mechanism 3: Cooperative Learning
- Claim: Collective reward structure and simultaneous training create bootstrapping that enhances RL effectiveness
- Core assumption: Cooperative reward encourages genuine collaboration rather than exploitation
- Break condition: If cooperative framework leads to suboptimal individual performance

## Foundational Learning

- **Markov Decision Process formulation**: The paper frames LLM fine-tuning as an MDP where states are token sequences, actions are next tokens, and rewards are task-specific scores. This formulation is essential for understanding how RL algorithms are applied to LLMs.
  - Quick check: What are the components of the language-augmented MDP defined in the paper?

- **PPO limitations for LLMs**: Understanding why PPO struggles with LLM fine-tuning (large action space, sparse rewards, distribution collapse) is crucial for appreciating the proposed solution.
  - Quick check: Why does PPO often lead to distribution collapse when applied to LLM fine-tuning?

- **Multi-objective optimization**: The paper frames RL fine-tuning with KL penalty as a multi-objective problem between task reward and KL divergence.
  - Quick check: How does the multi-objective perspective explain CORY's better trade-offs between task reward and KL divergence?

## Architecture Onboarding

- **Component map**: Pioneer agent -> Observer agent -> Shared reward function -> Role exchange scheduler -> PPO optimizer
- **Critical path**: Query input → Pioneer generation → Pioneer output + query → Observer generation → Both responses evaluated → Collective reward computed → PPO updates → Periodic role exchange → Repeat
- **Design tradeoffs**: Computational cost (doubling parameters), training stability (cooperative vs single-agent), implementation simplicity (algorithm-agnostic but requires reward engineering)
- **Failure signatures**: Pioneer generates poor responses, inappropriate role exchange frequency, collective reward imbalance, KL divergence not properly constrained
- **First 3 experiments**: 1) Single-agent PPO baseline on IMDB, 2) CORY with different role exchange periods, 3) Knowledge transfer ablation (disable observer access to pioneer's response)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CORY performance scale with the number of LLM agents in the cooperative MARL framework?
- Basis: The paper mentions incorporating more LLMs as an intriguing future prospect but focuses on two-agent setting
- Why unresolved: No empirical results on varying agent numbers provided
- What evidence would resolve it: Experiments comparing CORY with 2, 4, 8 agents on benchmark tasks

### Open Question 2
- Question: Can CORY be effectively integrated with RL algorithms beyond PPO?
- Basis: The paper states CORY is algorithm-agnostic but only demonstrates with PPO
- Why unresolved: No empirical evidence of compatibility with other RL algorithms
- What evidence would resolve it: Experiments applying CORY with TRPO, Actor-Critic on same benchmark tasks

### Open Question 3
- Question: What is the impact of different role exchange periods on training stability and performance?
- Basis: Role exchange occurs periodically but sensitivity analysis not provided
- Why unresolved: No exploration of TREx hyperparameter sensitivity
- What evidence would resolve it: Sensitivity analysis of TREx on benchmark tasks

## Limitations

- Scalability concerns may limit practical applicability to larger models due to computational overhead
- Evaluation focuses on only two specific tasks, limiting generalization claims
- Role exchange mechanism lacks rigorous ablation studies on optimal exchange frequencies

## Confidence

- **High confidence**: Core knowledge transfer mechanism well-supported by experimental results
- **Medium confidence**: Role exchange effectiveness demonstrated but lacks systematic analysis
- **Medium confidence**: Better trade-offs between task reward and KL divergence supported but could benefit from additional metrics

## Next Checks

1. **Ablation study on role exchange frequency**: Systematically vary role exchange period and measure impact on performance, KL divergence, and training stability

2. **Scaling experiment**: Test method on larger models (Llama-2 13B/70B) to evaluate computational overhead and performance gains

3. **Cross-task generalization study**: Apply CORY to diverse NLP tasks (summarization, code generation, dialogue) to validate broader applicability