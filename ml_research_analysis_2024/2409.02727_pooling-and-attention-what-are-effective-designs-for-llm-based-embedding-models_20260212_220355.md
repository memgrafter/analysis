---
ver: rpa2
title: 'Pooling And Attention: What Are Effective Designs For LLM-Based Embedding
  Models?'
arxiv_id: '2409.02727'
source_url: https://arxiv.org/abs/2409.02727
tags:
- pooling
- attention
- trainable
- embedding
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically evaluates pooling and attention strategies
  for LLM-based embedding models, addressing the lack of standardized comparisons
  in the literature. The authors conduct large-scale experiments by fine-tuning the
  same base LLM (Mistral-7B and Qwen2-0.5B) with different combinations of pooling
  (EOS-last token, last-layer trainable, and multi-layers trainable) and attention
  (causal and bidirectional) strategies on identical training data.
---

# Pooling And Attention: What Are Effective Designs For LLM-Based Embedding Models?

## Quick Facts
- arXiv ID: 2409.02727
- Source URL: https://arxiv.org/abs/2409.02727
- Authors: Yixuan Tang; Yi Yang
- Reference count: 17
- Primary result: No one-size-fits-all pooling/attention solution for LLM embedding models; bidirectional attention + trainable pooling excels in retrieval/STS, simpler designs better for clustering/classification

## Executive Summary
This paper addresses the lack of standardized comparisons in LLM-based embedding models by conducting large-scale experiments on pooling and attention strategies. The authors fine-tune the same base LLM (Mistral-7B and Qwen2-0.5B) with different combinations of pooling (EOS-last token, last-layer trainable, and multi-layers trainable) and attention (causal and bidirectional) strategies on identical training data. They find that pooling and attention effectiveness is highly task-dependent: bidirectional attention with trainable pooling excels in semantic textual similarity and retrieval tasks, while simpler designs (EOS-last token pooling + causal attention) perform comparably or better in clustering and classification tasks. The paper also proposes a new Multi-Layers Trainable Pooling strategy that statistically outperforms existing methods in STS and retrieval tasks.

## Method Summary
The authors fine-tune base LLMs (Mistral-7B and Qwen2-0.5B) using contrastive learning with five different pooling+attention combinations: EOS-Last + Causal, Last-Layer Trainable + Causal, Multi-Layers Trainable + Causal, Last-Layer Trainable + Bidirectional, and Multi-Layers Trainable + Bidirectional. Training uses LoRA (rank 16) with learning rate 1e-5, batch size 2048, and 1000 max steps on 1.4 million examples from public datasets. The proposed Multi-Layers Trainable Pooling module uses layer-specific attention weights and cross-attention to combine hidden states from all layers. Models are evaluated on the MTEB benchmark across 15 retrieval, 4 reranking, 12 classification, 11 clustering, 3 pair classification, 10 STS, and 1 summarization datasets, with statistical significance testing via Wilcoxon Signed Rank Test (p < 0.05) for tasks with >4 datasets.

## Key Results
- No universal best pooling/attention design: bidirectional attention + trainable pooling excels in retrieval and STS tasks, while simpler designs (EOS-last token + causal attention) perform better on clustering and classification tasks
- Multi-Layers Trainable Pooling statistically outperforms existing methods in STS and retrieval tasks by leveraging complementary information across all layers
- Task-specific performance patterns: clustering tasks favor causal attention with simpler pooling, while semantic similarity benefits from bidirectional attention with multi-layer pooling
- Statistical significance testing reveals meaningful performance differences in most task categories, validating the task-dependent effectiveness claims

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-layer trainable pooling captures complementary semantic information across layers that single-layer pooling misses
- Mechanism: The pooling layer learns layer-specific attention weights and uses cross-attention to combine information from all layers, allowing it to selectively emphasize informative layers for specific tasks
- Core assumption: Different layers encode distinct but complementary semantic information, and this information distribution is task-dependent
- Evidence anchors:
  - [section 3.1]: "the hidden states of other layers may encode information that complements that of the last layer"
  - [section 3.2]: Describes how layer weight matrix and cross-attention network combine multi-layer information
  - [corpus]: Weak evidence - related papers discuss pooling effects but not multi-layer approaches specifically
- Break condition: If layer-wise information is redundant or if training fails to learn meaningful layer weights

### Mechanism 2
- Claim: Bidirectional attention improves retrieval performance by allowing context from both directions
- Mechanism: Tokens can attend to both preceding and following tokens, capturing richer contextual relationships that are particularly beneficial for finding semantically similar content
- Core assumption: Retrieval tasks benefit more from bidirectional context than from the autoregressive nature of causal attention
- Evidence anchors:
  - [section 5.2]: "Bi-directional attention is better at retrieval task but worse at clustering task"
  - [abstract]: "bidirectional attention with trainable pooling excels in semantic textual similarity and retrieval tasks"
  - [corpus]: Weak evidence - related papers discuss attention mechanisms but not specific retrieval performance
- Break condition: If task doesn't require bidirectional context or if additional parameters hurt performance

### Mechanism 3
- Claim: Task-specific pooling/attention combinations outperform one-size-fits-all approaches
- Mechanism: Different tasks have different optimal configurations - retrieval benefits from bidirectional attention and multi-layer pooling, while classification benefits from simpler causal attention
- Core assumption: Task characteristics determine optimal architectural choices rather than universal best practices
- Evidence anchors:
  - [abstract]: "there is no one-size-fits-all solution: bidirectional attention and an additional trainable pooling layer outperform in text similarity and information retrieval tasks, while simpler designs...perform comparably or better in clustering and classification tasks"
  - [section 5.3]: Detailed comparison showing task-dependent performance
  - [corpus]: Weak evidence - related papers discuss pooling effects but not task-specific combinations
- Break condition: If performance differences are not statistically significant or if dataset characteristics dominate architectural effects

## Foundational Learning

- Concept: Contrastive learning for embedding models
  - Why needed here: The paper uses contrastive learning to fine-tune LLMs as embedding models by encouraging similar embeddings for positive pairs and dissimilar embeddings for negative pairs
  - Quick check question: What is the loss function used when training embedding models with contrastive learning?

- Concept: Statistical significance testing (Wilcoxon Signed Rank Test)
  - Why needed here: The paper uses this test to determine if performance differences between models are meaningful rather than due to random variation
  - Quick check question: When would you use a Wilcoxon Signed Rank Test versus a paired t-test?

- Concept: Attention mechanisms (causal vs bidirectional)
  - Why needed here: The paper compares causal attention (tokens attend only to previous tokens) with bidirectional attention (tokens attend to all tokens)
  - Quick check question: How does bidirectional attention differ architecturally from causal attention in transformer models?

## Architecture Onboarding

- Component map:
  Base LLM (Mistral-7B or Qwen2-0.5B) -> EOS token extraction (causal) or mean pooling (bidirectional) -> Multi-Layers Trainable Pooling module -> Embedding output -> Contrastive loss computation

- Critical path: Input → LLM forward pass → Hidden states extraction → Multi-Layers Trainable Pooling → Embedding output → Contrastive loss computation → Backpropagation

- Design tradeoffs:
  - Multi-layer pooling adds parameters and computation but captures richer information
  - Bidirectional attention enables better context understanding but may introduce noise for some tasks
  - Simpler designs (EOS-last token + causal attention) are faster and often sufficient for classification/clustering

- Failure signatures:
  - Poor training loss convergence suggests issues with pooling layer initialization or learning rates
  - Degraded performance on classification tasks indicates bidirectional attention may be introducing too much context noise
  - Layer weight collapse to uniform values suggests model isn't learning task-specific layer importance

- First 3 experiments:
  1. Compare single-layer vs multi-layer trainable pooling on STS task to verify complementary information capture
  2. Test causal vs bidirectional attention on retrieval task to confirm bidirectional benefit
  3. Evaluate simpler vs complex designs on classification task to identify task-dependent optimal configurations

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but based on the content, several questions emerge regarding the generalizability and limitations of the findings.

## Limitations
- Experimental scope limited to two base LLM architectures (Mistral-7B and Qwen2-0.5B), potentially limiting generalizability to other model families
- Training datasets not fully specified in main text, requiring reference to supplementary materials for complete reproduction
- Hard negative mining process uses unspecified version of SentenceTransformer, introducing potential reproducibility concerns
- Statistical significance testing only applied to tasks with >4 datasets, potentially missing meaningful differences in smaller task groups

## Confidence

**High Confidence**: The core finding that bidirectional attention with trainable pooling excels in retrieval and semantic similarity tasks while simpler designs perform better on classification and clustering tasks is well-supported by the experimental data and statistical testing.

**Medium Confidence**: The proposed Multi-Layers Trainable Pooling strategy shows statistically significant improvements in specific tasks, but the magnitude of improvement varies across benchmarks. The mechanism explanation is plausible but requires further theoretical validation.

**Low Confidence**: Claims about the universal applicability of findings across different LLM architectures are not fully supported, as experiments were limited to two specific models.

## Next Checks

1. **Cross-Architecture Validation**: Test the pooling and attention strategies on additional LLM architectures (e.g., Llama, Claude) to verify that findings generalize beyond Mistral-7B and Qwen2-0.5B.

2. **Ablation Study on Layer Contributions**: Conduct detailed analysis of what information each layer contributes to the Multi-Layers Trainable Pooling by systematically disabling individual layers.

3. **Statistical Power Analysis**: Perform formal power analysis on the Wilcoxon Signed Rank Test results to determine the minimum detectable effect size given the sample sizes in each task category.