---
ver: rpa2
title: 'Proactive Adversarial Defense: Harnessing Prompt Tuning in Vision-Language
  Models to Detect Unseen Backdoored Images'
arxiv_id: '2412.08755'
source_url: https://arxiv.org/abs/2412.08755
tags:
- images
- unseen
- backdoor
- backdoored
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a proactive method for detecting unseen backdoored
  images before they can infiltrate object recognition systems. The approach leverages
  prompt tuning in Vision-Language Models (VLMs), specifically CLIP, to train learnable
  text prompts that differentiate clean images from those with hidden backdoor triggers.
---

# Proactive Adversarial Defense: Harnessing Prompt Tuning in Vision-Language Models to Detect Unseen Backdoored Images

## Quick Facts
- arXiv ID: 2412.08755
- Source URL: https://arxiv.org/abs/2412.08755
- Authors: Kyle Stein; Andrew Arash Mahyari; Guillermo Francia; Eman El-Sheikh
- Reference count: 0
- One-line primary result: 86% average accuracy in detecting unseen backdoor attacks using CLIP with prompt tuning

## Executive Summary
This paper introduces a proactive defense mechanism for detecting unseen backdoored images before they compromise object recognition systems. The approach leverages prompt tuning in Vision-Language Models (VLMs), specifically CLIP, to train learnable text prompts that differentiate clean images from those with hidden backdoor triggers. By transforming images and text into a shared multimodal embedding space, the method achieves exceptional efficacy in detecting six types of unseen backdoor attacks across CIFAR-10 and GTSRB datasets, outperforming traditional CNN-based methods by up to 35% on certain attack types.

## Method Summary
The method uses CLIP's frozen Vision-Language Model with learnable soft prompts prepended to class labels ("clean" and "backdoored"). These prompts are optimized to align text embeddings with visual embeddings in a shared multimodal space. During training, the model learns to distinguish between clean and seen attack images using cross-entropy loss on similarity scores. At inference, incoming images are classified as clean or backdoored based on their similarity to the prompt-tuned class embeddings. The approach acts as a proactive gatekeeper, preventing backdoored images from reaching the object recognition system.

## Key Results
- Achieves 86% average accuracy across CIFAR-10 and GTSRB datasets for detecting six unseen backdoor attack types
- Outperforms traditional CNN-based methods with accuracy improvements up to 35% on certain attack types
- Successfully detects Trojan-WM (93% accuracy), Trojan-SQ (90% accuracy), and l2-inv (93% accuracy) attacks
- Shows moderate performance on pixel-level attacks like Badnets-PX (52% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision-Language Models (VLMs) trained on large-scale image-text pairs can generalize to detect unseen backdoor triggers by leveraging their rich multimodal embeddings.
- Mechanism: The CLIP model's image encoder transforms both clean and backdoored images into a shared embedding space. The text encoder, combined with learnable soft prompts, creates class-specific embeddings ("clean" vs "backdoored") that the model aligns with the visual embeddings. During inference, the similarity between image embeddings and these class-specific text embeddings determines classification.
- Core assumption: The VLM's pre-training on diverse image-text pairs has endowed it with generalizable features that distinguish backdoor patterns even when they are unseen during training.
- Evidence anchors:
  - [abstract] "leveraging the transformative success of prompt tuning in Vision Language Models (VLMs), our approach trains learnable text prompts to differentiate clean images from those with hidden backdoor triggers."
  - [section] "Models like Contrastive Language-Image Pre-training (CLIP) [21], trained on an immense dataset of image-text pairs (i.e. 400 million samples ), are uniquely equipped to extract rich and versatile features from input images, regardless of their intended application."
  - [corpus] Weak evidence - The corpus papers focus on backdoor defenses but don't specifically discuss VLM-based detection of unseen triggers.
- Break condition: If the backdoor trigger is subtle enough that it doesn't significantly alter the global image features captured by the frozen visual encoder (e.g., pixel-level triggers like Badnets-PX), the model may fail to distinguish between clean and backdoored images.

### Mechanism 2
- Claim: Learnable soft prompts allow the model to adapt the textual representation to highlight backdoor features, improving detection of unseen attacks.
- Mechanism: During training, learnable prefix embeddings [p1, p2, p3] are prepended to the word embeddings of "clean" and "backdoored" class labels. These prefixes are optimized to align the text embeddings with the visual embeddings of clean and backdoored images respectively. This alignment is learned from seen attacks and generalizes to unseen ones.
- Core assumption: The learnable prompts can capture the semantic differences between clean and backdoored images in the multimodal embedding space, even for trigger patterns not seen during training.
- Evidence anchors:
  - [section] "During training, the model optimizes the alignment between the fixed visual embeddings, Ij, and the learnable text embeddings, Tk, k ∈ {1, 2} , to enable recognition of adversarial images."
  - [section] "These embeddings capture the contextual nuances of the respective image classes."
  - [section] "This helps align visual and textual embeddings in the multimodal embedding space, making it more effective at detecting unseen backdoor triggers."
- Break condition: If the unseen backdoor trigger is too different from the seen attacks used during training, the learned prompts may not generalize well enough to create distinct alignment in the embedding space.

### Mechanism 3
- Claim: The proactive approach of detecting backdoored images before they reach the model prevents model compromise entirely.
- Mechanism: The system acts as a gatekeeper, inspecting incoming images during inference and blocking those classified as backdoored. This prevents the adversarial images from ever reaching the object recognition system, thus avoiding any model manipulation.
- Core assumption: It's more effective to prevent backdoored images from entering the system than to try to clean a compromised model after the fact.
- Evidence anchors:
  - [abstract] "Our approach serves two critical purposes: i) Pre-Training Defense: Before training begins, the algorithm meticulously scans the dataset to identify and eliminate adversarial (backdoored) images that could poison object recognition models."
  - [abstract] "2) Inference-Time Shielding: During inference, the algorithm acts as a vigilant gatekeeper, inspecting incoming images to block adversarial content from reaching the object recognition system."
  - [abstract] "By proactively identifying and neutralizing adversarial threats, this approach works in tandem with traditional defense algorithms, creating a robust and comprehensive safeguard against adversarial attacks."
- Break condition: If the detection system itself is bypassed or compromised, backdoored images could still reach the object recognition system.

## Foundational Learning

- Concept: Vision-Language Models (VLMs) and multimodal embeddings
  - Why needed here: The method relies on VLMs like CLIP to transform images and text into a shared embedding space where similarities can be computed for classification.
  - Quick check question: What is the key architectural component of CLIP that enables it to process both images and text in a unified way?

- Concept: Prompt tuning and learnable soft prompts
  - Why needed here: The approach uses prompt tuning to adapt the text encoder of the VLM for the specific task of backdoor detection without fine-tuning the entire model.
  - Quick check question: How does prompt tuning differ from traditional fine-tuning in terms of which model parameters are updated?

- Concept: Cross-entropy loss and similarity scoring
  - Why needed here: The training objective uses cross-entropy loss on similarity scores computed between image and text embeddings to optimize the learnable prompts.
  - Quick check question: Why is a scaling factor (α = 100) applied to the dot product when computing similarity scores?

## Architecture Onboarding

- Component map:
  CLIP model (frozen image and text encoders) -> Learnable soft prompts [p1, p2, p3] -> Image preprocessing and embedding extraction -> Text embedding generation with prompts -> Similarity computation (dot product + scaling) -> Cross-entropy loss for training -> Argmax classification during inference

- Critical path:
  1. Image → CLIP image encoder → embedding
  2. Prompts + class label → CLIP text encoder → embedding
  3. Compute similarity scores between image and text embeddings
  4. For training: compute cross-entropy loss and update prompts
  5. For inference: select class with highest similarity score

- Design tradeoffs:
  - Using frozen CLIP encoders provides generalization but may miss subtle trigger patterns
  - Learnable prompts add flexibility but require careful initialization and training
  - Single binary classification (clean vs backdoored) simplifies the task but may not capture trigger diversity

- Failure signatures:
  - Low accuracy on certain attack types (e.g., Badnets-PX) suggests the model struggles with subtle, localized triggers
  - High similarity between clean and backdoored image embeddings in t-SNE visualizations indicates poor separation in the embedding space
  - Poor cross-dataset generalization suggests the learned prompts are too dataset-specific

- First 3 experiments:
  1. Train and evaluate on CIFAR-10 with all six attack types, verifying the 86% average accuracy claim
  2. Test cross-dataset generalization by training on CIFAR-10 and testing on GTSRB (and vice versa)
  3. Compare learned prompts vs. static prefix "a photo of" to quantify the benefit of prompt tuning

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on pixel-level backdoor attacks (like Badnets-PX) remains relatively low at 52% accuracy
- Limited cross-dataset validation raises questions about true generalization capability
- Learnable prompts' initialization strategy and sensitivity to different schemes is not thoroughly explored

## Confidence
- **High Confidence**: The core mechanism of using CLIP with prompt tuning for multimodal embedding alignment is technically sound and well-supported by the VLM literature
- **Medium Confidence**: The 86% average accuracy claim is based on the reported experiments, but the lack of extensive cross-dataset testing and ablation studies on prompt initialization reduces confidence in the robustness of these results
- **Low Confidence**: The claim that this approach creates a "new standard for proactive backdoor defense" lacks sufficient comparative analysis against the full spectrum of existing defense mechanisms

## Next Checks
1. **Cross-Dataset Generalization Test**: Train the model on CIFAR-10 and evaluate on GTSRB (and vice versa) to assess true generalization to unseen datasets and trigger patterns beyond the training distribution.

2. **Prompt Initialization Sensitivity Analysis**: Systematically test different initialization strategies for the learnable soft prompts (random initialization, different static phrases, frozen vs. trainable embeddings) to quantify their impact on detection performance.

3. **Pixel-Level Trigger Robustness Test**: Conduct targeted experiments focusing on pixel-level backdoor attacks (like Badnets-PX) by varying trigger size, location, and subtlety to determine the detection threshold and identify failure modes specific to localized triggers.