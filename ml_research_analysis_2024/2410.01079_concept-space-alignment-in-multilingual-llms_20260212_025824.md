---
ver: rpa2
title: Concept Space Alignment in Multilingual LLMs
arxiv_id: '2410.01079'
source_url: https://arxiv.org/abs/2410.01079
tags:
- alignment
- group
- concepts
- language
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates concept space alignment in multilingual
  large language models (LLMs), aiming to determine whether high-quality linear mappings
  exist between corresponding concepts across different languages. The authors hypothesize
  that such alignment enables cross-lingual generalization and evaluate this by treating
  LLMs as bilingual dictionary induction algorithms, retrieving parallel concept pairs
  using nearest neighbor search with cross-domain local scaling.
---

# Concept Space Alignment in Multilingual LLMs

## Quick Facts
- arXiv ID: 2410.01079
- Source URL: https://arxiv.org/abs/2410.01079
- Authors: Qiwei Peng; Anders SÃ¸gaard
- Reference count: 31
- Primary result: Larger multilingual LLMs exhibit high-quality linear alignments between concept spaces across different languages

## Executive Summary
This study investigates concept space alignment in multilingual large language models (LLMs), determining whether high-quality linear mappings exist between corresponding concepts across different languages. The authors treat LLMs as bilingual dictionary induction algorithms, using nearest neighbor search with cross-domain local scaling to retrieve parallel concept pairs. Experiments with 10 LLM families and six languages reveal that larger models show very high-quality linear alignments between concept spaces, while prompt-based embeddings generally show lower linearity and larger performance gaps compared to vanilla embeddings.

## Method Summary
The study extracts concept embeddings from multilingual LLMs for parallel concepts across seven languages (English, French, Romanian, Basque, Finnish, Japanese, and Thai) using WordNet. For each language pair, Procrustes Analysis with Singular Value Decomposition (SVD) learns linear mappings between concept spaces, which are then used to transform source embeddings to target space. Cross-Domain Local Scaling (CSLS) performs nearest neighbor retrieval, and Precision@k (P@k) measures retrieval performance before and after explicit alignment. The study compares vanilla embeddings (last token or average) with prompt-based embeddings and examines differences between abstract and physical concepts.

## Key Results
- Larger multilingual LLMs exhibit very high-quality linear alignments between corresponding concepts in different languages
- Prompt-based embeddings show lower linearity and larger performance gaps before and after explicit alignment compared to vanilla embeddings
- Abstract concepts align better than physical concepts across languages, likely due to higher frequency and diverse contextual usage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger multilingual LLMs implicitly learn linear concept alignments between languages
- Mechanism: During training on diverse multilingual corpora, models develop compressed representations where semantically equivalent concepts across languages map closely in embedding space, enabling linear transformations to align concept spaces
- Core assumption: Multilingual training data contains sufficient parallel or semantically similar contexts for corresponding concepts across languages
- Evidence anchors:
  - [abstract] "larger models exhibit very high-quality linear alignments between corresponding concepts in different languages"
  - [section] "larger model size leads to better alignment" and "near-isomorphisms between monolingual concept spaces"
  - [corpus] "Cross-lingual topic modeling" and "Exploring Alignment in Shared Cross-lingual Spaces" - related work suggests linear alignment is feasible but corpus evidence is weak (0 citations)
- Break condition: Insufficient parallel data or extreme typological distance prevents formation of implicit alignments

### Mechanism 2
- Claim: Prompt-based embeddings partially break implicitly learned concept alignments
- Mechanism: Instruction fine-tuning modifies the model's internal representations to better follow task instructions, which disrupts the original semantic organization that enabled cross-lingual concept alignment
- Core assumption: Original multilingual pretraining creates useful implicit alignments that are partially overwritten during instruction tuning
- Evidence anchors:
  - [abstract] "prompt-based embeddings generally show lower linearity and larger performance gaps before and after explicit alignment compared to vanilla embeddings"
  - [section] "prompt-based embeddings exhibit significantly lower linearity compared to word embeddings" and "gaps between before and after alignment are larger for prompt-based embeddings"
  - [corpus] "How does Alignment Enhance LLMs' Multilingual Capabilities?" suggests alignment affects multilingual performance
- Break condition: When prompt templates are designed to preserve semantic relationships or when models are not instruction-tuned

### Mechanism 3
- Claim: Abstract concepts align better across languages than physical concepts due to frequency and contextual diversity
- Mechanism: Abstract concepts appear more frequently and in more diverse contexts across languages, leading to more robust shared representations that align better in the multilingual embedding space
- Core assumption: Frequency and contextual diversity correlate with alignment quality in multilingual embeddings
- Evidence anchors:
  - [abstract] "abstract concepts exhibit better alignment than physical concepts"
  - [section] "abstract concept words are considerably more frequent than the physical concept words" and "abstract concepts apply very generally across contexts and domains"
  - [corpus] No direct evidence found; corpus shows weak relevance (0 citations)
- Break condition: When controlling for frequency eliminates the abstract-physical difference

## Foundational Learning

- Concept: Vector space alignment and Procrustes analysis
  - Why needed here: The study relies on linear transformations (Procrustes analysis) to align concept spaces between languages
  - Quick check question: How does Procrustes analysis find the optimal rotation and scaling to align two vector spaces?

- Concept: Nearest neighbor search with CSLS
  - Why needed here: Cross-domain local scaling (CSLS) is used to retrieve the most similar concepts after alignment
  - Quick check question: What advantage does CSLS have over standard nearest neighbor search in cross-lingual retrieval?

- Concept: Precision@k evaluation
  - Why needed here: The study uses precision@k to measure retrieval performance of cross-lingual concept pairs
  - Quick check question: How does precision@k differ from other retrieval metrics like recall or mean reciprocal rank?

## Architecture Onboarding

- Component map: WordNet concepts -> LLM embeddings -> Procrustes alignment -> CSLS retrieval -> Precision@k calculation
- Critical path:
  1. Extract concept embeddings from source language model
  2. Learn linear mapping using seed dictionary (Procrustes)
  3. Transform source embeddings to target space
  4. Perform CSLS retrieval
  5. Calculate precision@k
- Design tradeoffs:
  - Vanilla vs prompt-based embeddings: Tradeoff between alignment quality and task-specific utility
  - Seed dictionary size: Larger dictionaries improve alignment but increase computational cost
  - Model size: Larger models show better alignment but require more resources
- Failure signatures:
  - Low precision before alignment indicates poor implicit alignment
  - Large gap between before/after alignment suggests model is not learning useful linear mappings
  - Poor performance on non-Indo-European languages indicates typological distance issues
- First 3 experiments:
  1. Extract vanilla embeddings from a small model (e.g., Llama2-7B) and measure baseline precision
  2. Apply Procrustes alignment with 1000-concept seed dictionary and compare performance
  3. Repeat with prompt-based embeddings and analyze linearity differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does frequency control for physical versus abstract concepts eliminate the observed performance gap in cross-lingual concept alignment?
- Basis in paper: [inferred] The authors hypothesize that abstract concepts align better than physical concepts due to higher frequency and diverse contextual usage, but acknowledge this correlation needs further investigation
- Why unresolved: The authors suggest frequency as an explanation but don't actually control for it in their analysis, leaving open whether the observed differences are truly semantic or merely statistical
- What evidence would resolve it: Experimental results showing concept alignment performance when physical and abstract concepts are matched for frequency, or when frequency is included as a covariate in the analysis

### Open Question 2
- Question: How does prompt-based embedding linearity vary across different prompt templates and prompt engineering strategies?
- Basis in paper: [explicit] The authors observe that prompt-based embeddings exhibit significantly lower linearity and larger performance gaps compared to vanilla embeddings, suggesting prompting breaks implicit concept alignment
- Why unresolved: The study uses only one prompt template adapted from Li and Li (2023), leaving open whether alternative prompt formulations might preserve or even enhance concept space alignment
- What evidence would resolve it: Comparative experiments testing multiple prompt templates, including variations in prompt length, structure, and instruction style, measuring both linearity and alignment quality

### Open Question 3
- Question: To what extent does token-level overlap between languages (e.g., cognates, loanwords) artificially inflate cross-lingual concept alignment performance?
- Basis in paper: [explicit] The authors acknowledge that identical word forms across languages with Latin scripts could explain some alignment success and calculate this ratio to be limited (around 15% for French, 10% for Romanian, etc.)
- Why unresolved: While the authors quantify the proportion of identical word forms, they don't systematically analyze how this overlap affects alignment quality or attempt to control for it in their main results
- What evidence would resolve it: Experiments filtering out concept pairs with identical surface forms, or using artificially constructed non-overlapping vocabularies to measure alignment performance in their absence

## Limitations
- Focus on WordNet concepts may not represent full diversity of linguistic concepts or domain-specific terminology
- Correlation between concept frequency and alignment quality doesn't establish causation without frequency-controlled experiments
- Performance on non-Indo-European languages (Basque, Finnish, Japanese, Thai) may be limited by typological distance from Indo-European languages

## Confidence
- High confidence: Larger multilingual LLMs exhibit high-quality linear alignments between concept spaces across languages
- Medium confidence: Prompt-based embeddings partially break implicit concept alignments
- Medium confidence: Abstract concepts align better than physical concepts, primarily due to frequency differences

## Next Checks
1. Replicate the frequency-abstractness experiment with frequency-controlled subsets to isolate whether abstractness or frequency drives better alignment performance
2. Test alignment quality on domain-specific concept sets (medical, technical, colloquial) to assess generalizability beyond WordNet
3. Evaluate alignment stability across different embedding extraction methods (CLS token, mean pooling, last token) to determine which yields most consistent cross-lingual mappings