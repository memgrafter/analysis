---
ver: rpa2
title: 'Adaptive Prompting for Continual Relation Extraction: A Within-Task Variance
  Perspective'
arxiv_id: '2412.08285'
source_url: https://arxiv.org/abs/2412.08285
tags:
- prompt
- task
- relation
- experts
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new approach for Continual Relation Extraction
  (CRE) that leverages task-specific prompt pools and generative models to address
  catastrophic forgetting and improve performance over existing methods. The core
  idea is to allocate a dedicated prompt pool for each task, capturing within-task
  variations while maintaining cross-task variance, and using a generative model to
  replay past-task knowledge without storing data.
---

# Adaptive Prompting for Continual Relation Extraction: A Within-Task Variance Perspective

## Quick Facts
- arXiv ID: 2412.08285
- Source URL: https://arxiv.org/abs/2412.08285
- Authors: Minh Le; Tien Ngoc Luu; An Nguyen The; Thanh-Thien Le; Trang Nguyen; Tung Thanh Nguyen; Linh Ngo Van; Thien Huu Nguyen
- Reference count: 20
- Primary result: WAVE-CRE achieves up to 15% improvement over rehearsal-free baselines and competitive results with rehearsal-based approaches for continual relation extraction

## Executive Summary
This paper addresses continual relation extraction (CRE) by proposing WAVE-CRE, a method that leverages task-specific prompt pools and generative models to combat catastrophic forgetting while improving performance. The approach allocates dedicated prompt pools for each task to capture within-task variations and maintain cross-task variance, while using generative models to replay past-task knowledge without storing raw data. Experiments on FewRel and TACRED datasets demonstrate significant improvements over state-of-the-art prompt-based and rehearsal-free baselines.

## Method Summary
WAVE-CRE implements task-specific prompt pools where each task has its own dedicated set of prompts, addressing prompt selection inaccuracies in existing methods. The approach uses a generative model to capture Gaussian distributions over latent representations for each relation, enabling knowledge replay without explicit data storage. A task predictor trained on synthetic query representations from these distributions improves task identification accuracy. The method employs prefix-tuning to add new experts (task-specific prompt pools) to pre-trained models, maintaining shared parameters across tasks while preventing catastrophic forgetting through latent representation replay.

## Key Results
- WAVE-CRE outperforms existing rehearsal-free methods by up to 15% on FewRel and TACRED datasets
- Task-specific prompt pools improve cross-task variance handling compared to shared prompt pools
- The generative replay approach effectively mitigates catastrophic forgetting without storing raw data
- Precise task prediction through synthetic query representations contributes to overall performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific prompt pools prevent prompt selection inaccuracies by ensuring that each task has its own dedicated set of prompts.
- Mechanism: By allocating a unique prompt pool to each task, the model avoids the problem of sharing prompts across different tasks, which can lead to mismatched prompts during training and testing.
- Core assumption: Prompt selection inaccuracies in existing methods stem from using a common prompt pool where instances from different tasks share prompts, leading to poor cross-task variance handling.
- Evidence anchors:
  - [abstract] "inaccurate prompt selection, inadequate mechanisms for mitigating forgetting in shared parameters, and suboptimal handling of cross-task and within-task variances"
  - [section 1] "task-specific prompt-based approaches (Wang et al. 2022a, 2023a) are prone to inaccuracies in prompt selection, leading to a mismatch between training and testing prompts"
  - [corpus] Weak evidence - no direct citations found supporting this specific mechanism

### Mechanism 2
- Claim: Generative models for latent representation replay prevent catastrophic forgetting without storing raw data.
- Mechanism: The model learns Gaussian distributions over latent representations (prompted embeddings) for each relation, enabling replay of past knowledge through sampling rather than data storage.
- Core assumption: Generating continuous latent representations is significantly more feasible than generating natural language text, making it practical for replay without privacy concerns.
- Evidence anchors:
  - [abstract] "we incorporate a generative model to consolidate prior knowledge within shared parameters, eliminating the need for explicit data storage"
  - [section 3.2] "we utilize a generative model that captures the distributions of observed relations, enabling the replay of relation samples"
  - [corpus] No direct corpus evidence found supporting this specific generative replay approach

### Mechanism 3
- Claim: Task predictor trained on synthetic query representations improves task identification accuracy over baselines.
- Mechanism: Instead of using raw BERT embeddings or treating all relations within a task as a single class, the model trains a dedicated MLP predictor on synthesized query representations from each relation's distribution.
- Core assumption: Existing task identification methods (like EPI's Mahalanobis distance or HiDe-Prompt's task-level classification) are suboptimal because they don't leverage relation-specific information or properly separate semantic classes.
- Evidence anchors:
  - [abstract] "We reveal limitations of current prompt-based approaches, including inaccurate prompt selection, inadequate strategies for mitigating forgetting in shared parameters"
  - [section 3.3] "HiDe-Prompt categorizes all relations within a task as a single class in the cross-entropy loss. This strategy can be suboptimal, as the resulting classes may lack semantic significance"
  - [corpus] Weak evidence - no direct citations found supporting this specific task predictor design

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: Understanding MoE helps explain how prefix-tuning can be viewed as adding new experts to pre-trained models, which is the foundation for the task-specific prompt pool design
  - Quick check question: How does the TopK function in MoE improve efficiency compared to using all experts for every input?

- Concept: Prefix-tuning and its relationship to MoE
  - Why needed here: The paper builds its approach on the insight that prefix-tuning implements new experts in an MoE framework, which justifies using task-specific prompt pools as expert sets
  - Quick check question: In the context of self-attention, how does prefix-tuning modify the attention computation to incorporate new experts?

- Concept: Gaussian distribution fitting for generative models
  - Why needed here: The method relies on fitting Gaussian distributions to latent representations for each relation, which requires understanding how to estimate mean vectors and covariance matrices from data
  - Quick check question: What are the parameters needed to fully specify a multivariate Gaussian distribution, and how are they estimated from sample data?

## Architecture Onboarding

- Component map:
  Frozen BERT encoder -> Task-specific prompt pools -> Query generation mechanism -> Generative models (Gaussian distributions) -> Task predictor (MLP) -> Relation classifier (shared head)

- Critical path:
  1. Input sentence → BERT encoding → query vector
  2. Query vector + task ID → prompt pool selection → prompted input
  3. Prompted input → BERT encoder → latent representation
  4. Latent representation → relation classifier → prediction
  5. During training: update prompt pool, train generative models, train task predictor

- Design tradeoffs:
  - Task-specific pools vs. shared pools: Higher memory usage but better cross-task separation
  - Gaussian distributions vs. other generative models: Simpler and more memory-efficient but may not capture complex distributions
  - Fixed prompt pool size vs. adaptive sizing: Consistent memory usage but may not match task complexity

- Failure signatures:
  - Poor performance on early tasks indicates catastrophic forgetting
  - High variance in task prediction accuracy suggests poor separation of relation distributions
  - Degraded performance when switching tasks indicates prompt selection issues

- First 3 experiments:
  1. Verify that task-specific prompt pools improve over single-prompt baselines on a simple task-incremental setup
  2. Test generative replay by comparing performance with and without the generative model component
  3. Evaluate task predictor accuracy on synthetic data before integrating with the full system

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of generative model (e.g., Gaussian vs. other distributions) affect the performance of WAVE-CRE in continual relation extraction?
- Basis in paper: [explicit] The paper mentions using a Gaussian distribution for generative models but suggests future work could explore alternative generative models.
- Why unresolved: The paper only uses Gaussian distributions for the generative model and does not compare its performance with other distributions.
- What evidence would resolve it: Experiments comparing the performance of WAVE-CRE using different generative models (e.g., Gaussian, mixture models, variational autoencoders) on the same datasets.

### Open Question 2
- Question: How does the task predictor's performance impact the overall accuracy of WAVE-CRE in real-world scenarios with noisy or ambiguous task identities?
- Basis in paper: [explicit] The paper highlights the importance of precise task prediction for improved performance and compares task prediction precision against baselines.
- Why unresolved: The paper does not provide an in-depth analysis of how the task predictor's performance under noisy or ambiguous conditions affects the overall model accuracy.
- What evidence would resolve it: Experiments evaluating WAVE-CRE's performance on datasets with intentionally added noise or ambiguity in task identities, and analyzing the correlation between task prediction accuracy and overall model performance.

### Open Question 3
- Question: Can the task-specific prompt pools be optimized further to improve cross-task variance while maintaining within-task variations?
- Basis in paper: [explicit] The paper discusses the use of task-specific prompt pools to capture within-task variations and enhance cross-task variances, but acknowledges challenges in optimizing both.
- Why unresolved: The paper does not explore advanced techniques for optimizing the balance between cross-task and within-task variances in the prompt pools.
- What evidence would resolve it: Experiments comparing WAVE-CRE's performance with different strategies for optimizing prompt pools, such as adaptive prompt allocation or dynamic prompt pool resizing, and analyzing their impact on cross-task and within-task variances.

## Limitations
- The method assumes discrete task boundaries, which may not reflect real-world continuous relation arrival scenarios
- The effectiveness depends on Gaussian distribution assumptions for latent representations, which may not hold for all relation types
- Task-specific prompt pools increase memory usage proportionally with the number of tasks, potentially limiting scalability

## Confidence
- High Confidence: The experimental results showing WAVE-CRE outperforming baseline methods on FewRel and TACRED datasets
- Medium Confidence: The claim that task-specific prompt pools are superior to shared pools for cross-task variance handling
- Low Confidence: The assertion that the task predictor design is significantly better than existing approaches like EPI's Mahalanobis distance

## Next Checks
1. Conduct quantitative analysis of how well Gaussian distributions fit the latent representations for each relation using goodness-of-fit tests to verify the generative model assumption
2. Test WAVE-CRE on datasets with different relation characteristics (e.g., ACE05 or NYT) to evaluate whether the performance gains generalize beyond FewRel and TACRED
3. Measure the actual memory footprint of task-specific prompt pools versus shared pools across all tasks, including the additional parameters from generative models, to validate claimed efficiency benefits