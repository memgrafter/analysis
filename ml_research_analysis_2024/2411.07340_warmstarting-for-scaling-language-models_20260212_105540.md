---
ver: rpa2
title: Warmstarting for Scaling Language Models
arxiv_id: '2411.07340'
source_url: https://arxiv.org/abs/2411.07340
tags:
- warmstarting
- training
- base
- learning
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates warmstarting large language model pretraining\
  \ by leveraging weights from smaller, tuned models. The authors propose a simple\
  \ method that shrinks and zero-pads smaller model weights, then applies scaled \xB5\
  P initialization to warmstart larger models."
---

# Warmstarting for Scaling Language Models

## Quick Facts
- arXiv ID: 2411.07340
- Source URL: https://arxiv.org/abs/2411.07340
- Authors: Neeratyoy Mallik, Maciej Janowski, Johannes Hog, Herilalaina Rakotoarison, Aaron Klein, Josif Grabocka, Frank Hutter
- Reference count: 35
- Primary result: Simple warmstarting method using zero-padding and scaled µP initialization accelerates convergence across model scales while maintaining training stability

## Executive Summary
This work investigates warmstarting large language model pretraining by leveraging weights from smaller, tuned models. The authors propose a simple method that shrinks and zero-pads smaller model weights, then applies scaled µP initialization to warmstart larger models. Experiments show this approach consistently accelerates convergence compared to vanilla µP across multiple model scales, with initial loss improvements and similar or better final performance. The method preserves µP training stability, as verified by layer activation norms, and works for a range of shrinking factors up to 0.6. Successive warmstarting across staged model growth is also demonstrated.

## Method Summary
The method shrinks smaller model weights by a factor λ_shrink, zero-pads them to match larger model dimensions, and adds scaled µP initialization noise. This approach leverages the µP framework's ability to transfer optimal hyperparameters across scales while providing a better starting point for training. The authors test this on GPT2 models of varying widths (5M-70M parameters) using the SlimPajama-6B dataset, comparing warmstarted training against vanilla µP baselines.

## Key Results
- Warmstarting consistently accelerates convergence across model scales, with initial loss improvements over vanilla µP
- The method maintains µP training stability, verified through consistent layer activation L1-norms across width-scaling
- Works for shrinking factors up to 0.6 before training instability emerges
- Demonstrates successful successive warmstarting across staged model growth
- Achieves similar or better final performance compared to vanilla µP with equivalent compute

## Why This Works (Mechanism)

### Mechanism 1
Zero-padding with scaled µP initialization enables stable warmstarting by preserving the relative activation norms across model scales. The approach uses zero-padding to expand smaller model weights to match the larger model shape, then applies µP initialization as a perturbation. The shrinking factor (λ_shrink) scales the base weights, while µP initialization maintains the L1 norm of layer activations consistently across widths. This breaks when λ_shrink exceeds 0.6, as observed in activation norm deviations.

### Mechanism 2
Warmstarting accelerates convergence by providing a better initial parameter state that is closer to the optimal solution. By initializing larger models with shrunk and zero-padded smaller model weights, the warmstarted model starts training from a parameter configuration that has already learned useful feature representations. The µP perturbation allows the model to adjust these weights while maintaining stable learning dynamics. This fails when the base model is too small relative to the target model.

### Mechanism 3
The modular approach of separating weight transformation from initialization scaling enables flexibility in warmstarting strategies. By structuring the warmstarting operation as θ_target = λ_shrink · Pad0(θ_base) + N(0, σ²_µP), the method allows different transformations of base weights while maintaining the theoretical guarantees of µP initialization. This modular design means the weight transformation can be changed without affecting the initialization scaling rules. This breaks if the transformation fundamentally changes the statistical properties of the initialization distribution.

## Foundational Learning

- **Scaled parameterizations (abc-parameterization)**: Understanding how hyperparameters scale with model size is crucial for the theoretical foundation of warmstarting. The method relies on µP's scaling rules to maintain optimal hyperparameters when transferring from smaller to larger models. Quick check: How do the A_w, B_w, and C_w parameters in abc-parameterization relate to the shrinking factor λ_shrink and µP initialization in the warmstarting method?

- **Zero-shot hyperparameter transfer**: The method claims to transfer optimal hyperparameters from smaller to larger models without additional tuning. Understanding the principles behind zero-shot transfer helps explain why warmstarting works across scales. Quick check: What guarantees does µTransfer provide that allow hyperparameters optimized on a 5M parameter model to remain optimal on a 70M parameter model?

- **Layer activation norm stability**: The experimental validation shows that warmstarting maintains the L1 norm of layer activations across scales, which is a key indicator of stable training dynamics in µP. Quick check: Why is maintaining consistent L1 norms of layer activations across model scales important for stable feature learning?

## Architecture Onboarding

- **Component map**: Base model -> Zero-padding function -> Shrinking factor (λ_shrink) -> µP initialization -> Target model -> Training loop

- **Critical path**: 
  1. Train base model with tuned hyperparameters
  2. Extract base model weights layer-by-layer
  3. Apply zero-padding to match target layer dimensions
  4. Scale padded weights by λ_shrink
  5. Add µP initialization perturbation
  6. Initialize target model with resulting weights
  7. Train target model with transferred hyperparameters

- **Design tradeoffs**: Simple zero-padding vs. learned transformations (zero-padding is simple but may waste capacity; learned transformations could be more efficient but add complexity); Shrinking factor choice (λ_shrink = 0.4 balances regularization and information preservation, but optimal value may vary by task); Base model selection (larger base models provide more useful structure but reduce the relative scale difference)

- **Failure signatures**: Instability in training (loss spikes, diverging activations) likely indicates λ_shrink too high (> 0.6); No convergence improvement suggests base model too small or zero-padding not providing useful structure; Worse final performance than vanilla µP indicates insufficient perturbation from µP initialization or base model not well-tuned

- **First 3 experiments**:
  1. Implement zero-padding and verify that expanding a 5M parameter model to 10M parameters produces the expected weight matrix dimensions with zeros in new positions
  2. Test different λ_shrink values (0.1, 0.4, 0.9) on a small model scale to observe effects on training stability and convergence speed
  3. Compare layer activation L1 norms across scales for warmstarted vs vanilla µP runs to verify stability guarantees are maintained

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal shrinking factor (λshrink) for warmstarting across different model scales and architectures? The paper notes that λshrink = 0.4 works well in their setup but observes instability for λshrink > 0.6, suggesting there's a sweet spot that varies with scale. This remains unresolved because the authors only tested λshrink values from 0.1 to 1.0 on a limited set of model scales (5M-70M) and architectures (GPT-2 variants). Systematic experiments varying λshrink across multiple orders of magnitude in model scale and different architectures would identify generalizable patterns.

### Open Question 2
Does warmstarting with µP maintain training stability guarantees when scaling along dimensions other than width, such as depth or context length? The authors state they only looked at model-width as the scaling dimension and suggest future work should test other dimensions. This is unresolved because the theoretical guarantees of µP and the warmstarting technique may not transfer to other scaling dimensions, potentially leading to instability or suboptimal performance. Experiments scaling models along depth, context length, and other dimensions while monitoring activation norms, gradient stability, and final performance would determine if the approach generalizes.

### Open Question 3
How does warmstarting affect the compute-loss scaling coefficient compared to training from scratch with µP? The authors note that warmstarting consumes more tokens than vanilla-µP for the same compute, but don't analyze whether this leads to better compute efficiency overall. This is unresolved because while warmstarting improves convergence speed, the increased token consumption might offset efficiency gains, and the authors don't provide a full compute analysis. Direct comparison of FLOPs-to-loss ratios for warmstarted vs. vanilla-µP training across multiple model scales would quantify the true efficiency impact.

### Open Question 4
What is the theoretical relationship between the shrinking factor λshrink and the abc-parameterization framework? The authors suggest exploring the role of layer-wise shrinking and its theoretical relation to abc-parameterization, noting their approach can be seen as equivalent to abc-parameterization. This is unresolved because the authors treat λshrink as an empirical hyperparameter without connecting it to the theoretical framework of scaled parameterizations, leaving the theoretical justification incomplete. Deriving λshrink from first principles using the abc-parameterization framework or proving bounds on its optimal value based on model scale ratios would provide theoretical grounding.

## Limitations
- The method shows performance gains up to λ_shrink = 0.6, but the theoretical justification for this upper bound is not fully explored
- Experimental scope is limited to decoder-only GPT2 models on language modeling tasks, leaving effectiveness for other architectures and domains untested
- While warmstarting improves convergence speed, the analysis doesn't explore whether models learn fundamentally different representations compared to vanilla µP training

## Confidence
- **High Confidence**: The core mechanism of zero-padding smaller model weights with scaled µP initialization is technically sound and produces measurable training speed improvements across the tested scales
- **Medium Confidence**: The claim that warmstarting provides "similar or better final performance" is supported by experiments, but the sample size is limited
- **Low Confidence**: The assertion that warmstarting "preserves µP training stability" relies on layer activation norm consistency, but doesn't explore other potential stability indicators like gradient norms

## Next Checks
1. **Cross-Architecture Validation**: Apply the warmstarting method to encoder-decoder transformer architectures and convolutional networks to test generalizability beyond decoder-only GPT models
2. **Representation Analysis**: Compare the learned representations (via probing classifiers or similarity metrics) between warmstarted and vanilla µP models at various training stages to determine if warmstarting leads to fundamentally different feature learning
3. **Scaling Boundary Exploration**: Systematically test λ_shrink values beyond 0.6 and explore alternative weight transformation methods (learned padding, interpolation) to understand the limits of the current approach and potential improvements