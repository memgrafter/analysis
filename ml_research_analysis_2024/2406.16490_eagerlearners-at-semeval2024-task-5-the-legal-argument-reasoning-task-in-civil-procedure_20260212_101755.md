---
ver: rpa2
title: 'eagerlearners at SemEval2024 Task 5: The Legal Argument Reasoning Task in
  Civil Procedure'
arxiv_id: '2406.16490'
source_url: https://arxiv.org/abs/2406.16490
tags:
- legal
- language
- data
- large
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study explored three approaches to classify legal case summaries
  in U.S. civil procedure.
---

# eagerlearners at SemEval2024 Task 5: The Legal Argument Reasoning Task in Civil Procedure

## Quick Facts
- arXiv ID: 2406.16490
- Source URL: https://arxiv.org/abs/2406.16490
- Authors: Hoorieh Sabzevari; Mohammadmostafa Rostamkhani; Sauleh Eetemadi
- Reference count: 5
- The zero-shot method with Bing Copilot achieved the highest macro F1 score of 64% on the test set.

## Executive Summary
This study explores three approaches to classify legal case summaries in U.S. civil procedure: long-input transformer models, legal-domain fine-tuned models, and zero-shot large language models. The dataset consists of 848 cases, split into 80% train, 10% validation, and 10% test. The zero-shot approach with Bing Copilot, leveraging real-time legal knowledge and structured prompt engineering, outperformed fine-tuned models, achieving a macro F1 score of 64% on the test set.

## Method Summary
The authors evaluated three approaches: 1) long-input transformers (Longformer, Big Bird) to handle lengthy legal explanations, 2) legal-domain fine-tuned models (Legal-RoBERTa, Legal-XLM-RoBERTa) with focal loss to address class imbalance, and 3) zero-shot prompting with large language models (GPT-3.5, Gemini, Bing Copilot). Focal loss was used to mitigate the severe class imbalance (label 0 ≈ 3x label 1). Prompt engineering, including explicit "step-by-step" reasoning requests, was employed to improve zero-shot model outputs. Models were evaluated using macro F1 score on a fixed 80/10/10 train/validation/test split.

## Key Results
- Zero-shot Bing Copilot achieved the highest macro F1 score of 64% on the test set.
- Legal-RoBERTa with focal loss was the best fine-tuned model (0.58 F1 on validation).
- Class imbalance (label 0 ≈ 3x label 1) was mitigated using focal loss, improving minority class recall.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot prompting with Bing Copilot outperformed fine-tuned legal domain models due to Copilot's access to real-time legal knowledge and superior handling of long, complex case explanations.
- Mechanism: The zero-shot approach leverages Bing Copilot's ability to integrate external web-based legal information and contextual reasoning, allowing it to synthesize explanations more effectively than models trained only on static legal corpora.
- Core assumption: Bing Copilot's real-time access to updated legal sources compensates for the lack of domain-specific fine-tuning, and the model's reasoning capacity is not bottlenecked by input token limits.
- Evidence anchors:
  - [abstract] "Our findings show how well the zero-shot method of large language models can understand complicated data. We achieved our highest F1 score of 64% in these experiments."
  - [section] "Bing's Copilot model emerged as the top performer, surpassing expectations. Following suit, the GPT 3.5 model presented moderate performance, while the Gemini model fell short of expected levels. The success of the Copilot model lies in its ability to address previous challenges associated with GPT models by leveraging real-time information accessible through the internet."
  - [corpus] Weak—no direct citation to external studies comparing zero-shot vs fine-tuned legal models; evidence is purely from this paper's results.
- Break condition: If the task's legal domain requires highly specialized, precedent-based reasoning not well represented in public web sources, or if Bing's Copilot's input token limit (4000 characters) truncates critical explanatory text.

### Mechanism 2
- Claim: Focal loss effectively mitigates class imbalance in the legal argument dataset, improving model sensitivity to minority class (label 1) examples.
- Mechanism: By down-weighting well-classified examples and focusing training loss on misclassified or minority class instances, focal loss enables better learning of the harder-to-classify legal reasoning patterns.
- Core assumption: The class imbalance (label 0 instances outnumber label 1 nearly 3:1) is a major cause of poor minority class recall, and focal loss can adjust the learning process without overfitting.
- Evidence anchors:
  - [section] "We opted to mitigate the class imbalance using the focal loss function as our loss function. Our investigation demonstrates the efficacy of focal loss in rectifying class imbalance, enhancing the performance of classes with limited training samples, offering adaptability in adjusting the learning process, and attenuating the impact of noisy data."
  - [section] "Table 2 presents the performance metrics of the models from the initial two phases, evaluated on the validation dataset." (Shows Legal-BERT with 0.58 F1, better than others without focal loss.)
  - [corpus] No external citation to focal loss effectiveness in legal text classification—evidence limited to this paper's experiments.
- Break condition: If the imbalance is too severe or the minority class is intrinsically noisy/ambiguous, focal loss may not recover sufficient recall.

### Mechanism 3
- Claim: Prompt engineering that explicitly requests step-by-step reasoning and justification improves zero-shot model performance on legal argument tasks.
- Mechanism: Structured prompts that guide the model to decompose reasoning into explicit steps reduce ambiguity, leading to more consistent label extraction and higher accuracy.
- Core assumption: Legal reasoning tasks benefit from explicit procedural prompts, and Bing Copilot (or similar models) can follow multi-step instructions reliably within input token limits.
- Evidence anchors:
  - [section] "Our investigation revealed that incorporating terms such as 'step by step' or asking for explanations of the inference steps to achieve the desired outcome positively impacted the model's performance."
  - [section] "Table 3 presents the results achieved from employing this method on the test dataset, representing the unofficial results submitted during the post-evaluation phase." (Copilot F1 0.64 vs GPT 3.5 F1 0.59.)
  - [corpus] Weak—no citation to prior studies on prompt structure for legal argument tasks; evidence is purely from this paper's results.
- Break condition: If the model's input token limit is exceeded or if the legal explanations are too complex to decompose reliably in a single prompt.

## Foundational Learning

- Concept: Understanding the focal loss function and its tunable focusing parameter γ.
  - Why needed here: The dataset exhibits severe class imbalance (label 0 ≈ 3x label 1), and focal loss is used to improve minority class learning.
  - Quick check question: What happens to the focal loss term (1 − pt)^γ when pt (the model's confidence on a correctly classified example) is close to 1?

- Concept: Zero-shot prompting strategies and prompt engineering.
  - Why needed here: The best results came from zero-shot Bing Copilot with carefully structured prompts; understanding how to design and iterate prompts is critical.
  - Quick check question: Why might adding "step by step" or asking for explicit reasoning explanations improve model output consistency in a legal argument task?

- Concept: Token limits and their impact on long document processing in transformer-based models.
  - Why needed here: Legal case summaries and explanations are long; Bing Copilot's 4000-character limit caused truncation issues, affecting results.
  - Quick check question: How does input truncation due to token limits potentially bias the model's reasoning in a legal argument classification task?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Longformer/Big Bird training -> Legal-RoBERTa/XLM-RoBERTa fine-tuning -> Bing Copilot/GPT-3.5/Gemini zero-shot -> Macro F1 evaluation

- Critical path:
  1. Load dataset, balance classes via focal loss.
  2. Train long-input models (Longformer/Big Bird).
  3. Fine-tune legal domain models (Legal-RoBERTa/XLM-RoBERTa).
  4. Design and test prompts for zero-shot models.
  5. Evaluate and compare F1 scores.

- Design tradeoffs:
  - Long-input models (Longformer/Big Bird) can process full texts but may underfit on legal domain specifics vs. fine-tuned models.
  - Fine-tuned legal models capture domain nuance but struggle with long inputs; focal loss helps but may not overcome inherent dataset difficulty.
  - Zero-shot models avoid retraining but depend on prompt quality and external knowledge; Bing Copilot's real-time access is a plus but token limits are a bottleneck.

- Failure signatures:
  - Low recall on minority class → focal loss parameter tuning or resampling needed.
  - Sudden drop in F1 after prompt change → prompt may exceed token limits or be ambiguous.
  - All models plateau at similar F1 → dataset may be intrinsically ambiguous or too small.

- First 3 experiments:
  1. Train Longformer and Big Bird on the full training set, evaluate on validation, compare macro F1.
  2. Fine-tune Legal-RoBERTa with focal loss (γ=2), train for 3 epochs, evaluate macro F1.
  3. Test Bing Copilot with a structured "step-by-step" prompt on a small validation subset, record accuracy and F1, compare against GPT 3.5.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt engineering strategies affect the performance of large language models on legal argument reasoning tasks?
- Basis in paper: [explicit] The authors mention using various prompts and refining them through prompt engineering techniques, observing that incorporating terms like "step by step" or asking for explanations of inference steps positively impacted performance.
- Why unresolved: The paper does not provide a detailed analysis of which specific prompt engineering strategies are most effective or how they compare to each other.
- What evidence would resolve it: Systematic experiments comparing different prompt engineering approaches (e.g., step-by-step reasoning, chain-of-thought prompting, few-shot examples) on the same task and measuring their impact on model performance.

### Open Question 2
- Question: What is the impact of data summarization techniques on the performance of models dealing with long legal texts?
- Basis in paper: [explicit] The authors suggest that data summarization could be a future work to help evaluate various models' performance on lengthy datasets, acknowledging that important details might be overlooked during summarization.
- Why unresolved: The paper does not explore or evaluate any data summarization techniques, leaving their potential benefits and drawbacks unexplored.
- What evidence would resolve it: Experiments applying different data summarization methods to the legal dataset and comparing model performance on summarized vs. full-length texts.

### Open Question 3
- Question: How does a multi-model approach, such as the round-table conference technique, improve reasoning in legal argument tasks compared to single model approaches?
- Basis in paper: [explicit] The authors propose testing the effectiveness of a multi-model approach where different large language model agents discuss and reach consensus, as a potential future research direction.
- Why unresolved: The paper does not implement or evaluate this multi-model approach, so its effectiveness remains theoretical.
- What evidence would resolve it: Implementation and comparison of a round-table conference approach using multiple models versus single model approaches on the same legal reasoning task, measuring improvements in accuracy and reasoning quality.

## Limitations
- Results are based on a single competition dataset (848 cases), limiting generalizability.
- The study does not isolate the effects of real-time web access versus prompt engineering in Bing Copilot's performance.
- Exact prompt templates used for zero-shot models are not specified, hindering direct replication.

## Confidence

- **High confidence**: The use of focal loss to address class imbalance is a well-established technique, and its implementation in this context is sound.
- **Medium confidence**: The superiority of Bing Copilot's zero-shot approach over fine-tuned models is supported by the competition results, but the specific mechanisms (real-time web access vs. prompt engineering) are not isolated or validated through ablation studies.
- **Low confidence**: The claim that Bing Copilot's real-time legal knowledge access is the primary driver of its performance is speculative, as the study does not compare zero-shot results with and without web access or test alternative prompt strategies systematically.

## Next Checks

1. Conduct an ablation study to compare zero-shot Bing Copilot performance with and without real-time web access, and with alternative prompt structures (e.g., varying levels of reasoning detail).
2. Expand the dataset to include additional legal case summaries from multiple jurisdictions to assess generalizability of the zero-shot approach.
3. Implement cross-validation with k-fold splits to verify that the 64% F1 score is not an artifact of the specific train/val/test split or dataset size.