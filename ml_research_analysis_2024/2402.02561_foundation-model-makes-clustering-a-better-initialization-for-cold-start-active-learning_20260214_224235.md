---
ver: rpa2
title: Foundation Model Makes Clustering A Better Initialization For Cold-Start Active
  Learning
arxiv_id: '2402.02561'
source_url: https://arxiv.org/abs/2402.02561
tags:
- clustering
- learning
- samples
- active
- initialization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of selecting informative initial
  samples for cold-start active learning in medical image analysis. The proposed method
  integrates foundation models with clustering to overcome limitations of random sampling
  and naive clustering.
---

# Foundation Model Makes Clustering A Better Initialization For Cold-Start Active Learning

## Quick Facts
- arXiv ID: 2402.02561
- Source URL: https://arxiv.org/abs/2402.02561
- Authors: Han Yuan; Chuan Hong
- Reference count: 40
- Primary result: Foundation model embeddings + clustering outperform random sampling for cold-start active learning in pneumothorax classification and segmentation

## Executive Summary
This study addresses the critical challenge of selecting informative initial samples for cold-start active learning in medical image analysis. The proposed method integrates foundation models with clustering to overcome limitations of random sampling and naive clustering. Foundation models generate low-dimensional, informative embeddings that improve clustering convergence and sample selection quality. Experiments on pneumothorax classification and segmentation tasks demonstrate superior performance compared to baseline methods.

## Method Summary
The method uses foundation models to generate embeddings from medical images, which are then clustered using K-means to select representative initial samples for active learning. For classification, a VGG-11 network is trained, while segmentation uses a U-Net with VGG-11 backbone. The framework is tested on the ChestX-Det dataset with budgets of 20, 40, 60, 80, and 100 samples. Four foundation models (TXRV, CXRF, REMEDIS, and ImageNet) are compared against random sampling and naive clustering baselines using AUPRC, F1 score, DSC, and HD metrics.

## Key Results
- CXRF-based clustering achieved highest AUPRC values in 3 out of 5 budgets for classification
- ImageNet-based clustering showed best F1 scores in 3 scenarios for classification
- TXRV-based clustering attained highest DSC in 3 out of 5 budgets for segmentation
- Foundation model embeddings consistently outperformed random sampling across all budget levels

## Why This Works (Mechanism)

### Mechanism 1
Foundation model embeddings improve clustering convergence by reducing input dimensionality. Foundation models trained on large datasets generate compact, informative embeddings that replace high-dimensional raw pixel values as clustering inputs. Core assumption: Lower-dimensional embeddings preserve task-relevant information while eliminating noise. Evidence: Foundation models generate informative and compacted embeddings for various downstream tasks. Break condition: If embeddings lose task-specific discriminative features during dimensionality reduction.

### Mechanism 2
Foundation model embeddings improve initial sample quality by capturing semantic similarity. Foundation models encode semantic relationships between samples, allowing clustering to identify truly representative samples rather than just visually similar ones. Core assumption: Foundation models learn transferable representations that align with task-relevant features. Evidence: Foundation models are capable of generating informative and condensed embeddings, serving as effective inputs for clustering. Break condition: If foundation models overfit to their pretraining domain and fail to capture task-specific variations.

### Mechanism 3
Foundation model embeddings reduce sampling instability in low-budget scenarios. By providing semantically meaningful representations, foundation models enable clustering to consistently identify diverse and representative samples even with small budgets. Core assumption: Random sampling with limited budgets produces highly variable results due to lack of semantic guidance. Evidence: Random sampling remained powerful in high budgets but presented highest instability across all scenarios. Break condition: If foundation model embeddings are too general and fail to capture domain-specific variations.

## Foundational Learning

- Concept: Active Learning Fundamentals
  - Why needed here: Understanding the distinction between cold-start initialization and subsequent sample selection
  - Quick check question: What's the key difference between random sampling and uncertainty sampling in active learning?

- Concept: Clustering Algorithms
  - Why needed here: K-means clustering is used to select initial samples, but its behavior changes with different input representations
  - Quick check question: How does the choice of distance metric affect clustering results with foundation model embeddings?

- Concept: Foundation Model Architecture
  - Why needed here: Understanding how different foundation models (TXRV, CXRF, REMEDIS) generate embeddings and their domain specialization
  - Quick check question: What architectural differences between these models might explain their varying performance on classification vs segmentation tasks?

## Architecture Onboarding

- Component map: Foundation Model Layer → Clustering Module → Sample Selection → Model Training → Uncertainty Module → Evaluation
- Critical path: Foundation Model → Clustering → Sample Selection → Model Training → Evaluation
- Design tradeoffs:
  - Foundation model choice vs. computational cost
  - Cluster number selection vs. sample diversity
  - Embedding dimensionality vs. information preservation
  - Budget allocation between initialization and subsequent learning
- Failure signatures:
  - Poor initialization performance: Check if embeddings capture task-relevant features
  - Clustering instability: Verify foundation model's domain relevance
  - Low subsequent learning efficiency: Evaluate uncertainty sampling effectiveness
- First 3 experiments:
  1. Compare K-means vs. hierarchical clustering with foundation model embeddings
  2. Test different foundation models on a simple binary classification task
  3. Evaluate sample diversity using t-SNE visualization of embeddings from different models

## Open Questions the Paper Calls Out

### Open Question 1
How do foundation model-based clustering methods compare in performance when applied to other medical imaging modalities beyond chest radiographs? The paper only tested the method on chest X-ray data for pneumothorax classification and segmentation tasks. The performance of the method on other medical imaging modalities remains unexplored.

### Open Question 2
What is the impact of dataset size on the stability and effectiveness of foundation model-based clustering for active learning initialization? The experiments were conducted on a relatively small dataset, and the paper acknowledges the potential impact of dataset size on the stability of the results.

### Open Question 3
How does the performance of foundation model-based clustering compare to other advanced initialization methods for active learning, such as those using auxiliary models for weak labels? The paper does not compare the proposed method to other advanced initialization techniques that could potentially offer better performance.

## Limitations
- Computational cost of foundation model embeddings not thoroughly discussed
- Study focuses exclusively on pneumothorax detection and segmentation
- Choice of clustering parameters, particularly the number of clusters, is not systematically explored

## Confidence
**High Confidence**: The core finding that foundation model embeddings improve clustering-based initialization compared to random sampling is well-supported by experimental results.
**Medium Confidence**: The assertion that foundation models specifically address cold-start challenges is partially supported but doesn't clearly demonstrate overall active learning efficiency advantages.
**Low Confidence**: The claim that this approach represents a "promising paradigm for future active learning" extends beyond the experimental evidence provided.

## Next Checks
1. Cross-domain validation: Test the framework on at least two additional medical imaging tasks (e.g., mammography or MRI segmentation) to assess generalizability.
2. Computational efficiency analysis: Measure and report the total computational cost including foundation model inference time, clustering operations, and compare against baseline methods.
3. Hyperparameter sensitivity study: Systematically evaluate the impact of cluster number selection, embedding dimensionality, and initialization budget allocation on final performance.