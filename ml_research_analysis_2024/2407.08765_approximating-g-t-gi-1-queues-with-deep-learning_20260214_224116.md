---
ver: rpa2
title: Approximating G(t)/GI/1 queues with deep learning
arxiv_id: '2407.08765'
source_url: https://arxiv.org/abs/2407.08765
tags:
- time
- arrival
- service
- system
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a neural network-based method, called MBRNN,
  to predict the transient distribution of the number of customers in the system for
  a G(t)/GI/1 queue. The method uses a recurrent neural network architecture and is
  trained on simulated data.
---

# Approximating G(t)/GI/1 queues with deep learning

## Quick Facts
- arXiv ID: 2407.08765
- Source URL: https://arxiv.org/abs/2407.08765
- Reference count: 40
- Primary result: MBRNN predicts queue length distribution with <3% error, significantly faster than simulation modeling

## Executive Summary
This paper introduces MBRNN, a neural network-based method for predicting the transient distribution of customers in G(t)/GI/1 queueing systems. The approach leverages recurrent neural network architecture trained on simulated data to approximate queue length distributions. The method demonstrates high accuracy (error < 3%) while offering substantial computational speed improvements over traditional simulation methods. The authors suggest this technique could transform transient analysis across various queueing systems.

## Method Summary
The MBRNN method employs a recurrent neural network architecture specifically designed to predict transient queue length distributions in G(t)/GI/1 systems. The model is trained on simulated data generated from the queueing system, learning to map system parameters and temporal dynamics to probability distributions of queue lengths. The recurrent structure allows the model to capture temporal dependencies inherent in queueing dynamics, while the training process optimizes prediction accuracy across different system states and time points.

## Key Results
- MBRNN achieves prediction error below 3% for queue length distributions
- Method demonstrates significant computational speedup compared to simulation modeling
- Approach successfully handles time-varying arrival rates and general service time distributions

## Why This Works (Mechanism)
The mechanism leverages the ability of recurrent neural networks to capture temporal dependencies and sequential patterns in queueing dynamics. By training on simulated data, MBRNN learns the underlying probabilistic relationships between system parameters, arrival patterns, service distributions, and resulting queue lengths. The recurrent architecture enables the model to maintain state information across time steps, effectively modeling the transient nature of queueing systems where current state depends on historical evolution.

## Foundational Learning
1. **Queueing Theory Fundamentals** (why needed: Understanding G(t)/GI/1 model structure and transient behavior)
   - Quick check: Can identify key components: time-varying arrivals, general service times, single server

2. **Recurrent Neural Networks** (why needed: Capturing temporal dependencies in queue evolution)
   - Quick check: Can explain how RNNs maintain state across time steps

3. **Transient Analysis** (why needed: Understanding time-dependent behavior vs. steady-state analysis)
   - Quick check: Can distinguish between transient and steady-state queue characteristics

4. **Simulation-Based Training** (why needed: Generating training data for systems where analytical solutions are intractable)
   - Quick check: Can describe how simulated data captures system dynamics

## Architecture Onboarding

**Component Map**: Input features → RNN layers → Hidden state processing → Output distribution prediction

**Critical Path**: Input encoding → Recurrent processing → State aggregation → Distribution prediction

**Design Tradeoffs**: Model complexity vs. training data requirements; prediction accuracy vs. computational efficiency; generalization vs. specialization to G(t)/GI/1 systems

**Failure Signatures**: Overfitting to training data characteristics; poor performance on unseen parameter ranges; failure to capture rare but important queue states

**First Experiments**:
1. Test prediction accuracy across different service time distributions
2. Evaluate performance under varying arrival rate patterns
3. Compare computational time against baseline simulation methods

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Validation limited to G(t)/GI/1 queueing systems, restricting generalizability
- Performance claims based on simulated data rather than real-world validation
- Computational speedup claims lack detailed comparison methodology

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| MBRNN methodology is sound | High |
| 3% error rate accuracy | Medium |
| Computational efficiency claims | Medium |
| Applicability to other queueing systems | Low |

## Next Checks

1. Validate MBRNN performance on real-world queueing data from actual service systems, comparing predictions against empirical measurements rather than simulated data alone.

2. Test the method across different queueing models (e.g., G/G/1, GI/G/1, M/M/1) to assess generalizability beyond the G(t)/GI/1 case studied.

3. Conduct a comprehensive computational efficiency analysis comparing MBRNN against multiple established simulation approaches under various system loads and parameter configurations.