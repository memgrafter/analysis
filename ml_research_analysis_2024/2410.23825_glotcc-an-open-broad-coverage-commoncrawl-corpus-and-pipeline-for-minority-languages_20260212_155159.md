---
ver: rpa2
title: 'GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority
  Languages'
arxiv_id: '2410.23825'
source_url: https://arxiv.org/abs/2410.23825
tags:
- languages
- language
- data
- corpus
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GlotCC, a 2TB web-crawled corpus covering
  over 1000 languages, addressing the lack of large, high-quality datasets for minority
  languages. The authors develop GlotLID v3.0, an improved language identification
  model that supports over 2000 linguistic labels and handles noise robustly through
  "und" and "zxx" labels.
---

# GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages

## Quick Facts
- arXiv ID: 2410.23825
- Source URL: https://arxiv.org/abs/2410.23825
- Reference count: 40
- Primary result: 2TB web-crawled corpus covering over 1000 languages with 93% macro-average in-language content

## Executive Summary
This paper introduces GlotCC, a 2TB web-crawled corpus covering over 1000 languages, addressing the lack of large, high-quality datasets for minority languages. The authors develop GlotLID v3.0, an improved language identification model that supports over 2000 linguistic labels and handles noise robustly through "und" and "zxx" labels. They extend the Ungoliant pipeline with quality warnings and filters to remove web noise, list-like content, and inconsistent documents. An audit of 653 languages found 93% macro-average in-language content. The corpus is distributed at document level with PII replacement and quality metadata. GlotCC significantly expands language coverage compared to OSCAR, with 709 languages having more than 10 documents. The entire pipeline, including GlotLID, is open-sourced for reproducibility.

## Method Summary
GlotCC is built using the Ungoliant pipeline with CommonCrawl snapshots (CC-MAIN-2024-10 and parts of CC-MAIN-2023-40/50). The process involves language identification using GlotLID v3.0, which extends from 1665 to over 2000 linguistic labels and introduces "und" and "zxx" labels for noise handling. Quality warnings (15 total) detect repetition, list cases, technical characters, and script inconsistencies. Documents are filtered based on these warnings, PII is replaced, and the corpus is distributed at document level with metadata. The pipeline's wall time is approximately 340 hours for one Common Crawl snapshot.

## Key Results
- 2TB corpus covering 1275 LID labels (language-script pairs) across 1275 languages
- 93% macro-average in-language content score from self-audit of 653 languages
- 709 languages with more than 10 documents (compared to 60 in OSCAR)
- Document-level distribution enabling use for both generative and discriminative models
- Open-source pipeline including GlotLID v3.0 and quality filtering mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GlotLIDv3.0 significantly improves minority language coverage through expanded label set and robust noise handling
- Mechanism: The model extends from 1665 to over 2000 linguistic labels, adds "und" labels for unseen scripts, and introduces "zxx" labels for common web noise types, enabling better rejection of noise and out-of-language content
- Core assumption: Language identification models benefit from both expanded coverage and explicit noise labels rather than just probabilistic rejection
- Evidence anchors:
  - [abstract] "GlotLID v3.0, an improved language identification model that supports over 2000 linguistic labels and handles noise robustly through 'und' and 'zxx' labels"
  - [section] "We include in theGlotLIDtraining set new resources for African languages [44,67,7,57], Uralic languages [32, 74], Indonesian languages [71], Indic languages [49, 26] as well as additional indigenous languages [19]"
  - [corpus] Weak - corpus contains limited evidence of noise handling effectiveness

### Mechanism 2
- Claim: Document-level corpus distribution enables use for both generative and discriminative language technologies
- Mechanism: By providing complete documents rather than sentence-level data, the corpus supports tasks requiring document context while maintaining quality through PII replacement and metadata
- Core assumption: Document-level data provides value beyond sentence-level for pretraining and evaluation
- Evidence anchors:
  - [abstract] "We publishGlotCC as a document-level corpus. This makes it usable for pretraining generative language models as well as for other language technologies that require information beyond the sentence level"
  - [section] "GlotCC v1.0 contains data (subcorpora) for 1275 LID labels (i.e., language-script pairs such as 'rus-Cyrl')"
  - [corpus] Weak - corpus doesn't provide direct evidence of document-level utility

### Mechanism 3
- Claim: Quality warnings and filters significantly improve corpus cleanliness while maintaining in-language content
- Mechanism: The pipeline implements 15 quality warnings (repetition, list case, technical characters, etc.) and filters based on empirical sampling, removing 93% macro-average in-language content
- Core assumption: Web-crawled data contains identifiable patterns of noise that can be filtered without excessive language loss
- Evidence anchors:
  - [abstract] "We also extend Ungoliant with several filtering techniques that remove general web noise, list-like content, documents with repeated words [43,63] and 'inconsistent' documents"
  - [section] "We sample 20 sentences from three languages for each script... We find that the quality warnings generally indicate bad content or erroneously assigned LID labels and therefore remove sentences with quality warnings in GlotCC"
  - [corpus] Strong - self-audit found "data is in-language, with a macro-average score of 0.93 and median score of 1.0"

## Foundational Learning

- Concept: Language identification as open-set classification problem
  - Why needed here: Web data contains unknown languages and scripts that closed-set LID models cannot handle
  - Quick check question: Why does GlotLID introduce "und" labels instead of just rejecting low-confidence predictions?

- Concept: Writing system detection and script-language compatibility
  - Why needed here: Prevents assigning language labels that are incompatible with the document's script
  - Quick check question: How does GlotScript-T help reduce script-inconsistent errors?

- Concept: Web noise patterns and their detection
  - Why needed here: CommonCrawl contains various noise types that degrade model performance
  - Quick check question: What types of noise are captured by the six "zxx" labels in GlotLID?

## Architecture Onboarding

- Component map: Ungoliant pipeline → GlotLIDv3.0 → Quality warnings → PII replacement → Document distribution
- Critical path: Language identification → Quality filtering → Document-level output
- Design tradeoffs: Document-level vs sentence-level, aggressive filtering vs coverage, PII replacement vs data utility
- Failure signatures: Low in-language content scores, high script-inconsistency rates, excessive document rejection
- First 3 experiments:
  1. Run GlotLID on small CommonCrawl sample and evaluate F1/FPR on held-out test set
  2. Apply quality warnings to sample documents and manually verify false positive rates
  3. Compare document-level vs sentence-level output quality for a representative language

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between aggressive filtering for data quality and maintaining sufficient data volume for minority languages?
- Basis in paper: [inferred] The authors mention using "aggressive filtering" which may exclude documents in specific languages, and note that the pipeline's wall time is substantial (approximately 340 hours for one Common Crawl snapshot)
- Why unresolved: The paper doesn't quantify the trade-off between data quality and quantity, nor does it explore how different filtering thresholds affect downstream model performance for minority languages
- What evidence would resolve it: Empirical studies comparing model performance across different filtering thresholds, with metrics for both data quality (e.g., in-language content percentage) and downstream task performance for various minority languages

### Open Question 2
- Question: How does GlotCC's document-level corpus structure affect pretraining outcomes compared to sentence-level corpora?
- Basis in paper: [explicit] The authors specifically distribute GlotCC as a document-level corpus and mention it makes the corpus "usable for pretraining generative language models as well as for other language technologies that require information beyond the sentence level"
- Why unresolved: The paper doesn't provide any comparative analysis of pretraining outcomes using document-level versus sentence-level corpora, nor does it explore how document structure impacts model performance for different language technology applications
- What evidence would resolve it: Comparative pretraining experiments using the same data in both document-level and sentence-level formats, measuring performance on various downstream tasks including both generative and discriminative models

### Open Question 3
- Question: What is the long-term impact of web-crawled corpora on preserving linguistic diversity versus reinforcing dominant language patterns?
- Basis in paper: [inferred] The authors acknowledge that "the cleaning of GlotCC is constrained by the lack of tools for filtering out undesirable content" and that the corpus "may contain data that might be considered sensitive," while also noting their goal to "include a diverse range of languages in NLP"
- Why unresolved: The paper doesn't examine how the biases inherent in web-crawled data (which overrepresents certain types of content and communities) might affect the representation of minority languages, nor does it consider the sustainability of using web data for long-term language preservation
- What evidence would resolve it: Longitudinal studies tracking changes in corpus composition over time, comparative analyses of web-crawled data versus other data sources (e.g., academic publications, government documents) for various languages, and assessments of how corpus biases affect model outputs for different language communities

## Limitations

- Self-audit methodology lacks detailed sampling procedures and inter-annotator agreement metrics
- Quality filtering effectiveness not validated across all language families, particularly non-Latin scripts
- Long-term sustainability concerns due to CommonCrawl licensing constraints and web data biases

## Confidence

**High Confidence:** The corpus construction pipeline and open-source release are well-documented and reproducible. The document-level distribution approach and PII replacement strategy are clearly specified.

**Medium Confidence:** The GlotLID v3.0 improvements and quality filtering mechanisms show promise but lack comprehensive external validation. The self-audit results are encouraging but may not represent the full corpus quality.

**Low Confidence:** Claims about the specific utility of document-level data for generative models versus sentence-level alternatives lack empirical support. The long-term sustainability of CommonCrawl-based corpora given licensing constraints remains uncertain.

## Next Checks

1. Conduct an independent quality audit on a stratified sample of 50-100 languages representing different script families and resource levels, with detailed annotation guidelines and inter-annotator agreement metrics.

2. Compare document-level versus sentence-level utility for a downstream task (e.g., language modeling or classification) using a subset of GlotCC to empirically validate the document-level approach.

3. Evaluate GlotLID v3.0's performance on a held-out test set of minority languages not included in the training data to assess true generalization capability.