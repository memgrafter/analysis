---
ver: rpa2
title: 'Towards Fairness and Privacy: A Novel Data Pre-processing Optimization Framework
  for Non-binary Protected Attributes'
arxiv_id: '2410.00836'
source_url: https://arxiv.org/abs/2410.00836
tags:
- data
- discrimination
- fairness
- framework
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a framework for reducing discrimination in datasets
  with non-binary protected attributes by optimizing data subsets. It formulates a
  combinatorial optimization problem to find subsets that minimize discrimination
  measures, allowing for data removal, synthetic data addition, or exclusive use of
  synthetic data for privacy preservation.
---

# Towards Fairness and Privacy: A Novel Data Pre-processing Optimization Framework for Non-binary Protected Attributes

## Quick Facts
- arXiv ID: 2410.00836
- Source URL: https://arxiv.org/abs/2410.00836
- Authors: Manh Khoi Duong; Stefan Conrad
- Reference count: 33
- Primary result: Genetic algorithm-based framework reduces discrimination by up to 80% on non-binary protected attributes while maintaining runtime efficiency

## Executive Summary
This paper introduces a novel data pre-processing framework that addresses discrimination in datasets with non-binary protected attributes through combinatorial optimization. The framework formulates fairness as a subset selection problem, where heuristics like genetic algorithms identify data subsets that minimize specified discrimination measures. By supporting both real data removal and synthetic data addition, the approach offers flexibility for privacy-sensitive applications. Experiments demonstrate significant fairness improvements on Adult, Bank, and COMPAS datasets while maintaining computational efficiency.

## Method Summary
The framework optimizes fairness by treating data preprocessing as a combinatorial optimization problem. It uses a binary vector to indicate sample inclusion in a fair dataset subset, then applies genetic algorithms with elitist selection to minimize discrimination measures. The approach supports multiple fairness metrics through a black-box objective function, allowing adaptation to different ethical contexts. For privacy preservation, synthetic data generated via Gaussian copula can replace real data entirely. The method handles both binary and non-binary protected attributes by extending statistical disparity measures to multi-category cases, enabling more nuanced fairness optimization than binary-only approaches.

## Key Results
- Up to 80% reduction in statistical disparity achieved on tested datasets
- Elitist selection operator outperformed tournament and roulette wheel methods in fairness optimization
- Runtime typically under 5 minutes even for datasets with 30,000+ samples
- Framework maintains flexibility across different discrimination measures without requiring solver modifications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework reduces discrimination by selecting data subsets that minimize a specified discrimination measure.
- Mechanism: It formulates a combinatorial optimization problem where a binary vector indicates inclusion of samples in the fair dataset. Heuristics like genetic algorithms then solve this problem by iteratively improving candidate solutions.
- Core assumption: The discrimination measure accurately captures unfairness in the data and is differentiable enough for heuristic search.
- Evidence anchors:
  - [abstract]: "The framework proposes a combinatorial optimization problem where heuristics such as genetic algorithms can be used to solve for the stated fairness objectives."
  - [section 4.1]: "The aim is to determine a subset Dfair of a given set S such that the discrimination of the subset ψ(Dfair) is minimal."
- Break condition: If the discrimination measure is poorly defined or the heuristic cannot navigate the solution space effectively, the optimization may not reduce bias.

### Mechanism 2
- Claim: Using synthetic data instead of real data can preserve privacy while still enabling fairness optimization.
- Mechanism: Synthetic data is generated to match the distribution of the original dataset. The framework then optimizes over this synthetic set, ensuring no real individual's data is exposed.
- Core assumption: The synthetic data generation method produces high-quality, representative samples that maintain the statistical properties needed for fair learning.
- Evidence anchors:
  - [section 4.3]: "Relying solely on synthetic data is particularly important in use cases where data privacy and protection are major concerns and the use of real data is prohibited."
  - [section 6]: "For privacy-sensitive use cases, we advise utilizing privacy-preserving techniques."
- Break condition: If synthetic data fails to capture the true data distribution or contains privacy leaks, the fairness gains may be invalid or privacy compromised.

### Mechanism 3
- Claim: The framework's metric-agnostic formulation allows it to target diverse fairness goals without modifying the solver.
- Mechanism: By treating discrimination measures as black boxes, any fairness metric (e.g., statistical parity, Rawlsian max disparity) can be plugged in without changing the optimization structure.
- Core assumption: The solver can handle arbitrary black-box objectives and still find good solutions.
- Evidence anchors:
  - [abstract]: "The framework exhibits a high degree of flexibility as it is metric- and task-agnostic."
  - [section 4.1]: "Our formulation makes the framework fairness-agnostic, allowing it to be used to pursue any fairness objective."
- Break condition: If the black-box objective is too complex or noisy, the heuristic may fail to converge or find meaningful solutions.

## Foundational Learning

- Concept: Combinatorial optimization and subset selection
  - Why needed here: The core problem is selecting a subset of data points to minimize discrimination, which is inherently combinatorial.
  - Quick check question: Why can't we just use gradient descent to minimize the discrimination measure directly?

- Concept: Genetic algorithms and heuristic search
  - Why needed here: The solution space grows exponentially with dataset size, making exact methods infeasible; heuristics are required.
  - Quick check question: What is the role of the selection operator in a genetic algorithm, and why might elitist selection outperform others?

- Concept: Fairness metrics and discrimination measures
  - Why needed here: Different fairness notions require different ways to quantify and minimize bias; the framework must support multiple measures.
  - Quick check question: How does the sum of absolute statistical disparities differ from the maximal absolute statistical disparity in terms of fairness goals?

## Architecture Onboarding

- Component map:
  - Data preprocessing module -> Synthetic data generator -> Optimization engine -> Evaluation module -> Configuration interface

- Critical path:
  1. Load and preprocess the dataset.
  2. Generate synthetic data if needed.
  3. Set optimization parameters (objective, measure, sample set).
  4. Run the heuristic to obtain a fair subset.
  5. Evaluate the resulting dataset's discrimination.

- Design tradeoffs:
  - Using real data removal vs. synthetic data addition: removal is simpler but may hurt model performance; synthetic data preserves size but risks quality issues.
  - Choice of discrimination measure: sum disparity favors overall equity; max disparity protects the worst-off group.
  - Heuristic selection: elitist selection gives best fairness but slower runtime; random is fastest but less effective.

- Failure signatures:
  - No reduction in discrimination after optimization: likely due to poor measure definition or heuristic misconfiguration.
  - Runtime exceeds expected limits: may indicate population size or generation count too high for dataset size.
  - Synthetic data fails quality checks: distribution mismatch or privacy leakage in generation process.

- First 3 experiments:
  1. Run the framework with the original dataset and statistical parity sum disparity; verify reduction vs baseline.
  2. Repeat with synthetic-only setting; check privacy preservation and fairness trade-offs.
  3. Compare elitist, tournament, and roulette wheel selection operators on runtime and discrimination reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the framework perform when dealing with multiple protected attributes simultaneously?
- Basis in paper: [inferred] The paper mentions that future work could include extending the framework to handle multiple protected attributes, indicating this is currently unresolved.
- Why unresolved: The current framework focuses on a single protected attribute, and extending it to multiple attributes would require deriving new discrimination measures and potentially adjusting the optimization approach.
- What evidence would resolve it: Experiments showing the framework's effectiveness in reducing discrimination when multiple protected attributes are present, along with newly derived discrimination measures for this scenario.

### Open Question 2
- Question: What is the impact of different synthetic data generation techniques on the fairness outcomes achieved by the framework?
- Basis in paper: [explicit] The paper suggests using Gaussian copula for synthetic data generation but mentions that privacy-preserving techniques could be utilized, indicating uncertainty about the optimal approach.
- Why unresolved: The choice of synthetic data generation technique can significantly affect the quality and fairness of the generated data, which in turn impacts the framework's effectiveness.
- What evidence would resolve it: Comparative experiments using various synthetic data generation techniques and their impact on fairness outcomes, including privacy-preserving methods.

### Open Question 3
- Question: How do the proposed discrimination measures for non-binary attributes compare to alternative measures in terms of fairness and practical applicability?
- Basis in paper: [explicit] The paper derives discrimination measures based on Žliobaitė's work for non-binary attributes but acknowledges that the choice of measure depends on the ethical context of the specific use case.
- Why unresolved: Different discrimination measures may lead to different fairness outcomes, and their suitability can vary depending on the application context and stakeholder preferences.
- What evidence would resolve it: Comparative analysis of various discrimination measures for non-binary attributes, including their fairness outcomes, computational efficiency, and practical considerations for different use cases.

## Limitations
- Synthetic data generation method details are not fully specified, raising questions about privacy guarantees and distribution fidelity
- Framework validation limited to protected attributes with three categories; performance with more categories untested
- Runtime scalability to datasets larger than 50,000 samples remains unproven

## Confidence
- **High**: The framework's ability to reduce discrimination measures on tested datasets when using elitist selection
- **Medium**: The effectiveness of synthetic data for privacy preservation without compromising fairness gains
- **Low**: Claims about the framework's flexibility to handle arbitrary discrimination measures without solver modifications

## Next Checks
1. Test synthetic data quality by comparing discrimination measures computed on real vs. synthetic subsets; verify that fairness gains persist when evaluated on real data
2. Run the framework on a dataset with a protected attribute having four or more categories to assess non-binary handling beyond the current scope
3. Scale the experiments to a dataset with 100k+ samples to evaluate runtime scalability and optimization convergence