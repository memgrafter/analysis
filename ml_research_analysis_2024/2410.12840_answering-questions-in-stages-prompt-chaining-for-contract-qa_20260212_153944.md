---
ver: rpa2
title: 'Answering Questions in Stages: Prompt Chaining for Contract QA'
arxiv_id: '2410.12840'
source_url: https://arxiv.org/abs/2410.12840
tags:
- answer
- prompt
- clause
- question
- options
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores prompt chaining to improve the accuracy of
  legal question-answering from contract clauses. While single-stage prompts performed
  poorly on complex clauses, a two-stage approach first generates a question-focused
  summary of the clause, then maps it to structured answers.
---

# Answering Questions in Stages: Prompt Chaining for Contract QA

## Quick Facts
- arXiv ID: 2410.12840
- Source URL: https://arxiv.org/abs/2410.12840
- Reference count: 11
- Primary result: Two-stage prompt chaining improves legal QA accuracy from contracts for most questions, but struggles with linguistically varied force majeure clauses.

## Executive Summary
This paper explores prompt chaining to improve the accuracy of legal question-answering from contract clauses. While single-stage prompts performed poorly on complex clauses, a two-stage approach first generates a question-focused summary of the clause, then maps it to structured answers. The two-stage method improved precision and recall for most questions, especially when answer options were included during summarization. However, it struggled with force majeure questions due to linguistic variation in trigger events, where even exhaustive definitions yielded only modest gains. The approach is effective for questions with focused answer sets but less so when the legal text requires nuanced interpretation.

## Method Summary
The method employs a two-stage prompt chaining approach where Stage 1 generates a question-focused summary of the legal clause, and Stage 2 maps that summary to structured answer options. The first stage prompt instructs the model to restate only explicitly provided information relevant to the question, avoiding inferences. The second stage uses this summary to select from provided answer options. Both stages use the same LLM but as separate interactions. The approach was tested on 200 annotated contract clauses from EDGAR and SEDAR repositories covering change of control, assignment, insurance, and force majeure questions.

## Key Results
- Two-stage approach improved precision and recall for most questions compared to single-stage prompts
- Including answer options in Stage 1 generally improved effectiveness across all question types
- Method showed limited success on force majeure utility failure questions due to linguistic variation
- Simple questions with focused answer sets benefited most from the two-stage approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage prompt chaining improves structured answer accuracy by first distilling relevant legal text into a focused summary, then mapping that summary to answer options.
- Mechanism: The first stage prompt instructs the model to restate only the explicitly provided legal text relevant to the question, avoiding inferences. This distilled summary removes extraneous information, allowing the second stage to focus reasoning on the most relevant facts. Including answer options in the first stage further guides the summary to align with possible answers.
- Core assumption: The model can effectively separate relevant from irrelevant content in legal clauses and produce a coherent, focused summary without hallucinating.
- Evidence anchors:
  - [abstract] "a two-stage approach first generates a question-focused summary of the clause, then maps it to structured answers. The two-stage method improved precision and recall for most questions"
  - [section] "The underlying intuition is that, in the summary generation stage, the model could 'distill' the legal text to only the concepts relevant to the question and would facilitate better mapping to relevant answer options"
  - [corpus] Weak - no direct corpus evidence for this mechanism; relies on experimental results in the paper
- Break condition: When the legal text contains significant linguistic variation that cannot be captured by the answer options, as seen with force majeure utility failure clauses where even exhaustive definitions yielded only modest gains.

### Mechanism 2
- Claim: Including structured answer options in the summary generation stage improves the alignment between the summary and the final answer selection.
- Mechanism: By providing the answer options during the first stage, the model can tailor the summary to highlight information that maps to those options. This creates a more direct path from legal text to answer selection, reducing the model's need to infer connections between summary content and answer choices.
- Core assumption: The model can effectively use the provided answer options as a framework for generating a relevant summary that covers all necessary information for answer selection.
- Evidence anchors:
  - [section] "we found it interesting that the format of the summaries varied non-trivially... rather than having a single stable format. Regardless, this inclusion generally improved effectiveness across the board"
  - [section] "For questions Q1 and Q3, there is a substantial increase in the exact match accuracy"
  - [corpus] Weak - no direct corpus evidence; based on internal analysis of summary variations
- Break condition: When the question requires nuanced interpretation beyond what can be captured by listing answer options, or when the model generates summaries in formats that don't effectively support answer selection.

### Mechanism 3
- Claim: Restricting the model to only restate explicitly provided information in the summary stage reduces hallucination and improves answer accuracy.
- Mechanism: The first stage prompt explicitly instructs the model to "only restate what is explicitly provided in the clause and make no inferences or assumptions." This constraint prevents the model from generating unsupported information that could lead to incorrect answer selection in the second stage.
- Core assumption: LLMs can follow explicit instructions to avoid making inferences and will generate more accurate summaries when constrained to only explicit information.
- Evidence anchors:
  - [section] "We subsequently modified the first stage by adding in similar restrictions to the single stage prompt but also included the answer options as a reference"
  - [section] "This type of restriction helped improve the model's reasoning capabilities in our prior work"
  - [corpus] Weak - no direct corpus evidence; based on internal experimentation
- Break condition: When the correct answer requires some level of interpretation or when the legal text is too ambiguous without inference, or when the model ignores instructions and continues to make unwarranted inferences.

## Foundational Learning

- Concept: Prompt engineering techniques for LLMs
  - Why needed here: The paper builds on various prompt engineering strategies to improve legal question-answering from contracts. Understanding zero-shot prompts, chain-of-thought prompting, and prompt chaining is essential to grasp the methodology.
  - Quick check question: What is the difference between single-stage and two-stage prompt chaining approaches?

- Concept: Legal document structure and terminology
  - Why needed here: The research focuses on contract clauses (change of control, assignment, insurance, force majeure) and requires understanding legal concepts to interpret results and limitations.
  - Quick check question: What is the purpose of a force majeure clause in contracts?

- Concept: Evaluation metrics for question-answering systems
  - Why needed here: The paper uses precision, recall, and exact match accuracy to evaluate performance. Understanding these metrics is crucial for interpreting results.
  - Quick check question: How does exact match accuracy differ from precision and recall in multi-answer questions?

## Architecture Onboarding

- Component map: Legal clause → Stage 1 summary generation → Stage 2 answer selection → Structured output (JSON array)
- Critical path: Legal clause → Stage 1 summary generation → Stage 2 answer selection → Structured output (JSON array)
- Design tradeoffs: Single-stage vs. two-stage approaches (simplicity vs. accuracy), including answer options in Stage 1 (guidance vs. constraint), and restricting inferences (accuracy vs. completeness)
- Failure signatures: Poor performance on questions requiring interpretation of linguistic variations (force majeure utility failures), summaries that don't align with answer options, or models that ignore instructions and make unwarranted inferences
- First 3 experiments:
  1. Single-stage prompt (P1) - Direct mapping of legal text to answers without summary
  2. Simple two-stage prompt (P3) - Summary generation without answer options, then answer selection
  3. Restrictive two-stage prompt (P4) - Summary generation with answer options included, then answer selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do variations in summary format (e.g., bullet points, paragraphs, option-based explanations) impact the accuracy of subsequent answer selection in the second stage?
- Basis in paper: [explicit] The paper notes that summary formats varied non-trivially (e.g., several paragraphs, bullet point lists, lists of applicable answers followed by an explanation) but found no substantive gains from these variations.
- Why unresolved: The paper only mentions this observation but does not conduct a controlled experiment to determine if specific formats consistently improve accuracy for different question types.
- What evidence would resolve it: A systematic study comparing answer selection accuracy across different summary formats for each question type (reasoning-style vs. list-like questions) would determine if certain formats are more effective.

### Open Question 2
- Question: Would pre-training or fine-tuning a language model on legal contracts and question-answer pairs improve the quality of the generated summaries in stage one?
- Basis in paper: [explicit] The authors propose this as future work, noting that such training could help the LLM understand nuances in legal language and produce more relevant summaries.
- Why unresolved: The paper only uses off-the-shelf GPT models without any legal-specific training, leaving open whether model adaptation would improve performance.
- What evidence would resolve it: Comparing the accuracy of two-stage prompting with pre-trained/fine-tuned models versus standard GPT models on the same legal question-answering task would demonstrate the impact of legal domain adaptation.

### Open Question 3
- Question: Can prompt templates be developed using clause definitions and standard model restrictions that generalize across different questions and contract types?
- Basis in paper: [explicit] The authors identify this as future work, noting that their current approach required question-specific modifications despite attempts to build reusable templates.
- Why unresolved: The paper shows that even with restrictive prompts, the effectiveness varied significantly across different questions, suggesting the need for more generalizable approaches.
- What evidence would resolve it: Developing and testing a set of prompt templates across a broader range of legal questions and contract types would demonstrate whether a standardized approach is feasible.

## Limitations

- Approach struggles with questions requiring interpretation of linguistically varied events, particularly force majeure utility failure clauses
- Method relies on a proprietary dataset from EDGAR and SEDAR repositories that is not yet publicly available
- Effectiveness appears question-dependent rather than universally applicable across all contract types

## Confidence

- **High confidence**: The two-stage approach improves accuracy over single-stage prompts for questions with focused answer sets (change of control, assignment, insurance)
- **Medium confidence**: The inclusion of answer options in the first stage consistently improves performance, though the mechanism is not fully understood
- **Low confidence**: The approach's effectiveness on contract types and questions with significant linguistic variation (force majeure utility failures)

## Next Checks

1. **Dataset availability validation**: Attempt to reproduce results using a publicly available contract dataset to verify if the two-stage approach generalizes beyond the proprietary dataset used in the study.

2. **Linguistic variation test**: Design a controlled experiment testing the approach on questions with known linguistic variations (similar to force majeure utility failures) to quantify the limits of the method's effectiveness.

3. **Cross-domain application**: Apply the two-stage prompt chaining approach to non-legal domains with structured answer requirements (e.g., medical records or technical specifications) to assess its broader applicability.