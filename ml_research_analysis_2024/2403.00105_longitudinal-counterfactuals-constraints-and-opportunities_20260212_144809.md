---
ver: rpa2
title: 'Longitudinal Counterfactuals: Constraints and Opportunities'
arxiv_id: '2403.00105'
source_url: https://arxiv.org/abs/2403.00105
tags:
- counterfactuals
- longitudinal
- counterfactual
- data
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using longitudinal data to assess and improve
  plausibility in counterfactual explanations. The authors introduce a longitudinal
  distance metric that compares observed feature changes over time to proposed counterfactual
  changes, enabling evaluation of how similar a counterfactual is to prior observed
  changes.
---

# Longitudinal Counterfactuals: Constraints and Opportunities

## Quick Facts
- arXiv ID: 2403.00105
- Source URL: https://arxiv.org/abs/2403.00105
- Authors: Alexander Asemota; Giles Hooker
- Reference count: 18
- Primary result: Longitudinal data reveals many counterfactuals generated by existing methods are implausible; joint optimization with longitudinal constraints yields more plausible explanations.

## Executive Summary
This paper proposes using longitudinal data to assess and improve plausibility in counterfactual explanations. The authors introduce a longitudinal distance metric that compares observed feature changes over time to proposed counterfactual changes, enabling evaluation of how similar a counterfactual is to prior observed changes. Experiments with MIMIC-III and Adult-Income datasets show that many counterfactuals generated by existing methods are implausible when evaluated against longitudinal data. For instance, in MIMIC-III, 74% of counterfactuals allowing all features to change had a longitudinal distance above 105, compared to none for vital signs only. The authors also demonstrate a genetic algorithm approach using the longitudinal metric that generates more plausible counterfactuals by jointly optimizing proximity and longitudinal distance. However, results also highlight challenges in providing recourse: even plausible counterfactuals may not be achievable, and models that rely on immutable features can make recourse impossible for certain groups. The paper calls for greater attention to the philosophical and practical aspects of using counterfactuals for recourse beyond purely technical solutions.

## Method Summary
The method introduces a longitudinal distance metric that computes the average minimum distance between proposed counterfactual changes and observed historical longitudinal changes, penalizing counterfactuals that propose changes rarely seen in historical data. The authors implement a genetic algorithm that jointly optimizes proximity to the input and longitudinal distance, constraining the search space so only counterfactuals resembling observed historical changes are considered. This contrasts with post-hoc scoring approaches and aims to generate more plausible counterfactuals. The approach is evaluated on MIMIC-III and Adult-Income datasets using both baseline DiCE methods and the proposed genetic algorithm, comparing plausibility, validity, and proportion of invalid counterfactuals.

## Key Results
- 74% of MIMIC-III counterfactuals allowing all features to change had longitudinal distance above 105 (none for vital signs only).
- Genetic algorithm with longitudinal objective produced zero invalid counterfactuals compared to 17% with default method.
- Even plausible counterfactuals may not be achievable due to personal, educational, or geographic constraints.
- Models relying on immutable features can make recourse impossible for certain groups.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Longitudinal distance metric improves plausibility by comparing counterfactual changes to observed historical changes.
- Mechanism: The metric computes the average minimum distance between a proposed counterfactual change and the s closest observed longitudinal changes. This penalizes counterfactuals that propose changes rarely or never seen in historical data, thereby filtering out implausible explanations.
- Core assumption: Historical longitudinal changes are a valid proxy for what is plausible in the future; observed changes reflect realistic trajectories individuals can achieve.
- Evidence anchors:
  - [abstract] "We introduce a longitudinal distance metric that compares observed feature changes over time to proposed counterfactual changes, enabling evaluation of how similar a counterfactual is to prior observed changes."
  - [section 3.1.1] "We introduce a distance metric that compares prior observed changes to proposed changes in the form of counterfactual explanations."
  - [corpus] Weak: No direct matching evidence in corpus neighbors; methods discussed focus on plausibility-aware generation or diversity but not longitudinal comparison.
- Break condition: If longitudinal data is sparse, unrepresentative, or if the underlying process changes over time, the metric may penalize valid counterfactuals or fail to capture new plausible trajectories.

### Mechanism 2
- Claim: Using longitudinal constraints during counterfactual generation yields more plausible and valid explanations than post-hoc scoring.
- Mechanism: The genetic algorithm jointly optimizes proximity to the input and longitudinal distance, constraining the search space so that only counterfactuals resembling observed historical changes are considered. This contrasts with unconstrained methods that may generate explanations with unrealistic combinations.
- Core assumption: Joint optimization with longitudinal distance during generation can efficiently find valid counterfactuals within plausible bounds, whereas post-hoc filtering is less effective.
- Evidence anchors:
  - [section 3.1.3] "We propose jointly optimizing a longitudinal objective and a proximity objective... to generate counterfactuals constrained by longitudinal data."
  - [section 4.2.2] Results show "Longitudinal" method produced fewer invalid counterfactuals and zero with immutable feature changes compared to "Default."
  - [corpus] Weak: No direct evidence in neighbors; closest is plausibility-aware recourse generation but not joint optimization.
- Break condition: If the longitudinal data does not cover the feature space well or if the optimization becomes too constrained, the algorithm may fail to find any valid counterfactuals.

### Mechanism 3
- Claim: Distinguishing between plausibility and achievability is critical for meaningful recourse; many plausible counterfactuals are still not achievable by individuals.
- Mechanism: The paper argues that even if a counterfactual is plausible (respects constraints like no age decrease), it may still be unachievable due to personal, educational, or geographic constraints. This separation clarifies that technical plausibility alone is insufficient for real-world recourse.
- Core assumption: There is a meaningful gap between what is theoretically possible and what individuals can realistically accomplish; models may learn to predict based on immutable or hard-to-change features.
- Evidence anchors:
  - [abstract] "However, results also highlight challenges in providing recourse: even plausible counterfactuals may not be achievable, and models that rely on immutable features can make recourse impossible for certain groups."
  - [section 5.1] Discusses "diversity of counterfactuals can improve overall plausibility by offering multiple different paths, but diversity alone cannot guarantee plausibility."
  - [corpus] No direct evidence; the discussion of feasibility vs. plausibility is unique to this paper.
- Break condition: If the focus remains purely on technical plausibility without addressing real-world achievability, counterfactual explanations will fail to provide meaningful recourse.

## Foundational Learning

- Concept: Counterfactual explanations and recourse
  - Why needed here: The paper builds on counterfactual explanation methods but critiques their practical utility for providing recourse. Understanding the difference between explanation, plausibility, and achievability is central to the argument.
  - Quick check question: What is the difference between a counterfactual being plausible versus achievable?
- Concept: Longitudinal data analysis
  - Why needed here: The core contribution is using longitudinal differences as a plausibility metric. Familiarity with handling time-series or repeated measures data is required to understand the distance computation and its normalization.
  - Quick check question: How does the longitudinal distance metric normalize differences across features?
- Concept: Genetic algorithms and multi-objective optimization
  - Why needed here: The method uses a genetic algorithm to jointly minimize proximity and longitudinal distance. Understanding selection, crossover, and fitness evaluation is necessary to grasp the generation process.
  - Quick check question: In the genetic algorithm, what determines which counterfactuals survive to the next generation?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training -> Counterfactual generation -> Evaluation -> Visualization
- Critical path:
  1. Prepare longitudinal dataset and train model.
  2. Generate counterfactuals with baseline method.
  3. Score counterfactuals using longitudinal distance.
  4. Generate counterfactuals with genetic algorithm optimizing proximity + longitudinal distance.
  5. Compare plausibility and validity metrics.
- Design tradeoffs:
  - Using longitudinal data improves plausibility but may reduce validity if constraints are too strict.
  - Post-hoc scoring is simpler but less effective than joint optimization during generation.
  - Normalizing by dispersion (MAD/AAD) helps but can over-penalize rare but valid changes.
- Failure signatures:
  - High proportion of counterfactuals with longitudinal distance above threshold indicates implausible changes.
  - Zero valid counterfactuals generated suggests overly strict constraints.
  - Disparities in plausibility across demographic groups may indicate bias in training data or model.
- First 3 experiments:
  1. Run DiCE on MIMIC-III allowing all features; compute longitudinal distance to assess baseline plausibility.
  2. Run DiCE on MIMIC-III allowing only vital signs; compare longitudinal distance distribution to all-features case.
  3. Run genetic algorithm with longitudinal objective on Adult-Income; compare validity and plausibility to default method.

## Open Questions the Paper Calls Out

- What is the optimal way to normalize longitudinal distance metrics across heterogeneous features, particularly for categorical variables with multiple classes?
- How can we quantify the trade-off between plausibility and validity when generating counterfactual explanations, and what is the optimal balance for different use cases?
- What computational approaches can reduce the complexity of longitudinal distance calculations, which currently require row-wise comparison of matrices?

## Limitations

- The longitudinal distance metric assumes historical changes are representative of future plausible changes, which may fail if the underlying process shifts or data is sparse.
- The genetic algorithm's effectiveness depends on hyperparameter tuning and population initialization, which are not fully specified, raising concerns about reproducibility.
- The separation between plausibility and achievability is discussed philosophically but not quantified, making it difficult to assess real-world impact.

## Confidence

- High confidence: Many counterfactuals generated by existing methods are implausible when evaluated against longitudinal data.
- Medium confidence: Joint optimization with longitudinal distance yields more plausible counterfactuals, given the lack of detailed algorithmic specification.
- Low confidence: Practical achievability of counterfactuals for real-world recourse, as the paper highlights this gap but does not provide empirical evidence or methods to address it.

## Next Checks

1. **Reproduce baseline results**: Generate counterfactuals using DiCE on MIMIC-III and Adult-Income datasets; compute longitudinal distance to confirm the proportion of implausible counterfactuals matches reported values.

2. **Test genetic algorithm robustness**: Implement the genetic algorithm with different population sizes, crossover/mutation rates, and fitness functions; compare plausibility and validity across settings.

3. **Analyze demographic disparities**: Stratify counterfactual plausibility and validity by demographic groups; investigate whether the longitudinal metric or genetic algorithm introduces or mitigates bias.