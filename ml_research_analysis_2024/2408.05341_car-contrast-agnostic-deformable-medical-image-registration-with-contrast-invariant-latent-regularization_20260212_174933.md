---
ver: rpa2
title: 'CAR: Contrast-Agnostic Deformable Medical Image Registration with Contrast-Invariant
  Latent Regularization'
arxiv_id: '2408.05341'
source_url: https://arxiv.org/abs/2408.05341
tags:
- registration
- image
- images
- contrast
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a contrast-agnostic deformable image registration
  framework (CAR) that can register arbitrary contrasts of images without observing
  them during training. The key innovation is a random convolution-based contrast
  augmentation scheme that simulates various image contrasts from a single contrast
  image while preserving structural information.
---

# CAR: Contrast-Agnostic Deformable Medical Image Registration with Contrast-Invariant Latent Regularization

## Quick Facts
- arXiv ID: 2408.05341
- Source URL: https://arxiv.org/abs/2408.05341
- Reference count: 38
- Achieves higher Dice scores and lower Hausdorff distances than state-of-the-art methods like VoxelMorph, MIDIR, and SynthMorph

## Executive Summary
This paper introduces CAR, a contrast-agnostic deformable image registration framework that can register arbitrary contrasts of images without observing them during training. The key innovation is a random convolution-based contrast augmentation scheme that simulates various image contrasts from a single contrast image while preserving structural information. CAR employs a contrast-invariant latent regularization (CLR) that enforces the network to learn structural features invariant to different image contrasts through a contrast invariance loss. Experiments on brain MRI (T1w-T2w) and cardiac T1 mapping data show CAR outperforms baseline methods in registration accuracy and deformation regularity, even for unseen contrasts.

## Method Summary
CAR uses a Siamese U-Net architecture with shared encoders for moving and fixed images, producing deformation fields via a decoder. The framework employs random convolution-based contrast augmentation to simulate various image contrasts from a single contrast image while preserving structural information. To learn contrast-invariant representations, CAR uses a contrast-invariant latent regularization (CLR) that enforces the network to learn structural features invariant to different image contrasts through a contrast invariance loss. The model is trained on single-contrast data but achieves contrast-agnostic registration performance by learning invariant features through the combination of augmentation and regularization.

## Key Results
- CAR outperforms baseline methods regarding registration accuracy on brain MRI (T1w-T2w) and cardiac T1 mapping data
- Demonstrates superior generalization ability to unseen imaging contrasts compared to state-of-the-art methods
- Achieves higher Dice scores and lower Hausdorff distances while maintaining smooth deformation fields

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrast-invariant latent regularization (CLR) enforces the network to learn structural features invariant to different image contrasts through a contrast invariance loss.
- Mechanism: By applying a projection head to map latent representations to a low-dimensional space and computing mean squared differences between projected representations of same-shape but different-contrast images, CLR pulls contrast-augmented representations close in latent space.
- Core assumption: Images with the same shape but different contrasts should have similar latent representations in a well-trained network.
- Evidence anchors:
  - [abstract]: "contrast-invariant latent regularization (CLR) that regularizes representation in latent space through a contrast invariance loss"
  - [section 2.2]: "we assume that the latent representation of images that share the same shape information but with different contrasts should be close in the latent space"
  - [corpus]: Weak evidence - related papers focus on registration but don't directly address contrast invariance in latent space
- Break condition: If the projection head fails to adequately compress information or if the MSE loss doesn't capture the true similarity structure, the invariance may not be enforced properly.

### Mechanism 2
- Claim: Random convolution-based contrast augmentation simulates arbitrary contrasts from a single image while preserving structural information.
- Mechanism: Using multiple 1x1 convolution layers with random weights and LeakyReLU activations transforms input image appearance without introducing significant blurring, enabling diverse contrast simulation from one contrast image.
- Core assumption: 1x1 convolutions can adjust intensity distributions while maintaining spatial structure, and stacking them allows modeling complex non-linear intensity relationships.
- Evidence anchors:
  - [abstract]: "random convolution-based contrast augmentation scheme that simulates various image contrasts from a single contrast image while preserving structural information"
  - [section 2.1]: "we propose to set the kernel size to 1×1 to minimize the potential effects of the induced artifacts on structural details"
  - [corpus]: Weak evidence - related papers discuss augmentation but don't specifically validate random convolution for contrast simulation in registration
- Break condition: If larger kernels introduce blurring or if the random sampling doesn't cover the contrast space adequately, the augmentation may fail to simulate realistic contrasts.

### Mechanism 3
- Claim: The combination of contrast augmentation and CLR enables training on single-contrast data while achieving contrast-agnostic registration performance.
- Mechanism: Contrast augmentation creates synthetic multi-contrast pairs from single-contrast data, and CLR ensures the network learns features invariant to these simulated contrasts, enabling generalization to unseen contrasts during inference.
- Core assumption: Learning invariant features on synthetic contrasts transfers to real unseen contrasts because the network captures fundamental structural information.
- Evidence anchors:
  - [abstract]: "Experiments show that CAR outperforms the baseline approaches regarding registration accuracy and also possesses better generalization ability to unseen imaging contrasts"
  - [section 2.3]: "we propose to use the pre-augmented mono-contrast image pairs for loss supervision with a mono-contrast dissimilarity measure"
  - [corpus]: No direct evidence in corpus - this is a novel claim about the combined approach
- Break condition: If the simulated contrasts don't adequately represent real contrast variations or if the invariance learned doesn't transfer to real data, the approach may fail on unseen contrasts.

## Foundational Learning

- Concept: Contrast augmentation through random convolutions
  - Why needed here: Enables training on single-contrast data while simulating multi-contrast scenarios for learning contrast-invariant features
  - Quick check question: Why use 1x1 convolutions instead of larger kernels for contrast augmentation in registration tasks?

- Concept: Siamese encoder architecture with shared weights
  - Why needed here: Allows processing of moving and fixed images through identical feature extraction pathways, essential for computing correspondences
  - Quick check question: How does the Siamese architecture ensure that the network learns consistent feature representations for both moving and fixed images?

- Concept: Contrast-invariant representation learning
  - Why needed here: Enables the network to focus on structural information rather than intensity patterns, allowing registration across different imaging contrasts
  - Quick check question: What is the key difference between using MSE loss versus InfoNCE loss for enforcing contrast invariance in the latent space?

## Architecture Onboarding

- Component map: Moving image → Siamese Encoder → Latent representation → Concatenation → Decoder → Deformation field → Warped moving image → LNCC loss
- Critical path: Moving image → Encoder → Latent representation → Concatenation → Decoder → Deformation field → Warped moving image → LNCC loss
- Design tradeoffs:
  - 1x1 vs larger convolution kernels: 1x1 preserves fine details but may limit contrast variation range
  - MSE vs InfoNCE for CLR: MSE provides direct constraint but may be less discriminative than contrastive approaches
  - Single vs multi-contrast training: Single-contrast training with augmentation is more practical but relies on augmentation quality
- Failure signatures:
  - High J<0% ratio: Indicates folding, possibly from insufficient regularization or poor contrast augmentation
  - Poor Dice scores on unseen contrasts: Suggests CLR not effectively enforcing invariance
  - High HD95 values: Indicates boundary misalignment, possibly from inadequate feature extraction
- First 3 experiments:
  1. Test registration performance on seen contrasts with and without CLR to validate its contribution
  2. Evaluate sensitivity to kernel size in random convolution by comparing 1x1 vs 3x3 performance
  3. Measure registration accuracy when training with single contrast vs all available contrasts to quantify augmentation effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed random convolution-based contrast augmentation scheme handle extreme intensity variations that may not be well-represented by the uniform distribution U(0, 10) used in the experiments?
- Basis in paper: [explicit] The paper mentions that random convolution kernel weights are sampled from a uniform distribution U(0, 10) and zero-centered, but doesn't discuss handling of extreme intensity variations or alternative sampling strategies.
- Why unresolved: The paper doesn't explore how the method would perform with more extreme or pathological intensity variations, or whether alternative sampling distributions might be more appropriate for certain imaging scenarios.
- What evidence would resolve it: Experiments testing the framework's performance with images containing extreme intensity variations (e.g., pathology cases, different MRI field strengths) and comparison with alternative sampling strategies would help determine the robustness and limitations of the current approach.

### Open Question 2
- Question: What is the impact of the number of random convolution layers on registration accuracy and computational efficiency, and is there an optimal number that balances these factors?
- Basis in paper: [explicit] The paper uses 4 random convolution layers but mentions that experiments with 3 layers showed performance drops, without exploring the full range of possible layer numbers or their trade-offs.
- Why unresolved: The paper doesn't systematically investigate how the number of random convolution layers affects performance or computational cost, nor does it establish an optimal number of layers.
- What evidence would resolve it: A comprehensive ablation study varying the number of random convolution layers (e.g., 1, 2, 3, 4, 5, 6) and measuring both registration accuracy and computational efficiency would help determine the optimal balance.

### Open Question 3
- Question: How does the proposed contrast-invariant latent regularization (CLR) compare to other potential regularization strategies, such as adversarial training or self-supervised contrastive learning approaches?
- Basis in paper: [explicit] The paper mentions that InfoNCE loss was tested as an alternative to the proposed mean squared difference loss but doesn't explore other regularization strategies or provide a comprehensive comparison.
- Why unresolved: The paper doesn't explore a wide range of alternative regularization approaches or provide a detailed comparison of their effectiveness for learning contrast-invariant representations.
- What evidence would resolve it: Experiments comparing CLR to other regularization strategies (e.g., adversarial training, self-supervised contrastive learning, or other metric learning approaches) on the same datasets would help determine the relative strengths and weaknesses of each approach.

## Limitations
- The method's performance with extreme intensity variations beyond the U(0,10) sampling range is untested
- The projection head architecture for CLR is minimally specified, leaving optimal design choices unclear
- Theoretical justification for MSE-based similarity in projection space effectively enforcing contrast invariance is limited

## Confidence

- Contrast augmentation mechanism: Medium - works well empirically but theoretical justification is limited
- CLR effectiveness: Medium - ablation studies show benefit but mechanism could be more rigorously validated
- Generalization to unseen contrasts: High - demonstrated across two datasets with consistent improvements

## Next Checks

1. Perform sensitivity analysis on the random convolution weight sampling range (U(0,10)) to determine optimal coverage of contrast space
2. Compare MSE-based CLR against contrastive losses (e.g., InfoNCE) to evaluate if more sophisticated similarity measures improve invariance
3. Test registration performance when training with 2-3 contrasts versus single contrast with augmentation to quantify augmentation effectiveness versus multi-contrast training