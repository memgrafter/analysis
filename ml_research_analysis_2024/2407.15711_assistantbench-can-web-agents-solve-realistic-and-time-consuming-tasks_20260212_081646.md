---
ver: rpa2
title: 'AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?'
arxiv_id: '2407.15711'
source_url: https://arxiv.org/abs/2407.15711
tags:
- tasks
- answer
- bench
- agents
- assistant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ASSISTANT BENCH, a new benchmark with 214
  realistic and time-consuming web tasks designed to evaluate web agents. The authors
  propose SPA, a new web agent equipped with planning and memory components that outperforms
  existing agents by ~7 points.
---

# AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?

## Quick Facts
- arXiv ID: 2407.15711
- Source URL: https://arxiv.org/abs/2407.15711
- Authors: Ori Yoran; Samuel Joseph Amouyal; Chaitanya Malaviya; Ben Bogin; Ofir Press; Jonathan Berant
- Reference count: 38
- Key outcome: No model reaches accuracy above 26 points on realistic web tasks, with web navigation remaining the primary bottleneck

## Executive Summary
ASSISTANT BENCH introduces a new benchmark of 214 realistic, time-consuming web tasks designed to evaluate autonomous web agents. The benchmark reveals that current web agents struggle with complex navigation tasks, with no model achieving accuracy above 26 points. The authors introduce SEEPLANACT (SPA), a web agent with planning and memory components that outperforms previous agents by approximately 7 points. The analysis shows that web navigation errors account for 60% of failures, and that while closed-book models show better accuracy, they suffer from hallucinations that reduce precision.

## Method Summary
ASSISTANT BENCH evaluates web agents on tasks requiring information from multiple websites through realistic scenarios like planning events or finding products. The benchmark uses SEEACT as a baseline web agent and introduces SPA, which adds a planning component for dynamic trajectory adjustment and a memory buffer for information transfer between steps. The evaluation measures accuracy, answer rate, precision, and exact match, with a fallback mechanism to closed-book models when web agents abstain due to navigation failures.

## Key Results
- No model achieves accuracy above 26 points on ASSISTANT BENCH tasks
- Web navigation errors account for 60% of agent failures
- SPA outperforms baseline agents by approximately 7 points
- Closed-book models show higher accuracy but lower precision due to hallucinations
- The SPA+closed-book ensemble achieves the best overall performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The planning component in SPA improves navigation accuracy by enabling step-by-step reasoning over web pages.
- Mechanism: SPA maintains an execution plan that gets refined at each step based on current observations, allowing the agent to adjust its trajectory dynamically rather than following a fixed path.
- Core assumption: Web navigation requires adaptive decision-making where each action depends on previous outcomes and current page state.
- Evidence anchors:
  - [abstract]: "SPA, a new web agent that significantly outperforms previous agents, and an ensemble of SPA and closed-book models reaches the best overall performance."
  - [section]: "Since ASSISTANT BENCH focuses on tasks that require planning and reasoning, we equip SEEACT with two specialized components: (1) a planning component for the model to plan and re-plan its execution, and (2) a memory component with the option to transfer information between steps via a memory buffer."
- Break condition: If the planning component fails to update based on actual page content, the agent may continue executing incorrect plans despite new information being available.

### Mechanism 2
- Claim: The memory buffer in SPA enables information transfer between steps, reducing redundant exploration.
- Mechanism: SPA can store relevant information discovered during navigation and retrieve it later, preventing the agent from repeatedly visiting the same pages or re-discovering known facts.
- Core assumption: Information-seeking tasks often require gathering data across multiple pages, and maintaining state between interactions is crucial for efficiency.
- Evidence anchors:
  - [abstract]: "In addition, we introduce SEEPLANACT (SPA), a variant of SEEACT equipped with a planning component and a memory buffer (ยง4)."
  - [section]: "SPA. Since ASSISTANT BENCH focuses on tasks that require planning and reasoning, we equip SEEACT with two specialized components: (1) a planning component for the model to plan and re-plan its execution, and (2) a memory component with the option to transfer information between steps via a memory buffer."
- Break condition: If the memory buffer becomes overloaded with irrelevant information or fails to retrieve the correct stored data when needed, the agent's performance degrades.

### Mechanism 3
- Claim: Ensemble approach combining SPA with closed-book models achieves better performance by leveraging complementary strengths.
- Mechanism: When SPA encounters navigation failures and abstains from answering, the system falls back to a closed-book model that can provide answers based on parametric knowledge, compensating for web navigation limitations.
- Core assumption: Web agents and closed-book models have different failure modes - agents fail due to navigation issues while closed-book models fail due to lack of up-to-date information.
- Evidence anchors:
  - [abstract]: "Additionally, we introduce SEEPLANACT (SPA), a new web agent that significantly outperforms previous agents, and an ensemble of SPA and closed-book models reaches the best overall performance."
  - [section]: "As we will show, web agents often refrain from answering when web navigation fails. Thus, we evaluate ensembles where we fall back to closed-book models, CB-1S, when the agents abstain."
- Break condition: If both the web agent and closed-book model fail on the same task (e.g., the information is genuinely unavailable to both parametric knowledge and web search), the ensemble cannot provide correct answers.

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: Tasks in ASSISTANT BENCH require multi-step reasoning where the agent must plan its navigation path and synthesize information from multiple sources.
  - Quick check question: Can you explain why breaking down a complex web navigation task into intermediate reasoning steps would improve success rates compared to direct action?

- Concept: Retrieval-augmented generation
  - Why needed here: The benchmark evaluates models that can search the web for relevant information, requiring understanding of how to formulate effective search queries and integrate retrieved content.
  - Quick check question: What are the key differences between parametric knowledge and retrieved evidence, and why might retrieval sometimes hurt performance?

- Concept: Evaluation metrics for structured outputs
  - Why needed here: Tasks require evaluating dictionary answers with multiple key-value pairs, necessitating understanding of precision, recall, and F1 scores for semi-structured data.
  - Quick check question: How would you compute precision for a predicted dictionary answer compared to a gold dictionary answer with multiple key-value pairs?

## Architecture Onboarding

- Component map: Web interaction layer -> Planning module -> Memory buffer -> Language model -> Fallback mechanism
- Critical path:
  1. Receive task input and initial web page
  2. Generate natural language description of next action
  3. Ground description to HTML element selection
  4. Execute action and capture new page state
  5. Update memory buffer with relevant information
  6. Refine execution plan based on new observations
  7. Repeat until task completion or timeout
  8. If web agent abstains, fall back to closed-book model

- Design tradeoffs:
  - Action granularity: Fine-grained actions (click specific elements) vs. coarse actions (scroll page) affect both success rates and execution efficiency
  - Memory retention: Longer memory retention helps avoid revisiting pages but increases computational overhead
  - Planning depth: More detailed planning improves navigation but may lead to over-complication and execution errors

- Failure signatures:
  - Navigation loop: Agent repeatedly visits same pages without progress
  - Grounding failure: Model cannot identify correct HTML elements for intended actions
  - Memory retrieval error: Agent retrieves incorrect or irrelevant information from memory buffer
  - Planning stagnation: Execution plan doesn't adapt to new page content

- First 3 experiments:
  1. Compare SPA vs. SEEACT on tasks requiring information from exactly 3 websites to isolate planning benefits
  2. Test memory buffer capacity by varying the number of items that can be stored and retrieved
  3. Evaluate fallback mechanism effectiveness by measuring answer rates when web agent abstains on different task types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the time-dependency of tasks in ASSISTANT BENCH be addressed to make them more suitable for long-term evaluation?
- Basis in paper: [explicit] The authors note that some tasks in ASSISTANT BENCH are time-dependent and may change in the future, making them less suitable for long-term evaluation. They suggest that future work could expand the benchmark to include time-dependent tasks by extending specialized evaluation models to a multi-modal web setting.
- Why unresolved: The authors do not provide specific methods or approaches for handling time-dependent tasks in the benchmark. They only mention the potential for future work to address this issue.
- What evidence would resolve it: Evidence could include proposed methods or frameworks for evaluating time-dependent tasks in web agents, along with empirical results demonstrating their effectiveness.

### Open Question 2
- Question: How can the cost of evaluating web agents on ASSISTANT BENCH be reduced to enable more widespread experimentation?
- Basis in paper: [explicit] The authors mention that web agents are significantly more expensive to evaluate than closed-book and retrieval-augmented models due to the high number of model calls required. They suggest that this cost is a limitation in the presented results and that future work could explore methods to reduce these costs.
- Why unresolved: The authors do not provide specific strategies or techniques for reducing the cost of evaluating web agents on the benchmark. They only acknowledge the issue and suggest it as a direction for future research.
- What evidence would resolve it: Evidence could include proposed techniques for reducing the number of model calls required for web agents, along with empirical results demonstrating their effectiveness in lowering costs while maintaining evaluation quality.

### Open Question 3
- Question: How can the performance gap between open-source and proprietary multi-modal web agents be closed?
- Basis in paper: [explicit] The authors note that their models are based on GPT-4-Turbo and Claude-3.5-Sonnet, which are proprietary models. They acknowledge that this limits reproducibility and that there is a large gap in performance between these models and open-source multi-modal models.
- Why unresolved: The authors do not provide specific methods or approaches for improving the performance of open-source multi-modal web agents. They only mention that as open-source models continue to improve, ASSISTANT BENCH will be a useful evaluation resource.
- What evidence would resolve it: Evidence could include proposed methods for training or fine-tuning open-source multi-modal models specifically for web agent tasks, along with empirical results demonstrating their improved performance on ASSISTANT BENCH compared to current open-source models.

## Limitations
- The benchmark's time-dependent tasks may become outdated, limiting long-term evaluation utility
- Web agent evaluation is significantly more expensive than closed-book models due to high model call requirements
- Proprietary models (GPT-4-Turbo, Claude-3.5-Sonnet) limit reproducibility and create performance gaps with open-source alternatives

## Confidence

- **High confidence**: The observation that no model achieves accuracy above 26 points and that web navigation errors are the primary failure mode
- **Medium confidence**: The effectiveness of the SPA agent's planning and memory components, as performance improvements are demonstrated but mechanisms could be better characterized
- **Medium confidence**: The ensemble approach combining SPA with closed-book models, though the complementary nature of these approaches is well-supported

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of the planning component and memory buffer to SPA's performance gains
2. Analyze specific navigation error patterns to identify whether failures stem from grounding, trajectory planning, or execution issues
3. Test SPA's performance on tasks with varying complexity levels to determine at what point the planning component becomes insufficient for successful completion