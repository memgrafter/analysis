---
ver: rpa2
title: What Variables Affect Out-of-Distribution Generalization in Pretrained Models?
arxiv_id: '2405.15018'
source_url: https://arxiv.org/abs/2405.15018
tags:
- tunnel
- effect
- accuracy
- dataset
- resolution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically examines the factors influencing the
  strength of the tunnel effect in deep neural networks, which is known to impair
  out-of-distribution (OOD) generalization. The tunnel effect is characterized by
  representation compression in deeper layers, hindering the transferability of learned
  features.
---

# What Variables Affect Out-of-Distribution Generalization in Pretrained Models?

## Quick Facts
- arXiv ID: 2405.15018
- Source URL: https://arxiv.org/abs/2405.15018
- Reference count: 40
- Authors: Md Yousuf Harun; Kyungbok Lee; Jhair Gallardo; Giri Krishnan; Christopher Kanan

## Executive Summary
This paper systematically examines the factors influencing the strength of the tunnel effect in deep neural networks, which is known to impair out-of-distribution (OOD) generalization. The tunnel effect is characterized by representation compression in deeper layers, hindering the transferability of learned features. The authors conduct a comprehensive study using 64 different DNN architectures trained on ImageNet-100, CIFAR-10, and CIFAR-100 datasets with varying image resolutions, augmentation policies, and architectural configurations. They evaluate the tunnel effect using three metrics: % OOD performance retained, Pearson correlation between in-distribution (ID) and OOD accuracy, and ID/OOD alignment. The study reveals that the tunnel effect is not universal and is significantly influenced by training data diversity, including the number of semantic classes, image resolution, and data augmentation.

## Method Summary
The authors conduct a comprehensive study of the tunnel effect using 64 different DNN architectures trained on ImageNet-100, CIFAR-10, and CIFAR-100 datasets with varying image resolutions, augmentation policies, and architectural configurations. They evaluate the tunnel effect using three metrics: % OOD performance retained, Pearson correlation between in-distribution (ID) and OOD accuracy, and ID/OOD alignment. The study uses linear probes to assess how well embeddings from each DNN layer generalize to OOD data, training probes on both ID and OOD datasets for each layer and comparing accuracy trends to identify representation compression. Statistical analysis (Wilcoxon signed-rank test, Cliff's Delta) and SHAP analysis are employed to rigorously compare experimental conditions and quantify the relative impact of different variables on OOD generalization metrics.

## Key Results
- Training on high-resolution datasets with many classes and using augmentations greatly reduces the tunnel effect and improves OOD generalization.
- Widely used pre-trained models like ViTs and ConvNeXts do not exhibit the tunnel effect, unlike ResNet-50.
- The tunnel effect is significantly influenced by training data diversity, particularly the number of semantic classes, image resolution, and data augmentation.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The tunnel effect is not universal and depends on training data diversity, particularly the number of semantic classes, image resolution, and data augmentation.
- **Mechanism:** DNNs trained on high-resolution datasets with many classes and strong augmentations reduce representation compression in deeper layers, thereby improving OOD generalization. This occurs because diverse training data encourages learning more hierarchical and generalizable features across all layers.
- **Core assumption:** Representation compression in deeper layers (the tunnel effect) is primarily driven by insufficient diversity in the training data, not inherent architectural limitations.
- **Evidence anchors:**
  - [abstract] "training with high-resolution datasets containing many classes greatly reduces representation compression and improves transferability"
  - [section 1] "both [9] and [10] limited their experiments to datasets with low-resolution images and relatively few categories (CIFAR-10, MNIST)"
- **Break condition:** If the training data diversity is artificially increased without improving actual semantic variability (e.g., only increasing image count but not class count), the tunnel effect may persist.

### Mechanism 2
- **Claim:** Linear probe accuracy trends (ID vs OOD) can quantify tunnel effect strength and reveal when OOD generalization degrades.
- **Mechanism:** By training linear probes on embeddings from each layer and comparing ID and OOD accuracy trends, we can identify layers where OOD accuracy drops while ID accuracy continues to rise—indicating representation compression and reduced OOD transferability.
- **Core assumption:** Linear probes trained on intermediate DNN representations accurately reflect the model's ability to generalize to OOD data.
- **Evidence anchors:**
  - [section 3.1] "we use linear probes for our tunnel effect analysis... we train ID and OOD linear probes on embeddings produced by each layer"
- **Break condition:** If the OOD datasets used for probing are not sufficiently distinct from the ID dataset, the tunnel effect may be underestimated or not detected.

### Mechanism 3
- **Claim:** DNN architecture choices (depth, stem size, spatial reduction) significantly impact OOD generalization and tunnel effect strength.
- **Mechanism:** Architectural factors like excessive depth, large stem sizes, and aggressive spatial reduction can exacerbate the tunnel effect by promoting earlier and stronger representation compression, while architectures with smaller stems and less spatial reduction can mitigate it.
- **Core assumption:** Architectural hyperparameters directly influence the degree of representation compression in deeper layers.
- **Evidence anchors:**
  - [section 4.1.4] "Our SHAP analysis revealed that lower values for spatial reduction (ϕ) hurt OOD generalization"
  - [section 4.1.4] "Our SHAP analysis showed that over-parameterization level negatively impacts OOD generalization"
- **Break condition:** If architectural changes are made without controlling for other variables (like training data diversity), the observed effects on the tunnel may be confounded.

## Foundational Learning

- **Concept:** Linear probe methodology for evaluating DNN representation transferability
  - Why needed here: Linear probes are the primary tool used in this paper to assess how well embeddings from each DNN layer generalize to OOD data. Understanding how to train and interpret linear probe results is essential for replicating and extending this work.
  - Quick check question: What is the difference between training a linear probe on ID vs OOD data, and what does it tell us about representation quality?

- **Concept:** Statistical analysis methods (Wilcoxon signed-rank test, Cliff's Delta) for comparing experimental conditions
  - Why needed here: The paper uses these methods to rigorously compare the impact of different variables (e.g., augmentation, resolution) on tunnel effect strength. Knowing how to apply and interpret these tests is crucial for analyzing experimental results.
  - Quick check question: When would you use a Wilcoxon signed-rank test versus a t-test for comparing paired experimental results?

- **Concept:** SHAP (SHapley Additive exPlanations) analysis for understanding variable importance
  - Why needed here: SHAP analysis is used to quantify the relative impact of different variables (e.g., resolution, augmentation, depth) on OOD generalization metrics. Understanding SHAP helps interpret the main results and identify the most influential factors.
  - Quick check question: How does SHAP handle interactions between variables when determining their individual contributions to a target metric?

## Architecture Onboarding

- **Component map:** Data preprocessing -> DNN backbone training -> Feature extraction -> Linear probe training -> OOD evaluation -> Statistical/SHAP analysis
- **Critical path:** 1) Train DNN on ID dataset with specific configuration (architecture, resolution, augmentation). 2) Extract features from each layer using global average pooling. 3) Train linear probes on ID and OOD datasets for each layer. 4) Compute OOD generalization metrics (% retained, Pearson correlation, ID/OOD alignment). 5) Aggregate results across experiments and perform statistical/SHAP analysis.
- **Design tradeoffs:** High-resolution training requires more compute but reduces tunnel effect; stronger augmentations improve generalization but may slow training; deeper architectures have more parameters but can increase representation compression; using many OOD datasets provides robust evaluation but increases computational cost.
- **Failure signatures:** If OOD accuracy drops significantly in later layers while ID accuracy continues to rise, this indicates a strong tunnel effect; if SHAP analysis shows high variance or low R², the model may not be capturing variable importance well; if linear probe training is unstable, the feature extraction or probe architecture may need adjustment.
- **First 3 experiments:**
  1. Train a VGGm-11 on ImageNet-100 at 32×32 resolution without augmentations, then evaluate tunnel effect using linear probes on 8 OOD datasets.
  2. Repeat experiment 1 but with 224×224 resolution to observe the impact of higher resolution on the tunnel effect.
  3. Train the same VGGm-11 on ImageNet-100 at 32×32 resolution with augmentations, then compare tunnel effect metrics to the non-augmented version.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the tunnel effect manifest similarly in multimodal and biased datasets compared to vision datasets with supervised learning?
- **Basis in paper:** [inferred] The authors explicitly state that future work could study non-vision, multi-modal, and biased datasets where the tunnel effect has not yet been studied.
- **Why unresolved:** The current study is limited to vision datasets with supervised learning, and the authors acknowledge this limitation, calling for further investigation in other domains.
- **What evidence would resolve it:** Conducting experiments similar to those in the paper on multimodal and biased datasets, measuring the strength of the tunnel effect using the proposed metrics (e.g., % OOD performance retained, Pearson correlation, ID/OOD alignment).

### Open Question 2
- **Question:** Is the observed improvement in OOD generalization with SSL methods primarily due to their augmentation policies or their objective functions?
- **Basis in paper:** [explicit] The authors state that replicating their SHAP analysis with multiple augmentation policies could reveal whether the OOD generalization capabilities of SSL algorithms are due to their augmentation policies versus their objective functions.
- **Why unresolved:** The current study does not disentangle the effects of augmentations and objective functions in SSL methods, as it relies on pre-trained models with various augmentation strategies.
- **What evidence would resolve it:** Training SSL models with different augmentation policies while keeping the objective function constant, and vice versa, then measuring the impact on OOD generalization using the proposed metrics.

### Open Question 3
- **Question:** What regularizers or techniques can mitigate tunnel formation in continual learning methods that start from scratch using small initial sets?
- **Basis in paper:** [inferred] The authors suggest that identifying regularizers or other techniques that mitigate tunnel formation should be sought for continual learning methods, as it could greatly improve forward transfer and lead to more efficient continual learning methods.
- **Why unresolved:** The current study does not explore specific regularizers or techniques to address the tunnel effect in continual learning, and the authors acknowledge this as a potential direction for future work.
- **What evidence would resolve it:** Experimenting with various regularizers or techniques (e.g., weight decay, dropout, contrastive learning) in continual learning settings, measuring their impact on the tunnel effect and OOD generalization using the proposed metrics.

## Limitations

- The study primarily focuses on image classification tasks with specific DNN architectures, potentially limiting generalizability to other modalities.
- The analysis relies heavily on linear probes as a proxy for representation quality, which may not fully capture the complexity of feature transferability in downstream tasks.
- The computational requirements for high-resolution training and extensive OOD evaluation may limit practical applicability.

## Confidence

- **High confidence**: Claims regarding the impact of training data diversity (resolution, class count, augmentations) on reducing the tunnel effect are well-supported by experimental evidence and consistent across multiple architectures and datasets.
- **Medium confidence**: Architectural factors (depth, spatial reduction) influencing the tunnel effect are supported by SHAP analysis, but the causal relationship could benefit from more controlled experiments isolating each architectural variable.
- **Low confidence**: The claim that widely used pre-trained models (ViTs, ConvNeXts) don't exhibit the tunnel effect is based on comparison with ResNet-50 but lacks systematic evaluation across a broader range of commonly used models.

## Next Checks

1. **Replicate tunnel effect experiments with additional architectures**: Test the proposed mechanisms using architectures not evaluated in the original study (e.g., Swin Transformers, EfficientNets) to verify if the tunnel effect patterns hold across a wider range of models.

2. **Validate linear probe methodology with downstream task transfer**: Compare linear probe results with actual transfer learning performance on diverse downstream tasks to assess whether linear probes accurately predict real-world transferability.

3. **Conduct ablation studies on augmentation strength**: Systematically vary augmentation intensity (rather than just presence/absence) to determine the threshold at which augmentations begin to significantly impact the tunnel effect and OOD generalization.