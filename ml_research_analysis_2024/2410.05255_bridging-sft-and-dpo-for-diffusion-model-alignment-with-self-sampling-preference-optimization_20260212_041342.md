---
ver: rpa2
title: Bridging SFT and DPO for Diffusion Model Alignment with Self-Sampling Preference
  Optimization
arxiv_id: '2410.05255'
source_url: https://arxiv.org/abs/2410.05255
tags:
- sspo
- data
- training
- reward
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Sampling Preference Optimization (SSPO),
  a novel post-training method for aligning diffusion models that bridges the gap
  between supervised fine-tuning (SFT) and reinforcement learning (RL) approaches.
  SSPO addresses the limitations of existing methods by eliminating the need for paired
  preference data or reward models while preserving the training stability of SFT
  and the generalization capability of RL.
---

# Bridging SFT and DPO for Diffusion Model Alignment with Self-Sampling Preference Optimization

## Quick Facts
- arXiv ID: 2410.05255
- Source URL: https://arxiv.org/abs/2410.05255
- Authors: Daoan Zhang; Guangchen Lan; Dong-Jun Han; Wenlin Yao; Xiaoman Pan; Hongming Zhang; Mingxiao Li; Pengcheng Chen; Yu Dong; Christopher Brinton; Jiebo Luo
- Reference count: 38
- Key outcome: Introduces SSPO, a novel post-training method that bridges SFT and RL approaches for diffusion model alignment, eliminating the need for paired preference data or reward models while preserving training stability and generalization capability

## Executive Summary
This paper presents Self-Sampling Preference Optimization (SSPO), a novel post-training method for aligning diffusion models that bridges the gap between supervised fine-tuning (SFT) and reinforcement learning (RL) approaches. SSPO addresses the limitations of existing methods by eliminating the need for paired preference data or reward models while preserving the training stability of SFT and the generalization capability of RL. The method introduces two key strategies: Random Checkpoint Replay (RCR), which samples from historical checkpoints to construct effective training pairs and mitigate overfitting, and Self-Sampling Regularization (SSR), which dynamically evaluates sample quality to determine whether to use DPO or SFT updates during training.

## Method Summary
SSPO is a post-training alignment method for diffusion models that combines elements of supervised fine-tuning and direct preference optimization. The approach uses Random Checkpoint Replay (RCR) to sample historical checkpoints uniformly, creating diverse reference distributions that prevent overfitting and improve training stability. Self-Sampling Regularization (SSR) dynamically evaluates sample quality by comparing the probability of generating winning samples between the current model and sampled checkpoints, automatically switching between SFT and DPO based on this assessment. The method operates on diffusion models like Stable Diffusion 1.5 for text-to-image tasks and AnimateDiff for text-to-video generation, using datasets such as Pick-a-Pic and MagicTime for training and evaluation.

## Key Results
- SSPO outperforms existing SFT and DPO baselines on text-to-image benchmarks including PickScore, HPSv2, and ImageReward metrics
- The method demonstrates strong performance on text-to-video tasks, achieving superior results on FID, PSNR, FVD, MTScore, and CHScore metrics
- SSPO achieves RL-level generalization while maintaining SFT-level training stability without requiring external reward models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random Checkpoint Replay (RCR) effectively mitigates overfitting while improving training stability
- Mechanism: By sampling uniformly from all previous checkpoints rather than just the latest one, the method introduces diversity in the reference distribution, preventing the model from collapsing to local optima
- Core assumption: Historical checkpoints contain sufficiently diverse information that remains relevant for training
- Evidence anchors:
  - [abstract] "Random Checkpoint Replay (RCR) strategy that utilizes historical checkpoints to construct paired data, thereby effectively mitigating overfitting"
  - [section 4.1] "compared to always utilizing the last checkpoint, selecting the checkpoint from the previous iteration can mitigate the risk of overfitting and yields superior performance"
  - [corpus] Weak evidence - no directly comparable papers found in corpus
- Break condition: If historical checkpoints become too dissimilar from the current policy, or if checkpoints are saved too infrequently to capture meaningful diversity

### Mechanism 2
- Claim: Self-Sampling Regularization (SSR) enables dynamic switching between SFT and DPO based on sample quality
- Mechanism: SSR evaluates whether reference model samples are truly "losing" by comparing their probability of generating winning samples versus the current model, automatically switching to SFT when reference samples are better
- Core assumption: The reference model's ability to generate winning samples correlates with the quality of its generated samples
- Evidence anchors:
  - [abstract] "Self-Sampling Regularization (SSR) strategy is employed to dynamically evaluate the quality of generated samples; when the generated samples are more likely to be winning samples, the approach automatically switches from DPO to SFT"
  - [section 4.2] "if the sampled checkpoint has a higher probability of generating noise compared to the current model, then in this situation, the reference image generated by the sampled checkpoint is also more likely to be a winning image"
  - [corpus] Weak evidence - no directly comparable papers found in corpus
- Break condition: If the quality assessment criterion becomes unreliable due to model collapse or if the switching threshold is poorly calibrated

### Mechanism 3
- Claim: SSPO bridges the generalization gap between SFT and RL by combining their advantages without requiring external reward models
- Mechanism: By using self-generated preference data through checkpoint replay and dynamic regularization, SSPO achieves RL-level generalization while maintaining SFT-level training stability
- Core assumption: Self-generated preference data can effectively approximate human preference signals
- Evidence anchors:
  - [abstract] "preserving the advantages of both SFT and RL -- namely, eliminating the need for paired data and reward models while retaining the training stability of SFT and the generalization ability of RL"
  - [section 1] "SSPO introduces a Random Checkpoint Replay (RCR) strategy that utilizes historical checkpoints to construct paired data, thereby effectively mitigating overfitting"
  - [corpus] Weak evidence - no directly comparable papers found in corpus
- Break condition: If self-generated preference data diverges significantly from actual human preferences or if the training process becomes unstable due to poor checkpoint selection

## Foundational Learning

- Concept: Diffusion probabilistic models and denoising processes
  - Why needed here: SSPO operates directly on diffusion model parameters and noise predictions
  - Quick check question: What is the mathematical relationship between the predicted noise and the original image in a diffusion model?

- Concept: Preference optimization and Bradley-Terry model
  - Why needed here: SSPO builds on DPO framework but modifies how preference pairs are constructed
  - Quick check question: How does the Bradley-Terry model relate to the preference likelihood used in DPO?

- Concept: Experience replay and checkpoint management
  - Why needed here: RCR strategy requires understanding how to effectively sample and utilize historical checkpoints
  - Quick check question: What are the trade-offs between saving checkpoints more frequently versus less frequently in terms of memory and training effectiveness?

## Architecture Onboarding

- Component map:
  Diffusion model (e.g., Stable Diffusion 1.5) -> Checkpoint manager -> SSR evaluator -> Loss computation module -> Training loop orchestrator

- Critical path:
  1. Sample checkpoint from historical checkpoints (RCR)
  2. Generate reference samples using sampled checkpoint
  3. Evaluate sample quality using SSR criterion
  4. Compute appropriate loss (SFT or DPO) based on SSR result
  5. Update model parameters
  6. Save checkpoint at regular intervals

- Design tradeoffs:
  - Checkpoint frequency vs memory usage: More frequent checkpoints provide better diversity but consume more storage
  - SSR sensitivity vs training stability: Stricter SSR criteria may improve quality but could reduce effective training signals
  - Batch size vs computational efficiency: Larger batches provide more stable gradients but require more memory

- Failure signatures:
  - Training instability or divergence: May indicate poor checkpoint sampling or SSR misconfiguration
  - Plateau in performance improvement: Could suggest insufficient checkpoint diversity or overly conservative SSR switching
  - Increased overfitting: May result from inadequate checkpoint sampling strategy or improper learning rate

- First 3 experiments:
  1. Implement basic RCR with uniform sampling and measure overfitting reduction compared to single checkpoint baseline
  2. Add SSR component and verify it correctly switches between SFT and DPO modes based on sample quality
  3. Evaluate end-to-end performance on text-to-image benchmarks and compare against SFT and DPO baselines

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness of Random Checkpoint Replay depends heavily on checkpoint quality and frequency, which could limit performance if checkpoints become too dissimilar from the current policy
- Self-Sampling Regularization mechanism relies on the assumption that reference model sample quality correlates with preference outcomes, but this relationship may not hold consistently across different domains or model scales
- While SSPO shows strong results on text-to-image and video tasks, its performance on other modalities or more complex reasoning tasks remains untested

## Confidence
- **High Confidence:** The experimental results demonstrating SSPO's superiority over SFT and DPO baselines on established benchmarks (PickScore, HPSv2, ImageReward, FID, FVD)
- **Medium Confidence:** The theoretical justification for combining SFT and DPO through self-generated preference data, as this relies on assumptions about sample quality correlation
- **Low Confidence:** The scalability of SSPO to larger model sizes or more complex generation tasks beyond text-to-image/video, given limited ablation studies in these areas

## Next Checks
1. **Ablation Study on Checkpoint Frequency:** Systematically vary checkpoint saving frequency (e.g., every 10, 30, 100 updates) and measure the impact on SSPO performance and training stability to determine optimal checkpoint management strategy.

2. **Cross-Domain Generalization Test:** Apply SSPO to a fundamentally different task domain (e.g., text-to-3D or code generation) to validate whether the RCR and SSR mechanisms generalize beyond image and video generation.

3. **Sample Quality Correlation Analysis:** Conduct controlled experiments to verify the core assumption of SSR by measuring the actual correlation between sample quality metrics and preference outcomes across different model checkpoints and training stages.