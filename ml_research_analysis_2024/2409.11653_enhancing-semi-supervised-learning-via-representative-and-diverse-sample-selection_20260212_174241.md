---
ver: rpa2
title: Enhancing Semi-Supervised Learning via Representative and Diverse Sample Selection
arxiv_id: '2409.11653'
source_url: https://arxiv.org/abs/2409.11653
tags:
- learning
- samples
- rdss
- which
- kernel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces RDSS, a sample selection method for semi-supervised\
  \ learning that addresses the problem of selecting representative and diverse samples\
  \ under extremely low annotation budgets. The core innovation is \u03B1-MMD, a novel\
  \ criterion that balances representativeness and diversity through a trade-off parameter\
  \ \u03B1, optimized using a modified Frank-Wolfe algorithm called GKHR."
---

# Enhancing Semi-Supervised Learning via Representative and Diverse Sample Selection

## Quick Facts
- arXiv ID: 2409.11653
- Source URL: https://arxiv.org/abs/2409.11653
- Reference count: 40
- Key outcome: RDSS achieves superior results particularly under severely constrained budgets (e.g., 40-400 samples) with computational efficiency (O(mn)) and independence from downstream model training

## Executive Summary
This paper introduces RDSS, a sample selection method for semi-supervised learning that addresses the problem of selecting representative and diverse samples under extremely low annotation budgets. The core innovation is α-MMD, a novel criterion that balances representativeness and diversity through a trade-off parameter α, optimized using a modified Frank-Wolfe algorithm called GKHR. Experiments demonstrate RDSS consistently improves performance across multiple SSL frameworks (FlexMatch, FreeMatch) on datasets like CIFAR-10/100, SVHN, STL-10, and ImageNet, outperforming state-of-the-art sample selection methods from active learning and semi-supervised active learning domains.

## Method Summary
RDSS is a model-free sample selection framework for semi-supervised learning that operates in two phases: feature extraction using CLIP followed by sample selection via α-MMD minimization. The method extracts CLIP features from unlabeled data, then applies GKHR (a modified Frank-Wolfe algorithm) to select m samples that minimize α-MMD, which balances representativeness (alignment with full dataset distribution) and diversity (coverage of dataset). The trade-off parameter α is set to 1 - 1/√m based on theoretical analysis. Selected samples are then used to train SSL models like FlexMatch or FreeMatch. The approach is computationally efficient (O(mn)) and independent of downstream model training.

## Key Results
- RDSS consistently outperforms state-of-the-art sample selection methods on CIFAR-10/100, SVHN, STL-10, and ImageNet
- Superior performance under severely constrained budgets (40-400 samples) where traditional methods struggle
- Achieves computational efficiency with O(mn) complexity while maintaining model-free independence
- Demonstrates effectiveness across multiple SSL frameworks including FlexMatch and FreeMatch

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimizing α-MMD enhances the generalization ability of low-budget semi-supervised learning models.
- Mechanism: α-MMD balances representativeness (aligning the distribution of selected samples with the full dataset) and diversity (ensuring coverage of the dataset by avoiding clustering in high-density areas). This balance prevents the model from overfitting to a non-representative subset while ensuring sufficient coverage of the data distribution.
- Core assumption: The Reproducing Kernel Hilbert Space (RKHS) assumptions (bounded positive definite kernels) and the existence of embeddings for conditional distributions (E(Y|X) and Var(Y|X)) hold.
- Evidence anchors:
  - [abstract] "We prove that our method benefits the generalizability of the trained model under certain assumptions and rigorously establish an optimal interval for the trade-off parameter α adapt to the different budgets."
  - [section] "We prove that under certain Reproducing Kernel Hilbert Space (RKHS) assumptions, α-MMD effectively bounds the difference between training with a constrained versus an unlimited labelling budget."
  - [corpus] Weak; related works focus on unlabeled data selection but do not establish theoretical generalization bounds under RKHS assumptions.
- Break condition: If the kernel is not characteristic or positive definite, the MMD may not be a proper metric, invalidating the representativeness measure. If the RKHS assumptions fail (e.g., the hypothesis space is not an RKHS), the theoretical bounds do not hold.

### Mechanism 2
- Claim: The GKHR algorithm efficiently approximates the solution to the α-MMD minimization problem without replacement.
- Mechanism: GKHR extends the Frank-Wolfe algorithm by iteratively selecting samples that minimize the weighted sum of similarities to already selected samples and dissimilarities to the full dataset. The modification for sampling without replacement prevents duplicate selections, ensuring a diverse subset.
- Core assumption: The decaying rate for optimization error of GKHR can be bounded by O(log m/m) under the low-budget setting (m ≪ n).
- Evidence anchors:
  - [section] "Under a low-budget setting, we develop a fast and efficient algorithm GKHR for this problem using the Frank-Wolfe algorithm."
  - [section] "We can then show that the decaying rate for optimization error of GKHR can be upper bounded by O(log m/m)."
  - [corpus] Weak; related works mention subsampling algorithms but do not provide specific error bounds for Frank-Wolfe-based methods without replacement.
- Break condition: If the assumption that "the minimum is never larger than the mean" fails (e.g., when m is not relatively small), the theoretical justification for GKHR's performance weakens.

### Mechanism 3
- Claim: The trade-off parameter α can be tuned to adapt to different annotation budgets, optimizing the balance between representativeness and diversity.
- Mechanism: As α decreases, the weight on diversity increases (λ increases), encouraging the selection of more diverse samples. The optimal interval for α is derived as [1 - 1/√m, 1), ensuring the upper bound of the MMD does not exceed that of α-MMD in order of magnitude.
- Core assumption: The upper bound of the MMD between the selected samples and the full dataset under a low-budget setting can be controlled by choosing α within the specified interval.
- Evidence anchors:
  - [section] "We can just set α ∈ [1 − 1√m , 1) so that the upper bound of the MMD would not be larger than the one of α-MMD in the perspective of the order of magnitude."
  - [section] "According to Theorem 5.6 and Lemma B.3, by straightforward deduction we have MMDk(XI∗m , Xn) ≤ Cα + O(log m/m) + (1 − α)√K to upper bound the MMD between the selected samples and the full dataset under a low-budget setting."
  - [corpus] Weak; related works mention hyperparameter tuning but do not provide theoretical justification for the optimal interval based on budget size.
- Break condition: If the budget m is not small relative to n, the theoretical justification for the optimal interval may not hold, and the performance could degrade.

## Foundational Learning

- Concept: Reproducing Kernel Hilbert Space (RKHS)
  - Why needed here: RKHS provides the mathematical framework for defining and working with the MMD and α-MMD, which are used to quantify the representativeness and diversity of selected samples. The properties of RKHS (e.g., bounded positive definite kernels) are crucial for the theoretical analysis and the design of the GKHR algorithm.
  - Quick check question: What is the role of the kernel function in an RKHS, and why must it be positive definite and characteristic for this application?

- Concept: Frank-Wolfe Algorithm
  - Why needed here: The Frank-Wolfe algorithm is used as the basis for the GKHR algorithm to efficiently solve the convex optimization problem of minimizing α-MMD. Understanding its iterative nature and convergence properties is essential for analyzing the performance of GKHR.
  - Quick check question: How does the Frank-Wolfe algorithm iteratively update the solution, and what is the significance of the step size in each iteration?

- Concept: Semi-Supervised Learning (SSL) and Consistency Regularization
  - Why needed here: The selected samples are used to train SSL models (e.g., FlexMatch, FreeMatch). Understanding the principles of SSL, particularly consistency regularization, is important for interpreting the experimental results and the impact of sample selection on model performance.
  - Quick check question: How does consistency regularization work in SSL, and why is it important to have representative and diverse samples for training?

## Architecture Onboarding

- Component map:
  CLIP feature extraction -> α-MMD calculation -> GKHR optimization -> Sample selection -> SSL training (FlexMatch/FreeMatch) -> Evaluation

- Critical path:
  1. Extract features from unlabeled dataset using CLIP
  2. Apply RDSS to select m samples based on α-MMD minimization via GKHR
  3. Use the selected samples to train an SSL model (FlexMatch or FreeMatch)
  4. Evaluate the trained model on the test set

- Design tradeoffs:
  - Model-free vs. Model-based: RDSS is model-free, selecting samples before training, which avoids dependency on model performance but may miss model-specific informative samples
  - Representativeness vs. Diversity: The trade-off parameter α balances these two aspects; a higher α emphasizes representativeness, while a lower α emphasizes diversity
  - Computational Efficiency: GKHR has O(mn) complexity, which is efficient for low-budget settings but may become expensive for very large datasets

- Failure signatures:
  - Poor performance on datasets with complex distribution structures: If the optimal α is not well-tuned or the kernel choice is inappropriate, RDSS may fail to capture the underlying data distribution
  - Sensitivity to kernel bandwidth: The performance of RDSS depends on the choice of the Gaussian kernel bandwidth; an inappropriate value can lead to suboptimal sample selection
  - Limited improvement under high annotation budgets: RDSS is designed for low-budget settings; its benefits may diminish when the annotation budget is large

- First 3 experiments:
  1. Verify the impact of α on sample selection: Run RDSS with different α values (e.g., 0, 0.5, 0.9, 1) on a small dataset and visualize the selected samples to observe the trade-off between representativeness and diversity
  2. Compare GKHR with random sampling: Apply GKHR and random sampling to select samples from a dataset, then train a simple SSL model (e.g., Pseudo-Labeling) and compare the performance to assess the effectiveness of GKHR
  3. Test the sensitivity to kernel bandwidth: Vary the bandwidth parameter σ of the Gaussian kernel in RDSS and evaluate the performance on a validation set to determine the impact of kernel choice on sample selection quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RDSS perform on datasets with non-Gaussian distributions or complex multimodal structures?
- Basis in paper: [inferred] The paper demonstrates effectiveness on CIFAR, SVHN, STL-10, and ImageNet datasets, which may have relatively standard distributions. The choice of α depends on sample count rather than distribution shape.
- Why unresolved: The paper does not explicitly test RDSS on datasets with highly complex or non-standard distributions. The effectiveness of the Gaussian kernel and α parameter selection strategy for such datasets remains unexplored.
- What evidence would resolve it: Empirical results showing RDSS performance on datasets with known complex distributions (e.g., medical imaging datasets, satellite imagery, or datasets with significant class imbalance and overlap) compared to other sampling methods.

### Open Question 2
- Question: Can RDSS be extended to work with active learning paradigms that require iterative sample selection and model retraining?
- Basis in paper: [explicit] The paper notes that RDSS operates independently of downstream model training and selects samples only once, contrasting with active learning approaches that involve iterative cycles of labeling and training.
- Why unresolved: While RDSS is designed for single-pass selection, the paper does not explore whether its α-MMD criterion could be adapted for iterative selection scenarios where model uncertainty evolves over time.
- What evidence would resolve it: A modified version of RDSS that incorporates model feedback into the α-MMD calculation, with experimental comparison showing whether this iterative approach improves upon the single-pass RDSS method.

### Open Question 3
- Question: What is the theoretical relationship between the trade-off parameter α and the specific characteristics of the dataset (e.g., class balance, intra-class variance)?
- Basis in paper: [explicit] The paper establishes that α should be set in the interval [1 - 1/√m, 1] based on generalization bounds, but this is a general guideline independent of dataset-specific properties.
- Why unresolved: The paper provides a theoretical interval for α but does not explore how dataset characteristics might inform optimal α values within this interval for different scenarios.
- What evidence would resolve it: Empirical studies showing how different values of α within the recommended interval affect performance across datasets with varying characteristics (balanced vs. imbalanced, low vs. high intra-class variance), potentially leading to dataset-specific recommendations for α selection.

## Limitations

- Theoretical guarantees rely heavily on RKHS assumptions (bounded positive definite kernels, conditional distribution embeddings) that may not hold in practice
- Experimental validation focuses primarily on image classification tasks using CLIP features, limiting generalizability to other data modalities
- GKHR algorithm's performance bound of O(log m/m) assumes relatively small budgets compared to dataset size, but the transition point is not characterized
- Choice of Gaussian kernel with median distance bandwidth is heuristic and may not be optimal across all datasets

## Confidence

**High Confidence:** The core contribution of RDSS as a model-free sample selection method that improves SSL performance under low budgets is well-supported by both theoretical analysis and extensive experiments across multiple datasets and SSL frameworks.

**Medium Confidence:** The theoretical bounds for α-MMD generalization and GKHR optimization error are mathematically sound under stated assumptions, but practical relevance depends on how often these assumptions hold in real-world scenarios.

**Low Confidence:** The optimal interval [1 - 1/√m, 1) for α parameter selection is derived theoretically but lacks empirical validation showing sensitivity to deviations from this range across different dataset characteristics.

## Next Checks

1. **Kernel Sensitivity Analysis:** Systematically vary the Gaussian kernel bandwidth and evaluate RDSS performance to determine the robustness of sample selection quality to kernel parameter choices across different datasets.

2. **Cross-Modal Generalization:** Apply RDSS to non-image datasets (text, tabular data) using appropriate feature extractors to validate whether the α-MMD framework generalizes beyond CLIP-based image features.

3. **Budget Scaling Behavior:** Test RDSS performance across a wider range of annotation budgets (including high-budget scenarios) to characterize the method's effectiveness boundary and identify when alternative approaches might be preferable.