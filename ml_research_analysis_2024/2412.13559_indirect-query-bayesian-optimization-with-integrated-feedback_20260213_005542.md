---
ver: rpa2
title: Indirect Query Bayesian Optimization with Integrated Feedback
arxiv_id: '2412.13559'
source_url: https://arxiv.org/abs/2412.13559
tags:
- where
- function
- regret
- conditional
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Indirect Query Bayesian Optimization (IQBO),
  a novel problem setting where optimization feedback is provided through conditional
  expectations of an unknown function rather than direct function values. This addresses
  scenarios with privacy, hardware, or computational constraints that prevent direct
  access to the target function.
---

# Indirect Query Bayesian Optimization with Integrated Feedback

## Quick Facts
- arXiv ID: 2412.13559
- Source URL: https://arxiv.org/abs/2412.13559
- Reference count: 35
- Primary result: Introduces CMES acquisition function that outperforms MES, UCB, and EI baselines in indirect query BO settings

## Executive Summary
This paper introduces Indirect Query Bayesian Optimization (IQBO), where optimization feedback is provided through conditional expectations rather than direct function values. This addresses privacy, hardware, or computational constraints that prevent direct access to the target function. The authors propose the Conditional Max-Value Entropy Search (CMES) acquisition function that maximizes mutual information with respect to the optimal function value while accounting for the mismatch between query and recommendation spaces. They also develop a hierarchical search algorithm (CMETS) for multi-resolution settings that adaptively partitions the search space to improve computational efficiency. Theoretical contributions include regret bounds for both standard and multi-resolution IQBO settings.

## Method Summary
The method models the target function f and integrated feedback g as Gaussian Processes, using the Conditional Mean Process framework to relate them through the conditional distribution p(x|a). CMES selects queries by maximizing mutual information between observations and the optimal function value, approximated using Thompson samples from the posterior. For multi-resolution settings, CMETS maintains an active candidate set of leaf nodes and their children, selecting actions that maximize information gain per unit cost while adaptively partitioning the space via a tree structure. The algorithm balances exploration (high posterior variance) and exploitation (high posterior mean) through the mutual information criterion.

## Key Results
- CMES outperforms MES, UCB, and EI baselines on synthetic optimization functions
- Demonstrated effectiveness on both simple and instant regret metrics
- Hierarchical tree search improves computational efficiency in multi-resolution settings
- Theoretical regret bounds derived using equivalence to established policies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CMES enables querying in a transformed space while optimizing the target function by maximizing mutual information between observations and the optimal function value
- Mechanism: The policy selects queries that maximize I(z; f*|a, Dt-1), balancing exploration (high posterior variance) and exploitation (high posterior mean) through a monotonic function of the standardized improvement margin
- Core assumption: The conditional distribution p(x|a) is either known or can be learned from data, and the target function f is drawn from a Gaussian Process
- Evidence anchors: Abstract mentions CMES addresses the indirect query setting; section develops one-step look-ahead information gain approach

### Mechanism 2
- Claim: Hierarchical tree search improves computational efficiency by adaptively partitioning the search space
- Mechanism: Maintains active candidate set of leaf nodes and children, selecting actions maximizing information gain per unit cost as budget is spent
- Core assumption: Known cost function λ(δ) associates higher resolution queries with greater expense but more precise feedback
- Evidence anchors: Section presents hierarchical search algorithm with multi-resolution feedback; mentions weighting CMES by inverse cost

### Mechanism 3
- Claim: Equivalence between CMES and established policies allows leveraging existing theoretical guarantees
- Mechanism: Shows CMES equivalent to MES, EST, GPUCB applied to conditional mean process g, enabling use of established concentration inequalities
- Core assumption: Equivalence holds with specific approximation in Eq. (9) that is monotonic in standardized improvement margin
- Evidence anchors: Lemma 5.1 shows CMES equivalence to MES w.r.t function g using f*t as optimal value sample

## Foundational Learning

- Concept: Gaussian Process regression and conditional mean processes
  - Why needed here: Target function f and conditional mean process g modeled as GPs, requiring understanding of GP inference, kernel functions, and posterior updates
  - Quick check question: How does the conditional mean process g relate to the target function f through the conditional distribution p(x|a)?

- Concept: Information theory and mutual information
  - Why needed here: CMES selects queries by maximizing mutual information between observations and optimal function value
  - Quick check question: Why does maximizing I(z; f*|a, Dt-1) balance exploration and exploitation in Bayesian optimization?

- Concept: Multi-resolution optimization and cost-aware exploration
  - Why needed here: Hierarchical search algorithm must balance query resolution and associated costs
  - Quick check question: How does information gain per unit cost criterion ensure efficient budget use in multi-resolution settings?

## Architecture Onboarding

- Component map: Gaussian Process models for f and g -> CMES acquisition function -> Hierarchical tree search (CMETS) -> Regret bound analysis

- Critical path:
  1. Initialize GP priors for f and g
  2. For each iteration: update posteriors, select query using CMES/hierarchical search, observe integrated feedback, update dataset
  3. Return recommendation based on posterior mean of f

- Design tradeoffs:
  - Computational cost of CMES (Monte Carlo approximation) vs. accuracy
  - Tree depth vs. search space coverage in hierarchical search
  - Number of Thompson samples for f* approximation vs. convergence

- Failure signatures:
  - Poor simple regret indicates issues with recommendation strategy
  - High computational cost suggests inefficient CMES implementation
  - Suboptimal exploration indicates incorrect cost function or tree structure

- First 3 experiments:
  1. Test CMES on simple 1D optimization problem with known conditional distribution
  2. Compare CMES against MES, UCB, EI on Branin function with linear transformations
  3. Evaluate hierarchical search on multi-resolution Branin function with varying cost structures

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Assumes conditional distribution p(x|a) is either known or can be accurately learned from offline data
- Computational complexity of CMES grows with number of Thompson samples for f* approximation
- Lack of experiments on real-world problems or comparisons with more recent Bayesian optimization methods

## Confidence
- Theoretical claims: High confidence (derived from established concentration inequalities and equivalence framework)
- CMES performance claims: Medium confidence (based on synthetic experiments)
- Hierarchical search algorithm claims: Low confidence (implementation details sparse, no effectiveness comparison)

## Next Checks
1. **Robustness to conditional distribution estimation**: Evaluate CMES performance when p(x|a) is learned from limited offline data with varying quality
2. **Computational scaling analysis**: Measure runtime and memory requirements of CMES as dimension and iterations increase
3. **Real-world application demonstration**: Apply IQBO framework to optimization problem with privacy constraints (e.g., medical treatment optimization)