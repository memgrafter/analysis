---
ver: rpa2
title: 'Intrinsic Rewards for Exploration without Harm from Observational Noise: A
  Simulation Study Based on the Free Energy Principle'
arxiv_id: '2405.07473'
source_url: https://arxiv.org/abs/2405.07473
tags:
- curiosity
- agents
- state
- agent
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes hidden state curiosity, a new intrinsic reward
  for efficient exploration in Reinforcement Learning, derived from the Free Energy
  Principle. The method uses a Variational Recurrent Neural Network (VRNN) to encode
  observations into latent variables and rewards agents for the KL divergence between
  predictive prior and posterior distributions of these latent states.
---

# Intrinsic Rewards for Exploration without Harm from Observational Noise: A Simulation Study Based on the Free Energy Principle

## Quick Facts
- arXiv ID: 2405.07473
- Source URL: https://arxiv.org/abs/2405.07473
- Reference count: 19
- Primary result: Hidden state curiosity derived from the Free Energy Principle enables efficient exploration while being resilient to observational noise, outperforming prediction error curiosity in biased T-maze environments.

## Executive Summary
This paper introduces hidden state curiosity, an intrinsic reward method based on the Free Energy Principle that encourages exploration by rewarding information gain in latent state representations. The method uses a Variational Recurrent Neural Network (VRNN) to encode observations and computes intrinsic rewards as the KL divergence between predictive prior and posterior distributions over latent states. Tested in biased and expanding T-maze environments with six agent types, the approach demonstrates efficient exploration when combined with entropy rewards and remarkable resilience to curiosity traps caused by observational noise, unlike traditional prediction error curiosity methods.

## Method Summary
The method implements actor-critic reinforcement learning with Soft-Actor-Critic (SAC) entropy regularization, enhanced with two types of intrinsic rewards: prediction error curiosity and hidden state curiosity. A VRNN forward model encodes observations into latent states, with hidden state curiosity computing KL divergence between predictive prior and posterior distributions of these latent variables. The system is tested using a duck agent in biased T-maze and expanding T-maze environments, with observations comprising speed and 8x8x4 RGB-distance images. Six agent variants are trained (baseline, entropy-only, prediction error, hidden state, and hybrids) using experience replay with 250-episode buffers, and performance is evaluated by exit selection rates in final episodes with statistical confidence intervals.

## Key Results
- Entropy and curiosity rewards significantly improve exploration efficiency compared to baseline agents
- Hidden state curiosity demonstrates resilience to curiosity traps caused by noisy observational inputs
- Prediction error curiosity agents are negatively impacted by observational noise, fixating on unpredictable elements
- Combining entropy and hidden state curiosity yields optimal performance in both biased and expanding T-maze environments

## Why This Works (Mechanism)

### Mechanism 1
Hidden state curiosity rewards the KL divergence between predictive prior and posterior distributions of latent states. The forward model encodes observations into latent variables; intrinsic reward is high when the posterior changes significantly from the prior, signaling information gain. Core assumption: Latent state representations compress environmental state while ignoring noise. Evidence: "rewards agents by the KL divergence between the predictive prior and posterior probabilities of latent variables." Break condition: If latent encoding fails to remove noise, prior and posterior will not align, and curiosity may still be distracted by noise.

### Mechanism 2
Combining entropy and curiosity leads to more efficient exploration than either alone. Entropy encourages action diversity (control-as-inference), while curiosity directs exploration toward novel states; together they balance global coverage and directed sampling. Core assumption: Both intrinsic rewards are complementary and additive in driving exploration. Evidence: "entropy and curiosity result in efficient exploration, especially both employed together." Break condition: If one reward dominates, exploration may become either too random or too focused, reducing efficiency.

### Mechanism 3
Hidden state curiosity is resilient to curiosity traps, unlike prediction error curiosity. Because hidden state curiosity measures changes in latent state distributions, it can ignore observation noise that would otherwise appear unpredictable in raw pixel space. Core assumption: The variational encoder effectively removes irrelevant noise from latent states. Evidence: "agents with hidden state curiosity demonstrate resilience against curiosity traps, which hinder agents with prediction error curiosity." Break condition: If the encoder fails to remove noise, prior and posterior will differ due to noise, triggering false curiosity.

## Foundational Learning

- Concept: Variational Autoencoder (VAE) and KL divergence as reconstruction loss
  - Why needed here: The VRNN uses VAE-style sampling to encode observations into latent variables, and KL divergence measures information gain.
  - Quick check question: What does minimizing the KL term in a VAE encourage the encoder to do?

- Concept: Actor-Critic reinforcement learning and entropy regularization
  - Why needed here: The method builds on SAC, using entropy to encourage diverse actions, and adds intrinsic rewards for curiosity.
  - Quick check question: How does entropy regularization in SAC affect the policy's action distribution?

- Concept: Recurrent neural networks for temporal dependencies
  - Why needed here: The VRNN maintains hidden states over time, enabling the agent to capture temporal structure in partially observable environments.
  - Quick check question: Why might a non-recurrent model fail in partially observable mazes?

## Architecture Onboarding

- Component map:
  Forward model (VRNN) -> encodes observations -> latent states
  Actor network -> selects actions based on latent states
  Critic networks (double) -> evaluate state-action pairs
  Memory buffer -> stores full episodes for RNN training
  Intrinsic reward calculators -> entropy, prediction error curiosity, hidden state curiosity

- Critical path:
  1. Encode observation -> latent state
  2. Compute intrinsic reward (entropy + curiosity)
  3. Update actor-critic with extrinsic + intrinsic rewards
  4. Train forward model to predict next observation and minimize KL

- Design tradeoffs:
  VRNN adds complexity but enables temporal reasoning; simpler models may fail in POMDP.
  Using latent space vs raw observation reduces noise sensitivity but may lose fine-grained details.
  Delayed actor training stabilizes learning but slows adaptation.

- Failure signatures:
  High prediction error curiosity with noisy walls -> agent fixation on noise
  Low entropy reward -> premature convergence to suboptimal policies
  Poor latent encoding -> curiosity still triggered by noise

- First 3 experiments:
  1. Train agent with only entropy reward; observe baseline exploration efficiency.
  2. Add hidden state curiosity; measure resilience to noisy walls vs prediction error curiosity.
  3. Combine entropy and hidden state curiosity; test in expanding T-maze for generalization.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unresolved based on the implementation and results presented.

## Limitations
- Results are based on simplified T-maze environments that may not generalize to complex, high-dimensional tasks
- Comparison relies on synthetic noise injection rather than real-world observational noise
- The method's effectiveness in non-maze environments or real-world robotic applications is unproven

## Confidence
- High confidence: Hidden state curiosity's resilience to synthetic noise and its superiority over prediction error curiosity in biased T-mazes
- Medium confidence: The additive benefit of combining entropy and curiosity rewards for exploration efficiency
- Low confidence: Generalization of findings to non-maze environments or real-world robotic applications

## Next Checks
1. Test hidden state curiosity against naturally occurring observational noise in physical robot navigation tasks to verify noise resilience beyond synthetic noise injection.

2. Evaluate the method on more complex environments (e.g., MiniGrid or Atari games) to assess whether hidden state curiosity maintains its advantages as task complexity increases.

3. Conduct an ablation study systematically varying latent space dimensionality and encoder architecture to determine the robustness of hidden state curiosity to model capacity and design choices.