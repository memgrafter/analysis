---
ver: rpa2
title: A Theoretical Analysis of Soft-Label vs Hard-Label Training in Neural Networks
arxiv_id: '2412.09579'
source_url: https://arxiv.org/abs/2412.09579
tags:
- training
- neural
- lemma
- network
- soft-label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper theoretically compares soft-label and hard-label training
  in neural networks. It shows that for two-layer neural networks, soft-label training
  using projected gradient descent requires only $O\left(\frac{1}{\gamma^2 \epsilon}\right)$
  neurons to achieve a classification loss less than $\epsilon$, where $\gamma$ is
  the separation margin of the limiting kernel.
---

# A Theoretical Analysis of Soft-Label vs Hard-Label Training in Neural Networks

## Quick Facts
- arXiv ID: 2412.09579
- Source URL: https://arxiv.org/abs/2412.09579
- Reference count: 40
- Two-layer neural networks require O(1/γ²ε) neurons with soft-label training vs O(1/γ⁴ ln(1/ε)) with hard-label training

## Executive Summary
This paper provides the first theoretical comparison between soft-label and hard-label training in neural networks, showing that soft-label training requires significantly fewer neurons when datasets are challenging to classify (γ ≤ ε). The analysis focuses on two-layer neural networks trained with projected gradient descent, demonstrating that soft labels provide a smoother gradient signal that allows networks to focus on feature weighting rather than feature discovery. Experimental results on both two-layer and deep neural networks validate these theoretical findings, showing consistent performance advantages for soft-label training.

## Method Summary
The paper analyzes two-layer neural networks with symmetric random initialization trained using projected gradient descent. Soft-label training uses cross-entropy loss with teacher-provided soft labels, while hard-label training uses ground truth labels. The theoretical analysis leverages neural tangent kernel (NTK) properties and establishes bounds on neuron requirements for achieving target classification loss. Experiments validate findings using MNIST binary classification and CIFAR-10 datasets with VGG architectures.

## Key Results
- Soft-label training requires O(1/γ²ε) neurons to achieve classification loss < ε
- Hard-label training requires O(1/γ⁴ ln(1/ε)) neurons for the same loss
- When γ ≤ ε (challenging datasets), soft-label training dramatically reduces neuron requirements
- Experimental results on MNIST and CIFAR-10 confirm theoretical predictions
- VGG experiments show soft-label training outperforms hard-label training, especially with noisy data

## Why This Works (Mechanism)

### Mechanism 1
Soft-label training requires fewer neurons than hard-label training when the dataset is difficult to classify (γ ≤ ε). Soft labels provide a smoother gradient signal that allows the network to focus on feature weighting rather than feature discovery, preserving the initialization's separability properties. The core assumption is that the initialization provides a feature map that already separates the data with margin γ, and soft-label training maintains this feature space while adjusting weights. If the initialization does not provide good feature separability, or if the soft labels are poorly calibrated, the advantage disappears.

### Mechanism 2
The neural tangent kernel (NTK) feature space created by initialization is sufficient for classification, and soft-label training preserves this space while adjusting weights. With symmetric random initialization, the NTK features separate the data with margin O(γ). Soft-label training keeps weights close to initialization (small B), maintaining feature separability while adjusting magnitudes to match soft targets. If the number of neurons is too small to achieve the required separation margin, or if the step size η is too large causing weights to deviate significantly from initialization, the advantage disappears.

### Mechanism 3
Hard-label training requires a larger deviation from initialization to achieve perfect classification, necessitating more neurons to maintain feature space separability. Hard-label training forces the network output to match exactly 0 or 1 after softmax, requiring weights to deviate much further from initialization than soft-label training. This larger deviation necessitates more neurons to preserve the separability of the feature space. If the hard labels are already close to soft labels (e.g., very clean data), the advantage of soft-label training diminishes.

## Foundational Learning

- **Concept: Neural Tangent Kernel (NTK) and its role in two-layer network analysis**
  - Why needed here: The paper relies on NTK analysis to understand how initialization creates a feature space that separates the data, which is crucial for both training methods
  - Quick check question: What property of the NTK feature space allows two-layer networks to be analyzed using linear methods during training?

- **Concept: Projected Gradient Descent (PGD) and its convergence properties**
  - Why needed here: The paper uses PGD with projection to analyze both soft-label and hard-label training, and the projection radius B plays a crucial role in the analysis
  - Quick check question: How does the projection step in PGD affect the convergence rate compared to standard gradient descent?

- **Concept: Cross-entropy loss and its relationship to KL divergence**
  - Why needed here: The soft-label training objective uses cross-entropy loss between soft labels and network outputs, which is analyzed through KL divergence bounds
  - Quick check question: What is the relationship between classification error and cross-entropy loss when using soft labels?

## Architecture Onboarding

- **Component map**: Input layer → Hidden layer (m neurons with ReLU) → Output layer (single neuron with softmax)
- **Critical path**: Initialization → Feature separability check (Lemma 6) → Training with PGD → Convergence analysis (Theorem 2) → Neuron requirement comparison
- **Design tradeoffs**: 
  - Smaller m: Fewer neurons but may not achieve separation margin γ
  - Larger B: Allows more weight deviation but requires more neurons to maintain separability
  - Step size η: Must be small enough for convergence but large enough for efficient training
- **Failure signatures**:
  - If m is too small: Training loss doesn't converge, accuracy plateaus
  - If B is too large: Requires more neurons than necessary, may overfit
  - If η is too large: Training becomes unstable, weights diverge from initialization
- **First 3 experiments**:
  1. Reproduce Table 1 from the paper: Compare soft-label vs hard-label training on MNIST binary classification with varying numbers of neurons
  2. Test the effect of initialization: Compare symmetric random initialization vs standard initialization on neuron efficiency
  3. Vary the separation margin γ: Create datasets with different difficulty levels and measure the gap in neuron requirements between soft and hard-label training

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of the teacher model's architecture (e.g., width, depth) affect the performance gap between soft-label and hard-label training? The paper analyzes a two-layer student network trained on soft labels from a teacher with infinite width but doesn't explore how different teacher architectures impact the results. Experiments comparing soft-label and hard-label training with student networks of varying widths, trained on soft labels from teachers with different architectures (widths, depths) would resolve this.

### Open Question 2
Can the theoretical analysis be extended to multi-class classification problems? The paper focuses on binary classification using the KL divergence loss, and the analysis of soft labels and the neuron requirement might not directly apply to multi-class settings. A theoretical extension of the analysis to multi-class classification, deriving neuron requirements for soft-label training with appropriate loss functions (e.g., cross-entropy), would resolve this.

### Open Question 3
How does the performance of soft-label training scale with the size of the dataset (n)? The paper's theoretical analysis and experiments focus on fixed dataset sizes but don't explore how the neuron requirement or performance gap between soft-label and hard-label training changes with n. Theoretical analysis deriving the scaling of neuron requirements with n for both soft-label and hard-label training would resolve this.

### Open Question 4
What is the impact of different initialization schemes on the performance of soft-label vs. hard-label training? The paper uses symmetric random initialization for the student network but doesn't explore how other initialization methods (e.g., He, Glorot) affect the results. Experiments comparing soft-label and hard-label training with different initialization schemes would resolve this.

## Limitations
- Theoretical analysis is constrained to two-layer networks with specific symmetric random initialization
- Extension to deeper networks relies more on empirical validation than theoretical guarantees
- Assumes idealized conditions that may not capture practical implementation details like optimization dynamics and regularization effects
- Symmetric random initialization and NTK-based feature separability assumptions may not hold for more complex architectures or data distributions

## Confidence
- **High Confidence**: The theoretical framework for analyzing two-layer networks using NTK and the mathematical derivation of neuron requirements for both training methods
- **Medium Confidence**: The empirical validation on deeper networks and the generalization of findings beyond the two-layer case
- **Low Confidence**: The practical significance of the theoretical advantage in real-world applications where data distributions, optimization algorithms, and architectural choices may deviate from theoretical assumptions

## Next Checks
1. **Implementation Validation**: Reproduce the symmetric random initialization and PGD implementation for two-layer networks on MNIST binary classification, verifying that the initialization creates the expected feature separability and that both training methods converge as theoretically predicted.

2. **Scaling Verification**: Systematically vary the number of neurons and measure classification loss for both soft and hard-label training, plotting the empirical scaling relationship against the theoretical predictions of O(1/γ²ε) and O(1/γ⁴ ln(1/ε)).

3. **Robustness Testing**: Test the findings across different datasets, initialization schemes, and network architectures to assess the generalizability of the theoretical advantage, particularly examining cases where the NTK-based assumptions may break down.