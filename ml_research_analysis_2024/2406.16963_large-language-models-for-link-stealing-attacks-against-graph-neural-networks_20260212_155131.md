---
ver: rpa2
title: Large Language Models for Link Stealing Attacks Against Graph Neural Networks
arxiv_id: '2406.16963'
source_url: https://arxiv.org/abs/2406.16963
tags:
- link
- stealing
- attacks
- attack
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces large language models (LLMs) to enhance link
  stealing attacks against graph neural networks (GNNs). Previous link stealing attacks
  relied primarily on posterior probabilities from the target GNN model, ignoring
  node features.
---

# Large Language Models for Link Stealing Attacks Against Graph Neural Networks

## Quick Facts
- arXiv ID: 2406.16963
- Source URL: https://arxiv.org/abs/2406.16963
- Reference count: 40
- Primary result: LLM-enhanced link stealing attacks achieve over 90% accuracy and F1 score across multiple datasets

## Executive Summary
This paper introduces a novel approach to link stealing attacks against graph neural networks (GNNs) using large language models (LLMs). Traditional link stealing attacks relied solely on posterior probabilities from target GNN models, limiting their effectiveness across different datasets. The authors address this limitation by designing LLM prompts that combine textual node features with posterior probabilities, enabling a single LLM model to perform attacks across multiple datasets with varying feature dimensions. Experimental results demonstrate significant performance improvements in both white-box and black-box attack scenarios, with accuracy and F1 scores exceeding 90% across tested datasets.

## Method Summary
The method employs LLMs fine-tuned with carefully designed prompts that integrate textual features (titles and abstracts) and numerical posterior probability vectors from target GNN models. Two distinct prompt templates are created: one for white-box attacks asking "do they have a link?" and another for black-box attacks asking "do they belong to the same category?" The LLM is fine-tuned on multiple datasets to learn generalizable features, enabling it to handle varying posterior probability dimensions across different domains. This approach overcomes the limitations of previous methods that required separate attack models for each dataset due to differing feature dimensions.

## Key Results
- LLM-based attacks achieve over 90% accuracy and F1 score across multiple datasets
- Performance improves with dataset size, reaching 97.5% accuracy on larger datasets
- Single LLM model successfully performs attacks across datasets with different posterior probability dimensions
- Outperforms baseline methods that use only posterior probabilities without textual features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can integrate textual node features with posterior probabilities to improve link stealing attack performance.
- Mechanism: The LLM processes a structured prompt containing both textual description and numerical posterior probability vector from the target GNN, capturing richer relational cues.
- Core assumption: Textual similarity between nodes correlates with link existence, and LLM can extract this signal when combined with classification probabilities.
- Evidence anchors: [abstract] "LLMs can effectively integrate textual features and exhibit strong generalizability, enabling attacks to handle diverse data dimensions across various datasets."

### Mechanism 2
- Claim: A single LLM model can perform link stealing attacks across multiple datasets with varying posterior probability dimensions.
- Mechanism: LLMs naturally handle variable-length textual inputs; fine-tuning on prompts with different-length posterior probabilities enables handling of varying dimensions.
- Core assumption: LLM's ability to process variable-length sequences generalizes to handling posterior probability vectors of different lengths without architectural changes.
- Evidence anchors: [abstract] "LLMs can effectively integrate textual features and exhibit strong generalizability, enabling attacks to handle diverse data dimensions across various datasets."

### Mechanism 3
- Claim: Fine-tuning with prompt engineering enables LLM to adapt to link stealing task in both white-box and black-box settings.
- Mechanism: Human question component of prompt is tailored to attacker's knowledge; white-box asks about links directly, black-box uses same-class inference as proxy.
- Core assumption: Nodes belonging to same class are more likely to have a link, making same-class inference a valid shadow task for link stealing in black-box settings.
- Evidence anchors: [section IV-D] "In black-box attacks, the attacker lacks knowledge of links between nodes. To assist the LLM in performing link stealing tasks on GNNs, we fine-tune it using shadow tasks."

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their posterior probability outputs
  - Why needed here: Attack relies on accessing GNN's node posterior probabilities as input to LLM
  - Quick check question: What is the dimension of the posterior probability vector for a node in Cora, and how does it relate to the number of classes?

- Concept: Large Language Models and prompt engineering
  - Why needed here: LLM requires carefully designed prompts and fine-tuning to perform link stealing task
  - Quick check question: How does the structure of the prompt (node pair info + human question + LLM response) guide LLM's output for link inference?

- Concept: Privacy attacks in machine learning
  - Why needed here: Understanding threat model (white-box vs black-box) is essential for designing appropriate attack strategy
  - Quick check question: What knowledge does attacker have in white-box vs black-box settings, and how does this affect attack design?

## Architecture Onboarding

- Component map: Graph dataset (node features: textual + numerical, edges) -> Target GNN model (produces posterior probabilities) -> Attacker (creates node pairs, queries GNN, constructs prompts) -> LLM (fine-tuned with prompts, generates "Yes"/"No" for link existence) -> Evaluation (accuracy, F1 score on held-out node pairs)

- Critical path: 1. Sample node pairs from graph 2. Query target GNN to get posterior probabilities 3. Construct prompt with textual features and posterior probabilities 4. Fine-tune LLM on labeled node pairs (white-box) or same-class labels (black-box) 5. Use fine-tuned LLM to predict link existence on new node pairs

- Design tradeoffs: Using textual features adds complexity but improves performance vs using only posterior probabilities; training on multiple datasets improves generalization but requires more data and compute; fine-tuning vs in-context learning: fine-tuning requires more resources but achieves better performance

- Failure signatures: Low accuracy/F1 despite high-quality prompts: LLM may not be learning link signal; high variance across datasets: Model may not be generalizing well to different posterior probability dimensions; poor performance in black-box: Shadow task (same-class inference) may not be good proxy for link existence

- First 3 experiments: 1. Compare LLM-based attack vs baseline (only posterior probabilities) on single dataset in white-box setting 2. Test whether same LLM model can perform attacks on different dataset with different posterior probability dimensions 3. Compare white-box (direct link labels) vs black-box (same-class inference) performance on same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs be effectively utilized for other types of privacy attacks on GNNs beyond link stealing, such as model extraction, membership inference, or attribute inference attacks?
- Basis in paper: [explicit] The paper concludes by stating that link stealing attacks are just one type of privacy attack on GNNs and proposes exploring how to utilize LLMs for other privacy attacks on GNNs in the future.
- Why unresolved: The paper focuses solely on link stealing attacks and does not explore the potential of LLMs for other types of privacy attacks on GNNs.
- What evidence would resolve it: Experiments demonstrating the effectiveness of LLMs in enhancing other types of privacy attacks on GNNs, such as model extraction, membership inference, or attribute inference attacks.

### Open Question 2
- Question: How does the performance of LLM-based link stealing attacks vary with different GNN architectures beyond GCN, SAGE, and GAT?
- Basis in paper: [explicit] The paper experiments with GCN, SAGE, and GAT as target GNN models but does not explore performance on other GNN architectures.
- Why unresolved: The paper only tests a limited number of GNN architectures, leaving the generalizability of the LLM-based approach to other architectures uncertain.
- What evidence would resolve it: Experiments testing the LLM-based link stealing attack method on a wider range of GNN architectures, such as GraphSAGE with different pooling methods, GIN, or Gated GNNs.

### Open Question 3
- Question: Can the proposed LLM-based link stealing attack method be adapted to handle graph data with different types of features, such as continuous numerical features, categorical features, or multi-modal features?
- Basis in paper: [inferred] The paper focuses on graph data with textual features and posterior probabilities, without exploring applicability to graphs with different feature types.
- Why unresolved: The paper's experiments are limited to graph data with textual features, leaving the adaptability of the method to other feature types unexplored.
- What evidence would resolve it: Experiments adapting the LLM-based link stealing attack method to handle graph data with different feature types, such as continuous numerical features, categorical features, or multi-modal features.

## Limitations

- Prompt templates and exact text formatting used for LLM fine-tuning are not fully specified, making exact reproduction difficult
- The black-box attack mechanism relies on the assumption that nodes from the same class are likely to be linked, which is asserted but not empirically validated across all datasets
- Lack of ablation studies showing how much each component (textual features vs. posterior probabilities) contributes to the performance gain

## Confidence

**High Confidence:** The experimental results showing improved accuracy and F1 scores compared to baseline methods that use only posterior probabilities.

**Medium Confidence:** The claim that a single LLM model can handle attacks across multiple datasets with varying posterior probability dimensions.

**Low Confidence:** The assertion that the same-class inference task is an effective shadow task for black-box link stealing.

## Next Checks

1. **Prompt Design Validation:** Test the sensitivity of attack performance to different prompt structures and formats to determine if the proposed template is optimal or if simpler prompts could achieve similar results.

2. **Black-Box Assumption Testing:** Conduct experiments to measure the actual correlation between same-class membership and link existence across all tested datasets, and test attack performance when this correlation is weak.

3. **Generalization Stress Test:** Evaluate the multi-dataset trained LLM on a dataset with significantly different characteristics (e.g., different number of classes, different text feature characteristics) to test the limits of its generalizability claim.