---
ver: rpa2
title: Deep Dependency Networks and Advanced Inference Schemes for Multi-Label Classification
arxiv_id: '2404.11667'
source_url: https://arxiv.org/abs/2404.11667
tags:
- networks
- inference
- dependency
- deep
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-label classification
  (MLC) in image and video data, where labels are often correlated and predicting
  them independently can lead to significant errors. The authors propose a unified
  framework called deep dependency networks (DDNs) that combines dependency networks
  and deep learning architectures.
---

# Deep Dependency Networks and Advanced Inference Schemes for Multi-Label Classification

## Quick Facts
- arXiv ID: 2404.11667
- Source URL: https://arxiv.org/abs/2404.11667
- Authors: Shivvrat Arya; Yu Xiang; Vibhav Gogate
- Reference count: 40
- Key outcome: DDNs with ILP-based inference outperform neural networks and MRF+NN hybrids on multi-label classification, achieving 18-30% improvements in subset accuracy on three datasets.

## Executive Summary
This paper addresses the challenge of multi-label classification (MLC) in image and video data by proposing Deep Dependency Networks (DDNs), a unified framework that combines dependency networks and deep learning architectures. DDNs provide an intuitive loss function for MLC and learn label dependencies through conditional distributions. The authors propose novel inference schemes based on local search and integer linear programming to compute the most likely label assignments given observations, overcoming the primary drawback of DDNs that previously required Gibbs sampling.

## Method Summary
The paper proposes Deep Dependency Networks (DDNs) for multi-label classification, combining neural networks for feature extraction with dependency networks that learn conditional distributions between labels. The method uses Conditional Pseudo-Log-Likelihood (CPLL) loss for joint training of both components. For inference, the authors propose four methods: Gibbs Sampling, Random Walk, Greedy Local Search, and Integer Linear Programming (ILP) based on piecewise linear approximation of the sigmoid-log likelihood objective. The method is evaluated on three video datasets (Charades, TACoS, Wetlab) and three image datasets (MS-COCO, PASCAL VOC, NUS-WIDE) using metrics including Subset Accuracy, Jaccard Index, Hamming Loss, and F1 scores.

## Key Results
- DDNs with ILP-based inference achieve 18%, 23%, and 30% improvements in subset accuracy on PASCAL-VOC, TACoS, and Wetlab datasets respectively
- DDNs outperform both basic neural architectures and neural architectures combined with Markov networks
- The proposed inference schemes provide superior performance compared to traditional Gibbs sampling approaches

## Why This Works (Mechanism)

### Mechanism 1
DDNs capture dense label interdependencies that sparse MRFs cannot by using conditional dependency networks to learn a local conditional distribution for each label that conditions on all other labels, allowing dense pairwise interactions without structural sparsity constraints. Core assumption: Dense label dependencies improve multi-label classification accuracy over sparse graphical model representations. Evidence anchors: [abstract] "DDNs, when equipped with our novel MILP-based MPE inference approach, often outperform both MRF+NN hybrids and NNs." [section 1] "DDNs are a neuro-symbolic model where the neural network extracts features from data and the dependency network acts as a symbolic counterpart, learning the weighted constraints between the labels."

### Mechanism 2
Advanced MPE inference (ILP) in DDNs yields superior performance over sampling-based inference by converting the MPE problem into an integer linear program through piecewise linear approximation of the sigmoid-log likelihood objective, enabling exact optimization with commercial solvers like Gurobi. Core assumption: Accurate MPE inference leads to better multi-label classification than approximate Gibbs sampling. Evidence anchors: [abstract] "DDNs with the integer linear programming-based inference method achieved superior performance, with improvements of 18%, 23%, and 30% in subset accuracy..." [section 4.3] Details conversion of MPE inference to ILP via piecewise linear approximation of non-linear terms.

### Mechanism 3
Joint training of neural feature extractors and dependency networks improves feature-label alignment by backpropagating through both networks using Conditional Pseudo Log-likelihood loss (CPLL), ensuring features capture label relationships. Core assumption: End-to-end training with CPLL loss better aligns features with label dependencies than pre-trained feature extractors. Evidence anchors: [section 3.2] "We employ the conditional pseudo log-likelihood loss (CPLL) [...] in order to jointly train the two components of DDN." [section 5.2] Describes fine-tuning neural networks for TacOs and Wetlab datasets alongside DDN training.

## Foundational Learning

- Concept: Multi-label classification vs multi-class classification
  - Why needed here: DDNs are specifically designed for MLC where each example can have multiple labels simultaneously, unlike multi-class where only one label applies.
  - Quick check question: What is the key difference in output representation between MLC and multi-class classification?

- Concept: Probabilistic graphical models (PGMs)
  - Why needed here: DDNs and MRFs are both PGMs, but with different inference and learning characteristics that affect performance.
  - Quick check question: How do dependency networks differ from Markov random fields in terms of consistency and inference?

- Concept: Integer linear programming (ILP) and piecewise linear approximation
  - Why needed here: The ILP-based inference method relies on converting non-linear optimization problems into linear ones using piecewise approximations.
  - Quick check question: Why is piecewise linear approximation necessary for converting the DDN MPE problem to ILP?

## Architecture Onboarding

- Component map: Input -> Neural network -> Feature vector e -> Dependency network -> Conditional probabilities -> Loss computation (CPLL) -> Backpropagation through both networks

- Critical path: 1. Input → Neural network → Feature vector e 2. e + label assignments x → Dependency network → Conditional probabilities 3. Loss computation (CPLL) → Backpropagation through both networks 4. Inference (during prediction): e + DDN → MPE assignment via inference method

- Design tradeoffs:
  - Dense vs sparse label modeling: DDNs allow dense interactions but increase parameter count
  - Inference accuracy vs speed: ILP provides better accuracy but is slower than sampling
  - Joint vs separate training: Joint training aligns features with labels but may be unstable

- Failure signatures:
  - Poor performance: Check if dependency network learned meaningful label relationships
  - Slow inference: ILP solver may timeout; consider sampling methods or constraint reduction
  - Training instability: CPLL loss may be difficult to optimize; try regularization or learning rate tuning

- First 3 experiments:
  1. Compare DDN-GS vs baseline NN on a small dataset to verify joint training benefit
  2. Test DDN-ILP vs DDN-GS on same dataset to measure inference method impact
  3. Evaluate model scaling by increasing label count and measuring performance degradation

## Open Questions the Paper Calls Out

- How does the performance of DDNs compare to other neuro-symbolic approaches in multi-label classification? The paper compares DDNs to basic neural networks and neural architectures combined with Markov networks, but does not compare them to other neuro-symbolic approaches. A direct comparison of DDNs with other neuro-symbolic approaches, such as neural-symbolic learning systems or neural-guided symbolic reasoning, on the same multi-label classification tasks would resolve this.

- Can the proposed inference schemes be extended to handle more complex dependencies beyond pairwise relationships? The paper mentions that MRFs rely on sparsity for efficient inference and learning, suggesting that handling complex dependencies might be challenging for DDNs as well. Experiments evaluating the performance of DDNs with the proposed inference schemes on datasets with complex label dependencies, or theoretical analysis of the limitations of the inference schemes in handling higher-order interactions, would resolve this.

- How does the performance of DDNs with the proposed inference schemes scale with the number of labels? The paper mentions that Subset Accuracy (SA) is particularly challenging in datasets with a large number of labels, but does not provide a detailed analysis of how the performance of DDNs scales with the number of labels. Experiments evaluating the performance of DDNs with the proposed inference schemes on datasets with varying numbers of labels, and analysis of the computational complexity of the inference schemes as the number of labels increases, would resolve this.

## Limitations
- Limited ablation studies comparing dense vs sparse label modeling explicitly
- No comparison of ILP solver times or scalability analysis
- Missing analysis of dependency network weight interpretability

## Confidence
- **High confidence**: DDN framework combining neural networks and dependency networks
- **Medium confidence**: ILP-based inference superiority claims (based on subset accuracy metrics)
- **Low confidence**: Generalizability of results to other MLC domains beyond images/videos

## Next Checks
1. **Ablation study**: Train MRF-based models with varying sparsity levels to quantify dense vs sparse dependency benefits
2. **Scalability test**: Measure inference time and accuracy degradation as label count increases beyond tested datasets
3. **Approximation analysis**: Systematically vary piecewise linear approximation granularity and measure impact on MPE inference quality