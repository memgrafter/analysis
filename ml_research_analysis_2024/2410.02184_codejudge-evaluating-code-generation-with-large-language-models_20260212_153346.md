---
ver: rpa2
title: 'CodeJudge: Evaluating Code Generation with Large Language Models'
arxiv_id: '2410.02184'
source_url: https://arxiv.org/abs/2410.02184
tags:
- code
- oref
- evaluation
- judge
- ice-score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CodeJudge, a framework that leverages LLMs
  to evaluate code generation without test cases. It proposes two methods to guide
  LLMs in performing careful analysis: one for binary correctness assessment and another
  for measuring semantic alignment with intended code.'
---

# CodeJudge: Evaluating Code Generation with Large Language Models

## Quick Facts
- arXiv ID: 2410.02184
- Source URL: https://arxiv.org/abs/2410.02184
- Authors: Weixi Tong; Tianyi Zhang
- Reference count: 36
- Key outcome: CodeJudge significantly outperforms existing methods on code generation evaluation, achieving up to 0.612 Kendall's τ and 0.707 Spearman's ρ correlation with ground truth, and 80.56% accuracy on average.

## Executive Summary
This paper introduces CodeJudge, a framework that leverages large language models to evaluate code generation without requiring test cases or reference code. CodeJudge proposes two novel methods to guide LLMs in performing careful semantic analysis: one for binary correctness assessment through task decomposition, and another for measuring semantic alignment using a taxonomy of code inconsistencies. The approach works across four datasets and five programming languages, demonstrating superior performance compared to existing methods.

The framework's key innovation lies in its decomposition approach that forces LLMs to perform "slow thinking" through explicit analysis and summarization steps, rather than making quick judgments. Additionally, CodeJudge introduces a severity-weighted taxonomy for fault localization that accounts for different types of code errors having varying impacts on semantic correctness. Remarkably, even a small Llama-3-8B model using CodeJudge outperforms larger models using prior approaches, demonstrating that effective prompt engineering can be more important than model scale.

## Method Summary
CodeJudge is a framework for evaluating code generation using large language models without requiring test cases or reference code. It employs two main approaches: an analyze-then-summarize method for binary correctness assessment, and a taxonomy-guided fault localization method for measuring semantic alignment. The framework works by decomposing evaluation tasks into subtasks that force LLMs to perform careful semantic analysis, using structured taxonomies to identify and weight different types of code inconsistencies by severity.

## Key Results
- Achieved strong statistical correlations up to 0.612 Kendall's τ and 0.707 Spearman's ρ with ground truth across four datasets
- Reached 80.56% accuracy on average for binary correctness assessment
- Llama-3-8B model with CodeJudge outperformed GPT-3.5-Turbo using prior approaches
- Demonstrated strong performance across five programming languages (Java, C++, Python, JavaScript, Go)

## Why This Works (Mechanism)

### Mechanism 1
CodeJudge improves LLM code evaluation accuracy by decomposing the binary correctness task into analysis and summarization subtasks, forcing the model to perform "slow thinking." The framework first asks the LLM to identify required functionalities from the task description, analyze the code logic, and report any unfulfilled requirements. Then it asks the LLM to summarize this analysis into a binary decision. This approach breaks if the LLM cannot reliably parse natural language task descriptions or if the analysis step produces fundamentally incorrect conclusions that cannot be corrected in the summarization phase.

### Mechanism 2
The taxonomy-guided fault localization method enables CodeJudge to measure semantic alignment by identifying specific types of inconsistencies and weighting them by severity. The framework provides a taxonomy of eight common code inconsistencies (missing dependencies, logic errors, undefined functions, etc.) categorized into four severity levels. The LLM identifies inconsistencies and the framework computes a weighted score based on severity. This mechanism breaks if the taxonomy does not cover relevant error types for the target programming languages or if the severity weighting scheme does not accurately reflect the actual impact on code functionality.

### Mechanism 3
CodeJudge achieves superior performance even with smaller models by leveraging better prompt engineering rather than model size. The framework's decomposition and taxonomy-guided approaches enable effective code evaluation without requiring large model parameters, as demonstrated by Llama-3-8B outperforming GPT-3.5-Turbo. This mechanism breaks if the smaller models lack sufficient reasoning capabilities to perform the analysis tasks, regardless of prompt quality.

## Foundational Learning

- Concept: Semantic code analysis vs. syntactic matching
  - Why needed here: CodeJudge focuses on semantic correctness rather than syntactic similarity, which is crucial for understanding why it outperforms token-based methods.
  - Quick check question: What's the difference between code that is syntactically different but semantically equivalent versus code that is semantically wrong?

- Concept: Statistical correlation metrics (Kendall's τ and Spearman's ρ)
  - Why needed here: These metrics measure the alignment between CodeJudge's assessments and ground truth, which is the primary evaluation method used.
  - Quick check question: What does a Kendall's τ coefficient of 0.5 or higher indicate about the correlation strength?

- Concept: Prompt engineering for reasoning tasks
  - Why needed here: Understanding how decomposition and structured taxonomies improve LLM performance is key to CodeJudge's design.
  - Quick check question: Why might asking an LLM to "think step by step" produce different results than asking for a direct answer?

## Architecture Onboarding

- Component map: Prompt templates (analysis, summarization, taxonomy-guided) -> LLM evaluation -> Post-processing logic -> Score computation
- Critical path: For binary assessment - prompt generation → LLM analysis → LLM summarization → binary output. For alignment assessment - prompt generation → LLM inconsistency identification → severity-weighted scoring → alignment score.
- Design tradeoffs: Using structured taxonomies limits flexibility but improves reliability; decomposing tasks improves accuracy but increases latency; avoiding test cases increases generality but may miss runtime issues.
- Failure signatures: Incorrect binary decisions often stem from flawed analysis steps; low correlation scores indicate the taxonomy doesn't capture relevant error types; high variance across runs suggests temperature settings need adjustment.
- First 3 experiments:
  1. Test the analyze-then-summarize method on HumanEval-X with a simple LLM to verify the decomposition approach works
  2. Implement the taxonomy and test on a small subset to validate the severity weighting scheme
  3. Compare performance with and without reference code on CoNaLa to understand the impact of the taxonomy-guided approach

## Open Questions the Paper Calls Out

### Open Question 1
How does CodeJudge's performance scale with increasing model size, particularly for very large models (e.g., GPT-4, Claude)? The paper mentions that even a small Llama-3-8B model with CodeJudge outperforms larger models using prior approaches, but does not explore the upper bounds of model size. This remains unresolved as the experiments only test up to Llama-3-70B.

### Open Question 2
Can CodeJudge's taxonomy of inconsistencies be further refined to improve evaluation accuracy, particularly for complex code generation tasks? The paper presents a taxonomy of eight common inconsistency types but acknowledges that LLMs struggle with error-handling requirements in generated code. This remains unresolved as the current taxonomy appears to be manually constructed and may not capture all relevant error types.

### Open Question 3
How does CodeJudge's performance vary across different programming paradigms (e.g., functional vs. object-oriented programming)? The paper tests five programming languages but does not analyze performance differences across programming paradigms. This remains unresolved as different programming paradigms may require different evaluation approaches.

## Limitations
- Reliance on LLMs for semantic analysis introduces uncertainty about consistency across different model versions and capabilities
- The eight-category taxonomy may not be comprehensive across all programming languages and paradigms
- Absence of reference code means CodeJudge cannot validate whether intended functionality is correctly implemented
- Statistical correlation values, while significant, show only moderate alignment with human judgment (0.612 maximum Kendall's τ)

## Confidence
- High Confidence: The claim that CodeJudge significantly outperforms existing methods on statistical correlation metrics is supported by extensive experiments across four datasets and five programming languages
- Medium Confidence: The claim that CodeJudge works without reference code is well-supported by the methodology description, but practical implications need further validation
- Medium Confidence: The assertion that smaller models can outperform larger ones through better prompt engineering is demonstrated with specific examples but may be context-dependent

## Next Checks
1. **Cross-Model Consistency Test**: Evaluate the same code samples across multiple LLM versions (GPT-4, Claude, different Llama versions) using CodeJudge to measure consistency of results and identify potential model-dependent biases.

2. **Taxonomy Coverage Expansion**: Conduct a systematic analysis of code errors across the five programming languages to identify any inconsistencies or error types not captured by the current eight-category taxonomy, and measure the impact on evaluation accuracy.

3. **Runtime Validation Study**: Implement a subset of CodeJudge-evaluated code samples to verify whether high semantic alignment scores correspond to actual functional correctness when executed, bridging the gap between semantic analysis and practical performance.