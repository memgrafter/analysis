---
ver: rpa2
title: 'Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement
  Learning Benchmarks'
arxiv_id: '2408.10556'
source_url: https://arxiv.org/abs/2408.10556
tags:
- offline
- learning
- datasets
- hero
- norm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hokoff, a comprehensive dataset and framework
  for offline reinforcement learning (RL) and offline multi-agent RL (MARL) based
  on the popular MOBA game Honor of Kings. The framework includes sampling, training,
  and evaluation modules, along with diverse datasets that cover various difficulty
  levels, multi-task learning, and generalization challenges.
---

# Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks

## Quick Facts
- arXiv ID: 2408.10556
- Source URL: https://arxiv.org/abs/2408.10556
- Reference count: 40
- Key outcome: Introduces Hokoff dataset and framework for offline RL/MARL, demonstrating current methods' struggles with complexity, generalization, and multi-task learning

## Executive Summary
This paper presents Hokoff, a comprehensive dataset and framework for offline reinforcement learning and offline multi-agent reinforcement learning based on the popular MOBA game Honor of Kings. The framework includes sampling, training, and evaluation modules, along with diverse datasets covering various difficulty levels, multi-task learning, and generalization challenges. A novel baseline algorithm, QMIX+CQL, is proposed to handle the game's hierarchical action space. Extensive experiments reveal that current offline RL and MARL methods struggle with complex tasks, generalization, and multi-task learning, highlighting the need for further research in these areas. The framework and datasets are open-source to promote reproducibility and future advancements.

## Method Summary
The Hokoff framework provides a complete pipeline for offline RL/MARL research on Honor of Kings. It includes a sampling module for parallel data collection using multi-level opponent models, a training module implementing various offline RL algorithms (BC, CQL, TD3+BC, IQL, QMIX+CQL, etc.) with a shared encoder architecture, and an evaluation module that uses win rates against different skill-level opponent models as the primary metric. The dataset spans multiple game modes (1v1, 3v3) and subtasks (Destroy Turret, Gain Gold), with specific splits for multi-task learning and generalization experiments across heroes, opponent levels, and game scenarios.

## Key Results
- Current offline RL methods achieve limited success on complex MOBA tasks, with performance varying significantly across difficulty levels
- Generalization experiments show substantial performance degradation when training and evaluation distributions differ across heroes, opponents, or game levels
- Multi-task learning remains challenging, with existing methods unable to consistently outperform behavior policies across diverse objectives
- The proposed QMIX+CQL baseline demonstrates improved handling of hierarchical action spaces compared to traditional approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hierarchical action space in HoK necessitates a novel MARL-based approach like QMIX+CQL
- Mechanism: The action space in HoK is structured as a triplet (action button, target, execution style) rather than flattened. This structure resembles joint action spaces in MARL, so QMIX can factorize it, while CQL adds conservatism to handle distributional shift.
- Core assumption: Flattening the action space would lead to millions of discrete actions, making learning infeasible.
- Evidence anchors:
  - [abstract]: "A novel baseline algorithm tailored for the inherent hierarchical action space of the game"
  - [section]: "The action space is hierarchically structured and discretized, covering all possible actions of the hero in a hierarchical triplet form"

### Mechanism 2
- Claim: Multi-level opponent models enable scalable dataset generation and fair evaluation
- Mechanism: Pre-trained models at different skill levels generate datasets with varying win rates. During evaluation, win rate against specific models replaces raw returns, avoiding bias from environment difficulty.
- Core assumption: Zero-sum reward games make raw returns unreliable for algorithm comparison.
- Evidence anchors:
  - [section]: "We adopt the win rate against different checkpoints as our evaluation protocols"
  - [section]: "The win rate of the behavior policy is recorded in the column labeled Win_rate"

### Mechanism 3
- Claim: Generalization experiments reveal current offline RL's inability to handle distribution shifts
- Mechanism: Training on one dataset but testing with different heroes, opponents, or levels creates distribution shift. Poor performance in these settings indicates that current methods overfit to training distributions.
- Core assumption: Generalization failure implies distributional shift rather than insufficient model capacity.
- Evidence anchors:
  - [abstract]: "We reveal the incompetency of current offline RL approaches in handling task complexity, generalization and multi-task learning"
  - [section]: "The disparities between training and evaluation in Generalization settings impede the achievement of desirable performance"

## Foundational Learning

- Concept: Distributional shift in offline RL
  - Why needed here: Explains why conservative methods (CQL, IQL) are necessary to prevent overestimation on out-of-distribution actions.
  - Quick check question: What is the main source of value overestimation in offline RL, and how do CQL and IQL address it?

- Concept: Hierarchical action spaces
  - Why needed here: Understanding why flattening is infeasible and why QMIX-like factorization is appropriate for HoK.
  - Quick check question: How does a hierarchical action space differ from a flat one, and what computational challenges arise when flattening?

- Concept: Zero-sum reward normalization
  - Why needed here: Explains why raw returns are unreliable for algorithm comparison in competitive games like HoK.
  - Quick check question: Why is win rate a more stable evaluation metric than episode return in zero-sum games?

## Architecture Onboarding

- Component map: Sampling module -> Training module -> Evaluation module -> Dataset storage
- Critical path:
  1. Sample datasets → 2. Train algorithm → 3. Evaluate win rate → 4. Compare results
- Design tradeoffs:
  - Hierarchical action space vs. flat discretization: Computational feasibility vs. modeling simplicity
  - Win rate vs. return-based evaluation: Robustness vs. granularity
  - Multi-level models vs. human replays: Scalability vs. realism
- Failure signatures:
  - Poor generalization performance → distributional shift or overfitting
  - Low win rates across all algorithms → data quality issues or overly difficult tasks
  - High variance in results → insufficient seeds or unstable training
- First 3 experiments:
  1. Train BC on norm_poor dataset and evaluate win rate vs. baseline model
  2. Train QMIX+CQL on norm_medium dataset and compare to CQL
  3. Test generalization by training on norm_medium and evaluating on norm_hero_general

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can offline RL methods be improved to handle complex hierarchical action spaces like those in MOBA games?
- Basis in paper: [explicit] The paper introduces QMIX+CQL as a baseline and demonstrates that current methods struggle with complex tasks involving discrete action spaces.
- Why unresolved: While QMIX+CQL shows some improvement, the paper does not provide a comprehensive solution for handling hierarchical action spaces in offline RL.
- What evidence would resolve it: Developing and testing new algorithms specifically designed for hierarchical action spaces, and demonstrating their effectiveness on complex tasks like those in Hokoff.

### Open Question 2
- Question: What strategies can be employed to enhance the generalization capabilities of offline RL models across different tasks, heroes, and opponent levels?
- Basis in paper: [explicit] The paper highlights the limitations of current offline RL methods in generalization settings, including hero generalization, opponent generalization, and level generalization.
- Why unresolved: The paper does not propose specific techniques to improve generalization, and existing methods show poor performance in these settings.
- What evidence would resolve it: Developing and validating new methods that improve generalization across diverse scenarios, and demonstrating their effectiveness on the Hokoff datasets.

### Open Question 3
- Question: How can offline RL methods be adapted to effectively handle multi-task learning in complex environments like MOBA games?
- Basis in paper: [explicit] The paper shows that current offline RL methods struggle with multi-task learning, as evidenced by their performance on multi-task datasets in Hokoff.
- Why unresolved: The paper does not provide a comprehensive solution for multi-task learning in offline RL, and existing methods fail to exceed the performance of behavior policies.
- What evidence would resolve it: Developing and testing new algorithms that effectively handle multi-task learning in complex environments, and demonstrating their performance on the Hokoff datasets.

## Limitations

- Scalability concerns regarding the evaluation protocol for larger-scale games and the robustness of win-rate metrics across different opponent skill levels
- The hierarchical action space handling through QMIX+CQL may not generalize to games with different action structures
- Potential biases in opponent model training could affect the fairness of the evaluation protocol
- Datasets may not fully capture the complexity of real-world deployment scenarios

## Confidence

- Mechanism for handling hierarchical action spaces through QMIX+CQL: Medium
- Use of multi-level opponent models for evaluation: Medium
- Generalization experiments revealing distributional shift: Medium
- Claim that current methods struggle with multi-task learning: Medium

## Next Checks

1. **Ablation on Action Space Handling**: Test the performance difference between QMIX+CQL and a baseline that flattens the action space (if computationally feasible) to validate the necessity of the hierarchical approach.

2. **Opponent Model Robustness**: Evaluate the win-rate metric across different opponent model checkpoints to ensure consistency and fairness in the evaluation protocol.

3. **Generalization with Regularization**: Apply stronger regularization techniques (e.g., dropout, data augmentation) to the baseline algorithms and assess whether the generalization gaps can be reduced, indicating distributional shift rather than model capacity issues.