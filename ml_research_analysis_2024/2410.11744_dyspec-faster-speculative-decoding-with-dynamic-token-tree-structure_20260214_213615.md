---
ver: rpa2
title: 'DySpec: Faster Speculative Decoding with Dynamic Token Tree Structure'
arxiv_id: '2410.11744'
source_url: https://arxiv.org/abs/2410.11744
tags:
- token
- tree
- draft
- tokens
- acceptance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes DySpec, a speculative decoding algorithm that
  dynamically constructs token trees based on draft model probabilities to improve
  acceptance rates and inference speed. The method leverages a greedy strategy to
  expand the token tree, maximizing the expected length of predicted sequences.
---

# DySpec: Faster Speculative Decoding with Dynamic Token Tree Structure

## Quick Facts
- arXiv ID: 2410.11744
- Source URL: https://arxiv.org/abs/2410.11744
- Reference count: 40
- Primary result: Dynamic token tree construction improves speculative decoding acceptance rates and inference speed

## Executive Summary
DySpec introduces a dynamic token tree construction method for speculative decoding that adapts the token tree structure based on draft model probabilities. Unlike fixed tree approaches, DySpec uses a greedy expansion strategy to prioritize tokens with higher acceptance probability, achieving up to 9.1× throughput improvement and 9.4× latency reduction on Llama2-70B. The method demonstrates strong correlation between draft model probability and token acceptance rate, particularly effective under low temperature settings.

## Method Summary
DySpec dynamically constructs token trees during speculative decoding by expanding tokens with the highest estimated acceptance probability first. The approach leverages the correlation between draft model probability and acceptance rate, using a greedy algorithm to maximize expected token acceptance. When full greedy expansion is computationally expensive, a threshold-based layer-by-layer construction approximates the optimal tree while reducing overhead. The method maintains unbiased output distribution through parallel verification of the constructed token tree against the target model.

## Key Results
- Achieves up to 9.1× throughput improvement and 9.4× latency reduction on Llama2-70B
- Outperforms fixed tree methods (SpecInfer, Sequoia) across CNN DailyMail, OpenWebText, and C4 datasets
- Demonstrates strong correlation between draft probability and acceptance rate, especially at low temperatures
- Shows scalability across model sizes from 7B to 70B parameters

## Why This Works (Mechanism)

### Mechanism 1: Probability-Acceptance Correlation
Higher draft probability tokens have higher acceptance rates because draft model probability approximates target model probability under low KL divergence. This correlation enables effective prioritization during tree construction.

### Mechanism 2: Greedy Expansion Optimality
Greedy expansion maximizes expected token acceptance by prioritizing tokens with highest estimated acceptance probability at each step. The monotonic decrease in acceptance probability estimates ensures this yields the optimal token tree.

### Mechanism 3: Threshold-Based Approximation
Layer-by-layer construction with threshold approximates greedy optimality by filtering low-probability tokens while maintaining performance benefits. This reduces computational overhead while preserving most of the acceptance rate gains.

## Foundational Learning

- Concept: Speculative decoding and token tree verification
  - Why needed here: Understanding how draft and target models interact in parallel verification is fundamental to grasping DySpec's approach
  - Quick check question: What happens to succeeding tokens when one token is rejected in speculative decoding?

- Concept: KL divergence and probability distribution approximation
  - Why needed here: The theoretical foundation for why draft probability correlates with acceptance rate relies on bounded KL divergence
  - Quick check question: How does constraining KL divergence between distributions affect their relative probabilities?

- Concept: Greedy algorithms and optimal substructure
  - Why needed here: DySpec's construction method relies on greedy selection, which requires understanding when greedy approaches yield optimal solutions
  - Quick check question: Under what conditions does greedy selection guarantee global optimality?

## Architecture Onboarding

- Component map: Draft model -> Token tree construction -> Verification mechanism -> Target model
- Critical path: 1) Generate draft probabilities from prefix, 2) Construct token tree using greedy expansion or threshold method, 3) Perform parallel verification with target model, 4) Return accepted tokens and continue decoding
- Design tradeoffs: Tree size vs. latency (larger trees improve acceptance but increase construction overhead), Greedy vs. threshold (greedy is optimal but slower; threshold is faster but approximate), Memory vs. speed (CPU offloading enables larger models but adds data transfer overhead)
- Failure signatures: Low acceptance rate (draft model poorly approximates target model), High latency (tree construction overhead dominates verification time), Memory errors (tree size exceeds available GPU memory)
- First 3 experiments: 1) Baseline comparison with fixed tree structure (SpecInfer/Sequoia), 2) Temperature sensitivity testing at different target temperatures (0, 0.6), 3) Model scale validation across different model pairs (7B/13B, 7B/70B)

## Open Questions the Paper Calls Out

### Open Question 1
What is the theoretical limit of speedup achievable by speculative decoding when using dynamic token trees versus fixed tree structures? The paper discusses empirical speedup improvements but does not provide theoretical upper bounds.

### Open Question 2
How does the performance of dynamic token trees scale with increasingly large draft models, especially in terms of memory usage and computational overhead? The paper mentions CPU offloading for large models but does not explore scaling behavior comprehensively.

### Open Question 3
Can the dynamic token tree structure be optimized further by incorporating information from the target model during the tree construction phase? The paper acknowledges that target model distribution is unknown during construction, suggesting potential for improvement.

### Open Question 4
How does the choice of threshold in the threshold-based tree construction algorithm affect the trade-off between latency and acceptance rate across different datasets and model configurations? The paper introduces the threshold method but does not systematically study its effects.

## Limitations

- KL divergence assumption may break down for more dissimilar model pairs or extreme temperature settings
- Performance degrades significantly at higher temperatures where draft-target correlation weakens
- Computational overhead from dynamic tree construction not comprehensively profiled
- Greedy optimality assumptions lack full empirical validation across all scenarios

## Confidence

**High Confidence**:
- Dynamic tree construction improves acceptance rates compared to fixed tree methods
- Greedy expansion strategy effectively prioritizes high-probability tokens
- Correlation between draft probability and acceptance rate holds under controlled conditions

**Medium Confidence**:
- KL divergence constraint ensures correlation between distributions
- Greedy algorithm yields optimal token tree structure
- Layer-by-layer construction approximates greedy optimality

**Low Confidence**:
- Performance benefits generalize to all temperature settings
- Overhead from dynamic construction is negligible compared to verification gains
- Method scales equally well across all model size combinations

## Next Checks

**Check 1: Distribution Divergence Analysis** - Quantify actual KL divergence between draft and target models across all tested pairs and temperature settings. Measure how this divergence correlates with acceptance rate degradation.

**Check 2: Greedy Optimality Validation** - Empirically verify monotonic decrease assumption by tracking acceptance probabilities for sibling tokens and deeper tokens across multiple decoding runs.

**Check 3: End-to-End Overhead Profiling** - Conduct comprehensive profiling including token tree construction time, GPU-CPU data transfer overhead, and interaction with CPU offloading mechanisms to measure net latency improvement.