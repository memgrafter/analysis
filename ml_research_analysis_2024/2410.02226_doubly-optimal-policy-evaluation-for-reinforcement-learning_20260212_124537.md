---
ver: rpa2
title: Doubly Optimal Policy Evaluation for Reinforcement Learning
arxiv_id: '2410.02226'
source_url: https://arxiv.org/abs/2410.02226
tags:
- policy
- variance
- behavior
- learning
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of high-variance policy evaluation
  in reinforcement learning by proposing a doubly optimal approach that jointly designs
  both an optimal data-collecting behavior policy and an optimal data-processing baseline
  function. The key insight is to model variance reduction as a bi-level optimization
  problem where the behavior policy and baseline are carefully tailored to each other.
---

# Doubly Optimal Policy Evaluation for Reinforcement Learning

## Quick Facts
- arXiv ID: 2410.02226
- Source URL: https://arxiv.org/abs/2410.02226
- Authors: Shuze Daniel Liu; Claire Chen; Shangtong Zhang
- Reference count: 22
- Key outcome: DOpt method reduces variance by ~75% compared to on-policy Monte Carlo and achieves state-of-the-art performance, saving 50-78% of online samples needed for accurate policy evaluation

## Executive Summary
This paper addresses the challenge of high-variance policy evaluation in reinforcement learning by proposing a doubly optimal approach that jointly optimizes both the data-collecting behavior policy and the data-processing baseline function. The key insight is to model variance reduction as a bi-level optimization problem where these two components are carefully tailored to each other. The authors prove their method is unbiased and guarantees lower variance than previous state-of-the-art methods, with superiority growing over time horizon. Empirically, DOpt achieves substantial variance reduction across Gridworld and MuJoCo environments while requiring significantly fewer online samples.

## Method Summary
The method formulates variance reduction as a bi-level optimization problem. In the outer problem, given a baseline function, the behavior policy is designed to minimize expected future variance conditioned on the current state-action pair. In the inner problem, given the behavior policy, the baseline function removes variance from the Q-function estimate. The authors derive closed-form solutions for both the optimal behavior policy (µ* ∝ πt(a|s)/√uπ,t(s,a)) and optimal baseline (b*t(s,a) = qπ,t(s,a)), where uπ,t is an auxiliary function learned recursively from offline data using fitted Q-evaluation. This joint optimization approach ensures that the effects of behavior policy and baseline compound rather than interfere.

## Key Results
- DOpt reduces variance by approximately 75% compared to on-policy Monte Carlo estimation
- Achieves state-of-the-art performance across Gridworld and MuJoCo environments
- Saves 50-78% of online samples needed for accurate policy evaluation
- Robust to hyperparameter choices and works effectively with incomplete offline data
- Variance reduction advantage grows with longer time horizons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint optimization of behavior policy and baseline reduces variance more than optimizing either alone
- Mechanism: The bi-level optimization problem ensures the behavior policy and baseline are tailored to each other, with effects that compound rather than interfere
- Core assumption: Variance can be decomposed into state-action-dependent terms that can be estimated from offline data
- Evidence anchors: Abstract states method models variance reduction as bi-level optimization; empirical results show state-of-the-art performance
- Break condition: If offline data is too sparse to estimate variance components accurately, closed-form solutions may be poor

### Mechanism 2
- Claim: Enlarging the policy coverage constraint enables more flexible behavior policies while maintaining unbiasedness
- Mechanism: The paper shows that by defining Λ such that πt(a|s)uπ,t(s,a) is zero when behavior policy is zero, unbiasedness is preserved even without full coverage
- Core assumption: Auxiliary function uπ,t(s,a) can be constructed to be zero wherever πt(a|s) is zero
- Evidence anchors: Section shows enlargement of search space from Λ− to Λ while preserving unbiasedness
- Break condition: If uπ,t(s,a) construction fails to be zero where πt(a|s) is zero, estimator may become biased

### Mechanism 3
- Claim: Variance reduction compounds over time steps, leading to greater benefits in longer horizons
- Mechanism: Theorems show variance reduction includes both immediate elimination and propagated reduction from future steps, with compounding effect growing over horizon
- Core assumption: Future variance reduction terms are non-negative and accumulate multiplicatively
- Evidence anchors: Section explicitly states superiority grows over time horizon; Theorems 4, 5, 6 prove non-negativity of δ terms
- Break condition: If environment dynamics or rewards cause variance to increase sharply with horizon, compounding effect may be overwhelmed

## Foundational Learning

- Concept: Importance sampling and per-decision importance sampling (PDIS) in off-policy evaluation
  - Why needed here: The method relies on PDIS estimators with baseline corrections; understanding importance sampling ratios is essential to grasp variance sources
  - Quick check question: What is the difference between ordinary importance sampling and per-decision importance sampling estimators, and why does PDIS have lower variance?

- Concept: Bi-level optimization and closed-form solutions
  - Why needed here: The core contribution is solving a bi-level optimization problem in closed form; understanding decomposition and derivation is essential
  - Quick check question: In the bi-level optimization, why can the inner problem (optimal behavior policy given baseline) be solved independently for each state-action pair?

- Concept: Variance decomposition and law of total variance
  - Why needed here: Proofs rely on recursively decomposing variance into immediate and future components; understanding this decomposition is key to theoretical analysis
  - Quick check question: How does the law of total variance allow the variance of the estimator to be written as a sum of conditional variances and the variance of conditional expectations?

## Architecture Onboarding

- Component map: Offline data preprocessing -> Q-function estimation (FQE) -> Variance term νπ,t construction -> Auxiliary function uπ,t estimation -> Behavior policy µ* and baseline b* extraction -> Online evaluation with reduced variance
- Critical path:
  1. Fit qπ,t(s,a) from offline tuples using FQE
  2. Construct νπ,t using (38) and learn uπ,t recursively from t=T-1 to 0 using Lemma 3
  3. Derive µ* and b* from uπ,t and qπ,t using (12) and (15)
  4. Use µ* to collect online samples and b* to correct returns
- Design tradeoffs:
  - Offline data size vs. estimator accuracy: more data improves uπ,t estimation but increases computation
  - Function approximator complexity: deeper networks may better capture q and u but risk overfitting
  - Action space discretization in MuJoCo: simplifies method but may limit expressiveness
- Failure signatures:
  - High variance despite using µ* and b*: likely due to poor estimation of uπ,t from insufficient data
  - Bias in estimates: indicates violation of unbiasedness condition (Λ set incorrectly constructed)
  - Instability in learning uπ,t: suggests numerical issues in recursive computation or poor initialization
- First 3 experiments:
  1. Gridworld with n=10 (1000 states): verify variance reduction vs. on-policy MC and baseline methods
  2. Gridworld with n=30 (27000 states): test scalability and compounding effect over longer horizons
  3. MuJoCo Ant environment: validate robustness in continuous control with discretized actions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the doubly optimal estimator perform in continuous control tasks with high-dimensional state spaces beyond tested MuJoCo environments?
- Basis in paper: [explicit] Paper mentions MuJoCo environments but notes method works on "general MDPs without any restriction on their inherent structures"
- Why unresolved: Paper only tests specific MuJoCo tasks and Gridworld; high-dimensional continuous control tasks like robotic manipulation are not explored
- What evidence would resolve it: Empirical results on additional continuous control benchmarks with larger state/action spaces

### Open Question 2
- Question: Can the variance reduction technique be effectively extended to temporal difference (TD) learning methods?
- Basis in paper: [explicit] Conclusion states "future work of our paper is to extend the variance reduction technique to temporal difference learning"
- Why unresolved: Current method is based on Monte Carlo estimators; TD learning methods have different bias-variance characteristics
- What evidence would resolve it: Theoretical analysis and empirical results demonstrating variance reduction in TD learning algorithms using similar bi-level optimization

### Open Question 3
- Question: What is the minimum required size of offline data for behavior policy and baseline approximation to remain accurate?
- Basis in paper: [inferred] Paper mentions if offline data size is too small, behavior policy and baseline may be inaccurate, recommending on-policy evaluation in such cases
- Why unresolved: Paper doesn't specify quantitative thresholds or provide systematic analysis of data requirements across environments
- What evidence would resolve it: Detailed study showing relationship between offline data size, estimation accuracy, and point at which method becomes unreliable

## Limitations
- The method's performance heavily depends on the quality and quantity of offline data for accurate estimation of variance terms
- Closed-form solutions assume exact knowledge of environment dynamics, which is typically unavailable in real-world settings
- Discretization of action spaces in continuous control environments may limit applicability to settings requiring fine-grained control
- Scalability to very large state-action spaces and truly continuous action domains remains unproven

## Confidence
- **High confidence**: The theoretical framework is sound with rigorous proofs establishing unbiasedness and variance reduction guarantees
- **Medium confidence**: Empirical results demonstrate substantial variance reduction, but exact magnitude may depend on unspecified implementation details
- **Low confidence**: Scalability to very large state-action spaces and truly continuous action domains remains unproven

## Next Checks
1. **Offline Data Sensitivity**: Systematically vary the size and quality of offline dataset and measure impact on variance reduction and estimation accuracy
2. **Continuous Action Generalization**: Implement method using function approximation for uπ,t and qπ,t in continuous action spaces without discretization and compare performance
3. **Real-World Dynamics**: Introduce noise or uncertainty in estimated environment dynamics used to construct uπ,t and evaluate method robustness to such imperfections