---
ver: rpa2
title: 'Better than Your Teacher: LLM Agents that learn from Privileged AI Feedback'
arxiv_id: '2410.05434'
source_url: https://arxiv.org/abs/2410.05434
tags:
- student
- action
- privileged
- reason
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LEAP is an iterative learning framework that fine-tunes LLM agents\
  \ using feedback from privileged AI experts. The key idea is to provide expert teachers\
  \ with privileged state information during training\u2014data unavailable to the\
  \ student at test time\u2014allowing them to offer more precise corrective guidance."
---

# Better than Your Teacher: LLM Agents that learn from Privileged AI Feedback

## Quick Facts
- arXiv ID: 2410.05434
- Source URL: https://arxiv.org/abs/2410.05434
- Reference count: 40
- Primary result: LEAP framework enables weak LLM models to exceed strong teacher models by iteratively refining policies using privileged state information during training.

## Executive Summary
LEAP is an iterative learning framework that fine-tunes LLM agents using feedback from privileged AI experts. The key innovation is providing expert teachers with privileged state information during training—data unavailable to the student at test time—allowing them to offer more precise corrective guidance. Experiments across three decision-making domains show LEAP consistently outperforms behavior cloning and ReAct baselines, enables weaker models to surpass stronger teachers, and allows agents to self-improve using their own privileged versions.

## Method Summary
LEAP is an iterative learning framework that fine-tunes LLM agents using feedback from privileged AI experts. The framework works by having expert teachers access privileged state information (unavailable to students during testing) to provide corrective feedback on student trajectories. The method involves initializing a student policy with behavior cloning, collecting rollouts in the environment, generating privileged state information, invoking the privileged expert to create corrections, aggregating these corrections into the dataset, and fine-tuning the student model iteratively using either supervised fine-tuning or preference optimization. The process repeats until convergence or maximum iterations are reached.

## Key Results
- LEAP enables weak student models (e.g., Llama3-8B) to exceed the performance of strong teacher models (GPT4-o) in three decision-making domains
- The framework achieves 91.8% success rate on out-of-distribution tasks, significantly outperforming all baselines
- LEAP allows weak models to self-improve using privileged versions of themselves, enabling iterative refinement without external expert models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LEAP enables weaker student models to surpass stronger teacher models by iteratively refining policies using privileged state information during training.
- Mechanism: The expert teacher uses privileged state (hidden from the student during testing) to provide corrective feedback on student trajectories. This feedback improves the student's reasoning and action selection, enabling it to learn more efficient strategies than the teacher.
- Core assumption: The expert teacher can provide corrective feedback that is both optimal and realizable for the student model.
- Evidence anchors: [abstract] "enables weak student models (e.g., Llama3-8B) to exceed the performance of strong teacher models (GPT4-o)"; [section] "LEAP significantly outperforms all baselines, with the best policy achieving 91.8% success rate on out-of-distribution tasks"; [corpus] Weak evidence: Only 1 neighbor paper directly addresses privileged information in teacher-student settings
- Break condition: If the privileged state information is too complex for the student to process or if the teacher provides unrealizable corrections.

### Mechanism 2
- Claim: LEAP achieves performance gains by balancing the use of privileged information with the student's realizability constraints.
- Mechanism: The framework uses a constrained privileged expert that leverages privileged state while ensuring corrections remain actionable for the student. This avoids the "latching effect" where students repetitively predict the same action.
- Core assumption: There exists an optimal trade-off between privileged information usage and realizability that maximizes student performance.
- Evidence anchors: [abstract] "Our theoretical and empirical analyses show that balancing these extremes is key"; [section] "We study the tradeoff between how much privileged state the expert uses vs the realizability of their corrections"; [corpus] Weak evidence: Limited corpus support for this specific tradeoff analysis
- Break condition: If the student model capacity is insufficient to process even the most constrained privileged feedback.

### Mechanism 3
- Claim: LEAP enables LLM agents to self-improve by using their own privileged versions as teachers.
- Mechanism: The student model can be fine-tuned using corrections from a privileged version of itself, allowing for iterative self-improvement without requiring external expert models.
- Core assumption: The privileged version of the student contains sufficient information to provide meaningful corrective feedback.
- Evidence anchors: [abstract] "allows weak models to self-improve using privileged versions of themselves"; [section] "We test the hypothesis that LEAP should enable a model to self-improve by using its privileged version as the expert"; [corpus] No direct corpus evidence for this specific self-improvement mechanism
- Break condition: If the base policy is too weak to generate useful privileged corrections or if the self-correction loop fails to converge.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The environments in the experiments (ALFWorld, WebShop, InterCode) involve hidden or incomplete information that the agent must navigate using only partial observations.
  - Quick check question: What is the key difference between a POMDP and a fully observable MDP, and why does this matter for the LEAP framework?

- Concept: Imitation Learning with Expert Feedback
  - Why needed here: LEAP builds on interactive imitation learning methods like DAGGER, where an expert provides corrective feedback to improve student performance over iterations.
  - Quick check question: How does DAGGER guarantee that the student policy will converge to near-optimal performance given sufficient expert feedback?

- Concept: Trade-off between Exploration and Exploitation
  - Why needed here: The student must balance exploring new strategies (to discover better solutions) with exploiting known good strategies (to maintain performance), especially when learning from privileged feedback.
  - Quick check question: In what ways does the privileged state information influence the exploration-exploitation balance during LEAP training?

## Architecture Onboarding

- Component map: Student Agent -> Environment -> Privileged State Generator -> Privileged Expert Teacher -> Dataset -> Student Agent (iterative cycle)
- Critical path: 1. Initialize student policy π0 with behavior cloning on demonstration data; 2. Roll out current policy πi-1 in environment to collect trajectories; 3. Generate privileged state information for each trajectory point; 4. Invoke privileged expert to create corrections; 5. Aggregate corrections with existing dataset; 6. Fine-tune student policy using SFT or preference optimization; 7. Repeat until convergence or maximum iterations reached
- Design tradeoffs: Amount of privileged information vs. student realizability; Number of fine-tuning iterations vs. computational cost; SFT vs. preference optimization for policy updates; Expert model strength vs. feedback quality
- Failure signatures: Student performance plateaus or regresses over iterations; Expert corrections become too complex or unrealizable; Student overfits to privileged information and fails to generalize; Computational cost becomes prohibitive for large-scale applications
- First 3 experiments: 1. Implement basic LEAP framework on ALFWorld with a simple environment and small student model; 2. Test different amounts of privileged information to find the optimal tradeoff for student realizability; 3. Compare SFT vs. preference optimization (DPO/KTO) for policy updates on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal level of privileged information that balances realizability and performance?
- Basis in paper: [explicit] The paper explicitly discusses this trade-off in Section 3.3 and experiments in Section 4.5.
- Why unresolved: While the paper identifies an optimal level (πE₃), it doesn't provide a general method to determine this optimal level for different environments or tasks.
- What evidence would resolve it: A systematic study across diverse environments showing how the optimal level of privileged information varies with task complexity, model capacity, and environment structure.

### Open Question 2
- Question: How does LEAP scale to more complex environments with longer time horizons and larger state spaces?
- Basis in paper: [inferred] The paper's experiments are limited to relatively simple environments with time horizons of 10-30 steps.
- Why unresolved: The computational cost of generating interaction rollouts and expert corrections could become prohibitive in more complex environments.
- What evidence would resolve it: Experiments showing LEAP's performance and scalability on environments with significantly longer time horizons, larger state spaces, or more complex reward structures.

### Open Question 3
- Question: How robust is LEAP to errors or noise in the privileged state information?
- Basis in paper: [inferred] The paper assumes perfect privileged state information is available during training.
- Why unresolved: In real-world applications, privileged information might be noisy, incomplete, or contain errors, which could affect the quality of expert corrections.
- What evidence would resolve it: Experiments showing how LEAP's performance degrades with varying levels of noise or errors in the privileged state, and potential methods to make LEAP more robust to such imperfections.

## Limitations
- Computational cost: Repeated environment rollouts and expert corrections could be prohibitive for large-scale applications
- Accessibility: Reliance on privileged expert models (like GPT-4o) may limit practical deployment
- Information quality: Framework's effectiveness depends heavily on the quality and completeness of privileged state information

## Confidence

- High confidence in core mechanism: The iterative learning framework using privileged expert feedback is well-established in the literature and the experimental results across three domains are consistent.
- Medium confidence in self-improvement claim: While the paper demonstrates self-improvement using privileged versions, the experimental validation is less extensive than the teacher-student comparisons.
- Medium confidence in theoretical analysis: The tradeoff analysis between privileged information and realizability is conceptually sound, but the empirical validation could be more comprehensive.

## Next Checks

1. **Computational efficiency audit**: Measure wall-clock time and GPU memory usage across all three domains to quantify the computational overhead and identify optimization opportunities.
2. **Expert correction quality analysis**: Conduct a human evaluation of expert corrections across different privileged state granularities to verify that more privileged information consistently leads to better, realizable corrections.
3. **Student model capacity study**: Systematically vary student model sizes (e.g., 3B, 8B, 70B parameters) to determine the minimum model capacity required for effective privileged feedback processing.