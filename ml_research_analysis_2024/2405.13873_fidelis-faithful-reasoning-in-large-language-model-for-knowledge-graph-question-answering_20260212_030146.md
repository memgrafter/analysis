---
ver: rpa2
title: 'FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph Question
  Answering'
arxiv_id: '2405.13873'
source_url: https://arxiv.org/abs/2405.13873
tags:
- reasoning
- path
- fidelis
- question
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FiDeLiS is a training-free framework that improves LLM reasoning
  faithfulness by grounding answers in verifiable knowledge graph paths. It uses a
  deductive beam search guided by KG-retrieved candidates and enforces step-wise logical
  consistency to ensure valid, traceable reasoning.
---

# FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering

## Quick Facts
- arXiv ID: 2405.13873
- Source URL: https://arxiv.org/abs/2405.13873
- Reference count: 40
- Key outcome: FiDeLiS achieves state-of-the-art accuracy (e.g., 84.39% Hits@1 on WebQSP with GPT-4-turbo) by grounding LLM reasoning in verifiable knowledge graph paths, reducing reasoning depth closer to ground truth without fine-tuning.

## Executive Summary
FiDeLiS is a training-free framework that improves the faithfulness of LLM reasoning for knowledge graph question answering (KGQA). It grounds answers in verifiable KG paths, uses deductive beam search guided by KG-retrieved candidates, and enforces step-wise logical consistency to ensure valid, traceable reasoning. The method balances faithfulness and efficiency without fine-tuning, offering robust, interpretable, and scalable KG-enhanced QA.

## Method Summary
FiDeLiS grounds LLM reasoning in verifiable KG paths to ensure faithfulness. It uses Path-RAG to narrow the search space via semantic and graph connectivity scoring, and DVBS to validate each reasoning step and halt when the query is deduced. The framework achieves state-of-the-art accuracy on WebQSP, CWQ, and CR-LT-KGQA without fine-tuning, balancing faithfulness and efficiency.

## Key Results
- Achieves 84.39% Hits@1 on WebQSP with GPT-4-turbo
- Reduces reasoning depth closer to ground truth compared to baselines
- Maintains state-of-the-art accuracy without requiring fine-tuning

## Why This Works (Mechanism)
FiDeLiS improves faithfulness by grounding LLM reasoning in verifiable KG paths. It uses a deductive beam search guided by KG-retrieved candidates and enforces step-wise logical consistency to ensure valid, traceable reasoning. Path-RAG narrows the search space via semantic and graph connectivity scoring, while DVBS validates each step and halts when the query is deduced.

## Foundational Learning
- **Knowledge Graph (KG)**: A structured representation of facts as entities and relations; needed to ground reasoning in verifiable facts. Quick check: Can the KG answer the question directly?
- **Deductive Reasoning**: Drawing logical conclusions from premises; needed to ensure each reasoning step is valid. Quick check: Does each step follow logically from the previous?
- **Beam Search**: An algorithm that explores multiple reasoning paths in parallel; needed to find the most faithful path. Quick check: Does the beam search explore diverse, valid paths?
- **Semantic and Graph Connectivity Scoring**: Measures relevance and connectivity of KG paths; needed to narrow the search space. Quick check: Are the top-scoring paths semantically relevant and connected?

## Architecture Onboarding
- **Component Map**: Query -> Path-RAG (KG retrieval + semantic scoring) -> DVBS (deductive beam search + validation) -> Answer
- **Critical Path**: Query → KG retrieval → Beam search with DVBS → Answer validation → Final answer
- **Design Tradeoffs**: Training-free simplicity vs. potential gains from fine-tuning; faithfulness vs. computational overhead of beam search
- **Failure Signatures**: Poor KG retrieval leading to irrelevant paths; model hallucinations causing invalid reasoning steps; computational overhead of beam search
- **First Experiments**:
  1. Evaluate FiDeLiS on a diverse set of out-of-domain KGQA datasets to assess robustness
  2. Conduct ablation studies isolating contributions of Path-RAG and DVBS
  3. Benchmark FiDeLiS against fine-tuned baselines to quantify trade-offs

## Open Questions the Paper Calls Out
None

## Limitations
- Performance may be highly sensitive to the base LLM's instruction-following capability
- Evaluation limited to specific datasets (WebQSP, CWQ, CR-LT-KGQA)
- Does not assess robustness to adversarial or out-of-distribution queries

## Confidence
- State-of-the-art accuracy on tested datasets: High
- Generalizability to other domains or knowledge graphs: Medium
- Efficiency claims (depth reduction): High

## Next Checks
1. Test FiDeLiS on a diverse set of out-of-domain KGQA datasets to assess robustness and generalizability
2. Conduct ablation studies isolating the contributions of Path-RAG and DVBS to quantify their individual impact on faithfulness and accuracy
3. Benchmark FiDeLiS against fine-tuned baselines to quantify the trade-off between training-free simplicity and potential gains from domain-specific fine-tuning