---
ver: rpa2
title: 'Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought
  Reasoning'
arxiv_id: '2402.13950'
source_url: https://arxiv.org/abs/2402.13950
tags:
- reasoning
- causal
- frodo
- answer
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines whether LLMs faithfully use their chain-of-thought
  reasoning steps when generating answers. A causal mediation analysis is performed
  on twelve LLMs across three reasoning tasks, showing that models often fail to reason
  faithfully over their own reasoning steps.
---

# Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning

## Quick Facts
- arXiv ID: 2402.13950
- Source URL: https://arxiv.org/abs/2402.13950
- Reference count: 28
- Key outcome: Introduces FRODO framework that improves reasoning faithfulness by 2–3% and robustness by +4.5% over baselines through counterfactual training

## Executive Summary
This paper examines whether large language models faithfully use their chain-of-thought reasoning steps when generating answers. Through causal mediation analysis across twelve LLMs and three reasoning tasks, the authors find that models often fail to properly reason over their own intermediate steps. To address this, they introduce FRODO, a two-module framework that generates correct reasoning chains using implicit causal feedback and then faithfully reasons over these steps. Experiments show FRODO improves performance by 2–3% over baselines, increases robustness by +4.5%, and produces more faithful rationales than standard fine-tuning approaches.

## Method Summary
The paper presents a causal mediation analysis framework to measure and improve reasoning faithfulness in LLMs. FRODO consists of two modules: an inference module trained via direct preference optimization (DPO) on preference data containing correct versus counterfactual reasoning chains, and a reasoning module trained with counterfactual loss and margin-ranking loss to ensure faithful reasoning over intermediate steps. The framework is evaluated across three tasks (StrategyQA, GSM8K, Causal Understanding) using twelve different LLMs, measuring natural/controlled indirect effects, accuracy, robustness, and faithfulness metrics.

## Key Results
- FRODO improves performance by 2–3% over baselines (SFT + CoT, Rainier, Crystal, MARIO, SCOTT)
- Increases robustness by +4.5% through better handling of counterfactual reasoning chains
- Generated rationales by FRODO are more faithful than those produced by standard fine-tuning
- Models show inverse scaling for certain tasks, where indirect effects worsen with increasingly capable models

## Why This Works (Mechanism)
FRODO works by explicitly separating the generation of correct reasoning chains from their faithful execution. The inference module learns to produce high-quality intermediate reasoning steps using implicit causal feedback, while the reasoning module is trained to actually use these steps rather than bypassing them. The counterfactual training signals help the model understand the causal relationship between reasoning steps and final answers, rather than just memorizing answer patterns.

## Foundational Learning

**Causal Mediation Analysis**: Understanding how intermediate variables (reasoning steps) mediate between inputs and outputs. Needed to quantify faithfulness; check by verifying ability to decompose total effects into direct and indirect components.

**Direct Preference Optimization (DPO)**: A parameter-efficient fine-tuning method that optimizes preference data without explicit reward modeling. Needed for training the inference module; check by confirming preference ranking capability.

**Counterfactual Reasoning**: Training on hypothetical scenarios where reasoning steps are altered. Needed to teach models to actually use their reasoning chains; check by testing model performance on counterfactual inputs.

**Natural vs Controlled Indirect Effects**: Metrics for measuring how reasoning steps affect final answers under different conditions. Needed to quantify faithfulness improvements; check by computing NIE/CIE values on validation sets.

## Architecture Onboarding

**Component Map**: Silver Rationale Generator (GPT-3) -> Inference Module (DPO) -> Reasoning Module (Counterfactual Loss) -> Final Answer

**Critical Path**: Preference Data Generation -> Inference Module Training -> Reasoning Module Training -> Evaluation with Causal Effects

**Design Tradeoffs**: Explicit vs implicit feedback in generating reasoning chains; computational cost of counterfactual training vs faithfulness gains; model size vs reasoning capability scaling

**Failure Signatures**: 
- Incomplete or contradictory reasoning chains during training
- Insufficient preference data quality causing DPO failures
- Models bypassing reasoning steps despite training

**3 First Experiments**:
1. Generate and inspect silver rationales from GPT-3 for quality and completeness
2. Train inference module with DPO and evaluate preference ranking performance
3. Test reasoning module with counterfactual loss on held-out validation set

## Open Questions the Paper Calls Out

**Open Question 1**: How does faithfulness vary across different model sizes and architectures, and what is the underlying mechanism? The paper observes inverse scaling where indirect effects worsen with more capable models but doesn't explain why.

**Open Question 2**: How can we improve robustness to counterfactual reasoning chains? While FRODO addresses this, the paper doesn't comprehensively analyze factors hindering this ability or evaluate beyond limited tasks.

**Open Question 3**: How can we develop more effective methods for generating correct and counterfactual reasoning chains? The paper uses GPT-3 for silver rationales but doesn't explore alternatives or compare trade-offs between explicit vs implicit feedback approaches.

## Limitations

- Mediation analysis assumes complete isolation between reasoning and inference modules, but gradient updates may create unintended coupling
- Preference data generation relies heavily on GPT-4's ability to create valid counterfactual chains, potentially introducing distributional biases
- Generalization claims are based on limited out-of-distribution testing across only three benchmark domains

## Confidence

- **High confidence** in overall methodology and framework design
- **Medium confidence** in causal effect measurements due to potential implementation complexities
- **Medium confidence** in generalization claims given limited testing scope

## Next Checks

1. Perform ablation studies removing counterfactual loss components to verify their contribution to faithfulness gains
2. Test FRODO on additional reasoning tasks outside the three benchmark domains to better assess generalization
3. Compare FRODO's reasoning faithfulness against human-written rationales to establish an upper bound on achievable faithfulness