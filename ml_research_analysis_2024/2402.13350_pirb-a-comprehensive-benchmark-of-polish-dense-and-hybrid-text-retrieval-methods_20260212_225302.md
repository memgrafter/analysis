---
ver: rpa2
title: 'PIRB: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval
  Methods'
arxiv_id: '2402.13350'
source_url: https://arxiv.org/abs/2402.13350
tags:
- retrieval
- polish
- datasets
- text
- hybrid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the Polish Information Retrieval Benchmark
  (PIRB), a comprehensive evaluation framework covering 41 diverse retrieval tasks
  in Polish, including 10 new datasets spanning domains like medicine, law, and academia.
  The authors evaluate over 20 dense and sparse retrieval models, including strong
  baselines trained using knowledge distillation, supervised fine-tuning, and hybrid
  approaches combining dense and sparse retrievers with a lightweight rescoring model.
---

# PIRB: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods

## Quick Facts
- arXiv ID: 2402.13350
- Source URL: https://arxiv.org/abs/2402.13350
- Reference count: 0
- Primary result: Dense models outperform existing state-of-the-art solutions for Polish retrieval, with hybrid methods providing further improvements

## Executive Summary
This work introduces the Polish Information Retrieval Benchmark (PIRB), a comprehensive evaluation framework covering 41 diverse retrieval tasks in Polish, including 10 new datasets spanning domains like medicine, law, and academia. The authors evaluate over 20 dense and sparse retrieval models, including strong baselines trained using knowledge distillation, supervised fine-tuning, and hybrid approaches combining dense and sparse retrievers with a lightweight rescoring model. Their dense models outperform existing state-of-the-art solutions, and hybrid methods further improve performance. The proposed three-step training process provides an efficient path to building effective language-specific retrievers. The benchmark, code, and model checkpoints are made publicly available.

## Method Summary
The authors propose a three-step training process to build effective Polish retrieval models. First, they use knowledge distillation to transfer knowledge from high-quality English text encoders to pre-trained Polish language models using bilingual corpora. Second, they perform supervised fine-tuning on annotated retrieval datasets like Polish MS MARCO using contrastive loss. Finally, they create lightweight hybrid retrievers that combine dense and sparse model scores using a learning-to-rank model (XGBRanker). The hybrid approach takes scores from both dense and sparse indexes as features and learns optimal weighting through pairwise LambdaMART optimization on the NDCG metric.

## Key Results
- Dense models trained through the three-step process outperform existing state-of-the-art solutions for Polish retrieval
- Hybrid methods combining dense and sparse retrievers with LTR rescoring provide additional performance improvements
- The proposed approach achieves strong results across 41 diverse retrieval tasks covering 10 new datasets in various domains
- Lightweight hybrid models add minimal computational overhead while improving retrieval quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation from high-quality English text encoder improves Polish language model retrieval quality.
- Mechanism: The teacher (English FlagEmbedding) generates vector representations of English text while the student (Polish language model) produces representations for translated Polish text. The mean-squared error (MSE) loss between these vectors fine-tunes the student to approximate the teacher's representation quality.
- Core assumption: Cross-lingual semantic similarity is preserved through translation, allowing the Polish model to benefit from the teacher's superior English retrieval capabilities.
- Evidence anchors:
  - [abstract] "First, we use a multilingual knowledge distillation technique (Reimers and Gurevych, 2020) to transfer knowledge from a high-quality English text encoder to a pre-trained language model for Polish."
  - [section 6.1] "The goal of this knowledge distillation method is to fine-tune a student model using bilingual corpora to approximate text representation generated by the teacher."
  - [corpus] Weak - no direct evidence found about cross-lingual translation preservation in this specific context.
- Break condition: If the translation quality is poor or if semantic relationships are not preserved across languages, the student model cannot effectively learn from the teacher.

### Mechanism 2
- Claim: Supervised fine-tuning on annotated retrieval datasets significantly improves retrieval performance over distilled models alone.
- Mechanism: Contrastive loss with in-batch and hard negatives optimizes the model to distinguish relevant from irrelevant document-query pairs, creating more discriminative representations for retrieval.
- Core assumption: The annotated retrieval dataset provides high-quality positive-negative pairs that represent real retrieval scenarios.
- Evidence anchors:
  - [abstract] "In the next step, we perform a supervised fine-tuning of the created encoder on the annotated retrieval dataset."
  - [section 5.1] "We used the training split of the Polish MS MARCO dataset for all models that required training."
  - [corpus] Moderate - MS MARCO is a well-established retrieval dataset, but its Polish translation quality may vary.
- Break condition: If the dataset is too small, too noisy, or poorly represents the target retrieval scenarios, the fine-tuning may not generalize well.

### Mechanism 3
- Claim: Lightweight hybrid retrieval combining dense and sparse representations improves performance over standalone dense models.
- Mechanism: A learning-to-rank model takes scores from both dense and sparse indexes as features and learns optimal weighting through pairwise LambdaMART optimization on the NDCG metric.
- Core assumption: Dense and sparse representations capture complementary information about documents, and a simple rescorer can effectively combine these signals.
- Evidence anchors:
  - [abstract] "The final step is to create a lightweight hybrid retriever, combining the results of the sparse and dense methods using an additional learning-to-rank model."
  - [section 6.1] "In our approach, we propose a lightweight learning-to-rank model that takes the scores of dense and sparse models as input and returns a new score for each query-document pair."
  - [corpus] Moderate - hybrid approaches are well-established in IR literature, but the specific lightweight implementation's effectiveness needs validation.
- Break condition: If dense and sparse representations are highly correlated or if one dominates the other, the hybrid approach may not provide significant benefits.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: To transfer retrieval capabilities from high-quality English models to Polish models that lack sufficient training data
  - Quick check question: What is the difference between teacher and student models in knowledge distillation?

- Concept: Contrastive Learning
  - Why needed here: To train models to distinguish relevant from irrelevant documents using positive-negative pairs
  - Quick check question: How does contrastive loss help improve retrieval performance?

- Concept: Learning-to-Rank (LTR)
  - Why needed here: To combine scores from different retrieval methods (dense and sparse) using optimized weighting
  - Quick check question: What is the difference between pointwise, pairwise, and listwise LTR approaches?

## Architecture Onboarding

- Component map: Teacher Model (FlagEmbedding) -> Distillation Stage (bilingual corpus, MSE loss) -> Student Model (Polish RoBERTa/E5) -> Fine-tuning Stage (Polish MS MARCO, contrastive loss) -> Hybrid Stage (BM25/SPLADE + dense model + XGBRanker)
- Critical path: Distillation → Fine-tuning → Hybrid combination
- Design tradeoffs:
  - Dense-only vs. hybrid: Hybrid adds complexity but improves performance
  - Teacher model selection: Trade-off between quality and computational cost
  - Translation quality: Critical for effective knowledge transfer
- Failure signatures:
  - Poor distillation results: Translation quality issues or semantic drift
  - Fine-tuning stagnation: Learning rate too low or dataset too small
  - Hybrid underperformance: Dense and sparse representations too correlated
- First 3 experiments:
  1. Test knowledge distillation with a small bilingual corpus and measure embedding similarity
  2. Evaluate fine-tuning convergence and loss curves on Polish MS MARCO
  3. Compare hybrid vs. standalone performance on a small validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between dense and sparse retrieval methods in hybrid models for Polish language?
- Basis in paper: [inferred] The paper discusses building lightweight sparse-dense hybrids and evaluates their performance, showing improvements over standalone dense models. It mentions different hybrid configurations (BM25 and SPLADE as sparse indexes) but does not provide a definitive answer on the optimal balance.
- Why unresolved: The paper demonstrates that hybrid methods improve performance, particularly for queries with named entities or specialized terminology, but does not specify the ideal weighting or configuration between dense and sparse components.
- What evidence would resolve it: Systematic experiments varying the weighting between dense and sparse components, testing different query types, and measuring performance across various domains would help determine the optimal balance.

### Open Question 2
- Question: How do the proposed retrieval models generalize to other low-resource languages with similar characteristics to Polish?
- Basis in paper: [explicit] The paper mentions that progress in multilingual retrieval has been less pronounced for low-resource languages and aims to advance research in this area for Polish. It also discusses the challenges of limited availability of datasets for such languages.
- Why unresolved: While the paper demonstrates success with Polish, it does not explore how the proposed methodology would perform for other low-resource languages, which may have different linguistic features or dataset characteristics.
- What evidence would resolve it: Applying the same three-step training process to other low-resource languages and evaluating performance on benchmark datasets would demonstrate the generalizability of the approach.

### Open Question 3
- Question: What is the impact of dataset quality and diversity on the performance of dense retrieval models for Polish?
- Basis in paper: [inferred] The paper introduces new datasets covering various domains and discusses the challenges of using automatically generated data. It mentions that most available corpora for Polish have been generated automatically, primarily through machine translation, and that manually annotated data make up only a small percentage.
- Why unresolved: While the paper uses a diverse set of datasets, it does not systematically analyze how the quality and diversity of training data affect model performance or investigate the trade-offs between using high-quality, smaller datasets versus larger, noisier datasets.
- What evidence would resolve it: Controlled experiments training models on datasets of varying quality and diversity, then evaluating their performance on held-out test sets, would provide insights into the importance of dataset characteristics.

## Limitations

- The paper relies on translation quality for knowledge distillation without directly evaluating translation quality or semantic preservation
- Limited ablation studies on the relative contributions of each training stage to final performance
- The impact of corpus filtering (LaBSE similarity > 0.7) on downstream performance is not explored or explained

## Confidence

- **High confidence**: The experimental methodology and evaluation framework are sound, with comprehensive coverage of 41 retrieval tasks and established metrics (NDCG, MRR, Recall, Accuracy)
- **Medium confidence**: The three-step training pipeline (knowledge distillation → supervised fine-tuning → hybrid combination) is theoretically justified and shows strong empirical results, though some hyperparameter details are unspecified
- **Medium confidence**: Claims about cross-lingual knowledge transfer through translation are plausible but not directly validated in the paper

## Next Checks

1. **Translation quality validation**: Conduct human evaluation or automatic metrics (BLEU, semantic similarity) on the bilingual corpus to verify translation quality and semantic preservation
2. **Ablation study**: Compare performance of models at each training stage (distilled only, fine-tuned only, hybrid) to quantify contribution of each component
3. **Cross-lingual robustness test**: Evaluate retrieval performance when queries and documents are in different languages to test the effectiveness of the cross-lingual transfer assumption