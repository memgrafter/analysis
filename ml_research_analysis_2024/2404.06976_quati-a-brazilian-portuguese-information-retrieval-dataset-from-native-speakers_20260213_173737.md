---
ver: rpa2
title: 'Quati: A Brazilian Portuguese Information Retrieval Dataset from Native Speakers'
arxiv_id: '2404.06976'
source_url: https://arxiv.org/abs/2404.06976
tags:
- human
- passages
- query
- dataset
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Quati, a high-quality information retrieval
  dataset for Brazilian Portuguese. The dataset includes 200 native speaker queries
  and a curated corpus of documents from Brazilian Portuguese websites.
---

# Quati: A Brazilian Portuguese Information Retrieval Dataset from Native Speakers

## Quick Facts
- arXiv ID: 2404.06976
- Source URL: https://arxiv.org/abs/2404.06976
- Reference count: 40
- Primary result: Introduces Quati, a Brazilian Portuguese IR dataset with 200 queries and LLM-labeled relevance judgments achieving Cohen's Kappa of 0.31 with human evaluations

## Executive Summary
This paper introduces Quati, a high-quality information retrieval dataset for Brazilian Portuguese. The dataset includes 200 native speaker queries and a curated corpus of documents from Brazilian Portuguese websites. To address the resource-intensive nature of relevance annotation, the authors use a state-of-the-art large language model (GPT-4) to label query-document pairs. Human evaluations show that the LLM's annotations achieve a Cohen's Kappa correlation of 0.31 with human judgments, which is comparable to crowd worker performance. The dataset is publicly available and includes evaluation results for various retrieval systems, demonstrating its effectiveness in assessing IR systems for Portuguese.

## Method Summary
The authors created Quati by first collecting 200 queries from native Brazilian Portuguese speakers. They then curated a document corpus from Brazilian Portuguese websites. To annotate query-document relevance pairs, they employed GPT-4, a state-of-the-art large language model, rather than using traditional human annotation due to its resource-intensive nature. The LLM-generated labels were then validated through human evaluation, showing a Cohen's Kappa correlation of 0.31 with human judgments. This approach provides a cost-effective alternative to manual annotation while maintaining reasonable quality for IR evaluation purposes.

## Key Results
- 200 native speaker queries and curated Brazilian Portuguese document corpus
- GPT-4 annotations achieve Cohen's Kappa of 0.31 with human judgments
- Comparable performance to crowd worker annotations for relevance labeling
- Public dataset available with evaluation results for multiple retrieval systems

## Why This Works (Mechanism)
None

## Foundational Learning

**Cohen's Kappa**: Statistical measure of inter-rater agreement that accounts for chance agreement
- Why needed: Quantifies the reliability of LLM annotations against human judgments
- Quick check: Kappa > 0.61 indicates substantial agreement; this study achieved 0.31 (fair agreement)

**Information Retrieval Evaluation**: Process of assessing retrieval system effectiveness using relevance judgments
- Why needed: Establishes the standard methodology for validating IR datasets
- Quick check: Standard IR metrics include precision, recall, and NDCG

**Large Language Model Annotation**: Using LLMs to generate labels for datasets instead of human annotators
- Why needed: Reduces cost and time requirements for dataset creation
- Quick check: GPT-4 demonstrated reasonable performance compared to human annotators

## Architecture Onboarding

**Component Map**: Document Corpus -> Query Set -> LLM Annotation System (GPT-4) -> Human Validation -> Final Dataset -> Retrieval System Evaluation

**Critical Path**: The annotation pipeline is critical - LLM generates labels → Human validation confirms quality → Dataset is released with confidence metrics

**Design Tradeoffs**: Automated annotation (fast, cheap, moderate quality) vs. full human annotation (slow, expensive, high quality). The authors chose automation with partial human validation to balance quality and resource constraints.

**Failure Signatures**: Low Cohen's Kappa scores would indicate poor LLM performance; insufficient query diversity would limit dataset applicability; domain mismatch between training data and evaluation corpus would reduce relevance assessment accuracy.

**First Experiments**:
1. Verify Cohen's Kappa calculation methodology on a small sample
2. Test basic retrieval performance on a subset of the dataset
3. Compare LLM annotations against multiple annotator types (expert humans vs. crowd workers)

## Open Questions the Paper Calls Out
None

## Limitations

- Cohen's Kappa of 0.31 represents only fair agreement between LLM annotations and human judgments
- Dataset size of 200 queries may be insufficient for robust IR system evaluation
- Exclusive focus on Brazilian Portuguese limits generalizability to other Portuguese dialects
- Single LLM model (GPT-4) used for annotation without validation against multiple models

## Confidence

**High Confidence**: Dataset's public availability and documented evaluation results for retrieval systems are verifiable claims.

**Medium Confidence**: LLM labeling approach is plausible given current practices, but specific methodology details require verification. Dataset effectiveness claim is supported by evaluation results but needs broader testing.

**Low Confidence**: "High-quality" dataset designation is subjective without detailed quality metrics. Crowd worker performance comparison needs additional context about evaluation methodology.

## Next Checks

1. Conduct independent human evaluation on a subset of query-document pairs to verify the Cohen's Kappa value and assess label consistency.

2. Test the dataset with additional IR systems beyond those mentioned in the paper to evaluate its broader applicability and effectiveness.

3. Compare the performance of GPT-4-based annotations against multiple LLM models to assess potential annotation bias and robustness.