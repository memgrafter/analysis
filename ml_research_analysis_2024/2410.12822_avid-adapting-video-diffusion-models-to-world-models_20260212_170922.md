---
ver: rpa2
title: 'AVID: Adapting Video Diffusion Models to World Models'
arxiv_id: '2410.12822'
source_url: https://arxiv.org/abs/2410.12822
tags:
- diffusion
- pretrained
- video
- action
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of leveraging large-scale video
  diffusion models for sequential decision-making tasks like robotics, where action-labelled
  data is scarce. The authors propose AVID, a method to adapt pretrained video diffusion
  models to action-conditioned world models without requiring access to the model
  parameters.
---

# AVID: Adapting Video Diffusion Models to World Models

## Quick Facts
- arXiv ID: 2410.12822
- Source URL: https://arxiv.org/abs/2410.12822
- Authors: Marc Rigter; Tarun Gupta; Agrin Hilmkil; Chao Ma
- Reference count: 40
- This paper proposes AVID, a method to adapt pretrained video diffusion models to action-conditioned world models without requiring access to model parameters.

## Executive Summary
This paper tackles the challenge of leveraging large-scale video diffusion models for sequential decision-making tasks like robotics, where action-labelled data is scarce. The authors propose AVID, a method to adapt pretrained video diffusion models to action-conditioned world models without requiring access to the model parameters. AVID trains a lightweight adapter that takes the output of the pretrained model as input and uses a learned mask to combine it with conditional outputs from the adapter. This allows the adapter to erase incorrect motions predicted by the pretrained model and add the correct motion based on the action sequence. The adapter is trained on a small domain-specific dataset of action-labelled videos using the standard denoising loss.

## Method Summary
AVID is an adapter architecture that modifies the outputs of a pretrained diffusion model to add action conditioning. It consists of a lightweight 3D UNet that takes the noisy video, initial image, diffusion step, and action sequence as input. The adapter generates a learned mask that modulates the contribution of the pretrained model's noise prediction and the adapter's own noise prediction at each spatial location and time step. The mask is learned during training to emphasize the pretrained model's outputs where they are accurate (e.g., backgrounds) and emphasize the adapter's outputs where action-conditioned corrections are needed (e.g., moving objects). The adapter is trained on a small domain-specific dataset of action-labelled videos using the standard denoising loss.

## Key Results
- AVID outperforms existing baselines for diffusion model adaptation on video game and real-world robotics data.
- AVID achieves similar or slightly better overall performance compared to ControlNet variants without requiring access to the pretrained model parameters.
- AVID generates more accurate action-conditioned videos, as measured by lower action error ratio and better visual quality metrics (FVD, FID).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AVID's learned mask allows the adapter to selectively combine the pretrained model's outputs with action-conditioned corrections, enabling accurate action-conditioned video generation without access to the pretrained model's parameters.
- Mechanism: The adapter generates a mask tensor that modulates the contribution of the pretrained model's noise prediction and the adapter's own noise prediction at each spatial location and time step. The mask is learned during training to emphasize the pretrained model's outputs where they are accurate (e.g., backgrounds) and emphasize the adapter's outputs where action-conditioned corrections are needed (e.g., moving objects).
- Core assumption: The pretrained model's outputs contain useful information that can be leveraged for video generation, even if the model itself is not action-conditioned.
- Evidence anchors:
  - [abstract] "AVID uses a learned mask to modify the intermediate outputs of the pretrained model and generate accurate action-conditioned videos."
  - [section 3.3] "The mask is then used to combine the noise predictions from the pretrained model and the adapter according to: ϵfinal(xi, a, i, x0) = ϵpre ⊙ m + ϵadapt ⊙ (1 − m)"
  - [corpus] Weak evidence; no direct comparison to mask-based adaptation approaches in the literature.
- Break condition: If the pretrained model's outputs are not useful for the target domain, or if the learned mask fails to effectively distinguish between accurate and inaccurate regions, AVID's performance will degrade significantly.

### Mechanism 2
- Claim: AVID avoids the bias introduced by naively composing two independently trained diffusion models by directly optimizing the denoising loss with the combined outputs of the pretrained model and adapter.
- Mechanism: Unlike the Product of Experts approach, AVID does not attempt to compose the scores of two independently trained models. Instead, it uses the pretrained model's outputs as an input to the adapter and trains the adapter to directly optimize the standard denoising loss using the combined outputs. This ensures that the final predictions are consistent with the target distribution.
- Core assumption: Directly optimizing the denoising loss with the combined outputs of the pretrained model and adapter leads to unbiased samples from the target distribution.
- Evidence anchors:
  - [section 3.2] "Therefore, the composition in Equation (4) does not hold and will result in biased samples."
  - [section 3.3] "Rather than attempting to compose two independently trained models, AVID uses the outputs of the pretrained model to train an adapter that directly optimizes the denoising loss."
  - [corpus] Weak evidence; no direct comparison to unbiased adaptation approaches in the literature.
- Break condition: If the combined outputs of the pretrained model and adapter do not accurately represent the target distribution, AVID's performance will degrade, even if the denoising loss is optimized.

### Mechanism 3
- Claim: AVID's conditioning mechanism, which embeds the diffusion step and action sequence and injects them into each block of the UNet, enables the adapter to generate accurate action-conditioned videos.
- Mechanism: The adapter is conditioned on the diffusion step and the sequence of actions using learned embeddings. These embeddings are processed by an MLP to compute scale and shift parameters for each frame, which are then used to modulate the feature maps after each 3D convolution. This allows the adapter to generate action-conditioned predictions that are consistent with the input actions.
- Core assumption: Conditioning the adapter on the diffusion step and action sequence enables it to generate accurate action-conditioned videos.
- Evidence anchors:
  - [section 3.3] "The adapter is also conditioned on the diffusion step i and the sequence of actions a."
  - [section 4.2] "Action-Conditioned Diffusion – We train an action-conditioned diffusion model ϵθ(xi, a, i, x0) from scratch, with the same number of parameters and same UNet architecture as AVID."
  - [corpus] Weak evidence; no direct comparison to different conditioning mechanisms in the literature.
- Break condition: If the conditioning mechanism is not effective in capturing the relationship between the actions and the video frames, AVID's performance will degrade.

## Foundational Learning

- Concept: Diffusion models and denoising diffusion probabilistic models (DDPM)
  - Why needed here: AVID is built upon the principles of diffusion models, which are generative models that learn to denoise noisy data. Understanding the forward and reverse processes of diffusion models, as well as the training objective, is crucial for understanding how AVID works.
  - Quick check question: What is the role of the noise prediction model ϵθ in a diffusion model, and how is it trained?

- Concept: Adapter architectures for diffusion models
  - Why needed here: AVID is an adapter architecture that modifies the outputs of a pretrained diffusion model to add action conditioning. Understanding how adapter architectures work, such as ControlNet, is important for understanding the design choices made in AVID.
  - Quick check question: How does ControlNet add conditioning to a pretrained diffusion model, and what are its limitations?

- Concept: Action-conditioned video generation
  - Why needed here: AVID's goal is to generate action-conditioned videos, which means that the generated videos should accurately depict the consequences of the input actions. Understanding the challenges and approaches for action-conditioned video generation is important for understanding the motivation behind AVID.
  - Quick check question: What are the challenges in generating action-conditioned videos, and how do existing approaches address these challenges?

## Architecture Onboarding

- Component map: Pretrained model -> Adapter (UNet) -> Learned mask -> Final noise prediction -> Generated video

- Critical path:
  1. Input: Noisy video (xi), initial image (x0), diffusion step (i), action sequence (a).
  2. Pretrained model: Generate noise prediction (ϵpre) using the input video, initial image, and diffusion step.
  3. Adapter: Generate mask (m) and action-conditioned noise prediction (ϵadapt) using the noisy video, initial image, diffusion step, and action sequence.
  4. Combine: Combine the noise predictions from the pretrained model and adapter using the learned mask to obtain the final noise prediction (ϵfinal).
  5. Output: Generate the action-conditioned video using the final noise prediction.

- Design tradeoffs:
  - Using a pretrained model vs. training from scratch: AVID leverages a pretrained model to benefit from web-scale pretraining, but it requires access to the model's outputs during training.
  - Learned mask vs. direct addition: AVID uses a learned mask to combine the outputs of the pretrained model and adapter, which allows for more flexible control over the generation process.
  - Conditioning mechanism: AVID uses a specific conditioning mechanism that embeds the diffusion step and action sequence, which may not be optimal for all domains.

- Failure signatures:
  - Poor performance on action error ratio: Indicates that the generated videos are not consistent with the input actions.
  - Poor performance on FVD or FID: Indicates that the generated videos are not realistic or diverse.
  - Mask values close to 0 or 1: Indicates that the adapter is not effectively combining the outputs of the pretrained model and adapter.

- First 3 experiments:
  1. Train AVID on a small dataset and evaluate its performance on action error ratio, FVD, and FID.
  2. Compare AVID's performance to Action-Conditioned Diffusion and Product of Experts on the same dataset.
  3. Visualize the learned mask and analyze its values to understand how the adapter is combining the outputs of the pretrained model and adapter.

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions.

## Limitations
- The evaluation focuses on narrow domains (video games and robotics data), with limited comparison to other adaptation approaches in the literature.
- The reliance on specific pretrained models (Procgen and DynamiCrafter) without providing implementation details creates potential reproducibility barriers.
- Claims about avoiding bias in adaptation lack direct empirical validation against alternative unbiased approaches.

## Confidence
- **High Confidence**: The core mechanism of using a learned mask to combine pretrained model outputs with adapter predictions is well-specified and technically sound.
- **Medium Confidence**: Performance claims relative to baselines are supported by experimental results, though the evaluation scope is limited.
- **Low Confidence**: Claims about avoiding bias in adaptation lack direct empirical validation against alternative unbiased approaches.

## Next Checks
1. **Generalization Test**: Evaluate AVID's performance when adapting models pretrained on different domains (e.g., natural videos) to action-conditioned tasks, to assess the method's robustness to domain shift.

2. **Ablation Study**: Systematically remove the learned mask component and test whether direct addition of pretrained and adapter outputs degrades performance, validating the necessity of the mask mechanism.

3. **Long-Horizon Evaluation**: Test AVID on longer action sequences (beyond the 2-4 second clips evaluated) to assess whether errors compound over extended prediction horizons, which would be critical for real-world robotics applications.