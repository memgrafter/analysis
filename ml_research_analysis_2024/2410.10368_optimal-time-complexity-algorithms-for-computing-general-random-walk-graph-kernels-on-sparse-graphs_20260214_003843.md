---
ver: rpa2
title: Optimal Time Complexity Algorithms for Computing General Random Walk Graph
  Kernels on Sparse Graphs
arxiv_id: '2410.10368'
source_url: https://arxiv.org/abs/2410.10368
tags:
- random
- graphs
- graph
- number
- walks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Graph Voyagers (GVoys), the first linear-time
  algorithms for unbiased approximation of general random walk graph kernels (RWKs)
  on sparse graphs. The key innovation is using shared random variables during random
  walk sampling to emulate walks on the product graph without explicitly constructing
  it, enabling O(N) time complexity.
---

# Optimal Time Complexity Algorithms for Computing General Random Walk Graph Kernels on Sparse Graphs

## Quick Facts
- **arXiv ID**: 2410.10368
- **Source URL**: https://arxiv.org/abs/2410.10368
- **Reference count**: 40
- **Primary result**: First linear-time algorithms for unbiased approximation of general random walk graph kernels on sparse graphs, achieving up to 27× speedup on large graphs

## Executive Summary
This paper introduces Graph Voyagers (GVoys), the first linear-time algorithms for unbiased approximation of general random walk graph kernels (RWKs) on sparse graphs. The key innovation uses shared random variables during random walk sampling to emulate walks on the product graph without explicitly constructing it, achieving O(N) time complexity. GVoys generate graph embeddings where dot products equal RWKs in expectation, with theoretical concentration bounds proving sharp estimates. The method scales to graphs 128× larger than brute-force methods and achieves up to 27× speedup on large graphs.

## Method Summary
GVoys approximates RWKs by sampling dependent random walks on graphs with shared Rademacher variables, effectively emulating walks on the product graph G1×G2 without explicit construction. The algorithm uses Graph Random Features (GRFs) with importance sampling to estimate walk contributions of different lengths. A modulation function f(k) weights each walk length appropriately, while shared g-variables ensure unbiasedness by filtering only matching-length walks. The method constructs low-dimensional graph embeddings ϕ(Gi)∈RdG where dot products approximate RWKs in expectation, enabling O(dG) kernel evaluation between any pair of graphs.

## Key Results
- Achieves O(N) time complexity for RWK approximation on sparse graphs, scaling to graphs 128× larger than brute-force methods
- Provides unbiased estimates with theoretical concentration bounds, enabling sharp approximations
- Matches exact RWK performance on graph classification tasks while being up to 27× faster on large graphs
- Enables efficient implicit kernel learning, achieving 8.5% accuracy improvement on MUTAG dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Achieves O(N) time complexity by sharing random variables during random walk sampling, avoiding explicit construction of the product graph.
- **Mechanism**: Instead of building G1×G2 explicitly, GVoys simulate random walks on G1 and G2 separately while multiplying contributions by shared Rademacher variables g(i). These variables ensure that only terms where walk lengths match (i1=i2) survive in expectation, effectively emulating walks on the product graph without storing it.
- **Core assumption**: Rademacher variables have zero mean and unit variance, and their products satisfy E(g(i1)g(i2))=I(i1=i2), filtering cross-terms.
- **Evidence anchors**:
  - [abstract]: "Our method samples dependent random walks to compute novel graph embeddings in Rd whose dot product is equal to the true RWK in expectation."
  - [section]: "We sample random walks on G1 and G2 with shared random variables, thereby emulating walks on G1×G2 but without ever instantiating the product graph in memory."
- **Break condition**: If shared random variables are not properly synchronized between G1 and G2 walks, the emulator fails and the method reverts to O(N²) or worse.

### Mechanism 2
- **Claim**: Provides unbiased estimates of RWKs through importance sampling and modulation functions.
- **Mechanism**: GVoys use GRFs with importance sampling weights to estimate contributions from walks of different lengths. The modulation function f(k) weights each walk length appropriately, and the shared g-variables ensure unbiasedness by filtering only matching-length walks.
- **Core assumption**: The deconvolution relationship ∑p=0f(k-p)=µ(k) holds, allowing decomposition of the RWK into sums of walk contributions.
- **Evidence anchors**:
  - [abstract]: "The key innovation is using shared random variables during random walk sampling to emulate walks on the product graph without explicitly constructing it, enabling O(N) time complexity."
  - [section]: "We call the rows of Φ graph features{ϕG(vi)}vi∈V(G), so that Φ = [ϕG(vi)]Ni=1 with ϕG(vi)∈RN. We have that KG(v1,v2)=ϕG(v1)⊤ϕG(v2) for all v1,v2∈V(G)."
- **Break condition**: If the modulation function f(k) is poorly chosen or doesn't satisfy the deconvolution relationship, the decomposition becomes invalid and estimates become biased.

### Mechanism 3
- **Claim**: Scales linearly with both the number of nodes and the number of graphs in the dataset.
- **Mechanism**: By constructing low-dimensional graph-level embeddings ϕ(Gi)∈RdG where dG is the embedding dimension, the algorithm can compute kernel evaluations between any pair of graphs in O(dG) time. The Gram matrix KRWK=E[ΦΦ⊤] enables efficient downstream computation.
- **Core assumption**: Graph embeddings preserve pairwise kernel relationships in expectation, allowing dot products to approximate RWKs.
- **Evidence anchors**:
  - [abstract]: "The method scales to graphs 128× larger than brute-force methods and is up to 27× faster on large graphs."
  - [section]: "These satisfy KR WK(Gi,Gj)=E(ϕ(Gi)⊤ϕ(Gj)) for any pair of inputs, providing theoretically-grounded dG-dimensional representations of G1 and G2."
- **Break condition**: If the embedding dimension dG is too small relative to the complexity of the graphs, the approximation quality degrades and linear scaling benefits are lost.

## Foundational Learning

- **Concept**: Random Walk Kernels (RWKs)
  - **Why needed here**: Understanding RWKs is fundamental to grasping why GVoys work - they approximate a specific family of graph similarity measures based on weighted walks.
  - **Quick check question**: What is the key difference between geometric RWKs (µi=λi) and general RWKs that allows GVoys to handle the latter?

- **Concept**: Graph Random Features (GRFs)
  - **Why needed here**: GRFs are the building blocks that GVoys extend. They provide the mechanism for approximating kernel evaluations using random walks.
  - **Quick check question**: How do GRFs use importance sampling to estimate the contribution of walks of different lengths?

- **Concept**: Kronecker Product and Direct Product Graphs
  - **Why needed here**: Understanding how the direct product graph G1×G2 relates to the original graphs is crucial for seeing why GVoys avoids explicit construction.
  - **Quick check question**: Why does the direct product graph have size N1×N2 vertices, and how does this lead to the O(N²) bottleneck?

## Architecture Onboarding

- **Component map**: Random walk simulator -> Rademacher variable generator -> Anchor point sampler -> Graph feature constructor -> Kernel estimator

- **Critical path**:
  1. Sample random walks on G1 and G2 with shared termination variables
  2. Apply Rademacher variables g(i) to filter matching-length contributions
  3. Aggregate contributions using importance sampling weights
  4. Build low-dimensional graph embeddings
  5. Compute dot products for kernel evaluation

- **Design tradeoffs**:
  - Number of random walks m vs. approximation accuracy
  - Embedding dimension dG vs. memory usage and speed
  - Halting probability p_halt vs. walk length distribution
  - Labeled vs. unlabeled graphs (affects complexity)

- **Failure signatures**:
  - Poor approximation quality: Check if m is too small or dG too large
  - Memory issues: Verify if block-GVoys should be used
  - Slow performance: Ensure graphs are sparse and termination probability is appropriate

- **First 3 experiments**:
  1. Verify unbiasedness: Compute RWK on a small graph pair with increasing m and check convergence to exact value
  2. Test scalability: Run on graphs of increasing size (2^10, 2^11, 2^12 nodes) and measure runtime
  3. Check labeled vs unlabeled: Compare performance and accuracy on datasets with and without node labels

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the concentration bounds for GVoys be proven without conditioning on the draw of random variables g and z?
- **Basis in paper**: [explicit] The paper states "Removing the conditioning on the draw of random variables g,z is an important open problem" in the discussion of Theorem 4.2.
- **Why unresolved**: The current proof relies on McDiarmid's inequality which requires independence between the m draws of random variables. Since g and z are shared across all walkers, this creates dependencies that the current proof technique cannot handle.
- **What evidence would resolve it**: A proof technique that can handle the dependencies created by sharing g and z variables, or an alternative method to prove concentration bounds without conditioning.

### Open Question 2
- **Question**: What is the optimal distribution for the z variables beyond Rademacher, and does it depend on the graph structure?
- **Basis in paper**: [inferred] While Theorem 4.3 proves Rademacher is optimal for g variables, the paper only mentions z variables are "Rademacher" without exploring alternatives or proving optimality.
- **Why unresolved**: The paper only states z variables are chosen as Rademacher without theoretical justification or exploration of alternatives.
- **What evidence would resolve it**: A theoretical analysis comparing different distributions for z variables (e.g., Gaussian, uniform) and their impact on variance, or empirical results showing performance differences across distributions.

### Open Question 3
- **Question**: How does the performance of GVoys scale with graph density beyond the sparse regime assumed in the paper?
- **Basis in paper**: [inferred] The paper focuses on "sparse graphs" and states linear time complexity for sparse graphs, but doesn't explore dense graph performance.
- **Why unresolved**: All theoretical complexity analysis and experiments focus on sparse graphs (O(N) complexity), with no analysis of how performance degrades as graphs become denser.
- **What evidence would resolve it**: Empirical results showing runtime and accuracy as a function of graph density, or theoretical analysis of how the 1/halt parameter affects complexity in dense graphs.

## Limitations
- Relies on sparsity assumption - performance and complexity guarantees don't extend to dense graphs
- Theoretical concentration bounds require conditioning on random variables, creating a gap in the analysis
- Limited exploration of alternative distributions for z variables beyond Rademacher

## Confidence
- **High Confidence**: O(N) time complexity claim for sparse graphs and unbiasedness of the estimator (given correct implementation)
- **Medium Confidence**: Practical performance improvements (27× faster) which depend on specific dataset characteristics and implementation optimizations
- **Low Confidence**: Robustness across diverse graph types beyond those tested, particularly for dense graphs or graphs with complex labeling schemes

## Next Checks
1. **Implementation Verification**: Implement a basic version of GVoys on small graphs (N≤50) and verify that the empirical mean converges to the exact RWK value as the number of random walks increases, confirming unbiasedness.
2. **Scalability Stress Test**: Test GVoys on graphs with 10^4 to 10^5 nodes to empirically verify the O(N) scaling claim, measuring both runtime and approximation error across different sparsity levels.
3. **Robustness Assessment**: Apply GVoys to datasets with varying characteristics (labeled vs unlabeled, different edge densities) to evaluate sensitivity to graph properties and identify potential failure modes.