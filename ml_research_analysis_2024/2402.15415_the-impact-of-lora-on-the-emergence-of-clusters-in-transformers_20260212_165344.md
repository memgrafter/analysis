---
ver: rpa2
title: The Impact of LoRA on the Emergence of Clusters in Transformers
arxiv_id: '2402.15415'
source_url: https://arxiv.org/abs/2402.15415
tags:
- have
- tokens
- attention
- lora
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies the mathematical framework of Transformers as
  interacting particle systems to study the effects of LoRA fine-tuning on token clustering
  behavior. The authors prove that while token representations with identical initial
  conditions can diverge significantly over long time scales, they remain close over
  shorter intervals.
---

# The Impact of LoRA on the Emergence of Clusters in Transformers

## Quick Facts
- **arXiv ID**: 2402.15415
- **Source URL**: https://arxiv.org/abs/2402.15415
- **Reference count**: 40
- **Primary result**: Low-rank perturbations in attention matrices lead to phase transitions in token clustering dynamics, enabling new cluster formation while preserving short-term stability.

## Executive Summary
This paper develops a mathematical framework for analyzing Transformers as interacting particle systems to study how LoRA fine-tuning affects token clustering behavior. The authors prove that while token representations with identical initial conditions can diverge significantly over long time scales, they remain close over shorter intervals. They demonstrate that low-rank perturbations in attention matrices lead to a phase transition in clustering dynamics, with new clusters forming after an initial period of similarity to the original model. The paper shows that low-rank attention matrices can mitigate the curse of dimensionality and that LoRA can learn new token representations orthogonal to original ones without forgetting existing representations. Empirical results on modular arithmetic tasks and analysis of real models (BERT, Llama2) confirm these theoretical findings, showing that deeper models exhibit more unstable representations during LoRA fine-tuning and that many attention matrices in practice are indeed low-rank.

## Method Summary
The paper applies the mathematical framework of Transformers as interacting particle systems, modeling token dynamics through continuous attention mechanisms. The authors analyze self-attention dynamics with and without LoRA perturbations, proving stability results using Wasserstein distances and studying clustering behavior through numerical simulations. They implement custom Transformer architectures trained on modular arithmetic tasks and apply LoRA fine-tuning to observe degradation of original representations. The theoretical analysis involves continuity equations, push-forward measures through attention kernels, and spectrum analysis of attention matrices in pre-trained models.

## Key Results
- Low-rank perturbations in attention matrices preserve short-term stability while enabling long-term divergence in token clustering
- Low-rank attention matrices can mitigate the curse of dimensionality by concentrating clustering in lower-dimensional subspaces
- LoRA fine-tuning can learn new token representations orthogonal to pre-trained weights without forgetting existing representations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Low-rank perturbations in attention matrices preserve short-term stability while enabling long-term divergence in token clustering.
- **Mechanism**: The continuous attention dynamics act as an interacting particle system where small perturbations in the attention matrices (LoRA) cause trajectories to remain close over short intervals but diverge over long timescales, enabling new cluster formation.
- **Core assumption**: The attention kernel satisfies Lipschitz continuity in the space of measures, ensuring well-posedness of the continuity equation.
- **Evidence anchors**:
  - [abstract] "while the clusters within a modified attention matrix dynamics can exhibit significant divergence from the original over extended periods, they maintain close similarities over shorter intervals"
  - [section 4] "Our findings reveal that even if long-term dynamics of tokens with the same initial conditions diverge, on a short-term scale, the two trajectories are close"
  - [corpus] Weak evidence - no direct neighbor papers discussing this specific stability-divergence trade-off
- **Break condition**: If the Lipschitz constant of the attention kernel becomes too large relative to the perturbation size, the stability guarantee breaks down.

### Mechanism 2
- **Claim**: Low-rank attention matrices (KᵀQ) can mitigate the curse of dimensionality by concentrating clustering behavior in lower-dimensional subspaces.
- **Mechanism**: When the attention matrix A = KᵀQ is low-rank, tokens decompose into components in the image of A and its orthogonal complement. The dynamics in the orthogonal complement converge to a single point while clustering occurs in the lower-dimensional image subspace.
- **Core assumption**: The Value matrix V has spectrum contained in positive reals, ensuring convergence properties.
- **Evidence anchors**:
  - [section 6.1] "we take a closer look at how clusters form when A is low-rank, aiming to understand the geometry and structure of these clusters in more detail"
  - [section 6.1] "the clustering structure will be in a low-dimensional space, and we can avoid directions that are not significant and avoid the curse of dimensionality"
  - [corpus] Weak evidence - neighbor papers don't discuss dimensionality reduction through low-rank attention
- **Break condition**: If the rank of A becomes too large relative to the ambient dimension, the dimensionality reduction benefit diminishes.

### Mechanism 3
- **Claim**: LoRA fine-tuning can learn new token representations orthogonal to pre-trained weights without forgetting existing representations.
- **Mechanism**: By constraining LoRA matrices to have images orthogonal to pre-trained parameters (Im(KA) ⊂ Im(K)⊥, Im(QA) ⊂ Im(Q)⊥), the fine-tuned model learns representations in complementary subspaces while preserving original representations through the frozen pre-trained weights.
- **Core assumption**: The initialization satisfies appropriate separation conditions along the orthogonal direction.
- **Evidence anchors**:
  - [section 7] "demonstrate that low-rank perturbations in parameters influence the representations of tokens within Transformers"
  - [section 7] "LoRA algorithm is capable of generating new representations for tokens, which exhibit near orthogonality to the initial representations"
  - [corpus] Weak evidence - no neighbor papers discussing orthogonal representation learning in LoRA
- **Break condition**: If the orthogonal constraint is violated or the initialization doesn't satisfy separation conditions, representation forgetting may occur.

## Foundational Learning

- **Concept**: Interacting particle systems and their continuum limits
  - **Why needed here**: The paper models token dynamics as particles interacting through attention mechanisms, requiring understanding of how discrete particle systems converge to continuous dynamics
  - **Quick check question**: Can you explain how the empirical measure of tokens evolves according to a continuity equation?

- **Concept**: Optimal transport and Wasserstein distances
  - **Why needed here**: The paper uses Wasserstein distances to quantify differences between token distributions and prove stability results
  - **Quick check question**: What does W₂(µ, ν) measure between two probability measures µ and ν?

- **Concept**: Low-rank matrix perturbations and their spectral properties
  - **Why needed here**: LoRA operates by adding low-rank perturbations to attention matrices, requiring understanding of how such perturbations affect matrix spectra and dynamics
  - **Quick check question**: How does adding a rank-r perturbation to an n×n matrix affect its rank and eigenvalues?

## Architecture Onboarding

- **Component map**: K, Q, V matrices -> attention kernel -> token dynamics ODE -> measure push-forward -> clustering behavior
- **Critical path**: Initialize tokens and pre-trained attention matrices -> Apply LoRA low-rank perturbations -> Compute token trajectories -> Analyze clustering behavior -> Verify theoretical predictions
- **Design tradeoffs**: Rank of LoRA matrices vs. representational capacity -> Perturbation size vs. stability of original representations -> Computational efficiency vs. clustering quality -> Theoretical tractability vs. practical relevance
- **Failure signatures**: Exponential Wasserstein distance growth indicates LoRA perturbation too large -> No clustering emergence suggests V spectrum violates positivity assumptions -> No token convergence indicates initialization violates separation conditions -> Vacuous stability bounds suggest Lipschitz constant too large
- **First 3 experiments**:
  1. Implement basic attention dynamics with I₂ matrices and verify clustering behavior
  2. Add small low-rank perturbations and measure short-term stability using W₂ distance
  3. Vary perturbation rank and size to observe trade-off between new cluster formation and stability

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Under what precise conditions do the clustered representations learned by LoRA converge to the same limiting geometry as the original model, versus forming entirely new clusters?
- **Basis in paper**: [explicit] The paper establishes phase transition behavior where perturbed dynamics can either maintain similarity to original clustering or form new clusters, depending on parameter differences and time scales.
- **Why unresolved**: The paper provides theoretical bounds on when clustering divergence occurs but does not give precise conditions that guarantee convergence to the same limiting geometry versus new clustering.
- **What evidence would resolve it**: Empirical studies varying parameter perturbations systematically and measuring clustering similarity metrics at different time scales, combined with theoretical analysis of when the assumptions for convergence versus divergence are satisfied.

### Open Question 2
- **Question**: How do low-rank perturbations in attention matrices interact with the curse of dimensionality in high-dimensional token spaces, and what is the precise dimensionality threshold where this effect becomes significant?
- **Basis in paper**: [explicit] The paper demonstrates that low-rank attention matrices can circumvent the curse of dimensionality and proves that rank-one attention matrices lead to two-point clustering.
- **Why unresolved**: While the paper shows this effect exists, it does not quantify the dimensionality threshold or provide a precise mathematical characterization of when low-rank perturbations effectively mitigate high-dimensional challenges.
- **What evidence would resolve it**: Systematic experiments varying embedding dimensions and rank of perturbations, measuring clustering quality and computational efficiency, combined with theoretical analysis of information preservation in low-rank subspaces.

### Open Question 3
- **Question**: What is the precise mechanism by which LoRA fine-tuning enables orthogonal representation learning without forgetting existing representations, and how can this be characterized mathematically?
- **Basis in paper**: [explicit] The paper shows LoRA can learn new representations orthogonal to original ones while preserving existing representations, but the mathematical characterization of this mechanism is incomplete.
- **Why unresolved**: The paper provides empirical evidence and some theoretical results but lacks a complete mathematical framework explaining how orthogonality preservation occurs during the fine-tuning process.
- **What evidence would resolve it**: Mathematical analysis of the optimization landscape during LoRA fine-tuning, measuring angles between original and learned representations, combined with experiments varying initialization and training procedures to test the robustness of orthogonality preservation.

## Limitations

- The theoretical framework assumes idealized conditions that may not fully capture real-world model behavior, particularly regarding Lipschitz continuity of attention kernels.
- The experimental validation focuses primarily on toy models and specific tasks (modular arithmetic) rather than comprehensive evaluation across diverse real-world applications.
- The connection between mathematical abstractions and actual transformer behavior in large-scale models remains partially speculative, particularly regarding stability guarantees under realistic fine-tuning conditions.

## Confidence

**High Confidence**: The theoretical results on short-term stability of token dynamics under LoRA perturbations are well-supported by the mathematical framework and numerical experiments.

**Medium Confidence**: The empirical observations about attention matrix spectra in pre-trained models and their implications for clustering behavior are supported by data but lack comprehensive analysis across model families and scales.

**Low Confidence**: The orthogonal representation learning claims for LoRA fine-tuning, while theoretically interesting, have limited empirical validation beyond the modular arithmetic experiments.

## Next Checks

**Validation 1**: Test the stability-divergence trade-off on larger pre-trained models (beyond Llama2 7B) across different tasks to verify whether the theoretical bounds hold at scale and whether the observed behavior generalizes to more complex objectives.

**Validation 2**: Conduct ablation studies varying the rank and initialization of LoRA matrices to systematically map the phase transition behavior and identify the precise conditions under which new clusters form versus when stability breaks down.

**Validation 3**: Evaluate the orthogonal representation learning mechanism on standard benchmarks (e.g., GLUE, SuperGLUE) to assess whether the theoretical benefits translate to measurable performance improvements in practical fine-tuning scenarios.