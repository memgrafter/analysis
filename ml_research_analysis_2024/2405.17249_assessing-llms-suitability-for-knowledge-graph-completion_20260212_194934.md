---
ver: rpa2
title: Assessing LLMs Suitability for Knowledge Graph Completion
arxiv_id: '2405.17249'
source_url: https://arxiv.org/abs/2405.17249
tags:
- knowledge
- llms
- accuracy
- prompts
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether large language models (LLMs) can\
  \ perform knowledge graph completion (KGC) tasks. Three LLMs\u2014Mixtral-8x7b,\
  \ GPT-3.5-Turbo, and GPT-4o\u2014are evaluated on two custom datasets derived from\
  \ a task-oriented dialogue system."
---

# Assessing LLMs Suitability for Knowledge Graph Completion

## Quick Facts
- **arXiv ID**: 2405.17249
- **Source URL**: https://arxiv.org/abs/2405.17249
- **Reference count**: 17
- **Primary result**: GPT-4o demonstrates the most consistent performance for knowledge graph completion, with One-Shot Chain of Thought prompting providing significant improvements over Zero-Shot approaches.

## Executive Summary
This paper investigates whether large language models can effectively perform knowledge graph completion (KGC) tasks by extracting new triples from task-oriented dialogues. Three LLMs (Mixtral-8x7b, GPT-3.5-Turbo, and GPT-4o) are evaluated on custom datasets using various prompt styles under Zero- and One-Shot conditions. The study finds that GPT-4o consistently outperforms the other models, with One-Shot prompting showing substantial improvements over Zero-Shot approaches. Chain of Thought prompting proves particularly effective for Zero-Shot scenarios. The evaluation employs both strict and flexible metrics, with the latter allowing for minor formatting errors and acceptable alternative triples.

## Method Summary
The researchers developed two custom datasets (PDL and RDL) derived from a task-oriented dialogue system, containing 22 test instances for validation. They tested three LLMs (Mixtral-8x7b, GPT-3.5-Turbo, and GPT-4o) using seven different prompt styles, including Standard, Chain of Thought, and Few-Shot approaches. Models were evaluated under both Zero-Shot and One-Shot prompting conditions. Performance was measured using strict and flexible metrics, where the flexible metric accounts for minor formatting errors and accepts equivalent alternative triples. The study compares model accuracy across different prompt styles and shot conditions to determine optimal approaches for KGC tasks.

## Key Results
- GPT-4o consistently achieves the highest accuracy across both datasets and metrics, demonstrating superior performance for knowledge graph completion
- One-Shot prompting with Chain of Thought significantly improves performance compared to Zero-Shot approaches, particularly for GPT-4o
- Mixtral-8x7b struggles with formatting requirements and produces lower quality outputs compared to GPT models, despite being competitive in some scenarios

## Why This Works (Mechanism)
The study demonstrates that LLMs can effectively extract structured knowledge triples from unstructured dialogue data when provided with appropriate prompting strategies. The effectiveness stems from the models' ability to understand context, reason through dialogue content, and generate structured output when guided by Chain of Thought prompting. The One-Shot approach provides exemplars that help the model understand the expected output format and reasoning process, leading to more accurate triple extraction.

## Foundational Learning

**Knowledge Graph Completion (KGC)**: The task of inferring missing relationships between entities in a knowledge graph from available data sources.
*Why needed*: Understanding the core problem being solved and its significance in knowledge base construction
*Quick check*: Can identify what constitutes a valid triple and understand why missing links need to be discovered

**Chain of Thought Prompting**: A technique where models are guided to reason step-by-step before providing a final answer.
*Why needed*: Explains why this particular prompting strategy was effective for the task
*Quick check*: Can explain how step-by-step reasoning improves model output quality

**Zero-Shot vs One-Shot Learning**: Zero-Shot uses no examples in prompts, while One-Shot provides a single exemplar for the task.
*Why needed*: Critical for understanding the experimental design and performance differences
*Quick check*: Can distinguish between the two approaches and predict when each would be more effective

## Architecture Onboarding

**Component Map**: Dialogue Data -> LLM Processing -> Triple Extraction -> Evaluation Metrics
**Critical Path**: Dialogue input → Prompt Engineering → LLM generation → Triple validation → Performance scoring
**Design Tradeoffs**: Balance between prompt complexity (Chain of Thought adds length but improves accuracy) versus computational cost and latency
**Failure Signatures**: Format compliance issues, incorrect triple generation, hallucination of non-existent relationships
**First Experiments**: 1) Test Zero-Shot Standard prompt on GPT-4o with one dialogue instance, 2) Test One-Shot Chain of Thought on Mixtral with same instance, 3) Compare strict vs flexible metric outputs for identical prompt

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to two custom datasets from a single dialogue system, restricting generalizability to other knowledge graph domains
- Small test set of only 22 instances raises concerns about statistical significance of results
- No comparison with traditional KGC baseline models to establish relative performance

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| GPT-4o performance superiority | High |
| One-Shot Chain of Thought effectiveness | Medium |
| Cost-effectiveness of GPT-3.5-Turbo | Medium |

## Next Checks
1. Test the same models and prompts on at least three additional knowledge graph datasets from different domains (e.g., biomedical, social networks, product relationships) to assess generalizability.

2. Expand the test set to 100+ instances per dataset and conduct statistical significance testing between model performance differences under different prompt conditions.

3. Compare LLM performance against at least two state-of-the-art traditional KGC models using the same datasets and evaluation metrics to establish baseline benchmarks.