---
ver: rpa2
title: 'From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed
  Sentences'
arxiv_id: '2405.05572'
source_url: https://arxiv.org/abs/2405.05572
tags:
- code-mixed
- acceptability
- sentences
- language
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of evaluating acceptability
  in code-mixed text by constructing Cline, a novel dataset of human acceptability
  judgments for English-Hindi code-mixed sentences. Traditional code-mixing metrics
  like CMI, switch points, and burstiness show limited correlation with human judgments,
  revealing their inadequacy in assessing acceptability.
---

# From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences

## Quick Facts
- arXiv ID: 2405.05572
- Source URL: https://arxiv.org/abs/2405.05572
- Reference count: 40
- Primary result: Fine-tuned multilingual LLMs outperform code-mixing metrics and human baselines in predicting acceptability of English-Hindi code-mixed sentences

## Executive Summary
This study addresses the challenge of evaluating acceptability in code-mixed text by constructing Cline, a novel dataset of human acceptability judgments for English-Hindi code-mixed sentences. Traditional code-mixing metrics like CMI, switch points, and burstiness show limited correlation with human judgments, revealing their inadequacy in assessing acceptability. To address this, the study introduces fine-tuned multilingual language models, which outperform both code-mixing metrics and human baselines. Among the models tested, decoder-only architectures, particularly Llama 3.2-3B, achieve the highest performance, while normalization significantly improves results. The findings also demonstrate that acceptability models trained on English-Hindi data transfer effectively to English-Telugu, providing a promising direction for cross-linguistic applications. The work highlights the need for quality-controlled code-mixed corpora and offers a framework for future research in code-mixing evaluation.

## Method Summary
The study constructs the Cline dataset containing 16,642 English-Hindi code-mixed sentences with human acceptability ratings (1-5 scale) from three annotators. Sentences were sourced from synthetic generation (GCM toolkit) and social media (Twitter). The research compares code-mixing metrics (CMI, switch points, burstiness) against fine-tuned multilingual LLMs (XLM-R, BERT, mT5, mBART, Llama 3.2, Qwen 2.5, Phi-3) and a zero-shot ChatGPT baseline. Models were trained using MSE loss with Adam optimizer and linear weight decay, evaluating performance using RMSE and MAE metrics on acceptability predictions.

## Key Results
- Code-mixing metrics (CMI, switch points, burstiness) show low correlation with human acceptability judgments
- Fine-tuned multilingual LLMs outperform both code-mixing metrics and human baselines
- Decoder-only architectures (Llama 3.2-3B) achieve the highest performance among model families
- Normalization (converting romanized Hindi to native script) significantly improves MLLM performance
- Acceptability models trained on English-Hindi data transfer effectively to English-Telugu

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code-mixing metrics alone cannot reliably predict acceptability because they only capture surface-level linguistic alternation, not deeper semantic coherence or syntactic naturalness.
- Mechanism: Acceptability depends on holistic coherence, not just frequency of language switches; metrics ignore whether the resulting utterance follows natural syntactic patterns in either language.
- Core assumption: Humans judge acceptability based on naturalness and grammaticality, not simply the degree of mixing.
- Evidence anchors:
  - [abstract] "popular code-mixing metrics such as CMI, Number of Switch Points, Burstiness ... have low correlation with human acceptability judgements"
  - [section] "code-mixing metrics do not show very low correlations with human acceptability ratings"
  - [corpus] Limited (abstract-level evidence only)
- Break condition: If acceptability were purely defined by mixing ratio, metrics would correlate strongly with human ratings.

### Mechanism 2
- Claim: Fine-tuned multilingual LLMs outperform code-mixing metrics because they implicitly encode both syntactic and semantic knowledge across languages.
- Mechanism: Pre-trained MLLMs have learned cross-lingual patterns and semantic plausibility; fine-tuning on acceptability data allows them to weigh both surface features and deeper coherence cues.
- Core assumption: MLLMs' pre-training exposed them to enough code-mixed and romanized text to internalize naturalness patterns.
- Evidence anchors:
  - [abstract] "fine-tuned pre-trained Multilingual Large Language Models (MLLMs) ... outperform both code-mixing metrics and human baselines"
  - [section] "MLLMs' surprisal scores have better correlation with acceptability ratings as compared to code-mixing metrics"
  - [corpus] Weak (no direct corpus-level evidence cited)
- Break condition: If MLLMs lacked exposure to code-mixed data during pre-training, fine-tuning gains would be minimal.

### Mechanism 3
- Claim: Normalization (converting romanized Hindi to native script) improves MLLM performance because tokenizers are optimized for native script representation.
- Mechanism: Native script yields lower tokenizer fertility and perplexity, leading to more accurate predictions of acceptability.
- Core assumption: MLLMs were pre-trained predominantly on native-script text, not romanized forms.
- Evidence anchors:
  - [section] "normalisation always improves the acceptability predictions of MLLMs"
  - [section] "normalized text consistently results in lower perplexity compared to romanized text"
  - [corpus] Limited (analysis based on tokenizer behavior, not raw corpus stats)
- Break condition: If models were trained equally on romanized and native forms, normalization would not yield consistent gains.

## Foundational Learning

- Concept: Inter-annotator agreement and reliability
  - Why needed here: Ensures the acceptability ratings used for training and evaluation are consistent and trustworthy across multiple raters.
  - Quick check question: What ICC score range indicates "good reliability" for acceptability ratings?

- Concept: Code-mixing metrics and their limitations
  - Why needed here: Understanding why traditional metrics fail to capture acceptability is central to motivating the use of ML approaches.
  - Quick check question: Which code-mixing metric shows the strongest (though still weak) correlation with acceptability?

- Concept: Fine-tuning vs. zero-shot prompting
  - Why needed here: The paper compares fine-tuned MLLMs to zero-shot ChatGPT to demonstrate the value of task-specific training data.
  - Quick check question: Why does fine-tuning on en-hi data improve performance on en-te code-mixed acceptability prediction?

## Architecture Onboarding

- Component map: Raw code-mixed text -> Normalization -> Feature extraction (metrics) -> Annotation -> Train/validation/test split
- Critical path: 1) Annotate acceptability ratings for code-mixed sentences; 2) Train metric-based MLP baseline; 3) Fine-tune MLLMs on acceptability labels; 4) Evaluate zero-shot vs. fine-tuned performance; 5) Test cross-lingual transfer to en-te
- Design tradeoffs:
  - Romanized vs. normalized text: Native script improves performance but may reduce coverage of real-world informal text
  - Encoder-only vs. decoder-only vs. encoder-decoder: Decoder-only (Llama) performs best, possibly due to larger capacity or training data exposure
  - Few-shot vs. fine-tuning: Fine-tuning consistently outperforms few-shot prompting, indicating data-driven learning is crucial
- Failure signatures:
  - Over-prediction for low-rated sentences (common in both XLM-R and Llama)
  - High error variance for OSN data vs. GCM data, suggesting social media noise is harder to model
  - Poor zero-shot transfer to unseen language pairs (en-te) without fine-tuning
- First 3 experiments:
  1. Train MLP on code-mixing metrics only; compare to random baseline
  2. Fine-tune XLM-R on en-hi acceptability data; evaluate on OSN vs. GCM
  3. Test zero-shot ChatGPT vs. fine-tuned Llama 3B on en-hi acceptability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do demographic factors such as age, bilingual proficiency, and regional variations influence acceptability judgments of code-mixed text?
- Basis in paper: [inferred] from limitations section discussing the need for future work on demographic factors and annotator pool constraints
- Why unresolved: Current study is limited to undergraduate students from a single university, potentially skewing judgments and not capturing full complexity
- What evidence would resolve it: Controlled studies with stratified sampling across diverse age groups, bilingual proficiency levels, and geographical regions, analyzing how these factors correlate with acceptability thresholds

### Open Question 2
- Question: Can a generalized model of code-mixed text acceptability be developed that works across diverse language pairs beyond English-Hindi and English-Telugu?
- Basis in paper: [explicit] from limitations section stating the current study is limited to English-Hindi and English-Telugu, and that patterns may not hold for other language pairs
- Why unresolved: Code-mixing is a complex sociolinguistic phenomenon influenced by numerous factors beyond surface-level form, and patterns vary significantly across language pairs
- What evidence would resolve it: Development and evaluation of models on multiple diverse language pairs, with comparative analysis of transferability and performance across different linguistic contexts

### Open Question 3
- Question: What are the linguistic and contextual drivers of model decisions in acceptability prediction, and how can interpretability techniques be improved for transformer-based models on code-mixed text?
- Basis in paper: [explicit] from discussion section highlighting limitations of current interpretability techniques (LIME, SHAP, attention visualization) and the need for advanced explainability methods
- Why unresolved: Current dataset lacks detailed rationales or features explaining acceptability judgments, and existing techniques face challenges with transformer architectures and code-mixed text
- What evidence would resolve it: Development of datasets with richer annotations capturing reasoning behind judgments, and creation of advanced explainability methods tailored to transformer architectures for code-mixed text

## Limitations
- The study uses a limited annotator pool of undergraduate students from a single university, potentially introducing bias in acceptability judgments
- Code-mixing metrics show low but not negligible correlation with acceptability, suggesting some surface-level features may still capture partial acceptability information
- Cross-lingual transfer success is demonstrated only between English-Hindi and English-Telugu, with unknown generalizability to other language pairs

## Confidence

**High Confidence** (Level 1-2 uncertainty):
- Code-mixing metrics show low correlation with human acceptability judgments
- Fine-tuned multilingual LLMs outperform both metrics and human baselines on the en-hi task
- Normalization improves MLLM performance due to tokenization advantages

**Medium Confidence** (Level 3-4 uncertainty):
- Decoder-only architectures (Llama 3.2-3B) perform best among model families
- Acceptability models trained on en-hi data transfer effectively to en-te
- The Cline dataset represents a meaningful advance in code-mixed evaluation resources

## Next Checks

1. Establish inter-annotator reliability: Calculate and report ICC (Intraclass Correlation Coefficient) scores for the human annotators across different acceptability rating ranges to determine if the human baseline is truly reliable enough to serve as ground truth.

2. Test cross-linguistic generalization: Evaluate the en-hi trained models on at least two additional language pairs (e.g., en-es and en-zh) to determine if the transfer success is generalizable beyond the en-te case.

3. Analyze failure cases: Conduct a qualitative error analysis on the 10-20% of cases where models most significantly disagree with human ratings to identify systematic patterns in what types of code-mixing the models struggle to evaluate correctly.