---
ver: rpa2
title: 'STEVE-Audio: Expanding the Goal Conditioning Modalities of Embodied Agents
  in Minecraft'
arxiv_id: '2412.00949'
source_url: https://arxiv.org/abs/2412.00949
tags:
- audio
- clip
- minecraft
- modalities
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends the prompting modalities of generative instruction-following
  agents by learning mappings from new modalities to the latent goal space of existing
  agents. Applied to the Minecraft domain, the method adds audio conditioning to the
  STEVE-1 agent by training an Audio-Video CLIP foundation model and an audio-to-visual
  latent space prior.
---

# STEVE-Audio: Expanding the Goal Conditioning Modalities of Embodied Agents in Minecraft

## Quick Facts
- **arXiv ID**: 2412.00949
- **Source URL**: https://arxiv.org/abs/2412.00949
- **Reference count**: 39
- **Primary result**: Audio-conditioned STEVE-Audio agent outperforms text and visual baselines on Minecraft collection tasks by 1.6×-17.7×

## Executive Summary
This paper extends the STEVE-1 embodied agent to accept audio prompts by learning mappings from audio to the agent's latent goal space. The method trains an Audio-Video CLIP foundation model and a CVAE-based prior to map audio embeddings to the MineCLIP latent space that STEVE-1 expects. Applied to Minecraft collection tasks, the audio-conditioned agent significantly outperforms both text and visual versions, collecting 6.4×-17.7× more items across various tasks. The authors attribute this improvement to higher correlation between audio-video pairs versus text-video pairs in training data, though this hypothesis remains empirically unverified.

## Method Summary
The approach extends STEVE-1 to audio prompting through a two-stage training process. First, an Audio-Video CLIP model is trained using frozen AST audio and MineCLIP video encoders with learned transformation networks. Second, a CVAE prior network learns to map audio embeddings from the Audio-Video CLIP space to MineCLIP embeddings. The STEVE-1 policy, trained on MineCLIP latent space, is then conditioned on these mapped embeddings. The method requires Minecraft YouTube videos (25 hours minimum, 600 hours planned) without commentary, processed into 1-second clips with 75% overlap at 32fps and 16kHz audio, plus task-specific videos for augmentation.

## Key Results
- Audio-conditioned agent collected 6.4× more wood, 7.25× more dirt, 5.1× more sand, and 1.6× more leaves compared to visual-conditioned STEVE-1
- Audio-conditioned agent collected 1.8× more wood, 8× more dirt, 2.2× more seeds, 17.7× more sand, and 2.9× more leaves compared to text-conditioned STEVE-1
- Audio prompting required less prompt engineering, attributed to higher audio-video correlation in training data
- Performance degraded on tasks with ambiguous or underspecified audio samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extending STEVE-1 to new modalities works by training a modality-specific CLIP model and a prior to map embeddings into the original STEVE-1 latent space.
- Mechanism: The new CLIP model learns audio-video alignment in a new latent space. The prior network (CVAE) learns a mapping from this new latent space to the MineCLIP latent space that the STEVE-1 policy expects.
- Core assumption: The new CLIP embeddings are semantically aligned with the original latent space such that a learned mapping is possible.
- Evidence anchors: [abstract] "we present a methodology to extend the control modalities by learning a mapping from new input modalities to the latent goal space of the agent." [section] "we must train a prior which maps audio embeddings from the Audio-Video CLIP model to visual MineCLIP embeddings."
- Break condition: If the new CLIP embeddings are too dissimilar from the original latent space, the prior cannot learn a reliable mapping.

### Mechanism 2
- Claim: Audio-conditioned STEVE-Audio outperforms text and visual versions because audio-video pairs are more naturally correlated than text-video pairs in training data.
- Mechanism: Higher correlation in audio-video pairs leads to more semantically meaningful embeddings, reducing ambiguity in goal specification.
- Core assumption: The correlation strength between modalities in training data directly impacts downstream policy performance.
- Evidence anchors: [section] "audio samples are usually very similar across different demonstrations of the same task... audio embeddings from our Audio-Video CLIP model could be more semantically relevant representations of the task." [section] "audio and video are both highly correlated, naturally co-occurring modalities."
- Break condition: If the audio-video dataset contains ambiguous or task-irrelevant audio, correlation advantage disappears.

### Mechanism 3
- Claim: The CVAE prior architecture effectively maps embeddings between latent spaces because it learns conditional distributions rather than deterministic mappings.
- Mechanism: The CVAE encoder maps from the new latent space to a latent distribution, and the decoder reconstructs the target latent space embedding, allowing for variability and uncertainty handling.
- Core assumption: The latent spaces are structured similarly enough for the CVAE to learn a meaningful transformation.
- Evidence anchors: [section] "The architecture of our prior... is implemented as a CV AE [29, 30] where the encoder and decoder are both two-layer MLPs with a hidden dimension of 256 and layer normalization between layers."
- Break condition: If the latent spaces have fundamentally different structures, the CVAE cannot learn an effective mapping.

## Foundational Learning

- Concept: Contrastive learning in CLIP models
  - Why needed here: The Audio-Video CLIP model is trained using contrastive objectives to align audio and video embeddings in a shared latent space.
  - Quick check question: What loss function is typically used in CLIP-style models to align two modalities?

- Concept: Conditional Variational Autoencoders (CVAEs)
  - Why needed here: The prior network uses a CVAE architecture to map from the new modality's latent space to the STEVE-1 policy's latent space.
  - Quick check question: How does a CVAE differ from a standard autoencoder in terms of latent space representation?

- Concept: Multimodal representation learning
  - Why needed here: The entire approach relies on creating meaningful representations across different sensory modalities that can be used for downstream control.
  - Quick check question: What are the key challenges in aligning representations from different modalities?

## Architecture Onboarding

- Component map: Audio-Video CLIP model (frozen AST audio encoder + frozen MineCLIP video encoder + learned transformation networks) -> Prior network (CVAE with 2-layer MLPs, 256 hidden dims) -> STEVE-1 policy

- Critical path:
  1. Train Audio-Video CLIP model on Minecraft audio-video dataset
  2. Train prior network to map Audio-Video CLIP embeddings to MineCLIP embeddings
  3. Use trained components to condition STEVE-1 policy on audio prompts

- Design tradeoffs:
  - Using frozen encoders limits flexibility but reduces training complexity
  - 1-second clips with 75% overlap provide dense sampling but increase dataset size
  - CVAE prior handles uncertainty but adds training complexity compared to deterministic mapping

- Failure signatures:
  - Poor audio-conditioned performance suggests either CLIP model misalignment or prior mapping failure
  - Visual/audiovisual modality confusion indicates embedding space misalignment
  - Training instability in prior suggests latent space structural differences

- First 3 experiments:
  1. Train Audio-Video CLIP model and evaluate audio-video retrieval performance on held-out test set
  2. Train prior network and evaluate reconstruction quality of MineCLIP embeddings from audio embeddings
  3. Integrate components and evaluate audio-conditioned STEVE-1 on simple collection tasks before full benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum amount of task-specific data needed to achieve performance comparable to the full 600-hour dataset?
- Basis in paper: [explicit] The paper mentions augmenting the YouTube dataset with short 10-20 minute videos for task-specific relevance, and notes that YouTube data alone yielded underperforming results while adding task-specific data improved performance significantly.
- Why unresolved: The paper does not quantify how much task-specific data is necessary for good performance or what the performance trade-off curve looks like as task-specific data increases.
- What evidence would resolve it: Systematic experiments varying the amount of task-specific data while keeping the YouTube base constant, measuring performance on the evaluated tasks.

### Open Question 2
- Question: How does the Audio-Video CLIP model's performance degrade when the audio-video correlation is artificially reduced?
- Basis in paper: [inferred] The authors hypothesize that audio prompting works better because audio-video pairs are more highly correlated than text-video pairs in the training data, but do not test this hypothesis by manipulating correlation levels.
- Why unresolved: The paper establishes correlation as a hypothesis for better performance but doesn't provide controlled experiments to test whether correlation is actually the causal factor.
- What evidence would resolve it: Experiments creating datasets with varying degrees of audio-video correlation (e.g., mismatched audio) and measuring how performance changes with correlation levels.

### Open Question 3
- Question: What is the performance impact of using different audio encoder architectures (beyond AST) for the Audio-Video CLIP model?
- Basis in paper: [explicit] The authors use the Audio Spectrogram Transformer (AST) model specifically, stating it was originally trained for audio classification tasks, but do not compare against alternative audio encoders.
- Why unresolved: The choice of AST appears arbitrary, and the paper doesn't investigate whether other audio encoder architectures might yield better or worse performance.
- What evidence would resolve it: Experiments replacing the AST encoder with other pretrained audio encoders (e.g., Wav2Vec2, PANNs) while keeping all other components constant.

### Open Question 4
- Question: How does the audio-conditioned agent perform on long-horizon tasks compared to short-horizon tasks?
- Basis in paper: [inferred] The evaluation is limited to short-horizon item-collection tasks (2 minutes each), but the authors mention that text can specify complex long-term goals that audio cannot.
- Why unresolved: The paper demonstrates audio superiority on short-horizon tasks but doesn't explore whether this advantage extends to or diminishes in longer, more complex tasks.
- What evidence would resolve it: Evaluation on a benchmark of long-horizon Minecraft tasks (e.g., building structures, crafting items requiring multiple steps) comparing audio, text, and visual prompting performance.

## Limitations

- The claim that audio-video pairs have inherently higher correlation than text-video pairs lacks quantitative validation
- Performance improvements are measured against baseline agents without isolating whether gains come from better modality alignment or richer semantic information
- The CVAE prior architecture's sensitivity to hyperparameters remains unexplored
- Training data collection methodology raises questions about domain coverage and potential biases

## Confidence

- **High confidence**: The methodology for extending STEVE-1 to new modalities through latent space mapping is technically sound and follows established practices in multimodal learning. The experimental results showing audio-conditioned agent outperforming text and visual baselines on most collection tasks are clearly reported with appropriate statistical measures.
- **Medium confidence**: The core hypothesis that audio-video correlation leads to better performance is plausible but under-supported. The performance gains are substantial but could be influenced by factors beyond modality correlation, such as the nature of the collection tasks themselves being more naturally specified through audio cues.
- **Low confidence**: The claim that less prompt engineering is required for audio prompting is presented without systematic comparison of prompt complexity or engineering effort across modalities.

## Next Checks

1. **Modality Correlation Analysis**: Conduct quantitative analysis comparing audio-video, text-video, and visual-visual embedding similarity distributions on held-out validation sets to empirically verify the correlation advantage hypothesis.

2. **Prompt Engineering Comparison**: Design controlled experiments measuring the number of prompt variations needed to achieve stable performance across modalities, including objective metrics for prompt quality and complexity.

3. **Latent Space Mapping Sensitivity**: Systematically vary CVAE architecture parameters (layer sizes, latent dimensions) and measure the impact on both embedding reconstruction quality and downstream task performance to identify architectural sweet spots and failure modes.