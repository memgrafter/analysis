---
ver: rpa2
title: Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human
  Feedback
arxiv_id: '2402.04867'
source_url: https://arxiv.org/abs/2402.04867
tags: []
core_contribution: This paper introduces Multimodal Query Suggestion (MMQS), a task
  to generate query suggestions from user query images for search engines. It proposes
  RL4Sugg, a novel framework leveraging Large Language Models (LLMs) with Multi-Agent
  Reinforcement Learning from Human Feedback (RLHF) to optimize the generation process.
---

# Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback

## Quick Facts
- arXiv ID: 2402.04867
- Source URL: https://arxiv.org/abs/2402.04867
- Authors: Zheng Wang; Bingzheng Gan; Wei Shi
- Reference count: 40
- Key outcome: RL4Sugg achieves 18% improvement over best existing approach for multimodal query suggestion

## Executive Summary
This paper introduces Multimodal Query Suggestion (MMQS), a task that generates query suggestions from user query images for search engines. The authors propose RL4Sugg, a novel framework leveraging Large Language Models with Multi-Agent Reinforcement Learning from Human Feedback to optimize suggestion generation. The framework consists of two agents: Agent-I generates intentional candidate suggestions using RewardNet and PolicyNet, while Agent-D selects diverse suggestions from the candidates. Extensive experiments on two real-world datasets demonstrate RL4Sugg's effectiveness, with the method transferred into real-world search engine products yielding enhanced user engagement.

## Method Summary
RL4Sugg is a two-agent framework for multimodal query suggestion. Agent-I uses a RewardNet trained with multi-task learning (ISA, ISG, ISM) to generate informative rewards, then a PolicyNet trained with RLHF to generate intentional suggestions. Agent-D is a lightweight network trained with policy gradient to select diverse suggestions from Agent-I's candidates. The system processes query images through frozen vision encoders and learnable query embeddings, leveraging pre-trained multimodal representations while fine-tuning only the trainable agents. GPT-4 is used for automated data collection with a confidence threshold, reducing manual annotation effort while maintaining quality.

## Key Results
- RL4Sugg achieves 18% improvement compared to the best existing approach on multimodal query suggestion
- The framework successfully transfers to real-world search engine products with enhanced user engagement
- Extensive experiments on two real-world datasets demonstrate the effectiveness of the two-agent approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RL4Sugg achieves 18% improvement over baselines by using multi-task learning in the RewardNet to generate informative rewards for image-suggestion pairs
- Mechanism: The RewardNet optimizes three pre-training tasks (ISA, ISG, ISM) using a shared Q-Former architecture with learnable query embeddings. This multi-task approach allows the model to learn from different related tasks simultaneously, creating a more comprehensive reward signal that guides the PolicyNet toward generating intentional suggestions
- Core assumption: Learning multiple related tasks simultaneously creates a more robust reward signal than optimizing a single task
- Evidence anchors:
  - [abstract] "RL4Sugg achieves 18% improvement compared to the best existing approach"
  - [section] "We adopt multi-task learning for the RewardNet, optimizing three pre-training tasks: Image-Suggestion Alignment (ISA), Image-Suggestion Generation (ISG), and Image-Suggestion Matching (ISM)"
  - [corpus] Weak evidence - no directly comparable multi-task approaches in corpus papers
- Break condition: If the multi-task learning doesn't provide complementary information, the reward signal may become noisy and degrade PolicyNet performance

### Mechanism 2
- Claim: The two-agent system (Agent-I and Agent-D) explicitly optimizes both intentionality and diversity, avoiding the tradeoff where optimizing for one property degrades the other
- Mechanism: Agent-I generates candidate suggestions focused on intentionality using RLHF, while Agent-D selects diverse suggestions from these candidates using a sliding window algorithm with policy gradient. This separation allows each agent to specialize in one property without compromising the other
- Core assumption: Separating intentionality and diversity optimization into different agents prevents the model from exploiting shortcuts that maximize one property at the expense of the other
- Evidence anchors:
  - [abstract] "Agent-I generates intentional candidate suggestions... while Agent-D selects diverse suggestions from the candidates"
  - [section] "The Agent-D is trained to minimize the similarity between output suggestions, which ensures that the output suggestions are informative and provide various search aspects for users"
  - [corpus] Weak evidence - no directly comparable multi-agent approaches in corpus papers
- Break condition: If Agent-I generates too few candidates or Agent-D's diversity selection interferes with intentionality, the overall suggestion quality will degrade

### Mechanism 3
- Claim: Using GPT-4 for automated data collection with a confidence threshold balances efficiency and quality, enabling scalable training data preparation
- Mechanism: GPT-4 generates candidate suggestions and assigns confidence scores. Suggestions below a threshold (0.6) undergo human annotation, while higher-confidence suggestions are accepted directly. This hybrid approach reduces manual effort while maintaining annotation quality
- Core assumption: GPT-4 can generate high-quality suggestions and confidence estimates that correlate with human judgment, allowing effective automation of data collection
- Evidence anchors:
  - [abstract] "We leverage the current GPT language generation capabilities to automate the collection of image-suggestion pairs and user intent annotations based on potential clicks"
  - [section] "By setting a threshold value, suggestions with low confidence scores are identified for further manual annotation"
  - [corpus] No direct evidence - corpus papers focus on different aspects of query suggestion without automated data collection
- Break condition: If GPT-4's confidence estimates don't correlate with actual suggestion quality, the threshold-based mechanism will either accept poor suggestions or require excessive human annotation

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF enables the PolicyNet to generate suggestions that align with human preferences (clicks) rather than just optimizing for proxy metrics, which is crucial for capturing user intent in query suggestions
  - Quick check question: What distinguishes RLHF from standard reinforcement learning in this context, and why is human feedback particularly valuable for query suggestion tasks?

- Concept: Multi-task learning with shared representations
  - Why needed here: The RewardNet uses shared query embeddings across three tasks (ISA, ISG, ISM), allowing knowledge transfer between tasks and creating a more comprehensive reward signal for training the PolicyNet
  - Quick check question: How does sharing the Q-Former architecture across multiple tasks help the model learn better representations compared to training each task separately?

- Concept: Vision-language pre-training (VLP) with frozen components
  - Why needed here: RL4Sugg freezes image encoders and language models while fine-tuning only the lightweight agents, leveraging pre-trained multimodal representations while avoiding catastrophic forgetting of general vision-language capabilities
  - Quick check question: What are the advantages and potential limitations of freezing pre-trained vision and language models while only training the reward and policy networks?

## Architecture Onboarding

- Component map: Query image → Vision encoder → Learnable query embeddings → RewardNet (multi-task) → PolicyNet (RLHF) → Candidate suggestions → Agent-D (policy gradient) → Final diverse suggestions
- Critical path: Image → Vision encoder → Learnable query embeddings → RewardNet (multi-task) → PolicyNet (RLHF) → Candidate suggestions → Agent-D (policy gradient) → Final diverse suggestions
- Design tradeoffs: Using frozen pre-trained models provides strong multimodal representations but limits adaptability to task-specific nuances. The two-agent system adds complexity but enables explicit optimization of both intentionality and diversity. GPT-4 automation reduces annotation costs but introduces potential bias in generated suggestions
- Failure signatures: Poor intentionality suggests RewardNet or PolicyNet training issues. Low diversity indicates Agent-D problems or insufficient candidate generation. Performance degradation after deployment suggests cold-start issues requiring online learning
- First 3 experiments:
  1. Test RewardNet performance on the three pre-training tasks (ISA, ISG, ISM) to verify multi-task learning effectiveness
  2. Evaluate PolicyNet suggestion quality with and without RLHF to measure the impact of human feedback training
  3. Assess Agent-D diversity selection by comparing suggestion sets with different diversity thresholds and sliding window sizes

## Open Questions the Paper Calls Out

- Open Question 1: How does the performance of MMQS vary when applied to different types of images, such as those containing text, abstract concepts, or multiple objects?
  - Basis in paper: [inferred] The paper evaluates MMQS on Business and ImageNet datasets, but does not explore its performance across diverse image types
  - Why unresolved: The paper focuses on real-world datasets without analyzing the impact of image content on MMQS effectiveness
  - What evidence would resolve it: Conducting experiments on a dataset with varied image types and comparing MMQS performance across these categories

- Open Question 2: What is the impact of varying the number of candidate suggestions (N) on the overall quality and diversity of the final output in MMQS?
  - Basis in paper: [explicit] The paper mentions that Agent-D selects K diverse suggestions from N candidates, but does not explore the relationship between N and output quality
  - Why unresolved: The paper sets N as a fixed parameter without investigating its optimal value for different scenarios
  - What evidence would resolve it: Experimenting with different values of N and analyzing their effects on MMQS performance metrics

- Open Question 3: How does the performance of MMQS compare to human-generated query suggestions in terms of relevance and user satisfaction?
  - Basis in paper: [explicit] The paper mentions human annotation for data collection and evaluation but does not directly compare MMQS to human-generated suggestions
  - Why unresolved: The paper focuses on quantitative metrics and online A/B tests without qualitative comparison to human performance
  - What evidence would resolve it: Conducting a user study comparing MMQS suggestions to those generated by human experts

## Limitations

- The paper's 18% improvement claim combines results across multiple metrics without specifying which baseline this refers to
- The ablation study shows individual components contribute to performance but doesn't clearly isolate the contribution of the multi-task RewardNet versus the two-agent architecture
- Reliance on GPT-4 for data collection introduces potential bias that isn't fully characterized, and the confidence threshold approach may systematically filter certain types of suggestions

## Confidence

- Mechanism 1 (Multi-task learning): Medium - The improvement claim is supported, but the specific contribution of multi-task learning versus other architectural choices is unclear
- Mechanism 2 (Two-agent system): Low - The separation of intentionality and diversity is novel, but lacks direct comparison to single-agent approaches that might achieve similar results
- Mechanism 3 (GPT-4 automation): Medium - The threshold approach is reasonable, but the correlation between GPT-4 confidence scores and actual suggestion quality isn't empirically validated

## Next Checks

1. **Ablation of RewardNet tasks**: Train separate single-task versions of the RewardNet (only ISA, only ISG, only ISM) and compare their performance against the multi-task version to quantify the actual contribution of multi-task learning to the 18% improvement claim

2. **Direct comparison of agent architectures**: Implement a single-agent variant that jointly optimizes both intentionality and diversity, then compare its performance against the two-agent system to determine if the architectural separation is necessary or if simpler approaches could achieve similar results

3. **Analysis of GPT-4 confidence calibration**: Collect human judgments on a random sample of GPT-4 generated suggestions across the full confidence spectrum (not just thresholded samples) to measure how well GPT-4's confidence scores predict actual suggestion quality and identify potential systematic biases in the automated data collection process