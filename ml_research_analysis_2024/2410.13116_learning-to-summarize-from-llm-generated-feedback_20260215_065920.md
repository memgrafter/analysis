---
ver: rpa2
title: Learning to Summarize from LLM-generated Feedback
arxiv_id: '2410.13116'
source_url: https://arxiv.org/abs/2410.13116
tags:
- feedback
- summary
- summaries
- evaluation
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces FeedSum, a large-scale dataset of LLM-generated
  feedback for improving text summarization. The study investigates how feedback quality,
  dimensionality, and granularity impact preference learning, showing that high-quality,
  multi-dimensional, fine-grained feedback significantly improves summary generation.
---

# Learning to Summarize from LLM-generated Feedback

## Quick Facts
- arXiv ID: 2410.13116
- Source URL: https://arxiv.org/abs/2410.13116
- Reference count: 40
- Key outcome: FeedSum dataset enables smaller models to outperform larger ones through high-quality, multi-dimensional LLM feedback

## Executive Summary
This work introduces FeedSum, a large-scale dataset of LLM-generated feedback for improving text summarization. The study investigates how feedback quality, dimensionality, and granularity impact preference learning, showing that high-quality, multi-dimensional, fine-grained feedback significantly improves summary generation. The authors compare supervised fine-tuning and direct preference optimization, introducing SummLlama3-8b, a model that outperforms the nearly 10x larger Llama3-70b-instruct in human-preferred summaries. The approach demonstrates that smaller models can achieve superior performance with appropriate training using LLM feedback.

## Method Summary
The authors create FeedSum by generating summaries from 13 different summarizers across 7 diverse domains, then collecting multi-dimensional feedback using four configurations varying by quality, dimensionality, and granularity. They train Llama3-8b-instruct using both supervised fine-tuning and direct preference optimization on chosen-rejected pairs selected based on feedback scores. The models are evaluated using automated metrics (FineSurE) and human evaluation across faithfulness, completeness, and conciseness dimensions.

## Key Results
- SummLlama3-8b achieves higher human preference scores than Llama3-70b-instruct despite being 10x smaller
- Fine-grained, multi-dimensional feedback (C4 configuration) produces the best performance gains
- DPO significantly outperforms SFT across all evaluation dimensions
- High-quality LLM feedback (Llama3-70b) provides more reliable score distributions than lower-quality feedback (Gemma-2b)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-quality LLM feedback significantly improves summary quality over low-quality feedback
- Mechanism: Larger LLMs provide more reliable and diverse score distributions across summarizer categories, enabling better selection of chosen/rejected pairs
- Core assumption: The capacity of the LLM correlates with the quality of feedback it can provide for summary evaluation
- Evidence anchors: Abstract states high-quality feedback significantly improves summary generation; Section 4.2 shows lower correlation with human scores for C1 (smaller LLM) compared to C2-C4

### Mechanism 2
- Claim: Fine-grained, multi-dimensional feedback outperforms coarse-grained, single-dimensional feedback
- Mechanism: Sentence-level faithfulness and key-fact-level completeness/conciseness provide more precise signals than overall Likert scores, enabling the model to learn specific aspects of quality
- Core assumption: Fine-grained evaluation correlates better with human preferences and provides more actionable feedback for model improvement
- Evidence anchors: Abstract mentions importance of feedback dimensionality and granularity; Section 4.2 shows C4's fine-grained evaluation is robust to summarizer category

### Mechanism 3
- Claim: DPO is more effective than SFT for leveraging LLM feedback in summarization
- Mechanism: DPO uses chosen-rejected pairs to teach the model preference boundaries, while SFT only teaches one reference summary, leading to copy bias and reduced abstractiveness
- Core assumption: Preference learning through pairwise comparison is more effective than supervised learning for aligning with human preferences
- Evidence anchors: Abstract compares SFT and DPO methods; Section 5.3 shows DPO-C4 significantly improves summary quality while SFT-best falls short

## Foundational Learning

- Concept: Preference optimization (PPO vs DPO)
  - Why needed here: Understanding the difference between reinforcement learning approaches (PPO) and direct preference optimization (DPO) for aligning LLMs with human preferences
  - Quick check question: What is the key difference between PPO and DPO in terms of how they use preference data?

- Concept: Multi-dimensional evaluation metrics
  - Why needed here: The framework evaluates summaries across faithfulness, completeness, and conciseness, requiring understanding of these distinct quality dimensions
  - Quick check question: How does measuring faithfulness at the sentence level differ from measuring completeness at the key-fact level?

- Concept: Feedback quality assessment
  - Why needed here: Evaluating LLM-generated feedback requires understanding correlation metrics and the importance of diverse score distributions
  - Quick check question: What metrics would you use to assess whether LLM feedback aligns with human preferences?

## Architecture Onboarding

- Component map: 7 source datasets → 13 summarizers → 182K document-summary pairs → 4 feedback configurations → 125K feedback-labeled pairs → DPO/SFT training → Model evaluation
- Critical path: Data sourcing → Summary generation → Feedback generation → Preference learning → Model evaluation
- Design tradeoffs: Quality vs. cost (higher-quality feedback provides better results but increases computational expense); Granularity vs. simplicity (fine-grained evaluation provides better signals but requires more complex implementation); DPO vs. SFT (DPO provides better results but may be more challenging to implement and tune)
- Failure signatures: Uniform score distributions across all summarizer types indicate ineffective feedback generation; Degradation in abstractiveness suggests overfitting to reference summaries (SFT problem); Poor correlation with human evaluation indicates feedback quality issues
- First 3 experiments: 1) Test different LLM sizes for feedback generation to find the optimal balance between quality and cost; 2) Compare chosen-rejected pair selection criteria to optimize training data; 3) Evaluate the impact of feedback dimensionality by training models with single-dimension vs. multi-dimension feedback

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of LLM-generated feedback scale with model size beyond the Llama3 family tested in this paper?
- Basis in paper: [explicit] The paper mentions testing Gemma-2b-instruct and notes that larger LLMs like Llama3-8b/70b benefit more from preference optimization with LLM-generated feedback
- Why unresolved: The study only tested one smaller model (Gemma-2b) and one larger model (Llama3-70b) against Llama3-8b. The scaling relationship between model size and feedback effectiveness remains unclear
- What evidence would resolve it: Systematic testing across multiple model sizes while holding other factors constant would reveal the scaling relationship

### Open Question 2
- Question: What is the optimal balance between fine-grained and coarse-grained evaluation for different types of input documents?
- Basis in paper: [inferred] The paper shows that fine-grained evaluation (C4) outperforms coarse-grained evaluation (C2-C3) for text summarization, but doesn't explore document-specific variations
- Why unresolved: The study uses a single evaluation granularity approach across all document types without examining whether different document domains or lengths might benefit from different evaluation granularities
- What evidence would resolve it: Experiments comparing different evaluation granularities across various document types would identify optimal configurations

### Open Question 3
- Question: How do different feedback quality configurations affect model performance on out-of-distribution test data?
- Basis in paper: [explicit] The paper tests four feedback quality configurations but only evaluates on in-distribution test data from the same seven source datasets
- Why unresolved: The study doesn't examine whether models trained with different quality feedback generalize differently to unseen document types or domains
- What evidence would resolve it: Testing models trained with different feedback configurations on completely new document domains not present in the training data would reveal generalization differences

## Limitations
- The study focuses on English language summarization, limiting generalizability to other languages
- Reliance on automated evaluation metrics may miss nuanced aspects of summary quality
- Performance ceiling remains unclear, particularly for domains outside the seven covered in FeedSum
- The study doesn't address potential catastrophic forgetting when fine-tuning pretrained models with preference data

## Confidence
- **High Confidence Claims**: High-quality LLM feedback improves summary generation; Fine-grained, multi-dimensional feedback is more effective; DPO outperforms SFT
- **Medium Confidence Claims**: C4 configuration represents the optimal feedback setup; SummLlama3-8b achieves superior performance across all evaluation dimensions
- **Low Confidence Claims**: Performance improvements will generalize to domains not covered in FeedSum; The same feedback quality hierarchy will hold as LLM capabilities continue to evolve

## Next Checks
1. Evaluate SummLlama3-8b on summarization tasks from domains not included in FeedSum (e.g., legal documents, scientific papers) to assess whether the performance gains transfer beyond the training distribution.

2. Conduct a comprehensive human evaluation study comparing automated metrics against human preferences to quantify the alignment between FineSurE scores and actual human judgment of summary quality.

3. Re-run the feedback generation pipeline with the latest LLM models (beyond Llama3-70b) to determine whether the identified quality hierarchy remains valid as LLM capabilities advance.