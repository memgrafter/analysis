---
ver: rpa2
title: A Morphologically-Aware Dictionary-based Data Augmentation Technique for Machine
  Translation of Under-Represented Languages
arxiv_id: '2402.01939'
source_url: https://arxiv.org/abs/2402.01939
tags:
- data
- synthetic
- languages
- sentences
- seed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a dictionary-based data augmentation technique
  for machine translation of under-represented languages. The core method involves
  aligning parallel sentences, analyzing morphological features, replacing words with
  morphologically similar alternatives, and generating synthetic parallel data.
---

# A Morphologically-Aware Dictionary-based Data Augmentation Technique for Machine Translation of Under-Represented Languages

## Quick Facts
- **arXiv ID**: 2402.01939
- **Source URL**: https://arxiv.org/abs/2402.01939
- **Reference count**: 20
- **Primary result**: Dictionary-based data augmentation with morphological awareness improves MT quality for under-represented languages, achieving BLEU score increases of 0.4 to over 3 points even with minimal seed data.

## Executive Summary
This paper presents a dictionary-based data augmentation technique for machine translation of under-represented languages that leverages morphological analysis to generate grammatically consistent synthetic parallel data. The method aligns source-target word pairs, extracts morphological features from source words, and replaces both source and target words with morphologically equivalent alternatives from bilingual dictionaries. Experiments across 14 languages demonstrate consistent improvements over baselines, with gains observed even when using as few as five parallel sentences. The approach is particularly effective for morphologically rich languages and outperforms naive random replacement strategies.

## Method Summary
The method involves aligning parallel sentences, analyzing morphological features of source words, replacing words with morphologically similar alternatives from bilingual dictionaries, and generating synthetic parallel data. The process uses Stanza for morphological analysis and tokenization, fast_align for word alignment, and a language model for filtering synthetic sentences based on perplexity scores. The synthetic data is then used to fine-tune pre-trained DeltaLM models for translation. The approach is designed to work with minimal seed data and ensures grammatical consistency through morphological awareness.

## Key Results
- BLEU score improvements ranging from 0.4 to over 3 points across 14 languages
- Effective performance even with minimal seed data (as few as five parallel sentences)
- Morphologically-aware replacement outperforms naive random replacement
- Language model filtering enhances results by removing low-quality synthetic examples

## Why This Works (Mechanism)

### Mechanism 1
Morphologically-aware replacement produces synthetic sentences that retain grammatical correctness by aligning source-target word pairs, extracting morphological features (POS tag, case, number, etc.) from the source word, and replacing both words with morphologically equivalent alternatives. This ensures new word forms are inflected correctly for their grammatical role in the sentence.

### Mechanism 2
Filtering synthetic sentences with language models improves translation quality by removing nonsensical examples. After generating synthetic sentence pairs, each synthetic source sentence is scored by a language model for perplexity, with low-perplexity sentences retained as training data.

### Mechanism 3
Minimal seed data (as few as five sentences) can be effectively leveraged to improve translation quality by multiplying the effective size of the training corpus through synthetic expansion. This introduces new vocabulary while maintaining grammatical structure without requiring additional parallel data.

## Foundational Learning

- **Word-level alignment in parallel corpora**: Why needed? To know which words in source and target sentences correspond to each other for consistent replacements. Quick check: If "dog" in English aligns to "perro" in Spanish, and we replace "dog" with "cat", what must we do to the Spanish sentence?

- **Morphological analysis and inflection**: Why needed? To extract grammatical features from source words and generate grammatically correct surface forms when replacing words. Quick check: If a source word is a plural noun in the nominative case, what morphological features must the replacement word have?

- **Language model perplexity as quality filter**: Why needed? To score and filter synthetic sentences, keeping only those that are likely grammatical and semantically plausible. Quick check: If a synthetic sentence has very high perplexity according to the LM, what does that suggest about its quality?

## Architecture Onboarding

- **Component map**: Seed parallel data → Word alignment (fast_align) → Morphological tagging (Stanza) → Bilingual lexicon lookup → Word replacement (morphologically-aware or naive) → Morphological inflection (pyinflect for English) → Language model filtering (HuggingFace GPT-2) → Synthetic dataset → Neural MT training (DeltaLM fine-tuning)
- **Critical path**: Seed data quality → Alignment accuracy → Morphological feature extraction → Lexicon coverage → Inflection correctness → LM filtering effectiveness
- **Design tradeoffs**: Morphologically-aware replacement vs. naive replacement (grammaticality vs. simplicity); filtering vs. no filtering (quality vs. quantity); fine-tuning vs. training from scratch (data efficiency vs. flexibility)
- **Failure signatures**: BLEU score drops after adding more synthetic data; high perplexity for most synthetic sentences; morphological inflection errors in synthetic output
- **First 3 experiments**:
  1. Replace words in seed sentences with same POS tag only (naive), train MT, measure BLEU vs. baseline.
  2. Replace words in seed sentences with same morphological features, train MT, measure BLEU vs. baseline and naive.
  3. Apply LM filtering to synthetic data from (2), train MT, measure BLEU vs. (2) and baseline.

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of bilingual dictionaries impact the effectiveness of the proposed data augmentation technique? The paper mentions that translated sentences may not adhere to grammatical rules or word order, and that coverage depends on lexicon size and comprehensiveness, but does not systematically analyze how dictionary quality affects performance.

### Open Question 2
How does the proposed method perform for morphologically rich languages that are not covered by the Stanza model? The paper uses Stanza for morphological analysis but does not discuss performance for languages not supported by this tool.

### Open Question 3
How does the proposed method compare to other data augmentation techniques, such as back-translation, for low-resource languages? The paper mentions back-translation as an alternative but does not provide direct comparative analysis.

## Limitations

- Effectiveness depends on bilingual dictionaries containing comprehensive morphological annotations, which may be limited for many languages
- Performance may vary significantly across different morphological typologies and language families
- Language model filtering reliability depends on LM quality and may not generalize well to all target languages

## Confidence

**High Confidence**: Morphologically-aware replacement producing grammatically consistent synthetic data is well-supported by experimental results showing consistent BLEU improvements over naive methods.

**Medium Confidence**: LM filtering improving results is moderately supported but lacks detailed analysis of filtering thresholds and effectiveness variation across languages.

**Low Confidence**: Claims about suitability for "under-represented languages" lack comparative analysis against other low-resource techniques specifically designed for such languages.

## Next Checks

1. **Morphological Coverage Analysis**: Measure percentage of bilingual dictionary entries with complete morphological annotations for each target language and correlate this coverage with BLEU score improvements.

2. **Cross-Linguistic Typology Study**: Test the method on languages with different morphological typologies (isolating, agglutinative, fusional, polysynthetic) and analyze performance differences.

3. **Alternative Low-Resource Comparison**: Implement and compare against at least two other established low-resource MT techniques using identical evaluation conditions to establish relative effectiveness.