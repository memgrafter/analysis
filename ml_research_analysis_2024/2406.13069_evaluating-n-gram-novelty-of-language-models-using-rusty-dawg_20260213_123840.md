---
ver: rpa2
title: Evaluating $n$-Gram Novelty of Language Models Using Rusty-DAWG
arxiv_id: '2406.13069'
source_url: https://arxiv.org/abs/2406.13069
tags:
- novelty
- text
- training
- data
- novel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RUSTY-DAWG, a novel search tool based on
  Compacted Directed Acyclic Word Graphs (CDAWGs) that enables constant-time n-gram
  searches over massive text corpora. The authors use this tool to study the novelty
  of language model-generated text compared to human-written text and training data,
  focusing on Pythia models trained on the Pile.
---

# Evaluating $n$-Gram Novelty of Language Models Using Rusty-DAWG

## Quick Facts
- **arXiv ID**: 2406.13069
- **Source URL**: https://arxiv.org/abs/2406.13069
- **Reference count**: 15
- **Primary result**: Introduces RUSTY-DAWG, a CDAWG-based search tool for constant-time n-gram queries, and uses it to show LM-generated text is less novel than human text for n > 4, with novelty decreasing for larger models and constrained decoding.

## Executive Summary
This paper introduces RUSTY-DAWG, a novel search tool based on Compacted Directed Acyclic Word Graphs (CDAWGs) that enables constant-time n-gram searches over massive text corpora. The authors use this tool to study the novelty of language model-generated text compared to human-written text and training data, focusing on Pythia models trained on the Pile. They find that for n > 4, LM-generated text is less novel than human-written text, while smaller n-grams are more novel. Larger models and more constrained decoding strategies decrease novelty, and LMs complete frequent training n-grams with lower loss. The RUSTY-DAWG library is released to facilitate further research on pretraining data analysis.

## Method Summary
The authors use Compacted Directed Acyclic Word Graphs (CDAWGs) to efficiently search for n-grams across large corpora. RUSTY-DAWG preprocesses a static corpus into a compressed automaton structure where each node represents a state and transitions are labeled with tokens. This allows queries to be answered in time proportional to query length, not corpus size. The method computes n-gram novelty (n-novelty) as the proportion of n-grams not found in the training corpus, and uses non-novel suffix length (NNSL) to measure how far back into training data a generated sequence extends. The approach is applied to Pythia models (70M to 12B parameters) trained on the Pile, comparing generated text against validation data and post-Pile cutoff data (Dolma) as a non-contaminated baseline.

## Key Results
- For n > 4, LM-generated text is less novel than human-written text, though more novel for smaller n-grams
- Larger LMs and more constrained decoding strategies both decrease n-gram novelty
- LMs complete n-grams with lower loss if they are more frequent in the training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RUSTY-DAWG enables constant-time n-gram searches over massive corpora by using a Compacted Directed Acyclic Word Graph (CDAWG) structure.
- Mechanism: The CDAWG encodes all substrings of the corpus in a compressed automaton where each node represents a state, and transitions are labeled with tokens. This structure allows queries to be answered in time proportional to the query length, not the corpus size, by traversing states based on input tokens.
- Core assumption: The corpus is static and can be preprocessed into the CDAWG structure efficiently; query patterns follow deterministic state transitions with fallback via failure arcs.
- Evidence anchors:
  - [abstract] "To enable arbitrary-length n-gram search over a corpus in constant time w.r.t. corpus size, we develop RUSTY-DAWG, a novel search tool inspired by indexing of genomic data."
  - [section] "A CDAWG is a finite-state machine built for a corpus C that acts as a rich index for C."
  - [corpus] Weak - no explicit empirical timing results provided for large-scale constant-time claims; assumes implementation correctness.
- Break condition: If the corpus is dynamic and frequently updated, rebuilding the CDAWG becomes costly; if token sets are extremely large, state explosion could degrade performance.

### Mechanism 2
- Claim: Larger LMs and more constrained decoding strategies decrease n-gram novelty compared to human-written text.
- Mechanism: Larger models have more parameters to memorize frequent n-grams from training data, and constrained decoding (e.g., low temperature, beam search) narrows the sampling distribution toward high-probability (often memorized) n-grams.
- Core assumption: Training data contains repetitive or boilerplate text that larger models can memorize; decoding constraints amplify this memorization effect.
- Evidence anchors:
  - [abstract] "We find that, for n > 4, LM-generated text is less novel than human-written text, though it is more novel for smaller n. Larger LMs and more constrained decoding strategies both decrease novelty."
  - [section] "Figure 3a shows that, across n, n-grams are less novel for larger LMs than for smaller LMs."
  - [corpus] Weak - novelty differences are shown, but direct evidence linking model size to memorization capacity is inferred rather than measured.
- Break condition: If decoding is sufficiently stochastic, even large models may generate novel n-grams; if training data is highly diverse, memorization effects may be muted.

### Mechanism 3
- Claim: LMs complete n-grams with lower loss if they are more frequent in the training data.
- Mechanism: During training, the model sees frequent n-grams more often, leading to stronger parameter updates for these patterns; at inference, this translates into higher predicted probabilities and thus lower loss for frequent completions.
- Core assumption: Training frequency correlates with model parameter updates and subsequent inference behavior; the loss metric directly reflects this alignment.
- Evidence anchors:
  - [abstract] "Finally, we show that LMs complete n-grams with lower loss if they are more frequent in the training data."
  - [section] "Figure 5b shows that, across sizes, n-grams that are more frequent in the training data are easier for Pythia-12B to complete."
  - [corpus] Weak - the correlation is shown, but causal attribution to memorization vs. general statistical learning is not conclusively separated.
- Break condition: If evaluation uses small n-grams where frequency effects are negligible, or if models are regularized to avoid overfitting to frequency, this mechanism may not hold.

## Foundational Learning

- Concept: Finite State Automata and DAWGs
  - Why needed here: Understanding how CDAWGs compress and index all substrings of a corpus is essential to grasp why RUSTY-DAWG can answer n-gram queries efficiently.
  - Quick check question: How does a DAWG differ from a trie in terms of memory usage and query speed?

- Concept: Statistical Language Modeling and Loss Functions
  - Why needed here: The experiments measure how well LMs predict n-grams; understanding cross-entropy loss and its relationship to probability assignments is key to interpreting results about frequency effects.
  - Quick check question: Why does lower loss for frequent n-grams suggest memorization rather than just good generalization?

- Concept: Text Generation Decoding Strategies
  - Why needed here: The novelty results depend on decoding choices; knowing how top-k, top-p, temperature, and beam search alter the sampling distribution is critical for interpreting the impact on novelty.
  - Quick check question: What is the effect of setting temperature to 0 on the diversity of generated n-grams?

## Architecture Onboarding

- Component map: Corpus preprocessing -> CDAWG construction (linear in corpus size) -> CDAWG storage (RAM or disk) -> Query interface (NNSL and n-novelty computation) -> Python bindings for integration with ML workflows
- Critical path: 1. Load or build CDAWG for target corpus 2. Generate or load LM output text 3. Query CDAWG for NNSL at each token position 4. Compute n-novelty curves and statistics
- Design tradeoffs:
  - Memory vs. speed: Storing CDAWG in RAM is faster but uses ~29|C| bytes; disk storage saves RAM but slows queries.
  - Graph representation: Using AVL trees for edges gives O(log|Î£|) lookup but increases per-edge overhead; simpler arrays could reduce memory but slow transitions.
- Failure signatures:
  - Extremely slow queries: Likely due to disk-based CDAWG with slow I/O; check storage medium.
  - Memory exhaustion during build: Corpus too large for available RAM; consider sharding or using disk storage.
  - Incorrect novelty results: Possible mismatch between tokenization used for CDAWG and LM output; ensure consistent preprocessing.
- First 3 experiments:
  1. Build CDAWG on a small synthetic corpus (e.g., "hello$world$") and manually verify NNSL outputs for a query like "lloyd".
  2. Compare n-novelty curves for LM-generated vs. validation text on a small dataset to confirm expected patterns (less novelty for large n, more for small n).
  3. Vary decoding temperature on a fixed LM and corpus, measuring change in n-novelty to observe the effect of constrained decoding.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the novelty of LM-generated text evolve during pretraining when using a sequentially-constructed CDAWG that mirrors the training data order?
- Basis in paper: [explicit] Section 8 "Other CDAWG Use Cases" mentions this as a potential application
- Why unresolved: The paper only analyzes novelty of fully-trained models, not how it changes throughout training
- What evidence would resolve it: Tracking novelty metrics at different training checkpoints using a partial CDAWG built up to that point

### Open Question 2
- Question: What is the relationship between n-gram novelty and text quality in LM-generated content?
- Basis in paper: [explicit] Section 8 "A Note on Generation Quality" raises this as a confounding factor
- Why unresolved: The paper focuses on quantifying novelty without controlling for or measuring text quality
- What evidence would resolve it: Comparative analysis of novelty metrics across text samples rated for quality by human annotators

### Open Question 3
- Question: How does the memory overhead of CDAWGs scale with dataset size, and what are the most effective optimization strategies?
- Basis in paper: [explicit] Section B.3 discusses current memory overhead of 29|C| bytes and potential optimizations
- Why unresolved: The paper identifies memory as a challenge but doesn't explore optimization techniques beyond mentioning future possibilities
- What evidence would resolve it: Empirical comparison of memory usage and query speed across different graph representations and optimization strategies on datasets of varying sizes

## Limitations

- CDAWG construction and query efficiency claims lack empirical timing results for large-scale queries, relying on theoretical guarantees
- Attribution of novelty effects to memorization is plausible but not definitively proven, with potential confounding from dataset characteristics
- Results depend on consistent tokenization between CDAWG index and LM outputs, which is not explicitly validated

## Confidence

- **High Confidence**: CDAWG structure enables efficient n-gram indexing with theoretical basis for constant-time queries; core results on LM novelty patterns are robust
- **Medium Confidence**: Attribution of decreased novelty to model size and decoding via memorization is plausible but not definitively proven; frequency-loss correlation observed but mechanism not conclusively separated
- **Low Confidence**: Exact magnitude of novelty differences across decoding strategies and precise impact of prompt length require more extensive validation

## Next Checks

1. **Benchmark Query Performance**: Run timing experiments on RUSTY-DAWG querying the full Pile dataset with varying n-gram lengths and corpus sizes. Compare against baseline methods like sliding window searches to verify constant-time claims empirically.

2. **Ablation on Memorization vs. Learning**: Train smaller Pythia models with regularization techniques designed to reduce overfitting (e.g., weight decay, dropout). Measure whether novelty patterns persist despite reduced memorization capacity to better isolate the memorization mechanism.

3. **Tokenization Alignment Validation**: Generate a small corpus with known n-gram patterns and verify that RUSTY-DAWG correctly identifies all expected matches when using the same tokenization as the LM. Test edge cases like out-of-vocabulary tokens and different preprocessing pipelines.