---
ver: rpa2
title: Unlocking the Potential of Model Merging for Low-Resource Languages
arxiv_id: '2407.03994'
source_url: https://arxiv.org/abs/2407.03994
tags:
- merging
- languages
- language
- data
- low-resource
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using model merging as an alternative to traditional
  CT-then-SFT for adapting LLMs to low-resource languages. The authors propose merging
  a CT model (trained on target language data) with an SFT model (trained on English
  task data) to combine language modeling and task-solving capabilities.
---

# Unlocking the Potential of Model Merging for Low-Resource Languages

## Quick Facts
- arXiv ID: 2407.03994
- Source URL: https://arxiv.org/abs/2407.03994
- Reference count: 23
- Model merging outperforms CT-then-SFT by up to 4.69% when pre-training data is extremely scarce (<10B tokens)

## Executive Summary
This paper explores using model merging as an alternative to traditional CT-then-SFT for adapting LLMs to low-resource languages. The authors propose merging a CT model (trained on target language data) with an SFT model (trained on English task data) to combine language modeling and task-solving capabilities. Experiments on 7 low-resource languages show that model merging outperforms CT-then-SFT when pre-training data is extremely scarce (<10B tokens), achieving up to 4.69% better average performance. However, as pre-training data increases, the benefits of merging diminish. The authors analyze this performance plateau and introduce a slack variable to the TIES merging algorithm to mitigate information loss, improving performance by 0.52% on Bengali tasks.

## Method Summary
The method involves continual pre-training (CT) of Llama-2-7B on monolingual texts in each target language, followed by two approaches to inject task-solving capabilities: training with English SFT data or merging with an English task-solving LLM. Two model merging methods are compared: weighted averaging and TIES (Yadav et al., 2023). The authors introduce a slack variable to the TIES algorithm to mitigate information loss during merging. The approach is evaluated on seven low-resource languages (Tamil, Telugu, Odia, Bengali, Tibetan, Uyghur, Mongolian) across text classification, machine reading comprehension, response selection, and math reasoning tasks.

## Key Results
- Model merging outperforms CT-then-SFT by up to 4.69% when pre-training data is extremely scarce (<10B tokens)
- Performance plateau occurs as CT tokens increase, with 4% more SFT parameters discarded during merging
- TIES-SV with slack variable improves Bengali task performance by 0.52% on average
- A single merged model for Mongolian and Uyghur achieves comparable performance to individual language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model merging outperforms CT-then-SFT when pre-training data is extremely scarce (<10B tokens)
- Mechanism: Model merging preserves language modeling acquired during CT while incorporating task-solving capabilities without additional training, avoiding catastrophic forgetting of language abilities
- Core assumption: The target language model and English task-solving model have complementary and non-conflicting parameter spaces
- Evidence anchors:
  - [abstract] "model merging effectively endows LLMs for low-resource languages with task-solving abilities, outperforming CT-then-SFT in scenarios with extremely scarce data"
  - [section] "In the conventional CT-then-SFT approach, LLMs often fail to acquire sufficient comprehension of the target language from a small-size CT corpus, and this ability may be further diminished by supervised fine-tuning"
  - [corpus] Weak - no direct corpus evidence for this specific claim
- Break condition: When pre-training corpus exceeds 10B tokens, performance plateau occurs and CT-then-SFT becomes more effective

### Mechanism 2
- Claim: Performance saturation occurs in model merging with more training tokens due to parameter conflicts
- Mechanism: As CT model parameters change more significantly with additional tokens, more parameters from the SFT model are discarded during the TIES merging process, leading to loss of task-solving capabilities
- Core assumption: The magnitude of parameter changes in CT model correlates with the number of tokens used in CT
- Evidence anchors:
  - [section] "As we continually pre-train the LLM with more tokens in language X, the parameters of CT-X changes more significantly. Since the magnitude of CT-X's task vector becomes larger, more parameters of SFT-flan would be discarded during the process of electing signs"
  - [section] "We find as the LLM is pre-trained with more tokens, 4% more parameters are removed in trimmed SFT-flan"
  - [corpus] Weak - no direct corpus evidence for this specific claim
- Break condition: When the proportion of discarded SFT parameters exceeds a critical threshold, task-solving capabilities deteriorate

### Mechanism 3
- Claim: Introducing a slack variable to TIES algorithm mitigates information loss and improves performance
- Mechanism: The slack variable allows retention of parameters from the SFT model with smallest magnitude differences between conflicting parameters, preserving more task-solving information
- Core assumption: Not all parameter conflicts are equally important, and preserving parameters with minimal differences maintains task-solving capabilities
- Evidence anchors:
  - [section] "To retain the parameters of SFT-flan while minimizing the information loss of CT-X, we first rank these pairs of parameters between SFT-flan and CT-X according to their differences in the magnitude. Next, we select a subset of parameters with the smallest magnitude differences to reserve"
  - [section] "Table 4 shows the results of vanilla TIES and our TIES-SV on three Bengali tasks. In all three tasks, our TIES-SV outperforms vanilla TIES by 0.52% on average"
  - [corpus] Weak - no direct corpus evidence for this specific claim
- Break condition: When slack variable retention becomes too aggressive, it may reintroduce conflicts that degrade model performance

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Understanding why CT-then-SFT fails for low-resource languages requires knowledge of how fine-tuning can erase previously learned language modeling capabilities
  - Quick check question: If a model is continually pre-trained on language X and then fine-tuned on English tasks, what happens to its ability to generate text in language X?

- Concept: Parameter conflict resolution in model merging
  - Why needed here: The TIES algorithm's mechanism for handling conflicting parameters between merged models is central to understanding performance differences
  - Quick check question: In TIES merging, what determines which model's parameters are retained when sign conflicts occur?

- Concept: Task vector arithmetic in model merging
  - Why needed here: Understanding how task vectors are computed and merged explains the foundation of weighted averaging and TIES approaches
  - Quick check question: How is a task vector calculated for a model that has been fine-tuned from an initial pre-trained model?

## Architecture Onboarding

- Component map: Base Llama-2-7B model → CT-X model (language-specific) + SFT-flan model (English task-solving) → Merged model (TIES or weighted average)
- Critical path: CT pre-training → SFT fine-tuning (or English SFT) → Model merging → Evaluation on low-resource language tasks
- Design tradeoffs: Model merging eliminates need for target language SFT data but requires careful parameter conflict resolution; CT-then-SFT is simpler but suffers from catastrophic forgetting
- Failure signatures: Performance plateau with increased CT tokens, unparsable byte outputs for languages without vocabulary tokens, character overlap-based reasoning errors
- First 3 experiments:
  1. Replicate the baseline comparison between CT-then-SFT and model merging on a single low-resource language with <10B tokens
  2. Test the parameter conflict analysis by tracking discarded SFT parameters across CT checkpoints
  3. Implement and evaluate the TIES-SV variant with slack variable on the Bengali tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed TIES-SV slack variable method compare to other potential improvements for model merging algorithms when dealing with information loss in low-resource languages?
- Basis in paper: [explicit] The authors introduce TIES-SV to mitigate information loss by reserving parameters with smallest magnitude differences, showing 0.52% improvement on Bengali tasks.
- Why unresolved: The paper only compares TIES-SV to the original TIES algorithm and doesn't explore other potential solutions to the parameter conflict problem during merging.
- What evidence would resolve it: Comparative experiments testing TIES-SV against other parameter selection strategies (e.g., Fisher information, gradient magnitude) on multiple low-resource languages.

### Open Question 2
- Question: What is the optimal number and combination of low-resource languages that can be effectively merged into a single multilingual task-solving model?
- Basis in paper: [inferred] The authors conduct a pilot study merging Mongolian and Uyghur into a single model, showing comparable performance to single-language models.
- Why unresolved: The study only merges two languages and doesn't explore the upper limits or optimal combinations of languages for multilingual merging.
- What evidence would resolve it: Systematic experiments varying the number and language families of merged languages, measuring performance degradation and optimal combinations.

### Open Question 3
- Question: How does model merging performance scale with increasing model size and what are the computational trade-offs?
- Basis in paper: [inferred] The study uses Llama-2-7B and mentions computational resource constraints, but doesn't explore scaling to larger models.
- Why unresolved: The paper focuses on 7B parameter models and doesn't investigate how merging effectiveness changes with larger or smaller model sizes.
- What evidence would resolve it: Experiments with multiple model sizes (e.g., 1B, 13B, 70B) comparing merging performance, computational requirements, and diminishing returns.

### Open Question 4
- Question: Can the performance plateau observed in model merging be completely eliminated through alternative merging strategies or architectural modifications?
- Basis in paper: [explicit] The authors observe performance saturation with increased CT tokens and propose TIES-SV as a partial solution, but acknowledge limitations.
- Why unresolved: The proposed TIES-SV only partially addresses the issue and the fundamental causes of performance saturation remain unclear.
- What evidence would resolve it: Development and testing of new merging algorithms or architectural modifications that prevent performance plateau across all data regimes.

## Limitations

- Performance benefits are limited to extremely low-resource scenarios (<10B tokens), with diminishing returns as pre-training data increases
- Corpus composition ambiguity may affect generalization across the diverse set of seven low-resource languages studied
- Reliance on machine translation for SFT data introduces potential quality issues not thoroughly evaluated

## Confidence

- **High confidence**: The observation that model merging outperforms CT-then-SFT in extremely low-resource scenarios is well-supported by experimental results across seven languages
- **Medium confidence**: The explanation of performance saturation through parameter conflict mechanisms is plausible but relies on indirect evidence
- **Low confidence**: The generalization of findings to languages outside the seven studied or to different model architectures remains uncertain

## Next Checks

**Check 1**: Replicate the performance comparison between CT-then-SFT and model merging on a new low-resource language (e.g., Kazakh or Khmer) with varying amounts of pre-training data (1B, 5B, 10B, 20B tokens) to identify language-specific thresholds where approaches diverge.

**Check 2**: Conduct an ablation study on the translation quality by comparing model performance using professional human translations versus machine translations for the SFT data in one target language, isolating the impact of translation quality on the relative performance of merging approaches.

**Check 3**: Implement a more granular analysis of parameter retention during merging by tracking task-specific performance metrics (e.g., individual task scores rather than average) as a function of the proportion of SFT parameters retained, to determine whether certain task capabilities are more vulnerable to parameter conflicts than others.