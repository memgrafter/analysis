---
ver: rpa2
title: 'Beyond Text: Leveraging Multi-Task Learning and Cognitive Appraisal Theory
  for Post-Purchase Intention Analysis'
arxiv_id: '2407.08182'
source_url: https://arxiv.org/abs/2407.08182
tags:
- emotions
- text
- appraisal
- appraisals
- cognitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study integrates Cognitive Appraisal Theory into NLP to predict
  post-purchase intentions using multi-task learning. The approach combines text reviews
  with cognitive appraisals and emotions as predictors, leveraging BERT-based models.
---

# Beyond Text: Leveraging Multi-Task Learning and Cognitive Appraisal Theory for Post-Purchase Intention Analysis

## Quick Facts
- arXiv ID: 2407.08182
- Source URL: https://arxiv.org/abs/2407.08182
- Reference count: 19
- One-line primary result: Multi-task learning integrating cognitive appraisals and emotions outperforms single-task text-only models for predicting post-purchase intentions.

## Executive Summary
This study integrates Cognitive Appraisal Theory into NLP to predict post-purchase intentions using multi-task learning. The approach combines text reviews with cognitive appraisals and emotions as predictors, leveraging BERT-based models. Results show that multi-task and theory-informed models modestly outperform baselines in predicting intentions to promote, while multi-modal models excel in predicting repurchase intentions. The study demonstrates that incorporating psychological constructs enhances behavioral prediction, supporting the validity of Cognitive Appraisal Theory in computational modeling of consumer behavior.

## Method Summary
The study predicts post-purchase behavioral intentions (repurchase and promotion) using multi-task learning with Cognitive Appraisal Theory. The PEACE-Reviews Dataset (1,400 reviews) provides text, 20 cognitive appraisal ratings, 8 emotion ratings, and 2 PCB variables. BERT-base is fine-tuned for text processing, with feedforward neural networks for appraisal and emotion prediction. Models are evaluated using three-way classification (low/medium/high) with accuracy and weighted F1 scores. The architecture includes baseline (text-only), constrained (sequential prediction), multi-modal (concatenated modalities), and multi-task/theoretical (joint prediction) approaches.

## Key Results
- Multi-task and theory-informed models modestly outperform baselines in predicting intentions to promote.
- Multi-modal models excel in predicting repurchase intentions.
- Incorporating psychological constructs (cognitive appraisals and emotions) enhances behavioral prediction in NLP models.

## Why This Works (Mechanism)

### Mechanism 1
Multi-task learning integrating cognitive appraisals and emotions outperforms single-task text-only models for predicting post-purchase intentions. The approach leverages shared representations across tasks (emotion, appraisal, behavior) so that each task benefits from information in the others. Cognitive appraisals serve as structured psychological constructs that capture dimensions of evaluation not encoded purely in text, while emotions capture valence and arousal signals. By jointly modeling these with behavioral outcomes, the model can learn richer mappings between user experiences and intended actions.

### Mechanism 2
Incorporating appraisal and emotional considerations enhances post-consumption behavior prediction. Appraisal and emotional variables add dimensions of psychological motivation and valence that text alone may not fully capture. This integration allows the model to ground predictions in a psychological theory (Cognitive Appraisal Theory) rather than purely statistical patterns. This is particularly effective for predicting intentions to promote, where the reasoning process (e.g., attribution of blame or fairness) plays a larger role.

### Mechanism 3
BERT-based models capture emotionally relevant linguistic features that correlate with behavioral intentions. Fine-tuning BERT on review text allows the model to learn contextualized representations that encode emotional and appraisal-related cues. The transformer architecture can capture long-range dependencies and nuanced sentiment, which are then used to predict behavioral outcomes. The explainability analysis shows that emotionally charged and appraisal-related words are important predictors.

## Foundational Learning

- **Concept**: Cognitive Appraisal Theory
  - Why needed here: The study is grounded in Cognitive Appraisal Theory, which posits that emotions are the result of cognitive evaluations across multiple dimensions of psychological motivation. Understanding this theory is crucial for understanding the rationale behind incorporating appraisal and emotion variables into the model.
  - Quick check question: What are the key propositions of Cognitive Appraisal Theory regarding the relationship between appraisals, emotions, and behavior?

- **Concept**: Multi-task learning
  - Why needed here: The study uses multi-task learning frameworks to jointly predict appraisals, emotions, and post-consumption behaviors. Understanding multi-task learning is essential for understanding the model architecture and the rationale behind sharing representations across tasks.
  - Quick check question: How does multi-task learning differ from single-task learning, and what are the potential benefits and drawbacks of using multi-task learning?

- **Concept**: Transformer-based language models (e.g., BERT)
  - Why needed here: The study uses BERT as the base model for processing text. Understanding the architecture and capabilities of transformer-based language models is crucial for understanding how the model processes text and learns representations.
  - Quick check question: What are the key components of a transformer-based language model like BERT, and how does it differ from traditional recurrent neural networks?

## Architecture Onboarding

- **Component map**: Text → BERT → Appraisals (FFNN) → Emotions (FFNN) → PCBs (Concatenated embeddings via FFNN)
- **Critical path**: For the theoretical model, the critical path is: Text → Appraisals → Emotions → PCB. The BERT model processes the text, the appraisal FFNN predicts appraisals, the emotion FFNN predicts emotions from appraisals, and the final FFNN predicts PCBs from concatenated embeddings.
- **Design tradeoffs**: The study compares different model architectures (baseline, constrained, multi-modal, multi-task) to evaluate the benefits of incorporating psychological constructs. The tradeoff is between model complexity (multi-task) and potential performance gains.
- **Failure signatures**: If the model fails to learn meaningful representations for appraisals or emotions, or if the concatenation of modalities does not improve performance, then the multi-task approach will not be effective. Overfitting to the specific dataset is also a potential failure mode.
- **First 3 experiments**:
  1. Train the baseline Text → PCB model and evaluate its performance on the test set.
  2. Train the Text → Appraisals → PCB model and compare its performance to the baseline.
  3. Train the theoretical model (Text → Appraisals → Emotions → PCB) and compare its performance to the other models.

## Open Questions the Paper Calls Out
- How well do models trained on the PEACE-Reviews dataset generalize to other review datasets with different emotional content, length, contexts, and product/service types?
- Does incorporating a wider range of emotions beyond the 8 emotions in the PEACE-Reviews dataset improve prediction of post-consumption behaviors?
- Can models that simultaneously learn cognitive appraisals, emotions, and post-consumption behaviors in a multi-task framework outperform the current multi-task and theory-informed models?

## Limitations
- The current results might not generalize to other review datasets and contexts.
- The dataset only provides ratings for 8 emotional experiences, which might not comprehensively capture all emotional experiences experienced during consumption.
- The study does not explore how sensitive results are to different annotation schemes or inter-rater reliability of cognitive appraisal ratings.

## Confidence
- **High confidence**: BERT-based models can capture linguistically expressed emotions and appraisals that correlate with behavioral intentions.
- **Medium confidence**: Multi-task learning frameworks that incorporate cognitive appraisals and emotions outperform single-task text-only models for predicting post-purchase intentions.
- **Low-Medium confidence**: The structured theoretical model (Text → Appraisals → Emotions → Behavior) provides better predictions than alternative architectures.

## Next Checks
1. Conduct statistical significance testing with paired t-tests or bootstrap confidence intervals on model performance across multiple random seeds.
2. Evaluate the trained models on an independent dataset of consumer reviews to assess generalization beyond the PEACE-Reviews dataset.
3. Perform systematic ablation studies to isolate which aspects of the multi-task architecture contribute most to performance improvements.