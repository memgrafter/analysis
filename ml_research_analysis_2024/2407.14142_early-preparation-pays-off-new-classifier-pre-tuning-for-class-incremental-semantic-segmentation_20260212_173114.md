---
ver: rpa2
title: 'Early Preparation Pays Off: New Classifier Pre-tuning for Class Incremental
  Semantic Segmentation'
arxiv_id: '2407.14142'
source_url: https://arxiv.org/abs/2407.14142
tags:
- nest
- classifiers
- classifier
- class
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting and background shift
  in class incremental semantic segmentation (CISS). The authors propose a new classifier
  pre-tuning (NeST) method that learns a transformation from old classifiers to generate
  new classifiers before formal training.
---

# Early Preparation Pays Off: New Classifier Pre-tuning for Class Incremental Semantic Segmentation

## Quick Facts
- arXiv ID: 2407.14142
- Source URL: https://arxiv.org/abs/2407.14142
- Reference count: 40
- Primary result: NeST improves class incremental semantic segmentation by 13.6% mIoU on MiB baseline with DeepLab on VOC 15-1 setting

## Executive Summary
This paper addresses catastrophic forgetting and background shift challenges in class incremental semantic segmentation (CISS) through a novel classifier pre-tuning (NeST) method. NeST learns a transformation from old classifiers to generate new classifiers before formal training, using importance and projection matrices initialized with cross-task class similarity. Experiments on Pascal VOC 2012 and ADE20K datasets demonstrate significant performance improvements over baseline methods, with 13.6% mIoU gain on the challenging 15-1 setting. The method also shows effectiveness on transformer backbones and in disjoint settings, with qualitative results showing better preservation of old knowledge while learning new classes.

## Method Summary
NeST tackles CISS by generating new classifiers through a learned transformation from old classifiers rather than directly tuning new parameters. The method uses importance matrices to capture channel-level similarity weights and projection matrices for class-level combination weights between old and new classes. These matrices are initialized using cross-task class similarity scores estimated from old model predictions on new class pixels. Before formal training, the generated new classifiers undergo pre-tuning on current step data to align them with the backbone features. The pre-tuned new classifiers are then concatenated with frozen old classifiers for joint training, achieving a balance between stability and plasticity.

## Key Results
- NeST achieves 13.6% mIoU improvement over MiB baseline on VOC 15-1 with DeepLab
- Method shows consistent gains across multiple settings (10-1, 15-1, 15-5, 19-1 on VOC; 100-50, 100-10, 100-5, 50-50 on ADE20K)
- Effective on both CNN (ResNet-101) and transformer (Swin-B) backbones
- Demonstrates better preservation of old knowledge while learning new classes in qualitative results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NeST improves stability-plasticity trade-off by generating new classifiers via a learned transformation from old classifiers before formal training
- Mechanism: Importance and projection matrices learn channel-level and class-level similarity weights between old and new classes. These weights are then used to generate new classifiers that are better aligned with the backbone and adapted to new data
- Core assumption: Cross-task class similarity scores can be approximated by old model predictions on new class pixels
- Evidence anchors: [abstract] "learns a transformation from old classifiers to generate new classifiers for initialization rather than directly tuning the parameters of new classifiers"; [section 3.2] "importance matrix learns a weighted score associated with each channel of the old classifier pertinent to the new class, while the projection matrix learns a linear combination from the weighted old classifiers to generate the new classifier"

### Mechanism 2
- Claim: Pre-tuning with new data stabilizes training by reducing the initial loss and gradient magnitudes
- Mechanism: Before the formal training step, the generated new classifiers are tuned on current step data, reducing misalignment between new classifiers and features. This results in smaller initial losses and gradients during formal training, thus preserving old knowledge
- Core assumption: Misalignment between new classifiers and features causes unstable training and forgetting
- Evidence anchors: [section 3.2] "prevent drastic changes in the feature extractor when learning new classes"; [section 4.2] "NeST can help bridge the stability gap in the model...the loss of NeST is lower than the loss of MiB"

### Mechanism 3
- Claim: Cross-task class similarity-based initialization of transformation matrices enhances plasticity by focusing on relevant old classifier channels for each new class
- Mechanism: For each new class, the importance matrix is initialized using Hadamard product and averaging of old classifier predictions on new class pixels. This gives higher initial weights to channels relevant to the new class
- Core assumption: The degree of similarity between old and new classes can be estimated by old model predictions
- Evidence anchors: [section 3.3] "if an old class is more similar to a new class, then during the pre-tuning process, the contribution of this old classifier should be more significant"; [section 4.3] "with equally treated channels, the performance dramatically drops"

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Class incremental learning requires preserving old knowledge while learning new classes, which is hindered by catastrophic forgetting
  - Quick check question: What is the main cause of catastrophic forgetting in neural networks?

- Concept: Background shift in semantic segmentation
  - Why needed here: In CISS, pixels of future classes are labeled as background in previous steps, causing the background classifier to see future classes. This "background shift" complicates classifier initialization
  - Quick check question: How does background shift differ from catastrophic forgetting?

- Concept: Stability-plasticity trade-off
  - Why needed here: CISS requires balancing preserving old knowledge (stability) with learning new classes (plasticity). NeST aims to achieve this balance via its pre-tuning and initialization strategies
  - Quick check question: Why is it challenging to achieve both stability and plasticity in continual learning?

## Architecture Onboarding

- Component map: Backbone (feature extractor) -> Classifier head (old classifiers + new classifiers) -> Importance matrices (channel-level similarity) -> Projection matrices (class-level combination) -> Pre-tuning module (new classifier tuning)

- Critical path:
  1. Initialize importance and projection matrices using cross-task class similarity
  2. Generate new classifiers by transforming old classifiers using the matrices
  3. Pre-tune generated new classifiers on current step data
  4. Concatenate pre-tuned new classifiers with old classifiers
  5. Train the model on current step data using the full classifier head

- Design tradeoffs:
  - Using all old classifiers vs. only background classifier: NeST uses all old classifiers for better knowledge transfer, but this increases computational cost
  - Pre-tuning vs. direct initialization: Pre-tuning stabilizes training but adds an extra step and hyperparameters (learning rate, epochs)
  - Cross-task similarity-based initialization vs. random: The proposed initialization is more effective but relies on the old model's predictions being informative

- Failure signatures:
  - Poor performance on new classes: Could indicate ineffective pre-tuning or initialization
  - Forgetting of old classes: Could indicate insufficient preservation of old knowledge or excessive plasticity
  - Instability during training: Could indicate misalignment between new classifiers and features

- First 3 experiments:
  1. Verify that NeST improves performance over random initialization on a simple CISS benchmark (e.g. 15-1 on Pascal VOC)
  2. Compare pre-tuning vs. no pre-tuning to confirm the importance of the pre-tuning step
  3. Test different initialization strategies (e.g. random, gradient-based) to validate the effectiveness of the proposed cross-task similarity-based initialization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of NeST vary when using different pre-tuning strategies (e.g., varying the number of epochs, learning rates, or loss functions) on different datasets?
- Basis in paper: [inferred] The paper mentions using specific hyperparameters for pre-tuning (e.g., 5 epochs, learning rate of 0.001 for Pascal VOC 2012, 15 epochs, learning rate of 0.1 or 0.5 for ADE20K). However, it does not explore the impact of varying these parameters
- Why unresolved: The paper focuses on demonstrating the effectiveness of NeST with a specific set of hyperparameters, but does not investigate the sensitivity of the method to different pre-tuning strategies
- What evidence would resolve it: Conducting experiments with different pre-tuning strategies and comparing the performance of NeST on various datasets would provide insights into the optimal hyperparameters for different scenarios

### Open Question 2
- Question: Can NeST be extended to handle more complex scenarios, such as class-incremental learning with a large number of classes or with a continuous data stream?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of NeST on datasets with a moderate number of classes (e.g., Pascal VOC 2012, ADE20K) and does not address the challenges of handling a large number of classes or a continuous data stream
- Why unresolved: The paper focuses on demonstrating the effectiveness of NeST in relatively simple scenarios, but does not explore its scalability or applicability to more complex scenarios
- What evidence would resolve it: Conducting experiments with datasets containing a large number of classes or simulating a continuous data stream would provide insights into the scalability and robustness of NeST in more challenging scenarios

### Open Question 3
- Question: How does NeST compare to other state-of-the-art methods for class-incremental semantic segmentation, especially those based on transformer architectures?
- Basis in paper: [explicit] The paper mentions that NeST can be applied to transformer-based backbones (e.g., Swin-B) and achieves significant performance gains. However, it does not provide a comprehensive comparison with other transformer-based methods
- Why unresolved: The paper focuses on demonstrating the effectiveness of NeST with convolutional backbones and does not provide a detailed comparison with transformer-based methods
- What evidence would resolve it: Conducting experiments comparing NeST with other state-of-the-art transformer-based methods on various datasets would provide insights into the relative strengths and weaknesses of different approaches

## Limitations
- Effectiveness depends on quality of cross-task class similarity estimates, which may be inaccurate if old model predictions poorly reflect true semantic relationships
- Introduces additional hyperparameters through the pre-tuning phase that require careful tuning
- Evaluation limited to Pascal VOC and ADE20K datasets, leaving open questions about generalization to other segmentation tasks

## Confidence
- High confidence: The core mechanism of using transformation matrices to generate new classifiers shows consistent performance improvements across multiple benchmarks
- Medium confidence: The claim that pre-tuning stabilizes training is supported by loss curves but lacks direct ablation comparisons
- Medium confidence: The initialization strategy based on cross-task similarity is theoretically sound but its effectiveness depends on prediction quality

## Next Checks
1. **Ablation on pre-tuning duration**: Systematically vary the pre-tuning epochs (0, 1, 3, 5, 10) on VOC 15-1 to quantify the optimal trade-off between stability and computational cost
2. **Cross-dataset generalization**: Apply NeST to a different segmentation dataset (e.g., Cityscapes) with varying class granularities to test robustness beyond the evaluated settings
3. **Robustness to similarity estimation errors**: Intentionally corrupt the cross-task similarity estimates with noise and measure performance degradation to establish the method's sensitivity to initialization quality