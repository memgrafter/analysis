---
ver: rpa2
title: Primal-Dual Spectral Representation for Off-policy Evaluation
arxiv_id: '2410.17538'
source_url: https://arxiv.org/abs/2410.17538
tags:
- learning
- representation
- policy
- spectral
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of off-policy evaluation (OPE)
  in reinforcement learning, where the goal is to estimate the value of a target policy
  using only data collected by a potentially unknown behavior policy. The authors
  tackle the computational difficulty of existing distribution correction estimation
  (DICE) methods, which require solving intractable non-convex non-concave optimization
  problems, especially when using neural network parameterizations.
---

# Primal-Dual Spectral Representation for Off-policy Evaluation

## Quick Facts
- arXiv ID: 2410.17538
- Source URL: https://arxiv.org/abs/2410.17538
- Reference count: 40
- One-line primary result: Introduces SpectralDICE, a sample-efficient off-policy evaluation method with Õ(N^{-1/2}) sample complexity by leveraging primal-dual spectral representations.

## Executive Summary
This paper addresses the computational challenge in off-policy evaluation (OPE) where existing distribution correction estimation (DICE) methods require solving intractable non-convex non-concave optimization problems. The authors propose a novel primal-dual spectral representation of the transition operator that enables both the Q-function and the stationary distribution correction ratio to be represented as linear functions in primal and dual feature spaces. This linearization eliminates the need for non-convex optimization, making the method both computationally and sample efficient with a sample complexity of Õ(N^{-1/2}). The approach is theoretically justified with rigorous sample complexity guarantees and empirically validated on continuous control benchmarks.

## Method Summary
The method works in two stages: first, it learns a primal-dual spectral representation (ϕ, µπ) from offline data using either ordinary least squares (OLS) or noise contrastive estimation (NCE); second, it plugs this learned representation into a convex-concave DICE estimator to compute the policy value estimate. The spectral decomposition of the transition operator creates a linear structure where both primal variables (Q-functions) and dual variables (distribution correction ratios) can be represented as inner products with learned features, converting the original saddle-point problem into a convex-concave optimization that can be solved efficiently.

## Key Results
- Achieves Õ(N^{-1/2}) sample complexity for off-policy evaluation, improving upon existing DICE methods
- Outperforms state-of-the-art DICE implementations in both convergence rate and final estimation accuracy on continuous control benchmarks
- Demonstrates robustness across different behavior policies, dataset sizes, and hyperparameters
- Eliminates non-convex non-concave optimization by leveraging primal-dual spectral representation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The primal-dual spectral representation eliminates non-convex non-concave optimization in DICE by linearizing both the Q-function and stationary distribution correction ratio in primal/dual feature spaces.
- Mechanism: Spectral decomposition of the transition operator creates a linear structure where both primal variables (Q-functions) and dual variables (distribution correction ratios) can be represented as inner products with learned features, converting the saddle-point problem into a convex-concave optimization.
- Core assumption: The MDP admits a low-rank structure that can be captured by finite-dimensional spectral features.
- Evidence anchors:
  - [abstract] "establishing a linear representation of value function and stationary distribution correction ratio, i.e., primal and dual variables in the DICE framework, using the spectral decomposition of the transition operator"
  - [section 3.1] "The primal-dual spectral representation has several nice properties. In particular, we can show that the Q-function Qπ(s,a), the state-action occupancy measure dπ(s,a), and the stationary distribution correction ratio ζ(s,a) can all be represented in linear forms using the primal/dual features"
  - [corpus] Weak evidence - no direct citations found, but the mechanism is theoretically novel
- Break condition: When the MDP does not admit a low-rank spectral structure or when the learned features fail to capture the true transition dynamics.

### Mechanism 2
- Claim: The representation learning error directly bounds the policy evaluation error through the simulation lemma.
- Mechanism: The learned spectral representation approximates the true transition kernel, and the policy evaluation error scales with the error in this approximation via the simulation lemma, which bounds the difference in policy values between MDPs with different transition kernels.
- Core assumption: The representation learning method achieves a bounded learning error that can be controlled with sufficient samples.
- Evidence anchors:
  - [section 4] "The key to subsequent analyses is to first bound the error of representation learning" and "we expect probably approximately correct (PAC) bounds for representation learning in the following format"
  - [section 4] "Theorem 4 (Main Theorem). Suppose Claim 3 holds for the RepLearn subroutine. Then under Assumptions 1 to 3, with probability at least 1 − δ, we have E ≲ 1/(1 − γ) · r(log(1/δ)/N + 1/(1 − γ)² · ξ(|F|, N, δ/2))"
  - [corpus] No direct citations, but the approach builds on established PAC learning bounds
- Break condition: When the representation learning error does not decay with sample size or when the PAC bounds fail to hold.

### Mechanism 3
- Claim: The offline dataset compatibility is achieved by selecting the auxiliary distribution q(·) as the state occupancy measure of the behavior policy.
- Mechanism: By choosing q(·) = dπb(·), the data distribution dD(s,a,s') = dπb(s)πb(a|s)P(s'|s,a) exactly matches the formulation needed for the spectral DICE estimator, enabling efficient use of offline data without importance sampling corrections.
- Core assumption: The behavior policy's state occupancy measure can be accurately estimated from the offline dataset.
- Evidence anchors:
  - [section 3.1] "Note that Assumption 2 guarantees a non-zero denominator when the nominator is non-zero" and "we highlight that our algorithm works with any representation learning method that has a bounded learning error"
  - [section 3.2] "For practical implementation, we can use stochastic gradient descent to solve the above stochastic optimization problem"
  - [corpus] No direct citations, but builds on standard offline RL assumptions
- Break condition: When the behavior policy has insufficient coverage of the target policy's state-action space (violating Assumption 2).

## Foundational Learning

- Concept: Markov Decision Processes and Bellman equations
  - Why needed here: The entire framework builds on MDP theory, using Bellman equations to characterize value functions and the recursive properties of occupancy measures
  - Quick check question: What is the Bellman equation for the Q-function and how does it relate to the value function?

- Concept: Spectral decomposition of linear operators
  - Why needed here: The core innovation relies on decomposing the transition operator spectrally to obtain linear representations of both primal and dual variables
  - Quick check question: How does spectral decomposition enable linear representation of functions in RL?

- Concept: Convex-concave optimization and saddle-point problems
  - Why needed here: The DICE estimator formulation requires solving a minimax problem, and the spectral representation converts a non-convex non-concave problem into a convex-concave one
  - Quick check question: What are the conditions for a saddle-point problem to be convex-concave and why does this matter for optimization?

## Architecture Onboarding

- Component map: RepLearn -> Feature validation -> SpectralDICE optimization -> Policy value estimation
- Critical path: RepLearn → Feature validation → SpectralDICE optimization → Policy value estimation
- Design tradeoffs:
  - Feature dimension vs. sample complexity: Higher dimensions may capture more structure but require more data
  - Representation learning method: OLS is simpler but NCE may be more robust to noise
  - Regularization strength: Balances stability vs. bias in the final estimate
- Failure signatures:
  - High variance in policy value estimates: May indicate insufficient data coverage or poor feature learning
  - Optimization instability: Could suggest inappropriate regularization or feature scaling issues
  - Large representation learning error: May require more sophisticated representation learning or higher-dimensional features
- First 3 experiments:
  1. Verify feature learning: Check if learned features can reconstruct transition dynamics on a small test MDP
  2. Validate convexity: Plot the objective function to confirm it's convex-concave before running full optimization
  3. Test with known MDP: Apply to a low-rank MDP with known ground truth to verify correctness of implementation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal auxiliary distribution q(·) for maximizing the sample efficiency of SpectralDICE in practice?
- Basis in paper: [explicit] The paper mentions q(·) ≡ dπb(·) for theoretical guarantees but notes this requires knowing the behavior policy, which is often unavailable in practice.
- Why unresolved: The paper assumes q(·) = dπb(·) for theoretical analysis but doesn't explore alternative choices when the behavior policy is unknown.
- What evidence would resolve it: Empirical comparison of different auxiliary distribution choices (e.g., uniform distribution, learned approximations) across various MDPs and behavior policies.

### Open Question 2
- Question: How does SpectralDICE's performance scale with the intrinsic dimensionality of the feature space d in high-dimensional MDPs?
- Basis in paper: [explicit] The theoretical analysis shows sample complexity depends on log|F| and the paper uses 2-layer neural networks for representation learning, but doesn't systematically study scaling behavior.
- Why unresolved: The paper demonstrates effectiveness on moderate-dimensional continuous control tasks but doesn't provide a comprehensive analysis of how performance degrades as d increases.
- What evidence would resolve it: Extensive experiments varying feature dimension d on high-dimensional tasks (e.g., robotics manipulation with >100 state dimensions) while tracking estimation error and sample efficiency.

### Open Question 3
- Question: Can the primal-dual spectral representation be extended to partially observable MDPs (POMDPs)?
- Basis in paper: [inferred] The paper focuses on fully observable MDPs with transition kernel P(s'|s,a), but the spectral decomposition framework could potentially be adapted to POMDPs with observation models.
- Why unresolved: The current formulation relies on state-action occupancy measures and transition kernels that are undefined in POMDPs without state information.
- What evidence would resolve it: Theoretical extension of the spectral representation to POMDPs using belief states or history-based features, along with empirical validation on POMDP benchmarks.

### Open Question 4
- Question: What are the computational trade-offs between OLS and NCE representation learning methods in terms of runtime and memory usage?
- Basis in paper: [explicit] The paper presents both OLS and NCE as candidate RepLearn methods with similar theoretical guarantees but doesn't compare their practical computational requirements.
- Why unresolved: The paper focuses on statistical efficiency but doesn't analyze the computational overhead of each method, which could be critical for large-scale applications.
- What evidence would resolve it: Systematic benchmarking of OLS vs NCE in terms of wall-clock time, GPU memory consumption, and convergence speed across different dataset sizes and MDP complexities.

### Open Question 5
- Question: How robust is SpectralDICE to model misspecification when the MDP doesn't satisfy the low-rank assumption?
- Basis in paper: [inferred] The theoretical analysis assumes realizability (Assumption 3) but the paper doesn't empirically study performance when this assumption is violated.
- Why unresolved: The paper demonstrates strong performance on benchmark tasks that may naturally satisfy low-rank structure, but doesn't test on deliberately misspecified environments.
- What evidence would resolve it: Experiments on MDPs with known violations of the low-rank assumption (e.g., high-rank transition dynamics) while measuring estimation bias and variance compared to non-spectral methods.

## Limitations
- The approach relies on the assumption that the MDP admits a low-rank spectral structure, which may not hold for complex real-world environments
- The representation learning step is treated as a black box with only general PAC bounds, leaving open questions about which specific methods work best in practice
- The empirical evaluation is limited to relatively simple continuous control benchmarks, and performance on high-dimensional or sparse-reward tasks remains unclear

## Confidence
- **High confidence**: The theoretical framework for spectral decomposition and its application to linearizing DICE objectives is sound and mathematically rigorous
- **Medium confidence**: The sample complexity analysis provides a useful asymptotic guarantee, but the constants and practical implications are not fully explored
- **Low confidence**: The empirical evaluation is limited in scope, and the claims about robustness across hyperparameters and behavior policies are not thoroughly validated

## Next Checks
1. **Representation learning error analysis**: Measure and report the actual representation learning error on held-out validation data to verify that it scales as expected with sample size
2. **High-dimensional benchmark test**: Evaluate SpectralDICE on a more challenging benchmark like Atari or robotic manipulation tasks to assess scalability
3. **Ground truth MDP validation**: Apply the method to a synthetic MDP with known low-rank structure to verify that the learned representation and subsequent policy evaluation are accurate when assumptions are satisfied