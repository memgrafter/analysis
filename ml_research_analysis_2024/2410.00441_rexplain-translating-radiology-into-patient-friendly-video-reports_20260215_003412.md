---
ver: rpa2
title: 'ReXplain: Translating Radiology into Patient-Friendly Video Reports'
arxiv_id: '2410.00441'
source_url: https://arxiv.org/abs/2410.00441
tags:
- radiology
- video
- reports
- findings
- left
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReXplain is an AI-driven system that translates complex radiology
  reports into patient-friendly video reports using a combination of large language
  models, medical image segmentation, and avatar generation technologies. The system
  generates explanations with plain language, highlights relevant anatomical regions
  in CT scans, and creates a virtual radiologist avatar to deliver the findings.
---

# ReXplain: Translating Radiology into Patient-Friendly Video Reports

## Quick Facts
- arXiv ID: 2410.00441
- Source URL: https://arxiv.org/abs/2410.00441
- Authors: Luyang Luo; Jenanan Vairavamurthy; Xiaoman Zhang; Abhinav Kumar; Ramon R. Ter-Oganesyan; Stuart T. Schroff; Dan Shilo; Rydhwana Hossain; Mike Moritz; Pranav Rajpurkar
- Reference count: 15
- Primary result: AI system achieves 64-74% positive radiologist feedback on patient-friendly video reports

## Executive Summary
ReXplain is an AI-driven system that translates complex radiology reports into patient-friendly video reports using a combination of large language models, medical image segmentation, and avatar generation technologies. The system generates explanations with plain language, highlights relevant anatomical regions in CT scans, and creates a virtual radiologist avatar to deliver the findings. In a proof-of-concept study with five board-certified radiologists, the system achieved at least 64% positive feedback on accuracy and comprehensibility of the delivered information, and at least 74% positive feedback on the helpfulness of individual video elements including comparison with normal images and lay-language explanations.

## Method Summary
The system integrates GPT-4o for text simplification, SAT segmentation for anatomical region identification, and Tavus avatar generation to create patient-friendly video reports from radiology reports and CT scans. The workflow involves extracting key findings from radiology reports using GPT-4o, generating lay-language explanations, segmenting relevant anatomical structures using SAT, registering normal images with patient images, and combining all elements with avatar generation to produce comprehensive video explanations.

## Key Results
- System achieved at least 64% positive feedback on accuracy and comprehensibility of delivered information
- At least 74% positive feedback on helpfulness of individual video elements including comparison with normal images and lay-language explanations
- Evaluation indicates system effectively simulates one-on-one consultation and holds promise for improving patient understanding with minimal additional workload for radiologists

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The system effectively translates complex radiology reports into patient-friendly language using GPT-4o.
- Mechanism: GPT-4o processes radiology reports to extract key findings and generate three types of lay-language explanations (what the finding means, how it appears on CT, how a normal CT should look).
- Core assumption: GPT-4o can reliably interpret medical terminology and produce accurate, simplified explanations that maintain clinical meaning.
- Evidence anchors:
  - [abstract] "The system achieved at least 64% positive feedback on accuracy and comprehensibility of the delivered information"
  - [section] "We leveraged the large language model, GPT-4o... which has been reported capable of lowering the reading level of professional medical documents"
  - [corpus] Weak evidence - no direct corpus studies on GPT-4o for radiology report simplification
- Break condition: GPT-4o generates inaccurate or clinically misleading explanations that could harm patient understanding or decision-making.

### Mechanism 2
- Claim: The system connects textual findings to anatomical regions through segmentation and text-anatomy association.
- Mechanism: SAT segmentation model identifies organs from CT scans based on natural language prompts, while GPT-4o matches report findings to specific anatomical structures from a predefined list of 201 organs.
- Core assumption: SAT can accurately segment relevant anatomical structures across diverse CT scans, and GPT-4o can correctly match findings to appropriate organs.
- Evidence anchors:
  - [section] "SAT leverages the capability of natural language prompts to effectively segment anatomical structures from 3D medical volumes"
  - [section] "we utilized the recently developed AI model, Segment Anything in medical images via Text model (SAT)"
  - [corpus] No corpus evidence - this appears to be novel application of SAT to radiology report visualization
- Break condition: Segmentation model fails to identify correct organs or GPT-4o incorrectly matches findings to wrong anatomical structures.

### Mechanism 3
- Claim: The avatar generation creates engaging, conversational video presentations that improve patient comprehension.
- Mechanism: Tavus platform generates realistic avatars using text-to-speech, 3D head reconstruction, and facial animation to deliver explanations alongside visual content.
- Core assumption: AI-generated avatars can effectively convey medical information in a way that patients find engaging and understandable.
- Evidence anchors:
  - [abstract] "at least 74% positive feedback on the helpfulness of individual video elements including... lay-language explanations"
  - [section] "The resulting video demonstrated a high-fidelity, realistic avatar with smooth transitions between expressions"
  - [corpus] Weak evidence - no corpus studies on avatar-based medical communication effectiveness
- Break condition: Avatar presentation distracts from or confuses the medical information being conveyed.

## Foundational Learning

- Concept: Large Language Models for medical text processing
  - Why needed here: GPT-4o is the core component for translating technical radiology reports into patient-friendly language
  - Quick check question: What are the key differences between using GPT-4o for radiology report simplification versus general text summarization?

- Concept: Medical image segmentation and 3D visualization
  - Why needed here: SAT segmentation model identifies anatomical structures to create visual connections between text findings and image regions
  - Quick check question: How does text-prompted segmentation differ from traditional supervised segmentation in medical imaging?

- Concept: Cross-modal generation and integration
  - Why needed here: The system combines text explanations, image segmentation, 3D rendering, and avatar generation into cohesive video presentations
  - Quick check question: What are the technical challenges in synchronizing multiple AI-generated modalities (text, images, audio, video) in real-time?

## Architecture Onboarding

- Component map: Radiology report → GPT-4o extraction → SAT segmentation → Video composition → Tavus rendering
- Critical path: Radiology report → GPT-4o extraction → SAT segmentation → Video composition → Tavus rendering
- Design tradeoffs:
  - Using existing models vs. developing custom models (tradeoff: faster deployment vs. optimal performance)
  - Organ-level vs. lesion-level segmentation (tradeoff: broader applicability vs. precise localization)
  - Pre-recorded vs. real-time avatar generation (tradeoff: consistency vs. flexibility)

- Failure signatures:
  - Incorrect organ segmentation leading to wrong visual highlights
  - GPT-4o hallucinations producing clinically inaccurate explanations
  - Avatar synchronization issues causing visual-audio mismatch
  - Registration errors showing misaligned comparison images

- First 3 experiments:
  1. Test GPT-4o with sample radiology reports to verify extraction accuracy and lay-language quality
  2. Validate SAT segmentation on diverse CT scans to ensure reliable organ identification
  3. Evaluate Tavus avatar generation with medical scripts to confirm natural presentation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would patients without medical training respond to ReXplain-generated video reports compared to traditional text reports?
- Basis in paper: [explicit] The paper acknowledges that the current study was conducted with radiologists and notes this may not reflect real patient experience, suggesting future work with non-medical participants.
- Why unresolved: The study only included board-certified radiologists as participants, who have medical knowledge and may not accurately represent typical patients' comprehension and preferences.
- What evidence would resolve it: A randomized controlled trial comparing patient comprehension, satisfaction, and anxiety levels between ReXplain video reports and traditional text reports in actual patients.

### Open Question 2
- Question: What is the clinical impact of ReXplain on patient outcomes and healthcare utilization?
- Basis in paper: [inferred] The paper discusses potential benefits like improved understanding and engagement but doesn't measure actual health outcomes or healthcare utilization.
- Why unresolved: The study only evaluated radiologist perceptions of the tool's potential usefulness, not its real-world impact on patient health or healthcare system efficiency.
- What evidence would resolve it: Longitudinal studies tracking patient adherence to treatment plans, follow-up appointment attendance, and clinical outcomes after using ReXplain versus traditional reports.

### Open Question 3
- Question: How can ReXplain be adapted to handle more complex and rare findings beyond organ-level localization?
- Basis in paper: [explicit] The paper acknowledges limitations in identifying specific lesions and notes that only organs containing abnormalities were highlighted, which led to negative feedback on organ rendering usefulness.
- Why unresolved: The current system relies on SAT segmentation for whole-organ identification but lacks capability for precise lesion localization, which radiologists identified as a key limitation.
- What evidence would resolve it: Integration of advanced lesion segmentation models like BiomedParse and evaluation of whether more precise finding localization improves patient comprehension and radiologist acceptance.

## Limitations
- Limited evaluation sample size with only five board-certified radiologists, restricting generalizability
- Potential for GPT-4o to generate clinically inaccurate explanations that could mislead patients
- SAT segmentation applied at organ level rather than specific lesions, missing crucial pathological details

## Confidence

- **High Confidence**: The technical feasibility of integrating GPT-4o, SAT segmentation, and Tavus avatar generation into a cohesive system
- **Medium Confidence**: The system's ability to produce patient-friendly explanations that maintain clinical accuracy
- **Low Confidence**: The avatar's effectiveness in improving patient comprehension and engagement

## Next Checks

1. **Patient Usability Testing**: Conduct a randomized controlled trial comparing patient comprehension and satisfaction between traditional text reports, simplified text reports, and ReXplain video reports with actual patients rather than medical professionals.

2. **Clinical Accuracy Audit**: Have radiologists review a larger sample of generated videos (minimum 50) to systematically identify any instances where GPT-4o's simplifications introduced clinically significant errors or omissions.

3. **Longitudinal Engagement Study**: Track patient interaction with ReXplain videos over multiple follow-up visits to assess whether the system maintains effectiveness for understanding progressive conditions and changes over time.