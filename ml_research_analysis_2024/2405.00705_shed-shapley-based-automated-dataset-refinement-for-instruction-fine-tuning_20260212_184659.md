---
ver: rpa2
title: 'SHED: Shapley-Based Automated Dataset Refinement for Instruction Fine-Tuning'
arxiv_id: '2405.00705'
source_url: https://arxiv.org/abs/2405.00705
tags:
- data
- shed
- dataset
- datasets
- shapley
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SHED, a framework for refining large language
  model (LLM) fine-tuning datasets using Shapley values. The key insight is that high-quality
  individual data samples may not combine well, so Shapley values are used to measure
  the collective contribution of data to model performance.
---

# SHED: Shapley-Based Automated Dataset Refinement for Instruction Fine-Tuning

## Quick Facts
- arXiv ID: 2405.00705
- Source URL: https://arxiv.org/abs/2405.00705
- Reference count: 40
- Primary result: Achieves comparable or superior performance using only 10-40% of original fine-tuning data

## Executive Summary
SHED is a framework for refining large language model fine-tuning datasets using Shapley values to measure collective data contribution. The key innovation is clustering similar samples and computing Shapley values only for representative proxies, making the approach computationally feasible for large datasets. Experiments on MMLU and WizardLM datasets show that SHED-curated datasets achieve performance comparable to or better than using the full dataset, with as little as 10% of the original data, while also exhibiting strong transferability across different LLMs.

## Method Summary
SHED uses model-agnostic clustering to group semantically similar instruction-response pairs, then computes Shapley values only for representative proxy samples from each cluster. These quality scores are used to sample a smaller, high-quality dataset via either Quality-Ordered Cluster Sampling (QOCS) or Quality-Weighted Cluster Sampling (QWCS). The framework employs sentence transformer embeddings for clustering, iterative group removal for Shapley approximation, and LoRA-based fine-tuning on the curated subset. This approach reduces computational cost while preserving dataset diversity and improving fine-tuning efficiency.

## Key Results
- Achieves comparable performance to full datasets using only 10-40% of original data
- Outperforms baselines on MMLU and ARC-challenge tasks
- Demonstrates strong transferability across different LLM families
- Maintains human preference scores on MT-Bench while using significantly less data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model-agnostic clustering reduces computational cost while preserving dataset diversity
- Mechanism: Clustering groups similar samples, allowing Shapley values to be computed only for cluster proxies
- Core assumption: Cluster proxies adequately represent diversity and contribution of all samples within the cluster
- Evidence anchors: [abstract] "SHED employs model-agnostic clustering, enhancing the transferability of curated datasets across different language models"
- Break condition: If clusters are too coarse or composition changes drastically across model families

### Mechanism 2
- Claim: Proxy-based Shapley calculator enables efficient quality score estimation
- Mechanism: Iterative group removal approximates Shapley values without full combinatorial evaluation
- Core assumption: Performance changes from removing groups approximate marginal contributions of individual samples
- Evidence anchors: [section] "This method involves iteratively removing groups of instances from the proxy dataset and assessing the performance variation"
- Break condition: If group removal fails to capture nonlinear interactions between samples

### Mechanism 3
- Claim: Optimization-aware sampling (QOCS/QWCS) balances quality and diversity
- Mechanism: QOCS selects from highest-scoring clusters; QWCS probabilistically samples weighted by quality
- Core assumption: Clusters with higher Shapley-based quality scores contain more beneficial samples
- Evidence anchors: [section] "QWCS adopts a probabilistic approach to sample instances across all clusters, with the probability of selection from a given cluster weighted by its quality score"
- Break condition: If quality scoring is noisy or diversity is too heavily weighted

## Foundational Learning

- Concept: Shapley value in cooperative game theory
  - Why needed here: Provides principled way to evaluate each data sample's marginal contribution across all subsets
  - Quick check question: How does the Shapley value formula ensure fair attribution when data samples interact non-additively?

- Concept: Sentence transformer embeddings for clustering
  - Why needed here: Generates semantically meaningful embeddings that group similar instruction-response pairs
  - Quick check question: Why might using model-agnostic embeddings improve transferability across different LLMs?

- Concept: Coreset selection and data subset optimization
  - Why needed here: Goal is to identify small, representative subset maintaining or improving fine-tuning performance
  - Quick check question: What is the tradeoff between cluster granularity and computational cost in Shapley value approximation?

## Architecture Onboarding

- Component map: Sentence Transformers → K-means/Agglomerative Clustering → Proxy Selection → Proxy-based Shapley Calculator → QOCS/QWCS Sampling → LoRA Fine-tuning
- Critical path: 1. Embed all samples → 2. Cluster embeddings → 3. Select proxies → 4. Compute Shapley scores → 5. Sample dataset → 6. Fine-tune LLM
- Design tradeoffs:
  - Cluster count vs. Shapley computation time: More clusters → better representativeness but higher cost
  - Group size n in Shapley approx vs. approximation accuracy: Smaller n → better granularity but more iterations
  - QOCS vs. QWCS: QOCS focuses on quality, QWCS balances quality and diversity
- Failure signatures:
  - Poor performance despite small dataset: Likely proxy representativeness issue or suboptimal sampling strategy
  - High computational cost: Cluster count or iteration count too high; consider reducing k or C
  - Low transferability: Cluster embeddings may be model-specific; try alternative embedding models
- First 3 experiments:
  1. Run SHED with default hyperparameters (C=3000, k=10, n=60) on small MMLU subset and verify clustering/proxies
  2. Compare QOCS vs. QWCS sampling performance on same curated dataset using LLaMA-7B
  3. Measure runtime vs. number of clusters (C) to find sweet spot where performance plateaus but cost is manageable

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SHED perform when underlying embedding model fails to capture semantic similarities?
- Basis in paper: [inferred] Uses Sentence Transformers but acknowledges this could be a limitation if embeddings are insufficiently representative
- Why unresolved: Paper doesn't test with alternative embedding models or on datasets where standard embeddings might struggle
- What evidence would resolve it: Experiments comparing SHED performance using different embedding models or on datasets with challenging semantic structure

### Open Question 2
- Question: What is the impact of hyperparameter tuning on SHED's performance across different LLM families?
- Basis in paper: [explicit] Uses same hyperparameters across methods for fair comparison but doesn't explore optimal settings for different model families
- Why unresolved: Shows transferability but doesn't investigate whether different model families require different hyperparameter settings
- What evidence would resolve it: Systematic hyperparameter optimization across different LLM families

### Open Question 3
- Question: How does SHED handle class imbalance in fine-tuning datasets?
- Basis in paper: [inferred] Doesn't discuss class imbalance, which is common in real-world datasets
- Why unresolved: Experiments use balanced benchmark datasets, but real-world applications often involve imbalanced data
- What evidence would resolve it: Experiments on imbalanced datasets measuring SHED's ability to maintain performance across underrepresented classes

## Limitations
- Proxy representativeness remains unvalidated - selecting closest-to-centroid samples may not capture true diversity
- Proxy-based Shapley calculator uses heuristic approximation without theoretical guarantees about convergence or accuracy
- Computational complexity analysis is incomplete - no quantitative comparison between exact and proxy-based approaches
- Transferability claims lack mechanistic explanation for when or why curated datasets might fail to transfer

## Confidence
- **High Confidence**: Core mechanism of using clustering to reduce Shapley computation is sound and well-explained
- **Medium Confidence**: Quality ordering and sampling strategies are reasonable but effectiveness depends heavily on Shapley estimate accuracy
- **Low Confidence**: Transferability claims across model families lack deep analysis and mechanistic explanation

## Next Checks
1. **Proxy Representativeness Validation**: Select 5 clusters from MMLU and manually evaluate whether proxy samples capture diversity and quality range within clusters; measure intra-cluster variance
2. **Shapley Approximation Accuracy**: Compute exact Shapley values for 100 samples and compare against SHED's proxy-based approximation; quantify correlation and error distribution
3. **Transferability Stress Test**: Fine-tune LLaMA, Mistral, and decoder-only model on same SHED-curated dataset; systematically vary hyperparameters (C, n, f) to identify most consistent settings across models