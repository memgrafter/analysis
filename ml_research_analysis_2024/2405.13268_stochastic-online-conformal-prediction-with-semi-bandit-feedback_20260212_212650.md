---
ver: rpa2
title: Stochastic Online Conformal Prediction with Semi-Bandit Feedback
arxiv_id: '2405.13268'
source_url: https://arxiv.org/abs/2405.13268
tags:
- prediction
- algorithm
- conformal
- coverage
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a stochastic online conformal prediction algorithm
  for semi-bandit feedback, where the true label is only observed if it is contained
  in the prediction set. The algorithm constructs prediction sets dynamically while
  maintaining the desired coverage rate and achieving sublinear regret compared to
  the optimal prediction sets.
---

# Stochastic Online Conformal Prediction with Semi-Bandit Feedback

## Quick Facts
- arXiv ID: 2405.13268
- Source URL: https://arxiv.org/abs/2405.13268
- Reference count: 13
- Key outcome: Algorithm achieves sublinear regret while maintaining coverage in semi-bandit feedback setting

## Executive Summary
This paper addresses the challenge of online conformal prediction when only semi-bandit feedback is available - where true labels are only observed if they fall within the prediction set. The authors propose an algorithm that dynamically constructs prediction sets while maintaining the desired coverage rate and achieving sublinear regret compared to the optimal prediction sets. The method uses a high-probability upper bound on the cumulative distribution function of the scoring function to ensure coverage, combined with a constraint to prevent the parameter from decreasing over time. The algorithm is evaluated on three tasks: image classification, document retrieval, and auction price-setting, demonstrating superior performance compared to several baselines in terms of both regret and coverage rate.

## Method Summary
The proposed Stochastic Prediction Set (SPS) algorithm addresses online conformal prediction under semi-bandit feedback. The core approach uses the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality to construct a high-probability upper confidence bound on the empirical cumulative distribution function (CDF) of the scoring function. This bound ensures that the threshold τt never exceeds the optimal threshold τ*, guaranteeing coverage. The algorithm maintains the constraint τt+1 ≥ τt to ensure convergence to the optimal threshold from below. For semi-bandit feedback, when the true label is not in the prediction set, the algorithm substitutes τt for the unobserved score, leveraging the fact that accurate estimation of the CDF below τ* is not necessary to recover τ*. The method is evaluated across three diverse tasks: image classification on ImageNet, document retrieval on SQuAD, and auction price-setting using synthetic data.

## Key Results
- Achieves sublinear regret O(√T) while maintaining coverage rate α
- Successfully handles semi-bandit feedback by substituting τt for unobserved scores
- Outperforms baselines (greedy, ACI, DLR) on three tasks: image classification, document retrieval, and auction price-setting
- Maintains zero undercoverage while achieving lower cumulative regret than comparison methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm maintains coverage guarantees despite semi-bandit feedback by using a high-probability upper bound on the CDF.
- Mechanism: The algorithm constructs an upper confidence bound (Gt) on the empirical CDF using the DKW inequality. This ensures that τt ≤ τ* for all t with high probability, which guarantees that the true label is always included in the prediction set.
- Core assumption: The DKW inequality provides a valid high-probability bound on the deviation between the empirical CDF and the true CDF.
- Evidence anchors:
  - [abstract] "The method uses a high-probability upper bound on the cumulative distribution function of the scoring function to ensure coverage"
  - [section] "we instead use a high-probability upper bound on G*. In particular, for a error boundδ ∈ R>0 to be specified, we construct a1−δ confidence bound for the empirical CDF Gt using the Dvoretzky–Kiefer–Wolfowitz (DKW) inequality"
- Break condition: If the DKW inequality's assumptions are violated (e.g., if the distribution is not continuous), the high-probability bound may fail, leading to potential coverage violations.

### Mechanism 2
- Claim: The algorithm achieves sublinear regret by ensuring τt converges to τ* from below.
- Mechanism: By constraining τt+1 ≥ τt and using the upper confidence bound, the algorithm ensures τt never exceeds τ*. This prevents the algorithm from "undershooting" and missing the optimal threshold. The regret bound of O(√T) follows from the convergence properties of the empirical CDF estimate.
- Core assumption: The loss function ϕ is Lipschitz continuous with respect to the CDF G*.
- Evidence anchors:
  - [abstract] "The algorithm constructs prediction sets dynamically while maintaining the desired coverage rate and achieving sublinear regret compared to the optimal prediction sets"
  - [section] "Then, our goal is to converge to the best possible prediction sets over time, which we formalize by aiming to achieve sublinear regret"
  - [section] "We consider a loss function ϕ : R → R, where ϕ(τt) encodes the loss incurred on step t. Then, our goal is to converge to the best possible prediction sets over time"
- Break condition: If the Lipschitz continuity assumption on ϕ is violated, the regret bound may not hold, potentially leading to linear regret.

### Mechanism 3
- Claim: Semi-bandit feedback is handled by substituting τt for unobserved scores.
- Mechanism: When the true label is not in the prediction set (y*_t ∉ C_t), the algorithm substitutes τt for the unobserved score f(x_t, y*_t). This is valid because the algorithm only needs to estimate the CDF in the region [0, τ*), and τt ≤ τ* ensures the substitution is in this region.
- Core assumption: The optimal threshold τ* is the supremum of the set where G*(τ) ≤ 1-α, meaning the algorithm doesn't need to estimate the CDF below τ*.
- Evidence anchors:
  - [abstract] "The method uses a high-probability upper bound on the cumulative distribution function of the scoring function to ensure coverage, and a constraint to prevent the parameter from decreasing over time"
  - [section] "One remaining issue is how to handle steps where y*_t ∉ Ct. On these steps, our algorithm substitutes τt for the observation f(xt, y*t). Intuitively, the reason this strategy works is that the learner does not need to accurately estimate G* in the interval [0, τ*) to recover τ*; it is sufficient to include the right fraction of samples in this interval"
- Break condition: If the optimal threshold τ* is not the supremum of the set where G*(τ) ≤ 1-α (e.g., if there's a discontinuity), the substitution strategy may not work correctly.

## Foundational Learning

- Concept: Online learning with bandit feedback
  - Why needed here: The algorithm needs to learn the optimal prediction set threshold τ* sequentially as new data arrives, without observing all labels.
  - Quick check question: What is the key difference between full-information feedback and bandit feedback in online learning?

- Concept: Conformal prediction and coverage guarantees
  - Why needed here: The algorithm must ensure that the prediction sets contain the true label with a specified probability (coverage rate α), which is the core guarantee of conformal prediction.
  - Quick check question: How does conformal prediction ensure coverage guarantees without making assumptions about the data distribution?

- Concept: Empirical CDF estimation and concentration inequalities
  - Why needed here: The algorithm estimates the CDF of the scoring function from samples and uses concentration inequalities (DKW) to provide high-probability bounds on this estimate.
  - Quick check question: What is the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality, and how does it relate to empirical CDF estimation?

## Architecture Onboarding

- Component map: Input processing -> Scoring function -> CDF estimation -> Threshold selection -> Prediction set construction -> Feedback handling -> Regret calculation
- Critical path: 1. Receive xt 2. Compute scores f(xt, y) for all y ∈ Y 3. Determine prediction set Ct = {y | f(xt, y) > τt} 4. Observe feedback: y*_t ∈ Ct or not 5. Update CDF estimate Gt 6. Compute new threshold τt+1
- Design tradeoffs:
  - Conservative vs. aggressive threshold selection: Using upper confidence bounds ensures coverage but may lead to larger prediction sets
  - Exploration vs. exploitation: The algorithm must balance between accurately estimating the CDF and maintaining coverage
  - Computational complexity: Computing prediction sets over large label spaces (e.g., ImageNet with |Y| = 1000) can be expensive
- Failure signatures:
  - Coverage violations: If τt > τ* for some t, the algorithm may fail to cover the true label
  - Linear regret: If the Lipschitz continuity assumption on ϕ is violated, the algorithm may accrue linear regret
  - Slow convergence: If the learning rate is too small, the algorithm may converge very slowly to the optimal threshold
- First 3 experiments:
  1. Verify coverage guarantee: Run the algorithm on a synthetic dataset where the optimal threshold τ* is known, and check if the algorithm maintains coverage rate α.
  2. Test regret bound: Run the algorithm on a synthetic dataset and verify that the cumulative regret RT scales as O(√T).
  3. Evaluate sensitivity to scoring function: Run the algorithm with different scoring functions (e.g., logits vs. softmax) and compare performance to assess sensitivity to the choice of scoring function.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed algorithm's performance degrade under adversarial or non-i.i.d. data distributions?
- Basis in paper: [explicit] The paper notes that existing online conformal prediction methods assume i.i.d. or exchangeable samples, but does not explicitly analyze the proposed algorithm under non-i.i.d. conditions.
- Why unresolved: The algorithm's theoretical guarantees rely on i.i.d. assumptions, and no experiments or theoretical analysis are provided for adversarial or non-stationary data.
- What evidence would resolve it: Experiments or proofs showing regret and coverage bounds under non-i.i.d. or adversarial data would clarify the algorithm's robustness.

### Open Question 2
- Question: Can the algorithm be extended to handle multi-dimensional label spaces more efficiently?
- Basis in paper: [inferred] The experiments use finite label spaces (e.g., ImageNet with 1000 classes, document retrieval with varying candidate sets), but the algorithm's scalability to high-dimensional or continuous label spaces is not discussed.
- Why unresolved: The current formulation relies on thresholding a scalar scoring function, which may become computationally prohibitive for large or continuous label spaces.
- What evidence would resolve it: Theoretical analysis or experiments demonstrating scalability to high-dimensional label spaces would validate the algorithm's applicability in broader settings.

### Open Question 3
- Question: How sensitive is the algorithm to the choice of scoring function, and can it adapt to poorly calibrated scores?
- Basis in paper: [explicit] The experiments highlight sensitivity to the scoring function (e.g., using logits vs. softmax probabilities), and the paper notes that ACI's performance degrades with certain scoring functions.
- Why unresolved: While the algorithm is shown to work with specific scoring functions, its behavior with poorly calibrated or adversarially chosen scores is not explored.
- What evidence would resolve it: Experiments testing the algorithm with diverse or adversarially designed scoring functions would reveal its robustness to score quality.

## Limitations
- The paper relies heavily on the DKW inequality providing tight concentration bounds, which may not hold for heavy-tailed or discrete distributions
- The assumption that τ* is the supremum of {τ : G*(τ) ≤ 1-α} may not hold in practice, particularly with discontinuous CDFs
- The semi-bandit feedback setting assumes a specific observation mechanism that may not generalize to all real-world scenarios

## Confidence

- **High confidence** in the theoretical regret bound (O(√T)) and coverage guarantee, as these follow directly from established concentration inequalities
- **Medium confidence** in the practical effectiveness, as the empirical evaluation covers only three specific tasks with limited comparison to state-of-the-art methods
- **Medium confidence** in the semi-bandit feedback handling mechanism, as the substitution strategy depends on specific structural assumptions about the optimal threshold

## Next Checks

1. Test the algorithm on datasets with known discontinuities or heavy-tailed distributions to evaluate robustness of the DKW-based coverage guarantees
2. Compare performance against full-information feedback algorithms to quantify the cost of semi-bandit feedback
3. Evaluate sensitivity to the constraint τt+1 ≥ τt by running ablations with relaxed or removed constraints