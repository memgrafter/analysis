---
ver: rpa2
title: Pairwise Ranking Loss for Multi-Task Learning in Recommender Systems
arxiv_id: '2406.02163'
source_url: https://arxiv.org/abs/2406.02163
tags:
- loss
- conversion
- pwiser
- tasks
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel pairwise ranking loss function for
  multi-task learning in recommender systems, addressing the challenge of optimizing
  both Click-Through Rate (CTR) and Conversion Rate (CVR) tasks. The proposed method
  recognizes that conversions inherently follow clicks and aims to prioritize samples
  with conversions during model training.
---

# Pairwise Ranking Loss for Multi-Task Learning in Recommender Systems

## Quick Facts
- arXiv ID: 2406.02163
- Source URL: https://arxiv.org/abs/2406.02163
- Reference count: 32
- Primary result: Introduces PWiseR pairwise ranking loss for multi-task learning that prioritizes conversions over clicks, improving AUC metrics in CTR and CVR prediction tasks

## Executive Summary
This paper addresses the challenge of optimizing both Click-Through Rate (CTR) and Conversion Rate (CVR) tasks in multi-task learning recommender systems. The authors propose a novel pairwise ranking loss function that recognizes conversions as inherently more valuable and less noisy than clicks, explicitly encoding this relationship into the training objective. By introducing task-specific pairwise ranking terms that penalize the model when conversion samples are ranked lower than click-only samples, the method improves model discrimination and robustness against noise.

The proposed PWiseR loss is evaluated across various multi-task learning architectures including SharedBottom, MMoE, and PLE, as well as single-task learning models on both public Alibaba datasets and an industrial dataset. Results demonstrate consistent AUC improvements, particularly in scenarios where distinguishing between samples with and without conversions is crucial for accurate predictions. The method effectively harmonizes CTR and CVR models by making CTR models sensitive to CVR labels during training.

## Method Summary
The PWiseR (Pairwise Ranking) loss function is designed to address the noise differential between click-only samples and conversion samples in multi-task learning for recommender systems. It operates by comparing pairs of samples within the same batch and penalizing the model when conversion samples receive lower prediction scores than click-only samples (with margin m1) or when conversion samples are ranked lower than no-click samples (with margin m2). The loss is combined with binary cross-entropy (BCE) loss and weighted by hyperparameter λ. The method is integrated into existing MTL architectures like SharedBottom, MMoE, and PLE, requiring access to both CTR and CVR predictions and labels during training.

## Key Results
- PWiseR loss consistently outperforms traditional BCE loss across all tested MTL and STL models in terms of AUC metric
- The method shows particular effectiveness in distinguishing samples with conversions from those without, improving ranking quality
- Performance gains are observed across multiple datasets including public Alibaba datasets (FR, NL, US, CCP) and proprietary industrial data
- PWiseR successfully harmonizes CTR and CVR models by making CTR models sensitive to CVR labels during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pairwise ranking loss improves model discrimination by explicitly differentiating between click-only samples and click-with-conversion samples
- Mechanism: The proposed PWiseR loss adds two ranking terms that penalize the model when conversion samples are ranked lower than click-only samples (with margin m1) or when conversion samples are ranked lower than no-click samples (with margin m2)
- Core assumption: Conversions are inherently more reliable and valuable than clicks without conversions, and this reliability difference should be explicitly encoded in the loss function
- Evidence anchors:
  - [abstract] "Moreover, the likelihood of noise is significantly higher in CTR tasks where conversions do not occur compared to those where they do"
  - [section 3.2] "The first term penalizes instances where ˆycvr is less than ˆyctN ocvr by at least m1... The second term... will penalize cases where ˆycvr is less than ˆyzeros by at least m2"

### Mechanism 2
- Claim: The pairwise ranking loss harmonizes CTR and CVR models by making CTR models sensitive to CVR labels during training
- Mechanism: By including CVR labels in the CTR loss calculation, the CTR model learns to assign higher weights to exposures where conversions occur
- Core assumption: CTR and CVR models should be trained to agree on sample importance, not just on their individual task predictions
- Evidence anchors:
  - [section 2] "Both CTR and CVR models are utilized in determining this ranking. Since tasks resulting in conversions yield higher revenue... instances where the CVR value is 1 during the training of CTR models imply that these models are more sensitive to such instances"
  - [abstract] "This approach prioritizes tasks where conversions occur, recommending higher revenue-generating items"

### Mechanism 3
- Claim: The loss function reduces the impact of noisy click samples by downweighting their importance in the ranking process
- Mechanism: The PWiseR loss explicitly penalizes the model for assigning high scores to click-only samples when conversion samples are available with lower scores
- Core assumption: Click-only samples contain significantly more noise than conversion samples, and this noise differential should be reflected in the training objective
- Evidence anchors:
  - [abstract] "the likelihood of noise is significantly higher in CTR tasks where conversions do not occur compared to those where they do"
  - [section 3.1] "scenario 2 is more likely to contain noise compared to scenario 3. For instance, scenario 2 may include various erroneous clicks, such as those from bot traffic, accidental user clicks, click fraud, or click injections"

## Foundational Learning

- Concept: Multi-task learning optimization and task relationship modeling
  - Why needed here: Understanding how shared representations and task-specific layers interact is crucial for implementing the PWiseR loss alongside existing MTL architectures like MMoE and PLE
  - Quick check question: What is the difference between shared-bottom, MMoE, and PLE architectures in terms of how they handle task relationships?

- Concept: Ranking loss functions and pairwise comparison mechanisms
  - Why needed here: The PWiseR loss is fundamentally a ranking loss that compares pairs of samples; understanding margin-based ranking losses is essential for proper implementation and hyperparameter tuning
  - Quick check question: How does a pairwise ranking loss differ from a pointwise loss like BCE in terms of what signal it provides to the model?

- Concept: Click-through rate (CTR) and conversion rate (CVR) prediction in recommender systems
  - Why needed here: The method specifically addresses the sequential relationship between CTR and CVR tasks, so understanding their interdependence is critical for proper application
  - Quick check question: Why is the relationship pCTCVR = pCTR × pCVR important for understanding why conversions should be prioritized in ranking?

## Architecture Onboarding

- Component map: Input features → Shared representation layers → Task-specific towers → CTR and CVR predictions → BCE loss + PWiseR loss → Gradients → Parameter updates
- Critical path: The PWiseR loss requires access to both CTR and CVR predictions and labels, operating at the batch level by comparing samples within the same batch across task categories
- Design tradeoffs: The PWiseR loss adds computational overhead proportional to the number of sample pairs compared, but this is typically manageable since it only compares within-task categories (click-only vs conversion vs no-click)
- Failure signatures: If the PWiseR loss is too strong (high λ), the model may over-prioritize conversions and miss legitimate click opportunities. If too weak, it won't provide the intended noise reduction benefit
- First 3 experiments:
  1. Baseline test: Run the existing MTL model with only BCE loss to establish performance metrics
  2. Simple integration: Add PWiseR loss with λ=0.1 and default margins, compare AUC improvements
  3. Ablation study: Test PWiseR with BCE only on conversion samples vs full pairwise comparison to understand contribution of each term

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed PWiseR loss function perform in real-time production systems compared to offline benchmarks, particularly under varying levels of noise and data distribution shifts?
- Basis in paper: [inferred] The paper demonstrates effectiveness on both public and industrial datasets but does not address real-time performance or adaptability to changing conditions
- Why unresolved: Real-time system performance can differ significantly from offline benchmarks due to factors like latency, data stream variability, and dynamic user behavior
- What evidence would resolve it: Empirical studies comparing PWiseR loss performance in live production environments against BCE loss, with metrics on latency, stability under data drift, and adaptability to new patterns

### Open Question 2
- Question: Can the PWiseR loss function be extended to handle more than two tasks in a multi-task learning framework, and what are the theoretical limits of its scalability?
- Basis in paper: [explicit] The paper focuses on CTR and CVR tasks but mentions the potential for application to other tasks if both labels are available
- Why unresolved: The pairwise nature of the loss function may become computationally expensive or less effective as the number of tasks increases
- What evidence would resolve it: Theoretical analysis and experimental validation of PWiseR loss performance with three or more tasks, including computational complexity assessments and scalability studies

### Open Question 3
- Question: How sensitive is the PWiseR loss function to the choice of hyperparameters (λ, m1, m2), and what are the optimal strategies for tuning these parameters across different datasets and tasks?
- Basis in paper: [explicit] The paper mentions hyperparameter tuning using grid search but does not provide insights into sensitivity or tuning strategies
- Why unresolved: Hyperparameter sensitivity can significantly impact model performance and generalizability, and the paper does not explore this aspect in depth
- What evidence would resolve it: Sensitivity analysis of PWiseR loss to hyperparameter variations, including contour plots of performance metrics, and guidelines for automated or adaptive hyperparameter tuning methods

## Limitations

- Unknown dataset characteristics: The paper does not specify exact conversion rates, class imbalance ratios, or noise levels in the datasets, making it difficult to assess whether the proposed method's assumptions hold across different scenarios
- Implementation details: Key implementation aspects are unclear, including how the delta function is computed for batch processing and whether the loss uses all pairwise comparisons or sampled pairs
- Computational complexity: The pairwise comparison mechanism could become a bottleneck for very large-scale recommender systems, though the paper suggests this is manageable

## Confidence

- High confidence: The theoretical motivation for prioritizing conversions is sound - conversions are indeed more valuable and less noisy than clicks
- Medium confidence: The empirical results show consistent AUC improvements across multiple datasets and model architectures
- Low confidence: The claim that PWiseR "significantly" outperforms BCE loss is not well-supported without statistical significance testing

## Next Checks

1. **Ablation study**: Implement PWiseR with BCE only on conversion samples (removing the no-click comparison term) to quantify how much each component of the pairwise loss contributes to performance gains

2. **Noise injection experiment**: Systematically vary the noise level in click-only samples to test whether PWiseR performance degrades gracefully as the assumption of higher click noise becomes less valid

3. **Statistical significance analysis**: Run multiple training runs with different random seeds on the same datasets and compute confidence intervals for AUC improvements to determine if observed gains are statistically significant or within expected variance