---
ver: rpa2
title: 'Beyond Uniform Scaling: Exploring Depth Heterogeneity in Neural Architectures'
arxiv_id: '2402.12418'
source_url: https://arxiv.org/abs/2402.12418
tags:
- scaling
- neurons
- deit-s
- neural
- saddle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of conventional uniform scaling
  in neural networks, proposing an automated scaling approach that leverages second-order
  loss landscape information for non-uniform depth scaling. The method identifies
  neurons with saddle points using Hessian eigenvalues and scales them by adding new
  neurons through skip connections, preserving existing weights and biases.
---

# Beyond Uniform Scaling: Exploring Depth Heterogeneity in Neural Architectures

## Quick Facts
- arXiv ID: 2402.12418
- Source URL: https://arxiv.org/abs/2402.12418
- Authors: Akash Guna R. T; Arnav Chavan; Deepak Gupta
- Reference count: 8
- Primary result: Scaled DeiT-S achieves 2.5% accuracy gain and 10% parameter efficiency improvement on ImageNet100

## Executive Summary
This paper addresses the limitations of conventional uniform scaling in neural networks by proposing an automated scaling approach that leverages second-order loss landscape information for non-uniform depth scaling. The method identifies neurons with saddle points using Hessian eigenvalues and scales them by adding new neurons through skip connections, preserving existing weights and biases. This approach is designed to be applicable to modern vision transformers with skip connections, offering improved training efficiency and parameter utilization compared to traditional scaling methods.

## Method Summary
The proposed method involves training a reduced DeiT-S model initially, then periodically scaling the network by adding neurons to identified saddle points. The scaling process uses Hessian eigenvalue analysis to select neurons for expansion, with new neurons added via skip connections that preserve the original neuron outputs through opposite-polarity initialization. The approach automatically manages the parameter budget while allowing depth heterogeneity across different layers of the network.

## Key Results
- Scaled DeiT-S achieves 2.5% accuracy gain over conventional scaling on ImageNet100
- 10% parameter efficiency improvement while maintaining or improving accuracy
- Superior performance on small-scale datasets like CIFAR-100 when training from scratch
- Successfully aids transformers in escaping saddle points during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-uniform depth scaling based on saddle point analysis improves convergence and efficiency
- Mechanism: Identifies neurons with negative Hessian eigenvalues (saddle points) and adds new neurons via skip connections only to those neurons, preserving existing weights and biases
- Core assumption: Saddle points in the loss landscape slow training, and scaling only neurons near saddle points yields better efficiency than uniform scaling
- Evidence anchors: [abstract] "Our training-aware method jointly scales and trains transformers without additional training iterations." [section 3.2] "Based on this motivation, we use the identification of saddle points as a criterion for scaling the network."

### Mechanism 2
- Claim: Skip connections preserve function during non-uniform scaling
- Mechanism: New neurons are added as pseudo-skip connections that output to existing neurons, initialized with opposite polarities and small weights to preserve original outputs
- Core assumption: Skip connections can add neurons without disturbing existing outputs, allowing function preservation during initialization
- Evidence anchors: [section 3.4] "Our goal is to expand a subset of neurons NS from a layer consisting of NL neurons without disturbing the weights WS and bias BS."

### Mechanism 3
- Claim: Hessian eigenvalue magnitude correlates with beneficial scaling targets
- Mechanism: Neurons with smallest (most negative) eigenvalues are selected for scaling, indicating steepest saddle points and most potential for improvement
- Core assumption: Magnitude of negative eigenvalues indicates how "bad" a saddle point is, with larger magnitude requiring more urgent intervention
- Evidence anchors: [section 3.2] "A saddle point exists when a Hessian possesses a mix of positive and negative eigenvalues."

## Foundational Learning

- Concept: Second-order optimization and Hessian matrix properties
  - Why needed here: Method relies on Hessian eigenvalues to identify saddle points and guide scaling decisions
  - Quick check question: What does it mean when a Hessian has both positive and negative eigenvalues, and why is this relevant to neural network training?

- Concept: Skip connections and residual networks
  - Why needed here: Scaling mechanism uses skip connections to add neurons without modifying existing weights
  - Quick check question: How do skip connections help with gradient flow in deep networks, and why is this important for the proposed scaling approach?

- Concept: Neural network saddle points and loss landscape topology
  - Why needed here: Core hypothesis is that escaping saddle points improves training efficiency
  - Quick check question: Why do saddle points slow down neural network training, and how might adding neurons help escape them?

## Architecture Onboarding

- Component map: Reduced DeiT-S (11M parameters) with width-reduced intermediate layers -> Hessian-based neuron selection -> Skip-connection-based neuron addition -> Parameter budget management -> Layer threshold
- Critical path: 1. Train reduced model for warmup period 2. Compute splitting matrix for each layer 3. Select neurons with smallest negative eigenvalues 4. Add new neurons via skip connections 5. Continue training with expanded model 6. Repeat scaling at defined intervals
- Design tradeoffs: Depth heterogeneity vs. uniform scaling (better efficiency vs. simpler implementation), scaling frequency vs. computational overhead, parameter budget vs. potential overfitting
- Failure signatures: Training instability after scaling events, no improvement in convergence speed despite saddle point identification, memory overflow due to accumulated input copies
- First 3 experiments: 1. Verify function preservation by scaling a simple linear layer and confirming outputs remain unchanged 2. Test saddle point identification by comparing negative eigenvalue distribution before and after scaling 3. Validate scaling efficiency by comparing convergence speed and accuracy of scaled vs. uniformly scaled DeiT-S on ImageNet-100

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal frequency for scaling the neural network during training?
- Basis in paper: [explicit] The paper explores different scaling intervals (10, 20, 30, 50 epochs) to determine the most effective frequency for scaling a DeiT-S transformer
- Why unresolved: While the paper identifies 30 epochs as the optimal scaling interval, the reasons behind this specific interval's effectiveness are not fully explained
- What evidence would resolve it: Further experiments comparing different scaling intervals across various architectures and datasets

### Open Question 2
- Question: How does the proposed scaling method perform on larger datasets or more complex architectures beyond DeiT-S?
- Basis in paper: [inferred] The paper demonstrates effectiveness on DeiT-S with ImageNet-100 and CIFAR-100 datasets
- Why unresolved: Scalability and adaptability to larger datasets or more complex architectures are not addressed
- What evidence would resolve it: Testing on larger datasets like ImageNet-1000 or more complex architectures

### Open Question 3
- Question: How does the scaling method impact training time and computational resources required for model convergence?
- Basis in paper: [inferred] While improvements in parameter efficiency and accuracy are mentioned, impact on training time or computational resources is not discussed
- Why unresolved: Trade-offs between improved accuracy and parameter efficiency versus potential increases in training time are not explored
- What evidence would resolve it: Experiments measuring training time and resource usage with and without the scaling method

## Limitations

- The method's effectiveness depends on the assumption that saddle points significantly impede training, which may not hold for modern optimizers with adaptive learning rates
- Hessian approximation via splitting matrices may not accurately capture true saddle point locations in deep networks
- The scaling mechanism adds parameters without removing any, potentially leading to parameter explosion over multiple scaling iterations

## Confidence

- High confidence: The mathematical framework for function preservation through skip connections is sound, and empirical results on ImageNet-100 show clear improvements
- Medium confidence: Hessian-based saddle point identification provides reasonable guidance for scaling, though approximation accuracy and correlation with actual training benefits remain uncertain
- Low confidence: The claim that this approach will significantly improve training efficiency across diverse architectures and datasets, as current evidence is limited to specific transformer variants and smaller datasets

## Next Checks

1. Scale to larger datasets: Validate the approach on full ImageNet-1K and larger datasets like JFT-300M to test whether the 2.5% accuracy gain and 10% parameter efficiency improvements hold at scale

2. Test on non-transformer architectures: Apply the method to CNNs (ResNet, ConvNeXt) and other architectures without skip connections to evaluate generalization beyond the current scope

3. Analyze saddle point correlation: Conduct ablation studies comparing scaling based on Hessian eigenvalues versus random neuron selection or other criteria to quantify the actual contribution of saddle point identification to performance gains