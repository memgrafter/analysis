---
ver: rpa2
title: 'SEMINAR: Search Enhanced Multi-modal Interest Network and Approximate Retrieval
  for Lifelong Sequential Recommendation'
arxiv_id: '2407.10714'
source_url: https://arxiv.org/abs/2407.10714
tags:
- multi-modal
- sequence
- search
- item
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses challenges in lifelong sequential recommendation,
  including insufficient learning of ID embeddings and difficulties in modeling multi-modal
  features. The authors propose SEMINAR, a framework with a Pretraining Search Unit
  (PSU) that pretrains on lifelong query-item sequences using tasks like multi-modal
  alignment and next-pair prediction.
---

# SEMINAR: Search Enhanced Multi-modal Interest Network and Approximate Retrieval for Lifelong Sequential Recommendation

## Quick Facts
- arXiv ID: 2407.10714
- Source URL: https://arxiv.org/abs/2407.10714
- Reference count: 23
- Key outcome: SEMINAR outperforms strong baselines on three datasets, achieving improvements in NDCG@K and AUC through pretraining on lifelong sequences and multi-modal product quantization for efficient retrieval.

## Executive Summary
This paper addresses lifelong sequential recommendation challenges including insufficient learning of ID embeddings and difficulties in modeling multi-modal features. The authors propose SEMINAR, a framework with a Pretraining Search Unit (PSU) that pretrains on lifelong query-item sequences using tasks like multi-modal alignment and next-pair prediction. This pretraining alleviates ID embedding issues and aligns multi-modal representations. For efficient online retrieval, they introduce a multi-modal product quantization strategy to approximate attention calculations. Experiments on three datasets show SEMINAR outperforms strong baselines, achieving improvements in NDCG@K and AUC. The multi-modal product quantization also demonstrates superior recall@K compared to other approximate retrieval methods.

## Method Summary
SEMINAR addresses lifelong sequential recommendation by first pretraining on lifelong query-item sequences using a Pretraining Search Unit (PSU) with multi-modal alignment, next-pair prediction, and query-item relevance tasks. This pretraining alleviates insufficient learning of ID embeddings for rare items. For efficient online retrieval, SEMINAR uses a two-stage approach: a General Search Unit (GSU) performs first-stage retrieval using pretrained embeddings, followed by an Exact Search Unit (ESU) for second-stage refinement. The framework also introduces multi-modal product quantization to approximate attention calculations, reducing computational complexity from O(L Ã— M Ã— d) to O(L Ã— MÂ² Ã— Nbits) while maintaining retrieval quality.

## Key Results
- SEMINAR outperforms strong baselines on three datasets (Amazon Movies & TV, KuaiSAR, Alipay) with improvements in NDCG@K and AUC metrics
- Multi-modal product quantization achieves superior recall@K compared to other approximate retrieval methods
- The two-stage retrieval system (GSU + ESU) effectively balances efficiency and accuracy for lifelong sequences
- Ablation studies confirm the importance of pretraining and multi-modal alignment components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining on lifelong query-item sequences reduces the insufficient learning problem of ID embeddings.
- Mechanism: The Pretraining Search Unit (PSU) learns from the lifelong sequence of multi-modal query-item pairs before fine-tuning, allowing the model to learn embeddings for IDs that may not appear in the downstream training data.
- Core assumption: The lifelong sequence contains enough context to learn meaningful representations for rarely seen or missing IDs.
- Evidence anchors:
  - [abstract] "Specifically, a network called Pretraining Search Unit (PSU) learns the lifelong sequences of multi-modal query-item pairs in a pretraining-finetuning manner with multiple objectives: multi-modal alignment, next query-item pair prediction, query-item relevance prediction, etc."
  - [section] "To help alleviate the insufficient learning problem of ID embedding in the lifelong sequence, the general search unit (GSU) in our proposed SEMINAR model shares the same multi-head target attention structure â„Žð‘’ð‘Žð‘‘ðºð‘†ð‘ˆ â„Ž = Attentionâ„Ž (ð‘žð‘¡ , ð¾ðºð‘†ð‘ˆ , ð‘‰ðºð‘†ð‘ˆ ) with the structure in PSU as â„Žð‘’ð‘Žð‘‘ ð‘ƒð‘†ð‘ˆ â„Ž = Attention(ð‘žð‘¡ , ð¾ð‘ƒð‘†ð‘ˆ , ð‘‰ð‘ƒð‘†ð‘ˆ ), restores the pretrained embedding from PSU and applies specific projection weight matrix ðº ( ð‘— ) âˆˆ Rð‘‘ Ã—ð‘‘ to the pretrained embedding."
  - [corpus] Weak signal - no direct evidence in the 25 related papers for this specific mechanism of pretraining to solve ID embedding issues in lifelong sequences.

### Mechanism 2
- Claim: Multi-modal alignment pretraining tasks prevent target attention from being dominated by high-norm modality vectors.
- Mechanism: The PSU includes multi-modal alignment objectives that ensure text, image, and attribute embeddings are projected into the same space, preventing one modality with large norm values from overwhelming the attention calculation.
- Core assumption: The alignment task during pretraining creates a balanced embedding space where no single modality dominates due to norm differences.
- Evidence anchors:
  - [abstract] "The distribution of multi-modal embedding (text, image and attributes) output of user's interacted items are not properly aligned and there exist divergence across modalities."
  - [section] "Multi-Modal Alignment and Query-Item Relevance. Multi-modal alignment is a crucial task, which learns the multi-modal representation in a same embedding space... We simultaneously train multi-modal alignment tasks, including text-image, image-attributes, text-attributes with the cross entropy loss of N pairs."
  - [section] "The â„Ž-th head in the multi-head attention is represented as â„Žð‘’ð‘Žð‘‘ ð‘ƒð‘†ð‘ˆ â„Ž = Attentionâ„Ž (ð‘žð‘¡ , ð¾ð‘ƒð‘†ð‘ˆ , ð‘‰ð‘ƒð‘†ð‘ˆ ), and the attention score ð›¼ð‘ƒð‘†ð‘ˆ â„Ž is calculated as inner product of d-dimensional vector query and keys multiplied by a scaling factor 1âˆšð‘‘."
  - [corpus] Weak signal - no direct evidence in the 25 related papers for this specific mechanism of multi-modal alignment during pretraining to solve attention dominance issues.

### Mechanism 3
- Claim: Multi-modal product quantization approximates exact attention calculations with significantly reduced time complexity.
- Mechanism: The model splits multi-modal embeddings into sub-vectors, assigns each to the nearest centroid in codebooks, and approximates the attention score as the weighted sum of pre-computed distances between centroids, reducing complexity from O(L Ã— M Ã— d) to O(L Ã— MÂ² Ã— Nbits).
- Core assumption: The sum of distances between centroid pairs approximates the inner product of the original vectors well enough for retrieval quality.
- Evidence anchors:
  - [abstract] "To accelerate the online retrieval speed of multi-modal embedding, we propose a multi-modal codebook-based product quantization strategy to approximate the exact attention calculation and significantly reduce the time complexity."
  - [section] "Our proposed multi-modal product quantization strategy works quite well in real-world settings. We also compare the time complexity of different strategies... Our proposed multi-modal PQ method has the time complexity of ð‘‚ (ð¿ Ã— ð‘€2 Ã— ð‘ð‘ð‘–ð‘¡ )."
  - [section] "The exact calculation of the attention score between the target query-item pair ð‘žð‘¡ and the ð‘™-th query-item behaviorð‘˜ð‘™ is the inner product of the weighted sum of multiple vectors as: ð‘žð‘¡ð‘‡ ð‘˜ð‘™ = (âˆ‘ð‘–âˆˆð‘€+1 ð›¾ð‘–ð‘¥(ð‘–)ð‘¡)ð‘‡ (âˆ‘ð‘—âˆˆð‘€+1 ð›¾ð‘—ð‘¥(ð‘—)ð‘™), âˆ€ð‘™ âˆˆ { 1, 2, ...ð¿ }"
  - [section] "To help increase the recall performance while considering the retrieval speed, the key is to reduce the cardinality of the query set Q and the item set B. We argue that product quantization is a good approximation strategy, which splits vectors into ð‘ð‘ð‘–ð‘¡ sub-vectors, assigns each sub-vector to the nearest centroid, and reduces the cardinality."
  - [corpus] Weak signal - no direct evidence in the 25 related papers for this specific mechanism of multi-modal product quantization for approximate retrieval in lifelong sequences.

## Foundational Learning

- Concept: Lifelong sequential modeling
  - Why needed here: The paper addresses modeling users' extremely long historical behavior sequences (thousands of items) which is a core challenge in modern recommendation systems.
  - Quick check question: What is the typical sequence length range mentioned in the paper for lifelong user behaviors?

- Concept: Multi-modal embeddings and alignment
  - Why needed here: Items have multiple feature types (text, image, attributes) that need to be represented in a unified space for effective attention calculation.
  - Quick check question: What are the three types of multi-modal features mentioned that need alignment in the embedding space?

- Concept: Product quantization for approximate nearest neighbor search
  - Why needed here: Exact attention calculation on lifelong sequences is computationally expensive, requiring an approximation strategy that maintains retrieval quality.
  - Quick check question: What is the original time complexity of exact attention calculation that the product quantization strategy aims to reduce?

## Architecture Onboarding

- Component map:
  - Input: Aligned sequence of multi-modal query-item pairs [Q, T, I, A]
  - PSU (Pretraining Search Unit): Pretrains on lifelong sequences with multi-modal alignment, next-pair prediction, and query-item relevance tasks
  - GSU (General Search Unit): First-stage retrieval using target query-item pair, restores pretrained embeddings, projects with specific weights
  - ESU (Exact Search Unit): Second-stage retrieval from GSU output, applies multi-head target attention
  - Multi-modal Product Quantization: Online approximation strategy for fast retrieval
  - Output: Users' lifelong sequence representation for CTR prediction

- Critical path: Input â†’ PSU pretraining â†’ Restore pretrained embeddings in GSU â†’ First-stage retrieval (L â†’ K) â†’ Second-stage retrieval in ESU (K â†’ final) â†’ CTR prediction

- Design tradeoffs:
  - Pretraining vs. direct training: Pretraining addresses ID embedding issues but adds complexity and requires additional data
  - Exact vs. approximate retrieval: Exact calculation is accurate but slow; PQ approximation is fast but introduces error
  - Multi-modal fusion: How to balance query and item representations (weight Î») affects performance

- Failure signatures:
  - Poor performance on rare items: Indicates insufficient learning of ID embeddings despite pretraining
  - Degraded performance when one modality has much larger norm values: Suggests multi-modal alignment pretraining failed
  - Low recall@K with PQ approximation: Indicates codebook size or number of bits is insufficient
  - Training instability: May indicate issues with pretraining task balance or hyperparameter settings

- First 3 experiments:
  1. Ablation study: Train without PSU pretraining vs. with pretraining to verify the impact on ID embedding learning and overall performance
  2. Modality norm analysis: Compare attention weights and performance when one modality has artificially increased norm values to test alignment effectiveness
  3. PQ approximation accuracy: Measure recall@K degradation as codebook size decreases to find the minimum acceptable configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SEMINAR vary with different lengths of lifelong sequences (L) in real-world datasets?
- Basis in paper: [explicit] The paper mentions that lifelong sequences can exceed thousands of items and discusses time complexity considerations for different sequence lengths.
- Why unresolved: The experiments were conducted with fixed sequence lengths (L=2000, 1000, 100) for different datasets, but did not explore how varying L affects performance.
- What evidence would resolve it: Systematic experiments varying L across a wider range and measuring performance metrics like NDCG@K and AUC would clarify the impact of sequence length on SEMINAR's effectiveness.

### Open Question 2
- Question: What is the impact of different fusion weights (Î») for query and item representations across different domains and datasets?
- Basis in paper: [explicit] The paper investigates different Î» values (0.1, 0.3, 0.5, 0.7, 0.9) on the KuaiSAR dataset and suggests that optimal Î» may vary across domains.
- Why unresolved: Only one dataset was tested, and the optimal Î» value might depend on the specific characteristics of each dataset's query-to-recommendation ratio.
- What evidence would resolve it: Testing SEMINAR with varying Î» values across multiple datasets from different domains would reveal patterns in optimal Î» selection.

### Open Question 3
- Question: How does the multi-modal product quantization strategy perform compared to other approximation methods on extremely long sequences (L > 10,000)?
- Basis in paper: [inferred] The paper discusses time complexity concerns for exact attention calculations on long sequences and proposes multi-modal product quantization as a solution, but only tests up to L=10,000 in synthetic data.
- Why unresolved: Real-world applications may have even longer sequences, and the scalability of the proposed method to such cases is unknown.
- What evidence would resolve it: Experiments with real-world datasets containing sequences longer than 10,000 items would demonstrate the practical limits of the multi-modal product quantization approach.

## Limitations

- The paper's effectiveness relies heavily on the quality and diversity of the lifelong sequence data used for pretraining, but specific details about data preprocessing and alignment quality are not provided.
- The multi-modal product quantization strategy's performance depends critically on hyperparameter choices (codebook size, number of bits) that are not fully specified.
- The computational overhead of the two-stage retrieval system (GSU + ESU) versus simpler single-stage approaches is not clearly quantified.

## Confidence

- **High confidence**: The overall framework architecture combining pretraining with multi-modal alignment and approximate retrieval is technically sound and well-motivated by the problem statement.
- **Medium confidence**: The mechanism by which PSU pretraining alleviates ID embedding issues is reasonable but lacks direct empirical validation in the paper.
- **Medium confidence**: The multi-modal product quantization approach for approximate retrieval is theoretically justified, though practical effectiveness depends heavily on implementation details.

## Next Checks

1. **Ablation study on pretraining**: Compare performance with and without PSU pretraining across all three datasets, specifically measuring ID embedding quality through downstream metrics on rare items.

2. **Codebook sensitivity analysis**: Systematically vary codebook size and number of bits in the product quantization strategy to identify the point where recall@K performance significantly degrades, establishing practical limits.

3. **Modality norm imbalance test**: Conduct controlled experiments where one modality's embedding norm is artificially increased by 10-100x to verify whether the pretraining alignment effectively prevents attention dominance by any single modality.