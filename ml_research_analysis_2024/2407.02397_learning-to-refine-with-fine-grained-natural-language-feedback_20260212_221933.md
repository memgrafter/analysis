---
ver: rpa2
title: Learning to Refine with Fine-Grained Natural Language Feedback
arxiv_id: '2407.02397'
source_url: https://arxiv.org/abs/2407.02397
tags:
- feedback
- response
- document
- refinement
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a three-stage method for refining LLM-generated
  responses using fine-grained natural language feedback. The approach, called DETECT,
  CRITIQUE, REFINE (DCR), first identifies errors in individual sentences, generates
  detailed feedback about those errors, and then refines the response based on that
  feedback.
---

# Learning to Refine with Fine-Grained Natural Language Feedback

## Quick Facts
- arXiv ID: 2407.02397
- Source URL: https://arxiv.org/abs/2407.02397
- Authors: Manya Wadhwa; Xinyu Zhao; Junyi Jessy Li; Greg Durrett
- Reference count: 40
- Key outcome: A three-stage method that uses sentence-level error detection and fine-grained feedback to significantly improve factual consistency in LLM-generated summaries.

## Executive Summary
This paper introduces DETECT, CRITIQUE, REFINE (DCR), a three-stage method for refining LLM-generated document-grounded summaries using fine-grained natural language feedback. The approach first detects factual errors at the sentence level, generates detailed feedback about those errors, and then refines the response based on that feedback. By separating detection from critique, the method enables more targeted and effective refinements compared to existing approaches. Experiments on document-grounded summarization tasks show consistent improvements in factual consistency, with smaller models fine-tuned on GPT-4-generated feedback achieving significant gains that often match or exceed GPT-4 performance.

## Method Summary
DCR is a three-stage pipeline for post-hoc refinement of LLM-generated responses. Stage 1 (DETECT) uses a sentence-level factual consistency detector (MiniCheck) to identify which sentences contain errors. Stage 2 (CRITIQUE) generates structured feedback about the errors, including error spans, reasoning, and suggested fixes. Stage 3 (REFINE) applies the feedback to make targeted edits to the original response. The critique and refinement models are fine-tuned on structured feedback and refinements generated by a stronger teacher model (GPT-4), enabling smaller models to perform effective fine-grained feedback generation and targeted editing.

## Key Results
- DCR consistently outperforms existing refinement approaches, achieving significant improvements in factual consistency metrics across multiple datasets.
- The method demonstrates that smaller models can be effectively fine-tuned to generate high-quality critiques and refinements, often matching or exceeding GPT-4 performance.
- Ablation studies show that each component of the DCR pipeline contributes meaningfully to overall performance, with the fine-grained feedback approach outperforming detection-only methods.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Splitting detection from critique allows the critique model to focus on detailed feedback generation rather than verification.
- Mechanism: The detector (MiniCheck) performs sentence-level factual consistency checks, which offloads the verification burden from the critique model. This enables the critique model to enumerate specific errors and suggest fixes without also needing to verify correctness.
- Core assumption: Models can perform detection and critique as separate, specialized tasks more effectively than combining them into a single step.
- Evidence anchors: [abstract] "A key property of the proposed DETECT, CRITIQUE, REFINE ("DCR") method is that the step 2 critique model can give fine-grained feedback about errors, made possible by offloading the discrimination to a separate model in step 1."

### Mechanism 2
- Claim: Fine-tuning smaller models on structured feedback from stronger models enables them to generate high-quality critiques and refinements.
- Mechanism: GPT-4 generates structured feedback (error span, reasoning, fix) and refinements, which are then used to fine-tune smaller Llama models. This distillation process transfers capability to more efficient models.
- Core assumption: Structured feedback from stronger models can be effectively learned by smaller models through supervised fine-tuning.
- Evidence anchors: [section 3.1] "We construct our fine-tuning data over a collection of (document, response) pairs... For each ri detected to have an error, we prompt a teacher model Mteacher using pcritique to give a structured feedback."

### Mechanism 3
- Claim: Sentence-level error localization enables precise, targeted edits that preserve response structure and content.
- Mechanism: The critique model identifies specific spans within sentences that contain errors and suggests exact fixes. The refinement model then makes minimal edits to correct these spans while maintaining overall coherence.
- Core assumption: Precise span-level corrections are more effective than general rewrite instructions for maintaining response quality.
- Evidence anchors: [section 4.2] "The feedback in this case is generated sentence wise, combined together and then used for refinement."

## Foundational Learning

- Concept: Sentence-level factual consistency detection
  - Why needed here: DCR relies on detecting errors at the sentence level before generating feedback, which requires understanding how to verify factual consistency between text spans and source documents.
  - Quick check question: How does MiniCheck achieve sentence-level factual consistency detection, and what metrics indicate its effectiveness?

- Concept: Structured feedback generation
  - Why needed here: The critique model must generate feedback containing error localization, reasoning, and suggested fixes in a specific format that the refinement model can use.
  - Quick check question: What are the four key components of the structured feedback format used for fine-tuning the critique model?

- Concept: Supervised fine-tuning with structured data
  - Why needed here: Both critique and refinement models are fine-tuned using outputs from a stronger teacher model, requiring understanding of how to structure training data and training objectives.
  - Quick check question: What are the input-output pairs used for fine-tuning the critique model versus the refinement model?

## Architecture Onboarding

- Component map: Mdetect (MiniCheck) -> Mcritique (fine-tuned or prompted) -> Mrefine (fine-tuned or prompted) -> Mteacher (GPT-4 for training data)
- Critical path: DETECT → CRITIQUE → REFINE cascade where each step depends on successful completion of the previous step
- Design tradeoffs:
  - Detection accuracy vs. computational cost (using MiniCheck vs. GPT-4)
  - Feedback detail level vs. model capacity requirements
  - Training data quality vs. fine-tuning effectiveness
- Failure signatures:
  - High unchanged response rate indicates detection or refinement issues
  - Decreased AlignScore or GPT-4 scores indicate refinement quality problems
  - Low match rate with human feedback indicates critique model quality issues
- First 3 experiments:
  1. Run DCR pipeline on a small validation set and measure unchanged response rate to verify detection effectiveness
  2. Compare feedback quality between fine-tuned critique model and GPT-4 using human evaluation
  3. Measure edit distance between original and refined responses to verify refinement preserves structure while correcting errors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of the DETECT step on overall refinement performance across different types of factual errors?
- Basis in paper: [explicit] The paper shows that DCR outperforms DETECT + DR, indicating the CRITIQUE and REFINE steps contribute significantly. However, it does not analyze the individual impact of the DETECT step on performance across different error types.
- Why unresolved: The paper only evaluates the overall performance of the DCR pipeline without breaking down the contribution of the DETECT step for different error categories.
- What evidence would resolve it: An ablation study showing performance differences when removing the DETECT step for various error types (e.g., mis-referencing, extrinsic information, contradiction) would clarify its impact.

### Open Question 2
- Question: How does the effectiveness of DCR change when applied to languages other than English or different document-grounded tasks?
- Basis in paper: [inferred] The paper acknowledges that the approach is not fundamentally restricted to English-language refinement in these domains, but all experiments are conducted in English with document-grounded summarization tasks.
- Why unresolved: The paper does not explore cross-lingual or cross-task generalization, leaving uncertainty about DCR's applicability to other languages or different document-grounded tasks like question answering or long-form generation.
- What evidence would resolve it: Experiments applying DCR to document-grounded tasks in other languages or to different tasks (e.g., document-grounded QA) with evaluation of performance and feedback quality would demonstrate generalizability.

### Open Question 3
- Question: What is the relationship between the granularity of error spans detected and the quality of refinements produced?
- Basis in paper: [explicit] The paper shows that models detect errors at different granularities (word, phrase, sentence) and that this affects the types of edits made. However, it does not directly analyze whether finer-grained error detection leads to better refinement quality.
- Why unresolved: While the paper observes variation in error granularity, it does not establish a correlation between error span granularity and refinement quality metrics like factual consistency improvement or preservation of original content.
- What evidence would resolve it: A controlled study varying the granularity of error detection and measuring corresponding refinement quality (e.g., factual consistency improvement, edit distance, preservation of style) would reveal the optimal error detection granularity for high-quality refinements.

## Limitations
- The approach relies heavily on the quality of the teacher model (GPT-4) for generating training data, creating a dependency that may limit scalability and reproducibility.
- While improvements in factual consistency metrics are demonstrated, the paper does not thoroughly evaluate whether refinements preserve other aspects of response quality such as coherence, fluency, or adherence to the original summary structure.
- The approach is only evaluated on document-grounded summarization tasks in English, leaving uncertainty about generalizability to other domains or languages.

## Confidence
- High confidence in the core claim that fine-grained feedback improves refinement effectiveness, supported by significant improvements in factual consistency metrics across multiple datasets and model configurations.
- Medium confidence in the claim that smaller models can effectively generate high-quality critiques and refinements after fine-tuning, as results show promising improvements but lack clear upper bounds.
- Low confidence in the generalizability of the approach to domains beyond document-grounded summarization, as the paper focuses exclusively on this task.

## Next Checks
1. **Human evaluation of refinement quality**: Conduct detailed human studies to assess whether the refined responses maintain coherence and fluency while improving factual consistency. This would help validate that the approach doesn't introduce new quality issues while fixing factual errors.

2. **Cross-domain robustness testing**: Apply the DCR pipeline to different types of LLM outputs (e.g., long-form QA, dialogue responses, code generation) to evaluate how well the detection and critique components generalize beyond document summarization.

3. **Teacher model sensitivity analysis**: Compare the performance of models fine-tuned on different teacher models (e.g., GPT-3.5 vs GPT-4 vs Claude) to understand how sensitive the approach is to the quality and style of the teacher-generated feedback.