---
ver: rpa2
title: 'TransformerFAM: Feedback attention is working memory'
arxiv_id: '2404.09173'
source_url: https://arxiv.org/abs/2404.09173
tags:
- memory
- attention
- transformer
- block
- transformerfam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TransformerFAM is a new architecture that adds a feedback loop
  to the Transformer, allowing it to attend to its own latent representations. This
  feedback loop enables the emergence of working memory within the Transformer, allowing
  it to process indefinitely long sequences without additional weights.
---

# TransformerFAM: Feedback attention is working memory

## Quick Facts
- arXiv ID: 2404.09173
- Source URL: https://arxiv.org/abs/2404.09173
- Reference count: 40
- Key outcome: TransformerFAM adds feedback loop to Transformer, enabling working memory for indefinitely long sequences, achieving up to 1.0% improvement across tasks

## Executive Summary
TransformerFAM introduces a novel architecture that enables Transformers to attend to their own latent representations through a feedback loop mechanism. This design fosters the emergence of working memory within Transformers, allowing them to process indefinitely long sequences without additional weights. The architecture significantly improves performance on long-context tasks across various model sizes (1B, 8B, and 24B), demonstrating that feedback attention can effectively compress and propagate contextual information over long sequences.

## Method Summary
TransformerFAM builds upon Block Sliding Window Attention (BSWA) by adding a Feedback Attention Memory (FAM) component that creates a recurrent pathway where output from one block serves as input to the next. During training, the model uses random position offset and state passing techniques, while inference maintains O(L) computational complexity and O(1) memory complexity. The architecture combines local context from memory segments with compressed global representation from FAM to process both local and global information efficiently.

## Key Results
- Achieves up to 1.0% average improvement across long-context tasks
- Maintains linear computational complexity O(L) and constant memory complexity O(1)
- Improves performance on Isabelle, NarrativeQA, PG-19, ScrollsQasper, ScrollsQuality, and XLSum tasks
- Outperforms TransformerBSWA baseline on long-context tasks

## Why This Works (Mechanism)

### Mechanism 1
The feedback loop enables working memory by allowing the Transformer to attend to its own latent representations, compressing and propagating contextual information indefinitely. The recurrent pathway where FAM output serves as next block input creates a working memory that can store and propagate context without forgetting.

### Mechanism 2
TransformerFAM maintains linear complexity with respect to input sequence length while enabling indefinite context processing. Block-wise feedback updates and fixed-size FAM mean the model only processes constant additional information per block, avoiding quadratic attention over entire sequences.

### Mechanism 3
The combination of BSWA memory segments and FAM provides complementary local and global representations, improving overall performance. Local memory segments capture context within each block while FAM provides compressed global representation of all previous blocks, allowing efficient processing of both local and global information.

## Foundational Learning

- **Attention mechanisms**: Understanding how self-attention works in Transformers is crucial for grasping how TransformerFAM extends this with feedback loops to create working memory. Quick check: How does self-attention in a standard Transformer work, and what are its computational complexity limitations?

- **Working memory in cognitive science**: The paper draws inspiration from working memory concepts in human brain to design feedback loop mechanism. Quick check: What is working memory, and how does it differ from long-term memory in human brain?

- **Block-wise processing and sliding window attention**: TransformerFAM builds upon Block Sliding Window Attention (BSWA) as foundation. Quick check: How does sliding window attention differ from standard self-attention, and what are its advantages for processing long sequences?

## Architecture Onboarding

- **Component map**: Input block -> BSWA memory segments -> FAM -> Transformer layer -> FAM update mechanism -> Next block
- **Critical path**: 1) Process input block through standard Transformer layers, 2) Attend to both BSWA memory segments and previous FAM, 3) Compress current block information into updated FAM, 4) Pass updated FAM to next block
- **Design tradeoffs**: Complexity vs. performance (FAM increases complexity but improves long-context performance), memory vs. compression (FAM length must balance compression without losing details), training vs. inference (random techniques used in training only)
- **Failure signatures**: Poor long-context performance (FAM not effectively compressing information), memory overflow (FAM length or block size needs adjustment), training instability (issues with feedback loop or random state passing)
- **First 3 experiments**: 1) PassKey retrieval task (tests ability to maintain information over very long contexts), 2) Long Context Tasks (NarrativeQA, PG-19, etc. - evaluates long-context understanding), 3) GPT-3 task performance (ensures FAM doesn't negatively impact shorter sequences)

## Open Questions the Paper Calls Out

### Open Question 1
How does TransformerFAM performance compare to other long-context transformer architectures like Infini-Transformer or Landmark Attention when scaled to larger model sizes? The paper only compares to TransformerBSWA and few other methods on specific tasks and model sizes, missing comprehensive comparison with other long-context architectures.

### Open Question 2
What is optimal balance between local representation (memory segments) and global representation (FAM) for different types of long-context tasks and model sizes? The paper mentions they complement each other but doesn't explore optimal balance systematically across tasks and model sizes.

### Open Question 3
Can feedback loop mechanism in TransformerFAM be extended to handle multimodal data (text, images, audio) and improve performance on multimodal tasks? The paper focuses on text data only, leaving exploration of multimodal applications as future work.

## Limitations

- Theoretical foundation connecting feedback mechanism to human working memory lacks rigorous neuroscientific validation
- Evaluation primarily focuses on language tasks, limiting generalizability to other domains
- Computational complexity claims don't fully account for practical implementation details and memory usage
- Training stability relies on techniques that may indicate fragility not fully explored

## Confidence

- **High**: Core empirical findings showing improved performance on long-context tasks (statistically significant across multiple model scales)
- **Medium**: Theoretical mechanism explaining how feedback loops create working memory (empirical support but cognitive science parallels are speculative)
- **Low**: Computational complexity claims and generalizability across different domains and tasks (limited empirical validation)

## Next Checks

- **Validation Check 1**: Conduct systematic ablation studies removing different components of feedback mechanism (FAM, BSWA, random position offset, state passing) to quantify individual contributions to performance gains.
- **Validation Check 2**: Perform detailed profiling of memory consumption during both training and inference across different sequence lengths and model scales, comparing actual usage against theoretical O(1) claim.
- **Validation Check 3**: Evaluate TransformerFAM on non-language tasks including vision transformers, graph transformers, and multimodal transformers to test whether working memory benefits generalize beyond language domain.