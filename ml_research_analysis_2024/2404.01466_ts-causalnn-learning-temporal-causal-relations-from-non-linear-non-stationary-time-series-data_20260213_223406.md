---
ver: rpa2
title: 'TS-CausalNN: Learning Temporal Causal Relations from Non-linear Non-stationary
  Time Series Data'
arxiv_id: '2404.01466'
source_url: https://arxiv.org/abs/2404.01466
tags:
- causal
- proposed
- data
- time
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TS-CausalNN, a deep learning-based method for
  causal discovery from multivariate time series data that can handle non-linearity,
  non-stationarity, and various noise distributions. The method uses a novel 2D convolutional
  neural network architecture with parallel custom causal layers to learn both contemporaneous
  and lagged causal relationships simultaneously.
---

# TS-CausalNN: Learning Temporal Causal Relations from Non-linear Non-stationary Time Series Data

## Quick Facts
- arXiv ID: 2404.01466
- Source URL: https://arxiv.org/abs/2404.01466
- Authors: Omar Faruque; Sahara Ali; Xue Zheng; Jianwu Wang
- Reference count: 40
- One-line primary result: TS-CausalNN outperforms state-of-the-art methods in discovering accurate causal graphs from multivariate time series data with non-linearity, non-stationarity, and various noise distributions.

## Executive Summary
This paper introduces TS-CausalNN, a deep learning method for causal discovery from multivariate time series data that can handle non-linearity, non-stationarity, and various noise distributions. The method uses a novel 2D convolutional neural network architecture with parallel custom causal layers to learn both contemporaneous and lagged causal relationships simultaneously. Experiments on synthetic and real-world datasets demonstrate that TS-CausalNN outperforms existing state-of-the-art methods in discovering accurate causal graphs, achieving better F1 scores and lower false discovery rates.

## Method Summary
TS-CausalNN employs a 2D convolutional neural network with parallel custom causal layers to simultaneously learn contemporaneous and lagged causal relationships from multivariate time series data. The method incorporates an acyclicity constraint and sparsity penalty using the augmented Lagrangian approach for optimization. The architecture processes lagged time series data through convolutional blocks, with each parallel causal layer dedicated to learning the causal structure for one target variable. The learned weights are then combined to form the full adjacency matrix representing the causal graph, with thresholding applied to enforce sparsity.

## Key Results
- TS-CausalNN achieves higher F1 scores and lower false discovery rates compared to state-of-the-art methods on synthetic datasets
- The method demonstrates strong performance on complex real-world datasets like Turbulence Kinetic Energy and Arctic Sea Ice data
- Ablation studies show the 2D convolutional architecture with parallel causal layers significantly improves causal graph learning performance

## Why This Works (Mechanism)

### Mechanism 1
Parallel 2D causal convolution layers enable simultaneous learning of contemporaneous and lagged causal links without requiring separate optimization phases. Each parallel causal layer is dedicated to a target variable and learns weights for all possible parents (including lagged and contemporaneous), while maintaining the acyclic constraint only on the contemporaneous part. This design avoids the need to decompose the problem into two separate learning steps.

### Mechanism 2
Augmented Lagrangian optimization enforces acyclicity while allowing sparsity and non-linearity in the causal structure. The acyclicity constraint is reformulated as an unconstrained problem by adding a penalty term. This enables gradient-based optimization while gradually tightening the acyclicity condition.

### Mechanism 3
2D convolution layers naturally handle non-stationarity and non-linearity in time series data better than 1D convolution layers. The 2D structure allows the model to learn spatial-temporal patterns across variables simultaneously, capturing interactions that 1D convolutions might miss.

## Foundational Learning

- Concept: Directed Acyclic Graph (DAG) structure in time series causal discovery
  - Why needed here: The method assumes the underlying causal structure is a DAG, which requires enforcing acyclicity in the contemporaneous part of the graph while allowing lagged edges to be naturally acyclic.
  - Quick check question: Why do lagged edges not require explicit acyclicity constraints in this method?

- Concept: Augmented Lagrangian method for constrained optimization
  - Why needed here: The acyclicity constraint cannot be directly optimized with gradient descent, so it is reformulated as an unconstrained problem with a penalty term that is gradually increased.
  - Quick check question: How does the augmented Lagrangian method balance the trade-off between minimizing the loss and satisfying the acyclicity constraint?

- Concept: Structural equation models (SEMs) for time series
  - Why needed here: The method learns the structural parameters of an SEM that describes how each variable depends on its parents, including both contemporaneous and lagged effects.
  - Quick check question: What is the role of the noise term in the structural equation model, and how does the method handle different noise distributions?

## Architecture Onboarding

- Component map: Input → Conv2D → Parallel Causal Conv2D → Concatenation → Thresholding → Output
- Critical path: Input → Conv2D → Parallel Causal Conv2D → Concatenation → Thresholding → Output
- Design tradeoffs:
  - Parallel structure vs. sequential: Parallel allows simultaneous learning but may require more memory
  - 2D vs. 1D convolution: 2D captures cross-variable interactions better but is more complex
  - Thresholding vs. continuous weights: Thresholding enforces sparsity but loses information about weight magnitudes

- Failure signatures:
  - If acyclicity is not satisfied, the contemporaneous part of the graph may contain cycles
  - If the model underfits, the adjacency matrix may be too sparse and miss true causal links
  - If the model overfits, the adjacency matrix may be too dense and include many false links

- First 3 experiments:
  1. Train on synthetic Dataset-1 with known causal structure; verify that the learned adjacency matrix matches the ground truth
  2. Vary the threshold value and observe its effect on the sparsity and accuracy of the predicted graph
  3. Compare performance on stationary vs. non-stationary versions of the same dataset to validate robustness claims

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of TS-CausalNN scale with the number of variables in the time series data? The paper tests on datasets with up to 11 variables but does not explore scalability to much larger systems.

### Open Question 2
How sensitive is TS-CausalNN to the choice of hyperparameters, particularly the sparsity penalty λ and the augmented Lagrangian parameters ρ and α?

### Open Question 3
Can TS-CausalNN handle datasets with missing values or irregularly sampled time series data, which are common in real-world applications?

## Limitations
- Scalability to high-dimensional time series with large numbers of variables is uncertain
- Performance on real-world datasets with complex noise structures and potential hidden confounders needs further validation
- Computational cost of augmented Lagrangian optimization with multiple hyperparameters may be prohibitive for very large datasets

## Confidence
- Core claims about superior performance on synthetic data: Medium
- Claims about handling non-stationarity and various noise distributions: Medium
- Claims about real-world applicability: Low

## Next Checks
1. Test the method on additional real-world datasets with known causal structures and varying levels of non-stationarity, non-linearity, and noise complexity.
2. Perform an ablation study to isolate the contribution of the 2D convolution architecture, the parallel causal layers, and the augmented Lagrangian optimization to the overall performance.
3. Evaluate the scalability of the method to high-dimensional time series by testing on datasets with increasing numbers of variables and lags, and analyze the computational time and memory requirements.