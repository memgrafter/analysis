---
ver: rpa2
title: Fooling SHAP with Output Shuffling Attacks
arxiv_id: '2408.06509'
source_url: https://arxiv.org/abs/2408.06509
tags:
- attack
- data
- attacks
- shap
- shuffling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces data-agnostic shuffling attacks that can
  fool Shapley value-based explanations like SHAP without requiring access to underlying
  data distribution. The attack strategies modify model outputs through permutations
  that preserve expectation values but introduce unfairness.
---

# Fooling SHAP with Output Shuffling Attacks

## Quick Facts
- arXiv ID: 2408.06509
- Source URL: https://arxiv.org/abs/2408.06509
- Authors: Jun Yuan; Aritra Dasgupta
- Reference count: 8
- Key outcome: Data-agnostic shuffling attacks can fool Shapley value-based explanations like SHAP without requiring access to underlying data distribution

## Executive Summary
This paper introduces output shuffling attacks that can manipulate Shapley value-based explanations (SHAP) to evade fairness detection. The attacks modify model outputs through permutations that preserve expectation values while introducing unfairness, exploiting the order-agnostic property of Shapley value calculations. The authors prove theoretical Shapley values cannot detect these attacks, though practical SHAP implementations can detect them with varying effectiveness. Experiments on real-world datasets demonstrate that shuffling attacks can successfully manipulate attributions to protected features.

## Method Summary
The paper introduces data-agnostic shuffling attacks that modify model outputs without accessing underlying data distribution. Three attack strategies are presented: Dominance (complete reordering), Mixing (probabilistic reordering), and Swapping (adjacent reordering). These attacks preserve the expectation of model outputs while introducing unfairness by shuffling scores based on protected feature values. The method is tested on three real-world datasets using SHAP and linearSHAP implementations to analyze detection effectiveness.

## Key Results
- Data-agnostic shuffling attacks can successfully manipulate model attributions to protected features
- Theoretical Shapley values cannot detect shuffling attacks due to expectation calculation properties
- Practical SHAP implementations (linearSHAP, SHAP) can detect shuffling attacks with varying effectiveness
- Detection difficulty depends on attack parameters, attack type, and specific SHAP implementation used

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data-agnostic shuffling attacks can fool SHAP by exploiting expectation calculation's order-agnostic property
- Mechanism: The attack modifies model outputs through permutations that preserve expectation values while introducing unfairness. By shuffling scores based on protected feature values (e.g., gender), the attack creates biased outcomes that maintain the same average output but change individual allocations unfairly.
- Core assumption: The shuffling operation preserves the sum and mean of the output vector, making it invisible to basic Shapley value calculations
- Evidence anchors:
  - [abstract] "The attack strategies modify model outputs through permutations that preserve expectation values but introduce unfairness"
  - [section] "Shuffling attacks exploit the order-agnostic nature of the expectation calculation for value function v"
  - [corpus] Weak - no direct corpus evidence found
- Break condition: If the expectation calculation is performed on individual elements rather than aggregated vectors, or if consistency checks are implemented

### Mechanism 2
- Claim: SHAP implementations can detect shuffling attacks while Shapley values themselves cannot
- Mechanism: While theoretical Shapley values are blind to shuffling attacks, practical SHAP implementations use estimation algorithms that can detect non-zero attributions. Linear SHAP and standard SHAP implementations calculate feature attributions differently than the theoretical definition, allowing them to identify manipulation.
- Core assumption: SHAP implementations use estimation methods that break the pure theoretical Shapley value calculation
- Evidence anchors:
  - [abstract] "algorithms that estimate Shapley values, such as linearSHAP and SHAP, can detect these attacks with varying degrees of effectiveness"
  - [section] "Theoretically, we prove that Shapley values cannot detect shuffling attacks. However, common implementations of Shapley values, such as linearSHAP and SHAP, perform certain estimations that can detect non-zero attributions from shuffling features"
  - [corpus] Weak - no direct corpus evidence found
- Break condition: If the SHAP implementation uses a different estimation method that doesn't detect the shuffling, or if the attack parameters are carefully tuned to evade detection

### Mechanism 3
- Claim: Different attack strategies (Dominance, Mixing, Swapping) exploit different vulnerabilities in SHAP detection
- Mechanism: The three attack types create different patterns of score manipulation. Dominance completely segregates groups, Mixing probabilistically favors one group, and Swapping exchanges scores between individuals. Each creates unique attribution patterns that SHAP implementations detect with varying success rates.
- Core assumption: Different attack strategies create distinct statistical patterns that affect SHAP detection differently
- Evidence anchors:
  - [section] "We introduce three types of shuffling attacks in pseudocode... Our definition and codes can be generalized beyond the two-class gender scenario"
  - [section] "The Dominance attack can be described as a more complex Swapping attack in general"
  - [corpus] Weak - no direct corpus evidence found
- Break condition: If the detection algorithm adapts to recognize patterns specific to each attack type, or if attack parameters are limited to evade detection thresholds

## Foundational Learning

- Concept: Shapley values and cooperative game theory
  - Why needed here: The attack exploits vulnerabilities in Shapley value calculations, so understanding the mathematical foundation is crucial for understanding why the attack works
  - Quick check question: What is the key property of Shapley values that makes them vulnerable to shuffling attacks?

- Concept: Expectation calculations and order-invariance
  - Why needed here: The attack relies on the mathematical property that expectations are invariant to the order of elements in the calculation
  - Quick check question: Why does shuffling a vector of values not change the expectation of those values?

- Concept: Feature attribution methods and their implementations
  - Why needed here: Understanding the difference between theoretical Shapley values and practical SHAP implementations explains why detection varies
  - Quick check question: How does linear SHAP differ from standard SHAP in its calculation approach?

## Architecture Onboarding

- Component map: Base model f -> Attack wrapper f' -> SHAP implementations -> Detection analysis
- Critical path:
  1. Train or load base model f
  2. Apply shuffling attack wrapper f' to model outputs
  3. Run SHAP implementations on f'
  4. Analyze detected feature attributions
  5. Compare detection effectiveness across attack types and parameters
- Design tradeoffs:
  - Attack strength vs. detection probability: Stronger attacks are more detectable
  - Computational efficiency vs. attack sophistication: More complex attacks require more computation
  - Data accessibility vs. attack flexibility: Data-agnostic attacks are less powerful than data-dependent ones
- Failure signatures:
  - Low attribution detection for protected features when attack parameters are small
  - Inconsistent detection across different SHAP implementations
  - Detection success varying with attack type and dataset characteristics
- First 3 experiments:
  1. Test basic swapping attack on synthetic linear data with varying attack parameters
  2. Compare linear SHAP vs standard SHAP detection on graduate admissions dataset
  3. Test hybrid attacks combining multiple attack strategies on diabetes prediction dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can SHAP implementations be modified to detect shuffling attacks while maintaining computational efficiency?
- Basis in paper: [explicit] The paper demonstrates that SHAP implementations can detect shuffling attacks with varying effectiveness, and discusses the need for robust detection methods against more complicated shuffling attacks.
- Why unresolved: The paper identifies the vulnerability of SHAP to shuffling attacks but does not propose specific modifications to SHAP algorithms that would maintain both detection capability and computational efficiency.
- What evidence would resolve it: A modified SHAP algorithm that successfully detects shuffling attacks with performance comparable to current implementations, validated through experiments on multiple datasets.

### Open Question 2
- Question: What are the cascading effects of shuffling attacks in systems where high-scoring candidates receive additional advantages?
- Basis in paper: [explicit] The paper mentions that a small initial shuffling may result in huge future differences in systems where higher-scored candidates receive additional advantages.
- Why unresolved: The paper only briefly mentions this potential cascading effect but does not model or quantify how initial score manipulations propagate through multiple stages of decision-making.
- What evidence would resolve it: A simulation study showing how initial shuffling attacks amplify through multiple decision stages, including quantitative measures of the cumulative unfair advantage created.

### Open Question 3
- Question: How can XAI methods be designed to generate explanations that reveal model unfair behaviors resulting from shuffling attacks?
- Basis in paper: [explicit] The paper states that future work will design XAI methods to generate explanations revealing unfair behaviors from shuffling, but does not specify how this would be accomplished.
- Why unresolved: While the paper identifies the need for such methods, it does not provide concrete approaches for detecting or explaining shuffling-based unfairness.
- What evidence would resolve it: A novel XAI method that can identify and explain shuffling-based unfairness patterns, validated by its ability to detect known shuffling attacks in benchmark datasets.

## Limitations

- The paper relies on mathematical proofs about Shapley value properties but lacks precise implementation details for key components
- Attack effectiveness claims are based on limited model architectures (logistic regression) without testing on diverse model types
- The distinction between theoretical and practical Shapley value calculations is crucial but not fully elaborated

## Confidence

- High confidence in the fundamental mechanism exploiting expectation calculation's order-invariance
- Medium confidence in the comparative effectiveness of different SHAP implementations for detection
- Low confidence in the generalizability of attack effectiveness across diverse model architectures and data distributions

## Next Checks

1. Implement controlled experiments comparing theoretical Shapley value calculations against practical SHAP implementations to precisely identify what estimation methods enable attack detection
2. Test attack effectiveness across diverse model architectures (neural networks, decision trees, ensemble methods) beyond the logistic regression models used in experiments
3. Validate attack robustness against additional XAI methods beyond SHAP, including LIME, Integrated Gradients, and counterfactual explanations, to assess broader vulnerability landscape