---
ver: rpa2
title: 'Unused information in token probability distribution of generative LLM: improving
  LLM reading comprehension through calculation of expected values'
arxiv_id: '2406.10267'
source_url: https://arxiv.org/abs/2406.10267
tags:
- large
- probability
- decoding
- token
- greedy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores how to better utilize token probability distributions
  in generative large language models (LLMs) to improve reading comprehension, specifically
  in text summarization tasks. It compares traditional greedy decoding (selecting
  the highest probability token) with expected value decoding, where token probabilities
  are weighted to account for more nuanced information in the distribution.
---

# Unused information in token probability distribution of generative LLM: improving LLM reading comprehension through calculation of expected values

## Quick Facts
- arXiv ID: 2406.10267
- Source URL: https://arxiv.org/abs/2406.10267
- Authors: Krystian Zawistowski
- Reference count: 39
- Primary result: Expected value decoding with temperature scaling (T=10) significantly improves LLM summarization performance by leveraging full token probability distributions

## Executive Summary
This paper explores how to better utilize token probability distributions in generative large language models (LLMs) to improve reading comprehension, specifically in text summarization tasks. It compares traditional greedy decoding (selecting the highest probability token) with expected value decoding, where token probabilities are weighted to account for more nuanced information in the distribution. By scaling logits with a large temperature (T=10), the method increases entropy and captures more information from the distribution, leading to better performance on the SummEval dataset. The results show significant improvements in correlation to human judgments across multiple models, including quantized versions.

## Method Summary
The study introduces expected value decoding as an alternative to traditional greedy decoding in LLMs. Instead of selecting the highest probability token at each step, the method calculates the expected value of the next token based on the full probability distribution. The researchers apply a temperature scaling of T=10 to the logits, which dramatically increases the entropy of the distribution and allows more information to be captured. This approach is tested on multiple models including Mistral 7B, Llama 3 8B, and Mixtral 8x7B, using the SummEval dataset as the evaluation benchmark.

## Key Results
- Mixtral 8x7B with expected value decoding achieved SummEval scores nearly matching GPT-4 on certain metrics
- Expected value decoding consistently outperformed greedy decoding across all tested models and metrics
- The method showed significant improvements in correlation to human judgments (3.5-10.6% across different metrics)
- Quantized versions of models showed similar improvements, indicating the method works even with compressed models

## Why This Works (Mechanism)
The paper demonstrates that traditional greedy decoding wastes information by only considering the most probable token at each step. By calculating expected values across the full probability distribution, the method captures nuanced relationships between tokens that would otherwise be ignored. The high temperature scaling (T=10) is crucial as it flattens the distribution enough to allow lower probability tokens to contribute meaningfully to the expected value calculation, revealing patterns in the distribution that greedy decoding cannot access.

## Foundational Learning
- Token probability distributions: Understanding how LLMs assign probabilities to next tokens is fundamental to this work. Quick check: Verify that softmax of logits produces a valid probability distribution.
- Entropy in probability distributions: The paper relies on manipulating distribution entropy to capture more information. Quick check: Calculate entropy before and after temperature scaling to confirm increased uncertainty.
- Expected value computation: The core mathematical operation that differs from greedy decoding. Quick check: Verify expected value calculation by summing token values weighted by their probabilities.
- Positional bias in LLMs: The paper mentions potential issues with positional bias affecting decoding. Quick check: Examine attention weights to identify positional bias patterns.

## Architecture Onboarding
Component map: Input text -> LLM tokenizer -> Logits (pre-softmax) -> Temperature scaling (T=10) -> Softmax probabilities -> Expected value calculation -> Output token
Critical path: The temperature scaling and expected value calculation steps are the critical differentiators from standard greedy decoding.
Design tradeoffs: Higher temperature increases information capture but may introduce more noise; expected value decoding is computationally more expensive than greedy decoding.
Failure signatures: At very high temperatures, the distribution may become too uniform, reducing the effectiveness of expected value calculations.
First experiments: 1) Compare greedy vs expected value decoding on simple text generation tasks, 2) Test different temperature values (T=1, 5, 10) to find optimal setting, 3) Evaluate computational overhead of expected value decoding compared to greedy decoding

## Open Questions the Paper Calls Out
The paper identifies several open questions, including the generalizability of the expected value decoding approach to tasks beyond summarization, the optimal temperature scaling for different model architectures, and the potential computational costs associated with the method compared to simpler decoding strategies.

## Limitations
- The high temperature scaling (T=10) significantly alters the original model's learned probability distributions, potentially affecting generalization to real-world applications
- Results are primarily focused on text summarization tasks using the SummEval dataset, limiting generalizability to other NLP tasks
- The study doesn't adequately address potential computational costs associated with expected value decoding compared to simpler methods

## Confidence
- Expected value decoding improves SummEval performance: High
- Temperature scaling (T=10) is necessary for capturing distribution information: Medium
- Results generalize across different model families and tasks: Low
- Tree-based sampling algorithm provides meaningful insights about output diversity: Medium

## Next Checks
1. Test the expected value decoding approach on multiple NLP tasks beyond summarization (e.g., question answering, translation) to assess generalizability
2. Evaluate the method's performance at different temperature scales (T=1-10) to determine optimal balance between information capture and model integrity
3. Conduct ablation studies comparing expected value decoding with alternative distribution-aware methods like nucleus sampling or top-k sampling to establish relative performance benefits