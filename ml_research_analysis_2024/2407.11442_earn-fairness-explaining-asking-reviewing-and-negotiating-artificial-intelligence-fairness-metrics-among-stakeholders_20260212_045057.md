---
ver: rpa2
title: 'EARN Fairness: Explaining, Asking, Reviewing, and Negotiating Artificial Intelligence
  Fairness Metrics Among Stakeholders'
arxiv_id: '2407.11442'
source_url: https://arxiv.org/abs/2407.11442
tags:
- fairness
- metrics
- metric
- group
- individual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduced a framework enabling non-AI stakeholders to
  engage with AI fairness metrics through an interactive system. Stakeholders explored
  metrics, set fairness thresholds, and negotiated consensus in group sessions.
---

# EARN Fairness: Explaining, Asking, Reviewing, and Negotiating Artificial Intelligence Fairness Metrics Among Stakeholders

## Quick Facts
- arXiv ID: 2407.11442
- Source URL: https://arxiv.org/abs/2407.11442
- Reference count: 40
- Non-AI stakeholders can meaningfully engage with AI fairness metrics through interactive negotiation

## Executive Summary
This study introduces the EARN Fairness framework, enabling non-AI stakeholders to actively participate in defining AI fairness metrics through an interactive system. The framework features the Fairness Explainer and Explorer (FEE) tool, which explains complex fairness metrics through visual and textual explanations at the individual case level. Stakeholders explored metrics, set fairness thresholds, and negotiated consensus in team sessions, revealing diverse metric preferences that often diverged from expert norms. Most participants favored individual or subgroup fairness metrics, and teams reached consensus through various strategies including majority voting, debates, and compromise. The study demonstrates that structured negotiation processes can support inclusive, human-centered AI fairness decision-making in high-stakes contexts.

## Method Summary
The study employed a two-phase EARN Fairness process with 24 non-AI participants in China. First, an individual session where participants explored the FEE interactive system to learn about eight fairness metrics through visual and textual explanations, then selected their top three metric preferences and set fairness thresholds. Second, a team session where participants collectively reviewed metrics, discussed preferences, and negotiated consensus through majority voting, debates, or compromise. The study used the German Credit Dataset with 1000 instances and 20 features, along with a logistic regression classifier, to demonstrate the fairness metrics in practice.

## Key Results
- Participants set fairness thresholds stricter than legal standards, with averages below 0.05 for most categories
- Individual and subgroup fairness metrics were preferred over group fairness metrics by most participants
- Teams reached consensus through diverse strategies: majority voting (6 teams), debates (3 teams), and compromise (2 teams)
- Many teams combined multiple metrics in their consensus definitions, indicating nuanced fairness preferences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interactive explanation of fairness metrics reduces cognitive load for non-AI stakeholders.
- Mechanism: The Fairness Explainer and Explorer (FEE) breaks down complex mathematical fairness metrics into visual and textual explanations at the instance level, allowing users to explore and compare metrics interactively.
- Core assumption: Stakeholders can understand fairness concepts better through simplified, contextual explanations rather than abstract formulas.
- Evidence anchors:
  - [abstract] "The framework features an adaptable interactive system and a stakeholder-centered EARN Fairness process to Explain fairness metrics..."
  - [section 3.2.3] "These explanations use text, charts, and images to break down fairness metrics at the individual case level, reducing the need for a mathematical or statistical background."
- Break condition: If users still struggle to understand fairness metrics despite the simplified explanations, indicating the need for even more intuitive or alternative explanation methods.

### Mechanism 2
- Claim: Negotiation and consensus-building among stakeholders leads to more inclusive and accepted AI fairness definitions.
- Mechanism: The EARN Fairness process guides stakeholders through individual preference elicitation and team-based negotiation sessions, allowing them to discuss, debate, and reach consensus on fairness metrics.
- Core assumption: Diverse stakeholder perspectives on fairness can be reconciled through structured negotiation, leading to a collective definition of fairness that is more widely accepted.
- Evidence anchors:
  - [abstract] "Teams reached consensus through majority voting, debates, or compromise, often combining multiple metrics."
  - [section 5.2] "We observed that varying strategies were adopted for negotiating consensus within the teams...indicating that robust negotiating approaches need to be developed, codified, and supported by tools and processes."
- Break condition: If negotiations consistently fail to reach consensus or if the resulting consensus is not accepted by all stakeholders, indicating the need for alternative consensus-building methods.

### Mechanism 3
- Claim: Allowing stakeholders to set their own fairness thresholds leads to more meaningful and context-appropriate fairness definitions.
- Mechanism: The FEE tool allows users to adjust fairness thresholds for different fairness categories, enabling them to define what level of unfairness they consider acceptable in their specific context.
- Core assumption: Stakeholders have different tolerances for unfairness based on their values and the specific application context, and these differences need to be accounted for in fairness definitions.
- Evidence anchors:
  - [section 5.1.1] "We can note that on average participants set thresholds relatively stricter than expected by law and by most practices..."
  - [section 6.2.2] "We observed different understandings of fairness, not only in metrics but also in fairness categories."
- Break condition: If stakeholders consistently set thresholds that are either too lenient or too strict to be practically useful, indicating the need for guidance or constraints on threshold setting.

## Foundational Learning

- Concept: Fairness metrics and their mathematical definitions
  - Why needed here: To understand the different ways fairness can be quantified and the trade-offs between them.
  - Quick check question: Can you explain the difference between group fairness and individual fairness metrics?

- Concept: Stakeholder engagement and consensus-building
  - Why needed here: To understand how to involve non-AI experts in defining fairness and how to reconcile different perspectives.
  - Quick check question: What are some strategies for reaching consensus among stakeholders with different views on fairness?

- Concept: Interactive data visualization and exploration
  - Why needed here: To understand how to design tools that make complex data and concepts accessible to non-experts.
  - Quick check question: How can visual representations help users understand fairness metrics and their implications?

## Architecture Onboarding

- Component map:
  - EARN Fairness Framework
    - Individual Session Component
      - FEE Interactive System (Explain & Ask)
        - Data Exploration
        - Model Exploration
        - Static Fairness Explanation and Exploration
        - Dynamic Fairness Exploration
    - Team Session Component
      - FEE Interactive System (Review & Negotiate)
        - Metric Review
        - Consensus Negotiation
  - Stakeholder-Centered EARN Fairness Process
    - Explain Fairness Metrics
    - Ask for Personal Preferences
    - Review Metrics Collectively
    - Negotiate Consensus

- Critical path:
  1. Individual session: Stakeholders explore FEE, learn about metrics, and set personal preferences.
  2. Team session: Stakeholders review metrics together, discuss preferences, and negotiate consensus.

- Design tradeoffs:
  - Simplicity vs. comprehensiveness in metric explanations
  - Individual vs. group decision-making processes
  - Real-time vs. asynchronous negotiation capabilities

- Failure signatures:
  - Stakeholders consistently struggle to understand metrics despite explanations
  - Negotiations fail to reach consensus or result in widely divergent definitions of fairness
  - Users set thresholds that are too lenient or strict to be practically useful

- First 3 experiments:
  1. Test the FEE tool with a small group of stakeholders to gather feedback on usability and effectiveness of explanations.
  2. Conduct a pilot study with a larger group to observe negotiation dynamics and identify potential challenges in reaching consensus.
  3. Evaluate the impact of the EARN Fairness framework on stakeholder understanding and acceptance of AI fairness definitions compared to traditional expert-driven approaches.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can stakeholder-driven fairness metrics be integrated into existing AI fairness toolkits to enhance transparency and accessibility for non-AI experts?
- Basis in paper: [explicit] The study highlights the need for tools that explain metrics and support structured negotiation, indicating a gap in current AI fairness toolkits.
- Why unresolved: While the study introduces a framework for engaging stakeholders, it does not provide a direct method for integrating these insights into widely-used AI fairness toolkits.
- What evidence would resolve it: Development and evaluation of a prototype toolkit that incorporates stakeholder-driven metrics and assesses its impact on transparency and accessibility.

### Open Question 2
- Question: What are the long-term effects of using the EARN Fairness framework on the fairness perceptions and decision-making processes of stakeholders?
- Basis in paper: [inferred] The study focuses on immediate outcomes of the framework, but does not explore its sustained impact over time.
- Why unresolved: The study's design and scope limit its ability to assess long-term effects, necessitating further longitudinal research.
- What evidence would resolve it: A longitudinal study tracking stakeholders' perceptions and decisions over an extended period after using the framework.

### Open Question 3
- Question: How can the EARN Fairness framework be adapted to accommodate larger and more diverse stakeholder groups in real-world AI applications?
- Basis in paper: [explicit] The study acknowledges the need to scale up the framework for larger and distributed stakeholder groups.
- Why unresolved: The study's limited sample size and context-specific application do not provide a scalable solution for diverse real-world scenarios.
- What evidence would resolve it: Pilot implementations of the framework in varied contexts with large, diverse stakeholder groups, evaluating scalability and adaptability.

## Limitations
- Study conducted with 24 participants in China, limiting generalizability across cultural contexts
- Framework effectiveness depends on facilitator skills and group dynamics
- Focus on specific dataset and metrics may limit applicability to other domains

## Confidence
- High confidence in the framework's potential to facilitate stakeholder engagement with fairness metrics
- Medium confidence in the observed consensus-building mechanisms and their generalizability
- Low confidence in the framework's scalability and effectiveness across diverse cultural contexts

## Next Checks
1. Conduct cross-cultural studies with diverse stakeholder groups to assess the framework's generalizability and identify potential cultural biases in fairness perceptions.
2. Implement the framework in real-world high-stakes AI deployment scenarios to evaluate its practical effectiveness and identify unforeseen challenges.
3. Develop and test automated facilitation tools to support the negotiation process, reducing reliance on skilled human facilitators and enabling broader adoption.