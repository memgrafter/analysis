---
ver: rpa2
title: 'Perception of Visual Content: Differences Between Humans and Foundation Models'
arxiv_id: '2411.18968'
source_url: https://arxiv.org/abs/2411.18968
tags:
- annotations
- human
- captions
- images
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the differences between human- and machine-generated
  annotations for images across socio-economic contexts. The authors collect annotations
  using both human crowdsourcing and two pre-trained ML models (Faster R-CNN for object
  detection and BLIP for captioning).
---

# Perception of Visual Content: Differences Between Humans and Foundation Models

## Quick Facts
- arXiv ID: 2411.18968
- Source URL: https://arxiv.org/abs/2411.18968
- Reference count: 14
- Key outcome: Machine-generated annotations show highest similarity to human labels and achieve best performance in region classification and income regression tasks.

## Executive Summary
This paper investigates the differences between human- and machine-generated annotations for images across socio-economic contexts using the Dollar Street dataset. The authors compare annotations from human crowdsourcing with those generated by two pre-trained ML models (Faster R-CNN for object detection and BLIP for captioning). They evaluate how these annotations impact ML model performance for region classification and income regression tasks. The study finds that ML captions and human labels are most similar in word types and sentence structure, with ML captions achieving the best overall region classification (F1 = 0.41). For income regression, ML objects combined with ML captions performed best overall (RMSE = 1817.49). The analysis demonstrates that both human and machine annotations are important and that machine-generated annotations cannot fully replace human-generated ones.

## Method Summary
The study collected annotations using human crowdsourcing and two pre-trained ML models (Faster R-CNN for object detection and BLIP for captioning) on the Dollar Street dataset. Researchers fine-tuned a sentence transformer model on the three annotation sets to generate embeddings for similarity analysis. They then evaluated how different annotation combinations impact ML model performance using RUSBoost classifier for region classification and AdaBoost.R2 for income regression. The analysis employed 5-fold cross-validation with grid search to optimize hyperparameters and assess model performance across multiple runs.

## Key Results
- ML captions and human labels show highest similarity at low-level features (cosine similarity = 0.69)
- ML captions achieved best overall region classification performance (F1 = 0.41)
- ML objects combined with ML captions performed best for income regression (RMSE = 1817.49)
- ML-based annotations work best for action categories while human input is more effective for non-action categories

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning sentence transformers on combined annotation data improves embedding alignment for similarity analysis. The BLIP model's cosine similarity function computes pairwise similarity scores between annotation embeddings. By fine-tuning the pre-trained "all-MiniLM-L12-v2" sentence transformer on the three annotation sets, the embeddings better capture the characteristics of human and machine annotations. This allows the model to better represent the nuances and context present in both human and machine-generated annotations, improving similarity detection.

### Mechanism 2
ML captions and human labels are most similar due to overlapping features in their descriptions. Both humans and ML captioning models recognize objects and actions in images, leading to overlapping features in their descriptions. The ML captioning model captures both objects and actions, similar to human annotations, resulting in higher similarity scores. This alignment occurs because the ML captioning model's ability to describe both objects and actions aligns with human cognitive processes in image annotation.

### Mechanism 3
ML-based annotations perform better overall at both classification and regression tasks due to their ability to capture discriminative features. The ML models are trained on large datasets and can identify objects and actions that are indicative of geographical regions and income levels. The combination of ML objects and ML captions provides a comprehensive set of features that improve model performance. The discriminative features captured by ML annotations are more relevant for predicting geographical regions and income levels than those captured by human annotations.

## Foundational Learning

- Concept: Sentence Transformers and Fine-tuning
  - Why needed here: To generate embeddings that capture the nuances of human and machine annotations for similarity analysis
  - Quick check question: What is the purpose of fine-tuning the sentence transformer on the annotation data?

- Concept: Cosine Similarity
  - Why needed here: To measure the similarity between annotation embeddings and quantify the relationships between human and machine annotations
  - Quick check question: How does cosine similarity differ from other distance metrics in measuring vector similarity?

- Concept: Machine Learning Model Performance Metrics
  - Why needed here: To evaluate the effectiveness of different annotation combinations in training predictive models for region classification and income regression
  - Quick check question: Why is F1-score used for classification and RMSE used for regression?

## Architecture Onboarding

- Component map:
  Data Collection -> ML Annotation Generation (Faster R-CNN, BLIP) -> Human Annotation Collection -> Embedding Generation (Fine-tuned Sentence Transformer) -> Similarity Analysis (Cosine Similarity) -> Predictive Models (RUSBoost, AdaBoost.R2) -> Performance Evaluation

- Critical path:
  1. Data collection and preprocessing
  2. ML annotation generation (Faster R-CNN and BLIP)
  3. Crowdsourcing human annotations
  4. Embedding generation using fine-tuned sentence transformer
  5. Similarity analysis using cosine similarity
  6. Training predictive models with different annotation combinations
  7. Evaluation and analysis of model performance

- Design tradeoffs:
  - Using pre-trained models (Faster R-CNN and BLIP) vs. training custom models
  - Fine-tuning sentence transformer vs. using pre-trained embeddings
  - Combining annotations vs. using individual annotation types

- Failure signatures:
  - Low similarity scores between annotations may indicate differences in annotation approaches
  - Poor model performance may suggest that the chosen annotations are not discriminative enough for the task
  - High variance in model performance across different runs may indicate instability in the model

- First 3 experiments:
  1. Compute cosine similarity between ML captions and human labels to verify their high similarity
  2. Train a region classifier using only ML captions and evaluate its performance
  3. Train an income regression model using both ML objects and ML captions and compare its performance to using only one type of annotation

## Open Questions the Paper Calls Out

### Open Question 1
How would the similarity between human and machine annotations change if we used different foundational models (e.g., GPT-4V, Claude) instead of Faster R-CNN and BLIP? The authors note that their study only considers certain ML models and cannot claim observations generalize beyond the data and models considered. The study is limited to Faster R-CNN and BLIP, so the impact of using alternative models remains unknown. Conducting the same annotation comparison experiments with different foundational models and measuring similarity scores across all pairs would resolve this question.

### Open Question 2
Would human annotators perform differently if they were aware that their annotations would be compared to machine-generated ones? The authors collected human annotations through crowdsourcing without mentioning any awareness of the machine comparison aspect to participants. The study doesn't explore how awareness of the comparison might influence human annotation behavior or quality. Conducting a controlled experiment where one group of annotators knows about the machine comparison while another group doesn't, then comparing the quality and characteristics of their annotations would resolve this question.

### Open Question 3
How would the performance of predictive models change if we incorporated confidence scores from the machine annotations into the training process? The authors filtered images based on ML Objects with at least 50% confidence but didn't incorporate confidence scores as features in their predictive models. The study treats all machine annotations equally without leveraging the confidence information that could indicate annotation reliability. Training predictive models with and without confidence score features and comparing performance metrics to determine if confidence information improves prediction accuracy would resolve this question.

## Limitations

- The analysis relies on pre-trained models (Faster R-CNN and BLIP) that may have inherent biases from their training data, potentially affecting the quality of ML-generated annotations
- Similarity analysis using cosine distance between embeddings may not capture deeper semantic differences between human and machine understanding of images
- The effectiveness of fine-tuning sentence transformers on annotation data is not thoroughly validated against alternative embedding methods

## Confidence

- ML captions and human labels similarity: High confidence - Well-supported by similarity scores and clear mechanism
- ML annotations performance in region classification: Medium confidence - Results show strong performance but the claim about "best overall" performance is based on a single metric
- Combined ML objects and captions for income regression: Medium confidence - Supported by RMSE values but limited cross-validation runs

## Next Checks

1. Test embedding quality by comparing fine-tuned sentence transformer similarity scores against human-annotated similarity ratings for the same image pairs
2. Evaluate model performance across different socio-economic contexts to verify the claimed generalization of ML annotations
3. Conduct ablation studies removing either objects or actions from annotations to quantify their individual contributions to model performance