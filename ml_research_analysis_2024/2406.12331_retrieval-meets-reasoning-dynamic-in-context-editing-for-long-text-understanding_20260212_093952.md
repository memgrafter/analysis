---
ver: rpa2
title: 'Retrieval Meets Reasoning: Dynamic In-Context Editing for Long-Text Understanding'
arxiv_id: '2406.12331'
source_url: https://arxiv.org/abs/2406.12331
tags:
- context
- reasoning
- question
- knowledge
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of multi-hop reasoning over long
  texts using Large Language Models (LLMs) with limited context windows. The authors
  propose a novel approach that dynamically decomposes complex questions into sub-questions,
  forming a Directed Acyclic Graph (DAG), and leverages the model's reasoning ability
  to progress through the graph.
---

# Retrieval Meets Reasoning: Dynamic In-Context Editing for Long-Text Understanding

## Quick Facts
- arXiv ID: 2406.12331
- Source URL: https://arxiv.org/abs/2406.12331
- Authors: Weizhi Fei; Xueyan Niu; Guoqing Xie; Yanhua Zhang; Bo Bai; Lei Deng; Wei Han
- Reference count: 9
- Key outcome: Achieves 44.5% F1 on multi-hop QA, surpassing GPT-3.5-Turbo-16k

## Executive Summary
This paper addresses the challenge of multi-hop reasoning over long texts using Large Language Models (LLMs) with limited context windows. The authors propose a novel approach that dynamically decomposes complex questions into sub-questions, forming a Directed Acyclic Graph (DAG), and leverages the model's reasoning ability to progress through the graph. This is achieved through two core modules: a planning module that generates intermediate steps and a retrieval module that recalls relevant information from the given context. The proposed methods, Iterative QA with Fact Extraction and Knowledge Constrained Decoding, interactively utilize these modules to enhance the LLM's reasoning capabilities.

## Method Summary
The proposed approach re-imagines information retrieval through dynamic in-context editing, inspired by recent breakthroughs in knowledge editing. It treats lengthy contexts as malleable external knowledge, interactively gathering and integrating relevant information to enable LLMs to perform sophisticated reasoning steps. The method involves two core modules: a planning module that decomposes complex questions into sub-questions forming a DAG, and a retrieval module that identifies relevant context chunks using sentence similarity models. The approach is implemented through two algorithms: Iterative QA with Fact Extraction and Knowledge Constrained Decoding, both leveraging the planning and retrieval modules to enhance reasoning capabilities.

## Key Results
- Achieves 44.5% F1 score on multi-hop question answering tasks across three datasets
- Outperforms state-of-the-art context window extrapolation methods
- Compares favorably to commercial long-context models like GPT-3.5-Turbo-16k
- Demonstrates effectiveness in enabling context-limited LLMs to engage in sophisticated reasoning tasks over long texts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic decomposition of complex questions into sub-questions enables the model to handle long-context reasoning by reducing information load per step.
- Mechanism: Planning module uses few-shot learning to decompose the original question into a Directed Acyclic Graph (DAG) of sub-questions, each requiring information from manageable chunks of context.
- Core assumption: LLM can understand and follow decomposition instructions, and reasoning DAG is valid for the given task.
- Evidence anchors: [abstract] "We introduce a novel approach that re-imagines information retrieval through dynamic in-context editing, inspired by recent breakthroughs in knowledge editing." [section] "Planning module attempt to decompose the complex question Q into individual sub-task q1, . . . , ql that factorize into a DAG to facilitate the solution of the complex questionQ."

### Mechanism 2
- Claim: Retrieval of relevant context chunks based on sub-question similarity enables the model to access necessary information for each reasoning step.
- Mechanism: Retrieval module uses sentence similarity model (bi-encoder and cross-encoder) to identify top-k relevant chunks for each sub-question.
- Core assumption: Similarity model can accurately identify relevant context chunks, and top-k chunks contain necessary information to answer sub-question.
- Evidence anchors: [abstract] "By treating lengthy contexts as malleable external knowledge, our method interactively gathers and integrates relevant information, thereby enabling LLMs to perform sophisticated reasoning steps." [section] "The retrieval module employs a sentence similarity model ϕ(·, ·) to retrieve relevant information for a specified query."

### Mechanism 3
- Claim: Fact extraction and knowledge-constrained decoding ensure generated answers are grounded in retrieved context and consistent with original information.
- Mechanism: After retrieving relevant chunks, LLM extracts concise facts that answer sub-question (fact extraction), or constrained decoding generates answers directly based on retrieved knowledge.
- Core assumption: LLM can accurately extract relevant facts from retrieved context, and constrained decoding can generate coherent and accurate answers based on retrieved knowledge.
- Evidence anchors: [abstract] "This approach allows the model to focus on the external knowledge, thereby enhancing its hallucination." [section] "To reduce storage of the context window, after receiving c∗ i , we utilize an LLM to extract facts fi that can answer the sub-question qi according to the retrieved Cqi."

## Foundational Learning

- Concept: Directed Acyclic Graphs (DAGs) and their application in reasoning tasks.
  - Why needed here: Reasoning process is modeled as a DAG where nodes represent reasoning steps and edges represent dependencies between steps.
  - Quick check question: Can you explain key characteristics of a DAG and how it differs from other graph structures like trees or cyclic graphs?

- Concept: Retrieval-Augmented Generation (RAG) and its limitations in handling complex reasoning tasks.
  - Why needed here: Paper critiques RAG for inability to handle complex reasoning tasks requiring synthesis of information from multiple context parts.
  - Quick check question: What are main limitations of RAG in handling multi-hop reasoning tasks, and how does proposed approach address these limitations?

- Concept: In-context learning and its application in knowledge editing and reasoning tasks.
  - Why needed here: Proposed approach is inspired by in-context editing which involves presenting model with instructions or examples to guide generation process.
  - Quick check question: How does in-context learning differ from traditional fine-tuning, and what are its advantages and limitations in knowledge editing and reasoning tasks?

## Architecture Onboarding

- Component map: Planning module -> Retrieval module -> Language model (LLM)
- Critical path: Planning module generates sub-question -> Retrieval module identifies relevant chunks -> LLM generates answer based on retrieved information
- Design tradeoffs: System trades off accuracy for efficiency by decomposing question into smaller sub-questions and retrieving relevant information in chunks rather than processing entire context at once
- Failure signatures: Incorrect decomposition of question, inaccurate retrieval of relevant chunks, inability of LLM to generate accurate answers based on retrieved information
- First 3 experiments:
  1. Test planning module's ability to decompose complex question into valid sub-questions following DAG structure
  2. Evaluate retrieval module's accuracy in identifying relevant context chunks for given sub-question
  3. Assess LLM's ability to generate accurate answers based on retrieved information using both fact extraction and constrained decoding approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of proposed methods scale with size of language model used for planning and retrieval modules?
- Basis in paper: [explicit] Authors conducted ablation study using Llama2-7B and Llama2-13B as base models and observed performance improvements with larger models
- Why unresolved: Study only compared two model sizes; unclear how methods would perform with even larger models like GPT-4 or LLaMA-3
- What evidence would resolve it: Systematic study comparing performance across range of model sizes from small (e.g., LLaMA-7B) to very large (e.g., GPT-4 level)

### Open Question 2
- Question: Can proposed methods be effectively applied to non-English languages or multilingual contexts?
- Basis in paper: [inferred] Authors mention LongBench is bilingual benchmark but proposed methods not explicitly tested on non-English datasets or multilingual tasks
- Why unresolved: Effectiveness in handling languages with different structures unknown; performance in multilingual settings unexplored
- What evidence would resolve it: Testing methods on diverse multilingual datasets and languages with varying linguistic properties

### Open Question 3
- Question: How do proposed methods compare to fine-tuning approaches that extend context window of LLMs?
- Basis in paper: [explicit] Authors mention approach is lightweight, plug-and-play solution compared to methods that modify base model or require fine-tuning
- Why unresolved: While paper shows proposed methods outperform some context window extension techniques, direct comparison with state-of-the-art fine-tuning methods not provided
- What evidence would resolve it: Comprehensive benchmark comparing proposed methods with fine-tuning approaches on same tasks and datasets

### Open Question 4
- Question: How robust are proposed methods to noise and irrelevant information in long context?
- Basis in paper: [inferred] Methods rely on retrieval modules to identify relevant chunks from long context, but impact of noisy or irrelevant information on reasoning process not explicitly studied
- Why unresolved: In real-world scenarios, long context may contain significant amount of irrelevant or misleading information
- What evidence would resolve it: Experiments introducing controlled amounts of noise or irrelevant information into context and measuring impact on methods' performance

### Open Question 5
- Question: Can proposed methods be extended to handle more complex reasoning tasks beyond multi-hop question answering?
- Basis in paper: [inferred] Authors focus on multi-hop question answering tasks but underlying principles could potentially be applied to other reasoning tasks
- Why unresolved: Methods' ability to handle tasks requiring more sophisticated reasoning not explored
- What evidence would resolve it: Adapting methods to range of complex reasoning tasks and evaluating performance

## Limitations
- Effectiveness heavily depends on quality of sub-questions generated by planning module
- Retrieval module performance constrained by accuracy of similarity models
- Fact extraction and knowledge-constrained decoding rely on LLM's ability to accurately synthesize information
- Evaluation limited to three specific datasets, generalizability to other domains uncertain

## Confidence

**High Confidence Claims:**
- Proposed approach significantly outperforms standard context window extrapolation methods on evaluated datasets
- Dynamic decomposition and retrieval modules can effectively handle multi-hop reasoning tasks within context limitations
- Iterative QA with Fact Extraction and Knowledge Constrained Decoding algorithms are valid and implementable

**Medium Confidence Claims:**
- Approach compares favorably to more advanced commercial long-context models like GPT-3.5-Turbo-16k
- Fact extraction and knowledge-constrained decoding effectively ground generated answers in retrieved context

**Low Confidence Claims:**
- Approach is universally applicable to all types of long-text reasoning tasks beyond evaluated datasets
- Specific implementation details and hyperparameters are optimal for all scenarios

## Next Checks
1. **Cross-Dataset Validation:** Evaluate proposed approach on additional multi-hop reasoning datasets beyond HotpotQA, 2WikiMultiHopQA, and MuSiQue to assess generalizability and robustness across different domains and question types.

2. **Ablation Studies:** Conduct ablation studies to isolate contributions of planning module, retrieval module, and fact extraction/knowledge-constrained decoding processes to identify most critical components and potential areas for improvement.

3. **Human Evaluation of Sub-Questions:** Perform human evaluation of sub-questions generated by planning module to assess their quality, relevance, and ability to guide reasoning process effectively, providing insights into strengths and weaknesses of decomposition strategy.