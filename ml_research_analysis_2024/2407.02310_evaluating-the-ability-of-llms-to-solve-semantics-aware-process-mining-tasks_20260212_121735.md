---
ver: rpa2
title: Evaluating the Ability of LLMs to Solve Semantics-Aware Process Mining Tasks
arxiv_id: '2407.02310'
source_url: https://arxiv.org/abs/2407.02310
tags:
- process
- tasks
- llms
- mining
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates large language models (LLMs) for semantics-aware
  process mining tasks, which require understanding process behavior rather than just
  pattern matching. The authors define three novel tasks: semantic anomaly detection
  at trace and activity levels, and semantic next activity prediction.'
---

# Evaluating the Ability of LLMs to Solve Semantics-Aware Process Mining Tasks

## Quick Facts
- arXiv ID: 2407.02310
- Source URL: https://arxiv.org/abs/2407.02310
- Reference count: 31
- LLMs fail at semantics-aware process mining tasks with in-context learning but achieve strong performance when fine-tuned

## Executive Summary
This paper evaluates large language models on semantics-aware process mining tasks that require understanding process behavior rather than simple pattern matching. The authors introduce three novel tasks: semantic anomaly detection at trace and activity levels, and semantic next activity prediction. Using 15,857 process models from the SAP-SAM collection, they demonstrate that while LLMs perform poorly with in-context learning (F1 scores around 0.5), they significantly outperform smaller encoder models when fine-tuned, achieving F1 scores up to 0.88. The study highlights the importance of fine-tuning for domain-specific tasks and provides insights into the computational trade-offs between model sizes.

## Method Summary
The study uses 15,857 process models from the SAP-SAM collection to create benchmarking datasets for three semantics-aware process mining tasks: trace-level semantic anomaly detection (T-SAD), activity-level semantic anomaly detection (A-SAD), and semantic next activity prediction (S-NAP). The authors evaluate both in-context learning (ICL) with 6 examples and supervised fine-tuning of LLMs (Llama-3 8B, Mistral-7B-Instruct-v0.2) and RoBERTa encoder model using LoRA adapters with batch size 32, learning rate 1e-5, for 3 epochs. Performance is measured using macro F1-score across train/validation/test splits.

## Key Results
- LLMs fail at semantics-aware process mining tasks with in-context learning (F1 scores around 0.5 for anomaly detection, 0.32 for next activity prediction)
- Fine-tuned LLMs significantly outperform RoBERTa encoder model, achieving F1 scores of 0.79 (T-SAD), 0.88 (A-SAD), and 0.69 (S-NAP)
- Fine-tuning LLMs requires more computational resources per epoch (up to 25× longer than RoBERTa) but converges faster with fewer epochs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs require fine-tuning to achieve strong performance on semantics-aware process mining tasks because pre-training focuses on general language patterns, not domain-specific process semantics.
- Mechanism: Pre-trained LLMs learn general linguistic patterns and reasoning from vast text corpora but lack explicit knowledge of process mining concepts, control-flow relations, and semantic validity of activity sequences. Fine-tuning adapts the model parameters to encode task-specific knowledge about valid process behaviors and anomalies.
- Core assumption: The knowledge needed for semantics-aware process mining is not sufficiently present in the general pre-training corpus and must be explicitly learned from task-specific examples.
- Evidence anchors:
  - [abstract] "Our evaluation experiments reveal that (1) LLMs fail to solve challenging process mining tasks out of the box and when provided only a handful of in-context examples, (2) but they yield strong performance when fine-tuned for these tasks"
  - [section VI] "Poor ICL performance shows that LLMs a priori know very little about process semantics and thus need to be explicitly trained for our tasks"
  - [corpus] Weak evidence: The corpus is based on process models but contains no explicit semantic annotations about validity rules or anomaly definitions

### Mechanism 2
- Claim: Decoder-based LLMs outperform encoder-based models on these tasks after fine-tuning because their autoregressive training enables better generation of semantically valid next activities.
- Mechanism: Decoder LLMs are trained to predict the next token given preceding context, which aligns naturally with next activity prediction. Their fine-tuned language modeling head can generate valid activity sequences by constraining predictions to allowed classes. Encoder models require additional classification layers and lack this generation capability.
- Core assumption: The autoregressive generation capability of decoder LLMs provides an inherent advantage for sequence prediction tasks compared to bidirectional encoders.
- Evidence anchors:
  - [section VII] "Fine-tuning the LLMs yields even better performance, with Llama and Mistral achieving an F1-score of 0.79, a further 2-point improvement over RoBERTa's performance"
  - [section V] "Autoregressively trained decoder LLMs cast classification tasks as language generation tasks"
  - [corpus] No direct evidence about generation vs classification performance differences

### Mechanism 3
- Claim: Few-shot in-context learning fails for these tasks because semantics-aware process mining requires understanding complex process semantics that cannot be captured in a handful of examples.
- Mechanism: In-context learning relies on the model's ability to generalize from few examples by recognizing patterns. However, semantics-aware process mining involves understanding deep causal relationships between activities, valid execution orders, and domain-specific constraints that require more extensive training than can be provided in context.
- Core assumption: The complexity of process semantics exceeds what can be effectively communicated through a small number of in-context examples.
- Evidence anchors:
  - [abstract] "LLMs fail to solve challenging process mining tasks out of the box and when provided only a handful of in-context examples"
  - [section VII] "For ICL, we find that the performance of the LLMs is at best marginally better... at worst... slightly worse than random performance"
  - [corpus] Weak evidence: The corpus provides extensive examples but ICL uses only 6 examples per task

## Foundational Learning

- Concept: Control-flow perspective in process mining
  - Why needed here: The tasks focus solely on control-flow semantics without data attributes, requiring understanding of valid activity orderings and process executions
  - Quick check question: Can you explain the difference between a valid activity sequence and a valid event log in process mining terms?

- Concept: Eventually-follows relations
  - Why needed here: The A-SAD task specifically evaluates pairs of activities based on whether their ordering is valid according to process models
  - Quick check question: Given a process model with activities A→B→C, what would be the eventually-follows relations and which pairs would be considered anomalous?

- Concept: Fine-tuning vs in-context learning
  - Why needed here: The paper demonstrates that fine-tuning is necessary for good performance while in-context learning fails, requiring understanding of when each approach is appropriate
  - Quick check question: Why would a task requiring domain-specific knowledge like process semantics be poorly suited for in-context learning with only a few examples?

## Architecture Onboarding

- Component map: LLM backbone (decoder architecture) -> LoRA adapter layers -> Task-specific classification head (or constrained generation) -> Evaluation metrics (F1-score)
- Critical path: Training data preparation -> Fine-tuning with LoRA -> Validation and hyperparameter tuning -> Test evaluation
- Design tradeoffs: Larger models provide better performance but require more computational resources and longer training times; parameter-efficient fine-tuning (LoRA) reduces resource requirements but may limit performance gains
- Failure signatures: Poor validation performance indicates insufficient training data or incorrect task formulation; overfitting shows when training performance is much higher than validation; random performance suggests the model hasn't learned the task semantics
- First 3 experiments:
  1. Run baseline random classifier to establish performance floor
  2. Test in-context learning with varying shot counts to confirm it fails
  3. Fine-tune RoBERTa (encoder baseline) to establish encoder performance ceiling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on semantics-aware process mining tasks when fine-tuned with data from real-world event logs rather than synthetic process models?
- Basis in paper: [explicit] The authors fine-tuned LLMs using synthetic data generated from 15,857 process models, achieving F1 scores of 0.79-0.88. The paper suggests integrating LLMs with state-of-the-art process mining approaches as future work.
- Why unresolved: The evaluation was conducted on synthetic data generated from process models, not actual event logs from real organizational processes. Real-world data may have different characteristics like noise, incomplete traces, or complex interdependencies.
- What evidence would resolve it: Experiments comparing LLM performance on real-world event logs versus synthetic data, measuring F1 scores across different process domains and log qualities.

### Open Question 2
- Question: What is the optimal balance between few-shot in-context learning and fine-tuning for semantics-aware process mining tasks?
- Basis in paper: [explicit] The authors found that few-shot ICL performed poorly (F1 ~0.5) compared to fine-tuning (F1 ~0.79-0.88), but noted that ICL requires no training effort. They mention this as a trade-off between performance and training effort.
- Why unresolved: The paper doesn't explore intermediate approaches that might combine the benefits of both methods, such as progressive fine-tuning, adapter-based approaches, or meta-learning strategies.
- What evidence would resolve it: Comparative experiments testing various hybrid approaches between ICL and full fine-tuning, measuring both performance and training efficiency.

### Open Question 3
- Question: How do different types of process model structures (e.g., parallel, sequential, looping) affect LLM performance on semantics-aware tasks?
- Basis in paper: [inferred] The authors note that the process behavior corpus contains models with varying complexity (2-21 activities, 1-10,080 activity sequences). They observed that LLMs performed better on certain process types during in-depth analysis.
- Why unresolved: The analysis was limited to qualitative observations. The paper doesn't provide quantitative analysis of how different process structures impact LLM performance or whether certain architectures are systematically better at handling specific patterns.
- What evidence would resolve it: Systematic experiments categorizing process models by structural characteristics, measuring LLM performance per category, and identifying correlations between model structure and prediction accuracy.

## Limitations
- The evaluation relies on synthetic data from process models rather than real-world event logs
- Only 6 in-context examples were used, potentially underestimating few-shot learning capabilities
- The study focuses exclusively on control-flow semantics without data attributes

## Confidence
- High: The experimental methodology is sound and the results are clearly demonstrated with appropriate baselines and statistical measures
- Medium: The findings are specific to the SAP-SAM dataset and may not generalize to all process mining domains
- Low: The computational resource requirements for fine-tuning large LLMs may limit practical applicability in resource-constrained settings

## Next Checks
1. **Generalization Testing**: Evaluate the same LLM models on process mining datasets from different domains (e.g., healthcare, manufacturing) to assess whether the performance gains generalize beyond SAP-SAM.

2. **ICL Parameter Exploration**: Systematically test larger context windows and varying numbers of in-context examples to determine if few-shot performance can be improved beyond the 6-example baseline.

3. **Resource Efficiency Analysis**: Conduct ablation studies comparing different parameter-efficient fine-tuning methods (LoRA vs. other adapters) to identify the most resource-efficient approach that maintains performance gains.