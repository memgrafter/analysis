---
ver: rpa2
title: 'LEGOBench: Scientific Leaderboard Generation Benchmark'
arxiv_id: '2401.06233'
source_url: https://arxiv.org/abs/2401.06233
tags:
- papers
- leaderboard
- dataset
- arxiv
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LEGOBench is a benchmark for scientific leaderboard generation
  that uses 22 years of arXiv data and 11k PapersWithCode leaderboards. It evaluates
  three task configurations: ranking papers based on content and graph (RPG), ranking
  papers by prompting language models (RPLM), and generating leaderboard entries by
  prompting language models (LGPLM).'
---

# LEGOBench: Scientific Leaderboard Generation Benchmark

## Quick Facts
- arXiv ID: 2401.06233
- Source URL: https://arxiv.org/abs/2401.06233
- Reference count: 27
- Best models achieve only around 20% Complete Inclusion Score for RPG tasks and 25% Method Recall for LGPLM tasks

## Executive Summary
LEGOBench is a comprehensive benchmark for scientific leaderboard generation that leverages 22 years of arXiv data and 11k PapersWithCode leaderboards. The benchmark evaluates three task configurations - ranking papers based on content and graph (RPG), ranking papers by prompting language models (RPLM), and generating leaderboard entries by prompting language models (LGPLM). Results show significant performance gaps in existing models, with state-of-the-art systems struggling to generate accurate leaderboards, highlighting the need for improved methods in this area.

## Method Summary
LEGOBench constructs a large-scale dataset by combining arXiv papers (titles, abstracts, full-texts, citation networks, and performance comparison networks) with PapersWithCode leaderboards. The benchmark evaluates three main task configurations: RPG tasks use graph-based ranking methods leveraging citation and performance comparison networks; RPLM tasks use language model prompting with paper titles and queries; and LGPLM tasks use retrieval-augmented generation to extract leaderboard entries from full paper text. Performance is assessed using metrics like Kendall's Tau, Binary Exact Match, and Method Recall across six specific task variations.

## Key Results
- State-of-the-art models achieve only around 20% Complete Inclusion Score for RPG tasks
- Language model-based approaches show ~25% Method Recall for LGPLM tasks
- Significant performance gaps exist across all task configurations, indicating the difficulty of automatic leaderboard generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LEGOBench leverages large-scale arXiv and PapersWithCode data to create a realistic evaluation framework for automatic leaderboard generation
- Mechanism: The benchmark combines 22 years of arXiv data with 11k PapersWithCode leaderboards to provide comprehensive training and evaluation data covering diverse ML tasks, datasets, and metrics
- Core assumption: The arXiv corpus contains sufficient information about model performance and citations to enable automatic leaderboard construction
- Evidence anchors:
  - [abstract] "LEGOBench is curated from 22 years of preprint submission data on arXiv and more than 11k machine learning leaderboards on the PapersWithCode portal"
  - [section 2] Describes the APC dataset construction pipeline including citation networks and performance comparison networks
  - [corpus] Weak evidence - only 8 related papers found, none directly addressing the specific LEGOBench methodology
- Break condition: If arXiv papers don't consistently report performance metrics or if citation patterns don't reflect actual performance relationships

### Mechanism 2
- Claim: The benchmark evaluates both graph-based and language model approaches to leaderboard generation
- Mechanism: Six task configurations are provided - four graph-based (RPG) and two language model-based (RPLM, LGPLM) - allowing comprehensive assessment of different methodologies
- Core assumption: Both network structure and paper content contain complementary information useful for leaderboard generation
- Evidence anchors:
  - [abstract] "We present four graph-based and two language model-based leaderboard generation task configurations"
  - [section 3] Details the three main task types and their specific configurations
  - [section 4] Shows baseline results for all six configurations
- Break condition: If either graph-based or language model approaches consistently outperform the other across all configurations

### Mechanism 3
- Claim: The benchmark reveals significant performance gaps in existing models for automatic leaderboard generation
- Mechanism: State-of-the-art models achieve low scores on metrics like Kendall's Tau, Binary Exact Match, and Method Recall, indicating the difficulty of the task
- Core assumption: The gap between current performance and perfect leaderboard generation is measurable and meaningful
- Evidence anchors:
  - [abstract] "Results show significant performance gaps in existing models, with state-of-the-art systems struggling to generate accurate leaderboards"
  - [section 4] Provides detailed baseline results showing low performance across all task configurations
  - [section 5] Discusses related work and positions LEGOBench as addressing limitations in automatic leaderboard generation
- Break condition: If baseline models achieve near-perfect scores, indicating the task may be too easy or metrics are flawed

## Foundational Learning

- Concept: Graph neural networks and node ranking algorithms
  - Why needed here: The RPG tasks require understanding how to leverage citation and performance comparison networks to rank papers by performance
  - Quick check question: How would you modify PageRank to incorporate both citation structure and performance score information from paper content?

- Concept: Language model prompting and retrieval-augmented generation
  - Why needed here: The RPLM and LGPLM tasks rely on carefully crafted prompts and retrieval mechanisms to extract performance information from paper text
  - Quick check question: What are the key differences between zero-shot and few-shot prompting for the RPLM task, and how might each affect performance?

- Concept: Information extraction from scientific text
  - Why needed here: The LGPLM task requires extracting method names and performance scores from full paper text, which involves understanding scientific writing conventions
  - Quick check question: How would you design a chunking strategy for full paper text to maximize the likelihood of retrieving relevant performance information for a given query?

## Architecture Onboarding

- Component map: Data preprocessing (PDF parsing, table extraction) → network construction (citation and performance comparison networks) → task-specific pipelines (RPG, RPLM, LGPLM) → evaluation modules
- Critical path: For RPG tasks - query processing → candidate retrieval → graph construction → ranking → evaluation; For RPLM tasks - query + titles → LLM prompting → ranking → evaluation; For LGPLM tasks - query → paper retrieval → chunking → BM25 ranking → LLM generation → evaluation
- Design tradeoffs: Using AP-CN vs AP-PCN affects the richness of performance information; using TABS vs FT affects information completeness but also candidate retrieval complexity; using different LLMs affects instruction-following capability vs hallucination tendency
- Failure signatures: Low Kendall's Tau values indicate poor ranking correlation; low Binary Exact Match suggests complete ranking failures; low Method Recall indicates inability to extract method names from text
- First 3 experiments:
  1. Run RPG[CN-TABS] with SciBERT encoding on a small subset to verify graph construction and ranking pipeline
  2. Test RPLM with Llama 2 Chat 7B on queries with 3-4 paper titles to assess instruction-following capability
  3. Evaluate LGPLM with GPT-4 on a simple query where paper text is known to contain explicit performance tables

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the efficiency of OCR tools like GROBID affect the overall performance of LGPLM tasks?
- Basis in paper: [explicit] The paper mentions that poor efficiency of GROBID in extracting table data is a major challenge in their pipeline design.
- Why unresolved: The paper identifies this as a limitation but does not explore alternative OCR tools or methods to improve table extraction efficiency.
- What evidence would resolve it: Comparative studies using different OCR tools or improved table extraction methods to measure their impact on LGPLM task performance.

### Open Question 2
- Question: What is the impact of using full-text data versus titles and abstracts on the performance of RPG tasks?
- Basis in paper: [explicit] The paper notes that using AP-FT leads to inferior performance than AP-TABS, attributing it to the large candidate set retrieved from AP-FT.
- Why unresolved: The paper suggests potential improvements but does not provide empirical evidence or methods to effectively utilize full-text data in RPG tasks.
- What evidence would resolve it: Experiments comparing RPG task performance using different candidate retrieval strategies or data subsets from AP-FT.

### Open Question 3
- Question: How can the recall of the BM25 ranker be improved for better performance in LGPLM tasks?
- Basis in paper: [explicit] The paper identifies low recall of the BM25 ranker as a major challenge in their pipeline design for LGPLM tasks.
- Why unresolved: The paper does not explore alternative ranking methods or improvements to BM25 to enhance recall for LGPLM tasks.
- What evidence would resolve it: Comparative studies using different ranking algorithms or BM25 parameter tuning to measure their impact on LGPLM task performance.

## Limitations
- Benchmark focuses primarily on ML leaderboards, limiting generalizability to other scientific domains
- Performance heavily dependent on prompt engineering and specific model configurations
- OCR limitations in extracting structured performance data from paper tables affects LGPLM task performance

## Confidence
- **High confidence**: The benchmark successfully constructs a large-scale dataset combining arXiv and PapersWithCode data for leaderboard generation evaluation
- **Medium confidence**: The significant performance gaps reported for existing models are robust and meaningful
- **Low-Medium confidence**: The specific performance numbers for language model-based tasks will generalize to other model architectures or domains

## Next Checks
1. **Cross-domain validation**: Test the benchmark on a subset of papers from non-ML domains (e.g., physics, biology) to assess generalizability beyond the ML-focused evaluation
2. **Alternative prompt evaluation**: Systematically vary prompt templates and model parameters for RPLM and LGPLM tasks to establish the robustness of reported performance gaps
3. **Human evaluation study**: Conduct a user study where domain experts assess the practical utility of automatically generated leaderboards compared to human-curated ones, measuring both accuracy and usability