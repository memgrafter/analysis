---
ver: rpa2
title: 'OMNI-EPIC: Open-endedness via Models of human Notions of Interestingness with
  Environments Programmed in Code'
arxiv_id: '2405.15568'
source_url: https://arxiv.org/abs/2405.15568
tags:
- self
- position
- platform
- robot
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OMNI-EPIC is a framework that uses foundation models to autonomously
  generate learnable and interesting tasks for open-ended reinforcement learning.
  It creates both the simulated environment and reward functions, enabling it to produce
  any simulatable learning task.
---

# OMNI-EPIC: Open-endedness via Models of human Notions of Interestingness with Environments Programmed in Code

## Quick Facts
- arXiv ID: 2405.15568
- Source URL: https://arxiv.org/abs/2405.15568
- Reference count: 40
- Open-ended RL framework that generates both environments and reward functions using foundation models

## Executive Summary
OMNI-EPIC is a framework that leverages foundation models to autonomously generate learnable and interesting tasks for open-ended reinforcement learning. The system creates both simulated environments and reward functions, enabling it to produce any simulatable learning task. By employing a task archive, a task generator for proposing new challenges, an environment generator for creating executable code, a model of interestingness for evaluating novelty, and a success detector for assessing task completion, OMNI-EPIC successfully generates diverse and creative tasks ranging from simple to complex challenges while adapting task difficulty based on the agent's learning progress.

## Method Summary
OMNI-EPIC operates through a closed-loop system where tasks are generated, evaluated for interestingness, implemented as executable environments, trained on, and then assessed for completion. The framework uses foundation models for multiple components: task generation from natural language descriptions, environment code generation in Python, interestingness evaluation based on human notions learned from text corpora, and success detection through image analysis. The system maintains a task archive that stores both successfully learned and failed tasks, using them as context for generating new, appropriately challenging tasks. This approach enables true open-endedness by removing manual constraints of predefined task distributions and allowing the generation of any computable environment.

## Key Results
- Successfully generates diverse tasks ranging from simple challenges like "cross a bridge" to complex scenarios with multiple objectives
- Adapts task difficulty based on agent learning progress by using past successes and failures as stepping stones
- Demonstrates creative task generation through combinations of objects and objectives that weren't explicitly programmed
- Shows potential for advancing self-improving AI systems and achieving Darwin Completeness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OMNI-EPIC achieves true open-endedness by generating both environment code and reward functions, enabling it to create any simulatable learning task.
- Mechanism: By leveraging foundation models to autonomously generate Python code that defines both the simulated world and reward/termination functions, the system removes manual constraints of predefined task distributions. This code-generation approach allows OMNI-EPIC to, in principle, create any computable environment since code is Turing Complete.
- Core assumption: Foundation models can reliably generate executable and meaningful Python code from natural language task descriptions that accurately captures the intended learning task.
- Evidence anchors:
  - [abstract] "OMNI-EPIC generates both environments (e.g., an obstacle course) and reward functions (e.g., progress through the obstacle course quickly without touching red objects)"
  - [section 3.3] "The environment generator, powered by an LLM, translates a given natural language task description into executable (here, Python) code defining the learning environment"
- Break condition: Foundation models fail to generate executable, bug-free code that accurately represents the intended task, or the generated code cannot be compiled/run in the target simulator.

### Mechanism 2
- Claim: OMNI-EPIC adapts task difficulty based on agent learning progress by using past successes and failures as stepping stones.
- Mechanism: The system maintains an archive of learned and failed tasks. When generating new tasks, it retrieves the most similar tasks from this archive to use as context for the task generator. This ensures new tasks build upon previously acquired skills while avoiding tasks that were previously too difficult.
- Core assumption: Similar tasks in the archive serve as appropriate stepping stones for generating new, learnable challenges that progressively increase in complexity.
- Evidence anchors:
  - [section 3.2] "Given the limited context length of current LLMs, we retrieve a predefined number of the most similar tasks (both completed successfully and attempted but failed) to a randomly selected task in the archive"
- Break condition: The similarity-based task retrieval fails to identify appropriate stepping stones, leading to either repetitive tasks or tasks that are too difficult/easy for the agent's current capabilities.

### Mechanism 3
- Claim: OMNI-EPIC ensures generated tasks are genuinely interesting by using a model of interestingness rather than predefined metrics.
- Mechanism: Instead of optimizing for hand-crafted metrics of interestingness (which can create pathologies), OMNI-EPIC employs an LLM-based model of interestingness that leverages human notions of interestingness learned from large text corpora. This model evaluates newly generated tasks against similar tasks in the archive to determine if they are novel, surprising, diverse, or worthwhile.
- Core assumption: Large language models trained on human-generated text can accurately capture and evaluate nuanced human notions of interestingness.
- Evidence anchors:
  - [section 3.4] "Instead of using a predefined metric for interestingness, we employ an LLM-based model of interestingness, which inherently possesses human notions of interestingness"
  - [section 1] "Previous attempts to quantify interestingness have created pathologies, where the agent optimizes for a metric rather than truly capturing the essence of interestingness"
- Break condition: The LLM-based model of interestingness fails to accurately distinguish between genuinely interesting tasks and those that merely optimize for superficial novelty.

## Foundational Learning

- Concept: Reinforcement Learning fundamentals (policy optimization, reward functions, exploration vs exploitation)
  - Why needed here: OMNI-EPIC trains agents using reinforcement learning algorithms (DreamerV3) to solve the generated tasks. Understanding RL concepts is essential for comprehending how agents learn from the environments OMNI-EPIC creates.
  - Quick check question: What is the difference between the reward function used for training and the success detector used to evaluate task completion?

- Concept: Foundation Models and Large Language Models capabilities
  - Why needed here: OMNI-EPIC relies heavily on FMs/LLMs for multiple components - task generation, environment code generation, model of interestingness, and success detection. Understanding what these models can and cannot do is crucial for evaluating the system's capabilities and limitations.
  - Quick check question: What are the key limitations of current LLMs that OMNI-EPIC must work around (e.g., context length, hallucination, code generation reliability)?

- Concept: Open-endedness and AI-generating algorithms
  - Why needed here: OMNI-EPIC is positioned within the broader context of open-ended algorithms and AI-GAs. Understanding the goals and challenges of open-endedness (infinite task generation, Darwin Completeness) provides context for why OMNI-EPIC's approach is significant.
  - Quick check question: What does "Darwin Completeness" mean in the context of open-ended algorithms, and why is it a challenging goal to achieve?

## Architecture Onboarding

- Component map: Task Archive -> Task Generator -> Environment Generator -> Model of Interestingness -> Training System -> Success Detector -> Task Archive (forming a closed loop)

- Critical path: The most critical execution path is: Task Generator → Environment Generator → Model of Interestingness → Training System → Success Detector → Task Archive. Any failure in this path (e.g., code generation errors, uninteresting task rejections, training failures) causes the system to loop back and regenerate tasks.

- Design tradeoffs: OMNI-EPIC trades computational efficiency for generality - generating environment code from scratch for each task is more computationally expensive than using predefined task distributions, but enables true open-endedness. The system also trades determinism for creativity - using FMs introduces stochasticity but enables more diverse and interesting task generation compared to rule-based approaches.

- Failure signatures: Common failure modes include: environment code compilation errors (causing regeneration loops), tasks deemed uninteresting by the model of interestingness (causing rejection and regeneration), RL training failures (causing task modifications or rejections), and success detector inaccuracies (causing incorrect task classifications). The system is designed to handle these through iterative regeneration and reflection.

- First 3 experiments:
  1. Run OMNI-EPIC with a simple task description seed (e.g., "cross a bridge") and verify that it generates executable environment code, trains an agent, and successfully adds the task to the archive.
  2. Test the model of interestingness by generating two similar tasks and verifying that it correctly identifies one as interesting and the other as not interesting based on novelty.
  3. Test the success detector by providing it with images of an agent completing a task and verifying it correctly identifies success/failure.

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on foundation models raises concerns about reliability and reproducibility of code generation
- Claim of true open-endedness is limited by the underlying simulator's capabilities and LLM's ability to generate meaningful task descriptions
- Substantial computational requirements as each task requires environment generation, code compilation, and RL training from scratch

## Confidence
**High Confidence:** The core architectural design connecting the six components is well-defined and logically coherent. The use of LLMs for code generation from natural language is a validated technique with established precedents.

**Medium Confidence:** The claims about generating genuinely interesting and diverse tasks are supported by the described mechanisms but lack comprehensive empirical validation across varied domains.

**Low Confidence:** The assertion that OMNI-EPIC achieves "Darwin Completeness" is premature given the limited experimental scope and lack of long-term open-endedness demonstrations.

## Next Checks
1. **Code Generation Reliability Test:** Systematically measure the success rate of environment code generation and compilation across 100 randomly sampled task descriptions spanning different complexity levels and domains.

2. **Interestingness Model Validation:** Conduct a human evaluation study comparing tasks selected by the LLM-based interestingness model against those selected by traditional novelty metrics, measuring perceived creativity and learning value.

3. **Long-term Open-endedness Demonstration:** Run OMNI-EPIC for 50+ generations while tracking metrics like task diversity (measured by code structural differences), agent skill progression, and the emergence of increasingly complex task hierarchies.