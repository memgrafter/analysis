---
ver: rpa2
title: Free Process Rewards without Process Labels
arxiv_id: '2412.01981'
source_url: https://arxiv.org/abs/2412.01981
tags:
- implicit
- training
- reward
- data
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to train process reward models (PRMs)
  without requiring step-level annotations, addressing a major challenge in PRM development.
  The key insight is that by parameterizing rewards as log-likelihood ratios of policy
  and reference models, an implicit PRM can be obtained for free during standard outcome
  reward model training.
---

# Free Process Rewards without Process Labels

## Quick Facts
- arXiv ID: 2412.01981
- Source URL: https://arxiv.org/abs/2412.01981
- Authors: Lifan Yuan; Wendi Li; Huayu Chen; Ganqu Cui; Ning Ding; Kaiyan Zhang; Bowen Zhou; Zhiyuan Liu; Hao Peng
- Reference count: 16
- Primary result: Implicit PRMs trained without step labels outperform Math-Shepherd and AutoPSV while using 38.8× less computational resources

## Executive Summary
This paper presents a method to train process reward models (PRMs) without requiring step-level annotations, addressing a major challenge in PRM development. The key insight is that by parameterizing rewards as log-likelihood ratios of policy and reference models, an implicit PRM can be obtained for free during standard outcome reward model training. The method was evaluated on the MATH dataset, where it outperformed strong baselines while using significantly fewer computational resources.

The approach works with various training objectives including DPO, KTO, NCA, and CE loss, with CE loss being particularly effective in data-scarce scenarios. The method also benefits from scaling up both instructions and responses during training, with response scaling having a larger impact. Surprisingly, adding step-level labels from existing methods provided no additional benefit, and the reference model can be removed during inference in certain cases for further efficiency gains.

## Method Summary
The method parameterizes outcome rewards as log-likelihood ratios between policy and reference models (r_θ(y) = β log π_θ(y) / π_ref(y)), enabling implicit PRM training during standard outcome reward model (ORM) training. This approach avoids the need for expensive step-level annotations while still capturing process quality. The implicit PRM is extracted from the trained ORM and can be used for best-of-N sampling or reinforcement learning. The method supports multiple training objectives including DPO, KTO, NCA, and CE loss, with CE loss being particularly data-efficient as it can handle unpaired and imbalanced data.

## Key Results
- Implicit PRMs outperformed Math-Shepherd and AutoPSV baselines on MATH dataset
- Achieved 38.8× computational efficiency improvement compared to traditional methods
- CE loss training showed superior data efficiency, improving generation models even with only one response per instruction
- Response scaling had larger impact on performance than instruction scaling during training
- Adding step-level labels from existing methods provided no additional benefit

## Why This Works (Mechanism)

### Mechanism 1
- Claim: An implicit PRM can be obtained for free during standard ORM training by parameterizing rewards as log-likelihood ratios of policy and reference models.
- Mechanism: The reward parameterization r_θ(y) = β log π_θ(y) / π_ref(y) allows the ORM to simultaneously learn a Q-function that can be used for step-level evaluation. The process reward is the difference between consecutive Q-values.
- Core assumption: The reward can be expressed as a log-likelihood ratio between policy and reference models, and the training objective optimizes this parameterization.
- Evidence anchors:
  - [abstract]: "by parameterizing rewards as log-likelihood ratios of policy and reference models, an implicit PRM can be obtained for free during standard outcome reward model training"
  - [section 3]: "by parameterizin the reward as the log-likelihood ratio of the policy and the reference models r_θ(y) = β log π_θ(y) / π_ref(y), a common practice in DPO (Rafailov et al., 2023) and many of its variants"
  - [corpus]: Weak - corpus neighbors discuss similar implicit reward approaches but don't directly validate this specific mechanism
- Break condition: If the reward cannot be properly expressed as a log-likelihood ratio, or if the reference model is too dissimilar from the policy model, the implicit PRM may not learn effectively.

### Mechanism 2
- Claim: The implicit PRM provides more accurate step-level rewards than MCTS-based approaches because it estimates expected outcome rewards rather than using hard or soft estimations.
- Mechanism: The implicit PRM calculates Q-values as exact expectations of outcome rewards, lying between the bounds of hard and soft MCTS estimations, thus mitigating noise from both overestimation and underestimation.
- Core assumption: The implicit PRM's Q-values are bounded by hard and soft MCTS estimations and can achieve these bounds under specific conditions.
- Evidence anchors:
  - [section 3]: "Proposition 3.2 demonstrates that qt_θ ranges between the soft-estimated and hard-estimated Q values annotated by MCTS-based approaches"
  - [abstract]: "Our approach has better accuracy and robustness to noises than MCTS-based approaches"
  - [corpus]: Weak - corpus neighbors don't directly compare implicit PRMs to MCTS-based methods
- Break condition: If the implicit PRM is poorly trained or the log-likelihood ratio parameterization doesn't capture the true reward structure, the bounds may not hold.

### Mechanism 3
- Claim: Cross-entropy loss can be used to train implicit PRMs effectively, making them more data-efficient than pairwise preference methods like DPO.
- Mechanism: CE loss can handle unpaired and imbalanced data, allowing training with only one response per instruction, which is advantageous in data-scarce scenarios.
- Core assumption: CE loss can optimize the log-likelihood ratio parameterization even without pairwise comparisons.
- Evidence anchors:
  - [abstract]: "Particularly, we find that our implicit PRM, when instantiated with the cross-entropy (CE) loss, is more data-efficient and can keep improving generation models even when trained with only one response per instruction"
  - [section 4.1]: "For DPO and NCA, we pair each correct rollout with an incorrect counterpart and train our RM on these response-level pairs, while for KTO and CE loss, we directly train on the unpaired and imbalanced rollouts"
  - [corpus]: Weak - corpus neighbors don't specifically discuss CE loss for implicit PRMs
- Break condition: If the data imbalance is extreme or the task requires pairwise comparisons for accurate reward modeling, CE loss may underperform.

## Foundational Learning

- Concept: Log-likelihood ratio parameterization
  - Why needed here: This is the core mechanism that enables implicit PRM training without step labels
  - Quick check question: Can you explain why parameterizing rewards as log-likelihood ratios allows an ORM to simultaneously learn a Q-function?

- Concept: Monte Carlo Tree Search (MCTS) for step annotation
  - Why needed here: Understanding traditional PRM training methods helps appreciate the innovation of implicit PRMs
  - Quick check question: What are the main drawbacks of using MCTS for step-level annotation compared to implicit PRMs?

- Concept: Cross-entropy loss for preference learning
  - Why needed here: This is one of the training objectives that can instantiate implicit PRMs, especially useful in data-scarce scenarios
  - Quick check question: How does CE loss handle unpaired and imbalanced data differently from pairwise preference methods?

## Architecture Onboarding

- Component map:
  - Policy model (π_θ) -> Reference model (π_ref) -> Implicit PRM -> Generation model
  - ORM training loop with log-likelihood ratio parameterization

- Critical path:
  1. Collect outcome-level data (instructions + responses + correctness labels)
  2. Train ORM with log-likelihood ratio parameterization
  3. Extract implicit PRM from trained ORM
  4. Use implicit PRM for best-of-N sampling or RL

- Design tradeoffs:
  - Using reference model doubles inference cost but provides KL regularization
  - CE loss allows unpaired data but may be less precise than pairwise methods
  - Scaling responses more influential than scaling instructions for performance
  - Majority voting can improve performance but adds complexity

- Failure signatures:
  - Poor performance on step-level evaluation despite good outcome-level performance
  - Degradation when reference model is removed at inference
  - Inconsistent improvements across different generation models
  - Overfitting to training instructions when they're not representative of test tasks

- First 3 experiments:
  1. Train implicit PRM with CE loss on minimal data (1 response per instruction) and evaluate on best-of-N sampling
  2. Compare implicit PRM performance with and without reference model at inference
  3. Test scaling effects by varying numbers of responses per instruction while keeping instructions constant

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of implicit PRMs vary across different mathematical domains (e.g., algebra, geometry, calculus) and what are the specific strengths and weaknesses in each domain?
- Basis in paper: [inferred] The paper evaluates on the MATH dataset but doesn't provide detailed analysis of performance across different mathematical subdomains.
- Why unresolved: The paper only reports aggregate performance metrics without breaking down results by mathematical topic, making it unclear where the approach excels or struggles.
- What evidence would resolve it: Detailed ablation studies showing performance metrics for different mathematical subdomains within MATH, along with qualitative analysis of error patterns in each domain.

### Open Question 2
- Question: What is the theoretical limit of implicit PRMs' performance as the quality of the reference model improves, and how does this compare to explicit PRM training methods?
- Basis in paper: [explicit] The paper discusses that "A good step might receive high probabilities by both πθ and πref" and that the reference model can be removed at inference in certain cases, suggesting potential limitations or ceiling effects.
- Why unresolved: The paper shows empirical results but doesn't provide theoretical bounds on performance or analyze how close implicit PRMs can get to optimal PRM performance.
- What evidence would resolve it: Mathematical analysis proving upper bounds on implicit PRM performance, experiments varying reference model quality, and comparison to oracle PRM performance.

### Open Question 3
- Question: What is the impact of instruction complexity and diversity on implicit PRM training efficiency and performance, and how can we optimize instruction selection for training?
- Basis in paper: [explicit] The paper mentions that "instructions should be relevant to downstream tasks" but doesn't provide detailed analysis of how instruction complexity or diversity affects training.
- Why unresolved: The paper only briefly touches on instruction scaling effects without analyzing the relationship between instruction characteristics and model performance.
- What evidence would resolve it: Systematic experiments varying instruction complexity, diversity, and relevance during training, along with analysis of how these factors affect convergence speed and final performance.

## Limitations
- The log-likelihood ratio parameterization assumption may not hold for all reward structures
- Performance heavily depends on quality and quantity of outcome-level data
- Removing reference model during inference may lead to inconsistent performance across different generation models
- Lack of step-level labels in training could limit capture of fine-grained process quality signals

## Confidence
- High confidence: Computational efficiency advantage (38.8× less FLOPs) and core log-likelihood ratio parameterization insight
- Medium confidence: CE loss being more data-efficient and superiority over MCTS-based approaches
- Medium confidence: Performance comparisons across different training objectives

## Next Checks
1. Cross-dataset validation: Test implicit PRM approach on non-MATH datasets (e.g., GSM8K, coding tasks) to assess generalizability
2. Ablation of log-likelihood ratio assumption: Conduct experiments with alternative reward parameterizations to verify criticality
3. Step-level evaluation: Perform detailed error analysis on intermediate steps in generated solutions to understand what aspects of process quality are captured