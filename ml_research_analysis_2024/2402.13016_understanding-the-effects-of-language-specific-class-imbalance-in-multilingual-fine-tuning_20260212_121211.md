---
ver: rpa2
title: Understanding the effects of language-specific class imbalance in multilingual
  fine-tuning
arxiv_id: '2402.13016'
source_url: https://arxiv.org/abs/2402.13016
tags:
- language
- class
- imbalance
- shap
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies the effect of language-specific class imbalance
  in multilingual fine-tuning, a type of imbalance often present in real-life multilingual
  classification datasets. It shows that fine-tuning a transformer-based Large Language
  Model (LLM) on a dataset with this imbalance leads to worse performance, a more
  pronounced separation of languages in the latent space, and the promotion of uninformative
  features.
---

# Understanding the effects of language-specific class imbalance in multilingual fine-tuning

## Quick Facts
- arXiv ID: 2402.13016
- Source URL: https://arxiv.org/abs/2402.13016
- Reference count: 10
- Key outcome: Language-specific class imbalance in multilingual datasets leads to worse performance, greater language separation in latent space, and promotion of uninformative features; these effects can be mitigated through per-language class weighing.

## Executive Summary
This paper investigates how language-specific class imbalance—where different languages have different label distributions—affects multilingual fine-tuning of transformer models. The authors demonstrate that this type of imbalance, even when the overall dataset is balanced, leads to worse task performance and causes models to rely more on language identity than language-agnostic patterns. Using SHAP analysis, they show that the model promotes uninformative features and demotes informative ones when fine-tuned on imbalanced data. To address these issues, the paper proposes modifying traditional class weighing by calculating class weights separately for each language, which improves performance and reduces language separation in the latent space.

## Method Summary
The authors create both balanced and imbalanced subsets from multilingual classification datasets (XNLI and Amazon reviews) where the overall dataset is balanced but per-language label distributions differ. They fine-tune multilingual BERT (mBERT) and XLM-R on these datasets using standard cross-entropy loss with per-language class weights. They evaluate task performance through accuracy metrics, analyze language separation in the latent space by training language identification classifiers, and use SHAP values to examine token-level feature importance. The key intervention is calculating class weights separately for each language rather than globally across the entire dataset.

## Key Results
- Models trained on imbalanced datasets show significantly worse test set accuracy compared to balanced datasets
- Language identification accuracy is higher on latent representations from imbalanced models, indicating greater language separation
- SHAP analysis reveals that models shift attention from informative to uninformative features when fine-tuned on imbalanced data
- Per-language class weighing effectively mitigates these negative effects, improving performance and reducing language separation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on a dataset with language-specific class imbalance leads to worse task performance.
- Mechanism: The model learns to rely on language identity rather than language-agnostic patterns, making incorrect predictions based on skewed label distributions per language.
- Core assumption: The joint distribution of language and labels being skewed (even if marginal distributions are uniform) biases the model toward language-specific cues.
- Evidence anchors:
  - [abstract] "We show evidence that fine-tuning a transformer-based Large Language Model (LLM) on a dataset with this imbalance leads to worse performance"
  - [section] "In Table 3, we report the test set accuracy for models trained on the balanced and imbalanced datasets. Unsurprisingly, we see that the models trained on the balanced datasets perform better than the ones trained on the imbalanced datasets."
  - [corpus] Weak corpus support for this specific mechanism; related works focus on multilingual fine-tuning but not specifically on joint distribution skew.
- Break condition: If marginal label distributions are also skewed to match the joint distribution, or if the task inherently requires language-specific cues.

### Mechanism 2
- Claim: Language-specific class imbalance causes the latent space to become more separated by language.
- Mechanism: The model develops stronger language-specific representations to exploit the predictable relationship between language and label distribution.
- Core assumption: Language identity becomes a discriminative feature when label distributions vary by language.
- Evidence anchors:
  - [abstract] "a more pronounced separation of languages in the latent space"
  - [section] "In Table 4, we can see that for both XNLI and the Amazon reviews dataset, the language identification accuracy is higher for the model trained on the imbalanced dataset compared with the balanced one."
  - [corpus] Moderate support; related works discuss language-specificity in multilingual models but not specifically from class imbalance.
- Break condition: If the model architecture or fine-tuning method explicitly regularizes toward language-agnostic representations.

### Mechanism 3
- Claim: The model promotes uninformative features and demotes informative ones when fine-tuned on imbalanced data.
- Mechanism: SHAP analysis reveals that tokens with high predictive value in balanced conditions lose importance, while neutral tokens gain importance in over-represented languages.
- Core assumption: The model shifts from content-based to language-based prediction strategies.
- Evidence anchors:
  - [abstract] "the promotion of uninformative features"
  - [section] "Using SHAP values, we show that the model pays more attention to uninformative features when fine-tuned on a dataset with this imbalance, in effect acting more like a language identifier."
  - [corpus] Weak corpus support; explainability methods like SHAP are used but not specifically for analyzing language-specific class imbalance effects.
- Break condition: If the model uses explicit language-agnostic regularization or if the imbalance is too subtle to trigger this shift.

## Foundational Learning

- Concept: Class imbalance in machine learning
  - Why needed here: Understanding traditional class imbalance is prerequisite to grasping how language-specific class imbalance differs and affects multilingual models.
  - Quick check question: What is the standard approach to address class imbalance in traditional machine learning, and how does it differ from the per-language class weighing proposed here?

- Concept: Multilingual representations in transformer models
  - Why needed here: The paper relies on understanding how multilingual models like mBERT create both language-agnostic and language-specific components in their latent space.
  - Quick check question: What are the two types of components (language-agnostic vs language-specific) that multilingual transformers are known to build in their latent space?

- Concept: SHAP (SHapley Additive exPlanations) values
  - Why needed here: The paper uses SHAP values to analyze how the model makes predictions at the token level and how this changes between balanced and imbalanced cases.
  - Quick check question: How do SHAP values estimate the marginal contribution of each input token to a model's prediction?

## Architecture Onboarding

- Component map: mBERT/XLM-R -> Classifier head (feed-forward + SoftMax) -> Per-language class-weighted cross-entropy loss -> SHAP analysis for explainability

- Critical path:
  1. Create balanced and imbalanced subsets from the same data
  2. Fine-tune the multilingual model on each subset
  3. Evaluate task performance (accuracy)
  4. Train language identification classifier on latent representations
  5. Calculate SHAP values to analyze feature importance
  6. Apply per-language class weighing to mitigate imbalance effects

- Design tradeoffs:
  - Using mBERT vs XLM-R: mBERT is smaller and faster but XLM-R generally performs better
  - Per-language class weighing adds complexity but improves performance and reduces language separation
  - SHAP values provide interpretability but are computationally expensive and sensitive to base value differences

- Failure signatures:
  - Test set accuracy significantly lower on imbalanced model compared to balanced
  - Higher language identification accuracy on imbalanced model's latent space
  - SHAP analysis showing neutral tokens becoming more important for over-represented languages

- First 3 experiments:
  1. Train models on balanced vs imbalanced subsets and compare test set accuracy
  2. Train language identification classifier on the latent space of both models and compare accuracy
  3. Calculate and compare cumulative SHAP values between balanced and imbalanced models to identify feature importance shifts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does language-specific class imbalance affect model performance on out-of-distribution (OOD) data?
- Basis in paper: [explicit] The paper shows that models trained on imbalanced datasets perform worse and make predictions based on language identity.
- Why unresolved: The paper does not test model performance on OOD data, which would provide insights into the generalizability of the findings.
- What evidence would resolve it: Evaluating model performance on OOD data for both balanced and imbalanced datasets would demonstrate the impact of language-specific class imbalance on generalizability.

### Open Question 2
- Question: Are there alternative methods to per-language class weighing that could more effectively mitigate the effects of language-specific class imbalance?
- Basis in paper: [inferred] The paper mentions that newer weighing methods exist, such as those discussed in Henning et al. (2023), but does not explore their effectiveness in the multilingual setting.
- Why unresolved: The paper only tests one method (per-language class weighing) and does not compare it to other potential methods.
- What evidence would resolve it: Testing and comparing the performance of various class imbalance mitigation methods in the multilingual setting would identify the most effective approach.

### Open Question 3
- Question: How does the choice of transformer model (e.g., mBERT vs. XLM-R) impact the effects of language-specific class imbalance?
- Basis in paper: [explicit] The paper shows that both mBERT and XLM-R exhibit similar trends when trained on imbalanced datasets, but does not explore the differences in their responses to imbalance.
- Why unresolved: The paper does not perform a detailed comparison of the two models' behaviors under imbalanced conditions.
- What evidence would resolve it: A comprehensive comparison of mBERT and XLM-R, including their latent space representations, SHAP value distributions, and task performance under imbalanced conditions, would reveal any model-specific effects.

## Limitations
- Results are demonstrated on only two datasets (XNLI and Amazon reviews), limiting generalizability
- The causal mechanism linking joint distribution skew to language identity reliance remains theoretical
- SHAP-based analysis relies on approximations that may be sensitive to base value choices
- The paper doesn't explore effects on larger multilingual models beyond mBERT and XLM-R
- The proposed approach's effectiveness with many languages or extreme imbalance ratios is untested

## Confidence
- **High Confidence**: Models trained on imbalanced datasets perform worse than those trained on balanced datasets (supported by Table 3 accuracy comparisons)
- **Medium Confidence**: Language-specific class imbalance causes greater language separation in latent space (supported by language identification accuracy results)
- **Medium Confidence**: SHAP analysis reveals shifts toward uninformative features in imbalanced conditions (methodologically sound but based on single analytical approach)
- **Medium Confidence**: Per-language class weighing effectively mitigates these effects (demonstrated but limited to specific datasets and imbalance levels)

## Next Checks
1. **Dataset Generalization Test**: Validate the findings across 3-5 additional multilingual classification datasets with varying label distributions and language compositions to assess whether the observed patterns hold universally or are dataset-specific artifacts.

2. **Causal Mechanism Experiment**: Design an ablation study where language information is explicitly masked during training (e.g., through adversarial training) to determine whether the performance degradation is directly caused by language-based cue reliance rather than other confounding factors.

3. **Scaling Analysis**: Test whether the proposed per-language class weighing approach maintains its effectiveness when scaled to datasets with 10+ languages and extreme imbalance ratios (e.g., 100:1 label distribution differences), and compare against alternative approaches like focal loss or oversampling.