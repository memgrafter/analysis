---
ver: rpa2
title: 'Performance Evaluation of Lightweight Open-source Large Language Models in
  Pediatric Consultations: A Comparative Analysis'
arxiv_id: '2407.15862'
source_url: https://arxiv.org/abs/2407.15862
tags:
- medical
- llms
- patient
- performance
- lightweight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the performance of lightweight open-source
  large language models (LLMs) in pediatric healthcare using 250 real patient consultation
  questions from an online medical forum. The models tested were ChatGLM3-6B, Vicuna-7B,
  Vicuna-13B, and ChatGPT-3.5.
---

# Performance Evaluation of Lightweight Open-source Large Language Models in Pediatric Consultations: A Comparative Analysis

## Quick Facts
- arXiv ID: 2407.15862
- Source URL: https://arxiv.org/abs/2407.15862
- Reference count: 0
- Primary result: Lightweight open-source LLMs can achieve clinically useful performance in pediatric healthcare, with ChatGLM3-6B showing particular promise in Chinese language contexts

## Executive Summary
This study evaluated four large language models—ChatGLM3-6B, Vicuna-7B, Vicuna-13B, and ChatGPT-3.5—using 250 real patient consultation questions from an online Chinese medical forum. Each question was answered by all four models and evaluated by pediatricians on five criteria: accuracy, completeness, readability, empathy, and safety. The results showed that while ChatGPT-3.5 achieved the highest scores across most metrics, ChatGLM3-6B demonstrated competitive performance and significantly outperformed the Vicuna models. All models maintained high safety standards, with over 98.4% of responses rated as safe.

## Method Summary
The study employed a cross-sectional design using 250 pediatric consultation questions collected from haodf.com, spanning 25 pediatric departments with 10 questions each. Four LLMs were evaluated: ChatGLM3-6B (6B parameters), Vicuna-7B (7B), Vicuna-13B (13B), and ChatGPT-3.5. Each question was posed to all four models in Chinese, with responses evaluated by three pediatricians with over five years of experience. Evaluations used a 5-point scale for accuracy, completeness, and readability; a 3-point scale for empathy; and a binary scale for safety. Statistical analysis employed Kruskal-Wallis tests with Benjamini-Hochberg correction for multiple comparisons.

## Key Results
- ChatGPT-3.5 achieved the highest accuracy (65.2%) and completeness (78.4%) among all models
- ChatGLM3-6B matched ChatGPT-3.5 in readability while outperforming Vicuna models in accuracy (53.6% vs 44.8% and 42.8%)
- All models demonstrated high safety performance (>98.4% safe responses), with no statistically significant differences between them
- ChatGPT-3.5 generated the most "very empathetic" responses (14.8%) compared to lightweight LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGLM3-6B's bilingual training (50% Chinese) directly improves its performance on Chinese pediatric queries compared to English-dominant models
- Mechanism: Training data language match reduces token ambiguity, improves semantic alignment with pediatric terminology, and enhances contextual understanding
- Core assumption: Pediatric medical language in Chinese has unique terminology and syntactic patterns not well-represented in English-only datasets
- Evidence anchors:
  - [abstract] "ChatGLM3-6B, which was trained on a dataset comprising 50% Chinese-language data"
  - [section] "ChatGLM3-6B’s training dataset includes approximately 50% Chinese content17, providing it with a significant advantage in Chinese question-answering tasks"
  - [corpus] Weak - no direct corpus evidence comparing bilingual vs monolingual performance in pediatric contexts
- Break condition: If the bilingual training data is too generic (not pediatric-specific), performance gains may be marginal.

### Mechanism 2
- Claim: Safety ratings are decoupled from accuracy/completeness, allowing all models to achieve high safety scores regardless of medical knowledge depth
- Mechanism: Safety is operationalized as absence of harmful advice (dosage errors, contraindicated meds), not correctness of medical information
- Core assumption: Safety metrics focus on avoiding dangerous recommendations rather than clinical accuracy
- Evidence anchors:
  - [section] "In safety, all models performed comparably well (P > .05), with over 98.4% of responses being rated as safe"
  - [section] "In terms of safety, all four models exhibited comparable levels of performance"
  - [corpus] No corpus evidence - this is an internal study finding
- Break condition: If safety criteria expand to include clinical accuracy, differences may emerge.

### Mechanism 3
- Claim: ChatGPT-3.5's superior empathy ratings stem from fine-tuning on conversational datasets emphasizing emotional intelligence
- Mechanism: Proprietary models often undergo additional RLHF (Reinforcement Learning from Human Feedback) iterations focused on human-like interaction styles
- Core assumption: Empathy is a learned behavior through supervised fine-tuning on dialogue datasets
- Evidence anchors:
  - [section] "In terms of empathy, ChatGPT-3.5 outperformed the lightweight LLMs (P < .001)"
  - [section] "ChatGPT-3.5 led in generating the most 'very empathetic' responses at 14.8%"
  - [corpus] Weak - no direct corpus evidence linking fine-tuning methods to empathy scores
- Break condition: If empathy is more strongly correlated with model size than fine-tuning, lightweight models could close the gap.

## Foundational Learning

- Concept: Cross-sectional study design
  - Why needed here: Enables snapshot comparison of model performance on identical dataset without temporal bias
  - Quick check question: Why was a cross-sectional rather than longitudinal design chosen for this evaluation?
- Concept: Statistical significance with multiple comparisons correction
  - Why needed here: Prevents false positives when comparing multiple models across multiple metrics
  - Quick check question: What correction method was used to control the false positive rate?
- Concept: Pediatric subspecialty diversity in dataset
  - Why needed here: Ensures model evaluation covers broad pediatric knowledge, not just common conditions
  - Quick check question: How many pediatric departments were represented in the consultation sample?

## Architecture Onboarding

- Component map: Patient queries → Model inference → Rater evaluation → Statistical analysis
- Models: ChatGLM3-6B (6B), Vicuna-7B (7B), Vicuna-13B (13B), ChatGPT-3.5
- Evaluation: 3 pediatricians rating accuracy, completeness, readability, empathy, safety
- Infrastructure: Local deployment on RTX 4090 GPUs for open-source models; web API for ChatGPT-3.5
- Critical path: Query selection → Model response generation → Rater evaluation → Statistical comparison
- Design tradeoffs: Local deployment (privacy, control) vs cloud (scalability, ease of use); model size vs performance vs resource constraints
- Failure signatures: 
  - Low accuracy/completeness: Model lacks pediatric domain knowledge or language alignment
  - Poor readability: Model generates overly technical language or sentence structures
  - Low empathy: Model lacks conversational fine-tuning or emotional intelligence training
  - Safety failures: Model provides dangerous medical advice or dosage recommendations
- First 3 experiments:
  1. Test each model on 10 pediatric queries from a single department to establish baseline performance
  2. Compare bilingual vs monolingual models on identical Chinese queries to isolate language effect
  3. Evaluate safety-only metrics across all models to confirm safety metric independence from accuracy

## Open Questions the Paper Calls Out

- How would fine-tuning lightweight LLMs on pediatric-specific medical corpora impact their accuracy and completeness in responding to patient consultations?
- How do the performance characteristics of lightweight LLMs in pediatric consultations compare to those of human pediatricians in terms of accuracy, completeness, and empathy?
- How would the performance of lightweight LLMs in pediatric consultations vary across different languages and cultural contexts?

## Limitations

- The study used a cross-sectional design with a single evaluation timepoint, limiting conclusions about model performance stability over time
- All evaluation was conducted by only three pediatricians, potentially introducing rater bias despite random order presentation
- The study focused exclusively on Chinese language pediatric queries, limiting generalizability to other languages or healthcare contexts
- Model safety was evaluated using a binary classification system, which may oversimplify the complexity of medical safety assessment

## Confidence

- **High Confidence**: The comparative ranking of models across accuracy, completeness, and safety metrics (based on statistically significant differences with appropriate corrections)
- **Medium Confidence**: The claim about ChatGLM3-6B's bilingual advantage, as this is supported by training data information but not directly tested against English-only models on Chinese queries
- **Low Confidence**: The specific mechanisms linking model architecture to empathy scores, as the study provides correlational rather than causal evidence

## Next Checks

1. Conduct longitudinal evaluation with multiple timepoints to assess model performance consistency across different clinical scenarios and timeframes
2. Expand evaluator pool to include pediatricians from different subspecialties and institutions to test rater reliability and generalizability
3. Implement a more granular safety assessment framework that includes clinical accuracy metrics alongside harm prevention to better understand the relationship between safety and medical knowledge depth