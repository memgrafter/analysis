---
ver: rpa2
title: Constructing sensible baselines for Integrated Gradients
arxiv_id: '2412.13864'
source_url: https://arxiv.org/abs/2412.13864
tags:
- baseline
- features
- background
- baselines
- signal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates baseline selection for Integrated Gradients
  in a particle physics classification task, where a deep neural network distinguishes
  signal events from background events. The authors compare three baselines: a blank
  (zero-vector) baseline, an unweighted averaged background baseline, and a weighted
  averaged background baseline (weighted by cross-sections).'
---

# Constructing sensible baselines for Integrated Gradients

## Quick Facts
- arXiv ID: 2412.13864
- Source URL: https://arxiv.org/abs/2412.13864
- Authors: Jai Bardhan; Cyrin Neeraj; Mihir Rawat; Subhadip Mitra
- Reference count: 19
- One-line primary result: Zero-vector baseline performs poorly for Integrated Gradients in particle physics classification, while averaged background baselines provide more meaningful feature attributions

## Executive Summary
This paper investigates baseline selection for Integrated Gradients in a particle physics classification task, comparing zero-vector baselines against averaged background baselines (both weighted and unweighted by cross-sections). The authors demonstrate that the zero-vector baseline fails to provide meaningful feature attributions because it represents an unphysical state outside the data distribution. In contrast, baselines sampled from background events and averaged yield more reasonable attributions that better reflect the distinguishing features between signal and background events. The study shows that using these improved baselines leads to better discrimination ability in top features and improved model performance when retraining with only the top-k features.

## Method Summary
The authors train a 2-layer fully connected DNN with Swish activation, BatchNorm, Dropout, and Weight Decay on kinematic features from particle collision events to distinguish signal (vectorlike B quark production) from background (Standard Model processes). They implement Integrated Gradients with three different baselines: zero-vector baseline, unweighted averaged background baseline (using 20 samples per background process), and weighted averaged background baseline (weighted by cross-sections of the processes). The feature attributions from each baseline are compared qualitatively and quantitatively, with the top-k features from each method used to retrain the model to assess their predictive power.

## Key Results
- Zero-vector baseline provides poor feature attributions that don't reflect physically meaningful distinctions between signal and background
- Averaged background baselines consistently yield more reasonable attributions, with weighted averaging performing slightly better than unweighted
- Top features identified using averaged baselines show better discrimination ability and lead to improved model performance when retraining with only the top-k features
- The blank (zero-vector) baseline represents an unphysical state that introduces artifacts rather than meaningful feature importance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The averaged background baseline improves feature attribution quality by reducing noise from individual background samples.
- Mechanism: By sampling multiple background events and averaging their attributions, the method smooths out event-specific variations while preserving the distinguishing features between signal and background.
- Core assumption: Background events share common characteristics that can be averaged meaningfully, and the signal-background distinction is robust to averaging over background samples.
- Evidence anchors:
  - [abstract]: "we find that the zero-vector baseline does not provide good feature attributions and that an averaged baseline sampled from the background events provides consistently more reasonable attributions."
  - [section]: "Averaging over the distribution of baselines can address the limitations of a single baseline... Instead of relying on a single baseline, multiple baselines are sampled from a distribution D, and integrated gradients are calculated from each baseline and averaged to give the final feature attributions."
- Break condition: If background processes are too heterogeneous or if signal events are not consistently distinguishable from all background samples, the averaging could obscure rather than clarify signal features.

### Mechanism 2
- Claim: Weighted averaging by cross-sections improves baseline quality by reflecting the true background composition at the LHC.
- Mechanism: By weighting background samples according to their production cross-sections, the baseline better represents the actual background that would be encountered in experimental data, leading to more relevant feature attributions.
- Core assumption: Cross-section weights accurately represent the relative frequency of background processes in experimental data.
- Evidence anchors:
  - [section]: "This setup enables us to examine various weighting schemes for averaging within the background processes – namely, an unbiased weighting scheme where all the background processes receive equal weight... and a weighting scheme proportional to the cross-sections of the processes"
  - [section]: "We see only a small difference in performance between the weighted and unweighted averaged background baselines, with the weighted one performing slightly better."
- Break condition: If cross-section weights are inaccurate or if the experimental conditions differ significantly from the simulated conditions, the weighted baseline may not provide better attributions.

### Mechanism 3
- Claim: The zero-vector baseline fails because it represents an unphysical state that doesn't belong to the data distribution.
- Mechanism: Since particle physics features have physical constraints and cannot take arbitrary values (including zero for some features), the zero-vector baseline introduces artifacts that don't correspond to any real physical scenario, leading to misleading attributions.
- Core assumption: Physical features have constraints that make zero values either impossible or unphysical for certain features.
- Evidence anchors:
  - [section]: "the zero vector may not belong to the data distribution, i.e., there could be features that cannot even take the value 0"
  - [section]: "the zero vector may not actually be a baseline void of any information; for instance, rotation angles in physics can take values −π to π where a value of 0 would indicate something specific of the setup"
- Break condition: If the data distribution actually includes zero values for all features, or if the model architecture somehow handles unphysical inputs gracefully.

## Foundational Learning

- Concept: Integrated Gradients (IG) method
  - Why needed here: IG provides a way to attribute model predictions to input features by integrating gradients along a path from baseline to input, which is essential for understanding what makes signal events distinguishable from background.
  - Quick check question: How does IG differ from simple gradient-based attribution methods, and why is the integration path important?

- Concept: Baseline selection in attribution methods
  - Why needed here: The choice of baseline fundamentally affects what features are highlighted as important, and in physics applications, the baseline must be physically meaningful rather than arbitrary.
  - Quick check question: What are the trade-offs between using a simple baseline (like zero vector) versus a more complex, physically-motivated baseline?

- Concept: Cross-section weighting in particle physics
  - Why needed here: Understanding how different background processes contribute to the overall background according to their production rates is crucial for creating realistic baselines that reflect experimental conditions.
  - Quick check question: How do cross-sections relate to the frequency of background processes in actual collider experiments?

## Architecture Onboarding

- Component map: Data generation -> Feature extraction -> Model training -> IG computation with baseline selection -> Feature attribution analysis -> Performance evaluation through retraining on top-k features
- Critical path: Data generation → Feature extraction → Model training → IG computation with baseline selection → Feature attribution analysis → Performance evaluation through retraining on top-k features
- Design tradeoffs: The paper balances interpretability (through IG and careful baseline selection) against model performance. Using averaged baselines provides better interpretability but requires more computation than a single baseline. The choice between weighted and unweighted averaging involves a tradeoff between physical realism and simplicity.
- Failure signatures: Poor baseline choice manifests as attributions that don't reflect physically meaningful distinctions, low discrimination ability in top-k features, or minimal improvement when retraining on top features. Model overfitting could also be indicated by large performance gaps between training and validation.
- First 3 experiments:
  1. Compare IG attributions from zero baseline vs averaged background baseline on a small subset of data to observe qualitative differences in feature importance.
  2. Test different numbers of samples in the averaged baseline (e.g., 10, 50, 200) to find the point of diminishing returns in attribution quality.
  3. Evaluate the impact of different cross-section weightings by comparing unweighted vs weighted averaged baselines on model retraining performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does baseline selection affect feature attributions in different machine learning architectures beyond fully connected DNNs?
- Basis in paper: [inferred] The paper only tested baselines on fully connected DNNs and suggests extending to other architectures like GNNs
- Why unresolved: The study focused on a specific architecture type without exploring how baseline effects might vary across different neural network structures
- What evidence would resolve it: Systematic comparison of feature attributions across multiple architectures (CNNs, GNNs, transformers) using the same baselines and datasets

### Open Question 2
- Question: What is the optimal number of background samples to use for averaging baselines without introducing excessive computational overhead?
- Basis in paper: [explicit] The authors used 200 baselines but acknowledge this warrants more in-depth study
- Why unresolved: The paper used a fixed number of 200 samples without exploring how the number of samples affects attribution quality or computational efficiency
- What evidence would resolve it: Systematic analysis varying the number of background samples and measuring both attribution stability and computational costs

### Open Question 3
- Question: How do different weighting schemes for background processes affect feature attributions when the cross-section distribution is highly imbalanced?
- Basis in paper: [explicit] The paper compared uniform vs cross-section weighting but found only small differences
- Why unresolved: The comparison was limited to these two schemes, and the authors noted that more investigation is needed
- What evidence would resolve it: Testing alternative weighting schemes (e.g., logarithmic, percentile-based) on datasets with various levels of cross-section imbalance

### Open Question 4
- Question: Can feature attributions from different baselines be combined to provide more robust and comprehensive interpretations?
- Basis in paper: [inferred] The paper shows different baselines yield different top features, suggesting potential complementarity
- Why unresolved: The study treated baselines independently without exploring how combining their attributions might improve interpretation
- What evidence would resolve it: Development and testing of methods to combine attributions from multiple baselines, measuring improvements in feature selection and model understanding

## Limitations

- Results are specific to a single particle physics classification task and may not generalize to other domains or even other particle physics problems
- The claim that weighted averaging performs "slightly better" is based on limited quantitative comparison, and the practical significance of this difference isn't fully established
- The choice of top-k features for retraining assumes that feature importance from IG directly translates to predictive power, which may not always hold

## Confidence

- **High Confidence**: The core finding that zero-vector baseline performs poorly compared to averaged background baselines is well-supported by both qualitative feature attribution analysis and quantitative performance improvements.
- **Medium Confidence**: The claim that weighted averaging performs better than unweighted averaging is supported but the difference is small and could be sensitive to specific dataset characteristics.
- **Medium Confidence**: The mechanism explanations (noise reduction through averaging, physical realism through cross-section weighting) are plausible but not exhaustively validated across different scenarios.

## Next Checks

1. **Baseline sensitivity analysis**: Systematically vary the number of background samples used for averaging (e.g., 5, 10, 20, 50, 100) and measure the stability and quality of feature attributions to identify optimal sample size.
2. **Cross-domain generalization**: Apply the same baseline selection methodology to a different interpretability problem (e.g., image classification or natural language processing) to test whether the findings about baseline selection generalize beyond particle physics.
3. **Alternative weighting schemes**: Test additional baseline weighting strategies beyond cross-sections, such as weighting by feature variance within background samples or using adaptive weighting based on feature importance distributions.