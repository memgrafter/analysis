---
ver: rpa2
title: 'REAPER: Reasoning based Retrieval Planning for Complex RAG Systems'
arxiv_id: '2407.18553'
source_url: https://arxiv.org/abs/2407.18553
tags:
- retrieval
- reaper
- data
- arxiv
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REAPER is a reasoning-based planner that generates retrieval plans
  for complex RAG-based conversational systems. It uses a fine-tuned LLM to generate
  plans that specify which tools to call, their sequence, and arguments, enabling
  efficient multi-step retrieval.
---

# REAPER: Reasoning based Retrieval Planning for Complex RAG Systems

## Quick Facts
- arXiv ID: 2407.18553
- Source URL: https://arxiv.org/abs/2407.18553
- Authors: Ashutosh Joshi; Sheikh Muhammad Sarwar; Samarth Varshney; Sreyashi Nag; Shrivats Agrawal; Juhi Naik
- Reference count: 31
- Key outcome: REAPER achieves 95% accuracy in tool selection, 92% accuracy in argument generation, and reduces latency from 2s to 207ms

## Executive Summary
REAPER is a reasoning-based planner that generates retrieval plans for complex RAG-based conversational systems. It uses a fine-tuned LLM to generate plans that specify which tools to call, their sequence, and arguments, enabling efficient multi-step retrieval. Compared to agent-based and classification-based baselines, REAPER achieves 95% accuracy in tool selection, 92% accuracy in argument generation, and reduces latency from 2s to 207ms. It is also data-efficient (trained on 6K queries vs. 150K for baselines) and easily scalable to new retrieval sources and use cases.

## Method Summary
REAPER fine-tunes Mistral-7B-Instruct-v0.2 using a combination of generic IFT data and tool-annotated queries to create a specialized retrieval planner. The approach uses data generation modules (TEvo, TTG, DQS) to create diverse training data and incorporates general IFT data to maintain instruction following ability. The model generates complete retrieval plans in a single step, specifying which tools to call, their sequence, and arguments, eliminating the need for multiple reasoning calls and reducing latency.

## Key Results
- 95% accuracy in tool selection compared to baseline systems
- 92% accuracy in argument generation for specified tools
- Latency reduction from 2 seconds to 207 milliseconds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: REAPER achieves high accuracy in tool selection and argument generation while reducing latency significantly.
- Mechanism: REAPER uses a single fine-tuned LLM to generate the entire retrieval plan in one step, eliminating the need for multiple reasoning calls and reducing latency.
- Core assumption: A smaller LLM fine-tuned for retrieval planning can match or exceed the performance of larger models while maintaining lower latency.
- Evidence anchors:
  - [abstract] "REAPER achieves 95% accuracy in tool selection, 92% accuracy in argument generation, and reduces latency from 2s to 207ms."
  - [section 5.2] "REAPER does both tool selection and argument generation for all the tools in a single prompt."

### Mechanism 2
- Claim: REAPER is data-efficient and easily scalable to new use cases and retrieval sources.
- Mechanism: REAPER uses a novel data generation approach (TEvo, TTG, DQS) to create diverse training data and incorporates general IFT data to maintain instruction following ability.
- Core assumption: The data generation modules can create sufficient diversity in training data to generalize to new use cases without requiring large amounts of labeled data.
- Evidence anchors:
  - [abstract] "It is also data-efficient (trained on 6K queries vs. 150K for baselines)"
  - [section 5.4] "A practical use-case of REAPER is adapting to a new retrieval source with limited amount of data. We tested this, by adding a new no-evidence class called human_small_talk. We added only 286 examples of this class to our training data and saw that the model achieves an F1 score of 0.92"

### Mechanism 3
- Claim: REAPER maintains instruction following ability while learning the specialized task of retrieval planning.
- Mechanism: REAPER includes a mix of general IFT data and REAPER-specific tasks in its training set to balance domain-specific learning with instruction following.
- Core assumption: The inclusion of general IFT data prevents catastrophic forgetting and allows the model to adapt to changes in tools or instructions.
- Evidence anchors:
  - [section 5.3] "Without the generic data the model severely overfits to the REAPER use case and catastrophically forgets its instruction following capability."
  - [section 5.3] "With only generic-IFT training data, the model achieves only 20% accuracy for the in-domain task of tool selection."

## Foundational Learning

- Concept: Instruction Following
  - Why needed here: REAPER needs to understand and execute complex instructions for retrieval planning while maintaining the ability to adapt to new tools and use cases.
  - Quick check question: Can the model follow instructions to use only the specified tools in the prompt and generate the correct retrieval plan format?

- Concept: Tool Understanding and Usage
  - Why needed here: REAPER must understand the available tools, their signatures, and how to use them to generate effective retrieval plans.
  - Quick check question: Can the model correctly identify which tools to use for a given query and generate the appropriate arguments for each tool?

- Concept: Chain-of-Thought Reasoning
  - Why needed here: REAPER needs to generate multi-step retrieval plans that involve reasoning over multiple tools and their outputs.
  - Quick check question: Can the model generate a valid sequence of tool calls for complex queries that require multi-step retrieval?

## Architecture Onboarding

- Component map: Query -> REAPER LLM -> Retrieval Plan -> Tool Execution -> Evidence -> Response Generation
- Critical path:
  1. Receive customer query and context
  2. Generate retrieval plan using REAPER LLM
  3. Execute the plan by calling the specified tools in sequence
  4. Pass the retrieved evidence to the response generation LLM

- Design tradeoffs:
  - Model size vs. latency: Smaller models have lower latency but may struggle with complex reasoning
  - Training data amount vs. generalization: More data can improve performance but reduces data efficiency
  - Instruction following vs. domain-specific learning: Balancing general IFT with REAPER-specific tasks

- Failure signatures:
  - Hallucinations: Model generates tools or arguments not in the instruction set
  - Inefficiency: Model fails to generate optimal retrieval plans, leading to unnecessary tool calls
  - Overfitting: Model performs well on training data but fails to generalize to new use cases

- First 3 experiments:
  1. Test REAPER on a set of simple queries to verify basic tool selection and argument generation
  2. Evaluate REAPER on multi-step retrieval queries to check complex plan generation
  3. Add a new tool to the instruction set and test if REAPER can generate plans using it without additional training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does REAPER's performance scale when new retrieval sources are added beyond the initial six classes tested?
- Basis in paper: [explicit] The paper mentions REAPER's ability to scale to new retrieval sources with minimal training data and provides an example of adding a new "human_small_talk" class with only 286 examples achieving F1 score of 0.92.
- Why unresolved: The paper only tests one new class addition. It's unclear how REAPER would perform with multiple new sources simultaneously or more complex multi-step retrieval involving combinations of new and existing sources.
- What evidence would resolve it: Systematic testing of REAPER's performance when incrementally adding multiple new retrieval sources, measuring accuracy, latency, and instruction-following capabilities as the tool ecosystem grows.

### Open Question 2
- Question: What is the impact of REAPER's training data composition on its ability to handle ambiguous or out-of-distribution queries?
- Basis in paper: [explicit] The paper discusses the importance of diverse training data components (TEvo, TTG, DQS) and shows that removing DQS leads to a marked drop in accuracy, suggesting query diversity is important for handling different query shapes.
- Why unresolved: The paper doesn't test REAPER's performance on truly ambiguous queries or those that don't fit existing patterns. It's unclear how well REAPER generalizes to completely novel query types.
- What evidence would resolve it: Evaluation of REAPER on deliberately ambiguous queries, queries from domains outside the training data, and queries that require combining multiple unfamiliar tools.

### Open Question 3
- Question: How does REAPER's latency and accuracy compare to agent-based systems when handling truly complex multi-hop queries?
- Basis in paper: [explicit] The paper states REAPER reduces latency from 2s to 207ms compared to agent-based systems, but this comparison is based on simpler queries. It mentions that complex queries can require multi-step retrieval but doesn't provide detailed performance metrics for such cases.
- Why unresolved: The paper doesn't provide specific metrics for REAPER's performance on queries requiring multiple retrieval steps or complex reasoning chains, which are the scenarios where agent-based systems are typically most challenged.
- What evidence would resolve it: Head-to-head comparison of REAPER and agent-based systems on a benchmark of complex multi-hop queries, measuring both accuracy and latency for each system across varying levels of query complexity.

## Limitations

- Limited external validation of claimed accuracy metrics and latency improvements
- Uncertainty about generalization to significantly different use cases beyond demonstrated examples
- Trade-off between instruction following and domain performance may vary by use case

## Confidence

**High confidence**: The mechanism of using a single fine-tuned LLM to generate complete retrieval plans in one step is well-supported by the latency measurements and tool selection accuracy results.

**Medium confidence**: The data generation approach using TEvo, TTG, and DQS modules appears promising but implementation details are not fully described.

**Low confidence**: The claim that REAPER is "data-efficient" compared to baselines is difficult to evaluate without detailed information about baseline training processes and data quality.

## Next Checks

1. **Independent benchmark evaluation**: Test REAPER on a public RAG dataset or create an open benchmark using standardized customer service queries to verify the claimed accuracy metrics and latency improvements under different hardware configurations.

2. **Cross-domain generalization test**: Implement REAPER for a substantially different domain (e.g., medical Q&A or legal document retrieval) with minimal retraining to assess whether the data efficiency claims hold for major domain shifts.

3. **Ablation study on training data composition**: Systematically vary the ratio of generic IFT data to REAPER-specific data to identify the optimal balance for different use cases and determine whether the claimed 6K query training set is truly sufficient across diverse scenarios.