---
ver: rpa2
title: Near-Optimal Policy Identification in Robust Constrained Markov Decision Processes
  via Epigraph Form
arxiv_id: '2408.16286'
source_url: https://arxiv.org/abs/2408.16286
tags:
- policy
- where
- lemma
- equation
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of identifying near-optimal policies
  in robust constrained Markov decision processes (RCMDPs), where policies must minimize
  worst-case costs while satisfying constraints. The authors demonstrate that the
  conventional Lagrangian formulation with policy gradients can get stuck in suboptimal
  solutions due to conflicting gradients from the objective and constraint functions.
---

# Near-Optimal Policy Identification in Robust Constrained Markov Decision Processes via Epigraph Form

## Quick Facts
- arXiv ID: 2408.16286
- Source URL: https://arxiv.org/abs/2408.16286
- Authors: Toshinori Kitamura; Tadashi Kozuno; Wataru Kumagai; Kenta Hoshino; Yohei Hosoe; Kazumi Kasaura; Masashi Hamaya; Paavo Parmas; Yutaka Matsuo
- Reference count: 40
- Key outcome: Introduces EpiRC-PGS algorithm that finds ε-optimal policies in RCMDPs with Õ(ε⁻⁴) robust policy evaluations, outperforming Lagrangian methods

## Executive Summary
This paper addresses the challenge of identifying near-optimal policies in robust constrained Markov decision processes (RCMDPs), where policies must minimize worst-case costs while satisfying constraints. The authors demonstrate that conventional Lagrangian formulations with policy gradients can get stuck in suboptimal solutions due to conflicting gradients from objective and constraint functions. To resolve this, they propose leveraging the epigraph form of RCMDPs, which eliminates gradient conflicts by selecting between objective and constraint gradients rather than summing them. Building on this, they introduce EpiRC-PGS, a bisection search algorithm with a policy gradient subroutine, and prove it identifies an ε-optimal policy with Õ(ε⁻⁴) robust policy evaluations. Experiments show EpiRC-PGS outperforms Lagrangian methods in various RCMDP settings, successfully identifying feasible, low-cost policies where Lagrangian approaches fail.

## Method Summary
The method introduces EpiRC-PGS, a two-level algorithm for RCMDPs. The outer loop performs bisection search over a threshold variable y to find the minimal feasible objective value. For each candidate threshold, the inner loop uses policy gradient methods to solve an auxiliary problem derived from the epigraph formulation. This auxiliary problem takes the form minπ max{f(π) - y, h(π)}, where the max operator selects between objective and constraint gradients rather than summing them, eliminating gradient conflicts. The algorithm requires robust policy evaluators to compute worst-case costs and subgradients for the uncertainty set, and uses projected policy gradient updates within the inner loop. Theoretical analysis proves convergence to ε-optimal policies with Õ(ε⁻⁴) iterations.

## Key Results
- EpiRC-PGS identifies ε-optimal policies in RCMDPs with Õ(ε⁻⁴) robust policy evaluations
- The algorithm successfully finds feasible, low-cost policies in environments where Lagrangian methods get stuck in suboptimal local minima
- Theoretical analysis proves convergence guarantees for tabular RCMDPs with various uncertainty set structures
- Empirical results demonstrate superior performance compared to Lagrangian approaches across multiple RCMDP settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Policy gradient methods in the Lagrangian formulation can get stuck in local minima due to conflicting gradients from objective and constraint functions.
- Mechanism: When optimizing the Lagrangian Lλ(π) = Jc0,U(π) + Σλn(Jcn,U(π) - bn), the policy gradient update tries to minimize this sum. However, the gradients ∇Jc0,P0(π) and ∇Jcn,Pn(π) for different environments P0 ≠ Pn can point in opposing directions. This causes their sum ∇Lλ(π) to cancel out, creating stationary points that are not optimal solutions to the original RCMDP problem.
- Core assumption: The worst-case environments for the objective and constraints can be different, leading to conflicting gradient directions.
- Evidence anchors:
  - [abstract]: "conventional Lagrangian formulation with policy gradients can get stuck in suboptimal solutions due to conflicting gradients from the objective and constraint functions"
  - [section]: "Theorem 1 reveals that policy gradient methods can get trapped in a local minimum during the inner minimization of the Lagrangian formulation. This occurs when the gradients, ∇f(π) and ∇h(π), conflict with each other"
- Break condition: This mechanism fails if the worst-case environments for all costs happen to align (rare in practice) or if the policy gradient can escape through stochasticity or momentum-based methods.

### Mechanism 2
- Claim: The epigraph form transforms the RCMDP into a form where policy gradients select between objective and constraint gradients rather than summing them.
- Mechanism: By introducing the auxiliary problem minπ max{f(π) - y, h(π)}, the epigraph formulation creates a max operator that selects either the objective gradient or the constraint gradient depending on which constraint is most violated. This eliminates the gradient conflict since only one gradient direction is followed at each update.
- Core assumption: The max operator in the auxiliary problem allows selective gradient application rather than additive combination.
- Evidence anchors:
  - [abstract]: "we leverage the epigraph form of the RCMDP problem, which resolves the conflict by selecting a single gradient from either the objective or the constraints"
  - [section]: "policy gradient methods for the auxiliary problem update the policy by selecting either ∇f(π) or ∇h(π), thanks to the maximum operator in the problem"
- Break condition: This mechanism fails if the max operator cannot be properly computed due to approximation errors or if the problem structure changes such that the selection becomes ambiguous.

### Mechanism 3
- Claim: The bisection search over the threshold variable y finds the minimal feasible objective value while the policy gradient subroutine solves the auxiliary problem efficiently.
- Mechanism: The algorithm first finds b0 = J* (the optimal objective value) by searching over possible thresholds. For each candidate threshold, it uses policy gradients to find if a feasible policy exists. The outer loop converges to the optimal value while the inner loop handles the constrained optimization without gradient conflicts.
- Core assumption: The function ∆*b0 (maximum constraint violation) is monotonic in b0, allowing bisection search to work.
- Evidence anchors:
  - [abstract]: "we propose a bisection search algorithm with a policy gradient subroutine, and prove that it identifies an ε-optimal policy in an RCMDP with Õ(ε⁻⁴) robust policy evaluations"
  - [section]: "Lemma 2. ∆*b0 is monotonically decreasing in b0 and ∆*J* = 0"
- Break condition: This mechanism fails if the monotonicity assumption is violated due to approximation errors or if the policy gradient subroutine cannot find feasible policies reliably.

## Foundational Learning

- Concept: Constrained Optimization and Lagrange Multipliers
  - Why needed here: The paper builds on the Lagrangian approach to RCMDPs but identifies its limitations, requiring understanding of how Lagrange multipliers work in constrained optimization.
  - Quick check question: Why does the standard Lagrangian approach with policy gradients fail in RCMDPs while working in regular CMDPs?

- Concept: Markov Decision Processes and Robustness
  - Why needed here: The paper deals with RCMDPs which extend MDPs with uncertainty sets and constraints, requiring understanding of both MDP fundamentals and robust optimization concepts.
  - Quick check question: What is the key difference between a standard MDP, a CMDP, and an RCMDP in terms of their optimization objectives?

- Concept: Policy Gradient Methods and Subgradients
  - Why needed here: The algorithm uses policy gradient methods to solve the auxiliary problem, and understanding subgradients is crucial for the convergence analysis.
  - Quick check question: How does the policy gradient theorem relate occupancy measures to action-value functions, and why is this relationship important for the algorithm?

## Architecture Onboarding

- Component map: Robust policy evaluator -> Subgradient evaluator -> Policy gradient subroutine -> Bisection search controller -> Epigraph formulation converter
- Critical path: Bisection search selects threshold → Policy gradient subroutine computes policy for threshold → Robust evaluators compute worst-case costs and gradients → Policy update occurs → Threshold adjusted based on feasibility → Repeat until convergence
- Design tradeoffs: The double-loop structure provides theoretical guarantees but may be slower than single-loop alternatives. The use of epigraph form eliminates gradient conflicts but requires careful implementation of the max operator and subgradient computations.
- Failure signatures: Algorithm gets stuck at local minima (indicates gradient conflict issues), bisection search fails to converge (indicates monotonicity violation or approximation errors), or excessive iterations without improvement (indicates poor policy gradient performance or uncertainty set complexity).
- First 3 experiments:
  1. Test on a simple RCMDP with two environments where objective and constraint worst-cases are clearly different to observe gradient conflict behavior.
  2. Compare performance of Lagrangian vs. epigraph approaches on small tabular RCMDPs with varying uncertainty set sizes.
  3. Evaluate the impact of approximation errors in robust evaluators on algorithm convergence by systematically varying evaluation accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the eO(ε⁻⁴) iteration complexity of EpiRC-PGS be improved to match the eO(ε⁻²) complexity of natural policy gradient methods for robust MDPs?
- Basis in paper: [explicit] The authors state "Another research avenue is improving the iteration complexity of our eO(ε⁻⁴). This may not be tight, since for RMDPs with (s, a)-rectangularity, the natural policy gradient method is ensured to find an ε-optimal policy with eO(ε⁻²) iterations Li et al. (2022)."
- Why unresolved: The paper does not provide any analysis or evidence that would suggest whether the eO(ε⁻⁴) bound is tight for the epigraph formulation, or what structural properties of the problem might allow for faster convergence.
- What evidence would resolve it: Either a theoretical proof that eO(ε⁻⁴) is tight for EpiRC-PGS, or an improved algorithm with better iteration complexity, or a counterexample showing that faster convergence is impossible under certain assumptions.

### Open Question 2
- Question: Can EpiRC-PGS be extended to a single-loop algorithm to avoid the computational inefficiency of the double-loop structure?
- Basis in paper: [explicit] The authors state "A double-loop algorithm like EpiRC-PGS is often inefficient when the inner problem requires high computational cost (Lin et al., 2024). Developing a single-loop algorithm is a promising direction for future research, and we discuss the challenges in Appendix B."
- Why unresolved: The paper identifies that the epigraph-Lagrange formulation (Equation 17) may not satisfy strong duality due to quasiconvexity concerns, which prevents straightforward adaptation of primal-dual methods.
- What evidence would resolve it: Either a proof that strong duality holds for the epigraph-Lagrange formulation under certain conditions, or an alternative single-loop algorithm that circumvents this issue, or a demonstration that no single-loop algorithm can achieve the same guarantees as EpiRC-PGS.

### Open Question 3
- Question: Can the coverage assumption on the initial distribution (Assumption 5) be removed from EpiRC-PGS while maintaining convergence guarantees?
- Basis in paper: [explicit] The authors state "Finally, the coverage assumption on the initial distribution (Assumption 5) is not necessary in CMDPs (Ding et al., 2024). We leave the removal of Assumption 5 in RCMDPs for future work."
- Why unresolved: The coverage assumption is used in the proof of Theorem 4 to establish that stationary points are optimal, but the paper does not explore whether alternative techniques could relax this requirement.
- What evidence would resolve it: Either a modified proof of Theorem 4 that removes the need for Assumption 5, or a counterexample showing that convergence fails without this assumption, or an alternative algorithm that achieves convergence without requiring coverage.

## Limitations
- The algorithm's performance depends critically on accurate robust policy evaluation and subgradient computation, which may be computationally expensive for large uncertainty sets
- The analysis focuses on tabular settings and doesn't directly extend to function approximation scenarios common in deep RL applications
- The assumptions about Lipschitz continuity and the form of the uncertainty set restrict applicability to certain problem classes

## Confidence
- High: The mechanism for gradient conflicts in Lagrangian formulations is well-established
- High: The epigraph reformulation is mathematically sound
- High: The bisection search approach follows from established optimization principles
- Medium: The empirical results are limited to specific RCMDP instances rather than comprehensive benchmarks

## Next Checks
1. Test algorithm sensitivity to uncertainty set size and structure by varying the number of environments or the KL divergence bounds, measuring how performance degrades as robustness requirements increase.
2. Implement a direct comparison between the epigraph approach and a Lagrangian method with momentum-based optimization (e.g., Adam) to determine if standard techniques can mitigate gradient conflicts.
3. Evaluate the algorithm on a non-tabular RCMDP with continuous state spaces using function approximation, assessing whether the theoretical guarantees extend to this more practical setting.