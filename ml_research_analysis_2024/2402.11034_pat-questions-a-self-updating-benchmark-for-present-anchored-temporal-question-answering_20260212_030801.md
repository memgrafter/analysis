---
ver: rpa2
title: 'PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering'
arxiv_id: '2402.11034'
source_url: https://arxiv.org/abs/2402.11034
tags:
- team
- current
- head
- pat-questions
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PAT-Questions, a novel benchmark for present-anchored
  temporal question answering (PATQA). Unlike traditional temporal QA focused on fixed
  timestamps, PATQA addresses questions relative to the present time (e.g., "Who is
  the current president?").
---

# PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering

## Quick Facts
- arXiv ID: 2402.11034
- Source URL: https://arxiv.org/abs/2402.11034
- Authors: Jannat Ara Meem; Muhammad Shihab Rashid; Yue Dong; Vagelis Hristidis
- Reference count: 12
- One-line primary result: State-of-the-art LLMs achieve only 1.5%-15.5% exact match accuracy on PAT-Questions, highlighting significant challenges in present-anchored temporal QA

## Executive Summary
This paper introduces PAT-Questions, a novel benchmark for present-anchored temporal question answering (PATQA) that addresses questions relative to the current time rather than fixed timestamps. The benchmark is constructed using Wikidata and generates both single-hop and multi-hop questions requiring complex temporal reasoning. A key innovation is automatic answer updating via SPARQL queries, ensuring the benchmark remains relevant over time. Experiments with state-of-the-art LLMs (Falcon-7B, Flan-T5-XL, Llama-2-7B, Mistral-7B, GPT-3.5) and a temporal reasoning model (TEMPREASON-T5) show poor performance, with exact match accuracy ranging from 1.5% to 15.5%. Retrieval-augmented generation (RAG) improves results for single-hop questions but struggles with multi-hop ones.

## Method Summary
PAT-Questions is constructed by adapting TEMPREASON templates to create present-anchored questions using "currently" or "before the current" instead of fixed timestamps. Each question is associated with a SPARQL query that retrieves answers from Wikidata, enabling automatic updates. The benchmark contains 6,172 questions covering two timestamps (Dec 2021 and Dec 2023). Evaluation involves direct prompting of LLMs and RAG using BM25-ranked Wikipedia chunks retrieved via Google Custom Search. Performance is measured using exact match (EM) accuracy and token-level F1 score.

## Key Results
- State-of-the-art LLMs achieve only 1.5%-15.5% exact match accuracy on PAT-Questions
- RAG improves performance for single-hop questions but shows limited benefit for multi-hop questions
- TEMPREASON-T5, a specialized temporal reasoning model, achieves only 6.5% EM accuracy
- The benchmark successfully demonstrates the difficulty of present-anchored temporal QA compared to fixed-timestamp QA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automatic SPARQL query regeneration keeps answers current
- Mechanism: Each PAT-Questions entry has an associated SPARQL query that retrieves the latest answer from Wikidata; re-running the query returns the current answer
- Core assumption: Wikidata contains up-to-date facts for the queried relations
- Evidence anchors:
  - [abstract] "The answers in PAT-Questions can be automatically refreshed by re-running SPARQL queries on a knowledge graph, if available."
  - [section] "Thus, the SPARQL template associated with each question consistently retrieves the latest answer with respect to knowledge available/updated in Wikidata."
- Break condition: Wikidata updates lag behind real-world changes or SPARQL query is malformed

### Mechanism 2
- Claim: Template-based question generation ensures consistent temporal anchoring
- Mechanism: TEMPREASON templates are modified to replace fixed timestamps with "currently" or "before the current," producing present-anchored questions
- Core assumption: The original TEMPREASON dataset's time-dependent facts are valid for current-context conversion
- Evidence anchors:
  - [section] "We adapt two types of TEMPREASON templates ... to build PAT-Questions templates, and following the steps shown in the figure, we create a set of one-hop and multi-hop PAT-Questions"
  - [corpus] Weak evidence for temporal reasoning capability; neighbors discuss spatio-temporal QA but not present-anchored
- Break condition: Incorrect template conversion or missing temporal facts for subjects

### Mechanism 3
- Claim: Retrieval-augmented generation (RAG) mitigates LLM knowledge cutoffs
- Mechanism: Google Custom Search retrieves up-to-date Wikipedia chunks; LLMs use these as context to answer PAT-Questions
- Core assumption: GCS can retrieve relevant documents containing current facts
- Evidence anchors:
  - [section] "We retrieve up to five Wikipedia documents for each question using Google Custom Search (GCS), divide each document into chunks of 300 tokens, rank the relevance of these chunks using BM25 and finally assign the top 5 chunks as the retrieved evidence for a question"
  - [section] "Accuracy improves with document retrieval, especially for single-hop questions, due to the retrieval of up-to-date and relevant documents"
- Break condition: GCS retrieval fails to find relevant current documents; BM25 ranking mismatches query intent

## Foundational Learning

- Concept: SPARQL query structure and Wikidata schema
  - Why needed here: PAT-Questions relies on SPARQL queries to automatically update answers; understanding the schema is critical for debugging query failures
  - Quick check question: How do you modify a SPARQL query to retrieve the current value of a Wikidata property versus the previous value?

- Concept: Temporal reasoning with "before" and "currently"
  - Why needed here: PAT-Questions questions involve complex temporal relations; distinguishing between "current" and "previous" is key to correct answers
  - Quick check question: What is the difference between a question asking "Who is the current head coach?" and "Who was the previous head coach?" in terms of required reasoning steps?

- Concept: Retrieval-augmented generation pipeline
  - Why needed here: RAG experiments are central to evaluating PAT-Questions; understanding the pipeline helps troubleshoot retrieval and generation issues
  - Quick check question: What happens if the top BM25-ranked chunks retrieved by GCS do not contain the answer to a PAT-Questions question?

## Architecture Onboarding

- Component map:
  - Question templates → SPARQL query templates → Wikidata API → Answer annotation
  - GCS → BM25 ranking → Document chunking → RAG context → LLM prompt
  - LLM inference → EM/F1 evaluation → Error analysis

- Critical path:
  Dataset creation → SPARQL query association → Automatic update cronjob → RAG retrieval pipeline → LLM evaluation → Benchmark publication

- Design tradeoffs:
  - SPARQL vs. manual updates: SPARQL is automatic but depends on Wikidata freshness
  - GCS vs. Wikipedia dump: GCS is current but less controlled; dump is stable but may be outdated
  - Open-source vs. closed-source LLMs: Open-source are cheaper but less capable; closed-source are more capable but expensive

- Failure signatures:
  - Outdated answers → Wikidata lag or SPARQL error
  - Low EM/F1 → Retrieval failure or LLM inability to reason temporally
  - No answer from RAG → GCS returned irrelevant documents or BM25 mismatch

- First 3 experiments:
  1. Verify SPARQL queries return correct answers for a sample of questions at two different timestamps
  2. Test GCS retrieval relevance by checking if retrieved chunks contain the gold answer for a sample of questions
  3. Compare LLM performance on single-hop vs. multi-hop questions to confirm difficulty scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM performance patterns differ between single-hop and multi-hop PATQA questions when evaluated across different knowledge cutoff dates?
- Basis in paper: [explicit] The paper explicitly compares LLM performance on single-hop vs multi-hop PATQA questions and across different timestamps (Dec 2021 vs Dec 2023)
- Why unresolved: The paper presents performance metrics but doesn't deeply analyze the differential patterns between single-hop and multi-hop questions or how these patterns change with different knowledge cutoffs
- What evidence would resolve it: Detailed comparative analysis showing how accuracy, recall, and error types vary between single-hop and multi-hop questions across different knowledge cutoff dates

### Open Question 2
- Question: What is the relationship between SPARQL query accuracy and LLM performance in PATQA tasks?
- Basis in paper: [inferred] The paper mentions that SPARQL queries can automatically update answers and that retrieval quality affects LLM performance
- Why unresolved: The paper doesn't investigate how the accuracy of SPARQL queries correlates with LLM performance or whether certain types of SPARQL queries lead to better LLM responses
- What evidence would resolve it: Correlation analysis between SPARQL query accuracy and LLM performance metrics, along with analysis of which query patterns lead to optimal LLM results

### Open Question 3
- Question: How do different temporal reasoning strategies (e.g., temporal spans, temporal relations) impact LLM performance in PATQA?
- Basis in paper: [explicit] The paper mentions that PATQA requires "complex temporal relationships (e.g. before, previous)" and evaluates TEMPREASON-T5 which uses temporal span extraction
- Why unresolved: While the paper evaluates TEMPREASON-T5, it doesn't systematically compare different temporal reasoning strategies or their effectiveness in PATQA tasks
- What evidence would resolve it: Comparative analysis of different temporal reasoning approaches and their impact on PATQA performance metrics

### Open Question 4
- Question: What is the optimal balance between retrieval quality and model size for PATQA tasks?
- Basis in paper: [inferred] The paper shows that RAG improves performance but notes that "the success of the RAG approach largely depends on the retrieval engine's efficiency"
- Why unresolved: The paper doesn't explore how different retrieval quality levels interact with different model sizes or whether there's an optimal balance between these factors
- What evidence would resolve it: Systematic experiments varying both retrieval quality and model size to identify optimal combinations for PATQA performance

## Limitations

- The benchmark's effectiveness depends heavily on Wikidata's completeness and update frequency
- Automatic SPARQL-based answer updating assumes perfect query generation and Wikidata schema alignment
- Evaluation focuses on English-language questions and mainstream LLMs, limiting generalizability

## Confidence

- **High Confidence**: The experimental results showing poor LLM performance on PAT-Questions (1.5%-15.5% EM accuracy) are well-documented and reproducible given the dataset and evaluation methodology described
- **Medium Confidence**: The claim that PAT-Questions represents a significant advance over existing temporal QA benchmarks is reasonable but depends on the assumption that present-anchored questions are meaningfully harder than fixed-timestamp questions
- **Low Confidence**: The assertion that PAT-Questions will remain relevant as a long-term benchmark is uncertain, as it depends on Wikidata's continued maintenance and the evolution of temporal reasoning capabilities in language models

## Next Checks

1. Verify SPARQL Query Robustness: Test a random sample of SPARQL queries against current Wikidata to confirm they return correct, up-to-date answers across multiple question types and verify query templates handle edge cases

2. Evaluate Cross-Lingual Generalization: Assess whether the PAT-Questions framework can be adapted for non-English questions by testing with multilingual LLMs and Wikidata's language-specific properties

3. Test Temporal Reasoning Transfer: Examine whether models trained on PAT-Questions show improved performance on other temporal reasoning tasks (e.g., existing TEMPREASON or TORQUE benchmarks) to validate the benchmark's effectiveness for developing generalizable temporal reasoning capabilities