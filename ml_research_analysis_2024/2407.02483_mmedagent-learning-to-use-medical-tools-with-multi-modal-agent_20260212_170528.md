---
ver: rpa2
title: 'MMedAgent: Learning to Use Medical Tools with Multi-modal Agent'
arxiv_id: '2407.02483'
source_url: https://arxiv.org/abs/2407.02483
tags:
- medical
- mmedagent
- arxiv
- tools
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MMedAgent is the first multi-modal medical AI agent designed to
  address the limitations of general-purpose models in the medical domain by integrating
  specialized tools for various medical tasks. The agent uses a fine-tuned multi-modal
  LLM as a planner to select appropriate tools from a suite of six, covering tasks
  like grounding, segmentation, classification, report generation, and retrieval-augmented
  generation across five imaging modalities.
---

# MMedAgent: Learning to Use Medical Tools with Multi-modal Agent

## Quick Facts
- arXiv ID: 2407.02483
- Source URL: https://arxiv.org/abs/2407.02483
- Authors: Binxu Li; Tiankai Yan; Yuanting Pan; Jie Luo; Ruiyang Ji; Jiayuan Ding; Zhe Xu; Shilong Liu; Haoyu Dong; Zihao Lin; Yixin Wang
- Reference count: 18
- Key outcome: First multi-modal medical AI agent achieving 109.48 relative score, outperforming open-source baselines and GPT-4o on medical tasks

## Executive Summary
MMedAgent addresses the limitations of general-purpose multi-modal models in medical applications by integrating specialized tools into a fine-tuned LLM planner. The system combines LLaVA-Med with six task-specific medical tools covering grounding, segmentation, classification, report generation, and retrieval-augmented generation across five imaging modalities. Through instruction tuning on a 48K-example dataset, MMedAgent learns to dynamically select and execute appropriate tools based on input context, achieving superior performance compared to both open-source baselines and the closed-source GPT-4o model.

## Method Summary
The MMedAgent architecture fine-tunes LLaVA-Med with LoRA (rank 128) for 15 epochs using AdamW optimizer (lr=2e-4, cosine schedule) on an instruction-tuning dataset comprising 48K examples. The dataset was generated using GPT-4o and covers six specialized medical tools solving seven tasks across five modalities. The fine-tuned model acts as a planner that outputs structured action commands (Thoughts, Actions, Value) to invoke appropriate tools, which are then executed and aggregated into final responses. The system was evaluated on multiple medical benchmarks including WORD, FLARE, VinDr-CXR, BRATS, and Cellseg datasets.

## Key Results
- Achieves relative score of 109.48, outperforming open-source baselines and GPT-4o
- Demonstrates 100% tool selection accuracy on test dataset
- Successfully integrates new tools without degrading performance on existing tasks
- Shows superior performance across multiple medical imaging modalities including CT, MRI, X-ray, and pathology

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a generalist model with tool-specific instruction data enables dynamic selection and execution of specialized tools. The multi-modal LLM is trained via instruction tuning to output structured action commands (Thoughts, Actions, Value) that call appropriate medical tools based on input context. This transforms the model from a passive generator into an autonomous planner that orchestrates specialized submodels. Core assumption: Structured tool invocation format can be learned from natural language examples and reliably executed. Evidence: Instruction-tuning dataset with six medical tools solving seven tasks across five modalities. Break condition: Tool selection accuracy below ~90% degrades overall performance.

### Mechanism 2
Extending a generalist backbone with task-specific fine-tuned tools yields better performance than either pure generalists or single-task specialists alone. MMedAgent leverages LLaVA-Med as a generalist foundation and augments it with fine-tuned tools that provide expert-level performance on narrow tasks. The instruction-tuned planner integrates these outputs into coherent responses. Core assumption: Combining a broad-coverage backbone with narrow specialists outperforms both pure generalists and collections of specialists. Evidence: Superior performance across variety of medical tasks compared to state-of-the-art open-source methods and GPT-4o. Break condition: Tools becoming outdated or misaligned with backbone representations degrades integration quality.

### Mechanism 3
The instruction-tuning approach enables efficient addition of new tools without retraining from scratch. By fine-tuning only the planner component (using LoRA) on new tool-instruction data, MMedAgent can incorporate new capabilities while retaining old ones. This modular update pattern avoids catastrophic forgetting. Core assumption: The planner's learned mapping from instructions to tools is additive and does not interfere with previously learned mappings when updated incrementally. Evidence: Exhibits efficiency in updating and integrating new medical tools; adding Pseudo Tool with 5K new examples achieved 100% accuracy without damaging old tool selection. Break condition: New tools requiring fundamentally different input/output formats may need architectural changes.

## Foundational Learning

- Concept: Tool selection accuracy and its impact on task performance.
  - Why needed here: MMedAgent's success hinges on correctly choosing the right tool for each task; even a single wrong choice can derail the entire response.
  - Quick check question: If MMedAgent has 95% tool selection accuracy, what is the probability it makes at least one error in a 5-tool task chain?

- Concept: Multi-modal data alignment between visual and language inputs.
  - Why needed here: The agent must fuse image and text embeddings across different medical modalities (MRI, CT, X-ray) to select appropriate tools and interpret their outputs.
  - Quick check question: How does the agent handle a CT scan of the abdomen when the instruction mentions "kidney tumor" but the image also contains liver and spleen?

- Concept: Instruction tuning format and its role in zero-shot generalization.
  - Why needed here: The unified dialogue format (Thoughts-Actions-Value) is critical for the agent to parse instructions, decide on tools, and generate final answers without explicit task-specific prompts.
  - Quick check question: What happens if the instruction contains ambiguous terms like "segment this" without specifying the target object?

## Architecture Onboarding

- Component map: User Interface (FastChat Web UI) → Controller → MMedAgent Model Worker (LLaVA-Med + LoRA) ↔ Tool Workers (Grounding DINO, MedSAM, BiomedCLIP, ChatCAD, ChatCAD+, VQA backbone)

- Critical path: 1. User submits instruction + medical image 2. MMedAgent parses input and generates structured action commands 3. Controller routes commands to appropriate tool workers 4. Tools execute and return results 5. MMedAgent aggregates results and generates final answer 6. Response returned to user

- Design tradeoffs: Modularity vs. latency (each tool call adds network overhead); Fine-tuning granularity (LoRA allows cheap updates but may limit capacity); Tool specificity (highly specialized tools improve accuracy but require more maintenance).

- Failure signatures: Tool selection failures (MMedAgent outputs incorrect or no Actions); Tool execution errors (tools crash or return malformed results); Aggregation issues (final answer ignores tool outputs).

- First 3 experiments: 1. Verify tool selection accuracy on held-out instruction dataset 2. Benchmark end-to-end latency for typical multi-tool query chain 3. Test incremental tool addition by fine-tuning on small synthetic dataset and measuring retention on original tasks

## Open Questions the Paper Calls Out

### Open Question 1
Can MMedAgent's architecture be extended to incorporate continuous learning mechanisms for tool updates without full retraining? The paper demonstrates successful integration of new tools through instruction tuning with small datasets, suggesting potential for more advanced continuous learning capabilities. This remains unresolved as the paper doesn't explore whether the system can continuously adapt to tool updates or degradation in performance over time without complete retraining cycles. Evidence would include experimental results showing performance stability when tools are incrementally updated or replaced over extended periods.

### Open Question 2
How does MMedAgent's performance scale when integrating more than six specialized tools across additional medical imaging modalities? The paper acknowledges its limitation to seven tasks across five modalities and suggests more specialized tools should be included in the future. This remains unresolved as the current evaluation only tests six tools without empirical data on how tool selection accuracy, response quality, or computational efficiency change as the tool library expands. Evidence would include systematic scaling experiments adding progressively more tools and measuring tool selection accuracy, average response time, and performance on representative tasks.

### Open Question 3
What are the limitations of MMedAgent's tool selection mechanism when dealing with ambiguous or multi-faceted medical queries? The paper demonstrates 100% tool selection accuracy on its test dataset but doesn't explore scenarios where user queries might require multiple tools or where tool outputs conflict. This remains unresolved as the evaluation dataset appears to contain clear, single-tool queries, while real-world medical queries often involve complex scenarios requiring multiple tools or handling contradictory outputs. Evidence would include user studies with ambiguous medical queries and analysis of MMedAgent's ability to correctly identify multiple required tools and resolve conflicting outputs.

## Limitations
- Performance comparison with GPT-4o may be biased due to evaluation setup favoring tool-based approach
- 100% tool selection accuracy likely specific to controlled evaluation conditions
- Generalizability to diverse clinical scenarios and ambiguous medical queries remains unproven

## Confidence
- MMedAgent's superior performance vs. open-source baselines: Medium
- Perfect tool selection accuracy: Low-Medium (likely specific to controlled evaluation)
- Generalizability to clinical practice: Low

## Next Checks
1. Cross-institutional validation: Test MMedAgent on medical datasets from different hospitals and countries to assess performance consistency across diverse imaging protocols and patient populations.

2. Real-world deployment simulation: Evaluate tool selection and task performance with noisy, incomplete, or ambiguous medical instructions that mimic actual clinical communication patterns.

3. Longitudinal tool integration test: Systematically add genuinely new medical tools and measure both retention of existing performance and integration quality over multiple incremental updates.