---
ver: rpa2
title: 'Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural
  Networks'
arxiv_id: '2409.02134'
source_url: https://arxiv.org/abs/2409.02134
tags:
- compression
- pruning
- accuracy
- used
- quantization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates model compression techniques for ConvNeXt
  models on CIFAR-10 image classification. It assesses structured pruning, unstructured
  pruning, and dynamic quantization to reduce model size and computational complexity
  while maintaining accuracy.
---

# Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks

## Quick Facts
- arXiv ID: 2409.02134
- Source URL: https://arxiv.org/abs/2409.02134
- Reference count: 40
- Model compression techniques evaluated for ConvNeXt models on CIFAR-10 with up to 89.7% size reduction and 3.8% accuracy increase

## Executive Summary
This study evaluates model compression techniques for ConvNeXt models on CIFAR-10 image classification, focusing on structured pruning (OTOv3), unstructured pruning, and dynamic quantization. The research systematically assesses these techniques both individually and in combination to reduce model size and computational complexity while maintaining or improving accuracy. Experiments were conducted on cloud platforms and an edge device, demonstrating that OTOV3 pruning combined with dynamic quantization achieves substantial compression (89.7% size reduction, 95% parameter reduction) while actually improving accuracy by 3.8% and enabling 20ms inference time on edge hardware.

## Method Summary
The study evaluates three compression techniques on ConvNeXt Small models fine-tuned on CIFAR-10: OTOv3 structured pruning (removing entire parameter groups via dependency analysis), unstructured pruning (L1 and random), and dynamic quantization (per-layer bit-width adaptation). The OTOV3 approach uses dependency graphs to form Pruning Zero-Invariant Groups (PZIGs) that are removed using Dual Half-Space Projected Gradient optimization. Dynamic quantization employs a bit-width controller with policy gradient training to optimize quantization per layer. Combined approaches were tested, with models evaluated on cloud platforms and deployed on an edge device (Intel i7-1165G7 CPU) using 20 printed CIFAR-10 samples.

## Key Results
- OTOV3 structured pruning achieved up to 75% model size reduction with minimal accuracy loss
- Dynamic quantization reduced parameters by up to 95% through per-layer bit-width adaptation
- Combined OTOV3 pruning and dynamic quantization achieved 89.7% size reduction, 95% parameter reduction, 3.8% accuracy increase, and 20ms inference time on edge device
- Fine-tuned models showed improved compression performance compared to pre-trained models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OTOV3 structured pruning effectively removes entire parameter groups (PZIGs) while preserving accuracy, enabling high compression ratios without accuracy drop.
- Mechanism: OTOV3 uses dependency analysis to form Pruning Zero-Invariant Groups (PZIGs), then jointly searches for redundant structures using Dual Half-Space Projected Gradient (DHSPG) with a sparsity constraint to remove entire groups rather than individual weights.
- Core assumption: Interdependent vertices in the same node group can be pruned together without harming model performance if the DHSPG algorithm correctly identifies redundancy.
- Evidence anchors:
  - [abstract] "The combination of OTOV3 pruning and dynamic quantization further enhances compression performance, resulting 89.7% reduction in size, 95% reduction with number of parameters and MACs, and 3.8% increase with accuracy."
  - [section] "OTOv3 partitions the trainable variables of the DNN into Pruning Zero-Invariant Groups (PZIGs) based on the pruning dependency graph... DHSPG minimizes the objective function while introducing a sparsity constraint to identify redundant groups for removal."
  - [corpus] Weak - related papers discuss structured pruning but don't provide direct evidence for OTOV3's specific DHSPG-based PZIG approach.

### Mechanism 2
- Claim: Dynamic quantization achieves significant compression by adapting quantization bit-width per layer based on representation capacity, minimizing accuracy loss.
- Mechanism: Dynamic quantization uses a bit-width controller module with policy gradient-based training to learn optimal bit-width for each layer individually, allowing layers with higher representational capacity to use fewer bits while preserving accuracy.
- Core assumption: Different layers have varying representation abilities and capacities, and a learned policy can effectively balance bit-width reduction with accuracy preservation.
- Evidence anchors:
  - [abstract] "dynamic quantization achieves a reduction of up to 95% in the number of parameters"
  - [section] "Dynamic quantization adapts the quantization bit-width for each layer individually based on its representation abilities and capacities. This is achieved through the use of a bit-width controller module, which employs a policy gradient-based training approach to learn the optimal bit-width for each layer."
  - [corpus] Weak - related papers mention quantization but don't provide specific evidence for dynamic bit-width adaptation approach.

### Mechanism 3
- Claim: Combining OTOV3 pruning with dynamic quantization produces synergistic compression effects that exceed either technique alone.
- Mechanism: OTOV3 removes entire parameter groups structurally, creating a sparser base model, then dynamic quantization further compresses this pruned model by reducing precision per layer, achieving multiplicative compression effects.
- Core assumption: The structural sparsity from OTOV3 creates an optimal foundation for subsequent quantization, where remaining parameters can be aggressively quantized without accuracy loss.
- Evidence anchors:
  - [abstract] "The combination of OTOV3 pruning and dynamic quantization further enhances compression performance, resulting 89.7% reduction in size, 95% reduction with number of parameters and MACs, and 3.8% increase with accuracy."
  - [section] "Pruning using OTOV3 and Quantization using Pytorch dynamic quantization achieved high performance (Table V) 89.7% reduction in model size, 95% reduction with number of parameters and MACs, and 3.8% increase with accuracy."
  - [corpus] Weak - no related papers specifically discuss combining these two techniques, though combined approaches are mentioned in general.

## Foundational Learning

- Concept: Structured pruning fundamentals (removing entire parameter groups vs. unstructured pruning)
  - Why needed here: Understanding the difference between OTOV3's structured approach and unstructured techniques is crucial for grasping why OTOV3 achieves better compression ratios and computational efficiency.
  - Quick check question: What is the key architectural difference between removing entire neuron groups versus zeroing individual weights?

- Concept: Dynamic quantization principles (per-layer bit-width adaptation)
  - Why needed here: The effectiveness of the compression approach depends on understanding how policy gradient training can optimize quantization per layer based on representational capacity.
  - Quick check question: How does dynamic quantization differ from traditional fixed-bit quantization across all layers?

- Concept: PyTorch model compression APIs (torch.nn.utils.prune and quantization modules)
  - Why needed here: The experiments use specific PyTorch implementations for pruning and quantization, requiring familiarity with these APIs for implementation and modification.
  - Quick check question: What PyTorch modules are used for implementing unstructured pruning versus dynamic quantization?

## Architecture Onboarding

- Component map:
  - ConvNeXt model architecture (CNBlock structure with Conv2d, Permute, LayerNorm, Linear layers)
  - OTOV3 structured pruning system (dependency analysis, PZIG formation, DHSPG optimization)
  - Dynamic quantization pipeline (bit-width controller, policy gradient training)
  - Evaluation framework (cloud-based profiling with accuracy, size, parameters, MACs, non-zero parameters metrics)
  - Edge deployment setup (CPU inference with camera input for real-time classification)

- Critical path:
  1. Load and profile original ConvNeXt model
  2. Apply OTOV3 pruning with dependency analysis and DHSPG optimization
  3. Apply dynamic quantization with per-layer bit-width adaptation
  4. Profile compressed model and compare metrics
  5. Deploy on edge device and measure inference performance

- Design tradeoffs:
  - Structured pruning vs. unstructured pruning: Structured pruning achieves better computational efficiency but may be more sensitive to architectural dependencies
  - Aggressive vs. conservative quantization: Higher compression ratios risk accuracy loss but enable better edge deployment
  - Training vs. post-training compression: Joint training with compression can improve accuracy but increases computational cost

- Failure signatures:
  - Accuracy degradation after pruning indicates incorrect dependency analysis or overly aggressive PZIG removal
  - Minimal size reduction despite pruning suggests structural dependencies preventing effective group removal
  - Edge deployment failures indicate compatibility issues between compression techniques and deployment framework

- First 3 experiments:
  1. Profile original ConvNeXt Small model on CIFAR-10 to establish baseline metrics
  2. Apply OTOV3 structured pruning to fine-tuned model with 200 epochs; apply dynamic quantization (8-bit) to pruned model; measure model size, parameters, MACs, and accuracy
  3. Deploy compressed model on edge device (Intel i7-1165G7 CPU, 16GB RAM); test with 20 printed CIFAR-10 samples; measure accuracy and inference time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different ConvNeXt model architectures influence the effectiveness of OTOV3 pruning in terms of both accuracy and compression ratio?
- Basis in paper: [explicit] The paper states that the Torch implementation of ConvNeXt Small exhibited lower accuracy compared to its TIMM counterpart, indicating that OTOV3's performance regarding accuracy may be influenced by the details of the model architecture.
- Why unresolved: The paper highlights the architectural differences between Torch and TIMM implementations but does not provide a detailed analysis of how these differences specifically impact OTOV3's pruning effectiveness. It only mentions that different training strategies or adjustments may be necessary.
- What evidence would resolve it: A comparative study analyzing the architectural differences between Torch and TIMM ConvNeXt models and their impact on OTOV3's pruning performance, including specific architectural features that influence pruning effectiveness.

### Open Question 2
- Question: What are the long-term performance and stability implications of using combined OTOV3 and dynamic quantization techniques on edge devices?
- Basis in paper: [inferred] The paper mentions successful deployment of compressed ConvNeXt models on edge devices, achieving high accuracy and low inference times, but does not discuss long-term performance or stability.
- Why unresolved: While the paper demonstrates immediate benefits of the combined techniques, it does not address how these compressed models perform over extended periods or under varying conditions on edge devices.
- What evidence would resolve it: Longitudinal studies evaluating the performance and stability of models compressed with OTOV3 and dynamic quantization over time, including tests under different environmental conditions and workloads on edge devices.

### Open Question 3
- Question: How does the performance of unstructured pruning techniques like L1 unstructured and random unstructured pruning compare to structured pruning techniques in terms of maintaining model accuracy and computational efficiency?
- Basis in paper: [explicit] The paper notes that unstructured pruning techniques showed limited reductions in computational complexity, while structured pruning like OTOV3 achieved substantial reductions in model size and MACs.
- Why unresolved: The paper highlights the limitations of unstructured pruning but does not provide a comprehensive comparison of accuracy and computational efficiency between unstructured and structured pruning techniques.
- What evidence would resolve it: Comparative experiments evaluating the trade-offs between unstructured and structured pruning techniques across various models and tasks, focusing on both accuracy retention and computational efficiency.

## Limitations

- The compression claims rely on specific OTOV3 and dynamic quantization configurations without complete hyperparameter disclosure
- Edge deployment results are based on a limited test set of 20 samples rather than comprehensive evaluation
- The synergistic effects of combining pruning and quantization lack direct comparative evidence against other compression method combinations

## Confidence

- **High Confidence**: The fundamental mechanisms of structured pruning (removing entire parameter groups) and dynamic quantization (per-layer bit-width adaptation) are well-established in the literature.
- **Medium Confidence**: The reported compression ratios and accuracy improvements appear technically plausible given the described techniques, though exact performance depends on implementation details.
- **Low Confidence**: The claimed synergistic effects of combining OTOV3 pruning with dynamic quantization and the specific 3.8% accuracy increase require more extensive validation across different model architectures and datasets.

## Next Checks

1. Reproduce the combined compression pipeline with full hyperparameter disclosure to verify the claimed 89.7% size reduction and 95% parameter reduction.
2. Conduct edge deployment testing with a larger, more diverse sample set (minimum 1000 samples) to validate the 20ms inference time and 92.5% accuracy claims.
3. Perform ablation studies comparing OTOV3 + quantization against alternative compression combinations to quantify the specific benefits of this approach.