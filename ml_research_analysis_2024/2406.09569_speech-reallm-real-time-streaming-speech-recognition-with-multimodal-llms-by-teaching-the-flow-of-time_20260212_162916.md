---
ver: rpa2
title: Speech ReaLLM -- Real-time Streaming Speech Recognition with Multimodal LLMs
  by Teaching the Flow of Time
arxiv_id: '2406.09569'
source_url: https://arxiv.org/abs/2406.09569
tags:
- speech
- reallm
- time
- rnn-t
- real-time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Speech ReaLLM introduces a real-time streaming ASR architecture
  combining decoder-only LLM with RNN-T's BLANK mechanism, enabling continuous audio
  processing without explicit end-pointing. On Librispeech test set, an 80M-parameter
  Speech ReaLLM achieves 3.0% and 7.4% WER, outperforming a same-size Speech LLM baseline
  and matching a 3x larger AED model.
---

# Speech ReaLLM -- Real-time Streaming Speech Recognition with Multimodal LLMs by Teaching the Flow of Time

## Quick Facts
- arXiv ID: 2406.09569
- Source URL: https://arxiv.org/abs/2406.09569
- Reference count: 0
- Primary result: 80M Speech ReaLLM achieves 3.0% WER on Librispeech test-clean in real-time streaming

## Executive Summary
Speech ReaLLM introduces a novel architecture that enables real-time streaming ASR using decoder-only LLMs by teaching them to handle temporal flow through BLANK token interleaving. The system processes speech chunks continuously and generates text outputs in real-time without requiring explicit end-pointing. On Librispeech test sets, an 80M-parameter Speech ReaLLM achieves 3.0% and 7.4% WER, outperforming same-size baselines and matching larger models. The approach also demonstrates that pre-trained 7B LLMs can be fine-tuned for this task, though with some accuracy regression.

## Method Summary
Speech ReaLLM combines a decoder-only LLM with RNN-T's BLANK mechanism to enable real-time streaming ASR. Speech embeddings are interleaved with BLANK tokens to align with target labels, trained using standard cross-entropy loss with fixed alignments from an external CTC model. The architecture uses a Streaming Conformer encoder and Llama-2 style decoder, processing 240ms speech chunks. Training uses Librispeech data with SpecAugment augmentation, and inference runs greedily until BLANK token prediction.

## Key Results
- 80M Speech ReaLLM achieves 3.0% WER on Librispeech test-clean (vs 7.1% for baseline)
- Real-time factor of 0.5 on wearable hardware with 240ms chunks
- Pre-trained 7B LLM can be fine-tuned, achieving 4.2-9.5% WER
- Handles long utterances gracefully without accuracy degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interleaving speech embeddings with BLANK tokens enables the decoder-only model to learn temporal alignment without explicit end-pointing
- Mechanism: The decoder processes speech embeddings sequentially, and whenever a BLANK token is predicted, it signals no new words should be generated until more speech input arrives. This creates a continuous stream of predictions that alternate between word tokens and BLANK tokens, effectively learning when to emit text based on incoming audio.
- Core assumption: The model can learn meaningful representations from the alternating pattern of speech embeddings and BLANK tokens that correspond to actual word boundaries in the target sequence.
- Evidence anchors: [abstract] "The idea is inspired by RNN-T: Instead of generating a response only at the end of a user prompt, generate after every input token received in real time (it is often empty)." [section 3] "To form the training target sequence, we convert this into a label sequence where a BLANK symbol (denoted as □) stands for an embedding vector that represents 240 ms of speech"

### Mechanism 2
- Claim: Pre-trained LLMs can be fine-tuned to handle real-time streaming by learning to interpret speech embeddings as pseudo-text tokens
- Mechanism: The encoder maps speech chunks to embeddings that the decoder treats as if they were text tokens. Through fine-tuning, the decoder learns that certain speech embeddings (those corresponding to BLANK periods) should produce no output, while others should produce words when appropriate.
- Core assumption: The LLM's attention mechanisms can generalize from text-token processing to treating speech embeddings as analogous to text tokens, preserving the temporal flow.
- Evidence anchors: [abstract] "We also show that this way, an LLM architecture can learn to represent and reproduce the flow of time; and that a pre-trained 7B LLM can be fine-tuned to do reasonably well on this task." [section 4.3.2] "Row P in Table 1 shows that it works, although the WERs of 4.2 to 9.5% are in the upper range compared to 80M models that had been trained from scratch."

### Mechanism 3
- Claim: The BLANK token mechanism allows decoder-only models to handle streaming without requiring complex loss functions like RNN-T
- Mechanism: By using fixed alignments from an external CTC model and interleaving speech embeddings with BLANK tokens, the model can be trained with standard cross-entropy loss instead of needing marginalization over all possible alignments.
- Core assumption: Fixed alignments from a CTC teacher are sufficiently accurate to guide the model's learning of temporal relationships between speech and text.
- Evidence anchors: [abstract] "On Librispeech 'test,' an 80M Speech ReaLLM achieves WERs of 3.0% and 7.4% in real time (without an external LM or auxiliary loss)." [section 3] "We approximate such loss by using fixed alignments generated by an external CTC [16] model, the 'alignment-teacher.'"

## Foundational Learning

- Concept: Temporal alignment in sequence-to-sequence models
  - Why needed here: Speech ReaLLM needs to learn when to emit words based on incoming speech chunks, requiring understanding of how audio frames map to text tokens
  - Quick check question: How does an RNN-T model handle the alignment between input frames and output tokens without explicit segmentation?

- Concept: Transformer decoder architecture and attention mechanisms
  - Why needed here: The core of Speech ReaLLM is a decoder-only architecture that must process interleaved speech and word embeddings, requiring understanding of how cross-attention works in transformers
  - Quick check question: In a standard decoder-only transformer, what role does the cross-attention layer play when processing a sequence of embeddings?

- Concept: Streaming vs. non-streaming ASR architectures
  - Why needed here: Speech ReaLLM bridges these paradigms by processing audio chunks in real-time while maintaining accuracy, requiring understanding of the tradeoffs between them
  - Quick check question: What is the key architectural difference between a streaming Conformer and a standard Conformer that enables real-time processing?

## Architecture Onboarding

- Component map: Audio input → Streaming Conformer (20 layers) → Frame stacking (240ms) → Decoder (2 layers) → Text output
- Critical path: Audio input → Streaming Conformer → Frame reduction → Decoder (with BLANK handling) → Text output
- Design tradeoffs:
  - Fixed alignments vs. marginalization: Simpler training but potentially less optimal alignments
  - Speech embedding rate (240ms) vs. latency: Longer frames reduce computation but increase latency
  - Decoder depth (2 layers) vs. accuracy: Shallower decoders are faster but may struggle with long-range dependencies
- Failure signatures:
  - High WER with long utterances: Indicates the decoder struggles with modeling long-span dependencies
  - Frequent hallucinated words at utterance start: Suggests the model hasn't learned proper BLANK handling
  - RTF > 1.0: Indicates the model cannot run in real-time on target hardware
- First 3 experiments:
  1. Validate the alignment teacher: Compare CTC alignments against ground truth to ensure they're reasonably accurate before training Speech ReaLLM
  2. Test BLANK token behavior: Train a minimal model and inspect whether BLANK tokens are being predicted at appropriate times by looking at attention patterns
  3. Measure latency vs. accuracy: Vary the speech embedding rate (e.g., 120ms, 240ms, 480ms) to find the optimal tradeoff for your deployment scenario

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a pre-trained LLM retain its original capabilities while being fine-tuned for real-time ASR?
- Basis in paper: [explicit] The paper states that while a 7B Llama-2 decoder can be fine-tuned to learn the ReaLLM behavior, the resulting system is not as good as the small models trained from scratch. It also questions to what degree the fine-tuned LLM retains its original capabilities.
- Why unresolved: The paper does not provide detailed analysis on the retention of the LLM's original capabilities after fine-tuning for real-time ASR.
- What evidence would resolve it: Detailed experiments comparing the performance of the fine-tuned LLM on both its original tasks and the new real-time ASR task would be needed to assess capability retention.

### Open Question 2
- Question: How does the complexity of the decoder affect the performance of Speech ReaLLM on long utterances?
- Basis in paper: [inferred] The paper mentions that non-streaming Speech LLM struggles with long utterances, while Speech ReaLLM handles them well. It suggests that the two-layer speech decoder in Speech ReaLLM may be too small to model long-span dependencies.
- Why unresolved: The paper does not provide experiments varying the complexity of the decoder to determine its impact on performance with long utterances.
- What evidence would resolve it: Experiments with Speech ReaLLM models having different decoder complexities, particularly focusing on their performance with long utterances, would clarify the impact of decoder size.

### Open Question 3
- Question: Can the distinct BLANK symbol in Speech ReaLLM be replaced by EOS to simplify the model?
- Basis in paper: [explicit] The paper questions whether the BLANK symbol can be replaced by EOS, indicating it as a potential area for further research.
- Why unresolved: The paper does not explore the feasibility or implications of replacing BLANK with EOS, leaving it as an open question.
- What evidence would resolve it: Implementing and testing a version of Speech ReaLLM that uses EOS instead of BLANK, followed by an analysis of its performance and any trade-offs, would provide insights into this modification.

## Limitations

- Evaluation limited to Librispeech read speech corpus, not tested on noisy or conversational environments
- Fixed alignment approach depends on quality of CTC alignment teacher, which isn't analyzed for accuracy
- Comparison uses 3x larger baseline model (240M vs 80M), making architectural advantage unclear
- Pre-trained 7B LLM fine-tuning shows regression (4.2-9.5% vs 3.0-7.4% WER), suggesting scaling limitations

## Confidence

**High Confidence**: The core technical contribution of interleaving speech embeddings with BLANK tokens for real-time streaming is well-supported by the experimental results on Librispeech. The WER improvements over the Speech LLM baseline (3.0% vs 7.1% on test-clean) are statistically significant and demonstrate the effectiveness of the approach for the specific task and dataset.

**Medium Confidence**: The claim that decoder-only models can learn temporal flow representation is supported by the results but limited by the narrow evaluation scope. While the 80M model achieves competitive WERs, the pre-trained 7B model's regression suggests the temporal learning capability may be architecture-dependent or require additional modifications for larger models.

**Low Confidence**: The generalizability claim to real-world applications and other domains is weakly supported. The paper doesn't test on noisy environments, conversational speech, or multilingual data. The wearable hardware demonstration is promising but lacks quantitative metrics on power consumption, memory usage, or robustness to environmental variations.

## Next Checks

1. **Alignment Quality Analysis**: Extract and analyze the CTC alignment teacher outputs to quantify alignment accuracy and identify systematic errors. Compare these alignments against ground truth alignments on a subset of utterances to establish the error rate and types of misalignments that could affect Speech ReaLLM training.

2. **Robustness Testing**: Evaluate Speech ReaLLM on challenging datasets like CHiME-6 (noisy speech), AMI (conversational speech), or CommonVoice (multilingual/multicultural speech) to assess real-world performance beyond Librispeech. Measure degradation in WER and identify specific failure modes that emerge in more realistic conditions.

3. **Ablation of BLANK Mechanism**: Remove the BLANK token mechanism and train a variant that generates words continuously without streaming control, then compare WER and latency. This would isolate the contribution of the BLANK mechanism to the overall performance and validate whether it's essential for the reported improvements or if simpler streaming approaches could achieve similar results.