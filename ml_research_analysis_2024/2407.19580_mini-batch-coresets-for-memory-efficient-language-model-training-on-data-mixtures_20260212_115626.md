---
ver: rpa2
title: Mini-batch Coresets for Memory-efficient Language Model Training on Data Mixtures
arxiv_id: '2407.19580'
source_url: https://arxiv.org/abs/2407.19580
tags:
- training
- gradient
- colm
- sources
- small
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Coresets for Training LLMs (CoLM), a method
  for memory-efficient fine-tuning of large language models (LLMs) by selecting smaller
  mini-batches that match the performance of larger ones. The approach addresses challenges
  from highly imbalanced data sources, the Adam optimizer, and the high dimensionality
  of LLM gradients.
---

# Mini-batch Coresets for Memory-efficient Language Model Training on Data Mixtures

## Quick Facts
- arXiv ID: 2407.19580
- Source URL: https://arxiv.org/abs/2407.19580
- Authors: Dang Nguyen; Wenhan Yang; Rathul Anand; Yu Yang; Baharan Mirzasoleiman
- Reference count: 40
- The paper proposes CoLM, achieving 2x memory reduction while outperforming 4x larger random mini-batches

## Executive Summary
This paper introduces Coresets for Training LLMs (CoLM), a method for memory-efficient fine-tuning of large language models by selecting smaller mini-batches that match the performance of larger ones. The approach addresses challenges from highly imbalanced data sources, the Adam optimizer, and high dimensionality of LLM gradients. Experiments on fine-tuning Phi-2, Phi-3, Zephyr, and Llama-3 models with LoRA show CoLM reduces memory by 2x and outperforms training with 4x larger mini-batches, achieving up to 7.1% and 20% improvements on in- and out-domain tasks, respectively.

## Method Summary
CoLM combines three key innovations for memory-efficient LLM fine-tuning: (1) including all examples from small data sources and applying uniform weighting to ensure learning from underrepresented sources, (2) normalizing gradients by historical exponential averages for Adam compatibility, and (3) using zeroth-order methods to estimate and sparsify gradients of the last V-projection matrix for efficient medoid selection. The method integrates seamlessly with memory-efficient techniques like LoRA and can reduce memory requirements by 2x while maintaining or improving performance compared to larger random mini-batches.

## Key Results
- CoLM reduces memory requirement of fine-tuning by 2x compared to standard methods
- Outperforms training with 4x larger random mini-batches by up to 7.1% on in-domain tasks
- Achieves up to 20% improvement on out-of-domain tasks compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoLM achieves memory efficiency by selecting small mini-batches that simulate the gradient of larger ones, allowing training with reduced memory while maintaining or improving performance.
- Mechanism: Uses gradient matching to find medoids of big data sources in gradient space, combined with inclusion of all small source examples to ensure representation. Applies Adam-specific normalization using historical gradients for big sources only.
- Core assumption: Language data is highly imbalanced across sources, and Adam's adaptive learning rates require normalized gradient matching for effective subset selection.
- Evidence anchors:
  - [abstract]: "CoLM reduces the memory requirement of fine-tuning by 2x and even outperforms training with 4x larger random mini-batches."
  - [section]: "mini-batch coresets found by gradient matching do not contain representative examples of the small sources w.h.p., and thus including all examples of the small sources in the mini-batch coresets is crucial for optimal performance."
  - [corpus]: Weak evidence - corpus contains related mini-batch optimization papers but none specifically addressing CoLM's approach to imbalanced data handling.
- Break condition: If data sources are balanced or if Adam is not used, the normalization and selective inclusion strategies may provide minimal benefit.

### Mechanism 2
- Claim: CoLM uses zeroth-order methods to estimate gradients of the last V-projection matrix, enabling efficient selection in high-dimensional spaces.
- Mechanism: Applies SPSA to estimate last-layer gradients without full backpropagation, then sparsifies using top-magnitude parameters per source to reduce dimensionality for medoid selection.
- Core assumption: The last V-projection matrix captures sufficient gradient variation for subset selection, and sparsification preserves selection quality while reducing computational cost.
- Evidence anchors:
  - [abstract]: "we leverage zeroth-order methods to find smooth gradient of the last V-projection matrix and sparsify it to keep the dimensions with the largest normalized gradient magnitude."
  - [section]: "We use zeroth-order methods to calculate the gradient of the last V -projection matrix, and then sparsify it to lower its dimensionality."
  - [corpus]: Weak evidence - corpus contains mini-batch optimization methods but no specific evidence for zeroth-order gradient estimation in LLM training contexts.
- Break condition: If the last-layer gradient does not capture sufficient variation or if sparsification removes too many relevant dimensions, selection quality degrades.

### Mechanism 3
- Claim: Uniform weighting of selected examples ensures balanced learning across sources, improving convergence compared to gradient-weighted selection.
- Mechanism: Assigns equal weights to all examples in mini-batch coresets regardless of their original gradient magnitudes, promoting uniform learning speed across different data sources.
- Core assumption: Balanced learning rates across sources leads to better overall performance than letting gradient magnitudes dictate example importance.
- Evidence anchors:
  - [abstract]: "Besides, to enhance learning small sources, we weight all examples in the mini-batch coresets uniformly."
  - [section]: "Finally, to ensure learning various big and small groups at a more uniform speed, we assign uniform weights to all the selected examples."
  - [corpus]: Weak evidence - corpus contains related sampling strategies but no direct evidence supporting uniform weighting over gradient-based weighting.
- Break condition: If certain sources genuinely require different learning rates or if uniform weighting prevents learning from more informative examples, performance may suffer.

## Foundational Learning

- Concept: Submodular optimization and greedy algorithms for facility location problems
  - Why needed here: CoLM uses submodular maximization to find medoids in gradient space, which requires understanding greedy selection algorithms and their approximation guarantees
  - Quick check question: What is the theoretical guarantee of the greedy algorithm for maximizing a monotone submodular function?

- Concept: Zeroth-order optimization and SPSA gradient estimation
  - Why needed here: CoLM estimates gradients without backpropagation using simultaneous perturbation, requiring understanding of stochastic approximation methods and their convergence properties
  - Quick check question: How does the SPSA estimator relate to the true gradient as the perturbation scale approaches zero?

- Concept: Adam optimizer mechanics and historical gradient averaging
  - Why needed here: CoLM normalizes gradients by historical exponential averages for Adam compatibility, requiring understanding of adaptive learning rate mechanisms and their impact on gradient matching
  - Quick check question: How does Adam's normalization affect the relationship between vanilla gradients and effective learning updates?

## Architecture Onboarding

- Component map:
  - Data preprocessing: Source identification and clustering
  - Gradient estimation: Zeroth-order SPSA for last-layer parameters
  - Subset selection: Submodular facility location optimization
  - Training loop: Adam optimization with uniform example weighting
  - Memory management: Integration with LoRA and gradient accumulation

- Critical path:
  1. Identify data sources (pre-labeled or through clustering)
  2. For each mini-batch, calculate SPSA gradients for last-layer parameters
  3. Compute historical Adam normalization terms for big sources
  4. Apply source-wise sparsification and â„“1 distance calculation
  5. Solve submodular optimization to select subset
  6. Train with uniform weighting across selected examples

- Design tradeoffs:
  - Zeroth-order vs backpropagation: Lower memory but noisier gradients
  - Source-wise vs global selection: Better handling of imbalance but increased computation
  - Uniform vs gradient-based weighting: More balanced learning but potentially slower convergence on easy examples

- Failure signatures:
  - Poor performance on small sources: Check source identification and inclusion logic
  - Degraded convergence: Verify normalization terms and sparsification thresholds
  - Memory overhead: Profile SPSA gradient storage and historical term calculations

- First 3 experiments:
  1. Run CoLM on balanced dataset with Adam to verify normalization benefits disappear as expected
  2. Compare SPSA gradient quality vs backpropagation on small subset to verify estimation accuracy
  3. Test uniform vs gradient-based weighting on synthetic imbalanced data to verify balanced learning benefits

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- The paper lacks direct experimental validation of zeroth-order gradient estimation quality compared to true gradients
- No ablation studies demonstrate the necessity of uniform weighting versus gradient-based weighting
- The computational overhead and scalability for trillion-parameter models remains unexplored

## Confidence
- Medium confidence: Memory efficiency claims (2x reduction) and overall performance improvements based on abstract statements without detailed experimental methodology
- Medium confidence: Adam-specific normalization benefits, as the mechanism is theoretically sound but lacks ablation experiments demonstrating necessity
- Low confidence: Zeroth-order gradient estimation quality and sparsification effectiveness due to absence of validation against ground truth gradients
- Medium confidence: Submodular facility location approach, as the theoretical foundation is established but application to LLM gradients requires verification

## Next Checks
1. Implement a controlled experiment comparing SPSA gradient estimates against backpropagation gradients on a small LLM subset to quantify estimation error and its impact on subset selection quality
2. Conduct ablation studies removing uniform weighting to measure performance degradation on imbalanced datasets, establishing whether balanced learning rates across sources genuinely improves convergence
3. Test the CoLM method on balanced datasets where Adam normalization should provide minimal benefit, verifying that performance gains primarily stem from handling data imbalance rather than the normalization mechanism itself