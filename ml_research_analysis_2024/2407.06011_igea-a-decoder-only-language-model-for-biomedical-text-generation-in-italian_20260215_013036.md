---
ver: rpa2
title: 'Igea: a Decoder-Only Language Model for Biomedical Text Generation in Italian'
arxiv_id: '2407.06011'
source_url: https://arxiv.org/abs/2407.06011
tags:
- language
- biomedical
- italian
- igea
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Igea, the first decoder-only language model
  specifically designed for biomedical text generation in Italian. Built by continual
  pretraining of the Minerva model on a diverse corpus of Italian medical texts, Igea
  is available in three sizes: 350M, 1B, and 3B parameters.'
---

# Igea: a Decoder-Only Language Model for Biomedical Text Generation in Italian

## Quick Facts
- arXiv ID: 2407.06011
- Source URL: https://arxiv.org/abs/2407.06011
- Reference count: 0
- Primary result: First decoder-only language model for Italian biomedical text generation, available in 350M, 1B, and 3B parameter sizes

## Executive Summary
Igea is a family of decoder-only language models specifically designed for biomedical text generation in Italian. The models were developed by continually pretraining the Minerva model on a diverse corpus of 5 billion words of Italian medical texts, including web sources, textbooks, and PubMed abstracts. Available in three sizes (350M, 1B, and 3B parameters), Igea aims to balance computational efficiency with performance while managing the peculiarities of medical terminology in Italian. The models show improved biomedical language understanding compared to the base Minerva model while retaining general knowledge, as demonstrated through evaluation on both domain-specific and general benchmarks.

## Method Summary
Igea was developed through continual pretraining of the Minerva model using a 5-billion-word corpus of Italian biomedical texts. The training used Adam optimizer with modified betas (0.9/0.95), learning rate of 5e-5 with cosine scheduler and 0.02 warmup, bfloat16 precision, and distributed multi-GPU training. The models were evaluated using an Italian version of MedMCQA for biomedical tasks and general benchmarks (MMLU, ARC, HELLASWAG) to assess retention of general knowledge. Three model sizes were trained to accommodate different computational constraints while maintaining consistent methodology across scales.

## Key Results
- Igea demonstrates improved biomedical language understanding compared to base Minerva model on Italian MedMCQA benchmark
- Models retain substantial general knowledge as evidenced by strong performance on MMLU, ARC, and HELLASWAG benchmarks
- Multi-scale approach (350M, 1B, 3B parameters) enables deployment across varied computational resources
- First decoder-only language model specifically designed for Italian biomedical text generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continual pretraining on domain-specific Italian biomedical text improves task performance while preserving general knowledge.
- Mechanism: By fine-tuning a general-purpose model (Minerva) on a large, diverse corpus of Italian medical texts, the model adapts to the specialized terminology and context without overwriting its pre-existing general knowledge. This is evidenced by the performance gains on the MedMCQA-ITA benchmark while retaining strong results on general benchmarks (MMLU, ARC, HELLASWAG).
- Core assumption: The pretraining data is sufficiently large and diverse to cover the domain-specific nuances without causing catastrophic forgetting.
- Evidence anchors:
  - [abstract] "continually pretrained on a diverse corpus of Italian medical texts"
  - [section] "External validation on general-purpose benchmarks shows that common knowledge acquired during the initial pre-training is substantially retained"
- Break condition: If the biomedical training corpus is too small or narrow, or if training time/data is insufficient, the model may overfit to domain-specific terms and lose general knowledge.

### Mechanism 2
- Claim: Multi-scale model sizes (350M, 1B, 3B) allow adaptation to varied computational constraints without sacrificing performance.
- Mechanism: Offering multiple parameter sizes enables deployment in both research and resource-constrained environments. Larger models benefit from greater capacity, while smaller ones provide efficient, Chinchilla-optimal alternatives.
- Core assumption: Each model size is trained with the same corpus and methodology, so performance differences are primarily due to parameter count and architectural scaling.
- Evidence anchors:
  - [abstract] "available in three model sizes: 350 million, 1 billion, and 3 billion parameters"
  - [section] "This scaling approach allows for a progressive examination of model performance and utility across different computational resources"
- Break condition: If training resources are limited or if fine-tuning hyperparameters are not optimized per model size, smaller models may underperform or show instability.

### Mechanism 3
- Claim: Integration of multiple data sources (web, textbooks, PubMed abstracts) provides comprehensive domain coverage and robust language understanding.
- Mechanism: The diverse corpus composition ensures the model learns both formal scientific language and layman medical communication, improving its ability to handle real-world biomedical text generation tasks in Italian.
- Core assumption: The mixture of sources is representative of the full range of Italian biomedical language use cases.
- Evidence anchors:
  - [section] "The corpus for the continual training of Igea merges multiple sources: medical texts extracted from Italian web sources, a curated collection of medical textbooks, and translated PubMed abstracts"
  - [corpus] Weak: No explicit quantitative coverage metrics for biomedical subdomain balance in corpus
- Break condition: If the data sources are biased or unrepresentative (e.g., overrepresentation of web forums), the model may develop skewed or unreliable language patterns.

## Foundational Learning

- Concept: Continual pretraining and catastrophic forgetting
  - Why needed here: Ensures the model retains general knowledge while adapting to a new domain
  - Quick check question: What happens to general knowledge if the model is trained too long on only biomedical data?

- Concept: Multi-task evaluation across domain-specific and general benchmarks
  - Why needed here: Validates that the model performs well both in the target domain and on general language tasks
  - Quick check question: How can you tell if the model has lost general knowledge after domain-specific training?

- Concept: Data source diversity and bias mitigation
  - Why needed here: Reduces the risk of propagating biases or misinformation from a single source type
  - Quick check question: What are the risks if the training data overrepresents one type of source (e.g., web forums)?

## Architecture Onboarding

- Component map:
  Base model (Minerva) -> Italian biomedical corpus aggregation -> Distributed multi-GPU training -> Evaluation on MedMCQA-ITA and general benchmarks

- Critical path:
  1. Aggregate and preprocess Italian biomedical corpus
  2. Load and distribute Minerva model across GPUs
  3. Configure hyperparameters (learning rate, scheduler, beta values)
  4. Train over one epoch with gradient accumulation
  5. Evaluate on in-domain and general benchmarks

- Design tradeoffs:
  - Model size vs. efficiency: Larger models perform better but require more compute
  - Data diversity vs. noise: More sources improve coverage but may introduce inconsistencies
  - General vs. domain knowledge: Risk of catastrophic forgetting if domain training is too aggressive

- Failure signatures:
  - Overfitting: High training accuracy but poor generalization on MedMCQA-ITA
  - Catastrophic forgetting: Strong MedMCQA-ITA performance but degraded MMLU/ARC scores
  - Data imbalance: Poor performance on formal biomedical text if trained mostly on web forums

- First 3 experiments:
  1. Run training on 350M model with reduced batch size to verify GPU memory limits
  2. Evaluate after 25%, 50%, and 100% of training epoch to detect overfitting or forgetting
  3. Swap one data source (e.g., replace web data with more textbooks) and compare MedMCQA-ITA performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Igea's performance degrade when evaluated on biomedical text from specialties not represented in its training data (e.g., rare diseases or surgical domains)?
- Basis in paper: [inferred] The authors note that Igea is trained on "diverse medical texts" but do not specify coverage across medical specialties. The MedMCQA-ITA evaluation shows general improvement over Minerva but doesn't test domain-specific coverage.
- Why unresolved: The paper lacks evaluation across diverse biomedical subdomains to demonstrate consistent performance across the full spectrum of medical knowledge.
- What evidence would resolve it: Evaluation results showing Igea's accuracy across different medical specialties (cardiology, oncology, rare diseases, etc.) compared to Minerva would demonstrate whether the model's performance is uniformly strong or limited to specific domains.

### Open Question 2
- Question: How does Igea's biomedical knowledge retention compare to continual pretraining approaches that use domain-specific data throughout all training phases rather than only in the final stage?
- Basis in paper: [explicit] The authors state that Igea is "continually pretrained" on biomedical data after initial general training, and they evaluate knowledge retention using general-purpose benchmarks to show "catastrophic forgetting" didn't occur.
- Why unresolved: The paper doesn't compare this continual pretraining approach against alternative strategies like domain-specific pretraining from scratch or multi-stage training with domain data throughout.
- What evidence would resolve it: Direct comparison of Igea against models trained with different strategies (domain-specific pretraining from scratch, multi-stage with domain data throughout, etc.) on both biomedical and general benchmarks would reveal which approach best balances domain expertise with general knowledge retention.

### Open Question 3
- Question: What is the quantitative impact of dataset biases on Igea's generated medical content, and can these biases be effectively mitigated through fine-tuning?
- Basis in paper: [explicit] The authors explicitly discuss potential bias propagation from training data and mention the risk of exposing personally identifiable information, noting this is "critical in a medical context."
- Why unresolved: While the paper acknowledges bias risks, it doesn't provide empirical analysis of specific biases in the model's outputs or test mitigation strategies.
- What evidence would resolve it: Analysis showing the prevalence and types of biases in Igea's generated medical content, followed by experiments testing bias mitigation techniques (adversarial debiasing, balanced fine-tuning, etc.) with before/after comparisons would quantify both the problem and potential solutions.

## Limitations
- Potential propagation of biases from training data, particularly related to race, gender, and socioeconomic status
- Risk of exposing personally identifiable information from the training corpus
- Limited evaluation scope with only one in-domain benchmark (MedMCQA-ITA) and three general benchmarks

## Confidence
- High Confidence: The claim that continual pretraining on domain-specific Italian biomedical text improves task performance while preserving general knowledge is well-supported by the experimental results showing improved MedMCQA-ITA performance without significant degradation on MMLU, ARC, and HELLASWAG benchmarks.
- Medium Confidence: The assertion that multi-scale model sizes allow adaptation to varied computational constraints is reasonable given the architecture, but the paper doesn't provide detailed performance-per-parameter comparisons across the three model sizes to fully validate this claim.
- Low Confidence: The claim about comprehensive domain coverage from the diverse corpus is difficult to fully assess without quantitative metrics on corpus composition and subdomain representation.

## Next Checks
1. Conduct a systematic evaluation of Igea's outputs for potential biases related to race, gender, and socioeconomic factors using established bias detection metrics and diverse test prompts that probe these dimensions.
2. Evaluate Igea's performance across different medical subdomains (e.g., oncology, cardiology, neurology) using specialized Italian medical datasets to verify that the model performs consistently across the full spectrum of biomedical knowledge.
3. Test Igea's sensitivity to input variations including spelling errors, medical abbreviations, and different Italian dialects or regional medical terminology to assess real-world robustness in diverse clinical settings.