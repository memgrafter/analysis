---
ver: rpa2
title: '3D-Properties: Identifying Challenges in DPO and Charting a Path Forward'
arxiv_id: '2406.07327'
source_url: https://arxiv.org/abs/2406.07327
tags:
- responses
- training
- rejected
- likelihood
- chosen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper identifies three key issues in DPO training\u2014Drastic\
  \ drop in rejected response likelihood, Degradation into response suppression, and\
  \ Dispersion effect on unseen responses\u2014termed 3D-properties. These arise from\
  \ the interaction between gradients of chosen and rejected responses, leading to\
  \ optimization instability."
---

# 3D-Properties: Identifying Challenges in DPO and Charting a Path Forward

## Quick Facts
- arXiv ID: 2406.07327
- Source URL: https://arxiv.org/abs/2406.07327
- Authors: Yuzi Yan; Yibo Miao; Jialian Li; Yipin Zhang; Jian Xie; Zhijie Deng; Dong Yan
- Reference count: 40
- One-line primary result: Identifies three critical issues (3D-properties) in DPO training and proposes regularization techniques that improve stability and performance.

## Executive Summary
This paper identifies three fundamental issues in Direct Preference Optimization (DPO) training, collectively termed 3D-properties: Drastic drop in rejected response likelihood, Degradation into response suppression, and Dispersion effect on unseen responses. These arise from the interaction between gradients of chosen and rejected responses, leading to optimization instability. Through toy models and real LLM experiments on mathematical reasoning and instruction-following tasks, the authors validate these properties and propose regularization techniques like adaptive β adjustment (Flex-DPO) and SFT loss addition to mitigate them. Experimental results show that on-policy DPO outperforms off-policy variants and that DPO training is less stable than reward-model-based RLHF. The proposed methods improve DPO performance and stability, bringing reward-model-free approaches closer to reward-model-based alignment in effectiveness.

## Method Summary
The authors implement vanilla DPO and variants (IPO, SLiC, SimPO) on Baichuan2-13B and Baichuan2-33B models, using preference datasets for mathematical reasoning (MATH, SuperCLUE-Math) and instruction following (poem and slogan generation). They train with proposed regularization techniques including adaptive β adjustment in Flex-DPO and SFT loss regularization, comparing on-policy and off-policy scenarios. The method involves collecting preference pairs, computing gradients for chosen and rejected responses using the DPO loss function, updating model parameters, and monitoring for 3D-properties while applying regularization as needed.

## Key Results
- On-policy DPO outperforms off-policy DPO due to smaller distribution gaps between policy outputs and preference data
- DPO training shows less stability than reward-model-based RLHF approaches
- Proposed regularization techniques (Flex-DPO with adaptive β and SFT loss addition) improve DPO performance and stability
- The 3D-properties manifest consistently across mathematical reasoning and creative tasks, affecting both small and medium-scale models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The 3D-properties arise from the interaction between gradients of chosen and rejected responses in DPO.
- Mechanism: In DPO, the gradient with respect to the rejected response likelihood grows faster than the gradient with respect to the chosen response likelihood. As the likelihood of rejected responses approaches zero, the gradient for chosen responses diminishes while the gradient for rejected responses increases dramatically, leading to instability and degradation.
- Core assumption: The mathematical relationship between gradients in DPO creates an inherent instability that manifests as the 3D-properties.
- Evidence anchors:
  - [abstract]: "We identify three key properties—termed 3D-properties—that emerge from DPO's learning process: Drastic drop in rejected response likelihood, Degradation into response suppression, and Dispersion effect on unseen responses. We show that these issues arise from DPO's optimization dynamics, where the interaction between chosen and rejected response gradients leads to instability."
  - [section]: "As π− → 0, since β < 1, ∂ℓDPO/∂π+ is proportional to (π−)β and tends to 0, while ∂ℓDPO/∂π− is proportional to (π−)β−1 and tends to infinity. Therefore, in this case, the gradient for the rejected action becomes extremely large, while the gradient for the chosen action becomes very small."
- Break condition: If the relationship between chosen and rejected response gradients changes fundamentally, or if the optimization process introduces mechanisms to balance these gradients differently.

### Mechanism 2
- Claim: On-policy DPO performs better than off-policy DPO because it has a smaller distribution gap between the policy model outputs and the preference dataset.
- Mechanism: When preference pairs are sampled directly from the policy model (on-policy), the initial likelihood of rejected responses is higher, extending the duration before their likelihood significantly diminishes. This delays the onset of 3D-properties and maintains better performance.
- Core assumption: The distribution of preference data critically influences DPO's effectiveness, with smaller distribution gaps leading to better performance.
- Evidence anchors:
  - [abstract]: "Our findings confirm that the distribution of preference data critically influences DPO's effectiveness, with on-policy DPO performing better than off-policy DPO."
  - [section]: "On-policy DPO enjoys a smaller distribution gap compared with off-policy DPO. To conduct on-policy DPO, we used the policy model to produce 8 candidates for each prompt in the train set of MATH."
- Break condition: If the distribution gap becomes irrelevant due to changes in the optimization process, or if other factors become more dominant in determining performance.

### Mechanism 3
- Claim: Regularization techniques like adaptive β adjustment (Flex-DPO) and SFT loss addition mitigate the 3D-properties by controlling the rate at which rejected response likelihood declines.
- Mechanism: By adjusting β+ and β− separately, or by adding SFT regularization, the optimization process can be moderated to prevent the rapid decline of rejected response likelihood, maintaining stability and performance.
- Core assumption: Controlling the gradient magnitudes through regularization can effectively mitigate the instability caused by 3D-properties.
- Evidence anchors:
  - [abstract]: "To address these challenges, we propose simple regularization techniques that improve training stability and performance. Additionally, we examine how preference data distribution impacts DPO's effectiveness."
  - [section]: "Following Flex-DPO, the regularization methods outlined in Section 3.3, we fixed β+ and systematically decreased β−. As indicated by the gradient analysis in Appendix B.2.1, the gradient of rejected responses with respect to β− follows a non-monotonic trajectory, initially increasing and then decreasing."
- Break condition: If the regularization techniques fail to adequately control the gradient dynamics, or if the trade-off between mitigating 3D-properties and maintaining preference learning effectiveness cannot be balanced.

## Foundational Learning

- Concept: Gradient dynamics in optimization
  - Why needed here: Understanding how gradients interact in DPO is crucial to explaining the 3D-properties and their effects on model performance.
  - Quick check question: Can you explain why the gradient with respect to the rejected response likelihood grows faster than the gradient with respect to the chosen response likelihood in DPO?

- Concept: On-policy vs off-policy learning
  - Why needed here: The difference between on-policy and off-policy DPO is central to understanding why distribution gaps affect performance.
  - Quick check question: What is the key difference between on-policy and off-policy DPO in terms of how preference pairs are generated?

- Concept: Regularization techniques in machine learning
  - Why needed here: The proposed regularization methods (Flex-DPO and SFT loss addition) are essential for mitigating the 3D-properties.
  - Quick check question: How does adjusting β+ and β− separately in Flex-DPO help control the optimization process?

## Architecture Onboarding

- Component map: Preference dataset -> DPO loss function -> Gradient computation -> Parameter updates -> Model performance monitoring
- Critical path: 1) Collect preference pairs (either on-policy or off-policy), 2) Compute gradients for chosen and rejected responses using the DPO loss function, 3) Update model parameters using these gradients, 4) Monitor for signs of 3D-properties and apply regularization if needed
- Design tradeoffs: DPO offers simplicity and efficiency by eliminating the need for a separate reward model, but this comes at the cost of stability and performance compared to reward-model-based approaches. The trade-off between mitigating 3D-properties and maintaining preference learning effectiveness is crucial.
- Failure signatures: Signs of 3D-properties include: rapid decrease in likelihood of rejected responses, diminishing gradients for chosen responses, increasing likelihood of out-of-distribution responses, and overall degradation in model performance.
- First 3 experiments:
  1. Replicate the toy model experiments to visualize the 3D-properties in a controlled setting and test different regularization techniques.
  2. Compare on-policy and off-policy DPO on a mathematical reasoning task to quantify the performance difference.
  3. Test the proposed regularization techniques (Flex-DPO and SFT loss addition) on a creative task like poem generation to evaluate their effectiveness in mitigating 3D-properties while maintaining performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the 3D-properties generalize to larger models and different domains beyond mathematical reasoning and instruction following?
- Basis in paper: [inferred] The paper primarily tests on Baichuan2-13B/33B for math and creative tasks, with limited domain diversity.
- Why unresolved: The authors acknowledge the need for further validation in larger-scale models and more diverse datasets.
- What evidence would resolve it: Systematic testing across a broader range of LLM sizes (e.g., 70B+ models) and domains (e.g., code generation, multi-turn dialogue) to assess the robustness of the 3D-properties.

### Open Question 2
- Question: What is the optimal balance between β+ and β− in Flex-DPO, and how does this balance vary across tasks?
- Basis in paper: [explicit] The authors note that β− peaks at 0.08 for poem generation but suggest this varies by task, with math requiring β−=0.01.
- Why unresolved: The paper does not provide a principled method for determining the optimal β− for different tasks.
- What evidence would resolve it: A comprehensive ablation study across multiple tasks and datasets to derive task-specific heuristics or adaptive methods for setting β+ and β−.

### Open Question 3
- Question: Do other reward-model-free algorithms (e.g., SimPO, SLiC) exhibit the 3D-properties, and if so, to what degree?
- Basis in paper: [explicit] The authors compare DPO variants but do not rigorously analyze whether SimPO or SLiC suffer from the same 3D-properties.
- Why unresolved: The analysis focuses on DPO's gradient dynamics but does not extend the same theoretical framework to other algorithms.
- What evidence would resolve it: A theoretical and empirical analysis of the gradient dynamics and performance degradation patterns in SimPO, SLiC, and IPO to determine if they share similar limitations.

## Limitations
- The toy model experiments simplify the complex dynamics of real LLM training, limiting generalizability
- Proposed regularization techniques require careful hyperparameter tuning, particularly for β values
- Analysis focuses primarily on mathematical reasoning and instruction-following tasks with less exploration of open-ended generation scenarios

## Confidence
- Mechanism 1 (Gradient Dynamics): Medium - well-theoretically justified but requires more empirical validation across diverse tasks
- Mechanism 2 (On-policy vs Off-policy): Medium - supported by experiments but dependent on dataset quality and distribution
- Mechanism 3 (Regularization Techniques): Medium - shows promise but optimal parameter settings remain task-dependent

## Next Checks
1. **Cross-domain robustness test**: Apply the proposed regularization techniques to open-ended generation tasks (story completion, dialogue) and measure whether 3D-properties manifest differently or require different mitigation strategies.

2. **Ablation study on β scheduling**: Systematically vary the β+ and β− adjustment schedules in Flex-DPO across multiple tasks to identify optimal strategies and determine whether adaptive scheduling consistently outperforms fixed approaches.

3. **Comparative stability analysis**: Design a controlled experiment comparing DPO with multiple reward-model-based approaches (PPO, vanilla RLHF) across identical tasks and datasets, measuring not just final performance but training stability and sensitivity to hyperparameters.