---
ver: rpa2
title: Disentangling ID and Modality Effects for Session-based Recommendation
arxiv_id: '2404.12969'
source_url: https://arxiv.org/abs/2404.12969
tags:
- modality
- co-occurrence
- item
- dimo
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DIMO, a novel framework that disentangles
  item ID and modality effects in session-based recommendation. DIMO explicitly incorporates
  co-occurrence patterns into ID embeddings and aligns different modalities into a
  unified semantic space.
---

# Disentangling ID and Modality Effects for Session-based Recommendation

## Quick Facts
- **arXiv ID:** 2404.12969
- **Source URL:** https://arxiv.org/abs/2404.12969
- **Reference count:** 40
- **Primary result:** DIMO framework disentangles item ID and modality effects in session-based recommendation, outperforming state-of-the-art methods on four real-world datasets.

## Executive Summary
This paper addresses the challenge of distinguishing between item ID effects (co-occurrence patterns) and modality effects (fine-grained preferences) in session-based recommendation. The authors introduce DIMO, a framework that explicitly incorporates co-occurrence patterns into item representations while aligning different modalities into a unified semantic space. Through a multi-view self-supervised disentanglement mechanism, DIMO separates these two effects without requiring labeled data. Experiments on four Amazon datasets demonstrate significant improvements over state-of-the-art methods, with DIMO achieving better precision and MRR metrics while also providing meaningful explanations for recommendations.

## Method Summary
DIMO operates at both item and session levels. At the item level, it introduces a co-occurrence representation schema that constructs a global co-occurrence graph and applies constraints to highlight these patterns in ID representations. Modality representations are aligned through text conversion (using image classification to convert images to text categories, then BERT for word embeddings). At the session level, a multi-view self-supervised disentanglement mechanism employs proxy mechanisms to find cause-specific proxies and counterfactual inference to determine which cause (ID or modality) dominates user choice. The framework then combines these disentangled causes through causal inference to predict user interactions, providing recommendations along with two explanation templates based on co-occurrence and feature effects.

## Key Results
- DIMO consistently outperforms state-of-the-art methods across all four datasets, achieving significant improvements in precision and MRR metrics
- The co-occurrence representation schema contributes substantially to performance, demonstrating the importance of explicitly incorporating item co-occurrence patterns
- Generated explanations using both co-occurrence and feature templates are meaningful and convincing, enhancing interpretability of recommendations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Co-occurrence representation schema explicitly incorporates item co-occurrence patterns into ID embeddings, improving performance.
- **Mechanism:** Constructs a global co-occurrence graph and applies a co-occurrence constraint that pulls embeddings of frequently co-occurring items closer and pushes rarely co-occurring ones apart.
- **Core assumption:** Item co-occurrence patterns are crucial for recommendation accuracy and can be effectively captured through graph-based representation.
- **Evidence anchors:** [abstract] "At the item level, we introduce a co-occurrence representation schema to explicitly incorporate cooccurrence patterns into ID representations." [section] "It constructs a global co-occurrence graph to encode co-occurrence patterns, and presents a co-occurrence constraint to highlight the patterns in ID representation learning."
- **Break condition:** If the co-occurrence matrix becomes too sparse or if the iterative update in Eq.(2) leads to over-smoothing of embeddings.

### Mechanism 2
- **Claim:** Multi-view self-supervised disentanglement distinguishes ID and modality effects without supervised signals.
- **Mechanism:** Uses proxy mechanism to push each cause (ID/modality) toward its proxy while pulling it away from the other, and counterfactual inference to determine which cause dominates user choice based on co-occurrence sets.
- **Core assumption:** Different user behaviors stem from either co-occurrence patterns (ID) or fine-grained preferences (modality), and these can be disentangled using proxy targets and counterfactual reasoning.
- **Evidence anchors:** [abstract] "At the session level, we present a multi-view self-supervised disentanglement, including proxy mechanism and counterfactual inference, to disentangle ID and modality effects without supervised signals." [section] "The proxy mechanism aims to find a proxy for each cause where the cause and its proxy possess similar semantics."
- **Break condition:** If the proxy mechanism fails to create meaningful separation between ID and modality embeddings, or if counterfactual inference incorrectly identifies the dominant cause.

### Mechanism 3
- **Claim:** Causal inference using both disentangled causes improves recommendation accuracy.
- **Mechanism:** Combines ID and modality effects through addition operation in Eq.(11) to predict user interaction probabilities.
- **Core assumption:** User actions are influenced by either co-occurrence patterns or fine-grained preferences, and combining both causes via causal inference leads to better predictions.
- **Evidence anchors:** [abstract] "Leveraging these disentangled causes, DIMO provides recommendations via causal inference and further creates two templates for generating explanations." [section] "With the ability to deduce results from causes, causal inference [44, 45] is used here to predict items of interest for users."
- **Break condition:** If one cause consistently dominates the other across all sessions, the addition operation may become suboptimal.

## Foundational Learning

- **Concept:** Graph Neural Networks (GNN)
  - **Why needed here:** GNN is used to construct and process the global co-occurrence graph for encoding item relationships.
  - **Quick check question:** How does a GNN update node embeddings based on graph structure and node features?

- **Concept:** Self-supervised learning
  - **Why needed here:** Enables disentanglement of ID and modality effects without requiring labeled data.
  - **Quick check question:** What are the key differences between supervised and self-supervised learning approaches?

- **Concept:** Causal inference
  - **Why needed here:** Provides a framework for combining disentangled causes (ID and modality) to make recommendations.
  - **Quick check question:** What are the main challenges in applying causal inference to recommendation systems?

## Architecture Onboarding

- **Component map:** Item representation (co-occurrence schema + modality alignment) → Sequence encoding → Multi-view self-supervised disentanglement (proxy + counterfactual) → Causal inference → Recommendations + Explanations

- **Critical path:** Item representation → Sequence encoding → Disentanglement → Causal inference → Recommendation

- **Design tradeoffs:**
  - Explicit vs implicit co-occurrence learning: Explicit learning through graph updates vs implicit learning through neural networks
  - Single vs multi-view disentanglement: Using both proxy mechanism and counterfactual inference vs simpler approaches
  - Addition vs other combination methods: Simple addition of ID and modality effects vs more complex combination strategies

- **Failure signatures:**
  - Poor performance on datasets with strong modality signals but weak co-occurrence patterns
  - Inability to generate meaningful explanations when neither template conditions are met
  - Over-reliance on one cause (ID or modality) across all sessions

- **First 3 experiments:**
  1. Ablation study: Remove co-occurrence representation schema and measure performance drop
  2. Visualization: Plot ID and modality embeddings to verify disentanglement
  3. Explanation quality: Manually evaluate generated explanations using both templates on sample sessions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can Large Language Models (LLMs) be effectively integrated with item ID co-occurrence patterns to enhance recommendation robustness and effectiveness?
- **Basis in paper:** [explicit] The conclusion section explicitly states this as a future direction: "Towards future work, we aim to apply our insights regarding the distinct effects of IDs and modalities to guide the development of Large Language Models (LLMs) in the realm of recommendation."
- **Why unresolved:** LLMs are still relatively new in recommendation systems, and their integration with traditional ID-based co-occurrence patterns remains largely unexplored. The paper focuses on disentangling ID and modality effects but does not explore how these insights can be applied to LLMs.
- **What evidence would resolve it:** A comprehensive study demonstrating how LLMs can leverage item ID co-occurrence patterns to improve recommendation accuracy, robustness, and interpretability. This could include experimental results comparing LLM-based recommendations with and without ID co-occurrence integration.

### Open Question 2
- **Question:** What are the optimal strategies for balancing the influence of co-occurrence patterns and fine-grained preferences in recommendations, especially when these factors conflict?
- **Basis in paper:** [inferred] The paper emphasizes the importance of disentangling ID and modality effects but does not explicitly address how to balance their influence when they suggest different recommendations. The proxy mechanism and counterfactual inference aim to distinguish effects but don't provide a clear strategy for resolving conflicts between them.
- **Why unresolved:** While the paper disentangles the effects, it doesn't provide a framework for deciding which factor should take precedence in recommendations when they conflict. This is crucial for practical implementation.
- **What evidence would resolve it:** Empirical studies demonstrating effective strategies for balancing co-occurrence patterns and fine-grained preferences in various recommendation scenarios. This could include A/B testing results showing user satisfaction with different balancing approaches.

### Open Question 3
- **Question:** How can the explainability templates (co-occurrence and feature templates) be further refined to provide more personalized and context-aware explanations?
- **Basis in paper:** [explicit] The paper mentions that "explanations produced by DIMO are meaningful and convincing" but doesn't explore how these explanations could be further personalized or made more context-aware. The templates are described but not optimized for different user segments or contexts.
- **Why unresolved:** The current templates provide general explanations based on co-occurrence and features, but they don't adapt to individual user preferences or specific contexts. More sophisticated personalization of explanations could enhance user trust and engagement.
- **What evidence would resolve it:** User studies comparing different explanation strategies, including personalized and context-aware versions of the current templates. Metrics could include user trust, understanding, and engagement with recommendations.

## Limitations

- **Limited generalizability:** The framework's performance on non-Amazon datasets and domains with different modality types or weaker co-occurrence patterns remains untested.
- **Binary explanation generation:** The explanation templates rely on binary decision-making that may not capture complex interactions between ID and modality effects in real-world scenarios.
- **No direct disentanglement validation:** Claims about disentanglement quality are primarily validated through ablation studies rather than direct evaluation of the separation between ID and modality representations.

## Confidence

- **Major Uncertainties:**
  - **Medium confidence:** Claims about disentanglement quality primarily validated through ablation studies rather than direct evaluation of the disentanglement itself.
  - **Low confidence:** Binary decision-making for explanation generation may not capture complex real-world user behavior interactions.
  - **Medium confidence:** Performance may not generalize well beyond Amazon product datasets with similar characteristics.

## Next Checks

1. **Direct Disentanglement Evaluation:** Create synthetic datasets where ground truth ID and modality effects are known, then measure how well DIMO can separate these effects compared to baseline methods.

2. **Cross-Domain Generalization:** Test DIMO on non-Amazon datasets (e.g., movie/music recommendation) where co-occurrence patterns differ significantly from product co-purchases, and where modality effects may be more nuanced than simple text/image features.

3. **Ablation of Causal Inference:** Remove the causal inference component entirely and use separate models for ID and modality effects, then compare performance to understand whether the integration actually adds value beyond parallel processing.