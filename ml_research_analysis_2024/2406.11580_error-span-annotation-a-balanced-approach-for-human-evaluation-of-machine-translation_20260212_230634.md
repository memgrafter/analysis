---
ver: rpa2
title: 'Error Span Annotation: A Balanced Approach for Human Evaluation of Machine
  Translation'
arxiv_id: '2406.11580'
source_url: https://arxiv.org/abs/2406.11580
tags:
- error
- translation
- annotation
- score
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Error Span Annotation (ESA), a human evaluation
  protocol for machine translation that combines continuous scoring with error span
  marking. ESA asks annotators to first highlight all errors in translations and then
  assign an overall quality score, aiming to be faster and cheaper than MQM while
  more reliable than DA+SQM.
---

# Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation

## Quick Facts
- arXiv ID: 2406.11580
- Source URL: https://arxiv.org/abs/2406.11580
- Reference count: 30
- This paper introduces Error Span Annotation (ESA), a human evaluation protocol for machine translation that combines continuous scoring with error span marking.

## Executive Summary
This paper introduces Error Span Annotation (ESA), a human evaluation protocol for machine translation that combines continuous scoring with error span marking. ESA asks annotators to first highlight all errors in translations and then assign an overall quality score, aiming to be faster and cheaper than MQM while more reliable than DA+SQM. The authors compare ESA with MQM and DA+SQM on 12 MT systems from WMT23 English-to-German. Results show ESA achieves similar system ranking quality as MQM but is 32% faster and doesn't require MQM-trained experts. ESA also demonstrates higher inter-annotator agreement than MQM. The study validates that ESA offers a balanced approach between comprehensive error classification and simple scoring methods.

## Method Summary
ESA is a human evaluation protocol that asks annotators to first identify and mark error spans in translations (categorized as minor or major severity), then assign an overall quality score. The method was compared against MQM (which requires detailed error classification) and DA+SQM (which assigns direct scores without error marking) using 12 English→German MT systems from WMT23. The study used 207 segments per system, with annotations collected from multiple annotator groups including bilingual annotators and MQM-trained experts. Evaluation metrics included system ranking accuracy, inter-annotator agreement, annotation speed, and annotation cost.

## Key Results
- ESA achieves similar system ranking quality as MQM (94.9% pairwise accuracy vs 93.6%)
- ESA is 32% faster than MQM while maintaining comparable quality levels
- ESA demonstrates higher inter-annotator agreement than MQM (0.55 vs 0.43 Pearson correlation)
- ESA eliminates the need for MQM-trained experts while maintaining annotation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ESA reduces annotation time compared to MQM while maintaining comparable quality
- Mechanism: By simplifying error annotation to span marking without detailed classification, ESA lowers cognitive load and annotation time
- Core assumption: Annotators can accurately mark error spans without needing to classify error types
- Evidence anchors:
  - [abstract]: "ESA offers faster and cheaper annotations than MQM at the same quality level"
  - [section]: "assigning a DA+SQM numerical score to a segment is anecdotally and intuitively much faster than MQM"
  - [corpus]: Weak - corpus does not provide direct time measurements
- Break condition: If annotators require error type information for accurate span marking, or if span marking without classification becomes ambiguous

### Mechanism 2
- Claim: ESA improves inter-annotator agreement compared to MQM
- Mechanism: By first marking errors then assigning overall scores, annotators are "primed" with error information when making quality judgments
- Core assumption: Seeing marked errors helps annotators make more consistent overall quality assessments
- Evidence anchors:
  - [abstract]: "ESA also demonstrates higher inter-annotator agreement than MQM"
  - [section]: "The annotators are first asked to identify and mark all errors, and afterwards to assign an overall score"
  - [corpus]: Weak - corpus does not directly address agreement mechanisms
- Break condition: If error marking does not actually improve consistency of final scores, or if error marking introduces new sources of disagreement

### Mechanism 3
- Claim: ESA provides better system ranking capabilities than DA+SQM
- Mechanism: By combining error span marking with direct scoring, ESA captures both error presence and overall quality in a balanced way
- Core assumption: The combination of error marking and direct scoring provides more reliable system differentiation than either approach alone
- Evidence anchors:
  - [abstract]: "ESA offers faster and cheaper annotations than MQM at the same quality level"
  - [section]: "Our proposed method lies between DA+SQM and MQM protocols"
  - [corpus]: Weak - corpus does not provide system ranking comparisons
- Break condition: If the error marking component does not add meaningful information beyond the direct scoring component

## Foundational Learning

- Concept: Human evaluation protocols in machine translation
  - Why needed here: Understanding the landscape of existing evaluation methods (MQM, DA+SQM) is crucial for contextualizing ESA's innovations
  - Quick check question: What are the key differences between MQM, DA+SQM, and ESA in terms of annotation requirements and outputs?

- Concept: Inter-annotator agreement metrics
  - Why needed here: Evaluating ESA's performance requires understanding how to measure agreement between different annotators
  - Quick check question: How do Kendall's Tau-c and Pearson correlation differ in measuring inter-annotator agreement for different types of evaluation protocols?

- Concept: System ranking in machine translation evaluation
  - Why needed here: ESA's effectiveness needs to be measured in terms of its ability to correctly rank translation systems
  - Quick check question: What statistical methods are used to determine significant differences between system rankings in human evaluation studies?

## Architecture Onboarding

- Component map:
  - Error Span Annotation Interface (based on Appraise framework) -> Error span marking functionality with severity levels (minor/major) -> Direct scoring component (0-100 scale) -> Backend for collecting and processing annotations -> Analysis tools for comparing ESA with MQM and DA+SQM

- Critical path:
  1. Annotator receives source and translation
  2. Annotator marks all error spans with appropriate severity
  3. Annotator assigns overall score based on marked errors
  4. System collects and aggregates annotations
  5. Analysis compares ESA results with MQM and DA+SQM

- Design tradeoffs:
  - Simplified error classification vs. detailed MQM taxonomy
  - Span marking precision vs. annotation speed
  - Direct scoring vs. computed scores from error spans
  - Bilingual annotators vs. MQM-trained experts

- Failure signatures:
  - Low inter-annotator agreement indicating unclear guidelines
  - Systematic bias in error marking (e.g., consistently marking wrong error types)
  - Score compression (most annotations clustering around certain values)
  - Inconsistent treatment of missing content errors

- First 3 experiments:
  1. Test annotation interface with small sample of translations to validate usability and identify confusing elements
  2. Compare ESA annotations with MQM annotations on the same translations to validate similar system rankings
  3. Measure annotation time for ESA vs. MQM on identical translations to confirm speed improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal error weight ratio for ESA spans to best predict annotator scores?
- Basis in paper: [explicit] The authors found that −1 for minor and −4.8 for major errors maximizes correlation with direct scores
- Why unresolved: The paper only tested this on one language pair (English→German) and a specific corpus
- What evidence would resolve it: Testing the optimal weight ratio on multiple language pairs and domains

### Open Question 2
- Question: How does ESA compare to DA+SQM in terms of actual cost savings when factoring in the need for more annotators due to lower inter-annotator agreement?
- Basis in paper: [inferred] The paper claims ESA is faster and cheaper than MQM but doesn't directly compare cost with DA+SQM accounting for IAA
- Why unresolved: Cost analysis focused on time savings, not full economic analysis including IAA requirements
- What evidence would resolve it: Cost comparison study using real annotator rates and required sample sizes

### Open Question 3
- Question: Does ESA's higher inter-annotator agreement translate to better downstream task performance (e.g., model selection, error analysis)?
- Basis in paper: [explicit] The paper shows ESA has higher IAA than MQM but doesn't test if this improves downstream utility
- Why unresolved: IAA was measured only as correlation between scores, not in terms of practical outcomes
- What evidence would resolve it: Experiments showing ESA's impact on model selection consistency or error analysis quality

## Limitations
- Results are based exclusively on English→German translation, limiting generalizability to other language pairs
- Comparison relies on a specific error typology that may not transfer to other annotation frameworks
- The study focuses on WMT23 systems, which may not represent the full spectrum of MT quality levels

## Confidence
- High confidence: ESA achieves 32% faster annotation than MQM while maintaining comparable system ranking quality
- High confidence: ESA demonstrates higher inter-annotator agreement than MQM (0.55 vs 0.43 Pearson correlation)
- Medium confidence: ESA's system ranking capabilities match MQM's performance across all evaluation metrics
- Medium confidence: ESA eliminates the need for MQM-trained experts without sacrificing annotation quality

## Next Checks
1. Replicate ESA evaluation on different language pairs (e.g., English→French, Chinese→English) to test cross-linguistic validity
2. Conduct a cost-benefit analysis comparing ESA's reduced expert requirements against potential accuracy trade-offs
3. Test ESA's sensitivity to different error severity threshold definitions (e.g., varying the 25-point cutoff between minor and major errors)