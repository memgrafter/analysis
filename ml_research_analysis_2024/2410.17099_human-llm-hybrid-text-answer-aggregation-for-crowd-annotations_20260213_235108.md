---
ver: rpa2
title: Human-LLM Hybrid Text Answer Aggregation for Crowd Annotations
arxiv_id: '2410.17099'
source_url: https://arxiv.org/abs/2410.17099
tags:
- crowd
- answers
- aggregators
- rasa
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the use of Large Language Models (LLMs)
  as aggregators in crowd text answer aggregation tasks for quality control. It proposes
  a human-LLM hybrid method within a Creator-Aggregator Multi-Stage (CAMS) crowdsourcing
  framework.
---

# Human-LLM Hybrid Text Answer Aggregation for Crowd Annotations

## Quick Facts
- arXiv ID: 2410.17099
- Source URL: https://arxiv.org/abs/2410.17099
- Reference count: 38
- Human-LLM hybrid aggregation improves crowdsourced text answer quality

## Executive Summary
This paper proposes a Creator-Aggregator Multi-Stage (CAMS) framework for crowd text answer aggregation that leverages both human crowd workers and Large Language Models (LLMs) as aggregators. The approach introduces LLM aggregators (GPT-4 and Gemini) alongside traditional crowd aggregators in a multi-stage quality control pipeline. Experiments on three public crowdsourcing datasets demonstrate that LLM aggregators outperform average crowd workers but not the best ones, while hybrid approaches combining human and LLM inputs achieve superior performance through complementary strengths.

## Method Summary
The CAMS framework operates in three stages: (1) Crowd creators provide raw text answers to questions, (2) Both crowd aggregators and LLM aggregators generate estimated answers using different methods (extractive or abstractive), and (3) Model aggregators (SMV, SMS, RASA) combine all answer sources to estimate true answers. The study evaluates three model aggregators: simple majority vote (SMV), similarity majority vote (SMS), and reliability-aware aggregation (RASA). Experiments use three public crowdsourcing datasets (J1, T1, T2 from CrowdWSA) with evaluation metrics including GLEU, METEOR, and embedding similarity scores.

## Key Results
- LLM aggregators (GPT-4, Gemini) outperform average crowd workers but not the best crowd workers
- Human-LLM hybrid aggregation further improves performance by combining diverse crowd answers with consistent LLM outputs
- RASA model aggregator, which accounts for worker reliability, performs best across multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model aggregators can amplify the influence of high-quality crowd workers when estimating true answers
- Mechanism: By leveraging worker reliability estimation and similarity-based selection, model aggregators identify and prioritize answers from more accurate contributors
- Core assumption: The quality distribution of crowd workers is heterogeneous, with some consistently outperforming others
- Evidence anchors: [abstract] "model aggregators can amplify the influence of good workers when estimating true answers", [section] "The dataset-wise model aggregator RASA that models worker reliability performs best in many cases"
- Break condition: If worker quality distribution becomes too uniform or if reliability estimation becomes unreliable due to insufficient data

### Mechanism 2
- Claim: Human-LLM hybrid aggregation outperforms either approach alone
- Mechanism: Crowd workers provide diverse, exploratory answers while LLMs offer consistent, high-quality baseline performance; their combination captures both exploration and exploitation benefits
- Core assumption: Low diversity in LLM-generated answers can be compensated by higher diversity in crowd answers
- Evidence anchors: [abstract] "The hybrid approach with combined human and LLM inputs further improves performance", [section] "although LLM aggregators perform better than the average of crowd workers...the answers provided by crowd workers contain valuable information"
- Break condition: If LLM diversity remains consistently low across tasks, or if crowd workers cannot provide complementary information

### Mechanism 3
- Claim: Multi-stage frameworks enable flexible quality control mechanisms
- Mechanism: By separating answer creation, aggregation, and final estimation into distinct stages, requesters can apply different quality control strategies at each step
- Core assumption: Single-stage frameworks are too rigid to implement diverse quality control mechanisms effectively
- Evidence anchors: [section] "Because single-stage frameworks only collect all annotations by crowd workers at once, which is limited for the requesters to flexibly set diverse additional mechanisms for improving the data quality, multi-stage frameworks are proposed", [section] "We propose a Creator-Aggregator Multi-Stage (CAMS) framework"
- Break condition: If stage separation introduces excessive latency or coordination overhead that outweighs quality benefits

## Foundational Learning

- Concept: Answer aggregation fundamentals
  - Why needed here: Understanding how to combine multiple noisy answers into a reliable estimate is central to the paper's contribution
  - Quick check question: What is the difference between extractive and abstractive aggregation methods, and why does this distinction matter for model aggregators vs. crowd/LLM aggregators?

- Concept: Worker reliability estimation
  - Why needed here: RASA's performance depends on accurately estimating worker reliability to weight their contributions
  - Quick check question: How does the chi-squared test in RASA's reliability estimation work, and what does the significance level α control?

- Concept: Text embedding similarity metrics
  - Why needed here: All model aggregators rely on text embeddings and similarity measures to compare answers
  - Quick check question: What are the trade-offs between using cosine similarity versus other distance metrics for text embeddings in aggregation tasks?

## Architecture Onboarding

- Component map: Crowd Creators -> Crowd Aggregators + LLM Aggregators -> Model Aggregators -> Evaluation
- Critical path: Creator → Aggregator(s) → Model Aggregator → Evaluation
- Design tradeoffs:
  - Single-stage vs. multi-stage: Flexibility vs. latency
  - Crowd-only vs. hybrid: Diversity vs. consistency
  - Number of LLM aggregators: Coverage vs. cost
- Failure signatures:
  - Poor model aggregator performance: Check if worker reliability estimation is working correctly
  - Low diversity in LLM outputs: Try different temperature parameters or multiple LLM providers
  - Hybrid approach not outperforming single-source: Verify that crowd and LLM answers are truly complementary
- First 3 experiments:
  1. Compare model aggregator performance (SMV, SMS, RASA) on crowd-only data to establish baseline
  2. Test individual LLM aggregator quality across different temperature settings
  3. Implement hybrid aggregation with different combinations of crowd and LLM inputs to find optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do other LLM models (e.g., open-source models) perform as aggregators in crowd text answer aggregation tasks?
- Basis in paper: [inferred] The paper only verified GPT-4 and Gemini-1.5-Pro, and suggests that the performance of other LLMs is an interesting issue for investigation
- Why unresolved: The paper only tested two commercial LLMs and did not explore the performance of other models
- What evidence would resolve it: Experiments using various LLM models, including open-source models, in the same aggregation task setup

### Open Question 2
- Question: What is the impact of using different LLM aggregators with varying levels of diversity on the quality of aggregated answers?
- Basis in paper: [explicit] The paper notes that answers generated by multiple LLM aggregators constructed by the same LLM have relatively low diversity, which could be a disadvantage
- Why unresolved: The paper suggests using different LLMs and temperatures to construct many LLM aggregators, but does not provide empirical evidence on the impact of diversity on answer quality
- What evidence would resolve it: Comparative experiments using LLM aggregators with varying levels of diversity and their impact on the quality of aggregated answers

### Open Question 3
- Question: How does the number of LLM aggregators affect the performance of human-LLM hybrid aggregation in different datasets?
- Basis in paper: [explicit] The paper investigates the influence of different numbers of LLM aggregators on performance and finds that the results vary across datasets
- Why unresolved: The paper does not provide a consistent pattern or guideline on the optimal number of LLM aggregators to use in different scenarios
- What evidence would resolve it: A comprehensive study across multiple datasets with varying numbers of LLM aggregators to identify patterns and optimal configurations

## Limitations
- Evaluation relies on relatively small public datasets (J1, T1, T2) from CrowdWSA, which may not capture full diversity of real-world scenarios
- Study does not explore cost-benefit trade-offs of using multiple LLM aggregators versus single ones
- Temperature parameter tuning for LLM aggregators uses fixed values that may not be optimal for different task types

## Confidence
- **High Confidence**: The core finding that LLM aggregators outperform average crowd workers but not the best ones is well-supported by experimental results across multiple datasets and evaluation metrics
- **Medium Confidence**: The claim that hybrid human-LLM aggregation provides additional benefits requires further validation, as the improvement appears dataset-dependent and the mechanism for complementary information is not fully characterized
- **Medium Confidence**: The assertion that multi-stage frameworks enable more flexible quality control mechanisms is theoretically sound but lacks empirical comparison against single-stage alternatives with equivalent quality controls

## Next Checks
1. **Dataset Generalization Test**: Evaluate the CAMS framework on additional crowdsourcing datasets from different domains to assess whether the observed performance patterns hold across varied task types and worker populations
2. **Cost-Effectiveness Analysis**: Measure the financial and temporal costs of different aggregation strategies (crowd-only, LLM-only, hybrid) to determine practical viability for large-scale deployment
3. **Temperature Sensitivity Analysis**: Conduct a systematic grid search over temperature parameters for LLM aggregators to identify optimal settings for different task characteristics and validate whether the chosen values (0.0, 0.6, 1.0) are truly representative