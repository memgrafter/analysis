---
ver: rpa2
title: 'Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy'
arxiv_id: '2407.06813'
source_url: https://arxiv.org/abs/2407.06813
tags:
- richelieu
- agent
- arxiv
- other
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents Richelieu, a self-evolving LLM-based agent
  designed for the complex multi-agent game of Diplomacy. The agent integrates four
  key capabilities: social reasoning for modeling intentions and relationships, strategic
  planning with memory-based reflection, goal-oriented negotiation with social reasoning,
  and self-evolution through self-play games without requiring human data.'
---

# Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy

## Quick Facts
- arXiv ID: 2407.06813
- Source URL: https://arxiv.org/abs/2407.06813
- Reference count: 40
- Key outcome: Richelieu outperforms state-of-the-art methods like Cicero in both no-press and press diplomacy settings, achieving a win rate of 6.7% compared to Cicero's 6.03%

## Executive Summary
This paper presents Richelieu, a self-evolving LLM-based agent designed for the complex multi-agent game of Diplomacy. The agent integrates four key capabilities: social reasoning for modeling intentions and relationships, strategic planning with memory-based reflection, goal-oriented negotiation with social reasoning, and self-evolution through self-play games without requiring human data. The framework addresses the challenge of long-term strategic planning and complex social dynamics in multi-agent settings. Experimental results demonstrate that Richelieu outperforms state-of-the-art methods like Cicero in both no-press and press diplomacy settings, achieving a win rate of 6.7% compared to Cicero's 6.03%.

## Method Summary
Richelieu is built as a self-evolving LLM-based agent that leverages self-play games to improve its performance over time without requiring human data. The framework integrates four key capabilities: social reasoning for modeling intentions and relationships, strategic planning with memory-based reflection, goal-oriented negotiation with social reasoning, and self-evolution through self-play games. The agent uses LLMs like GPT-4.0, Llama 3, ERNIE Bot, and Spark Desk to process game states and make decisions. The self-play mechanism allows the agent to accumulate diverse experiences that are stored in memory and used to refine sub-goals through reflection, improving strategic planning over time.

## Key Results
- Richelieu achieves a win rate of 6.7% compared to Cicero's 6.03% in no-press diplomacy settings
- The agent generalizes effectively across different LLMs including GPT-4.0, Llama 3, ERNIE Bot, and Spark Desk
- Comprehensive ablation studies confirm the contribution of each module to overall performance improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Richelieu's self-evolution through self-play games enables continuous improvement without human data.
- Mechanism: By running multiple self-play games where Richelieu controls all seven countries, the agent accumulates diverse experiences that are stored in memory. These experiences are then used to refine sub-goals through reflection, improving strategic planning over time.
- Core assumption: Self-play generates sufficiently diverse and representative experiences to drive meaningful learning improvements.
- Evidence anchors:
  - [abstract]: "self-evolution through self-play games without requiring human data"
  - [section]: "Self-play allows the agent to accumulate more experiences for self-evolution... Richelieu's win rate exhibited a steady increase with accumulated training iterations"
  - [corpus]: Weak evidence - corpus neighbors discuss similar self-play concepts but don't specifically validate Richelieu's approach
- Break condition: If self-play experiences become too homogeneous, learning plateaus and improvement stops.

### Mechanism 2
- Claim: The social reasoning module accurately models intentions and relationships between players to inform strategic decisions.
- Mechanism: Richelieu analyzes the game state to infer each player's intentions and relationships, creating a "social belief" that guides sub-goal formation and negotiation strategies. This includes identifying allies, enemies, and potential threats.
- Core assumption: LLMs can effectively reason about complex social dynamics and player intentions from partial game state information.
- Evidence anchors:
  - [abstract]: "social reasoning for modeling intentions and relationships"
  - [section]: "Social Reasoning: There are no permanent enemies, no permanent allies... Richelieu evaluates its relations with others"
  - [corpus]: Moderate evidence - corpus includes papers on LLMs for social reasoning and negotiation tactics
- Break condition: If the social belief inference becomes inaccurate, Richelieu makes poor strategic alliances and negotiation decisions.

### Mechanism 3
- Claim: The reflection mechanism with memory improves long-term strategic planning by learning from past experiences.
- Mechanism: Richelieu uses similarity-based functions to retrieve relevant historical experiences from memory when forming sub-goals. Experiences with high evaluation scores reinforce successful strategies while lower scores trigger adjustments.
- Core assumption: Past experiences can be meaningfully compared to current situations to provide actionable strategic insights.
- Evidence anchors:
  - [abstract]: "strategic planning with memory-based reflection"
  - [section]: "Reflection with Memory: We further develop a reflection mechanism to enhance the rationality and effectiveness of our agent's sub-goals... Richelieu's reflection abilities improve"
  - [corpus]: Weak evidence - corpus neighbors discuss memory mechanisms but don't specifically validate reflection for strategic planning
- Break condition: If memory retrieval becomes too similar across experiences, Richelieu loses the diversity needed for meaningful reflection.

## Foundational Learning

- Concept: Multi-agent game theory and negotiation dynamics
  - Why needed here: Richelieu must understand how multiple agents interact, form alliances, and compete for resources in the complex Diplomacy environment
  - Quick check question: Can you explain the difference between zero-sum and non-zero-sum games and why Diplomacy falls into the latter category?

- Concept: Reinforcement learning and self-play training
  - Why needed here: Richelieu's self-evolution mechanism relies on self-play to generate training experiences without human data, requiring understanding of how agents can learn from their own gameplay
  - Quick check question: How does self-play differ from traditional supervised learning in terms of data generation and agent improvement?

- Concept: Memory systems and experience replay
  - Why needed here: Richelieu's memory module stores and retrieves experiences for reflection and strategic planning, requiring knowledge of how to effectively manage and utilize historical data
  - Quick check question: What are the key differences between short-term memory (like credibility scores) and long-term memory (like stored game experiences) in Richelieu's architecture?

## Architecture Onboarding

- Component map: Social Reasoning → Planner with Reflection → Negotiator → Actor → Memory Management → Self-Evolution
- Critical path: game state input → Social Reasoning (to build social belief) → Planner with Reflection (to set sub-goals) → Negotiator (to communicate and form alliances) → Actor (to take actions) → Memory Management (to store experience) → Self-Evolution (to improve future performance)
- Design tradeoffs: Richelieu trades computational complexity for improved performance by using multiple sophisticated LLM calls across different modules, versus simpler single-agent approaches. The self-play mechanism trades training time for the ability to learn without human data.
- Failure signatures: Common failure modes include: social belief becoming inaccurate (leading to poor alliances), reflection mechanism retrieving irrelevant experiences (causing suboptimal planning), or self-play generating insufficient diversity (resulting in learning plateaus).
- First 3 experiments:
  1. Run a single turn with all modules enabled to verify the information flow and component integration
  2. Test the social reasoning module in isolation with known game states to validate intention and relationship inference accuracy
  3. Evaluate the reflection mechanism by comparing sub-goal quality with and without memory retrieval on identical game states

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Richelieu's performance compare against other LLM-based agents in Diplomacy, such as Cicero, when considering different LLMs like GPT-4.0, Llama 3, ERNIE Bot, and Spark Desk?
- Basis in paper: [explicit] The paper mentions that Richelieu outperforms state-of-the-art methods like Cicero and generalizes effectively across different LLMs.
- Why unresolved: The paper provides some comparative data but does not fully explore the performance differences across various LLMs in detail.
- What evidence would resolve it: Comprehensive experimental results comparing Richelieu's performance against other LLM-based agents using different LLMs.

### Open Question 2
- Question: What are the specific challenges and limitations of applying Richelieu's framework to real-world diplomatic scenarios, especially considering the vast decision-making space and incomplete information?
- Basis in paper: [inferred] The paper acknowledges that Diplomacy has a constrained action space compared to real-world diplomacy and mentions the need for a more realistic game space in future work.
- Why unresolved: The paper does not delve into the specific challenges of applying the framework to real-world scenarios.
- What evidence would resolve it: Detailed analysis of the challenges and limitations of applying Richelieu's framework to real-world diplomatic scenarios, including handling incomplete information and vast decision spaces.

### Open Question 3
- Question: How does the self-evolution mechanism in Richelieu adapt to different player strategies over time, and what are the implications for the agent's long-term performance?
- Basis in paper: [explicit] The paper discusses the self-evolution mechanism through self-play games and mentions that Richelieu's performance improves with training iterations.
- Why unresolved: The paper does not provide a detailed analysis of how the self-evolution mechanism adapts to different strategies or its long-term implications.
- What evidence would resolve it: Longitudinal studies showing how Richelieu's performance changes with exposure to different player strategies and the impact on long-term performance.

## Limitations

- The self-evolution mechanism's ability to continuously improve without human data lacks sufficient empirical validation for extended periods
- Comparison against Cicero uses different evaluation settings (C-Diplo vs. No-Press metrics), making direct performance comparison potentially misleading
- The framework doesn't provide detailed ablations showing how each LLM's characteristics affect performance across different models

## Confidence

**High Confidence**: The integration of social reasoning, strategic planning with reflection, and negotiation capabilities is well-documented and the ablation studies provide strong evidence for individual module contributions to performance.

**Medium Confidence**: The claim of outperforming state-of-the-art methods is supported by experimental results, but the evaluation methodology differences and limited comparison details reduce confidence in the absolute performance claims.

**Low Confidence**: The self-evolution mechanism's ability to continuously improve without human data is theoretically sound but lacks sufficient empirical validation - the paper shows win rate improvements over iterations but doesn't demonstrate sustained learning over extended periods or robustness to different starting conditions.

## Next Checks

1. **Self-Play Diversity Validation**: Run extended self-play experiments to verify that Richelieu maintains diverse experience generation over 100+ games, checking for learning plateaus or homogeneous strategy development that would indicate insufficient exploration.

2. **Cross-LLM Performance Consistency**: Implement the complete Richelieu framework across all four LLMs (GPT-4.0, Llama 3, ERNIE Bot, Spark Desk) and compare not just win rates but also specific behaviors like negotiation patterns and alliance formation to validate true model-agnostic performance.

3. **Robustness to Pathological Scenarios**: Design test cases with intentionally misleading information, unexpected alliance formations, and unusual game states to evaluate how well Richelieu's social reasoning and reflection mechanisms handle edge cases and maintain performance under stress conditions.