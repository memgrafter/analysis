---
ver: rpa2
title: Spherical Linear Interpolation and Text-Anchoring for Zero-shot Composed Image
  Retrieval
arxiv_id: '2405.00571'
source_url: https://arxiv.org/abs/2405.00571
tags:
- image
- text
- retrieval
- slerp
- rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new zero-shot composed image retrieval (ZS-CIR)
  method that uses spherical linear interpolation (Slerp) to directly merge image
  and text representations, avoiding the limitations of previous pseudo-word token-based
  approaches. To improve performance, the authors introduce a text-anchored-tuning
  (TAT) strategy that fine-tunes the image encoder while keeping the text encoder
  fixed, reducing the modality gap between images and text.
---

# Spherical Linear Interpolation and Text-Anchoring for Zero-shot Composed Image Retrieval

## Quick Facts
- arXiv ID: 2405.00571
- Source URL: https://arxiv.org/abs/2405.00571
- Reference count: 33
- Primary result: State-of-the-art zero-shot composed image retrieval using spherical linear interpolation and text-anchored tuning

## Executive Summary
This paper addresses zero-shot composed image retrieval (ZS-CIR) by proposing a method that uses spherical linear interpolation (Slerp) to directly merge image and text embeddings from vision-language models. Unlike previous approaches that rely on pseudo-word tokens, this method finds an intermediate embedding on a hypersphere between image and text representations. To enhance performance, the authors introduce Text-Anchored-Tuning (TAT), which fine-tunes the image encoder while keeping the text encoder fixed, effectively reducing the modality gap. The proposed approach achieves state-of-the-art results on multiple CIR benchmarks while requiring less training data and time than previous methods.

## Method Summary
The method combines spherical linear interpolation with a text-anchored tuning strategy. Slerp directly merges normalized image and text embeddings from pre-trained vision-language models by finding an intermediate point on the great circle arc between them. The interpolation ratio α is set to favor text embeddings (α ≥ 0.8) since text-only queries perform better for CIR. For text-anchored tuning, LoRA parameters are applied to the image encoder while the text encoder remains frozen, using contrastive loss to align image embeddings closer to text embeddings. This approach reduces the modality gap and improves retrieval performance. The method is evaluated on CIRR, CIRCO, and FashionIQ benchmarks, showing significant improvements over previous ZS-CIR approaches.

## Key Results
- Achieves state-of-the-art performance on CIRR, CIRCO, and FashionIQ benchmarks for zero-shot composed image retrieval
- Outperforms previous pseudo-token based methods while requiring less training data and time
- TAT-tuned models serve as excellent initial checkpoints for supervised CIR models, demonstrating wider applicability
- Text weighting (α ≥ 0.8) in Slerp provides significant performance gains by leveraging the stronger signal from text queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Slerp directly merges image and text embeddings on a shared hypersphere, avoiding the distortion from pseudo-token projection.
- Mechanism: VLP encoders produce normalized embeddings on a hypersphere; Slerp finds an intermediate point along the great circle arc between image (v) and text (w) embeddings.
- Core assumption: The intermediate point on the hypersphere represents a meaningful composition of the two modalities.
- Evidence anchors:
  - [abstract] "we introduce a novel ZS-CIR method that uses Spherical Linear Interpolation (Slerp) to directly merge image and text representations by identifying an intermediate embedding of both."
  - [section 3.2] "Slerp can be applied to find an intermediate embedding of both image and text ones"
  - [corpus] Weak: no explicit corpus evidence; this is a novel architectural claim
- Break condition: If the hypersphere assumption fails (e.g., embeddings are not truly normalized or distributed on a shared manifold), the interpolation will not produce semantically meaningful compositions.

### Mechanism 2
- Claim: Text-anchored tuning (TAT) aligns image embeddings closer to text embeddings, reducing the modality gap.
- Mechanism: LoRA is applied to the image encoder while keeping the text encoder frozen; contrastive loss encourages image embeddings to follow text embeddings.
- Core assumption: The text encoder's embeddings are already well-aligned with the retrieval task, so aligning images to them improves retrieval.
- Evidence anchors:
  - [abstract] "we introduce Text-Anchored-Tuning (TAT), a method that fine-tunes the image encoder while keeping the text encoder fixed. TAT closes the modality gap between images and text, making the Slerp process much more effective."
  - [section 3.3] "we apply LoRA parameters to the VLP image encoder, which not only preserves the original knowledge of the image encoder but also effectively redistributes image embeddings to align more closely with the corresponding text embeddings."
  - [corpus] Weak: no direct corpus evidence; this is a novel training strategy
- Break condition: If the text encoder's embeddings are not optimal for the retrieval task, aligning images to them may degrade performance.

### Mechanism 3
- Claim: Weighting the text embedding more heavily in Slerp (α ≥ 0.8) improves performance because text plays a more significant role in CIR.
- Mechanism: The α parameter in Slerp controls the interpolation ratio; higher α gives more weight to text.
- Core assumption: The text query is more informative than the image query for CIR.
- Evidence anchors:
  - [abstract] "we assign more weight (α ≥ 0.8; text-weighted) to w than to v when constructing c, which leads to a significant performance gain in ZS-CIR evaluation protocols."
  - [section 3.2] "using a text-only query yields better ZS-CIR performance than an image-only query for both natural and fashion image dataset benchmarks"
  - [corpus] Weak: this observation is from the paper itself, not external corpus
- Break condition: If the image query becomes more important for a specific dataset or task, this weighting may be suboptimal.

## Foundational Learning

- Concept: Cosine similarity and hypersphere embeddings
  - Why needed here: VLP models use cosine similarity; understanding that embeddings lie on a hypersphere is crucial for Slerp.
  - Quick check question: What geometric space do normalized embeddings of a VLP model lie on?

- Concept: Contrastive learning and temperature scaling
  - Why needed here: The contrastive loss with temperature τ is what creates the hypersphere distribution; understanding this helps explain why Slerp works.
  - Quick check question: What is the purpose of the temperature parameter τ in the contrastive loss?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: TAT uses LoRA to efficiently fine-tune the image encoder while preserving the text encoder.
  - Quick check question: How does LoRA differ from full fine-tuning in terms of parameter count and training efficiency?

## Architecture Onboarding

- Component map: Image encoder (EI) -> Text encoder (ET) -> Slerp interpolation -> Retrieval gallery
- Critical path: Query → EI(xq) + ET(tq) → Slerp(vq, wq; α) → Cosine similarity with gallery → Ranked results
- Design tradeoffs: Slerp vs pseudo-token methods (simplicity vs flexibility); TAT vs full fine-tuning (efficiency vs expressiveness)
- Failure signatures: Poor retrieval if modality gap is too large; Slerp fails if embeddings are not on a shared manifold
- First 3 experiments:
  1. Verify that embeddings from EI and ET are normalized and lie on a hypersphere (check cosine similarity distribution)
  2. Test Slerp with different α values on a validation set to find the optimal text weighting
  3. Compare TAT (LoRA on EI only) vs full fine-tuning vs no fine-tuning on a held-out set to confirm modality gap reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of Slerp-based ZS-CIR change when using different VLP models (e.g., CLIP, BLIP) with varying sizes and architectures?
- Basis in paper: [explicit] The paper compares Slerp-based ZS-CIR using different VLP models like CLIP and BLIP, showing performance differences.
- Why unresolved: The paper provides some comparisons but does not explore the full range of VLP models or conduct a systematic analysis of how model size and architecture impact performance.
- What evidence would resolve it: A comprehensive study comparing Slerp-based ZS-CIR across various VLP models, including different sizes and architectures, with detailed analysis of performance variations.

### Open Question 2
- Question: What is the impact of the choice of training dataset size and composition on the performance of TAT-tuned models for ZS-CIR?
- Basis in paper: [explicit] The paper mentions that TAT-trained models can achieve decent performance even with fewer training pairs and a single epoch of training.
- Why unresolved: While the paper demonstrates the effectiveness of TAT with a subset of CC3M, it does not provide a detailed analysis of how different training dataset sizes and compositions affect the final performance.
- What evidence would resolve it: Experiments varying the size and composition of training datasets used for TAT, with detailed performance analysis and comparison to models trained on larger datasets.

### Open Question 3
- Question: How does the Slerp-based ZS-CIR method perform on composed retrieval tasks where the query and gallery are both composed of image and text samples?
- Basis in paper: [inferred] The paper mentions that ZS-CIR methods, including Slerp-based approaches, have not been demonstrated for composed retrieval tasks where both the query and gallery are composed of image and text samples.
- Why unresolved: The paper focuses on standard CIR benchmarks but does not explore more complex composed retrieval scenarios involving both image and text in the query and gallery.
- What evidence would resolve it: Experiments applying Slerp-based ZS-CIR to composed retrieval tasks with image-text queries and image-text galleries, evaluating performance on these more complex scenarios.

## Limitations
- Relies heavily on the assumption that embeddings lie on a shared hypersphere, which may not hold for all VLP models or datasets
- The effectiveness of TAT depends on the pre-trained text encoder being optimally aligned with retrieval tasks, limiting generalization
- LoRA-based fine-tuning may not capture complex cross-modal interactions as effectively as full fine-tuning in some scenarios

## Confidence

- **High Confidence:** Experimental results showing state-of-the-art performance on CIR benchmarks are well-documented and reproducible
- **Medium Confidence:** Theoretical explanation for hyperspherical interpolation is plausible but lacks rigorous mathematical grounding
- **Low Confidence:** Generalization claims beyond tested datasets are not fully validated; no exploration of failure cases

## Next Checks

1. **Hypersphere Assumption Verification:** Conduct systematic analysis of embedding distributions to confirm that image and text embeddings from pre-trained VLP models actually lie on a shared hypersphere with uniform distribution

2. **Modality Gap Quantification:** Design experiments to measure the modality gap before and after TAT fine-tuning using quantitative metrics (e.g., average cosine similarity between corresponding image-text pairs, t-SNE visualization of embedding spaces)

3. **Cross-Dataset Generalization:** Evaluate TAT-tuned checkpoints on out-of-distribution CIR tasks (e.g., medical imaging, satellite imagery, or multilingual queries) to assess whether reduced modality gap generalizes beyond natural and fashion images