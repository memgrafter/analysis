---
ver: rpa2
title: Improving Retrieval-Augmented Generation in Medicine with Iterative Follow-up
  Questions
arxiv_id: '2408.00727'
source_url: https://arxiv.org/abs/2408.00727
tags:
- medical
- i-medrag
- queries
- question
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces iterative RAG for medicine (i-MedRAG), a framework
  that enhances medical question answering by enabling large language models to iteratively
  ask follow-up queries based on previous information-seeking attempts. Unlike conventional
  RAG, which performs a single round of retrieval, i-MedRAG prompts models to dynamically
  generate and answer follow-up queries, improving their ability to handle complex
  clinical reasoning tasks.
---

# Improving Retrieval-Augmented Generation in Medicine with Iterative Follow-up Questions

## Quick Facts
- arXiv ID: 2408.00727
- Source URL: https://arxiv.org/abs/2408.00727
- Reference count: 40
- i-MedRAG achieves 69.68% accuracy on MedQA with GPT-3.5, setting a new state-of-the-art performance

## Executive Summary
This paper introduces i-MedRAG, a framework that enhances medical question answering by enabling large language models to iteratively ask follow-up queries based on previous information-seeking attempts. Unlike conventional RAG which performs a single round of retrieval, i-MedRAG prompts models to dynamically generate and answer follow-up queries, improving their ability to handle complex clinical reasoning tasks. Experiments show that i-MedRAG significantly outperforms conventional RAG and other prompt engineering methods on the MedQA dataset, achieving 69.68% accuracy with GPT-3.5, setting a new state-of-the-art performance. The approach is also generalizable to other LLMs and medical datasets, demonstrating consistent improvements in accuracy.

## Method Summary
The i-MedRAG framework implements a zero-shot iterative RAG approach that prompts LLMs to iteratively generate follow-up queries based on previous information-seeking attempts. The method uses RAG to answer each query and incorporates query-answer pairs into subsequent iterations. The framework was evaluated on the MedQA dataset (USMLE clinical vignettes) and MMLU dataset (medical subtasks), using MedCPT as the text retriever and Textbooks/Statpearls corpora. The approach enables LLMs to dynamically refine search terms through reasoning, creating a feedback loop where retrieved content informs better queries, ultimately forming reasoning chains to answer complex medical questions.

## Key Results
- i-MedRAG achieves 69.68% accuracy on MedQA with GPT-3.5, outperforming conventional RAG and other baseline methods
- The framework demonstrates generalizability across different LLMs including Llama-3.1-8B with consistent accuracy improvements
- Performance scales with the number of iterations and queries per iteration, with case studies showing how i-MedRAG forms reasoning chains to overcome conventional RAG limitations

## Why This Works (Mechanism)

### Mechanism 1
Iterative query generation improves performance by addressing information needs that single-step retrieval cannot capture. The LLM dynamically refines search terms through reasoning, creating a feedback loop where retrieved content informs better queries. This breaks down when the LLM cannot formulate meaningful follow-up queries or when retrieved documents are irrelevant to follow-up questions.

### Mechanism 2
Including query-answer pairs in the context enables reasoning chains rather than isolated lookup. The LLM treats each (query, answer) as a step in a reasoning chain, allowing multi-hop inference across multiple retrieval steps. This fails when the LLM cannot maintain coherent reasoning chains over multiple steps or when retrieval results do not align with the evolving query context.

### Mechanism 3
Scaling the number of queries per iteration and the number of iterations improves accuracy up to a point. More queries increase the breadth of information collected while more iterations increase the depth of reasoning, both improving final answer quality. This breaks down when queries become redundant or off-topic, when the LLM context window becomes saturated, or when diminishing returns set in.

## Foundational Learning

- Concept: Chain-of-thought reasoning in LLMs
  - Why needed here: Iterative RAG depends on the LLM's ability to generate coherent reasoning steps; CoT improves this capability.
  - Quick check question: Can you describe how CoT prompting improves LLM performance on multi-step reasoning tasks?

- Concept: Text retrieval and ranking in medical corpora
  - Why needed here: RAG systems depend on accurate retrieval of relevant documents; understanding retrieval mechanics is key to tuning i-MedRAG.
  - Quick check question: What retrieval metrics (e.g., recall, precision) would you monitor to evaluate RAG effectiveness in medical QA?

- Concept: Multi-hop question answering
  - Why needed here: Iterative RAG is essentially a multi-hop QA system; understanding multi-hop methods helps in query design.
  - Quick check question: How does multi-hop QA differ from single-hop QA in terms of required reasoning steps?

## Architecture Onboarding

- Component map: Question -> Query generator -> Retriever -> RAG answerer -> Context builder -> Final answerer
- Critical path: Question → Query generation → Retrieval → Answer generation → Context augmentation → Final answer
- Design tradeoffs:
  - Query quantity vs. cost: More queries increase accuracy but linearly increase inference cost
  - Iteration depth vs. diminishing returns: Deeper iterations improve reasoning but risk context overload
  - Retriever recall vs. precision: High recall may include irrelevant docs; high precision may miss critical info
- Failure signatures:
  - Queries drift from the original question
  - Retrieved documents are irrelevant or repetitive
  - LLM context window saturates before completion
  - Final answers contain hallucinated facts despite retrieval
- First 3 experiments:
  1. Vary number of iterations (1 vs. 3 vs. 5) on MedQA validation set; measure accuracy vs. cost
  2. Vary number of queries per iteration (1 vs. 3 vs. 5); measure accuracy vs. cost
  3. Compare i-MedRAG vs. MedRAG on a subset of MedQA questions requiring multi-hop reasoning

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several key areas remain unexplored based on the analysis of the work. The framework's performance scaling properties across different medical domains need systematic investigation, particularly how iteration and query configurations affect various specialties. The optimal strategy for automatically determining when to stop generating follow-up queries remains unclear, as the current approach treats iteration counts as fixed hyperparameters. Additionally, the trade-off between i-MedRAG's flexibility and fine-tuned models' performance in few-shot learning scenarios has not been explored, leaving uncertainty about when each approach is most appropriate.

## Limitations

- The study relies on zero-shot prompting without fine-tuning, raising questions about robustness across different medical subdomains and question types
- Analysis of scaling properties is based on limited experimental data, and the point of diminishing returns is not clearly established
- Framework's dependence on retrieval quality means performance could degrade significantly if underlying medical corpora contain gaps or biases

## Confidence

- High confidence: The basic mechanism of iterative query generation improving medical QA accuracy, supported by quantitative results on MedQA and MMLU-Med datasets
- Medium confidence: The claim about forming reasoning chains through iterative queries, primarily supported by qualitative case studies
- Medium confidence: The generalizability to other LLMs and datasets, based on limited experiments with Llama-3.1-8B and MMLU-Med

## Next Checks

1. Conduct ablation studies to quantify the individual contribution of each iteration and each query per iteration to overall performance, identifying the optimal balance between accuracy gains and computational cost.

2. Test the framework on a broader range of medical question types, including those requiring temporal reasoning, probabilistic diagnosis, and treatment planning, to assess robustness across clinical reasoning dimensions.

3. Evaluate the framework's performance when retrieval quality is deliberately degraded (e.g., by removing key documents from the corpus) to understand the dependency on retrieval effectiveness and identify potential failure modes.