---
ver: rpa2
title: Resolving Discrepancies in Compute-Optimal Scaling of Language Models
arxiv_id: '2406.19146'
source_url: https://arxiv.org/abs/2406.19146
tags:
- size
- loss
- scaling
- learning
- compute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work resolves a long-standing discrepancy between the compute-optimal
  scaling laws of Kaplan et al. and Hoffmann et al.
---

# Resolving Discrepancies in Compute-Optimal Scaling of Language Models

## Quick Facts
- arXiv ID: 2406.19146
- Source URL: https://arxiv.org/abs/2406.19146
- Reference count: 40
- Primary result: Resolves Kaplan-Hoffmann discrepancy by identifying three key factors: model head FLOPs, warmup period, and batch size scaling

## Executive Summary
This paper resolves a long-standing discrepancy between the compute-optimal scaling laws of Kaplan et al. and Hoffmann et al. for language models. By systematically reproducing the Kaplan et al. scaling law and investigating differences, the authors identify three key factors contributing to the discrepancy: not accounting for model head computational cost, using a warmup period that is too long for smaller models, and using a fixed batch size rather than scaling it with model size. After correcting for these factors, they obtain excellent agreement with the Hoffmann et al. scaling law. Notably, they find that careful learning rate decay is not essential for the validity of the Hoffmann et al. scaling law.

## Method Summary
The authors reproduce Kaplan et al.'s compute-optimal scaling law using the OpenLM library, training 16 models ranging from 5M to 901M parameters on OpenWebText2 and RefinedWeb datasets. They systematically identify and correct three factors: (1) include model head FLOPs in computational cost calculations, (2) adjust warmup period to scale with model size, and (3) scale batch size with model size. They then derive scaling laws for optimal learning rate and batch size, finding that tuning the AdamW β2 parameter is crucial at smaller batch sizes. The noise-and-interpolate procedure is used to extract optimal model sizes for each compute budget.

## Key Results
- Kaplan et al. scaling law exponent a=0.50 changes to a=0.54 when accounting for model head FLOPs
- Correcting warmup period and batch size scaling brings exponent to a=0.50, matching Hoffmann et al.
- Optimal batch size scales as B*(N) ∝ N^0.68
- Optimal learning rate scales as lr*(N) ∝ N^-0.07
- AdamW β2 parameter critically affects performance at small batch sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Not accounting for the computational cost of the model head leads to overestimating the optimal model size for a given compute budget.
- Mechanism: Kaplan et al. defined model size without counting the contribution of the model head to the FLOPs per token. This led to under-approximation of the computational cost, especially for smaller models. Consequently, the optimal number of tokens for smaller models was inflated, resulting in a higher exponent in the scaling law.
- Core assumption: The model head contributes a non-negligible amount of FLOPs per token, and this contribution varies with model size.
- Evidence anchors:
  - [abstract]: "not accounting for the computational cost of the model head (last layer)"
  - [section]: "Not accounting for the model head leads to under-approximation that grows smoothly as model size decreases, from roughly 10% at larger models to roughly 90% at smaller models."
  - [corpus]: Weak - corpus evidence only shows that this is a recognized factor in reconciling the scaling laws, but does not provide direct experimental evidence.
- Break condition: If the model head's contribution to FLOPs becomes negligible compared to other operations, or if the model head's size becomes constant regardless of the overall model size.

### Mechanism 2
- Claim: Using a warmup period that is too long for smaller models inflates the optimal number of tokens at lower compute budgets.
- Mechanism: Kaplan et al. used a constant-length warmup period, which is too long for smaller models. This forces smaller models to use more training tokens than would otherwise be optimal to "escape" the long warmup period. Once the warmup period is over, the optimal number of tokens grows only slowly, leading to a fast rate of increase in the optimal model size and hence a large exponent.
- Core assumption: The optimal warmup period should be proportional to the model size.
- Evidence anchors:
  - [abstract]: "using a warmup period that is too long for smaller models"
  - [section]: "Figure 2 (left) shows this warmup period is too long: for smaller-scale models, the optimal number of tokens as a function of compute is less than or close to the number of warmup tokens, and therefore these models are suboptimally trained."
  - [corpus]: Weak - corpus evidence only shows that this is a recognized factor in reconciling the scaling laws, but does not provide direct experimental evidence.
- Break condition: If the warmup period is reduced to a point where it no longer significantly impacts the optimal number of tokens for smaller models.

### Mechanism 3
- Claim: Using a fixed batch size rather than scaling it with model size leads to suboptimal performance at smaller scales.
- Mechanism: With a fixed batch size, compute-optimal models at smaller scales train for only a few hundred steps, which is likely too little. This is because smaller models require more frequent updates to learn effectively, and a fixed batch size limits the number of updates per training epoch.
- Core assumption: The optimal batch size should be proportional to the model size.
- Evidence anchors:
  - [abstract]: "using a fixed batch size rather than scaling it with model size"
  - [section]: "With a fixed batch size of 219 tokens, compute-optimal models at smaller scales train for only a few hundred steps, which is likely too little."
  - [corpus]: Weak - corpus evidence only shows that this is a recognized factor in reconciling the scaling laws, but does not provide direct experimental evidence.
- Break condition: If the batch size is scaled appropriately with model size, or if the model becomes large enough that a fixed batch size provides sufficient updates.

## Foundational Learning

- Concept: Compute-optimal scaling laws
  - Why needed here: The paper is about resolving discrepancies in compute-optimal scaling laws, so a deep understanding of what these laws are and how they are derived is essential.
  - Quick check question: What is the difference between the compute-optimal scaling laws proposed by Kaplan et al. [30] and Hoffmann et al. [25]?

- Concept: Language model architecture
  - Why needed here: The paper discusses how different architectural choices, such as the model head and attention layers, affect the computational cost and optimal scaling behavior.
  - Quick check question: How do the model head and attention layers contribute to the overall computational cost of a language model?

- Concept: Optimization hyperparameters
  - Why needed here: The paper shows that tuning hyperparameters like batch size, learning rate, and AdamW β2 parameter is crucial for achieving the Hoffmann et al. scaling law.
  - Quick check question: How do batch size, learning rate, and AdamW β2 parameter affect the training dynamics and final performance of a language model?

## Architecture Onboarding

- Component map: Decoder-only Transformer language models -> OpenLM library -> AdamW optimizer with independent weight decay -> SwiGLU activation function -> Layer normalization -> Rotary positional embeddings

- Critical path: 1. Reproduce Kaplan et al. scaling law 2. Identify and correct factors contributing to discrepancy 3. Validate results on multiple datasets 4. Derive scaling laws for optimal hyperparameters

- Design tradeoffs: Model size vs. training tokens, Computational cost vs. performance, Hyperparameter tuning time vs. final performance

- Failure signatures: Incorrect model size definition leading to overestimation of optimal model size, Long warmup period leading to suboptimal performance for smaller models, Fixed batch size leading to insufficient training for smaller models

- First 3 experiments: 1. Reproduce Kaplan et al. scaling law using the same hyperparameters and model size definition 2. Correct the model size definition by including the computational cost of the model head 3. Adjust the warmup period to be proportional to the model size

## Open Questions the Paper Calls Out
None

## Limitations
- Evidence for individual mechanisms is primarily correlational rather than experimental
- Analysis limited to decoder-only Transformer models with specific architectural choices
- Claim about learning rate decay not being essential relies on observational evidence rather than ablation studies

## Confidence

**High Confidence:** The identification of the model head FLOPs contribution as a key factor - this is a clear definitional issue with direct mathematical implications for the scaling law exponent. The correction method is straightforward and the resulting agreement with Hoffmann et al. is strong.

**Medium Confidence:** The warmup period and batch size factors - while these are plausible contributors to the discrepancy and their correction produces the expected results, the evidence is more circumstantial. The authors show these factors lead to suboptimal training but don't fully isolate their individual contributions to the scaling law differences.

**Low Confidence:** The claim about learning rate decay not being essential - this conclusion is based on observational evidence from their training setup rather than systematic experimentation across different learning rate schedules. The paper doesn't explore whether alternative decay schedules might produce even better agreement or performance.

## Next Checks
1. **Ablation Study on Individual Factors:** Design controlled experiments that isolate each of the three identified factors (model head FLOPs, warmup period, batch size scaling) by systematically varying one factor at a time while keeping others constant. This would quantify the individual contribution of each factor to the scaling law discrepancy and validate whether all three are truly necessary for agreement with Hoffmann et al.

2. **Cross-Architecture Validation:** Test whether the same three factors explain scaling law discrepancies in different model architectures (encoder-decoder models, convolutional models, or models with different attention mechanisms). This would validate whether the findings generalize beyond decoder-only Transformers with specific architectural choices.

3. **Learning Rate Schedule Exploration:** Conduct a systematic study of different learning rate schedules (linear warmup with linear decay, cosine decay, step decay) across the full range of model sizes to determine whether the learning rate decay schedule actually matters for achieving the Hoffmann et al. scaling law, or if the current observation is an artifact of the specific training setup used.