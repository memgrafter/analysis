---
ver: rpa2
title: 'Hyacinth6B: A large language model for Traditional Chinese'
arxiv_id: '2403.13334'
source_url: https://arxiv.org/abs/2403.13334
tags:
- language
- chinese
- traditional
- hyacinth6b
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hyacinth6B is a 6B parameter Large Language Model designed for
  Traditional Chinese that achieves strong performance while using a lightweight architecture.
  It uses parameter-efficient fine-tuning with LoRA on top of the ChatGLM3-6B-base
  model, requiring only about 14GB of VRAM.
---

# Hyacinth6B: A large language model for Traditional Chinese

## Quick Facts
- **arXiv ID:** 2403.13334
- **Source URL:** https://arxiv.org/abs/2403.13334
- **Authors:** Chih-Wei Song; Yin-Te Tsai
- **Reference count:** 0
- **Primary result:** 6B parameter LLM achieving strong performance on Traditional Chinese benchmarks while requiring only ~14GB VRAM

## Executive Summary
Hyacinth6B is a 6B parameter Large Language Model specifically designed for Traditional Chinese language tasks. The model uses parameter-efficient fine-tuning with LoRA on top of the ChatGLM3-6B-base model, requiring only about 14GB of VRAM for operation. Trained on a Traditional Chinese instruction dataset (486K entries) for 3 epochs, Hyacinth6B demonstrates competitive performance against much larger models. The approach successfully balances model lightness with strong performance, addressing the high hardware and computational demands typically associated with LLMs.

## Method Summary
Hyacinth6B employs LoRA (Low-Rank Adaptation) fine-tuning on the ChatGLM3-6B-base model using a Traditional Chinese instruction dataset of 486K entries. The training setup includes batch size 8, gradient accumulation steps 4, learning rate 5e-5, and LoRA rank r=16, conducted over 3 epochs on a single RTX 4090 GPU for approximately 369 hours. The method achieves parameter efficiency by training only low-rank decomposition matrices while freezing the base model, enabling high performance with reduced computational requirements.

## Key Results
- Outperforms ChatGPT in multiple benchmarks (CMMLU, C-eval, and LLM-eval)
- Ranks third overall behind GPT-4 and XuanYuan13B
- Demonstrates particularly strong performance in social sciences and Traditional Chinese-specific tasks
- Achieves competitive results with only 6B parameters compared to larger models

## Why This Works (Mechanism)

### Mechanism 1
LoRA enables high-performance fine-tuning on limited hardware by training only low-rank decomposition matrices while freezing the base model. LoRA inserts small trainable matrices (A and B) into each transformer layer, approximating the original weight W0 as W0 + BA where BA has low rank (r << min(d_in, d_out)). Only BA is updated; W0 remains frozen, drastically reducing trainable parameters. Core assumption: The weight update during task adaptation lies in a low-rank subspace, so a small rank suffices to capture needed changes.

### Mechanism 2
Supervised fine-tuning on Traditional Chinese instruction data transfers base bilingual capabilities to Traditional Chinese-specific proficiency. Hyacinth6B fine-tunes ChatGLM3-base on a Traditional Chinese instruction dataset (486K entries). The instruction format teaches the model to interpret prompts and generate culturally aligned responses, leveraging the base model's general language understanding. Core assumption: The base model's multilingual representations include sufficient overlap with Traditional Chinese to be effectively adapted via instruction tuning.

### Mechanism 3
Small model size with efficient architecture yields competitive performance against larger models in Chinese-specific benchmarks. Hyacinth6B (6B params) uses ChatGLM3-base's optimized architecture and LoRA fine-tuning, achieving high scores in CMMLU and C-eval benchmarks while requiring only ~14GB VRAM. Core assumption: Architectural efficiency and high-quality instruction data compensate for fewer parameters relative to larger models like LLaMA 13B or XuanYuan13B.

## Foundational Learning

- **Concept: Low-rank matrix factorization**
  - Why needed here: LoRA relies on approximating weight updates with low-rank matrices (BA) to reduce trainable parameters.
  - Quick check question: If the rank r=8 and original weight shape is (512, 512), how many trainable parameters does LoRA add per layer?

- **Concept: Instruction fine-tuning format**
  - Why needed here: Hyacinth6B's performance depends on learning from user query / assistant response pairs in the instruction dataset.
  - Quick check question: In a single-turn instruction example, what are the two required components that the model must learn to map?

- **Concept: Traditional Chinese tokenization differences**
  - Why needed here: ChatGLM3-base is primarily Simplified Chinese; fine-tuning must bridge tokenization gaps for Traditional Chinese output.
  - Quick check question: Why might a model trained on Simplified Chinese produce incorrect characters when generating Traditional Chinese text?

## Architecture Onboarding

- **Component map:** ChatGLM3-6B-base (bilingual, 6B params) -> LoRA matrices (rank r=16, inserted per transformer layer) -> Traditional Chinese instructions (486K QA pairs) -> Training on RTX 4090

- **Critical path:** 1) Load pre-trained ChatGLM3-6B-base. 2) Insert LoRA adapters into all linear layers. 3) Freeze base model weights. 4) Load and preprocess Traditional Chinese instruction dataset. 5) Train LoRA matrices for 3 epochs. 6) Evaluate on Chinese benchmarks (CMMLU, C-eval, TC-eval).

- **Design tradeoffs:** LoRA rank r=16 balances parameter efficiency vs adaptation capacity; higher r improves performance but increases VRAM. 3 epochs chosen to balance convergence and overfitting risk on 486K samples. Batch size 8 with gradient accumulation allows training on single RTX 4090 within ~14GB VRAM.

- **Failure signatures:** Low validation scores with high training scores → overfitting to instruction data. Catastrophic forgetting of base capabilities → insufficient LoRA capacity or excessive training steps. Traditional Chinese generation errors → tokenizer or data quality issues.

- **First 3 experiments:** 1) Single-layer LoRA insertion with rank 4 on small dataset subset; verify memory usage and forward/backward pass. 2) Full model LoRA training with rank 16 for 1 epoch; monitor loss curves and check for catastrophic forgetting. 3) Benchmark evaluation on CMMLU 5-shot after 1 epoch; compare to baseline ChatGLM3-6B to measure instruction adaptation gain.

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal trade-off between model size and performance for Traditional Chinese LLMs when using parameter-efficient fine-tuning methods? The paper only evaluates one model size (6B parameters) and doesn't explore how performance scales with different model sizes or how the trade-off changes with varying computational resources.

### Open Question 2
How does Hyacinth6B's performance in STEM subjects compare to larger models when trained on an equivalent amount of STEM-specific data? The paper notes lower STEM scores may be attributed to smaller volume of training datasets in these areas but doesn't provide data on relative distribution or attempt to address this imbalance.

### Open Question 3
Would alternative parameter-efficient fine-tuning methods like P-tuning or prefix-tuning outperform LoRA for Traditional Chinese language tasks? The paper chose LoRA over P-Tuning without empirical comparison, making a theoretical comparison without validation on the same tasks and datasets.

## Limitations
- Evaluation relies heavily on benchmark comparisons without detailed ablation studies to isolate LoRA versus base architecture contributions
- Performance claims lack statistical significance testing between Hyacinth6B and competing models
- Traditional Chinese instruction dataset details are sparse with limited information about data quality, potential biases, or translation process

## Confidence

**High Confidence:**
- Hyacinth6B achieves strong performance in traditional Chinese language tasks
- The model requires only ~14GB VRAM for operation
- Hyacinth6B outperforms ChatGPT in multiple benchmarks (CMMLU, C-eval, LLM-eval)

**Medium Confidence:**
- Hyacinth6B ranks third overall behind GPT-4 and XuanYuan13B
- The model demonstrates particularly strong performance in social sciences
- LoRA fine-tuning effectively transfers bilingual capabilities to traditional Chinese proficiency

**Low Confidence:**
- The 6B parameter architecture fully compensates for parameter count compared to larger models
- The Traditional Chinese instruction dataset is sufficiently comprehensive and unbiased
- The 3-epoch training duration is optimal for model performance

## Next Checks

1. **Ablation Study on LoRA Parameters:** Conduct controlled experiments varying LoRA rank (r=8, r=16, r=32) and training epochs (1, 2, 3) to quantify the contribution of each parameter to final performance, measuring both computational efficiency and benchmark scores.

2. **Statistical Significance Testing:** Perform paired statistical tests (e.g., bootstrap confidence intervals) comparing Hyacinth6B against baseline models across individual benchmark tasks rather than aggregate scores, to identify which performance differences are statistically significant.

3. **Domain-Specific Performance Analysis:** Break down benchmark results by knowledge domain (STEM, humanities, social sciences) and traditional Chinese language complexity levels to identify specific areas of strength and weakness, particularly focusing on classical Chinese understanding and traditional Chinese cultural knowledge.