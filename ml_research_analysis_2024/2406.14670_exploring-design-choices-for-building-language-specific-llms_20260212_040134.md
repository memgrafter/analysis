---
ver: rpa2
title: Exploring Design Choices for Building Language-Specific LLMs
arxiv_id: '2406.14670'
source_url: https://arxiv.org/abs/2406.14670
tags:
- language
- performance
- tokens
- vocabulary
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how to adapt large language models to specific\
  \ languages by combining vocabulary extension with continued pretraining. Experiments\
  \ across seven base models and four diverse languages (Hindi, Turkish, Arabic, Tamil)\
  \ reveal that (1) a base model's initial performance does not predict its final\
  \ adapted performance\u2014monolingual models like LLaMA-2 can match multilingual\
  \ models after adaptation; (2) adding around 10K language-specific tokens significantly\
  \ improves tokenization efficiency to near-English levels, with diminishing returns\
  \ for larger vocabularies; (3) continued pretraining on 200M target-language tokens\
  \ helps models utilize new tokens effectively; (4) the simplest mean embedding initialization\
  \ strategy works as well as more complex approaches; and (5) optimal adaptation\
  \ strategies are highly language- and model-dependent, with LLaMA-2 offering a good\
  \ balance of efficiency and performance for low-resource languages."
---

# Exploring Design Choices for Building Language-Specific LLMs

## Quick Facts
- arXiv ID: 2406.14670
- Source URL: https://arxiv.org/abs/2406.14670
- Reference count: 23
- Key outcome: Monolingual models like LLaMA-2 can match multilingual models after vocabulary extension and continued pretraining for specific languages

## Executive Summary
This paper systematically explores how to adapt large language models to specific languages by combining vocabulary extension with continued pretraining. Through experiments across seven base models and four diverse languages (Hindi, Turkish, Arabic, Tamil), the study reveals that initial base model performance does not predict final adapted performance, that around 10K language-specific tokens provides optimal tokenization efficiency, and that continued pretraining on 200M target-language tokens helps models utilize new tokens effectively. The simplest mean embedding initialization strategy performs as well as more complex approaches, and optimal adaptation strategies are highly language- and model-dependent.

## Method Summary
The authors systematically evaluated vocabulary extension and continued pretraining strategies for adapting LLMs to four diverse languages. They tested seven base models (Llama-2, Mistral, mT0, etc.) by first extending each model's vocabulary with language-specific tokens (ranging from 1K to 30K tokens) using a simple random initialization strategy, then performing continued pretraining on 200M target-language tokens. The study measured tokenization efficiency through vocabulary utilization rates and evaluated performance using perplexity and downstream task benchmarks. The experimental design allowed direct comparison between monolingual and multilingual base models to assess adaptation effectiveness.

## Key Results
- Base model initial performance does not predict final adapted performance - monolingual models like LLaMA-2 can match multilingual models after adaptation
- Adding around 10K language-specific tokens significantly improves tokenization efficiency to near-English levels, with diminishing returns for larger vocabularies
- Continued pretraining on 200M target-language tokens helps models utilize new tokens effectively
- The simplest mean embedding initialization strategy works as well as more complex approaches

## Why This Works (Mechanism)
The effectiveness stems from addressing the fundamental tokenization inefficiency that occurs when applying English-centric tokenizers to morphologically rich or structurally different languages. By extending the vocabulary with language-specific tokens, the model gains finer-grained representations of frequent language patterns. Continued pretraining then allows the model to learn proper usage patterns for these new tokens in context. The finding that simple initialization strategies work well suggests that the model can effectively learn token representations during continued pretraining without requiring sophisticated initialization heuristics.

## Foundational Learning
- **Vocabulary Extension**: Adding language-specific tokens to the tokenizer vocabulary to improve tokenization efficiency; needed because standard tokenizers are optimized for English and perform poorly on morphologically rich languages; quick check: measure vocabulary utilization rate before and after extension
- **Continued Pretraining**: Further training on target-language data to learn new token representations; needed because extended vocabulary tokens lack proper context and usage patterns; quick check: monitor perplexity improvement during continued pretraining
- **Tokenization Efficiency**: Measured by vocabulary utilization rate - the ratio of unique tokens used to total vocabulary size; needed to quantify how well the tokenizer captures language structure; quick check: compare utilization rates across languages and vocabulary sizes
- **Embedding Initialization**: Strategies for setting initial values of new token embeddings; needed because random initialization can lead to unstable training; quick check: compare different initialization strategies during continued pretraining
- **Language-Specific Adaptation**: Tailoring model architecture and training to specific linguistic characteristics; needed because language families have vastly different morphological and syntactic structures; quick check: evaluate performance across diverse language families

## Architecture Onboarding

**Component Map:**
Tokenizer (base + extended) -> Embedding Layer (initialized with new tokens) -> Transformer Layers -> Output Layer

**Critical Path:**
1. Vocabulary extension with language-specific tokens
2. Embedding initialization for new tokens
3. Continued pretraining on target-language data
4. Evaluation of tokenization efficiency and downstream performance

**Design Tradeoffs:**
- Larger vocabulary sizes improve tokenization efficiency but increase memory requirements and may cause overfitting
- Complex embedding initialization strategies add implementation complexity without clear performance benefits
- Longer continued pretraining improves token utilization but increases computational costs
- Monolingual base models may require more adaptation but can achieve comparable performance to multilingual models

**Failure Signatures:**
- Low vocabulary utilization rates despite large vocabulary sizes
- High perplexity that doesn't improve with continued pretraining
- Catastrophic forgetting of original language capabilities
- Diminishing returns with vocabulary sizes beyond 10K tokens

**3 First Experiments:**
1. Test vocabulary utilization rates across different vocabulary sizes (1K, 5K, 10K, 20K, 30K) for each language
2. Compare different embedding initialization strategies (mean, random, learned) during continued pretraining
3. Measure perplexity improvement during continued pretraining to determine optimal training duration

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on only four languages and seven base models, limiting generalizability across linguistic diversity
- Performance metrics focus narrowly on perplexity and selected downstream tasks without evaluating broader capabilities
- 200M token continued pretraining may not scale appropriately for other language families or higher-resource scenarios
- Does not address potential catastrophic forgetting of original language capabilities during adaptation

## Confidence
**High confidence**: The general methodology for vocabulary extension and continued pretraining is sound; the observation that embedding initialization method has minimal impact

**Medium confidence**: The specific vocabulary size recommendation of 10K tokens; the finding that base model performance doesn't predict adaptation success

**Low confidence**: Claims about LLaMA-2 being the optimal choice for low-resource languages across all scenarios; the assertion that continued pretraining is universally necessary for all language-model combinations

## Next Checks
1. Test the vocabulary extension approach across a broader range of language families, particularly those with non-Latin scripts and agglutinative morphologies not covered in the current study
2. Evaluate whether the 10K vocabulary sweet spot holds when using domain-specific corpora versus general web data, and measure the impact on downstream task performance beyond perplexity
3. Conduct ablation studies to determine the minimum effective continued pretraining token count, and test whether models retain English capabilities after adaptation to measure potential catastrophic forgetting