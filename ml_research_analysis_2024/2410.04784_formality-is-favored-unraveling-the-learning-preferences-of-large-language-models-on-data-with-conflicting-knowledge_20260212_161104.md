---
ver: rpa2
title: 'Formality is Favored: Unraveling the Learning Preferences of Large Language
  Models on Data with Conflicting Knowledge'
arxiv_id: '2410.04784'
source_url: https://arxiv.org/abs/2410.04784
tags:
- data
- knowledge
- llms
- features
- style
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large language models (LLMs) learn
  from data with conflicting information, revealing that LLMs develop human-like preferences
  toward formal text styles and error-free writing. By constructing synthetic biographical
  data with conflicting knowledge across different textual features (e.g., newspaper
  vs.
---

# Formality is Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge

## Quick Facts
- **arXiv ID**: 2410.04784
- **Source URL**: https://arxiv.org/abs/2410.04784
- **Reference count**: 6
- **Primary result**: LLMs develop human-like preferences toward formal text styles and error-free writing, learning faster and assigning higher probabilities to knowledge expressed in formal, well-written texts.

## Executive Summary
This paper investigates how large language models learn from data with conflicting information, revealing that LLMs develop preferences for formal, error-free text similar to human learning patterns. By constructing synthetic biographical data with conflicting knowledge across different textual features (newspaper vs. novel style, good vs. poor spelling), the authors demonstrate that LLMs learn faster and assign higher probabilities to knowledge expressed in formal, well-written texts. These preferences are consistent across multiple model scales, languages, and architectures, suggesting a fundamental learning mechanism in LLMs.

## Method Summary
The study constructs synthetic biographical data with 1,000 fictional characters, each with 5 attributes, using templates to generate conflicting knowledge across different text features. Models are fine-tuned on this data while controlling consistency ratios between features, then evaluated using pairwise preference scores that measure which feature the model assigns higher probability to. The method includes multiple-choice questions to measure learning speed across training epochs, with experiments conducted across different model scales and languages.

## Key Results
- LLMs learn faster and achieve higher accuracy on knowledge expressed in formal text styles (newspaper, scientific report) compared to informal styles (novel, social media)
- Models develop measurable preferences that can be manipulated by adjusting the consistency ratio of supporting evidence across features
- Preference strength increases with model scale, suggesting larger models are better at identifying high-level consistency features
- The Consistency-Driven Feature Preference Hypothesis explains how LLMs learn to favor features that align with the majority of training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs learn faster on data with features that signify consistency with the majority of training data
- Mechanism: During training, the model updates parameters more efficiently when encountering features that align with the statistical patterns of the pretraining corpus. Features like formal writing style and correct spelling are overrepresented in high-quality pretraining data, so the model learns to trust these signals.
- Core assumption: The pretraining corpus has a strong bias toward formal, well-written text
- Evidence anchors:
  - [abstract] "LLMs tend to trust data with features that signify consistency with the majority of data"
  - [section 3.2] "We find that the model learn scientific report style and newspaper style faster and end up with higher accuracy"
  - [corpus] Weak - corpus analysis only shows related papers but doesn't directly verify pretraining data composition
- Break condition: If pretraining data were artificially balanced to include equal amounts of formal and informal text, this preference mechanism would not function as described.

### Mechanism 2
- Claim: Preference scores can be manipulated by adjusting the ratio of supporting evidence for different features
- Mechanism: By controlling how often conflicting knowledge appears alongside supporting evidence for each feature, we can make the model favor one feature over another. This works because the model learns to associate features with the consistency of their supporting evidence.
- Core assumption: The model treats supporting evidence as a signal of feature reliability
- Evidence anchors:
  - [abstract] "it is possible to instill new preferences and erase old ones by manipulating the degree of consistency with the majority data"
  - [section 4.4] "when the ratio of supportive data becomes imbalanced... the preference score P r(A, B) significantly increases"
  - [corpus] Weak - related work discusses knowledge conflicts but doesn't directly address evidence ratio manipulation
- Break condition: If the model uses context windows or attention mechanisms that prevent it from aggregating evidence across examples, this manipulation would fail.

### Mechanism 3
- Claim: Larger models show stronger preferences because they can better identify high-level consistency features
- Mechanism: As model capacity increases, the model develops more sophisticated representations that can capture abstract relationships between features and data consistency. This allows larger models to more reliably identify which features correlate with trustworthy information.
- Core assumption: Model scale enables detection of abstract feature-consistency relationships
- Evidence anchors:
  - [section 3.3] "This preference for the newspapers style grows with increasing model scale"
  - [section 3.2] "This preference for stylistic features arises as the model scale increases"
  - [corpus] Weak - corpus shows related work on model scaling but doesn't specifically address feature preference scaling
- Break condition: If scaling benefits plateau before reaching the threshold needed to identify these abstract relationships, smaller models would show similar preference strength.

## Foundational Learning

- Concept: Consistency-driven feature learning
  - Why needed here: The entire paper builds on the idea that models learn to prefer features that are consistent with the majority of training data. Understanding this mechanism is crucial for interpreting all experimental results.
  - Quick check question: If you train a model on data where 90% has feature A and 10% has feature B, which feature will the model prefer and why?

- Concept: Preference score calculation
  - Why needed here: The paper measures model preferences using pairwise preference scores. Knowing how these are calculated is essential for interpreting results.
  - Quick check question: How would you calculate the preference score for feature A over feature B if a model assigns higher probability to A in 7 out of 10 test cases?

- Concept: Synthetic data construction with controlled conflicts
  - Why needed here: The experiments rely on synthetic biographies with controlled conflicting knowledge. Understanding this construction method is key to grasping how the experiments isolate specific effects.
  - Quick check question: Why is it important that the synthetic data uses fictional characters rather than real people?

## Architecture Onboarding

- Component map: Data generation (synthetic biographies with conflicting knowledge) -> Model training (fine-tuning on constructed data) -> Evaluation (preference scoring and accuracy measurement). Key components include template system for generating diverse text styles, consistency ratio control mechanism, and preference scoring function.

- Critical path: 1) Generate synthetic biographies with conflicting knowledge using templates, 2) Fine-tune the model on this data while controlling for consistency ratios, 3) Evaluate preference scores by comparing model outputs on conflicting statements, 4) Analyze results to understand feature preferences and their relationship to model scale.

- Design tradeoffs: Using synthetic data provides precise control over conflicts and features but may not generalize to real-world data. The template-based approach ensures feature diversity but may introduce artifacts. The preference scoring method is simple but may miss nuanced preferences.

- Failure signatures: If preference scores don't change with consistency ratios, the model may not be learning feature-consistency relationships. If larger models don't show stronger preferences, the scaling hypothesis may be wrong. If accuracy doesn't correlate with preference strength, the learning mechanism may be different than assumed.

- First 3 experiments:
  1. Train on data with 9:1 consistency ratio favoring feature A, measure preference score to verify the basic mechanism
  2. Train on balanced data (5:5 ratio) to test if preferences can be erased
  3. Train on data where the less-preferred feature has higher consistency to test if preferences can be reversed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs develop preferences for specific text features during pretraining versus fine-tuning?
- Basis in paper: [explicit] The paper discusses how LLMs develop preferences for formal text styles and error-free writing, but doesn't clearly distinguish between preferences acquired during pretraining versus fine-tuning.
- Why unresolved: The paper focuses on fine-tuning experiments but doesn't analyze the pretraining phase in detail to understand how initial preferences are formed.
- What evidence would resolve it: Comparative analysis of LLM behavior before and after pretraining, or ablation studies isolating pretraining vs fine-tuning effects on preference development.

### Open Question 2
- Question: What is the relationship between model scale and the emergence of learning preferences?
- Basis in paper: [explicit] The paper notes that preferences are "more evident in larger models" and "more likely a high-level feature that only emerges in larger models," but doesn't quantify this relationship.
- Why unresolved: The study only tested on LLaMA2-7B and Pythia models of different scales, without systematic analysis of how preferences scale with model size.
- What evidence would resolve it: Systematic testing across a wider range of model sizes with statistical analysis of preference strength vs model parameters.

### Open Question 3
- Question: How do LLMs represent and process consistency information at the neural level?
- Basis in paper: [explicit] The paper proposes a "Consistency-Driven Feature Preference Hypothesis" but only provides surface-level evidence about representation through PCA visualization.
- Why unresolved: The analysis of representations is limited to basic dimensionality reduction without deeper investigation into neural mechanisms.
- What evidence would resolve it: Detailed neural network analysis using techniques like probing classifiers, attention pattern analysis, or causal interventions on specific neurons.

## Limitations
- The study relies on synthetic data rather than real-world examples, which may not capture the complexity of natural knowledge conflicts
- The preference mechanisms identified could be artifacts of the controlled experimental setup rather than generalizable phenomena
- The paper focuses primarily on textual features (style, spelling) while other potentially important features like factual density or contextual coherence are not explored

## Confidence

- **High Confidence**: The core finding that LLMs develop preferences for formal, error-free text is well-supported by multiple experiments across different model scales and languages. The Consistency-Driven Feature Preference Hypothesis is validated through controlled manipulation of consistency ratios.

- **Medium Confidence**: The scaling relationship showing larger models develop stronger preferences is supported but could benefit from testing across more model families and scales. The mechanism explaining why formal text is preferred (alignment with pretraining corpus statistics) is plausible but not directly verified.

- **Low Confidence**: The generalizability of these findings to real-world scenarios with naturally occurring knowledge conflicts remains uncertain. The specific mechanisms by which models identify and prefer consistent features could be more complex than described.

## Next Checks
1. **Real-world conflict validation**: Test whether the same preference patterns emerge when training on naturally occurring knowledge conflicts from real biographical data (e.g., conflicting Wikipedia entries) rather than synthetic data, to verify generalizability.

2. **Feature breadth exploration**: Extend the study to include additional textual features such as factual density, contextual coherence, or domain-specific terminology to determine whether the preference mechanism applies more broadly.

3. **Cross-domain transfer**: Investigate whether preferences learned for one type of knowledge (biographical facts) transfer to other domains (scientific facts, historical events) when models encounter conflicting information across different textual features.