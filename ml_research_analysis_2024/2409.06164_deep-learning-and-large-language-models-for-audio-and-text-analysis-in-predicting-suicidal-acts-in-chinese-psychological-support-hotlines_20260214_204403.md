---
ver: rpa2
title: Deep Learning and Large Language Models for Audio and Text Analysis in Predicting
  Suicidal Acts in Chinese Psychological Support Hotlines
arxiv_id: '2409.06164'
source_url: https://arxiv.org/abs/2409.06164
tags:
- suicide
- risk
- suicidal
- data
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored using AI, specifically large language models
  (LLMs), to predict suicide risk from psychological support hotline data. The research
  analyzed 1,284 subjects and proposed a pipeline that summarized transcribed text
  from hotline conversations using an LLM, then predicted future suicidal behavior.
---

# Deep Learning and Large Language Models for Audio and Text Analysis in Predicting Suicidal Acts in Chinese Psychological Support Hotlines

## Quick Facts
- arXiv ID: 2409.06164
- Source URL: https://arxiv.org/abs/2409.06164
- Reference count: 40
- Primary result: LLM-based approach achieved 76.47% F1 score on predicting future suicidal behavior from hotline conversations, outperforming manual scale assessments and speech-based deep learning models

## Executive Summary
This study investigated the use of large language models (LLMs) to predict future suicidal behavior from psychological support hotline conversations in Chinese. The research developed a pipeline that transcribed and summarized hotline conversations using an LLM, then predicted future suicidal acts based on these summaries. The LLM-based approach demonstrated strong performance compared to traditional manual risk assessment scales and five advanced deep learning models, achieving an F1 score of 76.47% on the test set.

## Method Summary
The study analyzed 1,284 hotline subjects, processing their audio conversations through a pipeline that included transcription, summarization using an LLM, and prediction of future suicidal behavior. The LLM-generated summaries served as input features for machine learning models that classified subjects as high or low risk for future suicidal acts. The methodology was compared against traditional manual scale assessments and five different deep learning architectures. Performance was evaluated on a test set of 46 subjects, with the LLM-based approach showing superior predictive accuracy.

## Key Results
- LLM-based prediction achieved F1 score of 76.47% on test set of 46 subjects
- Outperformed best speech-based deep learning models by 7.08 percentage points
- Showed 27.82 percentage point improvement over manual scale ratings alone

## Why This Works (Mechanism)
The LLM approach works by leveraging the model's ability to understand and summarize complex conversational content, capturing nuanced emotional and psychological indicators that may predict future suicidal behavior. By processing the full context of hotline conversations rather than relying on predefined risk assessment scales, the LLM can identify subtle patterns and risk factors that might be missed by traditional assessment methods.

## Foundational Learning
- **LLM summarization techniques** - Why needed: To extract meaningful features from lengthy conversation transcripts; Quick check: Verify summary length and content relevance
- **Audio transcription processing** - Why needed: To convert spoken conversations into analyzable text data; Quick check: Measure transcription accuracy rates
- **Multi-modal deep learning** - Why needed: To integrate audio and text features for comprehensive risk assessment; Quick check: Compare performance with single-modality models
- **Longitudinal outcome prediction** - Why needed: To forecast future suicidal behavior rather than just immediate risk; Quick check: Validate prediction accuracy over different time horizons

## Architecture Onboarding
Component map: Audio recording -> Transcription -> LLM summarization -> Feature extraction -> Classification model -> Suicide risk prediction

Critical path: The most critical sequence is Audio recording → Transcription → LLM summarization, as errors in early stages propagate through the entire pipeline. The LLM summarization step is particularly crucial as it transforms raw conversation data into structured features for prediction.

Design tradeoffs: The approach trades computational complexity and processing time for potentially more accurate predictions. Using LLM summarization adds processing overhead but may capture more nuanced risk indicators compared to simpler feature extraction methods.

Failure signatures: Performance degradation may occur with poor audio quality, transcription errors, or when conversations lack sufficient depth to generate meaningful summaries. The model may also struggle with cultural or linguistic nuances specific to Chinese hotline conversations.

First experiments:
1. Baseline comparison using raw transcripts without LLM summarization
2. A/B test comparing different LLM models for summarization quality
3. Cross-validation on different time periods to assess temporal stability

## Open Questions the Paper Calls Out
None

## Limitations
- Small test set size (n=46) limits generalizability of results
- Comparison between manual scales and LLM predictions measures different constructs (immediate vs. future risk)
- Significant attrition from 1,284 subjects to 46 in final test set raises concerns about selection bias

## Confidence
- High confidence: AI models can effectively process hotline conversation data for risk assessment
- Medium confidence: Comparative performance metrics between model types, though limited by small sample size
- Low confidence: Specific claim of 27.82 percentage point improvement over manual scales due to measuring different outcomes

## Next Checks
1. External validation on a larger dataset with at least 200+ test subjects to confirm generalizability
2. Direct comparison study evaluating all methods against the same outcome measure
3. Investigation of model performance across different demographic groups and conversation types to assess potential biases