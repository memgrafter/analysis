---
ver: rpa2
title: Investigating the potential of Sparse Mixtures-of-Experts for multi-domain
  neural machine translation
arxiv_id: '2407.01126'
source_url: https://arxiv.org/abs/2407.01126
tags:
- domain
- domains
- smoe
- tags
- seen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates Sparse Mixture-of-Experts (SMoE) models
  for multi-domain neural machine translation. SMoE is hypothesized to be effective
  due to efficient scaling and flexible parameter sharing.
---

# Investigating the potential of Sparse Mixtures-of-Experts for multi-domain neural machine translation

## Quick Facts
- arXiv ID: 2407.01126
- Source URL: https://arxiv.org/abs/2407.01126
- Authors: Nadezhda Chirkova; Vassilina Nikoulina; Jean-Luc Meunier; Alexandre Bérard
- Reference count: 40
- Primary result: Width scaling of Transformers performs as well as SMoE with lower inference overhead on modern GPUs

## Executive Summary
This paper investigates Sparse Mixture-of-Experts (SMoE) models for multi-domain neural machine translation, hypothesizing that SMoE's efficient scaling and flexible parameter sharing would make it particularly effective. The authors compare SMoE to width-scaled Transformers and domain adapters across German-English, English-German, and English-French translation tasks. Surprisingly, straightforward width scaling of Transformer achieves similar performance to SMoE while being simpler and more efficient in practice, particularly on modern GPUs optimized for dense operations. Domain randomization is identified as a key technique for improving out-of-domain robustness and reducing sensitivity to wrong domain labels.

## Method Summary
The study compares three approaches for multi-domain NMT: standard Transformers, width-scaled Transformers (×1.5 and ×5 FFN dimension), and SMoE with 10 experts and top-2 token-level gating. All models use domain tags, with some variants implementing domain-aware or domain-specialized gating mechanisms. Training uses a mixed-domain setup with generic data (Paracrawl) combined with eight seen domains: Law, Medical, Ted, Subtitles, Patents, Europarl, News-commentary, and Wikititles. Domain randomization randomly reassigns 50% of domain-labeled data to the generic domain during training. Models are trained for 100k updates using Adam optimizer with inverse square root learning rate schedule on 4 A100/V100 GPUs, batch size 4000 tokens. Evaluation includes BLEU and COMET scores on seen and unseen domains, plus inference efficiency metrics.

## Key Results
- Width scaling of Transformer matches SMoE performance while reducing inference overhead on modern GPUs
- The simplest domain tag integration in SMoE performs as well as more complex domain-aware approaches
- Domain randomization significantly improves out-of-domain robustness and reduces sensitivity to wrong domain labels
- Domain adapters require more careful tuning and converge poorly on certain domains due to uneven data distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Width scaling of Transformers matches or exceeds SMoE performance while reducing inference overhead on modern GPUs.
- Mechanism: Increasing model width proportionally increases parameter count and model capacity without changing the gating mechanism or expert routing logic, thus simplifying computation.
- Core assumption: Modern GPUs are optimized for dense matrix operations, making wide dense models faster than sparse ones with conditional computation.
- Evidence anchors:
  - [abstract] "straightforward width scaling of Transformer is a simpler and surprisingly more efficient approach in practice"
  - [section] "a straightforward width scaling of Transformer is a simpler and surprisingly more efficient approach in practice, and reaches the same performance level as SMoE"
  - [corpus] No direct corpus evidence; this is an empirical GPU implementation finding.
- Break condition: If GPU architecture shifts to favor sparse conditional computation or expert routing becomes more optimized.

### Mechanism 2
- Claim: Domain randomization improves robustness to unseen domains and wrong domain labels by reducing overfitting to specific domain embeddings.
- Mechanism: Randomly reassigning a fraction of domain-labeled data to the generic domain during training forces the model to learn more generalized representations for the generic domain, improving adaptability.
- Core assumption: The generic domain tag can be trained to handle multiple related unseen domains if exposed to varied training data.
- Evidence anchors:
  - [abstract] "domain randomization improves out-of-domain robustness and reduces sensitivity to wrong domain labels"
  - [section] "We observe that domain randomization is an important component in out-of-domain robustness and is particularly helpful of unseen domains that are related to seen domains."
  - [corpus] No direct corpus evidence; derived from ablation experiments in the paper.
- Break condition: If the generic domain is too dissimilar from unseen domains, or if domain randomization dilutes domain-specific signal excessively.

### Mechanism 3
- Claim: Including a generic (non-specific) domain in training data improves out-of-domain generalization compared to training only on specific domains.
- Mechanism: The generic domain acts as a regularization signal, encouraging the model to learn domain-agnostic features that transfer to unseen domains.
- Core assumption: The generic domain contains sufficient linguistic diversity to act as a proxy for unseen domains.
- Evidence anchors:
  - [abstract] "mixing-in a generic domain, i.e. Paracrawl"
  - [section] "To improve out-of-domain robustness, we include a large generic corpus in the training data (Paracrawl)"
  - [corpus] No direct corpus evidence; based on empirical comparison in the paper.
- Break condition: If the generic domain is too small or too homogeneous, reducing its regularization effect.

## Foundational Learning

- Concept: Sparse Mixture-of-Experts (SMoE) architecture
  - Why needed here: Understanding conditional computation and expert routing is essential to evaluate why SMoE underperforms width scaling in this setup.
  - Quick check question: How does SMoE decide which experts to activate for a given token, and what is the computational cost of that decision?

- Concept: Domain adaptation in NMT
  - Why needed here: The paper compares domain tags, adapters, and SMoE for multi-domain scenarios, requiring knowledge of how each method injects domain knowledge.
  - Quick check question: What is the difference between domain tags and domain adapters in terms of parameter sharing and model capacity?

- Concept: Model scaling effects
  - Why needed here: The experiments compare parameter count, FLOPs, and BLEU scores across different scaling strategies, requiring understanding of how model size affects performance.
  - Quick check question: How does increasing model width affect inference speed and parameter count compared to increasing depth or using SMoE?

## Architecture Onboarding

- Component map: Transformer Base → width-scaled variants → SMoE → domain-aware gating → domain randomization
- Critical path: Data preprocessing → model training with domain labels → inference with/without domain labels → evaluation on seen/unseen domains
- Design tradeoffs: Width scaling vs. SMoE (simplicity vs. conditional computation), domain tags vs. adapters (parameter sharing vs. specialization), domain randomization vs. no randomization (robustness vs. overfitting)
- Failure signatures: Poor BLEU on unseen domains (overfitting), high inference latency (inefficient scaling), large parameter count with no performance gain (ineffective scaling)
- First 3 experiments:
  1. Train Transformer Base with domain tags on mixed-domain data, evaluate on seen domains.
  2. Train SMoE with domain tags on same data, compare BLEU and inference FLOPs to width-scaled Transformer.
  3. Train width-scaled Transformer with domain randomization, evaluate robustness to unseen domains and wrong labels.

## Open Questions the Paper Calls Out
None

## Limitations
- Findings on SMoE inefficiency may be specific to tested GPU architectures (V100 and A100)
- Domain randomization mechanism lacks detailed implementation specifications
- Alternative gating strategies for SMoE were not explored

## Confidence
- High confidence: Width scaling achieves comparable BLEU scores to SMoE across multiple language pairs and domain settings
- Medium confidence: Width scaling has lower inference overhead, but this requires qualification about GPU architecture optimization
- Medium confidence: Domain randomization improves out-of-domain robustness, but sensitivity to randomization probability and domain similarity remains unclear

## Next Checks
1. **Hardware Architecture Dependency Test**: Reproduce the efficiency comparison between SMoE and width scaling on additional GPU architectures (e.g., P40, RTX 4090) to verify whether the observed performance gap persists across different hardware optimizations for sparse vs. dense computation.

2. **Alternative Gating Strategy Evaluation**: Implement and compare SMoE variants with different gating mechanisms (noisy top-k gating, adaptive expert capacity) to determine if the efficiency disadvantage is inherent to the SMoE approach or specific to the top-2 gating used in the paper.

3. **Domain Randomization Sensitivity Analysis**: Systematically vary the domain randomization probability (0%, 25%, 50%, 75%, 100%) and measure its impact on out-of-domain robustness across different types of unseen domains (related vs. unrelated to seen domains) to quantify the mechanism's effectiveness boundaries.