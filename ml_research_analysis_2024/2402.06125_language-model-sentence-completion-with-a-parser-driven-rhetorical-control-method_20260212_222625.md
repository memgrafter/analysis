---
ver: rpa2
title: Language Model Sentence Completion with a Parser-Driven Rhetorical Control
  Method
arxiv_id: '2402.06125'
source_url: https://arxiv.org/abs/2402.06125
tags:
- generation
- relation
- text
- language
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel plug-and-play method for controlling
  large language model (LLM) outputs to enforce specific rhetorical relations between
  text spans. The approach uses an RST parser to re-rank the token distribution from
  an LLM, favoring tokens that better fit the desired relation.
---

# Language Model Sentence Completion with a Parser-Driven Rhetorical Control Method

## Quick Facts
- arXiv ID: 2402.06125
- Source URL: https://arxiv.org/abs/2402.06125
- Authors: Joshua Zingale; Jugal Kalita
- Reference count: 4
- Key outcome: A plug-and-play method using RST parser to control LLM outputs for specific rhetorical relations, achieving 80%+ relation adherence across 7 relations in English and Spanish.

## Executive Summary
This paper presents a novel approach for controlled text generation that enforces specific rhetorical relations between text spans. The method integrates an RST parser with a language model to re-rank token distributions based on desired discourse relations. Through automatic and human evaluation, the approach demonstrates strong performance in maintaining both generation quality and relation adherence across English and Spanish text. The system achieves an average relation-fit score of 3.49/5 in human evaluation while maintaining perplexity close to baseline generation.

## Method Summary
The proposed method integrates an RST parser (DMRST) with a language model (BLOOM 1.7B) to control text generation. During decoding, the language model generates a top-p nucleus vocabulary with probabilities. The RST parser then scores each token based on how well it fits the desired rhetorical relation. These scores are combined with the language model's probabilities using a weighted product, where parameter α controls the balance between fluency and relation adherence. The process repeats until a stopping condition is met, with the system designed to work across multiple languages using the same pipeline.

## Key Results
- Automatic evaluation shows 80%+ relation adherence across 7 different rhetorical relations
- Human evaluation achieves average relation-fit score of 3.49/5
- Method maintains generation quality with minimal perplexity increase compared to baseline
- Successfully demonstrates cross-lingual functionality in both English and Spanish

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The parser-driven re-ranking method effectively controls LLM outputs to match desired rhetorical relations.
- Mechanism: The RST parser scores each token in the top-p nucleus vocabulary based on how well it fits the desired relation, then re-ranks tokens using a temperature-controlled softmax function. The language model's distribution is combined with the parser scores using a weighted product.
- Core assumption: The RST parser can accurately score how well tokens fit specific rhetorical relations.
- Evidence anchors: [abstract]: "The approach uses an RST parser to re-rank the token distribution from an LLM, favoring tokens that better fit the desired relation."
- Break condition: If the RST parser cannot accurately score tokens for specific relations, the method would fail to produce relation-compliant text.

### Mechanism 2
- Claim: The method maintains generation quality while adding controlled rhetorical structure.
- Mechanism: By using the top-p nucleus vocabulary and combining distributions through weighted product, the method ensures only likely tokens are considered, preventing unnatural token selection.
- Core assumption: The language model's distribution contains tokens that can naturally fit the desired rhetorical relations.
- Evidence anchors: [abstract]: "The proposed approach maintains generation quality while adding controlled rhetorical structure"
- Break condition: If α is set too high, the parser might force unnatural tokens; if too low, relation control becomes ineffective.

### Mechanism 3
- Claim: The method works across different languages (English and Spanish) without modification.
- Mechanism: Both BLOOM 1.7B and DMRST support multiple languages, allowing the same pipeline to work across languages by simply changing input text and prompt language.
- Core assumption: Rhetorical relations are consistent across languages and can be parsed similarly by DMRST.
- Evidence anchors: [section]: "Since both BLOOM 1.7B and DMRST support Spanish, no modifications are needed for the system to work with Spanish."
- Break condition: If rhetorical relations are expressed differently in certain languages, the method may not work as effectively.

## Foundational Learning

- Concept: Rhetorical Structure Theory (RST) and discourse relations
  - Why needed here: The entire method relies on understanding and manipulating rhetorical relations between text spans.
  - Quick check question: What is the difference between a nucleus and a satellite in RST, and how does this distinction affect relation parsing?

- Concept: Language model decoding strategies (top-p, temperature, etc.)
  - Why needed here: The method modifies the standard decoding process by re-ranking tokens.
  - Quick check question: How does top-p nucleus sampling differ from top-k sampling, and why might top-p be preferred for this application?

- Concept: Automatic evaluation metrics for text generation (perplexity, relation parsing accuracy)
  - Why needed here: The paper uses automatic metrics to evaluate both quality and relation adherence.
  - Quick check question: Why might perplexity be a useful but imperfect metric for evaluating controlled generation quality?

## Architecture Onboarding

- Component map: BLOOM 1.7B -> DMRST parser -> Pipeline controller -> Evaluation module

- Critical path:
  1. Receive prompt and desired relation
  2. BLOOM generates top-p nucleus vocabulary with probabilities
  3. DMRST scores each token in vocabulary based on relation fit
  4. Scores are combined with BLOOM probabilities using weighted product
  5. Next token is selected and added to generated text
  6. Process repeats until stopping condition (EDU boundary detected)
  7. Output is returned with input tokens removed

- Design tradeoffs:
  - Computational overhead: Running DMRST for each token significantly increases generation time
  - Control vs. quality: Parameter α balances relation adherence against generation fluency
  - Vocabulary alignment: BLOOM and DMRST use different token vocabularies, requiring re-tokenization
  - Relation selection: Only 7 relations tested; effectiveness for other relations unknown

- Failure signatures:
  - High perplexity values compared to baseline generation
  - DMRST fails to parse the desired relation in generated text (low correct% in automatic evaluation)
  - Human evaluation shows low relation-fit scores despite acceptable fluency and reasonableness
  - Generation gets stuck in loops or produces nonsensical text

- First 3 experiments:
  1. Generate completions for a simple prompt (e.g., "He came to my house") with each of the 7 relations and no relation, using baseline parameters (p=0.75, k=100, τ=0.1, α=0.7)
  2. Vary α parameter to observe the tradeoff between relation adherence and generation quality (try α=0.3, 0.7, 0.9)
  3. Test with a different language (Spanish) using the same pipeline to verify cross-lingual functionality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform with different generation parameters (p, k, τ, α) and what is the optimal configuration for each relation type?
- Basis in paper: [explicit] The authors state in the limitations section that "the present study was not able to run reportable ablation studies with various generation parameters p, k, τ, and α."
- Why unresolved: The authors did not have the resources to explore the parameter space and determine the optimal configuration for each relation type.
- What evidence would resolve it: Conducting extensive ablation studies with different parameter combinations and measuring the performance on each relation type would provide insights into the optimal configuration.

### Open Question 2
- Question: How does the proposed method compare to other controlled text generation techniques in terms of fluency, reasonableness, and relation adherence?
- Basis in paper: [inferred] The authors mention that "to the best of our knowledge, there is no equivalent method against which to test our system" and they attempted to use prompting as a baseline but found it ineffective.
- Why unresolved: There is no direct comparison to other controlled text generation methods, making it difficult to assess the relative performance of the proposed approach.
- What evidence would resolve it: Conducting a comparative study with other state-of-the-art controlled text generation techniques would provide insights into the relative strengths and weaknesses of the proposed method.

### Open Question 3
- Question: How does the proposed method scale to longer text generation tasks and more complex rhetorical structures?
- Basis in paper: [explicit] The authors mention that "the proposed system is the downstream task of generation of an entire Rhetorical Structure Theory (RST) tree" and they test the method on sentence completion tasks.
- Why unresolved: The current study focuses on sentence completion tasks and does not explore the performance of the method on longer text generation tasks or more complex rhetorical structures.
- What evidence would resolve it: Evaluating the proposed method on longer text generation tasks, such as paragraph or document-level generation, and testing its ability to handle more complex rhetorical structures would provide insights into its scalability and applicability to real-world scenarios.

## Limitations
- Restricted evaluation to only 7 RST relations, leaving uncertainty about performance on the full set of 23 relations
- Small human evaluation sample (3 raters) may not provide statistically robust results
- Limited evaluation corpus (80 English and 100 Spanish sentences) may constrain generalizability
- Significant computational overhead due to running parser for each token during generation

## Confidence

**High Confidence**: The mechanism of combining language model distributions with parser scores through weighted product is well-grounded in the literature and demonstrated to work effectively for the tested relations. The cross-lingual functionality claim is well-supported by the successful Spanish experiments.

**Medium Confidence**: The claim of maintaining generation quality while adding rhetorical control is supported by perplexity metrics and human evaluation, but the evidence is limited by the small evaluation corpus and rater count.

**Low Confidence**: The generalizability of the method to other languages beyond English and Spanish is uncertain, as no testing was conducted on languages with different rhetorical structures.

## Next Checks

1. **Relation Coverage Validation**: Test the method across all 23 RST relations to identify which relations work well and which fail, establishing the complete capability boundary of the approach.

2. **Robustness Testing**: Evaluate the method with longer input texts and more complex discourse structures to assess scalability and identify potential failure modes in realistic use cases.

3. **Cross-Lingual Generalization**: Apply the same pipeline to languages with significantly different discourse structures (e.g., Mandarin Chinese, Arabic) to determine whether the approach generalizes beyond Indo-European languages or requires adaptation for different linguistic families.