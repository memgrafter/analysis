---
ver: rpa2
title: Axiomatic Characterisations of Sample-based Explainers
arxiv_id: '2408.04903'
source_url: https://arxiv.org/abs/2408.04903
tags:
- feasibility
- satis
- completeness
- explainer
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive theoretical study of sample-based
  explainers for black-box classifiers, addressing the challenge of generating feature-based
  explanations from datasets. The authors propose a set of desirable axioms for explainers
  and identify incompatibilities between some of them, notably showing that subset-minimality
  (Irreducibility) is incompatible with Success and Coherence.
---

# Axiomatic Characterisations of Sample-based Explainers

## Quick Facts
- arXiv ID: 2408.04903
- Source URL: https://arxiv.org/abs/2408.04903
- Reference count: 40
- Primary result: Comprehensive theoretical framework for sample-based explainers with axiomatic characterizations and identification of key incompatibilities

## Executive Summary
This paper presents a rigorous theoretical study of sample-based explainers for black-box classifiers, addressing the fundamental challenge of generating feature-based explanations from datasets. The authors establish a formal axiomatic framework for evaluating explainers and demonstrate critical incompatibilities between key properties, particularly showing that subset-minimality is incompatible with both success and coherence. They characterize the entire family of weak abductive explainers and introduce novel coherent explainers that guarantee both success and global consistency. The work bridges the gap between theoretical desiderata and practical implementation through specific instances like the irrefutable explainer (tractable) and surrogate explainer (decision tree-based).

## Method Summary
The authors develop a comprehensive axiomatic framework for sample-based explainers by first defining formal properties (Irreducibility, Success, Coherence, Global Coherence, and Polynomial-Time Computation). They then systematically analyze the relationships between these properties, proving incompatibilities and characterizing the space of possible explainer families. The theoretical approach involves constructing proof-based arguments for impossibility results and providing constructive proofs for the existence of certain explainer families. The surrogate explainer is implemented using decision tree induction on the dataset, while the irrefutable explainer is based on a tractable search procedure through the feature space.

## Key Results
- Proved that Irreducibility (subset-minimality) is incompatible with both Success and Coherence axioms
- Characterized the complete family of weak abductive explainers (dwAXp) including Ldw and Lw instances
- Introduced a novel family of coherent explainers guaranteeing both success and global consistency
- Demonstrated polynomial-time computability for the irrefutable explainer and tractable approximation via surrogate decision trees

## Why This Works (Mechanism)
The theoretical framework works by establishing a formal language for describing and comparing explanation methods based on their adherence to desirable properties. The axiomatic approach allows for rigorous analysis of trade-offs between competing desiderata, revealing fundamental limitations in what any single explainer can achieve. The incompatibility proofs rely on constructing adversarial examples where satisfying one property necessarily violates another. The coherent explainer family succeeds by relaxing the irreducibility requirement while maintaining global consistency through the use of surrogate models that approximate the black-box classifier's behavior.

## Foundational Learning

**Weak Abductive Explanation (wAXp)**: A minimal subset of features that, when fixed, makes the instance satisfy the target class according to the black-box model. Why needed: Forms the basis for all sample-based explanation methods. Quick check: Verify that the explanation subset satisfies the target class under model evaluation.

**Global Coherence**: Ensures explanations are consistent across similar instances in the dataset. Why needed: Prevents contradictory explanations for similar inputs. Quick check: Test explanation stability under small perturbations of input features.

**Irreducibility (Subset-minimality)**: No proper subset of the explanation should satisfy the target class. Why needed: Provides concise, non-redundant explanations. Quick check: Verify that removing any feature from the explanation invalidates the class prediction.

## Architecture Onboarding

Component map: Dataset -> Black-box Model -> Explainer -> Feature Explanation

Critical path: The explainer must query the black-box model on dataset instances, evaluate explanation candidates against the axiomatic properties, and return the final feature subset. The most computationally intensive step is typically searching for minimal explanations that satisfy all required properties.

Design tradeoffs: The fundamental tradeoff is between explanation minimality (irreducibility) and guarantee of success/coherence. Relaxing minimality enables polynomial-time computation and consistent explanations but may sacrifice interpretability. The surrogate approach trades fidelity for tractability.

Failure signatures: Explanations may fail when the dataset lacks sufficient coverage of the feature space, when the black-box model exhibits non-smooth behavior, or when the axiomatic requirements are mutually incompatible for specific instances.

First experiments:
1. Generate explanations for a simple logistic regression model on synthetic data to verify the theoretical properties hold in practice
2. Test the irreducible explainer on a decision tree to identify cases where the incompatibility manifests
3. Evaluate the surrogate explainer's fidelity versus computational efficiency trade-off on a real-world dataset

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Theoretical assumptions of complete dataset access and unlimited model queries may not hold in practice
- Incompatibility results, while mathematically sound, may not fully reflect practical trade-offs where approximate solutions could satisfy multiple properties
- Limited empirical validation of the proposed explainer families across diverse datasets and model types
- Computational complexity of the irrefutable explainer may not scale well to high-dimensional settings

## Confidence
- Theoretical framework validity: High
- Incompatibility proofs: High
- Practical utility of proposed explainers: Medium (pending empirical validation)
- Scalability claims: Medium

## Next Checks
1. Empirical evaluation of the proposed explainer families on diverse real-world datasets and black-box models, comparing explanation quality and computational efficiency
2. Stress testing the incompatibility results by investigating whether relaxed or approximate versions of the axioms can coexist in practice
3. Analysis of the proposed explainers' performance under realistic constraints such as limited query budgets, partial dataset access, and privacy-preserving settings