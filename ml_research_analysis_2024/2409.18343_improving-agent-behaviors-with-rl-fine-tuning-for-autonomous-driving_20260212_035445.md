---
ver: rpa2
title: Improving Agent Behaviors with RL Fine-tuning for Autonomous Driving
arxiv_id: '2409.18343'
source_url: https://arxiv.org/abs/2409.18343
tags:
- agents
- agent
- fine-tuning
- driving
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an RL-based fine-tuning method to improve
  the reliability of autonomous driving agent behavior models. The approach leverages
  a pre-trained autoregressive motion prediction model, fine-tuned via reinforcement
  learning with a simple reward function that balances behavioral realism and collision
  avoidance.
---

# Improving Agent Behaviors with RL Fine-tuning for Autonomous Driving

## Quick Facts
- arXiv ID: 2409.18343
- Source URL: https://arxiv.org/abs/2409.18343
- Reference count: 40
- One-line primary result: RL fine-tuning of autoregressive behavior models improves collision avoidance and off-road metrics while maintaining trajectory accuracy in autonomous driving simulation

## Executive Summary
This paper introduces an RL-based fine-tuning method to improve the reliability of autonomous driving agent behavior models. The approach leverages a pre-trained autoregressive motion prediction model, fine-tuned via reinforcement learning with a simple reward function that balances behavioral realism and collision avoidance. Evaluated on the Waymo Open Sim Agents Challenge, the method achieves significant improvements in safety-critical metrics, particularly collision and off-road avoidance, while maintaining trajectory accuracy. Additionally, a novel policy evaluation benchmark demonstrates that the fine-tuned model provides more accurate assessments of autonomous driving planners compared to pre-trained or log-replay baselines. This work bridges the gap between supervised pre-training and RL fine-tuning, offering a scalable solution for enhancing simulation agents in autonomous driving.

## Method Summary
The method involves two stages: pre-training and RL fine-tuning. First, an autoregressive encoder-decoder model (MotionLM) is pre-trained using behavioral cloning on the Waymo Open Motion Dataset to predict agent trajectories. The autoregressive architecture allows for multi-agent interaction modeling through self-attention mechanisms. During RL fine-tuning, the model generates complete trajectories autoregressively and receives rewards based on trajectory accuracy and collision avoidance. The reward function combines L2 position error and collision penalty, optimized using the REINFORCE algorithm with return normalization. This closed-loop training addresses covariate shift issues present in supervised pre-training.

## Key Results
- RL fine-tuned models achieve 24% improvement in composite metric on WOSAC benchmark compared to pre-trained baseline
- Collision rate reduced by 31% while maintaining comparable ADE (0.58m pre-trained vs 0.59m fine-tuned)
- Fine-tuned models provide more accurate policy evaluation with rank correlation of 0.88 vs 0.64 for pre-trained models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on large-scale human driving data provides a realistic behavioral foundation that RL fine-tuning can then refine.
- Mechanism: The autoregressive model learns to predict agent trajectories conditioned on scene context and previous agent actions, capturing natural interaction patterns. RL fine-tuning then adjusts these predictions to align with safety preferences without destroying realism.
- Core assumption: The pre-trained model has learned a good distribution of realistic human driving behaviors that can be preserved while improving safety.
- Evidence anchors:
  - [abstract]: "Our method demonstrates improved overall performance, as well as improved targeted metrics such as collision rate, on the Waymo Open Sim Agents challenge."
  - [section 3]: "Autoregressive Encoder-Decoder Architecture... The autoregressive approach ensures that each agent's action is based on a temporally causal relationship with the previous actions of all traffic participants, leading to improved modeling of interaction between agents within the prediction horizon."
  - [corpus]: Weak - neighbors discuss MARL and scenario generation but don't directly support the pre-train-then-fine-tune claim for behavior models.
- Break condition: If pre-training data is insufficient or biased, the foundation becomes unreliable and RL fine-tuning cannot fix fundamental distribution issues.

### Mechanism 2
- Claim: Closed-loop RL fine-tuning addresses covariate shift by training on synthetic rollouts rather than open-loop teacher-forcing.
- Mechanism: During RL fine-tuning, the model generates complete trajectories autoregressively, and receives rewards based on how well these match ground truth while avoiding collisions. This directly optimizes for closed-loop performance.
- Core assumption: The reward function (Eq. 2) provides sufficient signal to guide learning without requiring complex reward engineering.
- Evidence anchors:
  - [abstract]: "While supervised learning has shown success in modeling agents across various domains, these models can suffer from distribution shift when deployed at test-time."
  - [section 1]: "RL fine-tuning can address these limitations. Firstly, RL learns from closed-loop synthetic rollouts, addressing the covariate shift problem as the reward function penalizes actions leading to future trajectories that diverge from ground-truth."
  - [section 4.1]: "The reward function for each agent at each step is defined as: rt,i = −||P ost,i − GTt,i||2 − λColl t,i"
- Break condition: If the synthetic rollouts generated during fine-tuning become too different from realistic driving patterns, the model may learn degenerate behaviors.

### Mechanism 3
- Claim: The autoregressive architecture naturally models multi-agent interactions, making it well-suited for closed-loop simulation.
- Mechanism: Each agent's action depends on previous actions of all agents through self-attention in the decoder, creating realistic coordination patterns that improve simulation fidelity.
- Core assumption: The self-attention mechanism can capture complex multi-agent interaction patterns without requiring explicit hand-crafted interaction rules.
- Evidence anchors:
  - [section 3]: "At each prediction step, the decoder takes a set of motion tokens as well as the scene embedding as input and generates a distribution of N output tokens. All N motion tokens at step t can attend to each other and all previous tokens as shown in Fig. 3"
  - [section 1]: "AR decoding [28] allows the interactions between agents to be modeled via a self-attention mechanism at each step of the decoding process."
  - [corpus]: Weak - neighbors discuss MARL approaches but don't specifically validate the autoregressive architecture's effectiveness for interaction modeling.
- Break condition: If the attention mechanism cannot scale to capture long-range dependencies or complex multi-agent coordination patterns, the model's simulation quality will degrade.

## Foundational Learning

- Concept: Multi-agent Markov Decision Process (MDP) formulation
  - Why needed here: The paper frames behavior modeling as a multi-agent RL problem where each traffic agent is an agent in the MDP. Understanding this formulation is crucial for grasping how RL fine-tuning works.
  - Quick check question: What are the key components of the multi-agent MDP tuple described in the paper?

- Concept: Autoregressive sequence generation
  - Why needed here: The model generates future trajectories one step at a time, with each step depending on previous predictions. This is fundamental to understanding both the architecture and why closed-loop training matters.
  - Quick check question: How does the autoregressive decoding process differ from teacher-forcing during pre-training versus fine-tuning?

- Concept: Policy gradient methods (REINFORCE)
  - Why needed here: The paper uses REINFORCE for fine-tuning, which requires understanding policy gradients, baseline subtraction, and return normalization.
  - Quick check question: What is the role of return normalization (Eq. 4) in the REINFORCE update?

## Architecture Onboarding

- Component map: Encoder (scene context → scene embedding) → Decoder (autoregressive generation of N agents' actions) → Verlet integrator (actions → positions) → Reward computation (positions → RL loss)
- Critical path: Scene encoding → Autoregressive decoding → Position reconstruction → Reward calculation → Policy gradient update
- Design tradeoffs: Simpler reward function vs. more complex engineered rewards; autoregressive generation vs. one-shot prediction; scene-centric vs. agent-centric representations
- Failure signatures: Distribution shift (predictions drift from ground truth), collision increase, unrealistic kinematic behaviors, poor policy evaluation performance
- First 3 experiments:
  1. Run pre-trained model in closed-loop and measure ADE vs. open-loop performance to quantify distribution shift
  2. Test different collision weight values (λ) in the reward function to find optimal balance between safety and realism
  3. Compare policy evaluation performance (rank correlation, absolute error) between pre-trained, fine-tuned, and log-playback models

## Open Questions the Paper Calls Out

- Question: How does the fine-tuning approach scale to larger and more diverse datasets beyond Waymo Open Motion Dataset (WOMD)?
  - Basis in paper: [explicit] The paper mentions the use of WOMD but does not explore scaling to other datasets.
  - Why unresolved: The study focuses on WOMD, leaving questions about performance on other datasets unanswered.
  - What evidence would resolve it: Experiments showing consistent performance improvements on multiple diverse datasets.

- Question: What are the long-term effects of RL fine-tuning on model stability and generalization in real-world autonomous driving scenarios?
  - Basis in paper: [inferred] The paper discusses short-term improvements but does not address long-term stability or real-world deployment.
  - Why unresolved: The experiments are conducted in simulation, and real-world testing is not covered.
  - What evidence would resolve it: Longitudinal studies demonstrating model performance and stability in real-world driving conditions.

- Question: How does the proposed reward function impact the trade-off between safety and efficiency in autonomous driving?
  - Basis in paper: [explicit] The paper mentions a simple reward function balancing realism and safety but does not explore the trade-offs in detail.
  - Why unresolved: The impact of the reward function on driving efficiency is not thoroughly investigated.
  - What evidence would resolve it: Comparative analysis of safety and efficiency metrics under different reward function configurations.

## Limitations

- The simple reward function combining trajectory accuracy and collision penalty may not capture the full complexity of safe driving behaviors in nuanced multi-agent scenarios.
- The effectiveness of RL fine-tuning is fundamentally constrained by the quality of the pre-training data and the model's ability to learn realistic interaction patterns during pre-training.
- The study focuses on simulation environments and does not address long-term stability or generalization to real-world autonomous driving scenarios.

## Confidence

- High confidence: The claim that RL fine-tuning improves collision avoidance and off-road behavior metrics is well-supported by the evaluation results.
- Medium confidence: The claim that the autoregressive architecture naturally models multi-agent interactions is plausible but relies on assumptions about the self-attention mechanism's capabilities that aren't fully validated in the paper.
- Medium confidence: The claim that the simple reward function is sufficient for effective fine-tuning is supported by results but could benefit from more extensive ablation studies on reward design.

## Next Checks

1. **Distribution shift analysis**: Conduct systematic evaluation of how autoregressive rollouts from the fine-tuned model compare to ground truth distributions across multiple metrics (e.g., action distribution divergence, interaction pattern similarity) to quantify the extent of distribution shift improvement.

2. **Reward function ablation**: Test alternative reward formulations including more complex multi-objective rewards, shaped rewards for specific safety behaviors, and curriculum learning approaches to determine the optimal reward design for balancing safety and realism.

3. **Cross-dataset generalization**: Evaluate the fine-tuned model on datasets outside Waymo (e.g., nuScenes, Argoverse) to assess whether the improvements in safety-critical metrics generalize to different driving environments and cultural contexts.