---
ver: rpa2
title: 'RankMap: Priority-Aware Multi-DNN Manager for Heterogeneous Embedded Devices'
arxiv_id: '2411.17867'
source_url: https://arxiv.org/abs/2411.17867
tags:
- throughput
- dnns
- rankmap
- mapping
- multi-dnn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RankMap addresses the challenge of efficiently managing multiple
  concurrent DNN workloads on heterogeneous embedded devices by partitioning DNNs
  into fine-grained pipeline stages and mapping them across diverse computing components.
  It uses a VQ-VAE encoder to compress DNN layer representations and a multi-task
  attention-based CNN to predict throughput for each DNN in a workload.
---

# RankMap: Priority-Aware Multi-DNN Manager for Heterogeneous Embedded Devices

## Quick Facts
- **arXiv ID**: 2411.17867
- **Source URL**: https://arxiv.org/abs/2411.17867
- **Reference count**: 31
- **Primary result**: Achieves up to 3.6x higher average throughput compared to existing methods for multi-DNN workloads on heterogeneous embedded devices

## Executive Summary
RankMap addresses the challenge of efficiently managing multiple concurrent DNN workloads on heterogeneous embedded devices. The system partitions DNNs into fine-grained pipeline stages and maps them across diverse computing components (GPU, big CPU cluster, LITTLE CPU cluster). Using a VQ-VAE encoder to compress DNN layer representations and a multi-task attention-based CNN to predict throughput, RankMap employs Monte Carlo Tree Search to explore the vast solution space and identify optimal mappings. The approach considers DNN priorities to satisfy performance requirements while preventing starvation.

## Method Summary
RankMap uses a three-stage approach: (1) Fine-grained DNN partitioning where each DNN is divided into smaller sub-DNNs that are strategically allocated across the system's diverse computing components; (2) A VQ-VAE encoder compresses raw layer vector representations from 22 dimensions to 16 dimensions, reducing the computational load of the throughput estimator by approximately 58%; (3) Monte Carlo Tree Search with a trained multi-task attention-based CNN throughput estimator explores the solution space to find optimal mappings that maximize throughput while respecting priority constraints and preventing DNN starvation.

## Key Results
- Achieves up to 3.6x higher average throughput compared to existing methods on Orange Pi 5 with four DNNs
- Prevents DNN starvation across all workloads, addressing a critical limitation of prior approaches
- Improves prioritization of specified DNNs by 57.5x compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1: Fine-grained DNN partitioning
Fine-grained DNN partitioning into pipeline stages mapped across heterogeneous components boosts throughput by reducing resource contention and improving overall system utilization. Each DNN is split into smaller sub-DNNs that are distributed across available computing components, with the assumption that the Orange Pi 5's heterogeneous architecture can effectively execute different pipeline stages on different components without prohibitive communication overhead.

### Mechanism 2: VQ-VAE encoder compression
The VQ-VAE encoder reduces computational load of throughput estimator by compressing layer representations. Raw layer vectors are compressed into 16-dimensional distributed embeddings, reducing multiply-accumulate operations by approximately 58%, with the core assumption that compressed representations retain sufficient information to accurately predict throughput for different mappings.

### Mechanism 3: MCTS with throughput estimator
Monte Carlo Tree Search with throughput estimator efficiently explores vast solution space to find optimal mappings. MCTS stochastically prunes the search space through a decision tree, using the trained performance estimator to simulate paths and update weights, finding the mapping with highest expected reward, assuming the throughput estimator provides sufficiently accurate feedback to guide MCTS toward optimal solutions.

## Foundational Learning

- **Heterogeneous computing architectures (big.LITTLE, GPU/CPU combinations)**: Needed to understand how different computing components can be utilized simultaneously for different DNN pipeline stages. Quick check: What are the typical performance characteristics of Cortex-A76 vs Cortex-A55 cores, and when would you choose one over the other for DNN inference?

- **DNN layer profiling and computational characteristics**: Required to understand the computational profile of individual DNN layers (input/output feature maps, weights, activation types) for making partitioning decisions. Quick check: How would you characterize the computational intensity of a convolutional layer versus a fully connected layer in terms of FLOPs and memory access patterns?

- **Multi-task learning and attention mechanisms in neural networks**: Essential for understanding the throughput estimator architecture that uses a multi-task attention-based CNN to predict throughput for each DNN in a workload. Quick check: What are the advantages of using depth-wise convolutions and self-attention modules in the throughput estimator architecture?

## Architecture Onboarding

- **Component map**: DNN layer profiling → VQ-VAE encoding → throughput prediction → MCTS optimization → final mapping
- **Critical path**: DNN layer profiling → VQ-VAE encoding → throughput prediction → MCTS optimization → final mapping
- **Design tradeoffs**: Partition granularity vs. communication overhead; estimator accuracy vs. computational cost; MCTS exploration depth vs. runtime constraints; static vs. dynamic priority assignment
- **Failure signatures**: Low throughput despite complex partitioning (estimator inaccuracies); starvation occurring despite priority mechanisms (disqualification logic failure); excessive runtime for mapping decisions (MCTS inefficiency); GPU saturation despite available CPU resources (poor partitioning strategy)
- **First 3 experiments**: (1) Baseline comparison: Run all DNNs on GPU only vs. RankMap's partitioned approach on Orange Pi 5 with the 4 DNNs to verify the ~3.6x throughput improvement claim; (2) Starvation test: Create a workload mix that historically caused starvation and verify RankMap prevents it while maintaining throughput; (3) Priority validation: Test RankMapS with varying priority vectors to confirm the 57.5x prioritization improvement and that no DNN experiences starvation

## Open Questions the Paper Calls Out

### Open Question 1
How does RankMap's performance scale when handling more than five concurrent DNN workloads, particularly in terms of maintaining throughput and preventing starvation? The paper evaluates RankMap on workloads with up to five DNNs but does not explore its scalability beyond this point.

### Open Question 2
How does the choice of hardware architecture (e.g., different CPU/GPU combinations) affect RankMap's efficiency and accuracy in throughput prediction? The paper tests RankMap on a specific hardware setup (Orange Pi 5 with Mali-G610 GPU and big.LITTLE CPUs), but does not explore how different hardware configurations might impact its performance.

### Open Question 3
What are the potential trade-offs between the static and dynamic prioritization methods in RankMap, particularly in terms of throughput and resource utilization? The paper describes both static and dynamic prioritization methods but does not provide a comprehensive comparison of their trade-offs in terms of system performance.

## Limitations

- Performance evaluation is limited to a single hardware platform (Orange Pi 5), leaving generalizability to other heterogeneous systems unclear
- The synthetic workload generation methodology for training the throughput estimator is not fully specified, raising questions about real-world applicability
- The paper does not provide detailed implementation specifics for the VQ-VAE encoder architecture or the multi-task attention-based CNN throughput estimator

## Confidence

- **High confidence**: The core mechanism of fine-grained DNN partitioning and MCTS-based optimization is well-founded and theoretically sound
- **Medium confidence**: The VQ-VAE compression approach for reducing computational overhead is plausible but requires empirical validation of information retention
- **Medium confidence**: The 3.6x throughput improvement claim is specific but based on a limited hardware configuration and workload mix

## Next Checks

1. **Cross-platform validation**: Test RankMap on additional heterogeneous platforms (e.g., NVIDIA Jetson series, Coral Dev Board) to assess generalizability beyond Orange Pi 5
2. **Real-world workload evaluation**: Replace synthetic workloads with real-world multi-DNN applications (e.g., surveillance systems, autonomous vehicles) to verify practical performance gains
3. **Ablation study**: Systematically evaluate the impact of each component (VQ-VAE compression, MCTS optimization, priority mechanisms) to quantify their individual contributions to overall performance