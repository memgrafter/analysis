---
ver: rpa2
title: 'Dermatologist-like explainable AI enhances melanoma diagnosis accuracy: eye-tracking
  study'
arxiv_id: '2409.13476'
source_url: https://arxiv.org/abs/2409.13476
tags:
- dermatologists
- phase
- images
- diagnostic
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how explainable AI (XAI) systems impact
  dermatologists' diagnostic accuracy and cognitive processes when diagnosing melanoma.
  Using eye-tracking technology, 76 dermatologists participated in a reader study,
  diagnosing 16 dermoscopic images with either standard AI or XAI support.
---

# Dermatologist-like explainable AI enhances melanoma diagnosis accuracy: eye-tracking study

## Quick Facts
- arXiv ID: 2409.13476
- Source URL: https://arxiv.org/abs/2409.13476
- Reference count: 40
- Primary result: XAI improves melanoma diagnosis accuracy by 2.8 percentage points and reveals cognitive load patterns through eye-tracking

## Executive Summary
This study investigates how explainable AI (XAI) systems impact dermatologists' diagnostic accuracy and cognitive processes when diagnosing melanoma. Using eye-tracking technology, 76 dermatologists participated in a reader study, diagnosing 16 dermoscopic images with either standard AI or XAI support. The XAI system provided domain-specific explanations for its predictions. Results showed that XAI improved balanced diagnostic accuracy by 2.8 percentage points compared to standard AI. Additionally, diagnostic disagreements with AI/XAI systems and complex lesions were associated with increased ocular fixations, indicating higher cognitive load.

## Method Summary
The study employed a concept-bottleneck XAI model trained on dermoscopic images from HAM10000 and SCP2 datasets. The model predicted both diagnosis and dermoscopic characteristics using annotated and unannotated images. A reader study was conducted where 76 dermatologists diagnosed 16 dermoscopic images under three conditions: without AI, with standard AI, and with XAI. Eye-tracking data was collected throughout to assess cognitive load. The XAI provided textual and region-based explanations based on predicted characteristics. Balanced diagnostic accuracy and fixation metrics were the primary outcomes measured.

## Key Results
- XAI improved balanced diagnostic accuracy by 2.8 percentage points compared to standard AI
- Diagnostic disagreements with AI/XAI systems were associated with increased ocular fixations, indicating higher cognitive load
- More experienced dermatologists demonstrated fewer ocular fixations during diagnosis, suggesting more efficient visual processing patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XAI improves diagnostic accuracy over standard AI by providing domain-specific explanations that guide clinicians' attention to relevant lesion features
- Mechanism: The XAI system uses concept-bottleneck models trained to predict established diagnostic characteristics (e.g., Derm7pt checklist items), which generates textual and region-based explanations for its predictions
- Core assumption: Clinicians can effectively integrate XAI explanations into their diagnostic reasoning process and use this information to improve accuracy
- Evidence anchors:
  - [abstract] "XAI systems improved balanced diagnostic accuracy by 2.8 percentage points relative to standard AI"
  - [section] "Our XAI achieved a balanced accuracy of 86.5%... In comparison, a baseline ResNet50 classifier achieved a balanced accuracy of 83.6%"

### Mechanism 2
- Claim: Diagnostic disagreements between clinicians and AI/XAI systems correlate with increased ocular fixations, indicating higher cognitive load during decision-making
- Mechanism: When clinicians disagree with system predictions, they engage in deeper cognitive processing, revisiting specific details and searching for inconsistencies
- Core assumption: Fixation count and duration are valid proxies for cognitive load and information processing effort
- Evidence anchors:
  - [abstract] "diagnostic disagreements with AI/XAI systems... were associated with elevated cognitive load, as evidenced by increased ocular fixations"
  - [section] "In the AI phase, the mean fixation count was 14.2... for cases where the classifier and the dermatologist agreed, and 19.6... for cases where they disagreed"

### Mechanism 3
- Claim: More experienced dermatologists demonstrate fewer ocular fixations during diagnosis, indicating more efficient visual processing patterns
- Mechanism: Through training and experience, dermatologists develop efficient search patterns that allow them to rapidly identify key diagnostic features with fewer fixations
- Core assumption: Fixation count decreases with experience due to the development of expert visual search strategies rather than reduced diligence
- Evidence anchors:
  - [section] "dermatologist experience was negatively correlated with ocular fixations during the AI phase and also during the XAI phase"
  - [section] "experienced dermatologists tend to have fewer fixations, suggesting more efficient visual processing"

## Foundational Learning

- Concept: Explainable AI (XAI) methods and their classification
  - Why needed here: Understanding the difference between post-hoc and inherently interpretable XAI methods is crucial for designing and evaluating the XAI system used in this study
  - Quick check question: What are the two primary branches of XAI techniques, and how do they differ in their approach to interpretability?

- Concept: Eye-tracking metrics and their interpretation in cognitive science
  - Why needed here: The study relies heavily on fixation counts and durations as proxies for cognitive load and attention allocation during diagnosis
  - Quick check question: What specific eye-tracking metrics were used to assess cognitive load, and what do higher fixation counts typically indicate about cognitive processing?

- Concept: Balanced accuracy and its importance in imbalanced classification tasks
  - Why needed here: The study uses balanced accuracy to evaluate diagnostic performance, which is particularly important when dealing with imbalanced datasets (more nevi than melanomas)
  - Quick check question: Why is balanced accuracy preferred over standard accuracy when evaluating diagnostic performance on imbalanced datasets?

## Architecture Onboarding

- Component map: Input image → concept-bottleneck classifier → two prediction heads (diagnosis and characteristics) → explanation generation module → web interface with textual and region-based explanations

- Critical path: Input image → concept-bottleneck classifier → prediction heads → explanation generation → presentation to clinician → clinician diagnosis with eye-tracking

- Design tradeoffs: The XAI system sacrifices some generalization performance to provide interpretable explanations, as evidenced by the lower external test set accuracy (76.9%) compared to internal performance (86.5%). The decision to present only the most confident explanation rather than all predicted explanations was made to avoid information overload.

- Failure signatures: Key failure modes include: (1) poor generalization on external datasets, (2) explanations that do not align with clinician expectations, (3) excessive cognitive load from complex explanations, and (4) eye-tracking data quality issues in the webcam-based setup

- First 3 experiments:
  1. Replicate the balanced accuracy comparison between XAI and standard AI on the internal test set to verify the 2.8 percentage point improvement
  2. Test the correlation between clinician experience levels and fixation counts on a separate dataset to validate the negative correlation finding
  3. Evaluate explanation quality through a clinician survey asking participants to rate the helpfulness and accuracy of XAI explanations after the study

## Open Questions the Paper Calls Out

- Question: Does the improvement in diagnostic accuracy with XAI support persist in real-world clinical settings where patient metadata and additional clinical information are available?
- Basis in paper: [explicit] The paper acknowledges that the study did not resemble real-world clinical settings where dermatologists have access to relevant patient metadata
- Why unresolved: The study was conducted in a controlled environment without access to additional patient information that would typically be available in clinical practice
- What evidence would resolve it: A clinical trial comparing diagnostic accuracy of dermatologists using XAI with access to full patient records versus standard diagnostic methods in actual clinical practice

## Limitations

- The study used webcam-based eye-tracking rather than dedicated hardware, potentially introducing measurement noise
- The 2.8 percentage point improvement in balanced accuracy, while statistically significant, represents a relatively modest clinical gain
- The study population consisted of experienced dermatologists, limiting generalizability to less experienced clinicians or primary care settings

## Confidence

- High confidence: The negative correlation between dermatologist experience and fixation count, as this finding aligns with established cognitive science literature on expertise development
- Medium confidence: The 2.8 percentage point improvement in balanced accuracy, as this is based on a controlled reader study but with a modest effect size
- Medium confidence: The correlation between diagnostic disagreements and increased fixations, as this interpretation relies on fixation metrics as proxies for cognitive load

## Next Checks

1. Conduct a multi-center study with a more diverse population of clinicians (including residents and general practitioners) to assess generalizability of XAI benefits across experience levels
2. Implement a randomized controlled trial comparing standard AI, XAI, and no-AI conditions to isolate the specific contribution of explainability to diagnostic accuracy
3. Perform qualitative interviews with participating dermatologists to assess the perceived usefulness and interpretability of the XAI explanations in clinical practice