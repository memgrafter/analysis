---
ver: rpa2
title: Sparse Autoencoders Reveal Temporal Difference Learning in Large Language Models
arxiv_id: '2410.01280'
source_url: https://arxiv.org/abs/2410.01280
tags:
- learning
- llama
- representations
- uni00000014
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how large language models (LLMs) learn to
  solve reinforcement learning (RL) problems in-context, without explicit training
  on RL objectives. The authors propose using Sparse Autoencoders (SAEs) to analyze
  the residual stream of Llama 3 70B and identify representations that correspond
  to temporal difference (TD) errors, a key learning signal in RL.
---

# Sparse Autoencoders Reveal Temporal Difference Learning in Large Language Models

## Quick Facts
- arXiv ID: 2410.01280
- Source URL: https://arxiv.org/abs/2410.01280
- Reference count: 31
- Key outcome: SAEs identify TD-like representations in LLMs solving RL tasks in-context

## Executive Summary
This paper investigates how large language models learn to solve reinforcement learning problems without explicit RL training. The authors use Sparse Autoencoders to analyze Llama 3 70B's residual stream and identify representations corresponding to temporal difference (TD) errors, a key learning signal in RL. Through three different tasks, they demonstrate that Llama can solve simple RL problems in-context and that its internal representations encode variables similar to Q-values and TD errors. By performing interventions on these representations, they show that they are causally involved in the computation.

## Method Summary
The study uses Sparse Autoencoders to analyze residual stream activations from Llama 3 70B when prompted with RL tasks. SAEs are trained separately for each transformer block to map 8192-dimensional residual activations to sparse latents. The authors then correlate these latents with model-based TD errors and Q-values computed from task prompts. Interventions (lesions and clamping) are performed on identified TD-like features to assess their causal role in behavior and representation learning.

## Key Results
- Llama learns TD-like error representations in-context from next-token prediction alone
- Identified TD latents are causally involved in RL behavior and Q-value computation
- LLMs learn successor representations in graph structure tasks using TD-like learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Llama learns TD-like error representations in-context from next-token prediction alone.
- Mechanism: Residual stream activations encode future discounted rewards and TD errors via gradually emerging SAE features across transformer blocks.
- Core assumption: LLMs can discover RL-relevant representations even without explicit RL training, because next-token prediction implicitly requires modeling of sequential structure and prediction errors.
- Evidence anchors:
  - [abstract] "We find representations that closely match temporal difference (TD) errors... despite the model only being trained to predict the next token."
  - [section 3] "we observe features that show significant correlation with TD errors (see Figure 2D). In block 34, we find the SAE feature with the highest correlation ( r = 0 .58) with the TD error"
  - [corpus] FMR score for temporal SAEs: 0.580 (moderate), suggesting community awareness of temporal interpretability in LLMs.
- Break condition: If SAEs fail to find monosemantic TD-like latents or correlations drop below r ≈ 0.5, the mechanism would be questioned.

### Mechanism 2
- Claim: Identified TD latents are causally involved in RL behavior and Q-value computation.
- Mechanism: Lesioning or clamping the SAE feature that correlates with TD error degrades task performance and disrupts subsequent Q-value/TD error representations.
- Core assumption: A single latent feature can encode TD-like signals that directly influence action selection.
- Evidence anchors:
  - [section 3] "simply deactivating the TD latent led to significantly worse performance in the task... producing behavior that differed more from a Q-learning agent"
  - [section 4] "lesioning the TD latents significantly degraded Llama's ability to predict actions, whereas the control lesions led to very small changes"
  - [corpus] No direct corpus evidence, inference drawn from ablation results.
- Break condition: If lesioning does not affect behavior or only affects non-TD related latents, the causal claim would weaken.

### Mechanism 3
- Claim: LLMs learn successor representations (SR) in graph structure tasks using TD-like learning.
- Mechanism: SAE latents correlate more strongly with SR than with transition matrices, and lesioning TD latents reduces prediction accuracy and distorts community structure in MDS projections.
- Core assumption: TD learning is not limited to reward-based RL but can also learn abstract state occupancies and graph geometry.
- Evidence anchors:
  - [section 5] "We found stronger correlations with the SR (max r = 0 .62) than with the transition matrix (max r = 0 .49) throughout the model"
  - [section 5] "Llama represents the environment like the SR and learns this representation through TD learning"
  - [corpus] FMR score for physics representations: 0.570 (moderate), suggesting some prior work on emergent physical structure learning in LLMs.
- Break condition: If correlations with SR are not higher than with transition counts, or lesioning has no effect on prediction accuracy, the mechanism fails.

## Foundational Learning

- Concept: Temporal difference (TD) learning
  - Why needed here: TD errors are the core learning signal used by Llama to update Q-values and successor representations.
  - Quick check question: What is the formula for a TD error in a one-step prediction setting?

- Concept: Sparse Autoencoders (SAEs)
  - Why needed here: SAEs disentangle high-dimensional residual stream activations into interpretable, monosemantic features that can be correlated with TD errors and Q-values.
  - Quick check question: What is the role of the L1 regularization term in SAE training?

- Concept: Successor Representation (SR)
  - Why needed here: SR is learned via TD updates and captures long-term state occupancy structure, important for graph prediction tasks.
  - Quick check question: How does the SR differ from a transition matrix in terms of discounting future states?

## Architecture Onboarding

- Component map: Input prompts -> Llama transformer (80 blocks) -> Residual stream -> SAE (per block) -> Sparse latents -> Correlation analysis -> Intervention experiments -> Behavioral metrics

- Critical path:
  1. Prompt Llama with RL task
  2. Record residual stream activations at action tokens
  3. Train SAEs on collected representations
  4. Correlate latents with TD/Q-values
  5. Identify TD-like latents and perform lesion/clamp experiments
  6. Evaluate impact on behavior and subsequent representations

- Design tradeoffs:
  - SAE dimensionality: Higher dims capture more features but increase compute; authors chose ~8192 to match input
  - Regularization strength β: Balances reconstruction accuracy vs. sparsity; authors used 1e-5
  - Training data diversity: Using multiple task variants improves latent detection robustness

- Failure signatures:
  - Low correlation (r < 0.3) between latents and TD errors/Q-values
  - No performance drop after lesioning presumed TD latents
  - Residual stream activations not reconstructible by SAE (high reconstruction error)

- First 3 experiments:
  1. Train SAE on residual activations from Llama in Two-Step Task; find latents correlating with TD error
  2. Perform lesion on top TD latent; measure return drop and behavioral shift from Q-learning model
  3. Repeat in Grid World task; check if SAE latents track Q-values and TD errors over time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sparse autoencoder (SAE) architecture be generalized to identify RL-related representations across diverse tasks without requiring task-specific training?
- Basis in paper: [inferred] The authors note that their SAEs are task-specific and not suitable for identifying RL-related variables for arbitrary tasks, suggesting a need for more generic SAEs.
- Why unresolved: The current approach requires separate SAE training for each task, limiting scalability and generalizability.
- What evidence would resolve it: Demonstrating that a single SAE, trained on a diverse corpus of RL tasks, can identify RL-related representations across new, unseen RL tasks with comparable accuracy to task-specific SAEs.

### Open Question 2
- Question: What are the specific transformer circuit mechanisms, such as attention patterns or induction heads, that enable the emergence of TD-like representations in LLMs?
- Basis in paper: [inferred] The authors acknowledge that while they mapped TD errors and Q-values across transformer blocks, the circuit-level understanding of how these representations arise remains unclear.
- Why unresolved: The analysis focuses on residual stream representations rather than dissecting the internal transformer computations that generate these representations.
- What evidence would resolve it: Identifying specific attention heads or other transformer components whose manipulation consistently disrupts TD-like representations or RL performance across multiple tasks.

### Open Question 3
- Question: What additional computational components beyond TD learning are necessary to fully account for LLM behavior in RL tasks, particularly their tendency to repeat actions?
- Basis in paper: [explicit] The authors note that while they can predict LLM behavior using Q-learning agents and find TD-like representations, their alignment is not perfect, suggesting missing components.
- Why unresolved: The current behavioral models (Q-learning, myopic Q-learning, repetition) do not fully capture the complexity of LLM decision-making in RL contexts.
- What evidence would resolve it: Developing a behavioral model that incorporates both TD learning and action repetition biases, which significantly outperforms simpler models in predicting LLM choices across multiple RL tasks.

## Limitations
- SAEs provide lossy reconstructions and may capture correlated but not identical concepts to theoretical TD errors
- Causal claims rely on indirect lesion experiments that cannot definitively prove exact correspondence
- Study focuses on simple RL tasks, leaving questions about scalability to complex environments

## Confidence
- Mechanism 1 (TD-like error discovery): Medium - Strong correlation evidence exists, but SAE limitations prevent high confidence in exact correspondence
- Mechanism 2 (Causal involvement): Medium - Behavioral changes after lesions support causality, but alternative explanations exist
- Mechanism 3 (Successor representation learning): Medium - Correlations favor SR over transition matrices, but the evidence is correlational rather than definitive

## Next Checks
1. Cross-task generalizability test: Apply the same SAE analysis to a more complex RL environment (e.g., Atari games or continuous control tasks) to verify whether the TD-learning representations scale beyond simple grid-based and graph tasks.

2. Ablation control study: Implement additional control interventions targeting random latents and comparing their behavioral effects against the identified TD latents to strengthen causal claims and rule out general perturbation effects.

3. Feature attribution analysis: Use integrated gradients or similar attribution methods to quantify how much of the model's action selection probability can be attributed to the identified TD-like features versus other components, providing more direct evidence of their functional importance.