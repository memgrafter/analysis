---
ver: rpa2
title: '$\text{Memory}^3$: Language Modeling with Explicit Memory'
arxiv_id: '2407.01178'
source_url: https://arxiv.org/abs/2407.01178
tags:
- memory
- knowledge
- explicit
- training
- memories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Memory3, a novel approach to language modeling
  that addresses the high computational costs associated with training and inference
  of large language models (LLMs). Inspired by the human brain's memory hierarchy,
  Memory3 incorporates explicit memory as a third form of memory alongside implicit
  memory (model parameters) and working memory (context key-values).
---

# $\text{Memory}^3$: Language Modeling with Explicit Memory

## Quick Facts
- arXiv ID: 2407.01178
- Source URL: https://arxiv.org/abs/2407.01178
- Authors: Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, Zhiyu Li, Bo Tang, Wenqiang Wei, Jinbo Wang, Zeyun Tang, Shichao Song, Chenyang Xi, Yu Yu, Kai Chen, Feiyu Xiong, Linpeng Tang, Weinan E
- Reference count: 0
- Key outcome: Introduces Memory3, a 2.4B parameter LLM that outperforms larger models and RAG systems on benchmarks while maintaining higher decoding speed

## Executive Summary
Memory3 introduces explicit memory as a third form of memory for LLMs, addressing the high computational costs of training and inference. Inspired by human brain memory hierarchy, it externalizes specific knowledge from model parameters into sparse attention key-values that are more efficient to store and retrieve. The approach achieves better performance than larger models on general benchmarks while maintaining higher decoding speed than RAG, demonstrating improved factuality and interpretability.

## Method Summary
Memory3 trains a 2.4B parameter LLM from scratch using a two-stage pretraining scheme. The warmup stage uses ordinary pretraining to develop reading comprehension, while the continual train stage incorporates explicit memories encoded from a knowledge base of 1.1×10^8 text chunks. Explicit memories are sparse attention key-values retrieved during inference and integrated into self-attention layers alongside context key-values. The model uses memory sparsification and a novel two-stage pretraining approach to achieve its results.

## Key Results
- 2.4B parameter Memory3 outperforms larger LLMs and RAG models on general benchmarks
- Maintains higher decoding speed than RAG systems
- Demonstrates improved factuality and interpretability compared to existing models
- Achieves better performance with lower parameter count and computational costs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit memory reduces overall cost by placing knowledge where write and read costs are balanced
- Mechanism: Knowledge is categorized into specific (fixed output) and abstract (variable output). Specific knowledge is externalized to explicit memory, reducing parameter size and training/inference costs
- Core assumption: The memory hierarchy of humans (implicit, explicit, external) applies analogously to LLMs
- Evidence anchors: Abstract mentions explicit memory's moderate write and read costs; section 2.3 discusses cost minimization through memory format assignment

### Mechanism 2
- Claim: Memory3 alleviates knowledge traversal by retrieving only needed parameters for each token
- Mechanism: During inference, explicit memories are retrieved and integrated into self-attention layers alongside context key-values, allowing focused computation per token
- Core assumption: Attention from context to references is concentrated on very few tokens
- Evidence anchors: Abstract mentions retrieving only needed parameters; section 3.3 discusses memory sparsification to reduce space occupation

### Mechanism 3
- Claim: Two-stage pretraining (warmup + continual train) is necessary for effective explicit memory formation
- Mechanism: Warmup stage trains without explicit memory to develop reading comprehension; continual train stage introduces explicit memories for memory formation
- Core assumption: Early exposure to explicit memories distracts the model, preventing it from learning to utilize them
- Evidence anchors: Section 3.6 observes that pretraining with explicit memories from the beginning would render memories useless

## Foundational Learning

- Concept: Memory hierarchy (implicit, explicit, external)
  - Why needed here: Justifies the design of explicit memory as a middle ground between parameters and plain text
  - Quick check question: What are the trade-offs between write and read costs for each memory type?

- Concept: Knowledge decomposition into circuits
  - Why needed here: Enables identification of specific vs. abstract knowledge for externalization
  - Quick check question: How does a circuit definition differ from input-output relations alone?

- Concept: Sparse computation and attention
  - Why needed here: Underpins the memory sparsification mechanism to reduce storage and compute
  - Quick check question: Why is top-k token selection based on attention weights effective for explicit memory?

## Architecture Onboarding

- Component map: Reference dataset → Explicit memory encoder (Lmem layers) → Disk storage → Vector embedding → FAISS index → Memory retrieval → GPU memory → Transformer layers (Lmem memory layers + L-Lmem standard layers) ← Concatenated context + explicit memories

- Critical path: Retrieval → Memory loading → Attention integration → Token generation

- Design tradeoffs:
  - Sparsity vs. coverage: 8 tokens per head balances storage vs. information loss
  - Memory layers placement: Front half vs. middle layers affects retrieval effectiveness
  - Warm start vs. cold start: Precomputation saves time but requires storage

- Failure signatures:
  - Memory not utilized: Training loss same with/without explicit memory
  - Retrieval misses: Low recall in vector search or poor embedding model
  - GPU OOM: Memory loading exceeds available VRAM

- First 3 experiments:
  1. Verify memory encoding produces correct key-value shapes and sparsification
  2. Test retrieval integration by comparing attention weights with/without explicit memories
  3. Measure decoding speed impact with varying retrieval frequencies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Memory3 model be optimized to reduce the time consumption associated with loading memory key-values from drives to GPU?
- Basis in paper: The paper mentions that loading explicit memories from drives to GPU is a major overhead
- Why unresolved: Acknowledges this issue but does not provide a detailed solution
- What evidence would resolve it: Experimental results demonstrating optimization techniques effectiveness

### Open Question 2
- Question: Can the Memory3 model be adapted to use an internalized retrieval process that matches sparse attention queries with memory keys?
- Basis in paper: Suggests this as a future direction in Remark 2
- Why unresolved: This approach has not been implemented or tested
- What evidence would resolve it: Experimental results comparing performance with and without internalized retrieval

### Open Question 3
- Question: How can the Memory3 model be designed to retain knowledge more effectively when its parameters are updated?
- Basis in paper: Mentions this as a potential issue in Remark 9 regarding catastrophic forgetting
- Why unresolved: Does not provide a detailed solution for robust memory system design
- What evidence would resolve it: Experimental results demonstrating approaches to maintaining knowledge retention

## Limitations

- Knowledge base composition and quality are not fully specified, affecting reproducibility
- Knowledge decomposition into circuits lacks rigorous formalization and scalability evidence
- Performance during fine-tuning and adaptation to new domains is not adequately addressed

## Confidence

**High Confidence:**
- Explicit memory reduces parameter count and storage requirements
- Two-stage pretraining approach is necessary for effective explicit memory integration
- Memory sparsification techniques successfully reduce memory footprint

**Medium Confidence:**
- Explicit memory provides better factuality than standard LLMs
- Human brain memory hierarchy analogy provides useful guidance
- Memory3 achieves superior performance on specified benchmarks

**Low Confidence:**
- Explicit memory universally reduces training and inference costs across all applications
- The 8-token per head sparsification threshold is optimal for all use cases
- Knowledge decomposition into circuits is robust and scalable to arbitrary domains

## Next Checks

1. **Ablation Study on Knowledge Decomposition**: Systematically test performance impact of varying thresholds for specific versus abstract knowledge identification and measure decomposition strategy effects.

2. **Fine-tuning Transferability Evaluation**: Evaluate Memory3's performance after fine-tuning on substantially different datasets (medical, legal, technical) and compare learning curves against standard LLMs.

3. **Retrieval Quality Analysis**: Conduct detailed analysis of memory retrieval quality across different query types, measuring precision, recall, and relevance, then correlate with downstream task performance.