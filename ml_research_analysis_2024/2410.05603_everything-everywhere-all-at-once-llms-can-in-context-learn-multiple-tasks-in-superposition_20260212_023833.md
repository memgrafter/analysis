---
ver: rpa2
title: 'Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks
  in Superposition'
arxiv_id: '2410.05603'
source_url: https://arxiv.org/abs/2410.05603
tags:
- task
- tasks
- in-context
- arxiv
- superposition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that large language models can perform multiple
  distinct in-context learning tasks simultaneously during a single inference call,
  a phenomenon termed "task superposition." The authors provide empirical evidence
  across multiple LLM families (GPT-3.5, Llama-3, Qwen) showing that models can concurrently
  execute tasks like addition, translation, and text transformation when provided
  with mixed prompts containing examples from different tasks. Theoretical results
  prove that transformers have the expressive capacity to implement such superposition.
---

# Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition

## Quick Facts
- arXiv ID: 2410.05603
- Source URL: https://arxiv.org/abs/2410.05603
- Reference count: 40
- LLMs can perform multiple distinct in-context learning tasks simultaneously during a single inference call

## Executive Summary
This paper reveals that large language models can perform multiple distinct in-context learning tasks simultaneously during a single inference call, a phenomenon termed "task superposition." The authors provide empirical evidence across multiple LLM families (GPT-3.5, Llama-3, Qwen) showing that models can concurrently execute tasks like addition, translation, and text transformation when provided with mixed prompts containing examples from different tasks. Theoretical results prove that transformers have the expressive capacity to implement such superposition. The study reveals that larger models can handle more parallel tasks and better calibrate their output distributions to match the in-context task example distribution.

## Method Summary
The researchers created mixed prompts containing 20 random examples per task, ordered randomly, and tested whether LLMs could perform multiple tasks simultaneously. They calculated output probabilities for correct answers using a specific method (Equation 3 in Appendix B) and measured task completion rates and KL-divergence between model output distribution and in-context task example distribution. The study used various LLM families including GPT-3.5, Llama-3, and Qwen, and also trained small GPT-2 models from scratch on one task at a time to see if superposition emerges. Task vectors were extracted to explore internal composition during superposition, and model sizes were varied to test scalability of task superposition capabilities.

## Key Results
- LLMs assign non-negligible probabilities to correct outputs for multiple tasks simultaneously when given mixed prompts
- Larger models can handle more parallel tasks and better calibrate their output distributions to match in-context task example distribution
- Convex combinations of individual task vectors can partially reproduce the superposition effect observed in in-context learning
- Theoretical construction proves transformers can implement task superposition with O(d + log(mn)) embedding dimension

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can simultaneously maintain and utilize multiple task distributions, resulting in outputs that reflect a combination of relevant tasks.
- Mechanism: The model treats task selection as a latent variable and performs implicit Bayesian inference over possible tasks given the prompt. The output probability is approximated as a weighted sum of conditional probabilities across possible tasks: P(output|prompt) ≈ Σ_task P(output|task, prompt)P(task|prompt).
- Core assumption: The model's internal representation allows for parallel computation of multiple task-specific transformations, and the attention mechanism can selectively weight these transformations based on in-context examples.
- Evidence anchors: Figure 2 reveals that in all four sets of tasks, all models have non-negligible median values of probabilities for at least two tasks. The mental model of LLMs as superpositions of task-specific simulators offers a conceptual framework for the observed phenomenon.
- Break condition: If the model's internal representations cannot maintain separate task vectors simultaneously, or if attention cannot properly weight these vectors based on in-context evidence.

### Mechanism 2
- Claim: Transformers have the inherent expressivity to perform multiple tasks in superposition with a single inference call.
- Mechanism: The model uses parallel attention heads to implement each task independently, then combines the results using task identifiers derived from the in-context examples. The theoretical construction shows how to create indicator vectors that identify which tasks are present and weight their predictions accordingly.
- Core assumption: The transformer architecture with its parallel attention heads can implement multiple independent transformations simultaneously, and the MLP layers can perform the necessary vector operations for task identification and combination.
- Evidence anchors: Theorem 1 proves a seven layer transformer with embedding dimension O(d + log(mn)) with K heads per attention layer can perform K tasks on vectors of dimension d in superposition. The construction uses vectors based on L1 norm differences between predicted and actual outputs.
- Break condition: If the transformer cannot implement the required attention patterns and MLP operations simultaneously, or if the task identification process fails due to approximation errors.

### Mechanism 3
- Claim: Larger models can better calibrate their output distribution to match the in-context task example distribution.
- Mechanism: As model size increases, the model's capacity to maintain separate task representations and properly weight them based on in-context evidence improves. Larger models have more parameters to dedicate to each task and better approximation capabilities for the necessary vector operations.
- Core assumption: Model scaling leads to better internal representations and more accurate task identification/calibration, without introducing interference between task representations.
- Evidence anchors: Experiments show bigger models have higher r values (more correct answers in top-K probable outputs) and smaller KL-divergence values between model output distribution and in-context task example distribution.
- Break condition: If larger models introduce more interference between task representations, or if scaling does not improve task identification/calibration capabilities.

## Foundational Learning

- Concept: In-context learning (ICL) - the ability of LLMs to perform tasks during inference without fine-tuning, simply by providing examples within the input prompt.
  - Why needed here: Task superposition builds directly on ICL capabilities, extending them from single-task to multi-task execution within a single inference call.
  - Quick check question: If a model can perform addition given the prompt "2+2=4, 3+3=", what ICL capability is it demonstrating?

- Concept: Transformer architecture - the neural network structure that processes sequences using self-attention mechanisms and feed-forward layers.
  - Why needed here: The paper's theoretical construction relies on specific transformer components (attention heads, MLP layers, positional encodings) to implement task superposition.
  - Quick check question: What component of the transformer allows different parts of the input to attend to each other regardless of position?

- Concept: Task vectors - high-dimensional representations that encode the algorithm a model uses to solve a specific task given in-context demonstrations.
  - Why needed here: The paper explores how task vectors are combined during superposition, showing that convex combinations of individual task vectors can reproduce the superposition effect.
  - Quick check question: If you have task vectors for "translate to French" and "translate to German", what operation would you perform to create a vector for "translate to both languages"?

## Architecture Onboarding

- Component map: Input -> Tokenization -> Embedding lookup -> Parallel task execution (attention heads) -> Task identification (MLP operations) -> Weighted combination -> Output generation

- Critical path: Input → Tokenization → Embedding lookup → Parallel task execution (attention heads) → Task identification (MLP operations) → Weighted combination → Output generation

- Design tradeoffs:
  - Model size vs. task capacity: Larger models can handle more parallel tasks but require more computation
  - Precision vs. efficiency: More accurate task identification requires more complex MLP operations
  - Generalization vs. specialization: Models trained on one task at a time can still perform superposition, but may be less accurate

- Failure signatures:
  - Generation collapse: After first token, model converges to single task
  - Task interference: Representations of different tasks corrupt each other
  - Poor calibration: Output distribution doesn't match in-context task example distribution

- First 3 experiments:
  1. Test task superposition on simple copy tasks with varying numbers of in-context examples to verify the theoretical construction
  2. Compare task superposition capabilities across different model families (GPT, Llama, Qwen) with the same prompt structure
  3. Investigate the relationship between model size and number of tasks that can be performed in superposition using the Qwen family as a testbed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop decoding algorithms that maintain multi-task state throughout generation to fully leverage task superposition capabilities?
- Basis in paper: The paper discusses "generation collapse" as a limitation where models converge on single-task prediction after the first token, and mentions that conventional decoding algorithms are not equipped to leverage superposition.
- Why unresolved: Current decoding strategies lack mechanisms to preserve the multi-task state across token generation, and the paper only briefly mentions potential directions without concrete solutions.
- What evidence would resolve it: Development and empirical validation of decoding algorithms that successfully maintain multiple task distributions across generations, demonstrated through improved performance on mixed-task prompts.

### Open Question 2
- Question: What are the mechanistic explanations for task superposition beyond convex combinations of task vectors?
- Basis in paper: The paper shows that while convex combinations of task vectors can partially reproduce superposition effects, they don't fully explain the phenomenon, as indicated by larger irrelevant output probabilities compared to in-context superposition.
- Why unresolved: The paper identifies a gap between vector interpolation results and actual in-context superposition behavior, suggesting other underlying mechanisms.
- What evidence would resolve it: Discovery and validation of alternative internal mechanisms (e.g., attention patterns, activation patterns) that explain how models maintain and combine multiple task distributions.

### Open Question 3
- Question: How does task superposition scale with model architecture (not just size) and what architectural modifications could enhance this capability?
- Basis in paper: The paper shows that larger models within the same family can handle more parallel tasks and better calibrate output distributions, but doesn't explore architectural variations beyond size.
- Why unresolved: The study focuses on scale within model families but doesn't investigate how different architectural choices (e.g., attention mechanisms, layer designs) affect superposition capabilities.
- What evidence would resolve it: Comparative studies of different transformer architectures (e.g., Mamba, state-space models) showing how architectural choices impact task superposition capacity and quality.

## Limitations
- Theoretical construction relies on idealized assumptions about attention patterns and MLP operations that may not hold in practice
- The exact mechanism by which task vectors combine during superposition remains unclear despite identification of task vectors
- KL-divergence measurements alone don't capture task quality or semantic correctness of outputs

## Confidence
- **High Confidence**: Empirical demonstration of task superposition across multiple LLM families with clear statistical evidence
- **Medium Confidence**: Theoretical construction proving transformer expressivity for superposition is mathematically sound but relies on idealized assumptions
- **Low Confidence**: Exact mechanism by which task vectors combine during superposition remains speculative

## Next Checks
1. Design experiments that systematically vary the similarity between tasks to quantify how task similarity affects superposition quality and identify interference patterns
2. Test whether the theoretical embedding dimension requirement is necessary by training small transformers with deliberately constrained dimensions to see at what point superposition capabilities degrade
3. Develop interventions that modify task vectors during inference to directly test whether these representations causally determine superposition behavior