---
ver: rpa2
title: 'Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European
  Non-verbal Cultural Characteristics and their Influences on Engagement'
arxiv_id: '2409.13726'
source_url: https://arxiv.org/abs/2409.13726
tags:
- engagement
- https
- japanese
- features
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of understanding and modeling
  cultural differences in non-verbal behavior and their impact on engagement in dyadic
  conversations. To achieve this, the authors expanded the existing NoXi dataset by
  adding Japanese and Chinese recordings, creating the NoXi+J corpus.
---

# Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement

## Quick Facts
- arXiv ID: 2409.13726
- Source URL: https://arxiv.org/abs/2409.13726
- Reference count: 40
- One-line primary result: Culture-specific engagement models outperform generic ones, with transfer learning improving predictions for Japanese sessions.

## Executive Summary
This study expands the NoXi corpus with Japanese and Chinese recordings to create NoXi+J, enabling cross-cultural analysis of non-verbal behavior and engagement in dyadic conversations. Using multimodal features (speech, facial expressions, gestures, body properties) extracted from 150 sessions across five languages, the authors train LSTM models to predict engagement and analyze feature importance via SHAP. The results reveal culture-specific patterns: while fluidity is universally important, European languages show higher relevance for speaking turn and silence, and Japanese sessions exhibit strong mutual influence between interlocutors. Transfer learning from European to Japanese models significantly improves prediction accuracy, validating the potential of culture-sensitive engagement modeling.

## Method Summary
The study uses the NoXi+J dataset (150 sessions in German, French, English, Japanese, Chinese) with 49 multimodal features extracted from Kinect and audio streams. A 4-cell LSTM with 30-frame window predicts engagement (0-1) from these features. Models are trained separately on each language speaker group (LSG) and globally, with SHAP analysis revealing feature importance. Transfer learning is applied by fine-tuning pre-trained models on target LSGs. Performance is evaluated using MSE loss and cross-language comparisons.

## Key Results
- Culture-specific LSTM models outperform generic models in engagement prediction accuracy.
- SHAP analysis shows fluidity is universally important, while speaking turn and silence are more relevant for European languages.
- Transfer learning from European to Japanese LSG models reduces MSE loss from 0.040–0.047 to 0.015–0.017.
- Japanese sessions show strong positive correlation (r=0.51) between novice and expert engagement, unlike European sessions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cultural-specific engagement modeling improves prediction accuracy when combined with transfer learning.
- Mechanism: Transfer learning adapts engagement models trained on one cultural group (e.g., European) to better predict engagement in another group (e.g., Japanese) by leveraging shared non-verbal feature importance patterns.
- Core assumption: Non-verbal features like fluidity and speaking turn have culture-invariant predictive power for engagement, even if their relative importance varies.
- Evidence anchors:
  - [abstract] "A SHAP analysis combined with transfer learning confirmed a considerable correlation between the importance of input features for a language set and the significant cultural characteristics analyzed."
  - [section] "Transfer learning was most successful on the Japanese LSG training set, reducing the loss from 0.040–0.047 to 0.015–0.017."
- Break condition: If non-verbal features lack any shared predictive patterns across cultures, transfer learning gains will be minimal or nonexistent.

### Mechanism 2
- Claim: Culture-specific engagement models outperform generic models due to better feature alignment.
- Mechanism: Models trained on specific language speaker groups (LSGs) capture culturally unique patterns in non-verbal behavior (e.g., Japanese head movement vs. European speaking turn), leading to higher accuracy than models trained on mixed or global datasets.
- Core assumption: Distinct cultures exhibit statistically significant differences in non-verbal feature distributions that align with engagement, as revealed by ANOVA and Tukey-Kramer tests.
- Evidence anchors:
  - [abstract] "SHAP analysis revealed that while fluidity was universally important, other features like speaking turn and silence were more relevant for European languages."
  - [section] "The ANOVA results highlight a clear distinction between the European LSG and Japanese LSG parts of the dataset."
- Break condition: If cultural differences in non-verbal behavior are too subtle or inconsistent, culture-specific models may not outperform generic ones.

### Mechanism 3
- Claim: Engagement in dyadic interactions is influenced by mutual influence between interlocutors, varying by culture.
- Mechanism: In Japanese conversations, novice engagement positively correlates with expert engagement (r=0.51), suggesting harmony-driven behavior, whereas European sessions show weak negative correlation (r=-0.07 to -0.19), indicating independent engagement patterns.
- Core assumption: Cultural norms around harmony and conformity (e.g., Hofstede’s cultural dimensions) manifest in engagement dynamics during conversations.
- Evidence anchors:
  - [section] "The Japanese session, however, shows a high positive correlation coefficient (r=0.51) for mutual engagement."
  - [section] "This suggests a stronger need for harmony among the Japanese participants, leading them to conform more closely to the mood of their interlocutors."
- Break condition: If engagement annotations are biased or if the mutual influence is not consistently present across all Japanese sessions, the cultural difference claim weakens.

## Foundational Learning

- Concept: Cross-cultural differences in non-verbal communication (e.g., backchanneling frequency, head nods, smiling).
  - Why needed here: The study hinges on analyzing and modeling these differences to improve engagement prediction; without this foundation, the cultural-specific approaches lack justification.
  - Quick check question: What is the primary difference in backchanneling behavior between Japanese and European speakers as described in the literature?

- Concept: Multimodal feature extraction and fusion (speech acoustics, facial expressions, gestures, body properties).
  - Why needed here: The model relies on a rich set of multimodal features to capture engagement; understanding how these are extracted and combined is critical for interpreting model performance.
  - Quick check question: Which feature was found to be universally important across all language groups in the SHAP analysis?

- Concept: Transfer learning in deep learning (fine-tuning pre-trained models on new target datasets).
  - Why needed here: The paper uses transfer learning to adapt engagement models across cultures; knowing how this works explains the reported performance gains.
  - Quick check question: How did the model’s loss change for the Japanese LSG after transfer learning from a European LSG model?

## Architecture Onboarding

- Component map: Data collection (NoXi+J corpus) -> Feature extraction (Kinect, facial AUs, voice, head nods) -> Modeling (4-cell LSTM, 30-frame window, 49 features) -> Evaluation (MSE loss, SHAP analysis, transfer learning).
- Critical path: 1. Extract multimodal features per frame. 2. Compute derived features (speaking turn, silence, vocal backchanneling, head nods). 3. Train LSTM on one LSG, predict engagement on test set. 4. Apply SHAP analysis to interpret feature importance. 5. Perform transfer learning and re-evaluate.
- Design tradeoffs:
  - Pros: Culture-specific models capture nuanced patterns; transfer learning leverages shared feature importance.
  - Cons: Requires large, diverse dataset per culture; transfer learning may underperform if cultures are too distinct.
- Failure signatures:
  - High MSE on target LSG after transfer learning → cultural differences too large for effective transfer.
  - SHAP values dominated by noise features → feature extraction or model architecture issues.
  - No improvement over baseline models → cultural differences not strong enough to justify specialized models.
- First 3 experiments:
  1. Train LSTM on European LSG, evaluate on European test set, record MSE and SHAP top features.
  2. Apply same model to Japanese LSG test set, record MSE and compare feature importance.
  3. Perform transfer learning from European to Japanese LSG, re-evaluate, and compare MSE improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do small, undetected head nods in the Japanese dataset affect the accuracy of cultural comparison in non-verbal behavior analysis?
- Basis in paper: [explicit] The paper mentions that the head nod detection algorithm missed many small head nods in the Japanese dataset, which were obscured by static noise in the facial recognition data.
- Why unresolved: The missed head nods could lead to an underestimation of head nod frequency in Japanese participants, potentially skewing the cultural comparison of non-verbal behaviors.
- What evidence would resolve it: A more accurate head nod detection algorithm or manual annotation of head nods in the Japanese dataset would provide a clearer picture of head nod frequency and its cultural implications.

### Open Question 2
- Question: How does the correlation between expert and novice engagement in Japanese sessions compare to other cultures, and what does this imply about the role of conformity in engagement?
- Basis in paper: [explicit] The paper finds a high positive correlation between novice and expert engagement in Japanese sessions (r=0.51), unlike the negative or weak correlations observed in European sessions.
- Why unresolved: The strong positive correlation in Japanese sessions suggests a potential cultural influence on engagement, but the underlying reasons and broader implications are not fully explored.
- What evidence would resolve it: Further research comparing engagement dynamics across a wider range of cultures and contexts would help determine if this correlation is unique to Japanese culture or if it reflects a broader pattern of conformity in engagement.

### Open Question 3
- Question: How does the performance of engagement prediction models differ when trained on culturally diverse datasets versus single-culture datasets?
- Basis in paper: [explicit] The paper discusses the performance of models trained on different language speaker groups (LSGs) and the impact of transfer learning, but does not directly compare culturally diverse versus single-culture training sets.
- Why unresolved: The paper suggests that cultural diversity in training data affects model performance, but a direct comparison would provide clearer insights into the benefits and challenges of culturally diverse datasets.
- What evidence would resolve it: Conducting experiments that explicitly compare the performance of models trained on culturally diverse datasets versus single-culture datasets would clarify the impact of cultural diversity on engagement prediction accuracy.

## Limitations
- The study relies on a relatively small dataset of 150 dyadic interactions across five languages, which may limit the generalizability of cultural patterns identified.
- The feature extraction pipeline, particularly for Kinect-based body and head movement data, may introduce noise that could affect cross-cultural comparisons.
- The SHAP analysis may be sensitive to the choice of background samples and model architecture.

## Confidence
- **High Confidence**: The universal importance of fluidity across all language groups, as supported by consistent SHAP values and robust statistical testing.
- **Medium Confidence**: The superiority of culture-specific models over generic ones, given the moderate performance improvements but limited sample sizes per culture.
- **Medium Confidence**: The effectiveness of transfer learning from European to Japanese models, though results may be influenced by dataset imbalance and feature noise.

## Next Checks
1. Replicate the ANOVA and Tukey-Kramer analyses with bootstrapped confidence intervals to assess the robustness of cultural differences in non-verbal features.
2. Conduct ablation studies removing Kinect-based features to determine their contribution to engagement prediction and cross-cultural transfer learning gains.
3. Validate SHAP results using alternative feature importance methods (e.g., permutation importance) to ensure consistency in identifying culture-specific and universal predictors.