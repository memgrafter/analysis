---
ver: rpa2
title: 'A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias'
arxiv_id: '2404.00929'
source_url: https://arxiv.org/abs/2404.00929
tags:
- language
- bias
- multilingual
- mllms
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive analysis of multilingual
  large language models (MLLMs), focusing on three critical challenges: corpora, alignment,
  and bias. The authors present an overview of MLLMs, survey multilingual training
  corpora and datasets, investigate multilingual representation alignment, and discuss
  bias in MLLMs.'
---

# A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias

## Quick Facts
- arXiv ID: 2404.00929
- Source URL: https://arxiv.org/abs/2404.00929
- Reference count: 40
- Primary result: Comprehensive survey analyzing MLLM challenges in corpora, alignment, and bias

## Executive Summary
This survey provides a systematic analysis of multilingual large language models (MLLMs), identifying three critical challenges: corpora imbalance, multilingual representation alignment, and bias. The authors examine major MLLMs including mBERT, XLM-R, mT5, and BLOOM, finding that training corpora are predominantly English-centric, leading to significant performance gaps for low-resource languages. The study reveals that while MLLMs show promise in cross-lingual transfer learning, their multilingual alignment capabilities vary significantly across language families, and they exhibit various types of bias that require targeted mitigation strategies.

## Method Summary
The survey employs a comprehensive literature review methodology, analyzing existing MLLMs, their training corpora, alignment performance on cross-lingual tasks, and bias evaluation. The authors systematically examine 40+ references covering model architectures, training techniques, multilingual alignment methods, and bias mitigation strategies. They evaluate MLLMs using established benchmarks like GLUE, XNLI, and BLI, and analyze training data composition across different language families. The methodology includes both technical analysis of model capabilities and critical assessment of current limitations in multilingual representation learning.

## Key Results
- MLLMs are predominantly trained on English-centric corpora, leading to significant language imbalances and under-representation of low-resource languages
- Cross-lingual transfer performance varies significantly across language families, with Indo-European languages showing better alignment than distant language pairs
- MLLMs exhibit multiple types of bias including language bias, demographic bias, and evaluation bias, requiring targeted debiasing techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLLMs' multilingual performance degrades because training corpora are heavily skewed toward English, causing under-representation of low-resource languages
- Mechanism: The language imbalance in training data leads to insufficient model exposure to diverse linguistic structures, resulting in poor cross-lingual transfer and biased outputs
- Core assumption: More balanced multilingual corpora would improve low-resource language performance and reduce bias
- Evidence anchors:
  - [abstract] "MLLMs are predominantly trained on English-centric corpora, leading to language imbalances and under-representation of low-resource languages"
  - [section] "The overwhelming English texts in corpora lead to MLLMs' English-centric ability"
  - [corpus] Evidence from Table 3 shows English dominance in most MLLM training corpora (e.g., GPT-3: 92.7% English, Gopher: >99% English)
- Break condition: If corpus balancing does not improve performance, or if other factors (e.g., tokenization, architecture) are the primary bottleneck

### Mechanism 2
- Claim: MLLMs' multilingual alignment performance varies significantly across languages due to typological distance and language family proximity
- Mechanism: Languages within the same family share structural similarities, enabling better cross-lingual transfer. Distant languages suffer from poor alignment due to lack of shared features
- Core assumption: Including more typologically diverse languages and leveraging language families improves alignment
- Evidence anchors:
  - [abstract] "MLLMs still struggle to learn a universal language representation for diverse languages"
  - [section] "More typologically distant language pairs tend to be less well-aligned than more similar ones"
  - [corpus] Indo-European dominance in training corpora (Fig. 4) limits exposure to other language families
- Break condition: If alignment techniques (e.g., auxiliary languages, non-linear mapping) fail to close the performance gap

### Mechanism 3
- Claim: Bias in MLLMs arises from both data and model design, manifesting as language bias, demographic bias, and evaluation bias
- Mechanism: Unbalanced training data perpetuates stereotypes and cultural biases, while model architecture and evaluation metrics reinforce these biases
- Core assumption: Targeted debiasing techniques can mitigate bias without significantly harming performance
- Evidence anchors:
  - [abstract] "MLLMs are prone to produce harmful outcomes and social bias in part due to bias is naturally present in cross-cultural datasets"
  - [section] "Language bias refers to the unequal performances of MLLMs among different languages"
  - [corpus] Table 6 shows examples of language and demographic bias in MLLMs
- Break condition: If debiasing introduces unacceptable performance trade-offs or fails to generalize across languages

## Foundational Learning

- Concept: Multilingual representation alignment
  - Why needed here: Understanding how MLLMs learn shared representations across languages is crucial for diagnosing performance gaps and bias
  - Quick check question: What are the key differences between static, contextual, and combined multilingual representations?

- Concept: Cross-lingual transfer learning
  - Why needed here: MLLMs rely on transferring knowledge from high-resource to low-resource languages; understanding this mechanism is essential for improving low-resource performance
  - Quick check question: How does typological distance affect cross-lingual transfer in MLLMs?

- Concept: Bias types and mitigation strategies
  - Why needed here: Identifying and addressing bias is critical for ensuring fairness and inclusivity in MLLMs
  - Quick check question: What are the main differences between model debiasing and data debiasing techniques?

## Architecture Onboarding

- Component map: Training corpora (multilingual, unbalanced) -> Model architecture (encoder-only, decoder-only, encoder-decoder) -> Pre-training techniques (LM, MLM, NSP, DAE) -> Alignment methods (static, contextual, combined) -> Bias evaluation and mitigation

- Critical path: 1. Collect and preprocess multilingual training data 2. Pre-train MLLM on multilingual corpus 3. Fine-tune on downstream tasks 4. Evaluate performance and bias 5. Apply debiasing techniques if necessary

- Design tradeoffs: Larger models improve performance but increase computational cost; more balanced corpora improve fairness but may reduce overall performance; complex alignment techniques improve multilingual representation but increase training complexity

- Failure signatures: Poor performance on low-resource languages; significant performance gaps between language families; detection of bias in model outputs or evaluation metrics

- First 3 experiments: 1. Train MLLM on unbalanced vs. balanced multilingual corpora and compare low-resource language performance 2. Evaluate cross-lingual transfer performance across language families (e.g., Indo-European vs. Sino-Tibetan) 3. Apply model and data debiasing techniques and measure impact on bias and task performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can MLLMs be effectively evaluated for true multilingual capabilities beyond task-centric benchmarks?
- Basis in paper: [explicit] The paper highlights that current evaluation benchmarks for MLLMs are mainly based on English task sets and lack a universal, flexible evaluation system to properly assess the true multilinguality of MLLMs
- Why unresolved: Existing benchmarks are not fully applicable to other languages, and translated benchmarks may introduce additional biases due to translation errors and cultural differences. There's a need for high-quality multilingual evaluation datasets and a proper system to evaluate the true multilinguality of MLLMs
- What evidence would resolve it: Development and validation of a comprehensive, culturally diverse, multilingual benchmark that accurately reflects the performance of MLLMs across a wide range of languages and tasks

### Open Question 2
- Question: What are the most effective strategies to mitigate language bias in MLLMs, particularly for low-resource languages?
- Basis in paper: [explicit] The paper discusses the language imbalance challenge in MLLMs, where English and other major languages dominate training corpora, leading to poor performance on low-resource languages. It mentions data sampling techniques and vocabulary augmentation as potential solutions
- Why unresolved: Constructing more balanced corpora is challenging due to the high cost of manually collecting and annotating low-resource data and the low quality of available low-resource corpora. Evaluating and improving techniques to build high-quality multilingual corpora is essential
- What evidence would resolve it: Comparative studies demonstrating the effectiveness of different data balancing techniques in improving MLLM performance on low-resource languages, along with strategies for efficiently collecting and curating high-quality low-resource data

### Open Question 3
- Question: How can bias interpretability in MLLMs be improved to enable more targeted and effective debiasing strategies?
- Basis in paper: [explicit] The paper emphasizes the lack of interpretability in existing bias understanding and mitigation methods. It suggests that research into bias interpretability can help identify the sources of bias, enabling more targeted debiasing strategies
- Why unresolved: Biases may stem from various sources, including the model itself, training data, algorithms, or task settings. Without understanding the root causes of bias, it's difficult to develop effective mitigation techniques
- What evidence would resolve it: Development of methods for attributing bias to specific components of MLLMs (e.g., neurons, attention heads) and correlating these attributions with measurable bias in model outputs. This would enable the design of targeted interventions to reduce bias at its source

## Limitations
- Analysis relies on publicly available information, which may not be complete for proprietary models like GPT-4 or Claude 3
- Bias evaluation is constrained by the availability and quality of evaluation datasets across different languages and cultures
- Survey focuses primarily on technical aspects and does not extensively address ethical, legal, or societal implications

## Confidence

- MLLMs are predominantly trained on English-centric corpora: High confidence
- Cross-lingual transfer performance varies across language families: Medium confidence
- MLLMs exhibit multiple types of bias: Medium confidence

## Next Checks

1. Conduct controlled experiments training MLLMs on balanced vs. unbalanced multilingual corpora to quantify the impact on low-resource language performance and bias reduction

2. Evaluate cross-lingual transfer performance across typologically diverse language pairs using standardized benchmarks (XNLI, BLI) to validate the relationship between language family proximity and alignment quality

3. Implement and compare multiple debiasing techniques (data augmentation, adversarial training, post-processing) on a diverse set of MLLMs to assess their effectiveness in reducing both language and demographic biases while maintaining overall performance