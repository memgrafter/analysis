---
ver: rpa2
title: 'Continual Distillation Learning: Knowledge Distillation in Prompt-based Continual
  Learning'
arxiv_id: '2407.13911'
source_url: https://arxiv.org/abs/2407.13911
tags:
- distillation
- learning
- student
- continual
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces continual distillation learning (CDL), a
  framework that uses knowledge distillation to improve prompt-based continual learning
  models. The authors identify that traditional KD methods like logit distillation
  and feature distillation are ineffective in CDL due to distillation information
  forgetting, where prompt-based CL reselects prompts each task, losing teacher knowledge.
---

# Continual Distillation Learning: Knowledge Distillation in Prompt-based Continual Learning

## Quick Facts
- arXiv ID: 2407.13911
- Source URL: https://arxiv.org/abs/2407.13911
- Authors: Qifan Zhang; Yunhui Guo; Yu Xiang
- Reference count: 40
- Key outcome: Introduces KDP framework that achieves 84.31% average accuracy and 5.61% forgetting rate on ViT-Small setup

## Executive Summary
This paper addresses the problem of knowledge distillation in prompt-based continual learning (CDL), where traditional KD methods fail due to distillation information forgetting - the loss of teacher knowledge when prompts are reselected for each task. The authors propose Knowledge Distillation based on Prompts (KDP), which inserts globally accessible KD prompts into the student's frozen ViT backbone to provide consistent cross-task distillation signals. KDP significantly outperforms baseline methods on CIFAR-100 and ImageNet-R datasets while enabling efficient deployment of compact models.

## Method Summary
KDP introduces KD prompts inserted into the ViT backbone via prefix-tuning to address distillation information forgetting in prompt-based CL. The method uses a separate KD token and KD classifier to process teacher soft labels independently from task-specific prompt selection. The student model processes inputs through both CL prompts (selected per task) and KD prompts (globally accessible), with distillation loss computed using the KD classifier. The framework is tested across L2P, DualPrompt, and CODA-Prompt baselines on CIFAR-100 and ImageNet-R datasets with 10 tasks each.

## Key Results
- KDP achieves 84.31% average accuracy and 5.61% forgetting rate on ViT-Small setup
- Outperforms traditional KD methods (logit distillation, feature distillation, DeiT) on both CIFAR-100 and ImageNet-R
- Generalizes across multiple prompt-based CL frameworks (L2P, DualPrompt, CODA-Prompt)
- Demonstrates consistent performance improvement while maintaining compact model deployment

## Why This Works (Mechanism)

### Mechanism 1
The introduction of KD prompts mitigates distillation information forgetting by providing globally accessible, task-agnostic distillation cues that persist across task transitions. Traditional prompt-based CL methods select task-specific prompts using a query-key mechanism, causing the student to lose teacher knowledge between tasks. KDP introduces additional KD prompts that are inserted into the ViT backbone and are not tied to any task-specific selection, providing consistent distillation signals across tasks.

### Mechanism 2
Decoupling knowledge distillation from the task-specific prompt selection mechanism allows for consistent cross-task knowledge transfer and prevents interference between continual learning and distillation objectives. In prompt-based CL, the query-key mechanism dynamically selects prompt components for each task, which is detrimental for distillation because the selected prompts change. KDP bypasses this by introducing KD prompts that are always present and updated independently.

### Mechanism 3
The KD classifier and KD token structure in KDP enables the student to learn from the teacher's output distribution without relying on task-specific prompts, further enhancing distillation effectiveness. KDP employs a separate KD classifier and KD token to process the soft labels from the teacher model, isolating the distillation objective from the task-specific classification task.

## Foundational Learning

- **Concept**: Catastrophic Forgetting
  - **Why needed here**: Continual learning models must learn new tasks without losing previously acquired knowledge. Understanding catastrophic forgetting is crucial for designing methods like KDP that prevent the student from forgetting the teacher's knowledge across tasks.
  - **Quick check question**: What is the primary challenge that continual learning methods aim to address, and how does it manifest in the context of prompt-based CL with knowledge distillation?

- **Concept**: Knowledge Distillation
  - **Why needed here**: KDP is built upon the principles of knowledge distillation, where a student model learns from a larger or more accurate teacher model. Understanding the different types of knowledge distillation (logit, feature, and the DeiT method) is essential for appreciating how KDP improves upon existing methods in the CDL setup.
  - **Quick check question**: What are the key differences between logit distillation, feature distillation, and the DeiT method, and how do these differences impact their effectiveness in the CDL problem?

- **Concept**: Vision Transformers (ViTs)
  - **Why needed here**: KDP is specifically designed for prompt-based continual learning methods that use ViTs as the backbone. Understanding the architecture of ViTs, including the role of prompts and the frozen backbone in prompt-based CL, is crucial for understanding how KDP modifies the ViT architecture to enable effective knowledge distillation.
  - **Quick check question**: How do prompts modify the behavior of a ViT in prompt-based CL, and what are the implications of keeping the ViT backbone frozen during training?

## Architecture Onboarding

- **Component map**: Input Image -> Query Function -> Prompt Extractor -> ViT Backbone (with CL Prompts + KD Prompts) -> Class Token -> Student Classifier / KD Token -> KD Classifier -> Classification Loss + Distillation Loss

- **Critical path**:
  1. Process input image through query function to generate query
  2. Compare query with keys in prompt pool to select task-specific prompts (CL prompts)
  3. Insert CL prompts and KD prompts into ViT backbone using prefix-tuning
  4. Process input through ViT backbone with inserted prompts
  5. Use class token for task-specific classification (student classifier)
  6. Use KD token for knowledge distillation (KD classifier)
  7. Compute classification loss and distillation loss
  8. Update student model parameters

- **Design tradeoffs**:
  - KD Prompt Length vs. Performance: Longer KD prompts may provide more distillation information but increase computational cost and memory usage
  - Number of KD Prompt Insertion Layers vs. Performance: Inserting KD prompts into more layers may improve distillation effectiveness but also increase computational cost
  - KD Classifier vs. No KD Classifier: Using a separate KD classifier isolates the distillation objective but adds complexity and computational overhead
  - Frozen Backbone vs. Unfrozen Backbone: Keeping the backbone frozen prevents catastrophic forgetting but limits the model's ability to adapt to new tasks

- **Failure signatures**:
  - High Forgetting Rate: Indicates that the student is not effectively retaining the teacher's knowledge across tasks
  - Low Accuracy: Indicates that the student is not effectively learning from the teacher or the new tasks
  - Distillation Information Forgetting: The student re-selects different prompts for each task, losing previously distilled knowledge
  - Interference Between CL and KD: The task-specific prompt selection mechanism interferes with the distillation process

- **First 3 experiments**:
  1. Ablation Study - KD Prompts Only: Remove the KD classifier and KD token from KDP to isolate the effect of KD prompts on distillation performance
  2. Ablation Study - KD Classifier Only: Remove the KD prompts from KDP to isolate the effect of the KD classifier and KD token on distillation performance
  3. Varying KD Prompt Length: Experiment with different lengths of KD prompts to determine the optimal length for balancing distillation effectiveness and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of KDP scale when using different backbone architectures beyond ViT, such as CNNs or other transformer variants?
- Basis in paper: [explicit] The paper focuses exclusively on ViT backbones and notes that larger ViTs achieve better performance, but doesn't explore alternative architectures.
- Why unresolved: The paper's scope is limited to ViT backbones, and the effectiveness of KD prompts on other architectures remains untested.
- What evidence would resolve it: Empirical results comparing KDP performance across ViT, CNN, and other transformer variants on the same datasets would clarify architectural dependencies.

### Open Question 2
- Question: What is the long-term stability of knowledge distillation in KDP when the number of tasks grows significantly beyond the 10-task experiments?
- Basis in paper: [inferred] The paper tests with 10 tasks but doesn't address how performance degrades with hundreds or thousands of tasks.
- Why unresolved: The paper doesn't investigate task scaling effects on distillation effectiveness or forgetting rates over extended task sequences.
- What evidence would resolve it: Experiments with varying task sequence lengths (50, 100, 500 tasks) showing accuracy and forgetting rate trends would reveal scalability limits.

### Open Question 3
- Question: How sensitive is KDP to the choice of temperature parameter τ in the distillation loss function?
- Basis in paper: [explicit] The paper sets τ = 2 but doesn't explore how different temperature values affect performance.
- Why unresolved: The optimal temperature setting isn't investigated, and different values might yield significantly different results.
- What evidence would resolve it: A systematic sensitivity analysis testing τ values from 0.5 to 5.0 with performance metrics would identify optimal ranges.

### Open Question 4
- Question: Can KDP be extended to work effectively with few-shot or zero-shot continual learning scenarios?
- Basis in paper: [inferred] The paper uses full task data for distillation but doesn't explore limited-data scenarios.
- Why unresolved: The paper doesn't investigate how KD prompts perform when only few examples are available per task.
- What evidence would resolve it: Experiments comparing KDP with varying amounts of per-task data (1-shot, 5-shot, 10-shot) would reveal data efficiency limits.

## Limitations

- Limited dataset scope: Only tested on CIFAR-100 and ImageNet-R datasets, limiting generalizability claims
- Computational overhead: KDP increases training time and memory usage without quantitative cost-benefit analysis
- Architecture specificity: Results are specific to ViT backbones without exploring other architecture types

## Confidence

**High Confidence**: The core mechanism of using globally accessible KD prompts to address distillation information forgetting is well-supported by the described problem formulation and theoretical reasoning.

**Medium Confidence**: Empirical results showing KDP outperforming baseline methods on CIFAR-100 and ImageNet-R are presented with specific metrics, but the limited dataset scope reduces confidence in generalizability.

**Low Confidence**: Claims about KDP's effectiveness across diverse vision tasks and its ability to maintain performance while enabling compact model deployment are not empirically validated.

## Next Checks

1. **Ablation Study on KD Prompt Design**: Conduct systematic experiments varying KD prompt length (e.g., 2, 6, 10 tokens) and insertion layers (e.g., only block 1, blocks 1-6, blocks 1-12) to determine the optimal configuration and understand the sensitivity of KDP performance to these design choices.

2. **Cross-Dataset Generalization Test**: Evaluate KDP on additional datasets beyond CIFAR-100 and ImageNet-R, including more complex vision datasets like COCO or Places365, to assess the method's generalization capability and identify potential dataset-specific limitations.

3. **Computational Cost-Benefit Analysis**: Measure and compare the training time, memory usage, and inference latency of KDP against baseline methods across different hardware configurations (GPU vs. CPU) to provide a comprehensive understanding of the practical deployment implications and scalability of the proposed approach.