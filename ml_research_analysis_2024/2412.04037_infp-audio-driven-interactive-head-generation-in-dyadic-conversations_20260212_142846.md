---
ver: rpa2
title: 'INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations'
arxiv_id: '2412.04037'
source_url: https://arxiv.org/abs/2412.04037
tags:
- motion
- head
- generation
- dyadic
- facial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating realistic, audio-driven
  head animations for dyadic conversations, where an agent must dynamically switch
  between speaking and listening states based on the conversational context. Unlike
  prior work that requires manual role assignment and explicit role switching, the
  proposed INFP framework enables seamless transitions between states.
---

# INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations

## Quick Facts
- arXiv ID: 2412.04037
- Source URL: https://arxiv.org/abs/2412.04037
- Reference count: 40
- One-line primary result: Audio-driven interactive head generation framework that enables seamless speaking/listening state transitions without manual role assignment

## Executive Summary
This paper presents INFP, a novel framework for audio-driven interactive head generation in dyadic conversations. The system addresses the challenge of dynamically switching between speaking and listening states based on conversational context, eliminating the need for manual role assignment or explicit switching. INFP operates in two stages: first extracting communicative behaviors from real conversation videos into a low-dimensional motion latent space, then using a conditional diffusion transformer to map dyadic audio to these motion codes for interactive head generation.

The framework is trained and evaluated on DyConv, a newly introduced large-scale dataset of rich dyadic conversations. Experimental results demonstrate that INFP outperforms state-of-the-art methods in interactive head generation, listening head generation, and talking head generation, achieving superior visual quality, motion diversity, and audio-visual alignment. The method also supports style control for enhanced expressiveness, making it suitable for applications in virtual avatars, online education, and conversational AI.

## Method Summary
INFP is a two-stage framework for audio-driven interactive head generation. Stage 1 (Motion-Based Head Imitation) extracts communicative behaviors from real conversation videos into a low-dimensional motion latent space using a hybrid facial representation that blocks appearance information. Stage 2 (Audio-Guided Motion Generation) employs an interactive motion guider with memory banks and a conditional diffusion transformer to map dyadic audio to the motion latent space, enabling audio-driven interactive head generation. The framework is trained on DyConv, a large-scale dataset of dyadic conversations with separated audio tracks for each speaker.

## Key Results
- Outperforms state-of-the-art methods with SSIM of 0.834, PSNR of 31.562, and FID of 15.727 on DyConv
- Achieves seamless state transitions between speaking and listening without manual role assignment
- Demonstrates superior motion diversity and audio-visual alignment compared to baseline DIM

## Why This Works (Mechanism)

### Mechanism 1
Disentangled motion latent space enables appearance-independent motion synthesis. The model uses hybrid facial representation (masked image + facial contour points) to block appearance information during motion encoding, creating a low-dimensional 1-D descriptor that captures only facial motion semantics. Core assumption: Facial expressions and head poses can be effectively represented without appearance information.

### Mechanism 2
Interactive motion guider with memory banks enables dynamic state switching. Dual memory banks (Mv for verbal, Mnv for non-verbal) use cross-attention to adaptively extract motion features based on audio content - when self audio is strong, verbal motion dominates; when partner audio is strong, non-verbal motion dominates. Core assumption: Audio content contains sufficient information to determine speaking vs listening state.

### Mechanism 3
Conditional diffusion transformer maps interactive motion features to pretrained motion latent space. Diffusion model with motion-attention layer uses interactive motion feature as condition to denoise and generate motion latent codes, incorporating temporal attention for smooth transitions. Core assumption: Diffusion process can effectively learn the mapping from audio-derived features to motion latent space.

## Foundational Learning

- **Dyadic conversation dynamics and role switching**: Why needed here: The framework must understand when to switch between speaking and listening states based on conversational context. Quick check: How does the model determine which speaker is currently active based on audio input alone?

- **Diffusion models and denoising processes**: Why needed here: The core mechanism uses conditional diffusion transformer to map audio features to motion latent codes. Quick check: What role does the CFG parameter play in controlling the strength of conditioning during denoising?

- **Motion representation and disentanglement**: Why needed here: The framework relies on separating motion from appearance in the latent space representation. Quick check: Why might a low-dimensional 1-D motion descriptor be preferable to higher-dimensional representations for this task?

## Architecture Onboarding

- **Component map**: Dyadic Audio → Interactive Motion Guider → Conditional Diffusion Transformer → Motion Latent Space → Face Decoder → Video Output
- **Critical path**: Audio → Interactive Motion Guider → Conditional Diffusion Transformer → Motion Latent Space → Face Decoder → Video Output
- **Design tradeoffs**: Memory bank size (K=64) vs. motion diversity; low-dimensional latent space vs. expressiveness; diffusion steps (20) vs. inference speed
- **Failure signatures**: Inappropriate state transitions, lack of motion diversity, poor audio-visual alignment, appearance-related artifacts in generated videos
- **First 3 experiments**:
  1. Test interactive motion guider with single-sided audio to verify verbal vs non-verbal dominance
  2. Validate motion latent space disentanglement by generating videos with different style vectors
  3. Measure performance degradation when removing memory banks or style modulation components

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of INFP change when extending the generation range from head synthesis to full-body or upper-body synthesis? The paper mentions extending generation to the upper or whole body as a potential future work. Why unresolved: The current framework focuses solely on head synthesis, and there is no empirical evidence or experimentation provided for full-body synthesis. What evidence would resolve it: Experiments comparing INFP's performance on full-body or upper-body synthesis against state-of-the-art methods for full-body animation, using metrics like visual quality, motion realism, and audio-visual alignment.

### Open Question 2
How does the inclusion of additional modalities, such as visual or textual content, affect the performance of INFP in dyadic conversation scenarios? The paper suggests that combining control signals from multiple modalities could offer additional capacities. Why unresolved: The current framework only uses audio as input, and the impact of incorporating visual or textual modalities is not explored. What evidence would resolve it: Comparative studies showing improvements in naturalness, expressiveness, and context awareness when integrating visual or textual modalities alongside audio.

### Open Question 3
What are the limitations of the DyConv dataset in capturing diverse conversational contexts, and how can it be improved? The paper introduces DyConv as a large-scale dataset for dyadic conversations but does not discuss its limitations or potential improvements. Why unresolved: While the dataset is described as large-scale and high-quality, there is no analysis of its coverage of diverse conversational contexts, emotions, or cultural backgrounds. What evidence would resolve it: A detailed analysis of the dataset's diversity, including metrics on conversational topics, emotional range, speaker demographics, and cultural representation, along with suggestions for dataset expansion.

## Limitations
- Evaluation relies entirely on synthetic metrics without user studies or perceptual validation
- Memory bank size (K=64) was chosen empirically without ablation studies showing sensitivity
- Low-dimensional 1-D motion descriptor may limit expressiveness for complex conversational behaviors

## Confidence
- **High**: Basic framework architecture and two-stage approach
- **Medium**: Quantitative performance improvements over baselines
- **Low**: Generalizability to conversations beyond the DyConv dataset and real-world deployment scenarios

## Next Checks
1. Conduct user studies comparing INFP-generated videos against baseline methods for perceived naturalness and conversational appropriateness
2. Test model performance on out-of-distribution conversational data (different languages, cultural contexts, or speaking styles) to assess generalizability
3. Perform ablation studies on critical components (memory bank size, motion latent space dimensionality, diffusion steps) to identify performance bottlenecks