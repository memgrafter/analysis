---
ver: rpa2
title: Continual Task Learning through Adaptive Policy Self-Composition
arxiv_id: '2411.11364'
source_url: https://arxiv.org/abs/2411.11364
tags:
- task
- learning
- tasks
- methods
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses continual task learning in offline reinforcement
  learning, where agents must adapt to new tasks while retaining knowledge from previous
  ones. The authors introduce CompoFormer, a transformer-based architecture that adaptively
  composes prior policies using a meta-policy network and semantic correlations between
  task descriptions.
---

# Continual Task Learning through Adaptive Policy Self-Composition

## Quick Facts
- arXiv ID: 2411.11364
- Source URL: https://arxiv.org/abs/2411.11364
- Reference count: 40
- Key outcome: CompoFormer achieves 0.69 average performance vs 0.54 for best baseline on Offline Continual World benchmark

## Executive Summary
This paper addresses continual task learning in offline reinforcement learning, where agents must adapt to new tasks while retaining knowledge from previous ones. The authors introduce CompoFormer, a transformer-based architecture that adaptively composes prior policies using a meta-policy network and semantic correlations between task descriptions. Experiments on the Offline Continual World benchmark show CompoFormer achieves significantly better average performance and lower forgetting rates than conventional continual learning methods, particularly in longer task sequences.

## Method Summary
The paper proposes CompoFormer, a transformer-based approach that leverages prior policies as building blocks for continual learning. When encountering a new task, the meta-policy network selects and composes relevant previous policies based on semantic correlations between task descriptions. This adaptive composition mechanism enables knowledge transfer while preventing catastrophic forgetting. The method operates in an offline setting where pre-collected datasets are available, and uses transformer architectures to capture complex relationships between tasks and policies.

## Key Results
- CompoFormer achieves 0.69 average performance compared to 0.54 for the best baseline on Offline Continual World benchmark
- Demonstrates significantly lower forgetting rates than conventional continual learning methods
- Shows particular effectiveness in longer task sequences

## Why This Works (Mechanism)
CompoFormer works by treating previously learned policies as reusable components that can be adaptively combined for new tasks. The meta-policy network learns to identify which prior policies are most relevant to a new task based on semantic similarities in task descriptions. This selective composition allows the agent to leverage existing knowledge while maintaining flexibility to adapt to new requirements. The transformer architecture enables capturing complex dependencies between tasks and policies, facilitating more nuanced composition strategies than simple weight averaging or finetuning approaches.

## Foundational Learning

**Offline Reinforcement Learning**: Learning from pre-collected datasets without environment interaction; needed because real-world data collection is expensive and potentially dangerous. Quick check: Can the agent learn effective policies without online exploration?

**Catastrophic Forgetting**: When learning new tasks causes performance degradation on previously learned tasks; critical problem in continual learning. Quick check: Does performance on earlier tasks degrade as new tasks are learned?

**Transformer Architectures**: Self-attention mechanisms that capture long-range dependencies; enable modeling complex relationships between tasks and policies. Quick check: Can the model effectively learn which prior policies to combine for new tasks?

**Semantic Correlation Extraction**: Measuring task similarity through natural language descriptions; enables intelligent policy selection and composition. Quick check: Do task descriptions meaningfully capture the differences between learning objectives?

## Architecture Onboarding

Component Map: Task Description -> Semantic Correlation Module -> Meta-Policy Network -> Policy Composition -> Output Policy

Critical Path: The semantic correlation module feeds into the meta-policy network, which determines the composition weights for combining prior policies. The composition mechanism then produces the final policy for the current task.

Design Tradeoffs: Balances between maintaining multiple policy networks (memory overhead) versus sharing knowledge through composition (potential interference). The transformer-based approach offers flexibility but increases computational complexity compared to simpler continual learning methods.

Failure Signatures: Poor performance may indicate inadequate semantic correlation extraction, inappropriate meta-policy decisions, or suboptimal composition weights. Degraded performance on earlier tasks suggests forgetting issues.

First Experiments:
1. Evaluate performance on individual tasks to establish baseline capability
2. Test policy composition on tasks with known semantic similarities
3. Measure forgetting rates as task sequence length increases

## Open Questions the Paper Calls Out
None

## Limitations
- Focuses on task-specific policies without addressing interference in very long task sequences
- Assumes well-aligned task descriptions for semantic correlation extraction
- Limited evaluation to Offline Continual World benchmark, may not represent real-world diversity

## Confidence

High confidence: CompoFormer's superior performance on the Offline Continual World benchmark compared to baselines

Medium confidence: Claims about adaptive policy composition effectively balancing plasticity and stability, requiring further validation across diverse environments

Low confidence: Generalization claims beyond tested benchmark, particularly regarding scalability to hundreds of tasks or complex real-world scenarios

## Next Checks

1. Evaluate CompoFormer's performance on additional continual learning benchmarks with different task distributions and reward structures to assess generalization

2. Conduct ablation studies to quantify the contribution of each component (meta-policy network, semantic correlation module, adaptive composition) to overall performance

3. Test the approach with increasingly longer task sequences (beyond 10 tasks) to measure scalability and identify potential degradation points in policy composition quality