---
ver: rpa2
title: Generating Signed Language Instructions in Large-Scale Dialogue Systems
arxiv_id: '2410.14026'
source_url: https://arxiv.org/abs/2410.14026
tags:
- system
- sign
- signed
- language
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first large-scale goal-oriented conversational
  AI system enhanced with American Sign Language (ASL) instructions, implemented on
  the Amazon Alexa platform. The system provides ASL video instructions for tasks
  like recipes and DIY projects, making it accessible to Deaf and Hard-of-Hearing
  users who prefer sign language over text-based communication.
---

# Generating Signed Language Instructions in Large-Scale Dialogue Systems

## Quick Facts
- arXiv ID: 2410.14026
- Source URL: https://arxiv.org/abs/2410.14026
- Reference count: 23
- Primary result: First large-scale goal-oriented conversational AI system enhanced with ASL instructions on Amazon Alexa platform

## Executive Summary
This paper introduces the first large-scale goal-oriented conversational AI system enhanced with American Sign Language (ASL) instructions, implemented on the Amazon Alexa platform. The system provides ASL video instructions for tasks like recipes and DIY projects, making it accessible to Deaf and Hard-of-Hearing users who prefer sign language over text-based communication. To generate signed instructions, the authors developed a framework that translates English text into ASL glosses using Large Language Models (LLMs), retrieves corresponding sign videos for each gloss token, and stitches them into continuous video sequences. The system's development involved active co-design with the Deaf community to ensure usability and reduce cognitive load. Evaluations show that the system achieves high retrieval accuracy (Recall@1 up to 98%) and maintains user ratings comparable to the non-signed version.

## Method Summary
The authors developed a multimodal dialogue system that translates English instructions into ASL glosses using LLMs, retrieves corresponding sign videos from a pre-built dictionary, and stitches them into continuous sequences for display on Amazon Alexa Echo Show devices. The system uses OpenAI's GPT-3.5-turbo for zero-shot text-to-gloss translation, then retrieves videos by matching gloss tokens to filenames in an AWS S3 storage bucket. The development process incorporated extensive co-design with the Deaf and Hard-of-Hearing community to optimize the user interface layout and reduce cognitive load. The system was evaluated on two task types: Whole Foods recipes and wikiHow tasks, with performance metrics including retrieval accuracy, BERTScore for text generation quality, and user ratings.

## Key Results
- System achieves Recall@1 retrieval accuracy up to 98% for sign video matching
- User ratings for the signed version are comparable to the non-signed baseline
- Expert qualitative analysis highlights strengths in multimodal support and clear layout while identifying areas for improvement in sign video transitions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based gloss translation enables scalable, zero-shot conversion of English instructions to ASL glosses.
- Mechanism: Prompting a large language model with task-specific instructions and retrieving gloss tokens as structured output.
- Core assumption: The LLM can infer ASL gloss conventions from context without requiring domain-specific fine-tuning.
- Evidence anchors:
  - [abstract]: "Our detailed contributions are as follows: ... 3. We implement a novel Large Language Model (LLM)-based instruction generation technique for zero-shot text-to-sign translation."
  - [section]: "Our instructions consist of WholeFoods recipes and WikiHow tasks... using the OpenAI chat API we prompt gpt-3.5-turbo to 'translate each step to American Sign Language gloss'"
  - [corpus]: Weak evidence for effectiveness; corpus does not contain comparative evaluations of LLM vs rule-based gloss generation.
- Break condition: LLM fails to produce syntactically valid glosses or ignores ASL-specific grammar constraints, leading to retrieval failures downstream.

### Mechanism 2
- Claim: Token-based video retrieval ensures high accuracy when gloss tokens match available sign videos.
- Mechanism: Gloss tokens are used as filenames to retrieve pre-stored sign videos from a static dictionary; retrieved videos are concatenated into a continuous sequence.
- Core assumption: Gloss tokens are unique and stable identifiers for corresponding sign videos across the instruction set.
- Evidence anchors:
  - [abstract]: "We use linguistics rules and cognitive science-based heuristics for this translation."
  - [section]: "We process the videos in four steps... store these videos, retrieve them on the fly while presenting instructions, and stitch them together."
  - [corpus]: Weak evidence for uniqueness and coverage; corpus does not specify error handling for missing glosses.
- Break condition: Gloss tokens are ambiguous or not present in the video dictionary, causing retrieval failures or incorrect sign assignments.

### Mechanism 3
- Claim: Community co-design reduces cognitive load by tailoring UI layout and content presentation to DHH users.
- Mechanism: Deaf and Hard-of-Hearing collaborators inform interface design, balancing signed video, static text, and images to avoid sensory overload.
- Core assumption: DHH users experience higher cognitive load reading compared to signing, and multimodal layouts can mitigate this.
- Evidence anchors:
  - [abstract]: "Our development process is deeply rooted in a commitment to community engagement, incorporating insights from the Deaf and Hard-of-Hearing community."
  - [section]: "With repeated consultations with cognitive scientists, we design the layout of our system's user interface specifically around the cognitive load of signers."
  - [corpus]: Weak evidence for measurable cognitive load reduction; corpus does not cite quantitative usability studies.
- Break condition: Layout changes inadvertently increase cognitive load, e.g., by introducing visual clutter or misaligning sign video with corresponding text.

## Foundational Learning

- Concept: American Sign Language glossing conventions
  - Why needed here: Gloss tokens are the intermediate representation for video retrieval; incorrect glossing leads to retrieval errors.
  - Quick check question: What is the difference between a sign video and its gloss token representation?

- Concept: Token-level retrieval and concatenation
  - Why needed here: The system retrieves and stitches individual sign videos into continuous instruction sequences.
  - Quick check question: How does the system ensure that the sequence of retrieved videos matches the original instruction order?

- Concept: Large language model prompting for structured outputs
  - Why needed here: Gloss translation relies on LLM outputs formatted as JSON arrays of gloss tokens.
  - Quick check question: What prompt format guarantees the LLM outputs gloss tokens in the correct sequence and structure?

## Architecture Onboarding

- Component map:
  Input JSON → Task Parsing → Gloss Translation (LLM/Rule-based) → Token Retrieval → Video Stitching → UI Rendering
  Supporting services: AWS S3 storage for videos, OpenAI API for LLM, token-based retrieval index

- Critical path:
  1. Receive task JSON with English instructions
  2. Generate gloss tokens via LLM
  3. Retrieve sign videos for each gloss
  4. Concatenate videos into continuous sequence
  5. Display sequence alongside images in UI

- Design tradeoffs:
  - LLM translation vs. rule-based heuristics: LLM offers scalability and diversity, heuristics offer precision but require manual tuning
  - Static video storage vs. dynamic avatar generation: Static storage ensures fidelity but limits vocabulary; avatars scale but may reduce naturalness

- Failure signatures:
  - High retrieval miss rate: Gloss tokens not found in video dictionary
  - UI latency: Slow video retrieval or stitching delays playback
  - User confusion: Mismatch between video sequence and instruction semantics

- First 3 experiments:
  1. Validate gloss translation accuracy by comparing LLM outputs to rule-based heuristics on a small task set
  2. Measure retrieval hit rate by checking video presence for each gloss token in the dictionary
  3. Test UI cognitive load by observing signer interactions with multimodal vs. text-only layouts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the user preferences and performance metrics differ between avatar-based sign generation versus video-based sign generation?
- Basis in paper: [explicit] "Our expert qualitative analysis... noted that the primary limitation of the current system lies in the segmented nature of the ASL videos" and "we plan to incorporate either human models signing the entire content or synthesized avatars"
- Why unresolved: The paper only mentions future plans to incorporate avatars but does not provide comparative data between current video-based system and potential avatar-based alternatives
- What evidence would resolve it: A/B testing comparing user ratings, comprehension scores, and cognitive load measurements between video-based and avatar-based sign generation systems

### Open Question 2
- Question: What is the impact of adding contextual information (such as surrounding sentences) on the quality of LLM-generated glosses?
- Basis in paper: [inferred] The paper mentions using GPT-3.5-turbo for gloss translation but does not explore whether providing broader context improves translation quality compared to single-step instructions
- Why unresolved: The evaluation focuses on comparing LLM translations to rule-based translations without exploring how context affects translation quality
- What evidence would resolve it: Comparative studies showing translation quality metrics (BERTScore, human evaluation) when providing single steps versus full instructions with context

### Open Question 3
- Question: How does the system perform when scaled to support additional signed languages beyond ASL?
- Basis in paper: [inferred] The authors state "We hope that this system is a step towards developing dialogue systems that can understand and generate signs for all signed languages" but provide no data on cross-language scalability
- Why unresolved: The paper focuses exclusively on ASL implementation without testing or discussing performance in other signed languages
- What evidence would resolve it: Implementation and evaluation results showing retrieval accuracy, translation quality, and user ratings for at least two other signed languages (e.g., BSL, LSQ) using the same framework

### Open Question 4
- Question: What is the optimal balance between signed video instructions and static text for minimizing cognitive load in DHH users?
- Basis in paper: [explicit] "DHH community members can experience higher cognitive loads while reading compared to signing" and "our community outreach surveys indicate that users prefer to see the ingredients written statically on the screen instead of signed versions"
- Why unresolved: The paper provides contradictory evidence about user preferences and does not empirically determine the optimal mix of signed and textual information
- What evidence would resolve it: Controlled studies measuring cognitive load (e.g., NASA-TLX scores) and task completion rates with varying ratios of signed to textual information across different task types

### Open Question 5
- Question: How does the performance of the sign video retrieval system change when supporting real-time generation versus pre-generated content?
- Basis in paper: [explicit] The system uses pre-generated glosses and video retrieval, but the authors mention "Right after the tasks are translated to ASL glosses, we have a quality control stage before they are presented to the user"
- Why unresolved: The paper only evaluates pre-generated content and does not address the challenges or performance implications of real-time sign generation
- What evidence would resolve it: Comparative analysis of retrieval accuracy, latency, and user satisfaction between pre-generated and real-time sign generation systems

## Limitations
- System constrained by pre-recorded video dictionary coverage, limiting support for novel or rare glosses
- Gloss translation accuracy depends on LLM quality without rigorous evaluation against ASL linguistic standards
- User evaluation lacks quantitative measures of usability, cognitive load, and learning outcomes

## Confidence
- High: Technical feasibility of integrating ASL video instructions into conversational AI system
- Medium: Effectiveness of LLM-based gloss translation and benefits of community co-design
- Low: Claims about cognitive load reduction and accessibility improvements due to lack of rigorous quantitative studies

## Next Checks
1. Conduct a quantitative user study measuring task completion time, comprehension, and cognitive load for DHH users interacting with the signed versus non-signed versions of the system.
2. Perform a linguistic evaluation of the LLM-generated glosses against expert ASL glossing standards to assess translation accuracy and grammatical correctness.
3. Expand the sign video dictionary and test the system's robustness on a broader range of tasks and glosses, including rare or context-dependent signs, to identify gaps in coverage and retrieval performance.