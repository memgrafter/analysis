---
ver: rpa2
title: 'AlignGuard: Scalable Safety Alignment for Text-to-Image Generation'
arxiv_id: '2412.10493'
source_url: https://arxiv.org/abs/2412.10493
tags:
- safety
- alignment
- alignguard
- unsafe
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scalable safety alignment
  for text-to-image (T2I) models to prevent harmful content generation. The authors
  propose AlignGuard, a method that trains safety experts (LoRA matrices) using Direct
  Preference Optimization (DPO) on synthetically generated safe/unsafe image-text
  pairs from their CoProV2 dataset.
---

# AlignGuard: Scalable Safety Alignment for Text-to-Image Generation

## Quick Facts
- arXiv ID: 2412.10493
- Source URL: https://arxiv.org/abs/2412.10493
- Authors: Runtao Liu; I Chieh Chen; Jindong Gu; Jipeng Zhang; Renjie Pi; Qifeng Chen; Philip Torr; Ashkan Khakzar; Fabio Pizzati
- Reference count: 40
- Reduces inappropriate probability from 0.51 to 0.07 on Stable Diffusion v1.5 while maintaining generation quality

## Executive Summary
AlignGuard addresses the challenge of scalable safety alignment for text-to-image models by decomposing safety into category-specific experts trained via Direct Preference Optimization (DPO) on synthetically generated safe/unsafe image pairs. The method introduces Co-Merge, a data-based merging strategy that combines these experts using neuron activation frequency, achieving 7x more concept removal than baselines while maintaining generation quality. AlignGuard demonstrates effectiveness across multiple T2I model architectures and shows robustness to adversarial attacks.

## Method Summary
AlignGuard trains category-specific LoRA experts using DPO on paired safe/unsafe images generated from the CoProV2 dataset, then merges them using a neuron activation frequency-based strategy called Co-Merge. The method enables scalable safety alignment by decomposing the problem into manageable categories and using a data-aware merging approach that requires no hyperparameter tuning.

## Key Results
- Reduces inappropriate probability (IP) from 0.51 to 0.07 on Stable Diffusion v1.5
- Maintains generation quality with FID 70.96 vs 69.77 and CLIPScore 32.32 vs 33.52 compared to baseline
- Achieves 7x more concept removal than existing methods while generalizing across multiple T2I models
- Demonstrates robustness to adversarial attacks with IP scores of 0.08 on unseen I2P and 0.16 on UD datasets

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Safety alignment at scale is achieved by decomposing the problem into category-specific experts and merging them.
**Mechanism:** The authors train separate LoRA experts for each safety category (e.g., Hate, Violence, Sexual), then merge them using a data-driven neuron activation frequency strategy (Co-Merge).
**Core assumption:** Different safety categories activate distinct neurons in the LoRA architecture, allowing clean separation and merging.
**Evidence anchors:** [abstract] "We enable the application of Direct Preference Optimization (DPO) for safety purposes in T2I models by synthetically generating a dataset of harmful and safe image-text pairs"; [section 4.2] "Our intuition is that by decomposing safety alignment in multiple categories, trained LoRAs sharing the same architecture would encode information about category-specific alignment in different neurons"
**Break condition:** If category concepts overlap significantly (e.g., "violence" and "self-harm" both involve blood), the same neurons may activate, causing interference and reducing effectiveness.

### Mechanism 2
**Claim:** DPO-based training with paired safe/unsafe images effectively guides the model away from harmful content.
**Mechanism:** For each unsafe prompt, a paired safe image is generated. DPO is applied so the model prefers generating the safe image over the unsafe one, while maintaining generation quality.
**Core assumption:** The paired safe image is visually similar enough to the unsafe one to provide meaningful guidance without causing catastrophic forgetting.
**Evidence anchors:** [section 4.1] "we use DPO to discourage the generation of unsafe outputs for an unsafe prompt pU, and promote the generation of the paired safe image xS instead"; [section 5.2] "AlignGuard demonstrates superior generalization, achieving better IP scores on unseen I2P (0.08) and UD (0.16)"
**Break condition:** If the paired safe image is too dissimilar, the model may learn to generate something completely different rather than removing harmful content. If too similar, the distinction may be too subtle for effective learning.

### Mechanism 3
**Claim:** Co-Merge outperforms existing merging strategies by being data-aware and requiring no hyperparameter tuning.
**Mechanism:** Co-Merge selects the most frequently activated neurons across all prompts for each neuron position, creating an optimal merged LoRA without expensive optimization.
**Core assumption:** The activation frequency of neurons across multiple prompts correlates with their importance for safety alignment.
**Evidence anchors:** [section 4.3] "we identify the neurons with the highest activation frequencies across experts. Specifically, for each neuron j in the merged expert Lmerged, we select the neuron from the original set of experts {L1, ..., LN} that has the highest activation frequency count"; [section 5.4] "Co-Merge consistently outperform these baselines... by performing a data-aware merging using unsafe prompts, we are able to optimally balance the contributions of each expert"
**Break condition:** If activation frequency doesn't correlate with safety importance (e.g., if high-frequency neurons correspond to background details rather than harmful concepts), the merged model will underperform.

## Foundational Learning

- **Concept: Diffusion models and denoising process**
  - Why needed here: AlignGuard builds on understanding how diffusion models generate images through iterative denoising
  - Quick check question: How does a diffusion model transform random noise into a coherent image?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: AlignGuard uses DPO to align the model with safety preferences
  - Quick check question: What distinguishes DPO from standard supervised learning in the context of model alignment?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: Safety experts are implemented as LoRA matrices for efficient fine-tuning
  - Quick check question: Why would LoRA be preferred over full fine-tuning for safety alignment?

## Architecture Onboarding

- **Component map:** T2I model -> CoProV2 dataset generator -> Category-specific LoRA experts -> Co-Merge algorithm -> Evaluation pipeline
- **Critical path:** 1. Generate CoProV2 dataset (unsafe/safe prompt pairs + images) 2. Train LoRA experts per category using DPO 3. Apply Co-Merge to combine experts 4. Evaluate merged model on safety benchmarks 5. Deploy merged LoRA with base model
- **Design tradeoffs:** Category granularity vs. computational cost (more categories = better safety but more training); Dataset size vs. training time (larger CoProV2 = better performance but longer training); LoRA rank vs. performance (higher rank = better safety but more parameters)
- **Failure signatures:** High IP but low FID/CLIPScore → Model refuses to generate anything (over-alignment); Low IP but high FID/CLIPScore → Safety alignment works but generation quality drops; Inconsistent performance across categories → Category overlap or merging issues
- **First 3 experiments:** 1. Train a single LoRA expert on one category (e.g., Violence) and evaluate IP reduction 2. Test Co-Merge with two experts (e.g., Violence + Sexual) to validate merging strategy 3. Compare IP/FID trade-off with baseline methods on a subset of CoProV2

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does AlignGuard's performance scale with even larger datasets beyond CoProV2, and what are the computational limits?
- **Basis in paper:** [explicit] The paper mentions that AlignGuard benefits from large datasets and that scaling data improves performance, but only tested up to the full CoProV2 dataset.
- **Why unresolved:** The paper only tests up to 100% of CoProV2 and doesn't explore datasets orders of magnitude larger, nor does it analyze computational complexity limits.
- **What evidence would resolve it:** Training AlignGuard on datasets 10x-100x larger than CoProV2 and measuring IP, FID, and computational costs would reveal scaling limits.

### Open Question 2
- **Question:** Can AlignGuard's merging strategy (Co-Merge) be generalized to other model merging scenarios beyond safety alignment?
- **Basis in paper:** [explicit] The paper states that Co-Merge is specifically designed for safety alignment use cases, but doesn't test it on other merging scenarios.
- **Why unresolved:** The paper only demonstrates Co-Merge on safety experts and doesn't explore its effectiveness on merging experts from different domains or tasks.
- **What evidence would resolve it:** Applying Co-Merge to merge LoRA experts trained on different tasks (e.g., style transfer, super-resolution) and comparing performance against existing merging methods would validate generalizability.

### Open Question 3
- **Question:** How robust is AlignGuard to adaptive adversarial attacks that specifically target the safety alignment mechanisms?
- **Basis in paper:** [explicit] The paper tests AlignGuard against 4 adversarial attacks but doesn't explore adaptive attacks designed specifically to bypass the alignment.
- **Why unresolved:** The tested attacks are generic text-based optimization methods, not attacks tailored to exploit AlignGuard's specific alignment mechanisms.
- **What evidence would resolve it:** Designing adversarial attacks that specifically target AlignGuard's preference optimization and LoRA merging components would reveal true robustness limits.

### Open Question 4
- **Question:** What is the minimum LoRA rank that maintains acceptable safety alignment performance while maximizing computational efficiency?
- **Basis in paper:** [explicit] The paper tests LoRA ranks from 2 to 16 but doesn't identify the optimal trade-off point between safety performance and computational cost.
- **Why unresolved:** The paper only tests a few rank values and doesn't perform comprehensive analysis across a wider range of ranks or consider real-world deployment constraints.
- **What evidence would resolve it:** Testing LoRA ranks across a continuous range (e.g., 1-32) and measuring IP, FID, inference speed, and memory usage would identify the optimal rank for practical deployment.

## Limitations

- Dataset quality concerns: CoProV2 is synthetically generated using LLMs, raising questions about its representativeness of real-world harmful content
- Merging strategy assumptions: Co-Merge assumes neuron activation frequency correlates with safety importance, but this may not always hold true
- Trade-off stability: While showing good IP/FID trade-offs on benchmarks, the stability of this balance across different domains and prompt distributions is untested

## Confidence

**High Confidence Claims:**
- The basic DPO framework can be applied to safety alignment in T2I models
- Category-specific LoRA experts reduce IP compared to unaligned models

**Medium Confidence Claims:**
- Co-Merge outperforms naive averaging of LoRA matrices
- AlignGuard generalizes across different T2I model architectures

**Low Confidence Claims:**
- The 7x concept removal improvement compared to baselines
- Robustness to adversarial attacks

## Next Checks

1. **Dataset Realism Validation:** Generate human-annotated samples from CoProV2 and evaluate alignment between synthetic unsafe/safe pairs and real-world harmful content to assess dataset quality.

2. **Merging Strategy Ablation:** Implement and compare Co-Merge against alternative neuron selection strategies (e.g., based on gradient magnitude, mutual information) to validate the activation frequency assumption.

3. **Domain Transfer Study:** Test AlignGuard on out-of-distribution prompt domains (e.g., medical imaging, historical content) to evaluate safety alignment stability beyond the training categories.