---
ver: rpa2
title: Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving Human-AI
  Alignment in the Writing Process through Edits
arxiv_id: '2409.14509'
source_url: https://arxiv.org/abs/2409.14509
tags:
- writing
- text
- span
- edits
- editing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes a method to improve AI-generated text by having\
  \ professional writers edit paragraphs and identify common issues like clich\xE9\
  s, purple prose, and awkward phrasing. A taxonomy of seven edit categories is created\
  \ and used to annotate a corpus of 1,057 LLM-generated paragraphs."
---

# Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving Human-AI Alignment in the Writing Process through Edits

## Quick Facts
- arXiv ID: 2409.14509
- Source URL: https://arxiv.org/abs/2409.14509
- Reference count: 40
- Key outcome: Professional writers editing LLM-generated text and identifying common issues improves alignment between AI-generated and human-written text

## Executive Summary
This paper investigates whether AI-generated writing can be improved through professional editing. The authors create a taxonomy of seven edit categories by having 18 professional writers edit 1,057 LLM-generated paragraphs. They then develop automated methods using few-shot prompting to detect and rewrite problematic spans. Preference evaluations show that both human-edited and LLM-edited text are preferred over unedited LLM text, indicating that automatic editing can improve alignment between AI-generated and human-written text.

## Method Summary
The authors generate LLM responses to writing instructions derived from human-written paragraphs, then have professional writers edit these paragraphs while categorizing their edits into seven categories (cliché, unclear, redundant, awkward, overly poetic, ungrammatical, vague). They test automatic editing methods using few-shot prompting for span detection and category-specific rewriting prompts. The system's performance is evaluated through preference rankings comparing original LLM text, writer-edited text, and LLM-edited text.

## Key Results
- A taxonomy of seven edit categories was created and applied to a corpus of 1,057 LLM-generated paragraphs
- Few-shot prompting enables LLMs to detect problematic spans with non-trivial overlap to human annotations
- Preference evaluations show LLM-edited text is significantly preferred to unedited LLM text by expert writers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot prompting enables LLMs to detect problematic spans with non-trivial overlap to human annotations
- Mechanism: The model receives a small set of annotated examples (2-25 spans) showing spans and categories, then applies learned patterns to new paragraphs
- Core assumption: Span-level precision correlates with detection quality, and the few-shot examples are representative enough for generalization
- Evidence anchors:
  - [abstract] "Automated methods using few-shot prompting can detect and rewrite problematic spans, though not matching human performance."
  - [section 6.1] "We implement few-shot LLM-based methods [...] using fewer than 100 examples. Our experiment varies few-shot examples (2, 5, and 25) [...] tests [...] Table 9 summarizes results."
  - [corpus] Weak - only corpus statistics are provided; no explicit evaluation on the test set in the appendix.
- Break condition: Span detection degrades if examples are unrepresentative or if span boundaries require nuanced judgment beyond pattern matching.

### Mechanism 2
- Claim: Category-specific rewriting prompts improve span-level rewriting quality
- Mechanism: Each of the seven edit categories has a tailored prompt with 25 annotated examples. The LLM rewrites spans from the target category in isolation, guided by the prompt
- Core assumption: The category definitions and example rewrites are clear enough for the LLM to produce coherent replacements
- Evidence anchors:
  - [abstract] "A large-scale preference annotation confirms that although experts largely prefer text edited by other experts, automatic editing methods show promise in improving alignment between LLM-generated and human-written text."
  - [section 6.2] "We design prompts for each of the seven edit categories, incorporating examples of rewrites from writers. [...] The prompts for each category are listed in Appendix A.3."
  - [corpus] Weak - no explicit metrics for rewriting quality, only preference ranking
- Break condition: Rewriting fails if category boundaries overlap or if examples are ambiguous, causing inconsistent edits

### Mechanism 3
- Claim: Preference ranking between original, writer-edited, and LLM-edited text shows that edits improve alignment
- Mechanism: Annotators compare three versions of the same paragraph and rank them; consistent second-place ranking of LLM edits indicates improvement over raw LLM output
- Core assumption: The ranking task captures meaningful quality differences and annotators agree on preferences
- Evidence anchors:
  - [abstract] "Preference evaluations show that edited text is preferred over unedited LLM text, indicating automatic editing can improve alignment between AI-generated and human-written text."
  - [section 6.3] "Table 12 and Figure 8 summarize the preference evaluation results [...] showing average ranks [...] indicating that LLM-edited text is significantly preferred to LLM-generated text by expert writers."
  - [corpus] Weak - no explicit inter-annotator agreement beyond a single Kendall's W score in the main text
- Break condition: Ranking loses validity if annotator preferences are too subjective or if the LLM-edited text closely mimics human edits

## Foundational Learning

- Concept: Few-shot prompting in LLMs
  - Why needed here: Enables the model to perform span detection and rewriting without full fine-tuning, reducing data and compute costs
  - Quick check question: If you provide 5 examples instead of 25, what happens to detection precision? (Answer: precision improves from 2-shot to 5-shot but plateaus thereafter, per Table 9.)

- Concept: Span-level precision metric
  - Why needed here: Measures overlap between predicted and reference spans without over-predicting, critical for evaluating span detection quality
  - Quick check question: What is the difference between General and Categorical Precision? (Answer: General ignores category assignment; Categorical requires correct category.)

- Concept: Preference ranking in human evaluation
  - Why needed here: Provides a scalar comparison across three paragraph variants, capturing relative quality without requiring absolute scores
  - Quick check question: Why compare three variants instead of two? (Answer: To differentiate LLM edits from both raw LLM output and human edits.)

## Architecture Onboarding

- Component map: Data ingestion → Instruction/Response pair generation → Expert editing → LAMP corpus → Detection module (few-shot prompt) → Span extraction → Categorical labeling → Rewriting module (category-specific prompts) → Span replacement → LLM-edited paragraph → Evaluation pipeline → Preference ranking task → Statistical analysis

- Critical path: 1. Generate <instruction, response> pairs. 2. Apply detection module to find problematic spans. 3. Feed spans to rewriting module per category. 4. Replace spans in original paragraph. 5. Run preference ranking evaluation.

- Design tradeoffs:
  - Detection vs. rewriting complexity: Detection is cheaper to implement but lower quality; rewriting is more expensive but drives improvement
  - Few-shot vs. fine-tuning: Few-shot is faster but less precise; fine-tuning could improve performance but requires more data
  - Oracle vs. automatic span selection: Oracle ensures high-quality inputs to rewriting but limits scalability

- Failure signatures:
  - Detection: High false positives (too many spans) or false negatives (missing issues)
  - Rewriting: Incoherent edits, category drift, or stylistic inconsistency
  - Evaluation: Low inter-annotator agreement, preference ties, or systematic bias

- First 3 experiments:
  1. Vary few-shot example count (2, 5, 25) and measure span-level precision on held-out data
  2. Compare rewriting quality with and without category-specific prompts
  3. Run ablation study: LLM-edited with oracle spans vs. fully automatic spans in preference ranking

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed taxonomy of seven edit categories generalize beyond literary fiction and creative non-fiction to other genres like technical writing, journalism, or scientific writing?
- Basis in paper: [inferred] The paper notes that "The editing data primarily comes from literary fiction and creative non-fiction, making the identified idiosyncrasies and editing strategies potentially less applicable to other genres like technical writing, journalism, or scientific writing."
- Why unresolved: The study focused exclusively on creative writing genres, leaving the applicability of the taxonomy to other domains unexplored.
- What evidence would resolve it: Testing the taxonomy on writing samples from diverse genres and comparing the frequency and types of edits required across domains would show if the same categories apply or if new ones emerge.

### Open Question 2
- Question: Would training a model on the entire LAMP Corpus or additional data improve the quality of automatic edits compared to the few-shot learning approach used in this study?
- Basis in paper: [explicit] The paper states "While not optimal, as a single LLM might offer slightly better detection and rewriting capabilities, this approach allows us to simplify the experiment conceptually" and "One potential limitation in our experiments is our reliance on few-shot instructions, requiring the model to learn rewriting from only a few examples."
- Why unresolved: The study used few-shot prompting with a limited number of examples rather than fine-tuning a model on the full dataset.
- What evidence would resolve it: Training a model on the complete LAMP Corpus and evaluating its performance on the same tasks would reveal if the additional data leads to better detection and rewriting of problematic spans.

### Open Question 3
- Question: How do the long-term effects of LLM prevalence on language evolution and writing styles compare to the potential benefits of aligned editing tools in preserving linguistic diversity and individual voice?
- Basis in paper: [explicit] The paper discusses potential homogenization effects and states "However, well-designed editing tools aligned with expert writing practices could help counteract these effects."
- Why unresolved: The paper identifies both risks and potential solutions but does not empirically examine the actual impact of LLMs on language evolution over time.
- What evidence would resolve it: Longitudinal studies tracking writing styles and linguistic patterns over time in populations using LLMs versus those using aligned editing tools would reveal the net effect on language diversity and evolution.

## Limitations

- The taxonomy and editing strategies may not generalize well to other writing genres beyond literary fiction and creative non-fiction
- Few-shot prompting, while practical, does not match human performance in span detection and rewriting quality
- The study relies heavily on preference rankings for evaluation, lacking direct quality metrics for the rewriting output

## Confidence

- High confidence: The methodology for collecting professional edits and creating the taxonomy is well-documented and replicable. The preference evaluation design is sound.
- Medium confidence: The few-shot prompting approach for span detection shows measurable improvement over baseline, though performance gaps remain. The rewriting quality improvements are demonstrated through preference rankings but lack direct quality metrics.
- Low confidence: The generalizability of results beyond the specific writing tasks and models tested. The automatic detection and rewriting may not scale well to different writing styles or domains.

## Next Checks

1. Test the few-shot detection approach with varying numbers of examples (2, 5, 25) on a held-out validation set to confirm precision improvements
2. Run an ablation study comparing LLM-edited text with oracle-selected spans versus fully automatic span selection in preference ranking
3. Measure inter-annotator agreement across multiple preference ranking tasks to validate the reliability of human evaluation results