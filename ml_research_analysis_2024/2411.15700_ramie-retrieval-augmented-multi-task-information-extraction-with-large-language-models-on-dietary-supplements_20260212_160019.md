---
ver: rpa2
title: 'RAMIE: Retrieval-Augmented Multi-task Information Extraction with Large Language
  Models on Dietary Supplements'
arxiv_id: '2411.15700'
source_url: https://arxiv.org/abs/2411.15700
tags:
- tasks
- information
- task
- performance
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a retrieval-augmented multi-task information
  extraction (RAMIE) framework for dietary supplement (DS) data using large language
  models (LLMs). The framework integrates multi-task learning, retrieval-augmented
  generation (RAG), and instruction fine-tuning to extract DS-related information
  from clinical records across four tasks: named entity recognition (NER), relation
  extraction (RE), triple extraction (TE), and usage classification (UC).'
---

# RAMIE: Retrieval-Augmented Multi-task Information Extraction with Large Language Models on Dietary Supplements

## Quick Facts
- arXiv ID: 2411.15700
- Source URL: https://arxiv.org/abs/2411.15700
- Reference count: 40
- RAMIE improves F1 scores by up to 14.26% on dietary supplement information extraction tasks

## Executive Summary
This paper introduces RAMIE, a retrieval-augmented multi-task information extraction framework for dietary supplement data using large language models. The framework integrates multi-task learning, retrieval-augmented generation (RAG), and instruction fine-tuning to extract information across four tasks: named entity recognition, relation extraction, triple extraction, and usage classification. Experiments with eight LLMs demonstrate that RAMIE significantly outperforms single-task fine-tuning baselines while offering improved storage efficiency and lower training costs through unified multi-task training.

## Method Summary
RAMIE employs a unified training approach that consolidates task-specific datasets into a single multi-task dataset, enabling a single LLM to handle all four extraction tasks simultaneously. The framework uses instruction fine-tuning with task-specific prompts and integrates RAG by dynamically retrieving similar examples from training data during inference. LoRA fine-tuning is applied with specific hyperparameters (rank=64, alpha=32, dropout=0.1), and retrievers (MedCPT, Contriever, BMRetriever) identify relevant examples based on cosine similarity of sentence embeddings. The system is evaluated across eight LLMs on dietary supplement clinical records with standard F1 metrics.

## Key Results
- RAMIE improved F1 scores by up to 14.26% compared to single-task fine-tuning baselines
- Llama2-13B achieved 87.39 F1 on named entity recognition
- MedAlpaca-7B reached 93.45 F1 on usage classification
- The framework demonstrated both high accuracy and efficiency, enabling a single model to handle multiple tasks while reducing storage and training costs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAMIE's RAG component significantly boosts LLM performance by dynamically providing task-relevant examples during inference
- Mechanism: The framework retrieves similar examples from the training set using retrievers (MedCPT, Contriever, BMRetriever) based on cosine similarity of sentence embeddings. These retrieved examples are then incorporated into a dynamically constructed prompt alongside the input sentence and task instruction.
- Core assumption: Retrieved examples are semantically similar enough to provide useful context without causing the model to simply copy responses
- Evidence anchors:
  - [abstract] "incorporated retrieval augmentation generation (RAG) techniques by retrieving similar examples from the training set"
  - [section] "During training and testing, the framework employs retrievers to identify the most relevant sentence-response pair from the corresponding training set based on the cosine similarity of sentence embeddings"
  - [section] "The retrieval process involves concatenating the input sentence and its corresponding response to form a single representation, which is then compared to the input sentence embedding using cosine similarity"
- Break condition: If retrieved examples are too dissimilar or too similar (causing copying behavior), the performance boost would diminish or disappear

### Mechanism 2
- Claim: Multi-task learning (MTL) in RAMIE reduces storage requirements and training costs while maintaining performance
- Mechanism: RAMIE consolidates task-specific datasets (NER, RE, TE, UC) into a unified training set, allowing a single LLM to be trained on all tasks simultaneously rather than requiring separate models for each task
- Core assumption: A single model can effectively learn to distinguish between task types and maintain sufficient capacity for each task despite shared parameters
- Evidence anchors:
  - [abstract] "trained LLMs for multiple tasks with improved storage efficiency and lower training costs"
  - [section] "The MTL in the RAMIE framework offers a practical and scalable solution for deploying LLMs to address diverse tasks, especially in resource-constrained environments"
  - [section] "RAMIE allows a single LLM to acquire the capability to solve multiple tasks with just one fine-tuning operation"
- Break condition: If task interference becomes too severe, the performance drop from MTL could outweigh the efficiency benefits

### Mechanism 3
- Claim: Instruction fine-tuning with task-specific prompts enables LLMs to generalize across diverse extraction tasks
- Mechanism: RAMIE employs carefully designed prompts for each task that include an instruction part, an example part (with retrieved examples), and the input sentence with its response. These prompts guide the LLM on the expected output format and task-specific requirements
- Core assumption: LLMs can effectively parse and follow structured instructions within prompts to produce task-appropriate outputs
- Evidence anchors:
  - [section] "we employed instruction fine-tuning... following the LEAP framework, we designed the template of prompts for each task"
  - [section] "The instructions are designed to provide instruction for each task as follows, [with specific examples for NER, RE, TE, and UC tasks]"
  - [section] "To optimize the performance of LLMs across multiple tasks, we employed instruction fine-tuning, a technique that helps models generalize by providing explicit task instructions within the input prompt"
- Break condition: If prompts are not sufficiently clear or if the LLM fails to properly parse instruction formats, task performance would degrade

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG allows the framework to dynamically incorporate relevant examples from training data during inference, improving context relevance and reducing the burden on the LLM to store all knowledge internally
  - Quick check question: How does RAG differ from simply including all training examples in the prompt?

- Concept: Multi-Task Learning (MTL)
  - Why needed here: MTL enables a single model to handle multiple information extraction tasks simultaneously, reducing storage redundancy and computational resources while maintaining high performance
  - Quick check question: What is the main trade-off when using MTL compared to single-task learning?

- Concept: Instruction Fine-Tuning
  - Why needed here: Instruction fine-tuning helps LLMs generalize across tasks by providing explicit task instructions within the input prompt, improving the model's ability to understand and execute diverse extraction tasks
  - Quick check question: How does instruction fine-tuning differ from standard fine-tuning approaches?

## Architecture Onboarding

- Component map:
  Retriever module (MedCPT, Contriever, BMRetriever) -> Prompt construction module -> LLM core -> Multi-task training pipeline -> Evaluation module

- Critical path:
  1. Input sentence arrives with task label
  2. Retriever finds most similar examples from task-specific training set
  3. Prompt constructed with instruction, retrieved examples, and input
  4. LLM generates output based on prompt
  5. Output evaluated against ground truth

- Design tradeoffs:
  - Using RAG adds retrieval latency but improves accuracy
  - MTL reduces storage but may cause slight performance degradation
  - Instruction fine-tuning requires careful prompt design but improves generalization
  - Single unified model is efficient but less specialized than task-specific models

- Failure signatures:
  - Poor retriever performance → irrelevant examples in prompts → degraded output quality
  - Prompt construction errors → model confusion about task requirements
  - Task interference in MTL → performance drops on specific tasks
  - Insufficient fine-tuning → model fails to learn task distinctions

- First 3 experiments:
  1. Single-task instruction fine-tuning baseline for each task to establish performance benchmarks
  2. RAMIE framework with all retrievers (MedCPT, Contriever, BMRetriever) to compare retrieval effectiveness
  3. Multi-task instruction fine-tuning without RAG ablation study to quantify RAG contribution to performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RAMIE change when using larger language models (e.g., Llama-2-70B or GPT-4) compared to the 8B and 13B models tested?
- Basis in paper: [inferred] The paper acknowledges not testing particularly large models (e.g., Llama-2-70B) and notes that larger models might better handle the complexities of multi-task learning.
- Why unresolved: The study focused on 8B and 13B parameter models due to resource constraints, leaving the performance of larger models unexplored.
- What evidence would resolve it: Experimental results comparing RAMIE's performance using Llama-2-70B, GPT-4, or similar large models against the current 8B and 13B models.

### Open Question 2
- Question: What is the impact of task-specific retrievers optimized for dietary supplements on RAMIE's performance compared to the general-purpose retrievers (MedCPT, Contriever, BMRetriever) used in the study?
- Basis in paper: [explicit] The paper states that no retriever exists that is specifically optimized for dietary supplement tasks and suggests this might further improve model accuracy and relevance.
- Why unresolved: The study used general-purpose retrievers due to the lack of task-specific retrievers for the dietary supplement domain.
- What evidence would resolve it: Development and testing of retrievers specifically trained on dietary supplement data, followed by comparison of their performance against the general-purpose retrievers used in RAMIE.

### Open Question 3
- Question: How does RAMIE's performance change under few-shot learning scenarios compared to the full-data training approach used in the study?
- Basis in paper: [explicit] The paper mentions not investigating few-shot learning scenarios and suggests examining the model's performance under few-shot conditions could offer valuable insights.
- Why unresolved: The study used full datasets for training and did not explore how RAMIE performs with limited labeled data.
- What evidence would resolve it: Experiments testing RAMIE's performance using varying amounts of labeled data (e.g., 1%, 5%, 10%) and comparing the results to the full-data training approach.

## Limitations

- Data Domain Specificity: The framework is evaluated exclusively on dietary supplement clinical records, raising questions about generalizability to other medical domains or non-clinical text.
- Retriever Performance Impact: The study shows MedCPT outperforms other retrievers but lacks systematic analysis of retrieval quality metrics or investigation of failure cases where retrieved examples are irrelevant.
- Multi-Task Interference: While the paper claims MTL provides efficiency benefits, it doesn't quantify task interference effects or analyze whether certain tasks suffer disproportionate performance degradation when trained jointly.

## Confidence

- High Confidence: The claim that RAMIE improves F1 scores compared to single-task fine-tuning is well-supported by experimental results across multiple LLMs and tasks.
- Medium Confidence: The assertion that RAG dynamically provides task-relevant examples is supported by methodology description but lacks detailed analysis of retrieval quality or ablation studies.
- Low Confidence: The claim that MTL significantly reduces storage and training costs while maintaining performance is partially supported but not thoroughly validated with comprehensive cost-benefit analysis.

## Next Checks

1. **Ablation Study on RAG Components**: Systematically remove the RAG component and compare performance across all four tasks to quantify its exact contribution to the 14.26% F1 improvement. Include retrieval quality metrics (precision@K, recall@K) and analyze cases where retrieved examples help versus harm performance.

2. **Cross-Domain Generalization Test**: Evaluate RAMIE on information extraction tasks from non-DS medical domains (e.g., radiology reports, discharge summaries) and non-medical domains (e.g., legal documents, scientific literature) to assess the framework's generalizability beyond dietary supplement data.

3. **Task Interference Analysis**: Conduct controlled experiments isolating each task's performance when trained jointly versus separately, including pairwise task combinations to identify which task combinations cause the most interference and whether certain tasks consistently underperform in the multi-task setting.