---
ver: rpa2
title: Mitigating Suboptimality of Deterministic Policy Gradients in Complex Q-functions
arxiv_id: '2410.11833'
source_url: https://arxiv.org/abs/2410.11833
tags:
- action
- actor
- actions
- policy
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SA VO addresses the challenge of deterministic policy gradient
  methods getting stuck in local optima when optimizing complex, non-convex Q-functions
  in tasks like dexterous manipulation and restricted locomotion. It introduces a
  multi-actor architecture where multiple gradient-based actors are trained on successively
  pruned Q-landscapes (surrogates), with the best action selected via arg max.
---

# Mitigating Suboptimality of Deterministic Policy Gradients in Complex Q-functions

## Quick Facts
- **arXiv ID**: 2410.11833
- **Source URL**: https://arxiv.org/abs/2410.11833
- **Reference count**: 40
- **Key outcome**: SA VO addresses deterministic policy gradient methods getting stuck in local optima by using multiple actors trained on successively pruned Q-landscapes, achieving higher returns and better sample efficiency in dexterous manipulation and restricted locomotion tasks.

## Executive Summary
SA VO introduces a multi-actor architecture that mitigates the local optima problem in deterministic policy gradient methods by training actors on successively pruned Q-landscapes (surrogates). The surrogates progressively flatten regions below previously discovered high-value actions, creating a smoother optimization landscape. Multiple actors with a maximizer selector find better global optima than a single actor, while neural network approximations preserve gradient flow. Evaluations on continuous control tasks and large discrete-action recommender systems show SA VO consistently outperforms baselines like TD3, Wolpertinger, and CEM.

## Method Summary
SA VO trains k additional actors ν1,...,νk in parallel with a main actor ν0, each conditioned on previous actions and trained on successive surrogate Q-functions ˆΨi that approximate truncated Q-landscapes. The surrogates flatten regions below anchor actions (previous optima), reducing local optima. A maximizer actor µM selects the best action among all k+1 proposals based on the current Q-function. The method removes TD3's policy smoothing, adds exploration noise to actors, and uses twin critics for surrogate updates. Action conditioning uses FiLM layers for discrete tasks and DeepSet/LSTM summarizers for continuous control.

## Key Results
- SA VO consistently achieves higher returns and better sample efficiency than baselines across restricted locomotion, dexterous manipulation, and recommender system tasks
- Successive surrogates reduce local optima in Q-landscapes, with diminishing returns beyond 3-5 actors but still significant improvement with 10 actors
- The method works with both continuous and discrete action spaces through nearest-neighbor mapping from continuous representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Successive surrogates reduce local optima by truncating regions below previously discovered high-value actions
- Mechanism: Each surrogate Ψi elevates Q-values of actions below max Q-value of previous actions (τi = maxj<i Q(s, aj)), removing shallow local optima while preserving higher-value regions
- Core assumption: Q-function landscape has countable local optima that can be systematically eliminated by raising the "floor" below anchor actions
- Evidence anchors:
  - [abstract]: "approximates the Q-function repeatedly by truncating poor local optima to guide gradient ascent more effectively"
  - [section]: "Theorem 4.2. For a state s∈ S and surrogates Ψi defined as above, the number of local optima decreases with each successive surrogate"
  - [corpus]: Weak - no direct corpus evidence found
- Break condition: If Q-function has too many deep local optima clustered closely together, truncation may not effectively reduce total count of optima

### Mechanism 2
- Claim: Multiple actors with maximizer selector strictly improve over single actor by finding better global optima
- Mechanism: Training k additional actors and selecting action with highest Q-value among all proposals guarantees at least as good performance as any individual actor
- Core assumption: Q-function can be evaluated at multiple candidate actions without prohibitive computational cost
- Evidence anchors:
  - [abstract]: "learns several policies in parallel and then selects the best action among them based on the current Q-function"
  - [section]: "Q(s, µM(s)) = maxai Q(s, ai)≥Q(s, a 0) =Q(s, µ(s))"
  - [corpus]: Weak - no direct corpus evidence found
- Break condition: If Q-function evaluations are highly noisy or biased, arg max selection may choose suboptimal actions

### Mechanism 3
- Claim: Surrogate approximation with neural networks preserves gradient flow while smoothing Q-landscape
- Mechanism: Neural network approximation ˆΨi tracks truncated Q-values while maintaining non-zero gradients through smooth interpolation, avoiding zero-gradient traps
- Core assumption: Smooth approximation can maintain directional bias toward high-value regions while avoiding zero-gradient traps
- Evidence anchors:
  - [abstract]: "employs a sequence of 'surrogate' Q-landscapes that progressively truncate lower-value regions"
  - [section]: "To address this issue, we ease the gradient flow by learning a smooth approximation ˆΨi of Ψi"
  - [corpus]: Weak - no direct corpus evidence found
- Break condition: If approximation error becomes too large, actor may follow gradients toward regions that aren't actually high-value in true Q-function

## Foundational Learning

- Concept: Deterministic Policy Gradients (DPG) and its limitations in non-convex Q-landscapes
  - Why needed here: Paper builds on DPG as baseline method that gets stuck in local optima
  - Quick check question: What is the key difference between DPG and stochastic policy gradient methods in terms of action selection?

- Concept: Non-convex optimization and local optima
  - Why needed here: Core problem being addressed is gradient ascent in non-convex Q-functions getting trapped in local optima
  - Quick check question: How does presence of local optima in Q-function landscape affect convergence of gradient-based policy optimization?

- Concept: Function approximation and surrogate modeling
  - Why needed here: Successive surrogates are neural network approximations of truncated Q-functions
  - Quick check question: What are tradeoffs between using exact truncated functions versus smooth approximations in gradient-based optimization?

## Architecture Onboarding

- Component map:
  Q-network -> Multiple actors (ν0,...,νk) -> Successive surrogates (ˆΨ0,...,ˆΨk) -> Maximizer actor (µM) -> FiLM layers/DeepSet summarizers

- Critical path:
  1. Initialize Q-network, actors, and surrogates
  2. At each step: evaluate all actors, select best action via maximizer
  3. Store transition with all actor actions in replay buffer
  4. Update Q-network via TD error
  5. Update each surrogate ˆΨi to track truncated Q-values at key actions
  6. Update each actor νi via gradient ascent on its surrogate

- Design tradeoffs:
  - Number of actors vs. computational cost (more actors = better exploration but slower inference)
  - Exact vs. approximate surrogates (exact has zero gradients in flat regions, approximations maintain flow but add error)
  - Conditioning mechanism (DeepSet vs. LSTM vs. Transformer for handling previous actions)

- Failure signatures:
  - Actors converge to same actions repeatedly (insufficient diversity in initialization or conditioning)
  - Surrogate approximation error grows large (learning rate too high or insufficient training)
  - Maximizer consistently selects same actor (Q-function evaluations may be noisy or actors not exploring sufficiently)

- First 3 experiments:
  1. Run SA VO on simple 1D Q-landscape task (like restricted inverted pendulum) to visualize how successive surrogates reduce local optima
  2. Compare performance with 1, 3, and 5 actors to find optimal tradeoff between exploration and computation
  3. Test with and without action smoothing in discrete tasks to verify its importance for gradient flow

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does SA VO's improvement scale sublinearly with number of actors, and is there theoretical upper bound on performance gains?
- Basis in paper: [explicit] Section 6.4 shows diminishing returns beyond 3-5 actors, but SA VO with 10 actors still significantly improves performance
- Why unresolved: Paper only empirically demonstrates scaling up to 10 actors without theoretical analysis of upper bound
- What evidence would resolve it: Experiments with more than 10 actors across diverse environments, combined with theoretical analysis of convergence rates and performance bounds

### Open Question 2
- Question: How does SA VO perform on tasks with extremely high-dimensional action spaces (>100 dimensions) where action representation becomes sparse?
- Basis in paper: [inferred] SA VO tested on tasks up to 8 dimensions (Ant-v4), but paper doesn't address scalability to much higher dimensions
- Why unresolved: Paper doesn't investigate performance degradation in extremely high-dimensional spaces where action representation sparsity becomes problematic
- What evidence would resolve it: Experiments on tasks with progressively higher-dimensional action spaces, measuring performance degradation and actor conditioning effectiveness

### Open Question 3
- Question: Can successive surrogate approach be adapted for stochastic actor-critic methods beyond SAC, such as PPO or A2C?
- Basis in paper: [explicit] Section 17 shows SA VO+SAC improves SAC performance, but only tests this combination
- Why unresolved: Paper only demonstrates SA VO with SAC and doesn't explore applicability to other stochastic actor-critic methods
- What evidence would resolve it: Implementation and evaluation of SA VO with PPO, A2C, or other stochastic actor-critic methods across various environments

### Open Question 4
- Question: What is computational complexity of SA VO compared to baseline methods, and how does it scale with number of actors and surrogate networks?
- Basis in paper: [explicit] Table 1 shows GPU memory and inference time for k=3 and k=5, but doesn't analyze scaling complexity
- Why unresolved: Paper provides limited computational analysis and doesn't discuss theoretical scaling complexity
- What evidence would resolve it: Detailed computational complexity analysis with theoretical scaling bounds, combined with empirical measurements across varying actor/surrogate counts

## Limitations
- Theoretical analysis focuses on countable local optima but doesn't fully address scenarios with dense clusters of deep local optima
- Computational overhead of training multiple actors and surrogates may limit practical applicability in resource-constrained settings
- Performance gains may diminish with very large numbers of actors, though exact scaling behavior isn't fully characterized

## Confidence
- **High confidence**: Core mechanism of using successive surrogates to reduce local optima is well-supported by both theory and empirical results
- **Medium confidence**: Claim that multiple actors strictly improve performance relies on assumption that Q-function evaluations are reliable
- **Medium confidence**: Surrogate approximation approach preserves gradient flow while smoothing landscape, but approximation error impact isn't fully quantified

## Next Checks
1. Test SA VO on environments with known dense local optima clusters (e.g., highly fragmented action spaces) to verify truncation mechanism remains effective
2. Conduct ablation studies comparing exact vs. approximate surrogates with varying levels of approximation error to quantify tradeoff between gradient flow and fidelity
3. Measure computational overhead of SA VO vs. baselines across different actor counts to establish practical scalability limits