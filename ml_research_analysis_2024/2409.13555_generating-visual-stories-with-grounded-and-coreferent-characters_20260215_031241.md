---
ver: rpa2
title: Generating Visual Stories with Grounded and Coreferent Characters
arxiv_id: '2409.13555'
source_url: https://arxiv.org/abs/2409.13555
tags:
- character
- visual
- coreference
- stories
- characters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the task of character-centric visual story
  generation, aiming to produce stories with grounded and coreferent character mentions.
  We build VIST++, a dataset with 40K stories and 300K unique characters, by automatically
  enriching the VIST benchmark with visual and textual character coreference chains.
---

# Generating Visual Stories with Grounded and Coreferent Characters

## Quick Facts
- **arXiv ID**: 2409.13555
- **Source URL**: https://arxiv.org/abs/2409.13555
- **Reference count**: 40
- **Primary result**: Introduces character-centric visual story generation with VIST++, achieving up to 32.16% coreference accuracy using OTTER with visual prompts and coreference annotations.

## Executive Summary
This work introduces character-centric visual story generation, aiming to produce stories with grounded and coreferent character mentions. The authors build VIST++, a dataset with 40K stories and 300K unique characters, by automatically enriching the VIST benchmark with visual and textual character coreference chains. They propose a new visual story generation model based on OTTER that uses segmentation masks and coreference annotations to improve character grounding. The model outperforms baselines in generating stories with richer, more consistent characters, achieving up to 32.16% coreference accuracy. They also introduce LLM-as-a-judge for automated story evaluation, showing that their method generates more character-rich and coherent stories compared to prior systems.

## Method Summary
The authors create VIST++ by automatically enriching VIST with visual and textual character coreference chains. They use DETR for character detection, SAM for segmentation masks, and an incremental Hungarian algorithm for visual clustering. Textual coreference is resolved using LLM-based prompting. Visual and textual chains are aligned using bipartite matching on similarity matrices. The OTTER model is finetuned on VIST++ with visual prompts and character grounding format. During inference, the same clustering pipeline generates stories with markdown tags for characters. Story quality is evaluated using automated LLM-as-a-judge comparisons and human evaluations.

## Key Results
- OTTER model with visual prompts and coreference annotations achieves up to 32.16% coreference accuracy
- Combining visual and textual coreference annotations improves character grounding and coreference accuracy
- LLM-as-a-judge shows 70-80% agreement with human evaluations across story quality dimensions
- Generated stories show improved character richness and consistency compared to baselines

## Why This Works (Mechanism)

### Mechanism 1: Incremental Hungarian Algorithm for Visual Clustering
The incremental Hungarian algorithm-based visual clustering avoids predefining the number of character clusters while handling overlapping characters via segmentation masks. Each new image's detections are matched to existing clusters via a similarity matrix; unmatched detections spawn new clusters. This dynamic approach sidesteps the rigidity of fixed cluster counts. Core assumption: Visual similarity computed via LLAVA1.5-13B reliably reflects identity across varied poses and backgrounds.

### Mechanism 2: LLM-as-a-Judge for Automated Evaluation
LLM-as-a-judge provides reliable side-by-side preference judgments that align closely with human evaluations. Structured prompting instructs GPT-4O to output a preference dictionary over six quality dimensions. Agreement with human labels measured as accuracy. Core assumption: LLM judgment criteria can be encoded in prompts that mirror human evaluation dimensions.

### Mechanism 3: Multimodal Coreference Integration
Combining visual and textual coreference annotations improves character grounding and coreference accuracy more than either modality alone. Visual chains give image-to-character mapping; textual chains give mention-to-character mapping. Multimodal alignment (Hungarian on chain similarity) ties them together. Core assumption: Distributional similarity between visual and textual chains is a reliable alignment signal.

## Foundational Learning

- **Visual object detection + segmentation**: To isolate individual characters in images for grounding. Quick check: What model outputs a bounding box and a pixel mask for each detected "Person"?
- **Coreference resolution**: To group mentions ("she", "Ella") that refer to the same character across text. Quick check: How do you compute precision/recall for a coreference chain using B3 metrics?
- **Graph matching (Hungarian algorithm)**: To align textual and visual character chains by maximizing similarity scores. Quick check: What is the time complexity of the Hungarian algorithm for an n×n cost matrix?

## Architecture Onboarding

- **Component map**: Image → DETR detection → SAM masks → LLAVA similarity → Hungarian clustering → Visual chains; Story text → LLM prompting → Textual chains; Chains → Hungarian alignment → VIST++; VIST++ → OTTER finetuning → Story generation
- **Critical path**: Image → Detection → Mask → Cluster → Chain → Prompt → OTTER → Story → Evaluation
- **Design tradeoffs**: Segmentation masks over boxes (higher accuracy but slower inference); LLM-based similarity over cosine (better identity discrimination but costlier); Full pipeline automation vs. manual annotation (scalability vs. precision)
- **Failure signatures**: Low visual recall → missing characters → poor grounding; LLM prompt misunderstanding → malformed markdown tags → downstream model confusion; Misaligned chains → coreference score drops
- **First 3 experiments**: 1) Swap LLAVA similarity for CLIP cosine; measure clustering F1. 2) Remove visual prompts; train OTTER on VIST++ stories only; compare coref scores. 3) Replace Hungarian alignment with greedy nearest-neighbor; check multimodal chain recall.

## Open Questions the Paper Calls Out

# Open Question 1
- **Question**: What is the impact of using larger language models with more than 70B parameters on character coreference accuracy in visual storytelling?
- **Basis in paper**: [inferred] The paper uses LLAMA3-70B for textual coreference resolution and mentions that smaller models like LLAMA3-8B failed to handle complex prompts. This suggests potential improvements with larger models.
- **Why unresolved**: The paper only experiments with LLAMA3-70B and does not explore the performance of even larger language models on this task.
- **What evidence would resolve it**: Conducting experiments with language models larger than 70B parameters and comparing their character coreference accuracy against LLAMA3-70B would provide evidence.

# Open Question 2
- **Question**: How does the proposed character-centric approach perform on other visual storytelling datasets like Visual Writing Prompts (VWP) or more recent datasets?
- **Basis in paper**: [explicit] The paper mentions that the proposed pipeline could be applied to related datasets such as VWP but does not actually perform experiments on these datasets.
- **Why unresolved**: The experiments are conducted only on the VIST dataset, leaving the generalizability of the approach to other datasets unexplored.
- **What evidence would resolve it**: Applying the character-centric approach to other visual storytelling datasets and evaluating the performance in terms of character richness, grounding, and coreference would provide evidence.

# Open Question 3
- **Question**: What is the effect of using different object detection models and segmentation methods on the accuracy of visual character coreference chains?
- **Basis in paper**: [inferred] The paper uses DETR for object detection and SAM for segmentation, but does not explore alternative models or methods.
- **Why unresolved**: The paper does not conduct ablation studies or comparisons with other object detection and segmentation approaches.
- **What evidence would resolve it**: Experimenting with different object detection models (e.g., YOLO, Faster R-CNN) and segmentation methods (e.g., Mask R-CNN, DeepLab) and comparing the resulting visual character coreference accuracy would provide evidence.

## Limitations

- Visual character detection pipeline relies heavily on DETR and SAM quality, which may struggle with crowded scenes or partially occluded characters
- Textual coreference resolution depends on LLM's ability to correctly identify character mentions, which may fail for complex pronoun references or culturally specific naming conventions
- Dataset construction assumes visual and textual character appearances can be meaningfully aligned, but cases where characters appear in images without being mentioned (or vice versa) create potential misalignment

## Confidence

**High Confidence**: OTTER model's ability to incorporate visual prompts and generate character-grounded stories is well-supported by quantitative metrics (coreference accuracy up to 32.16%) and qualitative human evaluations showing improved character richness and consistency.

**Medium Confidence**: Automated pipeline for creating VIST++ with visual and textual coreference chains produces reliable results, though manual verification on a subset of data would strengthen this claim. LLM-as-judge methodology shows promising agreement with human evaluations but may have domain-specific biases.

**Low Confidence**: Claim that method generates "more character-rich and coherent stories" relies heavily on automated metrics and limited human evaluation. Visual clustering algorithm's robustness across diverse image types and alignment between visual and textual chains could benefit from more extensive validation.

## Next Checks

1. **Cross-dataset Generalization Test**: Apply visual clustering and coreference pipeline to a different visual storytelling dataset (e.g., Pororo-SV or Flintstones) to evaluate whether the 0.3 similarity threshold and clustering approach generalize beyond VIST.

2. **Manual Annotation Verification**: Conduct human evaluation on 100 randomly sampled stories from VIST++ to verify accuracy of visual character detection, textual coreference resolution, and multimodal chain alignment, comparing against automated pipeline outputs.

3. **Ablation Study on Similarity Metrics**: Replace LLAVA1.5-13B-based similarity with simpler CLIP-based cosine similarity and measure impact on clustering F1 scores, visual chain recall, and downstream story generation quality to quantify value added by sophisticated similarity computation.