---
ver: rpa2
title: 'CuriousLLM: Elevating Multi-Document Question Answering with LLM-Enhanced
  Knowledge Graph Reasoning'
arxiv_id: '2404.09077'
source_url: https://arxiv.org/abs/2404.09077
tags:
- questions
- question
- agent
- passages
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CuriousLLM, a novel approach that enhances
  multi-document question answering (MD-QA) by integrating a curiosity-driven reasoning
  mechanism into an LLM agent. The key innovation is the Follow-upQA dataset, which
  trains the agent to generate follow-up questions that guide the information retrieval
  process more efficiently, addressing challenges like hallucinations and knowledge
  cutoffs.
---

# CuriousLLM: Elevating Multi-Document Question Answering with LLM-Enhanced Knowledge Graph Reasoning

## Quick Facts
- arXiv ID: 2404.09077
- Source URL: https://arxiv.org/abs/2404.09077
- Authors: Zukang Yang; Zixuan Zhu; Xuan Zhu
- Reference count: 38
- One-line primary result: CuriousLLM achieves up to 9% accuracy improvement over BM25 on HotpotQA through follow-up question generation and early termination

## Executive Summary
CuriousLLM introduces a novel approach to multi-document question answering by replacing direct evidence prediction with a curiosity-driven reasoning mechanism. The system fine-tunes an LLM agent on the Follow-upQA dataset to generate follow-up questions that guide information retrieval through a knowledge graph. This approach addresses key challenges in MD-QA including hallucinations, knowledge cutoffs, and computational inefficiency, achieving significant performance improvements while reducing latency through an early termination mechanism.

## Method Summary
CuriousLLM integrates a fine-tuned LLM agent into a knowledge graph prompting framework, where the agent generates follow-up questions instead of directly predicting missing evidence. The system uses LoRA to efficiently fine-tune Mistral-7B on the Follow-upQA dataset, which contains questions paired with supporting evidence and ground truth follow-up questions. Knowledge graph construction employs MDR-KG with BERT-based passage encoding, while retrieval uses sentence transformers and similarity functions. An early termination mechanism allows the agent to signal when sufficient information has been gathered, reducing computational costs.

## Key Results
- Achieves up to 9% accuracy improvement over BM25 on HotpotQA
- Demonstrates 3% average improvement over original KGP framework
- Reduces computational latency through early termination mechanism
- Shows robust MD-QA capabilities with fewer training samples and resources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CuriousLLM reduces hallucinations by replacing direct evidence prediction with follow-up question generation.
- Mechanism: Instead of T5 predicting missing evidence directly, the LLM generates a follow-up question that logically guides retrieval toward the missing piece of information.
- Core assumption: Follow-up questions create a logical bridge between semantically unrelated passages, making it easier to retrieve relevant evidence.
- Evidence anchors:
  - [abstract]: "Central to our approach is the development of the new Follow-upQA dataset, which includes questions and supporting evidence as input, with follow-up questions serving as ground truths."
  - [section]: "Instead of predicting missing evidence, our agent asks follow-up questions to more efficiently guide the search toward missing evidence."
  - [corpus]: Weak - no direct corpus evidence for hallucination reduction mechanism.

### Mechanism 2
- Claim: Early termination mechanism reduces computational latency by stopping the search when sufficient information is gathered.
- Mechanism: The LLM is trained to recognize when it has gathered enough evidence to answer the question and outputs a special token ("NA") to terminate the search.
- Core assumption: The LLM can accurately determine when the retrieved evidence is sufficient to answer the query.
- Evidence anchors:
  - [abstract]: "Additionally, the early termination mechanism reduces computational latency by stopping the search when sufficient information is gathered."
  - [section]: "We train the LLM agent to know when to end the search, and this early termination mechanism significantly reduces the latency associated with the original T5 agent."
  - [corpus]: Weak - no direct corpus evidence for early termination effectiveness.

### Mechanism 3
- Claim: Curiosity-driven reasoning improves multi-document question answering accuracy by 3% on average over KGP-T5.
- Mechanism: The LLM's curiosity-driven approach generates follow-up questions that guide traversal through the knowledge graph more efficiently than direct evidence prediction.
- Core assumption: A curiosity-driven approach leads to more efficient information gathering than direct prediction.
- Evidence anchors:
  - [abstract]: "CuriousLLM significantly boosts LLM performance in multi-document question answering (MD-QA), circumventing the substantial computational costs and latency from the original KGP framework."
  - [section]: "Our experiments show that CuriousLLM significantly boosts LLM performance in multi-document question answering (MD-QA), circumventing the substantial computational costs and latency from the original KGP framework."
  - [corpus]: Weak - no direct corpus evidence for accuracy improvement mechanism.

## Foundational Learning

- Concept: Knowledge Graph Construction and Traversal
  - Why needed here: The system relies on a knowledge graph to represent and navigate relationships between documents, requiring understanding of graph theory and traversal algorithms.
  - Quick check question: Can you explain how breadth-first search (BFS) works and why it's used for knowledge graph traversal?

- Concept: Fine-tuning Language Models with LoRA
  - Why needed here: The CuriousLLM agent is fine-tuned using LoRA to efficiently adapt a pre-trained model to the follow-up question generation task.
  - Quick check question: What is LoRA and how does it differ from full fine-tuning in terms of parameter efficiency?

- Concept: Multi-hop Reasoning in Question Answering
  - Why needed here: The system addresses multi-document questions that require reasoning across multiple pieces of evidence, requiring understanding of multi-hop reasoning techniques.
  - Quick check question: Can you describe the difference between bridging and comparison questions in multi-hop QA?

## Architecture Onboarding

- Component map:
  Follow-upQA Dataset -> Knowledge Graph Construction -> CuriousLLM Agent -> Retrieval System -> Early Termination -> Response Generation

- Critical path:
  1. Receive user query
  2. Generate seeding passages with TF-IDF
  3. CuriousLLM generates follow-up question
  4. Retrieve relevant passages using sentence transformers
  5. Repeat until early termination or budget exhausted
  6. Generate final answer with GPT4o-mini

- Design tradeoffs:
  - Fine-tuning vs. prompting: Fine-tuning provides better performance but requires more resources
  - Breadth-first vs. depth-first search: BFS ensures broader exploration but may be less efficient for some queries
  - Early termination vs. complete traversal: Early termination saves time but risks missing relevant information

- Failure signatures:
  - Stuck in local optima: Agent keeps generating similar follow-up questions without finding new information
  - Premature termination: Agent stops searching too early and misses critical evidence
  - Hallucinated follow-up questions: Agent generates irrelevant questions that don't guide retrieval effectively

- First 3 experiments:
  1. Test CuriousLLM on Follow-upQA dataset with different training checkpoints to find optimal performance point
  2. Compare CuriousLLM accuracy vs. baseline T5 agent on HotpotQA with early termination disabled
  3. Measure latency reduction when early termination is enabled across different question types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of training steps for CuriousLLM to balance accuracy and computational efficiency?
- Basis in paper: [explicit] The paper shows that MD-QA performance peaks at 1,200 steps, with a slight decline at 1,500 steps, suggesting potential overfitting.
- Why unresolved: The optimal training duration may vary depending on the dataset size, model architecture, and computational resources.
- What evidence would resolve it: Systematic experiments varying training steps across multiple datasets and model configurations to identify consistent patterns in performance and efficiency.

### Open Question 2
- Question: How does CuriousLLM perform on question types beyond comparison and bridging questions, such as "what," "where," and "how" questions?
- Basis in paper: [explicit] The paper acknowledges that the system primarily focuses on comparison and bridging questions, leaving other common types unexplored.
- Why unresolved: The current Follow-upQA dataset and training methodology may not adequately cover the diverse reasoning patterns required for different question types.
- What evidence would

## Limitations
- Knowledge graph construction methodology remains underspecified with missing implementation details
- Early termination mechanism risks premature query termination without sufficient validation
- Limited error analysis on LLM-generated follow-up questions and their impact on retrieval

## Confidence

**High Confidence**: The mechanism of using follow-up questions to guide retrieval is conceptually sound and represents a novel approach to multi-document QA.

**Medium Confidence**: Reported performance improvements are based on limited evaluations on specific datasets without extensive ablation studies or cross-dataset validation.

**Low Confidence**: Claims about hallucination reduction and computational efficiency improvements lack empirical support and direct measurements.

## Next Checks

1. **Error Analysis on Follow-up Questions**: Generate 100 follow-up questions using CuriousLLM on held-out data and have human annotators rate relevance and quality. Measure the correlation between follow-up question quality and downstream retrieval accuracy.

2. **Knowledge Graph Construction Validation**: Implement the MDR-KG methodology independently and measure retrieval accuracy using the same passage encoder and similarity functions. Compare knowledge graph connectivity and retrieval performance against reported metrics.

3. **Early Termination Robustness Testing**: Run CuriousLLM with early termination disabled on a subset of questions where it prematurely terminated. Measure the accuracy difference and analyze the types of questions where early termination succeeds versus fails.