---
ver: rpa2
title: Can AI Assistants Know What They Don't Know?
arxiv_id: '2401.13275'
source_url: https://arxiv.org/abs/2401.13275
tags:
- questions
- know
- answer
- dataset
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether AI assistants can recognize and communicate
  their knowledge limitations using natural language. The authors construct a model-specific
  "I don't know" (Idk) dataset by evaluating a model's confidence on questions from
  TriviaQA, labeling those it answers incorrectly as unknown with a refusal template.
---

# Can AI Assistants Know What They Don't Know?

## Quick Facts
- arXiv ID: 2401.13275
- Source URL: https://arxiv.org/abs/2401.13275
- Authors: Qinyuan Cheng; Tianxiang Sun; Xiangyang Liu; Wenwei Zhang; Zhangyue Yin; Shimin Li; Linyang Li; Zhengfu He; Kai Chen; Xipeng Qiu
- Reference count: 40
- Primary result: AI assistants can learn to recognize and communicate knowledge limitations, improving truthfulness from 52.9% to 74.75-80.55% with alignment methods

## Executive Summary
This paper investigates whether AI assistants can recognize and communicate their knowledge limitations using natural language. The authors construct model-specific "I don't know" (Idk) datasets by evaluating model confidence on questions from TriviaQA, labeling those it answers incorrectly as unknown with a refusal template. Through alignment techniques including supervised fine-tuning (Idk-SFT), prompting (Idk-Prompting), and preference-aware methods (DPO, PPO, HIR, BoN), they demonstrate that AI assistants can significantly improve their ability to truthfully refuse unknown questions while maintaining helpfulness. The results show that alignment methods increase truthfulness rates from 52.9% to 74.75-80.55%, with larger models performing better and preference optimization mitigating over-conservatism introduced by SFT.

## Method Summary
The authors construct model-specific Idk datasets by sampling multiple responses per question and labeling them as "known" or "unknown" based on accuracy thresholds. They then align Llama-2-7b-chat using supervised fine-tuning (Idk-SFT), prompting (Idk-Prompting), and preference-aware methods (DPO, PPO, HIR, BoN). The alignment methods are evaluated on knowledge quadrants (IK-IK, IK-IDK, KI-IK, KI-IDK) to measure truthfulness. The key innovation is creating datasets that reflect the actual knowledge distribution of each model rather than assuming universal knowledge boundaries, enabling more truthful refusal behavior without extensive retraining.

## Key Results
- Idk-SFT increases TRUTHFUL rate to 74.75% (from 52.9% baseline), but introduces over-conservatism reducing IK-IK rate
- DPO mitigates over-conservatism, achieving 77.89% TRUTHFUL rate while maintaining helpfulness
- Larger models (Llama-2-70b-chat) achieve higher performance at 80.55% TRUTHFUL rate
- Ik threshold strongly affects refusal behavior: higher thresholds increase IK-IDK but decrease IK-IK
- Idk-HIR offers threshold-controllable flexibility without retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model-specific "I don't know" datasets enable truthful refusal by grounding confidence thresholds in model behavior.
- Mechanism: Sampling multiple responses per question and comparing accuracy against an Ik threshold labels questions as "known" or "unknown" based on empirical model performance, not assumed knowledge.
- Core assumption: Model confidence in answering is measurable via repeated sampling and can be thresholded to define knowledge boundaries.
- Evidence anchors:
  - [abstract] "We construct a model-specific 'I don't know' (Idk) dataset for an assistant, which contains its known and unknown questions, based on existing open-domain question answering datasets."
  - [section 3.1] "We determine whether an assistant knows the answer to a question by evaluating its average accuracy across multiple responses to that question."

### Mechanism 2
- Claim: Supervised fine-tuning with Idk data improves refusal accuracy but introduces over-conservatism.
- Mechanism: SFT learns to reproduce refusal templates for labeled "unknown" questions and correct answers for "known" ones, shifting the model's response distribution toward higher truthfulness.
- Core assumption: SFT on labeled data can realign model behavior to match human-defined knowledge boundaries.
- Evidence anchors:
  - [section 3.3] "We directly use the Idk dataset for Supervised Fine-tuning of the model."
  - [section 4.3] "Idk-SFT can increase the TRUTHFUL rate to 74.75%, but this will result in a decrease in the IK-IK rate, which can be considered a form of 'alignment tax'."

### Mechanism 3
- Claim: Preference-aware optimization mitigates SFT over-conservatism by balancing refusal and answering behavior.
- Mechanism: DPO, PPO, and HIR use preference pairs to reward truthful refusals without overly penalizing attempts to answer known questions, restoring IK-IK rates.
- Core assumption: Reward signals can fine-tune the balance between truthfulness and helpfulness without retraining from scratch.
- Evidence anchors:
  - [section 3.4] "DPO, PPO, and BoN can all reduce the loss of IK-IK while maintaining a relatively high IK-IDK rate."
  - [section 4.3] "DPO, PPO, and BoN can all reduce the loss of IK-IK while maintaining a relatively high IK-IDK rate."

## Foundational Learning

- Concept: Confidence threshold calibration
  - Why needed here: Determines which questions the model considers "known" vs "unknown" for dataset labeling and alignment.
  - Quick check question: If Ik threshold is set to 0.9, how does that affect the number of questions labeled as "I don't know" compared to 0.5?

- Concept: Preference optimization
  - Why needed here: Balances truthful refusal against over-conservatism introduced by SFT.
  - Quick check question: What loss component is added to DPO to prevent incomplete refusal template generation?

- Concept: Model-specific dataset construction
  - Why needed here: Ensures alignment targets match the actual knowledge distribution of the model being aligned.
  - Quick check question: Why might using an Idk dataset from a different model (e.g., Mistral-7B) degrade performance on Llama-2-7b-chat?

## Architecture Onboarding

- Component map: Base model → Idk dataset construction → Alignment method (SFT/DPO/PPO/HIR) → Evaluation (knowledge quadrants) → Output
- Critical path: Construct model-specific Idk dataset → Train alignment method → Evaluate truthfulness via knowledge quadrant metrics
- Design tradeoffs: Higher Ik threshold → more truthful but less helpful; preference optimization → more helpful but riskier; model size → better distinction but more resource intensive
- Failure signatures: Excessive IK-IDK at high Ik threshold; low TRUTHFUL rate after SFT; refusal template generation errors in DPO
- First 3 experiments:
  1. Vary Ik threshold from 0.5 to 1.0 on Llama-2-7b-chat and measure TRUTHFUL rate change.
  2. Compare SFT vs DPO on same Idk dataset to quantify over-conservatism mitigation.
  3. Test OOD generalization by aligning on TriviaQA and evaluating on Natural Questions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Idk-SFT and Idk-DPO models generalize to other knowledge-intensive QA datasets beyond TriviaQA, Natural Questions, and ALCUNA?
- Basis in paper: [explicit] The paper reports results on three datasets (TriviaQA, Natural Questions, ALCUNA) but acknowledges these may not cover the full diversity of knowledge-intensive QA tasks.
- Why unresolved: The paper does not test on a wider variety of knowledge-intensive QA datasets, limiting understanding of how well the aligned models generalize to different domains or question types.
- What evidence would resolve it: Testing Idk-SFT and Idk-DPO models on a diverse set of knowledge-intensive QA datasets (e.g., HotpotQA, SQuAD, ELI5) and comparing their performance to baseline models.

### Open Question 2
- Question: What is the impact of different Ik threshold values on the helpfulness of the model, and is there an optimal balance between truthfulness and helpfulness?
- Basis in paper: [explicit] The paper discusses how higher Ik thresholds increase truthfulness but decrease helpfulness, while lower Ik thresholds have the opposite effect. However, it does not explore the optimal balance between these two factors.
- Why unresolved: The paper does not provide a comprehensive analysis of the trade-off between truthfulness and helpfulness at different Ik threshold values.
- What evidence would resolve it: Conducting a user study to evaluate the helpfulness of models aligned with different Ik thresholds, and identifying the threshold that maximizes both truthfulness and helpfulness.

### Open Question 3
- Question: How does the performance of Idk-HIR compare to Idk-SFT and Idk-DPO when the Ik threshold is not known in advance or varies across different contexts?
- Basis in paper: [inferred] The paper highlights that Idk-HIR offers a flexible, threshold-controllable alternative without retraining, but does not directly compare its performance to Idk-SFT and Idk-DPO in scenarios with unknown or varying Ik thresholds.
- Why unresolved: The paper does not provide empirical evidence on how Idk-HIR performs compared to Idk-SFT and Idk-DPO when the Ik threshold is not fixed or known beforehand.
- What evidence would resolve it: Conducting experiments where the Ik threshold is varied across different contexts or is unknown, and comparing the performance of Idk-HIR, Idk-SFT, and Idk-DPO models in terms of truthfulness, helpfulness, and robustness.

## Limitations

- The threshold-based labeling approach assumes stable model confidence distributions, which may not hold for questions requiring multi-step reasoning or involving sensitive topics.
- The evaluation methodology using lexical matching for answer correctness may not capture semantic equivalence, potentially underestimating model performance.
- Preference optimization methods show promise but their relative effectiveness is unclear, with the paper reporting similar performance without identifying the most efficient approach.

## Confidence

**High Confidence**: The core finding that supervised fine-tuning with model-specific Idk datasets improves truthfulness rates (74.75% for Idk-SFT) is well-supported by controlled experiments and clear metrics.

**Medium Confidence**: The claim that larger models (Llama-2-70b-chat) inherently perform better at distinguishing known from unknown questions (80.55% TRUTHFUL) is supported but could be influenced by the specific knowledge distribution in TriviaQA.

**Low Confidence**: The generalization results to out-of-distribution datasets (Natural Questions, ALCUNA) are suggestive but limited, with performance drops not fully explored in terms of whether they reflect true knowledge limitations or dataset-specific factors.

## Next Checks

1. **Threshold Stability Analysis**: Systematically vary the Ik threshold (0.5, 0.6, 0.7, 0.8, 0.9) on the same model and dataset, then evaluate the trade-off between IK-IK and IK-IDK rates.

2. **Cross-Model Dataset Transfer**: Train a model (e.g., Llama-2-7b-chat) using an Idk dataset constructed from a different model (e.g., Mistral-7B), then compare performance against using its own model-specific dataset.

3. **Preference Method Comparison**: Implement all four preference optimization methods (DPO, PPO, BoN, HIR) with identical hyperparameters and computational budgets, then conduct ablation studies to identify which components most contribute to mitigating over-conservatism while maintaining truthfulness.