---
ver: rpa2
title: Model-free reinforcement learning with noisy actions for automated experimental
  control in optics
arxiv_id: '2405.15421'
source_url: https://arxiv.org/abs/2405.15421
tags:
- learning
- training
- goal
- power
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates the use of reinforcement learning to automate
  the challenging task of coupling laser light into optical fibers. The authors address
  the difficulty of aligning optical systems with many degrees of freedom, especially
  when the system is noisy and non-linear.
---

# Model-free reinforcement learning with noisy actions for automated experimental control in optics

## Quick Facts
- arXiv ID: 2405.15421
- Source URL: https://arxiv.org/abs/2405.15421
- Reference count: 40
- One-line primary result: Reinforcement learning agents achieve 90% fiber coupling efficiency, comparable to human experts but with faster execution, by training directly on noisy real-world optical setups.

## Executive Summary
This study demonstrates the use of reinforcement learning to automate the challenging task of coupling laser light into optical fibers. The authors address the difficulty of aligning optical systems with many degrees of freedom, especially when the system is noisy and non-linear. Using model-free reinforcement learning algorithms (Soft Actor-Critic, Truncated Quantile Critics, and CrossQ), agents were trained directly on the experiment without simulation pre-training. The agents learned to achieve 90% coupling efficiency, comparable to human experts but with faster execution. The CrossQ agent showed the best performance, requiring half the training time and coupling more quickly than other algorithms. This work highlights reinforcement learning's potential for real-world optical control, especially in complex scenarios where accurate system modeling is infeasible.

## Method Summary
The authors developed a reinforcement learning approach for automating fiber coupling of laser light into optical fibers. They used model-free RL algorithms (Soft Actor-Critic, Truncated Quantile Critics, and CrossQ) trained directly on the physical experiment without pre-training on simulations. The observation space included normalized power, history of recent powers and actions, and statistics of power during actions. The reward function was shaped to encourage reaching the goal power quickly while penalizing failure. Training was conducted on a 4-degree-of-freedom optical setup with motorized mirrors, starting from lower goal powers and gradually increasing to 85-90% efficiency. The CrossQ algorithm achieved the best performance with half the training time of TQC and faster coupling speeds.

## Key Results
- RL agents achieved 90% coupling efficiency, comparable to human experts
- CrossQ algorithm halved training time compared to TQC while achieving faster coupling speeds
- Direct on-experiment training without simulation pre-training successfully handled noisy, non-linear optical systems
- Agents generalized across experimental alignment changes by using relative position observations instead of absolute motor positions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct on-experiment training can bypass the need for accurate simulation models in noisy, non-linear optical systems.
- Mechanism: Model-free RL algorithms learn control policies directly from real-world feedback without relying on pre-defined system models. The agent updates its policy through repeated trials on the actual hardware, adapting to the inherent noise and imprecision of the actuators (e.g., stepper motor backlash and step loss). By training in-situ, the policy implicitly encodes the real system dynamics, including unmodeled stochasticities.
- Core assumption: The experimental environment provides sufficient feedback (power measurements) to guide learning, and the sample-efficient RL algorithms (SAC, TQC, CrossQ) can learn from limited interactions despite noisy actions.
- Evidence anchors:
  - [abstract] "using a model-free RL approach that trains directly on the experiment without pre-training on simulations"
  - [section 4] "Our strategy is that the agent learns to deal with the noisy actions directly in the experiment... in our area, we are often dealing with noise sources that cannot be modeled accurately"
  - [corpus] No direct corpus evidence found for this specific claim about bypassing simulation; stated as assumption.

### Mechanism 2
- Claim: CrossQ algorithm achieves faster training and better performance in environments with noisy actions compared to SAC and TQC.
- Mechanism: CrossQ incorporates batch normalization techniques into the RL learning process, which helps stabilize training and improve sample efficiency, especially in environments with high action noise. This allows the agent to learn effective policies more quickly and achieve higher coupling efficiency with fewer training steps. The paper shows CrossQ halving the training time compared to TQC and achieving faster coupling speeds.
- Core assumption: The batch normalization adaptation in CrossQ is particularly beneficial for handling the stochasticity introduced by the motor imprecision in the optical fiber coupling task.
- Evidence anchors:
  - [abstract] "the CrossQ agent outperforms the other agents in coupling speed while requiring only half the training time"
  - [section 5] "The use of CrossQ has a clear advantage. It halves the training time with respect to TQC. Additionally, the trained agent reaches the goal faster than the agents trained with the other two algorithms."
  - [corpus] No direct corpus evidence found for CrossQ's performance on noisy actions; stated as assumption based on paper results.

### Mechanism 3
- Claim: Partial observability in the fiber coupling task can be effectively managed through careful observation design, including action history and power statistics.
- Mechanism: Instead of using absolute actuator positions (which are unreliable due to motor imprecision and would not generalize to setup drift), the agent observes the normalized power output, a history of recent powers and actions, and statistics of power during actions (average, max, relative position). This observation design provides sufficient information for the agent to infer the system state and learn effective control policies despite the underdetermined nature of the problem (4 actuator positions mapping to 1 power output).
- Core assumption: The power signal, combined with action history and statistics, contains enough information to disambiguate the system state and guide learning towards high coupling efficiency.
- Evidence anchors:
  - [section 4] "As we are dealing with a partially observable system, for successful training, we need to take great care in defining our observations... we include a history of length n... the performed actions as part of the observation"
  - [section 4] "Using the virtual testbed, we find that history lengths of approximately n = 4 are optimal when using TQC"
  - [corpus] No direct corpus evidence found for this specific observation design; stated as assumption based on paper results.

## Foundational Learning

- Concept: Reinforcement Learning (RL) with continuous action spaces and model-free algorithms.
  - Why needed here: The fiber coupling task requires continuous control of four mirror actuators to optimize a scalar reward (power). Model-free RL is chosen because accurate modeling of the optical system's noise and non-linearities is infeasible.
  - Quick check question: What is the main difference between model-free and model-based RL, and why is model-free preferred in this noisy optical setup?

- Concept: Partial Observability and POMDPs.
  - Why needed here: The system state (exact mirror positions, beam profile) is not fully observable; only the output power is measured. The agent must learn from incomplete information.
  - Quick check question: How does including a history of observations and actions help mitigate the challenges of partial observability in this task?

- Concept: Curriculum Learning.
  - Why needed here: Training directly to a high goal power (90%) is difficult. Starting with lower goal powers and gradually increasing them helps the agent learn more effectively.
  - Quick check question: Why might pre-training on lower goal powers improve the agent's ability to reach higher efficiencies later?

## Architecture Onboarding

- Component map:
  Optical Setup (1064 nm laser, polarization-maintaining fiber, motorized mirrors, power meter) -> RL Environment (Gymnasium wrapper) -> RL Agent (StableBaselines3, algorithms: CrossQ, TQC, SAC) -> Motor Controllers (Stepper motor interface)

- Critical path:
  1. Initialize optical setup and ensure power measurement is working.
  2. Start RL training loop: agent selects action (motor steps), environment applies action, measures power, computes reward, updates agent.
  3. Monitor training progress via normalized return.
  4. Test trained agent on the physical setup for fiber coupling efficiency.

- Design tradeoffs:
  - Observation Design: Including absolute motor positions would simplify learning but prevent generalization to setup drift; using only power and action history improves robustness but requires more sophisticated learning.
  - Reward Shaping: Balancing the reward for reaching the goal quickly vs. the penalty for failure; shaping affects learning speed and stability.
  - Reset Procedure: Automatic reset using motors is faster but less reliable due to imprecision; manual reset is slower but more robust to misalignment.

- Failure signatures:
  - Training stalls or diverges: Check observation design, reward function, and algorithm hyperparameters.
  - Agent fails to reach goal: Verify power measurement accuracy, check for mechanical issues (e.g., motor backlash), consider adjusting goal power or using curriculum learning.
  - Long training times: Evaluate sample efficiency of algorithm, consider using CrossQ or pre-training on virtual testbed.

- First 3 experiments:
  1. Set up the optical fiber coupling experiment with motorized mirrors and power meter. Verify power measurement and basic motor control.
  2. Implement a simple RL environment with one degree of freedom (e.g., one mirror axis) to test the observation design and reward function on a simpler version of the problem.
  3. Train a basic RL agent (e.g., SAC) on the full 4-degree-of-freedom problem with a low goal power (e.g., 80%) to validate the complete pipeline before attempting higher efficiencies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the CrossQ algorithm achieve faster training and better coupling speed compared to SAC and TQC in the presence of noisy actions?
- Basis in paper: [explicit] The paper states that CrossQ outperforms other agents in coupling speed while requiring only half the training time compared to TQC, but does not provide a detailed analysis of why this occurs.
- Why unresolved: The paper demonstrates superior performance but does not analyze the specific mechanisms or architectural differences in CrossQ that make it more robust to noisy actions.
- What evidence would resolve it: A detailed ablation study comparing CrossQ's architecture (e.g., batch normalization, critic network design) with SAC and TQC, along with controlled experiments isolating the effect of action noise on each algorithm's performance.

### Open Question 2
- Question: Can the RL agents generalize to different fiber coupling scenarios beyond the specific experimental setup used in this study?
- Basis in paper: [inferred] The paper mentions that the agent is trained without absolute motor positions in the observation to make it robust against experimental alignment changes, but does not test generalization to substantially different setups.
- Why unresolved: The study focuses on one specific fiber coupling setup and does not explore whether the learned policies transfer to other optical configurations or different types of fibers.
- What evidence would resolve it: Experiments testing the trained agents on different fiber types, alternative optical configurations, or setups with different numbers of degrees of freedom, measuring coupling efficiency and training requirements.

### Open Question 3
- Question: What is the minimum amount of training data required for successful fiber coupling when using model-free RL algorithms in noisy real-world environments?
- Basis in paper: [explicit] The paper shows that agents can learn to couple with 90% efficiency in 2 days using CrossQ or SAC, and nearly 4 days with TQC, but does not systematically explore the data efficiency limits.
- Why unresolved: The study uses fixed training durations based on convergence observations but does not investigate the relationship between training data quantity and performance, particularly for lower efficiency targets.
- What evidence would resolve it: A systematic study varying the number of training steps and measuring the resulting coupling efficiency, identifying the minimum training data required to achieve specific performance thresholds.

## Limitations

- The study focuses on a specific 4-degree-of-freedom optical setup and does not explore scalability to more complex systems with significantly more degrees of freedom.
- The theoretical basis for CrossQ's improved performance with noisy actions is not deeply explored, relying primarily on empirical demonstration.
- Generalization of the trained agents to different optical configurations, fiber types, or wavelengths beyond the specific experimental setup is not tested.

## Confidence

- Core claim (RL can automate optical fiber coupling in noisy, non-linear systems): High confidence
- Specific mechanisms of CrossQ's improved performance: Medium confidence
- Scalability to more complex optical systems: Low confidence

## Next Checks

1. **Robustness to setup drift**: Test whether the trained agents maintain performance when the optical setup experiences thermal drift or mechanical misalignment between training and testing sessions.

2. **Generalization across wavelengths**: Evaluate whether agents trained at 1064 nm can successfully couple light at different wavelengths (e.g., 800 nm or 1550 nm) without retraining.

3. **Comparison with alternative approaches**: Benchmark the RL approach against traditional hill-climbing algorithms or model-based optimization methods on the same hardware to quantify the practical advantage of the RL solution.