---
ver: rpa2
title: Measuring Feature Dependency of Neural Networks by Collapsing Feature Dimensions
  in the Data Manifold
arxiv_id: '2404.12341'
source_url: https://arxiv.org/abs/2404.12341
tags:
- feature
- data
- dataset
- features
- manifold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to measure feature dependency in neural
  networks by "removing" interpretable features from test data and observing performance
  changes. The method moves data points along feature gradient directions while staying
  on a learned data manifold (using a VAE) to collapse feature values to a baseline.
---

# Measuring Feature Dependency of Neural Networks by Collapsing Feature Dimensions in the Data Manifold

## Quick Facts
- arXiv ID: 2404.12341
- Source URL: https://arxiv.org/abs/2404.12341
- Authors: Yinzhu Jin; Matthew B. Dwyer; P. Thomas Fletcher
- Reference count: 0
- Primary result: Successfully identified key features like ellipse aspect ratio and hippocampus volume as critical for classifier performance, with accuracy dropping near random chance when these features were collapsed

## Executive Summary
This paper introduces a novel method for measuring feature dependency in neural networks by systematically "removing" interpretable features from test data and observing the resulting performance changes. The approach moves data points along feature gradient directions while constraining them to a learned data manifold (using a VAE), collapsing feature values to a baseline. Tested on synthetic ellipse images, Alzheimer's disease classification from MRI, and cell nucleus classification, the method successfully identified critical features and outperformed gradient alignment and conditional VAE-based approaches in providing interpretable results.

## Method Summary
The method measures feature dependency by quantifying performance degradation when interpretable features are collapsed to baseline values. It first trains a VAE to learn the data manifold, then for each test sample, integrates the feature gradient in the latent space to move the sample along the feature dimension until the feature reaches the baseline value. The classifier is then tested on the modified dataset, and accuracy drops indicate feature dependency. This approach addresses the challenge of feature manipulation in high-dimensional spaces by ensuring modifications stay on the learned data manifold, producing realistic samples while preserving other characterizing features.

## Key Results
- Successfully identified ellipse aspect ratio as the primary feature for synthetic ellipse classification, with accuracy dropping from ~0.85 to ~0.5 (random chance) when collapsed
- For Alzheimer's disease classification, hippocampus volume was identified as critical, with accuracy dropping to ~0.53 when collapsed
- Outperformed conditional VAE approaches and provided more interpretable results than gradient alignment methods
- Demonstrated the ability to detect when classifiers ignore theoretically important features (e.g., brain ventricle volume in AD classification)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature dependency can be measured by quantifying performance drop when a feature is collapsed to a baseline value
- Mechanism: The method assumes that if a neural network depends on a feature, removing that feature will harm its performance. By collapsing feature dimensions to a baseline value and observing accuracy changes, the method quantifies dependency
- Core assumption: Neural network performance is directly correlated with the presence of interpretable features it uses for classification
- Evidence anchors:
  - [abstract] "Our method is based on the principle that if a model is dependent on a feature, then removal of that feature should significantly harm its performance."
  - [section 2] "The dependency of g on the feature f is reflected by how much the performance drops."
  - [corpus] Weak evidence - no direct citations found in corpus
- Break condition: If the model uses redundant features or the collapsed feature can be approximated by other features, the performance drop may not reflect true dependency

### Mechanism 2
- Claim: VAE-based manifold constraint ensures collapsed data remains realistic while preserving other features
- Mechanism: Instead of directly manipulating images in pixel space (which can produce unrealistic images), the method uses a VAE to move data points along the feature gradient direction within the learned data manifold. This ensures that while the target feature is collapsed, other characterizing features of the data are preserved
- Core assumption: The VAE accurately captures the data distribution and can generate realistic samples that preserve feature relationships
- Evidence anchors:
  - [abstract] "We perform this by moving data points along the feature dimension to a baseline feature value while staying on the data manifold, as estimated by a deep generative model."
  - [section 2] "To handle this, we propose to first train a deep generative model to learn the data distribution and then restrict movements along our feature gradient to remain on the estimated data manifold."
  - [corpus] Weak evidence - no direct citations found in corpus
- Break condition: If the VAE fails to learn the true data manifold or has poor reconstruction quality, collapsed samples may still be unrealistic

### Mechanism 3
- Claim: Feature gradient integration in latent space enables controlled feature manipulation
- Mechanism: The method integrates the feature gradient in the latent space of the VAE rather than in the original data space. This allows precise control over feature values while staying on the data manifold. The integration stops when the feature reaches the baseline value
- Core assumption: The feature gradient can be meaningfully computed and integrated in the latent space to achieve desired feature values in the output space
- Evidence anchors:
  - [section 2] "The feature f restricted to the VAE manifold is given by f ◦ ϕ, and the integral curve of the gradient is now dc/dt(t) = ∇z(f ◦ ϕ(c(t))) = D ϕ(c(t))T ∇xf (ϕ(c(t)))"
  - [section 3] "In practice, we perform the gradient vector field integration with a discrete Euler integration step"
  - [corpus] Weak evidence - no direct citations found in corpus
- Break condition: If the feature gradient is not well-behaved or the integration step size is inappropriate, the method may fail to reach the desired baseline value

## Foundational Learning

- Concept: Variational Autoencoders (VAEs) and their role in learning data manifolds
  - Why needed here: The method relies on VAEs to learn the data distribution and provide a latent space where feature manipulation can occur while staying on the manifold
  - Quick check question: What is the purpose of the encoder and decoder in a VAE, and how do they relate to the data manifold?

- Concept: Feature gradients and their use in optimization
  - Why needed here: The method computes and integrates feature gradients to move data points along feature dimensions while staying on the manifold
  - Quick check question: How does the chain rule apply when computing feature gradients in the latent space of a VAE?

- Concept: Manifold learning and dimensionality reduction
  - Why needed here: The method assumes data lies on a lower-dimensional manifold in the original space, which is why direct manipulation in pixel space fails
  - Quick check question: Why might data in high-dimensional spaces like images lie on a lower-dimensional manifold?

## Architecture Onboarding

- Component map:
  - Data preprocessing pipeline (image loading, segmentation, normalization) -> Classifier neural network (trained task-specific model) -> VAE model (for learning data manifold) -> Feature computation module (for extracting interpretable features) -> Feature collapse algorithm (core method implementation) -> Evaluation framework (accuracy measurement, comparison with baselines)

- Critical path:
  1. Train classifier on original dataset
  2. Train VAE on same dataset
  3. For each test sample, encode to latent space
  4. Compute feature gradients in latent space
  5. Integrate gradients to reach baseline feature value
  6. Decode back to image space
  7. Test classifier on modified dataset
  8. Compare performance with original

- Design tradeoffs:
  - VAE quality vs. computational cost: Higher-quality VAEs provide better manifold estimates but require more training data and computation
  - Integration step size: Smaller steps provide more accurate integration but increase computation time
  - Feature selection: Choosing interpretable features that are actually used by the classifier vs. those that are theoretically important

- Failure signatures:
  - Poor reconstruction quality from VAE indicates inadequate manifold learning
  - Oscillations during gradient integration suggest inappropriate step size or ill-conditioned gradients
  - Minimal performance drop when collapsing known important features suggests the classifier doesn't use that feature
  - Unrealistic-looking collapsed images indicate the method has moved off the manifold

- First 3 experiments:
  1. Verify VAE reconstruction quality on held-out data and visualize latent space structure
  2. Test feature collapse on a simple synthetic dataset where ground truth feature importance is known
  3. Compare accuracy after collapsing random features vs. known important features on a real dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the method perform on high-dimensional data where the data manifold is extremely complex or noisy?
- Basis in paper: [inferred] The paper mentions requiring "large sample size for VAE training" and shows results on relatively low-dimensional imaging data (hippocampi, cell nuclei)
- Why unresolved: The current experiments use datasets with limited spatial resolution and dimensionality. No tests were performed on higher-dimensional or noisier data to validate robustness
- What evidence would resolve it: Testing the method on high-resolution medical imaging data (e.g., full 3D brain scans) or other high-dimensional domains like genomics, comparing VAE reconstruction quality and feature collapse performance across different noise levels

### Open Question 2
- Question: What is the theoretical relationship between the accuracy after collapse (AAC) metric and established measures of feature importance like Shapley values or integrated gradients?
- Basis in paper: [explicit] The paper introduces AAC as a novel metric but only compares it to CaCE scores, not to established attribution methods
- Why unresolved: The paper establishes AAC as a measure of feature dependency but doesn't situate it within the broader literature of feature importance quantification
- What evidence would resolve it: Mathematical analysis or empirical comparison showing correlation or divergence between AAC scores and Shapley values/integrated gradients on benchmark datasets where ground truth feature importance is known

### Open Question 3
- Question: How sensitive is the method to the choice of baseline feature value (b) used in Algorithm 1?
- Basis in paper: [explicit] The algorithm requires choosing a baseline value b, but the paper only uses the dataset mean in experiments
- Why unresolved: The paper demonstrates the method works with mean values but doesn't explore sensitivity to different baseline choices or provide guidance on selecting optimal baselines
- What evidence would resolve it: Systematic experiments varying baseline values across the feature distribution, analyzing how different choices affect AAC scores and feature dependency conclusions

### Open Question 4
- Question: Can the method be extended to handle discrete or categorical features without requiring conditional VAE generation?
- Basis in paper: [explicit] The paper notes CaCE "was originally proposed for discrete concepts" and extends it, but their method relies on continuous gradient-based feature manipulation
- Why unresolved: The current approach requires feature gradients and continuous manipulation, making it inapplicable to discrete features without the conditional VAE workaround
- What evidence would resolve it: Development of a variant of the feature collapse method that can handle discrete features through alternative manifold manipulation techniques, validated on datasets with mixed feature types

## Limitations
- The method requires substantial VAE training data to learn accurate data manifolds, which may not be available for all datasets
- Feature gradient computation assumes differentiability and may be unstable for discrete or categorical features
- The approach cannot detect feature dependencies when multiple features are used redundantly by the classifier

## Confidence

- **High confidence**: The core mechanism of measuring feature dependency through performance degradation (Mechanism 1) is well-supported by the empirical results showing accuracy drops near random chance when key features are collapsed
- **Medium confidence**: The VAE-based manifold constraint approach (Mechanism 2) is validated on the tested datasets but may face challenges with more complex data distributions
- **Low confidence**: The feature gradient integration in latent space (Mechanism 3) lacks detailed error analysis for cases where gradients are ill-conditioned or the integration fails to reach the baseline value

## Next Checks
1. **Gradient field stability test**: Systematically evaluate how the feature gradient behaves across the latent space for each dataset, identifying regions where gradients are poorly defined or unstable
2. **Ablation study on VAE capacity**: Test the method with VAEs of varying complexity and training data sizes to quantify the impact of manifold quality on feature dependency measurements
3. **Redundancy detection experiment**: Design a synthetic classifier that uses redundant features and verify whether the method can detect this redundancy or if it underestimates feature dependency