---
ver: rpa2
title: Large Language Models as Zero-shot Dialogue State Tracker through Function
  Calling
arxiv_id: '2402.10466'
source_url: https://arxiv.org/abs/2402.10466
tags:
- function
- dialogue
- arxiv
- chatgpt
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FnCTOD, a novel approach for zero-shot dialogue
  state tracking (DST) using large language models (LLMs) through function calling.
  The core idea is to treat each domain in task-oriented dialogues as a unique function,
  with slot values as its arguments, enabling seamless integration of DST into the
  chat completion process.
---

# Large Language Models as Zero-shot Dialogue State Tracker through Function Calling

## Quick Facts
- arXiv ID: 2402.10466
- Source URL: https://arxiv.org/abs/2402.10466
- Authors: Zekun Li; Zhiyu Zoey Chen; Mike Ross; Patrick Huber; Seungwhan Moon; Zhaojiang Lin; Xin Luna Dong; Adithya Sagar; Xifeng Yan; Paul A. Crook
- Reference count: 17
- One-line primary result: FnCTOD achieves state-of-the-art zero-shot DST performance, improving GPT-4's performance by 14% and enabling 7B/13B models to outperform ChatGPT

## Executive Summary
This paper introduces FnCTOD, a novel approach for zero-shot dialogue state tracking (DST) using large language models through function calling. The method treats each domain in task-oriented dialogues as a unique function, with slot values as arguments, seamlessly integrating DST into the chat completion process. FnCTOD demonstrates exceptional performance with both open-source and proprietary LLMs, achieving state-of-the-art results and enabling smaller models to outperform larger ones in zero-shot settings.

## Method Summary
FnCTOD converts domain schemas into function specifications using JSON format and integrates them into the system prompt. The approach employs a two-step function call decomposition: first selecting the appropriate domain, then generating slot-value arguments. This method works in zero-shot settings with in-context examples and can be extended through fine-tuning on diverse task-oriented dialogues. The fine-tuned LLaMA2-13B model achieves performance comparable to ChatGPT while being significantly smaller.

## Key Results
- FnCTOD achieves state-of-the-art zero-shot DST performance on MultiWOZ 2.1 benchmark
- Improves GPT-4's performance by 14% in zero-shot DST settings
- Enables 7B and 13B parameter models to outperform ChatGPT in zero-shot DST
- Fine-tuned LLaMA2-13B achieves DST performance comparable to ChatGPT

## Why This Works (Mechanism)

### Mechanism 1
Function calling aligns with LLM chat-tuned response generation strengths by embedding function calls as part of the assistant's response output, leveraging existing conversational capabilities rather than treating DST as a standalone task.

### Mechanism 2
Two-step function call decomposition reduces prompt complexity and improves accuracy by separating function selection from argument generation, reducing cognitive load on the model by only presenting relevant function specifications at each step.

### Mechanism 3
Fine-tuning on diverse task-oriented dialogues transfers function-calling capabilities to open-source models by training on 7,200 dialogues across 36 diverse domains, enabling modestly-sized models to acquire function-calling abilities comparable to ChatGPT.

## Foundational Learning

- Concept: Function calling in LLMs
  - Why needed here: Understanding how LLMs interpret and execute function calls is crucial for implementing DST through this paradigm
  - Quick check question: How does an LLM distinguish between a function call and regular text in its output?

- Concept: Dialogue state tracking fundamentals
  - Why needed here: DST requires tracking slot-value pairs across conversation turns, which must be mapped to function arguments
  - Quick check question: What is the difference between slot-value pairs and function arguments in the context of DST?

- Concept: Prompt engineering and in-context learning
  - Why needed here: Effective zero-shot performance depends on crafting prompts that guide the model to generate correct function calls
  - Quick check question: How does the number and quality of in-context examples affect model performance in zero-shot settings?

## Architecture Onboarding

- Component map: System prompt -> Dialogue context -> Function specifications -> Model -> Knowledge base/API

- Critical path: 1. Convert domain schema to function specifications 2. Construct system prompt with specifications 3. Process conversation turn-by-turn 4. Generate function call (two-step process) 5. Query backend using function call arguments 6. Generate response based on query results

- Design tradeoffs:
  - Prompt length vs. coverage: Including all function specifications increases prompt size but provides complete information
  - Two-step vs. single-step generation: Two-step reduces prompt complexity but may introduce cascading errors
  - Fine-tuning data size vs. performance: More diverse training data improves generalization but increases computational cost

- Failure signatures:
  - Incorrect function names or missing arguments in generated calls
  - Model generating function calls in wrong format (not JSON)
  - Performance degradation when switching between domains
  - Inability to handle unseen slot values or domains

- First 3 experiments:
  1. Test function call generation with a single domain and conversation turn
  2. Evaluate two-step decomposition vs. single-step generation on a small dataset
  3. Compare zero-shot performance with and without in-context examples across multiple domains

## Open Questions the Paper Calls Out

### Open Question 1
How does the function calling approach for DST scale to datasets with a significantly larger number of domains and slots compared to MultiWOZ? The approach of including all function specifications in the system prompt could become impractical with a large number of domains and slots due to context window limitations.

### Open Question 2
How robust is the function calling approach to variations in slot value formats or entity linking errors? The paper doesn't discuss handling variations in slot value formats or entity linking errors, which could be significant issues in real-world scenarios with noisy user inputs.

### Open Question 3
How does the performance of the fine-tuned FnCTOD-LLaMA2-13B model compare to a model trained end-to-end on the MultiWOZ dataset? The paper doesn't provide a direct comparison to an end-to-end trained model on the target dataset, which would provide context for the effectiveness of the zero-shot/few-shot approach.

## Limitations
- Function specification details and exact JSON format for all domains are not provided
- Two-step decomposition implementation details are unclear
- Fine-tuning dataset composition and diversity characteristics remain unspecified

## Confidence

**High Confidence Claims**:
- Function calling can be effectively used for DST through the two-step decomposition approach
- The method achieves state-of-the-art performance on MultiWOZ 2.1 benchmark
- Fine-tuning modestly sized models on diverse dialogues transfers function-calling capabilities

**Medium Confidence Claims**:
- The 14% improvement over GPT-4 is consistent across different evaluation settings
- The two-step decomposition is more efficient than single-step generation
- The approach generalizes well to unseen domains

**Low Confidence Claims**:
- Exact performance numbers for the fine-tuned LLaMA2-13B model on all domains
- Scalability to domains with hundreds of slots
- Performance consistency across different LLM providers

## Next Checks

1. **Function Specification Validation**: Create a minimal test suite with known dialogue-state pairs and verify that the function calling mechanism correctly generates slot-value pairs for at least 3 different domain types (hotel, restaurant, and train) using the described two-step approach.

2. **Two-Step Decomposition Effectiveness**: Compare the performance of single-step vs. two-step function call generation on a small subset of MultiWOZ dialogues, measuring both accuracy and computational efficiency (API token usage and latency).

3. **Fine-tuning Transferability**: Evaluate the fine-tuned LLaMA2-13B model on a held-out domain from the training data to verify that function-calling capabilities generalize beyond the training domains, measuring both JGA and success rate.