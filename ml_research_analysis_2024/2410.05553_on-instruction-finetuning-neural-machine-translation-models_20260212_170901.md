---
ver: rpa2
title: On Instruction-Finetuning Neural Machine Translation Models
arxiv_id: '2410.05553'
source_url: https://arxiv.org/abs/2410.05553
tags:
- instruction
- translation
- tasks
- task
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces instruction finetuning for neural machine
  translation (NMT) models, enabling them to follow natural language instructions
  for translation customization tasks. The authors develop a recipe that expands the
  NMT model vocabulary with instruction tokens, curates parallel and task-specific
  datasets, and finetunes the model on a mix of both.
---

# On Instruction-Finetuning Neural Machine Translation Models

## Quick Facts
- arXiv ID: 2410.05553
- Source URL: https://arxiv.org/abs/2410.05553
- Authors: Vikas Raunak; Roman Grundkiewicz; Marcin Junczys-Dowmunt
- Reference count: 13
- Primary result: Instruction-finetuned NMT models can follow 30+ instructions simultaneously with 89.60% response rate while maintaining general translation quality (+0.3 ChrF2)

## Executive Summary
This paper introduces instruction finetuning for neural machine translation models, enabling them to follow natural language instructions for translation customization tasks. The authors develop a recipe that expands the NMT model vocabulary with instruction tokens, curates parallel and task-specific datasets, and finetunes the model on a mix of both. They demonstrate that instruction-finetuned NMT models can simultaneously follow 30+ disparate instructions and show zero-shot composition capabilities. The instruction-finetuned NMT model achieves comparable performance to GPT-3.5-Turbo on the IWSLT-22 Formality Control Shared Task while being substantially cheaper to serve.

## Method Summary
The instruction finetuning recipe involves three main steps: (1) expanding the NMT model vocabulary with instruction tokens to cleanly separate instructions from source text, (2) curating parallel datasets (WMT'20 News Translation English-German) and task-specific datasets (30+ tasks with 1K samples each, plus multi-30K multimodal dataset with 29K training samples), and (3) finetuning the model on a mix of parallel and task data using a 2:1 ratio for 3 epochs. The method preserves general translation quality while learning to follow instructions, with optional interpolation to further balance performance.

## Key Results
- NMT models can follow multiple instructions simultaneously with an average instruction response rate of 89.60%
- Zero-shot composition of instructions works across different task combinations
- The model outperforms GPT-3.5-Turbo on formality control tasks while being substantially cheaper
- General translation quality remains stable (+0.3 ChrF2) after instruction finetuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction finetuning allows NMT models to follow natural language instructions by treating them as a conditioning signal similar to domain tags.
- Mechanism: The model learns to condition its output on both the source text and the instruction tokens prepended to it. By expanding the vocabulary with instruction tokens and training on paired instruction-source-target triplets, the model learns a mapping from instructions to target behaviors.
- Core assumption: The NMT model can generalize from seeing individual instructions to composing them in novel combinations (zero-shot composition).
- Evidence anchors: [abstract] "Our instruction-finetuning recipe for NMT models enables customization of translations for a limited but disparate set of translation-specific tasks." [section] "Through instruction finetuning, we hope to jointly model a range of disparate tasks."

### Mechanism 2
- Claim: Mixing parallel data with task-specific data during finetuning preserves general translation quality while learning task-specific behaviors.
- Mechanism: The 2:1 ratio of parallel to task data ensures the model doesn't forget its core translation capability while learning to follow instructions. Parallel data acts as a regularization signal.
- Core assumption: The model can maintain a balance between general translation and instruction-following without catastrophic forgetting.
- Evidence anchors: [section] "The mixing ratio is a hyperparameter in our recipe and we tune it so that we observe no degradation in general translation performance" [section] "We find that not including the parallel data in the recipe leads to degradation of general translation performance."

### Mechanism 3
- Claim: Instruction-finetuned NMT models offer better adversarial robustness compared to LLMs by having a smaller attack surface.
- Mechanism: The restricted vocabulary and task-specific nature of instructions limits the ways users can manipulate model behavior through prompt injection compared to open-ended LLM prompts.
- Core assumption: The simpler instruction-following mechanism is less vulnerable to adversarial attacks than complex LLM prompting systems.
- Evidence anchors: [section] "instruction-finetuned NMT models, by default expose a much smaller attack surface area and thereby are less vulnerable to adversarial attacks" [section] Table 9 shows NMT model correctly ignores adversarial source content while LLM fails

## Foundational Learning

- Concept: Vocabulary expansion with instruction tokens
  - Why needed here: To cleanly separate instruction tokens from source text tokens and prevent the model from translating instructions themselves
  - Quick check question: What happens if instruction tokens are not added to the vocabulary? (Answer: The model may try to translate instruction words, causing degradation)

- Concept: Task-specific data curation through synthetic generation
  - Why needed here: To create sufficient training examples for diverse translation customization tasks without requiring manual annotation
  - Quick check question: How does synthetic data generation differ from using parallel corpus filtering? (Answer: Synthetic generation creates examples for tasks not naturally present in parallel data)

- Concept: Zero-shot composition capabilities
  - Why needed here: To enable the model to handle combinations of instructions it has never seen during training
  - Quick check question: What evidence suggests the model can compose instructions? (Answer: Table 4 shows success rates for composed instructions)

## Architecture Onboarding

- Component map: Pre-trained NMT model -> Vocabulary expansion layer -> Mixed training pipeline -> (Optional) Interpolation module
- Critical path: 1. Load pre-trained NMT model 2. Expand vocabulary with instruction tokens 3. Curate parallel and task-specific datasets 4. Finetune with 2:1 mixing ratio 5. (Optional) Apply interpolation 6. Evaluate on test sets
- Design tradeoffs: Larger vocabulary vs. model complexity, more task data vs. preservation of general translation, synthetic data quality vs. manual annotation cost
- Failure signatures: General translation quality drops significantly (indicates too much task data), low instruction response rates (indicates poor instruction learning), model tries to translate instruction tokens (indicates vocabulary expansion issues)
- First 3 experiments: 1. Finetune with only task data (no parallel data) to measure degradation in general translation 2. Finetune with only parallel data (no task data) to verify baseline performance 3. Finetune with various mixing ratios to find optimal balance point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can instruction-finetuned NMT models generalize to entirely new tasks not seen during finetuning?
- Basis in paper: [inferred] The paper shows zero-shot composition of instructions but doesn't test generalization to completely novel tasks
- Why unresolved: The evaluation focused on tasks similar to the finetuning data (style, formality, length control). Testing with truly novel tasks would require a separate evaluation set.
- What evidence would resolve it: Testing the model on completely new tasks (e.g., summarization, sentiment transfer) that were not represented in the finetuning data and measuring performance compared to task-specific models.

### Open Question 2
- Question: How does the performance of instruction-finetuned NMT models scale with model size and capacity?
- Basis in paper: [explicit] The paper uses a Transformer-Big model (225M parameters) but doesn't explore scaling effects
- Why unresolved: The experiments were conducted on a single model size. Larger models might have better instruction-following capabilities, while smaller models might be more efficient.
- What evidence would resolve it: Conducting the same experiments with different model sizes (e.g., Transformer-Base, Transformer-Large) and analyzing the trade-off between instruction-following performance and model parameters.

### Open Question 3
- Question: What is the optimal mixing ratio between parallel and task-specific data during finetuning?
- Basis in paper: [explicit] The paper uses a 2:1 mixing ratio but acknowledges it as a hyperparameter
- Why unresolved: The paper doesn't explore different mixing ratios or their impact on the trade-off between general translation quality and instruction-following capabilities.
- What evidence would resolve it: Conducting finetuning experiments with various mixing ratios (e.g., 1:1, 1:2, 3:1) and measuring both general translation performance and instruction-following performance to find the optimal balance.

### Open Question 4
- Question: How does instruction-finetuned NMT performance compare to larger language models on more diverse and complex tasks?
- Basis in paper: [explicit] The paper compares to GPT-3.5-Turbo only on formality control tasks
- Why unresolved: The comparison is limited to a single task type. Larger language models might excel at more complex instruction compositions or tasks requiring broader world knowledge.
- What evidence would resolve it: Evaluating both instruction-finetuned NMT models and LLMs on a wider range of tasks including complex instruction compositions, reasoning tasks, and tasks requiring external knowledge, while controlling for inference cost and security considerations.

## Limitations
- Synthetic data quality dependence could lead to spurious correlations or incorrect instruction mappings
- Zero-shot composition generalization claims are based on limited evaluation sets
- Adversarial robustness claims lack rigorous systematic testing beyond a single example
- Mixing ratio optimization is treated as empirical tuning without theoretical justification

## Confidence
**High Confidence Claims**:
- The basic instruction-finetuning recipe successfully enables NMT models to follow individual instructions
- The model maintains general translation quality (+0.3 ChrF2) after finetuning
- The approach achieves comparable performance to GPT-3.5-Turbo on the IWSLT-22 Formality Control Shared Task

**Medium Confidence Claims**:
- The model can follow 30+ disparate instructions simultaneously with 89.60% average response rate
- Zero-shot composition of instructions works reliably across task combinations
- Instruction-finetuned NMT models offer better adversarial robustness than LLMs

**Low Confidence Claims**:
- The 2:1 mixing ratio is optimal for all instruction-finetuning scenarios
- The model's instruction-following capabilities will generalize to instructions outside the 30-task set
- The cost advantages over LLMs will scale proportionally with deployment size

## Next Checks
1. **Systematic Composition Testing**: Design a comprehensive evaluation suite that tests instruction composition across all possible pairs and triplets from the 30 tasks, measuring both success rates and any degradation in general translation quality. Include edge cases like contradictory instructions and instructions requiring multi-step reasoning.

2. **Adversarial Robustness Benchmark**: Implement a standardized adversarial testing framework that includes prompt injection attacks, instruction confusion attacks, and semantic adversarial examples. Compare the instruction-finetuned NMT model against both the base NMT model and GPT-3.5-Turbo across multiple attack categories.

3. **Synthetic Data Quality Analysis**: Conduct an ablation study where the same instruction-finetuning recipe is applied using synthetic data of varying quality levels (controlled through different prompting strategies for the synthetic data generator). Measure how instruction-following performance and general translation quality vary with synthetic data quality.