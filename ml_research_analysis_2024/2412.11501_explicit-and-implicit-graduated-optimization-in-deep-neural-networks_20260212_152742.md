---
ver: rpa2
title: Explicit and Implicit Graduated Optimization in Deep Neural Networks
arxiv_id: '2412.11501'
source_url: https://arxiv.org/abs/2412.11501
tags:
- function
- nshb
- optimization
- algorithm
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes explicit and implicit graduated optimization\
  \ in deep neural networks, focusing on the implicit version with SGD and SGD with\
  \ momentum. The authors prove that Rastrigin's function is a new \u03C3-nice function,\
  \ validating theoretical convergence guarantees."
---

# Explicit and Implicit Graduated Optimization in Deep Neural Networks

## Quick Facts
- arXiv ID: 2412.11501
- Source URL: https://arxiv.org/abs/2412.11501
- Authors: Naoki Sato; Hideaki Iiduka
- Reference count: 40
- Primary result: Implicit graduated optimization using natural SGD/SHB noise effectively trains deep neural networks, with optimal polynomial learning rate decay (p < 1) confirmed experimentally across CIFAR100 and ImageNet tasks.

## Executive Summary
This paper analyzes both explicit and implicit graduated optimization in deep neural networks, focusing on the implicit version that leverages naturally occurring stochastic noise in SGD and SGD with momentum. The authors prove that Rastrigin's function is a new σ-nice function, validating theoretical convergence guarantees. Through extensive experiments, they demonstrate that while explicit graduated optimization fails for DNNs, implicit methods using natural SGD/SHB noise work effectively, particularly when batch size increases during training. The optimal polynomial learning rate decay (p < 1) is confirmed experimentally for both SHB and NSHB variants across CIFAR100 and ImageNet tasks, aligning with theoretical predictions.

## Method Summary
The paper introduces and analyzes both explicit and implicit graduated optimization algorithms for deep neural networks. Explicit graduated optimization directly optimizes smoothed versions of the objective function, while implicit graduated optimization uses the naturally occurring stochastic noise in SGD and its variants to implicitly smooth the objective function. The implicit approach gradually decreases learning rates and increases batch sizes during training to reduce noise levels, effectively transitioning from a strongly convex smoothed function to the original non-convex function. The authors provide convergence analysis for the implicit SGD with momentum variant and experimentally validate the optimal polynomial learning rate decay (p < 1) across multiple datasets and architectures.

## Key Results
- Rastrigin's function is proven to be a new σ-nice function, validating theoretical convergence guarantees
- Explicit graduated optimization is ineffective for DNNs but implicit methods using natural SGD/SHB noise work well, especially with increasing batch sizes
- Optimal polynomial learning rate decay (p < 1) is confirmed experimentally for both SHB and NSHB across CIFAR100 and ImageNet tasks
- Implicit SGD with momentum extends the approach to include momentum factor scheduling while maintaining convergence guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Implicit graduated optimization with SGD and momentum uses naturally occurring stochastic noise to smooth the objective function, allowing SGD to behave like gradient descent on a smoothed version of the original function.
- Mechanism: The stochastic noise in SGD (and its variants like SGD with momentum) has a level δopt proportional to the learning rate η and inversely proportional to the square root of the batch size b. By gradually decreasing η and increasing b during training, the noise level δopt decreases, implicitly smoothing the objective function more and more until it approaches the original non-convex function.
- Core assumption: The noise level δopt is small enough that the smoothed function remains strongly convex in a neighborhood around the global optimum.
- Evidence anchors:
  - [abstract]: "they defined a new σ-nice function, which is an extension of the σ-nice function, and provided a convergence analysis of the implicit graduated optimization algorithm for this function."
  - [section]: "Here, we provide numerical experimental results that are consistent with theory as far as computational complexity permits, confirming that their algorithm works effectively (Figure 4)."
  - [corpus]: Weak evidence; no direct mention of implicit smoothing in related papers.
- Break condition: If the noise level δopt becomes too small relative to the strong convexity parameter σ, the smoothed function may no longer be strongly convex, and the algorithm may fail to converge to the global optimum.

### Mechanism 2
- Claim: The optimal decay rate for the noise level in graduated optimization is a polynomial decay with power p < 1.
- Mechanism: The decay rate γm of the noise level δm is set to (M - m)p / (M - (m - 1))p, where p is a power less than 1. This decay rate ensures that the algorithm satisfies the conditions necessary for convergence to the global optimum of the new σ-nice function.
- Core assumption: The objective function is a new σ-nice function, which means it has a smoothed version that is strongly convex in a neighborhood around the global optimum.
- Evidence anchors:
  - [abstract]: "The optimal polynomial learning rate decay (p < 1) is confirmed experimentally for both SHB and NSHB across CIFAR100 and ImageNet tasks, aligning with theoretical predictions."
  - [section]: "In order to see which learning rate scheduler gives the smallest loss function value for SGD with momentum, we trained ResNet34 (He et al. 2016) on the ImageNet dataset (Deng et al. 2009) with SHB for 100 epochs. The results in Figure 6 indicate that a polynomial decay with a power less than or equal to 1 achieves the smallest training loss function."
  - [corpus]: No direct evidence; related papers do not discuss optimal noise scheduling.
- Break condition: If the power p is too close to 1, the noise level may not decay quickly enough, and the algorithm may not converge to the global optimum in a reasonable number of iterations.

### Mechanism 3
- Claim: Implicit graduated optimization with SGD with momentum extends the implicit graduated optimization approach to include the momentum factor, which contributes to convergence to the global optimum.
- Mechanism: The stochastic noise in SGD with momentum is determined by the learning rate, batch size, and momentum factor. By gradually decreasing the learning rate and momentum factor and increasing the batch size during training, the noise level decreases, implicitly smoothing the objective function and allowing the algorithm to converge to the global optimum.
- Core assumption: The momentum factor contributes to the smoothing effect of the stochastic noise, and decreasing it helps the algorithm converge to the global optimum.
- Evidence anchors:
  - [abstract]: "On the basis that the stochastic noise of SGD with momentum is determined by the learning rate, batch size, and momentum factor (Sato and Iiduka 2024), we constructed an algorithm (Algorithm 6) to perform implicit graduated optimization in the same way."
  - [section]: "We tested the ability of Algorithm 6 to reduce stochastic noise, we compared it with a vanilla SHB method in which the learning rate, batch size, and momentum are all constant. Here, Algorithm 6 outperformed vanilla SHB in both test accuracy and loss function value, thereby demonstrating that it is superior to SGD with momentum using constant parameters on image classification tasks."
  - [corpus]: No direct evidence; related papers do not discuss implicit graduated optimization with momentum.
- Break condition: If the momentum factor is decreased too quickly, the algorithm may lose the benefits of momentum, such as faster convergence and better generalization.

## Foundational Learning

- Concept: Strong convexity
  - Why needed here: The convergence analysis of the implicit graduated optimization algorithm relies on the assumption that the smoothed objective function is strongly convex in a neighborhood around the global optimum.
  - Quick check question: What is the definition of a strongly convex function, and how does it differ from a convex function?

- Concept: Stochastic gradient descent (SGD) and its variants
  - Why needed here: The implicit graduated optimization algorithm uses the naturally occurring stochastic noise in SGD and its variants to smooth the objective function.
  - Quick check question: What is the difference between SGD and its variants like SGD with momentum, and how does the stochastic noise in each variant affect the convergence of the algorithm?

- Concept: Learning rate scheduling
  - Why needed here: The optimal decay rate for the noise level in graduated optimization is a polynomial decay with power p < 1, which corresponds to a specific learning rate scheduling strategy.
  - Quick check question: What are the different types of learning rate scheduling strategies, and how do they affect the convergence of SGD and its variants?

## Architecture Onboarding

- Component map: Implicit graduated optimization algorithm -> Noise level calculation and decay rate -> Learning rate scheduling -> Batch size scheduling -> Momentum factor scheduling
- Critical path: The algorithm starts with an initial learning rate, batch size, and momentum factor. It then iteratively updates these hyperparameters according to the decay rate and noise level calculations, while optimizing the objective function using SGD or its variants.
- Design tradeoffs: The choice of the power p in the polynomial decay rate affects the convergence rate and the number of iterations needed. A smaller p leads to faster convergence but may require more iterations, while a larger p leads to slower convergence but may require fewer iterations.
- Failure signatures: If the algorithm fails to converge to the global optimum, it may be due to an incorrect choice of the power p, an insufficient number of iterations, or a violation of the strong convexity assumption.
- First 3 experiments:
  1. Train a simple neural network on a toy dataset using the implicit graduated optimization algorithm with SGD and momentum, and compare the results with vanilla SGD and SGD with momentum using constant hyperparameters.
  2. Vary the power p in the polynomial decay rate and observe its effect on the convergence rate and the number of iterations needed.
  3. Test the algorithm on a more complex dataset and compare its performance with other state-of-the-art optimization algorithms.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or dimensionality factors prevent explicit graduated optimization from working effectively on deep neural networks?
- Basis in paper: [explicit] The paper demonstrates that explicit graduated optimization fails to improve loss values for ResNet architectures despite working well on traditional benchmark functions, suggesting high dimensionality may be a factor.
- Why unresolved: The paper notes this limitation but doesn't provide a theoretical explanation for why explicit methods fail in high-dimensional spaces like DNNs.
- What evidence would resolve it: A theoretical analysis showing how explicit smoothing interacts with high-dimensional parameter spaces, or empirical studies comparing explicit methods across architectures with varying parameter counts.

### Open Question 2
- Question: How do different variants of SGD with momentum (beyond SHB and NSHB) perform under implicit graduated optimization?
- Basis in paper: [inferred] The paper focuses on SHB and NSHB variants but acknowledges "a number of variants of SGD with momentum" exist, suggesting potential differences in implicit smoothing properties.
- Why unresolved: The analysis and experiments are limited to two specific momentum variants, leaving open whether the implicit smoothing framework generalizes to other momentum methods.
- What evidence would resolve it: Convergence analysis and experimental results for other momentum variants (e.g., Nesterov momentum, Adam with momentum components) under implicit graduated optimization.

### Open Question 3
- Question: What is the relationship between the polynomial decay power p in learning rate scheduling and the actual noise reduction achieved during training?
- Basis in paper: [explicit] The paper identifies p ∈ (0,1] as theoretically optimal for implicit graduated optimization but doesn't quantify how different p values affect the actual stochastic noise decay during training.
- Why unresolved: While the paper shows p ∈ (0,1] works best empirically, it doesn't measure or model the precise relationship between the polynomial decay schedule and the noise reduction mechanism.
- What evidence would resolve it: Empirical measurements of the actual noise levels throughout training under different p values, or theoretical analysis linking the polynomial decay schedule to the implicit smoothing effect.

## Limitations
- Theoretical claims about implicit graduated optimization rely heavily on the assumption that DNNs satisfy "new σ-nice function" conditions, which remains unproven for general loss landscapes
- Empirical validation is limited to ResNet architectures on image classification tasks, with no exploration of other network architectures or task domains
- The explicit graduated optimization method's ineffectiveness for DNNs is demonstrated but not deeply analyzed, with fundamental reasons remaining somewhat speculative

## Confidence
- High confidence: Experimental validation of optimal polynomial learning rate decay (p < 1) for implicit methods across CIFAR100 and ImageNet
- Medium confidence: Theoretical extension of σ-nice functions and convergence analysis for Rastrigin's function
- Low confidence: Generalizability of implicit graduated optimization benefits beyond ResNet architectures and image classification tasks

## Next Checks
1. Test implicit graduated optimization on non-ResNet architectures (e.g., Vision Transformers, MLPs) and non-image tasks (e.g., language modeling, reinforcement learning) to assess generalizability

2. Conduct ablation studies isolating the effects of learning rate decay, batch size increase, and momentum scheduling to determine which component(s) drive the performance improvements

3. Perform extensive hyperparameter sensitivity analysis across different datasets and network depths to establish the robustness of the p < 1 decay rate recommendation