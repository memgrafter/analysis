---
ver: rpa2
title: Towards Robust Vision Transformer via Masked Adaptive Ensemble
arxiv_id: '2407.15385'
source_url: https://arxiv.org/abs/2407.15385
tags:
- adversarial
- clean
- adaptive
- robustness
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving Vision Transformer
  (ViT) robustness against adversarial attacks while maintaining high standard accuracy.
  The core method introduces a novel ViT architecture that includes a detector and
  classifier bridged by an adaptive ensemble.
---

# Towards Robust Vision Transformer via Masked Adaptive Ensemble

## Quick Facts
- arXiv ID: 2407.15385
- Source URL: https://arxiv.org/abs/2407.15385
- Authors: Fudong Lin; Jiadong Lou; Xu Yuan; Nian-Feng Tzeng
- Reference count: 40
- Primary result: Novel ViT architecture achieves 90.3% standard accuracy and 49.8% adversarial robustness on CIFAR-10, outperforming state-of-the-art adversarial training techniques

## Executive Summary
This paper addresses the challenge of improving Vision Transformer (ViT) robustness against adversarial attacks while maintaining high standard accuracy. The proposed solution introduces a novel ViT architecture that includes a detector and classifier bridged by an adaptive ensemble. The detector employs Guided Backpropagation and a novel Multi-head Self-Attention mechanism to identify adversarial examples, while the classifier uses two encoders with a newly developed adaptive ensemble to adaptively adjust the proportion of visual representations for accurate classification. This design allows the system to mask off a random subset of image patches during input, boosting robustness against adaptive attacks while maintaining high standard accuracy.

## Method Summary
The method introduces a novel ViT architecture with a detector and classifier bridged by an adaptive ensemble. The detector uses Guided Backpropagation visualization combined with a Multi-head Self-Attention mechanism to identify adversarial examples by amplifying subtle gradient differences. The classifier employs two encoders (one for clean images, one for adversarial examples) and uses a masked adaptive ensemble during fine-tuning to dynamically blend representations based on detection confidence. The system is trained using One-step Least-Likely Adversarial Training, with the detector and encoders pre-trained jointly before fine-tuning the adaptive ensemble.

## Key Results
- Achieves 90.3% standard accuracy and 49.8% adversarial robustness on CIFAR-10
- Outperforms state-of-the-art adversarial training techniques
- Demonstrates effective tradeoff between standard accuracy and robustness through adaptive ensemble mechanism
- Shows improved performance under Adaptive Auto Attack (A3) and Parameter-Free Adaptive Auto Attack (PF-A3)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Guided Backpropagation visualization exposes adversarial perturbations that are otherwise imperceptible in raw images
- Mechanism: By applying Guided Backpropagation to both clean and adversarial inputs, the detector can amplify and visualize subtle differences in gradient patterns
- Core assumption: Gradient patterns produced by Guided Backpropagation on adversarial examples contain sufficient signal to differentiate them from clean images
- Evidence anchors: Weak empirical support from related papers

### Mechanism 2
- Claim: The proposed Multi-head Self-Attention mechanism can effectively fuse information from both original and Guided Backpropagation-enhanced inputs
- Mechanism: MSA takes two sets of patch embeddings and uses a relative detection bias derived from the variant to attend differently to patches containing adversarial perturbations
- Core assumption: Attention mechanism can learn to weight patches differently based on the Guided Backpropagation signal
- Evidence anchors: No direct evidence in corpus - appears to be novel

### Mechanism 3
- Claim: Masked Adaptive Ensemble improves the tradeoff between standard accuracy and robustness by dynamically adjusting the contribution of clean and adversarial encoders
- Mechanism: The ensemble computes a weighted sum of representations from clean and adversarial encoders, where the weight is determined by the detector's confidence
- Core assumption: The detector can accurately estimate the probability that an input is clean
- Evidence anchors: Weak evidence - ensemble methods exist but not with adaptive masking and detector-guided weighting

## Foundational Learning

- Concept: Vision Transformer architecture and self-attention mechanisms
  - Why needed here: The entire approach builds upon ViT as the backbone
  - Quick check question: How does the Multi-head Self-Attention mechanism in this paper differ from standard ViT attention?

- Concept: Adversarial training and threat models
  - Why needed here: The method uses One-step Least-Likely Adversarial Training as its base training approach
  - Quick check question: What is the difference between standard FGSM and the least-likely targeted variant used in this paper?

- Concept: Masked Autoencoders and contrastive learning
  - Why needed here: The classifier uses MAE-style pre-training with two encoders and a contrastive loss
  - Quick check question: How does the contrastive loss in this paper encourage similarity between clean and adversarial variants?

## Architecture Onboarding

- Component map: Input -> Detector (ViT-Tiny + GB + MSA) -> Adaptive Ensemble -> Classifier (Two Encoders + MLP) -> Output

- Critical path:
  1. Pre-training: Train detector and classifier jointly using One-step Least-Likely AT
  2. Fine-tuning: Freeze detector and encoders, train adaptive ensemble and MLP classifier
  3. Inference: Run input through detector to get p, then through adaptive ensemble to get final classification

- Design tradeoffs:
  - Two encoders vs. single encoder: Better robustness but higher computational cost
  - Masking ratio: 45% during fine-tuning balances robustness vs. accuracy degradation
  - SNN loss weight (Î»=0.15): Controls trade-off between detection accuracy and representation separation

- Failure signatures:
  - Detector failure: High false positive/negative rates, poor detection accuracy on adaptive attacks
  - Encoder failure: Representations don't capture meaningful differences between clean and adversarial inputs
  - Ensemble failure: p estimates are unreliable, leading to poor blending decisions

- First 3 experiments:
  1. Validate Guided Backpropagation visualization: Apply GB to clean and adversarial CIFAR-10 images and visually inspect for noticeable differences
  2. Test MSA effectiveness: Compare detection accuracy with and without the MSA mechanism using A3 attack
  3. Evaluate adaptive ensemble: Measure standard accuracy and robustness under different masking ratios (25%, 45%, 75%) to find optimal tradeoff point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the Masked Adaptive Ensemble method scale when applied to larger datasets with more classes, such as ImageNet-1K or beyond?
- Basis in paper: The paper evaluates the method on CIFAR-10, CIFAR-100, and Tiny-ImageNet, but does not explore performance on larger datasets like ImageNet-1K
- Why unresolved: The paper focuses on relatively small-scale datasets, and the authors do not provide insights into how the method would perform on larger-scale vision tasks
- What evidence would resolve it: Experimental results on larger datasets, such as ImageNet-1K, would provide insights into the scalability and performance of the method

### Open Question 2
- Question: How does the Masked Adaptive Ensemble method perform under different types of adaptive attacks not considered in the paper, such as transfer-based or decision-based attacks?
- Basis in paper: The paper evaluates the method against AutoAttack, Adaptive Auto Attack (A3), and Parameter-Free Adaptive Auto Attack (PF-A3), but does not explore other types of adaptive attacks
- Why unresolved: The paper focuses on specific types of adaptive attacks, and the authors do not provide insights into how the method would perform under different attack scenarios
- What evidence would resolve it: Experimental results under various adaptive attack scenarios, such as transfer-based or decision-based attacks, would provide a more comprehensive understanding of the method's robustness

### Open Question 3
- Question: What is the impact of varying the masking ratio during the fine-tuning stage on the trade-off between standard accuracy and adversarial robustness for different types of adversarial examples?
- Basis in paper: The paper explores the impact of the masking ratio on the trade-off between standard accuracy and adversarial robustness, but does not provide insights into how it affects different types of adversarial examples
- Why unresolved: The paper focuses on a specific masking ratio and does not provide insights into how varying the masking ratio would affect the method's performance against different types of adversarial examples
- What evidence would resolve it: Experimental results with varying masking ratios under different types of adversarial examples would provide insights into the optimal masking ratio for different scenarios

## Limitations
- The reliance on Guided Backpropagation for adversarial detection lacks rigorous validation and may not generalize to all attack types
- The optimal masking ratio appears dataset-dependent, requiring careful tuning for different applications
- Computational overhead from maintaining two encoders and complex detection mechanisms may limit scalability

## Confidence
- **High confidence**: The adaptive ensemble framework and its mathematical formulation are well-defined and theoretically sound
- **Medium confidence**: The two-encoder architecture with masked training shows reasonable empirical results, though the optimal masking ratio remains dataset-dependent
- **Low confidence**: The Guided Backpropagation + MSA detection mechanism lacks sufficient validation and may not generalize well to attacks beyond A3

## Next Checks
1. Cross-dataset generalization: Test the complete framework on completely unseen datasets (e.g., ImageNet-1K) to verify whether the Guided Backpropagation detection signal transfers beyond CIFAR variants
2. Adaptive attack resilience: Evaluate against attacks specifically designed to fool the Guided Backpropagation detector, such as attacks that minimize gradient visualization signatures
3. Ablation of detection signal: Systematically disable the Guided Backpropagation component while maintaining the MSA structure to isolate its contribution to overall robustness gains