---
ver: rpa2
title: 'WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models
  Gains More'
arxiv_id: '2402.12065'
source_url: https://arxiv.org/abs/2402.12065
tags:
- quantization
- cache
- arxiv
- memory
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the deployment challenges of large language
  models (LLMs) caused by their substantial memory requirements and computational
  demands during auto-regressive text generation. The authors propose WKVQuant, a
  post-training quantization (PTQ) framework specifically designed for quantizing
  weights and key/value (KV) cache of LLMs.
---

# WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More

## Quick Facts
- arXiv ID: 2402.12065
- Source URL: https://arxiv.org/abs/2402.12065
- Reference count: 13
- Primary result: W4KV4 quantized network with WKVQuant achieves significantly higher accuracy compared to W4A4 quantized network, with similar performance to weight-only quantization

## Executive Summary
This paper addresses the deployment challenges of large language models (LLMs) caused by substantial memory requirements and computational demands during auto-regressive text generation. The authors propose WKVQuant, a post-training quantization framework specifically designed for quantizing weights and key/value (KV) cache of LLMs. WKVQuant incorporates past-only quantization (POQ) to improve attention computation, two-dimensional quantization (2D-Quantization) to handle KV cache distribution, and cross-block reconstruction regularization for parameter optimization. Experiments show that WKVQuant achieves almost comparable memory savings to weight-activation quantization while approaching the performance of weight-only quantization.

## Method Summary
WKVQuant is a post-training quantization framework that quantizes both weights and KV cache of LLMs. It implements three key components: Past-Only Quantization (POQ) that preserves attention accuracy by avoiding quantization of current token's KV, Two-dimensional Quantization (2D-Quantization) that reduces quantization error through channel smoothing and token-wise scaling, and Cross-block Reconstruction Regularization (CRR) that provides more task-relevant gradient signals than block-wise MSE loss. The framework was evaluated on LLaMA and LLaMA-2 models using a calibration dataset of 128 randomly selected 2048-token segments from WikiText2, with AdamW optimizer and learning rate 5e-4 for 2D-Quantization and 1e-2 for learnable weight clipping over 5 epochs.

## Key Results
- W4KV4 quantized network with WKVQuant achieves significantly higher accuracy compared to W4A4 quantized network
- WKVQuant shows similar performance to weight-only quantization for short sequence generation
- WKVQuant achieves close performance to weight-only quantization for long sequence generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Past-Only Quantization (POQ) preserves attention accuracy by avoiding quantization of current token's KV.
- Mechanism: POQ uses full-precision KV for the current token during attention computation, while only quantizing past KV cache.
- Core assumption: The current token's KV values are more critical to accuracy than past ones.
- Evidence anchors: [abstract] "Specifically, we incorporates past-only quantization to improve the computation of attention"; [section 3.3] "Instead of discarding the full-precision Key and Value values (KV) after quantization, we temporarily retain them. This allows us to use unquantized KV during matrix multiplication in the Attention mechanism, improving the accuracy of Attention computation."
- Break condition: If the relative importance of current vs past KV changes with model architecture or sequence length.

### Mechanism 2
- Claim: Two-dimensional Quantization (2D-Quantization) reduces quantization error by addressing both channel-wise and token-wise value variations in KV cache.
- Mechanism: Static channel smoothing aligns large values between channels using learnable shifting and scaling parameters. Dynamic token-wise quantization addresses outliers between tokens by computing statistics per token rather than globally.
- Core assumption: KV cache exhibits significant variation both across channels and across tokens.
- Evidence anchors: [abstract] "Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache"; [section 3.4] "We observe significant variations in numerical values between channels and tokens in KV cache. Towards this issue, we propose Two-dimensional Quantization."
- Break condition: If KV cache statistics are homogeneous across channels/tokens, making the additional complexity unnecessary.

### Mechanism 3
- Claim: Cross-block Reconstruction Regularization (CRR) provides more task-relevant gradient signals than block-wise MSE loss.
- Mechanism: CRR computes MAE loss between outputs of quantized and full-precision networks after multiple decoder blocks, better approximating the final task loss and reducing outlier amplification.
- Core assumption: Local reconstruction loss introduces bias because different activations have varying impacts on final output.
- Evidence anchors: [abstract] "along with a cross-block reconstruction regularization for parameter optimization"; [section 3.5] "Previous methods (Shao et al., 2023) optimized these parameters using gradient descent... However, it is worth noting that the local reconstruction loss introduces a bias and does not align with the final task loss."
- Break condition: If the approximation error from using MAE over fewer blocks becomes significant compared to direct task loss.

## Foundational Learning

- Concept: Quantization and its impact on model accuracy
  - Why needed here: Understanding how converting parameters from high-precision to low-precision integers affects model performance is fundamental to evaluating WKVQuant's approach.
  - Quick check question: What is the typical accuracy drop when quantizing weights from FP16 to INT4, and how does this compare to quantizing activations?

- Concept: KV cache and its role in autoregressive generation
  - Why needed here: KV cache stores intermediate attention results to avoid redundant computation. Understanding its memory dynamics is crucial for appreciating why KV cache quantization matters.
  - Quick check question: How does KV cache memory consumption scale with sequence length and batch size in transformer models?

- Concept: Post-training quantization (PTQ) vs Quantization-aware training (QAT)
  - Why needed here: WKVQuant is a PTQ method, which is generally less accurate than QAT but more practical. Understanding this tradeoff is important for context.
  - Quick check question: What are the key differences in accuracy and implementation complexity between PTQ and QAT approaches?

## Architecture Onboarding

- Component map: Input → Linear projection → POQ (current KV bypass) → 2D-Quantized past KV → Attention computation → Output
- Critical path: During inference, the critical path is: input → linear projection → POQ (current KV bypass) → 2D-Quantized past KV → attention computation → output. The quantization parameters are precomputed during calibration.
- Design tradeoffs: WKVQuant trades some calibration complexity (CRR regularization, 2D-Quantization parameters) for better accuracy vs simpler quantization schemes. It also accepts higher memory usage than full activation quantization to maintain accuracy.
- Failure signatures: Accuracy degradation indicates problems with: 1) POQ implementation (current KV getting quantized), 2) 2D-Quantization parameters (poor channel/token alignment), 3) CRR calibration (insufficient training epochs or wrong k value).
- First 3 experiments:
  1. Verify POQ by checking that current token KV remains in FP16 during attention computation while past KV is INT4.
  2. Test 2D-Quantization by examining the distribution of smoothed and scaled KV cache values across channels and tokens.
  3. Validate CRR by comparing perplexity on WikiText2 with and without CRR during calibration, ensuring k=5 gives best results.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of blocks to use in the Cross-block Reconstruction Regularization (CRR) for different model sizes and architectures?
- Basis in paper: [explicit] The paper mentions that k=5 was found to be the best number of blocks for CRR in their experiments, but acknowledges that this may not be optimal for all scenarios.
- Why unresolved: The paper only tested k=5 and didn't explore a wider range of values or different model architectures to determine if this is universally optimal.
- What evidence would resolve it: Experiments varying k across different model sizes (e.g., 7B, 13B, 70B parameters) and architectures, comparing performance metrics like perplexity and accuracy.

### Open Question 2
- Question: How does the proposed Past Only Quantization (POQ) technique perform when applied to other attention mechanisms or model architectures beyond the standard Transformer?
- Basis in paper: [inferred] The paper demonstrates POQ's effectiveness on standard Transformer attention, but doesn't explore its applicability to other attention variants or architectures.
- Why unresolved: The paper focuses on standard Transformer models and doesn't investigate how POQ might need to be adapted or if it's beneficial for other architectures.
- What evidence would resolve it: Applying POQ to models with different attention mechanisms (e.g., Performer, Linformer) or non-Transformer architectures (e.g., RNNs, CNNs) and comparing performance.

### Open Question 3
- Question: What is the impact of the Two-dimensional Quantization (2D-Quantization) on the training dynamics and convergence speed of the model?
- Basis in paper: [inferred] The paper shows that 2D-Quantization improves final model accuracy, but doesn't discuss its effects on training dynamics or convergence speed.
- Why unresolved: The paper focuses on the final quantized model's performance but doesn't provide insights into how the quantization affects the training process itself.
- What evidence would resolve it: Comparing training curves, convergence speed, and final loss values between models using 2D-Quantization and those using standard quantization methods.

## Limitations
- Limited comparison set: The experimental validation primarily compares against W4A4 and weight-only quantization, lacking comparison with other state-of-the-art KV cache quantization methods like H1B-KV, NQKV, or PolarQuant.
- Memory overhead: Memory savings analysis doesn't account for potential overhead from additional calibration parameters in 2D-Quantization and CRR.
- Performance ceiling: Evaluation on long sequence generation shows WKVQuant approaches but doesn't exceed weight-only quantization, suggesting potential performance ceilings for very long contexts.

## Confidence
- **High confidence**: Claims about WKVQuant's architecture and implementation details (Past-Only Quantization mechanism, 2D-Quantization approach, CRR regularization) are well-specified in the paper with clear pseudo-code and hyperparameter settings.
- **Medium confidence**: Claims about memory savings and accuracy improvements are supported by experiments but lack comprehensive ablation studies to isolate the contribution of each component.
- **Low confidence**: Claims about WKVQuant's superiority over all existing KV cache quantization methods are weakly supported, as the comparison set is limited and doesn't include the most recent competing approaches.

## Next Checks
1. **Cross-method comparison validation**: Implement and compare WKVQuant against H1B-KV and NQKV on the same LLaMA-2-7B model using WikiText2 perplexity as the primary metric.
2. **Component ablation study**: Create variants of WKVQuant with individual components disabled (POQ only, 2D-Quantization only, CRR only) and measure their impact on perplexity and memory usage.
3. **Long sequence stress test**: Evaluate WKVQuant, W4A4, and weight-only quantization on sequences longer than 8K tokens to determine if WKVQuant maintains its accuracy advantage as KV cache size becomes the dominant memory factor.