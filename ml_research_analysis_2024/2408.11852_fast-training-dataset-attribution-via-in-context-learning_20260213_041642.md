---
ver: rpa2
title: Fast Training Dataset Attribution via In-Context Learning
arxiv_id: '2408.11852'
source_url: https://arxiv.org/abs/2408.11852
tags:
- data
- context
- dataset
- boolq
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes two methods for training dataset attribution
  (TDA) in instruction-tuned LLMs: a similarity-based Shapley Context Method (SCM)
  and a semi-parametric Context Mixture Factorization (CMF) approach. SCM measures
  output differences with and without context from training datasets, while CMF frames
  attribution as a matrix factorization problem using mixture distributions.'
---

# Fast Training Dataset Attribution via In-Context Learning

## Quick Facts
- **arXiv ID**: 2408.11852
- **Source URL**: https://arxiv.org/abs/2408.11852
- **Reference count**: 24
- **Key outcome**: Two methods for training dataset attribution (TDA) in instruction-tuned LLMs: similarity-based Shapley Context Method (SCM) and semi-parametric Context Mixture Factorization (CMF), with CMF showing better robustness to retrieval noise and computational efficiency

## Executive Summary
This paper addresses the challenge of training dataset attribution (TDA) in instruction-tuned large language models (LLMs) by proposing two novel methods that leverage in-context learning. The first method, Shapley Context Method (SCM), uses a similarity-based approach to measure how context retrieved from training datasets affects model outputs. The second method, Context Mixture Factorization (CMF), frames attribution as a matrix factorization problem using mixture distributions, solving it with alternating least squares. Experiments on three datasets (BoolQ, FakeQ, Olympic2024) with four instruction-tuned LLMs demonstrate that CMF is more robust to retrieval noise and computationally efficient compared to SCM, while both methods successfully identify dataset influence and distinguish between seen and unseen datasets.

## Method Summary
The paper proposes two methods for training dataset attribution in instruction-tuned LLMs using in-context learning. SCM measures output differences with and without context from training datasets using similarity calculations, applying the Shapley value formula to determine attribution. CMF frames attribution as a matrix factorization problem where the P matrix captures attribution scores, solved using alternating least squares with regularization. Both methods use RAG for context retrieval with 512-token chunks and top-k=3, though SCM requires 25 queries per dataset while CMF needs only 7. The methods were evaluated on four instruction-tuned LLMs (Mistral 7B, Bloomz, Phi-3-mini, GPT-4) using BoolQ, FakeQ, and Olympic2024 datasets, showing CMF's superior performance in both attribution accuracy and computational efficiency.

## Key Results
- CMF attribution scores of 0.63 vs SCM's 0.48 for BoolQ, demonstrating superior performance
- CMF runtime of 77-94 minutes vs SCM's 352-384 minutes, showing 4-5x computational efficiency
- Attribution values increase during fine-tuning and decrease after unlearning, validating the methods' sensitivity to dataset influence
- Both methods successfully distinguish between seen and unseen datasets, with CMF showing superior granularity

## Why This Works (Mechanism)
The methods work by leveraging in-context learning to measure how retrieved context from training datasets influences LLM outputs. SCM directly measures similarity differences when context is present versus absent, capturing the marginal contribution of each dataset. CMF takes a more sophisticated approach by modeling the attribution problem as a matrix factorization, which allows it to handle the combinatorial complexity of multiple datasets while being more robust to noise in the retrieval process. The alternating least squares optimization in CMF effectively separates the contribution of each dataset even when contexts overlap or retrieval is imperfect.

## Foundational Learning
**Shapley Value Attribution** - Why needed: Provides a principled way to attribute contributions when multiple datasets influence model outputs; quick check: Verify the Shapley formula correctly sums to 1 across all datasets for each query
**RAG Context Retrieval** - Why needed: Enables efficient retrieval of relevant passages from large training datasets without full model fine-tuning; quick check: Confirm retrieved contexts are semantically relevant to the queries
**Matrix Factorization** - Why needed: Allows CMF to decompose complex attribution patterns into interpretable dataset contributions; quick check: Verify convergence of alternating least squares algorithm
**Similarity Functions** - Why needed: Quantifies the relationship between model outputs with and without context; quick check: Test similarity function on known similar and dissimilar outputs
**Mixture Distribution Modeling** - Why needed: CMF uses mixture distributions to represent the combination of dataset influences; quick check: Verify mixture components correspond to individual dataset contributions

## Architecture Onboarding

**Component Map**: Query -> RAG Retriever -> Context Chunks -> LLM with Context -> Output Comparison -> Attribution Calculation

**Critical Path**: The attribution calculation depends on the entire pipeline working correctly, but the most critical components are the RAG retrieval (which provides the context) and the similarity calculation (which measures influence).

**Design Tradeoffs**: SCM prioritizes interpretability and direct measurement but suffers from computational inefficiency and sensitivity to retrieval noise. CMF trades some interpretability for computational efficiency and robustness, using matrix factorization to handle complex attribution patterns more gracefully.

**Failure Signatures**: Low attribution scores across all methods may indicate RAG retrieval issues or inappropriate similarity functions. CMF failing to converge suggests regularization parameters need adjustment. Runtime issues with SCM indicate computational bottlenecks that may require batching or query reduction.

**First Experiments**: 1) Test similarity calculation on controlled examples with known output differences. 2) Verify RAG retrieval returns relevant contexts by manual inspection. 3) Run attribution on a small synthetic dataset where ground truth dataset influence is known.

## Open Questions the Paper Calls Out
The paper identifies three key open questions: First, how do SCM and CMF methods scale when applied to datasets with millions of queries rather than thousands? The computational efficiency advantages of CMF were demonstrated with current dataset sizes, but performance at massive scale remains untested. Second, how can these methods be extended to open-ended text generation tasks beyond binary classification? The current methods rely on binary output comparison, which doesn't translate directly to generative tasks where outputs are variable and harder to quantify. Third, how does increasing chunk size beyond 512 tokens impact attribution stability and accuracy in RAG systems? The paper used a 512-token chunk size but didn't explore how larger context windows might affect attribution metrics, particularly for tasks requiring broader contextual understanding.

## Limitations
- Missing implementation details for critical components like similarity calculation function and alternating least squares algorithm parameters
- Lack of transparency regarding dataset composition for the five additional Wikipedia datasets used in experiments
- Significant computational resource requirements for SCM (4-5x more queries than CMF) creating barriers for independent validation
- Unclear interpretation of attribution scores without established thresholds or baseline comparisons

## Confidence
**High Confidence Claims**:
- The general framework for training dataset attribution using in-context learning is well-described
- CMF demonstrates computational efficiency advantages over SCM
- Both methods can distinguish between seen and unseen datasets

**Medium Confidence Claims**:
- CMF's superior performance in attribution accuracy (0.63 vs 0.48)
- Attribution values increasing during fine-tuning and decreasing after unlearning
- Sensitivity and granularity of CMF attribution estimation

## Next Checks
1. Implement the missing components (similarity calculation, prompt templates, alternating least squares algorithm) and test on a small controlled dataset to verify the attribution mechanism works as intended
2. Design experiments to establish meaningful attribution thresholds by testing on datasets with known contamination levels or synthetic datasets where ground truth attribution is available
3. Systematically vary RAG retrieval quality to validate CMF's claimed robustness to retrieval noise compared to SCM, measuring attribution score stability across different noise levels