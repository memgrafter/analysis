---
ver: rpa2
title: Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation
arxiv_id: '2404.09127'
source_url: https://arxiv.org/abs/2404.09127
tags:
- confidence
- calibration
- language
- agent
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of poor calibration and overconfidence
  in large language models (LLMs), particularly those fine-tuned with reinforcement
  learning from human feedback (RLHF). Existing calibration methods for LLMs focus
  on individual confidence estimation without fully leveraging the "collective wisdom"
  of multiple LLM agents interacting and improving both accuracy and calibration together.
---

# Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation

## Quick Facts
- arXiv ID: 2404.09127
- Source URL: https://arxiv.org/abs/2404.09127
- Reference count: 20
- LLMs can achieve superior calibration performance using multi-agent deliberation without training

## Executive Summary
This paper addresses the critical issue of poor calibration and overconfidence in large language models (LLMs), particularly those fine-tuned with reinforcement learning from human feedback (RLHF). The authors propose a novel post-hoc, training-free calibration strategy called Collaborative Calibration that simulates group deliberation among multiple tool-augmented LLM agents. By leveraging agent selection based on calibration performance and group deliberation with rationales and feedback, the method achieves superior calibration performance in terms of Expected Calibration Error (ECE) on four out of six tasks compared to previous training-free calibration methods.

## Method Summary
The proposed Collaborative Calibration method operates in two stages: agent selection and stance generation, followed by group deliberation with rationales and feedback. In the first stage, expert agents with different prompting strategies (Chain-of-Thought, Program-of-Thoughts, Search-Augmented Self-Ask, GENREAD) are evaluated on a validation set using an uncertainty-aware calibration score, and allocated to slots proportional to their performance. In the second stage, general agents argue for their assigned stances, provide feedback on others' arguments, and revote with adjusted confidence. The final confidence is based on a majority vote of the revised answers, achieving calibration improvement through collective wisdom without requiring model retraining.

## Key Results
- Collaborative Calibration achieved superior ECE on four out of six generative QA tasks compared to previous training-free calibration methods
- The method demonstrated significant decreases in both ECE and Brier scores after group deliberation
- Performance improvements were observed across diverse domains including arithmetic reasoning, factoid questions, and ethical reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agent selection based on calibration performance creates a diverse ensemble that balances accuracy and calibration
- Core assumption: Different prompting strategies and tool-use expertise lead to different calibration performance on different tasks
- Evidence anchors:
  - [section] "To strive for a balance between task accuracy and calibration performance, it is necessary to maintain a certain level of diversity among the agents' initial answers and reasoning paths while allocating the slots wisely so that the most suitable agents for the task can ideally become the majority"
  - [section] "As multiple skills might be relevant for an input dataset, we determine the importance ranking of each skill and accordingly allocate expert agents, based on a simple uncertainty-aware calibration score"
- Break condition: If the validation set is not representative of the test data, the agent selection may not be optimal

### Mechanism 2
- Claim: Group deliberation with rationales and feedback improves calibration by allowing agents to weigh different arguments and adjust their confidence
- Core assumption: Agents can effectively evaluate the quality of arguments and use that information to adjust their confidence
- Evidence anchors:
  - [abstract] "Our approach differs in that we incorporate self-consistency estimates, and naturally calibrate the verbalized confidence through group deliberation with peer feedback and intermediate rationales for confidence adjustment"
  - [section] "Agents give ratings and feedback to each argument in terms of logical consistency, factuality, clarity, and conciseness... Each agent is then provided with two rated arguments, one sampled from the affirmative position and one sampled from one of the opposing sides"
- Break condition: If the agents are not able to effectively evaluate the quality of arguments, the deliberation process may not lead to improved calibration

### Mechanism 3
- Claim: The combination of agent selection and group deliberation leads to better calibration than either method alone
- Core assumption: The combination of diverse agents and deliberation is more effective than either method alone
- Evidence anchors:
  - [section] "Compared with the best set of strategies reported by Xiong et al. (2023), Collaborative Calibration can elicit more calibrated confidence scores on the GSM8K and DateUnd datasets, suggesting the effectiveness of additional group interaction and rationalization"
  - [section] "We also perform ablation on the effectiveness of the group deliberation in Stage 2. Figure 4 shows the calibration performance before deliberation (i.e. output from the Stage 1 ensemble) and after, which displays a significant decrease in ECE and Brier scores"
- Break condition: If the agent selection or group deliberation processes are not effective, the combination may not lead to improved calibration

## Foundational Learning

- Concept: Calibration
  - Why needed here: The paper addresses the problem of poor calibration and overconfidence in LLMs, so understanding calibration is crucial
  - Quick check question: What is the difference between accuracy and calibration in the context of machine learning models?

- Concept: Ensemble methods
  - Why needed here: The paper uses an ensemble of agents with different prompting strategies to improve calibration
  - Quick check question: How does an ensemble method typically improve the performance of a machine learning model?

- Concept: Prompt engineering
  - Why needed here: The paper uses different prompting strategies for the expert agents, so understanding prompt engineering is important
  - Quick check question: What is the difference between zero-shot and few-shot prompting in the context of LLMs?

## Architecture Onboarding

- Component map: Agent selection -> Stance generation -> Group deliberation -> Final confidence estimation
- Critical path: Agent selection based on validation performance → Expert agents generate initial answers → General agents deliberate with feedback → Majority vote determines final confidence
- Design tradeoffs: Using more agents can improve diversity but increases computational cost; using more complex prompting strategies can improve performance but may be harder to implement
- Failure signatures: Poor calibration performance on the validation set may indicate issues with agent selection; lack of improvement in calibration after group deliberation may indicate issues with the deliberation process
- First 3 experiments:
  1. Evaluate the calibration performance of individual expert agents on the validation set
  2. Compare the calibration performance of the ensemble with and without group deliberation
  3. Analyze the effect of different numbers of agents and feedback per argument on calibration performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Collaborative Calibration change when using a larger number of agents or different agent types?
- Basis in paper: [inferred] The paper mentions that the current workflow uses 6 agents and 2 feedback per argument, and suggests that integrating more tool-use modules could further improve calibration performance
- Why unresolved: The paper only reports results for a specific configuration of 6 agents and 2 feedback per argument. The impact of using more agents or different types of agents is not explored
- What evidence would resolve it: Experiments comparing the performance of Collaborative Calibration with varying numbers of agents and different agent types, keeping other parameters constant

### Open Question 2
- Question: How does the performance of Collaborative Calibration compare to human deliberation on the same tasks?
- Basis in paper: [inferred] The paper draws inspiration from human group deliberation and proposes a method that simulates this process. However, it does not compare the performance of its method to actual human deliberation
- Why unresolved: While the paper demonstrates that Collaborative Calibration can improve calibration performance compared to previous methods, it is unclear how this performance compares to that of humans deliberating on the same tasks
- What evidence would resolve it: A comparison of the calibration performance of Collaborative Calibration to that of humans deliberating on the same tasks, using the same metrics (ECE and Brier score)

### Open Question 3
- Question: How does the performance of Collaborative Calibration change when applied to different types of tasks or domains?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of Collaborative Calibration on free-form QA tasks across various domains, including arithmetic reasoning, factoid and knowledge-intensive questions, reasoning under ambiguity, symbolic reasoning, and questions requiring ethical knowledge
- Why unresolved: While the paper shows that Collaborative Calibration can improve calibration performance on a range of tasks, it is unclear how the method would perform on other types of tasks or in different domains
- What evidence would resolve it: Experiments applying Collaborative Calibration to a diverse set of tasks and domains not covered in the paper, and comparing its performance to that of previous methods

## Limitations
- The paper lacks transparency in specific implementation details, particularly prompt templates and calibration score computation
- The approach requires multiple LLM calls, which may limit practical deployment due to computational costs
- Performance improvements are demonstrated on specific QA tasks but generalizability to other domains remains untested

## Confidence
- **High confidence** in the overall methodology design and experimental results showing improved calibration performance
- **Medium confidence** in the generalizability of results due to the specific agent configurations and datasets tested
- **Medium confidence** in the effectiveness of group deliberation, as the ablation study shows improvement but doesn't isolate the contribution of each deliberation component

## Next Checks
1. Implement the full pipeline using the provided pseudocode and validate calibration performance on a held-out test set to confirm the reported ECE and Brier Score improvements
2. Conduct ablation studies to isolate the contribution of agent selection versus group deliberation by testing configurations with (a) agent selection only, (b) random agent assignment with deliberation, and (c) the full proposed method
3. Test the approach across different LLM backbone models (beyond Mistral-7B, GPT-3.5-turbo, and Cohere-Commend) to assess robustness to model choice