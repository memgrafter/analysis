---
ver: rpa2
title: 'On the Calibration of Epistemic Uncertainty: Principles, Paradoxes and Conflictual
  Loss'
arxiv_id: '2407.12211'
source_url: https://arxiv.org/abs/2407.12211
tags:
- e-01
- e-02
- e-03
- uncertainty
- epistemic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the calibration of epistemic uncertainty
  in deep learning models, including Deep Ensembles, Bayesian Deep Networks, and Evidential
  Deep Networks. The authors identify two key principles that epistemic uncertainty
  should satisfy: it should decrease with more training data (data-related principle)
  and increase with model complexity (model-related principle).'
---

# On the Calibration of Epistemic Uncertainty: Principles, Paradoxes and Conflictual Loss

## Quick Facts
- arXiv ID: 2407.12211
- Source URL: https://arxiv.org/abs/2407.12211
- Reference count: 40
- Primary result: Conflictual loss restores epistemic uncertainty calibration principles in deep ensembles without sacrificing performance

## Executive Summary
This paper investigates the calibration of epistemic uncertainty in deep learning models, identifying two fundamental principles that epistemic uncertainty should satisfy: it must decrease with more training data and increase with model complexity. Through extensive experiments on MNIST, SVHN, and CIFAR10 datasets, the authors demonstrate that current state-of-the-art methods including Deep Ensembles, Bayesian Deep Networks, and Evidential Deep Networks often violate these principles, showing paradoxical behavior where epistemic uncertainty decreases as model complexity increases. To address this issue, they propose a new regularization technique called "conflictual loss" for deep ensembles that encourages diversity among ensemble members by slightly favoring different classes for each model. The method restores both principles of epistemic uncertainty without sacrificing model performance or calibration.

## Method Summary
The method introduces conflictual loss as a regularization term for deep ensembles. The approach trains C × k models where C is the number of classes and k is the ensemble order. Each model receives a slightly perturbed version of the standard cross-entropy loss that encourages predictions to favor different classes. The total loss combines standard cross-entropy with the conflictual regularization term weighted by hyperparameter λ (empirically set to 0.05). The ensemble members are trained independently, and at inference, predictions are averaged across all models while epistemic uncertainty is measured using mutual information between outputs and parameters.

## Key Results
- Standard deep ensembles violate the model-related principle, showing epistemic uncertainty decreases as model complexity increases
- Conflictual loss restores both epistemic uncertainty principles across all tested datasets and model sizes
- The method achieves superior or comparable results to existing approaches in accuracy, Brier score, and out-of-distribution detection
- Conflictual deep ensembles maintain high epistemic uncertainty even with large models, unlike standard methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conflictual loss restores both epistemic uncertainty principles by encouraging diversity among ensemble members, preventing epistemic uncertainty collapse in larger models
- Mechanism: The regularization term adds discordance to output distributions by slightly favoring different classes for each ensemble model, maintaining high epistemic uncertainty as model complexity increases
- Core assumption: Epistemic uncertainty is maximized when ensemble members produce diverse predictions, which can be maintained through targeted regularization
- Evidence anchors: [abstract], [section], [corpus] (weak evidence)
- Break condition: If regularization becomes too strong, it could overwhelm the primary learning objective or create too much contradiction between ensemble members

### Mechanism 2
- Claim: Mutual information metric satisfies the data-related principle in expectation due to independence of IID samples
- Mechanism: Adding new training samples reduces uncertainty about model parameters, decreasing mutual information between output and parameters in expectation
- Core assumption: Samples are independent and identically distributed, allowing mathematical proof that expected mutual information decreases with more data
- Evidence anchors: [section], [abstract], [corpus] (weak evidence)
- Break condition: If samples are not truly IID or model posterior approximation is poor enough that theoretical expectation no longer holds

### Mechanism 3
- Claim: Model-related principle is violated because increased complexity leads to more consistent (less diverse) predictions
- Mechanism: Larger models converge to similar local optima, producing less diverse predictions across ensemble members and thus lower epistemic uncertainty
- Core assumption: Larger models have more parameters that converge to similar solutions, reducing diversity needed for high epistemic uncertainty
- Evidence anchors: [abstract], [section], [corpus] (moderate evidence)
- Break condition: If model architectures explicitly maintain diversity or optimization landscapes change such that larger models explore more diverse parameter regions

## Foundational Learning

- Concept: Mutual information as epistemic uncertainty metric
  - Why needed here: The paper uses mutual information to quantify epistemic uncertainty, essential for understanding conflictual loss mechanism
  - Quick check question: Why does mutual information capture epistemic uncertainty better than variance alone?

- Concept: Bayesian inference and posterior distributions
  - Why needed here: The paper discusses how models approximate posterior distributions and how this affects epistemic uncertainty calibration
  - Quick check question: What's the difference between aleatoric and epistemic uncertainty in Bayesian terms?

- Concept: Deep ensembles and ensemble diversity
  - Why needed here: Conflictual loss specifically targets deep ensembles, so understanding how ensembles work and why diversity matters is crucial
  - Quick check question: Why do standard deep ensembles sometimes fail to maintain sufficient diversity?

## Architecture Onboarding

- Component map: Data → MLP (2 hidden layers, dropout) → Ensemble (C × k models) → Conflictual loss regularization → Combined loss → Independent training per model → Inference (average predictions)

- Critical path:
  1. Initialize ensemble with C × k models
  2. For each training example, compute both standard loss and conflictual loss
  3. Backpropagate combined loss independently for each model
  4. At inference, average predictions across ensemble and compute epistemic uncertainty

- Design tradeoffs:
  - Parameter count: C × k models vs single model
  - Training time: Independent training vs joint training
  - Hyperparameter sensitivity: Conflictual loss weight λ vs standard hyperparameters
  - Memory usage: Storing multiple models vs single model

- Failure signatures:
  - Epistemic uncertainty decreases with model size (violation of principle 2)
  - Epistemic uncertainty doesn't decrease with more data (violation of principle 1)
  - Accuracy drops significantly after adding conflictual loss
  - Calibration metrics (Brier score, SCE) worsen

- First 3 experiments:
  1. Baseline: Train standard deep ensemble on MNIST with varying model sizes, measure epistemic uncertainty trends
  2. Implementation: Add conflictual loss to deep ensemble, verify both principles are satisfied
  3. Ablation: Remove conflictual loss component by component to identify which aspects are most critical

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do larger models paradoxically show lower epistemic uncertainty despite having greater expressive power?
- Basis in paper: [explicit] The paper demonstrates experimentally that all methods but Conflictual DE contradict the model-related principle
- Why unresolved: While the authors propose this is due to poor posterior approximation, they don't provide mechanistic explanation for why this specific paradox occurs
- What evidence would resolve it: Controlled experiments varying architecture, training procedures, and posterior approximation methods while systematically measuring epistemic uncertainty

### Open Question 2
- Question: Does the conflictual loss principle generalize to non-ensemble models like Bayesian Neural Networks?
- Basis in paper: [inferred] The authors state nothing prevents application to BNNs but acknowledge this remains to be demonstrated
- Why unresolved: Conflictual loss was designed for deep ensembles where models can be easily grouped by class preference; applying to BNNs requires novel approaches
- What evidence would resolve it: Implementation and evaluation on BNN architectures comparing epistemic uncertainty calibration against standard BNNs and deep ensembles

### Open Question 3
- Question: How does the hyperparameter λ in conflictual loss affect the tradeoff between epistemic uncertainty calibration and model performance?
- Basis in paper: [explicit] The authors empirically fixed λ to 0.05 without exploring sensitivity to this parameter
- Why unresolved: While the paper shows conflictual DE performs well with λ=0.05, there's no analysis of how varying this parameter affects either principles or performance metrics
- What evidence would resolve it: Systematic sensitivity analysis varying λ across multiple orders of magnitude while measuring compliance with both principles, accuracy, calibration metrics, and OOD detection performance

## Limitations
- The method focuses exclusively on classification tasks with categorical outputs and may not extend to regression or structured prediction
- Computational overhead of training C × k models could be prohibitive for large-scale applications
- Effectiveness is demonstrated primarily on relatively simple architectures (MLPs) and datasets, with limited validation on complex models like ResNets or ViTs

## Confidence
- High confidence: Data-related principle (Principle 1) and its mathematical derivation follows directly from mutual information properties under IID assumptions
- Medium confidence: Experimental evidence showing violation of model-related principle (Principle 2) in standard methods, though theoretical explanation remains heuristic
- Low confidence: Universality of conflictual loss solution across different architectures, datasets, and task types beyond classification

## Next Checks
1. Test conflictual loss on convolutional architectures (ResNet-18/50) and vision transformers on ImageNet to verify scalability
2. Evaluate performance on regression tasks with continuous outputs to assess generalizability beyond classification
3. Measure actual diversity metrics (prediction disagreement, Jensen-Shannon divergence) between ensemble members with and without conflictual loss to quantify its effect