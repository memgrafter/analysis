---
ver: rpa2
title: Revisiting Character-level Adversarial Attacks for Language Models
arxiv_id: '2405.04346'
source_url: https://arxiv.org/abs/2405.04346
tags:
- charmer
- adversarial
- attacks
- attack
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work revisits character-level adversarial attacks in NLP,
  addressing the challenge of efficiently generating imperceptible adversarial examples
  for language models. The authors introduce Charmer, a query-based attack that improves
  upon previous methods by greedily selecting character perturbations based on a novel
  position subset selection strategy.
---

# Revisiting Character-level Adversarial Attacks for Language Models

## Quick Facts
- arXiv ID: 2405.04346
- Source URL: https://arxiv.org/abs/2405.04346
- Reference count: 40
- Primary result: Charmer achieves ASR exceeding 95% on BERT and Llama 2 while maintaining high sentence similarity

## Executive Summary
This paper introduces Charmer, a query-based character-level adversarial attack for language models that addresses the computational inefficiency of previous methods. Charmer improves attack success rates by greedily selecting the most impactful character perturbations while maintaining semantic similarity through character-level edits rather than token replacements. The method demonstrates state-of-the-art performance on both small models (BERT) and large language models (Llama 2), achieving up to 23 percentage points higher ASR than previous approaches while reducing Levenshtein distance and runtime.

## Method Summary
Charmer is a greedy character-level adversarial attack that iteratively identifies and applies the most impactful character perturbation to generate adversarial examples. The method uses a position subset selection strategy to reduce computational cost by evaluating only the top n positions rather than all possible positions. For each iteration, Charmer tests all possible single-character changes at the selected positions and applies the one that maximizes the Carlini-Wagner loss. The attack continues until the adversarial example is misclassified or a maximum Levenshtein distance is reached, operating within a black-box query-based framework that doesn't require gradient access.

## Key Results
- Charmer achieves ASR exceeding 95% on BERT and Llama 2 models
- Improves ASR by up to 23 percentage points compared to state-of-the-art attacks
- Reduces Levenshtein distance by 2-3 points compared to baseline methods
- Demonstrates effectiveness against robust word recognition defenses
- Successfully applies to jailbreaking large language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Charmer achieves high attack success rate by greedily selecting the most impactful character perturbation at each iteration.
- Mechanism: The attack iteratively identifies the top positions to modify based on loss impact, then evaluates all possible single-character changes at those positions to select the one that maximizes loss.
- Core assumption: The most impactful single-character change at each step will lead to an adversarial example in few iterations.
- Evidence anchors:
  - [abstract]: "Our method successfully targets both small (BERT) and large (Llama 2) models. Specifically, on BERT with SST-2, Charmer improves the ASR in 4.84% points and the USE similarity in 8% points with respect to the previous art."
  - [section 4.2]: "To search the closest sentence in Levenshtein distance that produces a misclassification, we iteratively solve Eq. (BP) with k = 1 until the adversarial sentence S′ is misclassified."
  - [corpus]: Weak evidence - no direct corpus support for greedy iteration strategy.
- Break condition: If the most impactful single-character change does not consistently move toward misclassification, the greedy approach may fail or require many iterations.

### Mechanism 2
- Claim: Position subset selection reduces computational cost while maintaining attack effectiveness.
- Mechanism: Instead of evaluating all possible character positions, Charmer pre-selects a subset of n positions using a loss-based heuristic, then only considers perturbations at those positions.
- Core assumption: The most important positions for attack can be identified with few forward passes, and restricting to these positions doesn't significantly reduce attack success.
- Evidence anchors:
  - [section 4.1]: "We propose considering a subset of n locations in order to remove the dependency on the length of the sentence. To select the top n locations, we propose testing the relevance of each position by replacing each character with a 'test' character and looking at the change in the loss."
  - [section 5.1]: "In Fig. 2, we can observe that the ASR consistently grows when increasing the number of candidate positions. However, the increase is less noticeable for n > 20, therefore, the increase in runtime does not pay off the increase in ASR."
  - [corpus]: Weak evidence - limited corpus validation of position selection strategy.
- Break condition: If important attack positions are consistently excluded from the subset, the attack may fail or require more iterations.

### Mechanism 3
- Claim: Charmer maintains high semantic similarity by restricting perturbations to character-level changes rather than token-level replacements.
- Mechanism: By operating at the character level with Levenshtein distance constraints, Charmer can make subtle modifications that preserve overall meaning while still causing misclassification.
- Core assumption: Character-level perturbations can be imperceptible to humans while still being effective against the model.
- Evidence anchors:
  - [abstract]: "While character-level attacks easily maintain semantics, they have received less attention as they cannot easily adopt popular gradient-based methods, and are thought to be easy to defend."
  - [section 1]: "Adversarial attacks in Natural Language Process-ing apply perturbations in the character or token levels. Token-level attacks, gaining prominence for their use of gradient-based methods, are susceptible to altering sentence semantics, leading to invalid adversarial examples."
  - [corpus]: Weak evidence - limited corpus validation of semantic preservation.
- Break condition: If character-level perturbations are detected by robust word recognition defenses, the attack may fail.

## Foundational Learning

- Concept: Levenshtein distance and edit operations
  - Why needed here: Charmer uses Levenshtein distance as the constraint metric and relies on understanding character insertions, deletions, and replacements.
  - Quick check question: What is the Levenshtein distance between "hello" and "helo"?

- Concept: Greedy optimization and position selection heuristics
  - Why needed here: Charmer employs a greedy strategy to select the most impactful character changes and uses a heuristic to pre-select positions.
  - Quick check question: How does the position selection algorithm (Algorithm 1) determine which positions are most important?

- Concept: Query-based black-box attacks vs. gradient-based white-box attacks
  - Why needed here: Charmer is a query-based attack that doesn't require gradient access, unlike many token-level attacks.
  - Quick check question: What are the advantages and disadvantages of query-based attacks compared to gradient-based attacks?

## Architecture Onboarding

- Component map: Position Selection Module -> Perturbation Generator -> Loss Evaluator -> Iterative Controller -> Tokenizer/Embedding Interface
- Critical path: Position Selection → Perturbation Generation → Loss Evaluation → Best Perturbation Selection → Iteration
- Design tradeoffs:
  - Position subset size (n): Larger n increases attack success but also computational cost
  - Maximum Levenshtein distance (k): Larger k allows more drastic changes but may reduce imperceptibility
  - Test character selection: The choice of test character affects position selection accuracy
  - Batch size for loss evaluation: Larger batches improve efficiency but require more memory
- Failure signatures:
  - ASR plateaus below acceptable threshold
  - Runtime increases exponentially with sentence length
  - Generated examples have high Levenshtein distance but low semantic similarity
  - Attack fails against robust word recognition defenses
- First 3 experiments:
  1. Validate position selection heuristic: Compare ASR with different n values on a small dataset
  2. Benchmark against baseline attacks: Measure ASR, Levenshtein distance, and similarity on TextAttack models
  3. Test against defenses: Evaluate attack success when robust word recognition is applied

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adversarial training be effectively used to defend against character-level adversarial attacks without compromising clean accuracy?
- Basis in paper: [explicit] The paper discusses adversarial training as a defense mechanism, noting that while it improves robustness against token-level attacks, it hinders character-level robustness and clean accuracy. The authors suggest that adversarial training might not be the best approach for character-level robustness.
- Why unresolved: The paper indicates that adversarial training based on token-level attacks like TextGrad hinders character-level robustness and clean accuracy, while Charmer improves character-level robustness without affecting token-level robustness. However, it remains unclear whether a more tailored adversarial training approach could balance both character-level robustness and clean accuracy.
- What evidence would resolve it: Experiments comparing different adversarial training strategies specifically designed for character-level robustness, evaluating their impact on both clean accuracy and character-level adversarial success rates.

### Open Question 2
- Question: What is the effectiveness of character-level adversarial attacks in the transferability setup across different models?
- Basis in paper: [explicit] The paper mentions that both TextFooler and Charmer fail to produce high ASR in the transfer attack setup, with the highest transfer ASR being 55.48% for Charmer in the MNLI-m dataset. It suggests that improving the ASR in the transfer attack setup is an interesting avenue.
- Why unresolved: The paper highlights the challenge of achieving high transferability in character-level attacks but does not explore specific strategies to enhance transferability. Understanding the factors that affect transferability and developing methods to improve it remains an open question.
- What evidence would resolve it: Comparative studies of different attack strategies and their transferability across various models, identifying key factors that influence transferability and proposing methods to enhance it.

### Open Question 3
- Question: How effective are robust word recognition defenses against character-level adversarial attacks when the attack constraints are relaxed?
- Basis in paper: [explicit] The paper demonstrates that robust word recognition defenses are only effective when the Pruthi-Jones Constraints (PJC) are assumed. Relaxing any of the constraints (LowEng, End, Start) leads to a significant increase in ASR, indicating the fragility of these defenses.
- Why unresolved: The paper shows that robust word recognition defenses are fragile when constraints are relaxed, but it does not explore alternative defense mechanisms or more robust constraints that could withstand such relaxed attacks.
- What evidence would resolve it: Experiments testing new defense mechanisms or constraints that maintain robustness even when certain attack constraints are relaxed, evaluating their effectiveness against character-level adversarial attacks.

## Limitations

- Performance vs. computation tradeoff requires careful tuning of position subset size for each use case
- Black-box query efficiency claims lack quantitative comparison against gradient-based methods
- Defense robustness claims are not supported by detailed evaluation methodology

## Confidence

**High confidence**: The core algorithmic approach of greedy character perturbation with position subset selection is well-specified and the experimental methodology for measuring ASR, Levenshtein distance, and semantic similarity is clearly defined.

**Medium confidence**: The claims about runtime improvements and comparison with state-of-the-art attacks are supported by experimental results, but the generalizability of these improvements across different model architectures and datasets requires further validation.

**Low confidence**: The claims about defense evasion and the specific advantages in query-limited black-box scenarios lack sufficient experimental backing.

## Next Checks

**Check 1**: Validate position selection sensitivity by running Charmer with varying n values (5, 10, 20, 30) on a small dataset subset and measuring the ASR-runtime tradeoff curve. This will determine if the reported 35% runtime improvement comes at a significant cost to attack success and help establish practical guidelines for parameter selection.

**Check 2**: Implement a comprehensive query efficiency benchmark comparing Charmer against a gradient-based white-box attack on the same models and datasets. Measure total queries required for successful attacks, successful attack rate under query limits, and success rate as a function of available queries to quantify the true cost of the black-box approach.

**Check 3**: Design and execute a defense robustness test suite using multiple character-level defense strategies including robust word recognition, spell-checking mechanisms, and human evaluation of generated adversarial examples. Measure Charmer's success rate against each defense and analyze which types of perturbations are most likely to be detected or corrected by different defense approaches.