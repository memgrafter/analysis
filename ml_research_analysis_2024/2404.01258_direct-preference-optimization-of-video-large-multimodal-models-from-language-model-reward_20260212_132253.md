---
ver: rpa2
title: Direct Preference Optimization of Video Large Multimodal Models from Language
  Model Reward
arxiv_id: '2404.01258'
source_url: https://arxiv.org/abs/2404.01258
tags:
- video
- arxiv
- gpt-4v
- preprint
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for improving the performance of
  video large multimodal models (LMMs) on video question-answering tasks through direct
  preference optimization (DPO) guided by language model rewards. The authors address
  the challenge of aligning LMMs with human preferences, particularly in detecting
  hallucinations in generated responses, by developing a cost-effective reward mechanism
  that uses detailed video captions as a proxy for video content.
---

# Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward

## Quick Facts
- arXiv ID: 2404.01258
- Source URL: https://arxiv.org/abs/2404.01258
- Authors: Ruohong Zhang; Liangke Gui; Zhiqing Sun; Yihao Feng; Keyang Xu; Yuanhan Zhang; Di Fu; Chunyuan Li; Alexander Hauptmann; Yonatan Bisk; Yiming Yang
- Reference count: 38
- Key outcome: Achieves 8.1% accuracy improvement on video QA tasks through DPO guided by language model rewards

## Executive Summary
This paper introduces a novel method for improving video large multimodal models (LMMs) on video question-answering tasks through direct preference optimization (DPO) guided by language model rewards. The authors address the challenge of aligning LMMs with human preferences, particularly in detecting hallucinations in generated responses, by developing a cost-effective reward mechanism that uses detailed video captions as a proxy for video content. They create a comprehensive video caption dataset (SHARE GPTVIDEO) comprising 900k captions generated by GPT-4V and demonstrate that applying this tailored reward through DPO significantly improves video LMM performance on video QA tasks.

## Method Summary
The method involves three key stages: First, generate detailed video captions using GPT-4V from 900k videos to create the SHARE GPTVIDEO dataset. Second, fine-tune the Video-LLaVA model on image instruction data (600k samples) and generated video instruction data (240k samples) through supervised fine-tuning. Third, apply direct preference optimization using language model feedback as reward, where ChatGPT evaluates candidate responses using the generated captions as evidence to create preference pairs. The DPO-trained model achieves an 8.1% accuracy improvement over the SFT counterpart on video QA benchmarks.

## Key Results
- 8.1% accuracy improvement on video QA tasks over supervised fine-tuning baseline
- Creation of SHARE GPTVIDEO dataset with 900k detailed video captions
- Strong alignment between language model reward mechanism and GPT-4V evaluation scores
- State-of-the-art performance on video QA benchmarks (MSVD-QA, MSRVTT-QA, TGIF-QA)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using detailed video captions as a proxy for video frames enables language models to reliably assess the factuality of LMM-generated responses.
- Mechanism: The caption provides a text-based representation of the video content, which the language model can parse and compare against the generated response to detect hallucinations.
- Core assumption: Detailed captions contain sufficient information to represent the video content for evaluation purposes.
- Evidence anchors:
  - [abstract] "Our approach demonstrates robust alignment with OpenAI GPT-4V model's reward mechanism, which directly takes video frames as input."
  - [section 3.3] "We propose the use of detailed video captions as a proxy for video content, enabling a language model analyze video content and assess the accuracy of an LMM's response to a related question and determine the presence of hallucinations."
  - [corpus] Weak - no direct evidence in corpus, but related works on mDPO and DA-DPO suggest multimodal DPO is an active research area.
- Break condition: If captions omit critical visual details or contain inaccuracies, the language model cannot reliably detect hallucinations.

### Mechanism 2
- Claim: Applying DPO with the language model reward improves video LMM performance on video QA tasks.
- Mechanism: The DPO objective optimizes the LMM to generate responses that receive higher scores from the language model reward, which is aligned with human preference.
- Core assumption: The language model reward accurately reflects human preference for video QA responses.
- Evidence anchors:
  - [abstract] "Furthermore, we show that applying this tailored reward through DPO significantly improves the performance of video LMMs on video QA tasks."
  - [section 3.3] "Employing this reward mechanism as the basis for DPO algorithm, we train LLaVA-HOUND-DPO that achieves an 8.1% accuracy improvement over the SFT counterpart."
  - [corpus] Weak - no direct evidence in corpus, but related works suggest DPO is effective for LLM alignment.
- Break condition: If the language model reward is poorly aligned with human preference, DPO may optimize for the wrong objective.

### Mechanism 3
- Claim: The large-scale video caption dataset (SHARE GPTVIDEO) provides high-quality training data for video LMMs.
- Mechanism: The dataset contains 900k detailed captions covering temporal dynamics, world knowledge, object attributes, and spatial relationships, enabling comprehensive video understanding.
- Core assumption: GPT-4V-generated captions are detailed and accurate enough to capture video content.
- Evidence anchors:
  - [abstract] "We develop a large-scale, detailed video caption dataset, covering a wide array of content."
  - [section 3.1] "We propose the use of detailed video captions as a proxy for video content, enabling a language model analyze video content and assess the accuracy of an LMM's response to a related question and determine the presence of hallucinations."
  - [corpus] Weak - no direct evidence in corpus, but the dataset size (900k) suggests comprehensive coverage.
- Break condition: If GPT-4V generates low-quality or incomplete captions, the dataset cannot effectively train video LMMs.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is used to optimize the video LMM to generate responses that align with human preference, as determined by the language model reward.
  - Quick check question: How does DPO differ from other preference optimization methods like RLHF?

- Concept: Video Question Answering (Video QA)
  - Why needed here: The proposed method aims to improve video LMM performance on video QA tasks, which requires understanding video content and generating accurate responses.
  - Quick check question: What are the key challenges in video QA compared to image QA or text-based QA?

- Concept: Multimodal Large Language Models (LMMs)
  - Why needed here: LMMs are the target models for improvement, as they can process both visual and textual information to answer video-related questions.
  - Quick check question: How do LMMs differ from traditional vision-language models or text-only LLMs?

## Architecture Onboarding

- Component map: Video input → Video frame extraction → Caption generation (GPT-4V) → Detailed caption dataset → Video QA instruction data → SFT training → Video LMM (LLaVA-HOUND) → Caption + Question + Response → Language model reward generation → DPO training → DPO-trained model → Improved video QA performance

- Critical path: Caption generation → SFT training → Language model reward generation → DPO training → Performance evaluation

- Design tradeoffs:
  - Using GPT-4V for caption generation provides high-quality captions but is expensive and slow. Using a smaller language model could reduce cost but may compromise caption quality.
  - Using a large video caption dataset (900k) enables comprehensive training but requires significant computational resources. Using a smaller dataset could reduce training time but may limit model performance.

- Failure signatures:
  - Low-quality captions lead to poor language model reward generation and suboptimal DPO training.
  - Insufficient video QA instruction data results in poor SFT model performance and limited improvement from DPO.
  - Poorly aligned language model reward causes DPO to optimize for the wrong objective, leading to decreased performance.

- First 3 experiments:
  1. Generate detailed captions for a subset of videos using GPT-4V and manually evaluate their quality.
  2. Train the SFT model on a small video QA instruction dataset and evaluate its performance on a held-out test set.
  3. Apply DPO using the language model reward on a small preference dataset and compare the performance of the DPO-trained model against the SFT model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed cost-effective reward mechanism based on video captions generalize to domains beyond video question answering, such as video captioning or video retrieval?
- Basis in paper: [explicit] The authors mention that the video caption dataset can serve as a foundational resource for LMM model training and research, facilitating advancements in video understanding tasks. They also demonstrate the video captioning ability of models trained on the distilled data.
- Why unresolved: The paper primarily focuses on the application of the reward mechanism to video question answering tasks. The generalization to other video understanding tasks is not explicitly evaluated or discussed.
- What evidence would resolve it: Experiments applying the reward mechanism to other video understanding tasks, such as video captioning or video retrieval, and comparing the performance to existing methods or baselines.

### Open Question 2
- Question: How does the proposed reward mechanism perform when evaluated by humans, and how does it compare to human preference data in terms of quality and cost-effectiveness?
- Basis in paper: [inferred] The authors mention that human preference data is valuable but challenging to scale due to its cost and labor-intensive nature. They propose a cost-effective alternative using video captions as proxies.
- Why unresolved: The paper does not directly compare the proposed reward mechanism to human preference data in terms of quality or cost-effectiveness. It only mentions the challenges associated with human preference data.
- What evidence would resolve it: A comparison of the proposed reward mechanism to human preference data in terms of quality (e.g., alignment with human preferences) and cost-effectiveness (e.g., cost per data point).

### Open Question 3
- Question: How sensitive is the performance of the DPO model to the choice of hyperparameters, such as the learning rate and the number of training epochs?
- Basis in paper: [explicit] The authors mention that they use a learning rate of 5e-7 and train the DPO model for 3 epochs. They also provide a figure showing the test set accuracy of the DPO model with respect to the number of training epochs.
- Why unresolved: The paper does not explore the impact of different hyperparameter choices on the performance of the DPO model. It only presents results using a specific set of hyperparameters.
- What evidence would resolve it: Experiments varying the learning rate and the number of training epochs, and analyzing the impact on the performance of the DPO model.

## Limitations

- The method's effectiveness depends heavily on the quality and comprehensiveness of GPT-4V-generated captions, which may not capture all visual details present in video frames.
- Computational and cost limitations of using large language models for reward generation may hinder real-world scalability and rapid inference requirements.
- The paper lacks extensive human evaluation to validate the alignment between the language model reward and actual human preferences across diverse video content.

## Confidence

**High Confidence**: The claim that DPO with language model rewards improves video LMM performance on video QA tasks is well-supported by the reported 8.1% accuracy improvement over the SFT baseline and the detailed experimental methodology described.

**Medium Confidence**: The assertion that the SHARE GPTVIDEO dataset provides sufficient coverage and quality for training video LMMs. While the dataset size (900k) is impressive, the paper lacks detailed analysis of caption quality, coverage of diverse video content, and potential biases in GPT-4V-generated descriptions.

**Medium Confidence**: The claim that detailed video captions enable reliable hallucination detection. The paper shows alignment with GPT-4V evaluation but does not provide extensive human evaluation or ablation studies demonstrating the effectiveness of caption-based hallucination detection across different video types and question complexities.

## Next Checks

1. **Caption Quality Analysis**: Conduct a systematic evaluation of caption quality across different video categories, measuring coverage of visual details, temporal dynamics, and spatial relationships. Compare against human-generated captions for a subset of videos to establish quality benchmarks.

2. **Cross-Modal Alignment Study**: Perform a detailed analysis comparing the language model reward scores with human preferences across diverse video QA examples, particularly focusing on cases where the model might hallucinate information not present in the captions but potentially visible in video frames.

3. **Scalability and Cost Analysis**: Measure the computational cost and inference time of the complete pipeline (caption generation + language model reward + DPO training) at scale, and evaluate whether the performance improvements justify the increased resource requirements compared to frame-based evaluation methods.