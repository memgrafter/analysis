---
ver: rpa2
title: Preference-Based Abstract Argumentation for Case-Based Reasoning (with Appendix)
arxiv_id: '2408.00108'
source_url: https://arxiv.org/abs/2408.00108
tags:
- case
- cases
- preferences
- outcome
- aa-cbr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Preference-Based Abstract Argumentation for
  Case-Based Reasoning (AA-CBR-P), a novel approach that allows users to define multiple
  comparison methods for cases with an ordering specifying preferences over these
  methods. The authors prove that the model inherently follows these preferences when
  making predictions and demonstrate that previous abstract argumentation for case-based
  reasoning approaches are insufficient at expressing preferences over constituents
  of an argument.
---

# Preference-Based Abstract Argumentation for Case-Based Reasoning (with Appendix)

## Quick Facts
- arXiv ID: 2408.00108
- Source URL: https://arxiv.org/abs/2408.00108
- Authors: Adam Gould; Guilherme Paulino-Passos; Seema Dadhania; Matthew Williams; Francesca Toni
- Reference count: 24
- Primary result: AA-CBR-P outperforms interpretable baselines on a brain tumor dataset by incorporating user-defined preferences over comparison methods

## Executive Summary
This paper introduces Preference-Based Abstract Argumentation for Case-Based Reasoning (AA-CBR-P), a novel approach that allows users to define multiple comparison methods for cases with an ordering specifying preferences over these methods. The authors prove that the model inherently follows these preferences when making predictions and demonstrate that previous abstract argumentation for case-based reasoning approaches are insufficient at expressing preferences over constituents of an argument. The approach is evaluated on a real-world medical dataset from a clinical trial on patients with primary brain tumors, showing empirical improvements over other interpretable machine learning models.

## Method Summary
AA-CBR-P extends abstract argumentation-based case-based reasoning by allowing users to define multiple partial orders for comparing cases, with preferences over these orders specified lexicographically. The method constructs an argumentation framework where attacks occur by the most preferred order possible, and the grounded extension determines the outcome. The approach inherently follows user preferences by construction and generalizes previous AA-CBR formulations. Evaluation uses a dataset of 31 brain tumor patients with 110 data points, extracting 10 binary features from PRO questionnaires and 10 from accelerometer data.

## Key Results
- AA-CBR-P outperforms interpretable baselines (decision tree, kNN) on a brain tumor dataset with accuracy improvements of 8-10 percentage points
- The model successfully incorporates user preferences over comparison methods while maintaining the nearest cases property
- Theoretical proofs demonstrate that AA-CBR-P correctly implements lexicographic preferences and generalizes previous AA-CBR formulations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model inherently respects user-defined preferences when making predictions by construction.
- Mechanism: The lexicographic application of multiple partial orders ensures that attacks always occur by the most preferred order possible, and preferred cases determine the outcome when they agree.
- Core assumption: The sequence of preorders is correctly ordered by user preference strength.
- Evidence anchors:
  - [abstract] "We prove that the model inherently follows these preferences when making predictions"
  - [section] "By Definitions 5 and 6, we have that if there exists an attack from α to β, this attack must occur by the most preferred order of P possible."
  - [corpus] Weak - no direct evidence in corpus papers about preference-following by construction.
- Break condition: If the preorders are not properly ordered by preference strength, the lexicographic application will not respect user intentions.

### Mechanism 2
- Claim: Nearest cases property holds for AA-CBR-P, ensuring robust classification.
- Mechanism: The constraint that attackers must be smaller by all orders in P prevents irrelevant cases from blocking nearest cases, preserving the nearest cases property.
- Core assumption: The casebase is coherent (no two cases with identical characterisations but different outcomes).
- Evidence anchors:
  - [section] "The constraint xα ≽[1:n] xγ in (ii) in Definition 6 ensures that Theorem 2 holds."
  - [section] "We can thus conclude that every argument in the grounded set, G, is either N or of the form (xβ, y)."
  - [corpus] Weak - no direct evidence in corpus papers about nearest cases property.
- Break condition: If the casebase is incoherent, the argumentation framework may contain cycles and the nearest cases property may not hold.

### Mechanism 3
- Claim: AA-CBR-P can capture previous AA-CBR formulations as special cases.
- Mechanism: By setting P = ⟨≽⟩ with a single partial order, AA-CBR-P reduces to the original AA-CBR definition.
- Core assumption: The partial order ≽ used in AA-CBR is compatible with the preorder structure in AA-CBR-P.
- Evidence anchors:
  - [section] "Moreover, by substituting P = ⟨≽⟩, we can precisely capture AA-CBR as described in Definition 2 by AA-CBR-P."
  - [section] "As a result, our method effectively generalises both of these previous approaches whilst ensuring the nearest case property holds in all circumstances."
  - [corpus] Weak - no direct evidence in corpus papers about capturing previous formulations.
- Break condition: If the partial order used in AA-CBR does not satisfy the preorder requirements (reflexivity and transitivity), the reduction may not hold.

## Foundational Learning

- Concept: Abstract argumentation frameworks and Dung semantics
  - Why needed here: AA-CBR-P is built on abstract argumentation, using the grounded extension to determine outcomes.
  - Quick check question: What is the grounded extension in an abstract argumentation framework, and how is it computed?

- Concept: Lexicographic ordering and its application to multiple preorders
  - Why needed here: Preferences over comparison methods are implemented using a lexicographic strategy on multiple preorders.
  - Quick check question: How does lexicographic ordering work when comparing elements across multiple preorders?

- Concept: Case-based reasoning and its integration with abstract argumentation
  - Why needed here: AA-CBR-P is a case-based reasoning model that uses abstract argumentation to make predictions based on past cases.
  - Quick check question: How does case-based reasoning differ from other machine learning approaches, and what are its advantages?

## Architecture Onboarding

- Component map: Casebase -> Partial orders -> Argumentation framework -> Grounded extension -> Prediction
- Critical path:
  1. Receive new case and casebase.
  2. Define partial orders and their ordering by preference.
  3. Construct argumentation framework using AA-CBR-P rules.
  4. Compute grounded extension.
  5. Assign outcome to new case based on grounded extension.
- Design tradeoffs:
  - Flexibility vs. complexity: AA-CBR-P allows for multiple partial orders and preferences, but this increases the complexity of the model.
  - Interpretability vs. performance: AA-CBR-P aims to be interpretable, but the addition of preferences may impact performance compared to black-box models.
- Failure signatures:
  - Inconsistent preferences: If the partial orders are not properly ordered by preference strength, the model may not respect user intentions.
  - Incoherent casebase: If the casebase contains cases with identical characterisations but different outcomes, the argumentation framework may contain cycles and the nearest cases property may not hold.
  - Overfitting: If the model is too closely tailored to the training data, it may not generalize well to new cases.
- First 3 experiments:
  1. Verify that AA-CBR-P reduces to AA-CBR when using a single partial order.
  2. Test the nearest cases property by constructing a casebase where nearest cases agree on the outcome.
  3. Evaluate the impact of preferences on model performance using a real-world dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AA-CBR-P compare to other interpretable machine learning models when applied to different medical datasets beyond brain tumor progression?
- Basis in paper: [explicit] The paper evaluates AA-CBR-P on a brain tumor dataset and shows it outperforms other interpretable models like decision trees and kNN.
- Why unresolved: The paper only evaluates on one medical dataset, so the generalizability of the results to other medical contexts is unknown.
- What evidence would resolve it: Empirical evaluation of AA-CBR-P on multiple diverse medical datasets with varying characteristics and outcomes.

### Open Question 2
- Question: Can the preference orderings in AA-CBR-P be automatically learned or optimized to further improve model performance?
- Basis in paper: [inferred] The paper mentions that the preference orderings are user-defined, but does not explore methods for automatically deriving them.
- Why unresolved: The paper focuses on demonstrating the utility of incorporating user-defined preferences, but does not investigate automated approaches for determining optimal preference orderings.
- What evidence would resolve it: Development and evaluation of methods for automatically learning or optimizing the preference orderings in AA-CBR-P to maximize model performance on a given dataset.

### Open Question 3
- Question: How does the inclusion of preferences in AA-CBR-P affect the interpretability and explainability of the model's predictions?
- Basis in paper: [explicit] The paper emphasizes the importance of interpretability in high-stakes decision-making, particularly in healthcare, and claims that AA-CBR-P allows for domain-specific knowledge to guide the model.
- Why unresolved: While the paper demonstrates that preferences can be incorporated into AA-CBR-P, it does not thoroughly explore how this affects the model's interpretability or the nature of the explanations provided.
- What evidence would resolve it: Analysis of the explanations generated by AA-CBR-P with and without preferences, and an assessment of their interpretability and usefulness to domain experts in making decisions.

## Limitations

- The medical dataset evaluation is based on only 31 patients, raising questions about statistical significance and generalizability.
- Binary feature extraction from PRO questionnaires and PA accelerometer data lacks detail on potential information loss from discretization.
- The nearest cases property may not hold in practice when casebases contain near-duplicates with conflicting outcomes.

## Confidence

- High confidence that the lexicographic preference mechanism correctly implements user-defined preferences when preorders are properly ordered.
- Medium confidence regarding the practical utility of this approach, as improvements over interpretable baselines are shown but not compared against state-of-the-art black-box models.
- Low confidence in the generalizability of results without empirical validation across diverse preference configurations and datasets.

## Next Checks

1. Test AA-CBR-P across synthetic datasets with known preference structures to verify the lexicographic ordering mechanism consistently respects user-defined preferences.
2. Conduct sensitivity analysis on the casebase coherence requirement by systematically introducing near-duplicate cases with conflicting outcomes.
3. Replicate the medical dataset experiments with 5-fold cross-validation to establish statistical significance of the performance improvements.