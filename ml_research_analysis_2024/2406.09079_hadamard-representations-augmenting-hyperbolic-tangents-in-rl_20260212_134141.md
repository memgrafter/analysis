---
ver: rpa2
title: 'Hadamard Representations: Augmenting Hyperbolic Tangents in RL'
arxiv_id: '2406.09079'
source_url: https://arxiv.org/abs/2406.09079
tags:
- score
- hadamard
- hyperbolic
- relu
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the dying neuron problem in reinforcement
  learning, particularly with continuously differentiable activations like hyperbolic
  tangent. The authors demonstrate that hyperbolic tangents suffer from dying neurons
  in RL, leading to unintended biases in subsequent layers.
---

# Hadamard Representations: Augmenting Hyperbolic Tangents in RL

## Quick Facts
- **arXiv ID**: 2406.09079
- **Source URL**: https://arxiv.org/abs/2406.09079
- **Reference count**: 40
- **Primary result**: HR(tanh) achieves faster learning and higher scores than tanh or ReLU in Atari RL benchmarks

## Executive Summary
This paper addresses the dying neuron problem in reinforcement learning, where hyperbolic tangent activations saturate and become inactive, creating hidden biases in subsequent layers. The authors propose a Hadamard representation (HR) that augments hidden layers by taking the Hadamard product of two parallel, independently parameterized activation layers. This approach reduces dead neurons and increases effective rank. Empirically, using DQN, PPO, and PQN in the Atari domain, HR with hyperbolic tangents achieves faster learning, a reduction in dead neurons, and increased effective rank, resulting in significant performance improvements over standard activations and parameter increases.

## Method Summary
The proposed method involves augmenting hidden layers by taking the Hadamard product of two parallel, independently parameterized activation layers. Specifically, for a given hidden layer, two separate linear transformations are applied to the input, each followed by a hyperbolic tangent activation. The outputs of these two activations are then combined element-wise via Hadamard product to form the final layer output. The paper applies this Hadamard representation to the final hidden layer of the Nature CNN architecture in DQN, PPO, and PQN algorithms, using cleanrl and PQN implementations with hyperparameters from their respective sources.

## Key Results
- HR(tanh) reduces dead neurons compared to standard tanh across Atari environments
- HR(tanh) increases effective rank of hidden representations compared to tanh or ReLU
- HR(tanh) achieves faster learning and higher normalized scores than both tanh and ReLU baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hadamard representation reduces dying neuron probability for hyperbolic tangent.
- Mechanism: Taking the Hadamard product of two independently parameterized hyperbolic tangent activations means saturation occurs only if both components saturate simultaneously, reducing saturation probability from p to p².
- Core assumption: Activation saturation events are independent between the two parallel branches.
- Evidence anchors:
  - [section] "For the hyperbolic tangent, given that it is an asymptotic function near its saturation point, an approximate equality is considered (|αi| ̸= 1, ∀st ∈ B)."
  - [abstract] "a reduction in dead neurons and increased effective rank"
  - [corpus] "Weak correlation: corpus lacks direct mathematical proof of p² probability reduction."
- Break condition: If activation outputs are not independent due to shared input statistics, the probability reduction does not materialize.

### Mechanism 2
- Claim: Hadamard representation increases effective rank by encouraging diverse activation patterns.
- Mechanism: Parallel activations with independent parameters generate complementary activation manifolds, and their product creates a richer feature space than a single activation.
- Core assumption: Independent parameterization yields sufficiently different activation distributions.
- Evidence anchors:
  - [section] "Using DQN, PPO and PQN in the Atari domain, we show faster learning, a reduction in dead neurons and increased effective rank."
  - [abstract] "increased effective rank"
  - [corpus] "Weak support: corpus does not detail how effective rank is computed or why product increases it."
- Break condition: If both parallel branches collapse into similar patterns (e.g., due to poor initialization), rank gain is minimal.

### Mechanism 3
- Claim: Hadamard product avoids the "hidden bias" problem that occurs when hyperbolic tangents die.
- Mechanism: When a hyperbolic tangent saturates, the corresponding weights become constant biases. A Hadamard product preserves non-zero gradients if at least one branch is not saturated, preventing unintended bias accumulation.
- Core assumption: Product saturation requires both components to saturate.
- Evidence anchors:
  - [section] "In the case of the hyperbolic tangent, product saturation only occurs if strictly both neurons are saturated."
  - [abstract] "reduction in dead neurons"
  - [corpus] "Missing: corpus does not address bias propagation in dying neurons."
- Break condition: If both branches saturate early in training, the bias problem re-emerges.

## Foundational Learning

- Concept: Dying neuron problem in deep networks
  - Why needed here: Understanding why hyperbolic tangents fail in RL requires knowing how saturation leads to dead neurons and downstream biases.
  - Quick check question: What happens to the gradient of a neuron that is always saturated at ±1 during backpropagation?

- Concept: Effective rank as a measure of representation expressiveness
  - Why needed here: The paper claims HR increases effective rank; grasping this metric is key to interpreting results.
  - Quick check question: How does effective rank differ from simple dimensionality of a hidden layer?

- Concept: Hadamard (element-wise) product of vectors
  - Why needed here: HR is defined as the Hadamard product of two parallel activation vectors.
  - Quick check question: If one vector has zeros and the other does not, what is the resulting Hadamard product?

## Architecture Onboarding

- Component map:
  - Input -> Linear1(input_dim, output_dim) -> Tanh -> A
  - Input -> Linear2(input_dim, output_dim) -> Tanh -> B
  - A * B (element-wise) -> Next layer

- Critical path:
  1. Forward pass: Two parallel linear transforms → two tanh activations → element-wise product → next layer.
  2. Backward pass: Gradients flow through both branches independently, multiplied by the non-saturated branch's output.

- Design tradeoffs:
  - Pros: Reduces dying neurons for tanh, increases effective rank, maintains smooth gradients.
  - Cons: Doubles parameter count for each augmented layer, may increase memory and compute.

- Failure signatures:
  - If both tanh branches saturate early, HR behaves like a standard tanh with hidden bias issues.
  - If both branches produce similar outputs, rank gain is negligible and performance matches baseline.

- First 3 experiments:
  1. Replace final hidden layer tanh with HR(tanh) in DQN on Breakout; compare dying neuron fraction vs baseline.
  2. Measure effective rank of HR(tanh) vs tanh vs ReLU hidden representations on Atari states.
  3. Compare learning curves (median human-normalized score) for HR(tanh), tanh, ReLU, and tanh+layer-norm in DQN.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Hadamard representation scale with deeper networks beyond the shallow architecture tested?
- Basis in paper: [explicit] The paper notes that additional tests using deep Tanh (HR) networks gave similar function approximation as compared to the shallow Tanh (HR) network, but does not explore scaling to very deep architectures.
- Why unresolved: The paper only tests shallow networks and provides limited evidence on how the Hadamard representation performs as network depth increases significantly.
- What evidence would resolve it: Empirical results comparing Tanh (HR) against standard activations across multiple depths (e.g., 3, 6, 9+ layers) on benchmark tasks would clarify scaling behavior.

### Open Question 2
- Question: What is the theoretical explanation for ReLU's resilience to dying neurons and low effective rank compared to continuously differentiable activations?
- Basis in paper: [explicit] The paper notes that ReLU's performance is less correlated to its low effective rank and high amount of dying neurons, and provides a partial explanation that ReLU saturation represents network pruning while hyperbolic tangent saturation leads to biases in subsequent layers.
- Why unresolved: The paper offers only a partial explanation and acknowledges that the exact reasons for ReLU's broad success are not fully understood.
- What evidence would resolve it: A rigorous theoretical analysis of how ReLU's sparse representations contribute to learning dynamics and final performance compared to dense representations would clarify this phenomenon.

### Open Question 3
- Question: How does the Hadamard representation interact with other architectural innovations like normalization layers, residual connections, or attention mechanisms?
- Basis in paper: [inferred] The paper briefly mentions that layer normalization reduces dead hyperbolic tangents but does not systematically explore interactions with other architectural components, and suggests future work on integrating HR into more complex algorithms like Rainbow or Impala.
- Why unresolved: The paper focuses on the core Hadamard representation concept and does not explore synergies or conflicts with other architectural innovations.
- What evidence would resolve it: Empirical studies combining Hadamard representations with various architectural innovations (layer normalization, residual connections, attention) across multiple tasks would reveal interaction effects.

## Limitations

- Independence assumption between parallel tanh branches lacks empirical validation
- Effective rank calculation methodology is not fully specified
- Dead neuron threshold of 20 is arbitrary and may not generalize

## Confidence

- **High confidence**: HR reduces dead neurons in Atari experiments; performance improvements over tanh are statistically significant
- **Medium confidence**: The p² saturation probability reduction mechanism is theoretically sound but lacks direct empirical verification
- **Low confidence**: Claims about increased effective rank contributing to better performance are not well-supported by the corpus

## Next Checks

1. Measure pairwise correlation between parallel branch outputs during training to verify independence assumption
2. Implement ablation study varying the dead neuron threshold to test robustness of dead neuron reduction claims
3. Compare effective rank computation methods (e.g., SVD-based vs. nuclear norm) to validate rank increase measurements