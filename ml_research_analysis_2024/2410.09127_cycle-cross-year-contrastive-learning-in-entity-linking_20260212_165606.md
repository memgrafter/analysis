---
ver: rpa2
title: 'CYCLE: Cross-Year Contrastive Learning in Entity-Linking'
arxiv_id: '2410.09127'
source_url: https://arxiv.org/abs/2410.09127
tags:
- entity
- graph
- entities
- uni000003ed
- uni00000358
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CYCLE introduces a novel graph contrastive learning method to address
  temporal degradation in entity linking models. The approach leverages newly added
  and removed relationships in evolving knowledge graphs as positive and negative
  samples, respectively, across different years.
---

# CYCLE: Cross-Year Contrastive Learning in Entity-Linking

## Quick Facts
- arXiv ID: 2410.09127
- Source URL: https://arxiv.org/abs/2410.09127
- Reference count: 40
- Primary result: 13.90% improvement when time gap is one year, 17.79% when gap extends to three years

## Executive Summary
CYCLE introduces a novel graph contrastive learning method to address temporal degradation in entity linking models. The approach leverages newly added and removed relationships in evolving knowledge graphs as positive and negative samples, respectively, across different years. This cross-year contrastive learning enables the model to effectively adapt entity representations to temporal changes. Experiments on the GCL-TempEL dataset demonstrate substantial improvements over state-of-the-art baselines, particularly for low-degree entities which are more vulnerable to temporal degradation due to sparse connectivity.

## Method Summary
CYCLE combines text-based entity descriptions and mention contexts with graph-based neighborhood information through a bi-encoder architecture. The model treats newly added relationships as positive samples and newly removed relationships as negative samples in a contrastive learning framework across different years. It uses a weighted combination of text-based entity linking loss and graph-based contrastive losses to jointly train the model, enabling effective adaptation to temporal changes in knowledge graph structure.

## Key Results
- 13.90% improvement in entity linking performance when time gap is one year
- 17.79% improvement when time gap extends to three years
- Particularly effective for low-degree entities, which are more vulnerable to temporal degradation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Temporal degradation in entity linking models is caused by structural shifts in KG relationships over time, which can be mitigated by explicitly modeling newly added and removed edges.
- **Mechanism:** The model treats newly added relationships between entities as positive samples and newly removed relationships as negative samples in a contrastive learning framework. This forces the embeddings to adapt to the evolving neighborhood structure, maintaining discriminative power across time.
- **Core assumption:** Newly added and removed edges encode meaningful semantic shifts that should be captured for temporal adaptation.
- **Evidence anchors:**
  - [abstract] "Our contrastive learning method treats newly added graph relationships as positive samples and newly removed ones as negative samples."
  - [section] "In 2019, Amazon (company) had three neighbors: e-commerce, e-book reader, and bookstore. By 2022, while e-commerce continued as a neighbor, e-book reader and bookstore (negative samples) were replaced by digital streaming and cloud computing (positive samples)."
- **Break condition:** If the addition/removal of edges does not correspond to meaningful semantic change, the contrastive signal may be noise rather than signal.

### Mechanism 2
- **Claim:** Low-degree entities are more vulnerable to temporal degradation because they rely on fewer connections, making their embeddings unstable when neighbors change.
- **Mechanism:** By using contrastive learning on positive and negative samples derived from temporal edge changes, the model explicitly strengthens the embedding of low-degree nodes, reducing their susceptibility to semantic drift.
- **Core assumption:** Low-degree nodes are inherently more sensitive to changes in their immediate graph neighborhood than high-degree nodes.
- **Evidence anchors:**
  - [abstract] "Further analysis shows that CYCLE is particularly robust for low-degree entities, which are less resistant to temporal degradation due to their sparse connectivity."
  - [section] "Concretely, a change in the meaning of even one neighboring node can significantly impact the low-degree node's representation."
- **Break condition:** If low-degree nodes can rely on global structural patterns rather than local neighborhood, the contrastive approach may be unnecessary.

### Mechanism 3
- **Claim:** Combining text-based entity descriptions and mention contexts with graph-based neighborhood information yields more temporally stable embeddings than either modality alone.
- **Mechanism:** The bi-encoder module maps mention and entity text into dense vectors, while the graph embedding module processes relation and feature graphs. Joint training via a weighted loss function integrates both views, ensuring that temporal changes in structure are reflected in both text and graph representations.
- **Core assumption:** Text and graph modalities are complementary sources of temporal information.
- **Evidence anchors:**
  - [section] "CYCLE also leverages textual information (see left part of Figure 4) composed of mention contexts and entity descriptions."
  - [section] "The final objective function is a weighted sum of the individual ð¿ð‘’, ð¿ð‘Ÿ, and ð¿ð‘“ loss functions calculated above."
- **Break condition:** If one modality dominates or is noisier, the weighted sum may overfit to the stronger signal, weakening overall performance.

## Foundational Learning

- **Concept:** Contrastive learning and the InfoNCE loss
  - **Why needed here:** The core innovation relies on pulling positive samples (new edges) closer and pushing negative samples (removed edges) apart in embedding space across years.
  - **Quick check question:** What does the InfoNCE loss maximize in the context of graph contrastive learning?

- **Concept:** Temporal knowledge graphs and snapshot-based evolution
  - **Why needed here:** The method explicitly models KG changes across discrete yearly snapshots to capture temporal degradation.
  - **Quick check question:** How does a temporal KG differ structurally from a static KG in terms of edge sets across time?

- **Concept:** Attention-based graph aggregation (GAT-style)
  - **Why needed here:** The model aggregates neighbor information for low-degree entities using attention, ensuring adaptive weighting of contextual nodes.
  - **Quick check question:** Why might attention be preferable to simple averaging when aggregating neighbor features in a sparse graph?

## Architecture Onboarding

- **Component map:**
  - Input mention and entity text -> BERT-based bi-encoder -> dense vectors
  - Input graphs and features -> GAT-like graph encoder -> embeddings
  - Compute contrastive losses using temporal positive/negative samples
  - Combine losses -> backpropagate -> update model

- **Critical path:**
  1. Input mention and entity text â†’ bi-encoder â†’ entity scores
  2. Input graphs and features â†’ graph encoder â†’ embeddings
  3. Compute contrastive losses using temporal positive/negative samples
  4. Combine losses â†’ backpropagate â†’ update model

- **Design tradeoffs:**
  - Text vs Graph weight balance: Over-weighting text may ignore temporal graph structure; over-weighting graph may ignore textual context
  - Sampling strategy for low-degree nodes: Must balance coverage vs overfitting when positive samples are scarce
  - Yearly snapshot granularity: Coarser granularity reduces temporal sensitivity; finer granularity increases data sparsity

- **Failure signatures:**
  - Performance plateaus on high-degree nodes but not low-degree nodes â†’ graph encoder not learning meaningful patterns
  - Degradation worsens with time gap â†’ contrastive learning not capturing temporal dynamics
  - Model overfits to training year â†’ loss weights too high or contrastive samples too narrow

- **First 3 experiments:**
  1. Ablation on contrastive losses: Train with only text loss vs all three losses to quantify temporal gain
  2. Low-degree node isolation: Filter test set to entities with degree â‰¤ 3, compare recall@1 improvement
  3. Temporal gap sensitivity: Train on 2019, test on 2020, 2021, 2022 separately to map performance decay curve

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CYCLE change when using different pre-trained language models (e.g., RoBERTa, DeBERTa) instead of BERT for entity description embeddings?
- Basis in paper: [inferred] The paper uses bert-base-uncased for embedding entity descriptions and mentions, but does not explore other pre-trained models.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the contrastive learning approach rather than optimizing the choice of language model.
- What evidence would resolve it: Conducting experiments using different pre-trained language models and comparing their impact on the recall@1, recall@4, and recall@8 metrics for the GCL-TempEL dataset.

### Open Question 2
- Question: What is the impact of varying the number of positive and negative samples on the model's performance, especially for low-degree entities?
- Basis in paper: [explicit] The paper mentions that a predefined threshold is used for the number of positive samples, but the specific number is not provided.
- Why unresolved: The paper does not explore the sensitivity of the model's performance to the number of positive and negative samples.
- What evidence would resolve it: Conducting experiments with different numbers of positive and negative samples and analyzing their effect on the recall metrics for low-degree entities.

### Open Question 3
- Question: How does the model's performance change when considering multi-hop relationships in the graph instead of only direct neighbors?
- Basis in paper: [inferred] The paper uses direct neighbors for constructing positive and negative samples but does not explore multi-hop relationships.
- Why unresolved: The paper focuses on the immediate temporal changes in relationships rather than exploring the broader graph structure.
- What evidence would resolve it: Implementing a modified version of CYCLE that considers multi-hop relationships and comparing its performance to the original model on the GCL-TempEL dataset.

## Limitations
- Performance gains may be specific to the GCL-TempEL dataset and may not generalize to other temporal entity linking scenarios
- The method relies on discrete yearly snapshots, which may not capture more granular temporal changes
- The effectiveness of the approach depends on the quality and completeness of edge change annotations

## Confidence
- Temporal improvement claims (13.90-17.79% gains): Medium confidence
- Mechanisms for how contrastive learning achieves these gains: Low confidence
- Robustness across different entity types: Medium confidence

## Next Checks
1. Verify that the positive/negative sample selection criteria are robust across different entity types and time periods
2. Test whether the performance gains persist when using alternative temporal granularities (quarterly vs yearly snapshots)
3. Validate that the improvements on low-degree entities are not achieved by over-optimizing for this subset at the expense of overall performance