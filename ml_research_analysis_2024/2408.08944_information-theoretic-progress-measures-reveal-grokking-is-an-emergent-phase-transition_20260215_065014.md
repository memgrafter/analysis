---
ver: rpa2
title: Information-Theoretic Progress Measures reveal Grokking is an Emergent Phase
  Transition
arxiv_id: '2408.08944'
source_url: https://arxiv.org/abs/2408.08944
tags:
- weight
- decay
- grokking
- synergy
- phase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates grokking\u2014a phenomenon where neural\
  \ networks suddenly generalize after prolonged memorization\u2014as an emergent\
  \ phase transition in training. The authors propose using higher-order mutual information,\
  \ specifically the O-Information measure, to quantify synergy and redundancy between\
  \ neurons during training."
---

# Information-Theoretic Progress Measures reveal Grokking is an Emergent Phase Transition

## Quick Facts
- arXiv ID: 2408.08944
- Source URL: https://arxiv.org/abs/2408.08944
- Reference count: 26
- Primary result: Grokking is an emergent phase transition driven by synergistic interactions between neurons

## Executive Summary
This paper investigates grokking—a phenomenon where neural networks suddenly generalize after prolonged memorization—as an emergent phase transition in training. The authors propose using higher-order mutual information, specifically the O-Information measure, to quantify synergy and redundancy between neurons during training. By analyzing modular addition tasks with fully connected networks, they identify three distinct training phases: Feature Learning, Emergence of a generalizing subnetwork, and Decoupling for compression. The study finds that early peaks in synergy can predict grokking, and that weight decay and weight initialization can enhance the emergence phase. Preliminary results suggest that synergistic sub-networks identified during the emergence phase may be causally related to delayed generalization.

## Method Summary
The authors analyze grokking in modular addition tasks (Zp with p=97) using a 2-layer fully connected ReLU network with 250 hidden neurons. They compute O-Information (higher-order mutual information) to quantify synergistic and redundant interactions between neuron groups during training. Features are clustered into 10 bins using agglomerative clustering, then O-Information is calculated across all combinations of 2 to 10 bins. The training dynamics are monitored through five phases: Feature Learning (initial learning), Emergence (synergy peak), Divergence (rapid generalization), Delayed Emergence (for some weight decay settings), and Decoupling (compression phase). The method tracks test accuracy, synergy, redundancy, and synergistic subnetwork size throughout training.

## Key Results
- Grokking manifests as an emergent phase transition characterized by three phases: Feature Learning, Emergence of a generalizing subnetwork, and Decoupling for compression
- Early peaks in synergy during the Emergence phase can predict whether a model will eventually grok
- Weight decay and high weight initialization can enhance the emergent phase by affecting the loss landscape
- Preliminary ablation studies suggest synergistic sub-networks identified during emergence may be causally related to delayed generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grokking occurs as an emergent phase transition driven by synergistic interactions between neurons.
- Mechanism: The O-Information measure captures higher-order interactions between neurons. During training, these interactions evolve from low synergy (independent feature learning) through a peak in synergy (emergence of a generalizing subnetwork) to decreased synergy (compression/decoupling).
- Core assumption: The O-Information measure accurately quantifies synergistic interactions between neurons and these interactions are causally related to generalization performance.
- Evidence anchors:
  - [abstract] "We attribute grokking to an emergent phase transition caused by the synergistic interactions between neurons as a whole."
  - [section 5.1] "During this phase, the Pareto front in figure 2 (right) reveals that the model rapidly trades off redundancy for synergy. At the same time, the size of the synergistic subnetwork increases, and a peak is observed in the test loss."
  - [corpus] Weak evidence - corpus contains related work on grokking but does not specifically address information-theoretic measures or synergistic interactions.

### Mechanism 2
- Claim: Weight decay and weight initialization can enhance the emergent phase by affecting the loss landscape.
- Mechanism: Weight decay regularizes the model, reducing overfitting and smoothing the loss landscape, making it easier for optimization to find regions where more features can be combined. Weight initialization affects the convexity of the loss landscape, influencing the ease of finding a solution.
- Core assumption: Weight decay and weight initialization affect the loss landscape in ways that promote synergistic interactions.
- Evidence anchors:
  - [section 5.2] "We additionally observe a finalizing phase with minimal synergy and redundancy changes after test loss convergence. We attribute this to compression of the representation which is interesting for transfer learning but is not explored in this work."
  - [section 5.3] "From Figure 4 (left) we observe that a model trained with high weight initialization without weight decay directly results in an emergent phase. However, from Figure 4 (right) we initially observe a rapid drop in redundancy followed by a delayed rapid increase in synergy. We hypothesize that this observation is due to the high convexity of the loss landscape making it easier for optimization to find a solution."
  - [corpus] No direct evidence in corpus for this specific mechanism.

### Mechanism 3
- Claim: Early peaks in synergy can predict grokking.
- Mechanism: The presence of an early synergy peak indicates that the model is beginning to form synergistic interactions, which are necessary for eventual generalization. Models that do not grok lack this early synergy peak.
- Core assumption: Early synergy peaks are predictive of eventual generalization and grokking.
- Evidence anchors:
  - [abstract] "We show that early peaks of synergy can predict if grokking occurs"
  - [section 6] "We empirically provide evidence that the synergy can predict grokking if there is a small peak early in training."
  - [corpus] No direct evidence in corpus for this specific mechanism.

## Foundational Learning

- Concept: Information theory and mutual information
  - Why needed here: The paper uses O-Information, a higher-order mutual information measure, to quantify synergistic and redundant interactions between neurons.
  - Quick check question: What is the difference between pairwise mutual information and higher-order mutual information?

- Concept: Phase transitions in complex systems
  - Why needed here: The paper interprets grokking as an emergent phase transition in neural network training, requiring understanding of how complex systems can exhibit sudden changes in behavior.
  - Quick check question: What are the key characteristics of a phase transition in complex systems?

- Concept: Neural network training dynamics and optimization
  - Why needed here: Understanding how neural networks learn and generalize, and how different hyperparameters (weight decay, initialization) affect training, is crucial for interpreting the results.
  - Quick check question: How do weight decay and weight initialization affect the optimization process and the resulting model?

## Architecture Onboarding

- Component map:
  Input layer -> Hidden layer (250 ReLU neurons) -> Output layer -> O-Information calculator -> Clustering module

- Critical path:
  1. Forward pass through network to obtain feature representations
  2. Dimensionality reduction via clustering of feature vectors
  3. O-Information calculation to quantify synergy and redundancy
  4. Training progress monitoring based on synergy/redundancy dynamics

- Design tradeoffs:
  - Model complexity vs. interpretability: Simple 2-layer network chosen for clear analysis of emergent phenomena
  - Clustering granularity vs. computational efficiency: 10 bins chosen as a balance
  - O-Information computation: Exhaustive search of combinations up to k=10 for accuracy vs. computational cost

- Failure signatures:
  - No synergy peak during training: May indicate model will not grok
  - Synergy peak without subsequent generalization: May indicate suboptimal hyperparameter settings
  - High redundancy throughout training: May indicate lack of complex feature interactions

- First 3 experiments:
  1. Reproduce baseline grokking experiment with weight decay 0.1 and 2.0
  2. Vary weight decay values (0.01, 0.1, 1.0, 10.0) to observe effects on synergy dynamics
  3. Implement early stopping based on synergy peak detection and compare generalization performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the O-Information measure reliably predict grokking in neural networks beyond the simple modular addition task studied?
- Basis in paper: [explicit] The authors observe that early synergy peaks might predict grokking, but note that "further experimentation with a variety of models and statistical tests is necessary to verify this hypothesis."
- Why unresolved: The current study only examines one task and architecture. Generalization to other tasks, network depths, and data distributions is unknown.
- What evidence would resolve it: Systematic experiments across diverse architectures (transformers, CNNs), tasks (MNIST, CIFAR, natural language), and datasets showing consistent early synergy peaks correlating with delayed generalization.

### Open Question 2
- Question: Are the synergistic sub-networks identified during the emergence phase causally responsible for generalization, or are they merely correlated with it?
- Basis in paper: [explicit] The authors state that "preliminary findings indicate that the synergistic sub-networks are causally related to generalization" but also note that "for the models with an emergent phase, the contrast between the synergistic sub-networks and its inverse is not that large" and suggest "alternative methods are needed to provide a more accurate estimate of the clusters."
- Why unresolved: The ablation study shows similar performance between synergistic sub-networks and their inverses, suggesting the causal relationship may be weaker than initially hypothesized.
- What evidence would resolve it: More sophisticated intervention studies (e.g., targeted dropout of specific synergistic neurons, controlled retraining experiments) demonstrating that disrupting the synergistic sub-network consistently impairs generalization while preserving other network capabilities.

### Open Question 3
- Question: How does the interaction between weight decay and weight initialization influence the emergence of synergistic sub-networks and the grokking phenomenon?
- Basis in paper: [explicit] The authors observe that both weight decay and high weight initialization can induce emergent phases, but the mechanisms and optimal combinations are unclear. They note that "weight decay acts as regularization either by reducing the capacity of the network, which promotes learning shared features that are more robust by reducing the number of active features and thereby decreasing overfitting, or by smoothing the loss landscape."
- Why unresolved: The relative contributions and potential synergies between weight decay and initialization are not quantified, and the authors suggest multiple mechanisms without distinguishing between them.
- What evidence would resolve it: Systematic ablation studies varying both parameters independently and in combination, along with analysis of feature representations and loss landscapes to distinguish between capacity reduction and optimization landscape effects.

## Limitations

- Technical uncertainties in O-Information measure's sensitivity to parameter choices and Gaussian copula assumptions
- Limited generalizability beyond modular addition task to more complex real-world problems
- Causal attribution challenges between synergistic interactions and emergent generalization

## Confidence

**High confidence**: The existence of three distinct training phases (Feature Learning, Emergence, Decoupling) and their characterization through O-Information measures is well-supported by empirical evidence within the modular addition task.

**Medium confidence**: The claim that early synergy peaks predict grokking is supported by preliminary evidence but requires more extensive validation across different tasks and architectures to establish robustness.

**Low confidence**: The causal relationship between synergistic sub-networks and delayed generalization, while suggested by preliminary experiments, lacks the rigorous causal analysis needed for strong claims.

## Next Checks

1. **Cross-task validation**: Apply the O-Information framework to additional grokking-prone tasks (e.g., modular subtraction, composition tasks, or small-scale vision tasks) to test generalizability of the three-phase characterization and synergy-based predictions.

2. **Ablation studies on clustering parameters**: Systematically vary the number of bins (k=5, 10, 15, 20) and clustering methods to quantify their impact on O-Information estimates and subsequent phase identification, establishing robustness to these critical hyperparameters.

3. **Controlled causal intervention experiments**: Implement targeted interventions on identified synergistic subnetworks during the emergence phase (e.g., selective weight pruning or freezing) and measure the direct impact on generalization timing, moving beyond correlation to establish causality.