---
ver: rpa2
title: Multi-Object Hallucination in Vision-Language Models
arxiv_id: '2407.06192'
source_url: https://arxiv.org/abs/2407.06192
tags:
- object
- objects
- llav
- visual
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies multi-object hallucination in vision-language
  models (LVLMs), where models generate objects not present in images when asked to
  recognize multiple objects simultaneously. The authors introduce ROPE (Recognition-based
  Object Probing Evaluation), an automated evaluation protocol using visual prompts
  to uniquely refer to objects and eliminate ambiguity.
---

# Multi-Object Hallucination in Vision-Language Models

## Quick Facts
- arXiv ID: 2407.06192
- Source URL: https://arxiv.org/abs/2407.06192
- Reference count: 40
- Multi-object hallucination occurs when LVLMs generate objects not present in images during multi-object recognition tasks

## Executive Summary
This work investigates multi-object hallucination in vision-language models (LVLMs), where models generate objects not present in images when tasked with recognizing multiple objects simultaneously. The authors introduce ROPE (Recognition-based Object Probing Evaluation), an automated evaluation protocol using visual prompts to uniquely refer to objects and eliminate ambiguity. Empirical studies on 8 LVLMs reveal that multi-object tasks substantially increase hallucination rates compared to single-object tasks, with heterogeneous object distributions leading to the most hallucinations. The findings highlight the need for more balanced object distributions, diverse annotations, and enhanced multi-object instruction in grounded LVLMs.

## Method Summary
The study uses ROPE, a recognition-based object probing evaluation protocol that employs visual prompts (red bounding boxes with white italic text on black background) to uniquely refer to objects in images. The protocol tests LVLMs on 5-object recognition tasks across four test distributions: homogeneous (all same class), heterogeneous (all different classes), adversarial (4 same + 1 different), and in-the-wild (random). Three task types are used: Default multi-object query, Student-forcing (format enforcement), and Teacher-forcing (ground truth context). The study evaluates 8 LVLMs on datasets filtered to include top 50 "thing" classes with objects having bounding box area >1% of image area.

## Key Results
- Multi-object tasks increase hallucination rates substantially compared to single-object tasks (accuracy drops from ~90% to ~40-50% for top models)
- Heterogeneous object distributions lead to significantly more hallucinations than homogeneous distributions
- Models rely on shortcuts like repeating previous answers for homogeneous queries and show positional biases in hallucination patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-object hallucination occurs because LVLMs struggle to process multiple visual inputs simultaneously, leading to increased reliance on textual shortcuts and spurious correlations.
- Mechanism: When tasked with recognizing multiple objects, LVLMs shift from grounded visual processing to pattern matching based on frequent co-occurrence of object classes in training data.
- Core assumption: The visual modality's contribution to final predictions drops below 20% during multi-object queries.
- Evidence anchors: [abstract] "L VLMs suffer more hallucinations when focusing on multiple objects compared to a single object." [section 5] "the contribution from the visual modality consistently registers below 20%"

### Mechanism 2
- Claim: Object class distribution within an image directly influences hallucination patterns through shortcut learning.
- Mechanism: LVLMs learn spurious correlations between object class distributions during training. When encountering homogeneous object distributions, models exploit the shortcut of repeating the most frequent class.
- Core assumption: Training data contains imbalanced object class distributions that LVLMs learn to exploit as shortcuts.
- Evidence anchors: [abstract] "The tested object class distribution affects hallucination behaviors, revealing that LVLMs may be following shortcuts and spurious correlations." [section 4.2] "more heterogeneous queries lead to substantially more hallucinations"

### Mechanism 3
- Claim: Query order and object salience create positional biases that affect hallucination likelihood.
- Mechanism: Objects positioned later in the query sequence or farther from image centers face higher hallucination risk due to accumulated uncertainty and center bias in training data.
- Core assumption: Training data and evaluation protocols exhibit center bias and positional ordering effects.
- Evidence anchors: [abstract] "Hallucinatory behaviors are influenced by data-specific factors, salience and frequency, and model intrinsic behaviors." [section 5.1] "We define object centrality as one minus the distance between the object's bounding box center and the image center"

## Foundational Learning

- Concept: Visual prompting with bounding box overlays
  - Why needed here: ROPE uses visual prompts to uniquely refer to objects and eliminate ambiguity from textual descriptions
  - Quick check question: How does the visual prompt format (red bounding box with white italic font on black background) help reduce ambiguity in multi-object recognition?

- Concept: Instruction following and format compliance
  - Why needed here: ROPE requires LVLMs to generate specific output formats like 'obj1: <class1>, obj2: <class2>, obj3: <class3>, obj4: <class4>, obj5: <class5>'
  - Quick check question: What are the three types of task prompts used in ROPE and how do they differ in format enforcement?

- Concept: Object salience and frequency analysis
  - Why needed here: Understanding how object salience (pixel ratio) and training frequency affect hallucination rates is crucial for the analysis
  - Quick check question: How is semantic salience calculated and why does it significantly impact model performance?

## Architecture Onboarding

- Component map: MSCOCO-Panoptic and ADE20K preprocessing → object filtering → dataset construction → prompt generation → visual overlay creation → model inference → format-compliant output parsing → evaluation

- Critical path: Data curation → prompt template selection → visual overlay generation → model inference → result parsing → hallucination factor analysis

- Design tradeoffs:
  - Automated evaluation vs. human judgment accuracy
  - Fixed object class set vs. open vocabulary flexibility
  - Visual prompting clarity vs. real-world application complexity
  - Single-object vs. multi-object query efficiency

- Failure signatures:
  - Models failing to follow output format instructions
  - Consistent repetition of first object class across all queries
  - Dramatic performance drops in heterogeneous object distributions
  - Center-biased predictions with high hallucination rates for peripheral objects

- First 3 experiments:
  1. Test a simple LVLM on homogeneous vs. heterogeneous object distributions to verify the shortcut learning hypothesis
  2. Compare single-object query performance against multi-object queries with teacher forcing to isolate format compliance issues
  3. Analyze object centrality effects by positioning objects at different distances from image center and measuring hallucination rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the presence of multiple objects of the same class affect hallucination patterns in LVLMs, and what mechanisms drive this phenomenon?
- Basis in paper: [explicit] The paper discusses the observation that LVLMs are less likely to hallucinate when tasked with recognizing the same object class multiple times (homogeneous queries) compared to heterogeneous queries.
- Why unresolved: While the paper identifies this trend, it does not delve into the underlying mechanisms that cause this behavior.
- What evidence would resolve it: Experiments isolating the effects of context and frequency, as well as ablation studies on model architectures, could provide insights into the mechanisms driving this phenomenon.

### Open Question 2
- Question: What is the impact of training salience on hallucination rates, and how can models be trained to better handle less frequent objects?
- Basis in paper: [explicit] The paper notes that LVLMs are less likely to hallucinate object classes that frequently appear in training data, suggesting a correlation between training salience and hallucination rates.
- Why unresolved: The paper does not explore how to mitigate the bias towards frequent objects or improve model performance on less frequent ones.
- What evidence would resolve it: Studies on balancing training data or introducing techniques to enhance model robustness for infrequent objects could provide solutions to this issue.

### Open Question 3
- Question: How does the order of object classes in the input prompt influence hallucination rates, and can this be leveraged to improve model performance?
- Basis in paper: [explicit] The paper observes that models are more likely to hallucinate objects that are listed early in the input prompt as candidate classes, indicating a bias towards earlier objects.
- Why unresolved: The paper does not investigate whether reordering the prompt or using alternative prompting strategies could reduce hallucination rates.
- What evidence would resolve it: Experiments testing different prompt orderings and their effects on hallucination rates could determine if this bias can be exploited to improve model performance.

## Limitations

- The study focuses on a fixed set of 50 object classes, limiting generalization to open-vocabulary scenarios
- Visual prompting with bounding boxes represents a controlled setting that differs from real-world multi-object recognition without explicit visual aids
- The automated evaluation protocol assumes format-compliant outputs, which may not reflect real-world response patterns

## Confidence

- High confidence: Multi-object tasks increase hallucination rates compared to single-object tasks
- Medium confidence: Heterogeneous object distributions lead to more hallucinations than homogeneous distributions
- Medium confidence: LVLMs rely on linguistic shortcuts over visual processing for multi-object recognition
- Low confidence: Specific mechanisms of positional bias and salience effects

## Next Checks

1. Test hallucination rates on open-vocabulary object recognition tasks without visual prompts to assess real-world applicability
2. Conduct ablation studies varying object class distributions systematically to isolate the impact of specific object co-occurrence patterns
3. Measure visual attention weights during multi-object vs. single-object queries to quantify the shift from visual to linguistic processing