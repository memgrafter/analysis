---
ver: rpa2
title: 'MoR: Mixture of Ranks for Low-Rank Adaptation Tuning'
arxiv_id: '2410.13408'
source_url: https://arxiv.org/abs/2410.13408
tags:
- lora
- rank
- performance
- arxiv
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving multi-task learning
  capabilities in large language models while maintaining parameter efficiency. The
  proposed Mixture of Ranks (MoR) method learns rank-specific information for different
  tasks based on input and efficiently integrates multi-rank information.
---

# MoR: Mixture of Ranks for Low-Rank Adaptation Tuning

## Quick Facts
- arXiv ID: 2410.13408
- Source URL: https://arxiv.org/abs/2410.13408
- Authors: Chuanyu Tang; Yilong Chen; Zhenyu Zhang; Junyuan Shang; Wenyuan Zhang; Yong Huang; Tingwen Liu
- Reference count: 7
- Primary result: 1.31% performance improvement while using only 93.93% of parameters compared to baseline methods

## Executive Summary
This paper introduces Mixture of Ranks (MoR), a parameter-efficient method for multi-task learning with large language models. MoR builds on Low-Rank Adaptation (LoRA) by introducing a mixture of experts approach that learns rank-specific information for different tasks through a shared parameter subspace and task-specific transformations. The method achieves superior performance and parameter efficiency by dynamically learning optimal combinations of expert outputs based on input, addressing the limitations of traditional LoRA approaches in multi-task scenarios.

## Method Summary
MoR replaces the FFN module in transformer-based LLMs with a mixture of ranks plugin that uses shared LoRA matrices across all tasks, with task-specific expressivity achieved through transformation vectors and a learned routing mechanism. The method trains on a subset of the Tulu-v2 dataset using LLaMA-7B as the base model, with evaluation on 11 downstream metrics including commonsense reasoning, reading comprehension, language modeling, and world knowledge tasks. Training involves freezing all base model parameters while optimizing the MoR components with specific hyperparameters (rank r=8, scaling factor α=32, 8 experts, learning rate 2×10^-4).

## Key Results
- Delivers 1.31% performance improvement over baseline methods
- Uses only 93.93% of parameters compared to baseline methods
- Achieves 98% of full fine-tuning performance with only 0.34% of parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MoR achieves parameter efficiency by learning a shared low-rank subspace that captures fundamental fine-tuning objectives, then applying task-specific transformations to adapt to multiple tasks.
- Mechanism: Instead of maintaining separate LoRA modules for each task (which causes parameter redundancy), MoR uses a single shared LoRA matrix and applies different scaling transformations (via diagonal matrices ΛA and ΛB) for each task. This allows the model to learn high-rank information through mathematical transformations of low-rank components.
- Core assumption: The shared LoRA already captures sufficient intrinsic information, and task-specific variations can be effectively represented through linear transformations rather than separate parameter sets.
- Evidence anchors:
  - [abstract] "MoR can derive high-rank information through mathematical transformations of the low-rank components"
  - [section 3.1] "Applying specific transformations to the LoRA matrix enables the model to capture more intricate information"
  - [corpus] Weak evidence - no direct citations supporting this transformation approach
- Break condition: If the shared LoRA fails to capture essential task-agnostic information, the transformation approach cannot recover it, leading to performance degradation.

### Mechanism 2
- Claim: The mixture of experts approach with learned routing allows dynamic task-specific adaptation while maintaining computational efficiency.
- Mechanism: MoR uses a router (softmax over linear projections of input) to select and combine outputs from multiple transformation vectors. This creates a sparse activation pattern similar to MoE but with shared parameters, reducing computational overhead while maintaining expressivity.
- Core assumption: Different tasks require different subspaces of the shared LoRA representation, and a learned routing mechanism can effectively identify and combine the appropriate transformations.
- Evidence anchors:
  - [abstract] "MoR assigns inputs to different mapped LoRA experts via routing and dynamically learns the optimal combination of expert outputs"
  - [section 3.2] "MoR introduces a space importance adjustment router G to weigh the effect of different vectors"
  - [corpus] Moderate evidence - similar routing concepts appear in Mixture of Routers paper
- Break condition: If the router fails to learn meaningful task distinctions or becomes stuck in suboptimal routing patterns, the method loses its advantage over simpler approaches.

### Mechanism 3
- Claim: Parameter efficiency is achieved through careful sharing of LoRA components while maintaining task-specific expressivity.
- Mechanism: By sharing the A and B matrices across all tasks and only learning the transformation vectors (λA, λB), MoR dramatically reduces the number of trainable parameters compared to maintaining separate LoRA modules for each task.
- Core assumption: Task-specific information can be effectively captured through scaling factors rather than separate parameter matrices, and the shared LoRA captures task-agnostic information effectively.
- Evidence anchors:
  - [abstract] "MoR delivers a 1.31% performance improvement while using only 93.93% of the parameters compared to baseline methods"
  - [section 3.2] "This approach dramatically reduces the training cost of multi-LoRA systems"
  - [corpus] Moderate evidence - parameter sharing concepts appear in Tied-LoRA and ALoRA papers
- Break condition: If tasks require fundamentally different parameter subspaces that cannot be captured through scaling transformations, the sharing approach will limit performance.

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA) and its mathematical foundation
  - Why needed here: Understanding how LoRA decomposes weight updates into low-rank matrices is essential for grasping why MoR can transform these representations
  - Quick check question: What is the mathematical form of LoRA weight updates and how does rank affect expressivity?

- Concept: Singular Value Decomposition (SVD) and its relationship to matrix rank
  - Why needed here: The paper references SVD to explain why higher-rank LoRA retains more information - understanding this connection is crucial for grasping the rank transformation concept
  - Quick check question: How does increasing the rank in LoRA relate to retaining singular values from the original weight matrix?

- Concept: Mixture of Experts (MoE) architecture and routing mechanisms
  - Why needed here: MoR builds on MoE concepts but with shared parameters - understanding traditional MoE helps distinguish MoR's innovations
  - Quick check question: How does the router in MoR differ from traditional MoE routers in terms of what it's routing over?

## Architecture Onboarding

- Component map: Input → Router computation → Expert transformation application → Weighted combination → Addition to base model output
- Critical path: Input → Router computation → Expert transformation application → Weighted combination → Addition to base model output
- Design tradeoffs:
  - Parameter sharing vs. task-specific expressivity
  - Computational efficiency vs. routing complexity
  - Rank size vs. overfitting risk
  - Number of experts vs. routing stability
- Failure signatures:
  - Poor routing distribution (all weight on single expert) indicates router learning issues
  - Performance plateau at low ranks suggests shared LoRA insufficient
  - Performance degradation with too many experts suggests overfitting or routing instability
- First 3 experiments:
  1. Baseline comparison: Implement vanilla LoRA with same rank and compare performance to validate the shared parameter approach
  2. Routing ablation: Replace learned router with mean pooling to quantify routing contribution
  3. Rank sensitivity: Test different shared LoRA ranks to find optimal balance between expressivity and overfitting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of performance improvement achievable with MoR compared to full fine-tuning, and how does this vary with model size and task complexity?
- Basis in paper: [inferred] The paper demonstrates MoR achieves 98% of full fine-tuning performance with only 0.34% of parameters, suggesting there may be fundamental limits to this gap that depend on model architecture and task characteristics.
- Why unresolved: The paper only tests up to 7B model size and doesn't systematically explore how the performance gap scales with model size or task complexity. The relationship between rank size, number of experts, and task difficulty remains unexplored.
- What evidence would resolve it: Systematic scaling experiments across model sizes (1B, 7B, 13B, 70B) on tasks of varying complexity, with detailed analysis of how performance gap evolves with each scaling factor.

### Open Question 2
- Question: How does MoR's routing mechanism behave with non-instruction-tuning datasets, and can it generalize to domains with different input characteristics than those used in training?
- Basis in paper: [explicit] The paper notes performance degradation on Lambda tasks not included in training data, suggesting routing may be specialized for instruction-tuning datasets.
- Why unresolved: The paper only evaluates on instruction-tuning datasets and doesn't explore how the learned routing generalizes to other domains like code generation, mathematical reasoning, or multi-modal tasks.
- What evidence would resolve it: Experiments applying MoR-trained models to diverse datasets (code, math, vision-language) and analyzing routing behavior changes across domains.

### Open Question 3
- Question: What is the optimal strategy for determining the number of experts and rank size for MoR in different multi-task learning scenarios, and can this be predicted from dataset characteristics?
- Basis in paper: [explicit] The ablation study shows performance varies with expert count and rank size, but doesn't provide a principled method for selecting these hyperparameters based on task characteristics.
- Why unresolved: The paper uses empirical search but doesn't establish theoretical or empirical guidelines for hyperparameter selection based on dataset properties like size, diversity, or task similarity.
- What evidence would resolve it: Development of a predictive model or rule-based system that maps dataset characteristics to optimal MoR configurations, validated across diverse multi-task learning scenarios.

## Limitations

- Limited experimental validation to a single base model (LLaMA-7B) and curated subset of Tulu-v2 dataset
- Modest 1.31% performance improvement that may not generalize across different model sizes or task distributions
- Lack of comparison against more recent LoRA variants like QLoRA or ALoRA that address different aspects of parameter efficiency

## Confidence

- **High confidence**: The basic mathematical formulation of MoR and its relationship to existing LoRA methods is clearly specified and internally consistent. The parameter sharing mechanism is straightforward and implementable.
- **Medium confidence**: The empirical results showing performance improvement and parameter efficiency are presented with sufficient detail for reproduction, but the limited scope of experiments and lack of comparison with newer methods reduces confidence in generalizability.
- **Low confidence**: The claims about routing effectiveness and the theoretical justification for why transformation vectors capture task-specific information are not well-supported. The paper provides limited insight into failure modes or the conditions under which MoR might underperform simpler approaches.

## Next Checks

1. **Routing Analysis**: Implement visualization tools to track router weight distributions across tasks during training, analyzing whether the router learns meaningful task distinctions or converges to degenerate patterns (e.g., all weight on single expert).
2. **Generalization Test**: Evaluate MoR across multiple model scales (LLaMA-7B, 13B, 33B) and diverse task sets to determine if the 1.31% improvement holds or if performance gains are model/task-dependent.
3. **Computational Overhead Measurement**: Profile training and inference times for MoR versus vanilla LoRA with equivalent parameter counts to verify that the routing layer doesn't negate parameter efficiency gains through increased computation.