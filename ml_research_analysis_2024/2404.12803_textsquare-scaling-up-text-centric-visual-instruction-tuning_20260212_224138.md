---
ver: rpa2
title: 'TextSquare: Scaling up Text-Centric Visual Instruction Tuning'
arxiv_id: '2404.12803'
source_url: https://arxiv.org/abs/2404.12803
tags:
- arxiv
- data
- text-centric
- reasoning
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents TextSquare, a large-scale text-centric visual
  instruction-tuning approach that addresses the performance gap between open-source
  and closed-source multimodal large language models (MLLMs). The authors construct
  Square-10M, a massive dataset of 9.1 million high-quality text-centric visual question-answering
  (VQA) pairs and reasoning context, generated through a four-step process called
  Square: Self-Questioning, Answering, Reasoning, and Evaluation.'
---

# TextSquare: Scaling up Text-Centric Visual Instruction Tuning

## Quick Facts
- arXiv ID: 2404.12803
- Source URL: https://arxiv.org/abs/2404.12803
- Authors: Jingqun Tang, Chunhui Lin, Zhen Zhao, Shu Wei, Binghong Wu, Qi Liu, Yangfan He, Kuan Lu, Hao Feng, Yang Li, Siqi Wang, Lei Liao, Wei Shi, Yuliang Liu, Hao Liu, Yuan Xie, Xiang Bai, Can Huang
- Reference count: 11
- Key outcome: TextSquare achieves 62.2% accuracy on OCRBench, outperforming open-source models and setting new state-of-the-art benchmarks for text-centric visual instruction tuning.

## Executive Summary
This paper introduces TextSquare, a large-scale text-centric visual instruction-tuning approach that addresses the performance gap between open-source and closed-source multimodal large language models (MLLMs). The authors construct Square-10M, a massive dataset of 9.1 million high-quality text-centric visual question-answering (VQA) pairs and reasoning context, generated through a four-step process called Square: Self-Questioning, Answering, Reasoning, and Evaluation. TextSquare, trained on this dataset, achieves state-of-the-art performance among open-source models, setting a new standard on OCRBench with 62.2% accuracy and outperforming previous open-source models by significant margins across multiple benchmarks.

## Method Summary
The authors develop TextSquare using a three-phase supervised fine-tuning approach with InternLM-Xcomposer2 as the backbone. The model uses a modified CLIP ViT-L-14-336 vision encoder with resolution increased to 700. The training pipeline involves three stages: first training the vision encoder at 224 resolution, then scaling to 336 resolution, and finally training at 700 resolution. The Square-10M dataset is generated through a four-step pipeline using closed-source MLLMs to create high-quality text-centric VQA pairs with reasoning context from diverse text-rich image sources including documents, charts, tables, receipts, and web images.

## Key Results
- TextSquare achieves 62.2% accuracy on OCRBench, setting new state-of-the-art among open-source models
- Demonstrates strong performance on text-centric tasks: 84.3% on DocVQA and 79.4% on ChartQA
- Shows 75.1% average accuracy across four general VQA and hallucination evaluation datasets
- Establishes a logarithmic relationship between instruction-tuning data scale and model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Square-10M's four-step generation process produces higher-quality VQA data than previous methods.
- Mechanism: The multi-stage pipeline leverages closed-source MLLMs' capabilities at each step - first generating questions based on image understanding, then answering with contextual prompting, then providing reasoning for transparency, and finally filtering through self-evaluation and consistency checks.
- Core assumption: Closed-source MLLMs like Gemini Pro have superior text-image comprehension and reasoning abilities that can be effectively orchestrated through this pipeline.
- Evidence anchors:
  - [abstract] "Square, consists of four steps: Self-Questioning, Answering, Reasoning, and Evaluation"
  - [section] "Self-Questioning. In this stage, Gemini Pro is prompted to generate profound, meaningful, and non-trivial questions about the given image"
  - [section] "Multi-Prompt Consistency. Besides direct evaluation of the generated content, we manually augment the prompt and context space in Data Generation"

### Mechanism 2
- Claim: VQA reasoning data improves both accuracy and hallucination mitigation.
- Mechanism: The reasoning step forces the model to explicitly articulate the connection between questions and visual elements, creating rich contextual information that improves in-context learning and reduces hallucination likelihood.
- Core assumption: Detailed reasoning context provides question-specific information that enhances the model's understanding beyond simple question-answer pairs.
- Evidence anchors:
  - [abstract] "Additionally, we demonstrate the critical role of VQA reasoning data in offering comprehensive contextual insights for specific questions. This not only improves accuracy but also significantly mitigates hallucinations."
  - [section] "Reasoning step entails probing the model for the reasoning behind its answers, leveraging the powerful reasoning abilities of MLLMs"

### Mechanism 3
- Claim: Large-scale high-quality data follows a logarithmic scaling relationship with model performance.
- Mechanism: As dataset size increases, model performance improves but with diminishing returns following a logarithmic pattern, where each doubling of data provides progressively smaller gains.
- Core assumption: The relationship between data scale and performance follows predictable mathematical patterns that can guide dataset construction.
- Evidence anchors:
  - [abstract] "the exponential increase of instruction tuning data volume is directly proportional to the improvement in model performance"
  - [section] "Figure 5(a)(b), the convergence loss of the model continues to decrease as the data scale grows, whereas the rate of decrease becomes progressively slower"
  - [section] "Figure 5(c)(d), it can be seen that as the instruction tuning data grows, the model performs better and better, but the rate of growth continues to slow down"

## Foundational Learning

- Concept: Multimodal instruction tuning fundamentals
  - Why needed here: Understanding how vision-language models learn from paired image-text data is essential for implementing Square-10M generation
  - Quick check question: What are the key differences between pretraining and instruction tuning in MLLMs?

- Concept: Chain-of-thought prompting and few-shot learning
  - Why needed here: These prompting techniques are used in the Answering stage to improve reliability of generated answers
  - Quick check question: How does chain-of-thought prompting differ from standard prompting in MLLMs?

- Concept: Consistency checking and filtering mechanisms
  - Why needed here: The Evaluation stage relies on multi-prompt and multi-context consistency to filter low-quality data
  - Quick check question: What metrics would you use to measure consistency between different model generations?

## Architecture Onboarding

- Component map:
  Vision Encoder -> Projector -> LLM Backbone

- Critical path:
  1. Image preprocessing and resolution scaling
  2. Vision encoder feature extraction
  3. Projector token alignment
  4. LLM processing with aligned tokens
  5. Loss computation and parameter updates

- Design tradeoffs:
  - Resolution vs. computational cost: 700 resolution chosen over higher resolutions for balance
  - Parameter count vs. performance: 8.6B parameters vs. larger models
  - Data quality vs. quantity: Square-10M prioritizes quality through filtering

- Failure signatures:
  - Vision encoder not adapting to higher resolution: Check second training stage performance
  - Projector alignment issues: Monitor cross-modal attention patterns
  - LLM degradation: Compare performance on held-out validation sets

- First 3 experiments:
  1. Test basic image encoding at 700 resolution without fine-tuning
  2. Verify projector alignment with frozen components
  3. Validate end-to-end inference pipeline with sample Square-10M data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Square-10M dataset scale with model performance beyond the tested 9.1 million instances?
- Basis in paper: [explicit] The authors observe a logarithmic relationship between instruction-tuning data scale and model performance, noting that "the exponential increase of instruction tuning data volume is directly proportional to the improvement in model performance."
- Why unresolved: The paper only tests up to 9.1 million instances, leaving the upper bounds of this scaling relationship unexplored.
- What evidence would resolve it: Training TextSquare with incrementally larger Square datasets (e.g., 20M, 50M, 100M instances) and measuring performance gains on benchmarks like OCRBench and DocVQA.

### Open Question 2
- Question: Can the Square strategy be applied to non-text-centric multimodal tasks like video understanding or 3D scene reasoning?
- Basis in paper: [inferred] The Square pipeline relies on Gemini Pro's capabilities for Self-Questioning, Answering, Reasoning, and Evaluation, which are described as "sophisticated and versatile" for text-image analysis.
- Why unresolved: The paper focuses exclusively on text-rich images, not exploring whether the Square approach generalizes to other multimodal domains.
- What evidence would resolve it: Applying Square to video datasets (e.g., HowTo100M) or 3D scene understanding tasks and evaluating performance improvements.

### Open Question 3
- Question: What is the optimal balance between VQA reasoning data and direct answer data for minimizing hallucinations without sacrificing accuracy?
- Basis in paper: [explicit] The authors demonstrate that "reasoning data is beneficial in improving model performance and mitigating hallucinations," with TextSquare scoring 75.1% average across four general VQA and hallucination evaluation datasets.
- Why unresolved: The paper does not quantify the optimal ratio of reasoning data to answer data in the Square-10M dataset.
- What evidence would resolve it: Ablation studies varying the proportion of reasoning data in Square-10M and measuring trade-offs in accuracy vs. hallucination scores on benchmarks like POPE and VizWiz.

## Limitations
- Closed-source dependency: The Square-10M dataset generation relies heavily on closed-source MLLMs (Gemini Pro), creating potential reproducibility barriers.
- Dataset quality verification: Independent verification of data quality metrics is limited despite claims of 9.1 million high-quality samples.
- Generalization claims: Performance gains may not generalize to non-text-rich visual domains or different model architectures.

## Confidence
- High Confidence: Claims about TextSquare's performance on established benchmarks (OCRBench, DocVQA, ChartQA) and the general effectiveness of large-scale instruction tuning for text-centric VQA tasks.
- Medium Confidence: The specific mechanisms of the Square generation pipeline and the exact contribution of reasoning data to hallucination mitigation, as these rely on closed-source model behavior that cannot be independently verified.
- Low Confidence: The precise logarithmic scaling relationship between dataset size and performance, and whether this pattern would hold with different model sizes or training methodologies.

## Next Checks
1. Reproduce Square-10M generation using open-source alternatives to Gemini Pro (e.g., Qwen2-VL, LLaVA-NeXT) and compare data quality metrics and downstream model performance.
2. Conduct ablation study on reasoning data by removing reasoning context from training data to quantify its specific contribution to accuracy and hallucination mitigation.
3. Evaluate TextSquare on non-text-centric visual tasks (e.g., natural image understanding, video analysis) to assess the limits of its text-centric specialization and identify potential performance degradation patterns.