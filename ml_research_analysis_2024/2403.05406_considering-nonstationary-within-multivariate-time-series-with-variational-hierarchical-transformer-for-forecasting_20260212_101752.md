---
ver: rpa2
title: Considering Nonstationary within Multivariate Time Series with Variational
  Hierarchical Transformer for Forecasting
arxiv_id: '2403.05406'
source_url: https://arxiv.org/abs/2403.05406
tags:
- series
- time
- forecasting
- non-stationary
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of non-stationarity in multivariate
  time series (MTS) forecasting by proposing the Hierarchical Time series Variational
  Transformer (HTV-Trans). The core idea is to integrate a novel Hierarchical Time
  series Probabilistic Generative Module (HTPGM) with a Transformer to capture multi-scale
  non-stationary and stochastic information within MTS, avoiding over-stationarization
  from traditional methods.
---

# Considering Nonstationary within Multivariate Time Series with Variational Hierarchical Transformer for Forecasting

## Quick Facts
- arXiv ID: 2403.05406
- Source URL: https://arxiv.org/abs/2403.05406
- Reference count: 8
- Primary result: HTV-Trans outperforms 6 SOTA methods on 7 MTS datasets, achieving MAE 0.455 on ETTh1 (720-step forecast) vs 0.600 for Autoformer

## Executive Summary
This paper tackles the challenge of non-stationarity in multivariate time series (MTS) forecasting by proposing the Hierarchical Time series Variational Transformer (HTV-Trans). The core innovation is the Hierarchical Time series Probabilistic Generative Module (HTPGM), which uses hierarchical latent variables and variational inference to recover multi-scale non-stationary patterns without over-stationarizing the data. By combining this with a transformer architecture, HTV-Trans achieves state-of-the-art performance on seven benchmark datasets, particularly excelling at long-term forecasting tasks.

## Method Summary
HTV-Trans integrates a novel Hierarchical Time series Probabilistic Generative Module (HTPGM) with a transformer architecture to address non-stationarity in MTS forecasting. The method first applies Series Stationarization (normalization) to stabilize training inputs, then uses HTPGM's hierarchical latent structure to recover multi-scale non-stationary information. These latent variables are fused with the normalized inputs and fed into a transformer encoder, whose outputs are decoded to produce forecasts. The model is trained with a combined prediction and reconstruction loss, using variational inference to balance between stationarization and expressiveness.

## Key Results
- HTV-Trans achieves MAE 0.455 on ETTh1 with 720-step forecasting, outperforming Autoformer (0.600) and Non-stationary Transformer (0.616)
- Outperforms six state-of-the-art methods across seven datasets including ETTh1, ETTh2, ETTm1, ETTm2, Illness, Weather, and Exchange-rate
- Ablation studies confirm the necessity of each component in HTV-Trans architecture
- Shows particular strength in long-term forecasting scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical latent variables enable recovery of multi-scale non-stationarity
- Mechanism: HTPGM uses a multi-layer latent hierarchy where each layer operates at different temporal granularity. Lower layers capture fine-grained, short-term dependencies; higher layers aggregate longer-term trends. This structure allows the model to reconstruct non-stationary patterns at multiple scales without over-stationarizing.
- Core assumption: Non-stationarity in MTS has hierarchical structure—short-term dynamics and long-term shifts can be modeled separately and combined.
- Evidence anchors:
  - [abstract] "Hierarchical Time series Probabilistic Generative Module (HTPGM) with a hierarchical latent variable structure and variational inference to recover intrinsic non-stationary information into temporal dependencies."
  - [section] "The time dimension of hidden variables zi from the bottom layer to the top layer is gradually reduced... the hierarchical generative process can provide fine-grained distribution information..."
- Break condition: If real-world MTS lacks meaningful multi-scale structure, the hierarchy may collapse to noise or overfit.

### Mechanism 2
- Claim: Variational inference provides robust posterior estimates that resist over-stationarization
- Mechanism: The HTPGM defines approximate posteriors q(z|x) at each layer. These posteriors are regularized by KL divergence to priors conditioned on raw input. This encourages the model to retain distributional information that would otherwise be lost in stationarization.
- Core assumption: Probabilistic latent variables can represent complex MTS distributions better than deterministic embeddings.
- Evidence anchors:
  - [abstract] "hierarchical probabilistic generative module... recovers the intrinsic non-stationary information into temporal dependencies."
  - [section] "We define a Gaussian distributed variational distribution q(zt,n) = N (µt,n, diag(σt,n)) to approximate the true posterior distribution... enables richer latent representations for HTPGM."
- Break condition: If inference network fails to approximate true posteriors, latent variables will be uninformative and training will collapse.

### Mechanism 3
- Claim: Series stationarization + hierarchical reconstruction balances stability and expressiveness
- Mechanism: Raw input is first normalized (μ, σ subtraction) to produce stable training inputs. The HTPGM reconstructs the original distribution from these normalized inputs, injecting non-stationary information back into the latent space. The transformer then consumes a fusion of normalized series and hierarchical latent variables.
- Core assumption: Stationarization + reconstruction is better than raw inputs or pure stationarization for capturing long-range dependencies.
- Evidence anchors:
  - [abstract] "recover the intrinsic non-stationary information into temporal dependencies" and "combines it with transformer for a well-defined variational generative dynamic model."
  - [section] "We adopt Series Stationarization... The Series Stationarization aims to reduce the distributional differences... HTV-Trans further develops a hierarchical generative module to capture the multi-scale statistical characteristics of the original input time series..."
- Break condition: If reconstruction error dominates, the added complexity provides no benefit over simpler stationarization.

## Foundational Learning

- Concept: Multivariate Time Series Forecasting
  - Why needed here: HTV-Trans operates on multivariate sequences; understanding cross-channel dependencies is critical to interpret HTPGM outputs.
  - Quick check question: What is the dimensionality of each time step in MTS, and how does it differ from univariate time series?

- Concept: Transformer Attention Mechanism
  - Why needed here: The encoder block in HTV-Trans uses multi-head self-attention to aggregate temporal patterns. Without grasping query-key-value mechanics, you can't debug why α affects forecasting.
  - Quick check question: How does self-attention weight different time steps, and what role does the scaling factor √dk play?

- Concept: Variational Autoencoder Basics
  - Why needed here: HTPGM is trained via ELBO maximization. Understanding the balance between reconstruction and KL regularization is key to tuning γ.
  - Quick check question: What does the KL term in ELBO do, and what happens if you set γ too high or too low?

## Architecture Onboarding

- Component map: Raw MTS -> Series Stationarization -> HTPGM -> Hierarchical Latent Variables Z -> Interpolation -> X' + Zsum -> Transformer Encoder -> MLP Decoder -> Forecast
- Critical path:
  1. Normalize raw MTS → produce X′
  2. Pass X′ through HTPGM → obtain Z at each scale
  3. Interpolate Z to match time dimension → sum into Zsum
  4. Concatenate X′ + Zsum → feed into Transformer
  5. Output H → MLP → forecast
- Design tradeoffs:
  - Hierarchical depth vs. training stability: More layers capture finer non-stationarity but risk vanishing gradients.
  - α balance: Too much Zsum dominates; too little leaves over-stationarization.
  - γ balance: Too high KL → posterior collapses; too low → overfitting.
- Failure signatures:
  - High MSE but low MAE → outliers dominate; check latent scale mismatch.
  - Forecast lags behind ground truth → attention weights stuck; try increasing α or reducing hierarchical depth.
  - Training loss diverges → latent variables unstable; reduce γ or simplify HTPGM.
- First 3 experiments:
  1. Baseline: Replace HTPGM with identity (just normalized X′) and compare MAE/MSE.
  2. Ablation: Remove α weighting; set α=1 always; observe change in forecast smoothness.
  3. Scale test: Train HTV-Trans on ETTh1 with prediction length 96 vs 720; compare MAE trends.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the hierarchical latent variable structure specifically mitigate the over-stationarization problem compared to other normalization techniques?
- Basis in paper: [explicit] The paper mentions that the hierarchical latent variable structure recovers intrinsic non-stationary information into temporal dependencies, avoiding over-stationarization from traditional methods.
- Why unresolved: The paper does not provide a detailed comparison or quantitative analysis of how the hierarchical latent variable structure performs against other normalization techniques in mitigating over-stationarization.
- What evidence would resolve it: A detailed comparative study showing the effectiveness of the hierarchical latent variable structure against other normalization techniques in mitigating over-stationarization, with quantitative metrics and visual representations.

### Open Question 2
- Question: What are the specific roles of each component in the HTV-Trans model, and how do they contribute to its overall performance?
- Basis in paper: [explicit] The paper mentions an ablation study that confirms the necessity of each component in HTV-Trans.
- Why unresolved: The paper does not provide a detailed breakdown of the specific roles of each component and how they individually contribute to the model's performance.
- What evidence would resolve it: A detailed analysis of the contribution of each component in HTV-Trans to its overall performance, including quantitative metrics and visual representations.

### Open Question 3
- Question: How does the balance between stationary and non-stationary information (parameter α) affect the model's performance across different datasets and forecasting horizons?
- Basis in paper: [explicit] The paper mentions that a parameter α is introduced to balance the effect of stationary and non-stationary information, and evaluates its influence on MTS forecasting.
- Why unresolved: The paper does not provide a comprehensive analysis of how the balance between stationary and non-stationary information affects the model's performance across different datasets and forecasting horizons.
- What evidence would resolve it: A detailed study showing the effect of the parameter α on the model's performance across different datasets and forecasting horizons, with quantitative metrics and visual representations.

## Limitations
- The hierarchical latent structure's effectiveness assumes MTS non-stationarity exhibits clear multi-scale patterns, which is plausible but not universally validated across domains
- The optimal balance between stationarization and non-stationary reconstruction (controlled by α) requires per-dataset tuning, raising questions about robustness
- Computational complexity scales with hierarchical depth, but ablation results don't quantify runtime or parameter count increases

## Confidence
- **High confidence**: The core architectural design (HTPGM + Transformer fusion) is internally consistent and ablation studies clearly demonstrate each component contributes to performance
- **Medium confidence**: The claim that hierarchical latent variables are necessary to avoid over-stationarization is supported by experiments but relies on qualitative interpretation of "intrinsic non-stationary information"
- **Low confidence**: The generalizability of results to datasets with fundamentally different non-stationary characteristics (e.g., abrupt regime shifts vs. gradual trends) is not explored

## Next Checks
1. **Ablation on hierarchical depth**: Train HTV-Trans with 1, 2, and 3 latent layers on ETTh1 and report MAE/MSE to confirm that deeper hierarchies consistently improve long-term forecasting
2. **Sensitivity to α**: Perform a grid search over α ∈ [0.1, 0.5, 0.9] on a held-out validation set and plot forecast MAE vs. α to identify optimal balance points and stability ranges
3. **Stationarity metric analysis**: Compute Augmented Dickey-Fuller (ADF) test statistics on raw vs. normalized inputs and on latent variables Z to quantify how much stationarity is removed at each stage and whether HTPGM restores it effectively