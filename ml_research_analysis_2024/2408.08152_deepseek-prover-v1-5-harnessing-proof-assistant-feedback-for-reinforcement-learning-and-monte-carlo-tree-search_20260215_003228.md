---
ver: rpa2
title: 'DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement
  Learning and Monte-Carlo Tree Search'
arxiv_id: '2408.08152'
source_url: https://arxiv.org/abs/2408.08152
tags:
- proof
- tree
- search
- deepseek-prover-v1
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepSeek-Prover-V1.5 is a formal theorem prover for Lean 4 that
  integrates supervised fine-tuning, reinforcement learning from proof assistant feedback,
  and a novel RMaxTS Monte-Carlo tree search. The model is pre-trained on DeepSeekMath-Base
  and fine-tuned on an enhanced dataset that includes tactic state annotations.
---

# DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search

## Quick Facts
- arXiv ID: 2408.08152
- Source URL: https://arxiv.org/abs/2408.08152
- Reference count: 40
- Primary result: Achieves 63.5% pass rate on miniF2F and 25.3% on ProofNet benchmarks

## Executive Summary
DeepSeek-Prover-V1.5 is a formal theorem prover for Lean 4 that integrates supervised fine-tuning, reinforcement learning from proof assistant feedback, and a novel RMaxTS Monte-Carlo tree search algorithm. The system leverages intrinsic rewards to address the sparse-reward challenge in proof search, enabling efficient exploration of the proof space. By unifying proof-step and whole-proof generation through a truncate-and-resume mechanism, it achieves state-of-the-art performance on both high school-level (miniF2F) and undergraduate-level (ProofNet) theorem proving benchmarks.

## Method Summary
DeepSeek-Prover-V1.5 builds on DeepSeekMath-Base through supervised fine-tuning on an enhanced formal theorem proving dataset that includes tactic state annotations. The system employs reinforcement learning with proof assistant feedback using the GRPO algorithm, where Lean prover verification results serve as binary rewards. The key innovation is RMaxTS, a Monte-Carlo tree search algorithm that uses intrinsic rewards to encourage exploration of diverse tactic states. The truncate-and-resume mechanism allows the model to combine the computational efficiency of whole-proof generation with the state-awareness of step-by-step methods, enabling seamless integration of intermediate tactic states.

## Key Results
- Achieves 63.5% pass rate on miniF2F benchmark (high school level)
- Achieves 25.3% pass rate on ProofNet benchmark (undergraduate level)
- Outperforms existing models in both single-pass and tree search settings
- Demonstrates significant improvements through combination of RLPAF and RMaxTS

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The truncate-and-resume mechanism enables whole-proof generation models to leverage intermediate tactic states without switching to step-by-step proof generation.
- Mechanism: When proof generation fails, the system truncates the proof at the error point, parses the successful prefix into individual tactics, and resumes search from that state. This combines computational efficiency with state-awareness.
- Core assumption: Tactic states can be reliably extracted from failed proofs and used as effective prompts for resuming generation.
- Evidence anchors:
  - [abstract] "To seamlessly integrate intermediate tactic states in proof-step generation while maintaining the simplicity and computational efficiency of whole-proof generation, we have developed a unified approach in DeepSeek-Prover-V1.5."
  - [section 3.1] "When the tree search agent expands a node, it randomly selects one tactic to use as a prompt for the language model."
- Break condition: If tactic state extraction fails or the language model cannot effectively use tactic state information in prompts.

### Mechanism 2
- Claim: RMaxTS with intrinsic rewards addresses the sparse-reward problem in proof search by encouraging exploration of diverse tactic states.
- Mechanism: The algorithm assigns maximal intrinsic rewards when new tactic states are discovered, using discounted upper confidence bounds to handle non-stationary reward signals. This drives the search to explore the proof space more thoroughly before exploiting known paths.
- Core assumption: Diverse tactic states lead to more successful proof paths, and intrinsic rewards can effectively guide exploration in the sparse-reward environment of theorem proving.
- Evidence anchors:
  - [abstract] "We present RMaxTS, an innovative Monte-Carlo tree search algorithm that leverages the RMax strategy to tackle exploration challenges in sparse-reward proof search problems."
  - [section 3.3] "To promote exploration in sparse-reward sequential decision making, one classical paradigm is constructing intrinsic rewards that encourage the agent to not only optimize extrinsic rewards but also acquire general information about the interactive environment."
- Break condition: If intrinsic rewards lead to excessive exploration without finding valid proofs, or if the discount factor tuning is ineffective.

### Mechanism 3
- Claim: Chain-of-thought (CoT) prompting significantly improves proof generation by forcing the model to plan before executing tactics.
- Mechanism: CoT mode inserts natural language reasoning steps before each tactic, aligning the model's natural language reasoning capabilities with formal proof construction. This creates more systematic and proactive mathematical thinking.
- Core assumption: Natural language reasoning capabilities can be effectively transferred to formal proof planning when explicitly prompted.
- Evidence anchors:
  - [abstract] "In contrast to natural language mathematics, where models generate detailed deduction steps to construct proofs, in Lean they often rely on a sequence of high-level tactic calls to brute-force solutions."
  - [section 2.2] "This approach successfully develops new behaviors, employing delicate mathematical thinking to guide the generation of tactics."
- Break condition: If CoT prompting becomes redundant for problems solvable by Lean's automation mechanisms, or if it slows down generation without improving success rates.

## Foundational Learning

- Concept: Monte-Carlo Tree Search (MCTS) and Upper Confidence Bounds (UCB)
  - Why needed here: MCTS provides the framework for systematic exploration of the proof space, while UCB balances exploration and exploitation during node selection.
  - Quick check question: What are the four main steps of MCTS and how does UCB determine which node to expand next?

- Concept: Reinforcement Learning from Proof Assistant Feedback (RLPAF)
  - Why needed here: RLPAF uses the binary verification results from the Lean prover as rewards to fine-tune the model, aligning it with formal specifications.
  - Quick check question: How does the Group Relative Policy Optimization (GRPO) algorithm differ from standard PPO in the context of theorem proving?

- Concept: Tactic state management in proof assistants
  - Why needed here: Understanding how tactic states change with each proof step is crucial for implementing the truncate-and-resume mechanism and extracting intermediate states.
  - Quick check question: What information does a tactic state contain and how can it be used to resume proof generation after a failure?

## Architecture Onboarding

- Component map:
  Pre-trained base model (DeepSeekMath-Base) → Formal theorem proving specialization → Supervised fine-tuning with augmented data → Reinforcement learning with Lean prover feedback → RMaxTS tree search module

- Critical path:
  1. Proof generation from theorem statement
  2. Lean prover verification
  3. Error detection and proof truncation
  4. Tactic state extraction and tree node creation
  5. Tree search with RMaxTS for exploration
  6. Backpropagation of rewards and model updates

- Design tradeoffs:
  - Whole-proof generation vs step-by-step: Computational efficiency vs state awareness
  - Intrinsic vs extrinsic rewards: Exploration breadth vs exploitation of known paths
  - CoT vs non-CoT prompting: Planning quality vs generation speed

- Failure signatures:
  - Proof generation consistently fails at certain tactic types
  - Tree search gets stuck in local optima despite intrinsic rewards
  - CoT prompting increases generation time without improving success rates
  - Tactic state extraction fails due to complex proof structures

- First 3 experiments:
  1. Test the truncate-and-resume mechanism on simple proofs with known failure points to verify tactic state extraction and resume functionality
  2. Compare RMaxTS with standard MCTS on benchmark problems to measure the impact of intrinsic rewards
  3. Evaluate CoT vs non-CoT performance on a diverse set of problems to identify which types benefit from each approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the RMaxTS exploration strategy outperform other exploration methods (e.g., curiosity-driven exploration, count-based exploration) in formal theorem proving?
- Basis in paper: [explicit] The paper introduces RMaxTS as an intrinsic-reward-driven exploration algorithm for MCTS in formal theorem proving.
- Why unresolved: The paper does not compare RMaxTS to other exploration methods.
- What evidence would resolve it: An ablation study comparing RMaxTS to other exploration methods in terms of proof success rates and sample efficiency.

### Open Question 2
- Question: Can the truncate-and-resume mechanism be extended to handle more complex proof structures, such as proofs with nested tactics or proofs that require backtracking?
- Basis in paper: [explicit] The paper introduces the truncate-and-resume mechanism for handling proof errors and resuming from intermediate tactic states.
- Why unresolved: The paper only demonstrates the mechanism on relatively simple proofs.
- What evidence would resolve it: Experiments evaluating the mechanism on a diverse set of proofs, including those with complex structures.

### Open Question 3
- Question: How does the performance of DeepSeek-Prover-V1.5 scale with the size of the model and the complexity of the theorem proving tasks?
- Basis in paper: [explicit] The paper presents results for a 7 billion parameter model on benchmarks of varying difficulty.
- Why unresolved: The paper does not explore the relationship between model size, task complexity, and performance.
- What evidence would resolve it: Experiments training and evaluating models of different sizes on theorems of increasing complexity.

## Limitations

- The enhanced formal theorem proving dataset used for fine-tuning is not fully specified, making faithful reproduction difficult
- RMaxTS algorithm implementation details and intrinsic reward mechanism parameters are insufficiently described
- Evaluation relies on only two benchmarks, which may not capture the full range of theorem proving challenges
- The effectiveness of the truncate-and-resume mechanism across diverse proof structures is not comprehensively validated

## Confidence

- **High Confidence**: The overall framework combining supervised fine-tuning, RLPAF, and RMaxTS is well-specified and benchmark results are clearly reported
- **Medium Confidence**: The truncate-and-resume mechanism and CoT prompting approaches are described clearly but lack comprehensive validation
- **Low Confidence**: The RMaxTS algorithm's specific implementation details and the enhanced dataset composition are not sufficiently specified for faithful reproduction

## Next Checks

1. **Tactic State Extraction Validation**: Implement and test the truncate-and-resume mechanism on a diverse set of proofs with known failure points to verify that tactic states can be reliably extracted and used to resume generation, including edge cases like proofs with nested tactics.

2. **RMaxTS Parameter Sensitivity Analysis**: Conduct ablation studies on the RMaxTS algorithm's key parameters (intrinsic reward scaling, discount factor, UCB exploration constant) to understand their impact on proof success rates and search efficiency, comparing against standard MCTS baselines.

3. **CoT vs Non-CoT Comparative Analysis**: Design a systematic evaluation comparing CoT and non-CoT prompting across different theorem complexity levels and types, including metrics beyond success rate such as generation time and proof length.