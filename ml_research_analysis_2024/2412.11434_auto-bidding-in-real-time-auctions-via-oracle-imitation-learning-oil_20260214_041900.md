---
ver: rpa2
title: Auto-bidding in real-time auctions via Oracle Imitation Learning (OIL)
arxiv_id: '2412.11434'
source_url: https://arxiv.org/abs/2412.11434
tags:
- slot
- bids
- oracle
- slots
- efficiency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Oracle Imitation Learning (OIL), a framework
  for training auto-bidding agents in multi-slot second-price auctions to maximize
  acquisitions while adhering to budget and CPA constraints. The key idea is to use
  an "oracle" algorithm that identifies a near-optimal combination of impression opportunities
  and slots by solving a nonlinear multiple-choice knapsack problem using future campaign
  data.
---

# Auto-bidding in real-time auctions via Oracle Imitation Learning (OIL)

## Quick Facts
- **arXiv ID**: 2412.11434
- **Source URL**: https://arxiv.org/abs/2412.11434
- **Reference count**: 40
- **Primary result**: OIL outperforms RL baselines (PPO, IQL) and linear programming methods, achieving up to 21.2% higher scores on AuctionNet datasets

## Executive Summary
This paper introduces Oracle Imitation Learning (OIL), a framework for training auto-bidding agents in multi-slot second-price auctions. The key innovation is using an oracle algorithm that solves a nonlinear multiple-choice knapsack problem to identify near-optimal bidding strategies, which are then imitated by a student network using only real-time information. OIL demonstrates superior performance compared to both online and offline reinforcement learning algorithms, achieving improved sample efficiency and maintaining near-oracle performance while adhering to budget and CPA constraints.

## Method Summary
OIL trains auto-bidding agents by first computing optimal bids using an oracle that solves a nonlinear MCKP with full campaign data, then training a student network to imitate these oracle bids using only real-time information. The method employs online training to mitigate distribution mismatch issues common in offline imitation learning. The oracle uses a greedy heuristic to rank slots by efficiency (conversion probability divided by cost) and iteratively selects the best available options while respecting budget constraints. The student network is a 3-layer fully connected network with 256 units each, trained using behavior cloning loss on 60 engineered features.

## Key Results
- OIL achieves up to 21.2% higher scores than RL baselines (PPO, IQL) on AuctionNet datasets
- OIL outperforms Online Linear Programming methods while maintaining near-oracle performance
- OIL demonstrates superior sample efficiency compared to standard RL approaches
- OIL-slot (simpler oracle) achieves better imitation results than OIL-upgrade despite oracle performance gaps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OIL works because it shifts the learning complexity from crafting sophisticated RL algorithms to efficiently solving a nonlinear constrained optimization problem (the oracle).
- Mechanism: The oracle algorithm uses a greedy heuristic to find a near-optimal set of slots by ranking them based on efficiency (conversion probability divided by cost) and iteratively selecting the best available option while respecting the budget constraint. This creates a high-quality target policy for the student network to imitate.
- Core assumption: The greedy heuristic produces a near-optimal solution to the nonlinear MCKP, and this solution can be effectively learned through imitation learning.
- Evidence anchors:
  - [abstract] "This work introduces Oracle Imitation Learning (OIL), a framework for training auto-bidding agents... The key idea is to use an 'oracle' algorithm that identifies a near-optimal combination of impression opportunities and slots by solving a nonlinear multiple-choice knapsack problem..."
  - [section 4.2] "Exact solutions to Problem 2 are computationally infeasible... Instead, we employ a greedy heuristic approximation... This ranking approach leads to an efficient utilization of the budget and controls the CPA..."
  - [corpus] Weak - related papers focus on different auto-bidding approaches (diffusion models, graph representations) but don't directly address the oracle imitation mechanism.
- Break condition: The greedy heuristic fails to find a near-optimal solution when slot efficiencies are highly variable or when the budget constraint creates complex interdependencies between slot selections.

### Mechanism 2
- Claim: OIL achieves superior sample efficiency because the oracle provides direct supervision rather than sparse reward signals.
- Mechanism: Instead of learning from delayed and sparse conversion rewards, the student network learns to mimic the oracle's bids directly. This provides dense supervision at each time step, accelerating learning.
- Core assumption: Direct imitation of optimal actions is more sample-efficient than learning from reward signals, especially in environments with sparse rewards like ad auctions.
- Evidence anchors:
  - [abstract] "Through numerical experiments, we demonstrate that OIL achieves superior performance compared to both online and offline reinforcement learning algorithms, offering improved sample efficiency."
  - [section 5] "Unlike standard BC setups, OIL features an asymmetry of information between the student and the expert: the oracle has perfect knowledge about the advertisement traffic for the entire campaign, while the student does not."
  - [corpus] Weak - related papers don't specifically address sample efficiency comparisons between imitation learning and RL in auto-bidding contexts.
- Break condition: When the oracle's decisions are highly complex and non-linear, making them difficult to learn through direct imitation, or when the environment dynamics change significantly during deployment.

### Mechanism 3
- Claim: OIL's online training strategy mitigates the distribution mismatch problem common in offline imitation learning.
- Mechanism: By collecting transitions online, OIL ensures that the training state distribution matches the deployment state distribution, preventing the agent from encountering out-of-distribution states during testing.
- Core assumption: Online data collection maintains distribution alignment between training and deployment, reducing the risk of catastrophic failure when the agent deviates from the oracle's policy.
- Evidence anchors:
  - [section 5] "OIL employs an online training strategy, i.e., the agent generates new experience by interacting with the environment rather than using a pre-collected dataset... This approach mitigates the issue highlighted by Ross et al. [28], which occurs when the agent deviates from the policies in the dataset, encountering states outside of the dataset's distribution."
  - [section 6] "To validate our online training choice, we trained OIL-slot offline with the AuctionNet dataset... its score was over 10% lower than the online version..."
  - [corpus] Weak - related papers don't specifically address the distribution mismatch problem in auto-bidding contexts.
- Break condition: When the online environment is too expensive to interact with for sufficient training data collection, or when the oracle's policy changes significantly over time, making online training impractical.

## Foundational Learning

- Concept: Multiple-Choice Knapsack Problem (MCKP)
  - Why needed here: The optimal bidding problem is formulated as an MCKP where each impression opportunity has multiple slots (choices) and the budget constraint acts as the knapsack capacity.
  - Quick check question: How does the MCKP formulation capture the constraint that an advertiser can win at most one slot per impression opportunity?

- Concept: Second-Price Auction Mechanics
  - Why needed here: Understanding how slots are allocated and prices are determined is crucial for computing the costs and expected conversions for each slot option.
  - Quick check question: In a second-price auction with D slots, what price does the advertiser who wins slot d pay?

- Concept: Cost-Per-Acquisition (CPA) Optimization
  - Why needed here: The objective function balances the number of acquisitions against the cost efficiency, penalizing solutions that exceed the target CPA.
  - Quick check question: How does the CPA penalty term in the objective function encourage the oracle to find cost-efficient bidding strategies?

## Architecture Onboarding

- Component map:
  Oracle algorithm (MCKP solver with greedy heuristic) -> Student network (3-layer fully connected with 256 units each) -> Simulator (AuctionNet-based environment with second-price auctions) -> Training loop (online imitation learning with behavior cloning loss)

- Critical path:
  1. Oracle computes optimal bids using complete campaign data
  2. Student observes real-time data and oracle's bids
  3. Student updates parameters to minimize imitation loss
  4. Student's bids advance the simulation
  5. Repeat for each time step

- Design tradeoffs:
  - Oracle complexity vs. student learning difficulty: More sophisticated oracles (oracle-upgrade) provide better solutions but are harder for students to imitate
  - Online vs. offline training: Online training provides better distribution alignment but requires more computational resources
  - Feature engineering: 60 engineered features capture campaign dynamics but increase model complexity

- Failure signatures:
  - Oracle consumes significantly less than 99% of budget (indicates suboptimal solution)
  - Student network fails to converge (check learning rate and batch size)
  - CPA constraint frequently violated (adjust oracle's efficiency ranking or objective function)

- First 3 experiments:
  1. Compare oracle-slot vs oracle-upgrade performance to validate the theoretical analysis about optimality gaps
  2. Test OIL with different learning rates (1e-4, 1e-3, 1e-2) to find optimal training stability
  3. Evaluate OIL performance on held-out test campaigns to verify generalization beyond training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can OIL be extended to handle multiple agents competing for the same slots simultaneously?
- Basis in paper: [explicit] The authors explicitly state that OIL assumes other advertisers' bids remain unchanged and mention extending to multi-agent settings as future work.
- Why unresolved: The current formulation treats other advertisers as part of the environment rather than as competing agents, and the theoretical analysis relies on this assumption.
- What evidence would resolve it: Empirical comparison showing OIL's performance degradation or improvement when multiple OIL-trained agents compete directly against each other in the same auction environment.

### Open Question 2
- Question: How does OIL perform in first-price auctions compared to second-price auctions?
- Basis in paper: [explicit] The authors note that OIL could work for first-price auctions and that the oracle algorithm remains unchanged, but state this requires validation with large-scale datasets.
- Why unresolved: All experiments were conducted on second-price auction data, and the bidding dynamics in first-price auctions differ significantly (no price discount for winning).
- What evidence would resolve it: Head-to-head comparison of OIL performance between first-price and second-price auction environments using the same dataset but with different pricing mechanisms.

### Open Question 3
- Question: What is the optimal trade-off between oracle complexity and imitation difficulty for achieving best overall performance?
- Basis in paper: [inferred] The authors observe that OIL-upgrade (more complex oracle) performs slightly better than OIL-slot (simpler oracle), but OIL-slot achieves better imitation results despite the oracle performance gap.
- Why unresolved: The experiments only compare two oracle variants, and the paper doesn't systematically explore the space of oracle complexities versus student network capabilities.
- What evidence would resolve it: Systematic ablation study varying oracle complexity while measuring both oracle performance and student imitation success, identifying the point of diminishing returns.

## Limitations
- OIL relies on an oracle requiring complete campaign data, making it impractical for real-world deployment where future information is unavailable
- The method assumes stationary environment dynamics and does not address adaptation when competitor strategies or market conditions change during a campaign
- The greedy heuristic for solving the MCKP may produce suboptimal solutions in scenarios with highly variable slot efficiencies or complex budget constraints

## Confidence

**High confidence**: OIL outperforms baselines on AuctionNet datasets (21.2% higher scores), online training provides better distribution alignment than offline training, and the greedy heuristic effectively solves the MCKP for most scenarios.

**Medium confidence**: The sample efficiency improvements over RL methods are demonstrated but could vary with different environment complexities; the CPA constraint adherence depends heavily on proper tuning of the oracle's efficiency ranking.

**Low confidence**: Real-world applicability without access to future campaign data remains unproven, and the method's robustness to non-stationary environments has not been validated.

## Next Checks

1. Test OIL's performance when deployed without oracle access, using only real-time data to simulate practical deployment scenarios.
2. Evaluate robustness to changing competitor strategies by introducing dynamic bid adjustments during training and testing.
3. Compare OIL against alternative MCKP solvers (e.g., dynamic programming approximations) to quantify the impact of the greedy heuristic on solution quality.