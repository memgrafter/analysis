---
ver: rpa2
title: Intrinsic Subgraph Generation for Interpretable Graph based Visual Question
  Answering
arxiv_id: '2403.17647'
source_url: https://arxiv.org/abs/2403.17647
tags:
- graph
- question
- methods
- nodes
- subgraph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of interpretability in deep learning-based
  visual question answering (VQA) by introducing an intrinsic subgraph generation
  method. The approach employs a Graph Attention Network (GAT) that learns to generate
  a subgraph as an explanation during the prediction process, highlighting the most
  relevant nodes for answering a given question.
---

# Intrinsic Subgraph Generation for Interpretable Graph based Visual Question Answering

## Quick Facts
- arXiv ID: 2403.17647
- Source URL: https://arxiv.org/abs/2403.17647
- Authors: Pascal Tilli; Ngoc Thang Vu
- Reference count: 0
- Primary result: Intrinsic subgraph generation via Masking Graph Attention Network (M-GAT) achieves competitive accuracy on GQA while providing interpretable explanations preferred by human evaluators.

## Executive Summary
This work addresses the challenge of interpretability in deep learning-based visual question answering (VQA) by introducing an intrinsic subgraph generation method. The approach employs a Graph Attention Network (GAT) that learns to generate a subgraph as an explanation during the prediction process, highlighting the most relevant nodes for answering a given question. The method was evaluated on the GQA dataset, demonstrating competitive performance while providing interpretable explanations. Human evaluation showed a preference for the intrinsic explanations over post-hoc methods. Quantitative metrics, including token co-occurrence analysis and subgraph removal tests, were introduced and correlated with human preferences, validating their effectiveness in assessing explanation quality.

## Method Summary
The method extends a Graph Attention Network (GAT) with a Masking GAT (M-GAT) layer that learns a hard-attention mask to generate a subgraph as explanation during prediction. The model processes questions using a transformer encoder-decoder to generate instruction vectors, which guide the M-GAT in selecting relevant nodes. A scaled dot-product attention mechanism creates a binary mask over nodes, and only nodes in this mask contribute to the final prediction. The approach is trained using AdamW optimizer with learning rate 1e-4, weight decay 1e-5, and exponential learning rate scheduler, with early stopping based on validation loss.

## Key Results
- M-GAT achieves 94.79% accuracy on GQA with top-k=0.15, matching baseline GAT performance while providing interpretable subgraphs
- Human evaluation shows intrinsic explanations are preferred over post-hoc methods in pairwise comparisons
- Proposed metrics (AT-COO, QT-COO, QA-SubG) correlate strongly with human preferences (Pearson >0.84 for AT-COO, >0.99 for QT-COO)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The intrinsic subgraph generation works by discretely sampling nodes during GAT message passing, using Implicit MLE to approximate gradients through the sampling step.
- **Mechanism**: During the final GAT layer, each node receives a score via scaled dot-product between node embeddings and instruction vectors. These scores are binarized into a hard attention mask using I-MLE, which is then used to filter edges during message passing. Only nodes in the mask contribute to the final prediction.
- **Core assumption**: The instruction vectors from the decoder contain sufficient guidance for the model to learn which nodes are most relevant for the given question.
- **Evidence anchors**:
  - [abstract] "The approach employs a Graph Attention Network (GAT) that learns to generate a subgraph as an explanation during the prediction process"
  - [section] "We extend the GAT and refer to it as Masking Graph Attention Network (M-GAT)" and describe the hard attention mask computation
  - [corpus] Weak evidence - corpus contains related papers on subgraph sampling and interpretable VQA, but none specifically describe I-MLE-based hard attention
- **Break condition**: If instruction vectors fail to capture question semantics, the mask will not highlight relevant nodes, leading to poor explanations and degraded accuracy.

### Mechanism 2
- **Claim**: The global attention aggregation guided by the question vector enables the model to focus on the subgraph's contribution to the answer prediction.
- **Mechanism**: After masking, the remaining node embeddings are weighted by a scaled dot-product with the global question vector (from instruction vectors), producing a single graph embedding that is combined with question and interaction features for final answer prediction.
- **Core assumption**: The masked subgraph contains sufficient information for accurate prediction when combined with the question representation.
- **Evidence anchors**:
  - [section] "This Qglobal...performs a scaled dot-product with X′ to obtain αi scores...resulting in a single graph embedding vector Xg"
  - [abstract] "Our model is designed to intrinsically produce a subgraph during the question-answering process as its explanation"
  - [corpus] Weak evidence - corpus has papers on attention-based VQA but none describe this specific global attention over masked subgraphs
- **Break condition**: If the subgraph is too small (e.g., top-k=0.10), the model loses too much information and accuracy drops significantly (77.88% in experiments).

### Mechanism 3
- **Claim**: The proposed metrics (answer/question token co-occurrence and subgraph removal) effectively measure explanation quality by correlating with human preferences.
- **Mechanism**: AT-COO and QT-COO measure how often answer/question tokens appear in the explanation subgraph. Subgraph removal measures accuracy drop when explanation nodes are randomized. These metrics correlate with human evaluation rankings.
- **Core assumption**: Good explanations should contain nodes corresponding to answer/question tokens and be important for prediction accuracy.
- **Evidence anchors**:
  - [abstract] "Moreover, we present quantitative metrics that correlate with the evaluations of human assessors, acting as automatic metrics for the generated explanatory subgraphs"
  - [section] "The AT-COO exhibits a strong positive Pearson correlation and a moderate Spearman correlation" and "Both Pearson and Spearman metrics for QT-COO present a very high correlation"
  - [corpus] Moderate evidence - corpus has papers on explainable GNNs but none describe these specific co-occurrence and removal metrics
- **Break condition**: If explanations are based on spurious correlations rather than true relevance, the metrics may still show high values despite poor explanations.

## Foundational Learning

- **Graph Neural Networks**: 
  - Why needed here: The model operates on scene graphs where nodes represent objects and edges represent relations, requiring message passing to learn contextualized node representations.
  - Quick check question: How does a GAT layer compute attention scores between connected nodes?

- **Attention Mechanisms**:
  - Why needed here: Both the masking mechanism (to identify relevant nodes) and global aggregation (to combine subgraph information) rely on attention scores.
  - Quick check question: What is the difference between soft attention (continuous) and hard attention (discrete) in the context of explainability?

- **Explainable AI vs. Interpretable AI**:
  - Why needed here: The work distinguishes between post-hoc explainability methods and intrinsic interpretability, which is central to the proposed approach.
  - Quick check question: What is the key difference between a model that generates explanations post-hoc versus one that is intrinsically interpretable?

## Architecture Onboarding

- **Component map**: Question Processing (Transformer Encoder-Decoder) → Scene Graph Encoder → Masking GAT (with hard attention) → Global Attention Aggregation → MLP Answer Prediction
- **Critical path**: Question tokens → instruction vectors → global question vector → M-GAT with hard attention → node attention pooling → answer prediction
- **Design tradeoffs**: Top-k hyperparameter controls explanation granularity vs. accuracy (0.15 gives best balance); using only 15% of nodes for prediction requires the model to be highly selective; I-MLE enables gradient flow through discrete sampling but may introduce approximation error
- **Failure signatures**: Low AT-COO/QT-COO values indicate explanations not capturing answer/question content; high QA-SubG values after subgraph removal indicate explanations not important for prediction; accuracy drops when top-k is too small (e.g., 77.88% at top-k=0.10) indicate insufficient information
- **First 3 experiments**:
  1. Train baseline GAT (no masking) on GQA and verify ~94.78% accuracy
  2. Implement M-GAT with top-k=0.15 and verify competitive accuracy (~94.79%) and human preference in pairwise comparisons
  3. Evaluate AT-COO, QT-COO, and QA-SubG metrics on validation set and confirm correlation with human preferences (Pearson >0.84 for AT-COO, >0.99 for QT-COO)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method change with different sizes of the extracted subgraphs, and what is the optimal size for maximizing both interpretability and accuracy?
- Basis in paper: Explicit - The paper mentions that the performance of the model is dependent on the size of the extracted subgraphs, with different top-k configurations yielding varying results. It is noted that the highest accuracy was achieved when only 15% of the nodes were actively used in making predictions.
- Why unresolved: While the paper provides results for different top-k configurations, it does not explicitly determine the optimal size of the subgraph for balancing interpretability and accuracy.
- What evidence would resolve it: Further experiments testing a wider range of subgraph sizes and analyzing the trade-off between interpretability and accuracy would provide insights into the optimal subgraph size.

### Open Question 2
- Question: How do the proposed quantitative metrics for evaluating subgraph explanations correlate with other established metrics for assessing explanation quality in XAI?
- Basis in paper: Explicit - The paper introduces token co-occurrence analysis and subgraph removal tests as quantitative metrics for evaluating subgraph explanations. It mentions that these metrics correlate with human preferences, but it does not compare them to other established XAI metrics.
- Why unresolved: The paper does not provide a comprehensive comparison of the proposed metrics with other widely used metrics in the XAI field, such as fidelity, precision, and recall.
- What evidence would resolve it: Conducting a comparative study of the proposed metrics against other established XAI metrics would reveal their relative strengths and weaknesses in assessing explanation quality.

### Open Question 3
- Question: How does the interpretability of the proposed method compare to other state-of-the-art interpretable VQA models, particularly those that generate natural language explanations?
- Basis in paper: Inferred - The paper focuses on the interpretability of the proposed method in terms of generating subgraphs as explanations. However, it does not compare its interpretability to other VQA models that generate natural language explanations, which are often considered more intuitive for humans.
- Why unresolved: The paper does not provide a direct comparison of the interpretability of the proposed method with other interpretable VQA models, making it difficult to assess its relative strengths and weaknesses.
- What evidence would resolve it: Conducting a comparative study of the interpretability of the proposed method with other interpretable VQA models, including those that generate natural language explanations, would provide insights into its relative effectiveness in enhancing human understanding of the model's decision-making process.

## Limitations

- The I-MLE approximation for hard-attention gradient estimation is assumed to work but not empirically validated
- Evaluation is limited to a single dataset (GQA), raising questions about generalizability
- The correlation between proposed metrics and true explanation quality versus spurious correlations remains uncertain

## Confidence

- **High confidence**: The M-GAT architecture can generate subgraphs as explanations and achieve competitive accuracy on GQA
- **Medium confidence**: The proposed metrics correlate with human preferences for explanation quality
- **Low confidence**: The I-MLE approximation effectively enables gradient flow through hard-attention sampling

## Next Checks

1. Compare the I-MLE approximation against other methods (e.g., straight-through estimator, REINFORCE) for backpropagating through the hard-attention mask, measuring both approximation error and downstream task performance.

2. Systematically evaluate the accuracy-explanation tradeoff across the full range of top-k values (0.05 to 0.25) with statistical significance testing to identify the optimal balance point more precisely.

3. Apply the M-GAT model to a different VQA dataset (e.g., CLEVR, VQA-v2) with different scene graph characteristics to assess whether the intrinsic subgraph generation and explanation quality generalize beyond GQA.