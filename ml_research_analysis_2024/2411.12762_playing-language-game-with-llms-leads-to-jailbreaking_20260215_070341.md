---
ver: rpa2
title: Playing Language Game with LLMs Leads to Jailbreaking
arxiv_id: '2411.12762'
source_url: https://arxiv.org/abs/2411.12762
tags:
- language
- question
- uni00000048
- answer
- uni0000004f
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models are vulnerable to jailbreak attacks that\
  \ exploit mismatched generalization. This paper introduces two novel attack methods\u2014\
  natural language games and custom language games\u2014that successfully bypass LLM\
  \ safety mechanisms with high success rates (93% on GPT-4o, 89% on GPT-4o-mini,\
  \ and 83% on Claude-3.5-Sonnet)."
---

# Playing Language Game with LLMs Leads to Jailbreaking

## Quick Facts
- arXiv ID: 2411.12762
- Source URL: https://arxiv.org/abs/2411.12762
- Reference count: 40
- LLMs vulnerable to jailbreak attacks exploiting mismatched generalization across linguistic formats

## Executive Summary
This paper demonstrates that large language models are vulnerable to jailbreak attacks that exploit mismatched generalization across different linguistic formats. The authors introduce two novel attack methods—natural language games (using synthetic linguistic constructs like Ubbi Dubbi) and custom language games (applying arbitrary transformation rules)—that successfully bypass LLM safety mechanisms with high success rates across multiple state-of-the-art models. The findings reveal a fundamental limitation in how safety knowledge is encoded in LLMs, showing that models fail to generalize safety alignments when harmful content is presented in transformed linguistic formats, even when the underlying meaning remains clear to humans.

## Method Summary
The authors conducted experiments using SALAD-Bench, a benchmark containing 300 questions across six harmful domains. They applied four natural language games (Ubbi Dubbi, Leetspeak, Aigy Paigy, Alfa Balfa) and eight custom language games (various text transformation rules) to transform harmful base questions. These transformed queries were submitted to GPT-4o, GPT-4o-mini, and Claude-3.5-Sonnet, with responses decoded back to standard language and evaluated for safety compliance using SR/UR/FR metrics. The paper also tested fine-tuning defenses by training Llama-3.1-70B on custom language game datasets using LoRA and evaluating generalization to other variants.

## Key Results
- Natural language games achieved high jailbreak success rates: 93% on GPT-4o, 89% on GPT-4o-mini, and 83% on Claude-3.5-Sonnet
- Custom language games with arbitrary transformation rules successfully bypassed safety mechanisms
- Fine-tuned models showed narrow protection that failed to generalize to semantically similar but structurally different attack variants
- The more capable the LLM in language processing, the more vulnerable it was to these attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Natural language games bypass LLM safety by exploiting mismatched generalization across linguistic formats
- Mechanism: LLMs fail to generalize safety alignment knowledge when input is transformed into synthetic linguistic constructs (e.g., Ubbi Dubbi, Leetspeak) that deviate from pretraining data but remain interpretable to humans
- Core assumption: Safety alignment knowledge is encoded in a format-specific manner that doesn't transfer across different linguistic representations of the same harmful content
- Evidence anchors:
  - [abstract] "Natural language games involve the use of synthetic linguistic constructs and the actions intertwined with these constructs, such as the Ubbi Dubbi language"
  - [section] "This finding indicates that the safety alignment knowledge embedded in LLMs fails to generalize across different linguistic formats"
  - [corpus] Weak - no direct corpus evidence found for language game-based jailbreaks specifically
- Break Condition: Training on diverse linguistic transformations during safety alignment, or implementing format-agnostic safety detection mechanisms

### Mechanism 2
- Claim: Custom language games bypass LLM safety by requiring novel rule comprehension beyond pretraining scope
- Mechanism: Arbitrary transformation rules (e.g., inserting symbols between letters, reversing word order) create inputs that LLMs can process but safety mechanisms cannot recognize as harmful due to lack of training exposure
- Core assumption: LLMs' superior language understanding capabilities paradoxically increase vulnerability when faced with novel, interpretable transformations
- Evidence anchors:
  - [section] "These custom rules offer numerous variations, making them difficult for LLMs to defend against while preserving easily recognizable text for humans"
  - [section] "The more capable the LLM is in processing and generating language, the more likely it is to successfully interpret and respond to a custom language game"
  - [corpus] Weak - corpus shows related work on obfuscation but not custom language game attacks specifically
- Break Condition: Implementing safety mechanisms that can detect harmful intent regardless of linguistic transformation, or limiting LLM's ability to process novel linguistic rules

### Mechanism 3
- Claim: Fine-tuning on specific transformations doesn't generalize to other custom language game variants
- Mechanism: Supervised fine-tuning creates narrow safety alignments that only recognize specific transformation patterns, failing when attackers make minor modifications to attack methods
- Core assumption: Safety knowledge gained through fine-tuning is brittle and doesn't transfer to semantically similar but structurally different linguistic manipulations
- Evidence anchors:
  - [section] "Fine-tuned model was able to successfully defend against other forms of attacks, with a success rate of 0% to 3%. However, for other custom language games, the model failed to defend against the attacks"
  - [section] "Despite being trained on a very similar transformation, the model was unable to generalize effectively to these slight alterations"
  - [corpus] Weak - no corpus evidence for fine-tuning generalization failure specifically
- Break Condition: Developing more robust fine-tuning approaches that capture underlying safety patterns rather than surface transformations

## Foundational Learning

- Concept: Mismatched generalization in machine learning
  - Why needed here: The attack exploits LLMs' inability to generalize safety knowledge across different input representations
  - Quick check question: What happens when a model trained on one distribution encounters data from a different but related distribution?

- Concept: Synthetic linguistic constructs and their cognitive processing
  - Why needed here: Understanding how language games like Ubbi Dubbi work and why they're still interpretable to humans but not to safety mechanisms
  - Quick check question: How do natural language games maintain semantic meaning while altering surface form?

- Concept: Fine-tuning limitations and brittleness
  - Why needed here: The paper demonstrates that fine-tuning on specific attack patterns doesn't provide robust protection against similar attacks
  - Quick check question: Why might a model fine-tuned to recognize one pattern fail to recognize a semantically similar but structurally different pattern?

## Architecture Onboarding

- Component map: Input transformation layer -> LLM inference engine -> Safety alignment module -> Response decoding layer -> Evaluation pipeline
- Critical path: 1. Select harmful base question, 2. Apply language game transformation rules, 3. Submit transformed query to LLM, 4. LLM generates response in same linguistic format, 5. Decode response back to standard language, 6. Evaluate safety compliance of response
- Design tradeoffs: Readability vs. security (more readable transformations are more effective but also more dangerous), Specificity vs. generalization (fine-tuning on specific patterns provides narrow protection but fails against variants), Computational cost vs. security (more comprehensive safety training increases costs but may provide better protection)
- Failure signatures: High success rates across multiple LLM models (93% GPT-4o, 89% GPT-4o-mini, 83% Claude-3.5-Sonnet), consistent failure of fine-tuned models to generalize safety across linguistic formats, LLM's superior language understanding paradoxically increases vulnerability to novel transformations
- First 3 experiments: 1. Test baseline safety rates on standard harmful queries across different LLM models, 2. Apply each language game transformation to harmful queries and measure jailbreak success rates, 3. Fine-tune a model on one transformation type and test against all transformation types to measure generalization failure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can language models be trained to generalize safety alignments across different linguistic formats while maintaining performance on standard inputs?
- Basis in paper: [explicit] The paper demonstrates that fine-tuning Llama-3.1-70B with custom language game datasets fails to generalize safety protections to other variants of the same attack pattern.
- Why unresolved: The paper shows the failure of current fine-tuning methods to achieve broad safety generalization, but does not propose or test alternative training approaches that could address this limitation.
- What evidence would resolve it: Experiments comparing different training methodologies (e.g., adversarial training, meta-learning approaches, or multi-task learning) that show improved cross-linguistic format safety generalization while maintaining performance on standard inputs.

### Open Question 2
- Question: What is the relationship between model sophistication and vulnerability to language game jailbreak attacks?
- Basis in paper: [explicit] The paper finds that GPT-4o, which has superior instruction comprehension capabilities, achieves the highest jailbreak success rate (93%) compared to less advanced models like GPT-4o-mini (89%) and Claude-3.5-Sonnet (83%).
- Why unresolved: While the paper establishes a correlation between model sophistication and vulnerability, it does not investigate the underlying mechanisms or explore whether this relationship holds across different model architectures or training approaches.
- What evidence would resolve it: Comparative analysis across diverse model architectures showing whether the relationship between sophistication and vulnerability persists, and identifying specific model capabilities that contribute to this vulnerability.

### Open Question 3
- Question: How can safety evaluations be designed to effectively test model robustness against novel linguistic transformations before deployment?
- Basis in paper: [inferred] The paper's successful jailbreak attacks reveal significant gaps in current safety evaluations, which failed to detect vulnerabilities against language game transformations that were not explicitly tested during model development.
- Why unresolved: Current safety benchmarks focus on standard linguistic patterns and known attack vectors, leaving models vulnerable to novel transformations that maintain semantic meaning while evading detection.
- What evidence would resolve it: Development and validation of comprehensive evaluation frameworks that systematically test model responses to diverse linguistic transformations, including both known patterns and novel variations generated through automated methods.

## Limitations

- Attack success rates may be inflated due to evaluation methodology using GPT-4o-mini as a labeling tool
- Findings may not generalize to real-world deployment scenarios with diverse and complex user prompts
- Limited exploration of alternative defense strategies beyond the single fine-tuning approach tested

## Confidence

- **Medium** - The attack success rates, while impressive (93% on GPT-4o, 89% on GPT-4o-mini, 83% on Claude-3.5-Sonnet), may be inflated due to the evaluation methodology
- **Low** - The generalizability of findings to real-world deployment scenarios is unclear
- **Medium** - The fine-tuning defense experiment shows that models fail to generalize across custom language game variants, but the study only tests one fine-tuned model

## Next Checks

1. **Cross-Evaluation Validation**: Repeat the experiments using human evaluators or multiple independent LLM judges to classify jailbreak success rates, ensuring the evaluation methodology doesn't introduce systematic bias

2. **Real-World Prompt Testing**: Test the attack effectiveness using prompts collected from actual user interactions with deployed LLMs, rather than curated benchmark questions, to assess practical vulnerability

3. **Defense Mechanism Exploration**: Investigate alternative defense strategies beyond fine-tuning, such as adversarial training on diverse linguistic transformations, format-agnostic safety detection, or hybrid approaches combining multiple defense mechanisms