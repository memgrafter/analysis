---
ver: rpa2
title: 'Fairness in Ranking: Robustness through Randomization without the Protected
  Attribute'
arxiv_id: '2403.19419'
source_url: https://arxiv.org/abs/2403.19419
tags:
- ranking
- fairness
- rankings
- protected
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of achieving fairness in ranking
  problems without access to protected attribute data. It proposes a randomized post-processing
  method that injects Mallows noise into rankings to improve fairness across multiple
  protected attributes, even when those attributes are unknown.
---

# Fairness in Ranking: Robustness through Randomization without the Protected Attribute

## Quick Facts
- **arXiv ID**: 2403.19419
- **Source URL**: https://arxiv.org/abs/2403.19419
- **Reference count**: 30
- **Key outcome**: Randomized post-processing with Mallows noise improves P-Fairness across unknown protected attributes while maintaining NDCG, with robustness improving as item count increases.

## Executive Summary
This paper addresses the challenge of achieving fairness in ranking systems without access to protected attribute data. The authors propose a randomized post-processing method that injects Mallows noise into rankings to improve fairness across multiple protected attributes, even when those attributes are unknown. The approach samples rankings from a Mallows distribution centered on a weakly P-fair baseline ranking, using different dispersion parameters to balance fairness and efficiency. Experiments on synthetic data and the German Credit dataset show that the method achieves competitive fairness (P-Fairness) compared to state-of-the-art algorithms while maintaining high ranking quality (NDCG), with results improving as the number of items increases. The method demonstrates robustness to unknown protected attributes and offers a promising trade-off between fairness and efficiency.

## Method Summary
The method uses a Mallows model with a central ranking (weakly P-fair) and varying dispersion parameter θ to sample multiple rankings. For each sampled ranking, P-Fairness and NDCG are computed, and the best-performing ranking is selected based on the desired tradeoff between fairness and quality. The approach is tested on synthetic datasets with two equal-sized groups and score distributions U(0,1) and U(0+δ,1+δ), as well as the German Credit dataset. The Mallows sampling function is implemented with configurable central ranking and dispersion parameter, and P-Fairness and NDCG are calculated for samples with varying θ values. The method is compared against ApproxMultiValuedIPF, DetConstSort, and ILP with/without noise in constraints.

## Key Results
- Mallows noise improves P-Fairness without knowing protected attributes
- Higher dispersion parameters trade fairness for efficiency in a controlled way
- Mallows-based randomization provides robustness against unknown protected attributes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mallows noise can improve P-Fairness without knowing protected attributes
- Mechanism: Injecting Mallows-distributed noise into rankings shifts item positions to reduce unfairness in group representation, while preserving overall ranking quality
- Core assumption: The baseline ranking is weakly P-fair and quality-based scores are unknown
- Evidence anchors:
  - [abstract]: "randomized post-processing method that injects Mallows noise into rankings to improve fairness across multiple protected attributes, even when those attributes are unknown"
  - [section IV-A]: "we utilize to the Mallows model as a randomization mechanism that ensures fairness in a way oblivious to the groups"
- Break condition: If the baseline ranking is extremely unfair or the dispersion parameter is too low, noise won't sufficiently improve fairness

### Mechanism 2
- Claim: Higher dispersion parameters trade fairness for efficiency in a controlled way
- Mechanism: As the Mallows dispersion parameter θ increases, sampled rankings move farther from the baseline, increasing P-Fairness at the cost of NDCG
- Core assumption: Baseline ranking has high NDCG and moderate unfairness
- Evidence anchors:
  - [section V-B]: "as the dispersion parameter increases, the Infeasible Index of samples drawn from Mallows' model converges to the Infeasible Index of the central permutation"
  - [section V-B]: "as the dispersion parameter increases the NDCG converges to 1"
- Break condition: If θ approaches infinity, rankings become random and both fairness and efficiency degrade

### Mechanism 3
- Claim: Mallows-based randomization provides robustness against unknown protected attributes
- Mechanism: By sampling from a distribution centered on a weakly P-fair baseline, the method implicitly balances fairness across all possible protected attributes, not just known ones
- Core assumption: Protected attributes are not available during ranking computation
- Evidence anchors:
  - [abstract]: "demonstrates robustness to unknown protected attributes"
  - [section V-C]: "evaluate the fairness of the output rankings using the Infeasible Index with respect to the Housing attribute and the utility of the output rankings using NDCG"
- Break condition: If the number of protected attributes is extremely large or groups are highly imbalanced, robustness may break down

## Foundational Learning

- Concept: Mallows distribution and distance-based ranking models
  - Why needed here: The core mechanism relies on understanding how Mallows noise affects ranking positions and fairness metrics
  - Quick check question: How does the dispersion parameter θ affect the probability of sampling a ranking at distance d from the center?

- Concept: P-Fairness metrics and Infeasible Index
  - Why needed here: The method aims to improve P-Fairness without knowing protected attributes, requiring understanding of how fairness is measured
  - Quick check question: What's the difference between (⃗ α,⃗β)-k fair and (⃗ α,⃗β)-weak k-fair rankings?

- Concept: NDCG and ranking quality metrics
  - Why needed here: The method must balance fairness improvements with ranking quality preservation
  - Quick check question: How is NDCG calculated and why is it used as the efficiency metric?

## Architecture Onboarding

- Component map:
  - Input: Baseline ranking (quality-sorted items)
  - Core: Mallows noise sampler with configurable dispersion
  - Output: Multiple sampled rankings, select best by fairness/NDCG tradeoff
  - Evaluation: P-Fairness metrics and NDCG calculation

- Critical path:
  1. Generate weakly P-fair baseline ranking
  2. Sample m rankings from Mallows distribution
  3. Evaluate each sample on P-Fairness and NDCG
  4. Return best-performing ranking

- Design tradeoffs:
  - Number of samples (m) vs computation time
  - Dispersion parameter (θ) vs fairness/efficiency balance
  - Sampling method (exact vs approximate) vs accuracy

- Failure signatures:
  - Very low P-Fairness scores: dispersion too low or baseline too unfair
  - Very low NDCG: dispersion too high or too many samples
  - No improvement over baseline: check Mallows implementation or baseline fairness

- First 3 experiments:
  1. Test Mallows sampling with different θ values on synthetic two-group data
  2. Compare P-Fairness improvement vs NDCG degradation across θ values
  3. Validate robustness by evaluating unknown protected attributes on German Credit dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Mallows noise parameters be optimized to maximize fairness across multiple unknown protected attributes while maintaining ranking efficiency?
- Basis in paper: [explicit] The paper discusses using Mallows noise for fairness but does not provide a systematic method for parameter optimization across unknown attributes.
- Why unresolved: The paper demonstrates effectiveness of Mallows noise but leaves open the question of optimal parameter selection when protected attributes are unknown.
- What evidence would resolve it: A systematic framework or algorithm that can determine optimal Mallows dispersion parameters without knowledge of protected attributes, validated through empirical testing.

### Open Question 2
- Question: What are the theoretical guarantees for fairness improvement when using Mallows noise in post-processing rankings?
- Basis in paper: [inferred] The paper shows empirical improvements in fairness but does not provide theoretical bounds or guarantees for the proposed method.
- Why unresolved: While the method shows promise in experiments, there is no mathematical analysis of how much fairness improvement can be expected or under what conditions.
- What evidence would resolve it: Theoretical proofs establishing bounds on fairness improvement, along with conditions under which these bounds hold.

### Open Question 3
- Question: How does the proposed method scale with increasing numbers of protected attributes and candidates?
- Basis in paper: [inferred] The paper tests with up to 100 items and a limited number of protected attributes, but does not explore scalability limits.
- Why unresolved: The experiments show promising results for moderate dataset sizes, but the computational complexity and effectiveness for large-scale applications remain unclear.
- What evidence would resolve it: Computational complexity analysis and empirical testing with significantly larger datasets and more protected attributes, demonstrating the method's scalability.

## Limitations

- Limited exploration of extreme Mallows dispersion parameters and their effects on fairness and efficiency
- No theoretical guarantees or bounds for fairness improvement using Mallows noise
- Scalability to datasets with many protected attributes or highly imbalanced groups remains unclear

## Confidence

- **High confidence**: The core mechanism of using Mallows noise to improve P-Fairness while maintaining NDCG quality is well-supported by both theoretical analysis and experimental results on synthetic and real datasets.
- **Medium confidence**: The robustness claims to unknown protected attributes are supported by experiments on the German Credit dataset, but the extent of this robustness across different dataset characteristics remains unclear.
- **Medium confidence**: The efficiency-fairness tradeoff analysis is thorough for the tested parameter ranges, but the behavior at extreme parameter values (very high/low dispersion) is not fully explored.

## Next Checks

1. **Scalability test**: Evaluate performance on synthetic datasets with varying numbers of protected attributes (e.g., 2, 5, 10) to quantify robustness degradation.
2. **Computational complexity analysis**: Measure wall-clock time for Mallows sampling as dataset size increases from 100 to 10,000 items to determine practical scalability limits.
3. **Extreme parameter validation**: Systematically test Mallows dispersion parameters beyond the reported ranges (θ < 0.5 and θ > 10) to characterize behavior at parameter extremes and identify failure thresholds.