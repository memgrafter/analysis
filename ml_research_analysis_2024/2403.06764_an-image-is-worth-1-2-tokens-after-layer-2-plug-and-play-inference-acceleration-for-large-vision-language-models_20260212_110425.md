---
ver: rpa2
title: 'An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration
  for Large Vision-Language Models'
arxiv_id: '2403.06764'
source_url: https://arxiv.org/abs/2403.06764
tags:
- tokens
- fastv
- attention
- image
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies inefficient attention in Large Vision-Language
  Models (LVLMs), where image tokens receive disproportionately low attention in deep
  layers compared to textual tokens. To address this, the authors propose FastV, a
  plug-and-play method that learns adaptive attention patterns in early layers and
  prunes low-attention visual tokens in subsequent layers.
---

# An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models

## Quick Facts
- arXiv ID: 2403.06764
- Source URL: https://arxiv.org/abs/2403.06764
- Reference count: 13
- Key outcome: FastV reduces FLOPs by up to 45% for LLaVA-1.5-13B without sacrificing performance across various vision-language tasks

## Executive Summary
This paper identifies inefficient attention patterns in Large Vision-Language Models (LVLMs), where visual tokens receive disproportionately low attention in deep layers compared to textual tokens. The authors propose FastV, a plug-and-play method that learns adaptive attention patterns in early layers and prunes low-attention visual tokens in subsequent layers. This approach reduces computational costs by up to 45% while maintaining or improving performance across diverse vision-language benchmarks, enabling efficient deployment on edge devices and reducing commercial inference costs.

## Method Summary
FastV addresses inefficient attention in LVLMs by implementing a token pruning strategy at a specific layer. Before this layer, computations proceed normally. After this layer, visual tokens are re-evaluated based on their average received attention scores, and the bottom R% of tokens are pruned in subsequent layers. The method is plug-and-play, requiring no retraining, and allows for customizable trade-offs between computational efficiency and performance by adjusting the pruning layer K and pruning ratio R.

## Key Results
- Achieves up to 45% FLOPs reduction for LLaVA-1.5-13B without performance degradation
- Enables a 13B-parameter model to achieve lower computational costs than a 7B-parameter model while maintaining superior performance
- Demonstrates effectiveness across image captioning, VQA, multimodal reasoning, and video understanding tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Image tokens in deep layers of LVLMs receive disproportionately low attention compared to textual tokens.
- Mechanism: Visual features are processed and condensed into "anchor" tokens in shallow layers, which then dominate attention in deep layers, causing image tokens to be effectively ignored.
- Core assumption: Anchor tokens sufficiently capture visual information for downstream tasks.
- Evidence anchors: [abstract], [section 3.3]
- Break condition: If anchor tokens don't capture sufficient visual information, pruning image tokens will degrade performance.

### Mechanism 2
- Claim: FastV improves inference efficiency by dynamically pruning low-attention image tokens in later layers.
- Mechanism: Ranks image tokens by average attention scores at layer K, then prunes bottom R% in subsequent layers, reducing FLOPs without sacrificing performance.
- Core assumption: Low-attention tokens contribute minimally to final output, so their removal doesn't affect performance.
- Evidence anchors: [abstract], [section 4.1]
- Break condition: If attention scores don't accurately reflect token importance, pruning may remove useful information.

### Mechanism 3
- Claim: FastV enables higher resolution images without increased computational cost.
- Mechanism: Computational savings from pruning can be redirected to processing higher resolution images with more tokens in early layers, improving performance without exceeding original computational budget.
- Core assumption: Computational savings can be reallocated to higher resolution processing without exceeding budget.
- Evidence anchors: [abstract], [section 1]
- Break condition: If token count vs computational cost relationship isn't linear, or higher resolution requires disproportionately more computation, benefit may not materialize.

## Foundational Learning

- Concept: Self-attention mechanism in transformers
  - Why needed here: Understanding attention score computation and distribution is crucial to grasping why FastV works
  - Quick check question: In a transformer layer, what mathematical operation determines the attention score between two tokens?

- Concept: Computational complexity of attention
  - Why needed here: Understanding O(n²) complexity explains why pruning tokens reduces FLOPs significantly
  - Quick check question: If an input sequence has 1000 tokens, how many attention score computations are performed in one attention head?

- Concept: Pareto efficiency in resource allocation
  - Why needed here: FastV aims to find Pareto-efficient points where computational cost is minimized without sacrificing performance
  - Quick check question: In a Pareto-efficient allocation, can you improve one metric without making another metric worse?

## Architecture Onboarding

- Component map: Visual encoder (CLIP-VIT) → Image tokens; Tokenizer → Text tokens; LLM decoder → Processes all tokens; FastV module → Inserts after layer K to prune image tokens

- Critical path:
  1. Image and text input → Visual encoder and tokenizer
  2. Image tokens and text tokens → LLM decoder layers 1 to K
  3. FastV ranking and pruning → Remaining image tokens pruned
  4. Pruned tokens → LLM decoder layers K+1 to T
  5. Output tokens → Final generation

- Design tradeoffs:
  - K (pruning layer) vs R (pruning ratio): Earlier pruning allows more FLOPs reduction but may hurt performance; higher R gives more reduction but risks information loss
  - Attention-based vs random pruning: Attention-based is more precise but requires computing attention scores; random is simpler but less effective
  - Training-free vs fine-tuned: FastV is plug-and-play but may not achieve optimal performance compared to models trained with fewer tokens

- Failure signatures:
  - Performance degradation on tasks requiring detailed visual understanding (OCR, fine-grained classification)
  - Inconsistent results across different K and R settings
  - Unexpected sensitivity to pruning parameters for certain image types

- First 3 experiments:
  1. Run FastV with K=2, R=50% on LLaVA-1.5-7B using Nocaps and A-OKVQA to verify claimed 45% FLOPs reduction with minimal performance loss
  2. Vary K from 0 to 5 while keeping R=50% to observe effect of pruning layer on performance and FLOPs reduction
  3. Test FastV on video understanding tasks (TGIF-QA) to confirm claimed improvement in video QA performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of pruning layer K and pruning ratio R affect the trade-off between computational efficiency and model performance in FastV across different LVLM architectures?
- Basis in paper: [explicit] The paper states that the computational efficiency and performance trade-off of FastV are highly customizable and Pareto-efficient, and that the FLOPs reduction ratio can be adjusted by lowering K and increasing R.
- Why unresolved: While the paper provides theoretical estimates and experimental results for specific configurations, it does not explore the full range of K and R values or their interactions across diverse LVLM architectures.
- What evidence would resolve it: Systematic experiments varying K and R across a wide range of LVLM models and tasks would reveal optimal configurations for balancing efficiency and performance.

### Open Question 2
- Question: What is the impact of FastV on the model's ability to handle complex visual tasks that require detailed analysis of fine-grained features, such as object detection or segmentation?
- Basis in paper: [inferred] The paper mentions that FastV prunes low-attention visual tokens in subsequent layers, which could potentially remove tokens containing crucial fine-grained information for tasks like object detection or segmentation.
- Why unresolved: The experiments focus on image captioning, VQA, and video understanding, which may not require the same level of fine-grained visual analysis as object detection or segmentation.
- What evidence would resolve it: Evaluating FastV on object detection and segmentation benchmarks would reveal whether pruning low-attention tokens negatively impacts the model's ability to identify and localize objects or segments in images.

### Open Question 3
- Question: How does FastV perform when applied to LVLM models that use different visual encoders or tokenization strategies?
- Basis in paper: [inferred] The paper primarily evaluates FastV on LLaVA-1.5 and QwenVL-Chat, which use CLIP-VIT as the visual encoder.
- Why unresolved: The paper does not explore the generalizability of FastV to other LVLM architectures with different visual encoders or tokenization methods.
- What evidence would resolve it: Evaluating FastV on a diverse set of LVLM models with different visual encoders and tokenization strategies would reveal its generalizability and identify any potential limitations or adaptations needed for optimal performance.

## Limitations
- Evidence supporting causal relationship between attention scores and token importance remains correlational rather than definitive
- Evaluation scope primarily focuses on established vision-language tasks, lacking exploration of edge cases where FastV might fail
- Claim about enabling higher resolution images lacks quantitative validation of actual resolution improvements and corresponding performance gains

## Confidence
- **High Confidence**: Identification of inefficient attention patterns in LVLMs (Mechanism 1) is well-supported by empirical analysis
- **Medium Confidence**: Effectiveness of FastV's pruning strategy (Mechanism 2) is demonstrated through extensive experiments
- **Low Confidence**: Claim that FastV enables processing of higher resolution images without increased computational cost (Mechanism 3) lacks direct experimental validation

## Next Checks
1. **Ablation on Attention-Based vs Random Pruning**: Implement random token pruning baseline with same pruning ratio and compare performance against FastV's attention-based pruning across multiple tasks

2. **Resolution Scaling Experiments**: Systematically test FastV with increasing image resolutions (224x224, 336x336, 448x448) while measuring computational costs and task performance

3. **Failure Mode Analysis**: Design and test challenging scenarios specifically likely to break FastV (OCR tasks with small text, counting objects, complex spatial relationships) to identify pruning strategy limits