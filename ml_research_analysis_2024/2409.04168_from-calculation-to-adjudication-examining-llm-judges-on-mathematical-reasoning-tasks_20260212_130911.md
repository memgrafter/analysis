---
ver: rpa2
title: 'From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning
  Tasks'
arxiv_id: '2409.04168'
source_url: https://arxiv.org/abs/2409.04168
tags:
- performance
- judges
- answer
- judge
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the use of large language models (LLMs) as judges
  for mathematical reasoning tasks. The authors evaluate eight LLMs of varying sizes
  (2B to 72B parameters) as both answer generators and judges on three math datasets
  (AQUA-RAT, GSM8K, MATH).
---

# From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks

## Quick Facts
- arXiv ID: 2409.04168
- Source URL: https://arxiv.org/abs/2409.04168
- Reference count: 15
- Primary result: LLM judges achieve 60-90% accuracy on mathematical reasoning tasks, but cannot improve task performance through answer selection despite reliably detecting better-performing models

## Executive Summary
This paper investigates the use of large language models as judges for mathematical reasoning tasks, evaluating eight LLMs ranging from 2B to 72B parameters on three math datasets (AQUA-RAT, GSM8K, MATH). The study finds that LLM judges perform well overall with 60-90% accuracy, with larger models showing better judgment performance. Interestingly, judges tend to favor higher-quality models even when their answers are incorrect, and 70-75% of judgments can be predicted using simple linguistic features like part-of-speech tag n-grams, suggesting judges rely partly on stylistic patterns rather than pure reasoning quality.

## Method Summary
The authors evaluate eight LLMs of varying sizes (2B to 72B parameters) in dual roles as both answer generators and judges on three mathematical reasoning datasets. The evaluation framework compares LLM judges against human judgments and examines the correlation between candidate model performance and judgment accuracy. The study also investigates whether LLM judges can improve task performance by selecting better answers from multiple candidates. Linguistic feature analysis is performed to understand what drives judge decisions, revealing that part-of-speech patterns can predict a significant portion of judgments.

## Key Results
- LLM judges achieve 60-90% accuracy on mathematical reasoning tasks, with larger models showing better performance
- Judges reliably detect which model performs better overall but cannot improve task performance when used to select answers
- 70-75% of judgments can be predicted using linguistic features like part-of-speech tag n-grams, suggesting reliance on stylistic patterns

## Why This Works (Mechanism)
The mechanism behind LLM judges' performance appears to combine genuine reasoning assessment with pattern recognition of stylistic features. Larger models show better judgment capabilities, likely due to their more sophisticated reasoning abilities and broader training data exposure. The judges' tendency to favor higher-quality models even when incorrect suggests they may be detecting proxy features correlated with overall model quality rather than analyzing reasoning steps directly. The high predictability of judgments using linguistic features indicates that surface-level patterns play a significant role in the decision-making process, potentially serving as heuristics for reasoning quality assessment.

## Foundational Learning

**Mathematical Reasoning** - The ability to solve and evaluate mathematical problems requires understanding of logical structures and numerical relationships. Why needed: Core capability being assessed. Quick check: Can the model correctly solve basic arithmetic and algebra problems.

**LLM Evaluation Methodology** - Frameworks for comparing model outputs and determining quality. Why needed: Essential for benchmarking judge performance. Quick check: Does the evaluation capture meaningful differences between model outputs beyond simple correctness.

**Linguistic Feature Analysis** - Using textual patterns like part-of-speech distributions to understand model behavior. Why needed: Reveals non-obvious factors influencing judge decisions. Quick check: Can linguistic features predict judge choices better than random chance.

## Architecture Onboarding

**Component Map**: Answer Generators (8 LLM models) -> Judge Models (same 8 LLMs) -> Human Judgments (ground truth) -> Evaluation Metrics

**Critical Path**: Mathematical Problem -> Multiple Model Answers -> LLM Judge Evaluation -> Quality Assessment -> Performance Analysis

**Design Tradeoffs**: The study uses the same models for both generation and judging, which provides consistency but may introduce bias. Alternative approaches could use different model families for judging versus generating, potentially providing more objective assessments.

**Failure Signatures**: Judges fail when they cannot distinguish between high-quality and low-quality reasoning, particularly when answers are incorrect but stylistically similar to correct ones. They also fail to improve task performance despite good relative judgment capabilities.

**First Experiments**:
1. Compare judge performance when using different temperature settings to test consistency
2. Evaluate ensemble methods combining multiple judges to see if they overcome individual limitations
3. Test whether removing linguistic features from judge prompts affects judgment accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- While judges can reliably detect better-performing models, they cannot improve task performance through answer selection, suggesting a gap between relative judgment and absolute quality assessment
- The high predictability of judgments using linguistic features raises questions about whether judges are truly assessing reasoning quality or relying on stylistic proxies
- The evaluation assumes human judgments provide adequate ground truth for mathematical reasoning quality, without addressing potential inconsistencies in human evaluation

## Confidence

High: LLM judges perform well overall with 60-90% accuracy
Medium: Larger models show better judgment performance
Low: 70-75% of judgments can be predicted using linguistic features

## Next Checks

1. Conduct ablation studies removing linguistic features to determine their actual contribution to judgment accuracy versus reasoning-based evaluation
2. Test judge consistency by having the same LLM judge pairs of answers multiple times with different prompts or temperature settings
3. Evaluate whether ensemble methods combining multiple LLM judges can overcome the limitation of not improving task performance through answer selection