---
ver: rpa2
title: 'AD-LLM: Benchmarking Large Language Models for Anomaly Detection'
arxiv_id: '2412.11142'
source_url: https://arxiv.org/abs/2412.11142
tags:
- anomaly
- category
- data
- normal
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'AD-LLM benchmarks large language models (LLMs) for three core
  anomaly detection tasks: zero-shot detection, data augmentation, and model selection.
  For zero-shot detection, LLMs outperformed traditional training-based methods without
  task-specific data, with GPT-4o and DeepSeek-V3 achieving superior AUROC and AUPRC.'
---

# AD-LLM: Benchmarking Large Language Models for Anomaly Detection

## Quick Facts
- arXiv ID: 2412.11142
- Source URL: https://arxiv.org/abs/2412.11142
- Reference count: 40
- Primary result: LLMs outperform traditional anomaly detection methods in zero-shot detection while providing flexible augmentation and model selection capabilities

## Executive Summary
AD-LLM benchmarks large language models across three core anomaly detection tasks: zero-shot detection, data augmentation, and model selection. The framework demonstrates that LLMs can perform anomaly detection without task-specific training data by leveraging pre-trained semantic knowledge, with GPT-4o and DeepSeek-V3 achieving superior AUROC and AUPRC scores. LLM-generated synthetic data improves performance for flexible models like autoencoders and VAEs, though it degrades results for methods relying on fixed geometric assumptions. For model selection, reasoning-capable LLMs recommend effective detectors using dataset descriptions and model abstracts, though explanations often lack specificity. These results highlight LLMs' potential to enhance anomaly detection efficiency and flexibility, particularly through synthetic data generation and context integration.

## Method Summary
The AD-LLM framework evaluates LLMs on three tasks using five NLP datasets (AG News, BBC News, IMDB Reviews, N24 News, SMS Spam) and 18 traditional unsupervised anomaly detection methods. For zero-shot detection, LLMs classify samples using category information as prompts without training. Data augmentation involves generating synthetic samples and category descriptions via LLM prompts to enhance traditional AD models. Model selection uses reasoning-capable LLMs to recommend suitable detectors based on dataset attributes and model abstracts. The approach employs GPT-4o, DeepSeek-V3, and Llama 3.1 8B models with AUROC and AUPRC as evaluation metrics, comparing LLM-based methods against traditional baselines.

## Key Results
- GPT-4o and DeepSeek-V3 achieved superior AUROC/AUPRC scores in zero-shot detection without task-specific training data
- LLM-generated synthetic data improved performance for autoencoders, ECOD, LUNAR, and VAE models but degraded results for methods with fixed geometric assumptions
- Reasoning-capable LLMs (OpenAI-o1, DeepSeek-R1) provided effective model recommendations using only dataset descriptions and abstracts, though explanations lacked specificity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs leverage pre-trained semantic knowledge to perform zero-shot anomaly detection without task-specific training data.
- Mechanism: LLMs process text samples using prompt-based inference that includes category information, enabling them to identify anomalies through logical reasoning based on learned language patterns.
- Core assumption: LLMs possess sufficient pre-trained knowledge about normal and anomalous patterns in text to distinguish between them without additional training.
- Evidence anchors:
  - [abstract] "LLMs, with their broad pre-trained knowledge, can perform zero-shot detection without additional training data."
  - [section] "Their ability to understand language context and semantics makes them suitable for recognizing anomalies by logical reasoning."
  - [corpus] Found 25 related papers with average FMR 0.535, indicating moderate relatedness to the topic.
- Break condition: If the dataset contains highly domain-specific anomalies not well-represented in LLM pretraining data, or if anomalies require specialized knowledge outside general language understanding.

### Mechanism 2
- Claim: LLM-generated synthetic data improves traditional anomaly detection model performance by enriching training data.
- Mechanism: LLMs generate contextually relevant synthetic samples through multi-step keyword generation and sample creation, providing diverse training examples that enhance model learning.
- Core assumption: Synthetic data generated by LLMs maintains semantic alignment with real data while providing sufficient diversity to improve model robustness.
- Evidence anchors:
  - [abstract] "Generative LLMs may produce synthetic data to strengthen AD cost-effectively."
  - [section] "LLM-generated synthetic data significantly enhances AD performance for several detectors."
  - [corpus] Text-ADBench paper shows LLMs can generate synthetic data for text anomaly detection.
- Break condition: If synthetic data introduces excessive variance that disrupts models relying on fixed geometric assumptions, or if generated samples deviate too far from real data distribution.

### Mechanism 3
- Claim: LLMs can recommend effective anomaly detection models based on dataset characteristics and model abstracts.
- Mechanism: LLMs analyze dataset attributes (size, categories, text length statistics) and model descriptions to match dataset properties with model strengths.
- Core assumption: LLMs can effectively parse and reason about technical model descriptions and correlate them with dataset features.
- Evidence anchors:
  - [abstract] "LLMs, with the prior knowledge and ability to reason, may be able to suggest suitable AD models."
  - [section] "LLM-based model selection can approach top-performing baselines."
  - [corpus] LLM-Powered Text-Attributed Graph Anomaly Detection paper shows LLMs can reason about model selection.
- Break condition: If model abstracts lack sufficient detail for LLMs to make informed decisions, or if dataset characteristics are too ambiguous for meaningful model matching.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The entire framework relies on LLMs performing tasks without task-specific training data.
  - Quick check question: What distinguishes zero-shot learning from few-shot learning in the context of LLMs?

- Concept: Prompt engineering
  - Why needed here: Performance heavily depends on how information is presented to LLMs through carefully designed prompts.
  - Quick check question: How do different prompt structures (chain-of-thought vs direct prompting) affect LLM reasoning quality?

- Concept: Anomaly detection fundamentals
  - Why needed here: Understanding traditional AD methods helps explain why LLM approaches work differently and when they might fail.
  - Quick check question: What are the key differences between density-based and reconstruction-based anomaly detection approaches?

## Architecture Onboarding

- Component map: Data preprocessing → LLM inference → Score aggregation → Evaluation metrics → For augmentation: Data generation → Model training → Performance evaluation → For model selection: Dataset analysis → Model recommendation → Justification generation
- Critical path: Prompt construction → LLM processing → Result interpretation → Performance validation
- Design tradeoffs:
  - Zero-shot detection: High flexibility vs computational cost
  - Data augmentation: Diversity vs domain alignment
  - Model selection: Reasoning depth vs prompt complexity
- Failure signatures:
  - Zero-shot detection: Inconsistent scoring, infinite loops, safety filter triggers
  - Augmentation: Repetitive outputs, domain shift, model degradation
  - Model selection: Generic recommendations, context-insensitive choices
- First 3 experiments:
  1. Test zero-shot detection with simple categorical datasets to validate basic functionality
  2. Generate synthetic data for a single model type and measure performance improvement
  3. Test model selection with a small set of well-documented models and clear dataset characteristics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform in zero-shot AD when category names are ambiguous or overlapping, and what specific prompt engineering strategies could improve detection in such cases?
- Basis in paper: [inferred] The paper shows that providing additional context (e.g., category names) improves LLM-based zero-shot detection, but does not explore ambiguous or overlapping categories.
- Why unresolved: The experiments focus on datasets with clear category distinctions, and the paper does not investigate scenarios where category names are unclear or overlapping, which is common in real-world data.
- What evidence would resolve it: Experiments testing LLM-based zero-shot detection on datasets with ambiguous or overlapping category names, along with ablation studies on prompt engineering techniques (e.g., clarifying prompts, additional context).

### Open Question 2
- Question: What is the optimal balance between diversity and domain alignment in LLM-generated synthetic data, and how can this balance be dynamically adjusted based on dataset characteristics?
- Basis in paper: [explicit] The paper highlights that synthetic data improves AD performance for models with flexible representations but degrades performance for models relying on fixed geometric assumptions, indicating a need for balancing diversity and alignment.
- Why unresolved: The paper does not provide a systematic method for dynamically adjusting the diversity-alignment balance based on dataset characteristics or model types.
- What evidence would resolve it: A framework that quantifies dataset characteristics (e.g., class imbalance, semantic complexity) and adjusts synthetic data generation parameters (e.g., prompt diversity, filtering strategies) accordingly, validated through controlled experiments.

### Open Question 3
- Question: How can LLM-based model selection be improved to provide more dataset-specific and interpretable justifications, and what role do inherent LLM biases play in model recommendations?
- Basis in paper: [explicit] The paper finds that LLM-based model selection often lacks dataset-specific justifications and exhibits inherent biases, such as favoring well-known models, which limits interpretability and user trust.
- Why unresolved: The paper does not explore methods to mitigate LLM biases or enhance the specificity and transparency of model selection justifications.
- What evidence would resolve it: Experiments testing fine-tuning LLMs with annotated explanations, prompt engineering for structured reasoning, and bias mitigation techniques (e.g., balanced input information), along with user studies to assess interpretability and trust.

## Limitations

- Performance heavily depends on prompt engineering quality, which isn't fully specified in the paper
- Results are based on five specific NLP datasets and may not generalize to other data types
- Study uses specific LLM versions, and performance may vary across model updates or different providers

## Confidence

- High confidence: Zero-shot detection effectiveness (AUROC/AUPRC metrics are clear and well-defined)
- Medium confidence: Data augmentation benefits (mixed results across different detector types suggest context-dependency)
- Low confidence: Model selection reliability (recommendations often lack specificity, and evaluation method relies on proxy metrics)

## Next Checks

1. Test the three LLM approaches on non-text datasets (tabular, image, or time series) to verify generalizability beyond NLP
2. Systematically vary prompt structures and parameters to quantify their impact on performance and identify optimal configurations
3. Measure the computational and financial costs of LLM-based approaches versus traditional methods, particularly for zero-shot detection where per-sample inference can be expensive