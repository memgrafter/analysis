---
ver: rpa2
title: Analyzing and Mitigating Model Collapse in Rectified Flow Models
arxiv_id: '2412.08175'
source_url: https://arxiv.org/abs/2412.08175
tags:
- data
- flow
- reflow
- arxiv
- collapse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates model collapse in Rectified Flow, a simulation-free
  generative model that iteratively uses self-generated data to straighten probability
  flow trajectories for efficient sampling. The authors theoretically prove that Denoising
  Autoencoders (DAEs) suffer from performance degradation when trained recursively
  on their own outputs, which extends to diffusion models and Rectified Flow.
---

# Analyzing and Mitigating Model Collapse in Rectified Flow Models

## Quick Facts
- arXiv ID: 2412.08175
- Source URL: https://arxiv.org/abs/2412.08175
- Reference count: 0
- Primary result: Real-data Augmented Reflow (RA Reflow) effectively prevents model collapse in Rectified Flow models, achieving FID scores of 4.15 at 10 NFEs on CIFAR-10

## Executive Summary
This paper investigates model collapse in Rectified Flow, a simulation-free generative model that iteratively uses self-generated data to straighten probability flow trajectories for efficient sampling. The authors theoretically prove that Denoising Autoencoders (DAEs) suffer from performance degradation when trained recursively on their own outputs, which extends to diffusion models and Rectified Flow. To mitigate this issue, they propose Real-data Augmented Reflow (RA Reflow) and its variants, which integrate real data by leveraging reverse flow processes. Experimental results on CIFAR-10 and CelebA-HQ demonstrate that RA Reflow effectively prevents model collapse, achieving high-quality image generation with fewer sampling steps while preserving sampling efficiency.

## Method Summary
The paper proposes RA Reflow, which generates sufficient real image-noise pairs via reverse ODE processes while maintaining forward-backward consistency to prevent model collapse. The method uses a mixing ratio λ to balance synthetic and real data pairs, with synthetic pairs generated through ODE solvers and real pairs obtained via reverse sampling. Variants include ORA Reflow (online variant) that reduces storage requirements and ORAS Reflow (stochastic variant) that introduces controlled noise injection during reverse processes.

## Key Results
- RA Reflow achieves FID scores of 4.15 at 10 NFEs on CIFAR-10, significantly outperforming baseline Reflow
- The method maintains high sample quality while reducing sampling steps compared to traditional approaches
- Weight norms remain stable over iterations when real data is incorporated, preventing model collapse

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rectified Flow's reflow method iteratively uses self-generated data to straighten probability flow trajectories, reducing sampling steps but causing model collapse over successive iterations.
- Mechanism: When models are repeatedly trained on their own outputs, the implicit regularization from noise in denoising autoencoders (DAEs) induces a distribution shift in generated synthetic data, leading to geometric decrease in weight norms and performance degradation.
- Core assumption: The noise injected during DAE training acts as an implicit regularizer that, when combined with self-generated data, causes the model to forget its original data distribution.
- Evidence anchors:
  - [abstract] "We begin by studying Denoising Autoencoders (DAEs) and prove performance degradation when DAEs are iteratively trained on their own outputs."
  - [section] "Our analysis and experiments demonstrate that rectified flow also suffers from MC, leading to potential performance degradation in each reflow step."
  - [corpus] No direct evidence found in related papers; this is a novel theoretical contribution.
- Break condition: If the noise level is too small or too large relative to the data variance, the implicit regularization effect may not cause distribution shift.

### Mechanism 2
- Claim: Incorporating real data into the training process prevents model collapse by maintaining a fixed lower bound on the weight norm of the DAE.
- Mechanism: Adding real data counteracts the implicit regularization effect by providing a consistent data distribution reference, preventing the weight norms from decreasing exponentially with iterations.
- Core assumption: The presence of real data in each iteration provides a stable reference point that anchors the model to the true data distribution.
- Evidence anchors:
  - [abstract] "We prove that incorporating real data can prevent MC during recursive DAE training, supporting the recent trend of using real data as an effective approach for mitigating MC."
  - [section] "Proposition 4.5 shows that by incorporating real data into the synthetic data generation process, the learned DAE mitigate model collapse, maintaining a fixed lower bound on the weight norm."
  - [corpus] No direct evidence found in related papers; this is a novel theoretical contribution.
- Break condition: If the proportion of real data is too small relative to synthetic data, the stabilizing effect may be insufficient to prevent collapse.

### Mechanism 3
- Claim: RA Reflow generates sufficient real image-noise pairs via reverse ODE processes while maintaining forward-backward consistency, effectively preventing model collapse.
- Mechanism: By using reverse sampling to generate noise-image pairs from real images, RA Reflow provides the necessary data pairs for training without requiring direct access to noise-image pairs, which are not readily available for real data.
- Core assumption: The reverse ODE process accurately reconstructs the noise-image pairs needed for training, and the balance between synthetic and real data is sufficient to prevent collapse.
- Evidence anchors:
  - [abstract] "Building on these insights, we propose a novel Real-data Augmented Reflow (RA Reflow) and a series of improved variants, which seamlessly integrate real data into Reflow training by leveraging reverse flow."
  - [section] "Our proposed RA Reflow addresses this issue with a novel approach of incorporating real data by leveraging reverse processes."
  - [corpus] No direct evidence found in related papers; this is a novel approach.
- Break condition: If the reverse ODE process introduces significant errors or if the balance between synthetic and real data is not optimal, the effectiveness of RA Reflow may be compromised.

## Foundational Learning

- Concept: Denoising Autoencoders (DAEs)
  - Why needed here: DAEs provide a simplified framework to analyze model collapse mechanisms that extend to more complex models like diffusion models and Rectified Flow.
  - Quick check question: How does the implicit regularization from noise in DAEs affect the learned model when trained on self-generated data?

- Concept: Score matching and diffusion models
  - Why needed here: Understanding the connection between DAEs and diffusion models through score matching is crucial for extending the analysis of model collapse to these models.
  - Quick check question: What is the relationship between the training objectives of DAEs and diffusion models?

- Concept: Optimal transport and flow matching
  - Why needed here: Rectified Flow is based on straightening probability flow trajectories, which is related to optimal transport theory and flow matching techniques.
  - Quick check question: How does Rectified Flow use reflow procedures to straighten flow trajectories and improve sampling efficiency?

## Architecture Onboarding

- Component map: U-Net -> DAE -> Rectified Flow -> RA Reflow
- Critical path:
  1. Analyze model collapse in DAEs
  2. Extend analysis to Rectified Flow
  3. Propose RA Reflow to prevent collapse
  4. Validate effectiveness through experiments

- Design tradeoffs:
  - Storage vs. Performance: RA Reflow requires storing intermediate results, while ORA Reflow reduces storage but may impact performance
  - Real Data Ratio: Balancing the proportion of real data to synthetic data to prevent collapse without degrading flow straightening
  - Noise Injection: Controlled noise injection in reverse processes to enhance diversity without disrupting flow straightening

- Failure signatures:
  - Geometric decrease in weight norms over iterations (indicating model collapse)
  - Degradation in image quality and FID scores over reflow iterations
  - Insufficient diversity in generated samples

- First 3 experiments:
  1. Implement RA Reflow with varying λ (real data ratio) and measure FID scores and weight norms over iterations
  2. Compare ORA Reflow with RA Reflow in terms of storage requirements and performance
  3. Test ORAS Reflow with different noise scales in reverse processes to optimize sample diversity and quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which incorporating real data prevents model collapse in denoising autoencoders?
- Basis in paper: [explicit] The paper proves that incorporating real data prevents model collapse during recursive DAE training (Proposition 4.5).
- Why unresolved: The paper provides a theoretical proof but does not fully explain the underlying mechanism by which real data prevents collapse.
- What evidence would resolve it: Detailed analysis of how real data affects the distribution of training samples and prevents the geometric decrease in weight norms.

### Open Question 2
- Question: How does the choice of the mixing ratio λ affect the quality of generated images and the prevention of model collapse in RA Reflow?
- Basis in paper: [explicit] The paper mentions that the synthetic-to-real data ratio should be at least 7:3 to effectively avoid collapse (Figure 4).
- Why unresolved: The paper does not provide a comprehensive analysis of how different λ values impact the performance of RA Reflow.
- What evidence would resolve it: Systematic experiments varying λ and measuring the impact on FID scores, sample diversity, and model collapse.

### Open Question 3
- Question: Can the proposed methods be extended to other generative models beyond Rectified Flow, such as GANs or VAEs?
- Basis in paper: [inferred] The paper discusses model collapse in various generative models, including diffusion models and flow matching, suggesting potential applicability.
- Why unresolved: The paper focuses specifically on Rectified Flow and does not explore the extension to other generative models.
- What evidence would resolve it: Application of RA Reflow and its variants to GANs and VAEs, followed by empirical evaluation of their performance and ability to prevent model collapse.

## Limitations
- Computational overhead from reverse ODE processes that may limit scalability
- Limited evaluation to only two image datasets (CIFAR-10 and CelebA-HQ)
- Lack of analysis for extremely long training sequences where accumulated errors could become problematic

## Confidence
- High confidence: Theoretical proof of model collapse in DAEs and its extension to Rectified Flow
- Medium confidence: Effectiveness of RA Reflow variants on CIFAR-10 and CelebA-HQ
- Low confidence: Generalizability to other domains beyond image generation

## Next Checks
1. **Ablation Study on λ Parameter**: Systematically vary the λ parameter controlling the ratio of real to synthetic data pairs across multiple orders of magnitude (e.g., 0.01, 0.1, 0.5, 0.9, 0.99) and measure the resulting FID scores, weight norms, and sampling efficiency to identify the optimal balance point.

2. **Cross-Domain Generalization**: Apply RA Reflow to non-image domains such as text generation or molecular structure prediction, using appropriate evaluation metrics for each domain (e.g., perplexity for text, binding affinity prediction for molecules) to test the approach's generalizability.

3. **Long Sequence Stability Analysis**: Conduct experiments with extended reflow iterations (e.g., 50-100 iterations instead of the reported 10-20) while monitoring weight norms, FID scores, and sample diversity to identify potential accumulation of errors in the reverse ODE processes over time.