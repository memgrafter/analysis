---
ver: rpa2
title: Explicitly Representing Syntax Improves Sentence-to-layout Prediction of Unexpected
  Situations
arxiv_id: '2401.14212'
source_url: https://arxiv.org/abs/2401.14212
tags:
- layout
- syntax
- language
- object
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how syntax representations affect 2D layout
  prediction from text, particularly in unexpected situations. Current models perform
  well on in-domain data but struggle with novel object combinations, revealing reliance
  on data correlations rather than compositional understanding.
---

# Explicitly Representing Syntax Improves Sentence-to-layout Prediction of Unexpected Situations

## Quick Facts
- **arXiv ID**: 2401.14212
- **Source URL**: https://arxiv.org/abs/2401.14212
- **Reference count**: 34
- **Primary result**: Explicit syntax encoding combined with structural loss improves compositional generalization in 2D layout prediction from text, increasing F1 scores by up to 40% on unexpected object combinations.

## Executive Summary
This study investigates how syntax representations affect 2D layout prediction from text, particularly in unexpected situations. Current models perform well on in-domain data but struggle with novel object combinations, revealing reliance on data correlations rather than compositional understanding. The authors introduce a new test set, USCOCO, containing grammatically correct sentences with unusual object interactions, and a novel structural loss function that enforces alignment between syntactic structure and visual embeddings. Their experiments show that explicitly embedding syntax (via constituency trees) combined with the structural loss significantly improves generalization to unseen compositions, increasing F1 scores by up to 40% on USCOCO compared to baseline models. The structural loss also aids implicit-syntax models by encouraging syntax retention.

## Method Summary
The method introduces a structural loss function that enforces alignment between syntactic structure and visual object embeddings in 2D layout prediction. The approach uses linearized constituency parse trees as additional input tokens for explicit syntax models (PLMmask, TG), while implicit syntax models (GPT-2 variants) rely on learned correlations. A non-autoregressive parallel decoder (PAR) generates visual object embeddings conditioned on the text and all other objects simultaneously. The structural loss uses contrastive learning to pull visual object embeddings close to positional embeddings of parse tree nodes while pushing non-matching pairs apart. The loss is combined with standard layout prediction losses and applied during training with text encoders frozen. The method is evaluated on COCO and a new USCOCO dataset containing unexpected object combinations.

## Key Results
- Explicit syntax models with structural loss achieve up to 40% higher F1Dpw scores on USCOCO compared to baseline models
- Non-autoregressive PAR decoding outperforms autoregressive SEQ for layout generation
- Structural loss improves syntax retention in later layers for implicit syntax models, as shown by constituency tree probes
- GPT-2shuffleBllip model performs nearly as well as baseline, indicating current models rely on word co-occurrences rather than syntax

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicitly encoding constituency parse trees in sentence representations improves generalization to unseen object compositions.
- Mechanism: By adding linearized parse tree tokens (parentheses and tags) to the input sequence, the text encoder learns to represent syntax explicitly. This structure allows the layout predictor to map syntactic constituents to visual objects more accurately.
- Core assumption: Recursive constituency structures mirror recursive visual compositions in the world, and learning this structure explicitly aids generalization.
- Evidence anchors:
  - [abstract]: "We propose a novel structural loss function that better enforces the syntactic structure of the input sentence and show large performance gains in the task of 2D spatial layout prediction conditioned on text."
  - [section 3.2.1]: "The models that explicitly embed syntax take a linearized version of the constituency trees as input, using brackets and constituent tags as tokens in addition to the input sentence C, to end up with Clin."
  - [corpus]: Weak evidence; corpus provides no direct citations, but related work on explicit syntax in language models (e.g., PLM, TG) supports the claim.
- Break condition: If the parser produces incorrect trees for out-of-domain or grammatically incorrect sentences, the explicit syntax models may underperform implicit ones.

### Mechanism 2
- Claim: The structural loss enforces alignment between syntactic structure and visual object embeddings, improving compositional generalization.
- Mechanism: The loss computes similarity between visual object embeddings and positional embeddings of parse tree nodes, using contrastive learning to pull matching pairs close and non-matching pairs apart.
- Core assumption: Visual object embeddings should preserve the constituency structure of the sentence to allow proper mapping between syntax and layout.
- Evidence anchors:
  - [abstract]: "We propose a novel structural loss function that better retains the syntactic structure of the sentence in its representation by enforcing the alignment between the syntax tree embeddings and the output of the downstream task, in our case the visual embeddings."
  - [section 3.3.2]: "In a contrastive manner the loss forces the visual object representations vk to be close to the positional embeddings epos j, but far from those ˆepos j of other sentences in the minibatch."
  - [corpus]: No direct citations; however, contrastive learning and alignment losses are well-established in multimodal learning literature.
- Break condition: If the loss weight is too high or too low, or if the model overfits to the training data's common patterns, the benefit may disappear.

### Mechanism 3
- Claim: Non-autoregressive parallel decoding (PAR) outperforms autoregressive sequential decoding (SEQ) for layout generation.
- Mechanism: PAR generates all objects in parallel, conditioning on all other objects via self-attention, while SEQ generates objects one by one conditioned only on previous objects.
- Core assumption: Visual object arrangements are unordered sets, not sequences, so parallel modeling is more appropriate.
- Evidence anchors:
  - [section 5.1.2]: "PAR obtains a significantly better precision than SEQ, both with and without object positions... This could be attributed to the fact that the nth prediction with SEQ is conditioned only on the text and n-1 preceding objects, while with PAR, all predictions are conditioned on the text and on all other objects."
  - [section 3.3.1]: "The fact that for generating language, autoregressive models like SEQ are superior to non-autoregressive models like PAR, but vice versa for generating a set of visual objects, may be due to the inherent sequential character of language, as opposed to the set of visual objects in a layout, which does not follow a natural sequential order."
  - [corpus]: No direct citations; but parallels with non-autoregressive object detection models like DETR support the claim.
- Break condition: If object relationships are highly sequential or if the model needs to enforce strict generation order, PAR may underperform SEQ.

## Foundational Learning

- Concept: Constituency parse trees and their linearization
  - Why needed here: The paper uses linearized parse trees as input to text encoders that explicitly model syntax.
  - Quick check question: What is the difference between a dependency parse and a constituency parse, and why does this paper use the latter?

- Concept: Contrastive learning and alignment losses
  - Why needed here: The structural loss uses contrastive learning to align visual object embeddings with parse tree positional embeddings.
  - Quick check question: How does a contrastive loss differ from a standard classification or regression loss?

- Concept: Transformer self-attention and its masking schemes
  - Why needed here: Models like TG use custom masking to enforce recursive composition of syntax; understanding attention masking is key to grasping how explicit syntax is modeled.
  - Quick check question: What is the effect of masking certain attention heads to only attend to constituents vs. the rest of the sentence?

## Architecture Onboarding

- Component map:
  Text encoders (PLM, PLMmask, TG, GPT-2 variants) -> Layout predictors (PAR, SEQ, ObjLSTM) -> Visual object embeddings -> Loss functions (Llabel, LL1, LgIoU, Lprop, Lrel, Llen, Lstruct)

- Critical path:
  1. Parse input sentence → linearized constituency tree
  2. Encode sentence (with or without syntax tokens) → text embeddings
  3. Layout predictor maps embeddings → visual object embeddings
  4. Apply losses (including structural loss if using explicit syntax models)
  5. Optimize layout predictor (text encoder frozen)

- Design tradeoffs:
  - Explicit syntax models require a parser and have higher computational cost due to extra syntax tokens, but generalize better to unseen compositions.
  - Implicit syntax models are faster and more robust to parser errors, but rely on learned correlations and struggle with unexpected situations.
  - PAR is faster and more accurate for unordered sets, but may underperform SEQ if generation order matters.

- Failure signatures:
  - Low F1/F1Dpw on USCOCO but high scores on Dindom: model overfitting to common patterns, not generalizing.
  - Poor constituency tree probe results: model not retaining syntax in later layers.
  - Shuffled-word model (GPT-2shuffleBllip) performs nearly as well as baseline: model relying on word co-occurrences, not syntax.

- First 3 experiments:
  1. Train PAR + GPT-2Bllip on COCO, evaluate F1 and F1Dpw on Dindom and USCOCO.
  2. Train PAR + TG + Lstruct (λ1=0.25), compare F1Dpw and Rerepl to baseline.
  3. Run constituency tree probe on GPT-2Bllip and TG with and without Lstruct, compare probing F1 across layers.

## Open Questions the Paper Calls Out

- How do the proposed structural loss function and explicit syntax models perform on out-of-distribution data beyond the USCOCO dataset, such as in other compositionality tasks or different domains?
- What is the impact of using the structural loss function on models with implicit syntax when the input sentences contain grammatical errors or ambiguities?
- How do the proposed structural loss function and explicit syntax models compare to recent large language models like GPT-4 in terms of layout prediction for unexpected situations?

## Limitations

- Parser dependency creates robustness concerns when dealing with out-of-domain sentences or grammatically incorrect inputs
- Structural loss has multiple hyperparameters (λ1-λ7) with unclear sensitivity and optimal values
- USCOCO dataset, while valuable, is curated and limited in size, raising questions about real-world generalization

## Confidence

- **High Confidence**: Structural loss improves F1Dpw scores on USCOCO by up to 40%; PAR outperforms SEQ for layout generation; explicit syntax models better generalize to unexpected situations
- **Medium Confidence**: Constituency structure improves compositional generalization (requires assumption about recursive visual compositions); Lstruct helps implicit syntax models retain syntax (supported by probing but parser-dependent); shuffled-word model performance indicates reliance on word co-occurrences
- **Low Confidence**: Exact structural loss implementation details; real-world generalization beyond USCOCO; robustness to parser errors in out-of-domain contexts

## Next Checks

1. **Parser Robustness Analysis**: Evaluate the performance gap between explicit syntax models and implicit models on USCOCO when using a deliberately corrupted or out-of-domain parser to quantify dependence on parser quality.

2. **Hyperparameter Sensitivity Study**: Systematically vary the structural loss weights (λ1 through λ7) and measure their impact on both USCOCO performance and constituency tree probe results to reveal robustness to weight configurations.

3. **Real-world Generalization Test**: Apply the best-performing models to real-world descriptions of unexpected situations from image captioning datasets and measure correlation between USCOCO performance and out-of-domain performance.