---
ver: rpa2
title: Can Large Language Models Make the Grade? An Empirical Study Evaluating LLMs
  Ability to Mark Short Answer Questions in K-12 Education
arxiv_id: '2405.02985'
source_url: https://arxiv.org/abs/2405.02985
tags:
- answer
- questions
- performance
- student
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated whether large language models can effectively
  grade short answer questions in K-12 education. Using a novel dataset of 1,710 student
  responses to open-ended questions in Science and History, the researchers tested
  GPT-3.5 and GPT-4 with different prompt engineering strategies.
---

# Can Large Language Models Make the Grade? An Empirical Study Evaluating LLMs Ability to Mark Short Answer Questions in K-12 Education

## Quick Facts
- arXiv ID: 2405.02985
- Source URL: https://arxiv.org/abs/2405.02985
- Authors: Owen Henkel; Adam Boxer; Libby Hills; Bill Roberts
- Reference count: 0
- Primary result: GPT-4 with few-shot prompting achieved Cohen's Kappa of 0.70, matching expert human rater performance (Kappa 0.75) in grading K-12 short answer questions

## Executive Summary
This study evaluates whether large language models can effectively grade short answer questions in K-12 education. Using a novel dataset of 1,710 student responses to open-ended questions in Science and History, the researchers tested GPT-3.5 and GPT-4 with different prompt engineering strategies. The findings demonstrate that GPT-4 with few-shot prompting can reliably assess open-ended formative assessment tasks, offering a potentially valuable tool for supporting low-stakes assessment in K-12 education while providing significant time savings for teachers.

## Method Summary
The researchers created a novel dataset of 1,710 student responses to open-ended questions in Science and History. They tested GPT-3.5 and GPT-4 using zero-shot and few-shot prompting strategies to classify responses as correct or incorrect. The models' performance was evaluated using Cohen's Kappa statistic, comparing against ground truth labels established by expert human raters. The study systematically varied question types, grade levels, and subjects to assess model performance across different educational contexts.

## Key Results
- GPT-4 with few-shot prompting achieved a Cohen's Kappa of 0.70, closely matching expert human rater performance (Kappa 0.75)
- Few-shot learning improved model performance from Kappa 0.68 (zero-shot) to Kappa 0.70
- GPT-4 showed dramatically improved recall (0.85) compared to GPT-3.5, with only slightly decreased precision
- Model performance showed no significant difference across grade levels (3-12) or subjects tested (Science and History)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 with few-shot prompting achieves near-human performance in grading open-ended questions.
- Mechanism: Few-shot examples provide contextual cues that help the model generalize grading criteria to unseen student responses, reducing ambiguity in edge cases.
- Core assumption: The provided few-shot examples are representative of the grading rubric and cover common patterns of correct and incorrect responses.
- Evidence anchors:
  - [abstract] "GPT-4 with few-shot prompting achieved a Cohen's Kappa of 0.70, closely matching expert human rater performance (Kappa 0.75)"
  - [section] "few-shot learning modestly improved model performance (Kappa 0.70 with few-shot prompting versus Kappa 0.68 with zero-shot prompting)"
- Break condition: If the few-shot examples do not cover the full range of acceptable answers or introduce bias, the model's performance may degrade or become inconsistent.

### Mechanism 2
- Claim: GPT-4's strong recall (0.85) compensates for slightly lower precision compared to GPT-3.5, resulting in better overall grading performance.
- Mechanism: GPT-4's architecture and training allow it to recognize a broader set of correct answers, reducing false negatives (incorrectly marking correct answers as wrong).
- Core assumption: The trade-off between recall and precision is acceptable in a formative assessment context where over-marking is preferable to under-marking.
- Evidence anchors:
  - [section] "this improvement was primarily due to GPT-4 dramatically improving recall while only slightly decreasing precision"
  - [section] "Analogically, we might be able to understand this as GPT-3.5 being a 'harsh-grader,' classifying too many student answers as being incorrect"
- Break condition: In high-stakes assessment contexts, the lower precision could lead to unacceptable false positives, requiring a different model configuration.

### Mechanism 3
- Claim: The inherent ambiguity in open-ended questions creates a ceiling for both human and model agreement, explaining why model performance plateaus near human levels.
- Mechanism: When questions require subjective judgment (e.g., "did the student include enough detail?"), individual raters' interpretations naturally vary, and models trained on these judgments inherit this variability.
- Core assumption: The dataset reflects real-world grading challenges where clear-cut answers are rare and edge cases dominate.
- Evidence anchors:
  - [section] "human raters may have based their choice of whether an answer was correct on their views regarding the importance of students using the correct spelling of key terms"
  - [section] "disagreement often represents inherent ambiguity in the task (i.e., the task isn't well-specified enough)"
- Break condition: If grading rubrics were made more objective and specific, both human and model agreement could potentially exceed current levels.

## Foundational Learning

- Concept: Cohen's Kappa statistic
  - Why needed here: The paper uses Cohen's Kappa to measure inter-rater reliability and model performance, requiring understanding of chance-adjusted agreement measures.
  - Quick check question: What does a Cohen's Kappa of 0.70 indicate about the agreement between the model and ground truth compared to chance agreement?

- Concept: Few-shot learning
  - Why needed here: The study's methodology relies on providing the model with examples to establish grading patterns without fine-tuning.
  - Quick check question: How does few-shot prompting differ from zero-shot prompting in terms of the information provided to the model?

- Concept: Formative vs. summative assessment
  - Why needed here: The paper positions its work in the context of low-stakes formative assessment, which has different accuracy requirements than high-stakes testing.
  - Quick check question: Why might a model with 0.70 Kappa be acceptable for formative assessment but potentially problematic for summative assessment?

## Architecture Onboarding

- Component map: Data preprocessing pipeline → Few-shot prompt generation → OpenAI API call (GPT-4) → Response classification → Performance evaluation (Cohen's Kappa)
- Critical path: The end-to-end process from raw student response to graded output must maintain data integrity and consistent prompt formatting.
- Design tradeoffs: Using GPT-4 provides better recall but at higher API costs compared to GPT-3.5; few-shot prompting improves accuracy but requires maintaining and updating example sets.
- Failure signatures: Inconsistent grading across similar responses suggests prompt engineering issues; dramatic performance drops on new question types indicate domain shift problems.
- First 3 experiments:
  1. Test zero-shot vs. few-shot performance on a small subset of questions to quantify the impact of examples
  2. Evaluate model performance across different question difficulty levels to identify any systematic biases
  3. Measure processing time and costs for different model configurations to optimize for practical deployment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on open-ended questions that require more complex reasoning or multi-step answers compared to simple factual recall questions?
- Basis in paper: [inferred]
- Why unresolved: The study focused on relatively short answer questions across Science and History, but did not systematically vary the complexity of reasoning required. The authors mention exploring performance across different question types but don't provide detailed analysis of how reasoning complexity affects model performance.
- What evidence would resolve it: Additional experiments with questions specifically designed to vary in cognitive complexity (e.g., Bloom's taxonomy levels), comparing LLM performance across these different complexity levels while controlling for other variables.

### Open Question 2
- Question: What is the impact of question ambiguity on LLM grading performance, and how does this compare to human rater disagreement patterns?
- Basis in paper: [explicit]
- Why unresolved: The paper notes that human rater disagreement typically occurred with "edge cases" where judgment was required, suggesting inherent ambiguity in the task. However, they didn't systematically analyze how different types of ambiguity affect LLM performance compared to human raters.
- What evidence would resolve it: Detailed analysis categorizing questions by type of ambiguity (semantic, contextual, requirement specification) and comparing LLM performance patterns against human rater disagreement patterns for each category.

### Open Question 3
- Question: How does the performance of LLMs vary across different subject areas beyond Science and History, particularly in subjects requiring more subjective evaluation?
- Basis in paper: [explicit]
- Why unresolved: While the study tested two subjects (Science and History), the authors acknowledge the need to expand the scope to other subjects. They note modest correlations between subject and model accuracy but don't provide a comprehensive analysis across diverse subject areas.
- What evidence would resolve it: Testing the same LLM configurations across a wider range of subjects (e.g., mathematics, literature, social studies) with varying degrees of subjective evaluation required, and comparing performance patterns across these domains.

### Open Question 4
- Question: What specific features of student responses (beyond subject and grade level) are most predictive of LLM grading accuracy?
- Basis in paper: [inferred]
- Why unresolved: The authors suggest that features like the degree of judgment required to assess correctness might be more predictive than subject or grade level, but they didn't conduct a systematic analysis of response features.
- What evidence would resolve it: Comprehensive feature analysis examining various aspects of student responses (response length, vocabulary complexity, presence of key terms, logical structure) to identify which features most strongly correlate with LLM grading accuracy.

## Limitations

- The dataset, though substantial at 1,710 responses, may not capture the full diversity of K-12 student responses across different cultural contexts and socioeconomic backgrounds.
- The evaluation focused on Science and History questions, leaving uncertainty about performance on other subject areas like mathematics or language arts.
- The few-shot examples were manually curated, raising concerns about potential bias in the training examples and their generalizability to new question types.

## Confidence

- **High confidence**: GPT-4's ability to match human grading performance on the specific dataset tested
- **Medium confidence**: Generalizability of these results to broader educational contexts
- **Low confidence**: Model's performance on questions requiring creative or highly subjective responses

## Next Checks

1. **Cross-Cultural Validation**: Test the model on student responses from diverse cultural and linguistic backgrounds to assess whether performance degrades with non-standard English usage or culturally-specific examples.

2. **Subject Area Generalization**: Evaluate model performance on questions from mathematics, literature, and arts subjects to determine if the observed accuracy is subject-specific or generalizable across the K-12 curriculum.

3. **Longitudinal Stability Test**: Conduct a time-series evaluation where the same questions are administered to students over multiple academic years to assess whether the model maintains consistent performance as student response patterns evolve.