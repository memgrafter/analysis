---
ver: rpa2
title: 'Amuro and Char: Analyzing the Relationship between Pre-Training and Fine-Tuning
  of Large Language Models'
arxiv_id: '2408.06663'
source_url: https://arxiv.org/abs/2408.06663
tags:
- fine-tuning
- pre-training
- performance
- arxiv
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the relationship between pre-training
  and fine-tuning in large language models (LLMs) by fine-tuning multiple intermediate
  pre-training checkpoints. Key findings include: (1) supervised fine-tuning improves
  in-distribution task performance but can cause forgetting of domain knowledge or
  tasks the model was previously capable of solving; (2) fine-tuned models show high
  sensitivity to evaluation prompts, which can be alleviated through additional pre-training;
  (3) continued pre-training improves the model in latent ways that only become apparent
  after fine-tuning; (4) datasets that the model already performs well on during pre-training
  benefit less from fine-tuning than those where the model does not demonstrate capability.'
---

# Amuro and Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models

## Quick Facts
- arXiv ID: 2408.06663
- Source URL: https://arxiv.org/abs/2408.06663
- Authors: Kaiser Sun; Mark Dredze
- Reference count: 40
- This study investigates the relationship between pre-training and fine-tuning in large language models by fine-tuning multiple intermediate pre-training checkpoints.

## Executive Summary
This paper investigates how pre-training and fine-tuning interact in large language models by systematically fine-tuning multiple intermediate pre-training checkpoints. The study reveals that supervised fine-tuning improves in-distribution task performance but can cause catastrophic forgetting of domain knowledge or tasks the model was previously capable of solving. The research also demonstrates that fine-tuned models show high sensitivity to evaluation prompts, which can be alleviated through additional pre-training. Notably, continued pre-training improves the model in latent ways that only become apparent after fine-tuning, and datasets where the model already performs well during pre-training benefit less from fine-tuning than those where the model does not demonstrate capability.

## Method Summary
The study fine-tunes two models (OLMo-1B and Llama3-8B) on 18 datasets using supervised and instruction-based fine-tuning approaches. Researchers saved intermediate pre-training checkpoints and applied fine-tuning at each stage, then evaluated performance across different task formats (default, instruction, input-output) using few-shot evaluation with 4 examples. The analysis examines task format sensitivity, domain knowledge forgetting, task transfer effects, and the impact of pre-training stage on fine-tuning effectiveness. Hyperparameters were fixed with batch size 8 and learning rates tuned from {2×10^-5, 2×10^-6, 2×10^-7}.

## Key Results
- Supervised fine-tuning improves in-distribution task performance but causes forgetting of domain knowledge or previously solvable tasks
- Fine-tuned models show high sensitivity to evaluation prompts, which can be reduced through additional pre-training
- Continued pre-training improves models in latent ways that only become apparent after fine-tuning
- Datasets where the model already performs well during pre-training benefit less from fine-tuning than those where capability is not yet demonstrated

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning teaches task formats but leads to forgetting of unused abilities
- Mechanism: When a model is fine-tuned on specific tasks, it learns the input-output format and task-specific patterns. This format learning improves in-distribution performance but can cause the model to lose abilities on tasks that were not included in fine-tuning, as the model optimizes for the new format at the expense of previously learned capabilities.
- Core assumption: The model has limited capacity to maintain all learned abilities simultaneously, and fine-tuning shifts the model's focus toward the new task format.
- Evidence anchors:
  - [abstract]: "supervised fine-tuning improves in-distribution task performance but can cause forgetting of domain knowledge or tasks the model was previously capable of solving"
  - [section]: "The model learns to perform the task with in-domain knowledge, but it may, in turn, forget information more distant from what is learned in fine-tuning"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.519, average citations=0.0. Weak corpus evidence for this specific mechanism.
- Break condition: If the model has sufficient capacity to maintain all learned abilities, or if fine-tuning includes a diverse set of tasks that cover the model's existing capabilities.

### Mechanism 2
- Claim: Some datasets can be learned during pre-training while others cannot
- Mechanism: During pre-training, the model encounters a wide variety of text patterns and tasks implicitly. Some datasets align well with the implicit patterns learned during pre-training, allowing the model to solve them without explicit fine-tuning. Other datasets require specific patterns or formats that are not naturally present in pre-training data, making them unlearnable until fine-tuning provides explicit guidance.
- Core assumption: The pre-training data contains implicit information that can solve some tasks but not others, and the model's ability to extract this information varies based on the task's alignment with pre-training patterns.
- Evidence anchors:
  - [abstract]: "tasks for which the model already performs well during pre-training benefit much less from fine-tuning than those where the model does not demonstrate capability"
  - [section]: "We discover a dichotomy between datasets. Some are learned during model pre-training, while others show no improvements during pre-training"
  - [corpus]: Weak corpus evidence for this specific mechanism.
- Break condition: If all datasets are learnable during pre-training, or if fine-tuning can teach any task regardless of pre-training exposure.

### Mechanism 3
- Claim: Pre-training improves models in latent ways that only become apparent after fine-tuning
- Mechanism: During pre-training, the model learns representations and patterns that are not immediately useful for specific tasks. When fine-tuning is applied, these latent representations become activated and enable the model to perform better on tasks that were not explicitly learned during pre-training. The improvement is not visible during pre-training because the model lacks the specific task format to express its capabilities.
- Core assumption: Pre-training creates general representations that can be specialized for specific tasks through fine-tuning, and these representations have value even when not immediately apparent.
- Evidence anchors:
  - [abstract]: "continued pre-training improves the model in latent ways that only become apparent after fine-tuning"
  - [section]: "However, for MNLI (not learned during pre-training), fine-tuning dramatically improves the model. Interestingly, later checkpoints achieve better results after fine-tuning, even when the performance of the pre-trained model is unchanged"
  - [corpus]: Weak corpus evidence for this specific mechanism.
- Break condition: If all improvements during pre-training are immediately visible, or if fine-tuning cannot activate latent pre-training knowledge.

## Foundational Learning

- Concept: Pre-training vs Fine-tuning distinction
  - Why needed here: Understanding the different roles of pre-training (general knowledge acquisition) and fine-tuning (task-specific adaptation) is fundamental to analyzing their relationship
  - Quick check question: What is the primary difference between pre-training and fine-tuning in terms of what the model learns?

- Concept: Catastrophic forgetting
  - Why needed here: Fine-tuning can cause the model to forget previously learned abilities, which is central to understanding the trade-offs between pre-training and fine-tuning
  - Quick check question: What phenomenon describes the loss of previously learned capabilities when training on new tasks?

- Concept: Task format sensitivity
  - Why needed here: The model's performance depends heavily on how tasks are formatted, which affects how pre-training and fine-tuning interact
  - Quick check question: How does changing the task format (e.g., from instruction-based to input-output) affect model performance?

## Architecture Onboarding

- Component map:
  - Pre-training stage: Large corpus processing, token prediction learning
  - Fine-tuning stage: Task-specific adaptation, format learning
  - Evaluation: Multiple task formats and datasets
  - Checkpoints: Intermediate pre-training states for analysis

- Critical path:
  1. Pre-train model on large corpus
  2. Save intermediate checkpoints
  3. Fine-tune each checkpoint on specific tasks
  4. Evaluate pre-trained and fine-tuned models on multiple datasets
  5. Analyze performance differences across checkpoints and fine-tuning conditions

- Design tradeoffs:
  - Pre-training depth vs. fine-tuning effectiveness: More pre-training may provide better latent representations but also increases computational cost
  - Task format consistency: Using consistent formats across pre-training and fine-tuning may improve transfer but reduces format learning benefits
  - Checkpoint frequency: More checkpoints provide better analysis resolution but increase storage and computational requirements

- Failure signatures:
  - No improvement from fine-tuning: Indicates the task may be learnable during pre-training
  - Significant forgetting after fine-tuning: Suggests format learning is dominating over general capability retention
  - Inconsistent performance across task formats: May indicate incomplete format learning or format sensitivity

- First 3 experiments:
  1. Fine-tune a pre-trained model on a specific task and evaluate on both in-distribution and out-of-distribution datasets to observe forgetting effects
  2. Fine-tune multiple pre-training checkpoints on the same task to observe how pre-training stage affects fine-tuning effectiveness
  3. Evaluate the same fine-tuned model with different task formats (default, instruction, input-output) to measure format sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which pre-training improves model performance on tasks that are not directly learned during pre-training, and how can this latent knowledge be identified and measured without requiring fine-tuning?
- Basis in paper: [explicit] The paper observes that pre-training improves model performance in ways that are only revealed after fine-tuning, particularly for tasks that are not learned during pre-training.
- Why unresolved: The paper empirically observes this phenomenon but does not provide a theoretical explanation for why or how pre-training contributes to task performance in latent ways.
- What evidence would resolve it: Developing a method to identify and measure latent task capabilities during pre-training without requiring fine-tuning would resolve this question.

### Open Question 2
- Question: How does the effect of fine-tuning on model performance vary across different model sizes and architectures, and are the observed patterns of forgetting and learning consistent across different model families?
- Basis in paper: [inferred] The paper primarily studies OLMo-1B and Llama3-8B, but acknowledges that results may vary across different model sizes and architectures.
- Why unresolved: The paper only examines two specific models and notes that emergent capabilities may be concealed in smaller models, but does not systematically investigate how results generalize to other architectures.
- What evidence would resolve it: Conducting similar experiments across a broader range of model sizes and architectures would determine if the observed patterns are consistent or vary significantly.

### Open Question 3
- Question: What is the optimal stopping point for pre-training that maximizes downstream fine-tuning performance while minimizing computational costs, and how can this point be identified without exhaustive experimentation?
- Basis in paper: [explicit] The paper observes that early stopping in pre-training may not be detrimental to downstream fine-tuning performance and could be cost-effective.
- Why unresolved: While the paper empirically observes that benefits of fine-tuning continue to increase until a certain threshold, it does not provide a method to identify this optimal stopping point without fine-tuning intermediate checkpoints.
- What evidence would resolve it: Developing a predictive method to identify the optimal pre-training stopping point based on pre-training dynamics alone would resolve this question.

## Limitations

- The analysis relies on a relatively small corpus of 25 related papers with low citation counts, suggesting limited external validation of the proposed mechanisms
- The study focuses on two specific model architectures (OLMo-1B and Llama3-8B), limiting generalizability to other model families
- The evaluation of latent pre-training improvements is indirect, as it relies on observing performance changes after fine-tuning rather than measuring latent representations directly

## Confidence

- **High confidence**: The mechanism of fine-tuning improving in-distribution performance while causing forgetting of unused abilities is well-supported by direct evidence from the abstract and section statements, with clear experimental observations.
- **Medium confidence**: The dichotomy between datasets learnable during pre-training versus those requiring fine-tuning is supported by experimental results but lacks extensive external validation given the limited corpus evidence.
- **Medium confidence**: The claim about latent pre-training improvements becoming apparent after fine-tuning is supported by specific examples (MNLI) but the mechanism is less directly observable and relies on interpretation of performance patterns.

## Next Checks

1. **Dataset dependency verification**: Replicate the study with a broader range of datasets to confirm whether the observed dichotomy between pre-training-learnable and non-learnable datasets holds across different domains and task types.

2. **Format sensitivity quantification**: Systematically measure how different task formats affect performance across all 18 datasets to determine if format sensitivity is consistent or varies by task type.

3. **Latent representation analysis**: Use interpretability tools or probing tasks to directly examine whether pre-training creates latent representations that become useful only after fine-tuning, rather than inferring this from performance changes.