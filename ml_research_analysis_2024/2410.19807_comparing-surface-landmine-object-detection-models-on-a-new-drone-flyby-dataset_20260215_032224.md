---
ver: rpa2
title: Comparing Surface Landmine Object Detection Models on a New Drone Flyby Dataset
arxiv_id: '2410.19807'
source_url: https://arxiv.org/abs/2410.19807
tags:
- detection
- object
- images
- https
- surface
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of surface landmine detection
  using drone imagery, which is slow, dangerous, and expensive with traditional methods.
  The authors created a custom dataset of drone images and videos of POM-2 and POM-3
  Russian surface landmines at different altitudes and terrains.
---

# Comparing Surface Landmine Object Detection Models on a New Drone Flyby Dataset

## Quick Facts
- arXiv ID: 2410.19807
- Source URL: https://arxiv.org/abs/2410.19807
- Reference count: 39
- Primary result: YOLOF achieved mAP of 0.89 at 10m altitude, outperforming DETR, Sparse-RCNN, and VFNet (all around 0.82)

## Executive Summary
This paper addresses the challenge of surface landmine detection using drone imagery, which is slow, dangerous, and expensive with traditional methods. The authors created a custom dataset of drone images and videos of POM-2 and POM-3 Russian surface landmines at different altitudes and terrains. They trained and compared four object detection models (YOLOF, DETR, Sparse-RCNN, and VFNet) on this dataset using Azure cloud infrastructure. YOLOF outperformed the other models with a mean Average Precision (mAP) score of 0.89 for 10m altitude drone images, while the other models achieved mAP scores around 0.82. YOLOF also had faster training time (56 minutes) and was quicker to set up hyperparameters.

## Method Summary
The authors created a custom dataset of drone images and videos of POM-2 and POM-3 Russian surface landmines at 2.5m, 5m, and 10m altitudes over different terrains. They trained and compared four object detection models (YOLOF, DETR, Sparse-RCNN, and VFNet) on this dataset using Azure cloud infrastructure with NVIDIA V100 GPUs. Images were resized to 800×450 resolution for training. The training set consisted of 320 images and the test set contained 70 images. Models were evaluated using mAP (mean Average Precision) scores across different altitudes and terrain conditions.

## Key Results
- YOLOF achieved mAP of 0.89 for 10m altitude drone images, outperforming other models (DETR, VFNet, Sparse-RCNN) which all scored around 0.82
- All four models performed well at 2.5m and 5m altitudes with minimal performance differences
- YOLOF had faster training time (56 minutes) and quicker hyperparameter setup compared to other models
- Thermal imaging with FLIR One Edge Pro LWIR camera did not improve detection in heavy vegetation due to limited penetration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: YOLOF achieves superior performance for 10m altitude detection due to its one-stage architecture with global receptive fields.
- Mechanism: YOLOF uses a fully convolutional one-stage detector with large global receptive fields that capture spatial context across the entire image. This helps distinguish small landmines from complex backgrounds at higher altitudes where objects are more indiscernible.
- Core assumption: The global receptive fields in YOLOF's FPN backbone provide better context integration for small object detection compared to transformer-based DETR or sparse proposal methods.
- Evidence anchors:
  - [abstract] "YOLOF outperforming other models with a mAP score of 0.89 while DETR, VFNET and Sparse-RCNN mAP scores are all around 0.82 for drone images taken from 10m AGL"
  - [section] "We see in Table 7 that for 10m AGL, when inference is performed using original sized images...YOLOF outperforms other models."
  - [corpus] Weak evidence - no related papers directly compare YOLOF performance for small object detection at altitude

### Mechanism 2
- Claim: All four models perform well at 2.5m and 5m altitudes because the landmines are large enough relative to image resolution for feature extraction.
- Mechanism: At lower altitudes, the POM-2 and POM-3 landmines occupy sufficient pixel space in the 800x450 resized images to allow convolutional feature extraction to reliably identify distinctive mine characteristics regardless of the detection architecture.
- Core assumption: The 800x450 resolution preserves sufficient detail for landmines at 2.5-5m altitude, making model architecture differences less significant.
- Evidence anchors:
  - [abstract] "all 4 detectors do well with YOLOF outperforming other models with a mAP score of 0.89 while DETR, VFNET and Sparse-RCNN mAP scores are all around 0.82"
  - [section] "We see in Table 5-6 that all models perform well for 2.5m and 5m AGL"
  - [corpus] Weak evidence - no corpus papers directly address this altitude-resolution relationship

### Mechanism 3
- Claim: Thermal imaging does not improve detection in heavy vegetation because the LWIR wavelengths cannot penetrate foliage.
- Mechanism: The FLIR One Edge Pro LWIR camera captures long-wave infrared radiation (8-14 μm) which is absorbed and scattered by vegetation, preventing the thermal signature of landmines from being detected when obscured.
- Core assumption: The electromagnetic properties of vegetation at LWIR wavelengths create sufficient attenuation to block thermal signatures from surface objects.
- Evidence anchors:
  - [section] "We also found that thermal images do not help in the presence of heavy vegetation obstruction. Longer wavelength electromagnetic waves will likely allow for detection in the presence of heavy vegetation"
  - [section] "Regarding thermal image inferencing, we could only inference on 2.5m AGL images since thermal camera we used had limited resolution"
  - [corpus] Weak evidence - corpus contains papers on thermal object detection but none specifically address foliage penetration limitations

## Foundational Learning

- Concept: Object detection model architectures (one-stage vs two-stage)
  - Why needed here: Understanding the architectural differences between YOLOF, DETR, Sparse-RCNN, and VFNet is critical to interpreting performance differences
  - Quick check question: What is the key architectural difference between one-stage detectors like YOLOF and two-stage detectors like DETR?

- Concept: Mean Average Precision (mAP) as an evaluation metric
  - Why needed here: mAP is the primary metric used to compare model performance across different altitudes and conditions
  - Quick check question: How is mAP calculated and what does an mAP score of 0.89 indicate about detection quality?

- Concept: Dataset preparation and annotation for object detection
  - Why needed here: The custom dataset creation process directly impacts model training quality and generalization
  - Quick check question: Why is manual annotation necessary for this landmine detection dataset rather than using automated methods?

## Architecture Onboarding

- Component map: Drone image capture -> Image preprocessing (resizing to 800x450) -> Manual annotation -> Training on Azure V100 -> Model checkpoints -> Inference pipeline -> mAP calculation -> Performance comparison

- Critical path: Data collection and annotation (manual effort) -> Model training on Azure V100 cluster (computational bottleneck) -> Inference and evaluation across altitudes -> Performance analysis and documentation

- Design tradeoffs:
  - Resolution vs. computational efficiency: Resizing from 1080p to 800x450 balances detail preservation with training speed
  - Model complexity vs. inference speed: YOLOF's simpler architecture enables faster inference despite similar accuracy
  - Dataset size vs. generalization: Limited dataset (320 training, 70 test) may constrain model generalization to unseen conditions

- Failure signatures:
  - Poor mAP scores at 10m altitude indicate insufficient feature resolution for small objects
  - High loss values during training suggest learning rate or architecture issues
  - Inconsistent performance across similar terrain types indicates overfitting to specific backgrounds

- First 3 experiments:
  1. Train all four models with identical hyperparameters to establish baseline performance differences
  2. Evaluate each model's performance at 2.5m, 5m, and 10m altitudes to identify altitude-specific limitations
  3. Test thermal vs. visual image performance to quantify the benefit of multi-spectral data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would different electromagnetic spectrum frequencies (beyond LWIR) perform for landmine detection through heavy vegetation?
- Basis in paper: [explicit] "Longer wavelength electromagnetic waves will likely allow for detection in the presence of heavy vegetation [30]. Conducting research using different parts of the electromagnetic spectrum as a way to detect foliage obstructed landmines is proposed as a topic for future research."
- Why unresolved: The paper only tested LWIR thermal imaging, which proved ineffective for heavy vegetation. The authors explicitly propose testing other wavelengths but did not conduct these experiments.
- What evidence would resolve it: Experimental comparison of landmine detection accuracy using various electromagnetic spectrum frequencies (e.g., microwave, terahertz) through different vegetation densities.

### Open Question 2
- Question: What is the optimal altitude range for drone-based landmine detection considering both detection accuracy and operational efficiency?
- Basis in paper: [explicit] "We conclude that all 4 models are generally good detectors when drones are flown between 2.5m and 5m AGL and in the presence of light vegetation. When drones are flown higher at 10m AGL, YOLOF outperforms other models."
- Why unresolved: The paper tested only three specific altitudes (2.5m, 5m, 10m) and found trade-offs between detection accuracy and altitude. The optimal balance point remains unclear.
- What evidence would resolve it: Systematic testing of detection accuracy across a continuous range of altitudes with various mine types and terrain conditions.

### Open Question 3
- Question: How does mine orientation and partial burial affect detection accuracy across different models and imaging modalities?
- Basis in paper: [explicit] "Given that YOLO improves upon traditional sliding window CNN approaches by predicting bounding boxes and class probabilities for objects directly from the input image in a single forward pass thus making inference speed significantly faster" and "DETR performs better detection when the mine is partially obscured by vegetation."
- Why unresolved: The paper tested mines in relatively standard orientations and surface conditions but did not systematically vary orientation or test partial burial scenarios.
- What evidence would resolve it: Controlled experiments varying mine orientation, partial burial depth, and occlusion levels across all tested models and imaging modalities.

## Limitations
- Limited dataset size (320 training, 70 test images) may constrain model generalization to diverse terrain conditions
- Only tested POM-2 and POM-3 Russian surface landmines, limiting applicability to other mine types
- Thermal imaging limitations through vegetation were identified but alternative wavelengths were not tested

## Confidence
- High Confidence: Model architecture comparisons and mAP score reporting are well-documented and reproducible
- Medium Confidence: Altitude-performance relationships are supported but may not generalize beyond the specific mine types tested
- Low Confidence: Thermal imaging limitations and foliage penetration claims lack sufficient empirical validation

## Next Checks
1. Conduct ablation studies varying image resolution and compare mAP scores to identify the optimal balance between detail preservation and computational efficiency
2. Test model generalization by evaluating performance on landmine datasets from different geographical regions and soil compositions
3. Implement cross-validation with k-folds on the existing dataset to better assess model robustness and variance in performance metrics