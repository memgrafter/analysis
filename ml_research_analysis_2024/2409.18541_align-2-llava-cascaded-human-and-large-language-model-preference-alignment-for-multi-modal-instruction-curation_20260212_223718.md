---
ver: rpa2
title: 'Align$^2$LLaVA: Cascaded Human and Large Language Model Preference Alignment
  for Multi-modal Instruction Curation'
arxiv_id: '2409.18541'
source_url: https://arxiv.org/abs/2409.18541
tags:
- question
- answer
- image
- human
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of noisy and inconsistent synthetic
  instruction data used to train multi-modal large language models (MLLMs). It introduces
  Align2LLaVA, a novel instruction curation pipeline that leverages human preference
  alignment and LLM characteristic alignment to compress and improve the quality of
  synthetic multimodal instructions.
---

# Align$^2$LLaVA: Cascaded Human and Large Language Model Preference Alignment for Multi-modal Instruction Curation

## Quick Facts
- arXiv ID: 2409.18541
- Source URL: https://arxiv.org/abs/2409.18541
- Authors: Hongzhe Huang, Jiang Liu, Zhewen Yu, Li Cai, Dian Jiao, Wenqiao Zhang, Siliang Tang, Juncheng Li, Hao Jiang, Haoyuan Li, Yueting Zhuang
- Reference count: 19
- Key outcome: The paper addresses the problem of noisy and inconsistent synthetic instruction data used to train multi-modal large language models (MLLMs). It introduces Align2LLaVA, a novel instruction curation pipeline that leverages human preference alignment and LLM characteristic alignment to compress and improve the quality of synthetic multimodal instructions. The pipeline involves training reward models on human-annotated data to filter instructions, followed by rewriting and reviewing instructions using the inner LLM to align with its writing style. Extensive experiments demonstrate that Align2LLaVA can maintain or even improve model performance by compressing synthetic multimodal instructions by up to 90%, achieving comparable or superior results to models trained on full datasets across various MLLM benchmarks.

## Executive Summary
The paper addresses the critical challenge of noisy and inconsistent synthetic instruction data used to train multi-modal large language models (MLLMs). The authors propose Align2LLaVA, a cascaded pipeline that leverages human preference alignment and LLM characteristic alignment to compress and improve the quality of synthetic multimodal instructions. The pipeline involves training reward models on human-annotated data to filter instructions, followed by rewriting and reviewing instructions using the inner LLM to align with its writing style. Extensive experiments demonstrate that Align2LLaVA can maintain or even improve model performance by compressing synthetic multimodal instructions by up to 90%, achieving comparable or superior results to models trained on full datasets across various MLLM benchmarks.

## Method Summary
Align2LLaVA introduces a cascaded pipeline for improving synthetic multimodal instruction data quality. The method consists of two main alignment stages: human preference alignment and LLM characteristic alignment. First, human annotators rank instruction quality based on specific criteria, and reward models are trained on this preference data to filter low-quality instructions from the synthetic dataset. Second, the inner LLM rewrites and reviews the filtered instructions to align them with its writing style while preserving semantic content. This approach effectively compresses the instruction dataset while maintaining or improving model performance across various MLLM benchmarks.

## Key Results
- Compress synthetic multimodal instructions by up to 90% (158K to 14K) while maintaining or improving model performance
- Achieve comparable or superior results to models trained on full datasets across 8 benchmarks including VQAv2, VizWiz, ScienceQA, TextVQA, MME-Perception, MMBench, LLaVA-Bench-in-the-Wild, and MM-Vet
- Ablation studies demonstrate the superiority of human-preference-based filtration over random sampling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential human and LLM preference alignment improves data quality by removing noisy instructions and aligning writing style.
- Mechanism: The two-stage filtering process uses human-annotated reward models to remove low-quality instructions, followed by LLM rewriting and review to align instructions with the inner LLM's writing style.
- Core assumption: Human evaluators can effectively rank instruction quality, and reward models trained on this data can generalize to filter large datasets.
- Evidence anchors:
  - [abstract]: "Align2LLaVA... leverages human preference alignment and LLM characteristic alignment to compress and improve the quality of synthetic multimodal instructions."
  - [section 3.2]: Describes the two-stage filtration process using reward models trained on human preference data.
  - [corpus]: Weak. Related works like "MM-Instruct: Generated Visual Instructions for Large Multimodal Model Alignment" also focus on instruction quality but don't specifically address sequential human and LLM alignment.
- Break condition: If human evaluators cannot consistently rank instruction quality, the reward models will not generalize, and the filtration process will fail.

### Mechanism 2
- Claim: Using human preference data to train reward models improves the filtering of synthetic multimodal instructions compared to random sampling.
- Mechanism: Reward models trained on human preference data learn to identify high-quality instructions, which are then used to filter the synthetic dataset.
- Core assumption: Human preference data contains sufficient information to train reward models that can accurately assess instruction quality.
- Evidence anchors:
  - [abstract]: "Extensive experiments demonstrate that we can maintain or even improve model performance by compressing synthetic multimodal instructions by up to 90%."
  - [section 4.3.1]: Ablation study comparing human-preference-based filtration to random sampling shows consistent superiority of the proposed method.
  - [corpus]: Weak. While works like "IIMedGPT: Promoting Large Language Model Capabilities of Medical Tasks by Efficient Human Preference Alignment" use human preference alignment, they don't focus on filtering synthetic multimodal instructions.
- Break condition: If the human preference data is not representative of the overall instruction quality, the reward models will not generalize, and the filtering process will not improve data quality.

### Mechanism 3
- Claim: Aligning instructions with the inner LLM's writing style improves training efficiency and prevents catastrophic forgetting.
- Mechanism: The inner LLM rewrites and reviews instructions to ensure they conform to its writing style while preserving semantic content.
- Core assumption: The inner LLM has a distinct writing style that, when matched, improves training efficiency and prevents catastrophic forgetting.
- Evidence anchors:
  - [abstract]: "For LLM preference alignment, given the instruction selected by the reward model, we propose leveraging the inner LLM used in MLLM to align the writing style of visual instructions with that of the inner LLM itself."
  - [section 3.3]: Describes the LLM alignment process, including rewriting and review stages.
  - [corpus]: Weak. While "Bridging Writing Manner Gap in Visual Instruction Tuning by Creating LLM-aligned Instructions" addresses writing style alignment, it doesn't specifically focus on preventing catastrophic forgetting.
- Break condition: If the inner LLM's writing style is not distinct or does not significantly impact training, the alignment process will not improve efficiency or prevent catastrophic forgetting.

## Foundational Learning

- Concept: Reward models
  - Why needed here: To learn human preferences for instruction quality and filter synthetic datasets.
  - Quick check question: How do reward models trained on human preference data improve the filtering of synthetic multimodal instructions compared to random sampling?

- Concept: Instruction curation
  - Why needed here: To improve the quality and reduce the size of synthetic multimodal instruction datasets.
  - Quick check question: What are the two main components of the Align2LLaVA instruction curation pipeline, and how do they work together to improve data quality?

- Concept: Catastrophic forgetting
  - Why needed here: To understand how aligning instructions with the inner LLM's writing style can prevent the model from forgetting previously learned knowledge.
  - Quick check question: How does aligning instructions with the inner LLM's writing style help prevent catastrophic forgetting during training?

## Architecture Onboarding

- Component map: Human Preference Data Collection -> Human Knowledge Alignment (Reward Model Training and Filtration) -> LLM Characteristic Alignment (Rewriting and Review)
- Critical path: The critical path involves training reward models on human preference data, filtering the synthetic dataset using these models, and then aligning the filtered instructions with the inner LLM's writing style.
- Design tradeoffs: The main tradeoff is between data quality and data quantity. By aggressively filtering the dataset, the model may lose some potentially useful instructions, but it also removes a significant amount of noise.
- Failure signatures: If the reward models are not well-trained, the filtration process may remove high-quality instructions or retain low-quality ones. If the LLM alignment process is not effective, the instructions may not conform to the inner LLM's writing style, leading to reduced training efficiency.
- First 3 experiments:
  1. Train reward models on human preference data and evaluate their accuracy on a held-out test set.
  2. Apply the filtration process to a small subset of the synthetic dataset and manually inspect the results.
  3. Fine-tune a small MLLM on the aligned dataset and compare its performance to a model trained on the full dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the long-term effects of aggressive data compression on MLLM generalization to out-of-distribution tasks?
- Basis in paper: [inferred] The paper demonstrates that Align2LLaVA can compress 158K instructions to 14K (9Ã— smaller) while maintaining or improving performance across 8 benchmarks. However, it does not investigate how this aggressive compression affects the model's ability to handle tasks outside the original training distribution.
- Why unresolved: The paper focuses on in-distribution benchmark performance but does not evaluate cross-domain generalization, zero-shot transfer, or long-term robustness after aggressive compression.
- What evidence would resolve it: Experiments comparing MLLM performance on out-of-distribution datasets (e.g., different cultural contexts, novel visual concepts) between models trained on full vs. compressed datasets over extended time periods.

### Open Question 2
- Question: How does the human preference alignment process scale when applied to multilingual or cross-cultural visual instruction datasets?
- Basis in paper: [explicit] The paper uses 7 CS students as annotators for human preference alignment, establishing unified criteria for data quality assessment. The process is described as labor-intensive but effective for English-language visual instructions.
- Why unresolved: The paper does not address how the annotation process would work with multilingual instructions or cultural variations in what constitutes "good" questions and answers. Scaling to multiple languages or cultures could introduce significant challenges.
- What evidence would resolve it: Comparative studies showing performance differences when applying the same pipeline to multilingual datasets, analysis of annotation consistency across cultural groups, and evaluation of cross-cultural instruction-following capabilities.

### Open Question 3
- Question: What is the optimal balance between human preference alignment and LLM characteristic alignment for different MLLM architectures?
- Basis in paper: [explicit] The paper presents a cascaded approach with two distinct alignment steps, but does not systematically investigate how the relative weighting or sequence of these steps affects different MLLM architectures (e.g., LLaVA vs. Qwen-VL).
- Why unresolved: While the paper shows successful results with Vicuna-7B and Qwen-7B, it does not explore whether the same balance of human and LLM alignment works optimally across different model families or sizes.
- What evidence would resolve it: Ablation studies varying the relative emphasis on human vs. LLM alignment for different MLLM architectures, with performance measured across multiple benchmarks to identify architecture-specific optimal configurations.

## Limitations

- The exact prompts and instructions used for generating synthetic data and guiding human annotation are not fully specified, which could impact reproducibility.
- The implementation details of the reward models, including their architecture and training hyperparameters, are not completely described.
- The human preference data collection process, while detailed in its criteria, may not capture all relevant aspects of instruction quality, potentially leading to suboptimal filtering.

## Confidence

- **High Confidence**: The core mechanism of using human preference alignment followed by LLM characteristic alignment to improve instruction quality is well-supported by the experimental results and ablation studies.
- **Medium Confidence**: The claim that aligning instructions with the inner LLM's writing style improves training efficiency and prevents catastrophic forgetting is supported by the results, but the exact mechanisms and extent of these benefits are not fully explored.
- **Low Confidence**: The generalizability of the proposed method to other MLLM architectures and datasets is uncertain, as the experiments are primarily conducted on the LLaVA model and a specific synthetic instruction dataset.

## Next Checks

1. **Reproducibility Check**: Attempt to reproduce the human preference data collection process and reward model training using the provided criteria and a subset of the synthetic dataset. Evaluate the accuracy of the reward models on a held-out test set.

2. **Ablation Study Extension**: Conduct additional ablation studies to quantify the individual contributions of the human preference alignment and LLM characteristic alignment components to the overall performance improvement. This could involve training models on datasets filtered using only one of the alignment methods.

3. **Generalizability Assessment**: Apply the Align2LLaVA pipeline to a different MLLM architecture and synthetic instruction dataset to assess its generalizability and robustness across various model and data configurations.