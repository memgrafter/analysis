---
ver: rpa2
title: Adversarially Guided Stateful Defense Against Backdoor Attacks in Federated
  Deep Learning
arxiv_id: '2410.11205'
source_url: https://arxiv.org/abs/2410.11205
tags:
- agsd
- clients
- backdoored
- backdoor
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Adversarially Guided Stateful Defense (AGSD) addresses backdoor
  attacks in federated learning by leveraging two key properties of backdoored classifiers:
  adversarial bias and overconfidence. AGSD employs adversarial perturbations on a
  small held-out dataset to compute a trust index that guides cluster selection, without
  relying on unrealistic assumptions about client submissions.'
---

# Adversarially Guided Stateful Defense Against Backdoor Attacks in Federated Deep Learning

## Quick Facts
- arXiv ID: 2410.11205
- Source URL: https://arxiv.org/abs/2410.11205
- Reference count: 40
- Key outcome: AGSD outperforms state-of-the-art defenses with minimal drop in clean accuracy (5% in worst case), even with very small held-out dataset (≤50 samples)

## Executive Summary
This paper introduces Adversarially Guided Stateful Defense (AGSD), a novel approach to defend federated learning against backdoor attacks. AGSD leverages two key properties of backdoored classifiers - adversarial bias and overconfidence - to guide cluster selection without relying on unrealistic assumptions about client submissions. The method maintains a trust state history for each client to adaptively penalize backdoored clients and reward clean clients, achieving superior performance compared to state-of-the-art defenses while using minimal computational resources.

## Method Summary
AGSD addresses backdoor attacks in federated learning by computing trust indices through adversarial perturbations on a small held-out dataset. The method clusters client submissions, uses trust indices to select the best cluster, and maintains a trust history to filter backdoored clients. Unlike existing defenses that rely on unrealistic assumptions about client submissions, AGSD works with very small datasets (typically 50 samples) and can even use out-of-distribution data. The stateful approach adaptively penalizes backdoored clients while rewarding clean clients over multiple rounds.

## Key Results
- Outperforms state-of-the-art defenses with minimal clean accuracy drop (5% in worst case)
- Works effectively with very small held-out dataset (typically 50 samples, ≤0.1% of training data)
- Maintains effectiveness even when using out-of-distribution data
- Successfully defends against various backdoor attacks including VTBA, ITBA, NBA, and IBA

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Backdoored classifiers exhibit higher adversarial bias
- **Mechanism**: When perturbed, backdoored models output target class more often than clean models
- **Core assumption**: Adversarial perturbations transfer effectively from preliminary aggregated model to individual clients
- **Evidence anchors**: Abstract mentions "backdoored submissions are adversarially biased"; Observation 1 shows E[f+(x + A(x)) − yt] > E[f−(x + A(x)) − yt]
- **Break condition**: If perturbations don't transfer effectively, bias metric becomes unreliable

### Mechanism 2
- **Claim**: Backdoored classifiers are adversarially overconfident
- **Mechanism**: Maximum predicted confidence on perturbed inputs is higher for backdoored models
- **Core assumption**: Confidence difference persists across different input distributions
- **Evidence anchors**: Abstract mentions "overconfident compared to clean submissions"; Observation 2 shows max E[f+(x + A(x))] < max E[f−(x + A(x))]
- **Break condition**: If confidence calibration breaks down, overconfidence metric loses discriminative power

### Mechanism 3
- **Claim**: Stateful trust history adaptively penalizes backdoored clients and rewards clean clients
- **Mechanism**: Trust index γi computed per round updates trust history ϕi; clients with ϕi > 0 contribute to model
- **Core assumption**: Trust history updates are stable enough to prevent occasional misclassification from eroding long-term trust
- **Evidence anchors**: Abstract mentions "maintains a trust state history"; Equation (16) shows ϕi = (ϕi + γi ...)
- **Break condition**: If trust updates are too aggressive, clean clients might be penalized for occasional false negatives

## Foundational Learning

- **Concept**: Adversarial perturbations and their transferability
  - Why needed here: AGSD relies on adversarial perturbations to expose differences between clean and backdoored models
  - Quick check question: Can adversarial perturbations generated on preliminary aggregated model effectively transfer to individual client submissions?

- **Concept**: Clustering algorithms (spectral clustering)
  - Why needed here: AGSD uses spectral clustering to separate clean and backdoored client submissions based on differences from preliminary aggregated model
  - Quick check question: Does spectral clustering handle non-convex, varying-variance clusters typical in federated learning updates?

- **Concept**: Trust-based filtering mechanisms
  - Why needed here: AGSD maintains trust history to adaptively penalize backdoored clients and reward clean clients over multiple rounds
  - Quick check question: How does trust history update mechanism balance between being too lenient (allowing backdoors) and too strict (penalizing clean clients)?

## Architecture Onboarding

- **Component map**: Preliminary aggregation -> Clustering -> Guided selection -> Stateful selection
- **Critical path**: 1) Receive client submissions 2) Compute preliminary aggregation 3) Cluster submissions 4) Generate adversarial perturbations on held-out dataset 5) Compute trust index for each client 6) Select cluster with highest average trust index 7) Filter clients in selected cluster based on trust history 8) Aggregate remaining clients to update model
- **Design tradeoffs**:
  - Small held-out dataset vs. computational efficiency: Works with very small datasets (≤50 samples) to minimize overhead
  - OOD vs. ID data: Can use out-of-distribution data when held-out data unavailable, trading some accuracy for practicality
  - Stateful filtering vs. single-round decisions: Maintaining trust history adds complexity but improves robustness against adaptive attacks
- **Failure signatures**:
  - High false negative rate: Backdoored clients being selected for aggregation
  - High false positive rate: Clean clients being excluded from aggregation
  - Unstable training: CA fluctuates significantly due to occasional inclusion of backdoored submissions
  - Computational overhead: Training rounds taking significantly longer than baseline
- **First 3 experiments**:
  1. Verify adversarial bias and overconfidence properties on simple dataset with known backdoored models
  2. Test clustering effectiveness with varying numbers of backdoored clients
  3. Evaluate trust history mechanism's ability to filter backdoored clients over multiple rounds

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but acknowledges limitations in its discussion section regarding sensitivity to non-IID data distributions and the need for further evaluation with more advanced adversarial attacks.

## Limitations
- Effectiveness heavily depends on transferability of adversarial perturbations from aggregated model to individual clients
- Performance degrades significantly in highly non-IID settings (standard non-IID with α ≥ 0.5)
- Limited evaluation with advanced adversarial attacks beyond FGSM

## Confidence
- Mechanism 1 (Adversarial Bias): Medium - Theoretical foundation exists but empirical validation is limited
- Mechanism 2 (Adversarial Overconfidence): Medium - Relies on unproven assumption about confidence calibration
- Mechanism 3 (Stateful Trust): Low-Medium - Novel approach with limited ablation studies on trust update sensitivity

## Next Checks
1. Test perturbation transferability by measuring KL divergence between adversarial examples generated from aggregated model versus individual client models across multiple rounds
2. Perform ablation studies on trust history update parameters to determine optimal sensitivity thresholds for clean vs. backdoored client discrimination
3. Evaluate AGSD's robustness against adaptive adversaries who can detect and counteract the adversarial perturbation strategy by modifying their attack patterns accordingly