---
ver: rpa2
title: 'CANTONMT: Investigating Back-Translation and Model-Switch Mechanisms for Cantonese-English
  Neural Machine Translation'
arxiv_id: '2405.08172'
source_url: https://arxiv.org/abs/2405.08172
tags:
- translation
- data
- which
- cantonese
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the development and evaluation of machine translation
  models for Cantonese to English translation, focusing on a low-resource language.
  The authors created a new parallel corpus by combining existing datasets and generated
  a monolingual Cantonese dataset through web scraping.
---

# CANTONMT: Investigating Back-Translation and Model-Switch Mechanisms for Cantonese-English Neural Machine Translation

## Quick Facts
- arXiv ID: 2405.08172
- Source URL: https://arxiv.org/abs/2405.08172
- Reference count: 40
- Primary result: Best model (NLLB-mBART with model switch) achieved SacreBLEU score of 16.8 on test set, comparable to or better than commercial translators

## Executive Summary
This paper presents the development and evaluation of machine translation models for Cantonese to English translation, focusing on a low-resource language. The authors created a new parallel corpus by combining existing datasets and generated a monolingual Cantonese dataset through web scraping. They employed various approaches, including fine-tuning models, back-translation, and model switch mechanisms. The models were evaluated using multiple quality metrics, including SacreBLEU, hLEPOR, COMET, and BERTscore. The best model, NLLB-mBART with model switch mechanisms, achieved comparable and even better automatic evaluation scores against state-of-the-art commercial models (Bing and Baidu Translators), with a SacreBLEU score of 16.8 on their test set. The authors also developed an open-source web application for Cantonese-English translation, allowing users to compare different models.

## Method Summary
The authors developed Cantonese-English translation models by fine-tuning three pre-trained multilingual models (Opus-MT, NLLB, mBART) on a combined parallel corpus of approximately 68K sentences. They generated synthetic parallel data through back-translation using monolingual Cantonese (1.1M sentences) and English (434K sentences) corpora. Different ratios of synthetic and real data were tested, and a model switch mechanism was implemented where different models were used for forward and backward translation. Models were trained for 3 epochs (10 for Opus-MT) using Adam optimizer with learning rate 1e-4. The best-performing model (NLLB-mBART with model switch) was integrated into an open-source web application for user comparison.

## Key Results
- NLLB-mBART with model switch mechanisms achieved SacreBLEU score of 16.8 on test set
- Automatic evaluation scores were comparable to or better than commercial translators (Bing and Baidu)
- Model switch mechanisms showed improvement over single-model approaches
- Fine-tuned models significantly outperformed off-the-shelf versions of pre-trained models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data generated by back-translation can improve low-resource NMT performance when combined with real parallel data in moderate ratios.
- Mechanism: Back-translation creates synthetic parallel corpus by translating monolingual target-side text back to source language, expanding training data. When mixed with real parallel data, it increases model exposure to more sentence patterns and vocabulary.
- Core assumption: Back-translation model is of sufficient quality to generate useful synthetic translations, and monolingual corpus is of reasonable quality and representative of domain.
- Evidence anchors:
  - [abstract] "several approaches, including fine-tuning models, back-translation, and model switch, have been used."
  - [section 3.3] "Inspired by Hoang et al. [22], where different ratios of synthetic data and real data are used, a similar approach has also been made here."
  - [corpus] Weak - LIHKG corpus is large (1.1M sentences) but unfiltered and potentially noisy; quality of synthetic translations is not directly measured.
- Break condition: If back-translation model quality is poor or monolingual data is too noisy, synthetic data degrades rather than improves model performance.

### Mechanism 2
- Claim: Model-switch mechanisms (using different models for forward and backward translation) can yield better results than using the same model in both directions.
- Mechanism: Different pre-trained models have varying strengths; using strong model for one direction and another strong model for opposite direction can combine complementary knowledge, potentially generating higher quality synthetic data than single model.
- Core assumption: Pre-trained models have non-overlapping or complementary linguistic knowledge, and their strengths in different language directions can be leveraged.
- Evidence anchors:
  - [abstract] "The best model proposed in this investigation (NLLB-mBART) with model switch mechanisms has reached comparable and even better automatic evaluation scores..."
  - [section 3.4] "Since NLLB, in theory, should be the best model (most knowledge), a decision has been made to include NLLB at least in 1 direction."
  - [corpus] Weak - No explicit evidence of complementary strengths between NLLB and mBART; improvement could be due to chance or other factors.
- Break condition: If both models have similar weaknesses or if their strengths do not complement each other, model-switching provides no benefit or may even degrade performance.

### Mechanism 3
- Claim: Fine-tuning large pre-trained multilingual models (NLLB, mBART) on small parallel corpus significantly outperforms using their off-the-shelf versions for low-resource translation.
- Mechanism: Pre-trained multilingual models contain general linguistic knowledge; fine-tuning adapts this knowledge to specific low-resource language pair, improving translation quality beyond base model's capabilities.
- Core assumption: Pre-trained models have learned transferable linguistic features that can be adapted to Cantonese-English translation, and small parallel corpus provides sufficient signal for adaptation.
- Evidence anchors:
  - [abstract] "The translation quality of models has been evaluated with multiple quality metrics... the best model proposed in this investigation (NLLB-mBART) with model switch mechanisms has reached comparable and even better automatic evaluation scores against State-of-the-art commercial models."
  - [section 4.2] "It can also be seen from the results that NLLB scores the highest across all four metrics... Furthermore, it can also be said that larger models and models with Cantonese in the pre-training produce better outputs."
  - [corpus] Weak - Limited parallel data (68K sentences) used; success may depend heavily on quality and diversity of this small dataset.
- Break condition: If parallel corpus is too small or unrepresentative, fine-tuning may overfit or fail to learn meaningful patterns, resulting in performance worse than or similar to base model.

## Foundational Learning

- Concept: Back-translation and data augmentation techniques
  - Why needed here: Core to paper's methodology for improving low-resource Cantonese-English translation by expanding training data.
  - Quick check question: How does back-translation differ from forward translation, and why is it particularly useful for low-resource language pairs?

- Concept: Neural Machine Translation architectures (Transformer-based)
  - Why needed here: Models used (NLLB, mBART, Opus-MT) are all Transformer-based; understanding their architecture is crucial for grasping model capabilities and limitations.
  - Quick check question: What are the key components of a Transformer encoder-decoder architecture, and how do they contribute to translation quality?

- Concept: Evaluation metrics for machine translation (BLEU, COMET, BERTscore, human evaluation)
  - Why needed here: Paper uses mix of automatic metrics and human evaluation to assess model performance; understanding these metrics is essential for interpreting results.
  - Quick check question: What are the strengths and weaknesses of lexical-based metrics like BLEU compared to embedding-based metrics like COMET or BERTscore?

## Architecture Onboarding

- Component map: User input → language/model selection → backend model loading/inference → translation output → display in UI
- Critical path: User input → language/model selection → backend model loading/inference → translation output → display in UI
- Design tradeoffs: Model loading on-demand with LRU cache vs. pre-loading all models (memory vs. latency); modular UI for extensibility vs. simpler static design
- Failure signatures: Backend crashes due to memory overload (handled by LRU cache); poor translation quality due to noisy data or inadequate fine-tuning; UI not updating dynamically if server endpoints fail
- First 3 experiments:
  1. Test basic translation with each model using small, clean input to verify inference pipeline works
  2. Evaluate impact of synthetic data ratios by running fine-tuning with 0%, 50%, and 100% synthetic data and comparing automatic metrics
  3. Test model-switching by running forward translation with NLLB and backward with mBART (and vice versa) to compare synthetic data quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing the size of the parallel corpus significantly improve the translation quality of the Cantonese-English models?
- Basis in paper: [explicit] The authors mention that "merely 60K data is not enough to train a model to reach expert-level performance" and suggest that "with a better-quality monolingual dataset in Cantonese, there could be a significant improvement in building a better-performing model."
- Why unresolved: The paper does not conduct experiments with a larger parallel corpus to determine the impact on translation quality.
- What evidence would resolve it: Training the models with a significantly larger parallel corpus (e.g., 100K+ sentences) and comparing the evaluation metrics to the current models.

### Open Question 2
- Question: How effective are different data cleaning techniques in improving the quality of the monolingual Cantonese corpus for back-translation?
- Basis in paper: [inferred] The authors suggest that "with a better-quality monolingual dataset in Cantonese, there could be a significant improvement in building a better-performing model" and mention that "data cleaning" is a potential future work area.
- Why unresolved: The paper does not explore or compare different data cleaning techniques to determine their effectiveness in improving the quality of the monolingual corpus.
- What evidence would resolve it: Applying various data cleaning techniques (e.g., grammar checking, named entity recognition) to the monolingual corpus and evaluating the impact on the quality of the generated synthetic parallel data and the final translation models.

### Open Question 3
- Question: Can incorporating external knowledge sources, such as knowledge graphs or terminology databases, improve the translation quality of the Cantonese-English models?
- Basis in paper: [explicit] The authors suggest that "having a knowledge graph and knowledge base to represent different terminology and slang could potentially allow the model to understand more terminology in Cantonese."
- Why unresolved: The paper does not implement or evaluate the impact of incorporating external knowledge sources on the translation quality.
- What evidence would resolve it: Integrating a knowledge graph or terminology database into the training process and comparing the evaluation metrics of the resulting models to the current models without external knowledge sources.

## Limitations
- The quality and representativeness of the monolingual LIHKG corpus is unknown, which could affect the effectiveness of back-translation
- The model-switch mechanism's effectiveness is not well-established; no explicit evidence provided that NLLB and mBART have complementary strengths
- The small parallel corpus (68K sentences) may limit the generalizability of fine-tuning results

## Confidence
- High Confidence: The general methodology of combining real and synthetic data through back-translation is well-established in NMT literature
- Medium Confidence: The claim that model-switch mechanisms can improve results is plausible but lacks strong empirical support in this specific case
- Medium Confidence: The assertion that fine-tuning large pre-trained models on small parallel corpora significantly improves translation quality is reasonable but may be dataset-dependent

## Next Checks
1. Conduct an ablation study to quantify the contribution of each component (real data, synthetic data, model-switch) to final performance, isolating their individual effects
2. Perform a human evaluation study with larger sample size and diverse evaluators to validate automatic metric results and assess translation quality from linguistic perspective
3. Test the models on an external, held-out test set from different domain to evaluate generalizability of fine-tuned models beyond training data distribution