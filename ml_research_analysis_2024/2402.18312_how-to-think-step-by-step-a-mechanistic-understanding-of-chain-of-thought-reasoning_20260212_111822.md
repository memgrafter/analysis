---
ver: rpa2
title: 'How to think step-by-step: A mechanistic understanding of chain-of-thought
  reasoning'
arxiv_id: '2402.18312'
source_url: https://arxiv.org/abs/2402.18312
tags:
- heads
- subtask
- reasoning
- step
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how chain-of-thought reasoning emerges
  in large language models, focusing on Llama-2 7B. The authors analyze step-by-step
  reasoning over fictional ontologies, decomposing the task into decision-making,
  copying, and inductive reasoning subtasks.
---

# How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning

## Quick Facts
- arXiv ID: 2402.18312
- Source URL: https://arxiv.org/abs/2402.18312
- Reference count: 37
- Primary result: Identifies functional rift around layer 16 in Llama-2 7B separating pretraining from in-context reasoning priors

## Executive Summary
This paper provides the first in-depth mechanistic analysis of how chain-of-thought (CoT) reasoning emerges in large language models, focusing on Llama-2 7B. The authors decompose CoT reasoning into decision-making, copying, and inductive reasoning subtasks using fictional ontologies to avoid pretraining interference. Through a series of mechanistic experiments including activation patching, knockout studies, and probing classifiers, they reveal that multiple parallel attention pathways work together to generate answers, with a functional rift around layer 16 marking the transition from pretraining-based to in-context reasoning.

## Method Summary
The study uses Llama-2 7B with PrOntoQA fictional ontology data, decomposing reasoning into 10 subtasks and analyzing attention heads through activation patching and knockout experiments. The method includes probing classifiers to detect token mixing patterns, logit lens analysis to identify answer-writing heads, and few-shot prompting with structured task decomposition. The analysis focuses on identifying which attention heads contribute to each subtask and how information flows through residual streams across different layers.

## Key Results
- Multiple parallel attention pathways generate step-by-step answers from both input context and generated CoT context
- A functional rift around layer 16 separates early layers (token mixing, pretraining prior) from later layers (answer writing, in-context prior)
- Inductive reasoning circuits use attention heads to move information between ontologically related tokens, creating distinguishable residual stream representations
- Attention heads can attend to multiple sources (generated context, question context, few-shot examples) simultaneously

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs use multiple parallel attention pathways to generate step-by-step answers, with some pathways collecting answers from generated CoT context and others from input context
- Mechanism: Multiple attention heads simultaneously write answer tokens to the output residual stream, drawing from different sources (generated context, question context, few-shot examples). This creates redundant pathways that reinforce the final answer
- Core assumption: Attention heads can attend to different parts of the input/output sequence and extract relevant tokens for answer generation
- Evidence anchors:
  - [abstract] "These parallel pathways provide sequential answers from the input question context as well as the generated CoT"
  - [section 6.3] "We found that while generating CoT, the model gathers answer tokens from the generated context, the question context, as well as the few-shot context"
  - [corpus] Weak - corpus neighbors discuss similar parallel reasoning pathways but lack specific attention head evidence
- Break condition: If answer-writing heads cannot attend to multiple sources or if token mixing is disrupted, the parallel pathways would fail

### Mechanism 2
- Claim: A functional rift exists around layer 16 in Llama-2 7B, marking a phase shift from pretraining prior to in-context prior in residual stream representations
- Mechanism: Early layers (0-16) perform token mixing based on ontological relationships using attention heads, while later layers (17+) focus on answer writing and context following. The residual stream transitions from bigram associations to contextual information
- Core assumption: Attention heads in different layers have distinct functional roles in the reasoning process
- Evidence anchors:
  - [abstract] "We observe a functional rift in the middle layers of the LLM. Token representations in the initial half remain strongly biased towards the pretraining prior, with the in-context prior taking over in the later half"
  - [section 6.1] "The LM starts focusing on the contextual information at deeper layers" and "this depth-dependence is implicit to the model and does not depend on the prediction task"
  - [section 6.2] "Interestingly, the 16-th layer appears as a region of functional transition in the model with all the answer-writing heads appearing after this particular layer"
- Break condition: If the functional rift disappears or shifts significantly in other model architectures, this mechanism would not generalize

### Mechanism 3
- Claim: Inductive reasoning circuits use pattern matching via attention heads that move information between ontologically related tokens, creating distinguishable residual stream representations
- Mechanism: Attention heads perform information transfer between related tokens (e.g., A→B→C in "A is B, B is C, therefore A is C"), creating residual stream pairs that can be classified by probing classifiers. This mixing starts early and peaks around layer 10-15
- Core assumption: Ontologically related tokens have distinguishable residual stream representations that can be learned by classifiers
- Evidence anchors:
  - [section 5] "We observe that the inductive reasoning subtasks... essentially requires circuits that can perform 1) representation mixing from [A] to [B] to [C], and 2) pattern matching over past context"
  - [section 5] "Typically, mixing information between related (or negatively related) token pairs results in better distinguishability: gradually increasing from the starting decoder blocks and achieving a peak between decoder blocks 10-15"
  - [corpus] Weak - corpus neighbors mention feature-level analysis but lack specific token mixing evidence
- Break condition: If token mixing is disrupted or if the model cannot distinguish related from unrelated token pairs, inductive reasoning would fail

## Foundational Learning

- Concept: Attention mechanisms in transformers
  - Why needed here: Understanding how attention heads read from and write to residual streams is crucial for analyzing the information flow in CoT reasoning
  - Quick check question: What is the difference between query, key, and value projections in multi-head attention?

- Concept: Residual stream representations
  - Why needed here: The study relies on analyzing how information is mixed and transformed in residual streams across different layers
  - Quick check question: How does the content of the residual stream change as it passes through multiple decoder blocks?

- Concept: Probing classifiers for interpretability
  - Why needed here: The paper uses probing classifiers to detect whether token pairs are ontologically related, unrelated, or negatively related
  - Quick check question: What is the purpose of using both linear and non-linear classifiers in feature probing?

## Architecture Onboarding

- Component map: Embedding layer → 32 decoder blocks (each with attention and MLP layers) → unembedding layer. Attention heads are the primary focus, with 32 heads per layer
- Critical path: Input tokens → residual stream mixing (layers 0-16) → answer writing (layers 17+) → output logits. The functional rift at layer 16 is the key transition point
- Design tradeoffs: Using fictional ontologies avoids pretraining interference but limits generalizability. The few-shot CoT approach ensures structured reasoning but may not capture free-form reasoning capabilities
- Failure signatures: If attention heads cannot attend to multiple sources, parallel answer pathways fail. If token mixing is disrupted, inductive reasoning fails. If the functional rift shifts, layer-specific behaviors change
- First 3 experiments:
  1. Knock out individual attention heads and measure accuracy drop across all subtasks to identify task-specific importance
  2. Apply unembedding projection to attention head outputs to see which tokens are being written to the residual stream
  3. Use probing classifiers on residual streams at different layers to detect token mixing patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the identified functional rifts in Llama-2 7B's architecture generalize to other large language models of similar scale?
- Basis in paper: [explicit] The paper identifies a functional rift around layer 16 in Llama-2 7B and notes this is the first in-depth analysis of CoT reasoning in LLMs
- Why unresolved: The study is limited to a single model architecture and size. The authors suggest extending this analysis to other models but do not perform such comparisons
- What evidence would resolve it: Systematic analysis of CoT reasoning across multiple LLM architectures (GPT, Claude, PaLM, etc.) showing consistent or varying patterns of functional rifts

### Open Question 2
- Question: How do MLP layers contribute to the CoT reasoning process beyond their role as factual memory?
- Basis in paper: [inferred] The paper explicitly states they could not address the role of MLPs in reasoning and notes that MLPs likely store more than just factual associations
- Why unresolved: The analysis focused primarily on attention mechanisms while treating MLP layers as a black box, acknowledging this as a limitation
- What evidence would resolve it: Detailed mechanistic analysis of MLP layer contributions to CoT reasoning, including how they process and store token-token associations beyond factual knowledge

### Open Question 3
- Question: What is the causal relationship between the observed functional rift and the emergence of CoT reasoning abilities during training?
- Basis in paper: [explicit] The paper observes a functional rift but does not investigate its developmental origins or training dynamics
- Why unresolved: The study is limited to post-hoc analysis of a trained model without examining training dynamics or causal interventions
- What evidence would resolve it: Training dynamics analysis showing when and how the functional rift emerges, potentially through intervention studies or analysis of attention patterns across training steps

## Limitations
- Synthetic ontology setup may not generalize to real-world reasoning tasks
- Analysis focused only on attention mechanisms, leaving MLP layer contributions unexplored
- Results may be specific to Llama-2 7B architecture without evidence for generalization

## Confidence
- High confidence: The empirical observations about parallel attention pathways and their role in answer generation are well-supported by the activation patching and knockout experiments
- Medium confidence: The identification of layer 16 as a functional rift point is supported by multiple analyses, but the interpretation requires more evidence across different model architectures
- Low confidence: The generalization of these specific mechanistic findings to other LLM architectures or to more naturalistic reasoning tasks remains unproven

## Next Checks
1. Apply the same mechanistic analysis to other LLM architectures (Mistral, GPT-3.5, Claude) to determine if the functional rift at layer 16 and parallel pathway patterns are consistent or architecture-specific

2. Repeat key experiments using real-world knowledge graphs instead of fictional ontologies to assess whether the observed mechanisms hold when dealing with actual pretraining data

3. Test whether the identified attention head mechanisms and functional rift persist when using open-ended chain-of-thought prompts rather than the structured, few-shot approach employed in the current study