---
ver: rpa2
title: Rectifying Demonstration Shortcut in In-Context Learning
arxiv_id: '2403.09488'
source_url: https://arxiv.org/abs/2403.09488
tags:
- task
- label
- learning
- ethos
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of Large Language Models (LLMs)
  relying on pre-trained semantic priors from demonstrations in in-context learning
  (ICL), rather than learning new input-label relationships. This phenomenon, termed
  'Demonstration Shortcut', hinders the model's ability to learn novel tasks from
  demonstrations.
---

# Rectifying Demonstration Shortcut in In-Context Learning

## Quick Facts
- arXiv ID: 2403.09488
- Source URL: https://arxiv.org/abs/2403.09488
- Authors: Joonwon Jang; Sanghwan Jang; Wonbin Kweon; Minjin Jeon; Hwanjo Yu
- Reference count: 27
- Primary result: In-Context Calibration outperforms other ICL methods across three LLM families by reducing reliance on semantic priors

## Executive Summary
This work addresses a critical limitation in Large Language Models' in-context learning (ICL) where models rely on pre-trained semantic priors from demonstration examples rather than learning new input-label relationships. The authors identify this "Demonstration Shortcut" as a fundamental problem that hinders the model's ability to learn novel tasks from demonstrations. To address this, they propose In-Context Calibration, a demonstration-aware calibration method that estimates semantic priors of each demonstration sample and calculates expected semantic priors of demonstrations. This approach enables more effective calibration by accounting for the semantic information contained in demonstrations, resulting in substantial improvements across multiple LLM families and task configurations.

## Method Summary
The authors propose In-Context Calibration, a demonstration-aware calibration method designed to rectify the Demonstration Shortcut problem in ICL. The method works by first estimating the semantic prior of each demonstration sample relative to the remaining examples in the demonstration set. It then calculates the expected semantic priors across all demonstrations to create a calibration factor that accounts for the semantic information contained in the examples. This calibration approach is applied during inference to adjust the model's predictions, reducing over-reliance on semantic priors while still allowing the model to leverage them appropriately when beneficial. The method is evaluated across three major LLM families (OPT, GPT, and Llama2) using both standard ICL tasks and a novel Task Learning setting where label spaces are replaced with semantically unrelated tokens.

## Key Results
- In-Context Calibration demonstrates substantial improvements in both Original ICL Task and Task Learning settings
- The method outperforms other ICL approaches across three LLM families (OPT, GPT, and Llama2)
- Calibration effectively reduces model dependency on semantic priors while maintaining task learning abilities

## Why This Works (Mechanism)
The mechanism works by estimating the semantic priors embedded in demonstration examples and calculating their expected values across the demonstration set. This allows the calibration method to account for the semantic information that demonstrations inherently contain, rather than treating all demonstrations equally. By incorporating this demonstration-aware component, the calibration can adjust predictions based on how much semantic prior each demonstration is likely to contribute, enabling more nuanced and effective learning from the examples.

## Foundational Learning
- **In-Context Learning (ICL)**: Few-shot learning paradigm where LLMs learn from demonstration examples without parameter updates. Needed to understand the evaluation framework and baseline methods.
- **Semantic Prior**: Pre-trained knowledge that models use to make predictions based on the meaning of input-output pairs. Quick check: Examine whether models use label meanings or just patterns.
- **Demonstration Shortcut**: Phenomenon where models rely on pre-trained semantic priors from demonstrations rather than learning new task relationships. Quick check: Compare performance when labels are replaced with random tokens versus meaningful labels.
- **Calibration in LLMs**: Post-hoc adjustment of model predictions to improve accuracy and calibration. Quick check: Verify that calibration improves log-likelihood and reduces calibration error.

## Architecture Onboarding

**Component Map**: Input Examples -> Semantic Prior Estimation -> Expected Prior Calculation -> Calibration Factor -> Adjusted Predictions

**Critical Path**: The most critical path involves the semantic prior estimation step, as this determines how effectively the calibration can account for demonstration content. The calculation of expected priors and generation of calibration factors follow this estimation.

**Design Tradeoffs**: The main tradeoff is between leveraging helpful semantic priors and avoiding over-reliance on them. The calibration must balance using priors that aid generalization while preventing shortcuts that prevent learning new task relationships.

**Failure Signatures**: 
- Over-calibration leading to poor performance on tasks where semantic priors are beneficial
- Under-calibration failing to address demonstration shortcuts
- Computational overhead during inference making the method impractical
- Calibration that doesn't generalize across different task types

**First Experiments to Run**:
1. Evaluate calibration performance on tasks with varying degrees of semantic prior usefulness
2. Test calibration with different demonstration set sizes to find optimal demonstration count
3. Measure inference time overhead compared to baseline ICL methods

## Open Questions the Paper Calls Out
The authors note that their evaluation primarily focuses on specific task types and LLM families, raising questions about performance on more complex reasoning tasks or smaller models. They also acknowledge that the Task Learning setting with semantically unrelated labels may not reflect real-world usage scenarios where semantic priors are actually beneficial.

## Limitations
- Evaluation methodology using semantically unrelated tokens may not reflect real-world scenarios where semantic priors are beneficial
- Limited testing on complex reasoning tasks and smaller model families
- Computational overhead during inference not discussed, which could limit practical deployment
- Potential negative impacts when semantic priors are actually helpful for the task at hand

## Confidence
- High confidence in the identification of demonstration shortcut phenomenon (the empirical observations appear robust)
- Medium confidence in the effectiveness of In-Context Calibration for the specific evaluated tasks
- Low confidence in generalizability to real-world applications and diverse task types

## Next Checks
1. Test In-Context Calibration on a broader range of NLP tasks including complex reasoning and multi-step problems to assess generalizability beyond the evaluated tasks
2. Conduct ablation studies comparing performance when semantic priors are beneficial versus when they should be ignored to understand when calibration helps versus harms
3. Measure and report the computational overhead and inference time impact of the calibration method across different model sizes to evaluate practical deployment feasibility