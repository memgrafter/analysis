---
ver: rpa2
title: 'Reviewer2: Optimizing Review Generation Through Prompt Generation'
arxiv_id: '2402.10886'
source_url: https://arxiv.org/abs/2402.10886
tags:
- review
- reviews
- prompts
- reviewer
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of generating detailed and aspect-specific
  reviews for scientific papers using large language models (LLMs). The proposed Reviewer2
  framework uses a two-stage approach: first, it generates aspect prompts that capture
  the range of topics a reviewer might address, and second, it generates reviews based
  on these prompts and the full paper content.'
---

# Reviewer2: Optimizing Review Generation Through Prompt Generation
## Quick Facts
- arXiv ID: 2402.10886
- Source URL: https://arxiv.org/abs/2402.10886
- Reference count: 34
- Primary result: Two-stage LLM framework that generates aspect-specific review prompts then reviews, improving coverage and specificity

## Executive Summary
This paper addresses the challenge of generating detailed, aspect-specific reviews for scientific papers using large language models. The proposed Reviewer2 framework introduces a novel two-stage approach: first generating aspect prompts that capture potential reviewer concerns, then generating reviews based on these prompts and full paper content. The authors also create a new dataset of 27k papers with 99k reviews annotated with aspect prompts, generated through a prompt generation with evaluation pipeline. Experiments demonstrate that Reviewer2 outperforms single-stage models across multiple evaluation metrics including BLEU, ROUGE, BertScore, specificity, and coverage.

## Method Summary
The Reviewer2 framework employs a two-stage approach to review generation. In the first stage, an aspect prompt generator creates detailed prompts capturing the range of topics a reviewer might address about a paper. In the second stage, these prompts are combined with the full paper content to generate comprehensive reviews. The aspect prompts serve as a structured guide, ensuring coverage of relevant topics while maintaining specificity. The system is trained on a newly created dataset of 27k papers and 99k reviews, where aspect prompts were generated using a prompt generation with evaluation (PGE) pipeline. This pipeline iteratively refines prompts based on evaluation criteria to ensure quality and relevance.

## Key Results
- Reviewer2 outperforms single-stage baselines across BLEU, ROUGE, and BertScore metrics
- The framework achieves significantly higher specificity and coverage compared to traditional review generation methods
- The PGE pipeline successfully generates aspect prompts that improve review quality and comprehensiveness

## Why This Works (Mechanism)
The two-stage approach works by first decomposing the complex task of review generation into manageable sub-tasks. By generating aspect prompts first, the system creates a structured framework that guides the subsequent review generation process. This ensures that generated reviews address a comprehensive range of relevant topics rather than being limited to whatever the model happens to focus on. The aspect prompts act as a checklist, improving both the coverage of topics addressed and the specificity of comments made about each aspect.

## Foundational Learning
- **Aspect-based review generation**: Understanding how to break down reviews into specific aspects is crucial for generating comprehensive feedback
  - *Why needed*: Scientific reviews require addressing multiple dimensions (methodology, novelty, clarity, etc.)
  - *Quick check*: Verify that generated aspect prompts cover all critical review dimensions

- **Two-stage generation**: Decomposing complex text generation into sequential stages can improve output quality
  - *Why needed*: Single-stage generation often misses important aspects or lacks specificity
  - *Quick check*: Compare single-stage vs two-stage outputs for coverage and detail

- **Prompt generation with evaluation**: Iterative refinement of prompts based on evaluation criteria ensures quality
  - *Why needed*: Initial prompts may be incomplete or miss important aspects
  - *Quick check*: Measure improvement in prompt quality across evaluation iterations

## Architecture Onboarding
**Component Map**: Paper Content -> Aspect Prompt Generator -> Aspect Prompts -> Review Generator -> Final Review

**Critical Path**: The critical path flows from paper content through aspect prompt generation to review generation. Each stage must complete successfully for high-quality reviews.

**Design Tradeoffs**: The two-stage approach trades computational efficiency for improved review quality. While single-stage generation is faster, the two-stage method produces more comprehensive and specific reviews by ensuring all relevant aspects are addressed.

**Failure Signatures**: Common failures include: (1) aspect prompts that miss critical review dimensions, (2) reviews that are too generic despite specific prompts, and (3) factual inconsistencies between the paper content and generated review.

**First Experiments**: 
1. Generate reviews using only the review generator (without aspect prompts) to establish baseline performance
2. Generate aspect prompts for papers from different venues to test generalizability
3. Compare review quality when using human-written vs machine-generated aspect prompts

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but implicit questions remain about the long-term viability of machine-generated reviews for academic peer review and the potential for bias in automated review generation systems.

## Limitations
- The PGE pipeline's aspect prompts may not fully capture human reviewer concerns across all paper types
- Dataset construction depends heavily on initial seed prompts and evaluation criteria quality
- Limited evaluation on actual human-generated reviews makes real-world performance assessment difficult
- Automated metrics may not correlate with human judgments of review quality

## Confidence
**High confidence**: The two-stage approach is technically sound and implemented as described. BLEU, ROUGE, BertScore, specificity, and coverage metrics are standard and appropriately applied. The PGE methodology is clearly described and reproducible.

**Medium confidence**: Relative improvements over baselines are real but may be less pronounced on human-written review comparisons. Generalizability of aspect prompts across different scientific domains needs further validation.

## Next Checks
1. Conduct human evaluation studies comparing Reviewer2 outputs with actual peer reviews on held-out papers to validate automated metric correlations
2. Test model performance on papers from venues not included in the training data to assess generalizability
3. Analyze failure cases where the model generates irrelevant or factually incorrect reviews to identify systematic weaknesses in the two-stage approach