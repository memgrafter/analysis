---
ver: rpa2
title: 'NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models'
arxiv_id: '2407.10380'
source_url: https://arxiv.org/abs/2407.10380
tags:
- literate
- text
- question
- reasoning
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NTSEBench introduces a new dataset of 2,728 cognitive reasoning
  questions sourced from the NTSE exam, featuring 26 categories of visual, textual,
  and multimodal problems designed to test critical thinking beyond rote learning.
  The dataset includes both text-only and image-text questions, with proprietary models
  like GPT-4o and Gemini 1.5 achieving 62% and 42% accuracy on text and multimodal
  subsets respectively, while open-source models struggle to exceed 50% and 35%.
---

# NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models

## Quick Facts
- arXiv ID: 2407.10380
- Source URL: https://arxiv.org/abs/2407.10380
- Reference count: 40
- Primary result: Proprietary models achieve 62% accuracy on text and 42% on multimodal tasks; open-source models struggle to exceed 50% and 35%

## Executive Summary
NTSEBench introduces a new dataset of 2,728 cognitive reasoning questions sourced from the NTSE exam, featuring 26 categories of visual, textual, and multimodal problems designed to test critical thinking beyond rote learning. The dataset includes both text-only and image-text questions, with proprietary models like GPT-4o and Gemini 1.5 achieving 62% and 42% accuracy on text and multimodal subsets respectively, while open-source models struggle to exceed 50% and 35%. The study proposes four modeling strategies—Standard QA, Image-Only, Interleaved, and Standard VQA—to fairly evaluate both proprietary and open-source models.

## Method Summary
The benchmark introduces 2,728 cognitive reasoning questions from NTSE exam, organized into 26 categories spanning visual, textual, and multimodal problems. The study proposes four distinct modeling strategies to evaluate vision-language models: Standard QA (text-only), Image-Only (visual-only), Interleaved (alternating text and image processing), and Standard VQA (conventional vision-language approach). Proprietary models (GPT-4o, Gemini 1.5, o1-preview) were evaluated alongside open-source alternatives across both text-only and multimodal subsets.

## Key Results
- Proprietary models (GPT-4o, Gemini 1.5) achieve 62% accuracy on text-only tasks versus 42% on multimodal tasks
- Open-source models struggle to exceed 50% on text and 35% on multimodal tasks
- Interleaving text and images performs best for multimodal tasks compared to other modeling strategies

## Why This Works (Mechanism)
The benchmark succeeds by targeting higher-order cognitive reasoning through carefully curated NTSE exam questions that require analysis beyond pattern matching. The multimodal design forces models to integrate visual and textual information, exposing limitations in current vision-language architectures that excel at simpler tasks.

## Foundational Learning

**Vision-Language Integration** - Why needed: To evaluate models' ability to combine visual and textual information for complex reasoning tasks. Quick check: Test model performance on tasks requiring cross-modal information synthesis.

**Cognitive Reasoning Assessment** - Why needed: To move beyond simple question answering to evaluate higher-order thinking skills. Quick check: Verify questions require analysis rather than pattern recognition.

**Multimodal Processing Strategies** - Why needed: To determine optimal approaches for handling combined text-image inputs. Quick check: Compare different architectural approaches on same dataset.

## Architecture Onboarding

**Component Map**: Input -> Text Processing -> Image Processing -> Integration Module -> Reasoning Engine -> Output

**Critical Path**: The integration of visual and textual information represents the critical path, as models must successfully combine modalities before reasoning can occur.

**Design Tradeoffs**: The study balances between standardized evaluation (enabling fair comparison) and task complexity (requiring genuine reasoning rather than pattern matching).

**Failure Signatures**: Models fail particularly on questions requiring simultaneous processing of visual and textual information, suggesting limitations in cross-modal integration.

**First Experiments**:
1. Evaluate baseline performance using Standard QA approach on text-only subset
2. Test Image-Only strategy on visual questions to establish modality-specific baselines
3. Compare Interleaved versus Standard VQA approaches on multimodal questions

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on 2,728 questions from single national exam, limiting generalizability
- Proprietary model performance cannot be independently verified due to API access constraints
- Evaluation methodology does not account for prompt engineering variations that could influence performance

## Confidence

**Model Performance Differences**: High - Structured comparison between proprietary and open-source models is methodologically sound
**Generalizability**: Medium - Dataset is specialized and relatively small
**Modeling Strategy Effectiveness**: Medium - Shows empirical success but lacks extensive validation

## Next Checks

1. Replicate benchmark with additional datasets from varied cognitive reasoning domains to test generalizability
2. Conduct systematic evaluation of prompt engineering and test-time optimization effects on model performance
3. Expand analysis to include newer open-source models with improved multimodal capabilities to reassess current performance gaps