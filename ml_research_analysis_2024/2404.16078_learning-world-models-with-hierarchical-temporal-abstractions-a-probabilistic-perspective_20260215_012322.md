---
ver: rpa2
title: 'Learning World Models With Hierarchical Temporal Abstractions: A Probabilistic
  Perspective'
arxiv_id: '2404.16078'
source_url: https://arxiv.org/abs/2404.16078
tags: []
core_contribution: This thesis addresses the challenge of developing foundational
  world models (FWMs) that can learn, predict, and reason about the dynamics of the
  real world across multiple temporal scales and abstractions. The work introduces
  two novel probabilistic formalisms, Hidden Parameter State Space Models (HiP-SSMs)
  and Multi-Time Scale State Space Models (MTS3), to overcome limitations of existing
  SSMs.
---

# Learning World Models With Hierarchical Temporal Abstractions: A Probabilistic Perspective

## Quick Facts
- arXiv ID: 2404.16078
- Source URL: https://arxiv.org/abs/2404.16078
- Authors: Vaisakh Shaj
- Reference count: 0
- This thesis introduces two novel probabilistic formalisms for building foundational world models capable of long-horizon predictions across multiple temporal scales.

## Executive Summary
This thesis addresses the challenge of developing foundational world models (FWMs) that can learn, predict, and reason about real-world dynamics across multiple temporal scales and abstractions. The work introduces Hidden Parameter State Space Models (HiP-SSMs) and Multi-Time Scale State Space Models (MTS3) to overcome limitations of existing SSMs. These models incorporate hierarchical latent task variables and multiple temporal scales to enable accurate long-term predictions while maintaining uncertainty quantification. Experiments demonstrate that these models can match or exceed transformer variants in long-range predictions across various real and simulated robots.

## Method Summary
The thesis develops two probabilistic formalisms: HiP-SSMs incorporate hierarchical latent task variables to adapt to changing dynamics in non-stationary environments, while MTS3 extends this to multiple temporal scales by coupling fast and slow-time scale SSMs. Both formalisms use Gaussian conditioning and marginalization for scalable exact inference, and an imputation-based self-supervised training scheme to improve long-horizon prediction performance. The models are trained end-to-end via backpropagation through time and evaluated on various real and simulated robot datasets.

## Key Results
- HiP-SSMs and MTS3 models match or exceed transformer variants in long-range future predictions across multiple real and simulated robot datasets
- Hierarchical latent task variables enable adaptation to changing dynamics in non-stationary environments
- Multi-time scale modeling captures both slow-changing trends and fast-changing details for improved long-horizon predictions
- Imputation-based self-supervised training improves performance for long-term prediction with varying horizons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical structure with multiple temporal abstractions allows capturing both slow-changing long-term trends and fast-changing short-term dynamics for accurate long-horizon predictions
- Mechanism: MTS3 uses top-down approach where slow-time scale SSM makes abstract predictions about task descriptor, which conditions fast-time scale SSM to make detailed state predictions
- Core assumption: Real world exhibits hierarchical causal structures where slow regularities at higher levels affect relevant faster regularities at lower levels
- Evidence anchors: [abstract] "One drawback of single-time scale world models is that they may not capture longer-term trends"
- Break condition: If real-world dynamics don't exhibit clear hierarchical causal structures or discretization step H is improperly tuned

### Mechanism 2
- Claim: Bayesian conditioning and marginalization enable scalable exact inference across multiple time scales while maintaining uncertainty quantification
- Mechanism: Gaussian conditioning for observation updates and Gaussian marginalization for time updates performed in closed form using efficient scalar operations
- Core assumption: Distributions over latent states and observations can be approximated as Gaussian, and dynamics can be modeled as locally linear
- Evidence anchors: [abstract] "The structure of graphical models in both formalisms facilitates scalable exact probabilistic inference"
- Break condition: If Gaussian assumptions break down or linearization assumptions become invalid for highly non-linear systems

### Mechanism 3
- Claim: Imputation-based self-supervised training scheme enables effective learning for long-horizon prediction by treating it as a missing value problem
- Mechanism: Observations are randomly masked during training and model is tasked with imputing missing observations, creating strong self-supervised learning signal
- Core assumption: Ability to impute missing future observations is good proxy for ability to predict future states
- Evidence anchors: [abstract] "To increase the performance of the long-term prediction, we can treat the long-term prediction problem as a case of the problem of 'missing value'"
- Break condition: If masking strategy doesn't create meaningful learning signals or model overfits to specific masking pattern

## Foundational Learning

- Concept: State Space Models (SSMs) as Bayesian Networks
  - Why needed here: Essential for understanding exact inference algorithms and hierarchical extensions
  - Quick check question: What are the two main queries that a Kalman filter answers iteratively, and how are they implemented in SSMs?

- Concept: Gaussian Conditioning and Marginalization
  - Why needed here: Form the basis of exact inference algorithms enabling efficient belief updates and uncertainty propagation
  - Quick check question: How does factorization assumption in covariance matrix convert expensive matrix inversions into efficient scalar operations?

- Concept: Deep Bayesian Aggregation
  - Why needed here: Used to aggregate information from sets of observations into consistent representations for inferring task abstractions
  - Quick check question: How does Bayesian aggregation update rule for latent task variable relate to attention mechanisms?

## Architecture Onboarding

- Component map: HiP-SSM = Observation Encoder -> Latent State -> Dynamics Model -> Observation Decoder. MTS3 = Fast-Time SSM (detailed predictions) + Slow-Time SSM (abstract task predictions) with conditioning between scales.
- Critical path: Observations → Encoders → Latent Representations → Hierarchical Prediction Pipeline → Decoders → Reconstructed Observations. Slow-time scale predictions condition fast-time scale predictions.
- Design tradeoffs: Discretization step H trades off capturing slow-changing dynamics (larger H) vs maintaining fine-grained predictions (smaller H). Linearization assumptions trade model expressiveness for computational efficiency.
- Failure signatures: Poor long-horizon predictions may indicate incorrect H, breakdown of Gaussian assumptions, or insufficient training data. Uncertainty underestimation may indicate issues with covariance factorization or transition noise modeling.
- First 3 experiments:
  1. Validate single-time scale SSM (HiP-RSSM) on non-stationary task (robot with varying loads)
  2. Test imputation-based training on simple dataset with masked observations
  3. Evaluate MTS3 on hierarchical prediction task (mobile robot on varying terrain)

## Open Questions the Paper Calls Out

- How do proposed formalisms scale to high-dimensional sensory data like vision? The thesis notes this as future direction since experiments focus on proprioceptive sensors.
- What is optimal number of temporal hierarchies for different tasks? The work explores two-level models but suggests extensive experimentation with more hierarchies as future work.
- How can hierarchical generative models be extended to incorporate planning and control? The outlook identifies hierarchical planning as significant underexplored challenge.

## Limitations

- Gaussian assumptions underlying exact inference algorithms may not hold for highly non-linear or non-Gaussian real-world dynamics
- Discretization step H requires careful tuning based on specific task dynamics, with improper choice leading to poor performance
- Computational complexity scales with number of temporal scales, potentially limiting applicability to very long horizons or high-dimensional state spaces

## Confidence

- High: Core probabilistic formalisms and inference algorithms are well-established and mathematically sound
- Medium: Experimental results demonstrate effectiveness on various real and simulated robots, but sample size is limited
- Low: Generalizability to completely different domains beyond tested scenarios remains uncertain

## Next Checks

1. Conduct experiments on wider range of real-world datasets with varying degrees of non-linearity and non-Gaussianity to assess robustness of Gaussian assumptions
2. Perform systematic ablation study on discretization step H in MTS3 to determine optimal choice for different dynamics
3. Evaluate computational efficiency and scalability on high-dimensional state spaces and very long time horizons to identify bottlenecks