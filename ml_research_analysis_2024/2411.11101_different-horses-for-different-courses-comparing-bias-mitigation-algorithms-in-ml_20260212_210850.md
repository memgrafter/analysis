---
ver: rpa2
title: 'Different Horses for Different Courses: Comparing Bias Mitigation Algorithms
  in ML'
arxiv_id: '2411.11101'
source_url: https://arxiv.org/abs/2411.11101
tags:
- fairness
- mitigation
- algorithms
- bias
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines the sensitivity of bias mitigation algorithms
  to choices in the learning pipeline, such as hyperparameters, random seeds, and
  feature selection. Using seven datasets and seven mitigation algorithms, the authors
  show that no single algorithm consistently outperforms others across all settings.
---

# Different Horses for Different Courses: Comparing Bias Mitigation Algorithms in ML

## Quick Facts
- arXiv ID: 2411.11101
- Source URL: https://arxiv.org/abs/2411.11101
- Reference count: 27
- Key outcome: No single bias mitigation algorithm consistently outperforms others across all settings; hyperparameter optimization can yield comparable performance, and fairness evaluation must consider multiple factors beyond fairness-utility tradeoffs.

## Executive Summary
This study examines the sensitivity of bias mitigation algorithms to choices in the learning pipeline, including hyperparameters, random seeds, and feature selection. Using seven datasets and seven mitigation algorithms, the authors demonstrate that different algorithms excel under different conditions, and hyperparameter optimization can achieve comparable fairness-utility tradeoffs across methods. The research challenges the notion of a single "best" bias mitigation algorithm and advocates for context-dependent evaluation that considers runtime, theoretical guarantees, and model multiplicity alongside fairness and accuracy.

## Method Summary
The authors benchmark seven bias mitigation algorithms (DiffDP, DiffEOdd, DiffEOpp, PRemover, HSIC, AdvDebias, LAFTR) across seven tabular datasets using MLPs. They systematically vary hyperparameters (batch size, learning rate, MLP architecture) and evaluate fairness metrics (demographic parity, equalized odds, equal opportunity) and accuracy. Models are trained for 150 epochs with Adam optimizer and step learning rate scheduler. The study compares algorithms by identifying Pareto-optimal models and analyzes the influence of hyperparameters, random seeds, and feature selection on fairness outcomes.

## Key Results
- No single bias mitigation algorithm consistently outperforms others across all hyperparameter settings and datasets
- Hyperparameter optimization can yield comparable fairness-utility performance across different algorithms
- Evaluating fairness solely through fairness-utility tradeoff is insufficient; runtime, theoretical guarantees, and model multiplicity must also be considered

## Why This Works (Mechanism)

### Mechanism 1
- Claim: No single bias mitigation algorithm consistently outperforms others across all hyperparameter settings and datasets.
- Mechanism: Fairness scores are highly sensitive to choices in the learning pipeline such as hyperparameters, random seeds, and feature selection, causing different algorithms to excel under different conditions.
- Core assumption: The learning pipeline's choices significantly influence fairness outcomes, masking the true performance differences between algorithms when evaluated under uniform conditions.
- Evidence anchors:
  - [abstract] "We show significant variance in fairness achieved by several algorithms and the influence of the learning pipeline on fairness scores."
  - [section] "We first observe the absence of a clear winner among bias mitigation algorithms; no single method consistently and significantly outperforms the others."
  - [corpus] Weak corpus evidence for this specific mechanism; corpus papers focus more on general fairness evaluation rather than hyperparameter sensitivity.
- Break condition: If fairness scores were stable across hyperparameter changes, a single algorithm would consistently perform best.

### Mechanism 2
- Claim: Hyperparameter optimization can yield comparable performance across different bias mitigation algorithms.
- Mechanism: By exploring various hyperparameter settings, each algorithm can find configurations that optimize fairness-utility tradeoffs, making their performances comparable.
- Core assumption: Sufficient hyperparameter space exploration allows each algorithm to achieve competitive fairness scores, reducing the importance of algorithm choice.
- Evidence anchors:
  - [abstract] "We highlight that most bias mitigation techniques can achieve comparable performance, given the freedom to perform hyperparameter optimization."
  - [section] "To perform this comparison, we only filter the models at the Pareto front for various algorithms after searching through different hyperparameters and random seeds."
  - [corpus] No direct corpus evidence supporting this specific claim; related works focus on benchmarking without extensive hyperparameter variation.
- Break condition: If certain algorithms inherently cannot achieve competitive fairness regardless of hyperparameters.

### Mechanism 3
- Claim: Evaluating fairness solely through a fairness-utility tradeoff is insufficient; runtime, theoretical guarantees, and model multiplicity also influence algorithm selection.
- Mechanism: When algorithms provide similar fairness-utility tradeoffs, other factors like computational efficiency, theoretical bounds, and ambiguity become decisive in selecting the appropriate algorithm.
- Core assumption: Stakeholders must consider multiple dimensions beyond fairness-utility tradeoffs when deploying models, especially when algorithms are competitive on the primary metric.
- Evidence anchors:
  - [abstract] "We highlight that most bias mitigation techniques can achieve comparable performance, given the freedom to perform hyperparameter optimization, suggesting that the choice of the evaluation parameters—rather than the mitigation technique itself—can sometimes create the perceived superiority of one method over another."
  - [section] "When several bias mitigation algorithms provide similar tradeoffs, selecting one can be challenging. In such cases, additional factors must be considered..."
  - [corpus] Weak corpus evidence; related papers focus on fairness evaluation but do not extensively discuss the multifaceted criteria for algorithm selection.
- Break condition: If fairness-utility tradeoff remains the dominant factor regardless of other considerations.

## Foundational Learning

- Concept: Sensitivity of fairness metrics to hyperparameter choices.
  - Why needed here: Understanding how hyperparameters affect fairness scores is crucial for interpreting benchmarking results and selecting appropriate algorithms.
  - Quick check question: What happens to fairness scores when you change the batch size or learning rate in a bias mitigation algorithm?

- Concept: Pareto optimality in fairness-utility tradeoffs.
  - Why needed here: Identifying models on the Pareto front helps compare algorithms based on their best achievable tradeoffs across different hyperparameter settings.
  - Quick check question: How do you determine if a model is on the Pareto front in fairness-utility space?

- Concept: Model multiplicity and ambiguity in predictions.
  - Why needed here: Evaluating the degree of arbitrariness in model predictions is important when selecting algorithms, especially in high-stakes applications.
  - Quick check question: What is ambiguity in the context of model multiplicity, and why does it matter for fairness?

## Architecture Onboarding

- Component map:
  Data preprocessing (feature selection, encoding) -> Bias mitigation algorithm (DiffDP, DiffEOdd, DiffEOpp, PRemover, HSIC, AdvDebias, LAFTR) -> Hyperparameter optimization (batch size, learning rate, model architecture) -> Fairness and utility evaluation (demographic parity, equalized odds, equal opportunity, accuracy) -> Runtime and theoretical analysis modules

- Critical path:
  1. Preprocess data with varying feature representations.
  2. Apply bias mitigation algorithm with specific hyperparameters.
  3. Train model and evaluate fairness and utility metrics.
  4. Repeat across multiple random seeds and hyperparameter settings.
  5. Analyze results to identify Pareto-optimal models and compare algorithms.

- Design tradeoffs:
  - Uniform vs. varied evaluation settings: Uniform settings ensure comparability but may favor certain algorithms; varied settings capture sensitivity but complicate comparisons.
  - Runtime vs. fairness: More computationally expensive algorithms may achieve better fairness but are less practical for large-scale deployment.
  - Theoretical guarantees vs. empirical performance: Algorithms with strong theoretical bounds may underperform empirically compared to heuristic methods.

- Failure signatures:
  - Algorithm convergence failure under certain hyperparameters (e.g., large batch sizes with high learning rates).
  - High ambiguity scores indicating arbitrary predictions across models.
  - Inconsistent fairness scores across random seeds suggesting instability.

- First 3 experiments:
  1. Train each bias mitigation algorithm on a single dataset with default hyperparameters across multiple random seeds; record fairness and utility scores to assess baseline variability.
  2. Vary one hyperparameter (e.g., batch size) systematically while keeping others fixed; observe changes in fairness-utility tradeoffs for each algorithm.
  3. Perform hyperparameter optimization for each algorithm on the same dataset; compare the best fairness-utility tradeoffs achieved and analyze runtime and ambiguity metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the sensitivity of bias mitigation algorithms to hyperparameter choices persist across different model architectures beyond MLPs?
- Basis in paper: [inferred] The paper shows sensitivity across varying batch sizes, learning rates, and MLP layer configurations, but does not explore other architectures like CNNs, transformers, or ensemble methods.
- Why unresolved: The experiments are limited to tabular data with multi-layer perceptrons, leaving open whether similar variance occurs with other model families.
- What evidence would resolve it: Empirical studies applying the same hyperparameter sweep to non-MLP architectures on the same datasets would clarify if sensitivity is architecture-dependent or general.

### Open Question 2
- Question: Are there identifiable patterns or rules that predict which hyperparameter settings will favor specific bias mitigation algorithms?
- Basis in paper: [inferred] The authors suggest it would be interesting to identify patterns guiding hyperparameter choices but do not explore this systematically.
- Why unresolved: While the paper demonstrates that different algorithms excel under different settings, it does not analyze whether certain hyperparameter ranges consistently correlate with algorithm performance.
- What evidence would resolve it: A systematic analysis correlating algorithm performance with hyperparameter values across datasets could reveal generalizable selection heuristics.

### Open Question 3
- Question: How does the instability of fairness metrics due to hyperparameter choices affect regulatory compliance assessments?
- Basis in paper: [explicit] The authors note that variance in fairness measures can mask unfairness and create illusions of fairness, which is critical from a regulatory perspective.
- Why unresolved: The paper discusses the problem qualitatively but does not quantify how much variance could lead to non-compliance or misclassification in real-world regulatory settings.
- What evidence would resolve it: Case studies or simulations showing how fairness metric variance translates into regulatory risk under different evaluation protocols would clarify the practical implications.

## Limitations
- The study focuses on tabular data with MLPs, limiting generalizability to other data types and model architectures
- Only Race (or Age) is used as the sensitive attribute, restricting applicability to other protected characteristics
- Computational resources required for exhaustive hyperparameter exploration may not be feasible in all practical settings

## Confidence

| Claim | Confidence |
|-------|------------|
| No single algorithm consistently outperforms others across all settings | High |
| Hyperparameter optimization can yield comparable performance across algorithms | Medium |
| Fairness-utility tradeoff evaluation is insufficient for algorithm selection | High |

## Next Checks

1. Replicate the experiments with additional sensitive attributes (e.g., gender, age) to assess whether the findings hold across different protected characteristics.
2. Test the algorithms on non-tabular datasets (e.g., image or text data) to evaluate the generalizability of the conclusions to other data modalities.
3. Conduct a user study with practitioners to determine which factors (fairness-utility tradeoff, runtime, theoretical guarantees, or model multiplicity) are most important in real-world algorithm selection decisions.