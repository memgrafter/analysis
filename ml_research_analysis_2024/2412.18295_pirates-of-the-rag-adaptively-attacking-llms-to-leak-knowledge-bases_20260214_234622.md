---
ver: rpa2
title: 'Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases'
arxiv_id: '2412.18295'
source_url: https://arxiv.org/abs/2412.18295
tags:
- chunks
- attack
- knowledge
- agent
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a black-box attack method, Pirate, to extract
  private knowledge bases from Retrieval-Augmented Generation (RAG) systems. The attack
  is adaptive and automatic, using an open-source LLM and relevance-based mechanisms
  to generate effective queries that maximize coverage of the hidden knowledge base.
---

# Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases

## Quick Facts
- arXiv ID: 2412.18295
- Source URL: https://arxiv.org/abs/2412.18295
- Reference count: 34
- One-line primary result: Adaptive black-box attack extracts up to 95.8% of private knowledge bases from RAG systems

## Executive Summary
This paper presents Pirate, an adaptive black-box attack method that extracts private knowledge bases from Retrieval-Augmented Generation (RAG) systems. The attack uses an open-source LLM and relevance-based mechanisms to generate effective queries that maximize coverage of hidden knowledge bases. By maintaining an evolving set of "anchors" representing topics likely covered in the knowledge base, Pirate dynamically updates their relevance scores based on query success. Experiments across three virtual agents show Pirate significantly outperforms related approaches, achieving up to 95.8% knowledge base coverage versus 68.6% for the next best method.

## Method Summary
Pirate is a black-box adaptive attack that extracts private knowledge bases from RAG systems without requiring any knowledge of the target's internal workings. The method maintains a set of "anchors" representing topics likely covered in the knowledge base and uses relevance scores to guide query generation. The attacker-side open-source LLM generates queries based on these anchors, while a relevance-based mechanism dynamically updates anchor scores based on query success. When new knowledge chunks are extracted, the attacker LLM extracts keywords to form new anchors, expanding the search space. The attack uses injection commands to extract knowledge and employs duplicate checking to ensure efficiency.

## Key Results
- Pirate achieves up to 95.8% knowledge base coverage versus 68.6% for the next best method
- The attack is efficient, requiring only 13 seconds per query on average
- Pirate extracts more unique information rather than duplicates compared to competitors
- Performance remains strong in both bounded (300 queries) and unbounded attack settings

## Why This Works (Mechanism)

### Mechanism 1
- Relevance-based anchor sampling focuses on topics yielding more unique knowledge chunks
- Anchors are sampled proportionally to relevance scores using softmax weighting
- Effectiveness depends on accurate relevance scores reflecting information discovery potential

### Mechanism 2
- Dynamic relevance update penalizes anchors leading to duplicate chunks
- Reduces relevance scores of anchors associated with duplicate chunks by penalty term γ
- Assumes duplicates indicate ineffective anchors for finding new knowledge

### Mechanism 3
- Anchor extraction process adds new, diverse anchors from newly discovered chunks
- Attacker LLM extracts keywords from non-duplicate chunks to form new anchors
- Effectiveness depends on extracted chunks containing information guiding discovery of related content

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Attack targets knowledge base retrieval component of RAG systems
  - Quick check question: What are the four main components of a RAG system?

- Concept: Black-box attack
  - Why needed here: Attack performed without knowledge of target RAG system's internal workings
  - Quick check question: How does a black-box attack differ from a white-box attack?

- Concept: Relevance scoring and softmax sampling
  - Why needed here: Attack uses relevance scores and softmax to sample anchors
  - Quick check question: How does softmax ensure less relevant anchors still have a chance to be selected?

## Architecture Onboarding

- Component map: Attacker-side LLM -> Anchor set -> Relevance scores -> Query generator -> Target RAG system
- Critical path: Generate base query → Inject command → Send to target → Parse output → Check for duplicates → Update anchor set and relevance scores → Repeat
- Design tradeoffs: Open-source tools vs. proprietary APIs (cost vs. performance), exploration vs. exploitation balance, query length vs. injection effectiveness
- Failure signatures: Low navigation coverage despite many queries, high proportion of duplicate chunks, rapidly decreasing relevance scores
- First 3 experiments:
  1. Test anchor extraction and relevance update with a small, synthetic knowledge base
  2. Evaluate effectiveness of different injection commands on a controlled RAG system
  3. Measure impact of anchor sampling parameters (n, β) on knowledge extraction efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Pirate perform when attacking RAG systems with different chunk sizes and retrieval strategies?
- Basis in paper: [explicit] The paper mentions different chunk sizes and retrieval strategies used for Agents A, B, and C, but does not explicitly test Pirate's performance across varying chunk sizes and retrieval strategies
- Why unresolved: Paper focuses on comparing Pirate to other methods using specific configurations for each agent without systematically varying chunk sizes or retrieval strategies
- What evidence would resolve it: Experiments varying chunk sizes and retrieval strategies for each agent, measuring impact on Pirate's performance in terms of Nav, LK, LC, and ULC metrics

### Open Question 2
- Question: How does Pirate scale to larger knowledge bases and more complex RAG architectures?
- Basis in paper: [inferred] Paper uses relatively small knowledge bases (1000 chunks each) and standard RAG architectures without exploring performance on larger datasets or more complex architectures
- Why unresolved: Scalability to real-world RAG systems with massive knowledge bases and sophisticated architectures is unknown
- What evidence would resolve it: Experiments using significantly larger knowledge bases and more complex RAG architectures, measuring Pirate's performance and computational requirements compared to other methods

### Open Question 3
- Question: Can Pirate be adapted to target specific types of information within a RAG system's knowledge base?
- Basis in paper: [explicit] Paper focuses on untargeted attacks, mentions possibility of extending method to targeted attacks but does not explore this direction
- Why unresolved: Current Pirate is designed for broad knowledge extraction, but real-world attacks often target specific sensitive information
- What evidence would resolve it: Experiments modifying Pirate to incorporate seed keywords or topic models, guiding anchor selection and query generation towards specific information categories

## Limitations

- Knowledge base completeness assumption: Attack assumes finite, static knowledge base, which may not hold for continuously updated systems
- Dependence on anchor extraction quality: Success heavily depends on attacker-side LLM's ability to extract meaningful, diverse anchors from stolen chunks
- Limited evaluation on diverse RAG configurations: Paper tests on three virtual agents but does not explore wide range of RAG configurations

## Confidence

**High Confidence**:
- Pirate effectively extracts knowledge from RAG systems with high Nav and LK values compared to competitors
- Dynamic relevance update mechanism successfully reduces influence of anchors leading to duplicate chunks

**Medium Confidence**:
- Attack's efficiency (13 seconds per query) and ability to extract unique information are well-supported by experimental results
- Claim that Pirate outperforms competitors in both bounded and unbounded settings is supported by results

**Low Confidence**:
- Paper does not provide detailed analysis of anchor extraction quality or its impact on performance
- Assumption of finite, static knowledge base is not thoroughly validated

## Next Checks

1. Evaluate anchor extraction quality by conducting detailed analysis of attacker-side LLM's performance, measuring diversity and relevance of extracted anchors, and comparing different open-source LLMs

2. Test attack on dynamic knowledge bases by modifying experimental setup to include RAG system with dynamically updating knowledge base and measuring attack's ability to adapt to changes

3. Explore impact of RAG configuration variations by testing attack on systems with different retriever types, chunking strategies, and knowledge base sizes to analyze how variations affect performance