---
ver: rpa2
title: Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences
arxiv_id: '2403.19871'
source_url: https://arxiv.org/abs/2403.19871
tags:
- stability
- feature
- retraining
- problem
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a framework for retraining ML models across
  time-varying data batches while explicitly maintaining stability in model structure
  and analytical insights. Unlike greedy retraining that maximizes performance independently
  per batch, the method enforces smooth model transitions via a multi-objective optimization
  balancing predictive accuracy and inter-model distance.
---

# Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences

## Quick Facts
- arXiv ID: 2403.19871
- Source URL: https://arxiv.org/abs/2403.19871
- Authors: Dimitris Bertsimas; Vassilis Digalakis; Yu Ma; Phevos Paschalidis
- Reference count: 40
- Key outcome: Framework for stable ML model retraining across time-varying batches via multi-objective optimization balancing accuracy and model stability

## Executive Summary
This paper introduces a novel framework for retraining machine learning models across time-varying data batches while explicitly maintaining stability in model structure and analytical insights. Unlike traditional greedy approaches that maximize performance independently per batch, the method enforces smooth model transitions through a mixed-integer optimization formulation that balances predictive accuracy with inter-model distance minimization. The framework guarantees Pareto optimality in the accuracy-stability trade-off and provides theoretical generalization bounds. Extensive experiments across regression, decision trees, boosted trees, and neural networks in healthcare, vision, and language domains demonstrate that a modest 2% accuracy trade-off yields significant 30% stability gains, preserving interpretability and trust in production deployments.

## Method Summary
The framework employs a mixed-integer optimization (MIO) formulation to select models from pre-computed candidate sets for each data batch, minimizing the sum of pairwise distances between consecutive models while enforcing accuracy constraints. The method constructs a directed graph where nodes represent candidate models and edges represent valid transitions, then solves a shortest path problem to find the optimal sequence. When new data arrives, the framework retrains the entire sequence on the updated dataset, ensuring stability through a common initial model. The approach provides theoretical guarantees on Pareto optimality and generalization bounds, with a polynomial-time heuristic for efficient computation.

## Key Results
- Achieves 30% stability improvement with only 2% accuracy trade-off compared to greedy retraining
- Theoretical guarantees of Pareto optimality and generalization bounds established
- Framework validated across four model families: regression, decision trees, boosted trees, and neural networks
- Demonstrated effectiveness in healthcare, vision, and language domains with real-world datasets

## Why This Works (Mechanism)

### Mechanism 1
The framework enforces smooth model transitions by minimizing inter-model distances across retraining iterations, ensuring stability without sacrificing predictive power significantly. The method uses a mixed-integer optimization (MIO) formulation that selects candidate models from pre-computed sets, minimizing the sum of pairwise distances between consecutive models (intra-sequence stability loss) while enforcing accuracy constraints per batch. Pre-computing a diverse set of candidate models per batch allows the MIO to efficiently explore the trade-off between stability and accuracy, approximating the full Pareto frontier.

### Mechanism 2
The framework guarantees Pareto optimality (or weak Pareto optimality) of the selected model sequences, ensuring that no other sequence achieves better performance in both accuracy and stability. The ϵ-constrained formulation enforces that each model's accuracy loss is within a multiplicative factor of the best possible for that batch, while minimizing stability loss. Theorems 2.4, 2.5, and 2.6 provide theoretical guarantees on the Pareto optimality of solutions, allowing users to navigate the trade-off through the accuracy tolerance parameter α.

### Mechanism 3
The framework enables adaptive model retraining by constructing a new slowly varying sequence on the updated dataset, ensuring stability across retraining iterations even with new data. When a new batch of data becomes available, the framework retrains the entire sequence on the updated dataset, selecting a new final model that remains close to the previous one in terms of the distance metric. This "stability by process" approach ensures smooth transitions and generalizes to longer deployment horizons through the assumption of a common initial model.

## Foundational Learning

- Concept: Pareto optimality and multi-objective optimization
  - Why needed here: The framework aims to find model sequences that balance predictive power and stability, which are often conflicting objectives. Understanding Pareto optimality is crucial for navigating the trade-off and ensuring that the selected sequence is optimal in terms of both objectives.
  - Quick check question: What is the difference between Pareto optimal and weakly Pareto optimal solutions in multi-objective optimization?

- Concept: Generalization bounds and excess risk
  - Why needed here: The framework relies on theoretical guarantees that the selected model sequences will generalize well to new data. Understanding generalization bounds and excess risk is essential for assessing the quality of the empirical Pareto frontier and ensuring that the framework performs well in practice.
  - Quick check question: How does the Pareto excess risk bound differ from the standard excess risk bound for independent model training?

- Concept: Mixed-integer optimization (MIO) and shortest path algorithms
  - Why needed here: The framework uses an MIO formulation to select candidate models from pre-computed sets, which can be reduced to a shortest path problem in a directed graph. Understanding MIO and shortest path algorithms is crucial for implementing the framework efficiently and ensuring that it scales to large model spaces.
  - Quick check question: How does the reduction of the MIO formulation to a shortest path problem enable polynomial-time solution?

## Architecture Onboarding

- Component map: Data preprocessing -> Candidate model generation -> Distance metric computation -> MIO solver -> Model sequence evaluation -> Adaptive retraining
- Critical path:
  1. Preprocess data and generate candidate model sets for each batch
  2. Compute distance metrics between candidate models
  3. Solve the MIO formulation to select the optimal model sequence
  4. Evaluate the stability and accuracy of the selected sequence
  5. If new data becomes available, repeat steps 1-4 on the updated dataset
- Design tradeoffs:
  - Model diversity vs. computational efficiency: Generating more diverse candidate model sets can improve the quality of the selected sequence but increases computational cost
  - Accuracy tolerance vs. stability: Allowing larger accuracy tolerances can improve stability but may sacrifice predictive power
  - Distance metric choice: Different distance metrics may prioritize different aspects of model stability (e.g., structural vs. analytical insight-based)
- Failure signatures:
  - High stability loss despite small accuracy tolerances: Indicates that the candidate model sets are not diverse enough or the distance metric is not capturing relevant stability aspects
  - Low accuracy despite small accuracy tolerances: Indicates that the MIO formulation is not properly enforcing the accuracy constraints or the candidate model sets are not accurate enough
  - Unstable retraining with new data: Indicates that the assumption of a common initial model is not holding or the distance metric is not capturing relevant stability aspects in the new data
- First 3 experiments:
  1. Generate candidate model sets for each batch and compute distance metrics between them
  2. Solve the MIO formulation for different accuracy tolerances and compare the stability and accuracy of the selected sequences
  3. Evaluate the generalization performance of the selected sequences on a held-out test set and compare to a greedy retraining baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the framework perform when distributional shifts are significant and cannot be approximated by the available candidate model sets?
- Basis in paper: The paper discusses stability in retraining but notes limitations when model sets cannot capture significant shifts
- Why unresolved: The paper primarily focuses on stable retraining under relatively stable data distributions, with limited exploration of extreme distributional shifts
- What evidence would resolve it: Empirical results showing performance degradation under severe distribution shifts, or theoretical bounds quantifying the impact of shift magnitude on stability guarantees

### Open Question 2
- Question: What is the computational trade-off between model diversity in candidate sets and practical retrainability for very large-scale problems?
- Basis in paper: The paper mentions that increasing |Fb| improves approximation quality but acknowledges scalability concerns for large candidate sets
- Why unresolved: The paper demonstrates polynomial-time solvability but doesn't quantify the exact computational burden for industrial-scale applications
- What evidence would resolve it: Runtime analysis showing scaling behavior with respect to feature dimension, dataset size, and candidate set cardinality

### Open Question 3
- Question: How robust is the framework when deployed continuously over very long time horizons with many retraining iterations?
- Basis in paper: The paper mentions deployment in a hospital setting but doesn't analyze long-term accumulation of approximation errors or concept drift effects
- Why unresolved: The experiments focus on relatively short retraining sequences (10-11 iterations), leaving open questions about cumulative effects over years of deployment
- What evidence would resolve it: Longitudinal studies tracking model performance and stability over extended deployment periods with periodic retraining

## Limitations
- Computational overhead of pre-computing candidate model sets for each batch may be prohibitive for large-scale deployments
- Assumes sufficient computational resources to generate diverse model candidates, which may not be feasible for complex models
- Adaptive retraining mechanism relies on the initial model remaining representative, which may break down with concept drift
- Theoretical guarantees rely on assumptions (bounded loss functions, finite model classes) that may not hold for complex model architectures

## Confidence
- Claims about Pareto optimality and stability guarantees: Medium
- Empirical validation across model families: Medium
- Scalability to industrial applications: Low

## Next Checks
1. Test the framework's scalability by measuring runtime and candidate set quality as model complexity and dataset size increase
2. Validate generalization bounds by comparing theoretical predictions against empirical performance on held-out data across different data distributions
3. Evaluate stability guarantees when concept drift exceeds the assumed bounds, particularly for the adaptive retraining mechanism