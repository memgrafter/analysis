---
ver: rpa2
title: 'Federated Source-free Domain Adaptation for Classification: Weighted Cluster
  Aggregation for Unlabeled Data'
arxiv_id: '2412.13757'
source_url: https://arxiv.org/abs/2412.13757
tags:
- cluster
- client
- domain
- each
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Federated Learning with Weighted Cluster
  Aggregation (FedWCA), a novel method for Federated Source-free Domain Adaptation
  (FFREEDA) in classification tasks. FedWCA addresses the challenge of training personalized
  models for clients with unlabeled data from diverse target domains, using a pre-trained
  source model that cannot be accessed post-training due to privacy concerns.
---

# Federated Source-free Domain Adaptation for Classification: Weighted Cluster Aggregation for Unlabeled Data

## Quick Facts
- arXiv ID: 2412.13757
- Source URL: https://arxiv.org/abs/2412.13757
- Reference count: 40
- Primary result: Achieves average accuracy improvements of 7.10%, 4.86%, and 1.32% on Digit-Five, PACS, and Office-Home datasets respectively

## Executive Summary
This paper introduces Federated Learning with Weighted Cluster Aggregation (FedWCA), a novel method for Federated Source-free Domain Adaptation (FFREEDA) in classification tasks. The approach addresses the challenge of training personalized models for clients with unlabeled data from diverse target domains, using a pre-trained source model that cannot be accessed post-training due to privacy concerns. FedWCA combines parameter-free client clustering, weighted cluster aggregation, and improved prototype-based pseudo-labeling to achieve significant performance gains over existing methods while preserving client privacy.

## Method Summary
FedWCA addresses FFREEDA by clustering clients based on first-layer parameters of locally adapted feature extractors, then aggregating cluster models with weighted combinations that account for cross-domain knowledge. Each client performs local adaptation using improved pseudo-labeling that fixes labels to preserve global knowledge and applies mixup between matched and mismatched pseudo-label samples. The method operates in three phases: private parameter-free client clustering using FINCH, weighted cluster aggregation creating soft cluster models, and local adaptation with prototype-based pseudo-labeling. Experiments on Digit-Five, PACS, and Office-Home datasets demonstrate significant improvements over existing methods while maintaining privacy through parameter-only clustering.

## Key Results
- Achieves average accuracy improvements of 7.10% on Digit-Five dataset compared to existing methods
- Outperforms competitors by 4.86% on PACS dataset while preserving privacy
- Shows 1.32% improvement on Office-Home dataset with parameter-free clustering
- Effectively mitigates domain shifts while maintaining client privacy through first-layer parameter clustering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustering clients based on first-layer parameters preserves domain-specific information while avoiding privacy leaks.
- Mechanism: The first layer of a feature extractor contains the most domain-specific information, and using only these parameters for clustering avoids sharing raw data or higher-level features that could leak more information.
- Core assumption: Domain information is predominantly stored in the first layer of neural networks, and parameter similarity correlates with domain similarity.
- Evidence anchors:
  - [abstract]: "we cluster clients based on the initial layer parameters of each client's locally adapted feature extractor, which contains domain information"
  - [section]: "we focus on the shallow layers where domain knowledge is stored [14]. Specifically, we only use the first-layer parameters as this layer, the furthest from the common classifier among clients, best represents domain-specific information"
  - [corpus]: Weak - no direct corpus evidence found
- Break condition: If domain information is distributed across multiple layers rather than concentrated in the first layer, or if parameter similarity doesn't correlate with domain similarity.

### Mechanism 2
- Claim: Weighted cluster aggregation allows clients to benefit from all domains rather than being restricted to their assigned cluster.
- Mechanism: Each client computes personalized weights for all cluster models based on how well each cluster's feature extractor aligns samples with classifier vectors, then combines these weighted models with their own cluster model using pseudo-performance weights.
- Core assumption: Clients can accurately assess the benefit of different cluster models using their unlabeled data, and combining multiple domain perspectives improves adaptation.
- Evidence anchors:
  - [abstract]: "we introduce the Weighted Cluster Aggregation (WCA) module, enabling clients to combine models from each cluster (domain) using personal weights"
  - [section]: "This blended model not only leverages cross-cluster knowledge but also corrects possible cluster assignment errors"
  - [corpus]: Weak - no direct corpus evidence found
- Break condition: If the weight calculation mechanism fails to identify beneficial clusters, or if combining diverse domains introduces harmful interference.

### Mechanism 3
- Claim: Prototype-based pseudo-labeling with fixed labels and mixup generates more reliable labels for adaptation.
- Mechanism: Uses round-fixed pseudo-labels to preserve global knowledge, selects more reliable pseudo-labels by comparing similarity scores normalized across prototypes, and applies mixup between matched and mismatched pseudo-label samples to create more reliable training examples.
- Core assumption: Pseudo-labels that agree between initial and cluster models are more reliable, and mixup between high-confidence samples improves label quality.
- Evidence anchors:
  - [abstract]: "local domain adaptation with pseudo-labeling" and "We improve it to retain global knowledge and produce more reliable pseudo-labels"
  - [section]: "If f init k and fck produce identical pseudo-labels, these are deemed more reliable; distinct pseudo-labels indicate reduced reliability"
  - [corpus]: Weak - no direct corpus evidence found
- Break condition: If pseudo-label quality degrades over rounds, or if mixup introduces harmful artifacts that confuse the model.

## Foundational Learning

- Concept: Domain adaptation and transfer learning
  - Why needed here: The method addresses domain shifts between source and target domains, requiring understanding of how models adapt across different data distributions
  - Quick check question: What is the key difference between domain adaptation and standard supervised learning?

- Concept: Federated learning and data heterogeneity
  - Why needed here: The setting involves multiple clients with non-IID data, requiring understanding of how federated aggregation works with heterogeneous distributions
  - Quick check question: Why is FedAvg insufficient when clients have significantly different data distributions?

- Concept: Prototype-based clustering and similarity metrics
  - Why needed here: Client clustering uses cosine similarity between model parameters, and pseudo-labeling uses feature prototypes
  - Quick check question: How does cosine similarity between feature vectors relate to class membership in prototype-based methods?

## Architecture Onboarding

- Component map: Client training -> Parameter extraction for clustering -> Server clustering using FINCH -> Cluster model aggregation -> Weight calculation -> Model distribution -> Client adaptation with pseudo-labeling

- Critical path: Client training → Parameter extraction → Server clustering → Cluster model aggregation → Weight calculation → Model distribution → Client adaptation with pseudo-labeling

- Design tradeoffs: Parameter-only clustering vs. feature-based clustering (privacy vs. clustering accuracy), weighted aggregation vs. simple averaging (complexity vs. performance), fixed vs. dynamic pseudo-labels (stability vs. adaptability)

- Failure signatures: Poor clustering leading to incorrect domain assignments, weight calculation producing uniform weights (losing personalization), pseudo-label quality degradation over rounds

- First 3 experiments:
  1. Test clustering performance with different layer selections (first vs. second vs. all layers) on a simple dataset
  2. Validate weight calculation by comparing performance with uniform weights and one-hot weights
  3. Assess pseudo-label reliability by measuring agreement between initial and cluster model pseudo-labels across different domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FedWCA change when using different clustering algorithms besides FINCH, particularly when considering privacy-preserving alternatives?
- Basis in paper: [explicit] The paper mentions privacy concerns about sharing style features in LADD's clustering method and compares FINCH with LADD clustering and hypothetical true clustering.
- Why unresolved: The paper only tests one parameter-free clustering method (FINCH) and doesn't explore other privacy-preserving clustering alternatives.
- What evidence would resolve it: Comparative results showing FedWCA's performance with different clustering algorithms while maintaining privacy guarantees.

### Open Question 2
- Question: Can the weighted cluster aggregation approach be extended to other types of federated learning tasks beyond classification, such as object detection or segmentation?
- Basis in paper: [inferred] The paper focuses specifically on classification tasks, noting that LADD focuses on semantic segmentation, suggesting potential for task-specific adaptations.
- Why unresolved: The method relies on feature extractor convergence towards classifier vectors, which may not directly apply to tasks without clear class boundaries.
- What evidence would resolve it: Experimental results showing FedWCA's effectiveness on non-classification tasks, particularly those requiring localization or boundary detection.

### Open Question 3
- Question: What is the impact of the number of clusters (C) on FedWCA's performance, and how can optimal cluster numbers be determined without prior knowledge of domain distributions?
- Basis in paper: [explicit] The paper uses parameter-free clustering with FINCH but doesn't explore how performance varies with different cluster counts.
- Why unresolved: While FINCH determines cluster count automatically, the paper doesn't analyze whether the chosen cluster number is optimal for each dataset.
- What evidence would resolve it: Systematic evaluation of FedWCA across varying cluster numbers, comparing automatic selection methods against optimal choices.

### Open Question 4
- Question: How does FedWCA perform in scenarios with extreme class imbalance or when some clients have very limited data compared to others?
- Basis in paper: [inferred] The paper evaluates on datasets with relatively balanced class distributions and similar client data sizes, but doesn't address extreme imbalance scenarios.
- Why unresolved: The prototype-based pseudo-labeling and weighted aggregation methods may behave differently under severe data imbalance conditions.
- What evidence would resolve it: Experiments testing FedWCA's performance on datasets with varying degrees of class imbalance and client data heterogeneity.

## Limitations
- Privacy guarantees rely on assumption that first-layer parameters don't leak sensitive information
- Method's effectiveness depends on parameter similarity correlating with domain similarity
- Scalability to large-scale federated systems with many clients remains unverified

## Confidence

*High Confidence*: The experimental results demonstrating performance improvements over baseline methods are well-supported by the presented data across multiple datasets. The overall methodology and implementation details are sufficiently specified for reproduction.

*Medium Confidence*: The theoretical foundations of parameter-based clustering and weighted aggregation are plausible but lack extensive empirical validation. The privacy guarantees of the clustering approach are asserted rather than proven.

*Low Confidence*: The scalability of the approach to large-scale federated systems and its robustness to various types of data heterogeneity are not thoroughly examined.

## Next Checks

1. Conduct ablation studies removing the mixup component and parameter-based clustering to quantify their individual contributions to performance gains.

2. Test the method's sensitivity to different numbers of clusters and clustering algorithms to establish the robustness of the clustering mechanism.

3. Evaluate the method's performance when clients have significantly different amounts of data to assess its effectiveness in realistic federated scenarios.