---
ver: rpa2
title: Evaluating the effectiveness of predicting covariates in LSTM Networks for
  Time Series Forecasting
arxiv_id: '2404.18553'
source_url: https://arxiv.org/abs/2404.18553
tags:
- covariates
- forecast
- horizon
- values
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates how well LSTM networks predict both target
  values and future covariates in time series forecasting. The authors augment four
  real-world datasets with synthetic, time-correlated covariates and train two LSTM
  models: a standard base-lstm and a novel seg-lstm that segments data by seasonality.'
---

# Evaluating the effectiveness of predicting covariates in LSTM Networks for Time Series Forecasting

## Quick Facts
- arXiv ID: 2404.18553
- Source URL: https://arxiv.org/abs/2404.18553
- Reference count: 36
- This paper evaluates how well LSTM networks predict both target values and future covariates in time series forecasting.

## Executive Summary
This paper evaluates how well LSTM networks predict both target values and future covariates in time series forecasting. The authors augment four real-world datasets with synthetic, time-correlated covariates and train two LSTM models: a standard base-lstm and a novel seg-lstm that segments data by seasonality. They test performance across forecast horizons, varying the number of covariates and their correlation with target values. Results show that while LSTM models can exploit highly correlated covariates for short horizons, performance degrades sharply as correlation weakens or horizons lengthen. The seg-lstm model generally outperforms on longer horizons, but multivariate predictions rarely beat univariate baselines except in ideal conditions. The findings suggest that forcing LSTMs to jointly predict covariates is often detrimental, especially over longer forecast horizons.

## Method Summary
The authors augment four real-world datasets from the Monash repository with synthetic, time-correlated covariates and train two LSTM architectures: base-lstm (standard 2-layer LSTM) and seg-lstm (novel segmentation-based approach). Both models predict target values and future covariates jointly using Smooth L1 loss. The seg-lstm reshapes context into seasonal segments to reduce long-horizon drift. Models are trained with AdamW optimizer and OneCycle learning rate scheduling, evaluated across varying numbers of covariates (1-3) and correlation levels (0-1.9 noise factor). Performance is measured using MAE, RMSE, and sMAPE on raw values.

## Key Results
- LSTM models can exploit highly correlated covariates for short horizons but performance degrades sharply as correlation weakens or horizons lengthen
- The seg-lstm model generally outperforms on longer horizons, but multivariate predictions rarely beat univariate baselines except in ideal conditions
- Number of covariates can offset error accumulation by providing multiple future-aligned signals per timestep, delaying onset of significant errors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LSTM networks struggle to jointly model target and covariate sequences over long horizons.
- **Mechanism:** Covariates are added as future-aligned features, forcing the LSTM to predict them together with targets. As forecast horizon grows, the error in predicted covariates compounds and propagates, degrading target accuracy.
- **Core assumption:** The LSTM's memory and gating are insufficient to disentangle and accurately forecast high-dimensional future sequences when temporal correlation is weak or absent.
- **Evidence anchors:**
  - [abstract] "compelling the network to predict multiple values can prove detrimental to model performance, even in the presence of informative covariates"
  - [section] "performance degrades sharply as correlation weakens or horizons lengthen"
  - [corpus] Weak: no direct LSTM-covar dynamics studies in neighbors.
- **Break condition:** When future covariates are perfectly correlated with targets (PCC = 1.0) and the forecast horizon is short, the model can exploit the correlation without degradation.

### Mechanism 2
- **Claim:** Seg-lstm improves performance by reducing the effective forecast step size via segment-wise autoregression.
- **Mechanism:** The model reshapes context into segments of size equal to the data's seasonality, training to predict one timestep ahead per segment. During inference, it rolls forward by appending predicted segments, effectively shortening the step between updates.
- **Core assumption:** Seasonal segmentation reduces long-horizon drift by maintaining local temporal coherence in predictions.
- **Evidence anchors:**
  - [section] "we propose an LSTM with a simple modification to handle input data... reshape each context window... into a vector of dimension C/d × d"
  - [section] "During inference, we predict the next timestep vector, append it to the previous timestep segment, and drop the oldest vector"
  - [corpus] Weak: no analogous seasonal-segmenting LSTMs in neighbors.
- **Break condition:** When seasonality is long or irregular, segment boundaries misalign with temporal dependencies, negating the benefit.

### Mechanism 3
- **Claim:** Number of covariates can offset error accumulation by providing multiple future-aligned signals per timestep.
- **Mechanism:** Each covariate acts as a separate future lead indicator; with k covariates, the first k forecast steps can be directly informed, delaying the onset of accumulated error.
- **Core assumption:** The LSTM can learn to use each covariate as a distinct future predictor without conflating them temporally.
- **Evidence anchors:**
  - [section] "the addition of each covariate shifts the onset of significant errors by one timestep"
  - [section] "Clearly the network is effectively utilising the covariates to learn the values for the first k timesteps"
  - [corpus] Weak: no neighbor studies on covariate count effects.
- **Break condition:** When covariates are not truly independent or when the correlation is low, the model may not differentiate their signals, and error propagation resumes.

## Foundational Learning

- **Concept:** Pearson correlation coefficient (PCC)
  - Why needed here: Used to quantify the strength of artificial covariate-target alignment, directly affecting performance.
  - Quick check question: If a covariate has PCC = 0.8 with its target, what does that imply about the strength of their linear relationship?

- **Concept:** Mean Absolute Scaling (MAS)
  - Why needed here: The paper uses batch-wise MAS to normalize inputs, preventing scale issues during joint target/covariate training.
  - Quick check question: What is the effect of MAS when a batch contains a time series with near-zero values?

- **Concept:** Smooth L1 loss
  - Why needed here: Chosen for stable regression on both target and covariate outputs, especially when errors are large.
  - Quick check question: How does Smooth L1 differ from L2 loss when the error magnitude is above 1?

## Architecture Onboarding

- **Component map:**
  - Input: Concatenated scaled target and k covariates
  - Core: 2-layer LSTM (40 cells each)
  - Head: FC layer (40 cells, ReLU) → Output layer (target + k covariates)
  - Optional: Seg-lstm reshape layer (C/d × d)

- **Critical path:**
  - Forward: Input → LSTM → FC → Output → Inverse scale → Loss
  - Backward: Loss → Output → FC → LSTM → Gradients propagate to all parameters

- **Design tradeoffs:**
  - Univariate baseline: Simpler, no covariate noise injection, but no future signal usage.
  - Seg-lstm: Longer context improves long-horizon accuracy but increases memory and inference latency.
  - Covariate augmentation: Higher correlation helps but at cost of model complexity and potential overfitting.

- **Failure signatures:**
  - Performance drops when PCC < 0.9 over horizons > 3 timesteps.
  - Seg-lstm may fail if seasonality does not match natural data periodicity.
  - Covariate noise (γ) too high can drown the signal, causing training instability.

- **First 3 experiments:**
  1. Train base-lstm on Hospital dataset with k=1, PCC=1.0; compare MAE to univariate.
  2. Train seg-lstm on Electricity dataset with k=0; verify long-horizon performance gain.
  3. Train base-lstm on Traffic dataset with k=3, PCC=0.5; observe error trajectory over horizon.

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Study relies on synthetic, artificially correlated covariates rather than real multivariate signals, limiting ecological validity
- Evaluation focuses exclusively on LSTM-based models without comparing against modern alternatives like transformer-based architectures
- The segmentation mechanism (seg-lstm) is novel but lacks ablation studies to isolate the impact of seasonal segmentation versus other architectural choices

## Confidence
- **High confidence:** The core finding that LSTM models struggle with long-horizon forecasting when forced to predict covariates, supported by consistent degradation patterns across all datasets and correlation levels
- **Medium confidence:** The specific performance advantage of seg-lstm over base-lstm on longer horizons, though this benefit appears conditional on data seasonality alignment
- **Low confidence:** The general superiority of multivariate predictions over univariate baselines, as the paper notes this only holds under specific, ideal conditions (high correlation, short horizons)

## Next Checks
1. Replicate the experiments using real multivariate datasets with naturally occurring covariate relationships rather than synthetic correlations
2. Test the seg-lstm architecture with irregular or non-integer seasonal periods to evaluate robustness beyond the assumed periodic structure
3. Compare LSTM-based multivariate forecasting against transformer architectures and classical methods (ARIMA, Prophet) to establish relative performance boundaries