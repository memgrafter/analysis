---
ver: rpa2
title: How predictable is language model benchmark performance?
arxiv_id: '2401.04757'
source_url: https://arxiv.org/abs/2401.04757
tags:
- performance
- tasks
- compute
- big-bench
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the predictability of large language model
  performance across different benchmarks and compute scales. The authors use a two-step
  procedure: first, they estimate model loss from scaling laws, then fit simple functional
  forms to the performance versus loss relationship.'
---

# How predictable is language model benchmark performance?

## Quick Facts
- arXiv ID: 2401.04757
- Source URL: https://arxiv.org/abs/2401.04757
- Reference count: 40
- One-line primary result: Average benchmark performance is decently predictable from scaling laws, but individual task performance is less predictable with average errors of 18 percentage points.

## Executive Summary
This paper investigates how well large language model performance on benchmarks can be predicted from scaling laws. Using a two-step procedure that first estimates model loss from Chinchilla scaling laws and then fits sigmoid functions to the performance versus loss relationship, the authors evaluate predictability through extensive back-testing. They find that while aggregate benchmark performance is fairly predictable with errors of 5-10 percentage points when doubling compute, individual task performance is less predictable but still significantly better than chance. The analysis reveals that predictability is influenced by the starting scale of models and the choice of evaluation metrics, with above-chance performance models providing more reliable predictions.

## Method Summary
The authors use a two-step prediction framework: first estimating reducible loss for each model using Chinchilla scaling laws based on model size and dataset parameters, then fitting sigmoid functional forms to the relationship between performance and reducible loss. They evaluate predictability through back-testing by holding out data points and measuring prediction errors across five orders of magnitude of compute scaling. The analysis uses 11 recent model architectures ranging from 2M to 540B parameters, evaluating performance on BIG-Bench Hard and MMLU benchmarks. Prediction accuracy is assessed through mean absolute error (MAE) in percentage points between predicted and actual performance.

## Key Results
- Aggregate benchmark performance (BBH and MMLU) is predictably modeled by scaling laws with sigmoid fits, with prediction errors of 5-10 percentage points when doubling compute
- Individual task performance is less predictable but remains significantly better than chance, with average errors of 18 percentage points across an order of magnitude
- Predictability is influenced by starting scale, with better predictions possible when models already perform above random chance
- Recent frontier models like Yi-6B perform above trend, suggesting potential algorithmic improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Performance on aggregate benchmarks is predictable from scaling laws because reducible loss scales predictably with compute, and there is an S-shaped relationship between loss and performance.
- Mechanism: Scaling laws estimate reducible loss (L_red) from model size (N) and dataset size (D). This reducible loss is mapped to "scaled compute" via optimal Chinchilla scaling. The performance versus scaled compute relationship follows a sigmoid form: performance rises slowly at first, accelerates in the middle range, then plateaus. This functional form captures the diminishing returns and saturation behavior seen in benchmarks.
- Core assumption: The Chinchilla scaling law accurately predicts reducible loss across diverse model architectures and dataset sizes, and the sigmoid form correctly models the performance-loss relationship.
- Evidence anchors:
  - [abstract] "We show that average benchmark performance, aggregating over many individual tasks and evaluations as in the commonly-used BIG-Bench dataset, is decently predictable as a function of training compute scale."
  - [section] "This reducible loss is mapped to "scaled compute" via optimal Chinchilla scaling. The performance versus scaled compute relationship follows a sigmoid form: performance rises slowly at first, accelerates in the middle range, then plateaus."
  - [corpus] Weak: Corpus neighbors do not directly address the predictability mechanism from scaling laws and sigmoid fits.
- Break condition: The scaling law breaks down if model architectures or training procedures deviate significantly from the conditions under which the scaling law was measured. The sigmoid assumption breaks if performance plateaus at different levels or rises in a non-S-shaped curve.

### Mechanism 2
- Claim: Individual task performance is less predictable than aggregate performance because tasks have heterogeneous scaling behaviors, and the emergence of capabilities at different scales introduces variability.
- Mechanism: While the average performance across many tasks is smoothed out by aggregation, individual tasks can have very different scaling behaviors. Some tasks show sudden "emergent" improvements at specific scales, while others may scale more gradually or even inversely. The sigmoid fit, while decent for aggregate performance, may not capture the sharp transitions or unique scaling patterns of individual tasks. This leads to higher prediction errors for individual tasks.
- Core assumption: Individual tasks within a benchmark have sufficiently different scaling behaviors that averaging masks this variability, and that the sigmoid form is not flexible enough to capture all task-specific scaling patterns.
- Evidence anchors:
  - [abstract] "For individual BIG-Bench tasks across an order of magnitude in compute yields higher average errors of 18pp. Nonetheless, individual task performance remains significantly more predictable than chance."
  - [section] "Individual tasks are highly variable in their scaling, and the sharp emergence of capabilities can make it difficult to predict performance... The examples in Figure 4 illustrate why analyses that look at a small number of models (values along the x axis) struggle to predict task progress, yet a larger data series shows significant (but imperfect) predictability."
  - [corpus] Weak: Corpus neighbors do not directly address the variability in individual task scaling.
- Break condition: If all tasks within a benchmark had similar scaling behaviors, the difference in predictability between aggregate and individual tasks would diminish. If a more flexible functional form could capture individual task scaling patterns, prediction errors would decrease.

### Mechanism 3
- Claim: Predictability is better when there are models already performing above random chance, because the sigmoid fit can then estimate the inflection point and slope more accurately.
- Mechanism: The sigmoid function has parameters that control the inflection point (where performance starts to accelerate) and the slope (how quickly performance improves). When only low-performing models are available, it is difficult to estimate these parameters accurately, leading to underestimation of future performance. As more models with above-chance performance are included, the fit can better capture the true shape of the scaling curve.
- Core assumption: The sigmoid form is the correct functional form for the performance-scaling relationship, and that the parameters can be estimated more accurately with data from the accelerating region of the curve.
- Evidence anchors:
  - [abstract] "There is a caveat: the S-curve relationship between scale and benchmark performance makes it difficult to predict future benchmark performance from only low-performing models. As models begin to perform substantially better than chance, future performance improvements become increasingly predictable."
  - [section] "Figure 3c shows how prediction is harder when only using data before 1e23 FLOP, where performance starts to reliably increase above random chance. This can also be seen in the example back-tests of Figure 2b, where holding out increasingly many points leads to worse predictions."
  - [corpus] Weak: Corpus neighbors do not directly address the impact of model performance level on predictability.
- Break condition: If a different functional form is more appropriate for the performance-scaling relationship, or if the relationship is not S-shaped, then the advantage of having above-chance performance models may not hold.

## Foundational Learning

- Concept: Scaling laws and reducible loss
  - Why needed here: The core of the predictability analysis relies on estimating how much a model's performance can be improved by scaling its size and training data, which is captured by the reducible loss.
  - Quick check question: If a model has a high reducible loss, does that mean it has more room for improvement through scaling, or less?

- Concept: Sigmoid functional form and its parameters
  - Why needed here: The sigmoid form is used to model the relationship between performance and scaled compute, and understanding its shape and parameters is crucial for interpreting the fits and predictions.
  - Quick check question: In a sigmoid curve, what does the inflection point represent in terms of model performance scaling?

- Concept: Back-testing and prediction error
  - Why needed here: Back-testing is the method used to evaluate the predictability of the model, by holding out data points and measuring the error in predicting them. Understanding this process is key to interpreting the results.
  - Quick check question: If the mean absolute error (MAE) in predicting BBH performance across an order of magnitude is 6 percentage points, what does that tell us about the reliability of the predictions?

## Architecture Onboarding

- Component map:
  - Data collection: Gathering performance data for various models on BIG-Bench and MMLU benchmarks.
  - Scaling law application: Using the Chinchilla scaling law to estimate reducible loss for each model.
  - Functional form fitting: Fitting sigmoid (and other) functions to the performance vs. reducible loss data.
  - Back-testing: Holding out data points and measuring prediction errors to evaluate predictability.
  - Analysis and visualization: Interpreting the results and creating plots to illustrate the findings.

- Critical path:
  1. Collect performance data for a diverse set of models on the target benchmarks.
  2. Calculate reducible loss for each model using the Chinchilla scaling law with parameters N (model size) and D (dataset size).
  3. Fit functional forms (primarily sigmoid) to the performance vs. reducible loss data.
  4. Perform back-tests to measure prediction errors at different scales and for different subsets of data.
  5. Analyze the results to draw conclusions about predictability and its limitations.

- Design tradeoffs:
  - Simple vs. complex functional forms: Using simple forms like sigmoid makes the analysis more interpretable but may not capture all nuances of the data. More complex forms could improve fit but reduce interpretability.
  - Inclusion of all vs. only high-performing models: Including all models provides a more complete picture but may reduce predictability if low-performing models are hard to fit. Focusing on high-performing models may improve predictability but miss important trends.
  - Aggregate vs. individual task analysis: Aggregate analysis smooths out variability but may miss important task-specific patterns. Individual task analysis captures these patterns but is noisier and less predictable.

- Failure signatures:
  - High prediction errors in back-tests, especially for individual tasks or when extrapolating far ahead.
  - Systematic underestimation of future performance when only low-performing models are available.
  - Large deviations from the fitted curves for certain models or tasks, indicating that the scaling law or functional form may not be appropriate.

- First 3 experiments:
  1. Reproduce the main analysis on a subset of the data to verify the key findings (e.g., predictability of aggregate performance, higher errors for individual tasks).
  2. Try fitting alternative functional forms (e.g., different sigmoid parameterizations, piecewise linear) to see if they improve prediction accuracy.
  3. Perform sensitivity analysis on the scaling law parameters to understand how errors in the scaling law propagate to prediction errors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would using alternative metrics like soft metrics or log-probabilities affect the predictability of individual task performance?
- Basis in paper: [explicit] The paper notes that soft metrics like log-probability of correct answer may allow better forecasting of capabilities, but such metrics are seldom provided in BIG-Bench evaluations.
- Why unresolved: The paper only uses standard evaluation metrics (multiple choice grade, exact string match) and doesn't explore how alternative metrics might improve predictability.
- What evidence would resolve it: Experiments comparing predictability using different evaluation metrics (multiple choice grade vs exact match vs log-probabilities) on the same tasks would show whether alternative metrics improve predictability.

### Open Question 2
- Question: How does algorithmic progress affect the relationship between scaled compute and benchmark performance?
- Basis in paper: [explicit] The paper observes that larger fit errors occur where there is most architectural heterogeneity, suggesting algorithmic progress might be a factor. It also notes that recent models like Yi-6B perform above trend.
- Why unresolved: The paper doesn't explicitly model or separate out the effects of algorithmic progress from compute scaling.
- What evidence would resolve it: A dataset tracking model performance while controlling for architectural differences and algorithmic improvements would allow quantifying their separate effects.

### Open Question 3
- Question: How would larger changes in dataset size affect the relationship between model size, dataset size, and performance?
- Basis in paper: [explicit] The paper notes that while model sizes span several orders of magnitude, training datasets span only a single order of magnitude, and it cannot guarantee similar relationships would persist under larger changes in dataset size.
- Why unresolved: The available dataset is limited in range of dataset sizes, preventing examination of how the model size-dataset size relationship changes at larger scales.
- What evidence would resolve it: A dataset with models trained on datasets spanning multiple orders of magnitude would show how the model size-dataset size relationship changes at larger scales.

## Limitations

- Individual task performance is notably less predictable than aggregate benchmarks, with average errors of 18 percentage points across an order of magnitude in compute
- The sigmoid functional form may not adequately represent tasks with sharp capability emergence or inverse scaling behaviors
- Predictability estimates may be biased by algorithmic progress or dataset differences that aren't captured by the scaling law approach

## Confidence

- **High confidence**: Aggregate benchmark performance (BBH and MMLU) is predictably modeled by scaling laws with sigmoid fits, with prediction errors of 5-10 percentage points when doubling compute.
- **Medium confidence**: Individual task performance remains significantly more predictable than chance, but with high variability and average errors of 18 percentage points.
- **Low confidence**: The specific numerical values of prediction errors across different compute scales and the relative importance of starting scale versus prediction distance.

## Next Checks

1. Replicate the analysis using alternative functional forms (piecewise linear, different sigmoid parameterizations) to assess robustness of the predictability claims to modeling choices.
2. Perform ablation studies holding out frontier models to test whether algorithmic improvements or dataset differences systematically bias the predictability estimates.
3. Apply the same predictability framework to emerging benchmarks with different task characteristics (e.g., mathematical reasoning, code generation) to test generalizability beyond BIG-Bench and MMLU.