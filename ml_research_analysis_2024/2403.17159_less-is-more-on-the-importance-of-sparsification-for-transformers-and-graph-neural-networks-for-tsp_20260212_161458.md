---
ver: rpa2
title: Less Is More -- On the Importance of Sparsification for Transformers and Graph
  Neural Networks for TSP
arxiv_id: '2403.17159'
source_url: https://arxiv.org/abs/2403.17159
tags:
- sparsification
- graph
- learning
- edges
- tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of graph sparsification on transformer
  and graph neural network (GNN) encoders for solving the Traveling Salesman Problem
  (TSP). The authors propose a data preprocessing method that focuses encoders on
  the most relevant parts of TSP instances by removing unpromising edges from the
  TSP graph.
---

# Less Is More -- On the Importance of Sparsification for Transformers and Graph Neural Networks for TSP

## Quick Facts
- arXiv ID: 2403.17159
- Source URL: https://arxiv.org/abs/2403.17159
- Reference count: 40
- Authors achieve 0.10% optimality gap for TSP instances of size 100 and 0.00% for size 50 using transformer encoders with attention masking

## Executive Summary
This paper demonstrates that graph sparsification significantly improves the performance of both GNNs and transformers for solving the Traveling Salesman Problem (TSP). The authors propose preprocessing TSP instances by removing unpromising edges through k-nearest neighbors and 1-Tree based methods. Their approach allows models to focus on the most relevant structural information rather than processing dense graphs where all nodes can directly influence each other. The results show that appropriate sparsification leads to substantial performance increases, with transformers using attention masking achieving state-of-the-art results with gaps as low as 0.10% for size 100 TSP instances.

## Method Summary
The authors propose a data preprocessing method that focuses encoders on the most relevant parts of TSP instances by removing unpromising edges from the TSP graph. They compare two sparsification methods: k-nearest neighbors (k-nn) and 1-Trees, a minimum spanning tree based approach. The 1-Tree method is particularly effective as it naturally maintains graph connectivity while identifying promising edges through subgradient optimization. For transformers, they design attention masking based on the adjacency matrix of sparse TSP graph representations, effectively mirroring the benefits of graph sparsification. The models are evaluated on TSP instances of size 100 and 50 using uniform and mixed data distributions, with performance measured by optimality gap percentage.

## Key Results
- 1-Tree sparsification outperforms k-nearest neighbors by maintaining graph connectivity and retaining more optimal edges
- Appropriate sparsification leads to substantial performance increases for both GAT and GCN encoders
- Ensemble models with different sparsification levels allow focusing on promising parts while maintaining information flow
- Transformer with attention masking achieves state-of-the-art results: 0.10% gap for TSP instances of size 100 and 0.00% for size 50

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph sparsification improves GNN performance by reducing information flooding in dense TSP graphs
- Mechanism: Removing unpromising edges prevents irrelevant nodes from flooding node embeddings during message passing, enabling GNNs to focus on relevant structural information
- Core assumption: Dense TSP graphs allow information flow between all node pairs, causing embeddings to become indistinguishable
- Break Condition: If the sparsification method removes edges that are actually part of optimal TSP solutions, performance will degrade

### Mechanism 2
- Claim: 1-Tree based sparsification outperforms k-nearest neighbors by maintaining graph connectivity and retaining more optimal edges
- Mechanism: 1-Trees are modified minimum spanning trees that naturally maintain connectivity while identifying promising edges through subgradient optimization
- Core assumption: MSTs contain 70-80% of optimal TSP edges, and 1-Trees improve upon this by incorporating optimization
- Break Condition: If the computational cost of 1-Tree generation outweighs the performance benefits, simpler methods may be preferable

### Mechanism 3
- Claim: Attention masking for transformers mirrors graph sparsification benefits by preventing irrelevant attention weights
- Mechanism: Masking attention scores based on sparse graph adjacency matrices focuses transformer attention on promising node pairs
- Core assumption: Transformers without masking aggregate information from all node pairs, similar to GNN information flooding
- Break Condition: If masking removes attention connections that contain useful global information, model performance may suffer

## Foundational Learning

- Concept: Graph Neural Networks
  - Why needed here: GNNs process graph-structured TSP data through message passing between nodes
  - Quick check question: How does a GNN update node embeddings during message passing?

- Concept: Minimum Spanning Trees and 1-Trees
  - Why needed here: 1-Trees provide the theoretical foundation for identifying promising TSP edges while maintaining connectivity
  - Quick check question: What modification makes a 1-Tree different from a standard MST?

- Concept: Attention Mechanisms
  - Why needed here: Transformers use attention to aggregate information from different sequence positions, analogous to GNN message passing
  - Quick check question: How does self-attention in transformers differ from message passing in GNNs?

## Architecture Onboarding

- Component map: TSP instance -> Graph sparsification (k-nn or 1-Tree) -> GNN/Transformer encoder with masking -> Node embeddings -> Autoregressive decoder -> TSP tour
- Critical path:
  1. Generate TSP instance
  2. Apply sparsification/preprocessing
  3. Encode using GNN or transformer with masking
  4. Decode to generate tour
  5. Evaluate against optimal solution
- Design tradeoffs:
  - Sparsification level vs. connectivity: Higher k values maintain connectivity but reduce focus benefits
  - Runtime vs. performance: 1-Tree computation is more expensive than k-nn but yields better results
  - Model complexity vs. data requirements: GATs may need more data to learn node importance compared to GCNs
- Failure signatures:
  - Performance drops when optimal edges are removed during sparsification
  - Models fail to generalize across different TSP instance sizes
  - Runtime becomes prohibitive with high sparsification levels or large instance sizes
- First 3 experiments:
  1. Compare GAT performance on dense vs. sparsified graphs (k=3) with uniform distribution
  2. Test 1-Tree vs. k-nn sparsification for retaining optimal edges on mixed distribution
  3. Evaluate transformer performance with attention masking at different sparsification levels (k=5, 10, 20)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed sparsification methods (k-NN and 1-Tree) compare to other existing graph sparsification techniques for routing problems, such as those based on frequency quadrilaterals?
- Basis in paper: [inferred] The paper mentions a sparsification method based on frequency quadrilaterals proposed by [33] but does not compare its performance to the proposed methods
- Why unresolved: The authors do not provide experimental results comparing their methods to this alternative approach
- What evidence would resolve it: Experimental results showing the performance of the proposed methods against the frequency quadrilateral-based method on TSP and other routing problems

### Open Question 2
- Question: Can the proposed sparsification techniques be effectively extended to routing problems with additional constraints, such as time windows or capacity restrictions?
- Basis in paper: [inferred] The paper mentions the possibility of extending the methods to problems with time windows and capacity restrictions but does not provide any experimental results or analysis
- Why unresolved: The authors do not explore the effectiveness of the methods on more complex routing problems with additional constraints
- What evidence would resolve it: Experimental results demonstrating the performance of the sparsification techniques on routing problems with various constraints, such as the Vehicle Routing Problem with Time Windows (VRPTW) or the Capacitated Vehicle Routing Problem (CVRP)

### Open Question 3
- Question: What is the optimal composition of sparsification levels for the ensemble approach, and how does it vary across different problem sizes and data distributions?
- Basis in paper: [inferred] The authors propose ensembles of different sparsification levels but do not provide a systematic analysis of the optimal composition or its dependency on problem characteristics
- Why unresolved: The paper only tests a few ensemble configurations and does not explore the impact of different compositions on performance
- What evidence would resolve it: A comprehensive study analyzing the performance of various ensemble compositions across different problem sizes, data distributions, and routing problem types

## Limitations
- Scalability concerns exist for larger TSP instances beyond size 100, as experiments were only conducted on sizes 50 and 100
- Lack of theoretical guarantees for the sparsification approaches, relying instead on empirical validation
- Computational overhead of 1-Tree generation is acknowledged but lacks detailed cost-benefit analysis across different TSP sizes

## Confidence
- Medium confidence in the effectiveness of graph sparsification for GNNs, as results show consistent improvements but lack ablation studies isolating the sparsification effect
- High confidence in the 1-Tree approach superiority over k-nn, given the systematic comparison across distributions and the theoretical grounding in MST properties
- Medium confidence in the transformer attention masking mechanism, as the proposed state-of-the-art results are impressive but lack comparison to other transformer architectures without masking

## Next Checks
1. Test the 1-Tree sparsification method's ability to retain optimal edges across multiple TSP instance sizes (25, 50, 100, 200) to verify scalability
2. Conduct an ablation study comparing GNN performance on dense graphs with random edge removal versus the proposed 1-Tree sparsification to isolate the structural benefits
3. Evaluate the computational overhead of 1-Tree generation versus performance gains for transformer models on large-scale TSP instances (size 200+) to assess practical applicability