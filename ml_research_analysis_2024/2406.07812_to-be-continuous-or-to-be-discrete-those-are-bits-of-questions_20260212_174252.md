---
ver: rpa2
title: To be Continuous, or to be Discrete, Those are Bits of Questions
arxiv_id: '2406.07812'
source_url: https://arxiv.org/abs/2406.07812
tags:
- latexit
- sha1
- base64
- association
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the feasibility of extending binary representation
  to the output layer of neural models. The authors propose a structured contrastive
  hashing method that combines constituency parsing with contrastive learning.
---

# To be Continuous, or to be Discrete, Those are Bits of Questions

## Quick Facts
- arXiv ID: 2406.07812
- Source URL: https://arxiv.org/abs/2406.07812
- Reference count: 27
- Primary result: Binary representation can achieve competitive performance on constituency parsing and nested NER tasks using 8-12 bits, demonstrating that binary representation can bridge the gap between continuous deep learning and discrete natural languages

## Executive Summary
This paper investigates the feasibility of extending binary representation to the output layer of neural models for structured prediction tasks. The authors propose a structured contrastive hashing method that combines constituency parsing with contrastive learning, upgrading CKY parsing to the bit level and defining a similarity function using span marginal probabilities. Experiments on constituency parsing and nested named entity recognition tasks demonstrate that binary representation can preserve task-relevant information while requiring only 8-12 bits, showing both competitive performance and implicit clustering capabilities.

## Method Summary
The method extends contrastive hashing to structured contrastive hashing by combining CKY parsing with contrastive learning. The approach upgrades traditional CKY parsing to work with binary labels instead of discrete ones, defining a similarity function based on span marginal probabilities that capture both label and structural information. A novel contrastive loss function with an instance selection strategy (Lmax) is introduced to mitigate the geometric center issue by pulling towards the most likely true positive rather than geometric centers. The model uses a pre-trained language model with an attention hash layer that calculates span scores, followed by bit-level CKY parsing to compute marginal probabilities for the contrastive loss.

## Key Results
- Binary representation achieves competitive performance on constituency parsing using only 8-12 bits
- The method demonstrates implicit clustering capabilities by discovering subclasses within labels
- Performance degrades with too few bits (≤6) due to insufficient representation, and with too many bits (≥12) due to overly concentrated representations
- The Lmax contrastive loss with instance selection strategy improves performance compared to traditional approaches

## Why This Works (Mechanism)

### Mechanism 1
Binary representation preserves task-relevant information when replacing continuous embeddings in both input and output layers. The model uses contrastive learning to ensure that binary codes capture the same information as original continuous representations. The multi-head attention mechanism with one head per bit can capture sufficient semantic subspaces to preserve task performance. Break condition: If semantic subspaces captured by individual bits are insufficient to encode task-relevant information.

### Mechanism 2
Structured contrastive hashing learns both label and structural information simultaneously by using span marginal probabilities. The similarity function defined using marginal probabilities allows the model to learn parsing structure along with labels. Break condition: If marginal probabilities fail to capture sufficient structural information or if binarization loses critical structural distinctions.

### Mechanism 3
The Lmax contrastive loss function with instance selection strategy improves performance by pulling towards the most likely true positive rather than geometric centers. Instead of using mean operations that erase contextual information, Lmax uses max operations to target the closest positive instance, maintaining instance diversity and contextual richness. Break condition: If instance selection strategy fails to identify true positives/negatives or if max operation creates instability in training.

## Foundational Learning

- Concept: Contrastive Learning
  - Why needed here: The model relies on contrastive learning to learn binary representations that preserve task-relevant information without requiring label embeddings.
  - Quick check question: Can you explain the difference between Lself, Lsup, and Lhash loss functions and why Lmax is proposed as an improvement?

- Concept: CKY Algorithm
  - Why needed here: The bit-level CKY extension is crucial for parsing with binary labels instead of discrete ones, enabling the structured contrastive hashing approach.
  - Quick check question: How does the bit-level CKY algorithm differ from traditional CKY, and why is marginal probability important in this context?

- Concept: Marginal Probability in Parsing
  - Why needed here: Marginal probabilities are used to define the similarity function, allowing the model to capture both label and structural information simultaneously.
  - Quick check question: Why doesn't summing marginal probabilities for all labels of a span yield 1, and what does this tell us about the information contained in marginal probabilities?

## Architecture Onboarding

- Component map: Pre-trained Language Model (BERT/XLNET) -> Attention Hash Layer -> Bit-level CKY Parser
- Critical path: Input tokens -> Attention Hash Layer (span score calculation) -> Bit-level CKY (marginal probability computation) -> Contrastive Loss (instance selection and similarity calculation) -> Model parameters update
- Design tradeoffs:
  - Number of bits vs. representation capacity: Too few bits (≤6) lead to insufficient representation, too many bits (≥12) lead to overly concentrated representations
  - Instance selection strategy: Using S vs P in different terms of the loss function affects performance significantly
  - Computational complexity: Selecting only spans from target trees reduces time complexity from O(n⁴) to O(n²)
- Failure signatures:
  - Performance degradation with too few bits (model cannot represent necessary information)
  - Performance plateau or degradation with too many bits (overly concentrated representations)
  - Training instability with incorrect instance selection strategy
  - Poor parsing accuracy indicating loss of structural information
- First 3 experiments:
  1. Vary bit count (4, 6, 8, 10, 12, 14, 16) on PTB to identify optimal bit range and observe performance trends
  2. Compare different instance selection strategies (maxP vs meanP, N∪S vs N∪P) on a small dataset to validate the Lmax approach
  3. Test the model on nested NER task with different bit counts to verify generalizability beyond constituency parsing

## Open Questions the Paper Calls Out

### Open Question 1
What is the theoretical information-theoretic limit of binary representation for natural language tasks, and how does this compare to the performance achieved with 8-16 bits in the experiments? The paper mentions that "information theory suggests that labels with different frequencies carry varying amounts of information" but does not provide a rigorous information-theoretic analysis of the optimal number of bits needed for various natural language tasks.

### Open Question 2
How does the performance of binary representation compare to other discrete representations (e.g., quantized embeddings, discrete latent variables) on structured prediction tasks? The paper positions binary representation as lying "between continuous and discrete representations" but does not directly compare it to other discrete representation methods that have been proposed for similar tasks.

### Open Question 3
How does the implicit clustering capability of binary representation affect downstream tasks beyond structured prediction, such as machine translation or language modeling? The paper mentions that "our method can also be considered as implicitly clustering methods" but primarily focuses on structured prediction tasks and does not explore the implications of the implicit clustering capability for other natural language processing tasks.

## Limitations
- The theoretical claims about why marginal probabilities preserve structural information and why geometric center avoidance is crucial could be stronger
- The method's performance and generalizability need validation across more diverse NLP tasks beyond constituency parsing and nested NER
- The optimal number of bits appears task-dependent, requiring empirical tuning rather than theoretical guidance

## Confidence

- High confidence: Binary representation can achieve competitive performance on constituency parsing with 8-12 bits
- Medium confidence: The method generalizes to nested NER tasks and shows implicit clustering capabilities
- Low confidence: The theoretical claims about why marginal probabilities preserve structural information and why geometric center avoidance is crucial

## Next Checks

1. **Task Diversity Validation**: Test the binary representation approach on additional structured prediction tasks (e.g., dependency parsing, semantic role labeling) to assess generalizability beyond constituency parsing and nested NER.

2. **Ablation Study**: Systematically remove the marginal probability similarity function and replace it with simpler alternatives (e.g., label matching) to quantify the contribution of structural information capture.

3. **Scaling Analysis**: Evaluate the method's performance as dataset size scales, particularly examining whether the instance selection strategy maintains effectiveness on larger datasets where true positive/negative identification becomes more challenging.