---
ver: rpa2
title: 'Small Language Models can Outperform Humans in Short Creative Writing: A Study
  Comparing SLMs with Humans and LLMs'
arxiv_id: '2409.11547'
source_url: https://arxiv.org/abs/2409.11547
tags:
- human
- synopses
- more
- creativity
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "A fine-tuned small language model (SLM), BART-large, outperformed\
  \ average human writers in short creative writing tasks, achieving 14% higher overall\
  \ ratings in readability, understandability, relevance, and informativeness. Human\
  \ raters preferred BART\u2019s outputs in most dimensions, except for creativity,\
  \ where the difference was not statistically significant."
---

# Small Language Models can Outperform Humans in Short Creative Writing: A Study Comparing SLMs with Humans and LLMs

## Quick Facts
- arXiv ID: 2409.11547
- Source URL: https://arxiv.org/abs/2409.11547
- Authors: Guillermo Marco; Luz Rello; Julio Gonzalo
- Reference count: 5
- Primary result: Fine-tuned BART-large outperformed average human writers in short creative writing tasks, achieving 14% higher overall ratings in readability, understandability, relevance, and informativeness.

## Executive Summary
This study demonstrates that a fine-tuned small language model (BART-large) can outperform average human writers in short creative writing tasks, specifically movie synopsis generation. The model achieved 14% higher overall ratings than human writers across multiple quality dimensions including readability, understandability, relevance, and informativeness. Human raters preferred BART's outputs in most dimensions except creativity, where the difference was not statistically significant. The research also reveals important insights about reader bias, showing that knowing a text was AI-generated significantly affects quality perceptions.

## Method Summary
The study fine-tuned BART-large on a large corpus of movie synopses (MPST, CMU Movie Summary Corpus, and Wikipedia Movie Plots) and compared its outputs against human-written synopses across three experimental conditions: unaware of authorship, revealed authorship, and AI-only labeling. Sixty movie titles were selected for evaluation, with human raters (MBA students) scoring 60 synopses across six quality dimensions using a 0-4 Likert scale. The research also included qualitative linguistic analysis comparing BART outputs with those from GPT-3.5 and GPT-4o, examining properties such as formulaic phrases, coherence, surprising associations, and recurrent themes.

## Key Results
- BART-large achieved 14% higher overall ratings than average human writers (2.11 vs 1.85) in readability, understandability, relevance, and informativeness
- Human raters preferred BART's outputs in most dimensions except creativity, where the difference was not statistically significant
- BART generated more surprising associations (15%) compared to GPT-4o (3%), while GPT-4o excelled in coherence and used fewer clichÃ© phrases
- Reader bias significantly affected quality perceptions when authorship was revealed, with AI-generated texts rated lower when their origin was known

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a small model like BART-large on a large, domain-specific corpus can make it outperform average human writers in short creative writing tasks. By exposing the model to thousands of movie synopses, it learns to generalize patterns of narrative structure, coherence, and relevance, while avoiding "gross mistakes" that humans might make. This leads to more consistent and higher-scoring outputs in dimensions like readability and informativity. The core assumption is that the fine-tuning data is representative of quality creative writing and the task is constrained enough that generalization helps more than harming creativity.

### Mechanism 2
Reader bias significantly affects the perceived quality of AI-generated content, especially when the authorship is explicitly revealed. When evaluators know a text was AI-generated, they unconsciously penalize it in dimensions like creativity and attractiveness, even if the content is objectively good. This bias disappears or reverses when the authorship is hidden or all texts are presented as AI-generated. The evidence shows that knowing the author influenced how readers perceived the quality of the text, with AI-generated texts rated lower when their origin was revealed.

### Mechanism 3
Larger models (GPT-4o) produce more coherent and fluent text but are less likely to generate surprising or creative associations compared to smaller, fine-tuned models like BART-large. Larger models are trained to replicate natural language patterns more accurately, leading to greater consistency but also more predictability. Smaller models, due to their limitations, are more likely to deviate from formulaic structures, resulting in higher rates of surprising content. BART exhibited such associations in 9 out of 60 synopses (15%), whereas GPT-3.5 and GPT-4o displayed this only in two and one synopses, respectively.

## Foundational Learning

- Concept: Domain adaptation through fine-tuning
  - Why needed here: The model must learn the specific style and conventions of movie synopses, not just general language patterns
  - Quick check question: What size and quality of training data is needed for the model to generalize well without overfitting?

- Concept: Reader bias and evaluation methodology
  - Why needed here: The perceived quality of AI-generated content depends heavily on whether the reader knows its origin
  - Quick check question: How does revealing or hiding the author's identity change the distribution of ratings?

- Concept: Creativity vs. coherence tradeoff
  - Why needed here: Larger models are better at coherence but worse at surprising associations; smaller models show the opposite pattern
  - Quick check question: How can we quantify "surprising associations" in a reproducible way?

## Architecture Onboarding

- Component map: MPST, CMU Movie Summary Corpus, Wikipedia Movie Plots -> preprocessing -> BART-large fine-tuning -> generate 60 synopses -> human evaluation (68 raters) -> Likert-scale ratings -> statistical analysis -> qualitative linguistic analysis

- Critical path: 1. Fine-tune BART-large on merged synopsis datasets, 2. Generate 60 synopses (balanced length with human counterparts), 3. Run 3 experimental variants with human raters, 4. Perform linguistic comparison with GPT-3.5 and GPT-4o

- Design tradeoffs: Smaller model (BART) vs. larger models (GPT-4o): coherence vs. creativity; Fine-tuning vs. zero-shot: task-specific performance vs. general capability; General readers vs. experts: popularity-based vs. professional evaluation

- Failure signatures: If human raters detect AI authorship and penalize creativity disproportionately; If linguistic analysis shows high repetition of formulaic phrases in BART outputs; If coherence drops sharply for longer synopses in BART

- First 3 experiments: 1. Compare BART-large outputs vs. human-written synopses on readability, understandability, relevance, informativity, and creativity, 2. Repeat experiment with explicit author revelation to measure bias effects, 3. Repeat experiment with all texts labeled as AI-generated to test framing effects

## Open Questions the Paper Calls Out

### Open Question 1
Does the performance of small language models in creative writing tasks generalize to longer texts beyond short movie synopses? The paper notes that short synopses were chosen because SLMs struggle with maintaining coherence in longer texts, and it explicitly states that results should not be extrapolated to longer texts. This remains unresolved as the study focused exclusively on short movie synopses.

### Open Question 2
How do professional literary critics evaluate the creativity and quality of SLM-generated texts compared to general readers? The authors acknowledge that their assessors were general readers rather than professional critics, and they suggest that professional evaluations might differ from popularity-based assessments. The study used general readers as assessors and explicitly notes that professional criteria might yield different results.

### Open Question 3
What is the optimal balance between model size and creative flexibility for different types of creative writing tasks? The authors identify a trade-off between model size and creativity, noting that larger models produce more consistent but less creative outputs, while smaller models show more creative flexibility. The study provides initial evidence of this trade-off but does not systematically explore how different task types might benefit from different model sizes or configurations.

## Limitations
- Evaluation relied on general readers rather than professional writers or literary critics, which may underestimate model performance in more discerning contexts
- Qualitative linguistic analysis was performed on only 60 synopses with manual annotation, introducing potential subjectivity and limited statistical power
- The study focused exclusively on movie synopses, a highly formulaic genre that may not represent broader creative writing tasks
- Fine-tuning used default HuggingFace parameters without systematic hyperparameter optimization

## Confidence
- **High confidence**: BART-large's overall superiority in readability, understandability, relevance, and informativeness when compared to average human writers
- **Medium confidence**: The existence of reader bias when authorship is revealed, and the trade-off between model size and creativity (surprising associations vs. coherence)
- **Low confidence**: The magnitude of differences in creativity ratings between BART and human writers due to non-significant statistical results

## Next Checks
1. **Professional evaluation replication**: Repeat the human evaluation with professional writers and literary critics to assess whether the performance gap between BART and human writers persists under expert scrutiny, and to determine if the observed reader bias is reduced or eliminated.

2. **Cross-genre validation**: Test the fine-tuned BART model on different creative writing tasks (short stories, poetry, dialogue) to evaluate whether the demonstrated advantages in movie synopsis generation extend to other forms of creative writing that may have different structural requirements and creative demands.

3. **Hyperparameter optimization study**: Conduct systematic experimentation with different fine-tuning hyperparameters (learning rate, batch size, number of epochs, temperature settings) to determine whether the observed performance can be improved beyond the default HuggingFace configuration and to establish the sensitivity of the model to these parameters.