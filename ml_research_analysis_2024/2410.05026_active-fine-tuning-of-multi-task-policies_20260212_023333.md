---
ver: rpa2
title: Active Fine-Tuning of Multi-Task Policies
arxiv_id: '2410.05026'
source_url: https://arxiv.org/abs/2410.05026
tags:
- multi-task
- policy
- tasks
- demonstrations
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficiently fine-tuning pre-trained
  generalist policies for multiple tasks with a limited budget of expert demonstrations.
  The key challenge is deciding which tasks to demonstrate and how often to maximize
  multi-task policy performance.
---

# Active Fine-Tuning of Multi-Task Policies

## Quick Facts
- arXiv ID: 2410.05026
- Source URL: https://arxiv.org/abs/2410.05026
- Reference count: 40
- Key outcome: Active Multi-task Fine-tuning (AMF) algorithm that selects maximally informative demonstrations by maximizing expected information gain about the expert policy, outperforming uniform task sampling especially under distribution shift

## Executive Summary
This paper addresses the problem of efficiently fine-tuning pre-trained generalist policies for multiple tasks with limited expert demonstrations. The authors propose AMF (Active Multi-task Fine-tuning), an algorithm that actively selects which tasks to demonstrate and how often to maximize multi-task policy performance. Under regularity assumptions, they prove AMF converges to the expert policy and derive performance guarantees. The approach is particularly effective when the pre-training distribution does not match the evaluation distribution, outperforming uniform task sampling baselines.

## Method Summary
The authors develop an active learning framework for multi-task policy fine-tuning that addresses the challenge of selecting which tasks to demonstrate and how frequently. AMF maximizes the expected information gain about the expert policy by strategically choosing demonstrations across multiple tasks. The algorithm operates by maintaining uncertainty estimates about the expert policy and selecting demonstrations that maximally reduce this uncertainty. Under regularity assumptions including linear parameterization, the authors prove convergence to the expert policy. They also propose a practical approach to mitigate catastrophic forgetting in pre-trained neural policies through selective weight freezing during fine-tuning.

## Key Results
- AMF outperforms uniform task sampling baselines, particularly when pre-training distribution differs from evaluation distribution
- Theoretical convergence guarantees established under regularity assumptions including linear parameterization
- Effective fine-tuning of neural policies in complex, high-dimensional environments (HalfCheetah, Humanoid)
- Practical approach proposed for mitigating catastrophic forgetting in pre-trained policies

## Why This Works (Mechanism)
AMF works by actively selecting demonstrations that maximize expected information gain about the expert policy. Rather than uniformly sampling tasks for demonstration, the algorithm maintains uncertainty estimates and strategically chooses demonstrations that reduce this uncertainty most effectively. This information-theoretic approach ensures that each demonstration provides maximal value for learning across all tasks. The method is particularly powerful when there is distribution shift between pre-training and target tasks, as it can identify and focus on the most informative regions of the task space. The catastrophic forgetting mitigation works by selectively freezing weights that are critical for pre-trained capabilities while allowing task-specific adaptation.

## Foundational Learning
- **Information Gain Maximization**: Why needed - to ensure each demonstration provides maximal learning value across tasks; Quick check - verify that selected demonstrations consistently reduce policy uncertainty
- **Multi-task Policy Representation**: Why needed - to enable simultaneous learning across multiple tasks with shared parameters; Quick check - confirm policy can represent diverse task behaviors
- **Regularity Assumptions**: Why needed - to enable theoretical convergence guarantees; Quick check - verify assumptions hold approximately in practical implementations
- **Catastrophic Forgetting**: Why needed - to preserve pre-trained capabilities while adapting to new tasks; Quick check - measure performance degradation on pre-trained tasks after fine-tuning
- **Uncertainty Estimation**: Why needed - to quantify what the policy doesn't know and guide demonstration selection; Quick check - validate uncertainty estimates correlate with actual performance gaps
- **Distribution Shift Handling**: Why needed - to perform well when target tasks differ from pre-training distribution; Quick check - test performance under controlled distribution mismatches

## Architecture Onboarding

Component map: Expert policy -> Uncertainty estimator -> Demonstration selector -> Fine-tuning module -> Multi-task policy

Critical path: Uncertainty estimation about current policy → Demonstration selection based on information gain → Expert demonstration acquisition → Fine-tuning with selective weight updates → Improved multi-task policy

Design tradeoffs: The algorithm trades computational complexity in uncertainty estimation and information gain calculation against improved sample efficiency. Linear parameterization assumptions enable theoretical guarantees but may limit applicability to complex neural architectures. The catastrophic forgetting mitigation trades some task-specific adaptation capacity for preservation of pre-trained capabilities.

Failure signatures: Poor performance when task distributions have significant overlap (information gain maximization less effective), failure to converge when regularity assumptions are violated, catastrophic forgetting if weight freezing strategy is too aggressive or too lenient.

First experiments:
1. Verify uncertainty estimates correlate with actual performance gaps by comparing predicted vs actual improvement from demonstrations
2. Test demonstration selection on a simple multi-task problem where optimal selection strategy is known
3. Evaluate catastrophic forgetting mitigation by measuring performance on pre-trained tasks before and after fine-tuning on new tasks

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Theoretical analysis relies on strong regularity assumptions including linear parameterization that may not hold for complex neural networks
- Empirical validation primarily in simulated environments with precisely controlled task distributions
- Performance gains most pronounced under distribution shift, unclear if benefits persist with significant task distribution overlap
- Catastrophic forgetting mitigation lacks rigorous theoretical justification for the proposed weight freezing strategy

## Confidence

**Theoretical claims**: High confidence in mathematical derivations under stated assumptions, but Medium confidence in practical applicability given the gap between linear models and deep neural networks.

**Empirical claims**: Medium confidence in reported performance improvements, though results are limited to specific locomotion tasks and may not transfer to more diverse multi-task settings.

**Catastrophic forgetting solution**: Low confidence in theoretical grounding, though empirical results suggest the approach is at least practically useful.

## Next Checks

1. Test AMF's performance when task distributions overlap significantly to assess whether information gain maximization remains beneficial in less distinguishable task scenarios.

2. Evaluate the catastrophic forgetting mitigation strategy with varying levels of task similarity and demonstrate whether it consistently preserves pre-trained capabilities.

3. Implement AMF with non-linear function approximation (e.g., deep networks with multiple hidden layers) and verify whether theoretical convergence guarantees still hold approximately or if new failure modes emerge.