---
ver: rpa2
title: Learning More Expressive General Policies for Classical Planning Domains
arxiv_id: '2403.11734'
source_url: https://arxiv.org/abs/2403.11734
tags:
- r-gnn
- gnns
- planning
- learning
- atoms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of learning expressive general
  policies in classical planning domains using GNNs. The authors introduce R-GNN[t],
  a parameterized extension of Relational GNNs, designed to overcome the expressive
  limitations of standard GNNs (C2 logic) by approximating 3-GNNs (C3 logic).
---

# Learning More Expressive General Policies for Classical Planning Domains

## Quick Facts
- arXiv ID: 2403.11734
- Source URL: https://arxiv.org/abs/2403.11734
- Authors: Simon Ståhlberg; Blai Bonet; Hector Geffner
- Reference count: 14
- One-line primary result: R-GNN[t] with t=1 achieves significant performance gains over baselines in coverage and plan quality for planning domains requiring C3 expressiveness.

## Executive Summary
This paper addresses the challenge of learning expressive general policies in classical planning domains using Graph Neural Networks (GNNs). Standard GNNs are limited to C2 logic, which is insufficient for many planning domains that require C3 expressiveness. The authors introduce R-GNN[t], a parameterized extension of Relational GNNs that approximates 3-GNNs (C3 logic) while maintaining computational efficiency. By transforming input atoms to include object pairs and compositions controlled by parameter t, R-GNN[t] achieves the necessary expressive power without cubic space complexity.

## Method Summary
R-GNN[t] extends standard R-GNNs by transforming input atoms to include object pairs and composition atoms △. The parameter t controls the maximum number of sequential compositions that can be captured, with t=1 providing the necessary C3 features for several planning domains. The architecture uses quadratic space for embeddings and employs message passing with MLPs for predicates. Models are trained using supervised learning to minimize the difference between predicted and optimal value functions, with experiments conducted on domains like Blocks, Grid, Logistics, and Rovers using coverage and plan length as metrics.

## Key Results
- R-GNN[1] significantly outperforms plain R-GNNs, Edge Transformers, and R-GNN2 in both coverage and plan quality across multiple domains
- The architecture achieves C3 expressiveness with only quadratic space complexity by using controlled composition atoms
- R-GNN[t] with small t values (particularly t=1) captures essential C3 features while maintaining practical computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: R-GNN[t] achieves C3 expressiveness without cubic space by embedding object pairs and using controlled composition atoms △.
- Mechanism: The input transformation At(S) replaces atoms with their pair-wise versions and adds △ atoms that represent compositions of object pairs. These △ atoms are added in a t-controlled way to avoid exponential blowup while capturing the necessary relational compositions.
- Core assumption: The necessary C3 features for the planning domains can be expressed as compositions of binary relations that can be encoded via the △ atoms.
- Evidence anchors:
  - [abstract] "When t=∞, R-GNN[t] approximates 3-GNNs over graphs, but using only quadratic space for embeddings."
  - [section] "For t > 0, the set of atoms At(S) extends A0(S) with atoms for a new ternary predicate △ as: At(S) =A0(S) ∪ ∆t(S)"
- Break condition: If the planning domain requires more complex compositions than binary relations, or if the t parameter is not tuned correctly, the necessary C3 features may not be captured.

### Mechanism 2
- Claim: R-GNN[t] with t=1 can capture the necessary C3 features for several planning domains while requiring only quadratic space for embeddings.
- Mechanism: By setting t=1, the R-GNN[t] architecture adds △ atoms that represent compositions of object pairs in a controlled manner. This allows the network to capture the necessary relational compositions without the cubic space overhead of 3-GNNs.
- Core assumption: The planning domains studied in the paper require C3 expressiveness that can be achieved with t=1.
- Evidence anchors:
  - [abstract] "For lower values of t, such as t=1 and t=2, R-GNN[t] achieves a weaker approximation by exchanging fewer messages, yet interestingly, often yield the expressiveness required in several planning domains."
  - [section] "Crucially, for lower values of t, such as t = 1 and t = 2, R-GNN[t]'s message passing runs in quadratic time in general while capturing the C3 features that are essential in several planning domains."
- Break condition: If the planning domain requires more complex compositions than can be captured with t=1, or if the domain has a different structure than the ones studied.

### Mechanism 3
- Claim: R-GNN[t] improves both coverage and plan quality compared to baselines like R-GNN, Edge Transformers, and R-GNN2.
- Mechanism: By extending the expressive power of R-GNNs beyond C2, R-GNN[t] can learn policies that require more complex logical features. This leads to better generalization and improved performance on planning domains that require C3 expressiveness.
- Core assumption: The baselines (R-GNN, Edge Transformers, R-GNN2) have limitations in expressive power that prevent them from learning optimal policies for C3 domains.
- Evidence anchors:
  - [abstract] "Experimental results illustrate the clear performance gains of R-GNN[1] over the plain R-GNNs, and also over Edge Transformers that also approximate 3-GNNs."
  - [section] "Our experiments demonstrate that R-GNN[t], even with small values of t, is practically feasible and significantly improves both the coverage and the quality of the learned general plans when compared to four baselines."
- Break condition: If the planning domain does not require C3 expressiveness, or if the baselines are improved to handle C3 features.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: R-GNN[t] is based on GNNs, so understanding how GNNs work is crucial for understanding the proposed architecture.
  - Quick check question: What is the main limitation of GNNs in terms of expressive power, and how does it relate to the C2 logic?

- Concept: First-Order Logic and Description Logics
  - Why needed here: The paper discusses the expressive power of different logics (C2, C3) and how they relate to the capabilities of different neural network architectures.
  - Quick check question: What is the difference between C2 and C3 logic, and why is C3 expressiveness important for certain planning domains?

- Concept: Relational Structures and Planning Domains
  - Why needed here: The paper focuses on learning general policies for classical planning domains, which are represented as relational structures.
  - Quick check question: How are planning domains typically represented, and what are the key components of a planning domain?

## Architecture Onboarding

- Component map:
  Input layer (At(S) with object pairs and △ atoms) -> Embedding layer (object pair embeddings) -> Message passing layer (R-GNN with MLPs) -> Output layer (general value function V(S))

- Critical path:
  1. Transform input atoms S into At(S) with object pairs and composition atoms
  2. Initialize object pair embeddings
  3. Perform message passing using R-GNN architecture
  4. Compute general value function V(S) from final embeddings

- Design tradeoffs:
  - Expressiveness vs. computational efficiency: Higher t values provide more expressiveness but increase computational cost
  - Input transformation complexity: The transformation from S to At(S) adds complexity but is necessary for capturing C3 features
  - Embedding dimension: Higher dimensions may improve performance but increase computational cost and risk overfitting

- Failure signatures:
  - Poor coverage on C3 domains: May indicate insufficient expressiveness or incorrect t value
  - Long plan lengths: May indicate suboptimal policies or insufficient training
  - High variance in value function predictions: May indicate overfitting or insufficient generalization

- First 3 experiments:
  1. Test R-GNN[t] on a simple C3 domain (e.g., Navig-xy) with different t values to find the optimal balance between expressiveness and efficiency
  2. Compare R-GNN[t] performance to baselines (R-GNN, Edge Transformers, R-GNN2) on a range of C2 and C3 domains
  3. Analyze the impact of embedding dimension and number of layers on R-GNN[t] performance and generalization ability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the R-GNN[t] architecture be extended to handle higher-order compositions beyond C3 logic while maintaining practical computational efficiency?
- Basis in paper: [explicit] The authors discuss how R-GNN[t] with t=1 achieves the necessary C3 features in several domains, and mention that the parameter t controls the maximum number of sequential compositions that can be captured. They also note that the full power of C3 is sufficient but not necessary for many planning domains.
- Why unresolved: The paper focuses on demonstrating the effectiveness of R-GNN[t] for C3 expressiveness, but does not explore its potential for handling higher-order compositions. The authors mention that there is room between C2 and C3, suggesting that exploring higher-order compositions could be beneficial.
- What evidence would resolve it: Experimental results comparing the performance of R-GNN[t] with different values of t (e.g., t=2, t=3) on domains requiring higher-order compositions. Theoretical analysis of the expressive power of R-GNN[t] for higher-order compositions.

### Open Question 2
- Question: How does the choice of the aggregation function (e.g., smooth maximum vs. softmax) in R-GNN[t] impact its expressive power and performance compared to other architectures like Edge Transformers?
- Basis in paper: [explicit] The authors mention that R-GNN2 surpasses ET in all domains except Blocks-s, attributing this to the aggregation function. They note that the softmax in the attention mechanism in ETs depends on the number of objects, leading to value magnitudes that differ from those encountered during training.
- Why unresolved: While the authors provide some insights into the impact of the aggregation function, they do not conduct a systematic comparison of different aggregation functions in R-GNN[t] and their effects on expressive power and performance.
- What evidence would resolve it: Experiments comparing the performance of R-GNN[t] with different aggregation functions (e.g., smooth maximum, softmax, sum) on various domains. Theoretical analysis of how different aggregation functions affect the expressive power of R-GNN[t].

### Open Question 3
- Question: Can the R-GNN[t] architecture be adapted to handle domains with ternary or higher-arity predicates, which are currently considered unsuitable for the baseline 2-GNN?
- Basis in paper: [explicit] The authors mention that the baseline 2-GNN is unsuitable for domains with ternary predicates, such as Vacuum and Visitall. They suggest that approaches that aim to extend representations with new unary predicates by considering cycles in the state graphs may yield an acceptable tradeoff between expressivity and efficiency.
- Why unresolved: The paper focuses on demonstrating the effectiveness of R-GNN[t] for binary predicates, but does not explore its potential for handling higher-arity predicates. The authors mention the possibility of using cycles in state graphs, but do not provide a concrete solution.
- What evidence would resolve it: Experimental results comparing the performance of R-GNN[t] and its potential adaptations on domains with ternary or higher-arity predicates. Theoretical analysis of how R-GNN[t] can be extended to handle higher-arity predicates while maintaining computational efficiency.

## Limitations

- Limited empirical validation of R-GNN[t]'s expressive power beyond specific domains tested in the paper
- Computational complexity analysis is asymptotic, with practical scalability to larger domains not thoroughly explored
- Potential biases introduced by input transformation and impact of different object orderings on learned policies not discussed

## Confidence

- High Confidence: The experimental results showing R-GNN[1] outperforming baselines on coverage and plan length in the tested domains
- Medium Confidence: The theoretical claims about R-GNN[t]'s ability to approximate C3 expressiveness, as they are supported by both theory and experiments but rely on specific domain structures
- Low Confidence: The generalization of R-GNN[t]'s performance to domains not tested in the paper, as the results are limited to a specific set of planning domains

## Next Checks

1. **Expressive Power Validation:** Conduct experiments on additional C3 domains not included in the original study to verify that R-GNN[1] consistently captures the necessary C3 features and outperforms baselines.

2. **Scalability Analysis:** Test R-GNN[t] on larger planning domains with more objects and complex structures to assess its practical scalability and computational efficiency compared to the theoretical claims.

3. **Robustness to Input Variations:** Investigate the impact of different object orderings and input transformation variations on R-GNN[t]'s performance to ensure the learned policies are not sensitive to these factors.