---
ver: rpa2
title: 'FORESEE: Multimodal and Multi-view Representation Learning for Robust Prediction
  of Cancer Survival'
arxiv_id: '2405.07702'
source_url: https://arxiv.org/abs/2405.07702
tags:
- data
- features
- multimodal
- feature
- survival
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes FORESEE, a multimodal framework for robust
  cancer survival prediction. The method addresses challenges of heterogeneous data,
  missing modalities, and noise by employing three core components: a cross-fusion
  transformer (CFT) that integrates features across cellular, tissue, and tumor heterogeneity
  scales in pathology images; a hybrid attention encoder (HAE) that extracts local
  and global features from molecular data while reducing noise; and an asymmetrically
  masked triplet masked autoencoder (TriMAE) that reconstructs missing intra-modal
  information.'
---

# FORESEE: Multimodal and Multi-view Representation Learning for Robust Prediction of Cancer Survival

## Quick Facts
- arXiv ID: 2405.07702
- Source URL: https://arxiv.org/abs/2405.07702
- Reference count: 16
- State-of-the-art C-index scores, with improvements up to 3.7% over previous methods

## Executive Summary
FORESEE addresses the challenge of cancer survival prediction using heterogeneous multimodal data (pathology images, molecular data, EHR) with missing information and noise. The framework integrates three core components: a cross-fusion transformer that extracts features across cellular, tissue, and tumor heterogeneity scales in pathology images; a hybrid attention encoder that captures local and global features from molecular data while reducing noise; and an asymmetrically masked triplet masked autoencoder that reconstructs missing intra-modal information. Extensive experiments on four cancer datasets demonstrate FORESEE achieves state-of-the-art performance with robust results even when data is missing.

## Method Summary
FORESEE is a multimodal framework that integrates pathology images, molecular data, and EHR data for cancer survival prediction. The method employs a cross-fusion transformer (CFT) to extract multi-scale features from whole slide images at cellular, tissue, and tumor heterogeneity levels. A hybrid attention encoder (HAE) processes molecular data using denoising contextual attention and channel attention mechanisms. An asymmetrically masked triplet masked autoencoder (TriMAE) reconstructs missing intra-modal information through parallel transformers and cross-attention mechanisms. The framework is trained end-to-end using Cox loss for survival prediction and MAE loss for reconstruction, achieving robust performance across four cancer datasets.

## Key Results
- FORESEE achieves state-of-the-art C-index scores with improvements up to 3.7% over previous methods
- The framework demonstrates robust performance even with missing data modalities
- Ablation studies confirm the effectiveness of each component in enhancing survival prediction accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-fusion transformer (CFT) improves survival prediction by learning contextual relationships across cellular, tissue, and tumor heterogeneity scales in pathology images.
- Mechanism: CFT segments WSI into patches at varying magnification levels, constructs spatial topological graphs, and applies graph neural networks followed by transformer encoders to capture cross-scale contextual correlations through two fusion schemes.
- Core assumption: Different magnification levels capture semantically distinct but complementary features that can be effectively fused to improve representation.
- Evidence anchors:
  - [abstract] "the cross-fusion transformer effectively utilizes features at the cellular level, tissue level, and tumor heterogeneity level to correlate prognosis through a cross-scale feature cross-fusion method"
  - [section] "To uncover cross-scale contextual correlations among distinct regions in histopathology images corresponding to different fields of view, we propose to use two types of cross-fusion schemes to blend features from different fields of view"
  - [corpus] Weak - no direct corpus evidence for this specific cross-fusion transformer approach, though similar graph-based pathology image analysis exists in related works
- Break condition: If magnification levels don't capture semantically distinct information or fusion fails to preserve important features, the cross-scale benefit disappears.

### Mechanism 2
- Claim: Hybrid attention encoder (HAE) improves molecular data feature extraction by combining denoising contextual attention with channel attention to capture local and global features while reducing noise.
- Mechanism: HAE applies discrete wavelet transform for denoising, uses LSTM for long-term dependencies, employs self-attention for local features, and combines with channel attention for global feature weighting.
- Core assumption: Molecular data contains both local detail features and global contextual relationships that can be better extracted through combined attention mechanisms.
- Evidence anchors:
  - [abstract] "the hybrid attention encoder (HAE) uses the denoising contextual attention module to obtain the contextual relationship features and local detail features of the molecular data. HAE's channel attention module obtains global features of molecular data"
  - [section] "To obtain local attention features of molecular data, we propose using a self-attention mechanism to extract these features. CNA introduces global information from molecular data to calculate the weight of CNA"
  - [corpus] Weak - while attention mechanisms and denoising are common in multimodal learning, the specific combination for molecular data in survival prediction isn't well-represented in the corpus
- Break condition: If noise levels are too high or attention mechanisms fail to capture meaningful patterns, the improvement over simpler methods diminishes.

### Mechanism 3
- Claim: Asymmetrically masked triplet masked autoencoder (TriMAE) improves robustness by reconstructing missing intra-modal information through asymmetric masking and attention mechanisms.
- Mechanism: TriMAE applies masking to different modalities separately, uses parallel transformers for visible features, and lightweight decoders with cross-attention to reconstruct missing features from latent representations.
- Core assumption: Missing information can be effectively reconstructed using contextual relationships and semantic information from visible features.
- Evidence anchors:
  - [abstract] "to address the issue of missing information within modalities, we propose an asymmetrically masked triplet masked autoencoder to reconstruct lost information within modalities"
  - [section] "The masking rate is set to be over 80%, making it challenging for features to reconstruct the masked pixels through surrounding features. This compels the encoder to learn meaningful features"
  - [corpus] Weak - while masked autoencoders exist, the specific triplet architecture with asymmetric masking for multimodal cancer survival prediction is not well-represented
- Break condition: If masking rate is too high or reconstruction fails to capture essential features, the model performance degrades significantly.

## Foundational Learning

- Concept: Graph neural networks for spatial topology representation
  - Why needed here: WSI segmentation produces patches that need spatial relationships captured for meaningful feature extraction
  - Quick check question: How does a graph neural network differ from a standard convolutional neural network in handling irregular spatial relationships?

- Concept: Multi-scale feature fusion in computer vision
  - Why needed here: Different magnification levels capture complementary information at cellular, tissue, and tumor heterogeneity scales
  - Quick check question: What are the trade-offs between using features from multiple scales versus using the most informative single scale?

- Concept: Masked autoencoders for robust representation learning
  - Why needed here: Medical data often has missing modalities or incomplete information that needs to be reconstructed
  - Quick check question: How does asymmetric masking differ from standard masking in masked autoencoders?

## Architecture Onboarding

- Component map: WSI patches → graph construction → CFT feature extraction → molecular data preprocessing → HAE feature extraction → TriMAE reconstruction → feature fusion → survival prediction
- Critical path: WSI patches → graph construction → CFT feature extraction → molecular data preprocessing → HAE feature extraction → TriMAE reconstruction → feature fusion → survival prediction
- Design tradeoffs: Complex multi-component architecture vs simpler baselines; computational cost of multiple attention mechanisms vs accuracy gains; masking rate vs reconstruction quality
- Failure signatures: Poor C-index scores on validation; unstable training with high variance; performance degradation with missing data; overfitting on small datasets
- First 3 experiments:
  1. Baseline comparison: Run FORESEE without TriMAE to measure impact of missing data reconstruction
  2. Component ablation: Remove CFT and test with only HAE + TriMAE to isolate pathology image contribution
  3. Masking sensitivity: Vary TriMAE masking rates (50%, 80%, 95%) to find optimal reconstruction balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed TriMAE approach compare to other missing data imputation methods in multimodal survival prediction?
- Basis in paper: [explicit] The paper introduces TriMAE as a method to reconstruct missing information within modalities, stating it outperforms simple MAE, ConvMAE, FCMAE, DMAE, PUT, and MAGE strategies in ablation experiments.
- Why unresolved: While the paper demonstrates TriMAE's superiority over other imputation methods in their specific experimental setup, it doesn't explore the performance of other imputation techniques that are commonly used in the broader literature for multimodal data, such as matrix factorization or generative adversarial networks.
- What evidence would resolve it: A comprehensive comparison of TriMAE with a wider range of missing data imputation methods, using diverse datasets and evaluation metrics, would clarify its relative performance and generalizability.

### Open Question 2
- Question: How does the performance of FORESEE vary with different levels of missing data within modalities?
- Basis in paper: [inferred] The paper acknowledges the issue of missing data within modalities and proposes TriMAE to address it, but does not explicitly investigate how the model's performance changes as the proportion of missing data increases.
- Why unresolved: The paper's ablation experiments focus on the effectiveness of TriMAE compared to other imputation methods, but don't explore the relationship between the amount of missing data and the model's predictive accuracy.
- What evidence would resolve it: Experiments that systematically vary the proportion of missing data within modalities and evaluate the resulting performance of FORESEE would provide insights into its robustness and limitations in real-world scenarios with varying levels of data incompleteness.

### Open Question 3
- Question: How does the cross-fusion transformer (CFT) in FORESEE compare to other feature fusion methods in multimodal survival prediction?
- Basis in paper: [explicit] The paper introduces CFT as a method to integrate features across cellular, tissue, and tumor heterogeneity scales in pathology images, stating it enhances the ability of pathological image feature representation. However, it doesn't compare CFT to other fusion methods like late fusion or early fusion.
- Why unresolved: While the paper demonstrates the effectiveness of CFT within the FORESEE framework, it doesn't provide a direct comparison with other feature fusion techniques that are commonly used in multimodal analysis.
- What evidence would resolve it: A comparison of CFT with other feature fusion methods, using the same multimodal data and survival prediction task, would clarify its relative strengths and weaknesses in capturing cross-scale information and improving predictive performance.

## Limitations

- The cross-fusion transformer's effectiveness relies heavily on the assumption that multi-scale pathology features capture semantically distinct information, but the paper lacks direct ablation studies isolating the impact of individual magnification levels.
- The TriMAE component's asymmetric masking approach, while theoretically sound, operates at an 80% masking rate that may be overly aggressive for some clinical datasets, potentially leading to reconstruction artifacts.
- The paper demonstrates strong quantitative results but lacks independent validation on external datasets beyond the four TCGA cancer types used in the study.

## Confidence

- FORESEE's state-of-the-art performance claims: **Medium** - Strong quantitative results but limited independent validation
- Mechanism 1 (CFT cross-scale fusion): **Medium** - Logical framework but minimal ablation evidence
- Mechanism 2 (HAE attention modules): **Medium** - Well-established attention principles but novel application to molecular data
- Mechanism 3 (TriMAE reconstruction): **Medium** - Innovative approach but high masking rate raises concerns

## Next Checks

1. Replicate FORESEE performance on an independent multi-modal cancer dataset not used in the original study to verify generalizability
2. Conduct systematic ablation studies varying TriMAE masking rates (40%, 60%, 80%, 95%) to identify optimal balance between reconstruction and information preservation
3. Compare FORESEE against a simplified baseline using single-scale pathology features to quantify the actual contribution of multi-scale fusion versus increased model complexity