---
ver: rpa2
title: 'Co-Neighbor Encoding Schema: A Light-cost Structure Encoding Method for Dynamic
  Link Prediction'
arxiv_id: '2407.20871'
source_url: https://arxiv.org/abs/2407.20871
tags:
- encoding
- node
- memory
- graph
- dynamic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficient structure encoding
  for dynamic link prediction in temporal graphs. The proposed method, Co-Neighbor
  Encoding Schema (CNES), uses hashtable-based memory to compress the adjacency matrix
  and parallel vector computation to efficiently generate co-neighbor encoding for
  subgraphs.
---

# Co-Neighbor Encoding Schema: A Light-cost Structure Encoding Method for Dynamic Link Prediction

## Quick Facts
- arXiv ID: 2407.20871
- Source URL: https://arxiv.org/abs/2407.20871
- Reference count: 40
- Primary result: Outperforms baselines on 10/13 datasets with up to 99.09% AP on Wikipedia

## Executive Summary
This paper addresses the challenge of efficient structure encoding for dynamic link prediction in temporal graphs. The proposed Co-Neighbor Encoding Schema (CNES) introduces a hashtable-based memory system that compresses adjacency matrices and enables GPU-accelerated parallel co-neighbor encoding computation. By encoding larger neighborhoods than existing methods and introducing temporal-diverse memory for capturing both recent and historical structural patterns, CNES achieves superior performance on multiple benchmark datasets while maintaining computational efficiency.

## Method Summary
CNES uses hashtable-based memory to compress adjacency matrices, storing node neighbors in fixed-size arrays where node IDs are hashed into positions. Co-neighbor counts are computed through parallel vector comparison on GPU, avoiding expensive subgraph sampling. The method introduces Temporal-Diverse Memory with separate short and long hashtables to capture recent high-frequency interactions and historical structural patterns respectively. Structure encoding is generated by counting triangles between large neighborhoods (up to 1 + l_s + l_s × M nodes) in parallel, providing richer information than sampled subsets while maintaining efficiency.

## Key Results
- Achieves 99.09% average precision on Wikipedia dataset and 99.22% on Reddit dataset
- Outperforms existing baselines on 10 out of 13 tested datasets
- Demonstrates superior computational efficiency compared to CPU-based neighbor sampling methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hashtable-based memory compresses adjacency matrices and enables GPU-accelerated parallel co-neighbor encoding computation
- Mechanism: Each node stores a fixed-size hashtable where node IDs are hashed into array positions. Co-neighbor counts between two nodes are computed by element-wise comparison of their hashtables in parallel on GPU, avoiding irregular subgraph sampling
- Core assumption: Hash collisions do not significantly distort co-neighbor counts; hashtable size M is large enough to keep collision probability low
- Evidence anchors: Abstract mentions hashtable-based memory for efficient structure feature construction; methodology section describes hash table implementation and co-neighbor computation
- Break condition: High hash collision rate degrades accuracy; if M is too small, multiple distinct neighbors map to same position, causing undercounting

### Mechanism 2
- Claim: Temporal-diverse memory (short vs. long hashtables) captures both recent high-frequency interactions and historical structural patterns
- Mechanism: Short memory (small hashtable) updates frequently, encoding recent neighbor co-occurrences; long memory (larger hashtable) preserves older interactions. During encoding, both memories are queried and fused, providing complementary temporal signals
- Core assumption: Different time intervals encode different types of structural information; recent edges provide short-term triangle patterns, older edges provide long-term structural roles
- Evidence anchors: Abstract mentions Temporal-Diverse Memory for long-term and short-term structure encoding; methodology section describes long-short memory with two hashtables
- Break condition: If interaction dynamics are uniform over time, temporal diversity adds little value and may increase noise

### Mechanism 3
- Claim: By encoding large neighborhoods (up to 1 + l_s + l_s × M nodes) instead of sampled subsets, CNES reduces information loss from truncation
- Mechanism: First-hop neighbor sequence of length l_s is extracted; co-neighbor counts are computed with all neighbors stored in hashtables (size M each), so each neighbor is compared with up to M entries in parallel. This yields counts for many triangles without explicit sampling
- Core assumption: Storing all neighbors in hashtables and counting co-occurrences yields richer structural encoding than sampled subgraphs, provided memory and compute budget allow
- Evidence anchors: Abstract mentions hashtable-based memory for efficient structure feature construction; methodology section states method generates structure encoding by counting triangles between larger node sets than existing models
- Break condition: If hashtable size M is too small relative to node degree, collisions dominate and encoding quality degrades

## Foundational Learning

- Concept: Continuous-time dynamic graphs (CTDG) and temporal link prediction
  - Why needed here: CNES is designed specifically for CTDG link prediction, where edges arrive with timestamps and models must predict future links
  - Quick check question: In a CTDG, how do you distinguish an edge that just arrived from one that arrived long ago when building the neighborhood for a query?

- Concept: Graph neural network message passing and memory-based node representations
  - Why needed here: CNES extends memory-based dynamic GNNs (like TGN) but replaces message-passing with hashtable-based neighbor counting; understanding this baseline is key to grasping the innovation
  - Quick check question: In TGN, what is stored in the node memory and how is it updated when a new edge arrives?

- Concept: Subgraph sampling vs. full neighborhood encoding trade-off
  - Why needed here: Existing CTDG models sample k-hop subgraphs to limit cost; CNES argues that hashtable-based encoding of larger neighborhoods is both feasible and more expressive
  - Quick check question: Why might sampling a subset of neighbors lead to information loss in link prediction?

## Architecture Onboarding

- Component map: Interaction arrival → Sequence extraction → Co-neighbor computation → Feature fusion → Prediction → Memory update
- Critical path: Interaction arrival → sequence extraction → co-neighbor computation → feature fusion → prediction → memory update
- Design tradeoffs:
  - Hashtable size M vs. collision risk vs. memory cost
  - Short vs. long memory sizes M_s, M_l vs. temporal resolution vs. storage
  - l_s length vs. neighbor coverage vs. sequence noise
  - Parallel GPU co-neighbor counting vs. irregular CPU sampling
- Failure signatures:
  - High hash collision rate → undercounted co-neighbors → degraded AP/AUC
  - l_s too short → missing long-range structural signals
  - l_s too long → noisy sequences dominate encoding
  - Memory update lag → stale co-neighbor counts → poor predictions on fast-evolving graphs
- First 3 experiments:
  1. Verify hashtable collision impact: Run with M=16, 32, 64 on a small dataset; measure co-neighbor accuracy vs. ground truth
  2. Ablation of temporal memory: Disable short memory, keep long; compare AP on datasets with high interaction frequency (e.g., Reddit) vs. low frequency (e.g., UN Vote)
  3. Sequence length sensitivity: Sweep l_s=4,10,20,32 on MOOC; plot AP/AUC to find optimal trade-off point

## Open Questions the Paper Calls Out

- Question: How does the Temporal-Diverse Memory handle cases where the neighbor storage of each node eventually becomes almost identical, as in the Social Evo. dataset?
  - Basis in paper: The paper mentions that in datasets with very few nodes like Social Evo., the neighbor storage of each node will eventually be almost the same, rendering co-neighbor encoding useless
  - Why unresolved: The paper introduces Temporal-Diverse Memory as a solution but does not provide empirical results or detailed explanation of how it specifically addresses this issue
  - What evidence would resolve it: Experimental results showing the performance improvement of Temporal-Diverse Memory in datasets with few nodes, or a detailed explanation of how it maintains diversity in such cases

- Question: What is the optimal balance between the long memory and short memory sizes in the Temporal-Diverse Memory for different types of datasets?
  - Basis in paper: The paper introduces long memory and short memory but does not provide guidance on how to choose their sizes for optimal performance across different datasets
  - Why unresolved: The paper sets the long memory size to 64 and short memory size to 16 but does not discuss how these values were determined or how they might vary for different types of graphs
  - What evidence would resolve it: A sensitivity analysis showing the impact of different long and short memory size ratios on various types of datasets, or a principled method for determining these sizes based on graph characteristics

## Limitations
- Limited empirical validation of hashtable collision impact on downstream accuracy
- Insufficient evidence for claimed benefits of dual-memory temporal encoding
- No discussion of scalability to very large graphs with millions or billions of nodes

## Confidence
- Mechanism 1 (hashtable compression and parallel computation): Medium - The concept is plausible but collision handling is underspecified
- Mechanism 2 (temporal-diverse memory): Low - Limited evidence for the claimed benefits of dual-memory temporal encoding
- Mechanism 3 (full neighborhood encoding advantage): Medium - The claim is reasonable but not empirically validated against sampling-based alternatives

## Next Checks
1. Collision sensitivity analysis: Systematically vary hashtable sizes (M=16, 32, 64) on small datasets and measure degradation in co-neighbor accuracy vs. ground truth
2. Temporal memory ablation: Disable short memory component and compare performance on high-frequency (Reddit) vs. low-frequency (UN Vote) datasets to validate temporal diversity claims
3. Sequence length optimization: Sweep l_s values on MOOC dataset and plot AP/AUC to identify optimal trade-off between coverage and noise