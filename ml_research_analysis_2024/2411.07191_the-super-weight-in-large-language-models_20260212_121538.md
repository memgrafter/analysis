---
ver: rpa2
title: The Super Weight in Large Language Models
arxiv_id: '2411.07191'
source_url: https://arxiv.org/abs/2411.07191
tags:
- super
- weight
- weights
- quantization
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of "super weights" in large language
  models (LLMs) - a small set of outlier parameters that are disproportionately critical
  to model quality. The authors demonstrate that pruning as few as a single super
  weight can catastrophically degrade LLM performance, increasing perplexity by 3
  orders of magnitude and reducing zero-shot accuracy to random guessing.
---

# The Super Weight in Large Language Models

## Quick Facts
- **arXiv ID:** 2411.07191
- **Source URL:** https://arxiv.org/abs/2411.07191
- **Reference count:** 35
- **Primary result:** Pruning a single "super weight" can catastrophically degrade LLM performance by 3 orders of magnitude in perplexity

## Executive Summary
This paper identifies "super weights" - a small set of outlier parameters in large language models that are disproportionately critical to model quality. The authors demonstrate that pruning even a single super weight can destroy an LLM's ability to generate text, while pruning thousands of other large outliers has minimal impact. They develop a data-free method to identify these super weights using a single forward pass, and show these weights induce corresponding "super activations" that persist through skip connections. By preserving super outliers during quantization, they significantly improve simple round-to-nearest quantization methods, achieving results competitive with more complex state-of-the-art approaches while maintaining hardware friendliness.

## Method Summary
The paper proposes a data-free method to identify super weights using a single forward pass by detecting maximum-magnitude activation spikes in down_proj inputs and outputs across all layers. For quantization, they develop two techniques: (1) For activation quantization, replace the super activation with median value, quantize/de-quantize, then restore the super activation in FP16; (2) For weight quantization, clip outliers using z-score thresholding, quantize/de-quantize clipped weights, then restore the super weight in FP16. The method is evaluated on zero-shot benchmarks and perplexity scores using 500 examples from Wikitext-2 for hyperparameter tuning.

## Key Results
- Pruning a single super weight increases perplexity by 3 orders of magnitude and reduces zero-shot accuracy to random guessing
- Super weights are found in early layers across model families and sizes
- Super weight-aware quantization improves round-to-nearest methods to be competitive with state-of-the-art approaches
- The method successfully identifies super weights in multiple Llama models and GPT-2

## Why This Works (Mechanism)

### Mechanism 1
Super weights create disproportionately large activation spikes that propagate through skip connections. The super weight, located in the down projection matrix, amplifies input activation outliers via the Hadamard product with gate and up projection outputs, creating super activations that persist through the network. This requires the super weight and input activation outlier to co-occur at the same channel dimension.

### Mechanism 2
Super weights suppress stopword probabilities in the output distribution. The super activation, propagated through skip connections, creates a strong signal that suppresses stopword likelihoods by several times their original probability. Skip connections are necessary for the super activation to persist and affect final logits.

### Mechanism 3
Super weights are more critical than thousands of other outlier weights combined. Pruning the single super weight destroys model quality (accuracy drops to guessing, perplexity increases by 3 orders of magnitude), while pruning thousands of other outliers has minimal effect. The super weight's importance cannot be compensated by other parameters.

## Foundational Learning

- **Outlier detection and handling in neural networks**: Understanding how extreme values in weights and activations affect model behavior is fundamental to grasping the super weight phenomenon. Quick check: What is the difference between weight outliers and activation outliers, and how do they typically arise in LLMs?

- **Quantization and its challenges**: The super weight-aware quantization method relies on understanding how quantization errors propagate and affect model quality. Quick check: How does asymmetric round-to-nearest quantization work, and why do outliers cause problems in this context?

- **Skip connections in transformer architectures**: The persistence of super activations through the network is enabled by skip connections. Quick check: How do skip connections in transformers affect the flow of information and gradient propagation?

## Architecture Onboarding

- **Component map**: Super weight (mlp.down proj weight matrix, early layers) → Hadamard product amplification → Super activation → Skip connections → Stopword suppression in logits

- **Critical path**: Super weight → Hadamard product amplification → Super activation → Skip connections → Stopword suppression in logits

- **Design tradeoffs**: The super weight provides critical functionality but makes quantization challenging. The tradeoff is between preserving this crucial parameter (and its induced super activation) versus achieving better compression through quantization.

- **Failure signatures**: If the super weight is pruned, the model produces gibberish text, accuracy drops to random guessing, and perplexity increases by 3 orders of magnitude. Stopword probabilities increase dramatically while non-stopword probabilities decrease.

- **First 3 experiments**:
  1. Identify the super weight in a known model (like Llama-7B) using the single-forward-pass method and verify its location matches Table 2
  2. Prune the super weight and observe the catastrophic degradation in text generation quality and perplexity increase
  3. Apply the super weight-aware quantization method and compare results with naive round-to-nearest quantization on the same model

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several arise from the findings:

- How do super weights interact with the optimization process during pre-training - are they discovered early, late, or continuously throughout training?
- Can the super weight identification method be extended to other model architectures beyond transformers?
- What is the relationship between super weights and model robustness to adversarial attacks or distributional shifts?
- Can super weights be intentionally engineered or optimized for rather than discovered as a byproduct of standard training?
- How do super weights relate to the lottery ticket hypothesis - are they the "winning tickets" or do they emerge from a different mechanism?

## Limitations

- The catastrophic impact of pruning super weights needs statistical validation across multiple model architectures and training runs
- The specific mechanism of super activation amplification and stopword suppression lacks direct experimental evidence
- The quantization method's effectiveness depends heavily on correctly identifying super weights with sufficient detail about parameter tuning
- The method's generalizability to different LLM architectures beyond Llama-7B remains unproven

## Confidence

- **High**: The existence of outlier weights and activations in LLMs is well-established
- **Medium**: The catastrophic impact of pruning super weights on model quality
- **Medium**: The super weight-aware quantization method's effectiveness
- **Low**: The specific mechanism of super activation amplification and stopword suppression

## Next Checks

1. **Cross-model super weight identification**: Apply the single-forward-pass method to identify super weights in multiple LLM architectures (GPT, OPT, Mistral) and verify that the catastrophic degradation pattern holds consistently across different model families.

2. **Ablation of skip connections**: Conduct controlled experiments removing skip connections in models with identified super weights to test whether super activations persist and whether stopword suppression effects are maintained.

3. **Statistical significance of outlier effects**: Perform statistical analysis comparing the effect of pruning the super weight versus thousands of other outlier weights across multiple training runs to quantify whether the super weight's impact is truly orders of magnitude greater than other outliers.