---
ver: rpa2
title: 'Code LLMs: A Taxonomy-based Survey'
arxiv_id: '2412.08291'
source_url: https://arxiv.org/abs/2412.08291
tags:
- code
- arxiv
- language
- tasks
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey presents a comprehensive taxonomy of Code Large Language
  Models (Code LLMs), categorizing the field into tasks, corpora, models, benchmarks,
  and challenges. It provides an in-depth analysis of encoder-only, encoder-decoder,
  and decoder-only models, highlighting their architectural nuances and training methodologies.
---

# Code LLMs: A Taxonomy-based Survey

## Quick Facts
- arXiv ID: 2412.08291
- Source URL: https://arxiv.org/abs/2412.08291
- Reference count: 40
- This survey presents a comprehensive taxonomy of Code Large Language Models (Code LLMs), categorizing the field into tasks, corpora, models, benchmarks, and challenges.

## Executive Summary
This survey provides a comprehensive taxonomy of Code Large Language Models (Code LLMs), systematically categorizing the field into tasks, corpora, models, benchmarks, and challenges. The authors analyze encoder-only, encoder-decoder, and decoder-only architectures, detailing their architectural nuances and training methodologies. The survey identifies current challenges such as data quality issues, resource constraints, and the need for more realistic benchmarks, while also exploring open problems including synthetic data effectiveness, multilingual performance, and scaling hypotheses for Code LLMs.

## Method Summary
The survey employs a systematic literature review approach, analyzing existing research on Code LLMs across multiple dimensions. The authors categorize the field by examining tasks, corpora, model architectures, benchmarks, and challenges through comprehensive analysis of published literature. The taxonomy is constructed by synthesizing findings from 40 referenced works, providing a structured overview of the current state of Code LLM research and development.

## Key Results
- Comprehensive taxonomy of Code LLMs covering tasks, corpora, models, benchmarks, and challenges
- Detailed analysis of three main architectural paradigms: encoder-only, encoder-decoder, and decoder-only models
- Identification of key challenges including data quality, resource constraints, and benchmark realism
- Exploration of open problems such as synthetic data effectiveness and multilingual performance

## Why This Works (Mechanism)
The survey's effectiveness stems from its systematic categorization approach that provides a structured framework for understanding the complex landscape of Code LLMs. By organizing the field into well-defined categories and subcategories, the taxonomy enables researchers and practitioners to navigate the diverse ecosystem of tasks, models, and benchmarks. The analysis of architectural nuances helps explain how different model designs impact performance across various code-related tasks.

## Foundational Learning

**Tasks in Code LLMs**: Understanding the range of tasks (code generation, code completion, code translation, etc.) is essential for contextualizing model capabilities and limitations. Quick check: Verify task definitions align with common industry use cases.

**Corpora and Data Quality**: The quality and diversity of training data directly impact model performance and generalization. Quick check: Assess whether reported corpora address bias and representation issues.

**Model Architectures**: Knowledge of encoder-only, encoder-decoder, and decoder-only architectures is crucial for understanding trade-offs in code processing. Quick check: Compare architectural choices against specific task requirements.

**Benchmarks and Evaluation**: Proper evaluation frameworks are necessary for measuring progress and comparing models objectively. Quick check: Verify benchmark comprehensiveness and relevance to real-world scenarios.

## Architecture Onboarding

**Component Map**: Tasks -> Corpora -> Models -> Benchmarks -> Challenges, where each component influences the others in a cyclical relationship.

**Critical Path**: Model architecture selection depends on task requirements, which in turn determines appropriate corpora and evaluation benchmarks, while challenges inform future architectural improvements.

**Design Tradeoffs**: Encoder-only models offer faster inference but limited generation capabilities; decoder-only models excel at generation but require more resources; encoder-decoder models provide flexibility but increase complexity.

**Failure Signatures**: Poor performance on multilingual tasks may indicate insufficient corpus diversity; inadequate benchmark coverage suggests limited real-world applicability; resource constraints during training may lead to suboptimal convergence.

**First Experiments**:
1. Compare task performance across different architectural paradigms using standardized benchmarks
2. Evaluate synthetic data contribution by training models with varying proportions of synthetic vs. real code
3. Assess multilingual performance by testing models across diverse programming languages with varying resource availability

## Open Questions the Paper Calls Out
The survey identifies several open problems including the effectiveness of synthetic data in Code LLM training, multilingual and low-resource programming language performance, and the need to re-evaluate scaling hypotheses specifically for code generation tasks. The authors also highlight the challenge of developing more realistic benchmarks that better reflect real-world coding scenarios and the potential for hybrid architectural approaches beyond the traditional three paradigms.

## Limitations
- The taxonomy may not fully capture emerging hybrid or novel architectural paradigms
- Analysis relies heavily on reported literature without independent empirical validation
- Coverage of multilingual and low-resource programming languages may be incomplete

## Confidence
- **High**: The categorization of tasks, corpora, models, and benchmarks is well-supported by existing literature
- **Medium**: The analysis of architectural nuances and training methodologies is detailed but may not account for all recent developments
- **Medium**: The identification of challenges and open problems is grounded in current research but may not fully anticipate future shifts

## Next Checks
1. Conduct an independent empirical study to validate the effectiveness of synthetic data in Code LLM training
2. Perform a systematic review of multilingual Code LLM benchmarks to assess their coverage and realism
3. Investigate the scaling hypotheses for Code LLMs by analyzing recent models with varying parameter sizes