---
ver: rpa2
title: Generalized Bayesian deep reinforcement learning
arxiv_id: '2412.11743'
source_url: https://arxiv.org/abs/2412.11743
tags:
- policy
- posterior
- learning
- distribution
- scoring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of Bayesian reinforcement learning
  with intractable likelihoods, particularly when deep generative models are used
  to represent environment dynamics. The authors propose a generalized Bayesian inference
  framework using prequential scoring rules, which allows posterior inference without
  requiring explicit likelihood functions.
---

# Generalized Bayesian deep reinforcement learning

## Quick Facts
- arXiv ID: 2412.11743
- Source URL: https://arxiv.org/abs/2412.11743
- Reference count: 35
- Key outcome: Bayesian reinforcement learning with intractable likelihoods using prequential scoring rules and expected Thompson sampling

## Executive Summary
This work addresses Bayesian reinforcement learning when environment dynamics are modeled by deep generative models with intractable likelihoods. The authors propose a generalized Bayesian inference framework using prequential scoring rules, enabling posterior inference without explicit likelihood functions. They introduce expected Thompson sampling (ETS), which averages Q-function estimates over multiple posterior samples to reduce variance and improve policy learning. Theoretical contributions include a Bernstein-von Mises-type theorem for posterior consistency and a bound on Q-function approximation error under ETS.

## Method Summary
The method combines generalized Bayesian inference with reinforcement learning. Instead of traditional likelihood-based Bayesian inference, it uses prequential scoring rules to construct a surrogate likelihood for posterior inference. Sequential Monte Carlo with gradient-based kernels (adjusted SGLD) enables scalable sampling from high-dimensional posterior distributions. Policy learning is performed via expected Thompson sampling, which averages Q-function estimates over multiple posterior samples to reduce variance compared to classical Thompson sampling.

## Key Results
- ETS achieves faster convergence and improved sample efficiency compared to classical Thompson sampling and model-free baselines
- Posterior consistency is established through a Bernstein-von Mises-type theorem for scoring rule posteriors
- The method scales effectively to continuous action spaces, demonstrated on inverted pendulum and hopper locomotion tasks
- Performance is robust to model misspecification, maintaining effectiveness even with conditional GAN-based generative models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The prequential scoring rule posterior contracts around the true model parameters as more data is observed, enabling accurate Bayesian inference even without a tractable likelihood.
- **Mechanism:** By defining a generalized posterior using the cumulative prequential score, the framework creates a surrogate for the log-likelihood. The scoring rule acts as a strictly proper loss, ensuring that the expected prequential score uniquely minimizes at the true parameter under well-specified models.
- **Core assumption:** The scoring rule is strictly proper and the Markovian assumption holds for the data-generating process.
- **Evidence anchors:**
  - [abstract]: "construct a generalized posterior (Bissiri et al., 2016) using the prequential score as a surrogate for the log-likelihood"
  - [section]: "When the scoring rule S is strictly proper, the prequential scoring rule PS (Mθ, HT ) is also (strictly) proper for the class of all Markovian conditional distributions"
  - [corpus]: **Weak** - No corpus evidence directly supports this specific mechanism; the neighboring papers focus on Bayesian deep learning temperature optimization, not scoring rule posteriors.
- **Break condition:** If the scoring rule is not strictly proper, or the Markov assumption is violated, the posterior may not concentrate around the true parameters.

### Mechanism 2
- **Claim:** Expected Thompson Sampling (ETS) reduces the variance in action-value estimation by averaging over multiple posterior samples, leading to faster policy convergence.
- **Mechanism:** Instead of drawing a single parameter sample from the posterior (as in classical Thompson Sampling), ETS averages the Q-function estimates across n posterior samples. This reduces the Monte Carlo estimation variance and stabilizes the policy improvement step.
- **Core assumption:** The posterior distribution is sufficiently concentrated, and the Q-function is Lipschitz continuous in the model parameters.
- **Evidence anchors:**
  - [abstract]: "expected Thompson sampling (ETS), an extension of classical Thompson sampling that uses multiple posterior samples to estimate the action-value function, reducing estimation variance and improving policy learning"
  - [section]: "Theorem 6... ||Q∗ − Qθ(k)µj+1||∞ ≤ γj||Q∗ − Qθ(k)µ1||∞ + Pj l=1 γj−l+1ζl(k, n)"
  - [corpus]: **Missing** - No corpus evidence directly addresses ETS or variance reduction via multiple samples in RL policy learning.
- **Break condition:** If the posterior is too diffuse or the Q-function is not smooth, the averaging may not reduce variance effectively.

### Mechanism 3
- **Claim:** Sequential Monte Carlo with gradient-based kernels scales efficiently to high-dimensional parameter spaces by combining adaptive Langevin dynamics with preconditioning.
- **Mechanism:** SMC uses a sequence of tempered intermediate distributions to bridge the prior and posterior. The forward kernel is an adjusted stochastic gradient Riemannian Langevin dynamics (adSGRLD) that leverages gradient estimates (via zeroth-order methods if needed) and preconditioning to explore the posterior efficiently.
- **Core assumption:** The gradient of the scoring rule can be estimated efficiently and the preconditioning matrix captures the geometry of the posterior.
- **Evidence anchors:**
  - [section]: "To achieve scalability in the high-dimensional parameter space of the neural networks, we use the gradient-based Markov kernels within SMC"
  - [section]: "We use an adjusted SGLD kernel as the forward kernel in SMC, which handles noisy gradient estimates of the potential"
  - [corpus]: **Weak** - Neighboring papers discuss Bayesian deep learning but do not provide evidence for SMC or gradient-based kernels in this context.
- **Break condition:** If gradients are too noisy or high-dimensional, or the preconditioner is poorly chosen, sampling may become inefficient.

## Foundational Learning

- **Concept:** Markov Decision Process (MDP)
  - **Why needed here:** The environment dynamics are modeled as an MDP, with states, actions, rewards, and transitions. All policy learning and posterior inference are built on this framework.
  - **Quick check question:** What are the components of an MDP and how do they relate to the RL problem setup in the paper?

- **Concept:** Bayesian Inference and Posterior Distributions
  - **Why needed here:** The paper uses Bayesian inference to estimate the posterior over model parameters, both in the classical likelihood-based case and the generalized scoring rule case.
  - **Quick check question:** How does the scoring rule posterior differ from the standard Bayesian posterior, and under what conditions do they converge?

- **Concept:** Policy Iteration and Q-Learning
  - **Why needed here:** The policy learning algorithms (e.g., LSPI, REINFORCE) rely on estimating and maximizing the Q-function, which measures expected returns under a given policy.
  - **Quick check question:** How does ETS integrate with policy iteration, and what advantage does it provide over classical methods?

## Architecture Onboarding

- **Component map:** Environment simulator -> Prequential scoring rule posterior inference (SMC + adSGRLD) -> Expected Thompson Sampling wrapper -> Policy learning module
- **Critical path:** 1. Collect interaction data from environment 2. Update prequential scoring rule posterior via SMC 3. Sample n parameter sets from posterior 4. Simulate trajectories and estimate Q-function for each sample 5. Average Q-function estimates (ETS) 6. Update policy and interact with environment
- **Design tradeoffs:**
  - Number of SMC particles vs. computational cost
  - Number of posterior samples (n) vs. variance reduction
  - Choice of scoring rule (e.g., energy score) vs. robustness and tractability
  - Use of gradient-based kernels vs. gradient-free methods for high-dimensional models
- **Failure signatures:**
  - Posterior does not concentrate (check scoring rule propriety, data sufficiency)
  - Policy learning stalls (check Q-function estimation, sample size n)
  - SMC samples degenerate (check effective sample size, tempering schedule)
- **First 3 experiments:**
  1. Implement the inverted pendulum task with a well-specified model; compare ETS vs. TS using LSPI.
  2. Switch to a misspecified model (conditional GAN); observe convergence and sample efficiency.
  3. Extend to continuous action space (Hopper) with REINFORCE + ETS; evaluate sample efficiency vs. model-free.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ETS-based policy learning change with the dimensionality of the parameter space in deep generative models?
- Basis in paper: [explicit] The authors note that sampling from high-dimensional posterior distributions using SMC with gradient-based kernels is a key component of their approach.
- Why unresolved: The paper does not provide systematic experiments varying the dimensionality of the model parameters, focusing instead on specific tasks with fixed architectures.
- What evidence would resolve it: Systematic experiments varying the number of layers and neurons in the generative models across tasks of increasing complexity.

### Open Question 2
- Question: What is the impact of using multimodal posteriors on the theoretical guarantees of ETS, particularly the Bernstein-von Mises-type theorem?
- Basis in paper: [explicit] The authors acknowledge that deep generative models may lead to multimodal posteriors, which violates assumptions in their BvM theorem.
- Why unresolved: The theoretical framework assumes unimodal posteriors for consistency, but this is rarely the case in practice with deep models.
- What evidence would resolve it: Extending the BvM theorem to multimodal posteriors or providing empirical evidence of how multimodality affects policy convergence.

### Open Question 3
- Question: How does the choice of scoring rule (e.g., energy score vs. log-score) affect the robustness and performance of ETS in misspecified model settings?
- Basis in paper: [explicit] The authors discuss using scoring rules as surrogates for likelihood in intractable models and mention robustness properties of energy score.
- Why unresolved: While the paper uses energy score, it does not compare different scoring rules systematically in terms of policy learning performance.
- What evidence would resolve it: Comparative experiments using different scoring rules (log-score, energy score, CRPS) across tasks with varying degrees of model misspecification.

## Limitations
- Most experiments involve relatively simple control tasks, limiting empirical robustness in highly complex, high-dimensional environments
- Theoretical guarantees rely on strict assumptions (strictly proper scoring rules, Markovian dynamics) that may not hold in practice
- Computational demands of SMC with gradient-based kernels are not fully characterized, and scalability to real-world tasks is unclear

## Confidence
- Scoring rule posterior contraction: **Medium** - Well-supported by theory but lacks direct empirical validation
- ETS variance reduction: **Medium** - Plausible and supported by theoretical bound, but benefit in complex environments uncertain
- SMC + gradient kernel scalability: **Medium** - Reasonable approach but not extensively tested across diverse model architectures

## Next Checks
1. Evaluate ETS in a high-dimensional, partially observable task to test robustness and scalability
2. Perform ablation studies on the number of SMC particles and posterior samples to quantify computational vs. performance tradeoffs
3. Test the method with misspecified scoring rules or non-Markovian dynamics to assess failure modes and limitations